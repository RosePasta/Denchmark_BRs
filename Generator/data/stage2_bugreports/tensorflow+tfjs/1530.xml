<bug id='1530' author='jessetrana' open_date='2019-04-20T04:00:09Z' closed_time='2020-01-17T14:48:33Z'>
	<summary>Possible performance drop in 1.0.x series for MobileNetV2 inference</summary>
	<description>
&lt;denchmark-h:h4&gt;1.0.0, 1.0.4, likely 1.0.x&lt;/denchmark-h&gt;

&lt;denchmark-h:h4&gt;Firefox 66.0.3 (64-bit)&lt;/denchmark-h&gt;

&lt;denchmark-h:h4&gt;Possible performance drop in 1.0.x series for MobileNetV2 inference&lt;/denchmark-h&gt;

&lt;denchmark-h:h4&gt;Encountered while upgrading the plugin here: https://addons.mozilla.org/en-US/firefox/addon/wingman-jr-filter/&lt;/denchmark-h&gt;

Recently I began migrating a Firefox plugin that utilizes tensorflow.js from 0.13.3 to 1.0.4. I have migrated several times before this with excellent results; however, this time I saw a significant performance drop when upgrading to the 1.0.x series. The nature of the plugin is that it does inference using a MobileNetV2-based net on inbound images; this fortunately makes it easy to test on large inference runs by e.g. visiting an image gallery. While the image sets were not entirely deterministic, I encountered a clear pattern of behavior. My results are as below:
0.13.3 - Avg of 178ms for 501 scanned images
0.15.3 - Avg of 181ms for 503 scanned images
1.0.0 - Avg of 394ms for 503 scanned images
1.0.4 - Avg of 407ms for 512 scanned images
So inference time more than doubled.
It should be noted that 0.13.3 was the last test performed, after the reduced performance seen in 1.0.4. The browser and system load should have been similar for both runs; the only two APIs that needed upgrades from 0.13.3 were tf.loadModel -&gt; tf.loadLayersModel and tf.fromPixels -&gt; tf.browser.fromPixels. These worked fine in 0.15.3 and are what the test was run with.
By the way, thanks to everyone who is pouring blood, sweat, and tears into this. I thoroughly enjoy this library.
Thanks for taking a look and let me know what other details would be helpful.
	</description>
	<comments>
		<comment id='1' author='jessetrana' date='2019-04-20T12:00:17Z'>
		Hi &lt;denchmark-link:https://github.com/jessetrana&gt;@jessetrana&lt;/denchmark-link&gt;
 - thanks for submitting this report. Could you try adding this line right after you import the library:
tf.ENV.set('WEBGL_CONV_IM2COL', false)
Would be curious to see the inference time with that change. Thanks!
		</comment>
		<comment id='2' author='jessetrana' date='2019-04-21T02:16:34Z'>
		Unfortunately no significant improvement.
1.0.4 with WEBGL_CONV_IM2COL=false - Avg of 414ms for 515 scanned images
Anything else you'd like me to try?
P.S. Here is the current settings of tf.ENV:
BACKEND: "webgl"
DEBUG: false
IS_BROWSER: true
WEBGL_CONV_IM2COL: false
WEBGL_CPU_FORWARD: true
WEBGL_DOWNLOAD_FLOAT_ENABLED: true
WEBGL_LAZILY_UNPACK: true
WEBGL_MAX_TEXTURE_SIZE: 16384
WEBGL_PACK: true
WEBGL_PACK_ARRAY_OPERATIONS: true
WEBGL_PACK_BATCHNORMALIZATION: true
WEBGL_PACK_BINARY_OPERATIONS: true
WEBGL_PACK_DEPTHWISECONV: true
WEBGL_RENDER_FLOAT32_ENABLED: true
WEBGL_SIZE_UPLOAD_UNIFORM: 4
WEBGL_VERSION: 2
		</comment>
		<comment id='3' author='jessetrana' date='2019-04-21T02:31:56Z'>
		Just for comparison, I also checked tf.ENV on 0.15.3 (without explicitly setting WEBGL_CONV_IM2COL), and the results seem to be significantly different:
​
BACKEND: "webgl"
DEBUG: false
IS_BROWSER: true
PROD: false
WEBGL_CONV_IM2COL: false
WEBGL_CPU_FORWARD: true
WEBGL_DOWNLOAD_FLOAT_ENABLED: true
WEBGL_LAZILY_UNPACK: false
WEBGL_MAX_TEXTURE_SIZE: 16384
WEBGL_NUM_MB_BEFORE_PAGING: 972000
WEBGL_PACK: false
WEBGL_PACK_BATCHNORMALIZATION: false
WEBGL_PACK_BINARY_OPERATIONS: false
WEBGL_PACK_DEPTHWISECONV: false
WEBGL_RENDER_FLOAT32_ENABLED: true
WEBGL_SIZE_UPLOAD_UNIFORM: 4
WEBGL_VERSION: 2
		</comment>
		<comment id='4' author='jessetrana' date='2019-04-21T11:09:01Z'>
		Thanks &lt;denchmark-link:https://github.com/jessetrana&gt;@jessetrana&lt;/denchmark-link&gt;
 - could you try this:
tf.ENV.set('WEBGL_PACK', false)
Let me know what you find - thanks!
		</comment>
		<comment id='5' author='jessetrana' date='2019-04-22T03:23:02Z'>
		That did the trick!
With 1.0.4, setting WEBGL_PACK=false, the ENV set looks like this.
BACKEND: "webgl"
DEBUG: false
IS_BROWSER: true
WEBGL_CONV_IM2COL: false
WEBGL_CPU_FORWARD: true
WEBGL_DOWNLOAD_FLOAT_ENABLED: true
WEBGL_LAZILY_UNPACK: false
WEBGL_MAX_TEXTURE_SIZE: 16384
WEBGL_PACK: false
WEBGL_PACK_ARRAY_OPERATIONS: false
WEBGL_PACK_BATCHNORMALIZATION: false
WEBGL_PACK_BINARY_OPERATIONS: false
WEBGL_PACK_DEPTHWISECONV: false
WEBGL_RENDER_FLOAT32_ENABLED: true
WEBGL_SIZE_UPLOAD_UNIFORM: 4
WEBGL_VERSION: 2
Avg of 169ms for 501 scanned images. That's on par and even a bit better!
I had also tried forcing WEBGL_PACK=true for 0.15.3 before your tip here to see if I could get slow performance by doing that, but that did not seem to trigger a slowdown. This leads me to think it is likely something in the way 1.0.x handles WEBGL_PACK.
While this gives me a definite workaround for the issue, are there any other steps you'd like me to take to help root cause the source of the problem?
Thanks for your time!
		</comment>
		<comment id='6' author='jessetrana' date='2019-04-24T12:57:34Z'>
		Thanks for doing this investigation &lt;denchmark-link:https://github.com/jessetrana&gt;@jessetrana&lt;/denchmark-link&gt;
 - this is very helpful information to have.
Very interesting that WEBGL_PACK=true for 0.15.3 did not harm performance - could you print the ENV after setting WEBGL_PACK=true and confirm that the individual op-level packing flags were also flipped to true? Their value should depend on WEBGL_PACK.
Do you have the model used for inference hosted somewhere I could take a look? e.g. a hosted folder somewhere with the weights and topology JSON file. Would be great to inspect it outside the context of your extension.
		</comment>
		<comment id='7' author='jessetrana' date='2019-04-24T23:17:47Z'>
		Yes, it appears that the individual op-level packing flags go to true. Let me see what I can do about the model etc. later, but here's the info for now:
0.15.3 with WEBGL_PACK=true
First Run
Model inference in 96ms and avg of 192ms for 517 scanned images
Second Run
Model inference in 135ms and avg of 188ms for 505 scanned images
BACKEND: "webgl"
DEBUG: false
IS_BROWSER: true
PROD: false
​WEBGL_CONV_IM2COL: true
​WEBGL_CPU_FORWARD: true
​WEBGL_DOWNLOAD_FLOAT_ENABLED: true
​WEBGL_LAZILY_UNPACK: true
​WEBGL_MAX_TEXTURE_SIZE: 16384
​WEBGL_NUM_MB_BEFORE_PAGING: 972000
​WEBGL_PACK: true
​WEBGL_PACK_BATCHNORMALIZATION: true
​WEBGL_PACK_BINARY_OPERATIONS: true
​WEBGL_PACK_DEPTHWISECONV: true
​WEBGL_RENDER_FLOAT32_ENABLED: true
WEBGL_SIZE_UPLOAD_UNIFORM: 4
WEBGL_VERSION: 2
		</comment>
		<comment id='8' author='jessetrana' date='2019-04-24T23:36:05Z'>
		Here is the plugin source and model topology (see the sqrx_18 folder) backpatched to 0.15.3 with WEBGL_PACK=true. (Note that while the code is being uploaded here it should still remain under the license of the plugin itself - which is quite permissive.  &lt;denchmark-link:https://addons.mozilla.org/en-US/firefox/addon/wingman-jr-filter/&gt;https://addons.mozilla.org/en-US/firefox/addon/wingman-jr-filter/&lt;/denchmark-link&gt;
)
&lt;denchmark-link:https://github.com/tensorflow/tfjs/files/3114523/wingman_jr_backpatch_to_0.15.3_with_weights.zip&gt;wingman_jr_backpatch_to_0.15.3_with_weights.zip&lt;/denchmark-link&gt;

This is a relatively generic MobileNetV2 based model with the Keras code around the model like this:
`    input_shape = (SIZE,SIZE,3)
&lt;denchmark-code&gt;#Build up the model base from MobileNet
alpha = 1.0
model_mn = keras.applications.MobileNetV2(
                            weights="imagenet",
                            #weights=None,
                            alpha=alpha,
                            include_top=False,
                            input_shape=(SIZE, SIZE, 3),
                            pooling=None)

#Piece it all together
x = model_mn.output
x = GlobalAveragePooling2D()(x)
x = Dense(128, name='fc_final', activation='relu')(x)
x = Dropout(0.45, name='dropout')(x)
x = Dense(NUM_CLASSES, name='classifier', activation='softmax')(x)
model = Model(input=[model_mn.input], output=[x])`
&lt;/denchmark-code&gt;

Good luck!
		</comment>
		<comment id='9' author='jessetrana' date='2019-04-29T01:17:45Z'>
		Were you able to get the model up and running? I know my running environment may have some interesting quirks due to being a browser plugin so I'm curious to know if you ran into failures under your standard test harnesses.
		</comment>
		<comment id='10' author='jessetrana' date='2020-01-17T14:48:32Z'>
		Hi &lt;denchmark-link:https://github.com/jessetrana&gt;@jessetrana&lt;/denchmark-link&gt;
 - I'm so sorry about the delay on this. I just tried to reproduce this issue with the model assets you shared in your previous comment. Here are the numbers I got (average inference times over 50 runs - lower is better):
Chrome
tfjs@0.15.3: 26.8 / 23.3 (mean / min)
tfjs@1.0.4: 24.8 / 19.2
Firefox
tfjs@0.15.3: 56.8 / 43.0
tfjs@1.0.4: 42.0 / 35.0
I'm going to close this, but please feel free to reopen if you're still experiencing the issue.
		</comment>
		<comment id='11' author='jessetrana' date='2020-01-20T05:43:30Z'>
		Well, for what it's worth I'm on the 1.2.x series now and don't have the performance problem. Might not be worth tracking down further even if there was an issue. Thanks &lt;denchmark-link:https://github.com/annxingyuan&gt;@annxingyuan&lt;/denchmark-link&gt;
 !
		</comment>
	</comments>
</bug>