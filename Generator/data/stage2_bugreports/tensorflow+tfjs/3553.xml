<bug id='3553' author='AmitMY' open_date='2020-07-04T14:18:16Z' closed_time='2020-07-13T22:17:06Z'>
	<summary>bug(converter): fails to convert model</summary>
	<description>
&lt;denchmark-h:h4&gt;TensorFlow.js version&lt;/denchmark-h&gt;


tensorflowjs  2.0.1.post1
tensorflow 2.2.0

&lt;denchmark-h:h4&gt;Browser version&lt;/denchmark-h&gt;

(Converter, no browser)
&lt;denchmark-h:h4&gt;Describe the problem or feature request&lt;/denchmark-h&gt;

tensorflowjs_converter fails to convert simple tensorflow model.
&lt;denchmark-h:h4&gt;Code to reproduce the bug / link to feature request&lt;/denchmark-h&gt;

Python model:
import tensorflow as tf
import numpy as np

# Create model
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dropout(0.5))
model.add(tf.keras.layers.LSTM(64, return_sequences=True, stateful=True))
model.add(tf.keras.layers.Dense(2))
model.build(input_shape=(1, 1, 25))
model.summary()

# Assume I fit the model
# model.fit(cool_data)

model.predict(np.random.randn(1, 1, 25))  # Set input shapes

# Save model
model.save("model")
Convert to tensorflow.js:
tensorflowjs_converter --input_format=tf_saved_model model web_model
(I also tried using the wizard, and other command variants)
Console output:
&lt;denchmark-code&gt;2020-07-04 17:12:26.010596: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-07-04 17:12:26.022300: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2199900000 Hz
2020-07-04 17:12:26.028111: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ed517829b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-04 17:12:26.028153: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-07-04 17:12:27.130305: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count &gt;= 8, compute capability &gt;= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)
2020-07-04 17:12:27.130427: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-07-04 17:12:27.153298: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize
2020-07-04 17:12:27.153357: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: Graph size after: 248 nodes (240), 428 edges (420), time = 8.011ms.
2020-07-04 17:12:27.153370: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.109ms.
WARNING:tensorflow:From /home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py:340: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.
WARNING:tensorflow:From /home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py:342: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
WARNING:tensorflow:From /home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflow/python/framework/graph_util_impl.py:359: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
Traceback (most recent call last):
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py", line 482, in convert_tf_saved_model
    frozen_graph = _freeze_saved_model_v2(concrete_func, control_flow_v2)
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py", line 352, in _freeze_saved_model_v2
    concrete_func, lower_control_flow=not control_flow_v2).graph
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflow/python/framework/convert_to_constants.py", line 679, in convert_variables_to_constants_v2
    func, lower_control_flow, aggressive_inlining)
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflow/python/framework/convert_to_constants.py", line 526, in _convert_variables_to_constants_v2_impl
    raise ValueError("Cannot find the Placeholder op that is an input "
ValueError: Cannot find the Placeholder op that is an input to the ReadVariableOp.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/bin/tensorflowjs_converter", line 8, in &lt;module&gt;
    sys.exit(pip_main())
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflowjs/converters/converter.py", line 735, in pip_main
    main([' '.join(sys.argv[1:])])
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflowjs/converters/converter.py", line 739, in main
    convert(argv[0].split(' '))
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflowjs/converters/converter.py", line 681, in convert
    control_flow_v2=args.control_flow_v2)
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py", line 485, in convert_tf_saved_model
    output_node_names)
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py", line 342, in _freeze_saved_model_v1
    sess, g.as_graph_def(), output_node_names)
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py", line 324, in new_func
    return func(*args, **kwargs)
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflow/python/framework/graph_util_impl.py", line 359, in convert_variables_to_constants
    inference_graph = extract_sub_graph(input_graph_def, output_node_names)
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py", line 324, in new_func
    return func(*args, **kwargs)
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflow/python/framework/graph_util_impl.py", line 205, in extract_sub_graph
    _assert_nodes_are_present(name_to_node, dest_nodes)
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflow/python/framework/graph_util_impl.py", line 160, in _assert_nodes_are_present
    assert d in name_to_node, "%s is not in graph" % d

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='AmitMY' date='2020-07-05T17:57:41Z'>
		Try to set the flag â€”control_flow_v2=true
		</comment>
		<comment id='2' author='AmitMY' date='2020-07-05T19:25:57Z'>
		Thanks &lt;denchmark-link:https://github.com/pyu10055&gt;@pyu10055&lt;/denchmark-link&gt;

Modified command:
tensorflowjs_converter --input_format=tf_saved_model --control_flow_v2=true model web_model
Error
&lt;denchmark-code&gt;2020-07-05 22:24:42.564041: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-07-05 22:24:42.580409: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2199900000 Hz
2020-07-05 22:24:42.586631: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ce7fcd09d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-05 22:24:42.586701: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-07-05 22:24:44.037808: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count &gt;= 8, compute capability &gt;= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)
2020-07-05 22:24:44.037965: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-07-05 22:24:44.055842: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize
2020-07-05 22:24:44.055875: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: Graph size after: 102 nodes (92), 168 edges (158), time = 4.307ms.
2020-07-05 22:24:44.055887: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: Graph size after: 102 nodes (0), 168 edges (0), time = 1.919ms.
2020-07-05 22:24:44.055898: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: __inference_while_body_1078_587
2020-07-05 22:24:44.055908: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-07-05 22:24:44.055919: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-07-05 22:24:44.055932: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: __inference_while_cond_1077_5375
2020-07-05 22:24:44.055950: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-07-05 22:24:44.055963: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.
WARNING:tensorflow:From /home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py:340: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.
WARNING:tensorflow:From /home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py:342: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
WARNING:tensorflow:From /home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflow/python/framework/graph_util_impl.py:359: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
Traceback (most recent call last):
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflow/python/framework/importer.py", line 497, in _import_graph_def_internal
    graph._c_graph, serialized, options)  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: Input 0 of node StatefulPartitionedCall/sequential/lstm/AssignVariableOp was passed float from Func/StatefulPartitionedCall/input/_2:0 incompatible with expected resource.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py", line 482, in convert_tf_saved_model
    frozen_graph = _freeze_saved_model_v2(concrete_func, control_flow_v2)
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py", line 352, in _freeze_saved_model_v2
    concrete_func, lower_control_flow=not control_flow_v2).graph
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflow/python/framework/convert_to_constants.py", line 680, in convert_variables_to_constants_v2
    return _construct_concrete_function(func, output_graph_def, converted_inputs)
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflow/python/framework/convert_to_constants.py", line 406, in _construct_concrete_function
    new_output_names)
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py", line 633, in function_from_graph_def
    wrapped_import = wrap_function(_imports_graph_def, [])
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py", line 611, in wrap_function
    collections={}),
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py", line 981, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py", line 86, in __call__
    return self.call_with_variable_creator_scope(self._fn)(*args, **kwargs)
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py", line 92, in wrapped
    return fn(*args, **kwargs)
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py", line 631, in _imports_graph_def
    importer.import_graph_def(graph_def, name="")
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflow/python/framework/importer.py", line 405, in import_graph_def
    producer_op_list=producer_op_list)
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflow/python/framework/importer.py", line 501, in _import_graph_def_internal
    raise ValueError(str(e))
ValueError: Input 0 of node StatefulPartitionedCall/sequential/lstm/AssignVariableOp was passed float from Func/StatefulPartitionedCall/input/_2:0 incompatible with expected resource.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/bin/tensorflowjs_converter", line 8, in &lt;module&gt;
    sys.exit(pip_main())
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflowjs/converters/converter.py", line 735, in pip_main
    main([' '.join(sys.argv[1:])])
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflowjs/converters/converter.py", line 739, in main
    convert(argv[0].split(' '))
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflowjs/converters/converter.py", line 681, in convert
    control_flow_v2=args.control_flow_v2)
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py", line 485, in convert_tf_saved_model
    output_node_names)
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py", line 342, in _freeze_saved_model_v1
    sess, g.as_graph_def(), output_node_names)
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py", line 324, in new_func
    return func(*args, **kwargs)
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflow/python/framework/graph_util_impl.py", line 359, in convert_variables_to_constants
    inference_graph = extract_sub_graph(input_graph_def, output_node_names)
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py", line 324, in new_func
    return func(*args, **kwargs)
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflow/python/framework/graph_util_impl.py", line 205, in extract_sub_graph
    _assert_nodes_are_present(name_to_node, dest_nodes)
  File "/home/nlp/amit/anaconda2/envs/meta-scholar/lib/python3.7/site-packages/tensorflow/python/framework/graph_util_impl.py", line 160, in _assert_nodes_are_present
    assert d in name_to_node, "%s is not in graph" % d
AssertionError: Identity is not in graph
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='AmitMY' date='2020-07-06T17:29:42Z'>
		&lt;denchmark-link:https://github.com/AmitMY&gt;@AmitMY&lt;/denchmark-link&gt;
 This looks like a TensorFlow internal error, investigating it with Keras team, please stay tuned.
		</comment>
		<comment id='4' author='AmitMY' date='2020-07-10T20:51:09Z'>
		&lt;denchmark-link:https://github.com/AmitMY&gt;@AmitMY&lt;/denchmark-link&gt;
 To work around this issue, you can try to save the model as HDF5 format, and use one of the following commands:
&lt;denchmark-code&gt;Keras HDF5 model example:
tensorflowjs_converter \
    --input_format=keras \
    /tmp/my_keras_model.h5 \
    /tmp/my_tfjs_model
tf.keras SavedModel example:
tensorflowjs_converter \
    --input_format=keras_saved_model \
    /tmp/my_tf_keras_saved_model/1542211770 \
    /tmp/my_tfjs_model
&lt;/denchmark-code&gt;

This would convert the python model to tfjs layers model.
		</comment>
		<comment id='5' author='AmitMY' date='2020-07-11T08:15:07Z'>
		The hdf5 model conversion works.
However, checking the result model JSON file, I see that stateful is false.
Before manually changing that to true for testing, I just loaded the model with tfjs:
const model = await  tf.loadLayersModel('assets/models/detector/model.json')
And got:

ERROR Error: Uncaught (in promise): Error: The first layer in a Sequential model must get an inputShape or batchInputShape argument.
Error: The first layer in a Sequential model must get an inputShape or batchInputShape argument.

Even though the JSON clearly states:
        "build_input_shape": [
          1,
          1,
          25
        ]
Full JSON:
{
  "format": "layers-model",
  "generatedBy": "keras v2.3.0-tf",
  "convertedBy": "TensorFlow.js Converter v2.0.1.post1",
  "modelTopology": {
    "keras_version": "2.3.0-tf",
    "backend": "tensorflow",
    "model_config": {
      "class_name": "Sequential",
      "config": {
        "name": "sequential",
        "layers": [
          {
            "class_name": "Dropout",
            "config": {
              "name": "dropout",
              "trainable": true,
              "dtype": "float32",
              "rate": 0.5,
              "noise_shape": null,
              "seed": null
            }
          },
          {
            "class_name": "LSTM",
            "config": {
              "name": "lstm",
              "trainable": true,
              "dtype": "float32",
              "return_sequences": true,
              "return_state": false,
              "go_backwards": false,
              "stateful": false,
              "unroll": false,
              "time_major": false,
              "units": 64,
              "activation": "tanh",
              "recurrent_activation": "sigmoid",
              "use_bias": true,
              "kernel_initializer": {
                "class_name": "GlorotUniform",
                "config": {
                  "seed": null
                }
              },
              "recurrent_initializer": {
                "class_name": "Orthogonal",
                "config": {
                  "gain": 1.0,
                  "seed": null
                }
              },
              "bias_initializer": {
                "class_name": "Zeros",
                "config": {}
              },
              "unit_forget_bias": true,
              "kernel_regularizer": null,
              "recurrent_regularizer": null,
              "bias_regularizer": null,
              "activity_regularizer": null,
              "kernel_constraint": null,
              "recurrent_constraint": null,
              "bias_constraint": null,
              "dropout": 0.0,
              "recurrent_dropout": 0.0,
              "implementation": 2
            }
          },
          {
            "class_name": "Dense",
            "config": {
              "name": "dense",
              "trainable": true,
              "dtype": "float32",
              "units": 2,
              "activation": "linear",
              "use_bias": true,
              "kernel_initializer": {
                "class_name": "GlorotUniform",
                "config": {
                  "seed": null
                }
              },
              "bias_initializer": {
                "class_name": "Zeros",
                "config": {}
              },
              "kernel_regularizer": null,
              "bias_regularizer": null,
              "activity_regularizer": null,
              "kernel_constraint": null,
              "bias_constraint": null
            }
          }
        ],
        "build_input_shape": [
          1,
          1,
          25
        ]
      }
    }
  },
  "weightsManifest": [
    {
      "paths": [
        "group1-shard1of1.bin"
      ],
      "weights": [
        {
          "name": "dense/kernel",
          "shape": [
            64,
            2
          ],
          "dtype": "float32"
        },
        {
          "name": "dense/bias",
          "shape": [
            2
          ],
          "dtype": "float32"
        },
        {
          "name": "lstm/lstm_cell/kernel",
          "shape": [
            25,
            256
          ],
          "dtype": "float32"
        },
        {
          "name": "lstm/lstm_cell/recurrent_kernel",
          "shape": [
            64,
            256
          ],
          "dtype": "float32"
        },
        {
          "name": "lstm/lstm_cell/bias",
          "shape": [
            256
          ],
          "dtype": "float32"
        }
      ]
    }
  ]
}
		</comment>
		<comment id='6' author='AmitMY' date='2020-07-13T18:57:01Z'>
		Web model
&lt;denchmark-link:https://github.com/tensorflow/tfjs/files/4914532/web_model.zip&gt;web_model.zip&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='AmitMY' date='2020-07-13T19:09:49Z'>
		&lt;denchmark-link:https://github.com/caisq&gt;@caisq&lt;/denchmark-link&gt;
 Hi Shanqing, can you help Amit to debug this issue? There are two questions:

when the input shape is not set
is it possible to feed the LSTM for the next run with states from previous inference?

		</comment>
		<comment id='8' author='AmitMY' date='2020-07-13T19:24:35Z'>
		&lt;denchmark-link:https://github.com/AmitMY&gt;@AmitMY&lt;/denchmark-link&gt;
 Regarding the "inputShape..." error you are getting, can you make sure you include the  kwarg when you construct the model in Python. That would involve changing the line
model.add(tf.keras.layers.Dropout(0.5))
to something like
model.add(tf.keras.layers.Dropout(0.5, input_shape=(num_steps, feature_length)))
input_shape is without the leading batch dimension. The reason why you need to do this is that
while recent versions  TensorFlow (tf.keras) supports models without input shape specified, TensorFlow.js
doesn't support that yet.
		</comment>
		<comment id='9' author='AmitMY' date='2020-07-13T19:44:34Z'>
		Thanks &lt;denchmark-link:https://github.com/caisq&gt;@caisq&lt;/denchmark-link&gt;

This also needed adding :
model.add(tf.keras.layers.Dropout(0.5, input_shape=(1, 25), batch_input_shape=(1, 1, 25)))
This adds to the Dropout config the following line:

"batch_input_shape": [1, 1, 25],

And loading the model works.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Now, regarding stateful LSTM.
My situation is that every time a new frame from the camera comes, I need to run inference on it, but it depends on the hidden state from the previous frames.
Running this prediction without saving the hidden state results in very very poor accuracy in my model.
Is there a way to get a stateful prediction?
		</comment>
		<comment id='10' author='AmitMY' date='2020-07-13T19:49:54Z'>
		Manually changing stateful to be true in the config, makes it work!
Thanks a lot!
I'll close tomorrow after confirming its working well
		</comment>
		<comment id='11' author='AmitMY' date='2020-07-13T19:54:25Z'>
		&lt;denchmark-link:https://github.com/caisq&gt;@caisq&lt;/denchmark-link&gt;
 Thanks for the help, &lt;denchmark-link:https://github.com/AmitMY&gt;@AmitMY&lt;/denchmark-link&gt;
 i am glad you got this working!
		</comment>
	</comments>
</bug>