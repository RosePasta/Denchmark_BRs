<bug id='1318' author='tafsiri' open_date='2019-03-04T21:08:47Z' closed_time='2019-03-06T03:30:34Z'>
	<summary>dataset.toArray may not return all elements of a dataset</summary>
	<description>
&lt;denchmark-h:h4&gt;TensorFlow.js version&lt;/denchmark-h&gt;

0.15.3
&lt;denchmark-h:h4&gt;Describe the problem or feature request&lt;/denchmark-h&gt;

Looking at &lt;denchmark-link:https://github.com/tensorflow/tfjs-data/blob/v0.2.3/src/dataset.ts#L467&gt;dataset.toArray()&lt;/denchmark-link&gt;
 it calls &lt;denchmark-link:https://github.com/tensorflow/tfjs-data/blob/v0.2.3/src/iterators/lazy_iterator.ts#L174&gt;iterator.collect()&lt;/denchmark-link&gt;
, which defaults to returning 1000 items. I guess this is probably to prevent exhausting user memory, but i think its a bug as a user would expect to get all the data elements back (at least in the case where the size of the dataset is known, in other cases I would expect an exception) given what the docs of the function say.
At least in the case of toArray, the maxItems param isn't present so the user cannot change this behavior.
Let me know if I'm missing anything.
	</description>
	<comments>
		<comment id='1' author='tafsiri' date='2019-03-04T21:42:29Z'>
		Yep, that's unintended, all right.  I hope it would have limited impact because we discourage using toArray() outside of small tests, but still it's a bug.
&lt;denchmark-link:https://github.com/kangyizhang&gt;@kangyizhang&lt;/denchmark-link&gt;
, can you add an optional  argument to ?  If it's provided, use it; else try to use the dataset size.  When the dataset size is known and below some threshold (maybe 100000?  1000 was too low) then we can return the array.  When the dataset size is known and above the threshold, definitely throw an exception explaining that  is needed.   If the dataset size is unknown we could either throw an exception, or just try to build the array anyway and throw the exception only if the array gets too big.
It would be safer to  the  argument, but that would now run afoul of the 1.0 API stability guarantee.  &lt;denchmark-link:https://github.com/nsthorat&gt;@nsthorat&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/dsmilkov&gt;@dsmilkov&lt;/denchmark-link&gt;
, wdyt?  Is it worth sneaking this in right away?  OK to change post 1.0?  Just no-go at this point?
As for the prefetch argument: that shouldn't really be part of collect() at all.  It would be much clearer to require prefetch(100).collect().  I think adding this argument was just an expedient way to make sure all the tests use prefetch, without modifying them individually.  Maybe we should add a special toArrayForTest() for that?  (Removing the prefetch argument would technically be another 1.0 API-breaking change, though I'd be shocked if anyone has actually used it outside of our tests).
		</comment>
		<comment id='2' author='tafsiri' date='2019-03-04T22:09:12Z'>
		I don't think toArray should take a maxItems. The user can use .take to limit how many they want. If we can always tell a stream is infinite then we can throw an exception (e.g. in something like webcam) else i think we should leave this to be generic. The user may know that their dataset is finite even if we don't have a size (e.g. from a generator). Beyond that I'd say the user should be responsible for managing how much they want to pull in memory with a method like this (which is generally speaking everything). So i'd say there isn't a breaking change required to resolve this.
		</comment>
		<comment id='3' author='tafsiri' date='2019-03-04T22:10:54Z'>
		+1 to toArray not taking maxItems.
		</comment>
		<comment id='4' author='tafsiri' date='2019-03-04T22:27:23Z'>
		OK, that makes sense too.  In that case I think the main change needed to resolve this bug is to remove the maxItems default value from collect().  Throwing an exception early on a known-infinite iterator is icing, but nonessential since that would end up throwing an OOM anyway just like a finite-but-too-large stream.  Removing that default is breaking in the sense that we're changing behavior that someone might rely on, but a) surely nobody does and b) we consider the behavior that we're changing to be a bug, so I think it's OK.
I also think it's OK to remove the prefetch argument from collect() (but then we do need to add the prefetches at the call sites in the tests).
Note collect() is on LazyIterator, which is technically exported, but is not yet included in the API docs.  So that's (imho) less subject to API stability concerns.
		</comment>
		<comment id='5' author='tafsiri' date='2019-03-04T23:06:51Z'>
		
+1 to toArray not taking maxItems.
+1 to overriding toArray() for any sensor (e.g. WebCam) to throw an error
Allow toArray() to work for datasets of unknown sizes, like a generator.

		</comment>
		<comment id='6' author='tafsiri' date='2019-03-05T04:17:58Z'>
		
+1 to toArray() not taking maxItems
+1 to remove prefetch from .collect(). I think it's ok to add .prefetch() in all tests, it's one time effort.
toArray() will throw an exception if the dataset size is infinity

Sent &lt;denchmark-link:https://github.com/tensorflow/tfjs-data/pull/155&gt;tensorflow/tfjs-data#155&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>