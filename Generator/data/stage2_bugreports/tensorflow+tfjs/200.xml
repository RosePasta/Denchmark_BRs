<bug id='200' author='chcaru' open_date='2018-04-18T04:52:00Z' closed_time='2018-05-26T16:35:21Z'>
	<summary>Memory leak during training</summary>
	<description>
&lt;denchmark-h:h4&gt;TensorFlow.js version&lt;/denchmark-h&gt;

"@tensorflow/tfjs@^0.9.1":
version "0.9.1"
dependencies:
"@tensorflow/tfjs-core" "0.7.1"
"@tensorflow/tfjs-layers" "0.4.1"
&lt;denchmark-h:h4&gt;Browser version&lt;/denchmark-h&gt;

68.0.3397.0 (Official Build) canary (64-bit)
&lt;denchmark-h:h4&gt;Describe the problem or feature request&lt;/denchmark-h&gt;

When training a simple model with fit(), I'm experiencing a huge memory leak. See below image. It grows to eventually consume my GPU's memory, and then it will consume my system's shared memory.
I've looked at the examples, and fit() is never surrounded by tidy(). I suppose that's because it already uses tidy() internally?
&lt;denchmark-h:h4&gt;Code to reproduce the bug / link to feature request&lt;/denchmark-h&gt;

This code is enough to reproduce the issue:
const inputs = tf.layers.input({ shape: [256], dtype: DType.float32 });
const dense1 = tf.layers.dense({ units: 128, activation: 'relu' }).apply(inputs);
const dense2 = tf.layers.dense({ units: 64, activation: 'relu' }).apply(dense1);
const outputs = tf.layers.dense({ units: 6, activation: 'softmax' }).apply(dense2);
const model = tf.model({ inputs, outputs });

model.compile({
    loss: 'categoricalCrossentropy',
    optimizer: 'adam',
});

const numSamples = 25632;
const xData = new Float32Array(numSamples * 256);
const yData = new Int32Array(numSamples);

const x = tf.tensor2d(xData, [numSamples, 256], 'float32');
const y = tf.oneHot(tf.tensor1d(yData, 'float32'), 6);

for (let i = 0; i &lt; 100; i++) {
    await model.fit(x, y, {
        batchSize: 64,
        epochs: 5,
        shuffle: true,
        validationSplit: .2,
    });
}
&lt;denchmark-link:https://user-images.githubusercontent.com/10172123/38912121-6a50bbde-4288-11e8-89a3-143a8174a79d.jpg&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='chcaru' date='2018-04-20T00:25:41Z'>
		&lt;denchmark-link:https://github.com/caisq&gt;@caisq&lt;/denchmark-link&gt;
 would you be able to take a look at this?
		</comment>
		<comment id='2' author='chcaru' date='2018-05-19T13:01:46Z'>
		What's the status on this?
		</comment>
		<comment id='3' author='chcaru' date='2018-05-21T15:27:52Z'>
		Several memory leaks in layers have been patched up (tfjs-layers/&lt;denchmark-link:https://github.com/tensorflow/tfjs/pull/186&gt;#186&lt;/denchmark-link&gt;
).  Not clear if this fixes the root cause though.   Can you retry the test using layers 0.6.1 or better?  Thanks!
		</comment>
		<comment id='4' author='chcaru' date='2018-05-22T15:08:59Z'>
		I'm using 0.11.1. Not sure what version of layers that implies. Still seeing a leak (judging from the number of tensors from tf.memory).
The problem could be elsewhere, of course, but the only potential culprit as far as I can see is setting values in a tensorBuffer.
		</comment>
		<comment id='5' author='chcaru' date='2018-05-22T15:22:09Z'>
		tfjs 0.11.1 uses tfjs-layers 0.6.1
I figured this out by looking at the package.json file
&lt;denchmark-link:https://github.com/caisq/tfjs-1/blob/dc599fdf73f9eb9e4940590d137546570c9012b4/package.json&gt;https://github.com/caisq/tfjs-1/blob/dc599fdf73f9eb9e4940590d137546570c9012b4/package.json&lt;/denchmark-link&gt;

Sounds like we need to dig deeper to find the root cause.   In the mean time, if this is blocking for you can you wrap your call to model.fit() in a tidy() block?
		</comment>
		<comment id='6' author='chcaru' date='2018-05-22T15:28:30Z'>
		&lt;denchmark-link:https://github.com/bileschi&gt;@bileschi&lt;/denchmark-link&gt;
 The memory optimization that went into the latest release is orthogonal to this issue. I have it on my TODO list to look at this issue soon.
		</comment>
		<comment id='7' author='chcaru' date='2018-05-24T14:30:17Z'>
		Hey there folks.
I am currently working on a tf.js (version 11.1) project and came across the memory leak issue. I looped 10 times over a model.fit function with a given epoch. As you can see on the screenshots below the number of tensors grew each epoch by a factor of [#data/batchSize]*epoch. I don't know if this information is of any value, but I felt like adding it here!
&lt;denchmark-link:https://user-images.githubusercontent.com/14012617/40491943-b1a034a2-5f6f-11e8-873b-98e564fad272.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/14012617/40491958-ba9052cc-5f6f-11e8-9b1a-eee1b62628e8.png&gt;&lt;/denchmark-link&gt;

For example:
batchSize 32 and epoch 1.
Each Iteration increases the number of tensors by 360 which is approx. [11518/32]
		</comment>
		<comment id='8' author='chcaru' date='2018-05-25T20:32:59Z'>
		Fix is on the way. See the referred PR above.
		</comment>
	</comments>
</bug>