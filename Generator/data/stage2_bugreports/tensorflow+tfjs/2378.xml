<bug id='2378' author='gitunit' open_date='2019-11-12T17:01:30Z' closed_time='2019-11-14T16:17:02Z'>
	<summary>webgl backend to accelerate inference with AutoML model</summary>
	<description>
&lt;denchmark-h:h4&gt;TensorFlow.js version&lt;/denchmark-h&gt;

1.3.1
&lt;denchmark-h:h4&gt;Browser version&lt;/denchmark-h&gt;

Version 78.0.3904.87 (Official Build) (64-bit)
&lt;denchmark-h:h4&gt;Describe the problem or feature request&lt;/denchmark-h&gt;

i've tried to set the backend to webgl but it has no effect on the performance.
im using an AutoML model which gives me results after appr. 5 sec. this is way to slow,
is there a way to accelerate, for example by setting the backend to webgl properly?
&lt;denchmark-h:h4&gt;Code to reproduce the bug / link to feature request&lt;/denchmark-h&gt;

you can reproduce with &lt;denchmark-link:https://github.com/tensorflow/tfjs/blob/master/tfjs-automl/code_snippets/object_detection.html&gt;this&lt;/denchmark-link&gt;
 example
	</description>
	<comments>
		<comment id='1' author='gitunit' date='2019-11-12T18:11:42Z'>
		&lt;denchmark-link:https://github.com/gitunit&gt;@gitunit&lt;/denchmark-link&gt;
 thank you for reporting can you please provide your system configuration and browser information ?
cc &lt;denchmark-link:https://github.com/annxingyuan&gt;@annxingyuan&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/dsmilkov&gt;@dsmilkov&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='gitunit' date='2019-11-12T18:54:32Z'>
		&lt;denchmark-link:https://github.com/gitunit&gt;@gitunit&lt;/denchmark-link&gt;
 You can run, tf.getBackend() to check if the webgl backend is enabled.
Also, FYI the first detection is slow since the backend are caching the shader programs, the prediction after the first one should much faster and stable.
		</comment>
		<comment id='3' author='gitunit' date='2019-11-13T09:56:51Z'>
		&lt;denchmark-link:https://github.com/rthadur&gt;@rthadur&lt;/denchmark-link&gt;
 Chrome Version 78.0.3904.87 (Official Build) (64-bit) on a notebook with Windows 10.
&lt;denchmark-link:https://github.com/pyu10055&gt;@pyu10055&lt;/denchmark-link&gt;
 i see, it works. i didn't know the first prediction takes longer, therefore i haven't bothered trying another one after the first. now i can definitely see a difference between backend 'cpu' and 'webgl'. thanks for clarification.
now i wonder how i might improve the UX, so the user doesn't notice this initial warm-up time. maybe just using a test image on load and only 'start' the web experience after the first prediction? do you have any suggestions for this?
		</comment>
		<comment id='4' author='gitunit' date='2019-11-13T18:59:47Z'>
		&lt;denchmark-link:https://github.com/gitunit&gt;@gitunit&lt;/denchmark-link&gt;
 after the model is loaded, you can run a warming up step, to feed the model prediction call with all zero tensor. tf.zeros([tensorShape])
		</comment>
		<comment id='5' author='gitunit' date='2019-11-14T16:17:02Z'>
		&lt;denchmark-link:https://github.com/pyu10055&gt;@pyu10055&lt;/denchmark-link&gt;
 got it, thx. basically same as loading a dummy image (i haven't noticed any performance gain using tf.zeros([tensorShape]).
		</comment>
	</comments>
</bug>