<bug id='808' author='iacovlev-pavel' open_date='2018-10-19T06:41:59Z' closed_time='2018-12-18T11:35:57Z'>
	<summary>tfjs-node works, tfjs-node-gpu fails with: Resource exhausted: OOM when allocating tensor with shape[2048]</summary>
	<description>
&lt;denchmark-h:h4&gt;TensorFlow.js version&lt;/denchmark-h&gt;

tfjs-node-gpu: 0.1.18
&lt;denchmark-h:h4&gt;Browser version&lt;/denchmark-h&gt;

NodeJS v8.12.0, Windows 10, CUDA 9
&lt;denchmark-h:h4&gt;Describe the problem or feature request&lt;/denchmark-h&gt;

I have code in python which blurs an image, it works on Windows in Python on GPU.
I am trying to do the same from node, I have successfully converted the script to and it works and produces exactly the same results.
When I try to run  I go OOM, with the following error:

&lt;denchmark-link:https://github.com/tensorflow/tfjs/files/2494671/log.txt&gt;log.txt&lt;/denchmark-link&gt;

Command used to convert my model:
Miniconda3\Scripts\tensorflowjs_converter.exe --input_format=tf_frozen_model --output_node_names="image_tensor,detection_boxes,detection_scores,detection_classes,num_detections" --saved_model_tags=serve frozen_inference_graph.pb web_model
Weights: &lt;denchmark-link:https://github.com/tensorflow/tfjs/files/2494685/weights_manifest.json.txt&gt;weights_manifest.json.txt&lt;/denchmark-link&gt;

Sample code in python: &lt;denchmark-link:https://github.com/tensorflow/tfjs/files/2494686/sample.py.txt&gt;sample.py.txt&lt;/denchmark-link&gt;

Sample code in js: &lt;denchmark-link:https://github.com/tensorflow/tfjs/files/2494694/sample.js.txt&gt;sample.js.txt&lt;/denchmark-link&gt;

What I have tried:

Tried with a very small input image, still failed.

Thoughts:
Could this be a tensorflowjs_converter problem, where it expects the model to be ran in the browser, and converts to more generic types like int32 to store the data where the original code in python uses uint8.
This is how I pass the image to the model: tf.tensor4d(values, outShape, 'int32');, as I understand there is no uint8 support ? also for some reason this operation takes ~40ms, is this expected ?
	</description>
	<comments>
		<comment id='1' author='iacovlev-pavel' date='2018-10-19T18:24:20Z'>
		Try installing 0.1.19 for @tensorflow/tfjs-node-gpu.
		</comment>
		<comment id='2' author='iacovlev-pavel' date='2018-10-19T19:30:57Z'>
		Thank you for the reply.
Deleted node_modules folder, bumped the version in package.json, npm install.
Ran the test again still got the error:
&lt;denchmark-code&gt;Message: OOM when allocating tensor with shape[2048] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
    at NodeJSKernelBackend.executeMultipleOutputs (sblur\node_modules\@tensorflow\tfjs-node-gpu\dist\nodejs_kernel_backend.js:109:43)
    at NodeJSKernelBackend.batchNormalization (jsblur\node_modules\@tensorflow\tfjs-node-gpu\dist\nodejs_kernel_backend.js:866:21)
    at environment_1.ENV.engine.runKernel.$x (sblur\node_modules\@tensorflow\tfjs-core\dist\ops\batchnorm.js:194:86)
    at jsblur\node_modules\@tensorflow\tfjs-core\dist\engine.js:123:26
    at Engine.scopedRun (jsblur\node_modules\@tensorflow\tfjs-core\dist\engine.js:101:23)
    at Engine.runKernel (jsblur\node_modules\@tensorflow\tfjs-core\dist\engine.js:121:14)
    at batchNormalization_ (jsblur\node_modules\@tensorflow\tfjs-core\dist\ops\batchnorm.js:194:40)
    at Object.batchNormalization (jsblur\node_modules\@tensorflow\tfjs-core\dist\ops\operation.js:23:29)
    at Object.exports.executeOp (jsblur\node_modules\@tensorflow\tfjs-converter\dist\src\operations\executors\normalization_executor.js:8:25)
    at Object.executeOp (jsblur\node_modules\@tensorflow\tfjs-converter\dist\src\operations\operation_executor.js:43:34)
&lt;/denchmark-code&gt;

Can attach the entire log if needed.
		</comment>
		<comment id='3' author='iacovlev-pavel' date='2018-10-19T19:42:37Z'>
		&lt;denchmark-link:https://github.com/iacovlev-pavel&gt;@iacovlev-pavel&lt;/denchmark-link&gt;

It's funny, I'm having the same issue. My first guess was that I'm running it in 32bits, having the 4GB memory limitation. I saw the task manager and effectively my node was handled by a 32bits console.
I have to try it in 64 bits to confirm this
		</comment>
		<comment id='4' author='iacovlev-pavel' date='2018-10-19T20:12:36Z'>
		&lt;denchmark-link:https://github.com/pyu10055&gt;@pyu10055&lt;/denchmark-link&gt;
 for reference. There is a converter patch coming that might help here: &lt;denchmark-link:https://github.com/tensorflow/tfjs-converter/pull/216&gt;tensorflow/tfjs-converter#216&lt;/denchmark-link&gt;

Do you know how large your model is?
		</comment>
		<comment id='5' author='iacovlev-pavel' date='2018-10-19T20:12:56Z'>
		Also - can you provide more device information (Win10 right?) which NVIDIA card/CUDA/etc.
		</comment>
		<comment id='6' author='iacovlev-pavel' date='2018-10-19T20:35:46Z'>
		&lt;denchmark-link:https://github.com/nkreeger&gt;@nkreeger&lt;/denchmark-link&gt;


Windows 10
NVIDIA GTX 960M, 4GB VRAM
CUDA 9.0
CUDNN 7.3.1.20

frozen_inference_graph.pb - 181 MB (converted model is the same size but split in chunks).
NVIDIA GTX 1080 is available, can try on it Tomorrow.
		</comment>
		<comment id='7' author='iacovlev-pavel' date='2018-10-22T08:19:27Z'>
		Tried with a NVIDIA GTX 1080 8GB, it works, no OOM errors with tfjs-node-gpu.
&lt;denchmark-code&gt;localhost/replica:0/task:0/device:GPU:0 with 6363 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
&lt;/denchmark-code&gt;

On NVIDIA GTX 960M with 4GB
Python: Works
tfjs-node-gpu: Fails, OOM.
P.S. Is it possible to load the Tensorflow frozen_inference_graph.pb directly and avoid tfjs-converter ?
		</comment>
		<comment id='8' author='iacovlev-pavel' date='2018-10-22T17:09:05Z'>
		Are you using Python with eager mode on?
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Mon, Oct 22, 2018 at 1:42 AM Pavel Iacovlev ***@***.***&gt; wrote:
 Tried with a NVIDIA GTX 1080 8GB, it works, no OOM errors with
 tfjs-node-gpu.

 localhost/replica:0/task:0/device:GPU:0 with 6363 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)

 On NVIDIA GTX 960M with 4GB
 Python: Works
 tfjs-node-gpu: Fails, OOM.

 P.S. Is it possible to load the Tensorflow frozen_inference_graph.pb
 directly and avoid tfjs-converter ?

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#808 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AASsZLqhCnin1Wv9VH42wZWJc4Kba33yks5unYT-gaJpZM4Xvwn0&gt;
 .



		</comment>
		<comment id='9' author='iacovlev-pavel' date='2018-10-23T08:05:45Z'>
		&lt;denchmark-link:https://github.com/nkreeger&gt;@nkreeger&lt;/denchmark-link&gt;
 I don't.
The model which I am running is faster_rcnn_resnet101, I have tried ssd_mobilenet_v2 and it seems that it does not have the memory issue.
Could it be that tfjs-converter does not efficiently convert some of the kernels or something among this line ?
		</comment>
		<comment id='10' author='iacovlev-pavel' date='2018-10-23T17:14:12Z'>
		We use TensorFlow's eager API for all execution (inference and training). It's possible that eager mode is not working well with certain GPUs. Non-eager (graph) mode builds a graph and can re-use Tensors. However, eager mode is easier to use but a cost of performance (each output must be re-alloc'd).
We do have one memory fix coming in the converter - &lt;denchmark-link:https://github.com/tensorflow/tfjs-converter/pull/214&gt;tensorflow/tfjs-converter#214&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='iacovlev-pavel' date='2018-10-24T06:21:44Z'>
		I have added tf.enable_eager_execution() to the python script, and it did not change much. Python version performed as before.
		</comment>
		<comment id='12' author='iacovlev-pavel' date='2018-10-25T08:24:32Z'>
		Some more additional info: I have tested with of the shelf models from &lt;denchmark-link:https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md&gt;https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md&lt;/denchmark-link&gt;


ssd_mobilenet_v2_coco - works
ssdlite_mobilenet_v2_coco - works
faster_rcnn_inception_v2_coco - works
faster_rcnn_resnet101_coco - fails with OOM in tfjs-node-gpu works in Python.

This is tested with NVIDIA GTX 960M 4GB, with NVIDIA GTX 1080 8GB everything works.
		</comment>
		<comment id='13' author='iacovlev-pavel' date='2018-10-28T18:42:54Z'>
		I'm running into the same issue. I'm running a frozen model for inference on a video stream. It works for about 2 minutes then I get a crash. It seems always to be after a period of time regardless of the inputs. The inference graph processes around 15 images per seconds.
Here is a relevent part (I hope) of my error:
&lt;denchmark-code&gt;Stats: 
Limit:                  7380579124
InUse:                  7380574720
MaxInUse:               7380574720
NumAllocs:                  856032
MaxAllocSize:            280559616
2018-10-28 14:32:12.367844: W tensorflow/core/common_runtime/bfc_allocator.cc:275] ****************************************************************************************************
2018-10-28 14:32:12.367860: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at cwise_ops_common.cc:70 : Resource exhausted: OOM when allocating tensor with shape[1,187,103,4] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc

&lt;/denchmark-code&gt;

It works with tfjs-node instead of tfjs-node-gpu (0.1.19).
I'm using a GeForce GTX 1070
		</comment>
		<comment id='14' author='iacovlev-pavel' date='2018-10-28T21:54:28Z'>
		I'll take a look at what's going on here this week.
		</comment>
		<comment id='15' author='iacovlev-pavel' date='2018-12-18T11:35:57Z'>
		I no longer get this error using the latest version of @tensorflow/tfjs-node-gpu: 0.1.21
		</comment>
	</comments>
</bug>