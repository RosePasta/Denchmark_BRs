<bug id='37' author='aikramer2' open_date='2017-03-14T00:14:37Z' closed_time='2017-04-28T22:34:29Z'>
	<summary>n_classes inference potential bug</summary>
	<description>
we infer the number of classes for classifiers by running model.predict on the dataset
say however you had data whereby the classifier only tended to output class 1 or class 2, though technically class 3 is possible
then if we perturbed data in such a way as to get a class 3 prediction back...
then wed have a bug
POTENTIAL SOLUTIONS:
1:
in the event that we see a different class in the perturbed data, just start the whole algorithm over with the updated information
2:
make those 3 model types separate functions, and for classifiers you specify the classes ( i dont like, what if the user messes up or doesnt know)
3:
we always have an additional null class. in the event that a new class is discovered (any new class), then it just inherits the effect of the null class, and the null class is displayed as "other classes"
	</description>
	<comments>
		<comment id='1' author='aikramer2' date='2017-03-22T20:04:46Z'>
		"we infer the number of classes for classifiers by running model.predict on the dataset". This seems to be very expensive. There could be an easier way to do this.

Just computing unique on the col:target_label could help us to do that if specified.
In the event, it is not we show a user how to handle the same and then pass the number of classes as a parameter.
at times, depending on the model trained, it could be exposed as classifier.classes_. Would suggest to check the default implementation in R and sklearn.

		</comment>
	</comments>
</bug>