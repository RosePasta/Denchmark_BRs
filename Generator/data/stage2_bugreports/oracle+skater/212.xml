<bug id='212' author='alay18' open_date='2018-03-26T09:32:12Z' closed_time='2018-06-17T09:30:57Z'>
	<summary>Kernel dies when running plot_partial_dependence</summary>
	<description>
Hello,
I'm currently having trouble running plot_partial_dependence for my XGBoost model.
My kernel dies when I try to run the script, and this happens also if I run partial_dependence:
The kernel appears to have died. It will restart automatically.
In terminal, it says:
malloc: *** error for object 0x100007f87ba95b8c: pointer being freed was not allocated
*** set a breakpoint in malloc_error_break to debug
There doesn't seem to be an issue when I run plot_partial_dependence for my random forest model.
Could someone please look into this? Thanks in advance for your help!
	</description>
	<comments>
		<comment id='1' author='alay18' date='2018-04-07T21:07:54Z'>
		Looks like this is an issue with multiprocessing/xgboost compatibility, see &lt;denchmark-link:https://github.com/dmlc/xgboost/issues/2163&gt;dmlc/xgboost#2163&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/pramitchoudhary&gt;@pramitchoudhary&lt;/denchmark-link&gt;
 might be worth checking to see if changing the spawner to a forkserver will be effective. Would need to check that thats a stable spawner on the major platforms.
&lt;denchmark-link:https://github.com/alay18&gt;@alay18&lt;/denchmark-link&gt;
 in mean time, can you try setting n_jobs=1 if you're running pdp with an xgboost model?
		</comment>
		<comment id='2' author='alay18' date='2018-04-09T08:25:58Z'>
		&lt;denchmark-link:https://github.com/aikramer2&gt;@aikramer2&lt;/denchmark-link&gt;
 Hey, I've tried setting n_jobs = -1 but the same thing happened. Please let me know if there's any updates on your side.
		</comment>
		<comment id='3' author='alay18' date='2018-04-09T16:08:53Z'>
		&lt;denchmark-link:https://github.com/alay18&gt;@alay18&lt;/denchmark-link&gt;
 try n_jobs=1, not -1. n_jobs=-1 will use as many processes as are available.
		</comment>
		<comment id='4' author='alay18' date='2018-04-09T16:14:52Z'>
		&lt;denchmark-link:https://github.com/aikramer2&gt;@aikramer2&lt;/denchmark-link&gt;
 thanks for pointing that out, just tried n_jobs = 1, the kernel still died.
		</comment>
		<comment id='5' author='alay18' date='2018-04-11T20:59:56Z'>
		thanks, &lt;denchmark-link:https://github.com/aikramer2&gt;@aikramer2&lt;/denchmark-link&gt;
. &lt;denchmark-link:https://github.com/alay18&gt;@alay18&lt;/denchmark-link&gt;
 did the issue resolve when using n_jobs=1? or are you still getting errors?
		</comment>
		<comment id='6' author='alay18' date='2018-04-12T11:51:45Z'>
		&lt;denchmark-link:https://github.com/pramitchoudhary&gt;@pramitchoudhary&lt;/denchmark-link&gt;
 hello, unfortunately using n_jobs = 1 did not resolve the issue.
		</comment>
		<comment id='7' author='alay18' date='2018-04-23T17:56:59Z'>
		&lt;denchmark-link:https://github.com/alay18&gt;@alay18&lt;/denchmark-link&gt;
 will it be possible to share the code(or example snippet) for the XGBoost model that you are trying to use. Apologies for the slow response, have been beaten down with releasing new features and documentation. Let's get this resolved.
		</comment>
		<comment id='8' author='alay18' date='2018-04-24T13:25:19Z'>
		&lt;denchmark-link:https://github.com/pramitchoudhary&gt;@pramitchoudhary&lt;/denchmark-link&gt;
 here it is:
#specify XGB for inner loop - grid search
xgb = XGBRegressor(objective='reg:linear', booster='gbtree', n_jobs= -1)
#XGB parameters for grid search
xgb_grid = {"max_depth" : [6],
"learning_rate" : [0.2],
"gamma" : [0],
"n_estimators" : [150],
"min_child_weight" : [1],
"base_score" : [0.5],
"subsample" : [1],
"max_delta_step" : [0],
"colsample_bytree" : [0.5],
"colsample_bylevel" : [0.4],
"reg_alpha" : [0],
"reg_lambda" : [60],
"scale_pos_weight" : [1]
}
clf = GridSearchCV(xgb, param_grid = xgb_grid, cv = inner_cv, n_jobs = -1)
clf.fit(X, y)
nested_score = cross_validate(clf, X, y, cv=outer_cv, n_jobs= -1, scoring = ('r2', 'neg_mean_squared_error'))
nested_scores_mean = nested_score['test_r2'].mean()
nested_rmse_mean = (np.sqrt(-nested_score['test_neg_mean_squared_error'])).mean()
print (nested_scores_mean)
print (nested_rmse_mean)
#Skater feature importance
skater_model = InMemoryModel(clf.predict, examples = X)
interpreter = Interpretation()
interpreter.load_data(X)
pdp = interpreter.partial_dependence.plot_partial_dependence(
feature_ids = ['marksandspencer'], modelinstance = skater_model, grid_resolution = 30, n_jobs = 1)
plt.show()
		</comment>
		<comment id='9' author='alay18' date='2018-04-25T15:42:58Z'>
		&lt;denchmark-link:https://github.com/aikramer2&gt;@aikramer2&lt;/denchmark-link&gt;
 will you have time to take a look at this and see if we can resolve it or is it something that we would need help from the XGBoost guys?
		</comment>
		<comment id='10' author='alay18' date='2018-05-16T21:06:14Z'>
		&lt;denchmark-link:https://github.com/aikramer2&gt;@aikramer2&lt;/denchmark-link&gt;
 Did you get a chance to take a look at this issue?
		</comment>
		<comment id='11' author='alay18' date='2018-06-16T17:35:42Z'>
		looking into this, apologies for all the delays.
		</comment>
		<comment id='12' author='alay18' date='2018-06-17T07:55:15Z'>
		&lt;denchmark-link:https://github.com/alay18&gt;@alay18&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/aikramer2&gt;@aikramer2&lt;/denchmark-link&gt;
 alright still a little blind on how XgBosst is handling concurrency in-terms of splitting the trees internally but there seems to be some reported issues in the way XgBoost handles thread pool internally and python's multiprocessing libraries. For more information checkout the &lt;denchmark-link:http://scikit-learn.org/stable/faq.html#why-do-i-sometime-get-a-crash-freeze-with-n-jobs-1-under-osx-or-linux&gt;FAQ&lt;/denchmark-link&gt;
. The link that &lt;denchmark-link:https://github.com/aikramer2&gt;@aikramer2&lt;/denchmark-link&gt;
 had referenced earlier contains good history on this thread. Also, you might also get a dead kernel issue with  and (I was not able to get it working).
Now, to get this working with a workaround, the below mentioned code worked for me,
%matplotlib inline
import matplotlib.pyplot
from sklearn.datasets import load_boston, load_breast_cancer
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.linear_model import LogisticRegression, LinearRegression
import matplotlib.pyplot as plt
&lt;denchmark-h:h4&gt;Reference for customizing matplotlib: https://matplotlib.org/users/style_sheets.html&lt;/denchmark-h&gt;

plt.style.use('ggplot')
import pandas as pd
import numpy as np
&lt;denchmark-h:h1&gt;Load Bosting housing data&lt;/denchmark-h&gt;

regressor_data = load_boston()
&lt;denchmark-h:h1&gt;Get information about the data&lt;/denchmark-h&gt;

print(regressor_data.DESCR)
regressor_X = regressor_data.data
regressor_y = regressor_data.target
from xgboost import XGBRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_validate
&lt;denchmark-h:h1&gt;XGRegressor example&lt;/denchmark-h&gt;

xgb = XGBRegressor(objective='reg:linear', booster='gbtree', n_jobs= -1)
#XGB parameters for grid search
xgb_grid = {"max_depth" : [6],
"learning_rate" : [0.2],
"gamma" : [0],
"n_estimators" : [150],
"min_child_weight" : [1],
"base_score" : [0.5],
"subsample" : [1],
"max_delta_step" : [0],
"colsample_bytree" : [0.5],
"colsample_bylevel" : [0.4],
"reg_alpha" : [0],
"reg_lambda" : [60],
"scale_pos_weight" : [1]
}
clf = GridSearchCV(xgb, param_grid = xgb_grid, cv = 3, n_jobs = -1)
clf.fit(regressor_X, regressor_y)
&lt;denchmark-h:h1&gt;Model Evaluation&lt;/denchmark-h&gt;

from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
y_hat = clf.predict(regressor_X)
print(mean_squared_error(regressor_y, y_hat))
print(r2_score(regressor_y, y_hat))
&lt;denchmark-h:h1&gt;Model Inference&lt;/denchmark-h&gt;

from skater.core.explanations import Interpretation
interpreter = Interpretation(regressor_X, feature_names=regressor_data.feature_names)
from skater.model import InMemoryModel
annotated_model = InMemoryModel(clf.predict, examples=regressor_X)
print("Number of classes: {}".format(annotated_model.n_classes))
print("Input shape: {}".format(annotated_model.input_shape))
print("Model Type: {}".format(annotated_model.model_type))
print("Output Shape: {}".format(annotated_model.output_shape))
print("Output Type: {}".format(annotated_model.output_type))
print("Returns Probabilities: {}".format(annotated_model.probability))
print("2-way partial dependence plots")
&lt;denchmark-h:h1&gt;Features can passed as a tuple for 2-way partial plot&lt;/denchmark-h&gt;

pdp_features = [('DIS', 'RM')]
interpreter.partial_dependence.plot_partial_dependence(
pdp_features, annotated_model, grid_resolution=30, n_jobs=1
)
print("1-way partial dependence plots")
&lt;denchmark-h:h1&gt;or as independent features for 1-way partial plots&lt;/denchmark-h&gt;

pdp_features = ['DIS', 'RM']
interpreter.partial_dependence.plot_partial_dependence(
pdp_features, annotated_model, grid_resolution=30, progressbar=False, n_jobs=1, with_variance=True
)
Will add as an example notebook as well, so that its easier for other folks.
		</comment>
		<comment id='13' author='alay18' date='2018-06-17T09:30:25Z'>
		&lt;denchmark-link:https://github.com/alay18&gt;@alay18&lt;/denchmark-link&gt;
  Added a notebook example &lt;denchmark-link:https://github.com/datascienceinc/Skater/blob/master/examples/xgboost_regression_example.ipynb&gt;here&lt;/denchmark-link&gt;

Marking this issue as resolved. For a better solution, will open a different ticket focusing on resolving the thread pool handling. Feel free to reopen the issue if things still dont work on your end.
		</comment>
		<comment id='14' author='alay18' date='2019-03-14T01:37:04Z'>
		I'm still having this issue on some notebooks
		</comment>
		<comment id='15' author='alay18' date='2019-03-14T01:51:08Z'>
		
@alay18 Added a notebook example here
Marking this issue as resolved. For a better solution, will open a different ticket focusing on resolving the thread pool handling. Feel free to reopen the issue if things still dont work on your end.

FYI this kills the kernel
		</comment>
		<comment id='16' author='alay18' date='2019-10-25T08:34:41Z'>
		I have this issue too when using xgboost.sklearn API. It never shows anything when I try to generate plots inside a notebook
		</comment>
	</comments>
</bug>