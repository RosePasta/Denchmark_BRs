<bug id='915' author='CESARDELATORRE' open_date='2020-04-12T21:39:30Z' closed_time='2020-05-17T16:03:13Z'>
	<summary>experiment_timeout_hours description should mention that it can be decimal (i.e. 0.25 for 15min)</summary>
	<description>
experiment_timeout_hours parameter's description should mention that it can be decimal (i.e. 0.25 for 15min).
Other than that it initially looks like the minimum value is 1h.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h4&gt;Document Details&lt;/denchmark-h&gt;

⚠ Do not edit this section. It is required for docs.microsoft.com ➟ GitHub issue linking.

ID: 0bc2b21e-6b1a-cb94-2857-147177a29d7c
Version Independent ID: d14620a6-a2f6-49f1-632e-73903d41de8c
Content: azureml.train.automl.automlconfig.AutoMLConfig class - Azure Machine Learning Python
Content Source: AzureML-Docset/stable/docs-ref-autogen/azureml-train-automl-client/azureml.train.automl.automlconfig.AutoMLConfig.yml
Service: machine-learning
Sub-service: core
GitHub Login: @j-martens
Microsoft Alias: jmartens

	</description>
	<comments>
		<comment id='1' author='CESARDELATORRE' date='2020-04-12T22:04:46Z'>
		Also mention the dataset's limit in size (rows * cols cannot be greater than 10,000,000) in order to be able to specify less than 1.0 hours, or you'll get this error:
"Message: The ExperimentTimeout should be set more than 60 minutes with an input data of rowscols(53569058=31070020) more than 10,000,000."
		</comment>
		<comment id='2' author='CESARDELATORRE' date='2020-04-13T01:25:47Z'>
		&lt;denchmark-link:https://github.com/CESARDELATORRE&gt;@CESARDELATORRE&lt;/denchmark-link&gt;
 Thanks for the feedback. We will update you shortly.
		</comment>
		<comment id='3' author='CESARDELATORRE' date='2020-04-21T15:32:29Z'>
		&lt;denchmark-link:https://github.com/CESARDELATORRE&gt;@CESARDELATORRE&lt;/denchmark-link&gt;
 regarding the dataset limit, how does that limit look for larger time frames?
		</comment>
		<comment id='4' author='CESARDELATORRE' date='2020-04-21T18:35:21Z'>
		The dataset size limit in Azure AutoML is 100GB if using AML remote compute runs using certain trainers/algorithms that will stream the data while training from Azure ML Datasets.
Those trainers are based on NimbusML/ML.NET that support data-streaming and would be automatically selected due to the data size.
If not using those trainers supporting data-streaming (i.e. if using AutoML local runs), then the limit will be in the local compute / VM memory and what it can handle when using Pandas Dataframe.
Is that what you are asking?
		</comment>
		<comment id='5' author='CESARDELATORRE' date='2020-04-29T17:30:59Z'>
		Docs work is done. Changes will appear in next sdk release 1-2 weeks.
		</comment>
		<comment id='6' author='CESARDELATORRE' date='2020-05-16T00:05:45Z'>
		&lt;denchmark-link:https://github.com/vmagelo&gt;@vmagelo&lt;/denchmark-link&gt;
 - If this is done, can you close/hide this issue?
It's still appearing in the doc page.
		</comment>
	</comments>
</bug>