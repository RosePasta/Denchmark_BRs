<bug id='774' author='BillmanH' open_date='2020-02-07T00:49:13Z' closed_time='2020-02-07T17:35:53Z'>
	<summary>explain_model() depreciation warning resolves to dead link</summary>
	<description>
azureml.85
WARNING - explain_model() will be deprecated. Please use the workflow described at the link https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/automated-machine-learning/model-explanation/auto-ml-model-explanation.ipynb to compute explanations for AutoML models.
Not sure what I was expecting. However it may be difficult to maintain links to notebooks for an infrastructure that changes this fast. Syncing notebooks with depreciation warnings sounds like a problem waiting to happen.
&lt;denchmark-link:https://user-images.githubusercontent.com/10108711/73991263-80536c80-4900-11ea-8a4a-b0c80ec690df.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='BillmanH' date='2020-02-07T01:36:23Z'>
		&lt;denchmark-link:https://github.com/BillmanH&gt;@BillmanH&lt;/denchmark-link&gt;
 Apologies for the broken link.
The correct link is &lt;denchmark-link:https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/automated-machine-learning/regression-hardware-performance-explanation-and-featurization/auto-ml-regression-hardware-performance-explanation-and-featurization.ipynb&gt;here&lt;/denchmark-link&gt;

Would you mind posting the version of SDK you are using?
		</comment>
		<comment id='2' author='BillmanH' date='2020-02-07T17:19:39Z'>
		- azureml-sdk[automl,explain,notebooks]==1.0.85
Sorry I thought I had. :)
		</comment>
		<comment id='3' author='BillmanH' date='2020-02-17T03:08:29Z'>
		&lt;denchmark-link:https://github.com/skasturi&gt;@skasturi&lt;/denchmark-link&gt;
 , Is this really the correct method? I mean this documentation is really making some leaps and bounds here. The depreciated  has been replaced with a process where you can upload a   (with no documentation regarding what this file does) and then run a pipeline step to explain the model? Where is the part where Azure is helpful in explaining the model?  I tried the steps prescribed but I got the error:  .  So Now in .85 we can't explain a model unless we built a script do explain it ourselves? This makes no sense.
		</comment>
		<comment id='4' author='BillmanH' date='2020-02-17T04:26:39Z'>
		&lt;denchmark-link:https://github.com/imatiach-msft&gt;@imatiach-msft&lt;/denchmark-link&gt;
 ?
		</comment>
		<comment id='5' author='BillmanH' date='2020-02-17T06:34:34Z'>
		&lt;denchmark-link:https://github.com/BillmanH&gt;@BillmanH&lt;/denchmark-link&gt;
 Sorry about that. Adding folks that might be able to help here.
&lt;denchmark-link:https://github.com/gaugup&gt;@gaugup&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jialiu103&gt;@jialiu103&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/rtanase&gt;@rtanase&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/cartacioS&gt;@cartacioS&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='BillmanH' date='2020-02-18T00:09:09Z'>
		After setting up the train_explainer.py step we found that the explanation returns a raw_explanations.get_feature_importance_dict() which is the global importance or general feature importance (similar to .predict_prob()) but we can't find any kind of local importance. There is no mention of it in the notebook as the notebook just directs the user to the custom dashboard widget, which only opens in a notebook. Is there something like the explain_model() that will give you the actual explanation values?
		</comment>
		<comment id='7' author='BillmanH' date='2020-02-18T14:39:58Z'>
		Hi &lt;denchmark-link:https://github.com/BillmanH&gt;@BillmanH&lt;/denchmark-link&gt;
, your concerns about the way explanation is done in this notebook are valid, but I'll leave it to the folks mentioned above to respond to those.
To answer your specific question above local importance values, you should be able to use the  and  properties to get the information you're looking for about each data point. It's quite a bit to look through, but the source code for the  class is &lt;denchmark-link:https://github.com/interpretml/interpret-community/blob/master/python/interpret_community/explanation/explanation.py&gt;here&lt;/denchmark-link&gt;
. Not every explanation is guaranteed to have local importances, but if it does, they will be accessible through the above property.
I and &lt;denchmark-link:https://github.com/imatiach-msft&gt;@imatiach-msft&lt;/denchmark-link&gt;
 are happy to answer any other model interpretability questions you have!
		</comment>
		<comment id='8' author='BillmanH' date='2020-02-18T15:43:26Z'>
		"The depreciated explain_model() has been replaced with a process where you can upload a train_explainer.py and then run a pipeline step to explain the model? Where is the part where Azure is helpful in explaining the model?"
&lt;denchmark-link:https://github.com/BillmanH&gt;@BillmanH&lt;/denchmark-link&gt;
 this notebook is running explanations remotely on azure compute.  You can also run them locally like in the previous notebook.  Maybe this notebook doesn't fit your scenario, as you don't want to submit a whole remote process to compute the explanations.
"After setting up the train_explainer.py step we found that the explanation returns a raw_explanations.get_feature_importance_dict() which is the global importance or general feature importance (similar to .predict_prob()) but we can't find any kind of local importance."
I believe in this example there should be a "explanation.local_importance_values" as &lt;denchmark-link:https://github.com/wamartin-aml&gt;@wamartin-aml&lt;/denchmark-link&gt;
  wrote above based on the code I looked through, but I haven't run the example.
		</comment>
		<comment id='9' author='BillmanH' date='2020-02-18T17:17:56Z'>
		&lt;denchmark-link:https://github.com/imatiach-msft&gt;@imatiach-msft&lt;/denchmark-link&gt;
  Because all pipeline steps are run in the MLS, none of them are run locally. So even having the  doesn't ever run  .    Yes you could run them locally and this helps troubleshooting and developing, but all of the pipeline is done in the cloud. The only addition to the newer version is that you have to build a separate explain_trainer script, submit that as a completely independent run, where before the container in the cloud could just do this. It feels like you have added a really large process to replace a single method call. Online or offline.
&lt;denchmark-link:https://github.com/wamartin-aml&gt;@wamartin-aml&lt;/denchmark-link&gt;
 thanks for that! That's probably what I'm looking for, kind of a separate thread and sorry for the divergence from the main topic.
		</comment>
	</comments>
</bug>