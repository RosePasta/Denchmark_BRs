<bug id='7106' author='elpollouk' open_date='2020-02-10T13:13:17Z' closed_time='2020-02-14T06:30:45Z'>
	<summary>[RLlib] Errors originating from environment workers cause training to stall</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

Ray: 0.8.0/0.8.1
OS: Ubuntu 16.04/18.04
Errors originating from environment step() or reset() functions cause training to stall when using  AsyncSamplesOptimizer (e.g. IMPALA) even if ignore_worker_failures: True is set in the experiment config.
If an environment request fails, the generator returned by  never runs to completion and so  isn't cleared down properly. This results in stale object ids being used on the next training iteration causing &lt;denchmark-link:https://github.com/ray-project/ray/issues/7105&gt;#7105&lt;/denchmark-link&gt;
 to manifest itself, stalling the experiment.
&lt;denchmark-h:h3&gt;Reproduction&lt;/denchmark-h&gt;


Set up an IMPALA experiment with external environments
After a few training iterations, kill one of the external environments

&lt;denchmark-h:h4&gt;Result:&lt;/denchmark-h&gt;

After Trainer._try_recover() has black listed the failed worker, the experiment will stall with the following call stack:
&lt;denchmark-code&gt;Thread 46562 (idle): "MainThread"
    get_objects (ray/worker.py:318)
    get (ray/worker.py:1450)
    ray_get_and_free (ray/rllib/utils/memory.py:33)
    _augment_with_replay (ray/rllib/optimizers/aso_aggregator.py:170)
    iter_train_batches (ray/rllib/optimizers/aso_aggregator.py:117)
    _step (ray/rllib/optimizers/async_samples_optimizer.py:178)
    step (ray/rllib/optimizers/async_samples_optimizer.py:136)
    _train (ray/rllib/agents/trainer_template.py:129)
    train (ray/tune/trainable.py:176)
    train (ray/rllib/agents/trainer.py:433)
    actor_method_executor (ray/function_manager.py:766)
    main_loop (ray/worker.py:433)
    &lt;module&gt; (ray/workers/default_worker.py:118)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Potential Fix&lt;/denchmark-h&gt;

Rather than using a list for self._fetching in TaskPool, you can use a queue so that items that are going to be freed are automatically removed from the list of tasks to be fetched. This also ensures that the fetching list is always up to date, even if the generator is stopped early.
Here's a example fix that I'm currently testing:
&lt;denchmark-link:https://gist.github.com/elpollouk/1d0cb83bd98c7a9fa4c9226c66ea07ee&gt;actors.py.diff&lt;/denchmark-link&gt;

I haven't dug into the access pattern for TaskPool beyond my current scenario, so I'm not sure if a lock should be added around modifications to self._fetching. I'm also unaware if there are any subtle side effects of removing items from the fetching list that the original authors were considering when implementing the current logic, so any thoughts here would be greatly appreciated.
	</description>
	<comments>
		<comment id='1' author='elpollouk' date='2020-02-11T06:57:01Z'>
		&lt;denchmark-link:https://github.com/elpollouk&gt;@elpollouk&lt;/denchmark-link&gt;
 could you try out the latest wheel and see if this is still an issue? The hang fix is merged in master as of &lt;denchmark-link:https://github.com/ray-project/ray/pull/7117&gt;#7117&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='elpollouk' date='2020-02-11T10:31:08Z'>
		Trying the latest nightly and the experiment does continue to run after knocking out workers.
However, it does appear that you're still re-using object ids for sample batches you've already processed after an error. Is this a concern?
The issue I was attempting to fix with my patch was that you are relying on the  generator to run to completion in order to maintain correct state in the . However, generator completion is never guaranteed as iteration can be stopped at any time either by an exception or other coded stopping condition. Although fixing the symptom of &lt;denchmark-link:https://github.com/ray-project/ray/issues/7105&gt;#7105&lt;/denchmark-link&gt;
 is working now, the fact that it possible for  to get itself into an inconsistent state will likely manifest itself again in the future via other non-obvious bugs.
		</comment>
		<comment id='3' author='elpollouk' date='2020-02-12T04:35:02Z'>
		The patch makes sense. Do you want to make a PR for it?
		</comment>
		<comment id='4' author='elpollouk' date='2020-02-12T17:26:33Z'>
		PR submitted, have fun!
		</comment>
	</comments>
</bug>