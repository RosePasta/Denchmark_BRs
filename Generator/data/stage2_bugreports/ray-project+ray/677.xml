<bug id='677' author='robertnishihara' open_date='2017-06-16T23:53:06Z' closed_time='2017-06-18T07:42:33Z'>
	<summary>Plasma manager being marked dead because call to Create takes 60 seconds.</summary>
	<description>
While debugging &lt;denchmark-link:https://github.com/ray-project/ray/issues/670&gt;#670&lt;/denchmark-link&gt;
, I came across the following problem.
The setup is the following.


One head node (m4.16xlarge)


99 other nodes (c4.xlarge)


Run the following workload on the head node
@ray.remote
def f():
    return 1

%time l = ray.get([f.remote() for _ in range(10000000)])


After several minutes, the plasma manager logs a fatal error for taking too long to send a heartbeat. Adding in some timing statements, the problem seems to be that the call
  Status s = conn-&gt;manager_state-&gt;plasma_conn-&gt;Create(
      object_id, data_size, NULL, metadata_size, &amp;(buf-&gt;data));
in process_data_request in plasma_manager.cc, which creates an object in the object store (and which is a blocking call), takes 60 seconds 


ray/src/plasma/plasma_manager.cc


         Line 911
      in
      96962cd






 Status s = conn-&gt;manager_state-&gt;plasma_conn-&gt;Create( 




 (this is an absurd amount of time). Note that at this point in the script, that line has only been called about 700 times. All prior calls to that line took around 0ms, and all subsequent calls to that line took around 6ms. Note that the number 700 seems very small. This could be because the vast majority of tasks are getting executed locally.
	</description>
	<comments>
		<comment id='1' author='robertnishihara' date='2017-06-17T05:21:17Z'>
		The problem is that the following. The store takes a while to get around to processing the manager's Create call because it is busy processing a get request from the driver for 10 million objects.
Possible solution

Large get requests to the store should probably be broken down into smaller requests (e.g., perhaps 10000 IDs at a time).
It shouldn't be necessary to address this problem, but large wait and fetch requests to the manager should probably be broken down as well.

Where should we break the request into smaller pieces? There are a couple of options.

In Python
In the plasma client
In the store and in the manager

		</comment>
		<comment id='2' author='robertnishihara' date='2017-06-17T17:55:26Z'>
		This problem seems to be fixed by the combination of &lt;denchmark-link:https://github.com/ray-project/ray/pull/678&gt;#678&lt;/denchmark-link&gt;
 and starting 20 Redis shards.
With only 1 shard, I still see the following error printed by the Redis shard.
&lt;denchmark-code&gt;18733:M 17 Jun 01:58:16.386 # Client id=23 addr=172.31.20.0:50278 fd=14 name= age=2717 idle=2055 flags=N db=0 sub=3 psub=0 multi=-1 qbuf=0 qbuf-free=0 obl=16372 oll=8186 omem=134217732 events=rw cmd=subscribe scheduled to be closed ASAP for overcoming of output buffer limits.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='robertnishihara' date='2017-06-18T07:42:33Z'>
		Closing because this is fixed as described above, but we need to give a better error message in the case where Redis gets overwhelmed.
		</comment>
	</comments>
</bug>