<bug id='7483' author='janblumenkamp' open_date='2020-03-06T11:18:26Z' closed_time='2020-03-08T18:00:16Z'>
	<summary>Segmentation fault when using PyTorch with GPU (PPO/PG/A2C).</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

In the latest wheels, PyTorch seems to be broken (I noticed this initially in &lt;denchmark-link:https://github.com/ray-project/ray/issues/7421&gt;#7421&lt;/denchmark-link&gt;
). A segmentation fault occurs when attempting to train a sample environment with PPO.
Ray version and other system information (Python version, TensorFlow version, OS):

Ray version 0.9.0.dev0 (latest wheel)
PyTorch version 1.4.0
Python version 3.6.9
Linux 5.3.0-28-generic 30~18.04.1-Ubuntu SMP x86_64 x86_64 x86_64 GNU/Linux

&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import ray
from ray.rllib.agents.ppo import PPOTrainer

ray.init()
agent = PPOTrainer({'use_pytorch': True}, 'CartPole-v0')
agent.train()
&lt;/denchmark-code&gt;

Output:
&lt;denchmark-code&gt;2020-03-06 11:14:17,818	INFO resource_spec.py:212 -- Starting Ray with 23.73 GiB memory available for workers and up to 11.87 GiB for objects. You can adjust these settings with ray.init(memory=&lt;bytes&gt;, object_store_memory=&lt;bytes&gt;).
2020-03-06 11:14:18,330	INFO services.py:1113 -- View the Ray dashboard at localhost:8265
2020-03-06 11:14:18,332	WARNING services.py:1442 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 9023520768 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2020-03-06 11:14:18,694	ERROR logger.py:185 -- pip install 'ray[tune]' to see TensorBoard files.
2020-03-06 11:14:18,695	WARNING logger.py:293 -- Could not instantiate TBXLogger: No module named 'tensorboardX'.
2020-03-06 11:14:18,697	INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
2020-03-06 11:14:23,046	WARNING util.py:37 -- Install gputil for GPU system monitoring.
*** Aborted at 1583493271 (unix time) try "date -d @1583493271" if you are using GNU date ***
PC: @                0x0 (unknown)
*** SIGSEGV (@0x1) received by PID 72266 (TID 0x7f0a6c7cf740) from PID 1; stack trace: ***
    @     0x7f0a6c235f20 (unknown)
    @     0x7f0a6c3855a1 (unknown)
    @     0x7f0a6c2544d3 _IO_vfprintf
    @     0x7f0a6c27f910 vsnprintf
    @     0x7f0a5fde6154 torch::formatMessage()
    @     0x7f0a5fde6476 torch::TypeError::TypeError()
    @     0x7f0a600d9cee torch::utils::(anonymous namespace)::new_with_tensor()
    @     0x7f0a600dcff7 torch::utils::legacy_tensor_ctor()
    @     0x7f0a5ff2a496 THPVariable_pynew()
    @           0x551b15 (unknown)
    @           0x5aa6ec _PyObject_FastCallKeywords
    @           0x50abb3 (unknown)
    @           0x50c5b9 _PyEval_EvalFrameDefault
    @           0x508245 (unknown)
    @           0x50a080 (unknown)
    @           0x50aa7d (unknown)
    @           0x50c5b9 _PyEval_EvalFrameDefault
    @           0x508245 (unknown)
    @           0x509642 _PyFunction_FastCallDict
    @           0x595311 (unknown)
    @           0x54a6ff (unknown)
    @           0x551b81 (unknown)
    @           0x5aa6ec _PyObject_FastCallKeywords
    @           0x50abb3 (unknown)
    @           0x50c5b9 _PyEval_EvalFrameDefault
    @           0x509d48 (unknown)
    @           0x50aa7d (unknown)
    @           0x50c5b9 _PyEval_EvalFrameDefault
    @           0x509d48 (unknown)
    @           0x50aa7d (unknown)
    @           0x50c5b9 _PyEval_EvalFrameDefault
    @           0x508245 (unknown)
Segmentation fault (core dumped)
&lt;/denchmark-code&gt;


 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='janblumenkamp' date='2020-03-06T11:44:29Z'>
		The segmentation fault was introduced with &lt;denchmark-link:https://github.com/ray-project/ray/commit/4198db5038dd465a5d5db91b0ca84d98391aec2d&gt;4198db5&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='2' author='janblumenkamp' date='2020-03-06T12:28:33Z'>
		Taking a look ...
		</comment>
		<comment id='3' author='janblumenkamp' date='2020-03-06T14:07:23Z'>
		Yes, getting the same error now (also with PG/A2C, but not A3C!), when using num_gpus=1, num_workers: [any number, incl. 0] AND --torch.
UPDATE: This is not(!) happening on older releases (good on 0.8.3), only in master.
		</comment>
		<comment id='4' author='janblumenkamp' date='2020-03-06T15:43:20Z'>
		I also get this error with
&lt;denchmark-code&gt;ray.init(num_gpus=0)
agent = PPOTrainer({'use_pytorch': True, 'num_gpus': 0, 'num_workers': 1}, 'CartPole-v0')
&lt;/denchmark-code&gt;

but nvidia-smi still lists the script, so I am not sure if the parameter even works.
		</comment>
		<comment id='5' author='janblumenkamp' date='2020-03-08T14:43:36Z'>
		I'm getting the same behaviour as &lt;denchmark-link:https://github.com/janblumenkamp&gt;@janblumenkamp&lt;/denchmark-link&gt;
 with setting  to 0 as well.
I built Ray today from 
		</comment>
		<comment id='6' author='janblumenkamp' date='2020-03-08T17:14:24Z'>
		I found the problem. Will do a PR today. ...
		</comment>
		<comment id='7' author='janblumenkamp' date='2020-03-08T18:00:16Z'>
		Please apply this PR, which fixes this issue and was tested on a GPU machine.
The bug was to wrap a torch.Tensor via another  inside the  class.
&lt;denchmark-link:https://github.com/ray-project/ray/pull/7508&gt;#7508&lt;/denchmark-link&gt;

Closing this issue.
		</comment>
	</comments>
</bug>