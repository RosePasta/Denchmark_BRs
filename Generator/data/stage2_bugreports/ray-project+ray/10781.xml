<bug id='10781' author='rkube' open_date='2020-09-14T20:06:31Z' closed_time='2020-09-15T15:51:08Z'>
	<summary>Segmentation faults in SLURM example</summary>
	<description>
Hi,
I'm trying to start a ray cluster from within slurm. For this I'm following the SLURM example here: &lt;denchmark-link:https://docs.ray.io/en/master/cluster/slurm.html?highlight=slurm#deploying-on-slurm&gt;https://docs.ray.io/en/master/cluster/slurm.html?highlight=slurm#deploying-on-slurm&lt;/denchmark-link&gt;

I'm experiencing IOError: 14 error messages when running the example program:
&lt;denchmark-code&gt;2020-09-14 16:14:53,908	WARNING utils.py:536 -- Detecting limited number of CPUs due to docker. In previous versions of Ray, CPU detection in containers was buggy. Please ensure that Ray has enough CPUs allocated. This message will be removed in future version of Ray. You can set the environment variable: `RAY_DISABLE_DOCKER_CPU_WARNING` to remove it now.
2020-09-14 16:14:54,358	INFO services.py:1164 -- View the Ray dashboard at http://10.33.25.39:8265
2020-09-14 16:14:59,444	WARNING utils.py:536 -- Detecting limited number of CPUs due to docker. In previous versions of Ray, CPU detection in containers was buggy. Please ensure that Ray has enough CPUs allocated. This message will be removed in future version of Ray. You can set the environment variable: `RAY_DISABLE_DOCKER_CPU_WARNING` to remove it now.
2020-09-14 16:14:59,434	INFO scripts.py:682 -- Local node IP: 10.33.25.42
2020-09-14 16:14:59,465	SUCC scripts.py:697 -- --------------------
2020-09-14 16:14:59,465	SUCC scripts.py:698 -- Ray runtime started.
2020-09-14 16:14:59,465	SUCC scripts.py:699 -- --------------------
2020-09-14 16:14:59,465	INFO scripts.py:701 -- To terminate the Ray runtime, run
2020-09-14 16:14:59,465	INFO scripts.py:702 --   ray stop
2020-09-14 16:14:59,554	WARNING utils.py:536 -- Detecting limited number of CPUs due to docker. In previous versions of Ray, CPU detection in containers was buggy. Please ensure that Ray has enough CPUs allocated. This message will be removed in future version of Ray. You can set the environment variable: `RAY_DISABLE_DOCKER_CPU_WARNING` to remove it now.
2020-09-14 16:14:59,596	WARNING utils.py:536 -- Detecting limited number of CPUs due to docker. In previous versions of Ray, CPU detection in containers was buggy. Please ensure that Ray has enough CPUs allocated. This message will be removed in future version of Ray. You can set the environment variable: `RAY_DISABLE_DOCKER_CPU_WARNING` to remove it now.
2020-09-14 16:14:59,548	INFO scripts.py:682 -- Local node IP: 10.33.25.41
2020-09-14 16:14:59,574	SUCC scripts.py:697 -- --------------------
2020-09-14 16:14:59,574	SUCC scripts.py:698 -- Ray runtime started.
2020-09-14 16:14:59,574	SUCC scripts.py:699 -- --------------------
2020-09-14 16:14:59,574	INFO scripts.py:701 -- To terminate the Ray runtime, run
2020-09-14 16:14:59,574	INFO scripts.py:702 --   ray stop
2020-09-14 16:14:59,604	WARNING utils.py:536 -- Detecting limited number of CPUs due to docker. In previous versions of Ray, CPU detection in containers was buggy. Please ensure that Ray has enough CPUs allocated. This message will be removed in future version of Ray. You can set the environment variable: `RAY_DISABLE_DOCKER_CPU_WARNING` to remove it now.
2020-09-14 16:14:59,592	INFO scripts.py:682 -- Local node IP: 10.33.25.40
2020-09-14 16:14:59,612	SUCC scripts.py:697 -- --------------------
2020-09-14 16:14:59,612	SUCC scripts.py:698 -- Ray runtime started.
2020-09-14 16:14:59,612	SUCC scripts.py:699 -- --------------------
2020-09-14 16:14:59,612	INFO scripts.py:701 -- To terminate the Ray runtime, run
2020-09-14 16:14:59,613	INFO scripts.py:702 --   ray stop
2020-09-14 16:14:59,594	INFO scripts.py:682 -- Local node IP: 10.33.25.40
2020-09-14 16:14:59,627	SUCC scripts.py:697 -- --------------------
2020-09-14 16:14:59,627	SUCC scripts.py:698 -- Ray runtime started.
2020-09-14 16:14:59,628	SUCC scripts.py:699 -- --------------------
2020-09-14 16:14:59,628	INFO scripts.py:701 -- To terminate the Ray runtime, run
2020-09-14 16:14:59,628	INFO scripts.py:702 --   ray stop
Starting with 15 cpus, redis_password=45d81d4e-406b-43fe-a542-1400a101b771, ip_head = traverse-k05g1:6379
2020-09-14 16:15:06,409	INFO worker.py:631 -- Connecting to existing Ray cluster at address: 10.33.25.39:6379
Nodes in the Ray cluster:
[{'NodeID': '6363a6c225aca3baac918c357ef036e39859e1c2', 'Alive': True, 'NodeManagerAddress': '10.33.25.42', 'NodeManagerHostname': 'traverse-k05g4', 'NodeManagerPort': 48349, 'ObjectManagerPort': 44861, 'ObjectStoreSocketName': '/tmp/ray/session_2020-09-14_16-14-53_905722_1932387/sockets/plasma_store', 'RayletSocketName': '/tmp/ray/session_2020-09-14_16-14-53_905722_1932387/sockets/raylet', 'MetricsExportPort': 56327, 'alive': True, 'Resources': {'CPU': 1.0, 'object_store_memory': 1541.0, 'GPU': 4.0, 'accelerator_type:V100': 1.0, 'memory': 5213.0, 'node:10.33.25.42': 1.0}}, {'NodeID': '3cccfac7ae07c349fd5087b4a6dddbd3efa4e5a5', 'Alive': True, 'NodeManagerAddress': '10.33.25.39', 'NodeManagerHostname': 'traverse-k05g1', 'NodeManagerPort': 62280, 'ObjectManagerPort': 46339, 'ObjectStoreSocketName': '/tmp/ray/session_2020-09-14_16-14-53_905722_1932387/sockets/plasma_store', 'RayletSocketName': '/tmp/ray/session_2020-09-14_16-14-53_905722_1932387/sockets/raylet', 'MetricsExportPort': 44552, 'alive': True, 'Resources': {'CPU': 1.0, 'node:10.33.25.39': 1.0, 'object_store_memory': 1541.0, 'GPU': 4.0, 'accelerator_type:V100': 1.0, 'memory': 5022.0}}, {'NodeID': 'dff082c5f157e56b7b6df773153793c58648569f', 'Alive': True, 'NodeManagerAddress': '10.33.25.41', 'NodeManagerHostname': 'traverse-k05g3', 'NodeManagerPort': 54790, 'ObjectManagerPort': 39415, 'ObjectStoreSocketName': '/tmp/ray/session_2020-09-14_16-14-53_905722_1932387/sockets/plasma_store', 'RayletSocketName': '/tmp/ray/session_2020-09-14_16-14-53_905722_1932387/sockets/raylet', 'MetricsExportPort': 61774, 'alive': True, 'Resources': {'CPU': 1.0, 'object_store_memory': 1540.0, 'GPU': 4.0, 'accelerator_type:V100': 1.0, 'memory': 5208.0, 'node:10.33.25.41': 1.0}}]
E0914 16:15:06.454917 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: failed to connect to all addresses
E0914 16:15:06.456924 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: failed to connect to all addresses
E0914 16:15:09.617141 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: Broken pipe
E0914 16:15:10.621474 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: Broken pipe
E0914 16:15:10.624989 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: failed to connect to all addresses
E0914 16:15:11.625656 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: Broken pipe
E0914 16:15:12.630774 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: Broken pipe
E0914 16:15:15.643636 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: Broken pipe
E0914 16:15:15.646374 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: failed to connect to all addresses
E0914 16:15:15.648901 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: failed to connect to all addresses
E0914 16:15:15.649963 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: failed to connect to all addresses
E0914 16:15:15.651664 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: failed to connect to all addresses
E0914 16:15:17.651527 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: Broken pipe
E0914 16:15:17.656320 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: failed to connect to all addresses
E0914 16:15:17.658658 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: failed to connect to all addresses
E0914 16:15:19.661664 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: Broken pipe
Counter({'10.33.25.39': 15})
15.222636461257935
E0914 16:15:21.675652 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: Broken pipe
E0914 16:15:21.677510 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: failed to connect to all addresses
E0914 16:15:21.679807 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: Broken pipe
E0914 16:15:21.680377 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: Broken pipe
E0914 16:15:21.682847 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: Broken pipe
E0914 16:15:23.687570 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: Broken pipe
E0914 16:15:23.692521 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: Broken pipe
E0914 16:15:24.691300 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: Broken pipe
E0914 16:15:24.696139 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: Broken pipe
E0914 16:15:24.699601 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: Broken pipe
E0914 16:15:25.696966 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: Broken pipe
E0914 16:15:25.700305 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: failed to connect to all addresses
E0914 16:15:25.702937 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: failed to connect to all addresses
E0914 16:15:25.703989 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: failed to connect to all addresses
E0914 16:15:25.705020 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: failed to connect to all addresses
E0914 16:15:25.706033 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: failed to connect to all addresses
E0914 16:15:25.707054 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: failed to connect to all addresses
E0914 16:15:25.708427 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: failed to connect to all addresses
E0914 16:15:25.710048 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: failed to connect to all addresses
E0914 16:15:25.711653 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: failed to connect to all addresses
E0914 16:15:25.712651 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: failed to connect to all addresses
E0914 16:15:26.703191 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: Broken pipe
E0914 16:15:26.707192 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: Broken pipe
E0914 16:15:27.708966 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: Broken pipe
E0914 16:15:27.713201 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: Broken pipe
E0914 16:15:27.715847 1932514 1932528 direct_task_transport.cc:324] Retrying attempt to schedule task at remote node. Error: IOError: 14: Broken pipe
2020-09-14 16:15:29,466	WARNING worker.py:1070 -- The node with node id 6363a6c225aca3baac918c357ef036e39859e1c2 has been marked dead because the detector has missed too many heartbeats from it.
2020-09-14 16:15:29,570	WARNING worker.py:1070 -- The node with node id dff082c5f157e56b7b6df773153793c58648569f has been marked dead because the detector has missed too many heartbeats from it.
Counter({'10.33.25.39': 15})
15.080694437026978
Counter({'10.33.25.39': 15})
15.080720901489258
Counter({'10.33.25.39': 15})
15.076481103897095
&lt;/denchmark-code&gt;

This is my runscript:
&lt;denchmark-code&gt;#!/bin/bash
#SBATCH --job-name=test
#SBATCH --cpus-per-task=5
#SBATCH --mem-per-cpu=1GB
#SBATCH --nodes=4
#SBATCH --tasks-per-node 1
#SBATCH --time=00:30:00
#SBATCH --reservation=test

worker_num=3 # Must be one less that the total number of nodes

module load openmpi

suffix='6379'
ip_head=`hostname`:$suffix
redis_password=$(uuidgen)

export ip_head # Exporting for latter access by trainer.py

ray start --block --head --dashboard-host 0.0.0.0 &amp; #--redis-port=6379 &amp;
#--redis-password=$redis_password &amp;
sleep 5
# Make sure the head successfully starts before any worker does, otherwise
# the worker will not be able to connect to redis. In case of longer delay,
# adjust the sleeptime above to ensure proper order.

#srun -x `hostname` -N $worker_num ray start --block --address=$ip_head
srun -x `hostname` -n $worker_num ray start --address $ip_head
#--redis-password=$redis_password &amp;
sleep 5
&lt;/denchmark-code&gt;

python -u trainer.py $redis_password 15 # Pass the total number of allocated CPUs
Here is trainer.py:
&lt;denchmark-code&gt;from collections import Counter
import os
import sys
import time
import ray

redis_password = sys.argv[1]
num_cpus = int(sys.argv[2])

print(f"Starting with {num_cpus} cpus, redis_password={redis_password}, ip_head = {os.environ['ip_head']}")

ray.init(address=os.environ["ip_head"])#, redis_password=redis_password)

print("Nodes in the Ray cluster:")
print(ray.nodes())

@ray.remote
def f():
    time.sleep(1)
    return ray.services.get_node_ip_address()

# The following takes one second (assuming that ray was able to access all of the allocated nodes).
for i in range(60):
    start = time.time()
    ip_addresses = ray.get([f.remote() for _ in range(num_cpus)])
    print(Counter(ip_addresses))
    end = time.time()
    print(end - start)
&lt;/denchmark-code&gt;

Any ideas what is going on?
	</description>
	<comments>
		<comment id='1' author='rkube' date='2020-09-14T20:49:21Z'>
		&lt;denchmark-link:https://github.com/rkube&gt;@rkube&lt;/denchmark-link&gt;
 any clue why  gets printed 5 times?
		</comment>
		<comment id='2' author='rkube' date='2020-09-14T22:24:12Z'>
		I believe this is coming from srun -x hostname -n $worker_num ray start --address $ip_head.
But it should only be 3 times, since worder_num=3. I'll take a look.
		</comment>
		<comment id='3' author='rkube' date='2020-09-14T22:31:22Z'>
		Sorry - 4 times. Question though; does it work if you remove that?
		</comment>
		<comment id='4' author='rkube' date='2020-09-15T01:46:49Z'>
		Hm, this is tricky. Apparently srun ignores the -x parameter (exclude the host on which the head node runs). It also ignores -N worker_num. I'll need to dig into slurm a bit to figure out how to properly launch the worker nodes on our installation.
		</comment>
		<comment id='5' author='rkube' date='2020-09-15T15:51:08Z'>
		Solved. The error was due to misconfiguration of slurm.


It is important to launch ray with --block to keep the process running. If --block is omitted the ray processes will quit immediately. This leads to lost connections.


The example slurm script can be simplified by unrolling the loop and using the srun parameters -w (--nodelist) and -x (--exclude).


Fix:
#!/bin/bash
#SBATCH --job-name=test
#SBATCH --cpus-per-task=5
#SBATCH --mem-per-cpu=1GB
#SBATCH --nodes=3
#SBATCH --tasks-per-node=1
#SBATCH --time=00:30:00
#SBATCH --reservation=test

worker_num=3 # Must be one less that the total number of nodes

suffix='6379'
ip_head=`hostname`:$suffix
export ip_head # Exporting for latter access by trainer.py

srun -N 1 -n 1 -c 5 -w `hostname` ray start --head --block --dashboard-host 0.0.0.0 --port=6379 --num-cpus 5 &amp;
sleep 5
# Make sure the head successfully starts before any worker does, otherwise
# the worker will not be able to connect to redis. In case of longer delay,
# adjust the sleeptime above to ensure proper order.

srun -N 3 -n 3 -c 5 -x `hostname` ray start --address $ip_head --block --num-cpus 5 &amp; 
sleep 5

# Note that we do not pass a redis password. The number of cpus needs to match nnodes*cpus-per-task
python -u trainer.py  15 # Pass the total number of allocated CPUs
		</comment>
		<comment id='6' author='rkube' date='2020-09-15T16:07:21Z'>
		hey &lt;denchmark-link:https://github.com/rkube&gt;@rkube&lt;/denchmark-link&gt;
 would you be interested to pushing a patch to the docs with a comment about this adjustment?
I'd be happy to do it if you're too busy!
		</comment>
		<comment id='7' author='rkube' date='2020-09-15T16:19:02Z'>
		
hey @rkube would you be interested to pushing a patch to the docs with a comment about this adjustment?
I'd be happy to do it if you're too busy!

No problem, I'll do it later today or tomorrow.
		</comment>
	</comments>
</bug>