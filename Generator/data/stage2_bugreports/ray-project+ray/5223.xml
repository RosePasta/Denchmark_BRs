<bug id='5223' author='ConeyLiu' open_date='2019-07-18T08:47:09Z' closed_time='2020-02-06T22:41:07Z'>
	<summary>Can't forward task to worker</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos 7
Ray installed from (source or binary): binary
Ray version: 0.7.2 and 0.8
Python version: 3.7
Exact command to reproduce:

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

I have installed three nodes cluster followed by the steps of Manual Cluster Setup. However, it can't forward task to workers. The raylet.err as follows:
&lt;denchmark-code&gt;WARNING: Logging before InitGoogleLogging() is written to STDERR
I0718 13:23:43.323923 41069 stats.h:48] Succeeded to initialize stats: exporter address is 127.0.0.1:8888
I0718 13:23:43.327697 41069 grpc_server.cc:26] ObjectManager server started, listening on port 36019.
I0718 13:23:43.331595 41069 grpc_server.cc:26] NodeManager server started, listening on port 35998.
I0718 13:29:14.056089 41069 node_manager.cc:2179] Failed to forward task ec3d4eacef369aaf0083f381191cb022 to node manager 9e2dc91c6fb9e5785d3010257af8f0bdc241e608
I0718 13:29:14.056623 41069 node_manager.cc:2179] Failed to forward task cde20bbe70ee98699efd250aaab06d7a to node manager 9e2dc91c6fb9e5785d3010257af8f0bdc241e608
I0718 13:29:14.068112 41069 node_manager.cc:2179] Failed to forward task 564ba1584451f50dde1b7da70951b856 to node manager 9e2dc91c6fb9e5785d3010257af8f0bdc241e608
I0718 13:29:14.068373 41069 node_manager.cc:2179] Failed to forward task 81a76cf75730da42983a241c7303b5e4 to node manager 9e2dc91c6fb9e5785d3010257af8f0bdc241e608
I0718 13:29:14.068522 41069 node_manager.cc:2179] Failed to forward task 536a8c0a9d7217909c2504ab0a04c39b to node manager 9e2dc91c6fb9e5785d3010257af8f0bdc241e608
I0718 13:29:14.068678 41069 node_manager.cc:2179] Failed to forward task 96b13c5b95f16bd8a0d5f72743c870e7 to node manager 9e2dc91c6fb9e5785d3010257af8f0bdc241e608
I0718 13:29:14.068814 41069 node_manager.cc:2179] Failed to forward task be6f339d5e2325b53df82abd0a9cd393 to node manager 9e2dc91c6fb9e5785d3010257af8f0bdc241e608
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='ConeyLiu' date='2019-07-18T08:48:39Z'>
		I have disabled firewall. The network should be OK because I can run Spark workloads.
		</comment>
		<comment id='2' author='ConeyLiu' date='2019-08-14T09:22:31Z'>
		same issue
		</comment>
		<comment id='3' author='ConeyLiu' date='2019-09-09T17:24:03Z'>
		Same issue here in &lt;denchmark-link:https://github.com/ray-project/ray/issues/5454&gt;#5454&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='ConeyLiu' date='2019-09-29T07:20:54Z'>
		&lt;denchmark-link:https://github.com/richardliaw&gt;@richardliaw&lt;/denchmark-link&gt;
 Wondering is there any plan to fix this issue? I was also facing the same exception while trying to run the  on multiple nodes with  (Ray 0.7.4 and 0.7.5)
		</comment>
		<comment id='5' author='ConeyLiu' date='2019-09-30T08:41:10Z'>
		&lt;denchmark-link:https://github.com/zhichao-li&gt;@zhichao-li&lt;/denchmark-link&gt;
 I definitely want to fix this issue, but I can't reproduce this on my setups. If you have any insights/more information about your cluster setup, that would be very helpful.
		</comment>
		<comment id='6' author='ConeyLiu' date='2019-09-30T15:31:48Z'>
		Facing the same issue. ALSO CentOS. It works on Ubuntu and Arch clusters using the same settings (but different cloud infrastructure). Looks like everyone affected is using CentOS. Could it be related to the OS?
		</comment>
		<comment id='7' author='ConeyLiu' date='2019-09-30T23:02:26Z'>
		Got it; I'll try this out now.
		</comment>
		<comment id='8' author='ConeyLiu' date='2019-10-01T01:33:58Z'>
		# An unique identifier for the head node and workers of this cluster.
cluster_name: centos-test

# The minimum number of workers nodes to launch in addition to the head
# node. This number should be &gt;= 0.
min_workers: 2

# The maximum number of workers nodes to launch in addition to the head
# node. This takes precedence over min_workers.
max_workers: 2

# Cloud-provider specific configuration.
provider:
    type: aws
    region: us-east-1
    availability_zone: us-east-1a

# How Ray will authenticate with newly launched nodes.
auth:
    ssh_user: centos

head_node:
    InstanceType: m4.large
    ImageId: ami-02eac2c0129f6376b
    InstanceMarketOptions:
        MarketType: spot

worker_nodes:
    InstanceType: m4.large
    ImageId: ami-02eac2c0129f6376b
    InstanceMarketOptions:
        MarketType: spot

# Files or directories to copy to the head and worker nodes. The format is a
# dictionary from REMOTE_PATH: LOCAL_PATH, e.g.
file_mounts: {
}

# List of shell commands to run to set up nodes.
setup_commands:
    - cat /usr/bin/yum | grep python2 || sudo sed -i -e 's usr/bin/python usr/bin/python2 ' /usr/bin/yum
    - sudo yum install epel-release -y
    - sudo yum install python-pip -y
    - sudo yum install python-devel -y
    - sudo yum groupinstall -y 'development tools'
    - sudo yum install -y https://centos7.iuscommunity.org/ius-release.rpm || true
    - sudo yum update -y
    - sudo yum install -y python36u python36u-libs python36u-devel python36u-pip
    - sudo ln -fs /usr/bin/python3 /usr/bin/python
    - sudo ln -fs /usr/bin/pip3 /usr/bin/pip
    - sudo /usr/bin/pip install -U pip
    - pip install --user ray ray[rllib] tabulate

# Custom commands that will be run on the head node after common setup.
head_setup_commands:
    - pip install --user boto3==1.4.8  # 1.4.8 adds InstanceMarketOptions

# Custom commands that will be run on worker nodes after common setup.
worker_setup_commands: []

# Command to start ray on the head node. You don't need to change this.
head_start_ray_commands:
     -  ray stop
     - export RAY_BACKEND_LOG_LEVEL=debug &amp;&amp; ulimit -c unlimited &amp;&amp; ray start --head --redis-port=6379 --autoscaling-config=~/ray_bootstrap_config.yaml

# Command to start ray on worker nodes. You don't need to change this.
worker_start_ray_commands:
    -  ray stop
    -  export RAY_BACKEND_LOG_LEVEL=debug &amp;&amp; ray start --redis-address=$RAY_HEAD_IP:6379
&lt;denchmark-link:https://github.com/NikEyX&gt;@NikEyX&lt;/denchmark-link&gt;
 still not able to repro - can you take a look at the  (I used py3) and see if there's anything inconsistent with your setup? I was able to run a distributed Tune job very easily.
		</comment>
		<comment id='9' author='ConeyLiu' date='2019-10-01T14:34:34Z'>
		Richard, unfortunately I cannot use the autoscaler at my work - and it seems that everyone affected by this bug is using a manual cluster set-up as well. I'll try to outline a manual replication of the error below - maybe you are able to replicate it with this.
Given two CentOS systems with ray 0.8.0.dev5, I am running on the first system the head-node:
ray start --head --redis-port=6379 --redis-shard-ports=6380 --node-manager-port=12345 --object-manager-port=12346 --resources='{"Driver": 1.0}' --num-cpus=0
Note num-cpus is set to zero, so that the test task is forced to be executed on the child node. All ports are open and have been tested with nc.
And on the second system:
ray start --redis-address=&lt;IP_OF_FIRST_SYSTEM&gt;:6379 --node-manager-port=12345 --object-manager-port=12346 --resources='{"Node": 1.0}'
On the head/driver system I then run a simple python test script:
import ray
ray.init(redis_address="&lt;IP_OF_FIRST_SYSTEM&gt;:6379")

@ray.remote
def test():
    return "Success"

print(ray.available_resources())
print("Getting result:")
print(ray.get(test.remote()))
At that step the system hangs at the ray.get() call and starts filling up /tmp with gigabytes of "can't forward task to worker".
		</comment>
		<comment id='10' author='ConeyLiu' date='2019-10-03T19:35:27Z'>
		Did you have a chance to run this &lt;denchmark-link:https://github.com/richardliaw&gt;@richardliaw&lt;/denchmark-link&gt;
 ?
		</comment>
		<comment id='11' author='ConeyLiu' date='2019-10-08T21:29:25Z'>
		I haven't gotten a chance to reproduce this on AWS yet; I'll keep trying later this week.
		</comment>
		<comment id='12' author='ConeyLiu' date='2019-10-11T02:31:09Z'>
		&lt;denchmark-link:https://github.com/NikEyX&gt;@NikEyX&lt;/denchmark-link&gt;
 I was facing the same issue on Ubuntu 16.04.2 LTS with manually setting up the cluster. Also, I can reproduce the error with your code snippet.
		</comment>
		<comment id='13' author='ConeyLiu' date='2019-10-11T20:31:15Z'>
		ohh, ok, I guess it's not just CentOS then. Something else must be going on. Maybe it's transparent pages related? There are a few warnings about that in my redis logs, but really I'm in the dark right now
		</comment>
		<comment id='14' author='ConeyLiu' date='2019-10-18T18:50:39Z'>
		OK I made a little bit of progress on this. I think you get hanging if one of the ports is not open. If the redis shard port is not open, you'll get some error about things not being able to be forwarded.
This is the set of ports and their permissions that I've opened. That's 5 inbound.
&lt;denchmark-link:https://user-images.githubusercontent.com/4529381/67119521-34878680-f19c-11e9-9986-9ba83aa30536.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/4529381/67120181-a2807d80-f19d-11e9-8869-2b308d917e11.png&gt;&lt;/denchmark-link&gt;

This allows me to do the following on Ubuntu 16.04 (the default AWS image):
&lt;denchmark-code&gt;In [1]: import ray
WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.

In [2]: ray.init(address="localhost:6379")
2019-10-18 18:43:26,954	WARNING worker.py:1426 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.
Out[2]:
{'node_ip_address': '172.31.66.177',
 'redis_address': '172.31.66.177:6379',
 'object_store_address': '/tmp/ray/session_2019-10-18_18-43-16_809550_5270/sockets/plasma_store',
 'raylet_socket_name': '/tmp/ray/session_2019-10-18_18-43-16_809550_5270/sockets/raylet',
 'webui_url': None,
 'session_dir': '/tmp/ray/session_2019-10-18_18-43-16_809550_5270'}

In [3]:
   ...: @ray.remote
   ...: def test():
   ...:     return "Success"
   ...:
   ...: print(ray.available_resources())
   ...: print("Getting result:")
   ...: print(ray.get(test.remote()))
{'object_store_memory': 19.0, 'memory': 62.0, 'CPU': 2.0, 'Node': 1.0, 'Driver': 1.0}
Getting result:
Success
&lt;/denchmark-code&gt;

This seems to pass the success criteria. Given this, I'm out of ideas. Can someone provide a reproducible setup on AWS?
		</comment>
		<comment id='15' author='ConeyLiu' date='2019-10-21T07:42:28Z'>
		&lt;denchmark-link:https://github.com/richardliaw&gt;@richardliaw&lt;/denchmark-link&gt;
 , I tried it again but somehow the problem was gone on my side. Maybe it did related to port usage as our environment is mixed with containers.
		</comment>
		<comment id='16' author='ConeyLiu' date='2020-02-20T12:11:37Z'>
		&lt;denchmark-link:https://github.com/edoakes&gt;@edoakes&lt;/denchmark-link&gt;
 This still reproduces on my Ubuntu 16.04. So def not CentOS related.
I'm using Docker, but did worker-&gt;master AND master-&gt;worker open ports validation (using telnet) on all relevant ports. Still getting:
&lt;denchmark-code&gt;I0220 14:08:16.620502  3799 node_manager.cc:2149] Failed to forward task dd27e8181ca37b579b915ac116b9e77f to node manager d38de4071fd540f93c2dd531915c327ef877ed8b
I0220 14:08:16.620538  3799 node_manager.cc:2149] Failed to forward task cb28d27bc06e6f7351e4fe808a746552 to node manager d38de4071fd540f93c2dd531915c327ef877ed8b
I0220 14:08:16.620571  3799 node_manager.cc:2149] Failed to forward task 0914db551ad6703455c31c73f43cb6bd to node manager d38de4071fd540f93c2dd531915c327ef877ed8b
I0220 14:08:16.620604  3799 node_manager.cc:2149] Failed to forward task 3af149e1823e9f4f4820fd5c142c5c06 to node manager d38de4071fd540f93c2dd531915c327ef877ed8b
I0220 14:08:16.620638  3799 node_manager.cc:2149] Failed to forward task 31926b5289be7c37ca65faea4acba383 to node manager d38de4071fd540f93c2dd531915c327ef877ed8b
I0220 14:08:16.620671  3799 node_manager.cc:2149] Failed to forward task ea2c39ad13cb341c681ad358bbc5658e to node manager d38de4071fd540f93c2dd531915c327ef877ed8b
I0220 14:08:16.620702  3799 node_manager.cc:2149] Failed to forward task 26649c352c25e5560d92004de53678fd to node manager d38de4071fd540f93c2dd531915c327ef877ed8b
I0220 14:08:16.620734  3799 node_manager.cc:2149] Failed to forward task 279fae0e57877a5931a54f43dfed3826 to node manager d38de4071fd540f93c2dd531915c327ef877ed8b
I0220 14:08:16.620770  3799 node_manager.cc:2149] Failed to forward task f03e9764afcf7c60bb9568c5e1865fb9 to node manager d38de4071fd540f93c2dd531915c327ef877ed8b
I0220 14:08:16.620805  3799 node_manager.cc:2149] Failed to forward task b7a7438c3149919928936679e0c02853 to node manager d38de4071fd540f93c2dd531915c327ef877ed8b
I0220 14:08:16.620836  3799 node_manager.cc:2149] Failed to forward task 763398d4abcb131b5a85681377c85b4d to node manager d38de4071fd540f93c2dd531915c327ef877ed8b
I0220 14:08:16.620862  3799 node_manager.cc:2149] Failed to forward task d30b526d2be295024f30895e00add546 to node manager d38de4071fd540f93c2dd531915c327ef877ed8b
I0220 14:08:16.620895  3799 node_manager.cc:2149] Failed to forward task ff237343da81f0ef334299ed90993428 to node manager d38de4071fd540f93c2dd531915c327ef877ed8b
I0220 14:08:16.620929  3799 node_manager.cc:2149] Failed to forward task 60945591c66e8a468153e975ec34745b to node manager d38de4071fd540f93c2dd531915c327ef877ed8b
I0220 14:08:16.620961  3799 node_manager.cc:2149] Failed to forward task df3e50eb1c1503bb6f7446700473889e to node manager d38de4071fd540f93c2dd531915c327ef877ed8b
I0220 14:08:16.620995  3799 node_manager.cc:2149] Failed to forward task 21e58caf15d4be68b02b94621b96e6b5 to node manager d38de4071fd540f93c2dd531915c327ef877ed8b
I0220 14:08:16.621026  3799 node_manager.cc:2149] Failed to forward task 28cfb580f25d19ca87f3c56aa7bb1ed5 to node manager d38de4071fd540f93c2dd531915c327ef877ed8b
I0220 14:08:16.621059  3799 node_manager.cc:2149] Failed to forward task 692e2e5979182b6c0dc1d32225fe91fb to node manager d38de4071fd540f93c2dd531915c327ef877ed8b
I0220 14:08:16.621093  3799 node_manager.cc:2149] Failed to forward task 24b95764977dd1e560e740e4d691b51c to node manager d38de4071fd540f93c2dd531915c327ef877ed8b
I0220 14:08:16.621125  3799 node_manager.cc:2149] Failed to forward task 4192e14f27da4c07b0a054821e8c9957 to node manager d38de4071fd540f93c2dd531915c327ef877ed8b
I0220 14:08:16.621157  3799 node_manager.cc:2149] Failed to forward task 1b224e26c744e77c3a6114fe818bf5df to node manager d38de4071fd540f93c2dd531915c327ef877ed8b
I0220 14:08:16.621188  3799 node_manager.cc:2149] Failed to forward task 89748c97707702ee74fe0ec4d2862130 to node manager d38de4071fd540f93c2dd531915c327ef877ed8b
I0220 14:08:16.621222  3799 node_manager.cc:2149] Failed to forward task 4ed35a12a861fdfd4cedbea2389746f4 to node manager d38de4071fd540f93c2dd531915c327ef877ed8b
I0220 14:08:16.621253  3799 node_manager.cc:2149] Failed to forward task 48a7516773ccd12de76de1541a0d3cde to node manager d38de4071fd540f93c2dd531915c327ef877ed8b
I0220 14:08:16.621284  3799 node_manager.cc:2149] Failed to forward task 0f10a5ccdf80489e0ebd2f338fbdd7c3 to node manager d38de4071fd540f93c2dd531915c327ef877ed8b
I0220 14:08:16.621315  3799 node_manager.cc:2149] Failed to forward task 95321507a469d200c63803995f5f9aa7 to node manager d38de4071fd540f93c2dd531915c327ef877ed8b
&lt;/denchmark-code&gt;

where d38de4071fd540f93c2dd531915c327ef877ed8b is my worker node's ID.
		</comment>
		<comment id='17' author='ConeyLiu' date='2020-02-20T14:29:18Z'>
		I guess this may be caused by proxy related problems. The upstream has a patch fixed the  problem: &lt;denchmark-link:https://github.com/ray-project/ray/pull/6744&gt;#6744&lt;/denchmark-link&gt;
. I think you maybe should try the 0.8.1 or upstream branch.
		</comment>
		<comment id='18' author='ConeyLiu' date='2020-02-20T15:55:24Z'>
		hi &lt;denchmark-link:https://github.com/ConeyLiu&gt;@ConeyLiu&lt;/denchmark-link&gt;
, please see my &lt;denchmark-link:https://github.com/ray-project/ray/issues/7242&gt;#7242&lt;/denchmark-link&gt;
, I'm not sure this has to do with proxy. I have tried ray 0.9.0dev0 as well, and it doesn't show any progress on top of 0.7.2
		</comment>
		<comment id='19' author='ConeyLiu' date='2020-02-20T16:05:12Z'>
		OK, I now managed to supply the worker node with its own external IP and see it inside the head node's resources list. This didn't solve the problem though. I'm still getting:
&lt;denchmark-code&gt;...
E0220 18:04:58.267797  1310 direct_task_transport.cc:146] Retrying attempt to schedule task at remote node. Error: IOError: 14: failed to connect to all addresses
E0220 18:04:58.269181  1310 direct_task_transport.cc:146] Retrying attempt to schedule task at remote node. Error: IOError: 14: failed to connect to all addresses
E0220 18:04:58.269912  1310 direct_task_transport.cc:146] Retrying attempt to schedule task at remote node. Error: IOError: 14: failed to connect to all addresses
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>