<bug id='7853' author='devinbarry' open_date='2020-04-01T13:52:34Z' closed_time='2020-11-27T05:24:53Z'>
	<summary>[rllib] ValueError: Reward should be finite scalar, got nan (&amp;lt;class 'float'&amp;gt;)</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

IMPALA with MountainCarContinuous-v0 crashes with the above error. There may be other setups that cause the same issue.
Its the same issue reported &lt;denchmark-link:https://github.com/ray-project/ray/issues/5604&gt;here&lt;/denchmark-link&gt;
. I made a comment on this ticket a few weeks ago, but was told to make a new ticket for it, so here it is:
&lt;denchmark-code&gt;ray.exceptions.RayTaskError(StopIteration): ray::RolloutWorker.sample_with_count() (pid=22945, ip=10.30.30.100)
(pid=22950)   File "python/ray/_raylet.pyx", line 445, in ray._raylet.execute_task
(pid=22950)   File "python/ray/_raylet.pyx", line 423, in ray._raylet.execute_task.function_executor
(pid=22950)   File "/home/axion/anaconda3/envs/training/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py", line 539, in sample_with_count
(pid=22950)     batch = self.sample()
(pid=22950)   File "/home/axion/anaconda3/envs/training/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py", line 492, in sample
(pid=22950)     batches = [self.input_reader.next()]
(pid=22950)   File "/home/axion/anaconda3/envs/training/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py", line 53, in next
(pid=22950)     batches = [self.get_data()]
(pid=22950)   File "/home/axion/anaconda3/envs/training/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py", line 96, in get_data
(pid=22950)     item = next(self.rollout_provider)
(pid=22950) StopIteration
(pid=22950) 2020-04-01 13:29:33,780	ERROR trainer.py:965 -- Blacklisting worker 14
(pid=22950) Traceback (most recent call last):
(pid=22950)   File "/home/axion/anaconda3/envs/training/lib/python3.7/site-packages/ray/rllib/agents/trainer.py", line 491, in train
(pid=22950)     result = Trainable.train(self)
(pid=22950)   File "/home/axion/anaconda3/envs/training/lib/python3.7/site-packages/ray/tune/trainable.py", line 261, in train
(pid=22950)     result = self._train()
(pid=22950)   File "/home/axion/anaconda3/envs/training/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py", line 154, in _train
(pid=22950)     fetches = self.optimizer.step()
(pid=22950)   File "/home/axion/anaconda3/envs/training/lib/python3.7/site-packages/ray/rllib/optimizers/async_samples_optimizer.py", line 132, in step
(pid=22950)     sample_timesteps, train_timesteps = self._step()
(pid=22950)   File "/home/axion/anaconda3/envs/training/lib/python3.7/site-packages/ray/rllib/optimizers/async_samples_optimizer.py", line 174, in _step
(pid=22950)     for train_batch in self.aggregator.iter_train_batches():
(pid=22950)   File "/home/axion/anaconda3/envs/training/lib/python3.7/site-packages/ray/rllib/optimizers/aso_aggregator.py", line 116, in iter_train_batches
(pid=22950)     blocking_wait=True, max_yield=max_yield)):
(pid=22950)   File "/home/axion/anaconda3/envs/training/lib/python3.7/site-packages/ray/rllib/optimizers/aso_aggregator.py", line 169, in _augment_with_replay
(pid=22950)     sample_batch = ray_get_and_free(sample_batch)
(pid=22950)   File "/home/axion/anaconda3/envs/training/lib/python3.7/site-packages/ray/rllib/utils/memory.py", line 29, in ray_get_and_free
(pid=22950)     result = ray.get(object_ids)
(pid=22950)   File "/home/axion/anaconda3/envs/training/lib/python3.7/site-packages/ray/worker.py", line 1502, in get
(pid=22950)     raise value.as_instanceof_cause()
(pid=22950) ray.exceptions.RayTaskError(ValueError): ray::RolloutWorker.sample() (pid=22924, ip=10.30.30.100)
(pid=22950)   File "python/ray/_raylet.pyx", line 445, in ray._raylet.execute_task
(pid=22950)   File "python/ray/_raylet.pyx", line 423, in ray._raylet.execute_task.function_executor
(pid=22950)   File "/home/axion/anaconda3/envs/training/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py", line 492, in sample
(pid=22950)     batches = [self.input_reader.next()]
(pid=22950)   File "/home/axion/anaconda3/envs/training/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py", line 53, in next
(pid=22950)     batches = [self.get_data()]
(pid=22950)   File "/home/axion/anaconda3/envs/training/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py", line 96, in get_data
(pid=22950)     item = next(self.rollout_provider)
(pid=22950)   File "/home/axion/anaconda3/envs/training/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py", line 359, in _env_runner
(pid=22950)     base_env.send_actions(actions_to_send)
(pid=22950)   File "/home/axion/anaconda3/envs/training/lib/python3.7/site-packages/ray/rllib/env/base_env.py", line 328, in send_actions
(pid=22950)     self.vector_env.vector_step(action_vector)
(pid=22950)   File "/home/axion/anaconda3/envs/training/lib/python3.7/site-packages/ray/rllib/env/vector_env.py", line 110, in vector_step
(pid=22950)     r, type(r)))
(pid=22950) ValueError: Reward should be finite scalar, got nan (&lt;class 'float'&gt;)
&lt;/denchmark-code&gt;

Ray version and other system information (Python version, TensorFlow version, OS):
Python 3.7.7 on Ubuntu 19.04
Also Python 3.7 running inside Docker (both Debian 10 and Ubuntu 18.04 images)
Ray 0.8.3 and earlier versions at least to 0.8.1
Also on the latest nightly version
Tensorflow==2.1.0
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

This one using AsyncHyperBandScheduler crashes almost immediately
&lt;denchmark-link:https://gist.github.com/devinbarry/d15dcb4b3d418dcc5d1ad1fb9c44ae44&gt;https://gist.github.com/devinbarry/d15dcb4b3d418dcc5d1ad1fb9c44ae44&lt;/denchmark-link&gt;

This one just doing regular training crashes after between 10 and 60 iterations. Just keep running it and it will definitely crash. It never doesn't crash.
&lt;denchmark-link:https://gist.github.com/devinbarry/1cc0c5aefff77b82e0dcf1e3a295b9a2&gt;https://gist.github.com/devinbarry/1cc0c5aefff77b82e0dcf1e3a295b9a2&lt;/denchmark-link&gt;

I should point out my main training workstation has 32 cores and 1 GPU so CPU and GPU are normally set to 32 and 1 respectively.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='devinbarry' date='2020-11-13T04:42:56Z'>
		Hi, I'm a bot from the Ray team :)
To help human contributors to focus on more relevant issues, I will automatically add the stale label to issues that have had no activity for more than 4 months.
If there is no further activity in the 14 days, the issue will be closed!

If you'd like to keep the issue open, just leave any comment, and the stale label will be removed!
If you'd like to get more attention to the issue, please tag one of Ray's contributors.

You can always ask for help on our &lt;denchmark-link:https://discuss.ray.io/&gt;discussion forum&lt;/denchmark-link&gt;
 or &lt;denchmark-link:https://github.com/ray-project/ray#getting-involved&gt;Ray's public slack channel&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='2' author='devinbarry' date='2020-11-27T05:24:49Z'>
		Hi again! The issue will be closed because there has been no more activity in the 14 days since the last message.
Please feel free to reopen or open a new issue if you'd still like it to be addressed.
Again, you can always ask for help on our &lt;denchmark-link:https://discuss.ray.io&gt;discussion forum&lt;/denchmark-link&gt;
 or &lt;denchmark-link:https://github.com/ray-project/ray#getting-involved&gt;Ray's public slack channel&lt;/denchmark-link&gt;
.
Thanks again for opening the issue!
		</comment>
	</comments>
</bug>