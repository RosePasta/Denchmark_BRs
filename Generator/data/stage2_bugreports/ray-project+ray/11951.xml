<bug id='11951' author='krfricke' open_date='2020-11-11T19:31:25Z' closed_time='2020-11-13T01:22:43Z'>
	<summary>[autoscaler] restarting ray with `ray up` switches object store location from /dev/shm to /tmp</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

Restarting the ray runtime on a cluster that uses docker with shm moves the plasma object store location to /tmp.
After starting the cluster the first time (scroll to end):
&lt;denchmark-code&gt;# ps aux | grep plasma_directory
root        8435  1.3  0.0 979289612 67788 ?     Sl   11:24   0:03 /root/anaconda3/lib/python3.7/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tmp/ray/session_2020-11-11_11-24-02_969644_8401/sockets/raylet --store_socket_name=/tmp/ray/session_2020-11-11_11-24-02_969644_8401/sockets/plasma_store --object_manager_port=8076 --min_worker_port=10000 --max_worker_port=10999 --node_manager_port=63984 --node_ip_address=172.31.16.137 --redis_address=172.31.16.137 --redis_port=6379 --num_initial_workers=64 --maximum_startup_concurrency=64 --static_resource_list=node:172.31.16.137,1.0,CPU,64,memory,455,object_store_memory,6580 --config_list=plasma_store_as_thread,True --python_worker_command=/root/anaconda3/bin/python /root/anaconda3/lib/python3.7/site-packages/ray/workers/default_worker.py --node-ip-address=172.31.16.137 --node-manager-port=63984 --object-store-name=/tmp/ray/session_2020-11-11_11-24-02_969644_8401/sockets/plasma_store --raylet-name=/tmp/ray/session_2020-11-11_11-24-02_969644_8401/sockets/raylet --redis-address=172.31.16.137:6379 --config-list=plasma_store_as_thread,True --temp-dir=/tmp/ray --metrics-agent-port=41883 --redis-password=5241590000000000 --java_worker_command= --cpp_worker_command= --redis_password=5241590000000000 --temp_dir=/tmp/ray --session_dir=/tmp/ray/session_2020-11-11_11-24-02_969644_8401 --metrics-agent-port=41883 --metrics_export_port=63079 --object_store_memory=500000000000 --plasma_directory=/dev/shm --head_node
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;ray up -y issue.yaml
&lt;/denchmark-code&gt;

After restarting (scroll to end):
&lt;denchmark-code&gt;root       13477  3.1  0.0 491008224 67388 ?     Sl   11:29   0:00 /root/anaconda3/lib/python3.7/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tmp/ray/session_2020-11-11_11-29-30_982983_13439/sockets/raylet --store_socket_name=/tmp/ray/session_2020-11-11_11-29-30_982983_13439/sockets/plasma_store --object_manager_port=8076 --min_worker_port=10000 --max_worker_port=10999 --node_manager_port=53349 --node_ip_address=172.31.16.137 --redis_address=172.31.16.137 --redis_port=6379 --num_initial_workers=64 --maximum_startup_concurrency=64 --static_resource_list=node:172.31.16.137,1.0,CPU,64,memory,455,object_store_memory,6580 --config_list=plasma_store_as_thread,True --python_worker_command=/root/anaconda3/bin/python /root/anaconda3/lib/python3.7/site-packages/ray/workers/default_worker.py --node-ip-address=172.31.16.137 --node-manager-port=53349 --object-store-name=/tmp/ray/session_2020-11-11_11-29-30_982983_13439/sockets/plasma_store --raylet-name=/tmp/ray/session_2020-11-11_11-29-30_982983_13439/sockets/raylet --redis-address=172.31.16.137:6379 --config-list=plasma_store_as_thread,True --temp-dir=/tmp/ray --metrics-agent-port=60879 --redis-password=5241590000000000 --java_worker_command= --cpp_worker_command= --redis_password=5241590000000000 --temp_dir=/tmp/ray --session_dir=/tmp/ray/session_2020-11-11_11-29-30_982983_13439 --metrics-agent-port=60879 --metrics_export_port=50207 --object_store_memory=500000000000 --plasma_directory=/tmp --head_node
root       14852  0.0  0.0   5192  2412 pts/10   S+   11:29   0:00 grep --color=auto plasma_directory
&lt;/denchmark-code&gt;

issue.yaml
&lt;denchmark-code&gt;cluster_name: stale_ref_issue
min_workers: 0
max_workers: 0
initial_workers: 0
autoscaling_mode: default
docker:
    image: 'anyscale/ray-ml:latest'
    container_name: ray_container
    pull_before_run: false
    run_options:
        - --privileged
        - --shm-size=510000000000
target_utilization_fraction: 0.8
idle_timeout_minutes: 5
provider:
    type: aws
    region: us-west-2
    availability_zone: us-west-2a
    # cache_stopped_nodes: false
auth:
    ssh_user: ubuntu
head_node:
    InstanceType: r5.16xlarge
    ImageId: ami-05ac7a76b4c679a79

worker_nodes:
    InstanceType: m5.xlarge
    ImageId: ami-05ac7a76b4c679a79
    InstanceMarketOptions:
        MarketType: spot
file_mounts: {
    "/root/xgboost-benchmark": "/Users/kai/coding/anyscale/projects/xgboost-benchmark",
}
cluster_synced_files:
    - /tmp/ray_tmp_mount/~/xgboost-benchmark
file_mounts_sync_continuously: true
initialization_commands: []
setup_commands: []
head_setup_commands: 
    - apt install -y lsof
worker_setup_commands: []
head_start_ray_commands:
    - ray stop
    - "ulimit -n 65536; ray start --head --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml --object-store-memory=500000000000 --resources='{\"actor_cpus\": 0}'"
worker_start_ray_commands:
    - ray stop
    - "ulimit -n 65536; ray start --address=$RAY_HEAD_IP:6379 --object-manager-port=8076 --resources='{\"actor_cpus\": 4}'"
metadata:
    anyscale:
        working_dir: ~/xgboost-benchmark
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
If we cannot run your script, we cannot fix your issue.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='krfricke' date='2020-11-11T20:13:07Z'>
		What is the output near the actual  ray start ?
Is there a message like:
&lt;denchmark-code&gt;2020-11-11 20:12:52,890 WARNING services.py:1625 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='krfricke' date='2020-11-11T21:36:19Z'>
		Yep, there is:
&lt;denchmark-code&gt;2020-11-11 11:29:31,384	INFO services.py:1166 -- View the Ray dashboard at http://localhost:8265
2020-11-11 11:29:31,386	WARNING services.py:1625 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 477999988736 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='krfricke' date='2020-11-11T21:38:04Z'>
		&lt;denchmark-link:https://github.com/krfricke&gt;@krfricke&lt;/denchmark-link&gt;
 Was the cluster up when you added the ? You might need to  and  again if that is the case.
		</comment>
		<comment id='4' author='krfricke' date='2020-11-11T22:06:42Z'>
		Yes it was. I think this is actually related to &lt;denchmark-link:https://github.com/ray-project/ray/issues/11950&gt;#11950&lt;/denchmark-link&gt;
:  is still occupied by 30 GB of stale file handles, which are cleared only after ray is restarted.
Is the size of /dev/shm checked after ?
		</comment>
		<comment id='5' author='krfricke' date='2020-11-11T22:34:15Z'>
		&lt;denchmark-link:https://github.com/krfricke&gt;@krfricke&lt;/denchmark-link&gt;
 Oh shoot! I meant  from your laptop to tear the cluster down.
		</comment>
		<comment id='6' author='krfricke' date='2020-11-12T09:25:31Z'>
		Yes, that works. However I was under the impression that running ray up on a running cluster is a viable way to just restart the ray runtime (and run setup commands etc), in fact I was using this quite a lot to save time on node startups.
		</comment>
		<comment id='7' author='krfricke' date='2020-11-12T16:16:48Z'>
		ray up will restart the ray runtime, but to change docker configuration (i.e. add --shm-size), you need to restart the docker runtime :/
		</comment>
		<comment id='8' author='krfricke' date='2020-11-12T16:22:30Z'>
		So the problem here is that I didn't change anything. I.e. --shm-size was set all the time, but only lead to using /dev/shm on the first start, not the second. Though it seems this is just due to /dev/shm still  being occupied by the object spilled from the last session.
		</comment>
	</comments>
</bug>