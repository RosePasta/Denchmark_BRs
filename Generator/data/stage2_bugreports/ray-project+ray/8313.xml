<bug id='8313' author='yutaizhou' open_date='2020-05-04T19:18:13Z' closed_time='2020-08-11T19:10:29Z'>
	<summary>Ray cannot identify non-base 10 GPU numbering on SLURM</summary>
	<description>
Hey all,
I am using Ray on SLURM equipped with 2 GPUs per node. I get the following error when I submit a job that acquired GPUs:
&lt;denchmark-code&gt;                                                                                                                                                                   
  1   /state/partition1/user/yutai/raytmp
  2   Traceback (most recent call last):
  3   File "/home/gridsan/yutai/.conda/envs/football2/bin/ray", line 5, in &lt;module&gt;
  4     from ray.scripts.scripts import main
  5   File "/home/gridsan/yutai/.conda/envs/football2/lib/python3.6/site-packages/ray/__init__.py", line 65, in &lt;module&gt;
  6     from ray.worker import (
  7   File "/home/gridsan/yutai/.conda/envs/football2/lib/python3.6/site-packages/ray/worker.py", line 488, in &lt;module&gt;
  8     global_worker = Worker()
  9   File "/home/gridsan/yutai/.conda/envs/football2/lib/python3.6/site-packages/ray/worker.py", line 118, in __init__
 10     self.original_gpu_ids = ray.utils.get_cuda_visible_devices()
 11   File "/home/gridsan/yutai/.conda/envs/football2/lib/python3.6/site-packages/ray/utils.py", line 288, in get_cuda_visible_devices
 12     return [int(i) for i in gpu_ids_str.split(",")]
 13   File "/home/gridsan/yutai/.conda/envs/football2/lib/python3.6/site-packages/ray/utils.py", line 288, in &lt;listcomp&gt;
 14     return [int(i) for i in gpu_ids_str.split(",")]
 15 srun: error: d-10-14-2: task 0: Exited with exit code 1
 16 ValueError: invalid literal for int() with base 10: 'GPU-77e3a4cd-22fb-5d31-b8a6-409df6133eeb'
 17 adding workers
 18 GPUS: GPU-77e3a4cd-22fb-5d31-b8a6-409df6133eeb,GPU-3de6a983-6da7-e1cf-ffc6-a285c71ff5d3
 19 Traceback (most recent call last):
 20   File "ray_test.py", line 5, in &lt;module&gt;
 21     import ray
 22   File "/home/gridsan/yutai/.conda/envs/football2/lib/python3.6/site-packages/ray/__init__.py", line 65, in &lt;module&gt;
 23     from ray.worker import (
 24   File "/home/gridsan/yutai/.conda/envs/football2/lib/python3.6/site-packages/ray/worker.py", line 488, in &lt;module&gt;
 25     global_worker = Worker()
 26   File "/home/gridsan/yutai/.conda/envs/football2/lib/python3.6/site-packages/ray/worker.py", line 118, in __init__
 27     self.original_gpu_ids = ray.utils.get_cuda_visible_devices()
 28   File "/home/gridsan/yutai/.conda/envs/football2/lib/python3.6/site-packages/ray/utils.py", line 288, in get_cuda_visible_devices
 29     return [int(i) for i in gpu_ids_str.split(",")]
 30   File "/home/gridsan/yutai/.conda/envs/football2/lib/python3.6/site-packages/ray/utils.py", line 288, in &lt;listcomp&gt;
 31     return [int(i) for i in gpu_ids_str.split(",")]
 32 ValueError: invalid literal for int() with base 10: 'GPU-77e3a4cd-22fb-5d31-b8a6-409df6133eeb'
&lt;/denchmark-code&gt;

echo "GPUS: $CUDA_VISIBLE_DEVICES" gets me the following:
GPUS: GPU-77e3a4cd-22fb-5d31-b8a6-409df6133eeb,GPU-3de6a983-6da7-e1cf-ffc6-a285c71ff5d3
I am using Ray 0.8.4, and Python 3.6.10, on Ubuntu.
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Here are my sbatch shell script and python script. Note there are small deviations from this SLURM example in the docs as our SLURM system team initially got the example to work and I am simply using their modifications. The conda environment shouldn't be the bottleneck here.
&lt;denchmark-code&gt;#!/bin/bash

#SBATCH -o %j.log
#SBATCH --job-name=ray_test
#SBATCH -n 4
#SBATCH -N 2
#SBATCH --gres=gpu:volta:2

export LC_ALL=C.UTF-8
export LANG=C.UTF-8

((worker_num=$SLURM_NNODES-1)) # Must be one less that the total number of nodes
echo $worker_num

# Set up environment
eval "$(conda shell.bash hook)"
conda activate football2
# or
# conda activate /home/gridsan/groups/ERGO_GRF/football_env/football

nodes=$(scontrol show hostnames $SLURM_JOB_NODELIST) # Getting the node names
nodes_array=( $nodes )

node1=${nodes_array[0]}

# Set IP address/port of head node
ip_prefix=$(srun --nodes=1 --ntasks=1 -w $node1 hostname --ip-address) # Making address
port='6379'
ip_head=$ip_prefix':'$port
export ip_head # Exporting for latter access by trainer.py

# Temporary directory for logging
tmpdir='/state/partition1/user/'$USER'/raytmp'
echo $tmpdir
mkdircmd='mkdir -p '$tmpdir

# Start the head
srun --nodes=1 --ntasks=1 -w $node1 $mkdircmd
srun --nodes=1 --ntasks=1 -w $node1 ray start --temp-dir=$tmpdir --block --head --redis-port=$port &amp;
sleep 5

# Start workers
echo "adding workers"
for ((  i=1; i&lt;=$worker_num; i++ ))
do
    node2=${nodes_array[$i]}
    srun --nodes=1 --ntasks=1 -w $node2 mkdir -p $tmpdir
    srun --nodes=1 --ntasks=1 -w $node2 ray start --temp-dir=$tmpdir --block --address=$ip_head &amp; # Starting the workers
done
echo "GPUS: $CUDA_VISIBLE_DEVICES"
python ray_test.py $SLURM_NTASKS # Pass the total number of allocated CPUs
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;# trainer.py
import os
import sys
import time
import ray
import torch 


ray.init(address=os.environ["ip_head"])

@ray.remote
def f():
  time.sleep(1)

# The following takes one second (assuming that ray was able to access all of the allocated nodes).
start = time.time()
num_cpus = int(sys.argv[1])
ray.get([f.remote() for _ in range(num_cpus)])
end = time.time()
print(end - start)

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='yutaizhou' date='2020-05-04T19:54:30Z'>
		Thanks for opening this issue &lt;denchmark-link:https://github.com/yutaizhou&gt;@yutaizhou&lt;/denchmark-link&gt;
 ! Is it possible to map the string back to the original device index (integers)?
		</comment>
		<comment id='2' author='yutaizhou' date='2020-05-05T00:35:44Z'>
		&lt;denchmark-link:https://github.com/richardliaw&gt;@richardliaw&lt;/denchmark-link&gt;
 thanks for checking in! I think I could, since every node in our HPC system only has 2 GPUs anyway. I could just map the 2 GPUs as 0 and 1? Not sure if that would work across the entire system though. I will try it
		</comment>
		<comment id='3' author='yutaizhou' date='2020-05-07T17:12:09Z'>
		&lt;denchmark-link:https://github.com/richardliaw&gt;@richardliaw&lt;/denchmark-link&gt;
 So I have tried remapping the GPU IDs but didn't get very far..

After talking to the HPC team, here is my understanding of the status and some direct quotes from them:


"We’re using NVIDIA’s UUID API to set the device names to the UUID, rather than the default. The problem with the default naming scheme (0,1,etc) is that it is not consistent. What’s listed as GPU 0 might change even within a job, which you can imagine would cause major problems if you have two people on a node, each allocated one GPU." -MITSC


In a shared environment like a HPC system, UUID seems to be the best (only?) way to to make sure people only have access to the GPUs that is allocated to them. Apparently TF and PyTorch don't have a problem with that. The HPC team essentially recommends Ray to accept non-default GPU naming scheme.


What do you think?
		</comment>
		<comment id='4' author='yutaizhou' date='2020-05-07T20:06:06Z'>
		OK got it. Let me see if we can prio this. cc &lt;denchmark-link:https://github.com/anabranch&gt;@anabranch&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='yutaizhou' date='2020-05-08T15:04:20Z'>
		Great, thank you Richard!
		</comment>
		<comment id='6' author='yutaizhou' date='2020-07-06T22:36:25Z'>
		&lt;denchmark-link:https://github.com/yutaizhou&gt;@yutaizhou&lt;/denchmark-link&gt;
 Did you find a fix? I'm having the same issue on PBS.
		</comment>
		<comment id='7' author='yutaizhou' date='2020-07-09T13:47:25Z'>
		&lt;denchmark-link:https://github.com/Graeme22&gt;@Graeme22&lt;/denchmark-link&gt;
 yes but it is very hacky, and I still believe this should be addressed upstream in Ray.
When you do srun to spin up nodes, add an extra unset.sh file:
&lt;denchmark-code&gt;srun --nodes=1 --ntasks=1 -w $node1 unset.sh &amp;&amp; ray start --temp-dir=$tmpdir --block --head --redis-port=$port &amp;
&lt;/denchmark-code&gt;

unset.sh
&lt;denchmark-code&gt;unset CUDA_VISIBLE_DEVICES
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='yutaizhou' date='2020-07-10T21:23:20Z'>
		For anyone having this issue, my quick fix was the following: Change line 288 of utils.py to this:
&lt;denchmark-code&gt;return gpu_ids_str.split(",")
&lt;/denchmark-code&gt;

The string based IDs still work as expected, and avoid casting error when they're not numerical.
		</comment>
		<comment id='9' author='yutaizhou' date='2020-08-11T19:10:29Z'>
		This should be fixed in &lt;denchmark-link:https://github.com/ray-project/ray/pull/9744&gt;#9744&lt;/denchmark-link&gt;
! Please ping (or open a new issue if not!)
		</comment>
	</comments>
</bug>