<bug id='8579' author='roireshef' open_date='2020-05-24T08:22:28Z' closed_time='2020-06-26T21:18:40Z'>
	<summary>User has no control over TorchPolicy migrating model to GPU [rllib]</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

in the constructor in /usr/local/lib/python3.6/dist-packages/ray/rllib/policy/torch_policy.py there's casting of torch model to GPU if CUDA is available, regardless of a user's preference not to use GPU. I believe this should be controlled by num_gpus and num_gpus_per_worker. Current setup ends up with incompatibility when rollout samples are on CPU and model on GPU, etc.
Ray version and other system information (Python version, TensorFlow version, OS):
Ray version is 0.8.5, Torch 1.3.0
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
Use any environment with PyTorch (I use custom Flow environment + A3C), and set num_gpus to 0.
If we cannot run your script, we cannot fix your issue.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='roireshef' date='2020-05-24T20:36:19Z'>
		Ray sets CUDA_VISIBLE_DEVICES based on those configs to assign GPUs, so it should be working as you mention.
To clarify, is the report that this restriction is not working as expected on a GPU machine (device being detected despite ray setting the env var)?
		</comment>
		<comment id='2' author='roireshef' date='2020-05-30T11:08:35Z'>
		&lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
 - the condition in this line is wrong &lt;denchmark-link:https://github.com/ray-project/ray/blob/master/rllib/policy/torch_policy.py#L77&gt;https://github.com/ray-project/ray/blob/master/rllib/policy/torch_policy.py#L77&lt;/denchmark-link&gt;

Even if CUDA is available, user doesn't always like to use it. The config param that controls using GPU or not AFAIK is num_gpus (or num_gpus_per_worker). If it is set to 0, there is no reason to move model tensors to cuda device here.
		</comment>
		<comment id='3' author='roireshef' date='2020-05-30T20:52:47Z'>
		Hmm shouldn't that return false if CUDA_VISIBLE_DEVICES is set to empty
string? Ray is setting that to selectively enable GPU for certain actors as
requested only.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Sat, May 30, 2020, 4:08 AM roireshef ***@***.***&gt; wrote:
 @ericl &lt;https://github.com/ericl&gt; - the condition in this line is wrong
 https://github.com/ray-project/ray/blob/master/rllib/policy/torch_policy.py#L77

 Even if CUDA is available, user doesn't always like to use it. The config
 param that controls using GPU or not AFAIK is num_gpus (or
 num_gpus_per_worker). If it is set to 0, there is no reason to move model
 tensors to cuda device here.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#8579 (comment)&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/AAADUSV2ADNJXENCVH3O3DTRUDSL7ANCNFSM4NIZKJWQ&gt;
 .



		</comment>
		<comment id='4' author='roireshef' date='2020-06-21T20:12:32Z'>
		&lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
 - Sorry it took me so much time to respond.
Are you suggesting I manually unset an environment variable named CUDA_VISIBLE_DEVICES? Or is it a Tune/RLlib config parameter?
Anyway, I thought it would be more natural to change:



ray/rllib/policy/torch_policy.py


         Line 80
      in
      14405b9






 if torch.cuda.is_available() else torch.device("cpu")) 





to:
&lt;denchmark-code&gt;if torch.cuda.is_available() and config["num_gpus"] &gt; 0 else torch.device("cpu"))
&lt;/denchmark-code&gt;

since it didn't make sense to find out a GPU is used if you already set num_gpus to 0.
Also, what happens in the case of IMPALA? was it tested to work? AFAIK in IMPALA the model should be stored in CPU mode (RAM) for the workers, while it is stored in GPU mode for the master/optimizer. There might be a need to condition on both num_gpus and num_gpus_per_worker?
		</comment>
		<comment id='5' author='roireshef' date='2020-06-21T20:59:40Z'>
		
since it didn't make sense to find out a GPU is used if you already set num_gpus to 0.

The reason this is the case is that num_gpus only applies to the learner. The workers are configured separately. Ray will ensure the GPUs are made visible as configured, so cuda.is_available() should be the right check where-ever the policy is created (there are copies of the policy created on both learner and workers).
Basically: always use the RLlib config to assign GPUs to the learner or workers, and Ray will do the right thing.
		</comment>
		<comment id='6' author='roireshef' date='2020-06-22T17:39:25Z'>
		
Basically: always use the RLlib config to assign GPUs to the learner or workers, and Ray will do the right thing.

&lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
 - I take it that you are referring to  and  when you write RLlib config. Well, if yes, then this is not the case today - if you set both of them to 0, you still get the model parameters moved to the GPU since cuda.is_available() returns True, and IMHO this is not the desired behavior. It sounds natural to me that  and  will override &lt;cuda.is_available() returns True&gt;, since those are the hooks for the user to control GPU usage in the config.
Do you agree?
		</comment>
		<comment id='7' author='roireshef' date='2020-06-22T17:51:55Z'>
		Are you saying that torch.cuda.is_available() returns True for you even if both are set to zero? That's very unexpected. To verify, what does this print for you:
&lt;denchmark-code&gt;@ray.remote(num_gpus=0)
def f():
   print(torch.cuda.is_available())
   print(os.environ.get("CUDA_VISIBLE_DEVICES"))

ray.init()
ray.get(f.remote())
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='roireshef' date='2020-06-22T20:12:20Z'>
		
Are you saying that torch.cuda.is_available() returns True for you even if both are set to zero? That's very unexpected. To verify, what does this print for you:
@ray.remote(num_gpus=0)
def f():
   print(torch.cuda.is_available())
   print(os.environ.get("CUDA_VISIBLE_DEVICES"))

ray.init()
ray.get(f.remote())


Short answer is yes. I can re-verify tomorrow with newest commit on 0.9dev, but last time I checked it (about a month ago) torch.cuda.is_available() returned True even when I set num_gpus=0
And this makes sense, right? Setting a Ray config param (num_gpus, for instance) shouldn't override PyTorch internals, AFAIK. When you call torch.cuda.is_avialable, torch uses its own internal information, unexposed to Ray config. Or did I miss anything...?
		</comment>
		<comment id='9' author='roireshef' date='2020-06-22T20:24:44Z'>
		The ray param overrides CUDA_VISIBLE_DEVICES, which should be respected by pytorch (it is respected by tensorflow, etc). There might be some bug where this isn't the case.
		</comment>
		<comment id='10' author='roireshef' date='2020-06-26T21:18:40Z'>
		This doesn't seem to be the case as of torch 1.4. I'm going to close this; feel free to reopen if you can reproduce otherwise with a recent torch version.
&lt;denchmark-code&gt;&gt;&gt;&gt; import torch
&gt;&gt;&gt; torch.__version__
'1.4.0'
&gt;&gt;&gt; torch.cuda.is_available()
True
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;&gt;&gt;&gt; import torch
&gt;&gt;&gt; import os
&gt;&gt;&gt; @ray.remote(num_gpus=0)
... def f():
...    print(torch.cuda.is_available())
...    print(os.environ.get("CUDA_VISIBLE_DEVICES"))
... 
&gt;&gt;&gt; 
&gt;&gt;&gt; f.remote()
ObjectID(f66d17bae2b0e765ffffffff010000c001000000)
&gt;&gt;&gt; (pid=61816) False
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>