<bug id='11457' author='ankur6ue' open_date='2020-10-19T01:10:39Z' closed_time='2020-12-15T17:53:13Z'>
	<summary>Unable to access Ray dashboard when running Ray on Kubernetes</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

Unable to access Ray dashboard when running Ray on a kubernetes cluster
Ray version and other system information (Python version, TensorFlow version, OS):
latest rayproject/autoscaler docker image
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
Just added containerPort: 8265 to expose 8265 on the head deployment. When I get the IP of the head pod using kubectl -get deployments -o wide and then go to that ip:8265, I get this site can't be reached..
Here's the ray-cluster.yaml:
&lt;denchmark-code&gt;# Ray head node service, allowing worker pods to discover the head node.
apiVersion: v1
kind: Service
metadata:
  namespace: ray
  name: ray-head
spec:
  ports:
    # Redis ports.
    - name: redis-primary
      port: 6379
      targetPort: 6379
    - name: redis-shard-0
      port: 6380
      targetPort: 6380
    - name: redis-shard-1
      port: 6381
      targetPort: 6381

    # Ray internal communication ports.
    - name: object-manager
      port: 12345
      targetPort: 12345
    - name: node-manager
      port: 12346
      targetPort: 12346
  selector:
    component: ray-head
---
apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: ray
  name: ray-head
spec:
  # Do not change this - Ray currently only supports one head node per cluster.
  replicas: 1
  selector:
    matchLabels:
      component: ray-head
      type: ray
  template:
    metadata:
      labels:
        component: ray-head
        type: ray
    spec:
      # If the head node goes down, the entire cluster (including all worker
      # nodes) will go down as well. If you want Kubernetes to bring up a new
      # head node in this case, set this to "Always," else set it to "Never."
      restartPolicy: Always

      # This volume allocates shared memory for Ray to use for its plasma
      # object store. If you do not provide this, Ray will fall back to
      # /tmp which cause slowdowns if is not a shared memory volume.
      volumes:
      - name: dshm
        emptyDir:
          medium: Memory
      containers:
        - name: ray-head
          image: rayproject/autoscaler
          # imagePullPolicy: Never
          imagePullPolicy: Always
          command: [ "/bin/bash", "-c", "--" ]
          args: 
            - "ray start --head --node-ip-address=$MY_POD_IP --port=6379 --redis-shard-ports=6380,6381 --num-cpus=$MY_CPU_REQUEST \
            --object-manager-port=12345 --node-manager-port=12346 --dashboard-port 8265 --redis-password=password --block"
          ports:
            - containerPort: 6379 # Redis port.
            - containerPort: 6380 # Redis port.
            - containerPort: 6381 # Redis port.
            - containerPort: 12345 # Ray internal communication.
            - containerPort: 12346 # Ray internal communication.
            - containerPort: 8265 # Ray dashboard

          # This volume allocates shared memory for Ray to use for its plasma
          # object store. If you do not provide this, Ray will fall back to
          # /tmp which cause slowdowns if is not a shared memory volume.
          volumeMounts:
            - mountPath: /dev/shm
              name: dshm
          env:
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP

            # This is used in the ray start command so that Ray can spawn the
            # correct number of processes. Omitting this may lead to degraded
            # performance.
            - name: MY_CPU_REQUEST
              value: "3"
            #   valueFrom:
            #    resourceFieldRef:
            #      resource: requests.cpu
          resources:
            requests:
              cpu: 100m
              memory: 512Mi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: ray
  name: ray-worker
spec:
  # Change this to scale the number of worker nodes started in the Ray cluster.
  replicas: 1
  selector:
    matchLabels:
      component: ray-worker
      type: ray
  template:
    metadata:
      labels:
        component: ray-worker
        type: ray
    spec:
      restartPolicy: Always
      volumes:
      - name: dshm
        emptyDir:
          medium: Memory
      containers:
      - name: ray-worker
        image: rayproject/autoscaler
        # imagePullPolicy: Never
        imagePullPolicy: Always
        command: ["/bin/bash", "-c", "--"]
        args:
          - "ray start --node-ip-address=$MY_POD_IP --num-cpus=$MY_CPU_REQUEST --address=$RAY_HEAD_SERVICE_HOST:$RAY_HEAD_SERVICE_PORT_REDIS_PRIMARY \
          --object-manager-port=12345 --node-manager-port=12346 --redis-password=password --block"
        ports:
          - containerPort: 12345 # Ray internal communication.
          - containerPort: 12346 # Ray internal communication.
        volumeMounts:
          - mountPath: /dev/shm
            name: dshm
        env:
          - name: MY_POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP

          # This is used in the ray start command so that Ray can spawn the
          # correct number of processes. Omitting this may lead to degraded
          # performance.
          - name: MY_CPU_REQUEST
            value: "3"
            # valueFrom:
            #  resourceFieldRef:
            #    resource: requests.cpu
        resources:
          requests:
            cpu: 100m
            memory: 512Mi
&lt;/denchmark-code&gt;

If we cannot run your script, we cannot fix your issue.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='ankur6ue' date='2020-10-19T11:02:24Z'>
		Similar issue here (I also experienced the same issue described above with google cloud service and ray autoscaler).
Please check the JS error below, I think it's the right lead to solve this.
I am using a single GCP VM in a VPN (setup with openvpn).
I ssh to the remote machine and start ray 1.0.0 with command:
ray start \
      --head \
      --port=6379 \
      --object-manager-port=8076 \
      --include-dashboard=true \
      --dashboard-host=0.0.0.0 \
      --dashboard-port=8266
I get
Local node IP: xx.xx.xx.xx
2020-10-19 10:25:10,664	INFO services.py:1164 -- View the Ray dashboard at http://xx.xx.xx.xx:8266

--------------------
Ray runtime started.
--------------------

Next steps
  To connect to this Ray runtime from another node, run
    ray start --address='xx.xx.xx.xx:6379' --redis-password='yyyyyyyyyyyyyyy'
  
  Alternatively, use the following Python code:
    import ray
    ray.init(address='auto', _redis_password='yyyyyyyyyyyyyyy')
  
  If connection fails, check your firewall settings and network configuration.
  
  To terminate the Ray runtime, run
    ray stop
At this point I can connect to the dashboard with no issues.
Then I start a local 0-resource node (otherwise I can't run ray.init on my local machine)
ray start --address='xx.xx.xx.xx:6379' --redis-password='yyyyyyyyyyyyyyy' --num-cpus=0 --num-gpus=0
From this point on the dashboard is blank. The page renders for a couple of frames, then it disappears. The anchor HTML object is there on the page but JS does not show anything.
I can't see anything relevant from the network log on the browser, but the browser console shows:
react-dom.production.min.js:209 TypeError: Cannot read property 'toFixed' of null
    at dn (GPU.tsx:92)
    at Qi (react-dom.production.min.js:153)
    at vl (react-dom.production.min.js:261)
    at cu (react-dom.production.min.js:246)
    at lu (react-dom.production.min.js:246)
    at Zl (react-dom.production.min.js:239)
    at react-dom.production.min.js:123
    at t.unstable_runWithPriority (scheduler.production.min.js:19)
    at Ho (react-dom.production.min.js:122)
    at Ko (react-dom.production.min.js:123)
If I stop my local ray process with ray stop, the JS error disappears and the dashboard is displayed again.
None of this has any effect on the python API. ray.init, ray.remote, ray.get, etc. all works fine.
		</comment>
		<comment id='2' author='ankur6ue' date='2020-10-23T16:31:42Z'>
		&lt;denchmark-link:https://github.com/Vysybyl&gt;@Vysybyl&lt;/denchmark-link&gt;
 I believe the issue you're seeing is one that &lt;denchmark-link:https://github.com/ray-project/ray/issues/11313&gt;#11313&lt;/denchmark-link&gt;
 is tracking. I'm working on a fix at the moment.
As far as the issue that &lt;denchmark-link:https://github.com/ankur6ue&gt;@ankur6ue&lt;/denchmark-link&gt;
 mentioned, I'm not sure what the cause is exactly. Could you provide any additional information like the dashboard log and error log? It's at  and 
		</comment>
		<comment id='3' author='ankur6ue' date='2020-10-24T11:58:10Z'>
		Hi &lt;denchmark-link:https://github.com/mfitton&gt;@mfitton&lt;/denchmark-link&gt;
,
Thanks a lot for taking care of this!
I don't have the original logs right now since I'm using as head node a cloud VM that is deleted once I finish using it.
I tried to replicate the issue running ray start --head on my local machine. If what follows is not enough I'll grab the logs from the remote VM next time I use it.
Strangely, I run into a similar but maybe unrelated issue (see point 2 below) trying to run a head node with 0 cpus.

I run

ray start --head --num-cpus=1 --num-gpus=0
The dashboard flickers for a second and then becomes blank
The error from the console is
react-dom.production.min.js:209 TypeError: Cannot read property 'find' of null
    at GRAM.tsx:119
    at Array.map (&lt;anonymous&gt;)
    at WorkerFeatureRenderFn (GRAM.tsx:118)
    at Qi (react-dom.production.min.js:153)
    at vl (react-dom.production.min.js:261)
    at cu (react-dom.production.min.js:246)
    at lu (react-dom.production.min.js:246)
    at Zl (react-dom.production.min.js:239)
    at react-dom.production.min.js:123
    at t.unstable_runWithPriority (scheduler.production.min.js:19)
el @ react-dom.production.min.js:209
n.callback @ react-dom.production.min.js:226
di @ react-dom.production.min.js:131
il @ react-dom.production.min.js:212
pu @ react-dom.production.min.js:255
t.unstable_runWithPriority @ scheduler.production.min.js:19
Ho @ react-dom.production.min.js:122
du @ react-dom.production.min.js:248
Zl @ react-dom.production.min.js:239
(anonymous) @ react-dom.production.min.js:123
t.unstable_runWithPriority @ scheduler.production.min.js:19
Ho @ react-dom.production.min.js:122
Ko @ react-dom.production.min.js:123
qo @ react-dom.production.min.js:122
eu @ react-dom.production.min.js:240
notify @ Subscription.js:19
t.notifyNestedSubs @ Subscription.js:92
t.handleChangeWrapper @ Subscription.js:97
v @ redux.js:221
(anonymous) @ index.js:11
(anonymous) @ redux.js:477
(anonymous) @ Dashboard.tsx:66
u @ runtime.js:45
(anonymous) @ runtime.js:274
forEach.e.&lt;computed&gt; @ runtime.js:97
r @ asyncToGenerator.js:3
l @ asyncToGenerator.js:25
Promise.then (async)
r @ asyncToGenerator.js:13
l @ asyncToGenerator.js:25
(anonymous) @ asyncToGenerator.js:32
(anonymous) @ asyncToGenerator.js:21
(anonymous) @ Dashboard.tsx:77
u @ runtime.js:45
(anonymous) @ runtime.js:274
forEach.e.&lt;computed&gt; @ runtime.js:97
r @ asyncToGenerator.js:3
l @ asyncToGenerator.js:25
(anonymous) @ asyncToGenerator.js:32
(anonymous) @ asyncToGenerator.js:21
(anonymous) @ main.8fe93ca5.chunk.js:1
il @ react-dom.production.min.js:212
pu @ react-dom.production.min.js:255
t.unstable_runWithPriority @ scheduler.production.min.js:19
Ho @ react-dom.production.min.js:122
du @ react-dom.production.min.js:248
Zl @ react-dom.production.min.js:239
Ql @ react-dom.production.min.js:230
_u @ react-dom.production.min.js:281
(anonymous) @ react-dom.production.min.js:284
tu @ react-dom.production.min.js:240
$u @ react-dom.production.min.js:284
t.render @ react-dom.production.min.js:290
134 @ index.tsx:6
l @ (index):1
121 @ main.8fe93ca5.chunk.js:1
l @ (index):1
r @ (index):1
t @ (index):1
(anonymous) @ main.8fe93ca5.chunk.js:1
Show 29 more frames
react-dom.production.min.js:209 TypeError: Cannot read property 'toFixed' of null
    at dn (GPU.tsx:92)
    at Qi (react-dom.production.min.js:153)
    at vl (react-dom.production.min.js:261)
    at cu (react-dom.production.min.js:246)
    at lu (react-dom.production.min.js:246)
    at Zl (react-dom.production.min.js:239)
    at react-dom.production.min.js:123
    at t.unstable_runWithPriority (scheduler.production.min.js:19)
    at Ho (react-dom.production.min.js:122)
    at Ko (react-dom.production.min.js:123)
The logs are
&lt;denchmark-link:https://github.com/ray-project/ray/files/5432832/ray_1cpu_0gpus_logs.zip&gt;ray_1cpu_0gpus_logs.zip&lt;/denchmark-link&gt;


I run

ray start --head --num-cpus=0 --num-gpus=0
(I'm not really expecting a 0-resource head node to work, but I think this should at least raise an error...)
The dashboard shows a json error
&lt;denchmark-link:https://user-images.githubusercontent.com/18312039/97081030-fb2ff500-15ff-11eb-94d9-1e5912dbea84.png&gt;&lt;/denchmark-link&gt;

The error from the console is
Failed to load resource: the server responded with a status of 500 (Internal Server Error)
:8265/api/raylet_info:1 
The logs are
&lt;denchmark-link:https://github.com/ray-project/ray/files/5432836/ray_0cpus_0gpus_logs.zip&gt;ray_0cpus_0gpus_logs.zip&lt;/denchmark-link&gt;

I'm on xubuntu (Ubuntu 20.04.1 LTS), conda env with python 3.8.3, ray version 1.0.0 from pypi.
		</comment>
		<comment id='4' author='ankur6ue' date='2020-10-26T17:57:00Z'>
		&lt;denchmark-link:https://github.com/Vysybyl&gt;@Vysybyl&lt;/denchmark-link&gt;
 Ah, I'm sorry you encountered this. There was a bug in the last couple versions of the dashboard that caused the page to not load when there are no workers in the cluster. If you start up some workers by launching tasks or actors, the page should display. That said, the issue is tracked here: &lt;denchmark-link:https://github.com/ray-project/ray/issues/11384&gt;#11384&lt;/denchmark-link&gt;

This issue is actually fixed in the most recent version of the dashboard (currently in the nightly, and slated for the 1.2.0 release in a few weeks).
		</comment>
		<comment id='5' author='ankur6ue' date='2020-11-03T18:41:12Z'>
		&lt;denchmark-link:https://github.com/ankur6ue&gt;@ankur6ue&lt;/denchmark-link&gt;
 can you verify if the issue is resolved in the latest version?
		</comment>
		<comment id='6' author='ankur6ue' date='2020-12-15T17:53:13Z'>
		I'm going to close as there hasn't been recent activity, and I believe the issue has been fixed. Please reopen if I've made a mistake.
		</comment>
		<comment id='7' author='ankur6ue' date='2021-01-15T18:11:56Z'>
		I believe this is not fixed  even with -p 8265 on docker run command, that doesn't work
on mac on stand alone docker (not kubernetes)
docker run --shm-size=100M -t --tty --interactive  -p 8265 rayproject/ray
(base) ray@8d3972dc8dbc:/$ ray start --head
Local node IP: 172.17.0.2
2021-01-15 10:06:00,245	INFO services.py:1173 -- View the Ray dashboard at &lt;denchmark-link:http://localhost:8265&gt;http://localhost:8265&lt;/denchmark-link&gt;

=&gt; not able to see the dashboard...
&lt;denchmark-link:https://github.com/mfitton&gt;@mfitton&lt;/denchmark-link&gt;
 please reopen
		</comment>
	</comments>
</bug>