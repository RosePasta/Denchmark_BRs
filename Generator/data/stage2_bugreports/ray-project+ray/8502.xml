<bug id='8502' author='edoakes' open_date='2020-05-19T15:37:00Z' closed_time='2020-12-08T17:36:54Z'>
	<summary>'ray memory' fails if there are many objects in scope</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

Helping a user debug OOM errors and asked them to run ray memory. ray memory crashed with the following output:
&lt;denchmark-code&gt;2020-05-19 02:13:32,283	INFO scripts.py:976 -- Connecting to Ray instance at 172.31.6.12:34940.
2020-05-19 02:13:32,284	WARNING worker.py:809 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.
(pid=5906) E0519 02:13:32.383447  5906 plasma_store_provider.cc:108] Failed to put object d47fe8ca624da001ffffffff010000c801000000 in object store because it is full. Object size is 196886 bytes.
(pid=5906) Waiting 1000ms for space to free up...
(pid=5906) 2020-05-19 02:13:32,594	INFO (unknown file):0 -- gc.collect() freed 10 refs in 0.11551751299975876 seconds
(pid=5771) E0519 02:13:32.686894  5771 plasma_store_provider.cc:118] Failed to put object 72e67d09154b35b1ffffffff010000c801000000 after 6 attempts. Plasma store status:
(pid=5771) num clients with quota: 0
(pid=5771) quota map size: 0
(pid=5771) pinned quota map size: 0
(pid=5771) allocated bytes: 19130609999
(pid=5771) allocation limit: 19130641612
(pid=5771) pinned bytes: 19130609999
(pid=5771) (global lru) capacity: 19130641612
(pid=5771) (global lru) used: 0%
(pid=5771) (global lru) num objects: 0
(pid=5771) (global lru) num evictions: 0
(pid=5771) (global lru) bytes evicted: 0
(pid=5771) ---
(pid=5771) --- Tip: Use the `ray memory` command to list active objects in the cluster.
(pid=5771) ---
(pid=5771) E0519 02:13:32.880080  5771 plasma_store_provider.cc:108] Failed to put object 1f5c36abed661dbeffffffff010000c801000000 in object store because it is full. Object size is 196886 bytes.
(pid=5771) Waiting 1000ms for space to free up...
(pid=5769) E0519 02:13:32.882894  5769 plasma_store_provider.cc:108] Failed to put object cb31822e7f0e3c70ffffffff010000c801000000 in object store because it is full. Object size is 196886 bytes.
(pid=5769) Waiting 2000ms for space to free up...
(pid=5771) 2020-05-19 02:13:33,215	INFO (unknown file):0 -- gc.collect() freed 10 refs in 0.23763301200006026 seconds
(pid=5906) E0519 02:13:33.383901  5906 plasma_store_provider.cc:108] Failed to put object d47fe8ca624da001ffffffff010000c801000000 in object store because it is full. Object size is 196886 bytes.
(pid=5906) Waiting 2000ms for space to free up...
Traceback (most recent call last):
  File "/home/ubuntu/src/seeweed/ml/bin/ray", line 8, in &lt;module&gt;
    sys.exit(main())
  File "/home/ubuntu/src/seeweed/ml/lib/python3.7/site-packages/ray/scripts/scripts.py", line 1028, in main
    return cli()
  File "/home/ubuntu/src/seeweed/ml/lib/python3.7/site-packages/click/core.py", line 829, in __call__
    return self.main(*args, **kwargs)
  File "/home/ubuntu/src/seeweed/ml/lib/python3.7/site-packages/click/core.py", line 782, in main
    rv = self.invoke(ctx)
  File "/home/ubuntu/src/seeweed/ml/lib/python3.7/site-packages/click/core.py", line 1259, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/home/ubuntu/src/seeweed/ml/lib/python3.7/site-packages/click/core.py", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/ubuntu/src/seeweed/ml/lib/python3.7/site-packages/click/core.py", line 610, in invoke
    return callback(*args, **kwargs)
  File "/home/ubuntu/src/seeweed/ml/lib/python3.7/site-packages/ray/scripts/scripts.py", line 978, in memory
    print(ray.internal.internal_api.memory_summary())
  File "/home/ubuntu/src/seeweed/ml/lib/python3.7/site-packages/ray/internal/internal_api.py", line 28, in memory_summary
    node_manager_pb2.FormatGlobalMemoryInfoRequest(), timeout=30.0)
  File "/home/ubuntu/src/seeweed/ml/lib/python3.7/site-packages/grpc/_channel.py", line 826, in __call__
    return _end_unary_response_blocking(state, call, False, None)
  File "/home/ubuntu/src/seeweed/ml/lib/python3.7/site-packages/grpc/_channel.py", line 729, in _end_unary_response_blocking
    raise _InactiveRpcError(state)
grpc._channel._InactiveRpcError: &lt;_InactiveRpcError of RPC that terminated with:
	status = StatusCode.RESOURCE_EXHAUSTED
	details = "Received message larger than max (28892999 vs. 4194304)"
	debug_error_string = "{"created":"@1589854413.712252174","description":"Received message larger than max (28892999 vs. 4194304)","file":"src/core/ext/filters/message_size/message_size_filter.cc","file_line":188,"grpc_status":8}"
&gt;
(pid=5771) E0519 02:13:33.880635  5771 plasma_store_provider.cc:108] Failed to put object 1f5c36abed661dbeffffffff010000c801000000 in object store because it is full. Object size is 196886 bytes.
(pid=5771) Waiting 2000ms for space to free up...
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='edoakes' date='2020-05-19T15:53:18Z'>
		This is probably related to the fact that the dashboard doesn't work well when it is running in a big cluster.
		</comment>
		<comment id='2' author='edoakes' date='2020-05-19T20:22:54Z'>
		This is hitting the internal gRPC message size limit. In C++ land we configure it to 100MB by default, but it looks like the python client does not have this setting set.
		</comment>
		<comment id='3' author='edoakes' date='2020-05-22T18:27:12Z'>
		Hi
I have exactly the same error:
&lt;denchmark-code&gt;INFO (unknown file):0 -- gc.collect() freed 10 refs in 0.11551751299975876 seconds 
INFO (unknown file):0 -- gc.collect() freed 10 refs in 0.11551751299975876 seconds
... etc
&lt;/denchmark-code&gt;

Interestingly (maybe it might help you investigate):it happens only when number of workers is superior to number of CPUs. When number of workers is &lt;= number of CPUs, it works fine
		</comment>
		<comment id='4' author='edoakes' date='2020-05-25T05:34:04Z'>
		&lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
 I will set this P1 because it looks pretty important for anyone who uses big clutsers. Let's find the assignee in the next planning.
		</comment>
		<comment id='5' author='edoakes' date='2020-08-28T00:30:29Z'>
		&lt;denchmark-link:https://github.com/pitoupitou&gt;@pitoupitou&lt;/denchmark-link&gt;
 Hi, are these gc.collect() messages normal behavior? I'm getting a lot of them, although my job is not erroring out.
		</comment>
	</comments>
</bug>