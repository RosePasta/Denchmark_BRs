<bug id='11295' author='PSZehnder' open_date='2020-10-09T04:33:06Z' closed_time='2020-10-09T05:13:47Z'>
	<summary>GCS Fails to find valid local IP on slurm cluster</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

Ray version and other system information (Python version, TensorFlow version, OS):
Ray 1.0.0
Python 3.7
CentOS 7
Cluster managed by slurm
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
I am attempting to run ray distributed across two nodes of a slurm managed cluster using a modified version of the slurm scripts provided &lt;denchmark-link:https://github.com/ray-project/ray/issues/826#issuecomment-522116599&gt;here&lt;/denchmark-link&gt;
:
&lt;denchmark-code&gt;#!/bin/bash

### This script works for any number of nodes, Ray will find and manage all resources
#SBATCH --nodes=2

### Give all resources to a single Ray task, ray can manage the resources internally
#SBATCH --ntasks-per-node=1
#SBATCH --gpus-per-task=1
#SBATCH --cpus-per-task=4

#SBATCH -o raylo_world.out
#SBATCH -J raylo_world

export PATH=$PATH:~/.local/bin

################# DON NOT CHANGE THINGS HERE UNLESS YOU KNOW WHAT YOU ARE DOING ###############
# This script is a modification to the implementation suggest by gregSchwartz18 here:
# https://github.com/ray-project/ray/issues/826#issuecomment-522116599
redis_password=$(uuidgen)
export redis_password

nodes=$(scontrol show hostnames $SLURM_JOB_NODELIST) # Getting the node names
nodes_array=( $nodes )

node_1=${nodes_array[0]}
ip=$(srun --nodes=1 --ntasks=1 -w $node_1 hostname --ip-address) # making redis-address
port=6379
ip_head=$ip:$port
export ip_head
echo "IP Head: $ip_head"

mkdir ray_temp

echo "STARTING HEAD at $node_1"
srun --nodes=1 --ntasks=1 -w $node_1 ray start --temp-dir=./ray_temp --block --head --port 6379 --redis-password $redis_password &amp;
sleep 30

worker_num=$(($SLURM_JOB_NUM_NODES - 1)) #number of nodes other than the head node
for ((  i=1; i&lt;=$worker_num; i++ ))
do
  node_i=${nodes_array[$i]}
  echo "STARTING WORKER $i at $node_i"
  srun --nodes=1 --ntasks=1 -w $node_i ray start --temp-dir=./ray_temp --block --address=$ip_head --redis-password=$redis_password &amp;
  sleep 5
done
##############################################################################################

#### call your code below
python ray_scripts/raylo_world.py
exit
&lt;/denchmark-code&gt;

where the contents of raylo_world.py is
&lt;denchmark-code&gt;import ray
import os
print(os.environ["ip_head"], os.environ["redis_password"])
ray.init(address='auto', _node_ip_address=os.environ["ip_head"].split(":")[0], _redis_password=os.environ["redis_password"]) 
print(ray.nodes())
&lt;/denchmark-code&gt;

The result is that the raylet on the worker node is killed with error code -6:
&lt;denchmark-code&gt;IP Head: X.X.X.X:6379
STARTING HEAD at node1
2020-10-08 20:59:43,678 INFO services.py:1166 -- View the Ray dashboard at http://localhost:8265
STARTING WORKER 1 at node2
2020-10-08 21:00:15,049 INFO scripts.py:568 -- Local node IP: X.X.X.X
2020-10-08 21:00:15,094 SUCC scripts.py:583 -- --------------------
2020-10-08 21:00:15,094 SUCC scripts.py:584 -- Ray runtime started.
2020-10-08 21:00:15,094 SUCC scripts.py:585 -- --------------------
2020-10-08 21:00:15,094 INFO scripts.py:587 -- To terminate the Ray runtime, run
2020-10-08 21:00:15,094 INFO scripts.py:588 --   ray stop
2020-10-08 21:00:15,094 INFO scripts.py:597 -- --block
2020-10-08 21:00:15,094 INFO scripts.py:599 -- This command will now block until terminated by a signal.
2020-10-08 21:00:15,094 INFO scripts.py:601 -- Runing subprocesses are monitored and a message will be printed if any of them terminate unexpectedly.
2020-10-08 21:00:17,098 ERR scripts.py:609 -- Some Ray subprcesses exited unexpectedly:
2020-10-08 21:00:17,098 ERR scripts.py:618 -- raylet [exit code=-6]
2020-10-08 21:00:17,098 ERR scripts.py:625 -- Remaining processes will be killed.
&lt;/denchmark-code&gt;

I looked at gcs_server.err in the session directory and it seems that the issue is that the gcs server is being run on 127.0.0.1:
&lt;denchmark-code&gt;E1008 20:59:43.682768 21887 21887 network_util.cc:62] Failed to find other valid local IP. Using 127.0.0.1, not possible to go distributed!
&lt;/denchmark-code&gt;

Is this a slurm/firewall/my cluster issue? Is there someway to force the GCS server to run 0.0.0.0 or similar?
Also, here is a similar and &lt;denchmark-link:https://github.com/ray-project/ray/issues/6902#issuecomment-701993590&gt;unresolved issue&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ray-project/ray/discussions/6902&gt;#6902&lt;/denchmark-link&gt;

Thank you!

[YES ] I have verified my script runs in a clean environment and reproduces the issue.
[YES ] I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='PSZehnder' date='2020-10-09T04:35:36Z'>
		Is the address printed by ray.init() correct? If so, it should've been resolved in the latest master.
		</comment>
		<comment id='2' author='PSZehnder' date='2020-10-09T04:58:53Z'>
		I just tried building ray from the master branch but I don't think I have bazel installed/set up correctly :/
The links to the nightly wheels are also broken (at least the linux ones, half way down the page &lt;denchmark-link:https://docs.ray.io/en/master/installation.html&gt;here&lt;/denchmark-link&gt;
)
Not to get off topic from the original question but any advice?
		</comment>
		<comment id='3' author='PSZehnder' date='2020-10-09T05:03:05Z'>
		&lt;denchmark-link:https://user-images.githubusercontent.com/18510752/95545044-aa8a8c00-09b1-11eb-98d3-29377f253af3.png&gt;&lt;/denchmark-link&gt;

Can you try downloading using this link? (ray install-nightly command might be broken at 1.0).
So what happened was gcs server resolves its addresses itself (from cpp code), but it had lots of issues in many private clusters. And I discovered all the private clusters usually have the correct IP address in Python (which is printed when you run ray.init). So I pushed a fix to just pass address from python to gcs server. This is included in the master.
If you have to use 1.0, then there's one ugly workaround. You can connect to the head node Redis using the redis client and overwrite GcsServerAddress to the correct address. (before you spawn worker nodes). This might work as workaround. But I'd like to see if using the latest master will fix your issue.
		</comment>
		<comment id='4' author='PSZehnder' date='2020-10-09T05:13:47Z'>
		Ah those links were broken a few minutes ago (all except the windows one). Were they maybe in the process of being built and uploaded to S3? Pip installed without a hitch now :)
Everything is working! Thank you so much for your help
		</comment>
		<comment id='5' author='PSZehnder' date='2020-10-09T05:32:52Z'>
		Awesome! Hope it helped :)!!
		</comment>
		<comment id='6' author='PSZehnder' date='2020-12-03T16:38:44Z'>
		&lt;denchmark-link:https://github.com/rkooo567&gt;@rkooo567&lt;/denchmark-link&gt;
 what version fixed it?
		</comment>
		<comment id='7' author='PSZehnder' date='2020-12-05T00:26:25Z'>
		Hmm, perhaps try 1.0 or the latest nightly version?
		</comment>
		<comment id='8' author='PSZehnder' date='2020-12-05T00:35:59Z'>
		I need the HPC support to build it. now the 1.0.0 is running and i cannot understand but the problem reported above
2020-10-08 21:00:17,098 ERR scripts.py:609 -- Some Ray subprcesses exited unexpectedly: 2020-10-08 21:00:17,098 ERR scripts.py:618 -- raylet [exit code=-6] 2020-10-08 21:00:17,098 ERR scripts.py:625 -- Remaining processes will be killed.
seems the same..
I'm ok with the last nightly version? or is there a different stable version i should get?
		</comment>
		<comment id='9' author='PSZehnder' date='2020-12-05T00:52:41Z'>
		Hmm, can you post all of the /tmp/ray/session_latest/logs/ files here?
		</comment>
		<comment id='10' author='PSZehnder' date='2020-12-05T21:46:09Z'>
		This is the log when i run the command

&lt;denchmark-link:https://github.com/ray-project/ray/files/5647908/logs.tar.gz&gt;logs.tar.gz&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>