<bug id='4536' author='stefanpantic' open_date='2019-04-02T09:38:38Z' closed_time='2019-04-07T07:36:19Z'>
	<summary>[rllib]Learner throws queue.Empty</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
Ray installed from (source or binary): source
Ray version: 0.7.0.dev2
Python version: 3.6.6
Exact command to reproduce: N/A

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

When trying to run ray on a cluster I sometimes get the following exception:
&lt;denchmark-code&gt;(pid=24123) Exception in thread Thread-1:
(pid=24123) Traceback (most recent call last):
(pid=24123)   File "/usr/lib/python3.6/threading.py", line 916, in _bootstrap_inner
(pid=24123)     self.run()
(pid=24123)   File "/usr/local/lib/python3.6/dist-packages/ray/rllib/optimizers/aso_learner.py", line 48, in run
(pid=24123)     self.step()
(pid=24123)   File "/usr/local/lib/python3.6/dist-packages/ray/rllib/optimizers/aso_learner.py", line 52, in step
(pid=24123)     batch, _ = self.minibatch_buffer.get()
(pid=24123)   File "/usr/local/lib/python3.6/dist-packages/ray/rllib/optimizers/aso_minibatch_buffer.py", line 38, in get
(pid=24123)     self.buffers[self.idx] = self.inqueue.get(timeout=60.0)
(pid=24123)   File "/usr/lib/python3.6/queue.py", line 172, in get
(pid=24123)     raise Empty
(pid=24123) queue.Empty
&lt;/denchmark-code&gt;

I've also had this happen when running training locally but just restarting training seems to make it go away. Any idea what could be causing this.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='stefanpantic' date='2019-04-02T09:45:46Z'>
		It could be starting the actors on a fresh cluster takes longer than 60 seconds. We should probably raise this timeout to a higher value...
		</comment>
	</comments>
</bug>