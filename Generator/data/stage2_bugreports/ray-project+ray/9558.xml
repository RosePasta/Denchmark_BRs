<bug id='9558' author='vish0910' open_date='2020-07-17T22:20:14Z' closed_time='2020-08-16T21:09:29Z'>
	<summary>[tune] Ray Cluster deployed in Kubernetes unable to sync checkpoint directories to the driver node</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

Getting the following error (Stacktrace attached):
&lt;denchmark-code&gt;2020-07-17 21:53:48,101	ERROR trial_runner.py:550 -- Trial TrainExample_fd24b_00001: Error handling checkpoint /root/ray_results/TrainExample/TrainExample_1_randomforestclassifier__n_estimators=5_2020-07-17_21-53-462l3hkjfs/checkpoint_1/
Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 546, in _process_trial_save
    trial.on_checkpoint(trial.saving_to)
  File "/opt/conda/lib/python3.6/site-packages/ray/tune/trial.py", line 448, in on_checkpoint
    self, checkpoint.value))
ray.tune.error.TuneError: Trial TrainExample_fd24b_00001: Checkpoint path /root/ray_results/TrainExample/TrainExample_1_randomforestclassifier__n_estimators=5_2020-07-17_21-53-462l3hkjfs/checkpoint_1/ not found after successful sync down.
&lt;/denchmark-code&gt;

This is output of ray_random_forest.py. From investigation it looks like the checkpoint directories ie. checkpoint_1, checkpoint_2 that were created in other nodes (workers) of the cluster do not get synced back to the driver node (from where I am running the python script).
Rest of the files in the Trial directories seem to be in sync.
This problem is not reproducible by running it on a single-machine with a cluster started up using bare init(). My guess is because all the workers point to the same file-system, and thus, the os.path.exist() returns True (This is reference to the line of code which checks and throws the above error)
Taking a look at validate_save_restore() to validate a Trainable, I have written a bare-bone script which proves that checkpoint directories are not synced across workers.
The stacktrace from checkpoint_validation_failed.py
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "code/checkpoint_validation_failed.py", line 115, in &lt;module&gt;
    _main()
  File "code/checkpoint_validation_failed.py", line 94, in _main
    print(f"VALIDATE TRAINABLE CLASS: {validate_save_restore(TrainExample)}")
  File "code/checkpoint_validation_failed.py", line 67, in validate_save_restore
    restore_check = ray.get(trainable_2.restore.remote(old_trainable))
  File "/opt/conda/lib/python3.6/site-packages/ray/worker.py", line 1526, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(FileNotFoundError): ray::TrainExample.restore() (pid=2704, ip=172.17.0.8)
  File "python/ray/_raylet.pyx", line 471, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 424, in ray._raylet.execute_task.function_executor
  File "/opt/conda/lib/python3.6/site-packages/ray/tune/trainable.py", line 444, in restore
    with open(checkpoint_path + ".tune_metadata", "rb") as f:
FileNotFoundError: [Errno 2] No such file or directory: '/root/ray_results/2020-07-17_21-43-17dbvo01tp/checkpoint_3/.tune_metadata'
command terminated with exit code 1
&lt;/denchmark-code&gt;

Here, Worker X is trying to restore checkpoint created by Worker Y.
Ray version and other system information (Python version, TensorFlow version, OS):
Setup:

Ray cluster running in K8s.
Ray version
pip list | grep ray ray 0.9.0.dev0
Docker image:
REPOSITORY                                                       TAG                 IMAGE ID            CREATED             SIZE rayproject/autoscaler                                            latest              3f8d747cb8af        7 days ago          5.25GB

Playing with sync_to_driver, sync_on_checkpoint parameters did not help.
Thanks!
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Step 1. Spin-up a Ray cluster using ray-cluster.yaml in a namespace called ray. ie

kubectl create -f ray-namespace.yaml
kubectl apply -f ray-cluster.yaml

Step 2. Copy the python script to any worker node.

kubectl -n ray get pods.
Copy WORKER_1_POD_NAME
Exec into pod kubectl -n ray exec -it &lt;WORKER_1_POD_NAME&gt; -- bin/bash
mkdir code
Copy the python script kubectl -n ray cp checkpoint_validation_failed.py  &lt;WORKER_1_POD_NAME&gt;:/code/checkpoint_validation_failed.py
Exit pod
Execute script kubectl -n ray exec &lt;WORKER_1_POD_NAME&gt; -- bin/bash -c "python code/checkpoint_validation_failed.py

You can do the same steps to run .
&lt;denchmark-link:https://github.com/ray-project/ray/files/4940566/ray_issue_related_files.zip&gt;ray_issue_related_files.zip&lt;/denchmark-link&gt;

The above zip contains:

ray-cluster.yaml
ray-namespace.yaml
checkpoint_validation_failed.py
ray_random_forest.py

If we cannot run your script, we cannot fix your issue.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='vish0910' date='2020-07-17T22:55:35Z'>
		Do you have an NFS for your kubernetes cluster? That might save you a lot of trouble.
		</comment>
		<comment id='2' author='vish0910' date='2020-07-17T23:00:44Z'>
		Hello &lt;denchmark-link:https://github.com/richardliaw&gt;@richardliaw&lt;/denchmark-link&gt;
,
Thank you for a quick reply.
No, I do not have NFS for the cluster. Does it have to be a NFS? How does ray sync files like

events.out.tfevents.1595022826.ray-worker-6d9f6b9b8d-g4zvk
params.json
params.pkl
progress.csv
result.json

But not checkpoint_1, checkpoint_2 etc.?
		</comment>
		<comment id='3' author='vish0910' date='2020-07-17T23:03:00Z'>
		The files are actually generated on the driver, not on the node. Do you have access to S3? S3 could work too.
		</comment>
		<comment id='4' author='vish0910' date='2020-07-17T23:38:23Z'>
		Okay. So what gets synced on sync_on_checkpoint?
sync_on_checkpoint (bool) â€“ Force sync-down of trial checkpoint to driver. If set to
False, checkpoint syncing from worker to driver is asynchronous and best-effort. This does
not affect persistent storage syncing. Defaults to True.
In the above documentation snippet I am interpreting checkpoint as the checkpoint_N directories described above.
Would the suggested S3 solution involve implementing DurableTrainable rather than Trainable?
Also I wanted to clarify, is the above issue a bug in Ray Tune and a network/cloud storage a work-around for it?
		</comment>
		<comment id='5' author='vish0910' date='2020-07-19T01:39:27Z'>
		Checkpoints (directories) are synced on sync_on_checkpoint, yep.
The above is just a limitation of our file-transfer mechanism; we use rsync to move files between VMs, but I think that mechanism breaks down on kubernetes.
The two approaches to workaround this limitation are to use 1. a networked storage or 2. to implement your own file-transfer protocol (check out sync_to_driver).
Does that make sense? Let me know if you have any further questions!
		</comment>
		<comment id='6' author='vish0910' date='2020-07-29T16:10:38Z'>
		Thank you Richard for this clarification.
Upon searching more, I found this: &lt;denchmark-link:https://serverfault.com/questions/741670/rsync-files-to-a-kubernetes-pod&gt;https://serverfault.com/questions/741670/rsync-files-to-a-kubernetes-pod&lt;/denchmark-link&gt;

I see similar solution applied to  in Ray. Maybe we can do the same for sync-checkpoint mechanism to resolve this issue. Or, we just implement it as a custom file-transfer protocol and make it part of the pallet.
On a side note, I have taken your suggestion to use a NFS. It has unblocked me from aggregating all my checkpoints.
Thanks once again.
		</comment>
		<comment id='7' author='vish0910' date='2020-07-29T20:45:16Z'>
		Great! yeah, KubernetesCommandRunner is what we will want to use. More than happy to accept a PR if you want to take a quick pass!
Otherwise, we'll try to get to this in the next couple of weeks.
		</comment>
		<comment id='8' author='vish0910' date='2020-07-30T15:08:08Z'>
		I spent sometime last-week to understand if I can fix it, but for me it was not a going to be a quick pass. So I will defer it to the team. Thanks once again Richard. :)
		</comment>
	</comments>
</bug>