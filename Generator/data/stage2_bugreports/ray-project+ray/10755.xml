<bug id='10755' author='n-gao' open_date='2020-09-12T07:20:16Z' closed_time='2020-09-13T07:19:51Z'>
	<summary>Ray is only using a single CPU core</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

I use ray to train using multiple GPUs with PyTorch though all processes only utilize a single core which impedes the GPU efficiency a lot (only about 20-30%).
Ray: 0.8.7
Python: 3.7.7
PyTorch: 1.5.1
&lt;denchmark-link:https://user-images.githubusercontent.com/9084670/92990041-02c88e00-f4d9-11ea-922d-a68cec8e10eb.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='n-gao' date='2020-09-12T08:08:36Z'>
		&lt;denchmark-link:https://github.com/n-gao&gt;@n-gao&lt;/denchmark-link&gt;
 Could you please share your train.py script? It’ll help us diagnose what’s going on.
		</comment>
		<comment id='2' author='n-gao' date='2020-09-12T08:12:54Z'>
		Also, is it possible that your Pytorch is installed via conda? Or to check whether there are settings for KMP/OMP threads pinning everything to 1 core?
		</comment>
		<comment id='3' author='n-gao' date='2020-09-12T08:36:37Z'>
		I installed PyTorch via pip.
Due to copyright etc. I sadly can not share my training script. I will see whether I can produce another example.
Also, I tested this on two completely different GPU clusters and it happens in both.
		</comment>
		<comment id='4' author='n-gao' date='2020-09-12T08:53:38Z'>
		is CUDA_VISIBLE_DEVICES set somewhere?
		</comment>
		<comment id='5' author='n-gao' date='2020-09-12T08:58:48Z'>
		The processes are properly distributed to the GPUs, but they only use a single core. Here is a simple example:
import ray
import torch
import tqdm


@ray.remote(num_gpus=0.5)
class Actor():
    def __init__(self) -&gt; None:
        torch.set_default_tensor_type('torch.cuda.FloatTensor')
        self.network = torch.nn.Sequential(
            torch.nn.Linear(1024, 2048),
            torch.nn.Sigmoid(),
            torch.nn.Linear(2048, 2048),
            torch.nn.Sigmoid(),
            torch.nn.Linear(2048, 2048),
            torch.nn.Sigmoid(),
            torch.nn.Linear(2048, 2048),
            torch.nn.Sigmoid(),
            torch.nn.Linear(2048, 2048),
            torch.nn.Sigmoid(),
            torch.nn.Linear(2048, 10)
        )

    def compute(self) -&gt; torch.Tensor:
        inp = torch.randn(1000, 1024)
        return self.network(inp).detach().cpu()


if __name__ == '__main__':
    ray.init()
    actors = [Actor.remote() for _ in range(8)]
    for _ in tqdm.trange(10000):
        ray.get([a.compute.remote() for a in actors])
Nvidia-smi:
&lt;denchmark-code&gt;| NVIDIA-SMI 440.100      Driver Version: 440.100      CUDA Version: 10.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:61:00.0 Off |                    0 |
| N/A   33C    P0    61W / 300W |   2457MiB / 16160MiB |     18%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-SXM2...  On   | 00000000:62:00.0 Off |                    0 |
| N/A   32C    P0   123W / 300W |   2457MiB / 16160MiB |     16%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |
| N/A   32C    P0    64W / 300W |   2457MiB / 16160MiB |     18%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |
| N/A   34C    P0   106W / 300W |   2457MiB / 16160MiB |     18%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     19167      C   ray::Actor                                  1223MiB |
|    0     19168      C   ray::Actor                                  1223MiB |
|    1     19164      C   ray::Actor                                  1223MiB |
|    1     19165      C   ray::Actor                                  1223MiB |
|    2     19162      C   ray::Actor                                  1223MiB |
|    2     19163      C   ray::Actor                                  1223MiB |
|    3     19160      C   ray::Actor                                  1223MiB |
|    3     19161      C   ray::Actor                                  1223MiB |
+-----------------------------------------------------------------------------+
&lt;/denchmark-code&gt;

Htop:
&lt;denchmark-link:https://user-images.githubusercontent.com/9084670/92991883-e3386200-f4e6-11ea-9b9d-2c7e397c2ab5.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='n-gao' date='2020-09-12T09:02:22Z'>
		That’s concerning - can you check if theres any explicit process affinity (
&lt;denchmark-link:https://unix.stackexchange.com/questions/425065/linux-how-to-know-which-processes-are-pinned-to-which-core&gt;https://unix.stackexchange.com/questions/425065/linux-how-to-know-which-processes-are-pinned-to-which-core&lt;/denchmark-link&gt;
)
within the initialization of each actor?

Also can you check to unset any OMP and KMP environment variables?

Let me know it this leads to anything!
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Sat, Sep 12, 2020 at 1:59 AM Nicholas Gao ***@***.***&gt; wrote:
 The processes are properly distributed to the GPUs, but they only use a
 single core. Here is a simple example:

 import rayimport torchimport tqdm

 @ray.remote(num_gpus=0.5)class Actor():
     def __init__(self) -&gt; None:
         torch.set_default_tensor_type('torch.cuda.FloatTensor')
         self.network = torch.nn.Sequential(
             torch.nn.Linear(1024, 2048),
             torch.nn.Sigmoid(),
             torch.nn.Linear(2048, 2048),
             torch.nn.Sigmoid(),
             torch.nn.Linear(2048, 2048),
             torch.nn.Sigmoid(),
             torch.nn.Linear(2048, 2048),
             torch.nn.Sigmoid(),
             torch.nn.Linear(2048, 2048),
             torch.nn.Sigmoid(),
             torch.nn.Linear(2048, 10)
         )

     def compute(self) -&gt; torch.Tensor:
         inp = torch.randn(1000, 1024)
         return self.network(inp).detach().cpu()

 if __name__ == '__main__':
     ray.init()
     actors = [Actor.remote() for _ in range(8)]
     for _ in tqdm.trange(10000):
         ray.get([a.compute.remote() for a in actors])

 Nvidia-smi:

 | NVIDIA-SMI 440.100      Driver Version: 440.100      CUDA Version: 10.2     |
 |-------------------------------+----------------------+----------------------+
 | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
 | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
 |===============================+======================+======================|
 |   0  Tesla V100-SXM2...  On   | 00000000:61:00.0 Off |                    0 |
 | N/A   33C    P0    61W / 300W |   2457MiB / 16160MiB |     18%      Default |
 +-------------------------------+----------------------+----------------------+
 |   1  Tesla V100-SXM2...  On   | 00000000:62:00.0 Off |                    0 |
 | N/A   32C    P0   123W / 300W |   2457MiB / 16160MiB |     16%      Default |
 +-------------------------------+----------------------+----------------------+
 |   2  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |
 | N/A   32C    P0    64W / 300W |   2457MiB / 16160MiB |     18%      Default |
 +-------------------------------+----------------------+----------------------+
 |   3  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |
 | N/A   34C    P0   106W / 300W |   2457MiB / 16160MiB |     18%      Default |
 +-------------------------------+----------------------+----------------------+

 +-----------------------------------------------------------------------------+
 | Processes:                                                       GPU Memory |
 |  GPU       PID   Type   Process name                             Usage      |
 |=============================================================================|
 |    0     19167      C   ray::Actor                                  1223MiB |
 |    0     19168      C   ray::Actor                                  1223MiB |
 |    1     19164      C   ray::Actor                                  1223MiB |
 |    1     19165      C   ray::Actor                                  1223MiB |
 |    2     19162      C   ray::Actor                                  1223MiB |
 |    2     19163      C   ray::Actor                                  1223MiB |
 |    3     19160      C   ray::Actor                                  1223MiB |
 |    3     19161      C   ray::Actor                                  1223MiB |
 +-----------------------------------------------------------------------------+

 Htop:
 [image: image]
 &lt;https://user-images.githubusercontent.com/9084670/92991883-e3386200-f4e6-11ea-9b9d-2c7e397c2ab5.png&gt;

 —
 You are receiving this because you commented.
 Reply to this email directly, view it on GitHub
 &lt;#10755 (comment)&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/ABCRZZKVKHIIWTFK6YNEPPTSFMZ5JANCNFSM4RJFB6DQ&gt;
 .



		</comment>
		<comment id='7' author='n-gao' date='2020-09-12T09:08:45Z'>
		&lt;denchmark-code&gt;pid 20036's current affinity list: 0
pid 20039's current affinity list: 0
pid 20044's current affinity list: 0
pid 20048's current affinity list: 0
pid 20049's current affinity list: 0
pid 20062's current affinity list: 0
pid 20063's current affinity list: 0
pid 20064's current affinity list: 0
pid 20065's current affinity list: 0
pid 20066's current affinity list: 0
&lt;/denchmark-code&gt;

These are the affinities for the ray processes.
What OMP and KMP environment variables (sorry I am unfamiliar with these)? There are no OMP or KMP variables set (printed via printenv).
		</comment>
		<comment id='8' author='n-gao' date='2020-09-12T17:33:30Z'>
		Hey &lt;denchmark-link:https://github.com/n-gao&gt;@n-gao&lt;/denchmark-link&gt;
,
Thanks a bunch for that output. It seems like the affinity list is not ideal (everything is being pinned to core 0). Ray doesn't manage CPU affinity, so this seems like a problem with an underlying library or environment.
You can read about OMP/KMP variables here: &lt;denchmark-link:https://software.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/optimization-and-programming-guide/openmp-support/openmp-library-support/thread-affinity-interface-linux-and-windows.html&gt;https://software.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/optimization-and-programming-guide/openmp-support/openmp-library-support/thread-affinity-interface-linux-and-windows.html&lt;/denchmark-link&gt;

I think we are very close. To further diagnose your issue, right after ray.init, can you do something like:
&lt;denchmark-code&gt;def check_envs():
	print("scan..")
	for ev in os.environ:
	    if "KMP" in ev:
	        print(ev, os.environ[ev])
	    if "OMP" in ev:
	        print(ev, os.environ[ev])
	print(".. finished scan")

ray.init()
check_envs()
&lt;/denchmark-code&gt;

and provide the output? This will help me identify whether there is an automatic setting of the environment variables.
Also, if you could execute check_envs() in the initializer of the Actor class, that'd be also helpful too.
		</comment>
		<comment id='9' author='n-gao' date='2020-09-13T06:53:14Z'>
		Main thread:
&lt;denchmark-code&gt;scan..
CONDA_PROMPT_MODIFIER (base)
PROMPT_COMMAND history -a; history -c; history -r;
OMP_NUM_THREADS 1
.. finished scan
&lt;/denchmark-code&gt;

Every actor has the same environment variables:
&lt;denchmark-code&gt;(pid=15565) scan..
(pid=15565) CONDA_PROMPT_MODIFIER (base)
(pid=15565) PROMPT_COMMAND history -a; history -c; history -r;
(pid=15565) OMP_NUM_THREADS 1
(pid=15565) .. finished scan
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='n-gao' date='2020-09-13T07:19:51Z'>
		I found the issue on my system, the compute node was attributed to me by slurm and without specifying the number of cpu cores, slurm forces all processes to a single core. I could reproduce the same issue with multiprocessing, so this is completely ray unrelated. Sorry for the confusion and thank you for your time!
		</comment>
		<comment id='11' author='n-gao' date='2020-09-13T07:26:17Z'>
		Awesome! That's a good catch - should we add it to the docs?
Also, be sure to do ModelActorClass = ray.remote(num_gpus=1)(ModelClass) &lt;= to use CUDA VISIBLE DEVICES.
		</comment>
		<comment id='12' author='n-gao' date='2020-09-13T07:30:10Z'>
		FYI, this is not solely Slurm specifiy, it seems that PBS does the same (had the same issue on a PBS cluster).
		</comment>
	</comments>
</bug>