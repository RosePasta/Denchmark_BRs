<bug id='12611' author='DanielWicz' open_date='2020-12-03T21:51:43Z' closed_time='2020-12-04T22:11:55Z'>
	<summary>DQN with Couriosity exploration, gives always zero reward</summary>
	<description>
While using couriosity with DQN, the reward during an episode is always 0.
Ray version and other system information (Python version, TensorFlow version, OS):
Python version: 3.7.8
Framework: Torch 1.7
OS: Ubuntu 20.10
Ray: 1.0.1
Code:
&lt;denchmark-code&gt;import ray
import ray.rllib.agents.dqn as dqn
import gym
ray.init(ignore_reinit_error=True)

config = dqn.DEFAULT_CONFIG.copy()
config["env"] = "FrozenLake-v0"
config["env_config"] = {
    "desc": [
        "SFFFFFFF",
        "FFFFFFFF",
        "FFFFFFFF",
        "FFFFFFFF",
        "FFFFFFFF",
        "FFFFFFFF",
        "FFFFFFFF",
        "FFFFFFFG",
    ],
    "is_slippery": True
}
config["num_workers"] = 0
config["num_envs_per_worker"] = 1
config["num_gpus"] = 0
# To terminate early and make
# Without couriosity, the reward should be always 0
config["horizon"] = 8
config["framework"] = "torch"

config["exploration_config"] = {
    "type": "Curiosity",  # &lt;- Use the Curiosity module for exploring.
    "eta": 1.0,  # Weight for intrinsic rewards before being added to extrinsic ones.
    "lr": 0.005,  # Learning rate of the curiosity (ICM) module.
    "feature_dim": 256,  # Dimensionality of the generated feature vectors.
    # Setup of the feature net (used to encode observations into feature (latent) vectors).
    "feature_net_config": {
        "fcnet_hiddens": [256],
        "fcnet_activation": "relu",
    },
    "inverse_net_hiddens": [256],  # Hidden layers of the "inverse" model.
    "inverse_net_activation": "relu",  # Activation of the "inverse" model.
    "forward_net_hiddens": [256],  # Hidden layers of the "forward" model.
    "forward_net_activation": "relu",  # Activation of the "forward" model.
    "beta": 0.2,  # Weight for the "forward" loss (beta) over the "inverse" loss (1.0 - beta).
    # Specify, which exploration sub-type to use (usually, the algo's "default"
    # exploration, e.g. EpsilonGreedy for DQN, StochasticSampling for PG/SAC).
    "sub_exploration": {
        "type": "EpsilonGreedy",
    }
}
agent = dqn.DQNTrainer(env="FrozenLake-v0", config=config)

for n in range(5):
    result = agent.train()
    print(n + 1,
          result["info"],
          result["timers"],
          result["timesteps_total"],
          "\n Episode reward",
          result["episode_reward_mean"]
          )
&lt;/denchmark-code&gt;

To run the DQN despite the bug, I used the workaround from issue &lt;denchmark-link:https://github.com/ray-project/ray/issues/11468&gt;#11468&lt;/denchmark-link&gt;


 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='DanielWicz' date='2020-12-04T22:11:55Z'>
		Okay, I misunderstood how the reward is reported. The reward is not added to the episode_reward, but it is still in the batch of experience. Therefore, a feature which shows intrinsic reward would be useful. I close the issue.
		</comment>
	</comments>
</bug>