<bug id='2707' author='whikwon' open_date='2018-08-22T01:35:20Z' closed_time='2018-09-04T03:02:04Z'>
	<summary>[rllib] MeanStdFilter value problem during compute action</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
Ray installed from (source or binary): source
Ray version: 0.5.0
Python version: 3.6.1
Exact command to reproduce:

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

When I run agents have been trained on for evaluation, I found that the agent doesn't work well like the rewards I've monitored.
And I've found that when I restore agent, the filter(MeanStdFilter) has somewhat strange values.
Have you ever heard of such a problem?
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

agent = cls(env="prosthetics", config=config)
agent.restore(checkpoint_path)
print(agent.local_evaluator.filters)
&gt;&gt;&gt; {'default': MeanStdFilter((158,), True, True, None, (n=128033777, mean_mean=-1.0975956375310988e+181, mean_std=inf), (n=0, mean_mean=0.0, mean_std=0.0))}
	</description>
	<comments>
		<comment id='1' author='whikwon' date='2018-08-22T02:43:28Z'>
		Is there any way to manage rolling means and stds for each coord of observation?
		</comment>
		<comment id='2' author='whikwon' date='2018-08-22T02:54:12Z'>
		Sorry, I've found it. MeanStdFilter handles rolling means and stds for each coord of observation.
How can i prevent inf values in MeanStdFilter?
		</comment>
		<comment id='3' author='whikwon' date='2018-08-22T05:26:02Z'>
		&lt;denchmark-link:https://github.com/richardliaw&gt;@richardliaw&lt;/denchmark-link&gt;
 is it possible to get an  during filter merges without some observation having an ?
		</comment>
		<comment id='4' author='whikwon' date='2018-08-22T05:59:38Z'>
		&lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
 I think I've found the problem.  referred from the blog &lt;denchmark-link:https://www.johndcook.com/blog/standard_deviation/&gt;https://www.johndcook.com/blog/standard_deviation/&lt;/denchmark-link&gt;
 and I think the logic calculating  is not same as the formula.
		</comment>
		<comment id='5' author='whikwon' date='2018-08-22T06:23:40Z'>
		What is the problem?
		</comment>
		<comment id='6' author='whikwon' date='2018-08-22T06:40:24Z'>
		I think code calculating self._S should look like this. Anyway, that might not cause inf problem... hmm
&lt;denchmark-link:https://github.com/whikwon/ray/blob/a58347bce12cb0fffcd8056b087f9090d8af3154/python/ray/rllib/utils/filter.py#L84-L87&gt;https://github.com/whikwon/ray/blob/a58347bce12cb0fffcd8056b087f9090d8af3154/python/ray/rllib/utils/filter.py#L84-L87&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='whikwon' date='2018-08-22T07:48:08Z'>
		You can probably add a print() to determine what update causes it to reach an inf value.
		</comment>
		<comment id='8' author='whikwon' date='2018-08-22T18:21:01Z'>
		cc &lt;denchmark-link:https://github.com/eugenevinitsky&gt;@eugenevinitsky&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='whikwon' date='2018-08-22T18:25:20Z'>
		Are your actions and states also exploding in value?
		</comment>
		<comment id='10' author='whikwon' date='2018-08-23T01:11:01Z'>
		&lt;denchmark-link:https://github.com/eugenevinitsky&gt;@eugenevinitsky&lt;/denchmark-link&gt;
 I'm checking now. The variance of some values ​​appears to be very large and those might cause the problem.
		</comment>
		<comment id='11' author='whikwon' date='2018-08-29T19:03:34Z'>
		I just wonder if this issue has been fixed or not? I've run into the same issue that the evaluation didn't perform as it supposed to be when the MeanStdFilter was used in the training process.
		</comment>
		<comment id='12' author='whikwon' date='2018-08-29T19:12:47Z'>
		&lt;denchmark-link:https://github.com/RodgerLuo&gt;@RodgerLuo&lt;/denchmark-link&gt;
 could you log the values of the MeanStdFilter and check if they seem to reasonably reflect the observation inputs? A good place to do this is in FilterManager.synchronize, or you can do it in the filter class itself.
		</comment>
		<comment id='13' author='whikwon' date='2018-08-30T00:57:48Z'>
		&lt;denchmark-link:https://github.com/RodgerLuo&gt;@RodgerLuo&lt;/denchmark-link&gt;
 Which environment have you used for training? Is there any abnormal feature in the env?
		</comment>
		<comment id='14' author='whikwon' date='2018-08-30T04:17:27Z'>
		&lt;denchmark-link:https://github.com/whikwon&gt;@whikwon&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
 Thanks for all the guidance. So here is what I've found:
The environment is Pendulum-v0. In agent.compute_action, I log the values of obs before and after the filter. As you will see below, the filtered values are either extremely small or large.
&lt;denchmark-code&gt;[-0.87094525  0.49138007 -2.64594034]
[-8.70945248e+07  4.91380071e+07 -2.64594034e+08]
 
[-0.81813912  0.57502033 -1.97910756]
[-8.18139124e+07  5.75020325e+07 -1.97910756e+08]
 
[-0.78058041  0.62505538 -1.25146981]
[-7.80580405e+07  6.25055383e+07 -1.25146981e+08]
 
[-0.74561678  0.66637499 -1.08267827]
[-7.45616776e+07  6.66374987e+07 -1.08267827e+08]
 
[-0.71548291  0.69863024 -0.88289703]
[-71548290.6009737   69863023.92595541 -88289703.02494954]
 
[-0.69208157  0.7218193  -0.65892435]
[-69208156.94613262  72181929.95562999 -65892435.08048297]
 
[-0.67686169  0.73611021 -0.41755988]
[-67686169.49385323  73611021.31643993 -41755987.61376046]
 
[-0.67074812  0.74168521 -0.16547722]
[-67074812.32289593  74168521.30013305 -16547721.62643052]
 
[-0.67422883  0.73852251  0.09405967]
[-67422882.58069217  73852250.50403133   9405966.79778121]
 
[-0.68740092  0.72627817  0.35968684]
[-68740091.88581493  72627816.76141532  35968683.70469721]
&lt;/denchmark-code&gt;

So it seems like the filter is not applied correctly in the code below:
&lt;denchmark-code&gt; filtered_obs = self.local_evaluator.filters[policy_id](
    observation, update=False) 
&lt;/denchmark-code&gt;

		</comment>
		<comment id='15' author='whikwon' date='2018-08-30T04:34:58Z'>
		Ah, that's the issue. Until update=True in the filter at least once, the initial observation are going to blow up passing through it
		</comment>
		<comment id='16' author='whikwon' date='2018-08-30T04:38:55Z'>
		This is a trained agent though right? So presumably the filter should have a valid value even with update=False, unless the state was not restored correctly. Maybe we should add a check that we don't try to apply un-initialized filters.
Btw which algorithm is this?
		</comment>
		<comment id='17' author='whikwon' date='2018-08-30T04:40:46Z'>
		Yes, it's a trained agent, and I used APEX DDPG to train.
		</comment>
		<comment id='18' author='whikwon' date='2018-08-30T04:56:14Z'>
		Could you throw in a print(self.local_evaluator.filters[policy_id].rs)? In particular I'm wondering if you're seeing n=0 (num samples), since I'm having a hard time reproducing this (I always see n &gt; 0, e.g., (n=1490, mean_mean=-0.5082556000843493, mean_std=1.887412412375233) after restoring with DDPG).`
		</comment>
		<comment id='19' author='whikwon' date='2018-08-30T05:04:53Z'>
		Ah, I see in &lt;denchmark-link:https://github.com/whikwon&gt;@whikwon&lt;/denchmark-link&gt;
 's initial post that n &gt; 0, but mean_std is infinity. So the question is whether the inf value is there already or is some bug in restoring the checkpoint.
To help confirm the issue, it would be great to get:

the str() of the filter before saving
the str() after restoring

If there is some script I can run (in a few min) to reproduce that would be ideal.
		</comment>
		<comment id='20' author='whikwon' date='2018-08-30T06:28:33Z'>
		OK, I'll make a log and share you. It might take few days to reproduce the error.
		</comment>
		<comment id='21' author='whikwon' date='2018-08-31T18:05:17Z'>
		&lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
 From my end, after throwing in a  right before , I've got this:
 filters: {'default': MeanStdFilter((3,), True, True, None, (n=0, mean_mean=0.0, mean_std=0.0), (n=0, mean_mean=0.0, mean_std=0.0))}
To reproudce the error, what I've done is to trian with the following hyper-parameters and evaluate a checkpoint. Please let me know if you can see the same error.
&lt;denchmark-code&gt;pendulum-apex-ddpg:
    env: Pendulum-v0
    run: APEX_DDPG
    checkpoint_freq: 1
    stop:
        training_iteration: 5 
    config:
        use_huber: True
        clip_rewards: False
        num_workers: 3
        n_step: 1
        target_network_update_freq: 50000
        tau: 1.0
        observation_filter: "MeanStdFilter"
        optimizer:
            num_replay_buffer_shards: 3
&lt;/denchmark-code&gt;

		</comment>
		<comment id='22' author='whikwon' date='2018-08-31T19:23:44Z'>
		Thanks &lt;denchmark-link:https://github.com/RodgerLuo&gt;@RodgerLuo&lt;/denchmark-link&gt;
 , I was able to reproduce and fix the issue here: &lt;denchmark-link:https://github.com/ray-project/ray/pull/2791&gt;#2791&lt;/denchmark-link&gt;

The problem was that in APEX the local filter was never updated, and we didn't do global filter synchronization.
This seems to be separate from the problem seen by &lt;denchmark-link:https://github.com/whikwon&gt;@whikwon&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>