<bug id='12717' author='meechos' open_date='2020-12-09T17:23:11Z' closed_time='2021-01-04T12:58:06Z'>
	<summary>[tune] quniform sampling results out of bound</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

The quantised uniform sampling results out of bound.  A dropout range of
&lt;denchmark-code&gt;dropout = tune.quniform(0.0, 0.9, 0.1)
&gt;1.08
&lt;/denchmark-code&gt;

Ray version and other system information (Python version, TensorFlow version, OS):
ray 1.0.1-post1
torch 1.4.0
python 3

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='meechos' date='2020-12-09T23:58:02Z'>
		cc &lt;denchmark-link:https://github.com/richardliaw&gt;@richardliaw&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='meechos' date='2020-12-16T10:32:42Z'>
		Hi &lt;denchmark-link:https://github.com/meechos&gt;@meechos&lt;/denchmark-link&gt;
, can you give a reproduction script? I can't reproduce the issue:
&lt;denchmark-code&gt;from ray import tune

dropout = tune.quniform(0.0, 0.9, 0.1)
sample = dropout.sample(size=1000000)
print(f"Min: {min(sample)} Max: {max(sample)}")
# Result: Min: 0.0 Max: 0.9
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='meechos' date='2020-12-16T10:35:38Z'>
		Are you using PopulationBasedTraining by any chance?
In short, PBT does hyperparameter mutations by multiplying existing parameters values by 0.8 or 1.2 - hence 1.08 (which is 0.9 * 1.2). This is expected behavior as discussed in the original PBT paper.
		</comment>
		<comment id='4' author='meechos' date='2021-01-04T12:58:06Z'>
		Closing because of inactivity - please feel free to reopen if this issue is not related to PBT or if you think we should introduce more thorough checks for this in PBT (in that case, please start a discussion on &lt;denchmark-link:https://discuss.ray.io/&gt;https://discuss.ray.io/&lt;/denchmark-link&gt;
 so others are encouraged to join in the discussion).
		</comment>
	</comments>
</bug>