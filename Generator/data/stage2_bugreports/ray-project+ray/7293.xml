<bug id='7293' author='falsemail' open_date='2020-02-24T13:37:04Z' closed_time='2020-03-23T19:40:23Z'>
	<summary>[rllib]custom metrics can be displayed on TensorBoard when override modelv2 's metrics method?</summary>
	<description>
I override metrics method of modelv2.py, return a dict of my custom metrics.
but the metrics can be displayed on tensorboard.
the algorithm I used is ppo
&lt;denchmark-code&gt;def metrics(self):
    """Override to return custom metrics from your model.

    The stats will be reported as part of the learner stats, i.e.,
        info:
            learner:
                model:
                    key1: metric1
                    key2: metric2

    Returns:
        Dict of string keys to scalar tensors.
    """
    return {}
&lt;/denchmark-code&gt;

Ray version and other system information (Python version, TensorFlow version, OS):
	</description>
	<comments>
		<comment id='1' author='falsemail' date='2020-02-24T20:56:43Z'>
		Can you please provide a reproduction script (see the "bug" issue template)?
		</comment>
		<comment id='2' author='falsemail' date='2020-02-25T08:25:34Z'>
		
Can you please provide a reproduction script (see the "bug" issue template)?

thanks for replay,here is my script below:
class HiRange(gym.Env):
"""Example of a custom env in which you have to walk down a corridor.
You can configure the length of the corridor via the env config."""
&lt;denchmark-code&gt;def __init__(self, config):
    # ctype class created
    self.action_space = Box(
        -1.0, 1.0, shape=(3,), dtype=np.float32)
    self.observation_space = Box(
        -1000.0, 1000.0, shape=(3,), dtype=np.float32)

    self.last_reward = 0
    self.x = 0
    self.y = 0
    self.z = 0
    self.distance = 0

def reset(self):
    self.last_reward = 0
    self.x = 0
    self.y = 0
    self.z = 0
    self.distance = 0
    self.target_pos = [1000,1000,1000]
    self.distance = np.sqrt(3*1000**2)
    self.distance_zero = np.sqrt(3*1000**2)
    self.pos=np.array([0,0,0])
    return np.array([0,0,0])

def step(self, actions):
    self.pos = self.pos + actions
    x = self.target_pos[0] - self.pos[0]
    y = self.target_pos[1] - self.pos[1]
    z = self.target_pos[2] - self.pos[2]

    distance = x * x + y * y + z * z
    distance = np.sqrt(distance)
    if abs(x) &lt;= 10 and abs(y) &lt;= 10 and abs(z) &lt;= 10:
        reward = (self.distance_zero - distance) / (self.distance_zero + 1e-10)
        done = True
    elif x &gt; 500 or y &gt; 500 or z &gt; 500: # Out of range
        reward = (self.distance_zero - distance) / (self.distance_zero + 1e-10) if not np.isnan(x * y * z) else -9
        done = True
    else:
        reward = (self.distance - distance) / (self.distance_zero + 1e-10)
        done = False
    self.distance = distance
    return self.pos, reward, done, {}
&lt;/denchmark-code&gt;

class CustomModel(RecurrentTFModelV2):
"""Example of using the Keras functional API to define a RNN model."""
&lt;denchmark-code&gt;def __init__(self,
             obs_space,
             action_space,
             num_outputs,
             model_config,
             name,
             hiddens_size=256,
             cell_size=64):
    super(CustomModel, self).__init__(obs_space, action_space, num_outputs,
                                      model_config, name)
    self.cell_size = cell_size

    # Define input layers
    input_layer = tf.keras.layers.Input(
        shape=(None, obs_space.shape[0]), name="inputs")
    state_in_h = tf.keras.layers.Input(shape=(cell_size,), name="h")
    state_in_c = tf.keras.layers.Input(shape=(cell_size,), name="c")
    seq_in = tf.keras.layers.Input(shape=(), name="seq_in")

    # Preprocess observation with a hidden layer and send to LSTM cell
    dense1 = tf.keras.layers.Dense(
        hiddens_size, activation=tf.nn.relu, name="dense1")(input_layer)
    lstm_out, state_h, state_c = tf.keras.layers.LSTM(
        cell_size, return_sequences=True, return_state=True, name="lstm")(
        inputs=dense1,
        mask=tf.sequence_mask(seq_in),
        initial_state=[state_in_h, state_in_c])

    # Postprocess LSTM output with another hidden layer and compute values
    logits = tf.keras.layers.Dense(
        self.num_outputs,
        activation=tf.keras.activations.linear,
        name="logits")(lstm_out)
    values = tf.keras.layers.Dense(
        256, activation=tf.nn.relu, name="values-1")(input_layer)
    batch_norm_v1 = tf.keras.layers.BatchNormalization()
    values = batch_norm_v1(values)

    values = tf.keras.layers.Dense(
        256, activation=tf.nn.relu, name="values-2")(values)

    batch_norm_v2 = tf.keras.layers.BatchNormalization()
    values = batch_norm_v2(values)
    self.update_ops_hi = batch_norm_v1.updates + batch_norm_v2.updates

    values = tf.keras.layers.Dense(
        1, activation=None, name="values-3")(values)
    # Create the RNN model
    self.rnn_model = tf.keras.Model(
        inputs=[input_layer, seq_in, state_in_h, state_in_c],
        outputs=[logits, values, state_h, state_c])
    self.register_variables(self.rnn_model.variables)
    self.rnn_model.summary()

@override(TFModelV2)
def update_ops(self):
    return self.update_ops_hi

@override(RecurrentTFModelV2)
def forward_rnn(self, inputs, state, seq_lens):
    model_out, self._value_out, h, c = self.rnn_model([inputs, seq_lens] +
                                                      state)
    return model_out, [h, c]

@override(ModelV2)
def get_initial_state(self):
    return [
        np.zeros(self.cell_size, np.float32),
        np.zeros(self.cell_size, np.float32),
    ]
@override(ModelV2)
def metrics(self):
    return {"value_output": self._value_out}

@override(ModelV2)
def value_function(self):
    return tf.reshape(self._value_out, [-1])
&lt;/denchmark-code&gt;

if name == "main":
# Can also register the env creator function explicitly with:
# register_env("corridor", lambda config: SimpleCorridor(config))
ray.init(webui_host='127.0.0.1')
&lt;denchmark-code&gt;def explore(config):
    # ensure we collect enough timesteps to do sgd
    if config["train_batch_size"] &lt; config["sgd_minibatch_size"] * 2:
        config["train_batch_size"] = config["sgd_minibatch_size"] * 2
    # ensure we run at least one sgd iter
    if config["num_sgd_iter"] &lt; 1:
        config["num_sgd_iter"] = 1
    return config

pbt = PopulationBasedTraining(
    time_attr="time_total_s",
    metric="episode_reward_mean",
    mode="max",
    perturbation_interval=1200,
    resample_probability=0.25,
    # Specifies the mutations of these hyperparams
    hyperparam_mutations={
        "lambda": lambda: random.uniform(0.9, 1.0),
        "clip_param": lambda: random.uniform(0.02, 0.5),
        "lr": [1e-3, 5e-4, 1e-4, 5e-5, 1e-5, 1e-6],
        "num_sgd_iter": lambda: random.randint(1, 30),
        "sgd_minibatch_size": lambda: random.randint(1024, 16384 * 2),
        "train_batch_size": lambda: random.randint(20000, 200000),
        # "sgd_minibatch_size": lambda: random.randint(102, 163 * 2),
        # "train_batch_size": lambda: random.randint(2000, 3000),
        "entropy_coeff": [0.0, 0.0001, 1e-6, 1e-8, 1e-9, 6e-10],
        # "env_config": lambda: {
        #     "class_level": phase,
        # },
    },
    custom_explore_fn=explore)

ModelCatalog.register_custom_model("my_model", CustomModel)
tune.run(
    "PPO",
    name='hiworld',
    checkpoint_freq=20,
    num_samples=2,
    scheduler=pbt,
    config={
        "env": HiRange,  # or "corridor" if registered above
        "model": {
            "custom_model": "my_model",
        },
        'num_workers': 3,
        "lambda": 0.95,
        "clip_param": 0.2,
        "entropy_coeff": 6e-10,
        "lr": 1e-5,
        "num_sgd_iter": sample_from(
            lambda spec: random.choice([10, 20, 30])),
        "sgd_minibatch_size": sample_from(
            lambda spec: random.choice([1024, 2048, 4096])),
        "train_batch_size": sample_from(
            lambda spec: random.choice([100000, 150000])),
        "env_config": {},
    },

)
&lt;/denchmark-code&gt;

and the result is :
Result for PPO_HiRange_9edaf3bc:
custom_metrics: {}
date: 2020-02-25_16-19-24
done: false
episode_len_mean: 1.0
episode_reward_max: 0.0009999999999999204
episode_reward_mean: 2.7741471189582446e-05
episode_reward_min: -0.0010000000000000516
episodes_this_iter: 100000
episodes_total: 300000
experiment_id: 7868003cfc6a4ea29255993ecec37173
experiment_tag: 0_num_sgd_iter=30,sgd_minibatch_size=4096,train_batch_size=100000
hostname: iZbp15xx9gqtg1muz0y8pwZ
info:
grad_time_ms: 39355.053
learner:
default_policy:
cur_kl_coeff: 0.05000000074505806
cur_lr: 9.999999747378752e-06
entropy: 4.191382884979248
entropy_coeff: 5.999999941330714e-10
kl: 0.0014330720296129584
policy_loss: -0.042286962270736694
total_loss: -0.04221513494849205
vf_explained_var: -1.4901161193847656e-08
vf_loss: 1.7163733900815714e-07
load_time_ms: 1765.43
num_steps_sampled: 300000
num_steps_trained: 294912
sample_time_ms: 81602.604
update_time_ms: 820.159
iterations_since_restore: 3
node_ip: 172.16.162.61
num_healthy_workers: 3
off_policy_estimator: {}
perf:
cpu_util_percent: 85.96818181818182
ram_util_percent: 42.67897727272727
pid: 1656
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
mean_env_wait_ms: 0.0669762693249851
mean_inference_ms: 1.4597577955574508
mean_processing_ms: 0.8858957409465554
time_since_restore: 375.51816630363464
time_this_iter_s: 123.7411642074585
time_total_s: 375.51816630363464
timestamp: 1582618764
timesteps_since_restore: 300000
timesteps_this_iter: 100000
timesteps_total: 300000
training_iteration: 3
trial_id: 9edaf3bc
as you saw, the custom metric wasn't in info, and can not be displayed in tensorboard~
		</comment>
	</comments>
</bug>