<bug id='7843' author='pengzhenghao' open_date='2020-04-01T03:14:32Z' closed_time='2020-04-03T17:44:59Z'>
	<summary>[rllib] PPO policy return all zeros for action_logp</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

Ray version and other system information (Python version, TensorFlow version, OS):
PPO policy return all zeros when computing the log probability of actions.
ray=0.8.2 (latest)
tensorflow=2.1.0
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
import ray
import numpy as np
from ray.rllib.agents.ppo import PPOTrainer

ray.init()

ppo_agent = PPOTrainer(env="BipedalWalker-v2")

ppo_agent.get_policy().get_session().run(
    ppo_agent.get_policy()._action_logp,
    feed_dict={
        ppo_agent.get_policy()._input_dict["obs"]: np.random.random((500, 24))
    }
)
Result in
&lt;denchmark-code&gt;array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0.], dtype=float32)
&lt;/denchmark-code&gt;

If we cannot run your script, we cannot fix your issue.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='pengzhenghao' date='2020-04-01T03:36:18Z'>
		The issue still happen in Ray=0.8.3
import ray
import numpy as np
from ray.rllib.agents.ppo import PPOTrainer

print(ray.__version__)

ray.init(ignore_reinit_error=True)

ppo_agent = PPOTrainer(env="BipedalWalker-v2")

ppo_agent.get_policy().get_session().run(
    ppo_agent.get_policy()._sampled_action_logp,
    feed_dict={
        ppo_agent.get_policy()._input_dict["obs"]: np.random.random((500, 24))
    }
)
		</comment>
		<comment id='2' author='pengzhenghao' date='2020-04-01T04:45:26Z'>
		Ray 0.8.1 works well! Something wrong with the updates introduced by 0.8.2
		</comment>
		<comment id='3' author='pengzhenghao' date='2020-04-01T05:38:11Z'>
		I did a bit of digging and it seems related to the is_exploring flag somehow being False for PPO in multi-GPU mode (works fine with simple_optimizer: True). &lt;denchmark-link:https://github.com/sven1977&gt;@sven1977&lt;/denchmark-link&gt;

I think this is since the explore placeholder isn't passed to copies in dynamic_tf_policy.copy().
		</comment>
		<comment id='4' author='pengzhenghao' date='2020-04-01T06:11:53Z'>
		Yeah, that's possible. I'll take a look.
		</comment>
		<comment id='5' author='pengzhenghao' date='2020-04-01T06:32:07Z'>
		Yeah, it's not copied along. Also default placeholder is True for TFPolicy, but False for DynamicTFPolicy (I made them both True now).
We are still not copying the is_training placeholder either? Shouldn't we do that as well?
		</comment>
		<comment id='6' author='pengzhenghao' date='2020-04-01T06:33:27Z'>
		I'm assuming the copy()-method is used for multi-GPU tower generation?
		</comment>
		<comment id='7' author='pengzhenghao' date='2020-04-01T06:37:09Z'>
		Also, you should probably rather do:
&lt;denchmark-code&gt;a, state_out, extras = trainer.get_policy().compute_actions([your_obs])

# Then:
action_probs = extras["action_prob"]
action_logp = extras["action_logp"]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='pengzhenghao' date='2020-04-01T07:44:08Z'>
		Yep, copy is only used to generate towers.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Tue, Mar 31, 2020, 11:37 PM Sven Mika ***@***.***&gt; wrote:
 Also, you should probably rather do:

 a, state_out, extras = trainer.get_policy().compute_actions([your_obs])

 # Then:
 action_probs = extras["action_prob"]
 action_logp = extras["action_logp"]

 —
 You are receiving this because you commented.
 Reply to this email directly, view it on GitHub
 &lt;#7843 (comment)&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/AAADUSWBDXCPIHZOF2WQW7LRKLOKFANCNFSM4LYK5SMQ&gt;
 .



		</comment>
		<comment id='9' author='pengzhenghao' date='2020-04-01T10:12:18Z'>
		&lt;denchmark-link:https://github.com/pengzhenghao&gt;@pengzhenghao&lt;/denchmark-link&gt;
 Could you try this PR and see whether it fixes your issue?
&lt;denchmark-link:https://github.com/ray-project/ray/pull/7846&gt;#7846&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='pengzhenghao' date='2020-04-02T01:32:07Z'>
		&lt;denchmark-link:https://user-images.githubusercontent.com/22206995/78201694-ca9e3780-74c4-11ea-950c-209674ae29a5.png&gt;&lt;/denchmark-link&gt;

Is this related the issue and PR?
		</comment>
	</comments>
</bug>