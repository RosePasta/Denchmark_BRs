<bug id='12389' author='richardliaw' open_date='2020-11-25T05:05:26Z' closed_time='2020-12-02T00:59:24Z'>
	<summary>[tune] resume is not working properly</summary>
	<description>
Hi,
My issue:
I would like to be able to resume a training run (for the time being a standard "Choice" brute force search), if the run interrupts...
but it does not work as I would expect.
In my example I started a grid search with ~450 different values via tune.Choice(). I used a cluster with roughly 50 nodes,  1 worker per node, and artificially stopped the tune run after some time. At this time, the remaining ~400 trials were not yet started. Then I try to resume the whole run by running the script again, this time with resume=True (or LOCAL/PROMPT).
However, tune only resumes and restarts those jobs that were already running on the nodes, i.e. the 50 that have local subfolders.
How can I make tune resume ALL 450 trials that are in principle part of this Experiment?
Best,
Thorsten
Originally posted by @thoglu in #12265
&lt;denchmark-link:https://github.com/richardliaw&gt;@richardliaw&lt;/denchmark-link&gt;
 -&gt; I was able to reproduce this after I ran:
&lt;denchmark-code&gt;#!/usr/bin/env python

import argparse
import time

import ray
from ray import tune
from ray.tune.schedulers import AsyncHyperBandScheduler


def evaluation_fn(step, width, height):
    time.sleep(0.1)
    return (0.1 + width * step / 100)**(-1) + height * 0.1


def easy_objective(config):
    # Hyperparameters
    width, height = config["width"], config["height"]

    for step in range(config["steps"]):
        # Iterative training function - can be an arbitrary training procedure
        intermediate_score = evaluation_fn(step, width, height)
        # Feed the score back back to Tune.
        import time
        time.sleep(1)
        tune.report(iterations=step, mean_loss=intermediate_score)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--smoke-test", action="store_true", help="Finish quickly for testing")
    parser.add_argument(
        "--ray-address",
        help="Address of Ray cluster for seamless distributed execution.",
        required=False)
    args, _ = parser.parse_known_args()
    ray.init(address=args.ray_address)

    # AsyncHyperBand enables aggressive early stopping of bad trials.
    # scheduler = AsyncHyperBandScheduler(grace_period=5, max_t=100)

    # 'training_iteration' is incremented every time `trainable.step` is called
    stopping_criteria = {"training_iteration": 1 if args.smoke_test else 9999}

    analysis = tune.run(
        easy_objective,
        resume=True,
        metric="mean_loss",
        mode="min",
        # scheduler=scheduler,
        stop=stopping_criteria,
        num_samples=20,
        local_dir="/root/ray_results/",
        verbose=1,
        resources_per_trial={
            "cpu": 1,
            "gpu": 0
        },
        config={  # Hyperparameter space
            "steps": 100,
            "width": tune.uniform(10, 100),
            "height": tune.uniform(0, 100),
        })
    print("Best hyperparameters found were: ", analysis.best_config)
&lt;/denchmark-code&gt;

and terminated the job, and reran it.
	</description>
	<comments>
		<comment id='1' author='richardliaw' date='2020-12-02T00:59:21Z'>
		This is a duplicate issue.
		</comment>
	</comments>
</bug>