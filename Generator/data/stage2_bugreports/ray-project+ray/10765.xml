<bug id='10765' author='bentzinir' open_date='2020-09-13T19:14:43Z' closed_time='2020-09-24T16:57:38Z'>
	<summary>[rllib] SAC/DDPG state preprocessor variables not included in grad calculation</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

It is not clear to me if the variables of a state-preprocessor, i.e. the core "forward" layers, are assigned to a gradient.
Assume I use sac (or ddpg) with a VisionNet as a state-preprocessor.
Those variables are not registered as actor variables, nor as q variables.
However, the actor (critic) gradient is selectively defined over actor (critic) variables only.
Is it possible that the variables of the state-preprocessor are not being trained?
Ray version and other system information (Python version, TensorFlow version, OS):
python3.6, ray 0.8, ubuntu 18
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
If we cannot run your script, we cannot fix your issue.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='bentzinir' date='2020-09-21T23:50:30Z'>
		Shouldn't they be registered by default by being members of nn.Module? You can probably check the .variables() list to see if they are indeed registered.
		</comment>
		<comment id='2' author='bentzinir' date='2020-09-22T06:40:31Z'>
		Forgive the misleading title. They are registered but do not belong to any gradient.
The sac implementation defines its gradients over a selected list of variables.
For example, the actor gradient is defined explicitly over the actor variables (see gradients_fn in sac_tf_policy.py). Those are the weights that are returned by policy_variables in sac_tf_model.py. However, this contains just the weights of the output heads (2 FC layers) and not the weights of the state-preprocessor.
While there are 24 registered variables, most of them belong to the cnn, the gradient of the policy is defined over 4 weights only.
In the continuous mode, there is usually no pre-processing. Therefore you won't see this behavior there
		</comment>
		<comment id='3' author='bentzinir' date='2020-09-22T06:42:48Z'>
		Good point. That does look like a problem, &lt;denchmark-link:https://github.com/michaelzhiluo&gt;@michaelzhiluo&lt;/denchmark-link&gt;
 can you have a closer look? cc &lt;denchmark-link:https://github.com/sven1977&gt;@sven1977&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='bentzinir' date='2020-09-22T08:32:07Z'>
		It makes sense that the preprocessor variables aren't part of the model, as intended in &lt;denchmark-link:https://docs.ray.io/en/latest/rllib-models.html&gt;data pipeline&lt;/denchmark-link&gt;
. Is it possible you can move this preprocessor the model (i.e. define a custom model)?
		</comment>
		<comment id='5' author='bentzinir' date='2020-09-22T11:10:11Z'>
		The current data pipeline supports a shared body configuration because the state-preprocessor is assumed to be (unique and) shared between the actor and the critic. I think that this should be left as a design choice.
		</comment>
		<comment id='6' author='bentzinir' date='2020-09-22T17:56:07Z'>
		&lt;denchmark-link:https://github.com/michaelzhiluo&gt;@michaelzhiluo&lt;/denchmark-link&gt;
 , that pipeline is referring to the preprocessor outside the model. &lt;denchmark-link:https://github.com/bentzinir&gt;@bentzinir&lt;/denchmark-link&gt;
 is talking about the state_preprocessor network in SAC which is differentiable through the loss.
		</comment>
		<comment id='7' author='bentzinir' date='2020-09-22T18:26:22Z'>
		Yes, this is a bug. The "preprocessor" here is not an RLlib Preprocessor object, but e.g. a simple Conv2D stack (chosen automatically if possible by RLlib) pre-attached to the Q- and policy models.
Will fix this. ...
		</comment>
		<comment id='8' author='bentzinir' date='2020-09-22T19:56:53Z'>
		Sorry, can you be more specific about the version of ray you are using, as well as some information on your config, tf/pytorch, etc..?
		</comment>
		<comment id='9' author='bentzinir' date='2020-09-22T19:57:02Z'>
		&lt;denchmark-link:https://github.com/bentzinir&gt;@bentzinir&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='bentzinir' date='2020-09-22T20:00:57Z'>
		I do see all variables registered correctly for tf as well as all parameters included in the nn.Module for pytorch:
rllib\agents\sac\tests\test_sac.py::TestSAC::test_sac_compilation
framework=torch|tf
env=MsPacmanNoFrameskip-v4
		</comment>
		<comment id='11' author='bentzinir' date='2020-09-22T20:03:15Z'>
		&lt;denchmark-link:https://github.com/sven1977&gt;@sven1977&lt;/denchmark-link&gt;
 see the comment &lt;denchmark-link:https://github.com/ray-project/ray/issues/10765#issuecomment-696537955&gt;#10765 (comment)&lt;/denchmark-link&gt;
, it seems the vars are registered but not included in the grad calcuation?
		</comment>
		<comment id='12' author='bentzinir' date='2020-09-22T20:06:31Z'>
		ray 0.8, TensorFlow 1. I think that this is also the case with ddpg.
		</comment>
		<comment id='13' author='bentzinir' date='2020-09-22T20:09:28Z'>
		Ah, yes, sorry, checking the custom gradient functions ...
		</comment>
		<comment id='14' author='bentzinir' date='2020-09-22T20:13:02Z'>
		Yeah, we don't calculate or apply gradients for these, true. Ok, I'll make the self._actor_optimizer take care of training the preprocessor vars. Any objections?
		</comment>
		<comment id='15' author='bentzinir' date='2020-09-22T20:40:42Z'>
		&lt;denchmark-link:https://github.com/ray-project/ray/pull/10959&gt;#10959&lt;/denchmark-link&gt;

		</comment>
		<comment id='16' author='bentzinir' date='2020-09-22T20:41:38Z'>
		DDPG still missing. Will PR (see above) tomorrow.
		</comment>
		<comment id='17' author='bentzinir' date='2020-09-23T05:18:56Z'>
		
Yeah, we don't calculate or apply gradients for these, true. Ok, I'll make the self._actor_optimizer take care of training the preprocessor vars. Any objections?

Makes sense, however, the main issue in my own opinion is to allow a separate body configuration as well.
Another thing, you might want to take the minimum of the Qs in the discrete actor loss function.
		</comment>
		<comment id='18' author='bentzinir' date='2020-09-23T06:39:57Z'>
		So true. We are planning on making Model-architecture configurations much simpler and more flexible in the future (~Q4, Q1 2021). What you should do if you want to separate into 2 Conv2D stacks is to just write a custom Model class that does this, then wrap the SACModel with that custom class.
		</comment>
		<comment id='19' author='bentzinir' date='2020-09-23T06:43:33Z'>
		Great catch on the min Q for discrete actions! I'll fix that.
		</comment>
		<comment id='20' author='bentzinir' date='2020-09-23T16:04:37Z'>
		I also had problems with this and assigned the variables to the critic loss, not the actor loss. My motivation for this was based on empirical evidence from the &lt;denchmark-link:https://arxiv.org/pdf/1801.00690.pdf&gt;deepmind control suite paper&lt;/denchmark-link&gt;
. Figure 7 and 8 show that the network barely learns anything when the CNN variables are optimized as part of the actor loss.
		</comment>
		<comment id='21' author='bentzinir' date='2020-09-24T08:38:13Z'>
		That's interesting. They say that separate bodies performed the worse, which is what I use now :(
In my opinion, though, such conclusions should be treated with caution.
I couldn't find Figure 8. I assume you referred to Fig7?
Any objections to closing the issue?
		</comment>
		<comment id='22' author='bentzinir' date='2020-09-24T11:23:49Z'>
		&lt;denchmark-link:https://github.com/bentzinir&gt;@bentzinir&lt;/denchmark-link&gt;
  : No objection to closing - thanks for your comment! Yes, sorry, I meant Figure 6 and 7.
Did you have any success with training a pixel based policy other than on Atari? So far, its eluding me and I would be grateful for any hints which environments and algorithms work!
		</comment>
		<comment id='23' author='bentzinir' date='2020-09-24T16:57:35Z'>
		It would be foolish of me to pretend to know the answer to this...
But here is my two cents:
Pixel-based policies usually go together with discrete actions. In this case, PPO is your go-to algorithm.
As a trust-region algorithm, it will provide you with "safe" optimization.
However, as an on-policy algorithm, you might be facing exploration challenges that are better to solve by tweaking the environment if possible.
		</comment>
	</comments>
</bug>