<bug id='6592' author='hartikainen' open_date='2019-12-24T08:57:58Z' closed_time='2020-02-07T19:37:08Z'>
	<summary>[tune] Custom contents from worker results directory are not fully synced to cloud</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

When saving files to the results directory on each trainable step on a worker, only the files from first worker iterations get stored to cloud. When I inspect the trial directories in the worker machines, they are empty and I can't see even the first files that have been synced to cloud.
All the files from the head node get synced as expected.
Ray version and other system information (Python version, TensorFlow version, OS):
&lt;denchmark-code&gt;$ python --version
Python 3.7.3
$ python -c "import ray; print(ray.__version__)"
0.8.0
&lt;/denchmark-code&gt;

Does the problem occur on the latest wheels? Not sure, only tried on the latest 0.8.0.
&lt;denchmark-h:h3&gt;Reproduction&lt;/denchmark-h&gt;

Run experiment with tune, such that each trainable step writes a file (e.g. an image) to a subdirectory inside the trial directory (e.g. plt.savefig(os.path.join(os.getcwd(), 'subdir', timestep_image_name))). Run this on cluster with at least 1 worker node. The files from the worker don't get correctly synced.
	</description>
	<comments>
		<comment id='1' author='hartikainen' date='2019-12-24T09:12:18Z'>
		Looking at the logs from the driver, I see this:
&lt;denchmark-code&gt;2019-12-24 09:07:36,904 ERROR ray_trial_executor.py:572 -- Trial id=bb440487-seed=6642: Error handling checkpoint /home/ubuntu/ray_results/gym/Ant/Pond-v0/2
019-12-22T23-35-26-pond-test-1/id=bb440487-seed=6642_2019-12-22_23-40-54h_qlwx3f/checkpoint_421/
Traceback (most recent call last):
  File "/home/ubuntu/.conda/envs/softlearning/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py", line 569, in save
    trial.on_checkpoint(checkpoint)
  File "/home/ubuntu/.conda/envs/softlearning/lib/python3.7/site-packages/ray/tune/trial.py", line 322, in on_checkpoint
    self.result_logger.wait()
  File "/home/ubuntu/.conda/envs/softlearning/lib/python3.7/site-packages/ray/tune/logger.py", line 446, in wait
    self._log_syncer.wait()
  File "/home/ubuntu/.conda/envs/softlearning/lib/python3.7/site-packages/ray/tune/syncer.py", line 225, in wait
    self.sync_client.wait()
  File "/home/ubuntu/.conda/envs/softlearning/lib/python3.7/site-packages/ray/tune/syncer.py", line 141, in wait
    raise TuneError("Sync error ({}): {}".format(code, error_msg))
ray.tune.error.TuneError: Sync error (23): ERROR: Skipping sender remove for changed file: checkpoint_421/checkpoint
rsync error: some files/attrs were not transferred (see previous errors) (code 23) at main.c(1668) [generator=3.1.2]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='hartikainen' date='2019-12-24T10:01:17Z'>
		
When I inspect the trial directories in the worker machines, they are empty and I can't see even the first files that have been synced to cloud.

This is a known bug caused by file removal post-rsync, but should be fixed in &lt;denchmark-link:https://github.com/ray-project/ray/pull/6376&gt;#6376&lt;/denchmark-link&gt;
 (disables the  flag)

only the files from first worker iterations get stored to cloud

how long was this experiment running for? maybe the files were synced initially from worker-&gt;head-&gt;cloud, then the next sync from head is 5 minutes later (iirc), so if it ran for less than that you won't see anything else. this is just a guess though.
the above mentioned PR will also introduce support to sync the worker logdir to cloud directly on trial checkpoint, so perhaps that will address this issue
		</comment>
		<comment id='3' author='hartikainen' date='2019-12-24T10:33:17Z'>
		
This is a known bug caused by file removal post-rsync, but should be fixed in #6376 (disables the --remove-source-files flag)

Ah sorry, I totally missed these. Great to see that this is being addressed already.


only the files from first worker iterations get stored to cloud

how long was this experiment running for? maybe the files were synced initially from worker-&gt;head-&gt;cloud, then the next sync from head is 5 minutes later (iirc), so if it ran for less than that you won't see anything else. this is just a guess though.

These have been running for ~36 hours.

the above mentioned PR will also introduce support to sync the worker logdir to cloud directly on trial checkpoint, so perhaps that will address this issue

Awesome!
Also noting that using ray==0.7.6 fixes this issues.
		</comment>
		<comment id='4' author='hartikainen' date='2020-02-06T22:50:25Z'>
		&lt;denchmark-link:https://github.com/richardliaw&gt;@richardliaw&lt;/denchmark-link&gt;
 was this fixed by &lt;denchmark-link:https://github.com/ray-project/ray/pull/6376&gt;#6376&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='5' author='hartikainen' date='2020-02-06T22:51:28Z'>
		cc &lt;denchmark-link:https://github.com/hartikainen&gt;@hartikainen&lt;/denchmark-link&gt;
 ?
		</comment>
		<comment id='6' author='hartikainen' date='2020-02-07T11:21:02Z'>
		Hmm, not 100% sure actually. I haven't seen the issue in a while, but it might be because some of my runs are still on the older version of ray.
		</comment>
		<comment id='7' author='hartikainen' date='2020-02-07T19:31:06Z'>
		I tested and couldn't repro fairly recently, I'd recommend just closing this for now and re-opening if anyone sees any issues.
		</comment>
	</comments>
</bug>