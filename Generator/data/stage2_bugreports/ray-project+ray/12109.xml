<bug id='12109' author='krfricke' open_date='2020-11-18T13:02:32Z' closed_time='2020-11-18T14:28:16Z'>
	<summary>[autoscaler] installing latest ray wheels leads to errors (no `ray status`, no worker nodes)</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

I'm using the attached cluster config to bin up a cluster with one head and two workers. After starting and attaching to the session I get
&lt;denchmark-code&gt;(base) root@ip-172-31-20-185:/# ray status
2020-11-18 05:01:16,471	INFO scripts.py:1356 -- Connecting to Ray instance at 172.31.20.185:6379.
2020-11-18 05:01:16,471	INFO worker.py:673 -- Connecting to existing Ray cluster at address: 172.31.20.185:6379
No cluster status.
&lt;/denchmark-code&gt;

Also, the worker nodes don't start up:
&lt;denchmark-code&gt;(base) root@ip-172-31-20-185:/# cat /tmp/ray/session_latest/logs/monitor.err 
Error in sys.excepthook:
Traceback (most recent call last):
  File "/root/anaconda3/lib/python3.7/site-packages/ray/worker.py", line 871, in custom_excepthook
    worker_id = global_worker.worker_id
AttributeError: 'Worker' object has no attribute 'worker_id'

Original exception was:
Traceback (most recent call last):
  File "/root/anaconda3/lib/python3.7/site-packages/ray/monitor.py", line 354, in &lt;module&gt;
    redis_password=args.redis_password)
  File "/root/anaconda3/lib/python3.7/site-packages/ray/monitor.py", line 110, in __init__
    self.load_metrics)
  File "/root/anaconda3/lib/python3.7/site-packages/ray/autoscaler/_private/autoscaler.py", line 74, in __init__
    self.reset(errors_fatal=True)
  File "/root/anaconda3/lib/python3.7/site-packages/ray/autoscaler/_private/autoscaler.py", line 387, in reset
    raise e
  File "/root/anaconda3/lib/python3.7/site-packages/ray/autoscaler/_private/autoscaler.py", line 349, in reset
    self.available_node_types = self.config["available_node_types"]
KeyError: 'available_node_types'
&lt;/denchmark-code&gt;

When I install latest ray (1.0.1) instead of the latest wheels in the cluster yaml, everything works fine.
It seems the ray version of the docker container is 1.0.0 per default.
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Local ray: latest master
cluster.yaml
&lt;denchmark-code&gt;cluster_name: kf_tune_testing
min_workers: 2
max_workers: 2
initial_workers: 2
autoscaling_mode: default
docker:
    image: 'anyscale/ray-ml:latest'
    container_name: ray_container
    pull_before_run: false
    run_options:
        - --privileged
target_utilization_fraction: 0.8
idle_timeout_minutes: 5
provider:
    type: aws
    region: us-west-2
    availability_zone: us-west-2a
    cache_stopped_nodes: false
auth:
    ssh_user: ubuntu
head_node:
    InstanceType: m5.xlarge
    ImageId: ami-05ac7a76b4c679a79
worker_nodes:
    InstanceType: m5.xlarge
    ImageId: ami-05ac7a76b4c679a79
    InstanceMarketOptions:
        MarketType: spot

file_mounts: {
    "/root/tune-tests": "/Users/kai/coding/anyscale/projects/tune-tests"
}
cluster_synced_files: []
file_mounts_sync_continuously: true
initialization_commands: []
setup_commands:
    # pip install -U ray
    - pip install -U https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-1.1.0.dev0-cp37-cp37m-manylinux2014_x86_64.whl
head_setup_commands: []
worker_setup_commands: []
head_start_ray_commands:
    - ray stop
    - "ulimit -n 65536; ray start --head --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml --resources='{\"actor_cpus\": 0}'"
worker_start_ray_commands:
    - ray stop
    - "ulimit -n 65536; ray start --address=$RAY_HEAD_IP:6379 --object-manager-port=8076 --resources='{\"actor_cpus\": 4}'"
metadata:
    anyscale:
        working_dir: ~/tune-tests
&lt;/denchmark-code&gt;

If we cannot run your script, we cannot fix your issue.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='krfricke' date='2020-11-18T13:57:43Z'>
		I am not able to reproduce this, I used the same yaml and I get the following after running ray status:
&lt;denchmark-code&gt;2020-11-18 05:56:47,993	INFO scripts.py:1356 -- Connecting to Ray instance at 172.31.28.125:6379.
2020-11-18 05:56:47,993	INFO worker.py:673 -- Connecting to existing Ray cluster at address: 172.31.28.125:6379
Cluster status: 2 nodes
 - MostDelayedHeartbeats: {'172.31.28.125': 0.18994450569152832, '172.31.24.231': 0.18990659713745117, '172.31.30.156': 0.18987488746643066}
 - NodeIdleSeconds: Min=1593 Mean=1626 Max=1694
 - ResourceUsage: 0.0/12.0 CPU, 0.0/8.0 actor_cpus, 0.0 GiB/32.09 GiB memory, 0.0 GiB/9.86 GiB object_store_memory
 - TimeSinceLastHeartbeat: Min=0 Mean=0 Max=0
Worker node types:
 - ray-legacy-worker-node-type: 2
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='krfricke' date='2020-11-18T14:28:16Z'>
		I just checked it again, and it seems I made a mistake reporting my environment (due to a local pyenv setting).
When I tested the config on filing the issue, I was on the latest ray release locally (1.0.1). This still doesn't work for me.
However, switching to latest master solves the problem. I thus assume the problem has been fixed or was due to a version mismatch.
Thanks for your help and sorry for my mistake.
		</comment>
	</comments>
</bug>