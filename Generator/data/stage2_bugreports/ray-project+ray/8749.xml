<bug id='8749' author='virtualluke' open_date='2020-06-02T19:15:32Z' closed_time='2020-06-11T21:47:28Z'>
	<summary>rerunning script causes error with 'Address already in use'</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

ray latest master
python 3.7.6
OS: redhat 7.7
I have a script that if I run it it works fine, the second time it runs fine to completion (no changes in parameters), but either on the 3rd or fourth time I get the following segfault:

$/opt/conda/bin/python test_program.py
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0602 18:09:25.494333 1188864 1188864 global_state_accessor.cc:25] Redis server address = 192.1681.1.183:6379, is test flag = 0
I0602 18:09:25.495889 1188864 1188864 redis_client.cc:141] RedisClient connected.
I0602 18:09:25.503971 1188864 1188864 redis_gcs_client.cc:88] RedisGcsClient Connected.
I0602 18:09:25.504889 1188864 1188864 service_based_gcs_client.cc:75] ServiceBasedGcsClient Connected.
E0602 18:09:25.512972839 1188864 server_chttp2.cc:40]        {"created":"@1591121365.512899346","description":"No address added out of total 1 resolved","file":"external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc","file_line":394,"referenced_errors":[{"created":"@1591121365.512896692","description":"Failed to add any wildcard listeners","file":"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_posix.cc","file_line":341,"referenced_errors":[{"created":"@1591121365.512886167","description":"Unable to configure socket","fd":15,"file":"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc","file_line":208,"referenced_errors":[{"created":"@1591121365.512882261","description":"Address already in use","errno":98,"file":"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc","file_line":181,"os_error":"Address already in use","syscall":"bind"}]},{"created":"@1591121365.512896152","description":"Unable to configure socket","fd":15,"file":"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc","file_line":208,"referenced_errors":[{"created":"@1591121365.512894081","description":"Address already in use","errno":98,"file":"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc","file_line":181,"os_error":"Address already in use","syscall":"bind"}]}]}]}
*** Aborted at 1591121365 (unix time) try "date -d @1591121365" if you are using GNU date ***
PC: @                0x0 (unknown)
*** SIGSEGV (@0x0) received by PID 1188864 (TID 0x7f27706b6740) from PID 0; stack trace: ***
@     0x7f2770297630 (unknown)

@     0x7f2768732002 grpc::ServerInterface::RegisteredAsyncRequest::IssueRequest()

@     0x7f27683e0c09 ray::rpc::CoreWorkerService::WithAsyncMethod_AssignTask&lt;&gt;::RequestAssignTask()

@     0x7f276841a1ab ray::rpc::ServerCallFactoryImpl&lt;&gt;::CreateCall()

@     0x7f276868ddc1 ray::rpc::GrpcServer::Run()

@     0x7f276842ea92 ray::CoreWorker::CoreWorker()

@     0x7f27684327f4 ray::CoreWorkerProcess::CreateWorker()

@     0x7f2768432d7f ray::CoreWorkerProcess::CoreWorkerProcess()

@     0x7f27684333cb ray::CoreWorkerProcess::Initialize()

@     0x7f276839d3c4 __pyx_pw_3ray_7_raylet_10CoreWorker_1__cinit__()

@     0x7f276839e4a5 __pyx_tp_new_3ray_7_raylet_CoreWorker()

@     0x563da5958dc9 _PyObject_FastCallKeywords

@     0x563da59a8e8f _PyEval_EvalFrameDefault

@     0x563da58fe030 _PyEval_EvalCodeWithName

@     0x563da5943917 _PyFunction_FastCallKeywords

@     0x563da59a50a6 _PyEval_EvalFrameDefault

@     0x563da58fd6f9 _PyEval_EvalCodeWithName

@     0x563da5943917 _PyFunction_FastCallKeywords

@     0x563da59a50a6 _PyEval_EvalFrameDefault

@     0x563da58fd6f9 _PyEval_EvalCodeWithName

@     0x563da58fe5f4 PyEval_EvalCodeEx

@     0x563da58fe61c PyEval_EvalCode

@     0x563da59ff974 run_mod

@     0x563da5a09cf1 PyRun_FileExFlags

@     0x563da5a09ee3 PyRun_SimpleFileExFlags

@     0x563da5a0af95 pymain_main

@     0x563da5a0b0bc _Py_UnixMain

@     0x7f276fedc545 __libc_start_main

@     0x563da59b3990 (unknown)

Segmentation fault

At this point ray is in a bad state and I need to restart the cluster.   Any help would be appreciated.
thanks,
Luke
	</description>
	<comments>
		<comment id='1' author='virtualluke' date='2020-06-02T19:27:50Z'>
		Thanks for reporting the issue! &lt;denchmark-link:https://github.com/virtualluke&gt;@virtualluke&lt;/denchmark-link&gt;
 !! Is it happening in the latest master?
		</comment>
		<comment id='2' author='virtualluke' date='2020-06-02T19:31:11Z'>
		It could be related to this issue. &lt;denchmark-link:https://github.com/ray-project/ray/issues/8715&gt;#8715&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='virtualluke' date='2020-06-02T19:31:46Z'>
		Would you also mind telling us

how you setup your autoscaler
what scripts you used?

		</comment>
		<comment id='4' author='virtualluke' date='2020-06-02T20:41:57Z'>
		not using autoscaler, just starting every node with ansible script that does a ray start
		</comment>
		<comment id='5' author='virtualluke' date='2020-06-02T20:45:32Z'>
		Yes, latest master
		</comment>
		<comment id='6' author='virtualluke' date='2020-06-02T23:26:11Z'>
		Can you try again with the latest master and see if you still have the issue?
		</comment>
		<comment id='7' author='virtualluke' date='2020-06-02T23:55:47Z'>
		&lt;denchmark-link:https://github.com/rkooo567&gt;@rkooo567&lt;/denchmark-link&gt;
 This seems like the problem addressed in &lt;denchmark-link:https://github.com/ray-project/ray/pull/8628&gt;#8628&lt;/denchmark-link&gt;
 .
&lt;denchmark-link:https://github.com/virtualluke&gt;@virtualluke&lt;/denchmark-link&gt;
, do you specify a port at all in your script?
		</comment>
		<comment id='8' author='virtualluke' date='2020-06-02T23:58:38Z'>
		&lt;denchmark-link:https://github.com/ijrsvt&gt;@ijrsvt&lt;/denchmark-link&gt;
 I am actually not sure if that will fix this issue. This issue happens because when the same job runs 3 times, some internal states are screwed. Maybe that PR can temporarily solve the issue, but if the cause is due to the broken states, some other issues can occur at different places. I assume this must be GCS related issues as we had huge changes lately.
		</comment>
		<comment id='9' author='virtualluke' date='2020-06-03T00:29:24Z'>
		&lt;denchmark-link:https://github.com/rkooo567&gt;@rkooo567&lt;/denchmark-link&gt;

The error here is that there is a fundamental Time-Of-Check/Time-Of-Use situation ingrained in the selection of the NodeManagerPort. This arose because we need to pass the NodeManager's port to as input to other commands starting. If we didn't have this requirement, we would just let GRPC choose its own port.
If the user does not provide one, Python selects one by finding the next available port and assigns that as the parameter for GRPC to use. Some time later, the NodeManager GRPC service is started. I'm assuming the bug here is that some new service is requesting the 'next available port' and the OS gives the same one that has been set for the NodeManager GRPC service.
&lt;denchmark-link:https://github.com/ray-project/ray/pull/8628&gt;#8628&lt;/denchmark-link&gt;
 binds to the port that GRPC uses and releases that port right before GRPC is started.
		</comment>
		<comment id='10' author='virtualluke' date='2020-06-03T00:38:13Z'>
		I am not sure if I 100% understood. Isn't the node manager port assignment happens when we instantiate nodes? (If not, that makes sense) This issue seems to happen after he instantiated all nodes, and all node managers are already started and running. I assume he uses ray.init(address='auto')? Does this still require node managers to be started?
		</comment>
		<comment id='11' author='virtualluke' date='2020-06-03T01:04:15Z'>
		I start ray on each of the worker nodes (in this case 12 nodes) with:
ray start --address=blah:6379 --object-store-memory=100000000000 --object-manager-port=12345 --node-manager-port=12346
I am using ray.init(address='auto') in my code
		</comment>
		<comment id='12' author='virtualluke' date='2020-06-03T01:11:17Z'>
		&lt;denchmark-link:https://github.com/virtualluke&gt;@virtualluke&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ray-project/ray/issues/8715&gt;#8715&lt;/denchmark-link&gt;
 =&gt; This has been merged (autoscaler monitor was broken whenever scripts exits because of our recent changes). I'd like to make sure your issue is independent from this issue! Would you mind trying out your script one more time with the latest master?
		</comment>
		<comment id='13' author='virtualluke' date='2020-06-03T01:15:22Z'>
		Do you know if &lt;denchmark-link:https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.9.0.dev0-cp37-cp37m-manylinux1_x86_64.whl&gt;https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.9.0.dev0-cp37-cp37m-manylinux1_x86_64.whl&lt;/denchmark-link&gt;
 has the commit ?  That is normally how I do my installs, and it seems like it varies a lot on when that image has a certain commit.
		</comment>
		<comment id='14' author='virtualluke' date='2020-06-03T01:17:22Z'>
		Yeah I believe it should be there. But other easy way to check this out is to see your /tmp/ray/session_latest/logs/monitor.err logs. Did you see any error from this logs?
		</comment>
		<comment id='15' author='virtualluke' date='2020-06-03T01:27:21Z'>
		my /tmp/ray/session_latest/logs/monitor.err is empty
		</comment>
		<comment id='16' author='virtualluke' date='2020-06-03T02:26:21Z'>
		Ahh sorry &lt;denchmark-link:https://github.com/rkooo567&gt;@rkooo567&lt;/denchmark-link&gt;
 didn't realize this was not relaunching the cluster.
		</comment>
		<comment id='17' author='virtualluke' date='2020-06-03T03:10:06Z'>
		&lt;denchmark-link:https://github.com/virtualluke&gt;@virtualluke&lt;/denchmark-link&gt;
 Gotcha. Then it is less likely the above PR will resolve this issue. But I still would like to recommend you to try the latest master. (Not sure if the link you gave me will download the
latest wheel actually, but you can use this link). &lt;denchmark-link:https://docs.ray.io/en/latest/installation.html#latest-snapshots-nightlies&gt;https://docs.ray.io/en/latest/installation.html#latest-snapshots-nightlies&lt;/denchmark-link&gt;

Also, if that doesn't resolve the issue, we need to find a way to reproduce the error. Would it be possible to have a minimal reproducible script to debug this issue?
		</comment>
		<comment id='18' author='virtualluke' date='2020-06-03T03:28:46Z'>
		That points to the same link, will install that now.
		</comment>
		<comment id='19' author='virtualluke' date='2020-06-03T04:24:28Z'>
		same crash (took 4 runs) with latest whl (just downloaded an hour ago).
		</comment>
		<comment id='20' author='virtualluke' date='2020-06-03T04:37:13Z'>
		I'm not sure if I can easily make a reproducible script.  The current script runs fine (last time 3 times in a  row) and then the next time dies immediately at the start. The script I am running takes around 7-8 minutes to run.  Trying again I get raylets all shutting down instead of segfault (beginning error messages are all the same, but fails slightly differently):

WARNING: Logging before InitGoogleLogging() is written to STDERR
I0603 04:18:43.024778 1211603 1211603 global_state_accessor.cc:25] Redis server address = 192.168.1.183:6379, is test flag = 0
I0603 04:18:43.026067 1211603 1211603 redis_client.cc:141] RedisClient connected.
I0603 04:18:43.034142 1211603 1211603 redis_gcs_client.cc:88] RedisGcsClient Connected.
I0603 04:18:43.035338 1211603 1211603 service_based_gcs_client.cc:75] ServiceBasedGcsClient Connected.
(pid=1144761, ip=192.168.1.180 E0603 04:18:49.392792446 1144761 server_chttp2.cc:40]        {"created":"@1591157929.392696436","description":"No address added out of total 1 resolved","file":"external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc","file_line":394,"referenced_errors":[{"created":"@1591157929.392693419","description":"Failed to add any wildcard listeners","file":"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_posix.cc","file_line":341,"referenced_errors":[{"created":"@1591157929.392675258","description":"Unable to configure socket","fd":35,"file":"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc","file_line":208,"referenced_errors":[{"created":"@1591157929.392666281","description":"Address already in use","errno":98,"file":"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc","file_line":181,"os_error":"Address already in use","syscall":"bind"}]},{"created":"@1591157929.392692797","description":"Unable to configure socket","fd":35,"file":"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc","file_line":208,"referenced_errors":[{"created":"@1591157929.392688890","description":"Address already in use","errno":98,"file":"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc","file_line":181,"os_error":"Address already in use","syscall":"bind"}]}]}]}
(pid=1144761, ip=192.168.1.180 *** Aborted at 1591157929 (unix time) try "date -d @1591157929" if you are using GNU date ***
(pid=1144761, ip=192.168.1.180 PC: @                0x0 (unknown)
(pid=1144761, ip=192.168.1.180 *** SIGSEGV (@0x557900000058) received by PID 1144761 (TID 0x7f416a7fb740) from PID 88; stack trace: ***
(pid=1144761, ip=192.168.1.180     @     0x7f416a3dc630 (unknown)
(pid=1144761, ip=192.168.1.180     @     0x7f41627a6432 grpc::ServerInterface::RegisteredAsyncRequest::IssueRequest()
(pid=1144761, ip=192.168.1.180     @     0x7f4162455039 ray::rpc::CoreWorkerService::WithAsyncMethod_AssignTask&lt;&gt;::RequestAssignTask()
(pid=1144761, ip=192.168.1.180     @     0x7f416248e5db ray::rpc::ServerCallFactoryImpl&lt;&gt;::CreateCall()
(pid=1144761, ip=192.168.1.180     @     0x7f41627021f1 ray::rpc::GrpcServer::Run()
(pid=1144761, ip=192.168.1.180     @     0x7f41624a2ec2 ray::CoreWorker::CoreWorker()
(pid=1144761, ip=192.168.1.180     @     0x7f41624a6c24 ray::CoreWorkerProcess::CreateWorker()
(pid=1144761, ip=192.168.1.180     @     0x7f41624a71af ray::CoreWorkerProcess::CoreWorkerProcess()
(pid=1144761, ip=192.168.1.180     @     0x7f41624a77fb ray::CoreWorkerProcess::Initialize()
(pid=1144761, ip=192.168.1.180     @     0x7f4162411774 pyx_pw_3ray_7_raylet_10CoreWorker_1__cinit()
(pid=1144761, ip=192.168.1.180     @     0x7f4162412855 __pyx_tp_new_3ray_7_raylet_CoreWorker()
(pid=1144761, ip=192.168.1.180     @     0x55798432fdc9 _PyObject_FastCallKeywords
(pid=1144761, ip=192.168.1.180     @     0x55798437fe8f _PyEval_EvalFrameDefault
(pid=1144761, ip=192.168.1.180     @     0x5579842d5030 _PyEval_EvalCodeWithName
(pid=1144761, ip=192.168.1.180     @     0x55798431a917 _PyFunction_FastCallKeywords
(pid=1144761, ip=192.168.1.180     @     0x55798437c0a6 _PyEval_EvalFrameDefault
(pid=1144761, ip=192.168.1.180     @     0x5579842d46f9 _PyEval_EvalCodeWithName
(pid=1144761, ip=192.168.1.180     @     0x5579842d55f4 PyEval_EvalCodeEx
(pid=1144761, ip=192.168.1.180     @     0x5579842d561c PyEval_EvalCode
(pid=1144761, ip=192.168.1.180     @     0x5579843d6974 run_mod
(pid=1144761, ip=192.168.1.180     @     0x5579843e0cf1 PyRun_FileExFlags
(pid=1144761, ip=192.168.1.180     @     0x5579843e0ee3 PyRun_SimpleFileExFlags
(pid=1144761, ip=192.168.1.180     @     0x5579843e1f95 pymain_main
(pid=1144761, ip=192.168.1.180     @     0x5579843e20bc _Py_UnixMain
(pid=1144761, ip=192.168.1.180     @     0x7f416a021545 __libc_start_main
(pid=1144761, ip=192.168.1.180     @     0x55798438a990 (unknown)
(pid=raylet, ip=192.168.1.180 F0603 04:18:49.453656 1143638 1143638 worker.cc:68]  Check failed: port_ &gt; 0
(pid=raylet, ip=192.168.1.180 *** Check failure stack trace: ***
(pid=raylet, ip=192.168.1.180     @     0x55ea8191581d  google::LogMessage::Fail()
(pid=raylet, ip=192.168.1.180     @     0x55ea81916c8c  google::LogMessage::SendToLog()
(pid=raylet, ip=192.168.1.180     @     0x55ea819154f9  google::LogMessage::Flush()
(pid=raylet, ip=192.168.1.180     @     0x55ea81915711  google::LogMessage::~LogMessage()
(pid=raylet, ip=192.168.1.180     @     0x55ea815cfd39  ray::RayLog::~RayLog()
(pid=raylet, ip=192.168.1.180     @     0x55ea8132bad1  ray::raylet::Worker::Port()
(pid=raylet, ip=192.168.1.180     @     0x55ea81374562  ray::raylet::NodeManager::ProcessDisconnectClientMessage()
(pid=raylet, ip=192.168.1.180     @     0x55ea81376dfd  ray::raylet::NodeManager::ProcessClientMessage()
(pid=raylet, ip=192.168.1.180     @     0x55ea812fb73d  ZNSt17_Function_handlerIFvSt10shared_ptrIN3ray16ClientConnectionEElPKhEZNS1_6raylet6Raylet12HandleAcceptERKN5boost6system10error_codeEEUlS3_lS5_E0_E9_M_invokeERKSt9_Any_dataS3_lS5
(pid=raylet, ip=192.168.1.180     @     0x55ea81591e52  ray::ClientConnection::ProcessMessage()
(pid=raylet, ip=192.168.1.180     @     0x55ea815955ee  ray::ClientConnection::ProcessMessageHeader()
(pid=raylet, ip=192.168.1.180     @     0x55ea8158ffd2  boost::asio::detail::read_op&lt;&gt;::operator()()
(pid=raylet, ip=192.168.1.180     @     0x55ea8159152a  boost::asio::detail::reactive_socket_recv_op&lt;&gt;::do_complete()
(pid=raylet, ip=192.168.1.180     @     0x55ea818a743f  boost::asio::detail::scheduler::do_run_one()
(pid=raylet, ip=192.168.1.180     @     0x55ea818a8941  boost::asio::detail::scheduler::run()
(pid=raylet, ip=192.168.1.180     @     0x55ea818a9972  boost::asio::io_context::run()
(pid=raylet, ip=192.168.1.180     @     0x55ea812dda51  main
(pid=raylet, ip=192.168.1.180     @     0x7fadff67e545  __libc_start_main
(pid=raylet, ip=192.168.1.180     @     0x55ea812eea71  (unknown)
(pid=1143672, ip=192.168.1.180 E0603 04:18:49.649658 1143672 1143775 core_worker.cc:647] Raylet failed. Shutting down.
...

		</comment>
		<comment id='21' author='virtualluke' date='2020-06-03T06:27:34Z'>
		I will find an assignee for this task. Thanks again for reporting this issue. Would you mind trying running this with 0.8.5 and see if it has the same issue? I'd like to figure out if it was an existing issue or is introduced lately.
		</comment>
		<comment id='22' author='virtualluke' date='2020-06-03T15:36:38Z'>
		Just tried 0.8.5 and don't see this issue.  I do notice a significant performance hit from 0.8.0 -&gt; 0.8.5, somewhere in the vicinity of 15-20% slower.  Guess that is a different issue.
		</comment>
		<comment id='23' author='virtualluke' date='2020-06-03T20:38:22Z'>
		related to &lt;denchmark-link:https://github.com/ray-project/ray/issues/8254&gt;#8254&lt;/denchmark-link&gt;
 ?
		</comment>
		<comment id='24' author='virtualluke' date='2020-06-03T23:22:25Z'>
		&lt;denchmark-link:https://github.com/virtualluke&gt;@virtualluke&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ray-project/ray/issues/8254&gt;#8254&lt;/denchmark-link&gt;
 is what &lt;denchmark-link:https://github.com/ijrsvt&gt;@ijrsvt&lt;/denchmark-link&gt;
 mentioned in the beginning of this thread. He found another part that could potentially have a fix for this problem. We will merge this ASAP.
		</comment>
		<comment id='25' author='virtualluke' date='2020-06-03T23:23:01Z'>
		&lt;denchmark-link:https://github.com/virtualluke&gt;@virtualluke&lt;/denchmark-link&gt;
 I think it is a similar issue--but in a different part of the Ray code.
		</comment>
		<comment id='26' author='virtualluke' date='2020-06-03T23:39:06Z'>
		thanks for looking into this.
		</comment>
		<comment id='27' author='virtualluke' date='2020-06-04T00:03:35Z'>
		Of course! We will update you as soon as it is merged so that you can try it!
		</comment>
		<comment id='28' author='virtualluke' date='2020-06-04T21:51:48Z'>
		&lt;denchmark-link:https://github.com/virtualluke&gt;@virtualluke&lt;/denchmark-link&gt;
 Can you try this wheel:
&lt;denchmark-link:https://custom-wheels.s3-us-west-2.amazonaws.com/ray-0.9.0.dev0-cp37-cp37m-manylinux1_x86_64.whl&gt;https://custom-wheels.s3-us-west-2.amazonaws.com/ray-0.9.0.dev0-cp37-cp37m-manylinux1_x86_64.whl&lt;/denchmark-link&gt;

		</comment>
		<comment id='29' author='virtualluke' date='2020-06-05T03:07:14Z'>
		trying the wheel now
		</comment>
		<comment id='30' author='virtualluke' date='2020-06-05T03:42:23Z'>
		Same result.  3rd run (after 2 successful runs) gave segfault on "Address already in use".
		</comment>
		<comment id='31' author='virtualluke' date='2020-06-05T07:22:57Z'>
		&lt;denchmark-link:https://github.com/virtualluke&gt;@virtualluke&lt;/denchmark-link&gt;
 Sorry to hear that... I was a bit surprised that the proposed fix didn't resolve your issue. Would you mind trying to initialize Ray with port ranges (&lt;denchmark-link:https://docs.ray.io/en/master/package-ref.html#cmdoption-ray-start-min-worker-port&gt;https://docs.ray.io/en/master/package-ref.html#cmdoption-ray-start-min-worker-port&lt;/denchmark-link&gt;
) that you are confident it will absolutely not be used by other processes? I'd like you to try so that I want to know if the issue comes from our code or race condition from the environment.
		</comment>
		<comment id='32' author='virtualluke' date='2020-06-05T14:08:56Z'>
		Nothing else is running on this cluster.  I can try the port ranges for sure but given the data below not sure what to set them to.  Here are some interesting data points that seemed surprising to me.
When I do a fresh ray start of the cluster (after a ray stop) and go to the head node (before any driver program is run).
$ netstat -anp | egrep 'redis|ray|gcs_server|plasma' |wc -l
15441
while running my python job on the head node (takes around 6-7 minutes) the first time:
$ netstat -anp | egrep 'redis|ray|gcs_server|plasma' |wc -l
30586
program finished, nothing else running:
$netstat -anp | egrep 'redis|ray|gcs_server|plasma' | wc -l
30507
run program again (2nd time),  now showing 39384 connections after the program is done (while it was running I saw total go to 39463)
running program 3rd time, during run total went to 46212, after it is at 46140
This continues and on about the 6th run all ports are used up and ray segfaults.  I am not sure why this particular run it took 6 tries where it has been happening on the 3rd or 4th run, same error though.
		</comment>
		<comment id='33' author='virtualluke' date='2020-06-05T17:04:58Z'>
		&lt;denchmark-link:https://github.com/virtualluke&gt;@virtualluke&lt;/denchmark-link&gt;
 Hmm. I'm trying a different build with a slightly different check--I am  confident that this build will work (or at least give a better warning).
&lt;denchmark-link:https://custom-wheels.s3-us-west-2.amazonaws.com/ray-0.9.0.dev01-cp37-cp37m-linux_x86_64.whl&gt;https://custom-wheels.s3-us-west-2.amazonaws.com/ray-0.9.0.dev01-cp37-cp37m-linux_x86_64.whl&lt;/denchmark-link&gt;

Also, can you try running

as well as

during execution?
		</comment>
		<comment id='34' author='virtualluke' date='2020-06-05T18:05:48Z'>
		ok grabbing this now, will run some tests soon
		</comment>
		<comment id='35' author='virtualluke' date='2020-06-05T19:21:02Z'>
		at cluster start before running any jobs:
netstat -anpt4 | egrep 'redis|ray|gcs_server|plasma' |wc -l  -&gt; 14291
netstat -anpt4 | egrep 'ray::' |wc -l -&gt; 1176
during run 1
netstat -anpt4 | egrep 'redis|ray|gcs_server|plasma' |wc -l  -&gt; 14906
netstat -anpt4 | egrep 'ray::' |wc -l -&gt; 1556
after run 1:
netstat -anpt4 | egrep 'redis|ray|gcs_server|plasma' |wc -l  -&gt; 14882
netstat -anpt4 | egrep 'ray::' |wc -l -&gt; 1556
during run 2:
netstat -anpt4 | egrep 'redis|ray|gcs_server|plasma' |wc -l  -&gt; 15248
netstat -anpt4 | egrep 'ray::' |wc -l -&gt; 1792
after run 2:
netstat -anpt4 | egrep 'redis|ray|gcs_server|plasma' |wc -l  -&gt; 15224
netstat -anpt4 | egrep 'ray::' |wc -l -&gt; 1792
during run 3:
netstat -anpt4 | egrep 'redis|ray|gcs_server|plasma' |wc -l  -&gt; 15438
netstat -anpt4 | egrep 'ray::' |wc -l -&gt; 1982
after run 3:
netstat -anpt4 | egrep 'redis|ray|gcs_server|plasma' |wc -l  -&gt; 15414
netstat -anpt4 | egrep 'ray::' |wc -l -&gt; 1982
during run 4:
netstat -anpt4 | egrep 'redis|ray|gcs_server|plasma' |wc -l  -&gt; 15712
netstat -anpt4 | egrep 'ray::' |wc -l -&gt; 2128
after run 4:
netstat -anpt4 | egrep 'redis|ray|gcs_server|plasma' |wc -l  -&gt; 15688
netstat -anpt4 | egrep 'ray::' |wc -l -&gt; 2128
during run 5:
netstat -anpt4 | egrep 'redis|ray|gcs_server|plasma' |wc -l  -&gt; 15963
netstat -anpt4 | egrep 'ray::' |wc -l -&gt; 2231
after run 5:
netstat -anpt4 | egrep 'redis|ray|gcs_server|plasma' |wc -l  -&gt; 15939
netstat -anpt4 | egrep 'ray::' |wc -l -&gt; 2231
--
I will continue to run this to see what happens over time but looks very promising, no crashes yet.  Will let you know in a an hour after another 5 runs are complete
		</comment>
		<comment id='36' author='virtualluke' date='2020-06-05T19:21:53Z'>
		&lt;denchmark-link:https://github.com/virtualluke&gt;@virtualluke&lt;/denchmark-link&gt;
  Thanks for the quick response! Glad to see that it is making it further :)
		</comment>
		<comment id='37' author='virtualluke' date='2020-06-05T20:31:41Z'>
		during run 6:
netstat -anpt4 | egrep 'redis|ray|gcs_server|plasma' |wc -l -&gt; 16214
netstat -anpt4 | egrep 'ray::' |wc -l -&gt; 2231
after run 6:
netstat -anpt4 | egrep 'redis|ray|gcs_server|plasma' |wc -l -&gt; 16190
netstat -anpt4 | egrep 'ray::' |wc -l -&gt; 2397
during run 7:
netstat -anpt4 | egrep 'redis|ray|gcs_server|plasma' |wc -l -&gt; 16288
netstat -anpt4 | egrep 'ray::' |wc -l -&gt; 2407
after run 7:
netstat -anpt4 | egrep 'redis|ray|gcs_server|plasma' |wc -l -&gt; 16264
netstat -anpt4 | egrep 'ray::' |wc -l -&gt; 2407
during run 8:
netstat -anpt4 | egrep 'redis|ray|gcs_server|plasma' |wc -l -&gt; 16479
netstat -anpt4 | egrep 'ray::' |wc -l -&gt; 2449
after run 8:
netstat -anpt4 | egrep 'redis|ray|gcs_server|plasma' |wc -l -&gt; 16455
netstat -anpt4 | egrep 'ray::' |wc -l -&gt; 2449
during run 9:
netstat -anpt4 | egrep 'redis|ray|gcs_server|plasma' |wc -l -&gt; 16486
netstat -anpt4 | egrep 'ray::' |wc -l -&gt; 2456
after run 9:
netstat -anpt4 | egrep 'redis|ray|gcs_server|plasma' |wc -l -&gt; 16462
netstat -anpt4 | egrep 'ray::' |wc -l -&gt; 2456
during run 10:
netstat -anpt4 | egrep 'redis|ray|gcs_server|plasma' |wc -l -&gt; 16495
netstat -anpt4 | egrep 'ray::' |wc -l -&gt; 2465
after run 10:
netstat -anpt4 | egrep 'redis|ray|gcs_server|plasma' |wc -l -&gt; 16471
netstat -anpt4 | egrep 'ray::' |wc -l -&gt; 2465
--
definitely a lot better, haven't got this far before.  Do you think this will continue to creep up until there will be a port problem?  At this rate it will take a long time.  Is this part of one of the PRs ?
		</comment>
		<comment id='38' author='virtualluke' date='2020-06-05T21:20:19Z'>
		Hmm, the PRs don't immediately fix the port problem (the climbing number of ports). Could you possibly capture and upload the results of the commands from before and after a run? If that's not possible--no worries
		</comment>
		<comment id='39' author='virtualluke' date='2020-06-08T12:27:49Z'>
		Did you mean for more runs or something else?  I would guess the current state works fine until the ports run out after a few hundred runs.
		</comment>
		<comment id='40' author='virtualluke' date='2020-06-08T18:35:07Z'>
		Are you running hundreds of runs? Or only a handful?
I was just hoping to look at something like:
Before run x
During run x
After run x
Where x can be any run number
		</comment>
		<comment id='41' author='virtualluke' date='2020-06-08T18:43:27Z'>
		I can script doing more for testing but the jobs in question take ~7 minutes to complete and usually run ~dozen times a day.  So hundreds will take a while.
Also, After run x = before run x+1 for all I have seen which is why I hadn't included that but should have mentioned.
I will try and figure out a short-circuited version of the job in question so it finishes quicker but still exhibits the problem.
		</comment>
		<comment id='42' author='virtualluke' date='2020-06-08T19:13:03Z'>
		Ahh okay--that makes sense with respect for after run x== before run x+1.
That would be great if you could provide us with some more context!
		</comment>
		<comment id='43' author='virtualluke' date='2020-06-11T21:47:28Z'>
		Close as the PR is merged
		</comment>
	</comments>
</bug>