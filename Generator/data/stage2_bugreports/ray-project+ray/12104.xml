<bug id='12104' author='richardliaw' open_date='2020-11-18T09:03:20Z' closed_time='2020-12-29T02:56:29Z'>
	<summary>[autoscaler] [docker] ray up on stopped node fails?</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

I ran ray up on a cluster that I had previously stopped.
The ray up was unable to finish properly due to a tmp call.
cc &lt;denchmark-link:https://github.com/ijrsvt&gt;@ijrsvt&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;(base) ➜  aws git:(docker-fix) ✗ ray up example-full.yaml -y
Cluster: default

Loaded cached provider configuration
If you experience issues with the cloud provider, try re-running the command with --no-config-cache.
AWS config
  IAM Profile: ray-autoscaler-v1 [default]
  EC2 Key pair (head &amp; workers): ray-autoscaler_2_us-west-2 [default]
  VPC Subnets (head &amp; workers): subnet-d26e76b6, subnet-503c0e26 [default]
  EC2 Security groups (head &amp; workers): sg-0e2ad6debfc8e0814 [default]
  EC2 AMI (head &amp; workers): ami-0a2363a9cff180a64

No head node found. Launching a new cluster. Confirm [y/N]: y [automatic, due to --yes]

Acquiring an up-to-date head node
  Reusing nodes i-0c90dd53359e25223. To disable reuse, set `cache_stopped_nodes: False` under `provider` in the cluster configuration.
  Stopping instances to reuse
  Launched a new head node
  Fetching the new head node

&lt;1/1&gt; Setting up head node
  Prepared bootstrap config
  New status: waiting-for-ssh
  [1/6] Waiting for SSH to become available
    Running `uptime` as a test.
    Waiting for IP
      Not yet available, retrying in 10 seconds
      Received: 35.165.135.138
ssh: connect to host 35.165.135.138 port 22: Connection refused
    SSH still not available SSH command failed., retrying in 5 seconds.
Warning: Permanently added '35.165.135.138' (ECDSA) to the list of known hosts.
 09:00:19 up 0 min,  1 user,  load average: 0.35, 0.08, 0.03
Shared connection to 35.165.135.138 closed.
    Success.
Shared connection to 35.165.135.138 closed.
latest-gpu: Pulling from rayproject/ray
Digest: sha256:9e330168fbeface86427d29b4a1a996bdefca42fff38ff155a29c8d1d1020b74
Status: Image is up to date for rayproject/ray:latest-gpu
docker.io/rayproject/ray:latest-gpu
Shared connection to 35.165.135.138 closed.
Shared connection to 35.165.135.138 closed.
Shared connection to 35.165.135.138 closed.
Shared connection to 35.165.135.138 closed.
NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.

Shared connection to 35.165.135.138 closed.
2020-11-18 01:00:24,949	WARNING command_runner.py:780 -- Nvidia Container Runtime is present, but no GPUs found.
Shared connection to 35.165.135.138 closed.
9191520da0ddb9d6259795e5a758a298934bdab25ee153e60ddcd350318ed789
Shared connection to 35.165.135.138 closed.
Shared connection to 35.165.135.138 closed.
lstat /tmp/ray_tmp_mount: no such file or directory
Shared connection to 35.165.135.138 closed.
  New status: update-failed
  !!!
  SSH command failed.
  !!!

  Failed to setup head node.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='richardliaw' date='2020-11-18T09:21:01Z'>
		&lt;denchmark-link:https://github.com/richardliaw&gt;@richardliaw&lt;/denchmark-link&gt;
 can you rerun with something like 
		</comment>
		<comment id='2' author='richardliaw' date='2020-11-18T11:25:25Z'>
		I get this error too &lt;denchmark-link:https://github.com/ijrsvt&gt;@ijrsvt&lt;/denchmark-link&gt;
 .
		</comment>
		<comment id='3' author='richardliaw' date='2020-11-20T23:58:09Z'>
		&lt;denchmark-link:https://github.com/richardliaw&gt;@richardliaw&lt;/denchmark-link&gt;
 Follow up question, did you happen to edit your YAML between shutting down and restarting?
		</comment>
		<comment id='4' author='richardliaw' date='2020-11-21T00:11:39Z'>
		hmmm maybe i modified the filemounts
		</comment>
		<comment id='5' author='richardliaw' date='2020-11-23T18:37:18Z'>
		Same thing is happening for me. I did modify the setup_commands on the yaml between shutting down and restarting.
		</comment>
		<comment id='6' author='richardliaw' date='2020-12-07T19:09:03Z'>
		+1
		</comment>
		<comment id='7' author='richardliaw' date='2020-12-14T19:30:51Z'>
		&lt;denchmark-link:https://github.com/AmeerHajAli&gt;@AmeerHajAli&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ijrsvt&gt;@ijrsvt&lt;/denchmark-link&gt;
 this sounds like a potential P0 issue, should we raise the priority?
		</comment>
		<comment id='8' author='richardliaw' date='2020-12-14T19:41:45Z'>
		I'm having a related issue with DockerSyncer on sync_down:

invalid output path: directory "/tmp/ray_tmp_mount/...

Interestingly earlier in the logs I see the directory being created:

VINFO command_runner.py:474 -- Running `^[[1mmkdir -p /tmp/ray_tmp_mount/...

Unfortunately I cannot create an issue as my code is proprietary and I likely won't have time to build a minimal example, but the issues are related at least with respect to the directory in question. I'm looking over the framework code now.
		</comment>
		<comment id='9' author='richardliaw' date='2020-12-14T19:53:56Z'>
		&lt;denchmark-link:https://github.com/AmeerHajAli&gt;@AmeerHajAli&lt;/denchmark-link&gt;
 can you please take a look at this ASAP? Not sure if it should be release blocking.
		</comment>
		<comment id='10' author='richardliaw' date='2020-12-15T12:22:16Z'>
		I think this is a release blocker.
Basically, it is straightforward to reproduce:

ray up
ray down
ray up -&gt; fails because it can't locate /tmp/ray_tmp_mount
@ijrsvt , I think the command runner is looking for the /tmp/ray_tmp_mount and fails because it assumes it is there in the cached node. But it seems like AWS cleans the /tmp directory on terminated nodes.

		</comment>
		<comment id='11' author='richardliaw' date='2020-12-15T23:19:35Z'>
		&lt;denchmark-link:https://github.com/AmeerHajAli&gt;@AmeerHajAli&lt;/denchmark-link&gt;
 This is not a new regression and I think that rushing this fix as a cherry pick is more risky than waiting for this fix to go through nightlies.
		</comment>
		<comment id='12' author='richardliaw' date='2020-12-18T18:25:33Z'>
		&lt;denchmark-link:https://github.com/ijrsvt&gt;@ijrsvt&lt;/denchmark-link&gt;
 , I think it is very common for someone to run  after a . I understand it is not a regression. But I am not sure what is the workaround here? The user will have to change the cluster name or work very hard to make it ray up again. If this is not a good reason for blocking a release then I am fine with it, but at least we should have some documentation somewhere telling the user what to do when he faces this issue.
		</comment>
		<comment id='13' author='richardliaw' date='2020-12-18T20:19:11Z'>
		Hey, just to help clarify, I think there's two conversations happening -- a conversation around deadline and conversation around importance.
&lt;denchmark-link:https://github.com/AmeerHajAli&gt;@AmeerHajAli&lt;/denchmark-link&gt;
 I think Ian and me and you absolutely agree that it is important to provide a workaround.
I think Ian was just pushing back on the "prioritization/deadline" for this task. In terms of prioritization, I think it's ok to do it within the time frame of this sprint or even the sprint after (since everyone is going on holidays anyways).
		</comment>
		<comment id='14' author='richardliaw' date='2020-12-18T20:55:29Z'>
		I definitely agree that this is important, I think a workaround should be to set the following in provider:
&lt;denchmark-code&gt;cache_stopped_nodes: False
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>