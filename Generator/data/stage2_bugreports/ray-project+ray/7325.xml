<bug id='7325' author='soundway' open_date='2020-02-25T23:06:39Z' closed_time='2020-02-27T02:44:35Z'>
	<summary>[rllib] Strange object store behavior for asynchronous training with a driver task.</summary>
	<description>
This is not a contribution.
Ray version: 0.8.1
Python version: 3.6.8
Tensorflow version: 1.15
OS: Ubuntu 18.04 Docker
Since 0.8.0, we’ve been observing strange memory issues for asynchronous training (e.g. IMPALA) when we run the driver task ourselves instead of through Tune. After hours of training, we’ll start seeing messages below every iteration:
WARNING worker.py:1063 -- The task with ID ffffffffffffffffffffffff0100 is a driver task and so the object created by ray.put could not be reconstructed.
These messages will appear increasingly more frequently. At the same time, there’s a sharp increase in memory usage of all the machines, until some worker runs out of memory.
We run everything in AWS with our own cluster management. We launch Ray by running ray start first on each instance (the trainer instance is the head), then running the script directly on the head node.
One of the setups that could reproduce this issue relatively quickly and reliably is as follows (usually less than 2 hours):

1 GPU p3.8xlarge instance.

The Redis server is also hosted here.
When running without Tune, we set the CPU+GPU available on this node to 0 so that all rollouts workers gets allocated on the worker machines, though this makes no difference.
We only use a single GPU for training (i.e. we aren’t using the TFMultiGPULearner). We picked p3.8xlarge mainly to ensure that we are not bottlenecked by memory.
30% of memory is allocated for object store.


4 CPU r5.12xlarge instance, 384GB RAM each.

30% of memory is allocated for object store.



We’ve tried setting RAY_FORCE_DIRECT=0, but it didn’t seem to make a difference other than that it might fail earlier.
The code to reproduce the behavior mentioned is below. In this case, we are training on Q*Bert with IMPALA.
import ray
import ray.rllib.agents.impala as impala
from ray.rllib.agents.trainer import with_base_config
from ray.tune.logger import pretty_print

if __name__ == '__main__':
    ray.init(address="auto")

    config = with_base_config(impala.DEFAULT_CONFIG, {
        "sample_batch_size": 20,
        "train_batch_size": 640,
        "num_workers": 256,
        "num_envs_per_worker": 4,
        "num_cpus_per_worker": 0.5,
        "num_gpus_per_worker": 0,
        "num_cpus_for_driver": 32,
        "num_gpus": 1,
        "broadcast_interval": 1,
        "max_sample_requests_in_flight_per_worker": 2,
        "min_iter_time_s": 10,
        "clip_rewards": True,
        "opt_type": "adam",
        "lr": 0.0005,
        "model": {"fcnet_hiddens": [2048, 2048]},
    })

    use_tune = False

    if use_tune:
        config["env"] = "QbertNoFrameskip-v4"
        ray.tune.run("IMPALA", stop={"episode_reward_mean": 30000}, config=config)
    else:
        trainer = impala.ImpalaTrainer(config=config, env="QbertNoFrameskip-v4")
        for i in range(40000):
            result = trainer.train()
            print(pretty_print(result))
The memory usage is shown below. The red curve is for the GPU instance, while all other colors are for the CPU workers. Note the sharp sudden increase in memory consumption at around 19:03.
&lt;denchmark-link:https://user-images.githubusercontent.com/61214581/75294913-6a89e680-57de-11ea-8dcc-80762fb65f38.png&gt;&lt;/denchmark-link&gt;

We also observe very strange network traffic behavior at the same time. The graph below shows the inbound traffic of each machine. There’s a sudden jump for one of the CPU worker machines.
&lt;denchmark-link:https://user-images.githubusercontent.com/61214581/75294939-74abe500-57de-11ea-854a-074224fb1b3e.png&gt;&lt;/denchmark-link&gt;

Correspondingly, for outbound traffic, we also observe strange behavior like below. It doesn’t make much sense that 3 of the other CPU workers are sending traffic to the green one.
&lt;denchmark-link:https://user-images.githubusercontent.com/61214581/75294986-9907c180-57de-11ea-9d67-d4aef7232ef7.png&gt;&lt;/denchmark-link&gt;

We’ve also tried running similar setup on larger instances with more memory available or adjusting the memory store sizes or the number of instances/workers while always assuring an abundance of resource is available, but eventually they’d all crash in similar fashion.
Note that these problems don’t seem to occur under the following conditions

Using Ray 0.7.3 (though we haven’t investigated exactly which commit broke this in between now and then)
For 0.8.0 onward, if we were to use Tune to do training with the same setup (i.e. setting use_tune in the script provide to True, and open up the resource on the GPU machine for scheduling the driver task), then at least so far we haven’t been able to reproduce these issues at all. Unfortunately, for a variety of reasons, we are unable to use Tune for our own task and we have to do the training with a driver task.
It doesn't seem to occur in synchronous training jobs.

It’d be great if we can get some insights on what might be causing these issues.
	</description>
	<comments>
		<comment id='1' author='soundway' date='2020-02-26T01:00:32Z'>
		This sounds similar to &lt;denchmark-link:https://github.com/ray-project/ray/issues/7154&gt;#7154&lt;/denchmark-link&gt;
, perhaps try 0.8.2 to see if it's fixed there?
		</comment>
		<comment id='2' author='soundway' date='2020-02-27T02:44:35Z'>
		It looks like it's fixed with the latest release. Thanks!
		</comment>
	</comments>
</bug>