<bug id='8411' author='jsuarez5341' open_date='2020-05-12T06:27:19Z' closed_time='2020-05-20T20:29:08Z'>
	<summary>[rllib] Apparently broken compute_actio function for multiagent environments</summary>
	<description>
Problem: Internal error thrown when calling compute_action in a multiagent policy
Details: Python 3.7 with the latest Ray on Ubuntu using a PyTorch policy
Solution: Remove the cast to list (e.g. [action] -&gt; action) on line 161 of ray-project/ray/blob/master/rllib/policy/policy.py
I assume the cast is required for single agent policies, so possibly just return action and cast to a list if the result is not iterable.
	</description>
	<comments>
		<comment id='1' author='jsuarez5341' date='2020-05-12T08:12:17Z'>
		Hey &lt;denchmark-link:https://github.com/jsuarez5341&gt;@jsuarez5341&lt;/denchmark-link&gt;
 not sure I understand the issue.
Which method call actually fails?

trainer.compute_action
policy.compute_single_action
policy.compute_actions
Could you post a small repro script?

		</comment>
		<comment id='2' author='jsuarez5341' date='2020-05-12T08:24:56Z'>
		We can't remove the [] around the action return, because it would break returning a single action (when passing in a batch-of-1 to the policy.compute_actions call).
I think the reason is somewhere else. Maybe the new nested action space support broke something.
		</comment>
		<comment id='3' author='jsuarez5341' date='2020-05-12T23:00:02Z'>
		Hmm is it that we can't desugar an array into a list? So we should be calling return_value[0] instead?
		</comment>
		<comment id='4' author='jsuarez5341' date='2020-05-20T09:58:05Z'>
		Got it. Policy.compute_single_action() Has to be:
&lt;denchmark-code&gt;        batched_action, state_out, info = self.compute_actions(
            [obs],
            state_batch,
            prev_action_batch=prev_action_batch,
            prev_reward_batch=prev_reward_batch,
            info_batch=info_batch,
            episodes=episodes,
            explore=explore,
            timestep=timestep)

        single_action = unbatch_actions(batched_action)
        assert len(single_action) == 1
        single_action = single_action[0]

        if clip_actions:
            single_action = clip_action(
                single_action, self.action_space_struct)

        # Return action, internal state(s), infos.
        return single_action, [s[0] for s in state_out], \
            {k: v[0] for k, v in info.items()}
&lt;/denchmark-code&gt;

In case of a complex nested action, the action comes back as batched (nested struct of different batches) and hence has to be converted into a single nested action.
Will PR now. ...
		</comment>
		<comment id='5' author='jsuarez5341' date='2020-05-20T10:31:14Z'>
		&lt;denchmark-link:https://github.com/jsuarez5341&gt;@jsuarez5341&lt;/denchmark-link&gt;
 Could you check this PR and let me know, whether this fixes the issue?
&lt;denchmark-link:https://github.com/ray-project/ray/pull/8514&gt;#8514&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>