<bug id='13396' author='sven1977' open_date='2021-01-13T09:59:32Z' closed_time='2021-01-13T10:18:25Z'>
	<summary>[RLlib] config.vf_share_layers should not silently overwrite `config.model.vf_share_layers` for PPO.</summary>
	<description>
Andrew R. raised this on discuss.ray.io:
&lt;denchmark-code&gt;I’ve noticed that several of the RLlib algorithms have a top-level config parameter named vf_share_layers.

However, vf_share_layers is also present in the nested model config.

Is this a bug? Is this intentional? Which to use?
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

Ray version and other system information (Python version, TensorFlow version, OS):
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a short code snippet (less than 50 lines if possible) that can be copy-pasted to reproduce the issue. The snippet should have no external library dependencies (i.e., use fake or mock data / environments):
If the code snippet cannot be run by itself, the issue will be closed with "needs-repro-script".

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='sven1977' date='2021-01-13T09:59:59Z'>
		There is a function in ppo_tf_policy.py, which overrides the model config with the top-level one, so the answer is to always use the top-level config key: vf_share_layers to set this, if available:
&lt;denchmark-code&gt;def setup_config(...):
   ...
   # Auto set the model option for VF layer sharing.
   config["model"]["vf_share_layers"] = config["vf_share_layers"]
&lt;/denchmark-code&gt;

We’ll fix this inconsistency and soft-deprecate the top-level config key. That way, if users correctly set the model’s config vf_share_layers, they don’t get bad surprises (b/c it’s silently overwritten by the Trainer’s value).
		</comment>
		<comment id='2' author='sven1977' date='2021-01-13T10:00:20Z'>
		So the workaround for now is:
Only use vf_share_layers in the top-level config for PPO, MAML, and MB-MPO, but not for any other algo.
For PPO, MAML, and MB-MPO, you must use the top-level key as the model config key will always be silently overwritten.
		</comment>
		<comment id='3' author='sven1977' date='2021-01-13T10:18:25Z'>
		This PR closes this issue:
&lt;denchmark-link:https://github.com/ray-project/ray/pull/13397&gt;#13397&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>