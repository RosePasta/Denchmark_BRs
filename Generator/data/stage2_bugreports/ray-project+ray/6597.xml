<bug id='6597' author='ProNoobLi' open_date='2019-12-25T06:26:11Z' closed_time='2019-12-27T07:01:33Z'>
	<summary>[tune]Failed to tf.train.Checkpoint inside of the Trainable.</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

Error shows that: 'checkpoint' was expecting a trackable object(an object derived from 'TrackableBase'), got 1.    When I tried to save checkpoint inside the Trainable class.
Does the problem occur on the latest wheels?
Ray 0.8.0.dev
&lt;denchmark-h:h3&gt;Reproduction&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
If we cannot run your script, we cannot fix your issue.
class Trainable(tune.Trainable):
    def _setup(self, config):
        # IMPORTANT: See the above note.
        import tensorflow as tf

        ##----dataset config----##
        # datasets, info = get_and_split_dataset(DATASET_NAME)
        # print(info)
        # self.train_ds, self.val_ds, self.test_ds = preprocess_data((datasets, info))
        # self.num_classes = info.features['label'].num_classes

        tfrecords_dir = "/data121/lijiayuan/test/classify_flowers/datasets/"
        self.train_ds, self.num_classes = get_dataset(tfrecords_dir, subset="train", batch_size=BATCH_SIZE)
        self.test_ds, _ = get_dataset(tfrecords_dir, subset="validation", batch_size=BATCH_SIZE)
        print("classes_num", self.num_classes)

        ##----model config----##
        self.model = tf.keras.applications.MobileNetV2( weights='imagenet')
        self.model = assemble_model(self.num_classes, model_name='MobileNetV2')     
        # self.model = MyModel(self.num_classes)

        ##----training config----##
        self.optimizer = configure_optimizer(config, NOTUNING_CONFIG)
        self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy()
        self.train_loss = tf.keras.metrics.Mean(name="train_loss")
        self.train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(
            name="train_accuracy")

        self.val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(
            name="val_accuracy")
        self.test_loss = tf.keras.metrics.Mean(name="test_loss")
        self.test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(
            name="test_accuracy")

        ##----checkpoint config----##
        ckpt = tf.train.Checkpoint(step=1, optimizer=self.optimizer, net=self.model)
        manager = tf.train.CheckpointManager(ckpt, './tf_ckpts', max_to_keep=3)

        @tf.function
        def train_step(images, labels):
            with tf.GradientTape() as tape:
                predictions = self.model(images)
                loss = self.loss_object(labels, predictions)
            gradients = tape.gradient(loss, self.model.trainable_variables)
            self.optimizer.apply_gradients(
                zip(gradients, self.model.trainable_variables))
            self.train_loss(loss)
            self.train_accuracy(labels, predictions)

        @tf.function 
        def val_step(images, labels):
            predictions = self.model(images)
            self.val_accuracy(labels, predictions)

        @tf.function
        def test_step(images, labels):
            predictions = self.model(images)
            # labels = tf.expand_dims(labels, -1)
            # t_loss = self.loss_object(labels, predictions)
            # self.test_loss(t_loss)
            self.test_accuracy(labels, predictions)

        self.tf_train_step = train_step
        # self.tf_val_step = val_step
        self.tf_test_step = test_step

    def _train(self):
        self.train_loss.reset_states()
        self.train_accuracy.reset_states()
        # self.test_loss.reset_states()
        self.test_accuracy.reset_states()
        self.val_accuracy.reset_states()

        for idx, (images, labels) in enumerate(self.train_ds):
            self.tf_train_step(images, labels)

        # for val_images, val_labels in self.val_ds:
        #     self.tf_val_step(val_images, val_labels)

        for test_images, test_labels in self.test_ds:
            self.tf_test_step(test_images, test_labels)


        # It is important to return tf.Tensors as numpy objects.
        print({
            "epoch": self.iteration,
            "train_loss": self.train_loss.result().numpy(),
            "train_accuracy": self.train_accuracy.result().numpy() * 100,
            # "val_accuracy": self.val_accuracy.result().numpy() * 100,
            "mean_accuracy": self.test_accuracy.result().numpy() * 100
        })
        return {
            "epoch": self.iteration,
            "train_loss": self.train_loss.result().numpy(),
            "accuracy": self.train_accuracy.result().numpy() * 100,
            # "val_accuracy": self.val_accuracy.result().numpy() * 100,
            "mean_accuracy": self.test_accuracy.result().numpy() * 100
        }

    def _save(self, temp_ckpt_dir):
        import tensorflow as tf
        print("temp_ckpt_dir", temp_ckpt_dir)
        ckpt_dir = os.path.join(temp_ckpt_dir)
        self.model.save(ckpt_dir, save_format='tf')
        # tf.saved_model.save(self.model, ckpt_dir)
        return temp_ckpt_dir

    def _restore(self):
        return None
	</description>
	<comments>
		<comment id='1' author='ProNoobLi' date='2019-12-25T09:49:35Z'>
		Note that I can't run this script because of missing dependencies.
How are you getting this exactly to fail? Does it simply fail at self.model.save(ckpt_dir, save_format='tf')?
		</comment>
		<comment id='2' author='ProNoobLi' date='2019-12-26T02:09:07Z'>
		
Note that I can't run this script because of missing dependencies.
How are you getting this exactly to fail? Does it simply fail at self.model.save(ckpt_dir, save_format='tf')?

Sorry about that...
My code has a customized dataset you can't run either. The bugs caused by the section:
 #----checkpoint config----##
        ckpt = tf.train.Checkpoint(step=1, optimizer=self.optimizer, net=self.model)
        manager = tf.train.CheckpointManager(ckpt, './tf_ckpts', max_to_keep=3)
&lt;denchmark-link:https://user-images.githubusercontent.com/33815430/71453707-b234c780-27c7-11ea-82fc-82b6d6371c49.png&gt;&lt;/denchmark-link&gt;

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from __future__ import unicode_literals

import argparse
import sys
import shutil
import numpy as np
from tensorflow.keras.layers import Dense, Flatten, Conv2D
from tensorflow.keras import Model
from tensorflow.keras.datasets.fashion_mnist import load_data
from ray.tune.schedulers import AsyncHyperBandScheduler
from ray.tune.suggest.bayesopt import BayesOptSearch
import tensorflow_datasets as tfds
import read_params
from train_config import configure_model, configure_optimizer, configure_lossfunc
from datasets.readtf_utils.dataset import get_dataset 


import os
import ray
from ray import tune

cur_dir = sys.argv[0]
cur_dir = cur_dir.split('/')
cur_dir.pop()


CUR_DIR = '/'.join(cur_dir)
SPLIT_WEIGHTS = (8,1,1)
SPLITS = tfds.Split.TRAIN.subsplit(weighted=SPLIT_WEIGHTS)
IMG_SIZE = 224
IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)
SHUFFLE_BUFFER_SIZE = 1000
BATCH_SIZE = 10
DATASET_NAME = 'tf_flowers'


class MyModel(Model):
    def __init__(self, num_classes, hiddens=128):
        super(MyModel, self).__init__()
        import tensorflow as tf 
        self.conv1 = Conv2D(32, 3, activation="relu")
        self.flatten = Flatten()
        self.d1 = Dense(hiddens, activation="relu")
        self.d2 = Dense(num_classes, activation="softmax")

    def call(self, x):
        x = self.conv1(x)
        x = self.flatten(x)
        x = self.d1(x)
        return self.d2(x)

def resize_normalize(image, label):
    import tensorflow as tf 
    image = tf.cast(image, tf.float32)
    if image.shape[-1] == 1:
        image = tf.concat([image, image, image], 2)
    image = tf.image.resize(image, method='mitchellcubic', size=(IMG_SIZE, IMG_SIZE))
    image = image / 255.0
    return image, label

def augment(image, label):
    import tensorflow as tf 
    image = tf.image.random_flip_left_right(image)
    image = tf.image.random_contrast(image, lower=0.0, upper=1.0)
    return image, label

def get_and_split_dataset(dataset_name):
    import tensorflow as tf 
    (raw_train, raw_val, raw_test), info = tfds.load(dataset_name,
                                                        split=list(SPLITS),
                                                        as_supervised=True,
                                                        with_info=True)

    return (raw_train, raw_val, raw_test), info

def get_dataset_stats(info):
    import tensorflow as tf 
    #---data info---#
    datasets_stats = {}
    train_num, val_num, test_num = (
      info.splits['train'].num_examples * weight/10 for weight in SPLIT_WEIGHTS
    )
    num_classes = info.features['label'].num_classes
    datasets_stats['train_num'] = train_num
    datasets_stats['val_num'] = val_num
    datasets_stats['test_num'] = test_num
    datasets_stats['classes_num'] = num_classes
    print(datasets_stats)
    return datasets_stats


def preprocess_data(datasets):
    import tensorflow as tf 
    (raw_train, raw_val, raw_test), info = datasets
    train = raw_train.map(resize_normalize)
    val = raw_val.map(resize_normalize)
    test = raw_test.map(resize_normalize)

    #---data augmentation---#
    # train = train.map(augment)

    #---data structure config----#
    shuffle_size = info.splits['train'].num_examples
    # train = train.shuffle(shuffle_size)
    train = train.batch(BATCH_SIZE)
    val = val.batch(BATCH_SIZE)
    test = test.batch(BATCH_SIZE)
    train = train.prefetch(tf.data.experimental.AUTOTUNE)
    return train, val, test


def assemble_model(num_classes, model_name='MobileNetV2'):
    import tensorflow as tf 
    base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                                    weights='imagenet',
                                                    include_top=True)
    model = tf.keras.Sequential([
                                base_model,
                                tf.keras.layers.Dense(500, activation='relu'),
                                tf.keras.layers.Dense(num_classes, activation='softmax')
                                ])
    return model




class Trainable(tune.Trainable):
    def _setup(self, config):
        # IMPORTANT: See the above note.
        import tensorflow as tf

        ##----dataset config----##
        # datasets, info = get_and_split_dataset(DATASET_NAME)
        # print(info)
        # self.train_ds, self.val_ds, self.test_ds = preprocess_data((datasets, info))
        # self.num_classes = info.features['label'].num_classes

        tfrecords_dir = "/data121/lijiayuan/test/classify_flowers/datasets/"
        self.train_ds, self.num_classes = get_dataset(tfrecords_dir, subset="train", batch_size=BATCH_SIZE)
        self.test_ds, _ = get_dataset(tfrecords_dir, subset="validation", batch_size=BATCH_SIZE)
        print("classes_num", self.num_classes)

        ##----model config----##
        # self.model = tf.keras.applications.MobileNetV2( weights='imagenet')
        # self.model = assemble_model(self.num_classes, model_name='MobileNetV2')     
        self.model = MyModel(self.num_classes)

        ##----training config----##
        self.optimizer = configure_optimizer(config, NOTUNING_CONFIG)
        self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy()
        self.train_loss = tf.keras.metrics.Mean(name="train_loss")
        self.train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(
            name="train_accuracy")

        self.val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(
            name="val_accuracy")
        self.test_loss = tf.keras.metrics.Mean(name="test_loss")
        self.test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(
            name="test_accuracy")

        #----checkpoint config----##
        ckpt = tf.train.Checkpoint(step=1, optimizer=self.optimizer, net=self.model)
        manager = tf.train.CheckpointManager(ckpt, './tf_ckpts', max_to_keep=3)

        @tf.function
        def train_step(images, labels):
            with tf.GradientTape() as tape:
                predictions = self.model(images)
                loss = self.loss_object(labels, predictions)
            gradients = tape.gradient(loss, self.model.trainable_variables)
            self.optimizer.apply_gradients(
                zip(gradients, self.model.trainable_variables))
            self.train_loss(loss)
            self.train_accuracy(labels, predictions)

        @tf.function 
        def val_step(images, labels):
            predictions = self.model(images)
            self.val_accuracy(labels, predictions)

        @tf.function
        def test_step(images, labels):
            predictions = self.model(images)
            # labels = tf.expand_dims(labels, -1)
            # t_loss = self.loss_object(labels, predictions)
            # self.test_loss(t_loss)
            self.test_accuracy(labels, predictions)

        self.tf_train_step = train_step
        # self.tf_val_step = val_step
        self.tf_test_step = test_step

    def _train(self):
        self.idx = 0
        self.train_loss.reset_states()
        self.train_accuracy.reset_states()
        # self.test_loss.reset_states()
        self.test_accuracy.reset_states()
        self.val_accuracy.reset_states()

        for idx, (images, labels) in enumerate(self.train_ds):
            self.idx = idx
            self.tf_train_step(images, labels)


        # for val_images, val_labels in self.val_ds:
        #     self.tf_val_step(val_images, val_labels)

        for test_images, test_labels in self.test_ds:
            self.tf_test_step(test_images, test_labels)


        # It is important to return tf.Tensors as numpy objects.
        print({
            "epoch": self.iteration,
            "train_loss": self.train_loss.result().numpy(),
            "train_accuracy": self.train_accuracy.result().numpy() * 100,
            # "val_accuracy": self.val_accuracy.result().numpy() * 100,
            "mean_accuracy": self.test_accuracy.result().numpy() * 100
        })
        return {
            "epoch": self.iteration,
            "train_loss": self.train_loss.result().numpy(),
            "accuracy": self.train_accuracy.result().numpy() * 100,
            # "val_accuracy": self.val_accuracy.result().numpy() * 100,
            "mean_accuracy": self.test_accuracy.result().numpy() * 100
        }

    # def _save(self, temp_ckpt_dir):
    #     import tensorflow as tf
    #     print("temp_ckpt_dir", temp_ckpt_dir)
    #     # self.model.save(ckpt_dir, save_format='tf')
    #     tf.saved_model.save(self.model, ckpt_dir)
    #     return temp_ckpt_dir

    # def _save(self, temp_ckpt_dir):        
    #     # Save JSON config to disk
    #     if self.idx == 0:
    #         return
    #     architecture_dir = os.path.join(temp_ckpt_dir, 'architecture.json')
    #     weights_dir = os.path.join(temp_ckpt_dir, 'weights.h5')
    #     json_config = self.model.to_json()
    #     with open(architecture_dir, 'w') as json_file:
    #         json_file.write(json_config)
    #     # Save weights to disk
    #     model.save_weights(weights_dir)

    def _restore(self):
        return None

def get_config(discrt_pm, cont_pm):
    config = {}
    for key, val in discrt_pm.items():
        config[key] = tune.choice(val)
    
    for key, val in cont_pm.items():
        config[key] = tune.uniform(val[0], val[1])

    return config

def copy_and_move_files(log_dir):
    label_name = "labels.txt"
    # pb_dir = os.path.join(best_log_dir, ckpt, pb_name)
    src_label_dir = os.path.join(CUR_DIR, "../datasets", label_name)
    dst_label_dir = os.path.join(CUR_DIR, "../pb_and_text")
    src_ckpt_dir = os.path.join(CUR_DIR, "../pb_and_text/checkpoints")
    dst_ckpt_dir = os.path.join(CUR_DIR, "../pb_and_text/checkpoints")

    if  os.path.exists(src_ckpt_dir):
        shutil.rmtree(src_ckpt_dir)
    
    shutil.copytree(log_dir, dst_ckpt_dir)
    shutil.copy(src_label_dir, dst_label_dir)

def compress_zip(in_dir, out_dir):
    in_dir = os.path.join(CUR_DIR, in_dir)
    out_dir = os.path.join(CUR_DIR, out_dir)

    zip_name = "archive"
    out_dir = os.path.join(out_dir, zip_name)
    print("Compress {}/checkpoints and {}/labels.txt into {}.zip".format(in_dir, in_dir, out_dir))
    if os.path.exists(out_dir):
        os.romve(out_dir)
    shutil.make_archive(out_dir, "zip", in_dir)



if __name__ == "__main__":
    import tensorflow as tf 
    ray.shutdown()
    ray.init()
    scheduler = AsyncHyperBandScheduler(time_attr='training_iteration', 
                                        metric="mean_accuracy", 
                                        mode="max")

    continue_params, discrete_params, notuning_params  = read_params.get_param_dict()
    print("continue_params:", continue_params)
    print("discrete_params:", discrete_params)
    print("notuning_params", notuning_params)
    config = get_config(discrete_params, continue_params)

    global NOTUNING_CONFIG
    NOTUNING_CONFIG = notuning_params

    analysis = tune.run(
        Trainable,
        stop={"training_iteration": 5},
        verbose=1,
        num_samples=2,
        scheduler=scheduler,
        config=config,
               
        # config= {
               # "hiddens": tune.choice([32, 64, 128]),
               # "learning_rate": tune.uniform(0.01, 0.1),
               # "optimizer": tune.choice(["sgd", "adam"])
               # },
        resources_per_trial={'gpu': 3, 'cpu': 20},
        checkpoint_freq=5,
        checkpoint_at_end=True,
        keep_checkpoints_num=1,
        max_failures=2, 
        local_dir="/data121/lijiayuan/test/classify_flowers/ray_results",
        name = "flowers2-example"
        )

    best_config = analysis.get_best_config(metric="mean_accuracy")
    print("Best config is", best_config)
    trials = analysis.trials
    print("All trials inform:", trials)

    best_logdir = analysis.get_best_logdir(metric="mean_accuracy", mode="max")
    print("best logdir:", best_logdir)

    copy_and_move_files(best_logdir)
    compress_zip(in_dir="../pb_and_text", out_dir="../zip_file")

		</comment>
		<comment id='3' author='ProNoobLi' date='2019-12-26T02:57:23Z'>
		Does it train at all? Otherwise, can you try simply doing this without starting Ray:
&lt;denchmark-code&gt;x = Trainable()
print(x.train())
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='ProNoobLi' date='2019-12-26T07:42:56Z'>
		
Does it train at all? Otherwise, can you try simply doing this without starting Ray:
x = Trainable()
print(x.train())


&lt;denchmark-link:https://user-images.githubusercontent.com/33815430/71464967-39992f80-27f6-11ea-8186-675f5e16ab29.png&gt;&lt;/denchmark-link&gt;

I am sure it is training.
Everything works if I comment the checkpoint section
        #----checkpoint config----##
        ckpt = tf.train.Checkpoint(step=1, optimizer=self.optimizer, net=self.model)
        manager = tf.train.CheckpointManager(ckpt, './tf_ckpts', max_to_keep=3)
		</comment>
		<comment id='5' author='ProNoobLi' date='2019-12-26T08:14:54Z'>
		Sorry; I meant you should take your custom Trainable class (which you've named as Trainable in the above snippets), and try running the train method for your custom Trainable class.
from your_script import Trainable

x = Trainable()
print(x.train())
This will verify if the Checkpoint code is instantiated/setup incorrectly, and it will do so outside the Ray runtime.
		</comment>
		<comment id='6' author='ProNoobLi' date='2019-12-27T02:59:49Z'>
		&lt;denchmark-link:https://user-images.githubusercontent.com/33815430/71498596-f394a800-2897-11ea-985b-a32773ec98ff.png&gt;&lt;/denchmark-link&gt;

Hi, I got the same error...
		</comment>
		<comment id='7' author='ProNoobLi' date='2019-12-27T04:01:33Z'>
		OK, note that this failure is unrelated to Tune if it fails like that.
You can also probably reproduce the same issue if you just didn't even subclass Trainable. Perhaps consider searching through the TF issues - (this is probably unrelated) &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/28080&gt;tensorflow/tensorflow#28080&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='ProNoobLi' date='2019-12-27T07:01:33Z'>
		
OK, note that this failure is unrelated to Tune if it fails like that.
You can also probably reproduce the same issue if you just didn't even subclass Trainable. Perhaps consider searching through the TF issues - (this is probably unrelated) tensorflow/tensorflow#28080

Thank you so much
		</comment>
	</comments>
</bug>