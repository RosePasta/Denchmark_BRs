<bug id='6847' author='eugenevinitsky' open_date='2020-01-19T21:45:16Z' closed_time='2020-01-19T22:44:07Z'>
	<summary>[tune], [rllib] Tune grid_search errors if you use an RLlib trainer</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

Ray version and other system information (Python version, TensorFlow version, OS): Ray 0.8.0, python 3.6.5, TF 2.0.0, Mac
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
If we cannot run your script, we cannot fix your issue.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

&lt;denchmark-code&gt;import ray
from ray.rllib.agents.ppo.ppo_policy import PPOTFPolicy
from ray.rllib.agents.ppo.ppo import DEFAULT_CONFIG

from ray.tune import run as run_tune
from ray.tune.registry import register_env

import gym
from ray import tune
from ray.rllib.agents.ppo import PPOTrainer
from gym.spaces import Box
import numpy as np

class DummyEnv(gym.Env):
    """Simple env in which the policy learns to repeat a previous observation
    token after a given delay."""

    def __init__(self):
        self.observation_space = Box(low=-1, high=1, shape=(2,))
        self.action_space = Box(low=-1, high=1, shape=(2,))
        self.step_num = 0

    def reset(self):
        self.step_num = 0.0
        return np.zeros(2)

    def step(self, action):
        self.step_num += 1
        if self.step_num &gt; 100:
            done = {'__all__': True}
        else:
            done = {'__all__': False}
        return np.zeros(2), 100, done, {}


if __name__ == '__main__':
    env = DummyEnv()
    config = DEFAULT_CONFIG
    config['train_batch_size'] = 500
    config['lr'] = tune.grid_search([1e-2, 1e-3])

    config['env'] = 'DummyEnv'
    env_creator = lambda config: DummyEnv()
    register_env('DummyEnv', env_creator)

    exp_dict = {
        'name': 'ProofOfConcept',
        'run_or_experiment': PPOTrainer,
        'stop': {
            'training_iteration': 100
        },
        'config': config,
    }

    ray.init(local_mode=True)
    run_tune(**exp_dict, queue_trials=False)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='eugenevinitsky' date='2020-01-19T21:47:03Z'>
		If you swap out PPOTrainer with 'PPO' everything works fine.
		</comment>
		<comment id='2' author='eugenevinitsky' date='2020-01-19T21:56:17Z'>
		This is the error btw:
&lt;denchmark-code&gt;2020-01-19 13:55:41,216	ERROR ray_trial_executor.py:249 -- Trial PPO_2_lr=5e-05: Unexpected error starting runner.
Traceback (most recent call last):
  File "/Users/eugenevinitsky/anaconda3/envs/sim2real/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py", line 240, in start_trial
    self._start_trial(trial, checkpoint)
  File "/Users/eugenevinitsky/anaconda3/envs/sim2real/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py", line 180, in _start_trial
    or trial.has_checkpoint()))
  File "/Users/eugenevinitsky/anaconda3/envs/sim2real/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py", line 130, in _setup_remote_runner
    return cls.remote(**kwargs)
  File "/Users/eugenevinitsky/anaconda3/envs/sim2real/lib/python3.6/site-packages/ray/actor.py", line 329, in remote
    return self._remote(args=args, kwargs=kwargs)
  File "/Users/eugenevinitsky/anaconda3/envs/sim2real/lib/python3.6/site-packages/ray/actor.py", line 471, in _remote
    *copy.deepcopy(args), **copy.deepcopy(kwargs))
  File "/Users/eugenevinitsky/anaconda3/envs/sim2real/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 83, in __init__
    Trainer.__init__(self, config, env, logger_creator)
  File "/Users/eugenevinitsky/anaconda3/envs/sim2real/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 404, in __init__
    Trainable.__init__(self, config, logger_creator)
  File "/Users/eugenevinitsky/anaconda3/envs/sim2real/lib/python3.6/site-packages/ray/tune/trainable.py", line 172, in __init__
    self._setup(copy.deepcopy(self.config))
  File "/Users/eugenevinitsky/anaconda3/envs/sim2real/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 510, in _setup
    self._allow_unknown_subkeys)
  File "/Users/eugenevinitsky/anaconda3/envs/sim2real/lib/python3.6/site-packages/ray/tune/utils/util.py", line 168, in deep_update
    deep_update(original[k], value, new_keys_allowed, [])
  File "/Users/eugenevinitsky/anaconda3/envs/sim2real/lib/python3.6/site-packages/ray/tune/utils/util.py", line 160, in deep_update
    for k, value in new_dict.items():
AttributeError: 'float' object has no attribute 'items'
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='eugenevinitsky' date='2020-01-19T22:34:22Z'>
		The error here is due to a weird self-mutation of the default config. the fix should be
&lt;denchmark-code&gt;if __name__ == '__main__':
    env = DummyEnv()
    config = DEFAULT_CONFIG.copy()
    config['train_batch_size'] = 500
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='eugenevinitsky' date='2020-01-19T22:44:04Z'>
		Oh nice, thanks!
		</comment>
		<comment id='5' author='eugenevinitsky' date='2020-01-28T13:43:07Z'>
		I think I managed to find an elegant solution to that. Please check &lt;denchmark-link:https://github.com/ray-project/ray/pull/6940&gt;#6940&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>