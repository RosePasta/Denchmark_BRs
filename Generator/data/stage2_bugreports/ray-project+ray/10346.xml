<bug id='10346' author='marcmm' open_date='2020-08-27T02:00:58Z' closed_time='2020-09-28T17:58:41Z'>
	<summary>RLLib RuntimeError with parametric DQN cartpole example running on head GPU node.</summary>
	<description>
&lt;denchmark-h:h3&gt;Model training fails while running cartpole example on distributed cluster with GPU&lt;/denchmark-h&gt;

I'm using the sample ParametricActionsCartPole code with minor modifications to run on a distributed cluster.
The code fails when the head node is set to use a GPU and works fine when running on CPU only mode.
python 3.6
ray[rllib]==0.8.7
torch==1.6.0
cuda10.0-cudnn7-ubuntu16.04
&lt;denchmark-h:h3&gt;Reproduction&lt;/denchmark-h&gt;

import argparse
import ray
from ray import tune
from ray.rllib.examples.env.parametric_actions_cartpole import 
ParametricActionsCartPole
from ray.rllib.examples.models.parametric_actions_model import 
ParametricActionsModel, TorchParametricActionsModel
from ray.rllib.models import ModelCatalog
from ray.rllib.utils.test_utils import check_learning_achieved
from ray.tune.registry import register_env
import ray
from ray.rllib import train
DEFAULT_RAY_ADDRESS = 'localhost:6379'
if name == "main":
&lt;denchmark-code&gt;train_parser = train.create_parser()
args = train_parser.parse_args()

if args.ray_address is None:
    args.ray_address = DEFAULT_RAY_ADDRESS
    
ray.init(address=args.ray_address)

register_env("pa_cartpole", lambda _: ParametricActionsCartPole(10))
ModelCatalog.register_custom_model(
    "pa_model", TorchParametricActionsModel
    if args.torch else ParametricActionsModel)

if args.run == "DQN":
    cfg = {
        # TODO(ekl) we need to set these to prevent the masked values
        # from being further processed in DistributionalQModel, which
        # would mess up the masking. It is possible to support these if we
        # defined a custom DistributionalQModel that is aware of masking.
        "hiddens": [],
        "dueling": False,
    }
else:
    cfg = {}

config = dict({
    "env": "pa_cartpole",
    "model": {
        "custom_model": "pa_model",
    },
    "num_gpus": 1,
    "num_workers": 3,
    "num_envs_per_worker": 1,
    "framework": "torch" if args.torch else "tf",
}, **cfg)

results = tune.run(args.run, stop=args.stop, config=config, verbose=1)

ray.shutdown()
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Error call stack&lt;/denchmark-h&gt;

DQN.train()ï¿½[39m (pid=193, ip=172.17.0.99)
File "python/ray/_raylet.pyx", line 479, in ray._raylet.execute_task
File "python/ray/_raylet.pyx", line 432, in ray._raylet.execute_task.function_executor
File "/azureml-envs/azureml_fc2d3da63be28a3eb510f9b8032c0216/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 522, in train
raise e
File "/azureml-envs/azureml_fc2d3da63be28a3eb510f9b8032c0216/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 508, in train
result = Trainable.train(self)
File "/azureml-envs/azureml_fc2d3da63be28a3eb510f9b8032c0216/lib/python3.6/site-packages/ray/tune/trainable.py", line 332, in train
result = self.step()
File "/azureml-envs/azureml_fc2d3da63be28a3eb510f9b8032c0216/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 110, in step
res = next(self.train_exec_impl)
File "/azureml-envs/azureml_fc2d3da63be28a3eb510f9b8032c0216/lib/python3.6/site-packages/ray/util/iter.py", line 758, in next
return next(self.built_iterator)
File "/azureml-envs/azureml_fc2d3da63be28a3eb510f9b8032c0216/lib/python3.6/site-packages/ray/util/iter.py", line 785, in apply_foreach
for item in it:
File "/azureml-envs/azureml_fc2d3da63be28a3eb510f9b8032c0216/lib/python3.6/site-packages/ray/util/iter.py", line 845, in apply_filter
for item in it:
File "/azureml-envs/azureml_fc2d3da63be28a3eb510f9b8032c0216/lib/python3.6/site-packages/ray/util/iter.py", line 845, in apply_filter
for item in it:
File "/azureml-envs/azureml_fc2d3da63be28a3eb510f9b8032c0216/lib/python3.6/site-packages/ray/util/iter.py", line 785, in apply_foreach
for item in it:
File "/azureml-envs/azureml_fc2d3da63be28a3eb510f9b8032c0216/lib/python3.6/site-packages/ray/util/iter.py", line 845, in apply_filter
for item in it:
File "/azureml-envs/azureml_fc2d3da63be28a3eb510f9b8032c0216/lib/python3.6/site-packages/ray/util/iter.py", line 1078, in build_union
item = next(it)
File "/azureml-envs/azureml_fc2d3da63be28a3eb510f9b8032c0216/lib/python3.6/site-packages/ray/util/iter.py", line 758, in next
return next(self.built_iterator)
File "/azureml-envs/azureml_fc2d3da63be28a3eb510f9b8032c0216/lib/python3.6/site-packages/ray/util/iter.py", line 785, in apply_foreach
for item in it:
File "/azureml-envs/azureml_fc2d3da63be28a3eb510f9b8032c0216/lib/python3.6/site-packages/ray/util/iter.py", line 785, in apply_foreach
for item in it:
File "/azureml-envs/azureml_fc2d3da63be28a3eb510f9b8032c0216/lib/python3.6/site-packages/ray/util/iter.py", line 785, in apply_foreach
for item in it:
File "/azureml-envs/azureml_fc2d3da63be28a3eb510f9b8032c0216/lib/python3.6/site-packages/ray/util/iter.py", line 793, in apply_foreach
result = fn(item)
File "/azureml-envs/azureml_fc2d3da63be28a3eb510f9b8032c0216/lib/python3.6/site-packages/ray/rllib/execution/train_ops.py", line 69, in call
info = self.workers.local_worker().learn_on_batch(batch)
File "/azureml-envs/azureml_fc2d3da63be28a3eb510f9b8032c0216/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py", line 752, in learn_on_batch
info_out[pid] = policy.learn_on_batch(batch)
File "/azureml-envs/azureml_fc2d3da63be28a3eb510f9b8032c0216/lib/python3.6/site-packages/ray/rllib/policy/torch_policy.py", line 328, in learn_on_batch
self._loss(self, self.model, self.dist_class, train_batch))
File "/azureml-envs/azureml_fc2d3da63be28a3eb510f9b8032c0216/lib/python3.6/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py", line 218, in build_q_losses
torch.where(q_t &gt; -float("inf"), q_t, torch.tensor(0.0)) *
RuntimeError: Expected condition, x and y to be on the same device, but condition is on cuda:0 and x and y are on cuda:0 and cpu respectively
	</description>
	<comments>
		<comment id='1' author='marcmm' date='2020-09-04T20:20:27Z'>
		I have the same issue on 0.8.7. Maybe this by &lt;denchmark-link:https://github.com/ray-project/ray/pull/10168&gt;#10168&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/ray-project/ray/pull/10177&gt;#10177&lt;/denchmark-link&gt;
. I haven't test that myself though.
		</comment>
		<comment id='2' author='marcmm' date='2020-09-28T17:58:41Z'>
		Should be fixed.
		</comment>
	</comments>
</bug>