<bug id='1219' author='robertnishihara' open_date='2017-11-14T07:47:47Z' closed_time='2018-01-26T22:18:46Z'>
	<summary>Submitting a large number of actor tasks in a loop gives reconstruction error.</summary>
	<description>
I'm seeing this issue on Ubuntu 16.04 with Python 3.6.2 (Anaconda) and the current Ray master (building from source).
I'm running the following.
import ray

ray.worker._init(start_ray_local=True, object_store_memory=100000000)

@ray.remote
class Foo():
    def m(self):
        return 1

actors = [Foo.remote() for _ in range(30)]

i = 0
while True:
    print(i)
    i += 1
    ids = []
    for _ in range(50):
        ids += [a.m.remote() for a in actors]
    ray.get(ids)
The relevant output is
&lt;denchmark-code&gt;0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
There is not enough space to create this object, so evicting 32259 objects to free up 20000580 bytes.
31
32
33
34
35
There is not enough space to create this object, so evicting 20708 objects to free up 12839980 bytes.
[INFO] (/home/ubuntu/ray/src/local_scheduler/local_scheduler_algorithm.cc:443) A task that has already been executed has been resubmitted, so we are ignoring it. This should only happen during reconstruction.
[INFO] (/home/ubuntu/ray/src/local_scheduler/local_scheduler_algorithm.cc:443) A task that has already been executed has been resubmitted, so we are ignoring it. This should only happen during reconstruction.
[INFO] (/home/ubuntu/ray/src/local_scheduler/local_scheduler_algorithm.cc:443) A task that has already been executed has been resubmitted, so we are ignoring it. This should only happen during reconstruction.
[INFO] (/home/ubuntu/ray/src/local_scheduler/local_scheduler_algorithm.cc:443) A task that has already been executed has been resubmitted, so we are ignoring it. This should only happen during reconstruction.
&lt;/denchmark-code&gt;

The message repeats a large number of times very quickly and then the script hangs.
	</description>
	<comments>
		<comment id='1' author='robertnishihara' date='2017-11-14T18:58:19Z'>
		The error goes away if I don't save dummy objects on the workers. I'm worried this may be the cause of a memory leak that I'm seeing (e.g., the dummy objects just accumulate forever).
Do we need to pin all of the dummy objects?
&lt;denchmark-link:https://github.com/stephanie-wang&gt;@stephanie-wang&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='robertnishihara' date='2017-11-16T20:18:00Z'>
		For now, we will have to pin all the objects for forked actor handles.
The plan for the future is to remove the dummy objects entirely from the object store. Only the local scheduler hosting the actor process needs to know about them, so we can have the actor process mock the object notifications that would normally come from the object store.
		</comment>
	</comments>
</bug>