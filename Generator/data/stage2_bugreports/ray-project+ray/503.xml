<bug id='503' author='robertnishihara' open_date='2017-05-02T05:29:34Z' closed_time='2017-06-11T06:34:57Z'>
	<summary>Failures when doing ray.get on large number of machines.</summary>
	<description>
On a twenty-node cluster, submitting a bunch of tasks and calling ray.get on the results often results in failures.
This may have something to do with the remote plasma managers trying to ship their objects to the local plasma manager but receiving an ECONNRESET.
&lt;denchmark-link:https://github.com/stephanie-wang&gt;@stephanie-wang&lt;/denchmark-link&gt;
 is working on this and has a better understanding of the problem.
	</description>
	<comments>
		<comment id='1' author='robertnishihara' date='2017-05-03T03:16:11Z'>
		This should be addressed by &lt;denchmark-link:https://github.com/ray-project/ray/pull/471&gt;#471&lt;/denchmark-link&gt;
.
I'll close this issue after testing it out.
		</comment>
		<comment id='2' author='robertnishihara' date='2017-05-19T04:08:11Z'>
		Looks like the original issue has been fixed.
		</comment>
		<comment id='3' author='robertnishihara' date='2017-05-19T19:16:47Z'>
		I'm reopening this because I'm seeing the same symptoms again on a 200-node cluster (m4.2xlarge).
I followed the documentation in &lt;denchmark-link:https://github.com/ray-project/ray/blob/master/doc/source/using-ray-on-a-large-cluster.rst&gt;https://github.com/ray-project/ray/blob/master/doc/source/using-ray-on-a-large-cluster.rst&lt;/denchmark-link&gt;
, and just ran the following (with the appropriate redis address). This was the first thing that I ran on the Ray cluster.
import ray
ray.init(redis_address="&lt;redis-address&gt;")
import time

@ray.remote
def f():
  time.sleep(0.01)
  return ray.services.get_node_ip_address()

# Get a list of the IP addresses of the nodes that have joined the cluster.
s = set(ray.get([f.remote() for _ in range(1000)]))
I then saw a ton of errors that looked like this
&lt;denchmark-code&gt;A nondeterministic task was reexecuted.

You can inspect errors by running

    ray.error_info()

If this driver is hanging, start a new one with

    ray.init(redis_address="172.31.17.149:6379")
&lt;/denchmark-code&gt;

That makes some sense because the task is non-deterministic (its output depends on where it runs).
The script then finished successfully with len(s) == 166 instead of the intended 200.
I then checked /tmp/raylogs/ on the head node, and it looks like the following.
&lt;denchmark-code&gt;WARNING:root:Timed out b'plasma_manager'
WARNING:root:Timed out b'plasma_manager'
WARNING:root:Timed out b'plasma_manager'
WARNING:root:Timed out b'plasma_manager'
WARNING:root:Timed out b'plasma_manager'
WARNING:root:Timed out b'plasma_manager'
WARNING:root:Timed out b'plasma_manager'
WARNING:root:Timed out b'plasma_manager'
WARNING:root:Timed out b'plasma_manager'
WARNING:root:Timed out b'plasma_manager'
WARNING:root:Timed out b'plasma_manager'
WARNING:root:Timed out b'plasma_manager'
WARNING:root:Timed out b'plasma_manager'
WARNING:root:Timed out b'plasma_manager'
WARNING:root:Timed out b'plasma_manager'
WARNING:root:Timed out b'plasma_manager'
WARNING:root:Timed out b'plasma_manager'
WARNING:root:Timed out b'plasma_manager'
WARNING:root:Timed out b'plasma_manager'
WARNING:root:Timed out b'plasma_manager'
WARNING:root:Timed out b'plasma_manager'
WARNING:root:Timed out b'plasma_manager'
WARNING:root:Timed out b'plasma_manager'
WARNING:root:Timed out b'plasma_manager'
WARNING:root:Timed out b'plasma_manager'
WARNING:root:Timed out b'plasma_manager'
WARNING:root:Timed out b'plasma_manager'
WARNING:root:Timed out b'plasma_manager'
WARNING:root:Timed out b'plasma_manager'
WARNING:root:Timed out b'plasma_manager'
WARNING:root:Timed out b'plasma_manager'
WARNING:root:Timed out b'plasma_manager'
WARNING:root:Timed out b'plasma_manager'
WARNING:root:Timed out b'plasma_manager'
WARNING:root:Removed b'plasma_manager', client ID 897e1a792540194904fe648d844651811d5b8bb9
WARNING:root:Removed b'plasma_manager', client ID 69abc865896ae0bd8630041df7cf0d47a2eee8d6
WARNING:root:Removed b'plasma_manager', client ID b51153ee55e2057e254b89061dd0f7f5fa0b0f96
WARNING:root:Removed b'plasma_manager', client ID 4988fdf5a7e1e804d842f40de375a157da5ff381
WARNING:root:Removed b'plasma_manager', client ID c0cb153835cf19613ec318ee8454856183932a85
WARNING:root:Removed b'plasma_manager', client ID c5d6a7b92a8e9116c5c8c954542b8d88f87d87ac
WARNING:root:Removed b'plasma_manager', client ID 7dabe1e6e3e9f19601e5a2e37870b290455f6773
WARNING:root:Removed b'plasma_manager', client ID 86dc9b6cc63bc41e820f9786048bfef5dbc98653
WARNING:root:Removed b'plasma_manager', client ID 95415497310429698dcd148d6e4d18c80cf260fb
WARNING:root:Removed b'plasma_manager', client ID 6ce91c4312587b110d8c3a6b18cedfe1078974b9
WARNING:root:Removed b'plasma_manager', client ID dd5943f9dab81b71d1ff6fd910849a127d3b8b4c
WARNING:root:Removed b'plasma_manager', client ID 25931d94fcca14ef3640abd4cf898fa8d8892445
WARNING:root:Removed b'plasma_manager', client ID fe237454664786fb461f93e5cd1d04104ad3ab86
WARNING:root:Removed b'plasma_manager', client ID 93b01aa907e547148f5c183d0c133b81f3b7267c
WARNING:root:Removed b'plasma_manager', client ID fbbbdb7e48cd3de18093858e70ba4e01b4cf8fec
WARNING:root:Removed b'plasma_manager', client ID 4aefc58373bd2bd0befba6011b4311d2bdcd0c72
WARNING:root:Removed b'plasma_manager', client ID 2940daa042f1e768680ececa0cd3c75b8b0ed626
WARNING:root:Removed b'plasma_manager', client ID 954ee6b0e5b7f5a519fc9358ee3de22b446be799
WARNING:root:Removed b'plasma_manager', client ID 11beb87b2813d84890524fd96d3e80653ae22cd5
WARNING:root:Removed b'plasma_manager', client ID e6711d6c812da41ce1595fd800ee172609c560df
WARNING:root:Removed b'plasma_manager', client ID 5975551d8fdcd595e668d79cec47e03f6462aa1c
WARNING:root:Removed b'plasma_manager', client ID 2788e6610c013eef3f387500e5a0a34ef918132b
WARNING:root:Removed b'plasma_manager', client ID 64eae0348357b3cddc8594e98b2c9f67aec4fc59
WARNING:root:Removed b'plasma_manager', client ID 44d6e69bc0204fb2261e0933a37f8358b7cb64c1
WARNING:root:Removed b'plasma_manager', client ID b7ac4219e01db2436a6ff5f27fd21e9b87273229
WARNING:root:Removed b'plasma_manager', client ID 92caa39bb7da78d06b8bc72ff00e6a42ea3b7e87
WARNING:root:Removed b'plasma_manager', client ID 66d83afc30cae6286514ccf56edc11d274407858
WARNING:root:Removed b'plasma_manager', client ID bc636d6190d261877acc82458797b72c6f2ba43e
WARNING:root:Removed b'plasma_manager', client ID 11877329de0a191f591e529170791c863d012309
WARNING:root:Removed b'plasma_manager', client ID 362547bc3103d074bdde1e0058d91e6ba4b2810d
WARNING:root:Removed b'plasma_manager', client ID a26e6a1b2c3d0cb715bb6b4f911bf38287e4e555
WARNING:root:Removed b'plasma_manager', client ID 48ea7f437c2c52a038f3630200537c2685a17490
WARNING:root:Removed b'plasma_manager', client ID 573566bf98565d969bf5df2bec6b117a35229a2f
WARNING:root:Removed b'plasma_manager', client ID 3cbffffb0584bc14f86f6c2cba82e20bd39169eb
WARNING:root:Marked 174 objects as lost.
&lt;/denchmark-code&gt;

I then ssh'ed to one of the nodes (arbitrarily chosen), which had been marked as dead (by checking ray.global_state.client_table()) to find one that was marked as dead.
Checking ps aux | grep "plasma_manager ", the plasma manager on that machine seems to still be alive, and its stdout/stderr log files were empty.
Subsequent calls to the following command succeed.
s = set(ray.get([f.remote() for _ in range(1000)]))
And they produce len(s) == 200.
cc &lt;denchmark-link:https://github.com/stephanie-wang&gt;@stephanie-wang&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='robertnishihara' date='2017-06-11T00:45:56Z'>
		Ok, after more investigation, the problem appears to be the following.
The problem is easily reproducible about 75% of the time on 100 m4.xlarge nodes by doing the following
import ray

ray.init(redis_address=...)

@ray.remote
def f():
    return 1

l = ray.get([f.remote() for _ in range(1000)])
The problem is with the call to connect, which is called by connect_inet_sock in io.cc, which is called by get_manager_connection in plasma_manager.cc. This call takes around 15 seconds on some machines. Since the monitor process doesn't receive any heartbeats for 10 seconds, it marks the plasma manager as dead.
The relevant line is here 


ray/src/common/io.cc


         Line 213
      in
      8d350f6






 if (connect(fd, (struct sockaddr *) &amp;addr, sizeof(addr)) != 0) { 




, which is called from here 


ray/src/plasma/plasma_manager.cc


         Line 804
      in
      8d350f6






 int fd = connect_inet_sock(ip_addr, port); 




.
When the above script runs, the 1000 tasks are scheduled across the 100 nodes. The manager on the head node (I'm assuming the script is run on the head node) then initiates TCP connections with all of the 99 remote plasma managers by calling connect_inet_sock, which under the hood calls connect. The 99 calls to connect_inet_sock start and finish within the span of about 55ms, with each call taking around 1ms (or less). These TCP connections are used to request objects.
Each remote plasma manager then initiates a TCP connection back to the plasma manager on the head node (to use for transferring the requested object) by calling connect_inet_sock, which under the hood calls connect. Some of these calls take around 10 seconds.
For each call to connect on a remote plasma manager, there should be a corresponding call to accept by the plasma manager on the head node. It is called by accept_client in io.cc, which is called by ClientConnection_listen in plasma_manager.cc, which is called by handle_new_client in plasma_manager.cc. These calls to accept each seem to take about 1ms (they are quick). The plasma manager is single threaded, and if you look at the timestamps for all of these calls to accept, they often happen in rapid succession, and sometimes there is a gap of 5 or 10 seconds between two calls.
Note that during all of this, the plasma manager never seems to have very high CPU utilization (as judged by a glance at top).
		</comment>
		<comment id='5' author='robertnishihara' date='2017-06-11T06:34:57Z'>
		This should be addressed by &lt;denchmark-link:https://github.com/ray-project/ray/pull/661&gt;#661&lt;/denchmark-link&gt;
.
		</comment>
	</comments>
</bug>