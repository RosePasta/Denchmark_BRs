<bug id='8769' author='ericl' open_date='2020-06-03T19:43:51Z' closed_time='2020-06-05T06:34:22Z'>
	<summary>[rllib] test_supported_spaces consistent OOM in travis</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

The second-to-last rllib travis build has been consistently failing since: &lt;denchmark-link:https://github.com/ray-project/ray/pull/8520/files&gt;https://github.com/ray-project/ray/pull/8520/files&lt;/denchmark-link&gt;

was merged.
&lt;denchmark-code&gt;E                       ray.memory_monitor.RayOutOfMemoryError: More than 95% of the memory on node travis-job-04150411-53df-4c8c-9bc4-2c378e131a11 is used (7.4 / 7.79 GB). The top 10 memory consumers are:
E                       
E                       PID	MEM	COMMAND
E                       30884	3.23GiB	ray::RolloutWorker
E                       26855	0.91GiB	bazel(ray) -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/home/travis/.cache/bazel/_bazel_travis/
E                       29471	0.46GiB	/home/travis/build/ray-project/ray/python/ray/core/src/ray/thirdparty/redis/src/redis-server *:10263
E                       29501	0.33GiB	ray::ARSTrainer.train()
&lt;/denchmark-code&gt;

See the build history here: &lt;denchmark-link:https://travis-ci.com/github/ray-project/ray/builds&gt;https://travis-ci.com/github/ray-project/ray/builds&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='ericl' date='2020-06-03T19:46:33Z'>
		This is particularly odd since ARS shouldn't be creating any RolloutWorker actors at all. Maybe it's something that got leaked from another test or ARS is a red herring.
		</comment>
		<comment id='2' author='ericl' date='2020-06-03T20:40:17Z'>
		Are we sure it's not a core bug? It's not just this one test. All tests at the end of the alphabet fail due to this memory problem: test_rollout, test_rollout_worker, test_supported_multi_agent, test_supported_spaces. These are the last 4 in the test dir. They all run fine locally.
		</comment>
		<comment id='3' author='ericl' date='2020-06-03T20:43:06Z'>
		All I know is it's consistently failing after that particular PR, and never had an issue before. Maybe try reverting it and see if that fixes it?
		</comment>
		<comment id='4' author='ericl' date='2020-06-03T20:44:39Z'>
		yeah. This PR in question added many tests for pytorch that were previously only run for tf. Maybe this just made stuff go OOM (+ spillage between tests?). Will check.
		</comment>
	</comments>
</bug>