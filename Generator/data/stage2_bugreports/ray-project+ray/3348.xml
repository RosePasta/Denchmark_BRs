<bug id='3348' author='llan-ml' open_date='2018-11-18T09:46:03Z' closed_time='2018-11-19T08:03:48Z'>
	<summary>[Tune] Some trials with running status are actually not running</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
Ray installed from (source or binary): source
Ray version: 5cbc597 with new gcs on
Python version: 3.6.5

&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 231/240 CPUs, 0/4 GPUs
Result logdir: /ray_results/TRAIN_MAML_PointEnv_2018-11-18_16-38-26
Result logdir: /ray_results/TRAIN_MetaSGD_PointEnv_2018-11-18_16-38-26
Result logdir: /ray_results/TRAIN_TASP_PointEnv_2018-11-18_16-38-26
PENDING trials:
 - MetaSGD_PointEnv_11_random_seed=2:	PENDING
 - MetaSGD_PointEnv_12_random_seed=3:	PENDING
 - MetaSGD_PointEnv_13_random_seed=4:	PENDING
 - MetaSGD_PointEnv_14_random_seed=5:	PENDING
 - MetaSGD_PointEnv_15_random_seed=6:	PENDING
  ... 9 not shown
 - TASP_PointEnv_25_random_seed=6:	PENDING
 - TASP_PointEnv_26_random_seed=7:	PENDING
 - TASP_PointEnv_27_random_seed=8:	PENDING
 - TASP_PointEnv_28_random_seed=9:	PENDING
 - TASP_PointEnv_29_random_seed=10:	PENDING
RUNNING trials:
 - MAML_PointEnv_0_random_seed=1:	RUNNING [pid=13797], 3576 s, 252 iter
 - MAML_PointEnv_1_random_seed=2:	RUNNING [pid=13776], 3586 s, 230 iter
 - MAML_PointEnv_2_random_seed=3:	RUNNING [gpu-server-00 pid=209957], 3569 s, 243 iter
 - MAML_PointEnv_3_random_seed=4:	RUNNING
 - MAML_PointEnv_4_random_seed=5:	RUNNING
  ... 1 not shown
 - MAML_PointEnv_6_random_seed=7:	RUNNING [gpu-server-00 pid=209955], 3572 s, 246 iter
 - MAML_PointEnv_7_random_seed=8:	RUNNING
 - MAML_PointEnv_8_random_seed=9:	RUNNING
 - MAML_PointEnv_9_random_seed=10:	RUNNING [pid=13813], 3579 s, 248 iter
 - MetaSGD_PointEnv_10_random_seed=1:	RUNNING [pid=13784], 3580 s, 233 iter

All the log files are normal. Do you have any ideas to look over this further?
	</description>
	<comments>
		<comment id='1' author='llan-ml' date='2018-11-19T02:34:02Z'>
		Hm; does this happen on a single node?
		</comment>
		<comment id='2' author='llan-ml' date='2018-11-19T08:03:48Z'>
		Hi &lt;denchmark-link:https://github.com/richardliaw&gt;@richardliaw&lt;/denchmark-link&gt;
 I think I missed some features of Actors and it's not a bug.
In my case, I define a global actor, of which methods will be called multiple times in each remote evaluator at each iteration. Since I just use @ray.remote to decorate the class, it will request 1 cpu each time a method is called, which cannot be declared in Trainable.default_resource_request. Thus, there are no enough resources at the backend for some trials that Tune adds at the beginning.



ray/python/ray/worker.py


        Lines 2510 to 2522
      in
      e4bb5d8






 # Set the actor default resources. 



 if num_cpus is None and num_gpus is None and resources is None: 



 # In the default case, actors acquire no resources for 



 # their lifetime, and actor methods will require 1 CPU. 



 cpus_to_use = DEFAULT_ACTOR_CREATION_CPUS_SIMPLE_CASE 



 actor_method_cpus = DEFAULT_ACTOR_METHOD_CPUS_SIMPLE_CASE 



 else: 



 # If any resources are specified, then all resources are 



 # acquired for the actor's lifetime and no resources are 



 # associated with methods. 



 cpus_to_use = (DEFAULT_ACTOR_CREATION_CPUS_SPECIFIED_CASE 



 if num_cpus is None else num_cpus) 



 actor_method_cpus = DEFAULT_ACTOR_METHOD_CPUS_SPECIFIED_CASE 





The problem is fixed by using @ray.remote(num_cpus=1) to decorate the class. I believe it would be great to add these comments to the docstrings.
		</comment>
	</comments>
</bug>