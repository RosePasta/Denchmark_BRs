<bug id='6372' author='esquires' open_date='2019-12-05T22:35:09Z' closed_time='2020-08-09T20:06:26Z'>
	<summary>Exception when using MultiDiscrete action spaces</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
Ray installed from (source or binary): binary (via pip)
Ray version: 0.7.6
Python version: 3.6.9
Exact command to reproduce: python rllib_cartpole.py for the following file

import gym.envs.classic_control

import ray
from ray import tune


class CustomCartpole(gym.envs.classic_control.CartPoleEnv):
    """Add a dimension to the cartpole action space that is ignored."""

    def __init__(self, env_config):
        super().__init__()
        # if override_actions is false this is just the Cartpole environment
        self.override_actions = env_config['override_actions']
        if self.override_actions:
            # 2 is the environment's normal action space
            # 4 is just a dummy number to give it an extra dimension
            self.action_space = gym.spaces.MultiDiscrete([2, 4])

    def step(self, action):
        # call the cartpole environment with the original action
        if self.override_actions:
            return super().step(action[0])
        else:
            return super().step(action)


def main():

    ray.init()
    tune.run(
        "PPO",
        stop={"episode_reward_mean": 200},
        config={
            "env": CustomCartpole,
            "env_config": {'override_actions': False},
            "num_gpus": 1,
            "num_workers": 1,
            "eager": False,
        },
    )


if __name__ == '__main__':
    main()
&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

I am trying to train on an environment with a MultiDiscrete action space. The above file recreates the issue with a simple adjustment to CartPole. When the file is run with , it trains with no problems. However, when using , rllib throws an error pasted below. It looks like this issue was previously discussed here: &lt;denchmark-link:https://github.com/ray-project/ray/issues/4866&gt;#4866&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/ray-project/ray/pull/4869&gt;#4869&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/home/esquires/.adt/venv-adt/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 515, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File "/home/esquires/.adt/venv-adt/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py", line 351, in fetch_result
    result = ray.get(trial_future[0])
  File "/home/esquires/.adt/venv-adt/lib/python3.6/site-packages/ray/worker.py", line 2121, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(InvalidArgumentError): �[36mray_worker�[39m (pid=30940, host=esquires-pc3)
  File "/home/esquires/.adt/venv-adt/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1356, in _do_call
    return fn(*args)
  File "/home/esquires/.adt/venv-adt/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1341, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/home/esquires/.adt/venv-adt/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1429, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [2] vs. [128]
	 [[{{node default_policy_1/tower_1/gradients_1/default_policy_1/tower_1/add_4_grad/BroadcastGradientArgs}}]]

During handling of the above exception, another exception occurred:

�[36mray_worker�[39m (pid=30940, host=esquires-pc3)
  File "/home/esquires/.adt/venv-adt/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 421, in train
    raise e
  File "/home/esquires/.adt/venv-adt/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 407, in train
    result = Trainable.train(self)
  File "/home/esquires/.adt/venv-adt/lib/python3.6/site-packages/ray/tune/trainable.py", line 176, in train
    result = self._train()
  File "/home/esquires/.adt/venv-adt/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 129, in _train
    fetches = self.optimizer.step()
  File "/home/esquires/.adt/venv-adt/lib/python3.6/site-packages/ray/rllib/optimizers/multi_gpu_optimizer.py", line 204, in step
    self.per_device_batch_size)
  File "/home/esquires/.adt/venv-adt/lib/python3.6/site-packages/ray/rllib/optimizers/multi_gpu_impl.py", line 260, in optimize
    return sess.run(fetches, feed_dict=feed_dict)
  File "/home/esquires/.adt/venv-adt/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 950, in run
    run_metadata_ptr)
  File "/home/esquires/.adt/venv-adt/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1173, in _run
    feed_dict_tensor, options, run_metadata)
  File "/home/esquires/.adt/venv-adt/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1350, in _do_run
    run_metadata)
  File "/home/esquires/.adt/venv-adt/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1370, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [2] vs. [128]
	 [[node default_policy_1/tower_1/gradients_1/default_policy_1/tower_1/add_4_grad/BroadcastGradientArgs (defined at /ray/rllib/agents/ppo/ppo_policy.py:211) ]]

Original stack trace for 'default_policy_1/tower_1/gradients_1/default_policy_1/tower_1/add_4_grad/BroadcastGradientArgs':
  File "/ray/workers/default_worker.py", line 98, in &lt;module&gt;
    ray.worker.global_worker.main_loop()
  File "/ray/rllib/agents/trainer_template.py", line 90, in __init__
    Trainer.__init__(self, config, env, logger_creator)
  File "/ray/rllib/agents/trainer.py", line 372, in __init__
    Trainable.__init__(self, config, logger_creator)
  File "/ray/tune/trainable.py", line 96, in __init__
    self._setup(copy.deepcopy(self.config))
  File "/ray/rllib/agents/trainer.py", line 492, in _setup
    self._init(self.config, self.env_creator)
  File "/ray/rllib/agents/trainer_template.py", line 111, in _init
    self.optimizer = make_policy_optimizer(self.workers, config)
  File "/ray/rllib/agents/ppo/ppo.py", line 89, in choose_policy_optimizer
    shuffle_sequences=config["shuffle_sequences"])
  File "/ray/rllib/optimizers/multi_gpu_optimizer.py", line 123, in __init__
    self.per_device_batch_size, policy.copy))
  File "/ray/rllib/optimizers/multi_gpu_impl.py", line 95, in __init__
    len(input_placeholders)))
  File "/ray/rllib/optimizers/multi_gpu_impl.py", line 297, in _setup_device
    graph_obj._loss)
  File "/ray/rllib/policy/tf_policy_template.py", line 168, in gradients
    return gradients_fn(self, optimizer, loss)
  File "/ray/rllib/agents/ppo/ppo_policy.py", line 211, in clip_gradients
    return optimizer.compute_gradients(loss, variables)
  File "/tensorflow/python/training/optimizer.py", line 512, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File "/tensorflow/python/ops/gradients_impl.py", line 158, in gradients
    unconnected_gradients)
  File "/tensorflow/python/ops/gradients_util.py", line 731, in _GradientsHelper
    lambda: grad_fn(op, *out_grads))
  File "/tensorflow/python/ops/gradients_util.py", line 403, in _MaybeCompile
    return grad_fn()  # Exit early
  File "/tensorflow/python/ops/gradients_util.py", line 731, in &lt;lambda&gt;
    lambda: grad_fn(op, *out_grads))
  File "/tensorflow/python/ops/math_grad.py", line 1004, in _AddGrad
    rx, ry = gen_array_ops.broadcast_gradient_args(sx, sy)
  File "/tensorflow/python/ops/gen_array_ops.py", line 829, in broadcast_gradient_args
    "BroadcastGradientArgs", s0=s0, s1=s1, name=name)
  File "/tensorflow/python/framework/op_def_library.py", line 788, in _apply_op_helper
    op_def=op_def)
  File "/tensorflow/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/tensorflow/python/framework/ops.py", line 3616, in create_op
    op_def=op_def)
  File "/tensorflow/python/framework/ops.py", line 2005, in __init__
    self._traceback = tf_stack.extract_stack()

...which was originally created as op 'default_policy_1/tower_1/add_4', defined at:
  File "/ray/workers/default_worker.py", line 98, in &lt;module&gt;
    ray.worker.global_worker.main_loop()
[elided 11 identical lines from previous traceback]
  File "/ray/rllib/optimizers/multi_gpu_impl.py", line 95, in __init__
    len(input_placeholders)))
  File "/ray/rllib/optimizers/multi_gpu_impl.py", line 295, in _setup_device
    graph_obj = self.build_graph(device_input_slices)
  File "/ray/rllib/policy/dynamic_tf_policy.py", line 237, in copy
    loss = instance._do_loss_init(input_dict)
  File "/ray/rllib/policy/dynamic_tf_policy.py", line 353, in _do_loss_init
    loss = self._loss_fn(self, self.model, self.dist_class, train_batch)
  File "/ray/rllib/agents/ppo/ppo_policy.py", line 146, in ppo_surrogate_loss
    model_config=policy.config["model"])
  File "/ray/rllib/agents/ppo/ppo_policy.py", line 106, in __init__
    vf_loss_coeff * vf_loss - entropy_coeff * curr_entropy)
  File "/tensorflow/python/ops/math_ops.py", line 884, in binary_op_wrapper
    return func(x, y, name=name)
  File "/tensorflow/python/ops/gen_math_ops.py", line 387, in add
    "Add", x=x, y=y, name=name)
  File "/tensorflow/python/framework/op_def_library.py", line 788, in _apply_op_helper
    op_def=op_def)
  File "/tensorflow/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/tensorflow/python/framework/ops.py", line 3616, in create_op
    op_def=op_def)
  File "/tensorflow/python/framework/ops.py", line 2005, in __init__
    self._traceback = tf_stack.extract_stack()
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='esquires' date='2019-12-06T02:00:08Z'>
		I believe our MultIDiscrete support is a bit half baked. Maybe try Tuple([Discrete(2), Discrete(4)]) instead? This should be an equivalent tuple space.
		</comment>
		<comment id='2' author='esquires' date='2019-12-06T11:29:25Z'>
		Thanks! That works for me.
		</comment>
		<comment id='3' author='esquires' date='2020-02-14T08:47:06Z'>
		This issue post saved my life. I spent 1 hour and didn't have any clue to solve this problem.
I think this is important, maybe we should put this in the document so others can reference.
		</comment>
		<comment id='4' author='esquires' date='2020-04-17T08:11:31Z'>
		Hi! I found a similar issue now when trying to run PPO or A3C with use_pytorch = True:
&lt;denchmark-code&gt;2020-04-17 09:56:48,264	ERROR trial_runner.py:521 -- Trial PPO_dummy_dict_env:DummyDictEnv-v0_00000: Error processing event.
Traceback (most recent call last):
  File "/home/twrona/dev/ray-with-backyard/venv/lib/python3.7/site-packages/ray/tune/trial_runner.py", line 467, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File "/home/twrona/dev/ray-with-backyard/venv/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py", line 381, in fetch_result
    result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)
  File "/home/twrona/dev/ray-with-backyard/venv/lib/python3.7/site-packages/ray/worker.py", line 1513, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): ray::PPO.train() (pid=22699, ip=192.168.1.15)
  File "python/ray/_raylet.pyx", line 452, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 407, in ray._raylet.execute_task.function_executor
  File "/home/twrona/dev/ray-with-backyard/venv/lib/python3.7/site-packages/ray/rllib/agents/trainer.py", line 502, in train
    raise e
  File "/home/twrona/dev/ray-with-backyard/venv/lib/python3.7/site-packages/ray/rllib/agents/trainer.py", line 491, in train
    result = Trainable.train(self)
  File "/home/twrona/dev/ray-with-backyard/venv/lib/python3.7/site-packages/ray/tune/trainable.py", line 261, in train
    result = self._train()
  File "/home/twrona/dev/ray-with-backyard/venv/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py", line 150, in _train
    fetches = self.optimizer.step()
  File "/home/twrona/dev/ray-with-backyard/venv/lib/python3.7/site-packages/ray/rllib/optimizers/sync_samples_optimizer.py", line 59, in step
    for e in self.workers.remote_workers()
  File "/home/twrona/dev/ray-with-backyard/venv/lib/python3.7/site-packages/ray/rllib/utils/memory.py", line 29, in ray_get_and_free
    result = ray.get(object_ids)
ray.exceptions.RayTaskError(TypeError): ray::RolloutWorker.sample() (pid=22701, ip=192.168.1.15)
  File "python/ray/_raylet.pyx", line 452, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 407, in ray._raylet.execute_task.function_executor
  File "/home/twrona/dev/ray-with-backyard/venv/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py", line 492, in sample
    batches = [self.input_reader.next()]
  File "/home/twrona/dev/ray-with-backyard/venv/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py", line 53, in next
    batches = [self.get_data()]
  File "/home/twrona/dev/ray-with-backyard/venv/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py", line 96, in get_data
    item = next(self.rollout_provider)
  File "/home/twrona/dev/ray-with-backyard/venv/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py", line 354, in _env_runner
    active_episodes)
  File "/home/twrona/dev/ray-with-backyard/venv/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py", line 602, in _do_policy_eval
    timestep=policy.global_timestep)
  File "/home/twrona/dev/ray-with-backyard/venv/lib/python3.7/site-packages/ray/rllib/policy/torch_policy.py", line 96, in compute_actions
    timestep, explore)
  File "/home/twrona/dev/ray-with-backyard/venv/lib/python3.7/site-packages/ray/rllib/utils/exploration/stochastic_sampling.py", line 66, in get_exploration_action
    action_dist = action_dist_class(distribution_inputs, model, **kwargs)
  File "/home/twrona/dev/ray-with-backyard/venv/lib/python3.7/site-packages/ray/rllib/models/torch/torch_action_dist.py", line 71, in __init__
    inputs_split = self.inputs.split(input_lens, dim=1)
  File "/home/twrona/dev/ray-with-backyard/venv/lib/python3.7/site-packages/torch/tensor.py", line 377, in split
    return super(Tensor, self).split_with_sizes(split_size, dim)
TypeError: split_with_sizes(): argument 'split_sizes' (position 1) must be tuple of ints, not numpy.ndarray
&lt;/denchmark-code&gt;

and here's my environment:
class DummyDictEnv(gym.Env):
    observation_space = gym.spaces.Dict(OrderedDict(
        key1=gym.spaces.Box(low=-1., high=1., shape=(5,)),
        key2=gym.spaces.Box(low=-1, high=1., shape=(10, 2))
    ))
    # action_space = gym.spaces.Discrete(10)
    action_space = gym.spaces.MultiDiscrete([10, 4])

    def step(self, action):
        obs = self.observation_space.sample()
        return obs, 0., False, {}

    def reset(self):
        obs = self.observation_space.sample()
        return obs

    def render(self, mode='human'):
        pass
Unfortunately, the solution with Tuple space doesn't work for PyTorch:
&lt;denchmark-code&gt;action_space = gym.spaces.Tuple([gym.spaces.Discrete(10), gym.spaces.Discrete(4)])
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;NotImplementedError: Tuple action spaces not supported for Pytorch.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='esquires' date='2020-04-17T11:13:40Z'>
		Seems like MultiDiscrete works for PyTorch with latest master.
		</comment>
		<comment id='6' author='esquires' date='2020-04-18T04:30:46Z'>
		Glad that you solved your problem. It's good to know that we can use MultiDiscrete space now. I'll find sometime to try it. Thank you for your information.
		</comment>
	</comments>
</bug>