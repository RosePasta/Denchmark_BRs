<bug id='8713' author='wuisawesome' open_date='2020-06-01T21:26:57Z' closed_time='2020-06-23T01:26:46Z'>
	<summary>[ParallelIterator] for_each_concur leaks memory</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

for_each_concur appears to leak memory. After iterating over items, they appear to remain pinned in memory.
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
&lt;denchmark-code&gt;import ray
from ray.util.iter import from_items
import numpy as np

ray.init()

def expand(x):
    print("Expanding...")
    return np.array([x for _ in range(2**26)])

def reduce(x):
    return np.sum(x)/x.shape[0]

it = from_items(list(range(1000)))
it = it.for_each(expand, max_concurrency=2)

for x in it.gather_async():
    print(reduce(x))
&lt;/denchmark-code&gt;

If we cannot run your script, we cannot fix your issue.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='wuisawesome' date='2020-06-03T03:25:35Z'>
		&lt;denchmark-link:https://github.com/wuisawesome&gt;@wuisawesome&lt;/denchmark-link&gt;
 Would you mind taking over this issue? For P1 tasks, each assignees will be responsible for resolving them within a month.
		</comment>
	</comments>
</bug>