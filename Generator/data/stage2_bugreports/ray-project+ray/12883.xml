<bug id='12883' author='richardliaw' open_date='2020-12-15T17:12:41Z' closed_time='2020-12-17T17:41:49Z'>
	<summary>[k8s] printenv occasionally fails</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

I'm running a kubernetes cluster with my docker image (richardliaw/horovod) and I get this:
&lt;denchmark-code&gt;  Updating cluster configuration. [hash=0194a452ebd82e0ab6eade7b4dd3a4f4f775d5de]
  New status: syncing-files
  [2/7] Processing file mounts
2020-12-15 09:03:24,116	INFO command_runner.py:169 -- NodeUpdater: ray-head-m5nvd: Running kubectl -n ray exec -it ray-head-m5nvd -- bash --login -c -i 'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; (mkdir -p ~)'
Error from server: error dialing backend: EOF
  New status: update-failed
  !!!
  Setup command `kubectl -n ray exec -it ray-head-m5nvd -- printenv HOME` failed with exit code 1. stderr:
  !!!

Exception in thread Thread-1:
Traceback (most recent call last):
  File "/Users/rliaw/miniconda3/lib/python3.7/threading.py", line 926, in _bootstrap_inner
    self.run()
  File "/Users/rliaw/miniconda3/lib/python3.7/site-packages/ray/autoscaler/_private/updater.py", line 124, in run
    self.do_update()
  File "/Users/rliaw/miniconda3/lib/python3.7/site-packages/ray/autoscaler/_private/updater.py", line 312, in do_update
    self.rsync_up, step_numbers=(1, NUM_SETUP_STEPS))
  File "/Users/rliaw/miniconda3/lib/python3.7/site-packages/ray/autoscaler/_private/updater.py", line 210, in sync_file_mounts
    do_sync(remote_path, local_path)
  File "/Users/rliaw/miniconda3/lib/python3.7/site-packages/ray/autoscaler/_private/updater.py", line 198, in do_sync
    local_path, remote_path, docker_mount_if_possible=True)
  File "/Users/rliaw/miniconda3/lib/python3.7/site-packages/ray/autoscaler/_private/updater.py", line 446, in rsync_up
    self.cmd_runner.run_rsync_up(source, target, options=options)
  File "/Users/rliaw/miniconda3/lib/python3.7/site-packages/ray/autoscaler/_private/command_runner.py", line 197, in run_rsync_up
    target = self._home + target[1:]
  File "/Users/rliaw/miniconda3/lib/python3.7/site-packages/ray/autoscaler/_private/command_runner.py", line 258, in _home
    raw_out = self.process_runner.check_output(joined_cmd, shell=True)
  File "/Users/rliaw/miniconda3/lib/python3.7/subprocess.py", line 395, in check_output
    **kwargs).stdout
  File "/Users/rliaw/miniconda3/lib/python3.7/subprocess.py", line 487, in run
    output=stdout, stderr=stderr)
subprocess.CalledProcessError: Command 'kubectl -n ray exec -it ray-head-m5nvd -- printenv HOME' returned non-zero exit status 1.

  Failed to setup head node.
&lt;/denchmark-code&gt;

during node startup.
cc @Gekho457  &lt;denchmark-link:https://github.com/ijrsvt&gt;@ijrsvt&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='richardliaw' date='2020-12-16T05:12:37Z'>
		Weird. What happens if you directly launch a pod with the same spec as the one in your head node config and then try the same kubectl exec command with that pod?
		</comment>
		<comment id='2' author='richardliaw' date='2020-12-16T05:16:03Z'>
		It's not easily reproducible; by kubectl exec'ing after this failure, you'll get a exit status 0 (successful execution)
		</comment>
		<comment id='3' author='richardliaw' date='2020-12-16T05:43:48Z'>
		Is it correct that you’re running ray up with a running cluster?
Any other useful context you can think of?
I wonder if just having KubernetesCommandRunner retry that command a couple times before giving up would be a passable temporary bandaid on this.
It would be great if we could get more detailed info on what happened than exit status 1 — need to look into the logging logic.
		</comment>
		<comment id='4' author='richardliaw' date='2020-12-16T08:10:52Z'>
		Hmm, looks like this is a GKE issue: &lt;denchmark-link:https://gitlab.com/gitlab-org/gitlab-runner/-/issues/3247&gt;https://gitlab.com/gitlab-org/gitlab-runner/-/issues/3247&lt;/denchmark-link&gt;

Maybe a retry makes sense?
		</comment>
		<comment id='5' author='richardliaw' date='2020-12-16T13:36:50Z'>
		yep, will add retries
Looks like it might be the same problem that arose in the autoscaler here: &lt;denchmark-link:https://github.com/ray-project/ray/issues/12255&gt;#12255&lt;/denchmark-link&gt;

except now it’s impacting the cluster launcher. I’m curious just how long these random API server outages are.
		</comment>
	</comments>
</bug>