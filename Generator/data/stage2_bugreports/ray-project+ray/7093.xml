<bug id='7093' author='waldroje' open_date='2020-02-08T16:04:29Z' closed_time='2020-02-18T17:51:21Z'>
	<summary>[tune][rllib]Experiments failing under 0.8.1</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

I upgraded to 0.8.1, and in running an experiment that I have no trouble completing under other versions with my custom env &amp; model, it failed at a fairly early stage, and did not seem to recover. I can't provide a script to duplicate, as I don't know what kind of toy model to put together, but here is the error message from the log and console.
&lt;denchmark-code&gt;== Status ==                                                                                                                                             [48/1841]
Memory usage on this node: 15.1/62.7 GiB
Using FIFO scheduling algorithm.
Resources requested: 65/72 CPUs, 1.0/1 GPUs, 0.0/76.86 GiB heap, 0.0/23.19 GiB objects
Result logdir: /home/svc-tai-dev/ray_results/2020/2/8/033658597943/CT1-W-Z240-LINUX1/front-time
Number of trials: 1 (1 RUNNING)
+------------------------------+----------+-------------------+--------+------------------+-------------+----------+
| Trial name                   | status   | loc               |   iter |   total time (s) |   timesteps |   reward |
|------------------------------+----------+-------------------+--------+------------------+-------------+----------|
| IMPALA_make_fpc_env_42c42a42 | RUNNING  | 10.36.65.54:25104 |    297 |          12288.8 |    34520832 | 0.954869 |
+------------------------------+----------+-------------------+--------+------------------+-------------+----------+


2020-02-08 02:02:33,561 ERROR trial_runner.py:480 -- Trial IMPALA_make_fpc_env_42c42a42: Error processing event.
Traceback (most recent call last):
  File "/home/svc-tai-dev/virt/algo_36/lib64/python3.6/site-packages/ray/tune/trial_runner.py", line 424, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File "/home/svc-tai-dev/virt/algo_36/lib64/python3.6/site-packages/ray/tune/ray_trial_executor.py", line 377, in fetch_result
    result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)
  File "/home/svc-tai-dev/virt/algo_36/lib64/python3.6/site-packages/ray/worker.py", line 1492, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError: ray::IMPALA.train() (pid=25104, ip=10.36.65.54)
  File "python/ray/_raylet.pyx", line 643, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 623, in function_executor
  File "/home/svc-tai-dev/virt/algo_36/lib64/python3.6/site-packages/ray/rllib/agents/trainer.py", line 443, in train
    raise e
  File "/home/svc-tai-dev/virt/algo_36/lib64/python3.6/site-packages/ray/rllib/agents/trainer.py", line 432, in train
    result = Trainable.train(self)
  File "/home/svc-tai-dev/virt/algo_36/lib64/python3.6/site-packages/ray/tune/trainable.py", line 254, in train
    result = self._train()
  File "/home/svc-tai-dev/virt/algo_36/lib64/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 122, in _train
    fetches = self.optimizer.step()
  File "/home/svc-tai-dev/virt/algo_36/lib64/python3.6/site-packages/ray/rllib/optimizers/async_samples_optimizer.py", line 131, in step
    sample_timesteps, train_timesteps = self._step()
  File "/home/svc-tai-dev/virt/algo_36/lib64/python3.6/site-packages/ray/rllib/optimizers/async_samples_optimizer.py", line 173, in _step
    for train_batch in self.aggregator.iter_train_batches():
  File "/home/svc-tai-dev/virt/algo_36/lib64/python3.6/site-packages/ray/rllib/optimizers/aso_aggregator.py", line 113, in iter_train_batches
    blocking_wait=True, max_yield=max_yield)):
  File "/home/svc-tai-dev/virt/algo_36/lib64/python3.6/site-packages/ray/rllib/optimizers/aso_aggregator.py", line 166, in _augment_with_replay
    sample_batch = ray_get_and_free(sample_batch)
  File "/home/svc-tai-dev/virt/algo_36/lib64/python3.6/site-packages/ray/rllib/utils/memory.py", line 29, in ray_get_and_free
    result = ray.get(object_ids)
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.

&lt;/denchmark-code&gt;

In the other logs, I see one python-worker-* log is very large, there are lots of lines such as
I0208 02:02:35.748330 25198 direct_actor_transport.cc:101] Failing pending tasks for actor d03b8d120100
Ray version and other system information (Python version, TensorFlow version, OS):
Ray 0.8.1, Python 3.6.8, TF 1.14
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
If we cannot run your script, we cannot fix your issue.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='waldroje' date='2020-02-09T03:13:08Z'>
		Do you have some firewall blocking certain port ranges? It sounds like gRPC is failing to connect actors
Also, please include the full logs.
		</comment>
		<comment id='2' author='waldroje' date='2020-02-10T13:30:47Z'>
		So yes, this clearly is something wrong in my local setup, as after you said that, I launched the same process in AWS and it's running fine....
It's not clear how I attach logs, as it wouldn't take the directory or the files as they are... I changed extension on log to get them to attach.. and one was to big so I tailed it to a new file...
&lt;denchmark-link:https://github.com/ray-project/ray/files/4180993/python-worker-83fdd37a870e042e5d26b5b386dfeeed7e905cf420200209-183613.11195.log&gt;python-worker-83fdd37a870e042e5d26b5b386dfeeed7e905cf420200209-183613.11195.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4180994/python-worker-84b04f5f3243b7d812861b258485021fdbbad5af20200209-183613.11194.log&gt;python-worker-84b04f5f3243b7d812861b258485021fdbbad5af20200209-183613.11194.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4180995/python-worker-1455c932b82f52d02aa0230f83c5f41e5a1e41b520200209-183613.11196.log&gt;python-worker-1455c932b82f52d02aa0230f83c5f41e5a1e41b520200209-183613.11196.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4180996/python-worker-f496a23fb606caf771408369cbc2cbf6afb77ba720200209-183613.11197.log&gt;python-worker-f496a23fb606caf771408369cbc2cbf6afb77ba720200209-183613.11197.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4180997/raylet.out.log&gt;raylet.out.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4180998/raylet_monitor.err.log&gt;raylet_monitor.err.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4180999/raylet_monitor.out.log&gt;raylet_monitor.out.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4181000/redis.out.log&gt;redis.out.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4181001/redis-shard_0.err.log&gt;redis-shard_0.err.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4181002/redis-shard_0.out.log&gt;redis-shard_0.out.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4181003/tailed-python-worker.log&gt;tailed-python-worker.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4181004/worker-0d14e77103d383e27c0289cbedc72a5d22b10105.err.log&gt;worker-0d14e77103d383e27c0289cbedc72a5d22b10105.err.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4181005/worker-0d14e77103d383e27c0289cbedc72a5d22b10105.out.log&gt;worker-0d14e77103d383e27c0289cbedc72a5d22b10105.out.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4181006/worker-8c317309e9883685a4dbaa10e9be0aa81f82f5ee.err.log&gt;worker-8c317309e9883685a4dbaa10e9be0aa81f82f5ee.err.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4181008/worker-8c317309e9883685a4dbaa10e9be0aa81f82f5ee.out.log&gt;worker-8c317309e9883685a4dbaa10e9be0aa81f82f5ee.out.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4181009/worker-9be88de4ae914d129d7f7f755e717b98d25bdeb1.err.log&gt;worker-9be88de4ae914d129d7f7f755e717b98d25bdeb1.err.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4181010/worker-9be88de4ae914d129d7f7f755e717b98d25bdeb1.out.log&gt;worker-9be88de4ae914d129d7f7f755e717b98d25bdeb1.out.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4181011/worker-9e2d5b1d7d3a63a5f06dc28d5479459462384361.err.log&gt;worker-9e2d5b1d7d3a63a5f06dc28d5479459462384361.err.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4181012/worker-9e2d5b1d7d3a63a5f06dc28d5479459462384361.out.log&gt;worker-9e2d5b1d7d3a63a5f06dc28d5479459462384361.out.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4181013/worker-87298e51cf7e4ba34acec35f6f6a8b85624269b0.err.log&gt;worker-87298e51cf7e4ba34acec35f6f6a8b85624269b0.err.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4181014/worker-87298e51cf7e4ba34acec35f6f6a8b85624269b0.out.log&gt;worker-87298e51cf7e4ba34acec35f6f6a8b85624269b0.out.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4181015/worker-b5eabd15182b7fe63cbc9b4a74321edb3610883c.err.log&gt;worker-b5eabd15182b7fe63cbc9b4a74321edb3610883c.err.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4181016/worker-b5eabd15182b7fe63cbc9b4a74321edb3610883c.out.log&gt;worker-b5eabd15182b7fe63cbc9b4a74321edb3610883c.out.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4181017/worker-c3a778924689a5083d98f463ac35cc6f79de1de0.err.log&gt;worker-c3a778924689a5083d98f463ac35cc6f79de1de0.err.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4181018/worker-c3a778924689a5083d98f463ac35cc6f79de1de0.out.log&gt;worker-c3a778924689a5083d98f463ac35cc6f79de1de0.out.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4181019/worker-d5bebd3e12119a4821e376b9f4d4af8af4930608.err.log&gt;worker-d5bebd3e12119a4821e376b9f4d4af8af4930608.err.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4181020/worker-d5bebd3e12119a4821e376b9f4d4af8af4930608.out.log&gt;worker-d5bebd3e12119a4821e376b9f4d4af8af4930608.out.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4181021/dashboard.err.log&gt;dashboard.err.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4181022/dashboard.out.log&gt;dashboard.out.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4181023/monitor.err.log&gt;monitor.err.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4181024/monitor.out.log&gt;monitor.out.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4181025/plasma_store.err.log&gt;plasma_store.err.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4181026/python-driver-0100ffffffffffffffffffffffffffffffffffff20200209-183940.11556.log&gt;python-driver-0100ffffffffffffffffffffffffffffffffffff20200209-183940.11556.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4181027/python-worker-3b961442a0ab7647d8f2bc4e1bc4780ae480da1420200209-183613.11199.log&gt;python-worker-3b961442a0ab7647d8f2bc4e1bc4780ae480da1420200209-183613.11199.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4181028/python-worker-5bd104b3fc472d76e55a9fcf1d7786c67c14da5420200209-183613.11198.log&gt;python-worker-5bd104b3fc472d76e55a9fcf1d7786c67c14da5420200209-183613.11198.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4181029/python-worker-69ebfc766355c79116bea43d262a5ec76ad471d820200209-183613.11201.log&gt;python-worker-69ebfc766355c79116bea43d262a5ec76ad471d820200209-183613.11201.log&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='waldroje' date='2020-02-11T13:07:19Z'>
		Following up on this, on AWS I was running similar experiment, but using pbt... 8 samples... each sample had run for about 3600s time_total, and about 4-5mm steps each... when it threw this error
&lt;denchmark-code&gt;`2020-02-11 02:20:43,914 ERROR worker.py:1003 -- Possible unhandled error from worker: ray::IDLE (pid=12254, ip=172.31.8.45)                                                         File "python/ray/_raylet.pyx", line 633, in ray._raylet.execute_task                                                                                                              File "python/ray/_raylet.pyx", line 634, in ray._raylet.execute_task                                                                                                              File "python/ray/_raylet.pyx", line 519, in ray._raylet.deserialize_args                                                                                                        ray.exceptions.UnreconstructableError: Object ffffffffffffffffffffffff0100008027000000 is lost (either LRU evicted or deleted by user) and cannot be reconstructed. Try increasin g the object store memory available with ray.init(object_store_memory=) or setting object store limits with `ray.remote(object_store_memory=). See also: https://ray .readthedocs.io/en/latest/memory-management.html
--
&lt;/denchmark-code&gt;

It's a p3..2xlarge head with a couple worker nodes... the monitor still shows 2/8 cpu working and 1/1 gpus, but it never actually stop the workers nodes... they have remained up even though there is zero activity. Each trial was using 32cpu, 0.50 gpu,
The last logs to be written to were raylet.out ...
&lt;denchmark-code&gt;W0211 02:12:23.778187  3020 node_manager.cc:3114] Worker 2aaabd52d59b4113af731139224ad04b968290e7 failed. Unpinning object 3d1c8ffaefcd281ebf85413401000080c9010000               I0211 02:12:26.109174  3020 node_manager.cc:2445] Finishing assigned actor task                                                                                                   W0211 02:12:26.109429  3020 node_manager.cc:2421] Actor not in reconstructing state, most likely it died before creation handler could run. Actor state is 2                      W0211 02:15:11.398264  3020 node_manager.cc:3114] Worker fef4d6e0a8c12ee41e49e933f156adcdb50a4e5b failed. Unpinning object ed001ae7a49a0bc34046ece2010000806e020000               I0211 02:15:16.672072  3020 node_manager.cc:2445] Finishing assigned actor task                                                                                                   W0211 02:20:37.299346  3020 node_manager.cc:3114] Worker bea351aecc04034d7752252c4e9109c697ae9e7a failed. Unpinning object b1b411b34c40483959cd9a06010000807e020000               I0211 02:20:38.904994  3020 node_manager.cc:2445] Finishing assigned actor task                                                                                                   W0211 02:20:38.905251  3020 node_manager.cc:2421] Actor not in reconstructing state, most likely it died before creation handler could run. Actor state is 2
--

&lt;/denchmark-code&gt;

plasma.err
&lt;denchmark-code&gt;I0211 02:13:03.968765  3019 eviction_policy.cc:134] There is not enough space to create this object, so evicting 60016 objects to free up 3798527536 bytes. The number of bytes i n use (before this eviction) is 18992575382.                                                                                                                                      I0211 02:15:11.305348  3019 store.cc:738] Disconnecting client on fd 17                                                                                                           I0211 02:15:11.307440  3019 store.cc:738] Disconnecting client on fd 16                                                                                                           I0211 02:15:11.310752  3019 store.cc:738] Disconnecting client on fd 10                                                                                                           I0211 02:20:37.206682  3019 store.cc:738] Disconnecting client on fd 19
--
&lt;/denchmark-code&gt;

python-driver
&lt;denchmark-code&gt;W0211 02:20:06.079690 31106 plasma_store_provider.cc:60] Trying to put an object that already existed in plasma: 0252ce15dc80dcaa4046ece2010000c801000000.                        W0211 02:20:06.094431 31106 plasma_store_provider.cc:60] Trying to put an object that already existed in plasma: d258dab2b6ab3552f3640c7c010000c801000000.                        W0211 02:20:06.113014 31106 plasma_store_provider.cc:60] Trying to put an object that already existed in plasma: c800887315bcca0bde6e78fc010000c801000000.                        I0211 02:20:37.202998 31132 direct_actor_transport.cc:101] Failing pending tasks for actor 59cd9a060100
--

&lt;/denchmark-code&gt;

python-worker-****
&lt;denchmark-code&gt;I0211 02:20:37.201301 12333 direct_actor_transport.cc:101] Failing pending tasks for actor a7c6a9b00100                                                                           I0211 02:20:37.201337 12333 core_worker.cc:828] received notification on actor, state=2, actor_id: a7c6a9b00100, ip address: 172.31.1.216, port: 46365, worker_id: 1678dcf6812ade 979d6101302c5d529f0b23a771, raylet_id: 5c80d5f83d8817ceb047fdd14501b089d3750b84                                                                                                   I0211 02:20:37.201592 12333 direct_actor_transport.cc:101] Failing pending tasks for actor 7cf199a90100                                                                           I0211 02:20:37.201618 12333 core_worker.cc:828] received notification on actor, state=2, actor_id: 7cf199a90100, ip address: 172.31.1.216, port: 42711, worker_id: 1bc706f1218eea 3cf545ac90fa903f3daf9d9fbb, raylet_id: 5c80d5f83d8817ceb047fdd14501b089d3750b84                                                                                                   I0211 02:20:37.202276 12253 redis_gcs_client.cc:164] RedisGcsClient Disconnected.
--

&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='waldroje' date='2020-02-11T19:08:09Z'>
		Cc &lt;denchmark-link:https://github.com/edoakes&gt;@edoakes&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='waldroje' date='2020-02-18T17:51:21Z'>
		I believe &lt;denchmark-link:https://github.com/ray-project/ray/pull/7201&gt;#7201&lt;/denchmark-link&gt;
 fixes these issues
		</comment>
	</comments>
</bug>