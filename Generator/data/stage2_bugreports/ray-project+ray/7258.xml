<bug id='7258' author='waldroje' open_date='2020-02-21T03:17:24Z' closed_time='2020-04-23T02:16:31Z'>
	<summary>[tune][rllib] "node manager has mistakenly been marked dead by the monitor" - from long running PBT</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

I was running PBT trials, and I thought after &lt;denchmark-link:https://github.com/ray-project/ray/pull/7201&gt;#7201&lt;/denchmark-link&gt;
 it might succeed, but after running for a decent amount of time, it crashed with the following.
&lt;denchmark-code&gt;== Status == 
Memory usage on this node: 58.9/184.7 GiB
PopulationBasedTraining: 15 checkpoints, 10 perturbs
Resources requested: 65/144 CPUs, 0.0/0 GPUs, 0.0/294.97 GiB heap, 0.0/43.65 GiB objects
Result logdir: /home/ubuntu/ray_results/2020/2/20/220836984702/ip-172-31-5-2/front-time
Number of trials: 8 (7 PAUSED, 1 RUNNING)
+------------------------------+----------+------------------+--------+------------------+-------------+-----------+
| Trial name                   | status   | loc              |   iter |   total time (s) |   timesteps |    reward |
|------------------------------+----------+------------------+--------+------------------+-------------+-----------|
| IMPALA_make_fpc_env_8b193446 | PAUSED   |                  |     50 |          2789.58 |     4183168 |  0.930425 |
| IMPALA_make_fpc_env_8b193447 | PAUSED   |                  |     50 |          2977.83 |     4131712 | -0.544352 |
| IMPALA_make_fpc_env_8b193448 | PAUSED   |                  |     50 |          2197.65 |     4217344 | -0.161039 |
| IMPALA_make_fpc_env_8b193449 | PAUSED   |                  |     50 |          1814.8  |     5104000 |  0.203119 |
| IMPALA_make_fpc_env_8b19344a | PAUSED   |                  |     50 |          2787.9  |     4180736 | -0.470465 |
| IMPALA_make_fpc_env_8b19344b | PAUSED   |                  |     50 |          3216.9  |     4077312 | -7.46704  |
| IMPALA_make_fpc_env_8b19344c | RUNNING  | 172.31.5.2:66208 |     49 |          2022.33 |     4590464 | -0.598401 |
| IMPALA_make_fpc_env_8b19344d | PAUSED   |                  |     40 |          1456.58 |     4065408 | -2.61389  |
+------------------------------+----------+------------------+--------+------------------+-------------+-----------+                                                                                                                                                                                                                                                                                                                                                                                                    
E0221 00:59:27.506393 21075 task_manager.cc:165] Task failed: IOError: sent task to dead actor: Type=ACTOR_TASK, Language=PYTHON, function_descriptor={type=PythonFunctionDescriptor, module_name=ray.rllib.agents.trainer_template, class_name=IMPALA, function_name=stop, function_hash=}, task_id=53576960b66fdad15bba91dc0100, job_id=0100 , num_args=0, num_returns=2, actor_task_spec={actor_id=5bba91dc0100, actor_caller_id=ffffffffffffffffffffffff0100, actor_counter=11}                                    
E0221 00:59:45.873986 21150 task_manager.cc:147] 3 retries left for task fffffffffffffffff9c5fe5b0100, attempting to resubmit.                                          
E0221 00:59:45.874063 21150 core_worker.cc:196] Will resubmit task after a 5 second delay: Type=ACTOR_CREATION_TASK, Language=PYTHON, function_descriptor={type=PythonF unctionDescriptor, module_name=ray.rllib.agents.trainer_template, class_name=IMPALA, function_name=__init__, function_hash=230012600}, task_id=fffffffffffffffff9c5fe5b 0100, job_id=0100, num_args=4, num_returns=1, actor_creation_task_spec={actor_id=f9c5fe5b0100, max_reconstructions=0, is_direct_call=1, max_concurrency=1, is_asyncio_a ctor=0, is_detached=0}                                                                                                                                                  
F0221 00:59:51.171756 21150 direct_task_transport.cc:153] Lost connection with local raylet. 
Error: IOError: 14: failed to connect to all addresses                    
 *** Check failure stack trace: *** 
@     0x7feef24a5d8d  google::LogMessage::Fail()
@     0x7feef24a71fc  google::LogMessage::SendToLog()
@     0x7feef24a5a69  google::LogMessage::Flush()
@     0x7feef24a5c81  google::LogMessage::~LogMessage()
@     0x7feef2252fa9  ray::RayLog::~RayLog()
@     0x7feef213e349  ray::CoreWorkerDirectTaskSubmitter::RetryLeaseRequest()
@     0x7feef2140b07 _ZNSt17_Function_handlerIFvRKN3ray6StatusERKNS0_3rpc23RequestWorkerLeaseReplyEEZNS0_29CoreWorkerDirectTaskSubmitter24RequestNewWorkerIfNeededERKSt5tupleIIiSt6vectorINS0_8ObjectIDESaISC_EENS0_7ActorIDEEEPKNS4_7AddressEEUlS3_S7_E_E9_M_invokeERKSt9_Any_dataS3_S7_
@     0x7feef2177ea5  ray::rpc::ClientCallImpl&lt;&gt;::OnReplyReceived()                                                                                                     @     0x7feef2106993 _ZN5boost4asio6detail18completion_handlerIZN3ray3rpc17ClientCallManager29PollEventsFromCompletionQueueEiEUlvE_E11do_completeEPvPNS1_19schedul er_operationERKNS_6system10error_codeEm
@     0x7feef2104e35  boost::asio::detail::scheduler::run()
@     0x7feef210a3b3  ray::CoreWorker::RunIOService()
@     0x7fee4ce789e0  (unknown)
@     0x7feef6db56db  start_thread
@     0x7feef70ee88f  clone
Aborted (core dumped)
--
&lt;/denchmark-code&gt;

Any insight? I'm running 2 x c5.18xlarge, no gpus, as my custom model is not very deep, and I've run this successfully in the past, definitely on 0,7,7,, but have generally been getting failure ever since 0.8.0,
A couple items from logs... working up from bottom in terms of recent output.
raylet.out
&lt;denchmark-code&gt;F0221 00:59:33.108211  4371 node_manager.cc:472]  Check failed: node_id != self_node_id_ Exiting because this node manager has mistakenly been marked dead by the monitor.
--

&lt;/denchmark-code&gt;

python-driver
&lt;denchmark-code&gt;I0221 00:58:42.699936 21150 direct_actor_transport.cc:101] Failing pending tasks for actor a63d60130100
I0221 00:58:42.699982 21150 core_worker.cc:897] received notification on actor, state=2, actor_id: a63d60130100, ip address: 172.31.5.2, port: 42913, worker_id: 4537e1 998f60fa3e5576a978b514b23fa9ac5713, raylet_id: c32ef7252b22fde98d0adc211081baa441a44b37
I0221 00:58:42.723261 21150 direct_task_transport.cc:34] Connected to 172.31.5.2:38085
I0221 00:58:47.063997 21150 core_worker.cc:897] received notification on actor, state=0, actor_id: 7982f88e0100, ip address: 172.31.5.2, port: 38085, worker_id: 47caa0 115cb69ec7c8bb3f7ffd21c1b0925dbaed, raylet_id: c32ef7252b22fde98d0adc211081baa441a44b37
I0221 00:59:18.804953 21150 direct_actor_transport.cc:101] Failing pending tasks for actor 7982f88e0100
I0221 00:59:18.804987 21150 core_worker.cc:897] received notification on actor, state=2, actor_id: 7982f88e0100, ip address: 172.31.5.2, port: 38085, worker_id: 47caa0 115cb69ec7c8bb3f7ffd21c1b0925dbaed, raylet_id: c32ef7252b22fde98d0adc211081baa441a44b37
I0221 00:59:18.805001 21150 direct_actor_transport.cc:101] Failing pending tasks for actor 5bba91dc0100
I0221 00:59:18.805004 21150 core_worker.cc:897] received notification on actor, state=2, actor_id: 5bba91dc0100, ip address: 172.31.5.2, port: 37067, worker_id: 9c5f44 9e201ec97bb8ccc771ff560db5a463ef56, raylet_id: c32ef7252b22fde98d0adc211081baa441a44b37
E0221 00:59:19.567294 21075 task_manager.cc:165] Task failed: IOError: sent task to dead actor: Type=ACTOR_TASK, Language=PYTHON, function_descriptor={type=PythonFunct ionDescriptor, module_name=ray.rllib.agents.trainer_template, class_name=IMPALA, function_name=save_to_object, function_hash=}, task_id=4703b247b808e1c97982f88e0100, j ob_id=0100, num_args=0, num_returns=2, actor_task_spec={actor_id=7982f88e0100, actor_caller_id=ffffffffffffffffffffffff0100, actor_counter=3}
E0221 00:59:19.571149 21075 task_manager.cc:165] Task failed: IOError: sent task to dead actor: Type=ACTOR_TASK, Language=PYTHON, function_descriptor={type=PythonFunct ionDescriptor, module_name=ray.rllib.agents.trainer_template, class_name=IMPALA, function_name=stop, function_hash=}, task_id=4d47f048bdbaeda87982f88e0100, job_id=0100 , num_args=0, num_returns=2, actor_task_spec={actor_id=7982f88e0100, actor_caller_id=ffffffffffffffffffffffff0100, actor_counter=4}
W0221 00:59:20.355378 21075 plasma_store_provider.cc:66] Trying to put an object that already existed in plasma: ab2b067417969e8c4756862d010000c801000000.
W0221 00:59:21.368199 21075 plasma_store_provider.cc:66] Trying to put an object that already existed in plasma: 35ba21f43e8607d08778027a010000c801000000.
W0221 00:59:22.377468 21075 plasma_store_provider.cc:66] Trying to put an object that already existed in plasma: bdd16c3b71214fe0686a406a010000c801000000.
W0221 00:59:23.761042 21075 plasma_store_provider.cc:66] Trying to put an object that already existed in plasma: acd84b982f24b7602203123e010000c801000000.
W0221 00:59:25.134455 21075 plasma_store_provider.cc:66] Trying to put an object that already existed in plasma: d271658bf2072ba67132dc3a010000c801000000.
W0221 00:59:26.955770 21075 plasma_store_provider.cc:66] Trying to put an object that already existed in plasma: 3ab66d48d45f6d4ebc823686010000c801000000.
W0221 00:59:27.456938 21075 plasma_store_provider.cc:66] Trying to put an object that already existed in plasma: 525ad1491b5cce6f34efea21010000c801000000.
E0221 00:59:27.506393 21075 task_manager.cc:165] Task failed: IOError: sent task to dead actor: Type=ACTOR_TASK, Language=PYTHON, function_descriptor={type=PythonFunct ionDescriptor, module_name=ray.rllib.agents.trainer_template, class_name=IMPALA, function_name=stop, function_hash=}, task_id=53576960b66fdad15bba91dc0100, job_id=0100 , num_args=0, num_returns=2, actor_task_spec={actor_id=5bba91dc0100, actor_caller_id=ffffffffffffffffffffffff0100, actor_counter=11}
I0221 00:59:31.302026 21150 direct_task_transport.cc:34] Connected to 172.31.5.2:41377
E0221 00:59:45.873986 21150 task_manager.cc:147] 3 retries left for task fffffffffffffffff9c5fe5b0100, attempting to resubmit.
E0221 00:59:45.874063 21150 core_worker.cc:196] Will resubmit task after a 5 second delay: Type=ACTOR_CREATION_TASK, Language=PYTHON, function_descriptor={type=PythonF unctionDescriptor, module_name=ray.rllib.agents.trainer_template, class_name=IMPALA, function_name=__init__, function_hash=230012600}, task_id=fffffffffffffffff9c5fe5b 0100, job_id=0100, num_args=4, num_returns=1, actor_creation_task_spec={actor_id=f9c5fe5b0100, max_reconstructions=0, is_direct_call=1, max_concurrency=1, is_asyncio_a ctor=0, is_detached=0}
I0221 00:59:45.874127 21150 raylet_client.cc:364] Error returning worker: IOError: 14: failed to connect to all addresses
F0221 00:59:51.171756 21150 direct_task_transport.cc:153] Lost connection with local raylet. Error: IOError: 14: failed to connect to all addresses
--
&lt;/denchmark-code&gt;

Happy to attach more info if you think it will help... let me know.  I can't provide scripts as it's all off custom models/envs...
Ray version and other system information (Python version, TensorFlow version, OS):
ray 0.9.0, python 3.6.8, tensorflow 2.1, aws ubuntu 18.04
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
If we cannot run your script, we cannot fix your issue.

 I have verified my script runs in a clean environment and reproduces the issue.
[x ] I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='waldroje' date='2020-02-21T18:18:08Z'>
		cc &lt;denchmark-link:https://github.com/edoakes&gt;@edoakes&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='waldroje' date='2020-02-21T19:25:01Z'>
		&lt;denchmark-link:https://github.com/waldroje&gt;@waldroje&lt;/denchmark-link&gt;
 can you make a reproduction script with a standard gym env?
		</comment>
		<comment id='3' author='waldroje' date='2020-02-21T19:57:00Z'>
		i'll try and see if I can replicate the behavior... will revert
		</comment>
		<comment id='4' author='waldroje' date='2020-02-25T12:34:34Z'>
		So.. I'm going back to square one, and going to build up from simple cases to more complex.... I have installed 0.8.2,
I created a simple pbt with a toy gym env that I can use in more complex cases... but I'm getting errors on what appears to be checkpoint intervals on the most basic case.  It seems the fetch_result is returning a checkpoint string rather than a result dictionary
Do you get the same behavior?
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/home/svc-tai-dev/virt/algo_36/lib64/python3.6/site-packages/ray/tune/trial_runner.py", line 536, in _process_trial_save
    trial.on_checkpoint(trial.saving_to)
  File "/home/svc-tai-dev/virt/algo_36/lib64/python3.6/site-packages/ray/tune/trial.py", line 400, in on_checkpoint
    if not os.path.exists(checkpoint.value):
  File "/usr/lib64/python3.6/genericpath.py", line 19, in exists
    os.stat(path)
TypeError: stat: path should be string, bytes, os.PathLike or integer, not dict
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4249964/pbt_dqn_test.zip&gt;pbt_dqn_test.zip&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='waldroje' date='2020-02-29T17:16:35Z'>
		I tested the attached script from my previous post on the latest wheel and it still errors... any ideas on this?
		</comment>
		<comment id='6' author='waldroje' date='2020-02-29T17:58:59Z'>
		OK thanks a lot for the script! Let me try running this; I think this could be an bug with how we handle checkpoints.
cc &lt;denchmark-link:https://github.com/ujvl&gt;@ujvl&lt;/denchmark-link&gt;

BTW, can you join the Ray slack so we can communicate faster?
		</comment>
		<comment id='7' author='waldroje' date='2020-02-29T18:43:40Z'>
		fwiw... i see the problem in trial_runner._process_trial_save where
checkpoint_value = self.trial_executor.fetch_result(trial) 
where checkpoint_value is supposed to be a path... but sometimes comes back as trial results... which then cascades into the errors...
yes, I will get up on Slack...
		</comment>
		<comment id='8' author='waldroje' date='2020-03-01T10:57:43Z'>
		looks like an edge case where the checkpoint interval and perturbation interval coincide, causing PBT to restart the trial (resulting in a train call dispatch) and tune to dispatch a save call in the same step. which violates the invariant that only 1 future per trial is queued.
imo we should consider refactoring pbt in a way that isn't directly interacting with the executor. like at most it should be calling back into trial_runner
as a more immediate fix we could just check the type of the return value before figuring out how to process it (ie before calling process_{save/train/restore}). or avoid calling save altogether if there's a future for the trial currently queued.
i prefer the former but &lt;denchmark-link:https://github.com/richardliaw&gt;@richardliaw&lt;/denchmark-link&gt;
 lmk if you have any other ideas i can push a quick fix
		</comment>
		<comment id='9' author='waldroje' date='2020-03-01T23:57:37Z'>
		Shouldn't the immediate fix be that PBT has a way to call "save" without affecting the event loop?
		</comment>
		<comment id='10' author='waldroje' date='2020-03-02T00:30:04Z'>
		pbt shouldn't be responsible for FT (memory checkpoints already don't affect the event loop). if you mean give PBT a way to start/restore trials without affecting the event loop then i agree
		</comment>
		<comment id='11' author='waldroje' date='2020-03-21T15:04:20Z'>
		Is there a quick fix on this that you can suggest which would enable me to use pbt on the current master?
		</comment>
		<comment id='12' author='waldroje' date='2020-04-22T03:06:23Z'>
		We just merged a fix - so the second issue in the thread should be resolved. &lt;denchmark-link:https://github.com/waldroje&gt;@waldroje&lt;/denchmark-link&gt;
 are you still seeing this issue?
		</comment>
		<comment id='13' author='waldroje' date='2020-04-22T03:19:51Z'>
		I saw that come through and merged it in and kicked off a run earlier… so far no problems, will let you know if it finishes w/ no problems.

On Apr 21, 2020, at 11:06 PM, Richard Liaw &lt;notifications@github.com&gt; wrote:


We just merged a fix - so the second issue in the thread should be resolved. &lt;denchmark-link:https://github.com/waldroje&gt;@waldroje&lt;/denchmark-link&gt;
 &lt;&lt;denchmark-link:https://github.com/waldroje&gt;https://github.com/waldroje&lt;/denchmark-link&gt;
&gt; are you still seeing this issue?

—
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub &lt;&lt;denchmark-link:https://github.com/ray-project/ray/issues/7258#issuecomment-617521371&gt;#7258 (comment)&lt;/denchmark-link&gt;
&gt;, or unsubscribe &lt;&lt;denchmark-link:https://github.com/notifications/unsubscribe-auth/AHSHWMPA2V6FJJWNGFVGD73RNZNLZANCNFSM4KY3LNVA&gt;https://github.com/notifications/unsubscribe-auth/AHSHWMPA2V6FJJWNGFVGD73RNZNLZANCNFSM4KY3LNVA&lt;/denchmark-link&gt;
&gt;.
		</comment>
		<comment id='14' author='waldroje' date='2020-04-22T04:04:06Z'>
		Awesome! sorry for the delay.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Tue, Apr 21, 2020 at 8:20 PM waldroje ***@***.***&gt; wrote:
 I saw that come through and merged it in and kicked off a run earlier… so
 far no problems, will let you know if it finishes w/ no problems.

 On Apr 21, 2020, at 11:06 PM, Richard Liaw ***@***.***&gt;
 wrote:


 We just merged a fix - so the second issue in the thread should be
 resolved. @waldroje &lt;https://github.com/waldroje&gt; are you still seeing
 this issue?

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub &lt;
 #7258 (comment)&gt;,
 or unsubscribe &lt;
 https://github.com/notifications/unsubscribe-auth/AHSHWMPA2V6FJJWNGFVGD73RNZNLZANCNFSM4KY3LNVA
 &gt;.


 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#7258 (comment)&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/ABCRZZP6X62Q2CUW45B3MM3RNZO6HANCNFSM4KY3LNVA&gt;
 .



		</comment>
		<comment id='15' author='waldroje' date='2020-04-23T01:31:52Z'>
		No issues after running all day… much appreciated!
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Apr 22, 2020, at 12:04 AM, Richard Liaw ***@***.***&gt; wrote:

 Awesome! sorry for the delay.

 On Tue, Apr 21, 2020 at 8:20 PM waldroje ***@***.***&gt; wrote:

 &gt; I saw that come through and merged it in and kicked off a run earlier… so
 &gt; far no problems, will let you know if it finishes w/ no problems.
 &gt;
 &gt; On Apr 21, 2020, at 11:06 PM, Richard Liaw ***@***.***&gt;
 &gt; wrote:
 &gt;
 &gt;
 &gt; We just merged a fix - so the second issue in the thread should be
 &gt; resolved. @waldroje &lt;https://github.com/waldroje&gt; are you still seeing
 &gt; this issue?
 &gt;
 &gt; —
 &gt; You are receiving this because you were mentioned.
 &gt; Reply to this email directly, view it on GitHub &lt;
 &gt; #7258 (comment)&gt;,
 &gt; or unsubscribe &lt;
 &gt; https://github.com/notifications/unsubscribe-auth/AHSHWMPA2V6FJJWNGFVGD73RNZNLZANCNFSM4KY3LNVA
 &gt; &gt;.
 &gt;
 &gt;
 &gt; —
 &gt; You are receiving this because you were mentioned.
 &gt; Reply to this email directly, view it on GitHub
 &gt; &lt;#7258 (comment)&gt;,
 &gt; or unsubscribe
 &gt; &lt;https://github.com/notifications/unsubscribe-auth/ABCRZZP6X62Q2CUW45B3MM3RNZO6HANCNFSM4KY3LNVA&gt;
 &gt; .
 &gt;
 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub, or unsubscribe.



		</comment>
		<comment id='16' author='waldroje' date='2020-04-23T02:16:30Z'>
		Amazing! thanks for the long long wait.
		</comment>
	</comments>
</bug>