<bug id='7154' author='thomaschn' open_date='2020-02-13T16:22:52Z' closed_time='2020-02-18T21:27:15Z'>
	<summary>[cluster] [rllib] Performance plummeting until Error "Failed to get Object"</summary>
	<description>
I'm working on a local ray cluster with 5 nodes (160 CPU and 2 GPU in total). There, training with rllib always starts out rather stable, but after a while learning slows down to a crawl and never recovers. Eventually, the nodes in the cluster randomly spit out various amounts of the following error message:

object_buffer_pool.cc:49] Failed to get object

This problem (very slow progress and sporadically "Failed to get object") persists until the cluster is restarted. So even restarting the script or running another script does not work properly.
&lt;denchmark-link:https://user-images.githubusercontent.com/61015896/74452979-1435b300-4e82-11ea-9020-1b61dd45f50e.png&gt;&lt;/denchmark-link&gt;

The colored bars indicate the time at which the nodes print the error message. Also, note that the RAM usage is increasing as soon as the throughput is dropping.
Ray version and other system information (Python version, TensorFlow version, OS):

Ray 0.9.0.dev0 (latest nightly, but stable 0.8.1 has the same problem)
Python 3.6.10 (Conda 4.7.12)
Ubuntu 18.04 and 16.04

Ray is installed in a conda environment on all nodes of the cluster. I'm starting the cluster with a yaml file similar to &lt;denchmark-link:https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/local/example-full.yaml&gt;example-full.yaml&lt;/denchmark-link&gt;
. However, to start ray inside the conda environment, I use the following commands:
&lt;denchmark-code&gt;head_start_ray_commands: 
    - &gt;-
        source *CONDA_FILE* &amp;&amp;
        conda activate *CONDA_ENV_NAME* &amp;&amp;
        ray stop &amp;&amp; 
        ulimit -c unlimited &amp;&amp;
        export CUDA_VISIBLE_DEVICES=0,1 &amp;&amp;
        ray start --head --redis-port=6379 --autoscaling-config=~/ray_bootstrap_config.yaml --include-webui=True --num-cpus=32 --num-gpus=2

worker_start_ray_commands:
    - &gt;-
        source *CONDA_FILE* &amp;&amp;
        conda activate *CONDA_ENV_NAME* &amp;&amp;
        ray stop &amp;&amp;  
        ray start --address=$RAY_HEAD_IP:6379 --num-cpus=32
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Reproduction&lt;/denchmark-h&gt;


(yaml file from &lt;denchmark-link:https://github.com/ray-project/rl-experiments/blob/master/pong-speedrun/pong-impala-fast.yaml&gt;rl-experiments&lt;/denchmark-link&gt;
 )


 I have verified my script runs in a clean environment and reproduces the issue.


 I have verified the issue also occurs with the latest wheels.


	</description>
	<comments>
		<comment id='1' author='thomaschn' date='2020-02-13T20:12:28Z'>
		It sounds like this is due to running out of object store memory. Pong observations are fairly large and with a very large number of workers this is likely to happen.

How much memory is available on the GPU node? For high throughput training you probably want at least several tens of gigabytes.
The object store is capped at 20GB by default, you can try increasing it on each node by setting object_store_memory config on ray start or init.

		</comment>
		<comment id='2' author='thomaschn' date='2020-02-13T20:54:03Z'>
		

There are 180GB of memory available on the GPU node. However, the error occured on all other nodes (90GB each). Could too little memory on the GPU node cause this kind of error on the other nodes?


I will try this again tomorrow. Is there a rule of thumb on how much object store memory will be needed?
However, I tried increasing the object_store_memory to 50GB on each node before (with a different environment) and it made no difference. Similarly, the ram usage was stable in the beginning and suddenly increased later on, when the performance dropped. From several prior test it looked like there is no regularity in the duration (or number of steps) it takes for the problem to occur - independent of the object store cap.


		</comment>
		<comment id='3' author='thomaschn' date='2020-02-13T20:57:09Z'>
		Yeah, it could be the CPU nodes running out. 50GB sounds like quite a lot though. A good rule of thumb is the obs size * sample batch size * num_workers * 5 (fudge factor), which shouldn't be anywhere near 50GB.
If increasing the object store size makes the run last longer before stalling, then it could be a memory leak. cc &lt;denchmark-link:https://github.com/edoakes&gt;@edoakes&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/stephanie-wang&gt;@stephanie-wang&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='thomaschn' date='2020-02-13T20:58:21Z'>
		Actually can you also include the full trace for object_buffer_pool.cc:49] Failed to get object ?
		</comment>
		<comment id='5' author='thomaschn' date='2020-02-13T21:07:14Z'>
		There is no full trace, as far as the error message is concerned. I could try to increase the logging level to DEBUG and hope for more insight.
		</comment>
		<comment id='6' author='thomaschn' date='2020-02-14T10:12:04Z'>
		I'm experiencing similar issues on a cluster of 6 DGX-2 where each has 500 GB object storage. Takes a while however before the error occurs.
I'll update if I get any useful input.
		</comment>
		<comment id='7' author='thomaschn' date='2020-02-14T16:28:03Z'>
		Regarding the object store limit
I ran the same experiment with 20GB (orange), 25GB (dark blue) and 40GB (light blue) object_store_memory on all nodes (50GB caused ray to crash after a few minutes - long before the memory even had a chance to fill up). The cluster consists of 4 CPU nodes (32 cores) with 93GB memory each and 1 GPU node (32 cores + 2 GPUs) with 186GB.
Here are the results:
&lt;denchmark-link:https://user-images.githubusercontent.com/61015896/74546433-d81a5500-4f4a-11ea-80bf-c64694c2471c.png&gt;&lt;/denchmark-link&gt;


Indeed, the available object store limit influences the time it takes for the slowdown to occur
However, the slowdown does not happen when the object store limit is hit, but some time later
The Failed to get object Error happens later still: both of the blue runs were stopped as soon as it occurred. Until then, there is no additional output whatsoever.
Also, the bigger the object store the bigger the memory increase after slowdown

Regarding the error message
I could not get it to output a full trace, even with RAY_BACKEND_LOG_LEVEL=debug. However, here is the output of /tmp/ray/session_latest/logs/plasma_store.err for the 25GB run (dark blue):

WARNING: Logging before InitGoogleLogging() is written to STDERR
I0214 14:00:41.087133 20184 store.cc:1228] Allowing the Plasma store to use up to 25GB of memory.
I0214 14:00:41.087306 20184 store.cc:1255] Starting object store with directory /dev/shm and huge page support disabled
I0214 14:15:44.313051 20184 eviction_policy.cc:134] There is not enough space to create this object, so evicting 1247 objects to free up 5001957671 bytes. The number of bytes in use (before this eviction) is 24999064639.
I0214 14:18:14.711824 20184 eviction_policy.cc:134] There is not enough space to create this object, so evicting 1247 objects to free up 5001957671 bytes. The number of bytes in use (before this eviction) is 24998456512.
I0214 14:20:42.372227 20184 eviction_policy.cc:134] There is not enough space to create this object, so evicting 1247 objects to free up 5001957671 bytes. The number of bytes in use (before this eviction) is 24999006161.
I0214 14:41:49.462220 20184 store.cc:738] Disconnecting client on fd 42
I0214 14:41:49.792961 20184 store.cc:738] Disconnecting client on fd 31
[...]

For context:

object store limit is reached at 15:15:40 (0.2 on x axis)
memory usage starts growing at 14:22:49 (0.3 on x axis)
run is terminated by me at 14:41

		</comment>
		<comment id='8' author='thomaschn' date='2020-02-14T19:20:00Z'>
		Thanks, this confirms it is a Ray backend bug.
		</comment>
		<comment id='9' author='thomaschn' date='2020-02-15T01:05:37Z'>
		I'm having trouble reproducing on (TF 1.15, Ray 0.8.1, Python 3.6, p2.16xl GPU node, m4.16xl CPU node):
&lt;denchmark-link:https://user-images.githubusercontent.com/14922/74578874-f5035800-4f4b-11ea-96e0-31372d34988c.png&gt;&lt;/denchmark-link&gt;

Orange is 20GB object store size, blue is 2GB. Both seem to be operating fine past 15-20M timesteps, which has crashed for all configs above. Maybe there is some difference in TF version? There could be some issue where TF is not releasing references to objects. I am using the latest DLAMI, with this yaml:
&lt;denchmark-code&gt;pong-impala-fast:
    env: PongNoFrameskip-v4
    run: IMPALA
    config:
        sample_batch_size: 50
        train_batch_size: 1000
        num_workers: 128
        num_envs_per_worker: 5
        broadcast_interval: 20
        max_sample_requests_in_flight_per_worker: 1
        num_data_loader_buffers: 4
        num_gpus: 2
        model:
          dim: 42
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='thomaschn' date='2020-02-17T13:40:36Z'>
		I've just tried the following versions on Python 3.6 with 2GB object storage:

TF 1.15 (both tensorflow and tensorflow-gpu)
TF 2.0.0
TF 2.1.0
ray 0.8.1
ray 0.9.0 (latest nightly)

On all of these versions, the problem was occurring. Interestingly, it was also occurring very consistently across these runs at a rather precise number of steps (~2.5M). And it confirmed once more that there is a clear relation between object store size and duration until failure.
I've tried to use ray stack to get some traces, but this requires sudo access which I don't have on those machines.
Also, I've noticed that the latest nightly prints the following error message during cluster startup, even though I'm not using docker:
&lt;denchmark-code&gt;2020-02-17 14:19:47,119 WARNING utils.py:406 -- Container memory is larger than system memory.
2020-02-17 14:19:47,119 WARNING utils.py:431 -- Container is reporting more memory usage than thesystem.
&lt;/denchmark-code&gt;

On an unrelated note: I'm surprised to see your throughput to match mine almost exactly. How did you achieve &lt;denchmark-link:https://github.com/ray-project/rl-experiments/blob/master/pong-speedrun/pong-impala.png&gt;almost double of that&lt;/denchmark-link&gt;
 with 128 workers on the &lt;denchmark-link:https://github.com/ray-project/rl-experiments&gt;Pong in 3 minutes&lt;/denchmark-link&gt;
 experiment?
		</comment>
		<comment id='11' author='thomaschn' date='2020-02-17T16:44:52Z'>
		I've ran some more experiments by changing the number of nodes in the cluster. It seems like the error only occures when there are more than 2 nodes in the cluster:

2 nodes (64 CPUs, 2 GPUs): stable
3 nodes (96 CPUs, 2 GPUs): 10M steps till error, little slowdown after object store &gt;= 2GB
5 nodes (160 CPUs, 2 GPUs): 2.5M steps till error, drastic slowdown after object store &gt;= 2GB

(each run with 63 workers, object store limit at 2GB)
		</comment>
		<comment id='12' author='thomaschn' date='2020-02-17T19:47:13Z'>
		I was able to reproduce this on a 5-node cluster with a p2.16xl head and 4 m4.16xl worker nodes.
After a few minutes, you start seeing "Failed to get object". The frequency of these errors seems to grow over time. Also, raylet CPU and memory seem to increase. The raylet reaches &gt;100% CPU usage on worker nodes.
I was not able to reproduce the "drastic slowdown" part but it could be just these nodes have so much memory it takes a long time to hit that point.
Here are logs from a raylet on a worker node: &lt;denchmark-link:https://gist.github.com/ericl/496873f85b3052039cec11e7aec56581&gt;https://gist.github.com/ericl/496873f85b3052039cec11e7aec56581&lt;/denchmark-link&gt;

The debug state of the worker node:
&lt;denchmark-link:https://gist.github.com/ericl/067ba33c5ee03718ab7075d327b1a98b&gt;https://gist.github.com/ericl/067ba33c5ee03718ab7075d327b1a98b&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='thomaschn' date='2020-02-17T19:53:00Z'>
		&lt;denchmark-link:https://github.com/thomaschn&gt;@thomaschn&lt;/denchmark-link&gt;
 can you confirm if you see the performance cliff even with ray==0.8.0? I see the error messages with that version, but am not if it is as severe.
		</comment>
		<comment id='14' author='thomaschn' date='2020-02-17T21:10:23Z'>
		Yes, I can imagine your nodes having enough memory to smooth out this issue for a while. After all, your nodes are not even at 10% memory usage, whereas mine are towards 20-50% in total. More specifically, I regularly noticed the nodes which reported Failed to get Object to run completely out of memory. So even 50% of memory usage in total often times means that the failing nodes are at 100%.
If I remember correctly, ray==0.8.0 crashed completely in those cases with this error message:
&lt;denchmark-code&gt; "Object {} is lost (either LRU evicted or deleted by user) and "
            "cannot be reconstructed. Try increasing the object store "
            "memory available with ray.init(object_store_memory=&lt;bytes&gt;) "
            "or setting object store limits with "
            "ray.remote(object_store_memory=&lt;bytes&gt;). See also: {}".format(
                self.object_id.hex(),
                "https://ray.readthedocs.io/en/latest/memory-management.html"
&lt;/denchmark-code&gt;

I will try to reproduce on 0.8.0 tomorrow. If it helps, I can provide the same logs as you did, &lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
 . In that case, from which files exactly are those logs?
		</comment>
		<comment id='15' author='thomaschn' date='2020-02-17T21:12:37Z'>
		The first log is the raylet.out from the worker node in /tmp/ray/session_latest/logs. The second one is the debug_state.txt in session_latest.
		</comment>
		<comment id='16' author='thomaschn' date='2020-02-17T21:28:53Z'>
		Current working hypothesis is something like this:

some spurious transfer failure causes a request to get stuck pending forever
the pending request consumes some amount of temporary buffer memory (and also object store memory)?
eventually enough pending requests accumulate to cause an OOM

The correlation of the events with filling up the object store could be due to transfer failures only occurring once the object store is filled.
		</comment>
		<comment id='17' author='thomaschn' date='2020-02-18T05:47:53Z'>
		&lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
 is this fixed by &lt;denchmark-link:https://github.com/ray-project/ray/pull/7201&gt;#7201&lt;/denchmark-link&gt;
?
&lt;denchmark-link:https://github.com/thomaschn&gt;@thomaschn&lt;/denchmark-link&gt;
 can you try install from latest development snapshot &lt;denchmark-link:https://ray.readthedocs.io/en/latest/installation.html#latest-snapshots-nightlies&gt;https://ray.readthedocs.io/en/latest/installation.html#latest-snapshots-nightlies&lt;/denchmark-link&gt;
 see if the fix address this issue?
		</comment>
		<comment id='18' author='thomaschn' date='2020-02-18T05:56:31Z'>
		&lt;denchmark-link:https://github.com/simon-mo&gt;@simon-mo&lt;/denchmark-link&gt;
 it gets rid of the object get errors in my testing. Would be great to get confirmation.
		</comment>
		<comment id='19' author='thomaschn' date='2020-02-18T15:01:37Z'>
		Thanks a lot!
Yes, I can confirm that this problem does not occur any more with the latest development snapshot. No Failed to get object, no drastic slowdown.
I've run some experiments with 2GB (orange) and 20GB (blue) object store limit on a 5 node cluster using 128 workers. For comparison: magenta is yesterdays "stable" run using 64 workers on 2 nodes with 2GB object store limit.
&lt;denchmark-link:https://user-images.githubusercontent.com/61015896/74747599-7d913980-5267-11ea-8ac1-7635f5b55e20.png&gt;&lt;/denchmark-link&gt;

However, the memory usage is still growing and the throughput noticeably decreasing over time. I'm not sure though whether this is the same issue or yet another problem.
		</comment>
		<comment id='20' author='thomaschn' date='2020-02-18T21:26:32Z'>
		I think that might be a TF issue. When I set num_gpus=1 and commented out the num_data_loader_buffers config, the slowdown vanished (orange line):
&lt;denchmark-link:https://user-images.githubusercontent.com/14922/74779380-38164180-5252-11ea-93fe-b803e811bb9d.png&gt;&lt;/denchmark-link&gt;

So it might be a multi-gpu or concurrent data loader caused leak.
		</comment>
		<comment id='21' author='thomaschn' date='2020-02-18T21:27:15Z'>
		I'm going to close this issue, feel free to open another one for the more gradual slowdown.
		</comment>
	</comments>
</bug>