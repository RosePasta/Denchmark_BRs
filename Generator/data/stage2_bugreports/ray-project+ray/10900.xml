<bug id='10900' author='richardliaw' open_date='2020-09-19T00:52:12Z' closed_time='2020-09-21T17:48:56Z'>
	<summary>[wandb] Multiple training iterations are not being logged.</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

Currently not seeing multiple iterations per each of the runs, despite calling tune.report multiple times: &lt;denchmark-link:https://wandb.ai/rliaw/RAY/groups/gcp_sep16_wmt?workspace=user-rliaw&gt;https://wandb.ai/rliaw/RAY/groups/gcp_sep16_wmt?workspace=user-rliaw&lt;/denchmark-link&gt;

Also, maybe we want to consider disabling the system metric monitoring, since it's not really accurate if using the logger.
cc &lt;denchmark-link:https://github.com/krfricke&gt;@krfricke&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='richardliaw' date='2020-09-20T01:35:02Z'>
		I think this happens due to updating the config during wandb.
You can repro using this yaml:
# An unique identifier for the head node and workers of this cluster.
cluster_name: transformers-gpu

# The maximum number of workers nodes to launch in addition to the head
# node. This takes precedence over min_workers. min_workers default to 0.
min_workers: 0
initial_workers: 0
max_workers: 0
# Cloud-provider specific configuration.
provider:
    type: aws
    cache_stopped_nodes: False
    region: us-west-2
    availability_zone: us-west-2a,us-west-2b

# How Ray will authenticate with newly launched nodes.
auth:
    ssh_user: ubuntu
    ssh_private_key: XXXXX

head_node:
    InstanceType: p3.8xlarge # 8 V100s
    ImageId: latest_dlami
    KeyName: XXXXXX
    BlockDeviceMappings:
        - DeviceName: /dev/sda1
          Ebs:
              VolumeSize: 300
    # InstanceMarketOptions:
    #     MarketType: spot
           # SpotOptions:
           #     MaxPrice: "9.0"


worker_nodes:
    InstanceType: p3.8xlarge
    ImageId: latest_dlami
    KeyName: XXXXX
    # Run workers on spot by default. Comment this out to use on-demand.
    BlockDeviceMappings:
        - DeviceName: /dev/sda1
          Ebs:
              VolumeSize: 300
    InstanceMarketOptions:
        MarketType: spot
        # SpotOptions:
        #     MaxPrice: "9.0"


file_mounts: {
    /home/ubuntu/transformers/: ~/dev/transformers,
    /home/ubuntu/anaconda3/lib/python3.7/site-packages/ray/tune/integration/wandb.py: /Users/rliaw/ray/python/ray/tune/integration/wandb.py
}

setup_commands:
    - cd transformers &amp;&amp; pip install -e .
    - pip install -U https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-1.1.0.dev0-cp37-cp37m-manylinux1_x86_64.whl
    - wget https://s3.amazonaws.com/datasets.huggingface.co/translation/wmt_en_ro.tar.gz &amp;&amp; tar -xzvf wmt_en_ro.tar.gz
    - pip install -q ipdb ray[tune] torch torchvision hyperopt
    - pip install wandb
    - cd transformers &amp;&amp; pip install -r examples/requirements.txt
    - pip install -U pytorch-lightning
    # Install apex.

This script:
from ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining
import ray
from ray import tune
from ray.tune import CLIReporter
from ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining
from functools import partial
import os
import sys
# from durbango import *

def fix_import_paths():
    import os
    import sys
    import transformers
    sys.path.append(
        os.path.join(
            os.path.dirname(transformers.__file__),
            "../../examples/seq2seq/"))

    sys.path.append(
        os.path.join(
            os.path.dirname(transformers.__file__),
            "../../examples/"))

fix_import_paths()


from pathlib import Path


def get_ray_slug(cfg):
    strang = ''
    for k,v in cfg.items():

        strang += f'{k}_{v}'
    for i in range(10000):
        test = f'rayruns/run_{i}'
        try:
            Path(test).mkdir(exist_ok=True,parents=True)
            break
        except Exception:
            continue

    return os.path.expanduser(test)

from ray.tune.logger import DEFAULT_LOGGERS
from ray.tune.integration.wandb import WandbLogger
def ray_main(args, config):
    fix_import_paths()
    for k,v in config.items():
        #assert hasattr(args, k), k
        setattr(args, k, v)
    args.output_dir = get_ray_slug(config)
    from finetune import main as ft_main
    ft_main(args)


def tune_helsinki_(args, num_samples=100, num_epochs=1):
    args.num_train_epochs = num_epochs
    # args.n_train = 10000
    search_space = {
        "learning_rate": tune.loguniform(1e-6, 0.5),
        "gradient_accumulation_steps": tune.choice([1, 8, 32, 128, 256]),
        "dropout": tune.uniform(0, 0.4),
        'label_smoothing': tune.uniform(0, 0.4),
    }
    # scheduler = ASHAScheduler(
    #      metric="val_avg_bleu",
    #      mode="max",
    #      max_t=num_epochs* int(1/args.val_check_interval),  # max number of reports until termination
    #      grace_period=1,
    #      reduction_factor=4,  # cut 1/4 of trials really quickly, and another 1/4 pretty soon
    #  )

    from ray.tune.suggest.hyperopt import HyperOptSearch

    searcher = HyperOptSearch(metric="val_avg_bleu", mode="max")
    reporter = CLIReporter(
        parameter_columns={
            "learning_rate": "lr",
            "gradient_accumulation_steps": "grad_accum",
            "dropout": "dropout",
            "label_smoothing": "l_smooth",

        },
        metric_columns={
            "val_avg_loss": "loss",
            "val_avg_bleu": "bleu",
            "global_step": "step",
            "training_iteration": "iter"
        }
    )
    config = search_space.copy()

    def datetime_now():
        import datetime
        return datetime.datetime.now().strftime("%H:%M:%S")

    config["wandb"] = {
        "project": "RAY",
        "group": f"gcp_sep16_wmt-{datetime_now()}",
        "api_key": "XXXX"
    }
    ray.init(log_to_driver=False, address="auto")
    tune.run(
        partial(
            ray_main,
            args,
        ),
        resources_per_trial={"gpu": args.gpus},
        config=config,
        num_samples=num_samples,
        search_alg=searcher,
        # scheduler=scheduler,
        progress_reporter=reporter,
        name="tune_helsinki_asha",
        loggers=DEFAULT_LOGGERS + (WandbLogger,),
        # max_failures=3,
        fail_fast=True,
    )

# Make default args
import argparse
DEFAULTS = {
    'logger': True,
    'checkpoint_callback': True,
    'early_stop_callback': False,
    'default_root_dir': None,
    'gradient_clip_val': 0,
    'eval_beams': 2,
    'process_position': 0,
    "eval_max_gen_length": 128,
    'num_nodes': 1,
    'num_processes': 1,
    'gpus': 1,
    'auto_select_gpus': False,
    'adafactor': False,
    #'tpu_cores': 0,
    'log_gpu_memory': None,
    'progress_bar_refresh_rate': 0,
    'overfit_batches': 0.0,
    'track_grad_norm': -1,
    'check_val_every_n_epoch': 1,
    'fast_dev_run': False,
    'accumulate_grad_batches': 1,
    'max_epochs': 1,
    'min_epochs': 1,
    'max_steps': None,
    'min_steps': None,
    'limit_train_batches': 1.0,
    'limit_val_batches': 1.0,
    'limit_test_batches': 1.0,
    'val_check_interval': 0.01,
    'log_save_interval': 100,
    'row_log_interval': 50,
    'distributed_backend': None,
    'precision': 32,
    'print_nan_grads': False,
    'weights_summary': 'top',
    'weights_save_path': None,
    'num_sanity_val_steps': 0,
    'truncated_bptt_steps': None,
    'resume_from_checkpoint': None,
    'profiler': None,
    'benchmark': False,
    'deterministic': False,
    'reload_dataloaders_every_epoch': True,
    'auto_lr_find': False,
    'replace_sampler_ddp': True,
    'terminate_on_nan': False,
    'auto_scale_batch_size': False,
    'prepare_data_per_node': True,
    'amp_level': 'O2',
    'val_percent_check': None,
    'test_percent_check': None,
    'train_percent_check': None,
    'overfit_pct': None,
    'model_name_or_path': 'sshleifer/student_marian_en_ro_6_3',
    'config_name': '',
    'tokenizer_name': 'sshleifer/student_marian_en_ro_6_3',
    'cache_dir': '',
    'encoder_layerdrop': None,
    'decoder_layerdrop': None,
    'dropout': None,
    'attention_dropout': None,
    'learning_rate': 0.0003,
    'lr_scheduler': 'linear',
    'weight_decay': 0.0,
    'adam_epsilon': 1e-08,
    'warmup_steps': 500,
    'num_workers': 4,
    'train_batch_size': 64,
    'eval_batch_size': 64,
    'output_dir': 'tmp',
    'fp16': True,
    'fp16_opt_level': 'O1',
    'do_train': True,
    'do_predict': True,
    'seed': 42,
    'data_dir': os.path.expanduser('~/wmt_en_ro'),
    'max_source_length': 128,
    'max_target_length': 128,
    'val_max_target_length': 128,
    'test_max_target_length': 128,
    'freeze_encoder': True,
    'freeze_embeds': True,
    'sortish_sampler': True,
    'logger_name': 'default',
    'n_train': -1,
    'n_val': 500,
    'n_test': -1,
    'task': 'translation',
    'label_smoothing': 0.1,
    'src_lang': '',
    'tgt_lang': '',
    'early_stopping_patience': -1,
    'val_metric': None,
    'save_top_k': 1,
}
args = argparse.Namespace(**DEFAULTS)

tune_helsinki_(args)
and this transformers repo:
&lt;denchmark-link:https://github.com/richardliaw/transformers/tree/c08795e789e5824baa06059baf84a4c5a703c7fb&gt;https://github.com/richardliaw/transformers/tree/c08795e789e5824baa06059baf84a4c5a703c7fb&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='richardliaw' date='2020-09-20T01:35:25Z'>
		ray up $YAML
ray submit $YAML $SCRIPT
		</comment>
		<comment id='3' author='richardliaw' date='2020-09-20T04:59:53Z'>
		Fix here works for me:
--- a/python/ray/tune/integration/wandb.py
+++ b/python/ray/tune/integration/wandb.py
@@ -193,7 +193,7 @@ class _WandbLoggingProcess(Process):
             if result == _WANDB_QUEUE_END:
                 break
             log, config_update = self._handle_result(result)
-            wandb.config.update(config_update, allow_val_change=True)
+            # wandb.config.update(config_update, allow_val_change=True)
             wandb.log(log)
         wandb.join()
		</comment>
		<comment id='4' author='richardliaw' date='2020-09-21T10:32:37Z'>
		I currently can't run the reproduction script, and I also can't find a reproduction with a simpler script and bogus logging calls. Can you print the output of the result, log and config_update variables?
		</comment>
		<comment id='5' author='richardliaw' date='2020-09-21T17:48:56Z'>
		This seems to have been fixed with the latest wandb release 0.10.2. With 0.10.1 the bug occurred, with 0.10.2 it doesn't. Closing this.
		</comment>
	</comments>
</bug>