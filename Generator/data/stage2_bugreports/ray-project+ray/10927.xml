<bug id='10927' author='Seraphli' open_date='2020-09-21T14:26:02Z' closed_time='2020-09-26T04:00:38Z'>
	<summary>[ray] Redis memory increase on head</summary>
	<description>
Ray version and other system information (Python version, TensorFlow version, OS): Ray 0.8.7, Python 3.8.5, Debian
We notice a memory increase problem on head machine.
We have done some researches to find out what happened.
First, we find a lot of keys increased in the Redis server.
&lt;denchmark-link:https://user-images.githubusercontent.com/1586755/93778411-77799600-fc58-11ea-91cd-137d8fb3bc89.png&gt;&lt;/denchmark-link&gt;

Then we output the increased keys
&lt;denchmark-link:https://user-images.githubusercontent.com/1586755/93778691-c0c9e580-fc58-11ea-9aef-19b9665e3885.png&gt;&lt;/denchmark-link&gt;

All new keys seem to have a 'PROFILE' prefix. And I search this prefix in the source code and find out this is added by raylet.
Our Redis memory seems to continually increase and eventually, the whole system crashed. Any advice about this issue is welcomed.
	</description>
	<comments>
		<comment id='1' author='Seraphli' date='2020-09-21T15:59:30Z'>
		Thanks for the report! There’s a PR to limit the number of entries profile table in Redis (, and it won’t be available in our next release, but will be included in the one after that). Are you using the nightly ray?
		</comment>
		<comment id='2' author='Seraphli' date='2020-09-21T16:00:20Z'>
		&lt;denchmark-link:https://github.com/ray-project/ray/pull/10888&gt;#10888&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='Seraphli' date='2020-09-21T16:32:11Z'>
		We are not using the nightly ray. To use ray we have to build a docker image first. So it will be difficult to use nightly version.
Because we run ray on a cluster with 25 computers, we have to allocate the head machine with 900G memory in order to let the experiment run longger.
		</comment>
		<comment id='4' author='Seraphli' date='2020-09-21T16:37:45Z'>
		Can this be fixed more early? This memory issue I think is a major issue. This will make the system crash quickly.
		</comment>
		<comment id='5' author='Seraphli' date='2020-09-21T17:01:29Z'>
		These entries should start getting evicted once you hit the redis_max_memory config. Try lowering that config in ray.init()?
		</comment>
		<comment id='6' author='Seraphli' date='2020-09-21T17:19:52Z'>
		Okay, I will try this. We actually set redis_max_memory config equal to the whole memory. Any suggestions of this config? And BTW,  is the evict setting turned on default?
		</comment>
		<comment id='7' author='Seraphli' date='2020-09-21T17:26:09Z'>
		Another question, will this config only evict the key with PROFILE perfix, or will it evict the oldest key? Because we have a lot of actor class, I think some of them will stay longer than the profile keys. I don't know whether this will be a problem.
		</comment>
		<comment id='8' author='Seraphli' date='2020-09-21T17:55:34Z'>
		I would recommend either going with the default setting, or something small like 500 megabytes. Ray does not use Redis for primary storage, only metadata.
The critical metadata tables like Actors are placed on the primary shard and won't get evicted -- only secondary tables like PROFILE are eligible for eviction.
		</comment>
		<comment id='9' author='Seraphli' date='2020-09-22T02:57:19Z'>
		Okay, I will try that out.
		</comment>
		<comment id='10' author='Seraphli' date='2020-09-26T04:00:38Z'>
		After I remove all memory settings and let ray to choose all memory settings, the memory increases much slower than before.  I think the problem is solved.
		</comment>
	</comments>
</bug>