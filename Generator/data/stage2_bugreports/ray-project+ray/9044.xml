<bug id='9044' author='amogkam' open_date='2020-06-19T19:19:27Z' closed_time='2020-06-26T01:36:59Z'>
	<summary>[Ray] Experiment crashes when raylet fails</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

I'm running a long running training+tuning experiment that is supposed to be fault tolerant, but the experiment sometimes crashes when a node fails due to actor_entry != actor_registry_.end(). Full stack trace is here:
&lt;denchmark-code&gt;2020-06-16 21:26:07,014	INFO commands.py:136 -- kill_node: Shutdown worker i-0baab8b79abe4649d
2020-06-16 21:26:07,014	INFO updater.py:201 -- NodeUpdater: i-0baab8b79abe4649d: Waiting for IP...
2020-06-16 21:26:07,014	INFO log_timer.py:22 -- NodeUpdater: i-0baab8b79abe4649d: Got IP  [LogTimer=0ms]
2020-06-16 21:26:07,014	INFO updater.py:254 -- NodeUpdater: i-0baab8b79abe4649d: Running ray stop on 34.222.57.174...
2020-06-16 21:26:07,014	INFO updater.py:255 -- Begin remote output from 34.222.57.174
�[2m�[33m(pid=raylet)�[0m I0616 21:26:07.080621  4564  4574 store.cc:733] Disconnecting client on fd 55
�[2m�[33m(pid=raylet)�[0m I0616 21:26:07.080756  4564  4574 store.cc:733] Disconnecting client on fd 68
�[2m�[33m(pid=raylet, ip=172.31.30.93)�[0m I0616 21:26:07.059882 111987 111997 store.cc:733] Disconnecting client on fd 86
�[2m�[33m(pid=raylet)�[0m W0616 21:26:07.510947  4564  4564 task_spec.cc:57] More than 391 types of tasks seen, this may reduce performance.
�[2m�[33m(pid=raylet)�[0m W0616 21:26:07.529908  4564  4564 task_spec.cc:57] More than 392 types of tasks seen, this may reduce performance.
�[2m�[33m(pid=raylet, ip=172.31.29.169)�[0m I0616 21:26:08.130998 112449 112449 main.cc:216] Raylet received SIGTERM, shutting down...
�[2m�[33m(pid=raylet, ip=172.31.29.169)�[0m I0616 21:26:08.131031 112449 112449 service_based_accessor.cc:435] Unregistering node info, node id = ad52f018fc4a2625a7585219432cbdda873d658f
�[2m�[33m(pid=raylet, ip=172.31.30.93)�[0m I0616 21:26:08.121438 111987 111987 service_based_accessor.cc:739] Received notification for node id = ad52f018fc4a2625a7585219432cbdda873d658f, IsAlive = 0
�[2m�[33m(pid=raylet, ip=172.31.30.93)�[0m I0616 21:26:08.121518 111987 111987 node_manager.cc:590] Actor c15ed3b70100 is disconnected, because its node ad52f018fc4a2625a7585219432cbdda873d658f is removed from cluster. It may be restarted.
�[2m�[33m(pid=raylet, ip=172.31.30.93)�[0m I0616 21:26:08.121753 111987 111987 node_manager.cc:647] Owner node ad52f018fc4a2625a7585219432cbdda873d658f died, killing leased worker 8e64034e18a75e7f77aa8a3b53b28d7cfe4523a2
E0616 21:26:08.166589  5536  5632 task_manager.cc:307] 3 retries left for task ffffffffffffffff9cf0db480100, attempting to resubmit.
E0616 21:26:08.166678  5536  5632 core_worker.cc:393] Will resubmit task after a 5000ms delay: Type=ACTOR_CREATION_TASK, Language=PYTHON, function_descriptor={type=PythonFunctionDescriptor, module_name=__main__, class_name=NoFaultToleranceTrainable, function_name=__init__, function_hash=2e2b29ff-ee75-487d-8fa0-0944f770ba3d}, task_id=ffffffffffffffff9cf0db480100, job_id=0100, num_args=4, num_returns=1, actor_creation_task_spec={actor_id=9cf0db480100, max_restarts=0, max_concurrency=1, is_asyncio_actor=0, is_detached=0}
�[2m�[33m(pid=raylet, ip=172.31.29.123)�[0m I0616 21:26:08.142746 121518 121518 service_based_accessor.cc:739] Received notification for node id = ad52f018fc4a2625a7585219432cbdda873d658f, IsAlive = 0
�[2m�[33m(pid=raylet, ip=172.31.29.123)�[0m I0616 21:26:08.142817 121518 121518 node_manager.cc:590] Actor c15ed3b70100 is disconnected, because its node ad52f018fc4a2625a7585219432cbdda873d658f is removed from cluster. It may be restarted.
�[2m�[33m(pid=raylet, ip=172.31.29.123)�[0m I0616 21:26:08.143023 121518 121518 node_manager.cc:647] Owner node ad52f018fc4a2625a7585219432cbdda873d658f died, killing leased worker 15e259eeac528503aa442004c27e06792a9f9d19
�[2m�[33m(pid=raylet)�[0m I0616 21:26:08.143133  4564  4564 service_based_accessor.cc:739] Received notification for node id = ad52f018fc4a2625a7585219432cbdda873d658f, IsAlive = 0
�[2m�[33m(pid=raylet)�[0m I0616 21:26:08.143239  4564  4564 node_manager.cc:590] Actor c15ed3b70100 is disconnected, because its node ad52f018fc4a2625a7585219432cbdda873d658f is removed from cluster. It may be restarted.
�[2m�[33m(pid=raylet)�[0m I0616 21:26:08.143527  4564  4564 node_manager.cc:647] Owner node ad52f018fc4a2625a7585219432cbdda873d658f died, killing leased worker e687b2e95f67268fbdd5aa7805709555f09ba4e2
�[2m�[33m(pid=raylet)�[0m I0616 21:26:08.145704  4564  4574 store.cc:733] Disconnecting client on fd 55
�[2m�[33m(pid=raylet)�[0m F0616 21:26:08.145843  4564  4564 node_manager.cc:1179]  Check failed: actor_entry != actor_registry_.end() 
�[2m�[33m(pid=raylet)�[0m *** Check failure stack trace: ***
�[2m�[33m(pid=raylet)�[0m     @     0x55d6786e893d  google::LogMessage::Fail()
�[2m�[33m(pid=raylet)�[0m     @     0x55d6786e9dac  google::LogMessage::SendToLog()
�[2m�[33m(pid=raylet)�[0m     @     0x55d6786e8619  google::LogMessage::Flush()
�[2m�[33m(pid=raylet)�[0m     @     0x55d6786e8831  google::LogMessage::~LogMessage()
�[2m�[33m(pid=raylet)�[0m     @     0x55d6786d0459  ray::RayLog::~RayLog()
�[2m�[33m(pid=raylet)�[0m     @     0x55d67843db61  ray::raylet::NodeManager::HandleDisconnectedActor()
�[2m�[33m(pid=raylet)�[0m     @     0x55d678441cac  ray::raylet::NodeManager::ProcessDisconnectClientMessage()
�[2m�[33m(pid=raylet)�[0m     @     0x55d67844409d  ray::raylet::NodeManager::ProcessClientMessage()
�[2m�[33m(pid=raylet)�[0m     @     0x55d6783c8b8d  _ZNSt17_Function_handlerIFvSt10shared_ptrIN3ray16ClientConnectionEElPKhEZNS1_6raylet6Raylet12HandleAcceptERKN5boost6system10error_codeEEUlS3_lS5_E0_E9_M_invokeERKSt9_Any_dataS3_lS5_
�[2m�[33m(pid=raylet)�[0m     @     0x55d678692b92  ray::ClientConnection::ProcessMessage()
�[2m�[33m(pid=raylet)�[0m     @     0x55d67869636e  ray::ClientConnection::ProcessMessageHeader()
�[2m�[33m(pid=raylet)�[0m     @     0x55d678690d12  boost::asio::detail::read_op&lt;&gt;::operator()()
�[2m�[33m(pid=raylet)�[0m     @     0x55d67869226a  boost::asio::detail::reactive_socket_recv_op&lt;&gt;::do_complete()
�[2m�[33m(pid=raylet)�[0m     @     0x55d6789c4f3f  boost::asio::detail::scheduler::do_run_one()
�[2m�[33m(pid=raylet)�[0m     @     0x55d6789c6441  boost::asio::detail::scheduler::run()
�[2m�[33m(pid=raylet)�[0m     @     0x55d6789c7472  boost::asio::io_context::run()
�[2m�[33m(pid=raylet)�[0m     @     0x55d6783aa9ab  main
�[2m�[33m(pid=raylet)�[0m     @     0x7fa081005830  __libc_start_main
�[2m�[33m(pid=raylet)�[0m     @     0x55d6783bbd51  (unknown)
�[2m�[36m(pid=110135)�[0m 2020-06-16 21:26:08,144	ERROR worker.py:374 -- SystemExit was raised from the worker
�[2m�[36m(pid=110135)�[0m Traceback (most recent call last):
�[2m�[36m(pid=110135)�[0m   File "python/ray/_raylet.pyx", line 514, in ray._raylet.task_execution_handler
�[2m�[36m(pid=110135)�[0m   File "python/ray/_raylet.pyx", line 335, in ray._raylet.execute_task
�[2m�[36m(pid=110135)�[0m   File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ray/function_manager.py", line 397, in load_actor_class
�[2m�[36m(pid=110135)�[0m     job_id, actor_creation_function_descriptor)
�[2m�[36m(pid=110135)�[0m   File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ray/function_manager.py", line 494, in _load_actor_class_from_gcs
�[2m�[36m(pid=110135)�[0m     actor_class = pickle.loads(pickled_class)
�[2m�[36m(pid=110135)�[0m   File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ray/util/sgd/__init__.py", line 1, in &lt;module&gt;
�[2m�[36m(pid=110135)�[0m     from ray.util.sgd.torch import TorchTrainer
�[2m�[36m(pid=110135)�[0m   File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ray/util/sgd/torch/__init__.py", line 9, in &lt;module&gt;
�[2m�[36m(pid=110135)�[0m     import torch  # noqa: F401
�[2m�[36m(pid=110135)�[0m   File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/__init__.py", line 16, in &lt;module&gt;
�[2m�[36m(pid=110135)�[0m     from ._utils import _import_dotted_name
�[2m�[36m(pid=110135)�[0m   File "&lt;frozen importlib._bootstrap&gt;", line 971, in _find_and_load
�[2m�[36m(pid=110135)�[0m   File "&lt;frozen importlib._bootstrap&gt;", line 951, in _find_and_load_unlocked
�[2m�[36m(pid=110135)�[0m   File "&lt;frozen importlib._bootstrap&gt;", line 894, in _find_spec
�[2m�[36m(pid=110135)�[0m   File "&lt;frozen importlib._bootstrap_external&gt;", line 1157, in find_spec
�[2m�[36m(pid=110135)�[0m   File "&lt;frozen importlib._bootstrap_external&gt;", line 1129, in _get_spec
�[2m�[36m(pid=110135)�[0m   File "&lt;frozen importlib._bootstrap_external&gt;", line 1270, in find_spec
�[2m�[36m(pid=110135)�[0m   File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ray/worker.py", line 371, in sigterm_handler
�[2m�[36m(pid=110135)�[0m     sys.exit(1)
�[2m�[36m(pid=110135)�[0m SystemExit: 1
�[2m�[33m(pid=raylet, ip=172.31.29.123)�[0m I0616 21:26:08.167119 121518 121518 node_manager.cc:740] [HeartbeatAdded]: received heartbeat from unknown client id ad52f018fc4a2625a7585219432cbdda873d658f
�[2m�[33m(pid=raylet, ip=172.31.30.93)�[0m I0616 21:26:08.145617 111987 111987 node_manager.cc:740] [HeartbeatAdded]: received heartbeat from unknown client id ad52f018fc4a2625a7585219432cbdda873d658f
Shared connection to 34.222.57.174 closed.

2020-06-16 21:26:08,341	WARNING worker.py:1047 -- A worker died or was killed while executing task ffffffffffffffff021c4d220100.
�[2m�[33m(pid=raylet, ip=172.31.29.123)�[0m I0616 21:26:08.342061 121518 121528 store.cc:733] Disconnecting client on fd 108
2020-06-16 21:26:08,352	WARNING worker.py:1047 -- A worker died or was killed while executing task ffffffffffffffff487f41020100.
�[2m�[33m(pid=raylet, ip=172.31.30.93)�[0m I0616 21:26:08.318150 111987 111997 store.cc:733] Disconnecting client on fd 119
�[2m�[33m(pid=raylet, ip=172.31.30.93)�[0m E0616 21:26:08.318152 111987 111987 node_manager.cc:3572] Failed to send get core worker stats request: IOError: 14: Connection reset by peer
�[2m�[36m(pid=110136)�[0m E0616 21:26:09.101554 110136 110154 core_worker.cc:663] Raylet failed. Shutting down.
2020-06-16 21:26:13,263	WARNING util.py:137 -- The `on_step_begin` operation took 6.382588863372803 seconds to complete, which may be a performance bottleneck.
F0616 21:26:14.024302  5536  5632 direct_task_transport.cc:240] IOError: 14: failed to connect to all addresses
*** Check failure stack trace: ***
    @     0x7fb39407df1d  google::LogMessage::Fail()
    @     0x7fb39407f38c  google::LogMessage::SendToLog()
    @     0x7fb39407dbf9  google::LogMessage::Flush()
    @     0x7fb39407de11  google::LogMessage::~LogMessage()
    @     0x7fb3940681b9  ray::RayLog::~RayLog()
    @     0x7fb393dd9fab  _ZZN3ray29CoreWorkerDirectTaskSubmitter24RequestNewWorkerIfNeededERKSt5tupleIIiSt6vectorINS_8ObjectIDESaIS3_EENS_7ActorIDEEEPKNS_3rpc7AddressEENKUlRKNS_6StatusERKNSA_23RequestWorkerLeaseReplyEE_clESG_SJ_
    @     0x7fb393e20cc5  ray::rpc::ClientCallImpl&lt;&gt;::OnReplyReceived()
    @     0x7fb393d8a770  _ZN5boost4asio6detail18completion_handlerIZN3ray3rpc17ClientCallManager29PollEventsFromCompletionQueueEiEUlvE_E11do_completeEPvPNS1_19scheduler_operationERKNS_6system10error_codeEm
    @     0x7fb3942b59df  boost::asio::detail::scheduler::do_run_one()
    @     0x7fb3942b65f1  boost::asio::detail::scheduler::run()
    @     0x7fb3942b7542  boost::asio::io_context::run()
    @     0x7fb393d713d0  ray::CoreWorker::RunIOService()
    @     0x7fb3e374df70  execute_native_thread_routine_compat
    @     0x7fb3ec4fb6ba  start_thread
    @     0x7fb3ec23141d  clone
&lt;/denchmark-code&gt;

Also, after this error occurs, I can no longer connect to the ray cluster with ray.init("auto"). It just hangs at this point:
&lt;denchmark-code&gt;I0619 19:20:58.061543 45398 45398 global_state_accessor.cc:25] Redis server address = 172.31.20.116:6379, is test flag = 0
I0619 19:20:58.062458 45398 45398 redis_client.cc:141] RedisClient connected.
I0619 19:20:58.084384 45398 45398 redis_gcs_client.cc:88] RedisGcsClient Connected.
I0619 19:20:58.085548 45398 45398 service_based_gcs_client.cc:75] ServiceBasedGcsClient Connected.
&lt;/denchmark-code&gt;

Ray version and other system information (Python version, TensorFlow version, OS):
Python 3.6, Latest ray wheel- 0.9.0, Pytorch 1.4.0
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
Follow steps to run ray/ci/long_running_distributed_tests/workloads/pytorch_pbt_failure.py. This script injects random failure to a multi-node cluster, but it might take a while for this exact issue to resurface again. I can provide further logs from my run if needed.
If we cannot run your script, we cannot fix your issue.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='amogkam' date='2020-06-19T19:24:53Z'>
		thanks for opening this &lt;denchmark-link:https://github.com/amogkam&gt;@amogkam&lt;/denchmark-link&gt;
 -
BTW, can you help spell out the steps to reproduce? It’ll reduce the friction needed to address this problem.
Also, can you provide a skeleton of the workload? what actors are being created, how do they interact with each other, etc. This will also allow core developers to figure out where to start.
&lt;denchmark-link:https://github.com/stephanie-wang&gt;@stephanie-wang&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/rkooo567&gt;@rkooo567&lt;/denchmark-link&gt;
 do you have any tips on how to improve the reporting on this issue? what more info to provide, etc
		</comment>
		<comment id='2' author='amogkam' date='2020-06-23T23:08:42Z'>
		&lt;denchmark-link:https://github.com/stephanie-wang&gt;@stephanie-wang&lt;/denchmark-link&gt;
 can confirm, but I think it is the known limitation that we don't handle raylet failures. I could find this information from code. 

		</comment>
		<comment id='3' author='amogkam' date='2020-06-23T23:53:15Z'>
		Well, the entire ray cluster shouldn't crash if a worker nodes goes down - i think it's just the core worker on that node that gets affected right?
		</comment>
		<comment id='4' author='amogkam' date='2020-06-24T01:17:12Z'>
		Yeah that’s correct. But if the raylet in the node that the driver is connected dies, it can break drivers (because fatal logs occur in the driver process’ core worker). I think it is the case for this crash
		</comment>
		<comment id='5' author='amogkam' date='2020-06-24T01:55:30Z'>
		OK right; so then the issue here is that somehow, a remote node failure causes a driver raylet crash.
		</comment>
		<comment id='6' author='amogkam' date='2020-06-24T01:57:48Z'>
		Yeah that’s my guess. I also saw this error from some other long running tests too. I suppose it will be resolved once we properly handle raylet failures (and there’s a milestone called Ray HA next quarter, so it will be addressed I assume).
		</comment>
		<comment id='7' author='amogkam' date='2020-06-26T01:36:59Z'>
		This will be fixed when GCS actor management is enabled (the entire code path that crashes is skipped).
		</comment>
	</comments>
</bug>