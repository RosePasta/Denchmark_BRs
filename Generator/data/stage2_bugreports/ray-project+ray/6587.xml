<bug id='6587' author='flying-mojo' open_date='2019-12-23T10:24:29Z' closed_time='2019-12-28T17:51:10Z'>
	<summary>Eager execution with Tuple actions space failing</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

When executing in eager mode with Tuple action space the execution fails (tried on IMPALA, APPO):
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 568, in _make_workers                                          │
logdir=self.logdir)                                                                                                                                        │
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/evaluation/worker_set.py", line 64, in init                                         │
RolloutWorker, env_creator, policy, 0, self._local_config)                                                                                                 │
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/evaluation/worker_set.py", line 220, in _make_worker                                    │
_fake_sampler=config.get("_fake_sampler", False))                                                                                                          │
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py", line 353, in init                                    │
policy_dict, policy_config)                                                                                                                                │
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py", line 766, in _build_policy_map                           │
policy_map[name] = cls(obs_space, act_space, merged_conf)                                                                                                  │
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/policy/eager_tf_policy.py", line 95, in init                                        │
super(TracedEagerPolicy, self).init(*args, **kwargs)                                                                                                   │
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/policy/eager_tf_policy.py", line 247, in init                                       │
self._initialize_loss_with_dummy_batch()                                                                                                                   │
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/policy/eager_tf_policy.py", line 522, in _initialize_loss_with_dummy_batch              │
loss_fn(self, self.model, self.dist_class, postprocessed_batch)                                                                                            │
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/agents/ppo/appo_policy.py", line 337, in build_appo_surrogate_loss                      │
mean_kl = make_time_major(prev_action_dist.multi_kl(action_dist))                                                                                          │
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/agents/ppo/appo_policy.py", line 250, in make_time_major                                │
**kw)                                                                                                                                                      │
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/agents/impala/vtrace_policy.py", line 144, in _make_time_major                          │
B = tf.shape(tensor)[0] // T                                                                                                                               │
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py", line 813, in _slice_helper                              │
name=name)                                                                                                                                                 │
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py", line 979, in strided_slice                              │
shrink_axis_mask=shrink_axis_mask)                                                                                                                         │
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_array_ops.py", line 10372, in strided_slice                        │
_six.raise_from(_core._status_to_exception(e.code, message), None)                                                                                         │
File "", line 3, in raise_from                                                                                                                       │
tensorflow.python.framework.errors_impl.InvalidArgumentError: slice index 0 of dimension 0 out of bounds. [Op:StridedSlice] name: default_policy/strided_slice/
Ray version and other system information (Python version, TensorFlow version, OS):
ray 0.8.0
Does the problem occur on the latest wheels?
&lt;denchmark-h:h3&gt;Reproduction&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
run ReversedAddition-v0 in eager mode:
rllib train --run APPO --env ReversedAddition-v0│--eager --trace
If we cannot run your script, we cannot fix your issue.
	</description>
	<comments>
		<comment id='1' author='flying-mojo' date='2019-12-26T21:42:16Z'>
		Thanks for reporting. Could you try out &lt;denchmark-link:https://github.com/ray-project/ray/pull/6615&gt;#6615&lt;/denchmark-link&gt;
 ?
		</comment>
		<comment id='2' author='flying-mojo' date='2019-12-28T13:04:15Z'>
		Thanks Eric! I will test it when i run in eager mode again. Looking at the &lt;denchmark-link:https://github.com/ray-project/ray/pull/6615&gt;#6615&lt;/denchmark-link&gt;
 changelist it should fix it.
		</comment>
	</comments>
</bug>