<bug id='3048' author='marlonjan' open_date='2018-10-11T16:31:15Z' closed_time='2018-10-21T07:21:33Z'>
	<summary>[RLlib] action passed into env.step has wrong shape with Tuple space</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS High Sierra 10.13.6
Ray installed from (source or binary): binary (pip)
Ray version: 0.5.3
Python version: 3.6.4
Exact command to reproduce: self.action_space = gym.spaces.Tuple([box, box]), see code below

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

It seems that RLlib doesn't handle gym.spaces.Tuple action spaces the way it should. Below is a minimal example to reproduce the crash. On my laptop the code crashes on the first call of the step function. This is the  interesting part of the output I get:
&lt;denchmark-code&gt;====================================
self.action_space.sample():  (array([0.20552675, 0.08976637], dtype=float32), array([-0.1526904 ,  0.29178822], dtype=float32))
                    action:  [array([[-3.955200e-04,  1.199285e+00]], dtype=float32), array([[ 1.1612587, -0.886429 ]], dtype=float32)]
====================================
&lt;/denchmark-code&gt;

As we can see, the action passed in contains 2d arrays, while 1d arrays are expected.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

import gym
import gym.spaces as spaces
import numpy as np
import ray
import ray.tune as tune
from ray.tune import register_env, Experiment


class CrashEnv(gym.Env):
    def __init__(self):
        box = spaces.Box(
            low=np.array([-1.0, -1.0]), high=np.array([1.0, 1.0]), dtype=np.float32
        )
        self.action_space = spaces.Tuple([box, box])
        self.observation_space = box  # not relevant for this example
        self.counter = 0

    def step(self, action):
        print("====================================")
        print("self.action_space.sample(): ", self.action_space.sample())
        print("                    action: ", action)
        print("====================================")
        
        # here it will crash
        assert self.action_space.contains(action)

        self.counter += 1

        observation = self.observation_space.sample()
        reward = 0.0
        done = self.counter &gt;= 100
        info = ""
        return observation, reward, done, info

    def reset(self):
        self.counter = 0
        return self.observation_space.sample()

    def render(self, mode="human"):
        pass


def env_creator(env_config):
    env = CrashEnv()
    print("env.action_space: ", env.action_space)
    print("env.observation_space: ", env.observation_space)
    return env


ENV_NAME = "crash_env"
register_env(ENV_NAME, env_creator)

if __name__ == "__main__":
    ray.init()
    tune.run_experiments(
        Experiment(
            name=ENV_NAME + "_experiment",
            run="PPO",
            stop={"episode_reward_mean": 1.0},
            checkpoint_at_end=True,
            config={"env": ENV_NAME, "num_gpus": 0, "num_workers": 1, "lr": 0.001},
        )
    )
Couldn't find any duplicate issues, hopefully I didn't overlook any.
Thanks for creating Ray and RLlib!
	</description>
	<comments>
		<comment id='1' author='marlonjan' date='2018-10-12T21:15:40Z'>
		&lt;denchmark-link:https://github.com/marlonjan&gt;@marlonjan&lt;/denchmark-link&gt;
 this should be fixed in &lt;denchmark-link:https://github.com/ray-project/ray/pull/3051&gt;#3051&lt;/denchmark-link&gt;
, would be great if you could give it a try.
		</comment>
	</comments>
</bug>