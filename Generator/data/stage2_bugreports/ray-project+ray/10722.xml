<bug id='10722' author='stephanie-wang' open_date='2020-09-10T19:13:43Z' closed_time='2020-09-11T22:35:04Z'>
	<summary>Can't start standalone Ray with ray.init()</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

Ray version and other system information (Python version, TensorFlow version, OS): nightly wheels, Ubuntu 18.04
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
Run ray.init().
There may just be something wrong with my environment setup. I wasn't seeing this issue before, but it came up suddenly and now ray.init() consistently fails to start.
The GCS logs in gcs_server.err seem to be the stacktrace at the root of the issue:
&lt;denchmark-code&gt;*** Aborted at 1599764989 (unix time) try "date -d @1599764989" if you are using GNU date ***
PC: @                0x0 (unknown)
*** SIGSEGV (@0x0) received by PID 9439 (TID 0x7fc57df817c0) from PID 0; stack trace: ***
    @     0x7fc57d4628a0 (unknown)
    @     0x55c3f8c12e20 NetIf::GetValidLocalIpCandidates()
    @     0x55c3f8c153f6 GetValidLocalIp()
    @     0x55c3f8a05b4f ray::gcs::GcsServer::StoreGcsServerAddressInRedis()
    @     0x55c3f8a06230 _ZNSt17_Function_handlerIFvvEZZN3ray3gcs9GcsServer5StartEvENKUlvE0_clEvEUlvE_E9_M_invokeERKSt9_Any_data
    @     0x55c3f8a41963 _ZZN3ray3gcs15GcsActorManager15LoadInitialDataERKSt8functionIFvvEEENKUlRKSt13unordered_mapINS_7ActorIDENS_3rpc14ActorTableDataESt4hashIS8_ESt8equal_toIS8_ESaISt4pairIKS8_SA_EEEE_clESL_
    @     0x55c3f8a20706 _ZNSt17_Function_handlerIFvRKSt13unordered_mapISsSsSt4hashISsESt8equal_toISsESaISt4pairIKSsSsEEEEZN3ray3gcs8GcsTableINSD_7ActorIDENSD_3rpc14ActorTableDataEE6GetAllERKSt8functionIFvRKS0_ISG_SI_S1_ISG_ES3_ISG_ESaIS5_IKSG_SI_EEEEEEUlSB_E_E9_M_invokeERKSt9_Any_dataSB_
    @     0x55c3f8b5956d _ZNSt17_Function_handlerIFvN3ray6StatusERKSt6vectorISsSaISsEEEZNS0_3gcs16RedisStoreClient12RedisScanner17ScanKeysAndValuesESsRKSt8functionIFvRKSt13unordered_mapISsSsSt4hashISsESt8equal_toISsESaISt4pairIKSsSsEEEEEEUlRKS1_S6_E_E9_M_invokeERKSt9_Any_dataS1_S6_
    @     0x55c3f8b55a02 _ZNSt17_Function_handlerIFvN3ray6StatusEEZNS0_3gcs16RedisStoreClient12RedisScanner8ScanKeysESsRKSt8functionIFvS1_RKSt6vectorISsSaISsEEEEEUlRKS1_E_E9_M_invokeERKSt9_Any_dataS1_
    @     0x55c3f8a471fd std::function&lt;&gt;::operator()()
    @     0x55c3f8b57bcf ray::gcs::RedisStoreClient::RedisScanner::Scan()
    @     0x55c3f8b59086 ray::gcs::RedisStoreClient::RedisScanner::OnScanCallback()
    @     0x55c3f8b592ae _ZNSt17_Function_handlerIFvSt10shared_ptrIN3ray3gcs13CallbackReplyEEEZNS2_16RedisStoreClient12RedisScanner4ScanESsRKSt8functionIFvNS1_6StatusEEEEUlRKS4_E_E9_M_invokeERKSt9_Any_dataS4_
    @     0x55c3f8ae464b _ZN5boost4asio6detail18completion_handlerIZN3ray3gcs20RedisCallbackManager12CallbackItem8DispatchERSt10shared_ptrINS4_13CallbackReplyEEEUlvE_E11do_completeEPvPNS1_19scheduler_operationERKNS_6system10error_codeEm
    @     0x55c3f8d5770f boost::asio::detail::scheduler::do_run_one()
    @     0x55c3f8d58bf1 boost::asio::detail::scheduler::run()
    @     0x55c3f8d59c22 boost::asio::io_context::run()
    @     0x55c3f897c2b7 main
    @     0x7fc57cc60b97 __libc_start_main
    @     0x55c3f8985d51 (unknown)
&lt;/denchmark-code&gt;

If we cannot run your script, we cannot fix your issue.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='stephanie-wang' date='2020-09-10T19:16:55Z'>
		Confirmed that it's working on 0.8.7 but not the nightly wheels.
		</comment>
		<comment id='2' author='stephanie-wang' date='2020-09-10T19:33:05Z'>
		FYI, &lt;denchmark-link:https://github.com/barakmich&gt;@barakmich&lt;/denchmark-link&gt;
.
It doesn't seem to be consistently reproducible, so I'll spend some time trying to fix it if it comes up again today. If I can't reproduce it, I'll remove the release-blocker tag.
		</comment>
		<comment id='3' author='stephanie-wang' date='2020-09-11T03:28:55Z'>
		Can't figure this out or reproduce at the moment, so I'm going to close for now.
		</comment>
		<comment id='4' author='stephanie-wang' date='2020-09-11T03:36:41Z'>
		Maybe there was a memory corruption issue in that method? We rewrote the gcs server network resolution code (so that it will work inside docker) after 0.8.7
		</comment>
		<comment id='5' author='stephanie-wang' date='2020-09-11T03:45:41Z'>
		Ah I didn't realize that. Looking at the code, it does seem very likely... I'll reopen this issue for now, but I'm not sure how to reproduce it.
		</comment>
		<comment id='6' author='stephanie-wang' date='2020-09-11T03:51:08Z'>
		Maybe we keep running asan tests on this method, we can probably detect them. I will spend some time later to write the test
		</comment>
	</comments>
</bug>