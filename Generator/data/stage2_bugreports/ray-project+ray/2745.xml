<bug id='2745' author='alexander-ossur' open_date='2018-08-25T18:00:57Z' closed_time='2018-08-25T21:56:35Z'>
	<summary>failed call to cuInit: CUDA_ERROR_NO_DEVICE</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 Desktop
Ray installed from (source or binary): binary
Ray version: 0.5.0
Python version:  Python 3.6.5 :: Anaconda, Inc.
Exact command to reproduce:

&lt;denchmark-h:h3&gt;More details&lt;/denchmark-h&gt;

&lt;denchmark-h:h4&gt;$ nvidia-smi&lt;/denchmark-h&gt;

Full device name is GeForce GTX 1080 ti

$ nvidia-smi
Sat Aug 25 19:48:32 2018
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 396.54                 Driver Version: 396.54                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 108...  Off  | 00000000:02:00.0  On |                  N/A |
|  0%   59C    P0    65W / 280W |    881MiB / 11177MiB |      1%      Default |
+-------------------------------+----------------------+----------------------+
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1293      G   /usr/lib/xorg/Xorg                            30MiB |
|    0      1990      G   /usr/bin/gnome-shell                          50MiB |
|    0      2192      G   /usr/lib/xorg/Xorg                           350MiB |
|    0      2320      G   /usr/bin/gnome-shell                         187MiB |
|    0      2910      G   ...dr/.local/share/Steam/ubuntu12_32/steam    40MiB |
|    0      2926      G   ./steamwebhelper                              25MiB |
|    0      4421      G   /usr/bin/nvidia-settings                       3MiB |
|    0      5915      G   ...-token=D7B50E3DB174AF6DDCE0200BFD3CCBA3   114MiB |
|    0      6108      G   ...-token=DDE57EE2E0DBC9F56D3C297532ED46C1    74MiB |
+-----------------------------------------------------------------------------+

&lt;denchmark-h:h4&gt;$ nvcc -V&lt;/denchmark-h&gt;


$ nvcc -V
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2017 NVIDIA Corporation
Built on Fri_Sep__1_21:08:03_CDT_2017
Cuda compilation tools, release 9.0, V9.0.176

&lt;denchmark-h:h4&gt;$ ldconfig -p | grep cuda&lt;/denchmark-h&gt;


$ ldconfig -p | grep cuda
libicudata.so.60 (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libicudata.so.60
libcuda.so.1 (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libcuda.so.1
libcuda.so.1 (libc6) =&gt; /usr/lib/i386-linux-gnu/libcuda.so.1
libcuda.so (libc6,x86-64) =&gt; /usr/lib/x86_64-linux-gnu/libcuda.so
libcuda.so (libc6) =&gt; /usr/lib/i386-linux-gnu/libcuda.so

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

Just testing a code from the documentation. It executes well, but running on CPU
&lt;denchmark-code&gt;import ray
import ray.rllib.agents.ppo as ppo
from ray.tune.logger import pretty_print

ray.init()
config = ppo.DEFAULT_CONFIG.copy()
agent = ppo.PPOAgent(config=config, env="CartPole-v0")

# Can optionally call agent.restore(path) to load a checkpoint.

for i in range(1000):
   # Perform one iteration of training the policy with PPO
   result = agent.train()
   print(pretty_print(result))

   if i % 100 == 0:
       checkpoint = agent.save()
       print("checkpoint saved at", checkpoint)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;


cd /home/aleksandr/Workspace/trad/tensorflow-test ; env "PYTHONIOENCODING=UTF-8" "PYTHONUNBUFFERED=1" /home/aleksandr/anaconda3/envs/ml-agents/bin/python /home/aleksandr/.vscode/extensions/ms-python.python-2018.7.1/pythonFiles/PythonTools/visualstudio_py_launcher.py /home/aleksandr/Workspace/trad/tensorflow-test 40205 34806ad9-833a-4524-8cd6-18ca4aa74f14 RedirectOutput,RedirectOutput /home/aleksandr/Workspace/trad/tensorflow-test/test.py
/home/aleksandr/anaconda3/envs/ml-agents/lib/python3.6/site-packages/h5py/init.py:36: FutureWarning: Conversion of the second argument of issubdtype from float to np.floating is deprecated. In future, it will be treated as np.float64 == np.dtype(float).type.
from ._conv import register_converters as _register_converters
Process STDOUT and STDERR is being redirected to /tmp/raylogs/.
Waiting for redis server at 127.0.0.1:12649 to respond...
Waiting for redis server at 127.0.0.1:26478 to respond...
Starting local scheduler with the following resources: {'CPU': 12, 'GPU': 1}.
======================================================================
View the web UI at http://localhost:8888/notebooks/ray_ui29358.ipynb?token=82f85a30c971061a7cc79edd15b302a743697685f6e408c9
Created LogSyncer for /home/aleksandr/ray_results/2018-08-25_19-45-354k61l7de -&gt; None
WARN: gym.spaces.Box autodetected dtype as &lt;class 'numpy.float32'&gt;. Please provide explicit dtype.
2018-08-25 19:45:35.494325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-08-25 19:45:35.495076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6575
pciBusID: 0000:02:00.0
totalMemory: 10.92GiB freeMemory: 9.90GiB
2018-08-25 19:45:35.495108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-08-25 19:45:35.826567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-25 19:45:35.826600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0
2018-08-25 19:45:35.826608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N
2018-08-25 19:45:35.826853: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9569 MB memory) -&gt; physical GPU (device:0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)
LocalMultiGPUOptimizer devices ['/cpu:0']
LocalMultiGPUOptimizer batch size 128
/home/aleksandr/anaconda3/envs/ml-agents/lib/python3.6/site-packages/h5py/init.py:36: FutureWarning: Conversion of the second argument of issubdtype from float to np.floating is deprecated. In future, it will be treated as np.float64 == np.dtype(float).type.
from ._conv import register_converters as _register_converters
/home/aleksandr/anaconda3/envs/ml-agents/lib/python3.6/site-packages/h5py/init.py:36: FutureWarning: Conversion of the second argument of issubdtype from float to np.floating is deprecated. In future, it will be treated as np.float64 == np.dtype(float).type.
from ._conv import register_converters as _register_converters
WARN: gym.spaces.Box autodetected dtype as &lt;class 'numpy.float32'&gt;. Please provide explicit dtype.
2018-08-25 19:45:48.618746: E tensorflow/stream_executor/cuda/cuda_driver.cc:397] failed call to cuInit: CUDA_ERROR_NO_DEVICE
2018-08-25 19:45:48.618785: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] retrieving CUDA diagnostic information for host: angl-ml
2018-08-25 19:45:48.618795: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:170] hostname: angl-ml
2018-08-25 19:45:48.618833: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:194] libcuda reported version is: 396.54.0
2018-08-25 19:45:48.618861: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:198] kernel reported version is: 396.54.0
2018-08-25 19:45:48.618872: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:305] kernel version seems to match DSO: 396.54.0
WARN: gym.spaces.Box autodetected dtype as &lt;class 'numpy.float32'&gt;. Please provide explicit dtype.
2018-08-25 19:45:48.724380: E tensorflow/stream_executor/cuda/cuda_driver.cc:397] failed call to cuInit: CUDA_ERROR_NO_DEVICE
2018-08-25 19:45:48.724417: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] retrieving CUDA diagnostic information for host: angl-ml
2018-08-25 19:45:48.724430: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:170] hostname: angl-ml
2018-08-25 19:45:48.724480: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:194] libcuda reported version is: 396.54.0
2018-08-25 19:45:48.724522: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:198] kernel reported version is: 396.54.0
2018-08-25 19:45:48.724539: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:305] kernel version seems to match DSO: 396.54.0
WARNING: Serializing objects of type &lt;class 'ray.rllib.evaluation.sample_batch.SampleBatch'&gt; by expanding them as dictionaries of their fields. This behavior may be incorrect in some cases.
WARNING: Serializing objects of type &lt;class 'ray.rllib.evaluation.sample_batch.SampleBatch'&gt; by expanding them as dictionaries of their fields. This behavior may be incorrect in some cases.
/home/aleksandr/anaconda3/envs/ml-agents/lib/python3.6/site-packages/h5py/init.py:36: FutureWarning: Conversion of the second argument of issubdtype from float to np.floating is deprecated. In future, it will be treated as np.float64 == np.dtype(float).type.
from ._conv import register_converters as _register_converters
/home/aleksandr/anaconda3/envs/ml-agents/lib/python3.6/site-packages/h5py/init.py:36: FutureWarning: Conversion of the second argument of issubdtype from float to np.floating is deprecated. In future, it will be treated as np.float64 == np.dtype(float).type.
from ._conv import register_converters as _register_converters
/home/aleksandr/anaconda3/envs/ml-agents/lib/python3.6/site-packages/h5py/init.py:36: FutureWarning: Conversion of the second argument of issubdtype from float to np.floating is deprecated. In future, it will be treated as np.float64 == np.dtype(float).type.
from ._conv import register_converters as _register_converters
/home/aleksandr/anaconda3/envs/ml-agents/lib/python3.6/site-packages/h5py/init.py:36: FutureWarning: Conversion of the second argument of issubdtype from float to np.floating is deprecated. In future, it will be treated as np.float64 == np.dtype(float).type.
from ._conv import register_converters as _register_converters
/home/aleksandr/anaconda3/envs/ml-agents/lib/python3.6/site-packages/h5py/init.py:36: FutureWarning: Conversion of the second argument of issubdtype from float to np.floating is deprecated. In future, it will be treated as np.float64 == np.dtype(float).type.
from ._conv import register_converters as _register_converters
/home/aleksandr/anaconda3/envs/ml-agents/lib/python3.6/site-packages/h5py/init.py:36: FutureWarning: Conversion of the second argument of issubdtype from float to np.floating is deprecated. In future, it will be treated as np.float64 == np.dtype(float).type.
from ._conv import register_converters as _register_converters
/home/aleksandr/anaconda3/envs/ml-agents/lib/python3.6/site-packages/h5py/init.py:36: FutureWarning: Conversion of the second argument of issubdtype from float to np.floating is deprecated. In future, it will be treated as np.float64 == np.dtype(float).type.
from ._conv import register_converters as _register_converters
/home/aleksandr/anaconda3/envs/ml-agents/lib/python3.6/site-packages/h5py/init.py:36: FutureWarning: Conversion of the second argument of issubdtype from float to np.floating is deprecated. In future, it will be treated as np.float64 == np.dtype(float).type.
from ._conv import register_converters as _register_converters
/home/aleksandr/anaconda3/envs/ml-agents/lib/python3.6/site-packages/h5py/init.py:36: FutureWarning: Conversion of the second argument of issubdtype from float to np.floating is deprecated. In future, it will be treated as np.float64 == np.dtype(float).type.
from ._conv import register_converters as _register_converters
/home/aleksandr/anaconda3/envs/ml-agents/lib/python3.6/site-packages/h5py/init.py:36: FutureWarning: Conversion of the second argument of issubdtype from float to np.floating is deprecated. In future, it will be treated as np.float64 == np.dtype(float).type.
from ._conv import register_converters as _register_converters
WARNING: Serializing objects of type &lt;class 'ray.rllib.utils.filter.MeanStdFilter'&gt; by expanding them as dictionaries of their fields. This behavior may be incorrect in some cases.
WARNING: Serializing objects of type &lt;class 'ray.rllib.utils.filter.MeanStdFilter'&gt; by expanding them as dictionaries of their fields. This behavior may be incorrect in some cases.
WARNING: Serializing objects of type &lt;class 'ray.rllib.utils.filter.RunningStat'&gt; by expanding them as dictionaries of their fields. This behavior may be incorrect in some cases.
WARNING: Serializing objects of type &lt;class 'ray.rllib.utils.filter.RunningStat'&gt; by expanding them as dictionaries of their fields. This behavior may be incorrect in some cases.
WARNING: Serializing objects of type &lt;class 'ray.rllib.evaluation.sampler.RolloutMetrics'&gt; by expanding them as dictionaries of their fields. This behavior may be incorrect in some cases.
/home/aleksandr/anaconda3/envs/ml-agents/lib/python3.6/site-packages/ray/tune/logger.py:177: FutureWarning: Conversion of the second argument of issubdtype from float to np.floating is deprecated. In future, it will be treated as np.float64 == np.dtype(float).type.
if np.issubdtype(value, float):
/home/aleksandr/anaconda3/envs/ml-agents/lib/python3.6/site-packages/ray/tune/logger.py:179: FutureWarning: Conversion of the second argument of issubdtype from int to np.signedinteger is deprecated. In future, it will be treated as np.int64 == np.dtype(int).type.
if np.issubdtype(value, int):
date: 2018-08-25_19-46-15
episode_len_mean: 22.756345177664976
episode_reward_max: 87.0
episode_reward_mean: 22.756345177664976
episode_reward_min: 9.0
episodes_total: 197
experiment_id: 8dc5aed1ee2f4c2997f868ff0e2fd6fc
hostname: angl-ml
info:
entropy: 0.6593437194824219
kl_coefficient: 0.30000000000000004
kl_divergence: 0.03459303453564644
policy_loss: -0.052506063133478165
total_loss: 1.6484274864196777
vf_loss: 169.4014892578125
node_ip: 192.168.100.5
pid: 12584
policy_reward_mean:
default: 22.756345177664976
time_this_iter_s: 12.50081753730774
time_total_s: 12.50081753730774
timestamp: 1535219175
timesteps_this_iter: 4483
timesteps_total: 4483
training_iteration: 1

&lt;denchmark-h:h3&gt;What I tried&lt;/denchmark-h&gt;

Changed environment settings
&lt;denchmark-code&gt;import os

os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"]="0"
&lt;/denchmark-code&gt;

Set a number of GPUs
ray.init (num_gpus=1)
	</description>
	<comments>
		<comment id='1' author='alexander-ossur' date='2018-08-25T18:05:46Z'>
		PPO controls it's GPU allocation internally. You need to set num_gpus: 1 in the config (I would also recommend running it with Tune which takes care of requesting resources from Ray)
		</comment>
		<comment id='2' author='alexander-ossur' date='2018-08-25T18:06:48Z'>
		Hm maybe the default example should include GPU settings
		</comment>
		<comment id='3' author='alexander-ossur' date='2018-08-25T18:20:37Z'>
		Setting num_gpus: 1 in the config still returns CUDA_ERROR_NO_DEVICE and continues running on CPU.
Good to mention though I see some GPU activity in NVIDIA X Server Settings. Sometimes it showing me GPU Utilization around 30%. All 12 CPU cores at 78%.
		</comment>
		<comment id='4' author='alexander-ossur' date='2018-08-25T21:54:32Z'>
		That's odd then, I'm guessing this is a tensorflow issue, or a harmless warning? You can check if the ops are placed on a GPU device by setting the config tf_session_args: {log_device_placement: true}
		</comment>
		<comment id='5' author='alexander-ossur' date='2018-08-25T21:55:46Z'>
		Oh wait, I see you're using the 0.5.0 binary release. This is a known issue resolved by: &lt;denchmark-link:https://github.com/ray-project/ray/issues/2532&gt;#2532&lt;/denchmark-link&gt;
 then.
		</comment>
		<comment id='6' author='alexander-ossur' date='2018-08-27T21:23:10Z'>
		Thank you &lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;

I applied the &lt;denchmark-link:https://github.com/ray-project/ray/pull/2535&gt;#2535&lt;/denchmark-link&gt;
 patch. Logs are now showing:

...
Using custom preprocessor my_preprocessor
LocalMultiGPUOptimizer devices ['/gpu:0']
Using custom model my_model
...

Although CUDA_ERROR_NO_DEVICE is still present

...
Using custom model my_model
Using custom model my_model
Using custom preprocessor my_preprocessor
2018-08-27 23:11:27.088185: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable deviceis detected
2018-08-27 23:11:27.088229: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] retrieving CUDA diagnostic information for host: angl-ml
2018-08-27 23:11:27.088240: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:170] hostname: angl-ml
2018-08-27 23:11:27.088284: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:194] libcuda reported version is: 396.54.0
2018-08-27 23:11:27.088317: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:198] kernel reported version is: 396.54.0
2018-08-27 23:11:27.088328: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:305] kernel version seems to match DSO: 396.54.0
Using custom model my_model
Using custom model my_model
Using custom preprocessor my_preprocessor
Using custom preprocessor my_preprocessor
...

In addition after the first post I made a tensorflow-gpu reinstall from a sources, and it's still showing CUDA_ERROR_NO_DEVICE.
		</comment>
		<comment id='7' author='alexander-ossur' date='2018-08-27T22:21:25Z'>
		Note that the PPO workers will not be using a GPU and may print that
warning. So it might be fine if you see GPU utilization in nvidia-smi.

You can also try other algorithms such as DQN: ./train.py -f
tuned_examples/pong-dqn.yaml
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Mon, Aug 27, 2018, 2:23 PM alexander-ossur ***@***.***&gt; wrote:
 Thank you @ericl &lt;https://github.com/ericl&gt;

 I applied the #2535 &lt;#2535&gt; patch.
 Logs are now showing:

 ...
 Using custom preprocessor my_preprocessor
 LocalMultiGPUOptimizer devices ['/gpu:0']
 Using custom model my_model
 ...

 Although *CUDA_ERROR_NO_DEVICE* is still present

 ...
 Using custom model my_model
 Using custom model my_model
 Using custom preprocessor my_preprocessor
 2018-08-27 23:11:27.088185: E
 tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit:
 CUDA_ERROR_NO_DEVICE: no CUDA-capable deviceis detected
 2018-08-27 23:11:27.088229: I
 tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] retrieving CUDA
 diagnostic information for host: angl-ml
 2018-08-27 23:11:27.088240: I
 tensorflow/stream_executor/cuda/cuda_diagnostics.cc:170] hostname: angl-ml
 2018-08-27 23:11:27.088284: I
 tensorflow/stream_executor/cuda/cuda_diagnostics.cc:194] libcuda reported
 version is: 396.54.0
 2018-08-27 23:11:27.088317: I
 tensorflow/stream_executor/cuda/cuda_diagnostics.cc:198] kernel reported
 version is: 396.54.0
 2018-08-27 23:11:27.088328: I
 tensorflow/stream_executor/cuda/cuda_diagnostics.cc:305] kernel version
 seems to match DSO: 396.54.0
 Using custom model my_model
 Using custom model my_model
 Using custom preprocessor my_preprocessor
 Using custom preprocessor my_preprocessor
 ...

 In addition after the first post I made a *tensorflow-gpu* reinstall from
 a sources, and it's still showing *CUDA_ERROR_NO_DEVICE*.

 —
 You are receiving this because you were mentioned.


 Reply to this email directly, view it on GitHub
 &lt;#2745 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AAA6SnAdz9Km8mtbs_yZ3Kt6mmek5zlbks5uVGNLgaJpZM4WMepb&gt;
 .



		</comment>
	</comments>
</bug>