<bug id='12052' author='danwallach' open_date='2020-11-16T23:10:31Z' closed_time='2020-11-28T05:47:47Z'>
	<summary>"slow start" launching worker processes on new nodes</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

This is with Ray 1.0.1 on Ubuntu 20.04 on AWS c5a servers.
I create a cluster with 13 worker computers, each a c5a-16xlarge node on AWS, so 64 vCPUs per worker computer (13*64 = 832). Watching the Ray dashboard, I can see the number of workers, at the bottom, versus the number of cores. It takes somewhere between 3.5 and 4 minutes for the number of worker processes to equal the number of cores.
Under Ray 1.0.0, the worker processes were launched at the start, and I'd immediately have full CPU utilization across my cluster. Now it takes nearly four minutes. Once some other issues get resolved, I'd like to increase the number of vCPUs by a factor of ten or more, at which point this "slow start" behavior would be the gating factor in my ability to achieve scalable performance.
(Right now, some other unrelated bugs are limiting my ability to add more workers.)
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

I ran a demo for &lt;denchmark-link:https://github.com/rkooo567&gt;@rkooo567&lt;/denchmark-link&gt;
 so he could see the behavior.
Desired fix? Some way of telling Ray to start worker processes immediately when nodes are launched.
	</description>
	<comments>
		<comment id='1' author='danwallach' date='2020-11-16T23:16:03Z'>
		cc &lt;denchmark-link:https://github.com/kfstorm&gt;@kfstorm&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='danwallach' date='2020-11-16T23:21:23Z'>
		&lt;denchmark-link:https://github.com/kfstorm&gt;@kfstorm&lt;/denchmark-link&gt;
 could there be more going on here than the worker slow start? IIRC it should just be a couple second delay, not minutes
		</comment>
		<comment id='3' author='danwallach' date='2020-11-16T23:53:29Z'>
		Btw, I will add process startup time to our set of metrics to troubleshoot this sort of performance issue better. Also, afaik, the process startup took less than a millisecond when we merged &lt;denchmark-link:https://github.com/kfstorm&gt;@kfstorm&lt;/denchmark-link&gt;
's PR, so it is a bit surprising.
		</comment>
		<comment id='4' author='danwallach' date='2020-11-17T03:27:35Z'>
		&lt;denchmark-link:https://github.com/rkooo567&gt;@rkooo567&lt;/denchmark-link&gt;
 is it possible to attach the repro script to this issue?
		</comment>
		<comment id='5' author='danwallach' date='2020-11-17T06:40:24Z'>
		Starting from 1.0.1, workers are started on demand. If we start 832 small normal tasks (e.g. each finishes within 1ms), we can't guarantee that every task will be executed on a different worker. Some tasks are very likely to be scheduled on the same worker because they are extremely fast to execute and when a task is scheduling, another task may have finished executing and an existing worker is now able to execute another task.
So it is possible that we need to run far more than 832 tasks to finally bring up 832 workers, and it may take a long time. This makes sense to me.
By the way, &lt;denchmark-link:https://github.com/danwallach&gt;@danwallach&lt;/denchmark-link&gt;
 what's your requirement that you need to have 64 workers running on a node?
		</comment>
		<comment id='6' author='danwallach' date='2020-11-17T14:01:17Z'>
		My tasks typically run for about 30 seconds, and I launch about 2500 of them in parallel at a time.
I'm using c5a-16xlarge nodes, so 64 vCPUs per node. I'd like to have all those vCPUs busy.
		</comment>
		<comment id='7' author='danwallach' date='2020-11-18T02:07:32Z'>
		30 seconds sounds like it should be triggering scale-up then. &lt;denchmark-link:https://github.com/danwallach&gt;@danwallach&lt;/denchmark-link&gt;
 can you provide a self-contained reproduction script? You can use mock tasks, like
&lt;denchmark-code&gt;@ray.remote
def f():
  time.sleep(30)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='danwallach' date='2020-11-18T03:35:51Z'>
		I've got a "no-op" version of my job that I built for &lt;denchmark-link:https://github.com/rkooo567&gt;@rkooo567&lt;/denchmark-link&gt;
 on a different bug. I haven't tried it yet with 1.0.1, but it should have the same behavior.
		</comment>
		<comment id='9' author='danwallach' date='2020-11-18T03:37:44Z'>
		Yeah &lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
 this is the same one that I ran about 3-4 weeks ago and couldnâ€™t reproduce the issue (different one) at that time. I remember it took some time to actually spawn all workers for that script too. I can share the repo with you (after dinner)
		</comment>
		<comment id='10' author='danwallach' date='2020-11-18T04:27:32Z'>
		&lt;denchmark-link:https://github.com/rkooo567/arlo-e2e-noop&gt;https://github.com/rkooo567/arlo-e2e-noop&lt;/denchmark-link&gt;

(This should be just runnable at least when I tried last time).
		</comment>
		<comment id='11' author='danwallach' date='2020-11-18T06:44:15Z'>
		&lt;denchmark-link:https://github.com/rkooo567&gt;@rkooo567&lt;/denchmark-link&gt;
 can we come up with a simpler repro? The problem with a complex repro is that the issue can always be an issue in the application code. Have you tried reproducing this on that cluster with a simple wave of tasks?
Another thing to try is reproducing the issue on a different cluster. If it's not possible there might be some environment specific problem (slow NFS mount, etc).
		</comment>
		<comment id='12' author='danwallach' date='2020-11-18T06:47:31Z'>
		Ah, I didn't do that last time because I couldn't reproduce the original issue I just ran -&gt; it worked -&gt; so I stopped working. I can spend some time later to make simpler reproducible scripts.

Another thing to try is reproducing the issue on a different cluster. If it's not possible there might be some environment specific problem (slow NFS mount, etc).

This is totally possible because it seems like the AMI I used didn't have the original issue, but Dan's ami had that for some reason (didn't verify). I will try reproducing it again and creating the simpler version of scripts that can possibly reproduce the same problems.
		</comment>
		<comment id='13' author='danwallach' date='2020-11-23T22:24:41Z'>
		Here's a simple repro script and results:
&lt;denchmark-code&gt;import ray
import time

ray.init(address="auto")

@ray.remote
def f(start):
    print("launched", time.time() - start)
    time.sleep(30)
    return time.time()

start = time.time()
results = [f.remote(start) for _ in range(800)]

results = ray.get(results)
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/14922/100022752-94d1fa00-2d98-11eb-98db-1e52ebdba990.png&gt;&lt;/denchmark-link&gt;

I believe there are two problems here:
(1) workers aren't being launched concurrently. So we wait for one worker to launch before launching another.
(2) tasks aren't being spilled to other nodes until workers are launched.
Interestingly this seems to be much better with RAY_ENABLE_NEW_SCHEDULER=1 (new scheduler).
		</comment>
		<comment id='14' author='danwallach' date='2020-11-23T22:39:45Z'>
		&lt;denchmark-link:https://github.com/kfstorm&gt;@kfstorm&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/stephanie-wang&gt;@stephanie-wang&lt;/denchmark-link&gt;
 what do you think the best way to tackle this issue is? It's a clear regression with multi-tenancy enabled.
I see a couple options:
(1) disable multi-tenancy by default until the new scheduler is ready (note that the disabling multi-tenancy flag doesn't seem to work!)
(2) try to fix the issue in the old scheduler (might be wasted effort)
(3) punt this issue until the new scheduler is ready (double down on enabling this by default)
Btw &lt;denchmark-link:https://github.com/danwallach&gt;@danwallach&lt;/denchmark-link&gt;
 seems like you can workaround the issue for now by setting RAY_ENABLE_NEW_SCHEDULER=1 during ray start (though this is an experimental feature).
		</comment>
		<comment id='15' author='danwallach' date='2020-11-23T22:48:24Z'>
		Actually, there were several PRs that were merged which can be broken when the multi-tenancy is off (for example, in logging, we assume every workers have job id now).
		</comment>
		<comment id='16' author='danwallach' date='2020-11-23T22:58:09Z'>
		
Interestingly this seems to be much better with RAY_ENABLE_NEW_SCHEDULER=1 (new scheduler).

I think the new scheduler is much more aggressive about calling WorkerPool::PopWorker. I think we could probably do the same in the old scheduler (of course we need to be careful about leaking workers), but the new scheduler is getting pretty darn close...
		</comment>
		<comment id='17' author='danwallach' date='2020-11-24T10:27:31Z'>
		&lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
 I tried your repro script and compared the output between &lt;denchmark-link:https://github.com/ray-project/ray/commit/f54f7b23f6f056f423f85248c307d0b09264fcef&gt;f54f7b2&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/ray-project/ray/commit/3504391fd2b65ef1777e3d361a23e126add107e5&gt;3504391&lt;/denchmark-link&gt;
.
There are several factors that caused this issue:

No initial worker is started when multi-tenancy is enabled.
The number of pending lease requests in a worker is limited to 1 per scheduling key.

So when the driver starts a bunch of tasks, it has to wait for the completion of a worker lease request before the next lease request can be sent.
Before 1.0.1, A lease request is quite fast because there are lots of initial workers stand by.
I also tried adding num_cpus=0 to @ray.remote in the repro script and here is the output of driver in a single node cluster (8 cores):
&lt;denchmark-code&gt;$ python test.py
2020-11-24 18:11:40,967	INFO worker.py:651 -- Connecting to existing Ray cluster at address: 100.88.111.11:6379
(pid=51911) launched 0.011482000350952148
(pid=51909) launched 0.013924360275268555
(pid=51908) launched 0.006989955902099609
(pid=51910) launched 0.009813785552978516
(pid=51905) launched 0.027834177017211914
(pid=51907) launched 0.011026620864868164
(pid=51906) launched 0.011239767074584961
(pid=51904) launched 0.044062137603759766
(pid=52137) launched 0.7869358062744141
(pid=52154) launched 1.3189525604248047
(pid=52172) launched 1.8928070068359375
(pid=52192) launched 2.431439161300659
(pid=52211) launched 3.1983485221862793
(pid=52238) launched 3.91973614692688
(pid=52255) launched 4.665119647979736
(pid=52285) launched 5.199910640716553
(pid=52303) launched 5.968857049942017
(pid=52321) launched 6.557571887969971
(pid=52339) launched 7.099905252456665
(pid=52360) launched 7.830197811126709
(pid=52379) launched 8.439499378204346
(pid=52398) launched 8.909159660339355
&lt;/denchmark-code&gt;

It looks straightforward to me that the "slow start" issue appears anyway once all the 8 initial workers are occupied.
So, is our current leasing strategy too conservative? I'm not sure why we need to limit pending lease requests.
BTW, why the behavior is different when the new scheduler is enabled? I assume the new scheduler is just a code refactoring.
		</comment>
		<comment id='18' author='danwallach' date='2020-11-24T19:56:38Z'>
		I'm not sure why the new scheduler behavior is different, I agree that is unexpected.
The reason we need to be conservative with leasing is to avoid overwhelming the raylet with requests and re-use leases as much as possible. However, this breaks if the lease response latency is non-trivial.
I'm not sure I see any super easy fixes here, either a smarter leasing algorithm or giving leases before the worker has fully started. Given this, I think the only sensible option is to revert the flag flip so the issue is fixed in 1.0.2, and we can work on a proper fix in the background. &lt;denchmark-link:https://github.com/rkooo567&gt;@rkooo567&lt;/denchmark-link&gt;
 what is the logging PR that also has to be reverted?
		</comment>
		<comment id='19' author='danwallach' date='2020-11-24T20:23:47Z'>
		I guess another option is to prewarm workers for some "default job" where the user doesn't specify any config, not sure if that is easy to do &lt;denchmark-link:https://github.com/kfstorm&gt;@kfstorm&lt;/denchmark-link&gt;

		</comment>
		<comment id='20' author='danwallach' date='2020-11-25T06:59:25Z'>
		Another option is to look at the client queue depth for the lease request (this should already be present), and optimistically prestart that number of workers if the pool hasn't reached it's max size.
This might be a pretty simple solution.
		</comment>
		<comment id='21' author='danwallach' date='2020-11-25T12:35:18Z'>
		
I guess another option is to prewarm workers for some "default job" where the user doesn't specify any config, not sure if that is easy to do @kfstorm

This won't fix the num_cpus=0 scenario, right? We'd better speed up the leasing process regardless of the number of initial workers.

Another option is to look at the client queue depth for the lease request (this should already be present), and optimistically prestart that number of workers if the pool hasn't reached it's max size.

I'm not sure what you mean. Maybe someone from your side to take a look?
		</comment>
		<comment id='22' author='danwallach' date='2020-11-25T21:14:38Z'>
		&lt;denchmark-link:https://github.com/kfstorm&gt;@kfstorm&lt;/denchmark-link&gt;
 sure, I'll take this one over. Hopefully we don't need to revert.
		</comment>
	</comments>
</bug>