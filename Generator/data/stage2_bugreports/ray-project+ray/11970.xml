<bug id='11970' author='iamhatesz' open_date='2020-11-12T12:36:23Z' closed_time='2020-11-25T11:32:14Z'>
	<summary>[rllib] RNN model receives empty state/seq_lens when upgraded from 1.0.0 to 1.0.1</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

A classical PyTorch RNN model (without RecurrentNetwork wrapper) does not receive proper state and seq_lens values after upgrading from Ray 1.0.0 to 1.0.1.
Ray version and other system information (Python version, TensorFlow version, OS): Ubuntu 20.04, Ray 1.0.0/1.0.1, PyTorch 1.6/1.7
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

This code works in 1.0.0, but it doesn't in 1.0.1 with an error:
&lt;denchmark-code&gt;    max_seq_len = flat_inputs.shape[0] // seq_lens.shape[0]
AttributeError: 'NoneType' object has no attribute 'shape'
&lt;/denchmark-code&gt;

import ray
import torch
import torch.nn as nn
import torch.nn.functional as F
from ray.rllib.models import ModelCatalog, ModelV2
from ray.rllib.models.torch.torch_modelv2 import TorchModelV2
from ray.rllib.policy.rnn_sequencing import add_time_dimension
from ray.rllib.utils import override
from ray.tune import tune


class CartPoleModel(TorchModelV2, nn.Module):
    def __init__(self, obs_space, action_space, num_outputs, model_config, name):
        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)
        nn.Module.__init__(self)

        self.fc1 = nn.Linear(4, 128)
        self.fc2 = nn.Linear(128, 128)
        self.lstm = nn.LSTM(128, 128, batch_first=True)
        self.action = nn.Linear(128, 2)
        self.value = nn.Linear(128, 1)
        self._value_out = None

    @override(TorchModelV2)
    def forward(self, input_dict, state, seq_lens):
        obs = input_dict["obs_flat"]
        x = F.relu(self.fc1(obs))
        x = F.relu(self.fc2(x))

        flat_inputs = input_dict["obs_flat"].float()
        max_seq_len = flat_inputs.shape[0] // seq_lens.shape[0]
        x_seq = add_time_dimension(
            x, max_seq_len=max_seq_len, framework="torch"
        )

        x, new_state = self.lstm(x_seq, (state[0].unsqueeze(0), state[1].unsqueeze(0)))

        action = self.action(x)
        value = self.value(x)
        self._value_out = value.reshape(-1)
        return torch.reshape(action, [-1, self.num_outputs]), [new_state[0].squeeze(0), new_state[1].squeeze(0)]

    @override(TorchModelV2)
    def value_function(self):
        return self._value_out

    @override(ModelV2)
    def get_initial_state(self):
        return [
            self.fc1.weight.new(
                1, 128
            )
            .zero_()
            .squeeze(0),
            self.fc1.weight.new(
                1, 128
            )
            .zero_()
            .squeeze(0),
        ]


ModelCatalog.register_custom_model("CartPoleModel", CartPoleModel)


if __name__ == "__main__":
    ray.init()

    tune.run(
        "PPO",
        checkpoint_freq=1,
        checkpoint_at_end=True,
        stop={"episode_reward_mean": 199.0},
        config={
            "env": "CartPole-v0",
            "framework": "torch",
            "model": {
                "custom_model": "CartPoleModel",
                "max_seq_len": 5,
            }
        }
    )

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='iamhatesz' date='2020-11-12T12:48:55Z'>
		From my investigation in 1.0.1 compute_actions_from_input_dict from TorchPolicy is being called instead of compute_actions, which populates state and seq_lens in a different way (not populating them at all in my case :)).
		</comment>
		<comment id='2' author='iamhatesz' date='2020-11-12T15:33:36Z'>
		The reason why I am using a pure  instead of a  was described by me in &lt;denchmark-link:https://github.com/ray-project/ray/issues/8099&gt;#8099&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='3' author='iamhatesz' date='2020-11-17T12:15:35Z'>
		So this is related with the new Trajectory View API. If I override _use_trajectory_view_api to False, then it works.
		</comment>
		<comment id='4' author='iamhatesz' date='2020-11-21T19:50:04Z'>
		I have the same error. I also set _use_trajectory_view_api to False but it still does not work. I tried to inherit from both ModelV2 and Recurrent, both gave the same error.
		</comment>
		<comment id='5' author='iamhatesz' date='2020-11-25T00:50:26Z'>
		Can confirm turning off trajectory view API works. Can we get migration instructions please?
		</comment>
		<comment id='6' author='iamhatesz' date='2020-11-25T11:27:56Z'>
		Taking a look now. ...
		</comment>
		<comment id='7' author='iamhatesz' date='2020-11-25T11:32:14Z'>
		Yes, this was caused by some changes related to the trajectory_view_api.
I confirmed this bug has been fixed in the current master branch.
Closing this issue. Feel free to re-open should this continue to be a problem in the latest master branch.
Thanks for filing this &lt;denchmark-link:https://github.com/iamhatesz&gt;@iamhatesz&lt;/denchmark-link&gt;
 !
		</comment>
		<comment id='8' author='iamhatesz' date='2020-12-12T18:11:40Z'>
		I have this problem as well.  State seems to be returned as emtpy list, in my case I am loading an already trained feature network. Here is my code and error.
&lt;denchmark-code&gt;class ActorCritic(TorchModelV2, nn.Module):
    def init_hidden(self, hidden_size):
        h0 = self.feature_net.fc2.weight.new(1, hidden_size).zero_()
        c0 = self.feature_net.fc2.weight.new(1, hidden_size).zero_()
        return (h0, c0)

    def __init__(self, obs_space, action_space, num_outputs, model_config, name):
        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)
        nn.Module.__init__(self)
        self.feature_net = FeatureNet(32)
        self.feature_net.load_state_dict(torch.load("fe_network.pth", map_location=torch.device('cpu')))
        for param in self.feature_net.parameters():
            param.requires_grad = True
        self.fc1 = self.feature_net.fc1
        self.fc2 = self.feature_net.fc2
        self.lstm = nn.LSTM(128, 32, 1, batch_first=True)
        self.action = nn.Linear(32, 3)
        self.value = nn.Linear(32, 1)
        self._value_out = None

    def forward(self, input_dict, state, seq_lens):
        obs = input_dict["obs_flat"]
        x = F.elu(self.fc2(F.elu(self.fc1(obs))))

        lstm_h = state[0]
        lstm_c = state[1]

        # Build the tuples
        lstm_hidden = (lstm_h.reshape(-1, 1, 32), lstm_c.reshape(-1, 1, 32))

        out, lstm_hidden = self.lstm(
            obs.view(-1, 1, 1), lstm_hidden
        )
        new_hidden_state = [
            lstm_hidden[0],
            lstm_hidden[1],
        ]

        action = self.action(x)
        value = self.value(x)
        self._value_out = value.reshape(-1)
        return torch.reshape(action, [-1, 3]), [new_hidden_state[0].squeeze(0), new_hidden_state[1].squeeze(0)]

    @override(TorchModelV2)
    def value_function(self):
        return self._value_out

    @override(ModelV2)
    def get_initial_state(self):
        # make hidden states on same device as model
        lstm_h, lstm_c = self.init_hidden(32)
        initial_state = [
            lstm_h,
            lstm_c,
        ]
        return initial_state

register_env("pick_place", env_creator)
ModelCatalog.register_custom_model("ActorCritic", ActorCritic)
ray.init()
trainer = a2c.A2CTrainer(env="pick_place", config={
    "framework": "torch",
    "model": {
        "custom_model": "ActorCritic",
        "max_seq_len": 1,
    },
})
trainer.train()

&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;pybullet build time: Oct  8 2020 00:10:46
2020-12-12 18:08:15,583 INFO services.py:1092 -- View the Ray dashboard at http://127.0.0.1:8265
2020-12-12 18:08:16,923 INFO trainer.py:619 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
2020-12-12 18:08:19,753 WARNING util.py:40 -- Install gputil for GPU system monitoring.
(pid=45761) /home/pitsillos/.pyenv/versions/bullet-introspection/lib/python3.7/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)
Traceback (most recent call last):
(pid=45761)   tensor = torch.from_numpy(np.asarray(item))
  File "rllib_choreograph.py", line 323, in &lt;module&gt;
(pid=45763) /home/pitsillos/.pyenv/versions/bullet-introspection/lib/python3.7/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)
(pid=45763)   tensor = torch.from_numpy(np.asarray(item))
    trainer.train()
  File "/home/pitsillos/.pyenv/versions/bullet-introspection/lib/python3.7/site-packages/ray/rllib/agents/trainer.py", line 517, in train
    raise e
  File "/home/pitsillos/.pyenv/versions/bullet-introspection/lib/python3.7/site-packages/ray/rllib/agents/trainer.py", line 506, in train
    result = Trainable.train(self)
  File "/home/pitsillos/.pyenv/versions/bullet-introspection/lib/python3.7/site-packages/ray/tune/trainable.py", line 336, in train
    result = self.step()
  File "/home/pitsillos/.pyenv/versions/bullet-introspection/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py", line 147, in step
    res = next(self.train_exec_impl)
  File "/home/pitsillos/.pyenv/versions/bullet-introspection/lib/python3.7/site-packages/ray/util/iter.py", line 756, in __next__
    return next(self.built_iterator)
  File "/home/pitsillos/.pyenv/versions/bullet-introspection/lib/python3.7/site-packages/ray/util/iter.py", line 783, in apply_foreach
    for item in it:
  File "/home/pitsillos/.pyenv/versions/bullet-introspection/lib/python3.7/site-packages/ray/util/iter.py", line 843, in apply_filter
    for item in it:
  File "/home/pitsillos/.pyenv/versions/bullet-introspection/lib/python3.7/site-packages/ray/util/iter.py", line 843, in apply_filter
    for item in it:
  File "/home/pitsillos/.pyenv/versions/bullet-introspection/lib/python3.7/site-packages/ray/util/iter.py", line 783, in apply_foreach
    for item in it:
  File "/home/pitsillos/.pyenv/versions/bullet-introspection/lib/python3.7/site-packages/ray/util/iter.py", line 876, in apply_flatten
    for item in it:
  File "/home/pitsillos/.pyenv/versions/bullet-introspection/lib/python3.7/site-packages/ray/util/iter.py", line 828, in add_wait_hooks
    item = next(it)
  File "/home/pitsillos/.pyenv/versions/bullet-introspection/lib/python3.7/site-packages/ray/util/iter.py", line 783, in apply_foreach
    for item in it:
  File "/home/pitsillos/.pyenv/versions/bullet-introspection/lib/python3.7/site-packages/ray/util/iter.py", line 783, in apply_foreach
    for item in it:
  File "/home/pitsillos/.pyenv/versions/bullet-introspection/lib/python3.7/site-packages/ray/util/iter.py", line 783, in apply_foreach
    for item in it:
  File "/home/pitsillos/.pyenv/versions/bullet-introspection/lib/python3.7/site-packages/ray/util/iter.py", line 471, in base_iterator
    yield ray.get(futures, timeout=timeout)
  File "/home/pitsillos/.pyenv/versions/bullet-introspection/lib/python3.7/site-packages/ray/worker.py", line 1452, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(IndexError): ray::RolloutWorker.par_iter_next() (pid=45761, ip=192.168.1.201)
  File "python/ray/_raylet.pyx", line 482, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 436, in ray._raylet.execute_task.function_executor
  File "/home/pitsillos/.pyenv/versions/bullet-introspection/lib/python3.7/site-packages/ray/util/iter.py", line 1152, in par_iter_next
    return next(self.local_it)
  File "/home/pitsillos/.pyenv/versions/bullet-introspection/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py", line 317, in gen_rollouts
    yield self.sample()
  File "/home/pitsillos/.pyenv/versions/bullet-introspection/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py", line 621, in sample
    batches = [self.input_reader.next()]
  File "/home/pitsillos/.pyenv/versions/bullet-introspection/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py", line 94, in next
    batches = [self.get_data()]
  File "/home/pitsillos/.pyenv/versions/bullet-introspection/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py", line 211, in get_data
    item = next(self.rollout_provider)
  File "/home/pitsillos/.pyenv/versions/bullet-introspection/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py", line 616, in _env_runner
    tf_sess=tf_sess,
  File "/home/pitsillos/.pyenv/versions/bullet-introspection/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py", line 1290, in _do_policy_eval_w_trajectory_view_api
    input_dict, timestep=policy.global_timestep)
  File "/home/pitsillos/.pyenv/versions/bullet-introspection/lib/python3.7/site-packages/ray/rllib/policy/torch_policy.py", line 205, in compute_actions_from_input_dict
    input_dict, state_batches, seq_lens, explore, timestep)
  File "/home/pitsillos/.pyenv/versions/bullet-introspection/lib/python3.7/site-packages/ray/rllib/policy/torch_policy.py", line 249, in _compute_action_helper
    seq_lens)
  File "/home/pitsillos/.pyenv/versions/bullet-introspection/lib/python3.7/site-packages/ray/rllib/models/modelv2.py", line 208, in __call__
    res = self.forward(restored, state or [], seq_lens)
  File "rllib_choreograph.py", line 48, in forward
    lstm_h = state[0]
IndexError: list index out of range
&lt;/denchmark-code&gt;

I am using a custom environment which I have made sure works fine and have had no problems with using it with rllib.  Can someone please give me some pointers, I am new to rllib!
		</comment>
		<comment id='9' author='iamhatesz' date='2020-12-17T11:38:28Z'>
		Hi, I'm experiencing the exact same problem as &lt;denchmark-link:https://github.com/npitsillos&gt;@npitsillos&lt;/denchmark-link&gt;
 , in my case I'm also using a custom environment, and it is able to train properly using PPO with TorchPolicy with LSTMs, but when testing it, the time I call agent.compute_action(obs) it throws the same error as for &lt;denchmark-link:https://github.com/npitsillos&gt;@npitsillos&lt;/denchmark-link&gt;
 . Could someone please help with this?
Also, I've read above something about the trajectory_view_api and some hard fixes, but I don't know (and have no clue of) where is this trajectory api located and which lines of code should be changed so that it works. If some of the users who have solved this bug share which file and in which line they made a change, it'd be helpful &lt;denchmark-link:https://github.com/iamhatesz&gt;@iamhatesz&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jsuarez5341&gt;@jsuarez5341&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>