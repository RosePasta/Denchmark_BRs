<bug id='5205' author='joneswong' open_date='2019-07-16T11:21:04Z' closed_time='2019-07-18T02:28:04Z'>
	<summary>[rllib] MARWIL action placeholders are inconsistent</summary>
	<description>
&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

The algorithm only requires that the policy is a stochastic one. The action space can be either discrete or continuous. However, in our current implementation, the action placeholder for inference purpose is constructed according to the corresponding action space, while that for training purpose is hard-coded for discrete actions.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/blob/master/python/ray/rllib/agents/marwil/marwil_policy.py#L93&gt;https://github.com/ray-project/ray/blob/master/python/ray/rllib/agents/marwil/marwil_policy.py#L93&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/blob/master/python/ray/rllib/agents/marwil/marwil_policy.py#L113&gt;https://github.com/ray-project/ray/blob/master/python/ray/rllib/agents/marwil/marwil_policy.py#L113&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='joneswong' date='2019-07-16T16:06:07Z'>
		Is the fix just to call get_action_placeholder() in both settings?
		</comment>
	</comments>
</bug>