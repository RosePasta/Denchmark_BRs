<bug id='12047' author='felipeeeantunes' open_date='2020-11-16T20:13:10Z' closed_time='2020-11-20T07:59:44Z'>
	<summary>[rllib] OffPolicyEstimator returning negative values for action_prob</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

OffPolicyEstimator class is using log_likelihoods instead of action probabilities, leading to negative values for action_prob. This bug affects the IS and WIS OPE results.
Ray version and other system information (Python version, TensorFlow version, OS):

ray==1.0.1dev commit hash d1182b827af9fc94be0cd9a6a3211c096aa4c0e3
numpy==1.19.2
tf-nightly-cpu==2.4.0.dev20201023
tb-nightly==2.4.0a20201001

&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

It's a conceptual problem, so we just need to look to at the code to understand:
def action_prob(self, batch: SampleBatchType) -&gt; np.ndarray:
        """Returns the probs for the batch actions for the current policy."""

        num_state_inputs = 0
        for k in batch.keys():
            if k.startswith("state_in_"):
                num_state_inputs += 1
        state_keys = ["state_in_{}".format(i) for i in range(num_state_inputs)]
        
        log_likelihoods: TensorType = self.policy.compute_log_likelihoods(
            actions=batch[SampleBatch.ACTIONS],
            obs_batch=batch[SampleBatch.CUR_OBS],
            state_batches=[batch[k] for k in state_keys],
            prev_action_batch=batch.data.get(SampleBatch.PREV_ACTIONS),
            prev_reward_batch=batch.data.get(SampleBatch.PREV_REWARDS))
        
        return convert_to_numpy(log_likelihoods)
A possible solution is to change the method to:
def action_prob(self, batch: SampleBatchType) -&gt; np.ndarray:
        """Returns the probs for the batch actions for the current policy."""

        num_state_inputs = 0
        for k in batch.keys():
            if k.startswith("state_in_"):
                num_state_inputs += 1
        state_keys = ["state_in_{}".format(i) for i in range(num_state_inputs)]
      
        actions = self.policy.compute_actions(
              obs_batch=batch[SampleBatch.CUR_OBS],
              state_batches=[batch[k] for k in state_keys],
              prev_action_batch=batch.data.get(SampleBatch.PREV_ACTIONS),
              prev_reward_batch=batch.data.get(SampleBatch.PREV_REWARDS),
              full_fetch=True
              )
        # Categorical case (e.g. DQN).
        if self.policy.dist_class in (Categorical, TorchCategorical):
            action_prob_dist = softmax(actions[2][SampleBatch.ACTION_DIST_INPUTS])
        # Deterministic (Gaussian actions, e.g. DDPG).
        elif self.policy.dist_class in [Deterministic, TorchDeterministic]:
            action_prob_dist = actions[2][SampleBatch.ACTION_DIST_INPUTS]
        else:
            raise NotImplementedError  # TODO(sven): Other action-dist cases.
        
        cur_action = batch[SampleBatch.ACTIONS]
        cur_action_prob = action_prob_dist[np.arange(cur_action.shape[0]), cur_action]
        
        return cur_action_prob 

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='felipeeeantunes' date='2020-11-17T16:33:55Z'>
		This bug is related to the issue &lt;denchmark-link:https://github.com/ray-project/ray/issues/10458&gt;#10458&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='2' author='felipeeeantunes' date='2020-11-19T12:32:55Z'>
		Yeah, I think returning the logp instead of the prob is indeed a bug. I'll fix it. However, I still think we should use policy.compute_log_likelihood() for that, for the following reason:
Let's take a look at rllib/offline/is_estimator.py:
What we are trying to do here inside the  estimate method is to calculate the importance sampling ratios (IS): p_old(actions)/p_new(actions), where actions are the actions from the historic(!) batch (not newly calculated ones). p_old(actions) is the probability that the old policy (the one used to collect the batch) calculated for the actions in the batch (not newly calculated actions!).
You can see this in this line here in rllib/offline/is_estimator.py:
rewards, old_prob = batch["rewards"], batch["action_prob"]
p_new(actions) is the probability computed by the current/updated policy (but for the exact same historic actions in the batch!). And this is why we have to use policy.compute_log_likelihoods(), to be able to pass in the actions from the batch, not re-calculate new actions, which may be different due to the policy having changed and outputting different actions for the observations in the batch than the old policy would have.
So the fix should rather be:
Keep compute_log_likelihoods(), but translate to probs by simply doing np.exp before returning from OffPolicyEstimator::action_probs().
The whole point of calculating these weights is to weight less those (historic) actions that the new policy would not produce that much anymore anyways and weight more those (historic) actions, which the new policy would produce a lot (compared to the old policy).
So even if some actions show up a lot in the historic data, we may still weight these less for our loss/updates b/c the new policy has learned that these are not great actions to pick from and it won't output these actions that much anymore.
		</comment>
	</comments>
</bug>