<bug id='11854' author='viotemp1' open_date='2020-11-06T08:31:03Z' closed_time='2020-11-10T00:02:55Z'>
	<summary>[tune] ray.tune HyperOptSearch - points_to_evaluate list</summary>
	<description>
Hello,
I tried HyperOptSearch with points_to_evaluate like below:
[{'tf_window_size': 15.0, 'tf_img_size': 64.0, 'tf_model_optimizer_movingaverage': 3, 'tf_model_dropout': 0.0, 'tf_model_optimizer': 8, 'tf_conv_layers': 2.0, 'tf_conv_no_neurons': 192.0, 'tf_lstm_layers': 2.0, 'tf_lstm_no_neurons': 128.0, 'tf_dense_layers': 2.0, 'tf_dense_no_neurons': 128.0}, {'tf_window_size': 12.0, 'tf_img_size': 64.0, 'tf_model_optimizer_movingaverage': 2, 'tf_model_dropout': 0.30000000000000004, 'tf_model_optimizer': 12, 'tf_conv_layers': 2.0, 'tf_conv_no_neurons': 128.0, 'tf_lstm_layers': 2.0, 'tf_lstm_no_neurons': 352.0, 'tf_dense_layers': 2.0, 'tf_dense_no_neurons': 192.0}]
I followed the example here: &lt;denchmark-link:https://docs.ray.io/en/master/tune/examples/hyperopt_example.html?highlight=points_to_evaluate#hyperopt-example&gt;hyperopt_example&lt;/denchmark-link&gt;

When I resumed the experiment, I see one trial using second hyperparameter dict from points_to_evaluate list, but not the first one.
When I start the experiment again - with resume = False - both points_to_evaluate are used for start
What to do to use both (or more) in the beginning in case of resume? Or it's not possible?
	</description>
	<comments>
		<comment id='1' author='viotemp1' date='2020-11-06T09:05:07Z'>
		Hi &lt;denchmark-link:https://github.com/viotemp1&gt;@viotemp1&lt;/denchmark-link&gt;
, could you please clarify the sequence of events here? It is not clear to me when you stopped the trial and what exactly you're resuming.
Are you starting a run, stopping it, changing the points_to_evaluate parameter and trying to resume?
How many trials are you starting in total?
It would be great if you could provide some more context here, and possibly a bit more code (e.g. your tune.run call)
		</comment>
		<comment id='2' author='viotemp1' date='2020-11-06T09:20:04Z'>
		Hi &lt;denchmark-link:https://github.com/krfricke&gt;@krfricke&lt;/denchmark-link&gt;
,
The points_to_evaluate I load from a csv file (where I save best 2 trials results of experiment using ray Callback on_trial_result)
I would like to use these 2 parameters as starting point whenever I run again the experiment or resume.
When starting experiment without resume, everything looks fine, the best two hyperparams loaded before from csv and mentioned in the first post are used in the trial 1 and 2. I'm using 3 concurrent trials for now because of limitations I have on GPU memory.
When starting experiment with resume (interrupted because of various reasons :)), I see only one hyperparam used in Trial 1.
Maybe points_to_evaluate should not be used at all when resume=True.
Here are some pieces from code:
&lt;denchmark-code&gt;    search_alg = HyperOptSearch(random_state_seed=hyperparameter_space_non_ray["tf_seed"], points_to_evaluate=points_to_evaluate)
    search_alg = ConcurrencyLimiter(search_alg, max_concurrent=max_concurrent)
nalysis = tune.run(
        tune.with_parameters(
            tune_fn,
            data_dir=data_dir,
            params_one_shot=params_one_shot,
            config_non_ray=hyperparameter_space_non_ray,
        ),
        config=hyperparameter_space,
        name=experiment_name,
        search_alg=search_alg,
        stop=stop,
        scheduler=scheduler,
        callbacks=[ray_Callback()],
        **experiment_config_metrics,
    )

points_to_evaluate
[{'tf_window_size': 15.0, 'tf_img_size': 64.0, 'tf_model_optimizer_movingaverage': 3, 'tf_model_dropout': 0.0, 'tf_model_optimizer': 8, 'tf_conv_layers': 2.0, 'tf_conv_no_neurons': 192.0, 'tf_lstm_layers': 2.0, 'tf_lstm_no_neurons': 128.0, 'tf_dense_layers': 2.0, 'tf_dense_no_neurons': 128.0}, {'tf_window_size': 12.0, 'tf_img_size': 64.0, 'tf_model_optimizer_movingaverage': 2, 'tf_model_dropout': 0.30000000000000004, 'tf_model_optimizer': 12, 'tf_conv_layers': 2.0, 'tf_conv_no_neurons': 128.0, 'tf_lstm_layers': 2.0, 'tf_lstm_no_neurons': 352.0, 'tf_dense_layers': 2.0, 'tf_dense_no_neurons': 192.0}]

params_list_ray
{'tf_window_size': &lt;ray.tune.sample.Float at 0x7fc760d8e070&gt;,
 'tf_img_size': &lt;ray.tune.sample.Float at 0x7fc760dc5970&gt;,
 'tf_model_optimizer_movingaverage': &lt;ray.tune.sample.Categorical at 0x7fc760dc04f0&gt;,
 'tf_model_dropout': &lt;ray.tune.sample.Float at 0x7fc760dc0190&gt;,
 'tf_model_optimizer': &lt;ray.tune.sample.Categorical at 0x7fc7609915b0&gt;,
 'tf_conv_layers': &lt;ray.tune.sample.Float at 0x7fc7609918b0&gt;,
 'tf_conv_no_neurons': &lt;ray.tune.sample.Float at 0x7fc760991430&gt;,
 'tf_lstm_layers': &lt;ray.tune.sample.Float at 0x7fc760db3280&gt;,
 'tf_lstm_no_neurons': &lt;ray.tune.sample.Float at 0x7fc760db3970&gt;,
 'tf_dense_layers': &lt;ray.tune.sample.Float at 0x7fc760dc9070&gt;,
 'tf_dense_no_neurons': &lt;ray.tune.sample.Float at 0x7fc760dc9a60&gt;,
 'log_sys_usage': True}

params_list_ray_config
{'verbose': 1,
 'resume':True, # False
 'metric': 'val_my_Metric',
 'mode': 'min',
 'num_samples': 200,
 'checkpoint_freq': 0,
 'keep_checkpoints_num': None,
 'checkpoint_score_attr': None,
 'checkpoint_at_end': True,
 'max_failures': 2,
 'raise_on_failed_trial': False,
 'queue_trials': False,
 'local_dir': '/home/myuser/PythonProjects/logs1/',
 'resources_per_trial': {'cpu': 3, 'gpu': 0.3}}
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='viotemp1' date='2020-11-09T17:30:37Z'>
		So when resuming a run the HyperoptSearch._hpopt_trials variable will be restored from the checkpoint. This means that everything passed as points_to_evaluate will be overwritten (and thus ignored).
We should consider throwing a warning for this.
However, if you started trials with this config before stopping (and resuming), the trials should have run on the initial call. Has one of the trials maybe finished? It won't be run again then.
		</comment>
		<comment id='4' author='viotemp1' date='2020-11-09T18:08:27Z'>
		Clear now. points_to_evaluate will have values from first run with resume=False.
Thanks a lot!
I'm testing also NevergradSearch these days. Any chance this search algorithm to have initial points_to_evaluate?
Regards,
		</comment>
		<comment id='5' author='viotemp1' date='2020-11-09T22:05:49Z'>
		I'm afraid there isn't, as is true for most other algorithms. However, I don't see why we shouldn't support this - as long as the algorithms accept function evaluations for arbitrary configurations, this should be straightforward to implement.
Would adding this to Nevergrad something you might want to look into?
		</comment>
		<comment id='6' author='viotemp1' date='2020-11-09T22:41:18Z'>
		Yes, sure. I tested most search algorithms and for now  can say that the best results I had with NevergradSearch. The issue with that is so many optimisers - I tested all of them from ng.optimizers.registry.items() :)
Should I open a new issue/request?
This original issue regarding points_to_evaluate can be closed. It is clear and normal to use initial values in case of resume.
		</comment>
		<comment id='7' author='viotemp1' date='2020-11-09T22:42:38Z'>
		A new pull-request would be awesome!
		</comment>
		<comment id='8' author='viotemp1' date='2020-11-09T22:47:14Z'>
		Here you got me :)
Never did this, not sure how to do it, sorry... But never too late to learn!
		</comment>
		<comment id='9' author='viotemp1' date='2020-11-09T22:51:37Z'>
		You can just get started and we're happy to help if you get stuck. Also feel free to ping us on the community slack if you need any support with this!
		</comment>
		<comment id='10' author='viotemp1' date='2020-11-09T22:52:05Z'>
		&lt;denchmark-link:https://forms.gle/9TSdDYUgxYs8SA9e8&gt;Here's the link to slack&lt;/denchmark-link&gt;
 just to be sure
		</comment>
		<comment id='11' author='viotemp1' date='2020-11-09T23:05:42Z'>
		OK. I joined slack (Vio Ubi) and will address my question about how to start from there.
Thanks, This current ticket can be closed.
Regards,
		</comment>
	</comments>
</bug>