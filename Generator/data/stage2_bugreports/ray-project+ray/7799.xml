<bug id='7799' author='simon-mo' open_date='2020-03-29T21:36:47Z' closed_time='2020-03-31T01:28:14Z'>
	<summary>Async Actor Segfault on PyTorch Tensor</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

Ray version and other system information (Python version, TensorFlow version, OS):
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
import torch
from ray import serve
import ray

serve.init()
@serve.route("/echo")
def echo(_, data):
    return torch.rand(1, 3, 244, 244)

tensor = torch.rand(1, 3, 244, 244)

print(ray.get(echo.remote(data=tensor)).shape)
This script is flaky, running it three times on my laptop can reproduce the output:
&lt;denchmark-code&gt;2020-03-29 14:33:56,630	INFO resource_spec.py:212 -- Starting Ray with 38.33 GiB memory available for workers and up to 0.09 GiB for objects. You can adjust these settings with ray.init(memory=&lt;bytes&gt;, object_store_memory=&lt;bytes&gt;).
2020-03-29 14:33:56,981	INFO services.py:1151 -- View the Ray dashboard at localhost:8265
(pid=42340) INFO:     Started server process [42340]
(pid=42340) INFO:     Waiting for application startup.
(pid=42340) INFO:     Application startup complete.
(pid=42340) INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
E0329 14:34:00.692930 29990912 task_manager.cc:254] Task failed: IOError: 14: Socket closed: Type=ACTOR_TASK, Language=PYTHON, function_descriptor={type=PythonFunctionDescriptor, module_name=ray.serve.policy, class_name=RandomPolicyQueueActor, function_name=enqueue_request, function_hash=}, task_id=b944ee5bb38dd1a5fbe69b320100, job_id=0100, num_args=4, num_returns=2, actor_task_spec={actor_id=fbe69b320100, actor_caller_id=ffffffffffffffffffffffff0100, actor_counter=2}
Traceback (most recent call last):
  File "srtml.py", line 12, in &lt;module&gt;
2020-03-29 14:34:00,694	WARNING worker.py:1072 -- A worker died or was killed while executing task fffffffffffffffffbe69b320100.
    print(ray.get(echo.remote(data=tensor)).shape)
  File "/Users/simonmo/Desktop/ray/ray/python/ray/worker.py", line 1515, in get
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
(pid=42331) *** Aborted at 1585517640 (unix time) try "date -d @1585517640" if you are using GNU date ***
(pid=42331) PC: @                0x0 (unknown)
(pid=42331) *** SIGSEGV (@0x66) received by PID 42331 (TID 0x7000078b2000) stack trace: ***
(pid=42333) E0329 14:34:00.692922 222285824 task_manager.cc:254] Task failed: IOError: 14: Socket closed: Type=ACTOR_TASK, Language=PYTHON, function_descriptor={type=PythonFunctionDescriptor, module_name=ray.serve.policy, class_name=RandomPolicyQueueActor, function_name=dequeue_request, function_hash=}, task_id=2f567df85d55d93afbe69b320100, job_id=0100, num_args=4, num_returns=2, actor_task_spec={actor_id=fbe69b320100, actor_caller_id=ffffffffffffffffb4cb21440100, actor_counter=1}
&lt;/denchmark-code&gt;

If we cannot run your script, we cannot fix your issue.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='simon-mo' date='2020-03-29T23:59:22Z'>
		Yeah, in general I don't trust passing Tensors around in Ray. Ideally, I can just pass in an entire pytorch nn Module into ray.put.
		</comment>
		<comment id='2' author='simon-mo' date='2020-03-30T00:41:00Z'>
		Ray cannot handle tensor input and output ?? ðŸ˜¨
Hmm somehow it's just async actor related.
import torch
import ray

ray.init()

@ray.remote
class Echo:
    async def echo(self, data):
        print("got data", data.shape)
        return data

e = Echo.remote()
tensor = torch.rand(1, 3, 244, 244)
print(ray.get(e.echo.remote(tensor)).shape)
		</comment>
	</comments>
</bug>