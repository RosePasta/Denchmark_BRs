<bug id='7058' author='akurniawan' open_date='2020-02-05T05:29:02Z' closed_time='2020-06-01T19:38:26Z'>
	<summary>[tune] Hyperopt IndexError: list index out of range</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

On space dictionary below, if I use hp.uniform everything run smoothly. However, if I change it to hp.choice I got IndexError: list index out of range
Ray version and other system information (Python version, TensorFlow version, OS):
Python: 3.7
OS: Catalina 10.15.2
Ray: 0.8.1
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
"""This test checks that HyperOpt is functional.

It also checks that it is usable with a separate scheduler.
"""
import ray
from ray.tune import run, Trainable
from ray.tune.schedulers import AsyncHyperBandScheduler, HyperBandScheduler
from ray.tune.suggest.hyperopt import HyperOptSearch


class TestingTrainable(Trainable):
    def _setup(self, config):
        self.activation = config["activation"]
        self.width = config.get("width", 10)
        # self.height = config["height"]

    def _train(self):
        assert type(self.activation) == str, \
            "Config is incorrect: {}".format(type(self.activation))
        # return {"mean_loss": (self.height - 14)**2 - abs(self.width - 3)}
        return {"mean_loss": 10}

    def _save(self, checkpoint_dir):
        return checkpoint_dir

    def _restore(self, checkpoint_path):
        pass


if __name__ == "__main__":
    import argparse
    from hyperopt import hp

    parser = argparse.ArgumentParser()
    parser.add_argument("--smoke-test",
                        action="store_true",
                        help="Finish quickly for testing")
    args, _ = parser.parse_known_args()
    ray.init()

    space = {
        # "width": hp.uniform("width", 0.001, 1),
        "width": hp.choice("width", [1]),
        # "height": hp.choice("height", [1, 2, 3]),
        # "height": hp.uniform("height", -100, 100),
        "activation": hp.choice("activation", ["relu", "tanh"]),
    }

    current_best_params = [
        {
            "width": 1,
            # "height": 2,
            "activation": 0,  # Activation will be relu
        },
        {
            "width": 4,
            # "height": 2,
            "activation": 1,  # Activation will be tanh
        }
    ]

    config = {
        "num_samples": 10 if args.smoke_test else 1000,
        "stop": {
            "training_iteration": 100
        },
    }
    algo = HyperOptSearch(space,
                          max_concurrent=4,
                          metric="mean_loss",
                          mode="min",
                          points_to_evaluate=current_best_params)
    scheduler = AsyncHyperBandScheduler(metric="mean_loss", mode="min")
    run(TestingTrainable, search_alg=algo, scheduler=scheduler, **config)

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='akurniawan' date='2020-02-05T20:05:31Z'>
		Hi &lt;denchmark-link:https://github.com/akurniawan&gt;@akurniawan&lt;/denchmark-link&gt;
, thanks a bunch for opening this issue. This seems like a big issue; I'll take a look at this later this week.
		</comment>
		<comment id='2' author='akurniawan' date='2020-02-07T09:13:54Z'>
		BTW &lt;denchmark-link:https://github.com/akurniawan&gt;@akurniawan&lt;/denchmark-link&gt;
, sorry for taking so long; I'll get to this on the weekend. If you want to take a crack at addressing the bug, you might want to start here: &lt;denchmark-link:https://github.com/hyperopt/hyperopt/issues/615&gt;hyperopt/hyperopt#615&lt;/denchmark-link&gt;
 which seems relevant.
		</comment>
		<comment id='3' author='akurniawan' date='2020-06-01T10:11:23Z'>
		The problem here lies in your specification of the current_best_params dict. With a hp.choice() parameter, the best param should refer to the index of the best parameter, not the value. If you use
&lt;denchmark-code&gt;    current_best_params = [
        {
            "width": 0,  # Width will be 1
            "activation": 0,  # Activation will be relu
        },
        {
            "width": 0,  # Width will be 1
            "activation": 1,  # Activation will be tanh
        }
    ]
&lt;/denchmark-code&gt;

it runs smoothly.
We will discuss if it makes sense to change this to a value-based setting.
		</comment>
	</comments>
</bug>