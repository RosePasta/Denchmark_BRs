<bug id='9202' author='DavidMChan' open_date='2020-06-30T03:39:06Z' closed_time='2020-09-22T23:48:54Z'>
	<summary>[raysgd] Invalid Device Ordinal in _set_cuda_device (Torch version 1.5)</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

Ray can sometimes encounter a cuda runtime error (101): invalid device ordinal when setting devices - even when setting the string correctly to devices that exist on the system. This is similar to &lt;denchmark-link:https://github.com/ray-project/ray/issues/5737&gt;#5737&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/ray-project/ray/issues/3772&gt;#3772&lt;/denchmark-link&gt;

I encountered this on the following machine:
OS: Ubuntu 18.04
Ray Version: 0.9.0.dev0 (master)
GPUs: 2x NVIDIA Titan (Maxwell)
CUDA version: 10.2
Torch version: 1.5
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

This error can be encountered sporadically by launching any job with two workers, with 1 GPU per worker.
Note: This bug is flaky, and does not always occur. I can't seem to trigger it at will, suggesting that there may be a race condition when setting the device from multiple threads simultaneously (meaning that it may not be a ray bug, and might be something upstream).
Traceback:
&lt;denchmark-code&gt;Reading in data...                                                                                                                                                                                                     
Building configurations...                                                                                                                                                                                             
Configuring Training -- Iteration 0                                                                                                                                                                                    
Training -- Iteration 0                                                                                                                                                                                                
2020-06-29 19:02:15,938 INFO resource_spec.py:221 -- Starting Ray with 36.18 GiB memory available for workers and up to 18.09 GiB for objects. You can adjust these settings with ray.init(memory=&lt;bytes&gt;, object_store_
memory=&lt;bytes&gt;).                                                                                                                                                                                                       
2020-06-29 19:02:16,650 INFO services.py:1192 -- View the Ray dashboard at localhost:8265                                                                                                                              
2020-06-29 19:02:18,434 INFO distributed_torch_runner.py:349 -- Setting local CUDA device: 1                                                                                                                           
THCudaCheck FAIL file=/pytorch/torch/csrc/cuda/Module.cpp line=59 error=101 : invalid device ordinal                                                                                                                   
2020-06-29 19:02:18,438 ERROR distributed_torch_runner.py:353 -- Failed to set local CUDA device.                                                                                                                      
Traceback (most recent call last):                                                                                                                                                                                     
  File "active/driver.py", line 261, in &lt;module&gt;                                                                                                                                                                       
    main()                                                                                                                                                                                                             
  File "/home/david/Apps/miniconda3/envs/pele/lib/python3.7/site-packages/click/core.py", line 829, in __call__                                                                                                        
    return self.main(*args, **kwargs)                                                                                                                                                                                  
  File "/home/david/Apps/miniconda3/envs/pele/lib/python3.7/site-packages/click/core.py", line 782, in main                                                                                                            
    rv = self.invoke(ctx)                                                                                                                                                                                              
  File "/home/david/Apps/miniconda3/envs/pele/lib/python3.7/site-packages/click/core.py", line 1066, in invoke                                                                                                         
    return ctx.invoke(self.callback, **ctx.params)                                                                                                                                                                     
  File "/home/david/Apps/miniconda3/envs/pele/lib/python3.7/site-packages/click/core.py", line 610, in invoke                                                                                                          
    return callback(*args, **kwargs)                                                                                                                                                                                   
  File "active/driver.py", line 157, in main                                                                                                                                                                           
    results, checkpoint_path = train(config)                                                                                                                                                                           
  File "/home/david/Projects/pele/pele/run/train.py", line 85, in train                                                                                                                                                
    trainer = experiment.get_torch_trainer()                                                                                                                                                                           
  File "/home/david/Projects/pele/pele/core/experiment.py", line 228, in get_torch_trainer                                                                                                                             
    config=config_dict)                                                                                                                                                                                                
  File "/home/david/Apps/miniconda3/envs/pele/lib/python3.7/site-packages/ray/util/sgd/torch/torch_trainer.py", line 276, in __init__                                                                                  
    self._start_workers(self.max_replicas)                                                                                                                                                                             
  File "/home/david/Apps/miniconda3/envs/pele/lib/python3.7/site-packages/ray/util/sgd/torch/torch_trainer.py", line 336, in _start_workers                                                                            
    **params)                                                                                                                                                                                                          
  File "/home/david/Apps/miniconda3/envs/pele/lib/python3.7/site-packages/ray/util/sgd/torch/distributed_torch_runner.py", line 307, in __init__                                                                       
    self._try_reserve_and_set_resources(num_cpus, num_gpus)                                                                                                                                                            
  File "/home/david/Apps/miniconda3/envs/pele/lib/python3.7/site-packages/ray/util/sgd/torch/distributed_torch_runner.py", line 336, in _try_reserve_and_set_resources                                                 
    self._set_cuda_device(reserved_cuda_device)                                                                                                                                                                        
  File "/home/david/Apps/miniconda3/envs/pele/lib/python3.7/site-packages/ray/util/sgd/torch/distributed_torch_runner.py", line 351, in _set_cuda_device                                                               
    torch.cuda.set_device(int(self.local_cuda_device))                                                                                                                                                                 
  File "/home/david/Apps/miniconda3/envs/pele/lib/python3.7/site-packages/torch/cuda/__init__.py", line 292, in set_device                                                                                             
    torch._C._cuda_setDevice(device)                                                                                                                                                                                   
RuntimeError: cuda runtime error (101) : invalid device ordinal at /pytorch/torch/csrc/cuda/Module.cpp:59 
&lt;/denchmark-code&gt;

If we cannot run your script, we cannot fix your issue.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='DavidMChan' date='2020-06-30T03:42:44Z'>
		Thanks for opening this! Two questions:

Are you setting cuda visible devices?
if not, can you see if torch.cuda.set_device(1) or torch.cuda.set_device(0) or torch.cuda.set_device("1")  works on your interpreter?

		</comment>
		<comment id='2' author='DavidMChan' date='2020-06-30T03:44:55Z'>
		Thanks for following up -

I am not using CUDA_VISIBLE_DEVICES
All of torch.cuda.set_device(1), torch.cuda.set_device(0) and torch.cuda.set_device('cuda:1') work as expected in my interpreter. Note that in torch 1.5, '1' is not a valid device string.

		</comment>
		<comment id='3' author='DavidMChan' date='2020-06-30T04:01:07Z'>
		ah hm I guess this might be a torch 1.5 thing -
we set the cuda visible devices at runtime to mask GPUs. IIRC in torch 1.4 the setting of cuda visible devices at runtime isn't respected, while maybe torch 1.5 it is.
The temporary fix is to set os.environ("CUDA_VISIBLE_DEVICES")="0" right before TorchTrainer.
I'll try to get to this this week. Let me know if the temporary fix works for you - sorry about the inconvenience!
		</comment>
		<comment id='4' author='DavidMChan' date='2020-06-30T04:06:25Z'>
		Thanks! I'll add that to my training scripts. It's not a major priority (since I can just rerun it a few times, and that seems to solve the problem), but it is a bit annoying.
		</comment>
		<comment id='5' author='DavidMChan' date='2020-06-30T04:31:10Z'>
		Thanks! tip: just be careful not to set it before ray.init.
		</comment>
		<comment id='6' author='DavidMChan' date='2020-07-05T21:51:09Z'>
		So it seems like this doesn't consistently solve the issue. I believe that the issue is that ray sets CUDA_VISIBLE_DEVICES, however the local_device_string is always the same as the gpu number as in CUDA_VISIBLE_DEVICES (i.e. if the GPU visible is 3, then local_device_string is 3, where it should be 0, since it's the 0'th index when visible to torch).
Additionally, to make matters worse  with NCCL 2.8, it looks like CUDA_VISIBLE_DEVICES is respected, meaning that when you get to DDP, all of the reserved devices have to be visible in CUDA_VISIBLE_DEVICES. (&lt;denchmark-link:https://github.com/NVIDIA/nccl/issues/129&gt;NVIDIA/nccl#129&lt;/denchmark-link&gt;
)
Removing the set CUDA_VISIBLE_DEVICES from here:



ray/python/ray/util/sgd/torch/distributed_torch_runner.py


         Line 319
      in
      f43d934






 os.environ["CUDA_VISIBLE_DEVICES"] = reserved_cuda_device 





allows the script to continue correctly until failing in the DDP stage with a cuda memory illegal access error.
This doesn't seem to be an issue with Torch 1.4/1.5 (since I can reproduce it with both scenarios). It also seems to occur in cuda 10.0, 10.2 and 11.0
This may also be causing &lt;denchmark-link:https://github.com/ray-project/ray/issues/9258&gt;#9258&lt;/denchmark-link&gt;
, however I don't have enough information yet to debug it.
&lt;denchmark-link:https://github.com/richardliaw&gt;@richardliaw&lt;/denchmark-link&gt;
 - If you'd like to talk offline, I can give you access to a gcp instance where you can play with the code/errors.
		</comment>
		<comment id='7' author='DavidMChan' date='2020-07-06T08:24:23Z'>
		btw, I think this actually should be fixed in &lt;denchmark-link:https://github.com/ray-project/ray/pull/9260&gt;#9260&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='DavidMChan' date='2020-07-06T22:39:53Z'>
		I can now reproduce the bug consistently with the script here: &lt;denchmark-link:https://gist.github.com/DavidMChan/a3b88dbe16dacd368c9d3415acc174ae&gt;https://gist.github.com/DavidMChan/a3b88dbe16dacd368c9d3415acc174ae&lt;/denchmark-link&gt;

The code is even worse if you move the ray.init()/ray.shutdown() inside the loop or increase the number of workers beyond 4.
I'm not convinced that the fix &lt;denchmark-link:https://github.com/ray-project/ray/pull/9260&gt;#9260&lt;/denchmark-link&gt;
 fixes the problem entirely, since I still have the same issues in my larger code (however it reduces the number of issues caused). Adding  still encounters a race condition for the first thread initializing cuda. There may be deeper issues with multiple trainers than just these bugs (but I don't know, since my development loop is pretty slow).
		</comment>
		<comment id='9' author='DavidMChan' date='2020-07-06T23:01:05Z'>
		hm got it; I think to really fix this issue we'll have to have an option to make all workers run on separate processes.
		</comment>
		<comment id='10' author='DavidMChan' date='2020-09-22T23:48:54Z'>
		This is actually fixed in &lt;denchmark-link:https://github.com/ray-project/ray/pull/10539&gt;#10539&lt;/denchmark-link&gt;
!
		</comment>
	</comments>
</bug>