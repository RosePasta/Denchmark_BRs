<bug id='8407' author='ncarrara' open_date='2020-05-11T22:10:43Z' closed_time='2020-06-25T12:15:51Z'>
	<summary>[rllib] RNN sequencing error when using tuple as env_state with QMIX</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

Hi, I want to try QMIX, with 2 agents, each agent receives an observation gym.spaces.Box(0,1, (3,)), and the env_state (the one fed to the mixing network) is the concatenation of those observations: gym.spaces.Tuple((Box(0,1, (3,),Box(0,1, (3,))).
On this configuration:
Python 3.7.4
&lt;denchmark-code&gt;lsb_release -a
No LSB modules are available.
Distributor ID:	Ubuntu
Description:	Ubuntu 18.04.4 LTS
Release:	18.04
Codename:	bionic
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;&gt;&gt;&gt; import ray
&gt;&gt;&gt; ray.__version__
'0.9.0.dev0'
&lt;/denchmark-code&gt;

I have the following error:
&lt;denchmark-code&gt;E0511 18:01:47.573905  8350 core_worker.cc:1068] Pushed Error with JobID: 0100 of type: task with message: ray::QMIX.train() (pid=8350, ip=192.168.0.14)
  File "python/ray/_raylet.pyx", line 463, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 417, in ray._raylet.execute_task.function_executor
  File "/home/ncarrara/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer.py", line 498, in train
    raise e
  File "/home/ncarrara/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer.py", line 484, in train
    result = Trainable.train(self)
  File "/home/ncarrara/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py", line 261, in train
    result = self._train()
  File "/home/ncarrara/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py", line 151, in _train
    fetches = self.optimizer.step()
  File "/home/ncarrara/anaconda3/lib/python3.7/site-packages/ray/rllib/optimizers/sync_batch_replay_optimizer.py", line 88, in step
    return self._optimize()
  File "/home/ncarrara/anaconda3/lib/python3.7/site-packages/ray/rllib/optimizers/sync_batch_replay_optimizer.py", line 112, in _optimize
    info_dict = self.workers.local_worker().learn_on_batch(samples)
  File "/home/ncarrara/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py", line 649, in learn_on_batch
    info_out[pid] = policy.learn_on_batch(batch)
  File "/home/ncarrara/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/qmix/qmix_policy.py", line 329, in learn_on_batch
    dynamic_max=True)
  File "/home/ncarrara/anaconda3/lib/python3.7/site-packages/ray/rllib/policy/rnn_sequencing.py", line 231, in chop_into_sequences
    f_pad[seq_base + seq_offset] = f[i]
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Reproduction&lt;/denchmark-h&gt;

This is a modified version of the two-steps game example:
&lt;denchmark-code&gt;from gym.spaces import Tuple, Dict, Discrete, Box
import numpy as np

import ray
from ray import tune
from ray.tune import register_env
from ray.rllib.env.multi_agent_env import MultiAgentEnv
from ray.rllib.agents.qmix.qmix_policy import ENV_STATE

action_space = Discrete(2)
agent_obs_space = Box(0, 1, (3,))
observation_space = Dict({
    "obs": agent_obs_space,
    ENV_STATE: Tuple((agent_obs_space, agent_obs_space))
})
grouped_obs_space = Tuple([observation_space, observation_space])
grouped_action_space = Tuple([action_space, action_space])


class TwoStepGame(MultiAgentEnv):

    def __init__(self, config):
        self.state = None
        self.agent_1 = 0
        self.agent_2 = 1
        self.observation_space = grouped_obs_space

    def reset(self):
        return self.obs()

    def step(self, action_dict):
        done = np.random.random(1) &gt; 0.5
        rewards = {
            self.agent_1: 1.0,
            self.agent_2: 1.0
        }
        dones = {"__all__": done}
        infos = {}
        return self.obs(), rewards, dones, infos

    def obs(self):
        obs_agent_1 = [0.1, 0.2, 0.3]
        obs_agent_2 = [0.4, 0.5, 0.6]
        state = tuple([obs_agent_1, obs_agent_2])
        obs = {
            self.agent_1: {
                "obs": obs_agent_1,
                ENV_STATE: state
            },
            self.agent_2: {
                "obs": obs_agent_2,
                ENV_STATE: state
            }
        }
        return obs


if __name__ == "__main__":
    grouping = {
        "group_1": [0, 1],
    }

    register_env(
        "grouped_twostep",
        lambda config: TwoStepGame(config).with_agent_groups(
            grouping, obs_space=grouped_obs_space, act_space=grouped_action_space))

    config = {
        "num_workers": 0,
        "mixer": "qmix",
        "env_config": {}
    }

    ray.init(num_cpus=2 or None, local_mode=True)
    tune.run(
        "QMIX",
        stop={
            "timesteps_total": 10,
        },
        config=dict(config, **{
            "env": "grouped_twostep"
        }),
    )
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='ncarrara' date='2020-05-12T15:02:15Z'>
		It works if we specify
&lt;denchmark-code&gt;observation_space = Dict({
    "obs": agent_obs_space,
    ENV_STATE: Box(0, 1, (6,)),#Tuple((agent_obs_space, agent_obs_space))
})
&lt;/denchmark-code&gt;

and
state = [obs_agent_1] + [obs_agent_2] # tuple([obs_agent_1, obs_agent_2])
		</comment>
		<comment id='2' author='ncarrara' date='2020-06-25T12:15:51Z'>
		Hey &lt;denchmark-link:https://github.com/ncarrara&gt;@ncarrara&lt;/denchmark-link&gt;
 . Thanks for filing this issue. It's similar to this one here (&lt;denchmark-link:https://github.com/ray-project/ray/issues/8523&gt;#8523&lt;/denchmark-link&gt;
) and the following PR fixes it:
&lt;denchmark-link:https://github.com/ray-project/ray/pull/9139&gt;#9139&lt;/denchmark-link&gt;

Closing this now. Feel free to re-open, should the problem still occur on your end.
		</comment>
	</comments>
</bug>