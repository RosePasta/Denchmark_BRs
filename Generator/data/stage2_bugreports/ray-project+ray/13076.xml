<bug id='13076' author='Vysybyl' open_date='2020-12-24T14:57:22Z' closed_time='2021-01-16T02:48:37Z'>
	<summary>Missing heartbeat and killing nodes when working with heavy objects in slow network environments</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

My local node is missing heartbeat and getting killed when I try to run a task on a heavy object. The issue seems to be related with a slower network connection.
The code below is just to reproduce the issue
Ray version and other system information (Python version, TensorFlow version, OS):
Ray 1.0.0 installed from pip on python 3.8.5 in a conda environment.
Xubuntu 20.04.1 LTS
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

On the head node (n1-standard-16 Google Cloud Compute instance running Ubuntu 20.04.1 LTS) I start ray running:
ulimit -n 65536;       ray start       --head       --port=6379       --object-manager-port=8076       --include-dashboard=true       --dashboard-host=0.0.0.0       --dashboard-port=8266  --lru-evict
On my local machine I run:
ray start --address="XX.XX.XX.XX:6379" --redis-password='YYYYYY' --num-cpus=0
The two machines are connected through a VPN. I've been using this configuration for at least several weeks with no issues. The average ping takes about 30ms. My internet speed is 10Mbps in download and 0.8Mbps in upload :(.
At home, where I have a Gigabit connection, the issue has never presented itself. I'm currently visiting some relatives thus I have a weaker network here.
On a jupyter book, the following code causes the local node to crash. Afterwards, I have to restart ray both locally and on the server in order to be able to run ray.init again without errors.
&lt;denchmark-code&gt;import ray

ray.init(address='XX.XX.XX.XX:6379')

long_string = 'a' * 10000
long_list = [long_string for _ in range(10000)]
long_list_ref = ray.put(long_list)
type(ray.get(long_list_ref))  # list   # just to confirm that serialization is not an issue

@ray.remote
def fx(lst):
    print(len(lst))
    print(len(lst[0]))
    return 'OK'

ray.get(fx.remote(['aaaa', 'bbb', 'ccc']))  # OK
ray.get(fx.remote(ray.put(['aaaa', 'bbb', 'ccc'])))  # OK

fx.remote(long_list_ref)   # This fails after a while
&lt;/denchmark-code&gt;

After some minutes of silence I get (IP ZZZ is my local machine):
&lt;denchmark-code&gt;2020-12-24 14:59:59,768	WARNING worker.py:1072 -- The node with node id ff7d8f14bc739e46376bd48484409524b8e1d92d has been marked dead because the detector has missed too many heartbeats from it.
(pid=raylet, ip=ZZZ) F1224 15:00:01.267652 13075 13075 node_manager.cc:767]  Check failed: node_id != self_node_id_ Exiting because this node manager has mistakenly been marked dead by the monitor: GCS didn't receive heartbeats within timeout 30000 ms. This is likely since the machine or raylet became overloaded.
(pid=raylet, ip=ZZZ) *** Check failure stack trace: ***
(pid=raylet, ip=ZZZ)     @     0x55b5bd1109cd  google::LogMessage::Fail()
(pid=raylet, ip=ZZZ)     @     0x55b5bd111b2c  google::LogMessage::SendToLog()
(pid=raylet, ip=ZZZ)     @     0x55b5bd1106a9  google::LogMessage::Flush()
(pid=raylet, ip=ZZZ)     @     0x55b5bd1108c1  google::LogMessage::~LogMessage()
(pid=raylet, ip=ZZZ)     @     0x55b5bd0c8ad9  ray::RayLog::~RayLog()
(pid=raylet, ip=ZZZ)     @     0x55b5bcde38f0  ray::raylet::NodeManager::NodeRemoved()
(pid=raylet, ip=ZZZ)     @     0x55b5bcde3acc  _ZNSt17_Function_handlerIFvRKN3ray8ClientIDERKNS0_3rpc11GcsNodeInfoEEZNS0_6raylet11NodeManager11RegisterGcsEvEUlS3_S7_E0_E9_M_invokeERKSt9_Any_dataS3_S7_
(pid=raylet, ip=ZZZ)     @     0x55b5bced1f48  ray::gcs::ServiceBasedNodeInfoAccessor::HandleNotification()
(pid=raylet, ip=ZZZ)     @     0x55b5bced21f6  _ZNSt17_Function_handlerIFvRKSsS1_EZZN3ray3gcs28ServiceBasedNodeInfoAccessor26AsyncSubscribeToNodeChangeERKSt8functionIFvRKNS3_8ClientIDERKNS3_3rpc11GcsNodeInfoEEERKS6_IFvNS3_6StatusEEEENKUlSM_E0_clESM_EUlS1_S1_E_E9_M_invokeERKSt9_Any_dataS1_S1_
(pid=raylet, ip=ZZZ)     @     0x55b5bcedc02a  _ZNSt17_Function_handlerIFvSt10shared_ptrIN3ray3gcs13CallbackReplyEEEZNS2_9GcsPubSub24ExecuteCommandIfPossibleERKSsRNS6_7ChannelEEUlS4_E_E9_M_invokeERKSt9_Any_dataS4_
(pid=raylet, ip=ZZZ)     @     0x55b5bceddb0b  _ZN5boost4asio6detail18completion_handlerIZN3ray3gcs20RedisCallbackManager12CallbackItem8DispatchERSt10shared_ptrINS4_13CallbackReplyEEEUlvE_E11do_completeEPvPNS1_19scheduler_operationERKNS_6system10error_codeEm
(pid=raylet, ip=ZZZ)     @     0x55b5bd40e46f  boost::asio::detail::scheduler::do_run_one()
(pid=raylet, ip=ZZZ)     @     0x55b5bd40f971  boost::asio::detail::scheduler::run()
(pid=raylet, ip=ZZZ)     @     0x55b5bd4109a2  boost::asio::io_context::run()
(pid=raylet, ip=ZZZ)     @     0x55b5bcd33fde  main
(pid=raylet, ip=ZZZ)     @     0x7f85f88df0b3  __libc_start_main
(pid=raylet, ip=ZZZ)     @     0x55b5bcd4abb1  (unknown)
&lt;/denchmark-code&gt;

While I'm waiting for the result, ping to the remote machine slows down to 20 seconds (20000ms). The remote machine CPU usage never goes above 12%. My local machine CPU usage is also chillin'.
I think this could be related with &lt;denchmark-link:https://github.com/ray-project/ray/issues/12774&gt;#12774&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ray-project/ray/issues/8251&gt;#8251&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ray-project/ray/pull/11869&gt;#11869&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ray-project/ray/issues/11624&gt;#11624&lt;/denchmark-link&gt;
. I'm opening a new issue since I think the snippet to reproduce it is simpler and since none of those issues mention slow network connections.
I will try to increase the heartbeat timeout and counts, as mentioned &lt;denchmark-link:https://github.com/ray-project/ray/issues/8251#issuecomment-627621403&gt;here&lt;/denchmark-link&gt;
, but I think this should be addressed in a more stable way.
Many thanks in advance!

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='Vysybyl' date='2020-12-27T20:46:20Z'>
		Do you mind trying setting up a lower nice value and see if this is still reproducible? (I'd like to know if we can mitigate the issue by giving higher priority to the process, or the process itself has code that hangs due to intensive IO load). &lt;denchmark-link:https://www.tecmint.com/set-linux-process-priority-using-nice-and-renice-commands/&gt;https://www.tecmint.com/set-linux-process-priority-using-nice-and-renice-commands/&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='Vysybyl' date='2020-12-27T20:47:27Z'>
		Also, it'll be nice if you can get the traceback of the crashed process. Not sure if you can use this command for a crashed process, but you can try;
&lt;denchmark-code&gt;sudo gdb -batch -ex "thread apply all bt" -p &lt;raylet_pid&gt;
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='Vysybyl' date='2020-12-27T20:49:05Z'>
		And the most importantly, if you can find out a reproducible script, we are likely to fix the issue easily (though I am not sure how easy it will be to reproduce this).
		</comment>
		<comment id='4' author='Vysybyl' date='2021-01-04T18:16:01Z'>
		Let me know after you tried the suggestions I mentioned above. I will get back to you after that.
		</comment>
		<comment id='5' author='Vysybyl' date='2021-01-07T13:02:52Z'>
		Hi &lt;denchmark-link:https://github.com/rkooo567&gt;@rkooo567&lt;/denchmark-link&gt;
,
Thanks a lot for the feedback.
Sorry for the late reply. Unfortunately, I currently have no time to give you more information or do more testings.
Also, I am back to my place where the internet connection is fast and the issue never reappeared.
If you still wants to troubleshoot, I think you should be able to easily reproduce the issue if you set up a remote head node connected over a slow network connection (how slow the connection should be is specified in my original message). I'm not really a network guy but I guess you should be able to artificially slow down your connection to a specific IP with the proper NW settings, and I'm sure this can be done over LAN (no need for a VPN).
Otherwise, please feel free to close the issue, although I bet other people will run into it.
I will reopen it with more information if I have the time.
Thanks,
		</comment>
		<comment id='6' author='Vysybyl' date='2021-01-11T12:51:44Z'>
		Hello &lt;denchmark-link:https://github.com/rkooo567&gt;@rkooo567&lt;/denchmark-link&gt;
,
I'm once again using a (slightly) slower network and the issue reappeared.
I have noticed that the exception on the client process is thrown by &lt;denchmark-link:https://github.com/ray-project/ray/blob/93006c2ba52e059e6fda52694762ad3cbcdef461/src/ray/gcs/gcs_client/service_based_gcs_client.cc#L230&gt;this guard failing&lt;/denchmark-link&gt;
 which in turn is configured &lt;denchmark-link:https://github.com/ray-project/ray/blob/5d50d37f45ae30913dd28fafbee11cb5025f497f/src/ray/common/ray_config_def.h#L266&gt;here&lt;/denchmark-link&gt;

I think the issue could be solved (or at least troubleshot) by hacking some of the default parameters I see in RayConfig file.
Is there any way to change these parameters when I start up the node using the CLI? I couldn't find any documentation about those, but it seems to me that many of these "max_retries" and "timeouts" are too tight for my current setup (local worker node on premises + remote head on the cloud).
Many thanks in advance,
		</comment>
		<comment id='7' author='Vysybyl' date='2021-01-16T02:48:13Z'>
		I think this is actually just normal. Ray's raylets are considered as dead if it didn't send heartbeat for 30 seconds. I think really slow network basically defer raylet thread, which leads it to miss heartbeat for 30 seconds.
You can use _system_config to modify the Ray Config files. Look at 


ray/python/ray/tests/test_object_spilling.py


         Line 50
      in
      1179db1






 ray.init(_system_config={ 




 for more details.
		</comment>
		<comment id='8' author='Vysybyl' date='2021-01-16T02:48:37Z'>
		I will close the issue for now since it looks like it is resolved. Feel free to reopen if necessary :)
		</comment>
		<comment id='9' author='Vysybyl' date='2021-01-16T14:21:10Z'>
		
I think this is actually just normal. Ray's raylets are considered as dead if it didn't send heartbeat for 30 seconds. I think really slow network basically defer raylet thread, which leads it to miss heartbeat for 30 seconds.
You can use _system_config to modify the Ray Config files. Look at



ray/python/ray/tests/test_object_spilling.py


         Line 50
      in
      1179db1






 ray.init(_system_config={ 





for more details.

Thanks &lt;denchmark-link:https://github.com/rkooo567&gt;@rkooo567&lt;/denchmark-link&gt;
, but I must say that my local node is killed  a few seconds (maybe 5 or 10, surely less than 30) since ray is started on head and worker nodes. I am hitting another timeout, not the 30 seconds heartbeat limit. I will play around with the other settings and see what happens.
I will keep you posted.
		</comment>
	</comments>
</bug>