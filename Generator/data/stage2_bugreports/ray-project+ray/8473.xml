<bug id='8473' author='matej-macak' open_date='2020-05-17T15:41:04Z' closed_time='2020-05-20T18:31:21Z'>
	<summary>[rllib] [tune] Memory increase when training environment in rllib and tune</summary>
	<description>
Hi,
I am trying to run a simple environment that constantly runs out of memory using PPO trainer but likely other algorithms using tune.
I checked the following issues before reporting the bug:
&lt;denchmark-link:https://github.com/ray-project/ray/issues/3884&gt;#3884&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/issues/4877&gt;#4877&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/issues/651&gt;#651&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/issues/3777&gt;#3777&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;My system:&lt;/denchmark-h&gt;

HP Spectre x360
16GB RAM
Core i7-8750H
Ubuntu 18.04
Ray version: 0.8.4
&lt;denchmark-h:h3&gt;Error&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;Failure # 1 (occurred at 2020-05-17_16-19-34)
Traceback (most recent call last):
  File "/home/matej/Programming/venvs/mlbase/lib/python3.7/site-packages/ray/tune/trial_runner.py", line 467, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File "/home/matej/Programming/venvs/mlbase/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py", line 381, in fetch_result
    result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)
  File "/home/matej/Programming/venvs/mlbase/lib/python3.7/site-packages/ray/worker.py", line 1513, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(RayOutOfMemoryError): �[36mray::PPO�[39m (pid=448, ip=172.20.1.205)
  File "python/ray/_raylet.pyx", line 415, in ray._raylet.execute_task
  File "/home/matej/Programming/venvs/mlbase/lib/python3.7/site-packages/ray/memory_monitor.py", line 120, in raise_if_low_memory
    self.error_threshold))
ray.memory_monitor.RayOutOfMemoryError: More than 95% of the memory on node matej-HP-Spectre-x360 is used (14.51 / 15.26 GB). The top 10 memory consumers are:

PID	MEM	COMMAND
22378	6.27GiB	ray::PPO.train()
448	0.49GiB	ray::PPO
22348	0.45GiB	/home/matej/Programming/venvs/mlbase/bin/python -u /home/matej/Programming/venvs/mlbase/lib/python3.
755	0.27GiB	ray::RolloutWorker
22375	0.27GiB	ray::RolloutWorker
22376	0.27GiB	ray::RolloutWorker
754	0.27GiB	ray::RolloutWorker
757	0.27GiB	ray::RolloutWorker
22372	0.27GiB	ray::RolloutWorker
22377	0.26GiB	ray::RolloutWorker

In addition, up to 0.94 GiB of shared memory is currently being used by the Ray object store. You can set the object store size with the `object_store_memory` parameter when starting Ray.
---
--- Tip: Use the `ray memory` command to list active objects in the cluster.
---
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Status before error&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;
== Status ==
Memory usage on this node: 14.4/15.3 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.
Using FIFO scheduling algorithm.
Resources requested: 12/12 CPUs, 0/1 GPUs, 0.0/7.81 GiB heap, 0.0/1.32 GiB objects
Result logdir: /home/matej/ray_results/PPO
Number of trials: 16 (2 ERROR, 12 PENDING, 2 RUNNING)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Reproduction&lt;/denchmark-h&gt;

The steps to reproduce:
&lt;denchmark-code&gt;import gym
from gym import spaces

from stable_baselines import DQN, PPO2

import numpy as np
import time

import ray
from ray import tune
from ray.tune.registry import register_env

class CustomEnvironment(gym.Env):
    
    def __init__(self, dimensions = 1000, seed = 0):
        
        np.random.seed(seed)
        
        self.dimensions = dimensions
        
        # Set the spaces for gym
        self.action_space = spaces.Box(low = 0, high = 1, shape = (self.dimensions, ))
        self.observation_space = spaces.Box(low = 0, high = 1, shape = (self.dimensions, ))
        
        self.deltas = np.clip(np.random.rand(self.dimensions) / 10, a_min = 0.01, a_max = 0.1)
        self.observation = np.random.rand(self.dimensions)

        self.start_time = time.time()
    
    def step(self, action):
        
        action_delta = np.abs(self.observation - action)
        
        reward = np.sum(np.clip(action_delta - self.deltas, a_min = 0, a_max = 1) * (-1))        
        
        if reward &lt; 0:        
            pass
        else:
            reward = np.sum(action)
                    
        info = {}
        done = True
                
        return self.observation, reward, done, info
    
    def reset(self):
        
        return self.observation
    
    
if __name__ == "__main__":

    memory = 8000*1024*1024

    ray.init(memory=memory, 
             object_store_memory=2000*1024*1024, 
             driver_object_store_memory=1000*1024*1024, 
             redis_max_memory=1000*1024*1024)
    
    backend = "ray"
    timesteps = 1000000    
    
    if backend == "stable":
        env = CustomEnvironment()

        # Instantiate the agent
        model = PPO2("MlpPolicy", env)
        
        model.learn(timesteps)
    
    elif backend == "ray":
        
        register_env("CustomEnvironment", lambda config: CustomEnvironment())
        
        tune.run(
            "PPO",
            stop={"episode_reward_mean": 200},
            config={
                "env": "CustomEnvironment",
                "num_gpus": 0,
                "num_workers": 5,
                "train_batch_size": tune.grid_search([25, 50]),
                "eager": False,
                "sgd_minibatch_size": tune.grid_search([1, 2]),
                "num_sgd_iter": tune.grid_search([1, 5]),
                "rollout_fragment_length": tune.grid_search([1, 5]),
            },
            verbose = 1
        )


&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='matej-macak' date='2020-05-20T15:57:27Z'>
		Hi &lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
, I have done some more testing of this bug and discovered that in the  ray this is significantly diminished so could potentially be linked to the &lt;denchmark-link:https://github.com/ray-project/ray/pull/8037&gt;#8037&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='matej-macak' date='2020-05-20T18:25:57Z'>
		Oh, that's good news. To confirm, the memory increase is no longer a problem? I think a very modest amount of increase over time is expected based on memory fragmentation.
		</comment>
		<comment id='3' author='matej-macak' date='2020-05-20T18:30:22Z'>
		Yes I think we can close this because in 0.8.5 that code should work without memory increase, in 0.8.4 it breaks down though. I ran it for a very long time without any memory leak now on 0.8.5.
		</comment>
		<comment id='4' author='matej-macak' date='2020-05-20T18:31:21Z'>
		Awesome
		</comment>
		<comment id='5' author='matej-macak' date='2020-06-03T16:47:16Z'>
		Hi, I am experiencing this issue with version 0.8.5. The memory is increasing at a very slow pace... I am training PPO (no exp replay). Any help would be extremely appreciated. In 24h of run over 16 cores it floods my memory (64GB).
		</comment>
		<comment id='6' author='matej-macak' date='2020-06-03T16:53:26Z'>
		Hi &lt;denchmark-link:https://github.com/ivallesp&gt;@ivallesp&lt;/denchmark-link&gt;
, try setting the following parameters based on the number of cores you have to see if this works in your : , ,  and . See &lt;denchmark-link:https://docs.ray.io/en/latest/memory-management.html&gt;here&lt;/denchmark-link&gt;

What I think would be helpful in general is knowing what are the general parameters that usually work as I had to do a lot of trial and error. I used the values in the link as a reference and it worked for me.
		</comment>
		<comment id='7' author='matej-macak' date='2020-06-03T17:25:09Z'>
		Can you file a new bug with a reproduction script? Also, Ray now does automatic garbage collection. I wouldn't generally recommend setting the memory settings manually due to this.
		</comment>
	</comments>
</bug>