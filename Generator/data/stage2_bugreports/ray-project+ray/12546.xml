<bug id='12546' author='rkooo567' open_date='2020-12-01T19:32:07Z' closed_time='2020-12-09T19:55:21Z'>
	<summary>[Core] GCS based Actor Scheduling discourages actor colocation.</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

Might be related to &lt;denchmark-link:https://github.com/ray-project/ray/issues/11239&gt;#11239&lt;/denchmark-link&gt;
.
Before we enabled GCS-based-actor-scheduling (&lt; 0.8.7), actors were always tried to be scheduled on the same node as the owner if possible (by our worker lease protocol). But after 0.8.7, actor lease requests are just sent to the random node instead of being sent to the node of the owner. This leads actors to be distributed well across the cluster.
This could've introduced unexpected regression in rllib or Tune. If actors are not located in the same node, there will be additional network cost/object transfer cost which could lead to slow down/OOM (especially when our broadcast was not throttled).
We can resolve the issue by always first trying to schedule actors on the same node as the owner instead of a random node. This should be pretty easy/straightforward fix.
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
If we cannot run your script, we cannot fix your issue.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='rkooo567' date='2020-12-01T19:34:20Z'>
		cc &lt;denchmark-link:https://github.com/roireshef&gt;@roireshef&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='rkooo567' date='2020-12-01T21:38:15Z'>
		cc &lt;denchmark-link:https://github.com/ffbin&gt;@ffbin&lt;/denchmark-link&gt;
 Do you have some bandwidth to take care of it?
		</comment>
		<comment id='3' author='rkooo567' date='2020-12-02T03:37:39Z'>
		You can assign it to me. I implemented this in the previous version, but it was lost in the GCS-based-actor-scheduling .
		</comment>
		<comment id='4' author='rkooo567' date='2020-12-02T04:05:59Z'>
		SGTM. btw, we should probably do this only when there are resource requirements. Otherwise, every actor created by
@ray.remote
class Actor:
    pass
will have num_cpus=0 meaning every actor will be scheduled on the same node as a creator.
		</comment>
		<comment id='5' author='rkooo567' date='2020-12-02T07:28:39Z'>
		
SGTM. btw, we should probably do this only when there are resource requirements. Otherwise, every actor created by
@ray.remote
class Actor:
    pass
will have num_cpus=0 meaning every actor will be scheduled on the same node as a creator.

Ok, thanks.
		</comment>
		<comment id='6' author='rkooo567' date='2020-12-08T06:10:32Z'>
		&lt;denchmark-link:https://github.com/ffbin&gt;@ffbin&lt;/denchmark-link&gt;
 Do you have a plan to work on it soon?
		</comment>
		<comment id='7' author='rkooo567' date='2020-12-08T06:35:52Z'>
		
@ffbin Do you have a plan to work on it soon?

Yeah, i am working on it now. I want to abstract NodeSelector separately, so that PG and actor can reuse a set of interfaces when scheduling and selecting nodes.
		</comment>
		<comment id='8' author='rkooo567' date='2020-12-08T06:55:05Z'>
		&lt;denchmark-link:https://github.com/rkooo567&gt;@rkooo567&lt;/denchmark-link&gt;

class ResourceScheduler {
vector ScheduleResourceSets(vector resource_sets, SchedulingPolicy policy) {
// select nodes
// substract available resources // AquireResources
}
}
ActorScheduler:

ScheduleResourceSets(pending_actors, soft_spread) -&gt; Node list
foreach actor do: lease workers;
foreach if lease fails: ReturnResources(); Add actor to pending list.

PgScheduler:

GCS receivs a PG creation request;
ScheduleResourceSets(bundles_resources, pg.strategy) -&gt; Node List
Allocate resources for bundles on raylets.
4.1 if all resources allocated -&gt; AddResources(resources_pg_ids); {cpu: 1} -&gt; {CPU_PG_xxx: 1} Node1{CPU: 7, CPU_PG_1: 3}
4.2 else(any bundle fails): -&gt; ReturnResources(all_bundles);

We will also support batch scheduling actors.
		</comment>
		<comment id='9' author='rkooo567' date='2020-12-08T18:13:29Z'>
		The inteface LGTM. But this line;
&lt;denchmark-code&gt;ScheduleResourceSets(pending_actors, soft_spread) -&gt; Node list
&lt;/denchmark-code&gt;

is different from this issue's policy right?
Shouldn't we call it sth like soft_schedule_with_owner or something?
		</comment>
		<comment id='10' author='rkooo567' date='2020-12-09T01:53:05Z'>
		soft_spread is just a sample, it will support spread / pack / strict_ spread/strict_ pack. I will submit a PR this week.
		</comment>
		<comment id='11' author='rkooo567' date='2020-12-09T06:31:47Z'>
		&lt;denchmark-link:https://github.com/rkooo567&gt;@rkooo567&lt;/denchmark-link&gt;
 If we want to hotfix this into the release branch, the fix should be minimal. Can we do something small (&lt;50 LOC patch) without the refactoring initially?
		</comment>
		<comment id='12' author='rkooo567' date='2020-12-09T09:45:29Z'>
		
@rkooo567 If we want to hotfix this into the release branch, the fix should be minimal. Can we do something small (&lt;50 LOC patch) without the refactoring initially?

&lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
 I think i can submit a hotfix PR today.
		</comment>
		<comment id='13' author='rkooo567' date='2020-12-09T09:45:40Z'>
		

@rkooo567 If we want to hotfix this into the release branch, the fix should be minimal. Can we do something small (&lt;50 LOC patch) without the refactoring initially?

@ericl I have submit a hotfix PR (#12707) , pls help take a look, thanks. cc @rkooo567

		</comment>
	</comments>
</bug>