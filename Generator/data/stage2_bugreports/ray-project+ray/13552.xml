<bug id='13552' author='ugurkanates' open_date='2021-01-19T13:26:06Z' closed_time='2021-01-21T13:36:15Z'>
	<summary>[rllib] Custom Evaluation doesn't work as intended - skips the data collected for episode</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

Ray version and other system information (Python version, TensorFlow version, OS):
Ray : 1.1
macOS 10.15 Catalina , Ubuntu Linux 16.04 - 18.04 LTS
NVIDIA CUDA Version : 10.2
NVIDIA CuDNN Version : 8.x.x
PyTorch Version : 1.7.1 (Stable)
Python Version : 3.8 (handled by conda)
Conda Version : 4.9.2
Itâ€™s the custom evaluation part. We are using custom_evaluation parameter given in dictionary to set up our own eval method. (In picture below) .
This custom evaluation is basically adapted from rllib examples , nothing too fancy or anything.
But when we try   &amp; print we see our episode that was collected  is not used during this custom_evaluation part.
We set like to collect 64 steps for example and we printed values of each step , it worked ok. And when it finishes in theory it should input that data to custom_evaluation and we should see it work.
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

&lt;denchmark-link:https://camo.githubusercontent.com/c96bca9e95448c543155fdd8752acc5f77ce75c65b80fc98452b98ddb1013cc5/68747470733a2f2f692e6962622e636f2f71376e4732506b2f53637265656e73686f742d323032312d30312d31392d61742d31312d32322d32312e706e67&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://camo.githubusercontent.com/1d538313d1223d597a2c07de3b0513f60829319aead7053878cfd187f32dd9c4/68747470733a2f2f692e6962622e636f2f30596d704277442f53637265656e73686f742d323032312d30312d31392d61742d31312d32332d30312e706e67&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://camo.githubusercontent.com/917276eace433c26098de89d59c6acaca81965380995722575f9f13d5d0b3caf/68747470733a2f2f692e6962622e636f2f6a5452487870542f53637265656e73686f742d323032312d30312d31392d61742d31312d32332d30382e706e67&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://camo.githubusercontent.com/39aea859292bf413e9987393823255fa32760bd1721aa4d3e8475d633d900d31/68747470733a2f2f692e6962622e636f2f684b66583442502f53637265656e73686f742d323032312d30312d31392d61742d31312d32352d33332e706e67&gt;&lt;/denchmark-link&gt;

We put a reproduction of our env. It runs with pybullets and we put everything to a script.
Our file's to reproduce the issue.
&lt;denchmark-link:https://drive.google.com/file/d/1xRgJytBo41Rj3Oi2oVGD8-jSdJ2KQJRo/view?usp=sharing&gt;https://drive.google.com/file/d/1xRgJytBo41Rj3Oi2oVGD8-jSdJ2KQJRo/view?usp=sharing&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='ugurkanates' date='2021-01-19T13:26:45Z'>
		&lt;denchmark-link:https://github.com/sven1977&gt;@sven1977&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='ugurkanates' date='2021-01-21T13:36:15Z'>
		We actually solved the issue. We mistakenly override  input_evaluation =[] but we should have just put input_evaluation['simulation']
		</comment>
	</comments>
</bug>