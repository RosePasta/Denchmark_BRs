<bug id='1925' author='robertnishihara' open_date='2018-04-19T20:30:10Z' closed_time='2018-04-20T18:34:30Z'>
	<summary>Local scheduler crashes when job is killed.</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


OS Platform and Distribution: Ubuntu
Ray installed from (source or binary): pip
Ray version: 0.4.0
Python version: Python 3.6.1 :: Continuum Analytics, Inc.

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

I don't have a minimal example at the moment, but I can reproduce the issue by running a specific application and then hitting Ctrl-C in the middle. The local scheduler crashes with
&lt;denchmark-code&gt;/ray/src/common/state/redis.cc:979 Check failed: reply-&gt;type != REDIS_REPLY_ERROR reply-&gt;str is Cannot update a task that doesn't exist yet
&lt;/denchmark-code&gt;

Also
&lt;denchmark-code&gt;/opt/conda/lib/python3.6/site-packages/ray/local_scheduler/../core/src/local_scheduler/local_scheduler[0x43a0f5]
/opt/conda/lib/python3.6/site-packages/ray/local_scheduler/../core/src/local_scheduler/local_scheduler(_ZN3ray8internal7CerrLogD1Ev+0x8e)[0x44633e]
/opt/conda/lib/python3.6/site-packages/ray/local_scheduler/../core/src/local_scheduler/local_scheduler(_Z32redis_task_table_update_callbackP17redisAsyncContextPvS1_+0x216)[0x463136]
/opt/conda/lib/python3.6/site-packages/ray/local_scheduler/../core/src/local_scheduler/local_scheduler(redisProcessCallbacks+0x83)[0x4b1833]
/opt/conda/lib/python3.6/site-packages/ray/local_scheduler/../core/src/local_scheduler/local_scheduler(aeProcessEvents+0x227)[0x477f47]
/opt/conda/lib/python3.6/site-packages/ray/local_scheduler/../core/src/local_scheduler/local_scheduler(aeMain+0x2b)[0x47826b]
/opt/conda/lib/python3.6/site-packages/ray/local_scheduler/../core/src/local_scheduler/local_scheduler(main+0x562)[0x43a9b2]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0)[0x7fd936d55830]
/opt/conda/lib/python3.6/site-packages/ray/local_scheduler/../core/src/local_scheduler/local_scheduler[0x43c921]
&lt;/denchmark-code&gt;

cc &lt;denchmark-link:https://github.com/unixpickle&gt;@unixpickle&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='robertnishihara' date='2018-04-20T05:33:14Z'>
		I believe the issue is the following. When the job is killed, the monitor cleans up the task and object table entries in Redis for that job.
The local scheduler also attempts to update the states of the tasks in Redis. The relevant line is 


ray/src/local_scheduler/local_scheduler.cc


         Line 625
      in
      aa07f1c






 task_table_update(state-&gt;db, worker-&gt;task_in_progress, NULL, NULL, NULL); 





Changing that task_table_update to task_table_add_task works around the issue. So does commenting out the line in the monitor that cleans up the task/object table. 


ray/python/ray/monitor.py


         Line 432
      in
      aa07f1c






 self._clean_up_entries_for_driver(driver_id) 





Another approach is to allow the task_table_update command to fail. There's no reason that the local scheduler has to die when trying to update a task that has been removed.
		</comment>
	</comments>
</bug>