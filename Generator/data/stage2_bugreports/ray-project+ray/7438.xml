<bug id='7438' author='soundway' open_date='2020-03-04T03:08:17Z' closed_time='2020-03-04T09:48:08Z'>
	<summary>[rllib] Evaluation doesn’t work properly for PyTorch.</summary>
	<description>
This is not a contribution.
Ray version: 0.8.1
Python version: 3.6.8
Pytorch version: 1.4
OS: Ubuntu 18.04 Docker
Training with Pytorch with evaluation enabled doesn’t work properly as it ends up calling tensorflow code. Here's a script that reproduces the problem:
import gym
from gym.spaces import Box
from ray import tune

class DummyEnv(gym.Env):
    def __init__(self, config):
        self.action_space = Box(0.0, 1.0, shape=(1,))
        self.observation_space = Box(0.0, 1.0, shape=(1, ))

    def reset(self):
        self.num_steps = 0
        return [0.0]

    def step(self, action):
        self.num_steps += 1
        return [0.0], 1.0, True if self.num_steps == 30 else False, {}

tune.run(
    "PPO",
    config={
        "env": DummyEnv,
        "use_pytorch": True,
        "num_workers": 1,
        "evaluation_interval": 1,
        "evaluation_config": {"seed": 1},
    }
)
This will result in the following exception:
&lt;denchmark-code&gt;ray.exceptions.RayTaskError(AssertionError): ray::PPO.train() (pid=33990)
  File "python/ray/_raylet.pyx", line 452, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 430, in ray._raylet.execute_task.function_executor
  File "/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/trainer.py", line 513, in train
    evaluation_metrics = self._evaluate()
  File "/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/trainer.py", line 685, in _evaluate
    lambda w: w.restore(ray.get(weights)))
  File "/usr/local/lib/python3.6/dist-packages/ray/rllib/evaluation/worker_set.py", line 110, in foreach_worker
    local_result = [func(self.local_worker())]
  File "/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/trainer.py", line 685, in &lt;lambda&gt;
    lambda w: w.restore(ray.get(weights)))
  File "/usr/local/lib/python3.6/dist-packages/ray/rllib/evaluation/rollout_worker.py", line 764, in restore
    self.policy_map[pid].set_state(state)
  File "/usr/local/lib/python3.6/dist-packages/ray/rllib/policy/policy.py", line 320, in set_state
    self.set_weights(state)
  File "/usr/local/lib/python3.6/dist-packages/ray/rllib/policy/tf_policy.py", line 290, in set_weights
    return self._variables.set_weights(weights)
  File "/usr/local/lib/python3.6/dist-packages/ray/experimental/tf_utils.py", line 185, in set_weights
    assert assign_list, ("No variables in the input matched those in the "
AssertionError: No variables in the input matched those in the network. Possible cause: Two networks were defined in the same TensorFlow graph. To fix this, place each network definition in its own tf.Graph.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='soundway' date='2020-03-04T09:34:54Z'>
		This is a bug (seems to have been there for a while). Fixing this now ...
		</comment>
		<comment id='2' author='soundway' date='2020-03-04T09:48:08Z'>
		Thanks for filing this! Please use this PR here for a fix (will be merged into master in the next days):
&lt;denchmark-link:https://github.com/ray-project/ray/pull/7443&gt;#7443&lt;/denchmark-link&gt;

The problem was that the default policy (always TF) would be used for evaluation workers, no matter the setting in config[use_pytorch].
		</comment>
	</comments>
</bug>