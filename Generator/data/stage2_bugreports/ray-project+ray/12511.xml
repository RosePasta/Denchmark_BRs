<bug id='12511' author='ndalton12' open_date='2020-12-01T01:09:08Z' closed_time='2020-12-02T06:17:41Z'>
	<summary>[ray][autoscaler?] Ray not recognizing worker node resources</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

Ray version and other system information (Python version, TensorFlow version, OS):
I'm running ray[rllib]==1.0.1.post1 on Python 3.7, debian 10, CUDA 11 on GCP. The setup is generally a standard n1-standard-2 instance for both the head and worker nodes, each with a NVIDIA Tesla T4 GPU. Ray recognizes the resources (including the GPU) of the head node, but none of the resources on the worker node.
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Using the following script, I run ray up config.yaml which produces a head and worker node with the correct CPU and GPU resources on GCP as expected. However running import ray; ray.init(address="auto"); print(ray.cluster_resources()) only shows the head nodes resources, i.e. it lists that only 1 GPU and 2 CPU cores are present, when there should be 2 GPU and 4 CPU cores present. This is not a CUDA issue since torch recognizes the GPUs on both the worker and head.
The yaml config:
&lt;denchmark-code&gt;# An unique identifier for the head node and workers of this cluster.
cluster_name: XXXX # this identifier cannot have an underscore in it!!! the gcp regex will reject it :(

# The minimum number of workers nodes to launch in addition to the head
# node. This number should be &gt;= 0.
min_workers: 1

# The maximum number of workers nodes to launch in addition to the head
# node. This takes precedence over min_workers.
max_workers: 1

target_utilization_fraction: 1.0

# If a node is idle for this many minutes, it will be removed.
idle_timeout_minutes: 5

# Cloud-provider specific configuration.
provider:
    type: gcp
    region: us-central1
    availability_zone: us-central1-f
    project_id: XXXXXX # Globally unique project id

# How Ray will authenticate with newly launched nodes.
auth:
    ssh_user: XXX
# By default Ray creates a new private keypair, but you can also use your own.
# If you do so, make sure to also set "KeyName" in the head and worker node
# configurations below. This requires that you have added the key into the
# project wide meta-data.
#    ssh_private_key: /path/to/your/key.pem

# Provider-specific config for the head node, e.g. instance type. By default
# Ray will auto-configure unspecified fields such as subnets and ssh-keys.
# For more documentation on available fields, see:
# https://cloud.google.com/compute/docs/reference/rest/v1/instances/insert
head_node:
    machineType: n1-standard-2
    disks:
      - boot: true
        autoDelete: true
        type: PERSISTENT
        initializeParams:
          diskSizeGb: 50
          # See https://cloud.google.com/compute/docs/images for more images
          sourceImage: projects/ml-images/global/images/c2-deeplearning-pytorch-1-6-cu110-v20201105-debian-10
    guestAccelerators:
      - acceleratorType: zones/us-central1-f/acceleratorTypes/nvidia-tesla-t4
        acceleratorCount: 1
    scheduling:
      - onHostMaintenance: "TERMINATE"

    # Additional options can be found in in the compute docs at
    # https://cloud.google.com/compute/docs/reference/rest/v1/instances/insert

worker_nodes:
    machineType: n1-standard-2
    disks:
      - boot: true
        autoDelete: true
        type: PERSISTENT
        initializeParams:
          diskSizeGb: 50
          # See https://cloud.google.com/compute/docs/images for more images
          sourceImage: projects/ml-images/global/images/c2-deeplearning-pytorch-1-6-cu110-v20201105-debian-10
    # Run workers on preemtible instance by default.
    # Comment this out to use on-demand.
#    scheduling:
#      - preemptible: true
    guestAccelerators:
      - acceleratorType: zones/us-central1-f/acceleratorTypes/nvidia-tesla-t4
        acceleratorCount: 1
    scheduling:
      - onHostMaintenance: "TERMINATE"


# Files or directories to copy to the head and worker nodes. The format is a
# dictionary from REMOTE_PATH: LOCAL_PATH, e.g.
file_mounts: {
    "/home/niall_dalton12/AMPED": "/Users/niall/code/AMPED",
}

# Whether changes to directories in file_mounts or cluster_synced_files in the head node
# should sync to the worker node continuously
file_mounts_sync_continuously: False

# Patterns for files to exclude when running rsync up or rsync down
rsync_exclude:
    - "**/.git"
    - "**/.git/**"

# Pattern files to use for filtering out files when running rsync up or rsync down. The file is searched for
# in the source directory and recursively through all subdirectories. For example, if .gitignore is provided
# as a value, the behavior will match git's behavior for finding and using .gitignore files.
rsync_filter:
    - ".gitignore"

# List of commands that will be run before `setup_commands`. If docker is
# enabled, these commands will run outside the container and before docker
# is setup.
initialization_commands: []

# List of shell commands to run to set up nodes.
setup_commands:
  - pip install -U ray[rllib]==1.0.1.post1
  - pip install -U gym-super-mario-bros==7.3.2
  - cd /home/niall_dalton12/AMPED/ &amp;&amp; pip install -e .

# Custom commands that will be run on the head node after common setup.
head_setup_commands:
  - pip install google-api-python-client==1.7.8

# Custom commands that will be run on worker nodes after common setup.
worker_setup_commands: []

# Command to start ray on the head node. You don't need to change this.
head_start_ray_commands:
    - ray stop
    - &gt;-
      ulimit -n 65536;
      ray start
      --head
      --port=6379
      --object-manager-port=8076
      --autoscaling-config=~/ray_bootstrap_config.yaml

# Command to start ray on worker nodes. You don't need to change this.
worker_start_ray_commands:
    - ray stop
    - &gt;-
      ulimit -n 65536;
      ray start
      --address=$RAY_HEAD_IP:6379
      --object-manager-port=8076
&lt;/denchmark-code&gt;

If we cannot run your script, we cannot fix your issue.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='ndalton12' date='2020-12-02T06:17:41Z'>
		I think this problem might have been caused by the worker nodes not starting as expected on GCP when using a GPU because they need to run an extra y command to accept installation of nvidia drivers. Not totally certain though. This problem does not occur when the worker nodes do not have GPUs.
		</comment>
	</comments>
</bug>