<bug id='10810' author='jrtcppv' open_date='2020-09-15T19:55:05Z' closed_time='2020-10-12T20:49:12Z'>
	<summary>[rllib] How to train beyond 2^31 timesteps?</summary>
	<description>
I am using ray 0.8.5. I would like to continue training beyond 2 billion timesteps but am hitting an error related to the timesteps variable being of type np.int32. Is there a way to force the model to use np.int64 without forking ray? Specifically I am using a RecurrentTFModelV2 with APPO.
	</description>
	<comments>
		<comment id='1' author='jrtcppv' date='2020-09-17T06:42:56Z'>
		Hmm I think this is a bug we'd have to fix. Probably the tensor is defined as int32 somewhere? cc &lt;denchmark-link:https://github.com/sven1977&gt;@sven1977&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='jrtcppv' date='2020-09-17T18:01:32Z'>
		Yeah, could be one of the timestep tensors used mainly for exploration that overflows. Taking a look. ...
		</comment>
		<comment id='3' author='jrtcppv' date='2020-10-09T07:53:57Z'>
		This PR fixes the problem:
&lt;denchmark-link:https://github.com/ray-project/ray/pull/11301&gt;#11301&lt;/denchmark-link&gt;

There is also a test case now confirming good ts counting behavior by the different policies (ray/rllib/tests/test_timesteps.py).
		</comment>
		<comment id='4' author='jrtcppv' date='2020-10-09T07:56:27Z'>
		&lt;denchmark-link:https://github.com/jrtcppv&gt;@jrtcppv&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='jrtcppv' date='2020-10-09T07:56:35Z'>
		Thanks for filing this issue! :)
		</comment>
		<comment id='6' author='jrtcppv' date='2020-10-09T12:20:52Z'>
		No problem, thank you for taking care of it and for all the work you do on this excellent library!
		</comment>
	</comments>
</bug>