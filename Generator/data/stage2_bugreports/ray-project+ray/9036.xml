<bug id='9036' author='Cryptiex' open_date='2020-06-19T11:36:27Z' closed_time='2020-07-18T07:58:16Z'>
	<summary>[tune] Population-based training: broken when using keep_checkpoint_num</summary>
	<description>
When using population-based training TUNE stops after some times throwing the following error:
There are paused trials, but no more pending trials with sufficient resources.
This is caused by not finding the latest checkpoint:
&lt;denchmark-code&gt;Failure # 1 (occurred at 2020-06-19_11-26-36)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/ray/tune/ray_trial_executor.py", line 294, in start_trial
    self._start_trial(trial, checkpoint, train=train)
  File "/usr/local/lib/python3.6/dist-packages/ray/tune/ray_trial_executor.py", line 235, in _start_trial
    self.restore(trial, checkpoint)
  File "/usr/local/lib/python3.6/dist-packages/ray/tune/ray_trial_executor.py", line 673, in restore
    data_dict = TrainableUtil.pickle_checkpoint(value)
  File "/usr/local/lib/python3.6/dist-packages/ray/tune/trainable.py", line 62, in pickle_checkpoint
    checkpoint_dir = TrainableUtil.find_checkpoint_dir(checkpoint_path)
  File "/usr/local/lib/python3.6/dist-packages/ray/tune/trainable.py", line 87, in find_checkpoint_dir
    raise FileNotFoundError("Path does not exist", checkpoint_path)
FileNotFoundError: [Errno Path does not exist] /content/TRASH_TUNE_PBT_oversampling_mimic_densenet121/TUNE_Model_0_2020-06-19_11-24-215xncry9c/checkpoint_6/
&lt;/denchmark-code&gt;

The error appears to be somewhat random since it only appears after quite some iterations
The error can be reproduced in this &lt;denchmark-link:https://colab.research.google.com/drive/1-o896bEUm7DTvS24Do0btlqbSHre49MH?usp=sharing&gt;colab notebook&lt;/denchmark-link&gt;
. 
&lt;denchmark-link:https://github.com/richardliaw&gt;@richardliaw&lt;/denchmark-link&gt;
 Is this related to &lt;denchmark-link:https://github.com/ray-project/ray/issues/8772&gt;#8772&lt;/denchmark-link&gt;
 ?
	</description>
	<comments>
		<comment id='1' author='Cryptiex' date='2020-06-19T19:37:07Z'>
		Yeah this seems like an issue. We'll take a look into this.
		</comment>
		<comment id='2' author='Cryptiex' date='2020-06-19T21:05:32Z'>
		This is the issue (just for reference):
&lt;denchmark-code&gt;from ray.tune.schedulers import  PopulationBasedTraining

scheduler = PopulationBasedTraining(
            time_attr='training_iteration',
            metric='mean_accuracy',
            mode='max',
            perturbation_interval=1,
            log_config=True,
            hyperparam_mutations={      
                "lr": lambda: 10 ** np.random.uniform(-2, -5)
            })

train_config = {'lr': 0.01}


analysis = tune.run(
        TUNE_Model,         # Reference to the model class. 
        config=train_config, 
        fail_fast=True,         # Stop after first error
        num_samples=4,          # Num of different hyperparameter configurations
        search_alg=None,     # Search algotihm like Hyperopt, Nevergrad...
        resources_per_trial={"gpu": 1}, # Requested resources per TRIAL. If set to fraction, total_GPUS/res_p_trial can be run in parallel
        global_checkpoint_period=np.inf,   # Do not save checkpoints based on time interval
        checkpoint_freq = 1,        # Save checkpoint every time the checkpoint_score_attr improves
        checkpoint_at_end = True,   
        keep_checkpoints_num = 2,   # Keep only the best checkpoint
        checkpoint_score_attr = 'mean_accuracy', # Metric used to compare checkpoints
        verbose=1,
        scheduler=scheduler,             # Stop bad trials early
        name="TRASH_TUNE_PBT_oversampling_mimic_densenet121",
        local_dir="/content",
        stop={                          # Stop a single trial if one of the conditions are met
            "mean_accuracy": 0.85,
            "training_iteration": 15})
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='Cryptiex' date='2020-06-20T01:09:07Z'>
		I think the temporary fix actually is to comment this out:
&lt;denchmark-code&gt;        keep_checkpoints_num = 2,   # Keep only the best checkpoint
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='Cryptiex' date='2020-06-20T21:15:00Z'>
		Thanks for the fast reply! Commenting out keep_checkpoints_num worked for me as a temporary fix. Any idea where this is coming from? Does the PBT scheduler reference an already deleted checkpoint?
		</comment>
		<comment id='6' author='Cryptiex' date='2020-06-20T21:47:54Z'>
		Yeah, I think it's something about not properly bookkeeping the checkpoints on disk.
		</comment>
		<comment id='7' author='Cryptiex' date='2020-06-30T10:20:31Z'>
		Any updates on this? The temporary fix seems not to work as indicated by &lt;denchmark-link:https://github.com/ray-project/ray/issues/8441&gt;#8441&lt;/denchmark-link&gt;
 and my own experiences
		</comment>
		<comment id='8' author='Cryptiex' date='2020-12-04T04:40:23Z'>
		Hi,
I think this problem still persists in another form, mainly in the pickling function which apparently is not using protocol=4, resulting in an error when objects larger than 4GB have to pickled (e.g. during checkpointing). I am using PBT with keep_checkpoints_num=2.
&lt;denchmark-code&gt;Failure # 1 (occurred at 2020-12-03_20-27-31)
Traceback (most recent call last):
  File "/u/ftaj/anaconda3/envs/drp/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py", line 348, in start_trial
    self._start_trial(trial, checkpoint, train=train)
  File "/u/ftaj/anaconda3/envs/drp/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py", line 289, in _start_trial
    self.restore(trial, checkpoint)
  File "/u/ftaj/anaconda3/envs/drp/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py", line 733, in restore
    obj = TrainableUtil.checkpoint_to_object(value)
  File "/u/ftaj/anaconda3/envs/drp/lib/python3.7/site-packages/ray/tune/trainable.py", line 86, in checkpoint_to_object
    data_dict = TrainableUtil.pickle_checkpoint(checkpoint_path)
  File "/u/ftaj/anaconda3/envs/drp/lib/python3.7/site-packages/ray/tune/trainable.py", line 80, in pickle_checkpoint
    "data": data,
OverflowError: cannot serialize a bytes object larger than 4 GiB
&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='Cryptiex' date='2020-12-04T05:08:00Z'>
		&lt;denchmark-link:https://github.com/FarzanT&gt;@FarzanT&lt;/denchmark-link&gt;
 thanks for opening this; I think your issue is unrelated to the original post. Can you create a new issue + try it out on the latest Ray wheels?
		</comment>
		<comment id='10' author='Cryptiex' date='2020-12-04T05:57:22Z'>
		&lt;denchmark-link:https://github.com/richardliaw&gt;@richardliaw&lt;/denchmark-link&gt;
 Thank you for yout quick response, but before I open another issue I have to say that installing the nightlies resulted in the following problems:

tensorboardx became a requirement all of a sudden (or didn't install with the package)
I got lots of other errors:

&lt;denchmark-code&gt;(pid=raylet) E1204 00:52:43.674732050   27963 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {"created":"@1607061163.674715559","description":"SO_REUSEPORT unavailable on compiling system","file":"src/core/lib/iomgr/socket_utils_common_posix.cc","file_line":192}
2020-12-04 00:52:44,454 WARNING worker.py:1011 -- The agent on node ucn107-74.hpc.oicr.on.ca failed with the following error:
Traceback (most recent call last):
  File "/u/ftaj/anaconda3/envs/drp/lib/python3.7/site-packages/ray/new_dashboard/agent.py", line 300, in &lt;module&gt;
    loop.run_until_complete(agent.run())
  File "/u/ftaj/anaconda3/envs/drp/lib/python3.7/asyncio/base_events.py", line 584, in run_until_complete
    return future.result()
  File "/u/ftaj/anaconda3/envs/drp/lib/python3.7/site-packages/ray/new_dashboard/agent.py", line 174, in run
    agent_ip_address=self.ip))
  File "/u/ftaj/anaconda3/envs/drp/lib/python3.7/site-packages/grpc/experimental/aio/_call.py", line 286, in __await__
    self._cython_call._status)
grpc.experimental.aio._call.AioRpcError: &lt;AioRpcError of RPC that terminated with:
        status = StatusCode.UNAVAILABLE
        details = "failed to connect to all addresses"
        debug_error_string = "{"created":"@1607061164.441617149","description":"Failed to pick subchannel","file":"src/core/ext/filters/client_channel/client_channel.cc","file_line":4090,"referenced_errors":[{"created":"@1607061164.441609893","description":"failed to connect to all addresses","file":"src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc","file_line":394,"grpc_status":14}]}"
&gt;
&lt;/denchmark-code&gt;

With the normal pip install, the PBT would train all trials to the iteration where the perturbation was supposed to happen, then throw that pickle error. Now it's throwing the above error right from the beginning.
Any help is appreciated!
		</comment>
		<comment id='11' author='Cryptiex' date='2020-12-04T09:52:11Z'>
		Other errors look weird... well for tensorboardx, you should run also ‘pip
install “ray[tune]”’.

Can you create an issue for other errors?
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Thu, Dec 3, 2020 at 9:57 PM Farzan Taj ***@***.***&gt; wrote:
 @richardliaw &lt;https://github.com/richardliaw&gt; Thank you for yout quick
 response, but before I open another issue I have to say that installing the
 nightlies resulted in the following problems:

    1. tensorboardx became a requirement all of a sudden (or didn't
    install with the package)
    2. I got lots of other errors:

 (pid=raylet) E1204 00:52:43.674732050   27963 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: ***@***.***","description":"SO_REUSEPORT unavailable on compiling system","file":"src/core/lib/iomgr/socket_utils_common_posix.cc","file_line":192}
 2020-12-04 00:52:44,454 WARNING worker.py:1011 -- The agent on node ucn107-74.hpc.oicr.on.ca failed with the following error:
 Traceback (most recent call last):
   File "/u/ftaj/anaconda3/envs/drp/lib/python3.7/site-packages/ray/new_dashboard/agent.py", line 300, in &lt;module&gt;
     loop.run_until_complete(agent.run())
   File "/u/ftaj/anaconda3/envs/drp/lib/python3.7/asyncio/base_events.py", line 584, in run_until_complete
     return future.result()
   File "/u/ftaj/anaconda3/envs/drp/lib/python3.7/site-packages/ray/new_dashboard/agent.py", line 174, in run
     agent_ip_address=self.ip))
   File "/u/ftaj/anaconda3/envs/drp/lib/python3.7/site-packages/grpc/experimental/aio/_call.py", line 286, in __await__
     self._cython_call._status)
 grpc.experimental.aio._call.AioRpcError: &lt;AioRpcError of RPC that terminated with:
         status = StatusCode.UNAVAILABLE
         details = "failed to connect to all addresses"
         debug_error_string = ***@***.***","description":"Failed to pick ***@***.***","description":"failed to connect to all addresses","file":"src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc","file_line":394,"grpc_status":14}]}"
 &gt;

 With the normal pip install, the PBT would train all trials to the
 iteration where the perturbation was supposed to happen, then throw that
 pickle error. Now it's throwing the above error right from the beginning.

 Any help is appreciated!

 —
 You are receiving this because you were mentioned.


 Reply to this email directly, view it on GitHub
 &lt;#9036 (comment)&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/ABCRZZLQLBLFL3CNVAUTTPDSTB25BANCNFSM4OCUXNBA&gt;
 .



		</comment>
	</comments>
</bug>