<bug id='8715' author='stephanie-wang' open_date='2020-06-01T23:13:07Z' closed_time='2020-06-02T23:06:37Z'>
	<summary>[autoscaler] Cluster becomes unusable when job exits.</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

Ray version and other system information (Python version, TensorFlow version, OS): 0.9dev
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
If we cannot run your script, we cannot fix your issue.
I ran with the default AWS autoscaler config (python/ray/autoscaler/aws/example-full.yaml), modified the minimum to 2 workers. Steps to reproduce:

SSH into head node.
Run a Ray script from bash (while True: ray.get(foo.remote()) works)
Cancel Ray script with CTRL+C.

Then, in /tmp/ray/session_latest/logs/monitor.err, I get this:
&lt;denchmark-code&gt;2020-06-01 23:06:52,035 INFO autoscaler.py:619 -- StandardAutoscaler: 2/2 target nodes (0 pending) (2 updating) (bringup=True)
2020-06-01 23:06:52,035 INFO autoscaler.py:620 -- LoadMetrics: MostDelayedHeartbeats={'172.30.0.103': 0.1708686351776123}, NodeIdleSeconds=Min=0 Mean=0 Max=0, NumNodesConnected=1, NumNodesUsed=0.02, ResourceUsage=1.0/64.0 CPU, 0.0 GiB/178.57 GiB memory, 0.0/1.0 node:172.30.0.103, 0.0 GiB/55.73 GiB object_store_memory, TimeSinceLastHeartbeat=Min=0 Mean=0 Max=0
2020-06-01 23:06:56,727 INFO updater.py:257 -- NodeUpdater: i-0fe7dc6a0f3fd63be: Running uptime on 172.30.1.111...
2020-06-01 23:06:56,733 INFO updater.py:257 -- NodeUpdater: i-0a74d5ee1dadf1b74: Running uptime on 172.30.1.30...
Warning: Permanently added '172.30.1.111' (ECDSA) to the list of known hosts.^M
Warning: Permanently added '172.30.1.30' (ECDSA) to the list of known hosts.^M
Error in monitor loop
Traceback (most recent call last):
  File "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/monitor.py", line 277, in run
    self._run()
  File "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/monitor.py", line 235, in _run
    self.process_messages()
  File "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/monitor.py", line 187, in process_messages
    message_handler(channel, data)
  File "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/monitor.py", line 125, in xray_job_notification_handler
    job_data = gcs_entries.entries[0]
IndexError: list index (0) out of range
2020-06-01 23:06:56,765 ERROR autoscaler.py:645 -- StandardAutoscaler: kill_workers triggered
2020-06-01 23:06:56,828 INFO node_provider.py:353 -- AWSNodeProvider: terminating nodes ['i-0fe7dc6a0f3fd63be', 'i-0a74d5ee1dadf1b74'] (spot nodes cannot be stopped, only terminated)
2020-06-01 23:06:56,997 ERROR autoscaler.py:650 -- StandardAutoscaler: terminated 2 node(s)
2020-06-01 23:06:57,123 INFO commands.py:93 -- teardown_cluster: Keeping 2 nodes...
Monitor: Cleanup exception. Trying again...
mux_client_request_session: read from master failed: Broken pipe^M
Failed to connect to new control master^M
mux_client_request_session: read from master failed: Broken pipe^M
Failed to connect to new control master^M
2020-06-01 23:06:59,270 INFO commands.py:93 -- teardown_cluster: Keeping 2 nodes...
Monitor: Cleanup exception. Trying again...
2020-06-01 23:07:01,385 INFO commands.py:93 -- teardown_cluster: Keeping 2 nodes...
Monitor: Cleanup exception. Trying again...
2020-06-01 23:07:02,231 INFO updater.py:257 -- NodeUpdater: i-0a74d5ee1dadf1b74: Running uptime on 172.30.1.30...
ssh: connect to host 172.30.1.30 port 22: Connection refused^M
2020-06-01 23:07:02,243 INFO updater.py:257 -- NodeUpdater: i-0fe7dc6a0f3fd63be: Running uptime on 172.30.1.111...
ssh: connect to host 172.30.1.111 port 22: Connection refused^M
2020-06-01 23:07:03,513 INFO commands.py:93 -- teardown_cluster: Keeping 2 nodes...
Monitor: Cleanup exception. Trying again...
2020-06-01 23:07:05,622 INFO commands.py:93 -- teardown_cluster: Keeping 2 nodes...
Monitor: Cleanup exception. Trying again...
&lt;/denchmark-code&gt;

Other times I get the same Python exception and teardown messages, but not the infinitely repeating messages about "Cleanup exception". However, the messages just stop completely and no new worker nodes are started. I didn't confirm, but this might be if I wait for the worker nodes to come up completely before starting the job.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='stephanie-wang' date='2020-06-01T23:13:32Z'>
		cc &lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
, might be a P0?
		</comment>
		<comment id='2' author='stephanie-wang' date='2020-06-01T23:22:53Z'>
		Actually, this just happened again without a keyboard interrupt, I think because my driver exited.
		</comment>
		<comment id='3' author='stephanie-wang' date='2020-06-01T23:28:20Z'>
		Reproduced the same error with this short script, head node with 0 CPUs, and 2 worker nodes with lots of CPUs:
&lt;denchmark-code&gt;import ray
import time

ray.init(address="auto")


@ray.remote
def foo():
    return


for _ in range(100):
    ray.get(foo.remote())
    time.sleep(0.1)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='stephanie-wang' date='2020-06-02T01:22:47Z'>
		&lt;denchmark-link:https://github.com/stephanie-wang&gt;@stephanie-wang&lt;/denchmark-link&gt;
 We should find an assignee for this task as it is a release blocker. I will post it in the slack channel!!
		</comment>
		<comment id='5' author='stephanie-wang' date='2020-06-02T06:22:08Z'>
		After this PR is merged, &lt;denchmark-link:https://github.com/ray-project/ray/pull/8401/files&gt;https://github.com/ray-project/ray/pull/8401/files&lt;/denchmark-link&gt;
, we stopped using AsyncAppend, but we started using Put. This makes this part of code incompatible. &lt;denchmark-link:https://github.com/ffbin&gt;@ffbin&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='stephanie-wang' date='2020-06-02T06:26:08Z'>
		I think we should be careful when we start using the store_client next time because it seems like it will change some storing mechanism. =&gt; Seems like autoscaler tests are more like "unit tests".
		</comment>
		<comment id='7' author='stephanie-wang' date='2020-06-02T11:35:11Z'>
		&lt;denchmark-link:https://github.com/rkooo567&gt;@rkooo567&lt;/denchmark-link&gt;
 Thanks! dashboard.py is also need change, i'll add comments to your PR.
		</comment>
		<comment id='8' author='stephanie-wang' date='2020-06-02T16:16:48Z'>
		&lt;denchmark-link:https://github.com/ffbin&gt;@ffbin&lt;/denchmark-link&gt;
 Thanks for finding that out!
		</comment>
	</comments>
</bug>