<bug id='7121' author='yongjun823' open_date='2020-02-11T13:16:33Z' closed_time='2020-02-27T11:02:20Z'>
	<summary>[rllib] How can I record the full episode results of  rllib?</summary>
	<description>
&lt;denchmark-h:h3&gt;What is your question?&lt;/denchmark-h&gt;

Ray version and other system information (Python version, TensorFlow version, OS):
&lt;denchmark-code&gt;Python version: Python 3.6.10 :: Anaconda, Inc.
Anaconda version: 4.7.12
TensorFlow version: 1.15.0
OS: Ubuntu 16.04
ray version: 0.8.1
&lt;/denchmark-code&gt;

Hello
I've trained an RL model using rllib
I have tested the breakout environment and the agent is running successfully.
Here is the python code I used. I used Rllib's python API
If you test at openai gym after learning, it works successfully.
I want to record the results of the agent.
Agent test was executed using the rollout command of rllib.
My result photo is below.
This is the command I executed
&lt;denchmark-code&gt;rllib rollout checkpoint_4301/checkpoint-4301 \
    --run PPO --env BreakoutNoFrameskip-v4 --monitor --config '{"monitor": true}'
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/19910566/74238350-f6990a00-4d18-11ea-97b5-681da6163a18.png&gt;&lt;/denchmark-link&gt;

When testing with rollout, on average 8 games run.
Many episodes ran, but only the results of one game (four lives in the case of breakout) were saved as videos.
And I think it's a mix of different game parts.
I additionally tested ms pacman.
Pacman's recording showed the same problem.
I have attached my rllib model and the recorded results file.
I would be grateful if you could tell me a document or a way to help me record the video.
&lt;denchmark-link:https://github.com/ray-project/ray/files/4185820/PPO_BreakoutNoFrameskip-v4_2020-02-11_21-32-52m48o1b48.zip&gt;PPO_BreakoutNoFrameskip-v4_2020-02-11_21-32-52m48o1b48.zip&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/ray-project/ray/files/4185821/BreakoutModel.zip&gt;BreakoutModel.zip&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='yongjun823' date='2020-02-12T14:09:44Z'>
		Thanks for this issue. This sounds almost like a bug. &lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
 could you comment? If yes, I'll take a look to get this fixed.
		</comment>
		<comment id='2' author='yongjun823' date='2020-02-12T21:42:09Z'>
		Hm, all rollout.py does is adding env = gym.wrappers.Monitor(...). Is it possible this is an issue with the env wrapper configuration from gym?
		</comment>
		<comment id='3' author='yongjun823' date='2020-02-13T06:16:57Z'>
		Thank you for the answer.
This is the Python code I used to run rllib.
I tested the result using rollout.py.
The gym env environment uses BreakoutNoFrameskip-v4 and didn't fix it.
Can num_workers or num_envs_per_worker be a problem?
import ray
import ray.rllib.agents.ppo as ppo
from ray.tune.logger import pretty_print

ray.init()

config = ppo.DEFAULT_CONFIG.copy()

config["num_gpus"] = 4
config["num_workers"] = 10 
config["num_envs_per_worker"] = 5
config["train_batch_size"] = 50000
config["sample_batch_size"] = 1000
config["sgd_minibatch_size"] = 5000
config["eager"] = False

trainer = ppo.PPOTrainer(config=config, env="BreakoutNoFrameskip-v4")

for i in range(100000):

   if i % 100 == 0:

       checkpoint = trainer.save()
       print("checkpoint saved at", checkpoint)
		</comment>
		<comment id='4' author='yongjun823' date='2020-02-26T20:18:46Z'>
		I can reproduce the issue with CartPole as well. It's only recording a very short sequence of the rolled out episode, and only a few of these as well (4 out of 50 rolled out episodes have mpeg snippet files). Taking a look. ...
		</comment>
		<comment id='5' author='yongjun823' date='2020-02-27T11:02:20Z'>
		Ok, checkout this PR and let me know, whether this fixes your problems:
&lt;denchmark-link:https://github.com/ray-project/ray/pull/7347&gt;#7347&lt;/denchmark-link&gt;

I separated the --monitor option from the --out option (which used to be silently(!) required to record any videos into the --out + "monitor" directory).
The new command line would be (tested on CartPole with episodes of 1000 ts):
rllib rollout [your checkpoint file] --run PPO --env BreakoutNoFrameskip-v4 --video-dir [some dir where all(!) episode videos will be stored in full length]
		</comment>
	</comments>
</bug>