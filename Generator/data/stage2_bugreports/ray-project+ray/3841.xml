<bug id='3841' author='bjg2' open_date='2019-01-24T16:17:10Z' closed_time='2020-11-28T22:36:40Z'>
	<summary>[rllib] IMPALA implementation drops one experience per unroll</summary>
	<description>
&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

When calculating VTraceLoss in VTracePolicyGraph all the experiences from the unroll are taken except for the last one, which is used for bootstrap_value. As far as I can see, that experience is never learned on (that action/reward/behavior is just thrown away and not repeated in the next unroll). That experience should be repeated as the first experience of the next unroll.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

In VTrace implementation in &lt;denchmark-link:https://github.com/ray-project/ray/blob/master/python/ray/rllib/agents/impala/vtrace_policy_graph.py&gt;https://github.com/ray-project/ray/blob/master/python/ray/rllib/agents/impala/vtrace_policy_graph.py&lt;/denchmark-link&gt;
, take a look of VTraceLoss arguments (at the moment lines between 175 and 189). I haven't found the place in the code where the experience is repeated in the next batch.
	</description>
	<comments>
		<comment id='1' author='bjg2' date='2019-01-24T19:23:36Z'>
		Just curious, is this an issue for you? I can see it being a problem if episode lengths line up exactly with the batch size, but otherwise you just lose some tiny fraction of sample efficiency.
		</comment>
		<comment id='2' author='bjg2' date='2019-01-25T10:05:32Z'>
		Well, I wouldn't say it is a big issue, but the system drops some experiences. It could happen that experience carries important information, like a big reward or important action or something. In the &lt;denchmark-link:https://github.com/deepmind/scalable_agent&gt;deepmind implementation&lt;/denchmark-link&gt;
, they send the same experience again in the next unroll.
		</comment>
		<comment id='3' author='bjg2' date='2019-01-25T19:25:58Z'>
		Right, we could potentially do the same thing by having the remote workers duplicate that last piece of experience across adjacent batches.
		</comment>
		<comment id='4' author='bjg2' date='2019-01-25T20:28:22Z'>
		I'm fairly new to RL field and am familiar only with IMPALA, so I'm not sure how that experience repeat in the remote workers would abstract to other policies. But regarding IMPALA, sounds good!
		</comment>
		<comment id='5' author='bjg2' date='2019-01-25T20:54:00Z'>
		Yeah I think it would be an option for IMPALA only, though the learning
impact won't be huge, since you're just randomly increasing the probability
of some samples -- so no bias, but a bit of extra variance.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Fri, Jan 25, 2019, 12:28 PM bjg2 ***@***.***&gt; wrote:
 I'm fairly new to RL field and am familiar only with IMPALA, so I'm not
 sure how that experience repeat in the remote workers would abstract to
 other policies. But regarding IMPALA, sounds good!

 —
 You are receiving this because you commented.
 Reply to this email directly, view it on GitHub
 &lt;#3841 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AAA6Si83otT99op4ALqpok5bsrd6hAooks5vG2jsgaJpZM4aRNrx&gt;
 .



		</comment>
		<comment id='6' author='bjg2' date='2019-01-28T12:54:26Z'>
		You would not randomly increasing the probability of some samples, but would learn on samples that are currently not learned on. So you would not have them more than other samples, you would learn from each sample exactly once (currently rllib is not learning from the last sample of the unroll).
		</comment>
		<comment id='7' author='bjg2' date='2019-01-28T17:59:28Z'>
		Yeah, I was referring to other algorithms if we were to enable repeat for
them. Though the converse is true, since the unrolls are random slices of
episodes there is no bias (unless the episode lengths align with batch
sizes, in which case learning could be severely impacted).
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Mon, Jan 28, 2019, 4:54 AM bjg2 ***@***.***&gt; wrote:
 You would not randomly increasing the probability of some samples, but
 would learn on samples that are currently not learned on. So you would not
 have them more than other samples, you would learn from each sample exactly
 once (currently rllib is not learning from the last sample of the unroll).

 —
 You are receiving this because you commented.
 Reply to this email directly, view it on GitHub
 &lt;#3841 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AAA6StvAhrIi3uAdY7CSX-DpjusB0USEks5vHvMJgaJpZM4aRNrx&gt;
 .



		</comment>
		<comment id='8' author='bjg2' date='2019-01-29T10:43:50Z'>
		Oh, I misunderstood, sounds good!
		</comment>
		<comment id='9' author='bjg2' date='2020-11-14T02:33:34Z'>
		Hi, I'm a bot from the Ray team :)
To help human contributors to focus on more relevant issues, I will automatically add the stale label to issues that have had no activity for more than 4 months.
If there is no further activity in the 14 days, the issue will be closed!

If you'd like to keep the issue open, just leave any comment, and the stale label will be removed!
If you'd like to get more attention to the issue, please tag one of Ray's contributors.

You can always ask for help on our &lt;denchmark-link:https://discuss.ray.io/&gt;discussion forum&lt;/denchmark-link&gt;
 or &lt;denchmark-link:https://github.com/ray-project/ray#getting-involved&gt;Ray's public slack channel&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='10' author='bjg2' date='2020-11-28T22:36:28Z'>
		Hi again! The issue will be closed because there has been no more activity in the 14 days since the last message.
Please feel free to reopen or open a new issue if you'd still like it to be addressed.
Again, you can always ask for help on our &lt;denchmark-link:https://discuss.ray.io&gt;discussion forum&lt;/denchmark-link&gt;
 or &lt;denchmark-link:https://github.com/ray-project/ray#getting-involved&gt;Ray's public slack channel&lt;/denchmark-link&gt;
.
Thanks again for opening the issue!
		</comment>
	</comments>
</bug>