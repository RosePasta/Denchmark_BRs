<bug id='3994' author='Anya28' open_date='2019-02-08T17:39:22Z' closed_time='2019-08-01T17:18:14Z'>
	<summary>Pytorch A3C - unstable runs</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
Ray installed from (source or binary): source
Ray version: 0.5.3
Python version: 3.6.7
Exact command to reproduce:

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

I have tried running the pytorch version of A3C algorithm, keeping all the configs as they are and just changing the flag "use_pytorch" to True. Varying the number of workers and environments (CartPole, Qbert, SpaceInvaders), in most of the cases pytorch A3C would start running and after a while would crash before the end of the experiment, with the error that the "worker died or was killed". Sometimes, however, it would run till the end of the experiment (reach 500000 steps in my case). Has anyone else come across this issue? Thank you!
	</description>
	<comments>
		<comment id='1' author='Anya28' date='2019-02-08T17:50:46Z'>
		Have you tried A2C? It usually works better than A3C and doesn't have these sort of thread safety issues.
		</comment>
		<comment id='2' author='Anya28' date='2019-02-08T18:01:10Z'>
		We should probably fix this though, I think it's either a thread safety bug in plasma, or we forgot to grab a lock when accessing some PyTorch state.
		</comment>
		<comment id='3' author='Anya28' date='2019-02-08T18:54:52Z'>
		
Have you tried A2C? It usually works better than A3C and doesn't have these sort of thread safety issues. We should probably fix this though, I think it's either a thread safety bug in plasma, or we forgot to grab a lock when accessing some PyTorch state.

I have just tried it now, it is stable comparing to pytorch A3C. However it is a synchronous implementation, so depending on the variance in environment, it can be very slow. Does it mean that for pytorch it is not possible to use asynchronous update atm?
As to the locks, I also have a question on that. In the torch_policy_graph, the lock from python threading module is put in every function. The reasoning for that, that I saw in the PR comments,  is that "it makes the (specifically) Pytorch code more stable etc". However, I would like to understand why lock is especially important for Pytorch, and how would that work with the idea of asynchronous update for the algorithm? Once one thread grabs a lock, the others will be blocked till the lock is released, and so as the number of threads is increasing, having a lock in each computational function would make the implementation progressively slower?
Thank you for your replies!
		</comment>
		<comment id='4' author='Anya28' date='2019-02-08T19:54:25Z'>
		
I have just tried it now, it is stable comparing to pytorch A3C. However it is a synchronous implementation, so depending on the variance in environment, it can be very slow.

That's true, though if you have a GPU A2C can be much faster anyways. For example, in the benchmarks here A2C gets twice the Qbert reward in 1 hour, with less workers: &lt;denchmark-link:https://github.com/ray-project/rl-experiments#impala-and-a2c&gt;https://github.com/ray-project/rl-experiments#impala-and-a2c&lt;/denchmark-link&gt;


Does it mean that for pytorch it is not possible to use asynchronous update atm?

By the way, I noticed you're using a quite old version of Ray, it's likely upgrading will help. There have been a number of plasma bug fixes since then.

Once one thread grabs a lock, the others will be blocked till the lock is released, and so as the number of threads is increasing, having a lock in each computational function would make the implementation progressively slower?

The lock will add overhead within each worker, however RLlib scales by adding multiple worker processes. So the lock overhead does not get worse with increased parallelism.
		</comment>
		<comment id='5' author='Anya28' date='2019-08-01T17:15:30Z'>
		&lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
 any progress with AsyncSampler and PyTorch compatibility? Can you elaborate on what's the issue?
		</comment>
	</comments>
</bug>