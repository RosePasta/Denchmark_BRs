<bug id='13234' author='KaleabTessera' open_date='2021-01-06T11:10:34Z' closed_time='2021-01-10T10:10:29Z'>
	<summary>[tune] Incorrect number of samples for ASHAScheduler</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

I am using tune and running random search &amp; bayesian opt with ASHAScheduler scheduler. I set the num_samples=50, but only 20 samples are run. Sometimes it runs 30 samples. Shouldn't all samples be run? Is there something in ASHAScheduler that limits the amount of samples?
Versions:
&lt;denchmark-code&gt;ray==1.1.0
torch==1.7.0
Python 3.7.9
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Here is the tune code snippet.
&lt;denchmark-code&gt;ray_config['density'] = tune.sample_from(
        lambda spec: np.random.uniform(0, 1.0, num_layers))

sched = ASHAScheduler(
	time_attr='training_iteration',
	metric='accuracy',
	mode='max',
	max_t=ray_config['epochs'],
	grace_period=ray_config['epochs']/10,
	reduction_factor=3,
	brackets=1)


all_trials = tune.run(PytorchTrainable,
	search_alg=alg,
	scheduler=sched,
	local_dir=ray_config['local_dir'],
	num_samples=50,
	name=ray_config['ray_name'],
	resume=ray_config['resume'],
	resources_per_trial={
	  "cpu": ray_config['num_cpu'],
	  "gpu": int(not ray_config['no_cuda'])
	},
	verbose=1,
	config=ray_config,
	stop={
	  "training_iteration": ray_config['epochs']},
	raise_on_failed_trial=False,
	queue_trials=True)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='KaleabTessera' date='2021-01-06T11:18:18Z'>
		Here is the &lt;denchmark-link:https://drive.google.com/file/d/17wj6UwKXItFJKdv6FnUt-G1BFPa9LuFA/view?usp=sharing&gt;exp state file.&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='KaleabTessera' date='2021-01-06T11:50:43Z'>
		Yes, Ray Tune should still run all 50 samples for at least one iteration. On a first glance your code looks correct. Can you provide a full reproducible script for us to try out?
		</comment>
		<comment id='3' author='KaleabTessera' date='2021-01-06T11:54:01Z'>
		Just to be sure, you're passing a float here: grace_period=ray_config['epochs']/10,, can you try passing 	grace_period=ray_config['epochs']//10, (an integer) instead? We don't do type checking in the ASHA scheduler and maybe something odd happens if a value below 1 but above 0 is passed.
		</comment>
		<comment id='4' author='KaleabTessera' date='2021-01-06T16:24:13Z'>
		I am rerunning this with  max_failures=-1. I think the issue might have something to do with a resumed trail. Replicating this with smoke tests seem to run all samples. I am rerunning the larger exps, I will close if works without resuming.
		</comment>
		<comment id='5' author='KaleabTessera' date='2021-01-07T07:24:55Z'>
		&lt;denchmark-link:https://github.com/krfricke&gt;@krfricke&lt;/denchmark-link&gt;
 So I was able to reproduce this using .
Looking at the logs, I see that duplicate params for search space are being suggested and skipped:
&lt;denchmark-code&gt;2021-01-06 22:17:05,566 INFO bayesopt.py:287 -- Skipping duplicated config: {'density_0': 0.21018943325648579, 'density_1': 1.0, 'density_2': 0.8103608253273101, 'density_3': 0.38187212380363045, 'density_4': 0.14669551098281888}.
2021-01-06 22:17:20,265 INFO bayesopt.py:287 -- Skipping duplicated config: {'density_0': 0.21018930256281929, 'density_1': 1.0, 'density_2': 0.810360935012639, 'density_3': 0.38187230954535634, 'density_4': 0.14669548946371516}.
2021-01-06 22:17:34,943 INFO bayesopt.py:287 -- Skipping duplicated config: {'density_0': 0.21018930908893008, 'density_1': 1.0, 'density_2': 0.8103608766763609, 'density_3': 0.381872222632432, 'density_4': 0.14669547727642002}.
&lt;/denchmark-code&gt;

I assume after 3 duplicates, the system stops? My run stopped after 11 trails. Also, I am not sure if increasing random_search_steps(currently =10) might help?
I am going to try to increase the kappa in BO, to force the alg to explore more and hopefully, no less duplicate param suggestions.
		</comment>
		<comment id='6' author='KaleabTessera' date='2021-01-07T08:59:38Z'>
		Yes, the maximum repeats per config is configured via the patience parameter, but if skip_duplicates=True, these trials are not run again. Duplicate configurations can come up if BO converges quickly to an acquisition function, which could be related to the objective. So it seems like what you experience could be this, or maybe a combination of this and resuming trials. It's hard to say without more context.
If you suspect there's something wrong in Tune's scheduling or search algorithms, we'll need a full reproduction script to look into this.
		</comment>
		<comment id='7' author='KaleabTessera' date='2021-01-10T10:10:29Z'>
		Yeah this seems to be a function of BO converging quickly, setting a higher  resulted in more exploration and so all samples were run. Thanks &lt;denchmark-link:https://github.com/krfricke&gt;@krfricke&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>