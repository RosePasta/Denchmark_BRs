<bug id='7659' author='suprmat95' open_date='2020-03-19T13:15:55Z' closed_time='2020-04-04T15:21:22Z'>
	<summary>[rllib]  execution error of rllib train and rollout of a custom MultiAgentEnv</summary>
	<description>
Hello everyone,
I wrote my custom MultiAgentEnv
class StockTradingMultiAgent(MultiAgentEnv):
I tried to train it by code using tune and by trainer.train() and it works correctly.
But now I want to do some analysis using an environment trained executing a defined number of steps.
So I registered my environment with the id StockMultiAgent-v0 .
So I executed the command:
rllib train --run PPO --env StockMultiAgent-v0
but I got this  error

Failure # 1 (occurred at 2020-03-19_13-57-33)
Traceback (most recent call last):
File "/Users/matteo/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py", line 459, in _process_trial
result = self.trial_executor.fetch_result(trial)
File "/Users/matteo/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py", line 377, in fetch_result
result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)
File "/Users/matteo/anaconda3/lib/python3.7/site-packages/ray/worker.py", line 1504, in get
raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(AttributeError): �[36mray::PPO.init()�[39m (pid=1164, ip=172.20.10.4)
File "python/ray/_raylet.pyx", line 437, in ray._raylet.execute_task
File "python/ray/_raylet.pyx", line 449, in ray._raylet.execute_task
File "python/ray/_raylet.pyx", line 450, in ray._raylet.execute_task
File "python/ray/_raylet.pyx", line 452, in ray._raylet.execute_task
File "python/ray/_raylet.pyx", line 430, in ray._raylet.execute_task.function_executor
File "/Users/matteo/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py", line 86, in init
Trainer.init(self, config, env, logger_creator)
File "/Users/matteo/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer.py", line 447, in init
super().init(config, logger_creator)
File "/Users/matteo/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py", line 172, in init
self._setup(copy.deepcopy(self.config))
File "/Users/matteo/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer.py", line 591, in _setup
self._init(self.config, self.env_creator)
File "/Users/matteo/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py", line 105, in _init
self.config["num_workers"])
File "/Users/matteo/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer.py", line 658, in _make_workers
logdir=self.logdir)
File "/Users/matteo/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/worker_set.py", line 60, in init
RolloutWorker, env_creator, policy, 0, self._local_config)
File "/Users/matteo/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/worker_set.py", line 262, in _make_worker
_fake_sampler=config.get("_fake_sampler", False))
File "/Users/matteo/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py", line 282, in init
self.env = _validate_env(env_creator(env_context))
File "/Users/matteo/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer.py", line 549, in 
self.env_creator = lambda env_config: gym.make(env)
File "/Users/matteo/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py", line 142, in make
return registry.make(id, **kwargs)
File "/Users/matteo/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py", line 87, in make
env = spec.make(**kwargs)
File "/Users/matteo/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py", line 62, in make
env.unwrapped.spec = selAttributeError: 'StockTradingMultiAgent' object has no attribute 'unwrapped'
I have to implement the unwrapped attribute? what it means?

I also tried to execute rllib rollout starting by a checkpoint previously generated by code, but I also got errors.
Do you have any idea?
Thanks,
Matteo
	</description>
	<comments>
		<comment id='1' author='suprmat95' date='2020-03-19T19:58:54Z'>
		You can't use the gym registry to register custom envs (and they cannot be passed via --run from the command line). See: &lt;denchmark-link:https://ray.readthedocs.io/en/latest/rllib-env.html#configuring-environments&gt;https://ray.readthedocs.io/en/latest/rllib-env.html#configuring-environments&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='suprmat95' date='2020-03-19T21:29:44Z'>
		Thank you for your answer. Can you suggest me how to do the rollout of my custom environment?
Because I understand how to train it but not how to run it after train.
		</comment>
		<comment id='3' author='suprmat95' date='2020-03-19T21:33:52Z'>
		Uncomment this line here to register your custom env: &lt;denchmark-link:https://github.com/ray-project/ray/blob/master/rllib/rollout.py#L33&gt;https://github.com/ray-project/ray/blob/master/rllib/rollout.py#L33&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='suprmat95' date='2020-04-04T15:21:22Z'>
		Really thank you, now it's work!
		</comment>
	</comments>
</bug>