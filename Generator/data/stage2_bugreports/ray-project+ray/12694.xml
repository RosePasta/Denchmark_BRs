<bug id='12694' author='richardliaw' open_date='2020-12-09T01:10:38Z' closed_time='2020-12-09T02:50:00Z'>
	<summary>Resource scheduling error message is misleading/incorrect</summary>
	<description>
I'm seeing this error message in my stderr.
It says I have some number of workers that cannot be available, but then subsequently in the same sentence, claims to actually have precisely those resources available!
&lt;denchmark-code&gt;2020-12-08 17:03:39,197	WARNING worker.py:1011 -- The actor 
or task with ID ffffffffffffffff5cf29c5b01000000 cannot be scheduled right now. 
It requires {node:10.27.27.214: 0.010000} for placement, 
but this node only has remaining {memory: 24.365234 GiB}, 
{object_store_memory: 8.398438 GiB}, {node:10.27.27.214: 0.850000}. 
In total there are 0 pending tasks and 1 pending actors on this node. 
This is likely due to all cluster resources being claimed by actors. 
To resolve the issue, consider creating fewer actors or increase the 
resources available to this Ray cluster. You can ignore this message 
if this Ray cluster is expected to auto-scale.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='richardliaw' date='2020-12-09T01:12:09Z'>
		(this is a new application that I'm working with, so I'll investigate why this is the case).
cc &lt;denchmark-link:https://github.com/rkooo567&gt;@rkooo567&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/wuisawesome&gt;@wuisawesome&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='richardliaw' date='2020-12-09T01:19:37Z'>
		This message is printed as a deadlock warning, and I guess the issue is just a deadlock warning happens before we actually schedule the task.
		</comment>
		<comment id='3' author='richardliaw' date='2020-12-09T01:32:05Z'>
		&lt;denchmark-link:https://github.com/rkooo567&gt;@rkooo567&lt;/denchmark-link&gt;
 Can you explain to me what you mean by deadlock warning? To me, it seems like we're requesting resources, and the resources are available.
		</comment>
		<comment id='4' author='richardliaw' date='2020-12-09T01:43:11Z'>
		So this is printed as a part of
&lt;denchmark-code&gt;  /// Push an error to the driver if this node is full of actors and so we are
  /// unable to schedule new tasks or actors at all.
  void WarnResourceDeadlock();
&lt;/denchmark-code&gt;

But it seems like there's a timing issue that this warning can be called when it is not supposed to.
		</comment>
		<comment id='5' author='richardliaw' date='2020-12-09T01:50:37Z'>
		&lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
 It looks like if there's no available worker (meaning  returns nullptr), we can have tasks can be scheduled with existing resources, but still in the READY state, which will be printed as a part of resource deadlock warning? (Look at the line with )
    // FIFO order within each class.
    for (const auto &amp;task_id : it-&gt;second) {
      const auto &amp;task = local_queues_.GetTaskOfState(task_id, TaskState::READY);
      if (!local_available_resources_.Contains(task_resources)) {
        // All the tasks in it.second have the same resource shape, so
        // once the first task is not feasible, we can break out of this loop
        break;
      }

      // Try to get an idle worker to execute this task. If nullptr, there
      // aren't any available workers so we can't assign the task.
      std::shared_ptr&lt;WorkerInterface&gt; worker =
          worker_pool_.PopWorker(task.GetTaskSpecification());
      if (worker != nullptr) {
        AssignTask(worker, task, &amp;post_assign_callbacks);
      }
    }
  }
		</comment>
		<comment id='6' author='richardliaw' date='2020-12-09T01:54:18Z'>
		Let's verify this doesn't happen when the new scheduler is turned on.
		</comment>
		<comment id='7' author='richardliaw' date='2020-12-09T01:56:13Z'>
		&lt;denchmark-link:https://github.com/wuisawesome&gt;@wuisawesome&lt;/denchmark-link&gt;
 This won't because we don't use new scheduler queue for the resource deadlock warning judgement logic.
Also I found the test has been actually skipped;
@pytest.mark.skip(reason="TODO detect resource deadlock")
def test_warning_for_resource_deadlock(error_pubsub, shutdown_only):
    p = error_pubsub
    # Check that we get warning messages for infeasible tasks.
    ray.init(num_cpus=1)

    @ray.remote(num_cpus=1)
    class Foo:
        def f(self):
            return 0

    @ray.remote
    def f():
        # Creating both actors is not possible.
        actors = [Foo.remote() for _ in range(2)]
        for a in actors:
            ray.get(a.f.remote())

    # Run in a task to check we handle the blocked task case correctly
    f.remote()
    errors = get_error_message(p, 1, ray_constants.RESOURCE_DEADLOCK_ERROR)
    assert len(errors) == 1
    assert errors[0].type == ray_constants.RESOURCE_DEADLOCK_ERROR
		</comment>
		<comment id='8' author='richardliaw' date='2020-12-09T01:56:23Z'>
		Should we enable it for the new scheduler?
		</comment>
		<comment id='9' author='richardliaw' date='2020-12-09T02:02:26Z'>
		Right we have to implement it, so we should just make sure this bug doesn't exist once we do turn it on.
Let me know if I'm missing something here.
		</comment>
		<comment id='10' author='richardliaw' date='2020-12-09T02:50:00Z'>
		Duplicates &lt;denchmark-link:https://github.com/ray-project/ray/issues/12695&gt;#12695&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>