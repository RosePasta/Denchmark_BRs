<bug id='8688' author='PgLoLo' open_date='2020-05-31T10:06:32Z' closed_time='2020-07-15T16:09:13Z'>
	<summary>[rllib] valuation script tryies  to convert pytorch cuda tensor to numpy</summary>
	<description>
When using rllib and turning on evaluation, rllib tries to convert pytorch cuda tensor to numpy and fails with exception "TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
Simple reproduced script:
from ray import tune
from ray.rllib.agents.dqn import ApexTrainer


ray.init()


tune.run(
    ApexTrainer,
    stop={"episode_reward_mean": 20000},
    config={
        "env": "SpaceInvaders-v0",
        "num_gpus": 1,
        "num_workers": 16,
        "use_pytorch": True,
        "evaluation_interval": 1,
    },
)
Is that really a bug or am I doing something wrong? Any workarounds?
	</description>
	<comments>
		<comment id='1' author='PgLoLo' date='2020-05-31T10:06:44Z'>
		Full exception:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/data/miniconda3_for_docker/envs/rl_issude/lib/python3.8/site-packages/ray/tune/trial_runner.py", line 467, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File "/data/miniconda3_for_docker/envs/rl_issude/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py", line 431, in fetch_result
    result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)
  File "/data/miniconda3_for_docker/envs/rl_issude/lib/python3.8/site-packages/ray/worker.py", line 1515, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): ray::APEX.train() (pid=19984, ip=172.17.0.2)
  File "python/ray/_raylet.pyx", line 463, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 417, in ray._raylet.execute_task.function_executor
  File "/data/miniconda3_for_docker/envs/rl_issude/lib/python3.8/site-packages/ray/rllib/agents/trainer.py", line 514, in train
    evaluation_metrics = self._evaluate()
  File "/data/miniconda3_for_docker/envs/rl_issude/lib/python3.8/site-packages/ray/rllib/agents/trainer.py", line 698, in _evaluate
    self.evaluation_workers.local_worker().sample()
  File "/data/miniconda3_for_docker/envs/rl_issude/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py", line 510, in sample
    batches = [self.input_reader.next()]
  File "/data/miniconda3_for_docker/envs/rl_issude/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py", line 54, in next
    batches = [self.get_data()]
  File "/data/miniconda3_for_docker/envs/rl_issude/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py", line 98, in get_data
    item = next(self.rollout_provider)
  File "/data/miniconda3_for_docker/envs/rl_issude/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py", line 346, in _env_runner
    active_envs, to_eval, outputs = _process_observations(
  File "/data/miniconda3_for_docker/envs/rl_issude/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py", line 499, in _process_observations
    outputs.append(episode.batch_builder.build_and_reset(episode))
  File "/data/miniconda3_for_docker/envs/rl_issude/lib/python3.8/site-packages/ray/rllib/evaluation/sample_batch_builder.py", line 205, in build_and_reset
    self.postprocess_batch_so_far(episode)
  File "/data/miniconda3_for_docker/envs/rl_issude/lib/python3.8/site-packages/ray/rllib/evaluation/sample_batch_builder.py", line 152, in postprocess_batch_so_far
    post_batches[agent_id] = policy.postprocess_trajectory(
  File "/data/miniconda3_for_docker/envs/rl_issude/lib/python3.8/site-packages/ray/rllib/policy/torch_policy_template.py", line 156, in postprocess_trajectory
    return postprocess_fn(self, sample_batch,
  File "/data/miniconda3_for_docker/envs/rl_issude/lib/python3.8/site-packages/ray/rllib/agents/dqn/dqn_tf_policy.py", line 377, in postprocess_nstep_and_prio
    np.abs(td_errors) + policy.config["prioritized_replay_eps"])
  File "/data/miniconda3_for_docker/envs/rl_issude/lib/python3.8/site-packages/torch/tensor.py", line 492, in __array__
    return self.numpy()
TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='PgLoLo' date='2020-05-31T19:03:40Z'>
		I think we just forgot to call .cpu() first. Unfortunately I'm not sure how to test this is always correct since pytorch has no way to mock GPUs.
		</comment>
		<comment id='3' author='PgLoLo' date='2020-07-15T15:52:47Z'>
		Working on this now ...
		</comment>
		<comment id='4' author='PgLoLo' date='2020-07-15T15:55:14Z'>
		This PR will have a fix for this problem. For now, just change line ~380 in dqn_tf_policy.py to
&lt;denchmark-code&gt;        new_priorities = (
            np.abs(td_errors.cpu()) + policy.config["prioritized_replay_eps"])
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='PgLoLo' date='2020-07-15T15:55:30Z'>
		&lt;denchmark-link:https://github.com/ray-project/ray/pull/9497&gt;#9497&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='PgLoLo' date='2020-07-15T15:55:40Z'>
		&lt;denchmark-link:https://github.com/PgLoLo&gt;@PgLoLo&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='PgLoLo' date='2020-07-15T16:09:12Z'>
		This will be merged later today. ...
Closing this now. Feel free to re-open should the above fix not work on your end.
Thanks for filing this!
		</comment>
	</comments>
</bug>