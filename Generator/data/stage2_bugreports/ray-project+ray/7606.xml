<bug id='7606' author='guyk1971' open_date='2020-03-15T05:59:25Z' closed_time='2020-03-16T18:19:59Z'>
	<summary>[rllib] offline dataset example doesnt work ("NaN of Inf found in input tensor")</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

Trying to run the 2 command lines in the offline dataset example :
first to create the offline dataset (which completes successfully):
rllib train  --run=PG --env=CartPole-v0 --config='{"output": "/tmp/cartpole-out",\ "output_max_file_size": 5000000}' --stop='{"timesteps_total": 100000}'
and then :
rllib train --run=DQN --env=CartPole-v0 --config='{"input": "/tmp/cartpole-out",\ "input_evaluation": [],"explore": false}'
and this command runs but doesnt train because of a warning:
"WARNING:root:NaN or Inf found in input tensor."
Ray version and other system information (Python version, TensorFlow version, OS):
Ray version : 0.9.0.dev0 (the latest wheel version)
python: 3.7.6
tensorflow 2.0.0
Ubuntu 18.04
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
no script. simply follow your example on offline dataset. as written above.
I also tried to change the algorithm that generates the data to ppo and dqn. all behave the same.
If we cannot run your script, we cannot fix your issue.

[v ] I have verified my script runs in a clean environment and reproduces the issue.
[v ] I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='guyk1971' date='2020-03-15T22:17:58Z'>
		The warning seems to be a red herring, it seems to be working when I tried it (in offline mode reward is always a NaN unless you have "simulation" included in "input_evaluation". You can try &lt;denchmark-link:https://github.com/ray-project/ray/pull/7611/files&gt;https://github.com/ray-project/ray/pull/7611/files&lt;/denchmark-link&gt;
 which suppresses the warning.
		</comment>
		<comment id='2' author='guyk1971' date='2020-03-16T05:00:41Z'>
		Eric, thanks a lot for the prompt and effective response ! indeed, my bad for not reading the documentation carefully (maybe worth a note in the offline dataset documentation section about the exepected warning if not feeding 'simulation' mode) .
BTW, I'm exploring rllib after trying other libraries (coach, tf-agents, baselines...) and it looks like great framework so far. keep up the great work ! (In the near term I'll be looking into batch rl and will try to implement the recent algorithms that was found effective for it like BCQ/ DBCQ). Thanks a lot for the help !!
what's next ? should I close this issue ?
		</comment>
		<comment id='3' author='guyk1971' date='2020-03-16T05:22:04Z'>
		Yeah good point! I'll make sure to update that. Feel free to file any other issues that come up.
		</comment>
		<comment id='4' author='guyk1971' date='2020-06-25T17:03:35Z'>
		Â´Hi &lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
! Why is the reward always NaN in an offline mode? Can you explain, please? And is it  normal the episodes_this_iter and episodes_total be always equals to zero?
		</comment>
	</comments>
</bug>