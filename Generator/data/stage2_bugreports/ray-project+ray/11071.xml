<bug id='11071' author='lasagnaphil' open_date='2020-09-28T12:10:20Z' closed_time='2020-10-06T05:07:59Z'>
	<summary>[rllib] Torch checkpoint taken on GPU fails to deserialize on CPU</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

While trying to load a checkpoint in RLlib using Torch backend and GPU, I immediately get the following error:
&lt;denchmark-code&gt;2020-09-28 20:49:56,066 ERROR worker.py:1032 -- Possible unhandled error from worker: ray::RolloutWorker.restore() (pid=2205302, ip=147.46.240.48)
  File "python/ray/_raylet.pyx", line 484, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 438, in ray._raylet.execute_task.function_executor
  File "/home/lasagnaphil/envs/MASS/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py", line 915, in restore
    objs = pickle.loads(objs)
  File "/home/lasagnaphil/envs/MASS/lib/python3.8/site-packages/torch/storage.py", line 142, in _load_from_bytes
    return torch.load(io.BytesIO(b))
  File "/home/lasagnaphil/envs/MASS/lib/python3.8/site-packages/torch/serialization.py", line 585, in load
    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
  File "/home/lasagnaphil/envs/MASS/lib/python3.8/site-packages/torch/serialization.py", line 765, in _legacy_load
    result = unpickler.load()
  File "/home/lasagnaphil/envs/MASS/lib/python3.8/site-packages/torch/serialization.py", line 721, in persistent_load
    deserialized_objects[root_key] = restore_location(obj, location)
  File "/home/lasagnaphil/envs/MASS/lib/python3.8/site-packages/torch/serialization.py", line 174, in default_restore_location
    result = fn(storage, location)
  File "/home/lasagnaphil/envs/MASS/lib/python3.8/site-packages/torch/serialization.py", line 150, in _cuda_deserialize
    device = validate_cuda_device(location)
  File "/home/lasagnaphil/envs/MASS/lib/python3.8/site-packages/torch/serialization.py", line 134, in validate_cuda_device
    raise RuntimeError('Attempting to deserialize object on a CUDA '
RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.
&lt;/denchmark-code&gt;

Although the training continues to run despite the error message, the training seems to start from scratch and not from where I left off, which seems to me that the model is not loaded properly.
&lt;denchmark-link:https://user-images.githubusercontent.com/11910667/94430234-87e8be00-01ce-11eb-8c17-c285d6cb2755.png&gt;&lt;/denchmark-link&gt;

This happens only when using Torch and GPU for the training (num_gpus=1). But checkpoints work fine when only using CPU (num_gpus=0). Are there any potential workarounds or bugfixes for this problem?
Ray version and other system information (Python version, TensorFlow version, OS):
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
ray_test.py:
import ray
from ray import tune
from ray.rllib.agents.ppo import PPOTrainer

import argparse
import os
from pathlib import Path

parser = argparse.ArgumentParser()
parser.add_argument("--checkpoint", type=str)

args = parser.parse_args()

ray.init(num_cpus=32, num_gpus=1)

# Using config from tuned_examples folder
config = {
    "framework": "torch",
    "env": "CartPole-v0",
    "num_gpus": 1,
    "num_workers": 1,

    "gamma": 0.99,
    "lr": 0.0003,
    "observation_filter": "MeanStdFilter",
    "num_sgd_iter": 6,
    "vf_share_layers": True,
    "vf_loss_coeff": 0.01,
    "model": {"fcnet_hiddens": [32], "fcnet_activation": "linear"}
}

if args.checkpoint:
    tune.run(PPOTrainer, config=config, 
        checkpoint_freq=10, 
        local_dir=os.environ["PWD"] + "/ray_results",
        restore=args.checkpoint)
else:
    tune.run(PPOTrainer, config=config,
        checkpoint_freq=10,
        local_dir=os.environ["PWD"] + "/ray_results")
Starting the training: python ray_test.py
Loading from a checkpoint (for example): python ray_test.py --checkpoint ray_results/PPO/PPO_CartPole-v0_77d6c_00000_0_2020-09-28_20-48-04/checkpoint_20/checkpoint-20
If we cannot run your script, we cannot fix your issue.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='lasagnaphil' date='2020-09-28T18:08:08Z'>
		&lt;denchmark-link:https://github.com/sven1977&gt;@sven1977&lt;/denchmark-link&gt;
 I think this is since we serializing tensors instead of numpy arrays for torch--- can you look into it? I remember there was another issue reported around too but can't find it.
Fix is just to always call .numpy() when-ever saving/checkpointing a model state.
		</comment>
		<comment id='2' author='lasagnaphil' date='2020-10-05T04:48:51Z'>
		I examined the code for the TorchPolicy class, and the culprit was that the optimizer states weren't serializing properly (The optimizer weights needed calls for  and  just as the model weights did.) I fixed this and created a pull request which solves the problem (at least for my desktop GPU setup): &lt;denchmark-link:https://github.com/ray-project/ray/pull/11208&gt;#11208&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='lasagnaphil' date='2020-10-06T05:07:59Z'>
		Closing this issue since the PR is merged. Thanks!
		</comment>
	</comments>
</bug>