<bug id='12206' author='krfricke' open_date='2020-11-20T13:37:51Z' closed_time='2020-12-08T22:48:59Z'>
	<summary>[core/autoscaler] ray status SIGSEGV: No address out of total 1 resolved</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

When trying to run a large scale cluster (200 nodes in this example), ray status sometimes throws SIGSEGVs. Additionally, the nodes do not connect to the head node.
Output:
&lt;denchmark-code&gt;(pid=raylet, ip=172.31.20.194) *** Aborted at 1605879229 (unix time) try "date -d @1605879229" if you are using GNU date ***
(pid=raylet, ip=172.31.20.194) PC: @                0x0 (unknown)
(pid=raylet, ip=172.31.20.194) *** SIGSEGV (@0x58) received by PID 1630 (TID 0x7f88ddfc37c0) from PID 88; stack trace: ***
(pid=raylet, ip=172.31.20.194)     @     0x7f88de1f53c0 (unknown)
(pid=raylet, ip=172.31.20.194)     @     0x55aee6ded0e2 grpc::ServerInterface::RegisteredAsyncRequest::IssueRequest()
(pid=raylet, ip=172.31.20.194)     @     0x55aee6a78429 ray::rpc::ObjectManagerService::WithAsyncMethod_Push&lt;&gt;::RequestPush()
(pid=raylet, ip=172.31.20.194)     @     0x55aee6a87a1b ray::rpc::ServerCallFactoryImpl&lt;&gt;::CreateCall()
(pid=raylet, ip=172.31.20.194)     @     0x55aee6d0be19 ray::rpc::GrpcServer::Run()
(pid=raylet, ip=172.31.20.194)     @     0x55aee6a7c59e ray::ObjectManager::StartRpcService()
(pid=raylet, ip=172.31.20.194)     @     0x55aee6a8f2bc ray::ObjectManager::ObjectManager()
(pid=raylet, ip=172.31.20.194)     @     0x55aee69dee20 ray::raylet::Raylet::Raylet()
(pid=raylet, ip=172.31.20.194)     @     0x55aee69b596b _ZZ4mainENKUlN3ray6StatusEN5boost8optionalISt13unordered_mapISsSsSt4hashISsESt8equal_toISsESaISt4pairIKSsSsEEEEEE_clES0_SD_
(pid=raylet, ip=172.31.20.194)     @     0x55aee69b6af1 _ZNSt17_Function_handlerIFvN3ray6StatusERKN5boost8optionalISt13unordered_mapISsSsSt4hashISsESt8equal_toISsESaISt4pairIKSsSsEEEEEEZ4mainEUlS1_SE_E_E9_M_invokeERKSt9_Any_dataS1_SG_
(pid=raylet, ip=172.31.20.194)     @     0x55aee6b37a2c _ZZN3ray3gcs28ServiceBasedNodeInfoAccessor22AsyncGetInternalConfigERKSt8functionIFvNS_6StatusERKN5boost8optionalISt13unordered_mapISsSsSt4hashISsESt8equal_toISsESaISt4pairIKSsSsEEEEEEEENKUlRKS3_RKNS_3rpc22GetInternalConfigReplyEE_clESO_SS_
(pid=raylet, ip=172.31.20.194)     @     0x55aee6aeb33f _ZNSt17_Function_handlerIFvRKN3ray6StatusERKNS0_3rpc22GetInternalConfigReplyEEZNS4_12GcsRpcClient17GetInternalConfigERKNS4_24GetInternalConfigRequestERKSt8functionIS8_EEUlS3_S7_E_E9_M_invokeERKSt9_Any_dataS3_S7_
(pid=raylet, ip=172.31.20.194)     @     0x55aee6aeb43d ray::rpc::ClientCallImpl&lt;&gt;::OnReplyReceived()
(pid=raylet, ip=172.31.20.194)     @     0x55aee6a18800 _ZN5boost4asio6detail18completion_handlerIZN3ray3rpc17ClientCallManager29PollEventsFromCompletionQueueEiEUlvE_E11do_completeEPvPNS1_19scheduler_operationERKNS_6system10error_codeEm
(pid=raylet, ip=172.31.20.194)     @     0x55aee708070f boost::asio::detail::scheduler::do_run_one()
&lt;/denchmark-code&gt;

This is just the output for one node. I have the same output for the other 201 nodes...
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Cluster config: &lt;denchmark-link:https://github.com/ray-project/xgboost_ray/blob/master/xgboost_ray/tests/release/cpu_large_scale.yaml&gt;https://github.com/ray-project/xgboost_ray/blob/master/xgboost_ray/tests/release/cpu_large_scale.yaml&lt;/denchmark-link&gt;


 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='krfricke' date='2020-11-20T14:57:21Z'>
		Update: I get this sometimes after starting a benchmark script. It might be a general issue that comes up with many nodes:
&lt;denchmark-code&gt;(base) root@ip-172-31-30-231:/release_tests# python benchmark_cpu_gpu.py 200 10 8000
2020-11-20 06:55:00,056 INFO worker.py:651 -- Connecting to existing Ray cluster at address: 172.31.30.231:6379                                                                                                                                                                    2020-11-20 06:55:01,352 WARNING worker.py:1091 -- The actor or task with ID ffffffffffffffffbbb9cbf608000000 is pending and cannot currently be scheduled. It requires {} for execution and {CPU: 1.000000} for placement, but this node only has remaining {actor_cpus: 4.000000},
 {node:172.31.26.235: 1.000000}, {object_store_memory: 3.076172 GiB}, {CPU: 4.000000}, {memory: 10.449219 GiB}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
2020-11-20 06:55:01,517 WARNING worker.py:1091 -- The actor or task with ID ffffffffffffffffc23e60dd08000000 is pending and cannot currently be scheduled. It requires {} for execution and {CPU: 1.000000} for placement, but this node only has remaining {actor_cpus: 4.000000}, {node:172.31.18.252: 1.000000}, {CPU: 4.000000}, {memory: 10.351562 GiB}, {object_store_memory: 3.027344 GiB}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, co
nsider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
(pid=raylet, ip=172.31.19.101) E1120 06:55:02.190124912    1060 server_chttp2.cc:40]        {"created":"@1605884102.190040718","description":"No address added out of total 1 resolved","file":"external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc
","file_line":394,"referenced_errors":[{"created":"@1605884102.190038437","description":"Failed to add any wildcard listeners","file":"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_posix.cc","file_line":341,"referenced_errors":[{"created":"@1605884102.189969464
","description":"Unable to configure socket","fd":32,"file":"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc","file_line":208,"referenced_errors":[{"created":"@1605884102.189960718","description":"Address already in use","errno":98,"file":"e
xternal/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc","file_line":181,"os_error":"Address already in use","syscall":"bind"}]},{"created":"@1605884102.190037585","description":"Unable to configure socket","fd":32,"file":"external/com_github_grpc_gr
pc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc","file_line":208,"referenced_errors":[{"created":"@1605884102.190033903","description":"Address already in use","errno":98,"file":"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc","file_
line":181,"os_error":"Address already in use","syscall":"bind"}]}]}]}
(pid=raylet, ip=172.31.19.101) *** Aborted at 1605884102 (unix time) try "date -d @1605884102" if you are using GNU date ***
(pid=raylet, ip=172.31.19.101) PC: @                0x0 (unknown)
(pid=raylet, ip=172.31.19.101) *** SIGSEGV (@0x58) received by PID 1060 (TID 0x7f1f3f4587c0) from PID 88; stack trace: ***
(pid=raylet, ip=172.31.19.101)     @     0x7f1f3f68a3c0 (unknown)
(pid=raylet, ip=172.31.19.101)     @     0x563a035cf0e2 grpc::ServerInterface::RegisteredAsyncRequest::IssueRequest()
(pid=raylet, ip=172.31.19.101)     @     0x563a0325a429 ray::rpc::ObjectManagerService::WithAsyncMethod_Push&lt;&gt;::RequestPush()
(pid=raylet, ip=172.31.19.101)     @     0x563a03269a1b ray::rpc::ServerCallFactoryImpl&lt;&gt;::CreateCall()
(pid=raylet, ip=172.31.19.101)     @     0x563a034ede19 ray::rpc::GrpcServer::Run()
(pid=raylet, ip=172.31.19.101)     @     0x563a0325e59e ray::ObjectManager::StartRpcService()
(pid=raylet, ip=172.31.19.101)     @     0x563a032712bc ray::ObjectManager::ObjectManager()
(pid=raylet, ip=172.31.19.101)     @     0x563a031c0e20 ray::raylet::Raylet::Raylet()             
(pid=raylet, ip=172.31.19.101)     @     0x563a0319796b _ZZ4mainENKUlN3ray6StatusEN5boost8optionalISt13unordered_mapISsSsSt4hashISsESt8equal_toISsESaISt4pairIKSsSsEEEEEE_clES0_SD_
(pid=raylet, ip=172.31.19.101)     @     0x563a03198af1 _ZNSt17_Function_handlerIFvN3ray6StatusERKN5boost8optionalISt13unordered_mapISsSsSt4hashISsESt8equal_toISsESaISt4pairIKSsSsEEEEEEZ4mainEUlS1_SE_E_E9_M_invokeERKSt9_Any_dataS1_SG_
(pid=raylet, ip=172.31.19.101)     @     0x563a03319a2c _ZZN3ray3gcs28ServiceBasedNodeInfoAccessor22AsyncGetInternalConfigERKSt8functionIFvNS_6StatusERKN5boost8optionalISt13unordered_mapISsSsSt4hashISsESt8equal_toISsESaISt4pairIKSsSsEEEEEEEENKUlRKS3_RKNS_3rpc22GetInternalCon
figReplyEE_clESO_SS_                                                                                                                     
(pid=raylet, ip=172.31.19.101)     @     0x563a032cd33f _ZNSt17_Function_handlerIFvRKN3ray6StatusERKNS0_3rpc22GetInternalConfigReplyEEZNS4_12GcsRpcClient17GetInternalConfigERKNS4_24GetInternalConfigRequestERKSt8functionIS8_EEUlS3_S7_E_E9_M_invokeERKSt9_Any_dataS3_S7_
(pid=raylet, ip=172.31.19.101)     @     0x563a032cd43d ray::rpc::ClientCallImpl&lt;&gt;::OnReplyReceived()
(pid=raylet, ip=172.31.19.101)     @     0x563a031fa800 _ZN5boost4asio6detail18completion_handlerIZN3ray3rpc17ClientCallManager29PollEventsFromCompletionQueueEiEUlvE_E11do_completeEPvPNS1_19scheduler_operationERKNS_6system10error_codeEm
(pid=raylet, ip=172.31.19.101)     @     0x563a0386270f boost::asio::detail::scheduler::do_run_one()
(pid=raylet, ip=172.31.19.101)     @     0x563a03863c11 boost::asio::detail::scheduler::run()                
(pid=raylet, ip=172.31.19.101)     @     0x563a03864c42 boost::asio::io_context::run()
(pid=raylet, ip=172.31.19.101)     @     0x563a03177cbc main                                                                             
(pid=raylet, ip=172.31.19.101)     @     0x7f1f3f4840b3 __libc_start_main
(pid=raylet, ip=172.31.19.101)     @     0x563a0318a621 (unknown)                                                                        
2020-11-20 06:55:03,190 WARNING worker.py:1091 -- The actor or task with ID ffffffffffffffff84eb106208000000 is pending and cannot currently be scheduled. It requires {} for execution and {CPU: 1.000000} for placement, but this node only has remaining {actor_cpus: 4.000000},
 {node:172.31.29.90: 1.000000}, {CPU: 4.000000}, {memory: 10.351562 GiB}, {object_store_memory: 3.027344 GiB}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, con
sider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
2020-11-20 06:55:03,211 WARNING worker.py:1091 -- The actor or task with ID ffffffffffffffff11b9d6bb08000000 is pending and cannot currently be scheduled. It requires {actor_cpus: 4.000000}, {CPU: 4.000000} for execution and {actor_cpus: 4.000000}, {CPU: 4.000000} for placem
ent, but this node only has remaining {actor_cpus: 4.000000}, {node:172.31.18.42: 1.000000}, {CPU: 4.000000}, {memory: 10.351562 GiB}, {object_store_memory: 3.027344 GiB}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster
 resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
2020-11-20 06:55:03,528 WARNING worker.py:1091 -- The actor or task with ID ffffffffffffffff3904136408000000 is pending and cannot currently be scheduled. It requires {} for execution and {CPU: 1.000000} for placement, but this node only has remaining {actor_cpus: 4.000000},
 {node:172.31.21.58: 1.000000}, {CPU: 4.000000}, {memory: 10.351562 GiB}, {object_store_memory: 3.027344 GiB}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, con
sider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='krfricke' date='2020-11-20T14:59:25Z'>
		Sorry, I can't do much more than pasting stacktraces here... this came up when I started the script another time:
&lt;denchmark-code&gt;(pid=raylet, ip=172.31.22.42) E1120 06:57:42.976581780    1386 server_chttp2.cc:40]        {"created":"@1605884262.976524749","description":"No address added out of total 1 resolved","file":"external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc"
,"file_line":394,"referenced_errors":[{"created":"@1605884262.976523327","description":"Failed to add any wildcard listeners","file":"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_posix.cc","file_line":341,"referenced_errors":[{"created":"@1605884262.976506765"
,"description":"Unable to configure socket","fd":32,"file":"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc","file_line":208,"referenced_errors":[{"created":"@1605884262.976499568","description":"Address already in use","errno":98,"file":"ex
ternal/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc","file_line":181,"os_error":"Address already in use","syscall":"bind"}]},{"created":"@1605884262.976523014","description":"Unable to configure socket","fd":32,"file":"external/com_github_grpc_grp
c/src/core/lib/iomgr/tcp_server_utils_posix_common.cc","file_line":208,"referenced_errors":[{"created":"@1605884262.976520794","description":"Address already in use","errno":98,"file":"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc","file_l
ine":181,"os_error":"Address already in use","syscall":"bind"}]}]}]}                                                                                                                                                                                                               
(pid=raylet, ip=172.31.22.42) *** Aborted at 1605884262 (unix time) try "date -d @1605884262" if you are using GNU date ***                                                                                                                                                        
(pid=raylet, ip=172.31.22.42) PC: @                0x0 (unknown)                                                                                                                                                                                                                   
(pid=raylet, ip=172.31.22.42) *** SIGSEGV (@0x58) received by PID 1386 (TID 0x7f4b1705f7c0) from PID 88; stack trace: ***                                                                                                                                                          
(pid=raylet, ip=172.31.22.42)     @     0x7f4b172913c0 (unknown)                                                                                                                                                                                                                   
(pid=raylet, ip=172.31.22.42)     @     0x559f55a870e2 grpc::ServerInterface::RegisteredAsyncRequest::IssueRequest()                                                                                                                                                               
(pid=raylet, ip=172.31.22.42)     @     0x559f55712429 ray::rpc::ObjectManagerService::WithAsyncMethod_Push&lt;&gt;::RequestPush()                                                                                                                                                       
(pid=raylet, ip=172.31.22.42)     @     0x559f55721a1b ray::rpc::ServerCallFactoryImpl&lt;&gt;::CreateCall()                                                                                                                                                                             
(pid=raylet, ip=172.31.22.42)     @     0x559f559a5e19 ray::rpc::GrpcServer::Run()                                                                                                                                                                                                 
(pid=raylet, ip=172.31.22.42)     @     0x559f5571659e ray::ObjectManager::StartRpcService()                                                                                                                                                                                       
(pid=raylet, ip=172.31.22.42)     @     0x559f557292bc ray::ObjectManager::ObjectManager()
(pid=raylet, ip=172.31.22.42)     @     0x559f55678e20 ray::raylet::Raylet::Raylet()                                        
(pid=raylet, ip=172.31.22.42)     @     0x559f5564f96b _ZZ4mainENKUlN3ray6StatusEN5boost8optionalISt13unordered_mapISsSsSt4hashISsESt8equal_toISsESaISt4pairIKSsSsEEEEEE_clES0_SD_
(pid=raylet, ip=172.31.22.42)     @     0x559f55650af1 _ZNSt17_Function_handlerIFvN3ray6StatusERKN5boost8optionalISt13unordered_mapISsSsSt4hashISsESt8equal_toISsESaISt4pairIKSsSsEEEEEEZ4mainEUlS1_SE_E_E9_M_invokeERKSt9_Any_dataS1_SG_
(pid=raylet, ip=172.31.22.42)     @     0x559f557d1a2c _ZZN3ray3gcs28ServiceBasedNodeInfoAccessor22AsyncGetInternalConfigERKSt8functionIFvNS_6StatusERKN5boost8optionalISt13unordered_mapISsSsSt4hashISsESt8equal_toISsESaISt4pairIKSsSsEEEEEEEENKUlRKS3_RKNS_3rpc22GetInternalConf
igReplyEE_clESO_SS_                                                                                                                      
(pid=raylet, ip=172.31.22.42)     @     0x559f5578533f _ZNSt17_Function_handlerIFvRKN3ray6StatusERKNS0_3rpc22GetInternalConfigReplyEEZNS4_12GcsRpcClient17GetInternalConfigERKNS4_24GetInternalConfigRequestERKSt8functionIS8_EEUlS3_S7_E_E9_M_invokeERKSt9_Any_dataS3_S7_
(pid=raylet, ip=172.31.22.42)     @     0x559f5578543d ray::rpc::ClientCallImpl&lt;&gt;::OnReplyReceived()   
(pid=raylet, ip=172.31.22.42)     @     0x559f556b2800 _ZN5boost4asio6detail18completion_handlerIZN3ray3rpc17ClientCallManager29PollEventsFromCompletionQueueEiEUlvE_E11do_completeEPvPNS1_19scheduler_operationERKNS_6system10error_codeEm
(pid=raylet, ip=172.31.22.42)     @     0x559f55d1a70f boost::asio::detail::scheduler::do_run_one()
(pid=raylet, ip=172.31.22.42)     @     0x559f55d1bc11 boost::asio::detail::scheduler::run()
(pid=raylet, ip=172.31.22.42)     @     0x559f55d1cc42 boost::asio::io_context::run()
(pid=raylet, ip=172.31.22.42)     @     0x559f5562fcbc main                                                                                                                                                                                                                        
(pid=raylet, ip=172.31.22.42)     @     0x7f4b1708b0b3 __libc_start_main                                                                                                                                                                                                           
(pid=raylet, ip=172.31.22.42)     @     0x559f55642621 (unknown)                                                                                                                                                                                                                   
(pid=raylet, ip=172.31.19.220)     @     0x561208566c11 boost::asio::detail::scheduler::run()
(pid=raylet, ip=172.31.19.220)     @     0x561208567c42 boost::asio::io_context::run()                                                                                                                                                                                             
(pid=raylet, ip=172.31.19.220)     @     0x561207e7acbc main                                                                             
(pid=raylet, ip=172.31.19.220)     @     0x7f43dda630b3 __libc_start_main                                                                                                                                                                                                          
(pid=raylet, ip=172.31.19.220)     @     0x561207e8d621 (unknown)
(pid=raylet, ip=172.31.30.130)     @     0x55d6e486970f boost::asio::detail::scheduler::do_run_one()
(pid=raylet, ip=172.31.30.130)     @     0x55d6e486ac11 boost::asio::detail::scheduler::run()
(pid=raylet, ip=172.31.30.130)     @     0x55d6e486bc42 boost::asio::io_context::run()
(pid=raylet, ip=172.31.30.130)     @     0x55d6e417ecbc main
(pid=raylet, ip=172.31.30.130)     @     0x7f820e6fa0b3 __libc_start_main
(pid=raylet, ip=172.31.30.130)     @     0x55d6e4191621 (unknown)
2020-11-20 06:57:43,046 WARNING worker.py:1091 -- Traceback (most recent call last):
  File "python/ray/_raylet.pyx", line 551, in ray._raylet.task_execution_handler
  File "python/ray/_raylet.pyx", line 380, in ray._raylet.execute_task
  File "/root/anaconda3/lib/python3.7/site-packages/ray/function_manager.py", line 245, in get_execution_info
    self._wait_for_function(function_descriptor, job_id)
  File "/root/anaconda3/lib/python3.7/site-packages/ray/function_manager.py", line 303, in _wait_for_function
    and (function_descriptor.function_id in
AttributeError: 'ray._raylet.EmptyFunctionDescriptor' object has no attribute 'function_id'
An unexpected internal error occurred while the worker was executing a task.
2020-11-20 06:57:43,046 WARNING worker.py:1091 -- A worker died or was killed while executing task ffffffffffffffff4f6414a50a000000.
(pid=37836) 2020-11-20 06:57:43,041     ERROR worker.py:384 -- SystemExit was raised from the worker
(pid=37836) Traceback (most recent call last):
(pid=37836)   File "python/ray/_raylet.pyx", line 551, in ray._raylet.task_execution_handler
(pid=37836)   File "python/ray/_raylet.pyx", line 380, in ray._raylet.execute_task
(pid=37836)   File "/root/anaconda3/lib/python3.7/site-packages/ray/function_manager.py", line 245, in get_execution_info
(pid=37836)     self._wait_for_function(function_descriptor, job_id)
(pid=37836)   File "/root/anaconda3/lib/python3.7/site-packages/ray/function_manager.py", line 303, in _wait_for_function
(pid=37836)     and (function_descriptor.function_id in
(pid=37836) AttributeError: 'ray._raylet.EmptyFunctionDescriptor' object has no attribute 'function_id'
(pid=37836) 
(pid=37836) During handling of the above exception, another exception occurred:
(pid=37836) 
(pid=37836) Traceback (most recent call last):
(pid=37836)   File "python/ray/_raylet.pyx", line 563, in ray._raylet.task_execution_handler
(pid=37836) SystemExit: 1
(pid=raylet, ip=172.31.21.235) E1120 06:54:35.696456   712   712 process.cc:498] Failed to kill process 884 with error system:3: No such process
(pid=raylet, ip=172.31.21.235) E1120 06:54:35.699002   712   712 process.cc:498] Failed to kill process 884 with error system:3: No such process
(pid=raylet, ip=172.31.21.235) E1120 06:54:51.408203   712   712 process.cc:498] Failed to kill process 974 with error system:3: No such process
(pid=raylet, ip=172.31.21.235) E1120 06:55:56.010509  1112  1112 process.cc:498] Failed to kill process 1143 with error system:3: No such process
(pid=raylet, ip=172.31.21.235) E1120 06:55:56.013386  1112  1112 process.cc:498] Failed to kill process 1143 with error system:3: No such process
(pid=raylet, ip=172.31.21.235) E1120 06:57:43.102960286    1311 server_chttp2.cc:40]        {"created":"@1605884263.102863563","description":"No address added out of total 1 resolved","file":"external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc
","file_line":394,"referenced_errors":[{"created":"@1605884263.102861168","description":"Failed to add any wildcard listeners","file":"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_posix.cc","file_line":341,"referenced_errors":[{"created":"@1605884263.102831597
","description":"Unable to configure socket","fd":32,"file":"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc","file_line":208,"referenced_errors":[{"created":"@1605884263.102821826","description":"Address already in use","errno":98,"file":"e
xternal/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc","file_line":181,"os_error":"Address already in use","syscall":"bind"}]},{"created":"@1605884263.102860272","description":"Unable to configure socket","fd":32,"file":"external/com_github_grpc_gr
pc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc","file_line":208,"referenced_errors":[{"created":"@1605884263.102856851","description":"Address already in use","errno":98,"file":"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc","file_
line":181,"os_error":"Address already in use","syscall":"bind"}]}]}]}
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='krfricke' date='2020-11-20T18:20:02Z'>
		I'll separate this into a couple different issues.
		</comment>
		<comment id='4' author='krfricke' date='2020-11-20T18:40:45Z'>
		Do you have a lot of workers at each node? Also, did you set min-worker-port and max-worker-port?
		</comment>
		<comment id='5' author='krfricke' date='2020-11-20T22:22:52Z'>
		No, 4 workers per node. 800 total workers.
		</comment>
		<comment id='6' author='krfricke' date='2020-11-21T02:45:58Z'>
		If this happens on all nodes it is highly likely you configure --node-manager-port on the port that is already bound. Can you check if that's the case?
		</comment>
		<comment id='7' author='krfricke' date='2020-11-23T09:56:46Z'>
		Do you mean --object-manager-port? If that's the case then ray down did not clean up all resources. I'll check this next time I see the error (should be soon)
		</comment>
		<comment id='8' author='krfricke' date='2020-12-08T22:48:59Z'>
		Duplicates &lt;denchmark-link:https://github.com/ray-project/ray/issues/12213&gt;#12213&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>