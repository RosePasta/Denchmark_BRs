<bug id='8660' author='icaropires' open_date='2020-05-28T15:25:42Z' closed_time='2020-08-13T21:29:48Z'>
	<summary>[autoscaler] Local Cluster with Docker: Check failed: !local_node_id_.IsNil() This node is disconnected.</summary>
	<description>
&lt;denchmark-h:h3&gt;What is your question?&lt;/denchmark-h&gt;

I've been trying to run a cluster using two hosts with docker. But after running ray up -y cluster.yml, I get the following situation after some minutes:
&lt;denchmark-link:https://user-images.githubusercontent.com/18473249/83157458-feb76280-a0d9-11ea-842b-2d34d1ace91a.png&gt;&lt;/denchmark-link&gt;

Workers keeps incresing by 16 (number of cores) on the worker node, and the same error popping up on the logs.
View all logs:
&lt;denchmark-code&gt;F0528 14:53:04.237076   138 service_based_accessor.cc:320]  Check failed: !local_node_id_.IsNil() This node is disconnected.
*** Check failure stack trace: ***
    @     0x55bb8f0f676d  google::LogMessage::Fail()
    @     0x55bb8f0f7bdc  google::LogMessage::SendToLog()
    @     0x55bb8f0f6449  google::LogMessage::Flush()
    @     0x55bb8f0f6661  google::LogMessage::~LogMessage()
    @     0x55bb8edb7029  ray::RayLog::~RayLog()
    @     0x55bb8ec285fc  ray::gcs::ServiceBasedNodeInfoAccessor::UnregisterSelf()
    @     0x55bb8eb37bd4  ray::raylet::Raylet::Stop()
    @     0x55bb8eb1c943  _ZZ4mainENKUlRKN5boost6system10error_codeEiE_clES3_i.isra.2014
    @     0x55bb8eb1cd58  _ZN5boost4asio6detail14signal_handlerIZ4mainEUlRKNS_6system10error_codeEiE_NS1_18io_object_executorINS0_8executorEEEE11do_completeEPvPNS1_19scheduler_operationES6_m
    @     0x55bb8f08841f  boost::asio::detail::scheduler::do_run_one()
    @     0x55bb8f089921  boost::asio::detail::scheduler::run()
    @     0x55bb8f08a7c2  boost::asio::io_context::run()
    @     0x55bb8eb06669  main
    @     0x7fa7e5fd909b  __libc_start_main
    @     0x55bb8eb17331  (unknown)
&lt;/denchmark-code&gt;

Relevant messages on logs/monitor.*
&lt;denchmark-code&gt;2020-05-28 14:52:26,124 WARNING autoscaler.py:536 -- StandardAutoscaler: host02: No heartbeat in 30.2060968875885s, restarting Ray to recover...      
2020-05-28 14:53:01,399 WARNING autoscaler.py:536 -- StandardAutoscaler: host02: No heartbeat in 30.24538493156433s, restarting Ray to recover...                              
2020-05-28 14:53:36,620 WARNING autoscaler.py:536 -- StandardAutoscaler: host02: No heartbeat in 30.10585641860962s, restarting Ray to recover...                              
2020-05-28 14:54:11,807 WARNING autoscaler.py:536 -- StandardAutoscaler: host02: No heartbeat in 30.12926173210144s, restarting Ray to recover...                              
2020-05-28 14:54:46,926 WARNING autoscaler.py:536 -- StandardAutoscaler: host02: No heartbeat in 30.117576360702515s, restarting Ray to recover...                             
2020-05-28 14:55:22,223 WARNING autoscaler.py:536 -- StandardAutoscaler: host02: No heartbeat in 30.251209259033203s, restarting Ray to recover...                             
2020-05-28 14:55:57,461 WARNING autoscaler.py:536 -- StandardAutoscaler: host02: No heartbeat in 30.201629877090454s, restarting Ray to recover...           
&lt;/denchmark-code&gt;

cluster.yml:
&lt;denchmark-code&gt;cluster_name: test                                                                               
                  
min_workers: 2         
initial_workers: 2      
max_workers: 2        
                         
docker:                  
  image: "python:3.8.3"             
  container_name: "ray"             
  run_options: ["--shm-size 25GB"]    
                                    
provider:                     
    type: local               
    head_ip: host01           
    worker_ips: [host02]           
                                   
auth:                              
  ssh_user: root                                                                                                                                              
  ssh_private_key: ~/.ssh/id_rsa                                                                                                                              
                                                                                                                                                              
head_start_ray_commands:                                                                                                                                      
 - ray stop                                                                                                                                                     
 - ulimit -c unlimited &amp;&amp; ray start --head --redis-port=6379 --redis-password='' --webui-host=0.0.0.0 --autoscaling-config=~/ray_bootstrap_config.yaml        
                                                                    
worker_start_ray_commands:                                            
 - ray stop                                                         
 - ray start --address=$RAY_HEAD_IP:6379 --redis-password=''
&lt;/denchmark-code&gt;

I wasn't able to detect any blocked ports by the firewall (using telnet, netstat and nc).
Ray version: 0.8.5 and 0.9.0-dev
OS: Centos 7, kernel 4.4.96
Python version: 3.6
	</description>
	<comments>
		<comment id='1' author='icaropires' date='2020-05-29T16:52:18Z'>
		&lt;denchmark-link:https://github.com/ijrsvt&gt;@ijrsvt&lt;/denchmark-link&gt;
 This could possibly be heartbeat lag. I saw several cases when CPU load is high, heartbeats start to lag and are not reached on time, and it marks nodes as dead. &lt;denchmark-link:https://github.com/icaropires&gt;@icaropires&lt;/denchmark-link&gt;
 can you loosen the resource limit of your containers? That says, can you loosen the memory and cpu constraint on Docker containers a lot and see if the same error happens?
&lt;denchmark-link:https://docs.docker.com/config/containers/resource_constraints/&gt;https://docs.docker.com/config/containers/resource_constraints/&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='icaropires' date='2020-05-29T17:36:40Z'>
		
@ijrsvt This could possibly be heartbeat lag. I saw several cases when CPU load is high, heartbeats start to lag and are not reached on time, and it marks nodes as dead. @icaropires can you loosen the resource limit of your containers? That says, can you loosen the memory and cpu constraint on Docker containers a lot and see if the same error happens?
https://docs.docker.com/config/containers/resource_constraints/

Thanks for the response &lt;denchmark-link:https://github.com/rkooo567&gt;@rkooo567&lt;/denchmark-link&gt;
 . But I don't think that's the problem, because I wasn't running any script (I've just ran ) and I haven't set any constraints. But, anyway, I've tried to run with:
&lt;denchmark-code&gt;run_options: ["--shm-size 25GB --cpus=16 --memory=50g"]
&lt;/denchmark-code&gt;

and got the same behavior. It seems to me that the heartbeats are never reaching the head node
		</comment>
		<comment id='3' author='icaropires' date='2020-05-29T18:10:54Z'>
		&lt;denchmark-link:https://github.com/icaropires&gt;@icaropires&lt;/denchmark-link&gt;
 Thanks for the quick response. We will surely investigate it. Btw, is it urgent for you? Can you use some different solutions until we figure out the root cause (like running without Docker)?
		</comment>
		<comment id='4' author='icaropires' date='2020-05-29T20:23:44Z'>
		
@icaropires Thanks for the quick response. We will surely investigate it. Btw, is it urgent for you? Can you use some different solutions until we figure out the root cause (like running without Docker)?

&lt;denchmark-link:https://github.com/rkooo567&gt;@rkooo567&lt;/denchmark-link&gt;
 I was kind of testing something. But I'll figure it out, thanks for asking!
		</comment>
		<comment id='5' author='icaropires' date='2020-05-31T09:02:16Z'>
		No problem! Please share your solution if you figured this out. We tried to fix this problem as it happens to many users, but we failed to reproduce it several times ourselves!
		</comment>
		<comment id='6' author='icaropires' date='2020-08-04T14:20:35Z'>
		&lt;denchmark-link:https://github.com/rkooo567&gt;@rkooo567&lt;/denchmark-link&gt;
 The problem is the indicated at &lt;denchmark-link:https://github.com/ray-project/ray/issues/8648&gt;#8648&lt;/denchmark-link&gt;
 ! The  is being set to localhost in head node, inside or outside the container. I've had success with the following steps:

ray start the head node
connect to redis using redis-cli and set  GcsServerAddress to &lt;my_ip:&lt;same port as before&gt;
ray start other nodes

(I've tested inside containers, but it should work outside as well)
ray version: 0.8.6
I don't know the reason this is happening, in other environments it gets the correct IP
		</comment>
		<comment id='7' author='icaropires' date='2020-08-04T15:17:03Z'>
		Would you mind checking what was GCSServerAddress in Redis before you manually set?
		</comment>
		<comment id='8' author='icaropires' date='2020-08-04T15:59:12Z'>
		
Would you mind checking what was GCSServerAddress in Redis before you manually set?

&lt;denchmark-link:https://github.com/rkooo567&gt;@rkooo567&lt;/denchmark-link&gt;
 No problem! It's :
output from cat /tmp/ray/session_latest/logs/gcs_server.out:
&lt;denchmark-code&gt;I0804 12:51:42.988648    27    27 redis_client.cc:141] RedisClient connected.
I0804 12:51:42.997473    27    27 redis_gcs_client.cc:88] RedisGcsClient Connected.
I0804 12:51:42.998004    27    27 gcs_redis_failure_detector.cc:29] Starting redis failure detector.
I0804 12:51:42.998190    27    27 gcs_actor_manager.cc:737] Loading initial data.
I0804 12:51:42.998234    27    27 gcs_object_manager.cc:270] Loading initial data.
I0804 12:51:42.998255    27    27 gcs_node_manager.cc:344] Loading initial data.
I0804 12:51:42.998407    27    27 gcs_actor_manager.cc:761] Finished loading initial data.
I0804 12:51:42.998421    27    27 gcs_object_manager.cc:285] Finished loading initial data.
I0804 12:51:42.998502    27    27 gcs_node_manager.cc:361] Finished loading initial data.
I0804 12:51:42.998709    27    27 grpc_server.cc:74] GcsServer server started, listening on port 33494.
I0804 12:51:43.106926    27    27 gcs_server.cc:217] Gcs server address = 127.0.0.1:33494
I0804 12:51:43.106974    27    27 gcs_server.cc:221] Finished setting gcs server address: 127.0.0.1:33494
I0804 12:51:43.221514    27    27 gcs_node_manager.cc:135] Registering node info, node id = 94ff5ed7b153cf383852da1001e32b69fa78e04b
I0804 12:51:43.221841    27    27 gcs_node_manager.cc:140] Finished registering node info, node id = 94ff5ed7b153cf383852da1001e32b69fa78e04b

&lt;/denchmark-code&gt;

output from redis-cli:
&lt;denchmark-code&gt;head:6379&gt; get GcsServerAddress
"127.0.0.1:33494"
&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='icaropires' date='2020-08-05T05:25:29Z'>
		Really appreciate it! Would you mind trying one more thing actually? Could you run this command in the head node and see what's the IP addresses in this case?
import ray
ray.init(address='auto')
print(ray.services.get_node_ip_address())
The GCS server IP address is resolved in CPP, and it seems like it sometimes doesn't resolve the correct local IP addresses (It first pings Google DNS server, and if it fails, it uses asio resolver to get local IP addresses. If both fails, the address is set to be the localhost (your case)). I'd like to check if Python's IP resolution also fails when you run it inside Docker containers.
		</comment>
		<comment id='10' author='icaropires' date='2020-08-05T15:14:53Z'>
		
Really appreciate it! Would you mind trying one more thing actually? Could you run this command in the head node and see what's the IP addresses in this case?
import ray
ray.init(address='auto')
print(ray.services.get_node_ip_address())
The GCS server IP address is resolved in CPP, and it seems like it sometimes doesn't resolve the correct local IP addresses (It first pings Google DNS server, and if it fails, it uses asio resolver to get local IP addresses. If both fails, the address is set to be the localhost (your case)). I'd like to check if Python's IP resolution also fails when you run it inside Docker containers.

Sure! Running the commands:
Python 3.8.2 (default, Jul 16 2020, 14:00:26) 
[GCC 9.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; import ray
&gt;&gt;&gt; ray.init(address='auto')
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0805 11:38:39.371309   611   611 global_state_accessor.cc:25] Redis server address = &lt;my ip&gt;:6379, is test flag = 0
I0805 11:38:39.372180   611   611 redis_client.cc:141] RedisClient connected.
I0805 11:38:39.380892   611   611 redis_gcs_client.cc:88] RedisGcsClient Connected.
I0805 11:38:39.381640   611   611 service_based_gcs_client.cc:75] ServiceBasedGcsClient Connected.
{'node_ip_address': '&lt;my ip&gt;', 'raylet_ip_address': '&lt;my ip&gt;, 'redis_address': '&lt;my ip&gt;:6379', 'object_store_address': '/tmp/ray/session_2020-08-05_11-37-59_349964_327/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2020-08-05_11-37-59_349964_327/sockets/raylet', 'webui_url': 'localhost:8265', 'session_dir': '/tmp/ray/session_2020-08-05_11-37-59_349964_327'}
&gt;&gt;&gt; print(ray.services.get_node_ip_address())
&lt;my ip&gt;
I've only replaced my valid IP for &lt;my ip&gt; for security reasons, but it's always the same and is not 127.0.0.1.
&lt;denchmark-h:h2&gt;Some more informations&lt;/denchmark-h&gt;


it first pings Google DNS server



ping 8.8.8.8 works from my head node ðŸ¤”


This "GcsServerAddress as localhost" behaviour is happening outside the Docker containers too!


In ray version 0.8.6, when opening the dashboard the behaviour is that described in #8805


In ray version 0.8.5, when not using autoscaler, no problems can be seen in dashboard, but only the head node resources are used


I'll be happy to provide any more details if needed ðŸ˜ƒ
		</comment>
		<comment id='11' author='icaropires' date='2020-08-07T21:44:41Z'>
		&lt;denchmark-link:https://github.com/ijrsvt&gt;@ijrsvt&lt;/denchmark-link&gt;
 I think the issue is that  timeout is too short in CPP code. I can probably push the PR and see if it fixes the issue. If you have any other thought, please let me know!
&lt;denchmark-link:https://github.com/icaropires&gt;@icaropires&lt;/denchmark-link&gt;
 Would you mind downloading the latest Ray and test if my fix will help you solve the problem in some time like next week?
		</comment>
		<comment id='12' author='icaropires' date='2020-08-07T21:53:24Z'>
		
@ijrsvt I think the issue is that GcsServerAddress timeout is too short in CPP code. I can probably push the PR and see if it fixes the issue. If you have any other thought, please let me know!
@icaropires Would you mind downloading the latest Ray and test if my fix will help you solve the problem in some time like next week?

&lt;denchmark-link:https://github.com/rkooo567&gt;@rkooo567&lt;/denchmark-link&gt;
 No problem, just let me know. If I get some time, I'll also try to investigate
		</comment>
		<comment id='13' author='icaropires' date='2020-08-09T07:12:11Z'>
		I think I was able to solve it. I will update &lt;denchmark-link:https://github.com/ray-project/ray/pull/10004&gt;#10004&lt;/denchmark-link&gt;
  with the checkings and this issue with more details as soon as possible.
		</comment>
		<comment id='14' author='icaropires' date='2020-08-10T01:10:19Z'>
		&lt;denchmark-link:https://github.com/rkooo567&gt;@rkooo567&lt;/denchmark-link&gt;
 I've tried to increase timeout to like 2s but I've got the same behaviour even  my ping response being less than 100ms to 8.8.8.8.
So I invested in trying to fix the fallback option, the local resolve, and implemented the solution at &lt;denchmark-link:https://github.com/ray-project/ray/pull/10004&gt;#10004&lt;/denchmark-link&gt;
 .
But I guess that even being able to suggest the right IP most of the times, there might be situations in which the best is let the user specify GcsServerAddress (if there are more than one valid IP, for example), probably by passing a flag on CLI.
		</comment>
	</comments>
</bug>