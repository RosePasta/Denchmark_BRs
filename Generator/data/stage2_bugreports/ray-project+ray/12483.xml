<bug id='12483' author='VenezianoMauro' open_date='2020-11-29T20:23:42Z' closed_time='2020-12-11T13:51:57Z'>
	<summary>[rllib] Discrete(20) Observation_space  gets converted to Box(-1,1,(20,)) inside agent</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

Running tutorial:
&lt;denchmark-link:https://colab.research.google.com/github/ray-project/tutorial/blob/master/rllib_exercises/rllib_colab.ipynb&gt;https://colab.research.google.com/github/ray-project/tutorial/blob/master/rllib_exercises/rllib_colab.ipynb&lt;/denchmark-link&gt;

Observation space is Discrete(20) in the environment. When training there is no problem but when  doing trainer.compute_action(state) raises ValueError: ('Observation ({}) outside given space ({})!', array(0), Box(-1.0, 1.0, (20,), float32))
Ray version and other system information (Python version, TensorFlow version, OS):
Ray version 1.0.1.post1
Python version 3.7.9
Pytorch version 1.7.0+cpu
OS windows 10
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
&lt;denchmark-code&gt;import gym
from gym import spaces
import numpy as np
import ray
from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG
ray.init(ignore_reinit_error=True, log_to_driver=False)

class ChainEnv(gym.Env):
    def __init__(self, env_config = None):
        env_config = env_config or {}
        self.n = env_config.get("n", 20)
        self.small_reward = env_config.get("small", 2)  # payout for 'backwards' action
        self.large_reward = env_config.get("large", 10)  # payout at end of chain for 'forwards' action
        self.state = 0  # Start at beginning of the chain
        self._horizon = self.n
        self._counter = 0  # For terminating the episode
        self._setup_spaces()
    
    def _setup_spaces(self):
        ##############
        # TODO: Implement this so that it passes tests
        self.action_space = spaces.Discrete(2)
        self.observation_space = spaces.Discrete(self.n)
        ##############

    def step(self, action):
        assert self.action_space.contains(action)
        if action == 1:  # 'backwards': go back to the beginning, get small reward
            ##############
            # TODO 2: Implement this so that it passes tests
            reward = self.small_reward
            ##############
            self.state = 0
        elif self.state &lt; self.n - 1:  # 'forwards': go up along the chain
            ##############
            # TODO 2: Implement this so that it passes tests
            reward = 0
            self.state += 1
        else:  # 'forwards': stay at the end of the chain, collect large reward
            ##############
            # TODO 2: Implement this so that it passes tests
            reward = self.large_reward
            ##############
        self._counter += 1
        done = self._counter &gt;= self._horizon
        return self.state, reward, done, {}

    def reset(self):
        self.state = 0
        self._counter = 0
        return self.state
    
trainer_config = DEFAULT_CONFIG.copy()
trainer_config['num_workers'] = 1
trainer_config["train_batch_size"] = 400
trainer_config["sgd_minibatch_size"] = 64
trainer_config["num_sgd_iter"] = 10
trainer_config["framework"] = "torch"

trainer = PPOTrainer(trainer_config, ChainEnv);
for i in range(2):
    print("Training iteration {}...".format(i))
    trainer.train()

env = ChainEnv({})
state = env.reset()

done = False
max_state = -1
cumulative_reward = 0

while not done:
    state = np.array(state)
    action = trainer.compute_action(state)
    state, reward, done, results = env.step(action)
    max_state = max(max_state, state)
    cumulative_reward += reward

print("Cumulative reward you've received is: {}. Congratulations!".format(cumulative_reward))
print("Max state you've visited is: {}. This is out of {} states.".format(max_state, env.n))
&lt;/denchmark-code&gt;

If we cannot run your script, we cannot fix your issue.

[yes ] I have verified my script runs in a clean environment and reproduces the issue.
[ yes] I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='VenezianoMauro' date='2020-12-11T13:27:43Z'>
		Thanks for filing this &lt;denchmark-link:https://github.com/VenezianoMauro&gt;@VenezianoMauro&lt;/denchmark-link&gt;
 ! Taking a look rn.
		</comment>
		<comment id='2' author='VenezianoMauro' date='2020-12-11T13:48:15Z'>
		Hmm, apparently our OneHotPreprocessor (the one that transforms int observations into one-hot Boxes) does not store the "original_space" property in the new space, leading the Trainer's local_worker to believe that its Policy has the Box space as its original one (instead of Discrete(20)).
This is fixed with this PR: &lt;denchmark-link:https://github.com/ray-project/ray/pull/12787&gt;#12787&lt;/denchmark-link&gt;

As a workaround for right now, replace the if-block in line in rllib/models/preprocessors.py:~83
&lt;denchmark-code&gt;        if (isinstance(self, TupleFlatteningPreprocessor)
                or isinstance(self, DictFlatteningPreprocessor)
                or isinstance(self, RepeatedValuesPreprocessor)):
&lt;/denchmark-code&gt;

with this one here:
&lt;denchmark-code&gt;        if isinstance(self, (DictFlatteningPreprocessor, OneHotPreprocessor,
                             RepeatedValuesPreprocessor,
                             TupleFlatteningPreprocessor)):
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='VenezianoMauro' date='2020-12-11T13:51:09Z'>
		This is also related to this issue here: &lt;denchmark-link:https://github.com/ray-project/ray/issues/12516&gt;#12516&lt;/denchmark-link&gt;

Yet, slightly different bug.
		</comment>
		<comment id='4' author='VenezianoMauro' date='2020-12-11T13:51:57Z'>
		I'm closing this issue. Please feel free to re-open it should the above solution/PR not work on your end.
		</comment>
	</comments>
</bug>