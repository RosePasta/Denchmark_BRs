<bug id='7107' author='sven1977' open_date='2020-02-10T15:41:58Z' closed_time='2020-02-19T17:52:34Z'>
	<summary>[RLlib] Off-policy estimation faulty (IS weights may be calculated incorrectly)</summary>
	<description>
The importance sampling weights for our off-policy estimators ("is" and "wis") are calculated incorrectly in case the new policy produces a different action than the old one (given the same observation). This is due to the new policy being queried for the action-prob via compute_actions([offline batch]), w/o caring about the action being produced (this new action may be different from the old one, making the calculated new action prob useless).
Ray version and other system information (Python version, TensorFlow version, OS):
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
If we cannot run your script, we cannot fix your issue.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='sven1977' date='2020-02-19T17:52:34Z'>
		Will close this: PR &lt;denchmark-link:https://github.com/ray-project/ray/pull/7124&gt;#7124&lt;/denchmark-link&gt;

fixes this issue.
There is now a  method in Policy, which is used by the off-policy evaluators (instead of compute_actions).
		</comment>
	</comments>
</bug>