<bug id='12713' author='afaul' open_date='2020-12-09T16:08:03Z' closed_time='2020-12-09T16:49:21Z'>
	<summary>[rllib] compute_action broken since Version 1.0.1</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

I'm using the compute_action function of Trainers to evaluate the trained agents.
Since Version 1.0.1 the compute_action function, at least of the PPOTrainer, don't accepts Dict-based observation spaces as parameter.
Ray version and other system information (Python version, TensorFlow version, OS):
Python 3.7.7
TensorFlow 2.2.0
OS: Windows
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a short code snippet (less than 50 lines if possible) that can be copy-pasted to reproduce the issue. The snippet should have no external library dependencies (i.e., use fake or mock data / environments):
&lt;denchmark-code&gt;import gym
import numpy as np
import ray.rllib.agents.ppo as ppo
import ray

class Env_class(gym.Env):
    def __init__(self, config):
        self.observation_space = gym.spaces.Dict({
            "a": gym.spaces.Box(low=np.NINF, high=np.inf, shape=(1,)),
            "b": gym.spaces.Box(low=0, high=np.inf, shape=(1,)),
            "c": gym.spaces.Box(low=0, high=np.inf, shape=(1,)),
            "d": gym.spaces.Box(low=0, high=np.inf, shape=(1,)),
        })
        self.action_space = gym.spaces.Discrete(51)
        self.stepcount = 0

    def reset(self):
        self.stepcount = 0
        observation = {"a": [-10], "b": [10], "c": [5], "d": [15]}
        return observation

    def step(self, action):
        observation = {"a": [-20], "b": [20], "c": [15], "d": [25] }
        reward = 5
        if self.stepcount &gt; 5:
            episode_ended = True
        else:
            episode_ended = False
        additional_information = {}
        self.stepcount += 1
        return observation, reward, episode_ended, additional_information

env_config = {}
# instantiate env class
env = Env_class(env_config)

ray.init()
config = ppo.DEFAULT_CONFIG.copy()
agent = ppo.PPOTrainer(config=config, env=Env_class)

# run until episode ends
episode_reward = 0
done = False
obs = env.reset()
while not done:
    action = agent.compute_action(obs)
    print(f"action: {action}")
    obs, reward, done, info = env.step(action)
    episode_reward += reward

&lt;/denchmark-code&gt;

If the code snippet cannot be run by itself, the issue will be closed with "needs-repro-script".

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='afaul' date='2020-12-09T16:48:34Z'>
		Thanks for filing this &lt;denchmark-link:https://github.com/afaul&gt;@afaul&lt;/denchmark-link&gt;
 !
I think this is the same issue as this one here, which is fixed now:
&lt;denchmark-link:https://github.com/ray-project/ray/issues/12516&gt;#12516&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='afaul' date='2020-12-09T16:49:21Z'>
		Closing this due to being a duplicate.
		</comment>
	</comments>
</bug>