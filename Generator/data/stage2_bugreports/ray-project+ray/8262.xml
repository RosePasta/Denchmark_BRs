<bug id='8262' author='royf' open_date='2020-05-01T03:07:02Z' closed_time='2021-01-02T19:05:24Z'>
	<summary>[rllib] Replay buffer compression fails</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

Ray version and other system information (Python version, TensorFlow version, OS):
Ray: 0.8.4
Python: 3.8.2
OS: macOS 10.15.4
cloudpickle: 1.3.0
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

import ray
import tensorflow as tf
import torch
from ray import tune
from ray.rllib.agents.trainer import with_common_config
from ray.rllib.agents.trainer_template import build_trainer
from ray.rllib.optimizers import SyncReplayOptimizer
from ray.rllib.policy.tf_policy_template import build_tf_policy
from ray.rllib.policy.torch_policy_template import build_torch_policy


def example_tf_loss_fn(_policy, model, _dist_class, _train_batch):
    return tf.reduce_sum(model.variables()[0] * 0.)


ExampleTFPolicy = build_tf_policy(
    name="ExampleTFPolicy",
    loss_fn=example_tf_loss_fn,
)


def example_torch_loss_fn(_policy, _model, _dist_class, _train_batch):
    return torch.tensor(0., requires_grad=True)


ExampleTorchPolicy = build_torch_policy(
    name="ExampleTorchPolicy",
    loss_fn=example_torch_loss_fn,
)

DEFAULT_CONFIG = with_common_config({
    "use_pytorch": True,
    "num_workers": 1,
})


def make_policy_optimizer(workers, _config):
    return SyncReplayOptimizer(workers, prioritized_replay=False)


ExampleTrainer = build_trainer(
    name="Example",
    default_policy=ExampleTorchPolicy,
    default_config=DEFAULT_CONFIG,
    make_policy_optimizer=make_policy_optimizer,
)

if __name__ == "__main__":
    ray.init()
    tune.run(
        ExampleTrainer,
        config={
            "env": "CartPole-v1",
        })
The same also fails for default_policy=ExampleTFPolicy, but not for num_workers=2 (with either TF or Torch).

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='royf' date='2020-05-01T18:45:19Z'>
		What's the error? It seems to run for me up to 70k timesteps. Maybe pip install ray[rllib] or pip install lz4?
		</comment>
		<comment id='2' author='royf' date='2020-05-01T18:55:54Z'>
		I did install with pip install ray[rllib], and lz4 version 3.0.2 was installed as a dependency.
The error after what looks like 1000 timesteps is:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/usr/local/Caskroom/miniconda/base/envs/example/lib/python3.8/site-packages/ray/tune/trial_runner.py", line 467, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File "/usr/local/Caskroom/miniconda/base/envs/example/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py", line 381, in fetch_result
    result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)
  File "/usr/local/Caskroom/miniconda/base/envs/example/lib/python3.8/site-packages/ray/worker.py", line 1513, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(AttributeError): ray::Example.train() (pid=20624, ip=192.168.1.2)
  File "python/ray/_raylet.pyx", line 452, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 407, in ray._raylet.execute_task.function_executor
  File "/usr/local/Caskroom/miniconda/base/envs/example/lib/python3.8/site-packages/ray/rllib/agents/trainer.py", line 505, in train
    raise e
  File "/usr/local/Caskroom/miniconda/base/envs/example/lib/python3.8/site-packages/ray/rllib/agents/trainer.py", line 491, in train
    result = Trainable.train(self)
  File "/usr/local/Caskroom/miniconda/base/envs/example/lib/python3.8/site-packages/ray/tune/trainable.py", line 261, in train
    result = self._train()
  File "/usr/local/Caskroom/miniconda/base/envs/example/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py", line 150, in _train
    fetches = self.optimizer.step()
  File "/usr/local/Caskroom/miniconda/base/envs/example/lib/python3.8/site-packages/ray/rllib/optimizers/sync_replay_optimizer.py", line 139, in step
    self._optimize()
  File "/usr/local/Caskroom/miniconda/base/envs/example/lib/python3.8/site-packages/ray/rllib/optimizers/sync_replay_optimizer.py", line 159, in _optimize
    samples = self._replay()
  File "/usr/local/Caskroom/miniconda/base/envs/example/lib/python3.8/site-packages/ray/rllib/optimizers/sync_replay_optimizer.py", line 202, in _replay
    dones) = replay_buffer.sample_with_idxes(idxes)
  File "/usr/local/Caskroom/miniconda/base/envs/example/lib/python3.8/site-packages/ray/rllib/optimizers/replay_buffer.py", line 74, in sample_with_idxes
    return self._encode_sample(idxes)
  File "/usr/local/Caskroom/miniconda/base/envs/example/lib/python3.8/site-packages/ray/rllib/optimizers/replay_buffer.py", line 58, in _encode_sample
    obses_t.append(np.array(unpack_if_needed(obs_t), copy=False))
  File "/usr/local/Caskroom/miniconda/base/envs/example/lib/python3.8/site-packages/ray/rllib/utils/compression.py", line 57, in unpack_if_needed
    data = unpack(data)
  File "/usr/local/Caskroom/miniconda/base/envs/example/lib/python3.8/site-packages/ray/rllib/utils/compression.py", line 50, in unpack
    data = pickle.loads(data)
  File "/usr/local/Caskroom/miniconda/base/envs/example/lib/python3.8/site-packages/ray/cloudpickle/cloudpickle_fast.py", line 420, in _numpy_frombuffer
    array.setflags(write=isinstance(buffer, bytearray) or not buffer.readonly)
AttributeError: 'bytes' object has no attribute 'readonly'
&lt;/denchmark-code&gt;

But it seems the problem is not in pickle.loads, it's earlier in pickle.dumps.
		</comment>
		<comment id='3' author='royf' date='2020-05-01T19:11:42Z'>
		Oh weird, maybe this is a Python 3.8 pickling issue? cc &lt;denchmark-link:https://github.com/pcmoritz&gt;@pcmoritz&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/suquark&gt;@suquark&lt;/denchmark-link&gt;
 if you've seen anything like this
		</comment>
		<comment id='4' author='royf' date='2020-05-02T01:27:29Z'>
		I can confirm that I don't see this issue in a Python 3.7 environment.
		</comment>
		<comment id='5' author='royf' date='2020-06-16T16:42:19Z'>
		I am having the same issue on my machine using the code above (modified to use the tensorflow policy). I am also having the issue with my own, different code. The trace is:
Traceback (most recent call last): File "/home/jamie/anaconda3/envs/excursionrl/lib/python3.8/site-packages/ray/tune/trial_runner.py", line 467, in _process_trial result = self.trial_executor.fetch_result(trial) File "/home/jamie/anaconda3/envs/excursionrl/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py", line 431, in fetch_result result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT) File "/home/jamie/anaconda3/envs/excursionrl/lib/python3.8/site-packages/ray/worker.py", line 1515, in get raise value.as_instanceof_cause() ray.exceptions.RayTaskError(AttributeError): �[36mray::Example.train()�[39m (pid=2206, ip=192.168.0.24) File "python/ray/_raylet.pyx", line 463, in ray._raylet.execute_task File "python/ray/_raylet.pyx", line 417, in ray._raylet.execute_task.function_executor File "/home/jamie/anaconda3/envs/excursionrl/lib/python3.8/site-packages/ray/rllib/agents/trainer.py", line 498, in train raise e File "/home/jamie/anaconda3/envs/excursionrl/lib/python3.8/site-packages/ray/rllib/agents/trainer.py", line 484, in train result = Trainable.train(self) File "/home/jamie/anaconda3/envs/excursionrl/lib/python3.8/site-packages/ray/tune/trainable.py", line 261, in train result = self._train() File "/home/jamie/anaconda3/envs/excursionrl/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py", line 151, in _train fetches = self.optimizer.step() File "/home/jamie/anaconda3/envs/excursionrl/lib/python3.8/site-packages/ray/rllib/optimizers/sync_replay_optimizer.py", line 146, in step self._optimize() File "/home/jamie/anaconda3/envs/excursionrl/lib/python3.8/site-packages/ray/rllib/optimizers/sync_replay_optimizer.py", line 172, in _optimize samples = self._replay() File "/home/jamie/anaconda3/envs/excursionrl/lib/python3.8/site-packages/ray/rllib/optimizers/sync_replay_optimizer.py", line 220, in _replay dones) = replay_buffer.sample_with_idxes(idxes) File "/home/jamie/anaconda3/envs/excursionrl/lib/python3.8/site-packages/ray/rllib/optimizers/replay_buffer.py", line 74, in sample_with_idxes return self._encode_sample(idxes) File "/home/jamie/anaconda3/envs/excursionrl/lib/python3.8/site-packages/ray/rllib/optimizers/replay_buffer.py", line 58, in _encode_sample obses_t.append(np.array(unpack_if_needed(obs_t), copy=False)) File "/home/jamie/anaconda3/envs/excursionrl/lib/python3.8/site-packages/ray/rllib/utils/compression.py", line 57, in unpack_if_needed data = unpack(data) File "/home/jamie/anaconda3/envs/excursionrl/lib/python3.8/site-packages/ray/rllib/utils/compression.py", line 50, in unpack data = pickle.loads(data) File "/home/jamie/anaconda3/envs/excursionrl/lib/python3.8/site-packages/ray/cloudpickle/cloudpickle_fast.py", line 420, in _numpy_frombuffer array.setflags(write=isinstance(buffer, bytearray) or not buffer.readonly) AttributeError: 'bytes' object has no attribute 'readonly'
I am using python 3.8 on Linux.
		</comment>
		<comment id='6' author='royf' date='2020-08-21T17:50:00Z'>
		Just ran into the same issue (Ubuntu 20.04, Python 3.8, Ray: 0.9.0.dev0, Cloudpickle: 1.5.0). I could hack a solution by commenting out the line
array.setflags(write=isinstance(buffer, bytearray) or not buffer.readonly) in the function _numpy_frombuffer in the file ray/cloudpickle/cloudpickle_fast.py.
What is the specific purpose of this line? It might be worthwhile doing a check if buffer.readonly throws an error and set a default if it does.
		</comment>
		<comment id='7' author='royf' date='2020-12-19T18:59:33Z'>
		Hi, I'm a bot from the Ray team :)
To help human contributors to focus on more relevant issues, I will automatically add the stale label to issues that have had no activity for more than 4 months.
If there is no further activity in the 14 days, the issue will be closed!

If you'd like to keep the issue open, just leave any comment, and the stale label will be removed!
If you'd like to get more attention to the issue, please tag one of Ray's contributors.

You can always ask for help on our &lt;denchmark-link:https://discuss.ray.io/&gt;discussion forum&lt;/denchmark-link&gt;
 or &lt;denchmark-link:https://github.com/ray-project/ray#getting-involved&gt;Ray's public slack channel&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='8' author='royf' date='2021-01-02T19:05:23Z'>
		Hi again! The issue will be closed because there has been no more activity in the 14 days since the last message.
Please feel free to reopen or open a new issue if you'd still like it to be addressed.
Again, you can always ask for help on our &lt;denchmark-link:https://discuss.ray.io&gt;discussion forum&lt;/denchmark-link&gt;
 or &lt;denchmark-link:https://github.com/ray-project/ray#getting-involved&gt;Ray's public slack channel&lt;/denchmark-link&gt;
.
Thanks again for opening the issue!
		</comment>
	</comments>
</bug>