<bug id='9071' author='msloma144' open_date='2020-06-21T17:25:08Z' closed_time='2021-01-19T13:21:27Z'>
	<summary>[rllib] State shapes incorrect using custom model (TorchModelV2, TFModelV2) (PPO)</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

It seems that the states being passed to TorchModelV2 and TFModelV2 are incorrect, as the shapes don't seem to match up. Please see the stack traces below. Note that I am using PPO. Also, I do not want to use the RecurrentNetwork as I need more control than that provides.
Ray version and other system information (Python version, TensorFlow version, OS):
Python 3.8.3, ray: 0.9.0.dev0, torch: 1.5.1, tensorflow: 2.2.0, WSL Ubuntu 18.04.4 LTS
&lt;denchmark-h:h3&gt;Keras/Tensorflow Error&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;2020-06-21 13:09:32,571	ERROR trial_runner.py:524 -- Trial PPO_TestingGym_f28cf_00000: Error processing event.
Traceback (most recent call last):
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/tune/trial_runner.py", line 472, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py", line 430, in fetch_result
    result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/worker.py", line 1478, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(InvalidArgumentError): ray::PPO.train() (pid=20426, ip=192.168.2.105)
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/tensorflow/python/client/session.py", line 1349, in _run_fn
    return self._call_tf_sessionrun(options, feed_dict, fetch_list,
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/tensorflow/python/client/session.py", line 1441, in _call_tf_sessionrun
    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,
tensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [120,64] vs. [6,64]
	 [[{{node default_policy_1/tower_1/model_1/lstm/while/lstm_cell/add_6}}]]

During handling of the above exception, another exception occurred:

ray::PPO.train() (pid=20426, ip=192.168.2.105)
  File "python/ray/_raylet.pyx", line 443, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 446, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 447, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 401, in ray._raylet.execute_task.function_executor
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/agents/trainer.py", line 520, in train
    raise e
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/agents/trainer.py", line 506, in train
    result = Trainable.train(self)
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/tune/trainable.py", line 317, in train
    result = self._train()
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py", line 137, in _train
    return self._train_exec_impl()
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py", line 175, in _train_exec_impl
    res = next(self.train_exec_impl)
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py", line 731, in __next__
    return next(self.built_iterator)
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py", line 744, in apply_foreach
    for item in it:
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py", line 744, in apply_foreach
    for item in it:
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py", line 814, in apply_filter
    for item in it:
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py", line 814, in apply_filter
    for item in it:
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py", line 744, in apply_foreach
    for item in it:
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py", line 744, in apply_foreach
    for item in it:
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py", line 752, in apply_foreach
    result = fn(item)
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/execution/train_ops.py", line 204, in __call__
    batch_fetches = optimizer.optimize(
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/execution/multi_gpu_impl.py", line 257, in optimize
    return sess.run(fetches, feed_dict=feed_dict)
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/tensorflow/python/client/session.py", line 957, in run
    result = self._run(None, fetches, feed_dict, options_ptr,
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/tensorflow/python/client/session.py", line 1180, in _run
    results = self._do_run(handle, final_targets, final_fetches,
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/tensorflow/python/client/session.py", line 1358, in _do_run
    return self._do_call(_run_fn, feeds, fetches, targets, options,
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/tensorflow/python/client/session.py", line 1384, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [120,64] vs. [6,64]
	 [[node default_policy_1/tower_1/model_1/lstm/while/lstm_cell/add_6 (defined at mnt/c/Users/user/Desktop/RLlib_Issue/rnn_model.py:81) ]]
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Pytorch Error&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;2020-06-21 13:14:26,130	ERROR trial_runner.py:524 -- Trial PPO_TestingGym_a4dcc_00000: Error processing event.
Traceback (most recent call last):
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/tune/trial_runner.py", line 472, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py", line 430, in fetch_result
    result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/worker.py", line 1478, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(RuntimeError): ray::PPO.train() (pid=21085, ip=192.168.2.105)
  File "python/ray/_raylet.pyx", line 447, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 401, in ray._raylet.execute_task.function_executor
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/agents/trainer.py", line 520, in train
    raise e
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/agents/trainer.py", line 506, in train
    result = Trainable.train(self)
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/tune/trainable.py", line 317, in train
    result = self._train()
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py", line 137, in _train
    return self._train_exec_impl()
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py", line 175, in _train_exec_impl
    res = next(self.train_exec_impl)
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py", line 731, in __next__
    return next(self.built_iterator)
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py", line 744, in apply_foreach
    for item in it:
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py", line 744, in apply_foreach
    for item in it:
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py", line 814, in apply_filter
    for item in it:
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py", line 814, in apply_filter
    for item in it:
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py", line 744, in apply_foreach
    for item in it:
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py", line 744, in apply_foreach
    for item in it:
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py", line 752, in apply_foreach
    result = fn(item)
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/execution/train_ops.py", line 62, in __call__
    info = do_minibatch_sgd(
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/utils/sgd.py", line 114, in do_minibatch_sgd
    batch_fetches = (local_worker.learn_on_batch(
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py", line 737, in learn_on_batch
    info_out[pid] = policy.learn_on_batch(batch)
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/policy/torch_policy.py", line 242, in learn_on_batch
    self._loss(self, self.model, self.dist_class, train_batch))
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/agents/ppo/ppo_torch_policy.py", line 113, in ppo_surrogate_loss
    logits, state = model.from_batch(train_batch)
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/models/modelv2.py", line 224, in from_batch
    return self.__call__(input_dict, states, train_batch.get("seq_lens"))
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/models/modelv2.py", line 181, in __call__
    res = self.forward(restored, state or [], seq_lens)
  File "/mnt/c/Users/user/Desktop/RLlib_Issue/rnn_model.py", line 166, in forward
    self._features, [h, c] = self.lstm(x, [torch.unsqueeze(state[0], 0),
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/torch/nn/modules/rnn.py", line 567, in forward
    self.check_forward_args(input, hx, batch_sizes)
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/torch/nn/modules/rnn.py", line 522, in check_forward_args
    self.check_hidden_size(hidden[0], expected_hidden_size,
  File "/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/torch/nn/modules/rnn.py", line 187, in check_hidden_size
    raise RuntimeError(msg.format(expected_hidden_size, tuple(hx.size())))
RuntimeError: Expected hidden[0] size (1, 140, 256), got (1, 7, 256)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
If we cannot run your script, we cannot fix your issue.

[ X ] I have verified my script runs in a clean environment and reproduces the issue. (created new conda environment and installed all packages from scratch using pip)
[ X ] I have verified the issue also occurs with the latest wheels.

&lt;denchmark-h:h4&gt;rllib_ppo_agent.py&lt;/denchmark-h&gt;

from testing_gym import TestingGym
from ray.rllib.models import ModelCatalog
from ray.tune.registry import register_env
from rnn_model import TorchRNNModel, RNNModel
from ray import tune

timesteps = 5


def env_creator(env_config):
    env = TestingGym()
    return env  # return an env instance


if __name__ == "__main__":
    register_env("TestingGym", env_creator)
    # also have had issues with TF models
    ModelCatalog.register_custom_model("torch_model", TorchRNNModel)
    ModelCatalog.register_custom_model("keras_model",  RNNModel)

    tune.run(
        "A2C",
        stop={"episodes_total": 500},
        checkpoint_at_end=True,
        checkpoint_freq=100,
        config={
            "env": "TestingGym",
            "num_workers": 14,
            "env_config": {},
            "lr": 0.000001,
            "framework": "torch",
            "model": {
                "custom_model_config":
                    {
                        "timesteps": timesteps
                    },
                "fcnet_hiddens": [256, 256, 256, 256],
                "custom_model": "torch_model",
            }
        },
        local_dir="./results", )
&lt;denchmark-h:h4&gt;rnn_model.py&lt;/denchmark-h&gt;

import numpy as np

from ray.rllib.models.modelv2 import ModelV2
from ray.rllib.models.preprocessors import get_preprocessor
from ray.rllib.models.tf.tf_modelv2 import TFModelV2
from ray.rllib.utils.annotations import override
from ray.rllib.utils.framework import try_import_tf, try_import_torch
from ray.rllib.models.torch.torch_modelv2 import TorchModelV2

tf = try_import_tf()
torch, nn = try_import_torch()


class RNNModel(TFModelV2):
    """Example of using the Keras functional API to define a RNN model."""

    def __init__(self,
                 obs_space,
                 action_space,
                 num_outputs,
                 model_config,
                 name,
                 hiddens_size=256,
                 cell_size=64,
                 timesteps=5):
        super(RNNModel, self).__init__(obs_space, action_space, num_outputs,
                                       model_config, name)
        self.obs_space = obs_space
        self.cell_size = cell_size
        self.timesteps = timesteps

        print(f"OBS SPACE: {obs_space.shape}")
        # Define input layers
        input_layer = tf.keras.layers.Input(
            shape=(timesteps, int(obs_space.shape[0]/self.timesteps)), name="inputs")

        state_in_h = tf.keras.layers.Input(shape=(cell_size, ), name="h")
        state_in_c = tf.keras.layers.Input(shape=(cell_size, ), name="c")
        #seq_in = tf.keras.layers.Input(shape=(), name="seq_in", dtype=tf.int32)

        # Preprocess observation with a hidden layer and send to LSTM cell
        dense1 = tf.keras.layers.Dense(
            hiddens_size, activation=tf.nn.sigmoid, name="dense1")(input_layer)
        lstm_out, state_h, state_c = tf.keras.layers.LSTM(
            cell_size,
            #return_sequences=True,
            return_state=True, name="lstm")(
                inputs=dense1,
                #mask=tf.sequence_mask(seq_in),
                initial_state=[state_in_h, state_in_c])
        #flats = tf.keras.layers.Flatten()(lstm_out)
        # Postprocess LSTM output with another hidden layer and compute values

        _ = lstm_out
        for units in model_config["fcnet_hiddens"]:
            _ = tf.keras.layers.Dense(
                units,
                activation=tf.keras.activations.sigmoid)(_)

        logits = tf.keras.layers.Dense(
            self.num_outputs,
            activation=tf.keras.activations.linear,
            name="logits")(_)
        values = tf.keras.layers.Dense(
            1, activation=None, name="values")(_)

        # Create the RNN model
        self.rnn_model = tf.keras.Model(
            inputs=[input_layer, state_in_h, state_in_c],
            outputs=[logits, values, state_h, state_c])
        self.register_variables(self.rnn_model.variables)
        self.rnn_model.summary()

    @override(TFModelV2)
    def forward(self, inputs, state, seq_lens):
        print("forward")
        print(f"INPUTS: {state}")
        inputs = inputs['obs']
        inputs = tf.reshape(tensor=inputs, shape=[-1, self.timesteps, int(self.obs_space.shape[0]/self.timesteps)])

        model_out, self._value_out, h, c = self.rnn_model([inputs,] + state)
        return model_out, [h, c]

    @override(ModelV2)
    def get_initial_state(self):
        return [
            np.zeros(self.cell_size, np.float32),
            np.zeros(self.cell_size, np.float32),
        ]

    @override(ModelV2)
    def value_function(self):
        return tf.reshape(self._value_out, [-1])


class TorchRNNModel(TorchModelV2, nn.Module):
    def __init__(self,
                 obs_space,
                 action_space,
                 num_outputs,
                 model_config,
                 name,
                 fc_size=64,
                 lstm_state_size=256,
                 num_symbols=5,
                 timesteps=5):
        super().__init__(obs_space, action_space, num_outputs, model_config,
                         name)
        nn.Module.__init__(self)
        self.timesteps = timesteps
        self.num_symbols = num_symbols

        self.obs_size = get_preprocessor(obs_space)(obs_space).size
        print(f"RNN Obs Size: {self.obs_size}")
        self.obs_size = int(self.obs_size/self.timesteps)
        self.fc_size = fc_size
        self.lstm_state_size = lstm_state_size

        # Build the Module from fc + LSTM + 2xfc (action + value outs).
        self.fc1 = nn.Linear(self.obs_size, self.fc_size)
        self.lstm = nn.LSTM(self.fc_size, self.lstm_state_size, batch_first=True)
        self.action_branch = nn.Linear(self.lstm_state_size, num_outputs)
        self.value_branch = nn.Linear(self.lstm_state_size, 1)
        # Holds the current "base" output (before logits layer).
        self._features = None

    @override(ModelV2)
    def get_initial_state(self):
        # Place hidden states on same device as model.
        h = [
            self.fc1.weight.new(1, self.lstm_state_size).zero_().squeeze(0),
            self.fc1.weight.new(1, self.lstm_state_size).zero_().squeeze(0)
        ]
        print(f"Inital State: {h[0].shape},  {h[1].shape}")
        return h

    @override(ModelV2)
    def value_function(self):
        assert self._features is not None, "must call forward() first"
        return torch.reshape(self.value_branch(self._features), [-1])

    @override(ModelV2)
    def forward(self, inputs, state, seq_lens):
        """
        Feeds `inputs` (B x T x ..) through the Gru Unit.

        Returns the resulting outputs as a sequence (B x T x ...).
        Values are stored in self._cur_value in simple (B) shape (where B
        contains both the B and T dims!).

        Returns:
            NN Outputs (B x T x ...) as sequence.
            The state batches as a List of two items (c- and h-states).
        """
        print("forward")
        #print(f"INPUTS: {state}")
        inputs = inputs['obs']
        # if not isinstance(inputs, tuple):
        inputs = torch.reshape(input=inputs, shape=(-1, self.timesteps, int(self.obs_size)))
        print(f"inputs shape: {inputs.shape}")
        print(f"state sizes: h {torch.unsqueeze(state[0], 0).shape}, c {torch.unsqueeze(state[1], 0).shape}")
        # embedding_input = inputs[:, :, :self.num_symbols]
        # inputs = inputs[:, :, self.num_symbols:]

        x = nn.functional.relu(self.fc1(inputs))
        self._features, [h, c] = self.lstm(x, [torch.unsqueeze(state[0], 0),
                                               torch.unsqueeze(state[1], 0)])
        print(f"state size after: h {h.shape}, c {c.shape}")
        print(f"LSTM shape: {self._features.shape}")
        self._features = self._features[:, -1, :]
        print(f"LSTM shape After: {self._features.shape}")
        action_out = self.action_branch(self._features)
        print(f"action shape: {action_out.shape}")

        return action_out, [torch.squeeze(h, 0), torch.squeeze(c, 0)]
&lt;denchmark-h:h4&gt;testing_gym.py&lt;/denchmark-h&gt;

import gym
from gym import error, spaces, utils
from gym.utils import seeding
import numpy as np
import sys


class TestingGym(gym.Env):
    metadata = {'render.modes': ['human']}

    def __init__(self, timesteps=5):
        self.timesteps = timesteps

        super(TestingGym, self).__init__()

        self.reward_range = (-sys.float_info.max-1, sys.float_info.max)

        self.action_space = spaces.Box(low=np.array([0, 0]), high=np.array([4, 1]), dtype=np.float16)

        self.done_counter = 0
        self.obs_length = 15
        self.observation_space = spaces.Box(low=-sys.float_info.max-1, high=sys.float_info.max, shape=(self.timesteps * self.obs_length,), dtype=np.float32)

    def _initial_observation(self):
        curr_obs = np.random.random((self.timesteps, self.obs_length))
        curr_obs = curr_obs.reshape((self.timesteps * self.obs_length,))
        print(f"Obs Length: {curr_obs.shape}")
        return curr_obs

    def step(self, action):
        self.done_counter += 1

        curr_obs = np.random.random((self.timesteps, self.obs_length))
        curr_obs = curr_obs.reshape((self.timesteps * self.obs_length,))

        if self.done_counter &gt; 1000:
            done = True
        else:
            done = False

        print(f"Obs Length: {curr_obs.shape}")
        return curr_obs, 1, done, {}

    def reset(self):
        self.done_counter = 0

        return self._initial_observation()
	</description>
	<comments>
		<comment id='1' author='msloma144' date='2020-12-26T02:42:22Z'>
		Thank you for sharing this, as I am facing the same challenge in recent days using TorchRNNModel with TorchModelV2.  I believe it happens on my experiment after workers finish collecting experiences and batch shape changes.
Ray version and other system information (Python version, TensorFlow version, OS):
Python 3.7.6, ray: 1.1.0, torch: 1.7.0a0+b7147fe, WSL Ubuntu 18.04.4 LTS
RuntimeError: Expected hidden[0] size (1, 1, 128), got (1, 32, 128)
		</comment>
		<comment id='2' author='msloma144' date='2020-12-30T00:23:57Z'>
		Is there a workaround to get the correct shape the first time around?  It seems very connected to this thread #&lt;denchmark-link:https://github.com/ray-project/ray/issues/12509&gt;#12509&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='msloma144' date='2020-12-30T01:51:17Z'>
		Strange, I'm getting a different error on your torch script, related to the states list being empty.
The reason is that A3C uses a buggy ValueFunction mixin, which assumes that no RNN is being used. I'll fix this now and re-try. ...
		</comment>
		<comment id='4' author='msloma144' date='2020-12-30T02:08:52Z'>
		Thanks for looking at this Sven.  My results are the same when using A3C/A2C.  If we change it over to PPO then we get the state shape error.
&lt;denchmark-code&gt;state sizes: h torch.Size([1, 32, 256]), c torch.Size([1, 32, 256])
state size after: h torch.Size([1, 32, 256]), c torch.Size([1, 32, 256])
LSTM shape: torch.Size([32, 5, 256])
LSTM shape After: torch.Size([32, 256])
action shape: torch.Size([32, 4])
* forward pass #1
inputs shape: torch.Size([1, 5, 15])
state sizes: h torch.Size([1, 1, 256]), c torch.Size([1, 1, 256])
state size after: h torch.Size([1, 1, 256]), c torch.Size([1, 1, 256])
LSTM shape: torch.Size([1, 5, 256])
LSTM shape After: torch.Size([1, 256])
action shape: torch.Size([1, 4])
* forward pass #2
inputs shape: torch.Size([32, 5, 15])
state sizes: h torch.Size([1, 4, 256]), c torch.Size([1, 4, 256])
&lt;/denchmark-code&gt;

RuntimeError: Expected hidden[0] size (1, 32, 256), got [1, 4, 256]
		</comment>
		<comment id='5' author='msloma144' date='2020-12-30T03:56:53Z'>
		Ok, getting the same error now after having fixed these legacy A2C issues. ....
		</comment>
	</comments>
</bug>