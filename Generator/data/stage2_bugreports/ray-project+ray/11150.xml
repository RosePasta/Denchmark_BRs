<bug id='11150' author='richardliaw' open_date='2020-10-01T07:45:49Z' closed_time='2020-10-12T17:50:46Z'>
	<summary>[placement group] Too many open files when waiting for autoscaling w/ placement group</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

Ray version and other system information (Python version, TensorFlow version, OS): master, linux, latest_dlami
(pid=raylet, ip=172.31.72.57) E1001 02:26:23.387923  4663  4663 node_manager.cc:3324] Failed to send get core worker stats request: IOError: 14: failed to connect to all addresses
2020-10-01 02:26:24,030	WARNING worker.py:1082 -- The actor or task with ID 7e627883d6564be8ffffffff03000000 is infeasible and cannot currently be scheduled. It requires {CPU_group_0_e02990ac77d1b4f9ea623fee95bebb8f: 0.001000}, {CPU_group_e02990ac77d1b4f9ea623fee95bebb8f: 0.001000} for execution and {CPU_group_0_e02990ac77d1b4f9ea623fee95bebb8f: 0.001000}, {CPU_group_e02990ac77d1b4f9ea623fee95bebb8f: 0.001000} for placement, however there are no nodes in the cluster that can provide the requested resources. To resolve this issue, consider reducing the resource requests of this task or add nodes that can fit the task.
(pid=raylet, ip=172.31.72.57) E1001 02:26:24.639464  4663  4663 node_manager.cc:3324] Failed to send get core worker stats request: IOError: 14: failed to connect to all addresses
(pid=raylet, ip=172.31.72.57) F1001 02:26:32.537320  4663  4663 service_based_gcs_client.cc:207] Couldn't reconnect to GCS server. The last attempted GCS server address was 172.31.92.82:37407
(pid=raylet, ip=172.31.72.57) *** Check failure stack trace: ***
(pid=raylet, ip=172.31.72.57)     @     0x559a0a70bded  google::LogMessage::Fail()
(pid=raylet, ip=172.31.72.57)     @     0x559a0a70d25c  google::LogMessage::SendToLog()
(pid=raylet, ip=172.31.72.57)     @     0x559a0a70bac9  google::LogMessage::Flush()
(pid=raylet, ip=172.31.72.57)     @     0x559a0a70bce1  google::LogMessage::~LogMessage()
(pid=raylet, ip=172.31.72.57)     @     0x559a0a6c14f9  ray::RayLog::~RayLog()
(pid=raylet, ip=172.31.72.57)     @     0x559a0a46babd  ray::gcs::ServiceBasedGcsClient::ReconnectGcsServer()
(pid=raylet, ip=172.31.72.57)     @     0x559a0a46bbb5  ray::gcs::ServiceBasedGcsClient::GcsServiceFailureDetected()
(pid=raylet, ip=172.31.72.57)     @     0x559a0a47718a  _ZNSt17_Function_handlerIFvRKN3ray6StatusERKNS0_3rpc20ReportHeartbeatReplyEEZNS4_12GcsRpcClient15ReportHeartbeatERKNS4_22ReportHeartbeatRequestERKSt8functionIS8_EEUlS3_S7_E_E9_M_invokeERKSt9_Any_dataS3_S7_
(pid=raylet, ip=172.31.72.57)     @     0x559a0a47721d  ray::rpc::ClientCallImpl&lt;&gt;::OnReplyReceived()
(pid=raylet, ip=172.31.72.57)     @     0x559a0a3a56b0  _ZN5boost4asio6detail18completion_handlerIZN3ray3rpc17ClientCallManager29PollEventsFromCompletionQueueEiEUlvE_E11do_completeEPvPNS1_19scheduler_operationERKNS_6system10error_codeEm
(pid=raylet, ip=172.31.72.57)     @     0x559a0aa0e7cf  boost::asio::detail::scheduler::do_run_one()
(pid=raylet, ip=172.31.72.57)     @     0x559a0aa0fcd1  boost::asio::detail::scheduler::run()
(pid=raylet, ip=172.31.72.57)     @     0x559a0aa10d02  boost::asio::io_context::run()
(pid=raylet, ip=172.31.72.57)     @     0x559a0a31e92e  main
(pid=raylet, ip=172.31.72.57)     @     0x7f1e75911b97  __libc_start_main
(pid=raylet, ip=172.31.72.57)     @     0x559a0a335621  (unknown)
(pid=4704, ip=172.31.72.57) E1001 02:26:33.541347  4704  4741 core_worker.cc:705] Raylet failed. Shutting down.
(pid=4708, ip=172.31.72.57) E1001 02:26:33.548130  4708  4748 core_worker.cc:705] Raylet failed. Shutting down.
(pid=4706, ip=172.31.72.57) E1001 02:26:33.595137  4706  4766 core_worker.cc:705] Raylet failed. Shutting down.
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
Runnable example:
&lt;denchmark-code&gt;# https://github.com/ray-project/ray/pull/11061
git checkout richardliaw/colocation-sgd

cd TUNEDIR
ray up gpu.yaml # (see below)
ray submit gpu.yaml examples/ddp_mnist_torch.py -- --use-gpu --cluster -n 8 --workers-per-node=4
&lt;/denchmark-code&gt;

# An unique identifier for the head node and workers of this cluster.
cluster_name: gpu

# The maximum number of workers nodes to launch in addition to the head
# node. This takes precedence over min_workers. min_workers default to 0.
min_workers: 0
initial_workers: 0
max_workers: 3

# Cloud-provider specific configuration.
provider:
    type: aws
    region: us-east-1

# How Ray will authenticate with newly launched nodes.
auth:
    ssh_user: ubuntu

head_node:
    InstanceType: p3.8xlarge
    # InstanceType: g4dn.xlarge
    ImageId: latest_dlami
    # Set primary volume to 25 GiB
    BlockDeviceMappings:
        - DeviceName: /dev/sda1
          Ebs:
              VolumeSize: 300


worker_nodes:
    InstanceType: p3.8xlarge
    ImageId: latest_dlami

file_mounts: {
    /home/ubuntu/tune/: &lt;PATH TO TUNE DIR&gt;, i.e. # ray/python/ray/tune/
}

setup_commands:
  - ray || pip install -U https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-1.1.0.dev0-cp37-cp37m-manylinux1_x86_64.whl
  - pip install ipdb ray[tune] torch torchvision
  - rm -rf  /home/ubuntu/anaconda3/lib/python3.7/site-packages/ray/tune
  - cp -r /home/ubuntu/tune/  /home/ubuntu/anaconda3/lib/python3.7/site-packages/ray/tune

# Custom commands that will be run on the head node after common setup.
head_setup_commands: []

# Custom commands that will be run on worker nodes after common setup.
worker_setup_commands: []

# # Command to start ray on the head node. You don't need to change this.
head_start_ray_commands:
    - ray stop --force
    - ray start --head --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml --object-store-memory=1000000000

# Command to start ray on worker nodes. You don't need to change this.
worker_start_ray_commands:
    - ray stop --force
    - ray start --address=$RAY_HEAD_IP:6379 --object-manager-port=8076 --object-store-memory=1000000000
This will cause segfault.
If we cannot run your script, we cannot fix your issue.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='richardliaw' date='2020-10-01T23:11:34Z'>
		I'm not totally sure if the title is actually the issue though:
&lt;denchmark-code&gt;==&gt; ./gcs_server.err &lt;==
E1001 20:18:06.466866356   41156 tcp_server_posix.cc:213]    Failed accept4: Too many open files

==&gt; ./gcs_server.out &lt;==
I1001 22:40:43.906790 41149 41149 gcs_placement_group_scheduler.cc:204] Scheduling placement group unnamed_group, bundles size = 2
W1001 22:40:43.906796 41149 41149 gcs_placement_group_scheduler.cc:211] Failed to schedule placement group unnamed_group, because no nodes are available.
W1001 22:40:43.906796 41149 41149 gcs_placement_group_scheduler.cc:211] Failed to schedule placement group unnamed_group, because no nodes are available.
W1001 22:40:43.906803 41149 41149 gcs_placement_group_manager.cc:123] Failed to create placement group unnamed_group, try again.
W1001 22:40:43.906803 41149 41149 gcs_placement_group_manager.cc:123] Failed to create placement group unnamed_group, try again.
I1001 22:40:43.906908 41149 41149 gcs_placement_group_scheduler.cc:204] Scheduling placement group unnamed_group, bundles size = 2
W1001 22:40:43.906919 41149 41149 gcs_placement_group_scheduler.cc:211] Failed to schedule placement group unnamed_group, because no nodes are available.
W1001 22:40:43.906919 41149 41149 gcs_placement_group_scheduler.cc:211] Failed to schedule placement group unnamed_group, because no nodes are available.
W1001 22:40:43.906927 41149 41149 gcs_placement_group_manager.cc:123] Failed to create placement group unnamed_group, try again.
W1001 22:40:43.906927 41149 41149 gcs_placement_group_manager.cc:123] Failed to create placement group unnamed_group, try again.
&lt;/denchmark-code&gt;

There's a time gap here in the logs.
		</comment>
		<comment id='2' author='richardliaw' date='2020-10-01T23:16:46Z'>
		Wait how come is it possible? Did you run multiple instances?
		</comment>
		<comment id='3' author='richardliaw' date='2020-10-01T23:16:55Z'>
		or the test runs more than a couple hours?
		</comment>
		<comment id='4' author='richardliaw' date='2020-10-02T00:01:58Z'>
		not sure looks like logs didnt clear
		</comment>
		<comment id='5' author='richardliaw' date='2020-10-02T02:24:00Z'>
		&lt;denchmark-link:https://github.com/richardliaw&gt;@richardliaw&lt;/denchmark-link&gt;
 can this be reproduced locally with fake multi node? After how long does it crash?
		</comment>
		<comment id='6' author='richardliaw' date='2020-10-02T03:07:09Z'>
		&lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
 No, and on a cluster = immediately
		</comment>
		<comment id='7' author='richardliaw' date='2020-10-02T05:29:59Z'>
		Hmm.. that's really weird. When you run it without placement group, does it succeed?
		</comment>
		<comment id='8' author='richardliaw' date='2020-10-02T05:30:18Z'>
		Maybe we have some corrupted memory access that silently kills the process without an error? (but then it is weird it only happens in the cluster mode)
		</comment>
		<comment id='9' author='richardliaw' date='2020-10-07T00:47:02Z'>
		Initial update;
When I started clusters with ulimit -c unlimited; ray start..., I couldn't reproduce the error. But I saw the script fails when an --use-gpu arg is specified. I could reproduce it in my local cluster_util.
from ray.cluster_utils import Cluster

cluster = Cluster()
num_nodes = 4
for i in range(num_nodes):
    cluster.add_node(
        redis_port=6379 if i == 0 else None,
        num_cpus=32,
        num_gpus=4)

import time
time.sleep(10000000)
Richard will try one more time and see if he could reproduce the following error. If he could, we will probably have pair programming to fix the issue!
		</comment>
		<comment id='10' author='richardliaw' date='2020-10-08T00:26:56Z'>
		&lt;denchmark-link:https://github.com/rkooo567&gt;@rkooo567&lt;/denchmark-link&gt;
 how were you able to figure out the check failure? Was it in the gcs log (don't see it posted anywhere above).
I'm wondering if there is some issue here where we aren't routing GCS logs to the driver like we do raylet logs. Maybe we need to add it to the log monitor regex to propagate these kind of errors.
		</comment>
		<comment id='11' author='richardliaw' date='2020-10-08T01:41:41Z'>
		&lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
 I created a PR for that; &lt;denchmark-link:https://github.com/ray-project/ray/pull/11265&gt;#11265&lt;/denchmark-link&gt;

Btw, the check failure was technically a separate issue. This is what happened;


Richard ran his colocation example with autoscaler =&gt; This failed =&gt; I debugged quickly and figured out there was this check failure =&gt; I looked at code and figured iyt when it could happen. Since I already knew how to fix, he didn't post it.


And after that Richard run it with fixed sized cluster =&gt; This error happened =&gt; Richard posted this issue =&gt; I couldn't reproduce =&gt; He also couldn't reproduce in the latest wheel =&gt; We assume this was fixed somehow.


		</comment>
		<comment id='12' author='richardliaw' date='2020-10-08T01:44:24Z'>
		oh and yes. I found the check failure from gcs_server.err
		</comment>
	</comments>
</bug>