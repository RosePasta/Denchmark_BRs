<bug id='4307' author='AmeerHajAli' open_date='2019-03-08T09:54:55Z' closed_time='2019-10-24T20:13:50Z'>
	<summary>rollout fails with non default lstm</summary>
	<description>
when I train with the default lstm and run (notice &lt;&lt;"model": {"use_lstm":'true'}&gt;&gt; at the end):
&lt;denchmark-code&gt;rllib rollout /nscratch/qijing.huang/PPO_HLSMultiEnv_0_2019-03-02_22-46-02wsjpyww5/checkpoint_180/checkpoint-180 --run PPO --env HLS-v0 --steps 24 --no-render --config '{"env_config": {"pgm": "dfadd.c", "verbose": 'true', "pgm_dir": "/scratch/qijing.huang/LegUp/legup-4.0/examples/chstone/dfadd/", "run_dir": "run_0", "orig_and_normalize": 'true', "log_obs_reward": 'true'}, "model": {"use_lstm":'true'}}'
&lt;/denchmark-code&gt;

it works.
But when I train the model with &lt;&lt;"model": {"use_lstm":'true', **"max_seq_len": 5, "lstm_use_prev_action_reward": 'true'}&gt;&gt;
and try to rollout it out with:
&lt;denchmark-code&gt;rllib rollout /home/eecs/ameerh/ray_results/ppo_lstm/PPO_HLSMultiEnv_0_2019-03-08_00-44-06r4nf1wd9/checkpoint_5/checkpoint-5 --run PPO --env HLS-v0 --steps 24 --no-render --config '{"env_config": {"pgm": "dfadd.c", "verbose": 'true', "pgm_dir": "/scratch/qijing.huang/LegUp/legup-4.0/examples/chstone/dfadd/", "run_dir": "run_0", "orig_and_normalize": 'true', "log_obs_reward": 'true'}, **"model": {"use_lstm":'true', "max_seq_len": 5, "lstm_use_prev_action_reward": 'true'}**}'
&lt;/denchmark-code&gt;

It fails with this error:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/scratch/qijing.huang/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1322, in _do_call
    return fn(*args)
  File "/scratch/qijing.huang/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1307, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/scratch/qijing.huang/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1409, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'default/action_1' with dtype int64 and shape [?]
	 [[Node: default/action_1 = Placeholder[dtype=DT_INT64, shape=[?], _device="/job:localhost/replica:0/task:0/device:GPU:0"]()]]
	 [[Node: default/rnn/while/Exit_3/_417 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_278_default/rnn/while/Exit_3", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/qijing.huang/anaconda3/lib/python3.6/site-packages/ray/rllib/utils/tf_run_builder.py", line 47, in get
    self.feed_dict, os.environ.get("TF_TIMELINE_DIR"))
  File "/scratch/qijing.huang/anaconda3/lib/python3.6/site-packages/ray/rllib/utils/tf_run_builder.py", line 85, in run_timeline
    fetches = sess.run(ops, feed_dict=feed_dict)
  File "/scratch/qijing.huang/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 900, in run
    run_metadata_ptr)
  File "/scratch/qijing.huang/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File "/scratch/qijing.huang/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1316, in _do_run
    run_metadata)
  File "/scratch/qijing.huang/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'default/action_1' with dtype int64 and shape [?]
	 [[Node: default/action_1 = Placeholder[dtype=DT_INT64, shape=[?], _device="/job:localhost/replica:0/task:0/device:GPU:0"]()]]
	 [[Node: default/rnn/while/Exit_3/_417 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_278_default/rnn/while/Exit_3", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

Caused by op 'default/action_1', defined at:
  File "/scratch/qijing.huang/anaconda3/bin/rllib", line 11, in &lt;module&gt;
    sys.exit(cli())
  File "/scratch/qijing.huang/anaconda3/lib/python3.6/site-packages/ray/rllib/scripts.py", line 40, in cli
    rollout.run(options, rollout_parser)
  File "/scratch/qijing.huang/anaconda3/lib/python3.6/site-packages/ray/rllib/rollout.py", line 100, in run
    agent = cls(env=args.env, config=config)
  File "/scratch/qijing.huang/anaconda3/lib/python3.6/site-packages/ray/rllib/agents/agent.py", line 248, in __init__
    Trainable.__init__(self, config, logger_creator)
  File "/scratch/qijing.huang/anaconda3/lib/python3.6/site-packages/ray/tune/trainable.py", line 88, in __init__
    self._setup(copy.deepcopy(self.config))
  File "/scratch/qijing.huang/anaconda3/lib/python3.6/site-packages/ray/rllib/agents/agent.py", line 319, in _setup
    self._init()
  File "/scratch/qijing.huang/anaconda3/lib/python3.6/site-packages/ray/rllib/agents/ppo/ppo.py", line 75, in _init
    self.env_creator, self._policy_graph)
  File "/scratch/qijing.huang/anaconda3/lib/python3.6/site-packages/ray/rllib/agents/agent.py", line 439, in make_local_evaluator
    config["local_evaluator_tf_session_args"]
  File "/scratch/qijing.huang/anaconda3/lib/python3.6/site-packages/ray/rllib/agents/agent.py", line 579, in _make_evaluator
    output_creator=output_creator)
  File "/scratch/qijing.huang/anaconda3/lib/python3.6/site-packages/ray/rllib/evaluation/policy_evaluator.py", line 270, in __init__
    self._build_policy_map(policy_dict, policy_config)
  File "/scratch/qijing.huang/anaconda3/lib/python3.6/site-packages/ray/rllib/evaluation/policy_evaluator.py", line 594, in _build_policy_map
    policy_map[name] = cls(obs_space, act_space, merged_conf)
  File "/scratch/qijing.huang/anaconda3/lib/python3.6/site-packages/ray/rllib/agents/ppo/ppo_policy_graph.py", line 142, in __init__
    prev_actions_ph = ModelCatalog.get_action_placeholder(action_space)
  File "/scratch/qijing.huang/anaconda3/lib/python3.6/site-packages/ray/rllib/models/catalog.py", line 150, in get_action_placeholder
    return tf.placeholder(tf.int64, shape=(None, ), name="action")
  File "/scratch/qijing.huang/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py", line 1734, in placeholder
    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)
  File "/scratch/qijing.huang/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py", line 4924, in placeholder
    "Placeholder", dtype=dtype, shape=shape, name=name)
  File "/scratch/qijing.huang/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/scratch/qijing.huang/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 3414, in create_op
    op_def=op_def)
  File "/scratch/qijing.huang/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1740, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'default/action_1' with dtype int64 and shape [?]
	 [[Node: default/action_1 = Placeholder[dtype=DT_INT64, shape=[?], _device="/job:localhost/replica:0/task:0/device:GPU:0"]()]]
	 [[Node: default/rnn/while/Exit_3/_417 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_278_default/rnn/while/Exit_3", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/qijing.huang/anaconda3/bin/rllib", line 11, in &lt;module&gt;
    sys.exit(cli())
  File "/scratch/qijing.huang/anaconda3/lib/python3.6/site-packages/ray/rllib/scripts.py", line 40, in cli
    rollout.run(options, rollout_parser)
  File "/scratch/qijing.huang/anaconda3/lib/python3.6/site-packages/ray/rllib/rollout.py", line 103, in run
    rollout(agent, args.env, num_steps, args.out, args.no_render)
  File "/scratch/qijing.huang/anaconda3/lib/python3.6/site-packages/ray/rllib/rollout.py", line 134, in rollout
    state, state=state_init)
  File "/scratch/qijing.huang/anaconda3/lib/python3.6/site-packages/ray/rllib/agents/agent.py", line 380, in compute_action
    filtered_obs, state, prev_action, prev_reward, info)
  File "/scratch/qijing.huang/anaconda3/lib/python3.6/site-packages/ray/rllib/evaluation/policy_graph.py", line 115, in compute_single_action
    episodes=episodes)
  File "/scratch/qijing.huang/anaconda3/lib/python3.6/site-packages/ray/rllib/evaluation/tf_policy_graph.py", line 164, in compute_actions
    return builder.get(fetches)
  File "/scratch/qijing.huang/anaconda3/lib/python3.6/site-packages/ray/rllib/utils/tf_run_builder.py", line 50, in get
    self.fetches, self.feed_dict))
ValueError: Error fetching: [&lt;tf.Tensor 'default/Squeeze:0' shape=(?,) dtype=int64&gt;, &lt;tf.Tensor 'default/rnn/while/Exit_3:0' shape=(?, 256) dtype=float32&gt;, &lt;tf.Tensor 'default/rnn/while/Exit_4:0' shape=(?, 256) dtype=float32&gt;, {'vf_preds': &lt;tf.Tensor 'default/value_function/Reshape:0' shape=(?,) dtype=float32&gt;, 'logits': &lt;tf.Tensor 'default/add:0' shape=(?, 45) dtype=float32&gt;}], feed_dict={&lt;tf.Tensor 'default/obs:0' shape=(?, 112) dtype=float32&gt;: [array([-1.06409477e-01, -8.50873256e-01, -9.99063163e-01, -9.85599544e-01,
       -5.42063368e-01, -1.06124935e+00, -1.08591644e+00, -5.42505840e-01,
       -8.86472780e-01, -8.46849891e-01, -1.88844748e-01, -8.50973328e-01,
       -1.63467521e-02, -1.15469912e+00, -8.50939778e-01, -1.08479177e+00,
       -5.78306000e-01, -6.70322766e-01, -1.07181469e+00, -1.39179516e+00,
       -5.86559290e-01, -1.11476158e+00, -1.10322075e+00, -1.06124935e+00,
       -1.51273882e+00, -4.02534189e-01, -7.29427538e-01, -4.38874107e-01,
       -7.66750215e-01, -2.89747194e-01, -1.14888772e+00, -2.46290114e-01,
       -1.08479177e+00, -1.24305280e+00, -5.95312915e-01, -7.27312541e-01,
       -1.29177601e+00, -7.11755635e-01, -4.59420264e-01,  1.43678153e+00,
       -8.77652619e-01, -1.08427663e+00, -5.02966723e-01, -1.68211479e-01,
        1.89750196e+00, -6.58439861e-01,  4.38707719e-01, -9.75121514e-01,
       -8.08009806e-01, -4.53405398e-01, -1.17154706e+00, -1.19055232e+00,
       -9.37596900e-01, -1.08494410e+00, -9.22778281e-01, -7.81024400e-01,
       -2.39592654e+00, -8.85376176e-01, -7.08556097e-01, -6.73350187e-01,
       -7.10970719e-01, -6.99164824e-01, -1.35108477e+00, -7.56155257e-01,
       -7.45227289e-01, -1.30239778e+00, -1.75812026e+00, -8.86152880e-01,
       -1.87926967e+01, -8.50411468e-01, -8.86159223e-01, -8.78202538e-01,
       -1.00983408e+00, -1.07252822e+00, -9.67455750e-01, -5.91927078e-01,
       -2.20402553e-01, -9.13439382e-01, -1.72068073e-01, -6.99164824e-01,
       -7.43725385e-01, -1.05667716e+00, -9.08485357e-01,  6.35904671e-01,
       -3.19841059e-01, -2.25343011e-02, -9.79711260e-01, -2.25463516e-01,
       -8.78202538e-01, -8.65142208e-01, -3.70832579e-01, -1.18761885e+00,
       -7.78508220e-01,  3.85444973e-01, -2.88126371e+00, -1.02081353e+00,
       -9.05109081e-01, -1.24851826e+00, -4.38869085e-01, -4.23549805e-01,
       -2.30324403e+00,  4.74428253e-01, -2.01533446e+00, -8.46008159e-01,
       -6.04664742e-01, -3.73717406e-01, -9.94018194e-01, -4.84346407e-01,
        2.81080886e-01, -1.25895485e+00, -8.19207488e-01,  2.72670206e-01])], &lt;tf.Tensor 'default/seq_lens_1:0' shape=(?,) dtype=int32&gt;: array([1.]), &lt;tf.Tensor 'default/PlaceholderWithDefault:0' shape=() dtype=bool&gt;: False, &lt;tf.Tensor 'default/c:0' shape=(?, 256) dtype=float32&gt;: [array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0.], dtype=float32)], &lt;tf.Tensor 'default/h:0' shape=(?, 256) dtype=float32&gt;: [array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0.], dtype=float32)]}
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='AmeerHajAli' date='2019-03-12T01:13:20Z'>
		Ah this seems to be since we don't pass in prev_action and prev_reward to agent.compute_action() in the rollout.py file. Would you have time to submit a fix here?
		</comment>
		<comment id='2' author='AmeerHajAli' date='2019-04-06T17:55:09Z'>
		Ray 0.6.5 in Google Colab.
I'm actually getting a similar error except with the seq_lens_1:
tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'default/q_func/seq_lens_1' with dtype int32 and shape [?] [[{{node default/q_func/seq_lens_1}}]]
with this setup:
'run_experiments({
"demo": {
"run": "DQN",
"env": "CartPole-v1",
"stop": {
#"episode_reward_mean": 0.001,
#"training_iteration": 1,
#"timesteps_total": 1000,
"episodes_total": 1,
},
"config": {
"num_gpus": 1,
#"num_workers": 1,
#"lr": 0.0001,
#"observation_filter": "MeanStdFilter",
#"env_config": envconfig,
&lt;denchmark-code&gt;        # === MODEL ===
        "model": {"use_lstm": True},
    }
}
&lt;/denchmark-code&gt;

})'
I have my own environment I'm using, but was able to reproduce using this CartPole env. Any idea how I can fix this?
		</comment>
		<comment id='3' author='AmeerHajAli' date='2019-04-06T22:27:56Z'>
		RNN policies aren't currently supported in DQN (this is being worked on in
the R2D2 issue).
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Sat, Apr 6, 2019, 10:55 AM Evan Walters ***@***.***&gt; wrote:
 Ray 0.6.5 in Google Colab.

 I'm actually getting a similar error except with the seq_lens_1:

 tensorflow.python.framework.errors_impl.InvalidArgumentError: You must
 feed a value for placeholder tensor 'default/q_func/seq_lens_1' with dtype
 int32 and shape [?] [[{{node default/q_func/seq_lens_1}}]]

 with this setup:
 'run_experiments({
 "demo": {
 "run": "DQN",
 "env": "CartPole-v1",
 "stop": {
 #"episode_reward_mean": 0.001,
 #"training_iteration": 1,
 #"timesteps_total": 1000,
 "episodes_total": 1,
 },
 "config": {
 "num_gpus": 1,
 #"num_workers": 1,
 #"lr": 0.0001,
 #"observation_filter": "MeanStdFilter",
 #"env_config": envconfig,

         # === MODEL ===
         "model": {"use_lstm": True},
     }
 }

 })'

 I have my own environment I'm using, but was able to reproduce using this
 CartPole env. Any idea how I can fix this?

 —
 You are receiving this because you were assigned.
 Reply to this email directly, view it on GitHub
 &lt;#4307 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AAA6Stvrl4Nw4W_EWPXxc5wJaQwY7fKIks5veN-EgaJpZM4blBgW&gt;
 .



		</comment>
		<comment id='4' author='AmeerHajAli' date='2019-04-07T22:46:34Z'>
		Ahh I see. Awesome, can’t wait for r2d2.
		</comment>
	</comments>
</bug>