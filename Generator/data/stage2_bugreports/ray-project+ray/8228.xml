<bug id='8228' author='pmacalpine' open_date='2020-04-29T08:19:23Z' closed_time='2020-05-05T19:36:43Z'>
	<summary>[rllib] Wrong number of steps per iteration using experimental distributed execution API</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

When using the experimental distributed execution API (on by default) an iteration isn't ended when timesteps_per_iteration steps have occurred, but instead only when at least timesteps_per_iteration+1 steps have occurred.  If I turn off the experimental distributed execution API (use_exec_api = False) the iterations are correctly ended when timesteps_per_iteration steps have occurred.
I believe this problem is happening because this &lt;denchmark-link:https://github.com/ray-project/ray/blob/2298f6fb40cb6d348b5b48593c93cdd58ddd1f29/rllib/execution/metric_ops.py#L162&gt;code&lt;/denchmark-link&gt;
 should be "&gt;=" not "&gt;".
Ray version and other system information (Python version, TensorFlow version, OS):
Running on Azure NC6s_v2 machine
ray: 0.9.0.dev0 (latest wheels installed on 4/29/20)
python: 3.7.6
tensorflow: 2.1
torch: 1.4.0
OS: Ubuntu 16.04.6 LTS
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
I'm running the following sample code using A2C with a batch size of 200 and timesteps_per_iteration = 1000 so iterations should each be 1000 steps, however iterations are always 1200 steps.  If I set use_exec_api = False iterations are 1000 steps which is correct.  Note that I have min_iter_time_s = 0 so time isn't an issue.
If I change this &lt;denchmark-link:https://github.com/ray-project/ray/blob/2298f6fb40cb6d348b5b48593c93cdd58ddd1f29/rllib/execution/metric_ops.py#L162&gt;code&lt;/denchmark-link&gt;
 to be "&gt;=" instead of "&gt;" the problem is fixed.
"""Example of a custom gym environment and model. Run this for a demo.

This example shows:
  - using a custom environment
  - using a custom model
  - using Tune for grid search

You can visualize experiment results in ~/ray_results using TensorBoard.
"""

import numpy as np
import gym
from ray.rllib.models import ModelCatalog
from ray.rllib.models.tf.tf_modelv2 import TFModelV2
from ray.rllib.models.tf.fcnet_v2 import FullyConnectedNetwork
from gym.spaces import Discrete, Box

import ray
from ray import tune
from ray.rllib.utils import try_import_tf
from ray.tune import grid_search

tf = try_import_tf()


class SimpleCorridor(gym.Env):
    """Example of a custom env in which you have to walk down a corridor.

    You can configure the length of the corridor via the env config."""

    def __init__(self, config):
        self.end_pos = config["corridor_length"]
        self.cur_pos = 0
        self.action_space = Discrete(2)
        self.observation_space = Box(
            0.0, self.end_pos, shape=(1, ), dtype=np.float32)

    def reset(self):
        self.cur_pos = 0
        return [self.cur_pos]

    def step(self, action):
        assert action in [0, 1], action
        if action == 0 and self.cur_pos &gt; 0:
            self.cur_pos -= 1
        elif action == 1:
            self.cur_pos += 1
        done = self.cur_pos &gt;= self.end_pos
        return [self.cur_pos], 1 if done else 0, done, {}


class CustomModel(TFModelV2):
    """Example of a custom model that just delegates to a fc-net."""

    def __init__(self, obs_space, action_space, num_outputs, model_config,
                 name):
        super(CustomModel, self).__init__(obs_space, action_space, num_outputs,
                                          model_config, name)
        self.model = FullyConnectedNetwork(obs_space, action_space,
                                           num_outputs, model_config, name)
        self.register_variables(self.model.variables())

    def forward(self, input_dict, state, seq_lens):
        return self.model.forward(input_dict, state, seq_lens)

    def value_function(self):
        return self.model.value_function()


if __name__ == "__main__":
    # Can also register the env creator function explicitly with:
    # register_env("corridor", lambda config: SimpleCorridor(config))
    ray.init()
    ModelCatalog.register_custom_model("my_model", CustomModel)
    tune.run(
        "A2C",
        stop={
            "timesteps_total": 10000,
        },
        config={
            "env": SimpleCorridor,  # or "corridor" if registered above
            "model": {
                "custom_model": "my_model",
            },
            "lr": 1e-2,
            "num_workers": 1,  # parallelism
            "env_config": {
                "corridor_length": 5,
            },
            "min_iter_time_s": 0,
            "timesteps_per_iteration": 1000,
            "train_batch_size": 200,
            "use_exec_api": True,
        },
        verbose=1,
    )
If we cannot run your script, we cannot fix your issue.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='pmacalpine' date='2020-04-29T19:38:09Z'>
		Is the "&gt;=" fix enough or are there any other issues?
		</comment>
		<comment id='2' author='pmacalpine' date='2020-04-29T19:54:25Z'>
		I think the  "&gt;=" fix is all there is to it -- it fixes the problem for me.
		</comment>
		<comment id='3' author='pmacalpine' date='2020-04-29T20:00:35Z'>
		Ok awesome, I rolled it into &lt;denchmark-link:https://github.com/ray-project/ray/pull/8221&gt;#8221&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>