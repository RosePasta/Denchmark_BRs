<bug id='11670' author='andrewredd' open_date='2020-10-28T13:10:45Z' closed_time='2020-11-15T05:41:12Z'>
	<summary>[tune] Schedulers should automatically handle nans/inf values</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

I get the following when I run the following code
&lt;denchmark-code&gt;  def tune_asha(num_samples=400, num_epochs=25, gpus_per_trial=0.1):
        config = {
            "logger_config": {"mlflow_experiment_id": experiment_id},
            "batch_size": 1024,
            "learning_rate": tune.uniform(1e-5, 1e-3),
            "layer1_2_size": tune.uniform(128, 512),
            "layer3": tune.uniform(64, 256),
            "layer4": tune.uniform(32, 128),
            "layer5": tune.uniform(16, 64),
            "layer1_2_d": tune.uniform(0.01, 0.2),
            "layer3_4_5_d": tune.uniform(0.01, 0.05),
            "emb_dropout": tune.uniform(0.01, 0.1),
            "emb_max": tune.uniform(64, 256),
            "gradient_clipping": tune.uniform(5, 500),
            "accul_batches": 10,
            "optimizer_weight_decay": tune.uniform(0, 0.001),
            "lr_scheduler_patience": tune.uniform(1, 5),
            "lr_scheduler_reduction_factor": tune.uniform(0.1, 0.25),
        }

        scheduler = ASHAScheduler(
            metric="loss",
            mode="min",
            max_t=num_epochs,
            grace_period=7,
            reduction_factor=2,
        )

        reporter = CLIReporter(
            parameter_columns=[
                "layer1_2_size",
                "layer3",
                "layer4",
                "layer5",
                "learning_rate",
                "batch_size",
            ],
            metric_columns=["loss", "training_iteration"],
        )

        algo = BayesOptSearch(
            metric="loss",
            mode="min",
            utility_kwargs={"kind": "ucb", "kappa": 2.5, "xi": 0.0},
            random_search_steps=30,
        )
        algo = ConcurrencyLimiter(algo, max_concurrent=10)

        return tune.run(
            partial(train_tune, num_epochs=num_epochs, num_gpus=gpus_per_trial),
            name=experiment_name,
            resources_per_trial={"cpu": 1, "gpu": gpus_per_trial},
            config=config,
            local_dir=".",
            search_alg=algo,
            scheduler=scheduler,
            num_samples=num_samples,
            progress_reporter=reporter,
            loggers=DEFAULT_LOGGERS + (MLFLowLogger,),
        )
&lt;/denchmark-code&gt;

When it's seeding the space there are no issues but as soon as it is on trial 30, I get the following error
&lt;denchmark-code&gt;  File "tune_r_learner_bo.py", line 116, in &lt;module&gt;
    main()
  File "tune_r_learner_bo.py", line 110, in main
    expr_obj = tune_asha()
  File "tune_r_learner_bo.py", line 107, in tune_asha
    loggers=DEFAULT_LOGGERS + (MLFLowLogger,),
  File "/opt/conda/lib/python3.7/site-packages/ray/tune/tune.py", line 411, in run
    runner.step()
  File "/opt/conda/lib/python3.7/site-packages/ray/tune/trial_runner.py", line 561, in step
    next_trial = self._get_next_trial()  # blocking
  File "/opt/conda/lib/python3.7/site-packages/ray/tune/trial_runner.py", line 658, in _get_next_trial
    self._update_trial_queue(blocking=wait_for_trial)
  File "/opt/conda/lib/python3.7/site-packages/ray/tune/trial_runner.py", line 1011, in _update_trial_queue
    trial = self._search_alg.next_trial()
  File "/opt/conda/lib/python3.7/site-packages/ray/tune/suggest/search_generator.py", line 114, in next_trial
    self._experiment.dir_name)
  File "/opt/conda/lib/python3.7/site-packages/ray/tune/suggest/search_generator.py", line 121, in create_trial_if_possible
    suggested_config = self.searcher.suggest(trial_id)
  File "/opt/conda/lib/python3.7/site-packages/ray/tune/suggest/suggestion.py", line 346, in suggest
    suggestion = self.searcher.suggest(trial_id)
  File "/opt/conda/lib/python3.7/site-packages/ray/tune/suggest/bayesopt.py", line 257, in suggest
    config = self.optimizer.suggest(self.utility)
  File "/opt/conda/lib/python3.7/site-packages/bayes_opt/bayesian_optimization.py", line 128, in suggest
    self._gp.fit(self._space.params, self._space.target)
  File "/opt/conda/lib/python3.7/site-packages/sklearn/gaussian_process/_gpr.py", line 190, in fit
    ensure_2d=True, dtype="numeric")
  File "/opt/conda/lib/python3.7/site-packages/sklearn/base.py", line 432, in _validate_data
    X, y = check_X_y(X, y, **check_params)
  File "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py", line 72, in inner_f
    return f(**kwargs)
  File "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py", line 805, in check_X_y
    ensure_2d=False, dtype=None)
  File "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py", line 72, in inner_f
    return f(**kwargs)
  File "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py", line 645, in check_array
    allow_nan=force_all_finite == 'allow-nan')
  File "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py", line 99, in _assert_all_finite
    msg_dtype if msg_dtype is not None else X.dtype)
ValueError: Input contains NaN, infinity or a value too large for dtype('float64').
&lt;/denchmark-code&gt;

My neural network is capable of inf and Nan losses but will not always generate these values. Further I would expect there to be a workaround if it does produce a Nan I just haven't been able to find it.
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
I'm in a docker container that is based off the latest pytorch image.
I'm building Ray from the latest wheel (or the latest from Monday)
&lt;denchmark-link:https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-1.1.0.dev0-cp37-cp37m-manylinux1_x86_64.whl&gt;https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-1.1.0.dev0-cp37-cp37m-manylinux1_x86_64.whl&lt;/denchmark-link&gt;

ray==1.1.0.dev0
Python 3.7.7
bayesian-optimization==1.2.0
If we cannot run your script, we cannot fix your issue.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='andrewredd' date='2020-10-28T18:07:30Z'>
		&lt;denchmark-link:https://github.com/andrewredd&gt;@andrewredd&lt;/denchmark-link&gt;
 thanks a bunch for raising this issue! We'll be sure to get to this in our 1.0.2 release.
		</comment>
		<comment id='2' author='andrewredd' date='2020-10-29T00:28:01Z'>
		Thanks Richard, do you know what the rough timeline for 1.0.2 would be?
&lt;denchmark-link:#&gt;‚Ä¶&lt;/denchmark-link&gt;


On Wed, Oct 28, 2020 at 2:07 PM Richard Liaw ***@***.***&gt; wrote:
 @andrewredd &lt;https://github.com/andrewredd&gt; thanks a bunch for raising
 this issue! We'll be sure to get to this in our 1.0.2 release.

 ‚Äî
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#11670 (comment)&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/ALQO4BW5G66ZC6Q5YTCHSIDSNBMXDANCNFSM4TCJVTDQ&gt;
 .



		</comment>
		<comment id='3' author='andrewredd' date='2020-10-29T00:40:45Z'>
		In a month. However, &lt;denchmark-link:https://github.com/andrewredd&gt;@andrewredd&lt;/denchmark-link&gt;
 can you you filter out inf/nan values for now? i.e., if a value is nan or inf, just set it to a very high or very low number?
		</comment>
		<comment id='4' author='andrewredd' date='2020-10-29T21:28:54Z'>
		thanks &lt;denchmark-link:https://github.com/richardliaw&gt;@richardliaw&lt;/denchmark-link&gt;
 that resolved my question
		</comment>
		<comment id='5' author='andrewredd' date='2020-10-29T23:16:41Z'>
		let's keep this open so we can track a fix for improving the error message :)
		</comment>
		<comment id='6' author='andrewredd' date='2020-10-29T23:36:22Z'>
		üëç
&lt;denchmark-link:#&gt;‚Ä¶&lt;/denchmark-link&gt;


On Thu, Oct 29, 2020 at 7:16 PM Richard Liaw ***@***.***&gt; wrote:
 let's keep this open so we can track a fix for improving the error message
 :)

 ‚Äî
 You are receiving this because you modified the open/close state.
 Reply to this email directly, view it on GitHub
 &lt;#11670 (comment)&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/ALQO4BTBH56JHMYKTLVI4X3SNHZWPANCNFSM4TCJVTDQ&gt;
 .



		</comment>
		<comment id='7' author='andrewredd' date='2020-11-09T19:19:47Z'>
		(opening to address Schedulers)
		</comment>
		<comment id='8' author='andrewredd' date='2020-11-09T21:44:52Z'>
		I tested &lt;denchmark-link:https://github.com/ray-project/ray/pull/11835&gt;#11835&lt;/denchmark-link&gt;
 with schedulers and it seems to work fine. Added unit tests for this as well: &lt;denchmark-link:https://github.com/ray-project/ray/pull/11835/files#diff-3883e3ca074a1c1aa4339e6a35f1baeff54eac485cacf65522b650a5df16c063&gt;https://github.com/ray-project/ray/pull/11835/files#diff-3883e3ca074a1c1aa4339e6a35f1baeff54eac485cacf65522b650a5df16c063&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='andrewredd' date='2020-11-09T22:36:04Z'>
		I think the schedulers will run, but I'm not sure if the behavior is expected (i.e., everything becomes a nan?)
		</comment>
		<comment id='10' author='andrewredd' date='2020-11-15T05:41:12Z'>
		Well, I'll close this for now.
		</comment>
		<comment id='11' author='andrewredd' date='2020-11-16T13:47:59Z'>
		Hm right, schedulers should filter out NaNs for quantile calculation etc. Let's discuss intended behavior here - if a trial reports NaN e.g. to Hyperband, should it continue to run (assuming it might recover) or stop (assuming it is broken)? It's straightforward for searchers to just ignore results, but schedulers have to make a decision.
Some possibilities:

Stop trial immediately
Assign a large negative (for mode=max) or position (for mode=min) value, so the trial might be filtered next time (e.g. in Hyperband) and won't be exploited (in PBT)
Do nothing (like right now) - might lead to weird results if inf is reported. Here we would put the burden on the user to only report valid values.

It seems low priority since this is a user output validation issue, but I'd opt for option 2 with the possibility to disable this through an ENV variable. It should also log_once if this occurs.
		</comment>
	</comments>
</bug>