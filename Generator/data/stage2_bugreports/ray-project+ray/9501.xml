<bug id='9501' author='vinhdiesal' open_date='2020-07-15T12:07:36Z' closed_time='2020-07-16T00:24:45Z'>
	<summary>[ray][tune] Not using all resources for distributed training.</summary>
	<description>
I manually created a Ray cluster with the following hardware:

Alienware - 2 NVIDIA GPUs
Razer - 2 NVIDIA GPUs
IMAC
The ray cluster shows that I have 4 GPUs and 22 Cores totaling up to 33 workers. I want to use distribute the training with all my machines in the cluster.


In the resources I tried to use all CPUs and GPUs in the ray tune as follows:
&lt;denchmark-code&gt;tune.run(
        train_mnist,
        name="exp",
        stop={
            "mean_accuracy": 0.99,
            "training_iteration": 5 if args.smoke_test else 300
        },
        num_samples=10,
        resources_per_trial={
            "cpu": 22,
            "gpu": 4
        },
        config={
            "threads": 2,
            "lr": tune.sample_from(lambda spec: np.random.uniform(0.001, 0.1)),
            "momentum": tune.sample_from(
                lambda spec: np.random.uniform(0.1, 0.9)),
            "hidden": tune.sample_from(
                lambda spec: np.random.randint(32, 512)),
        })
&lt;/denchmark-code&gt;

However, when I get the following error and it doesn't run:
&lt;denchmark-code&gt;2020-07-15 05:00:28,947	WARNING worker.py:1047 -- The actor or task with ID ffffffffffffffffcb3a83890600 is infeasible and cannot currently be scheduled. It requires {CPU: 22.000000}, {GPU: 4.000000} for execution and {CPU: 22.000000}, {GPU: 4.000000} for placement, however there are no nodes in the cluster that can provide the requested resources. To resolve this issue, consider reducing the resource requests of this task or add nodes that can fit the task.
&lt;/denchmark-code&gt;

When I reduced the resources to 10 CPUs and 2 GPUs, it runs, but it only utilizes one node which is the Alienware and the other nodes are idle as can be seen by the screenshot.
How can I train using the tune.run function to use all my resources in training.
Thanks,
vinhdiesal
	</description>
	<comments>
		<comment id='1' author='vinhdiesal' date='2020-07-15T15:43:36Z'>
		You are requesting 22 CPUs and 4 GPUs for each trial. If you lower it to the number of CPUs and GPUs needed for each trial independently (which we might be able to help you determine if you share the actual training code), then Ray can start more jobs.
Trials can only be scheduled on a single node (though you can call remote functions from within a trial that can be queued independently).
		</comment>
		<comment id='2' author='vinhdiesal' date='2020-07-15T15:44:31Z'>
		When you reduced the resources to 10/2, you are still asking for each trial to have 10 CPUs which only your Alienware node is able to provide. Hence, it is the only node being utilized.
		</comment>
		<comment id='3' author='vinhdiesal' date='2020-07-15T19:08:07Z'>
		The training code is just a CNN for MNIST, please show how you can allocate resources independently to run more jobs.
I want to use all my machines.
&lt;denchmark-code&gt;def train_mnist(config):
    # https://github.com/tensorflow/tensorflow/issues/32159
    import tensorflow as tf
    batch_size = 128
    num_classes = 10
    epochs = 12
    IMG_ROWS, IMG_COLS = 28, 28
    INPUT_SHAPE = (IMG_ROWS, IMG_COLS, 1)
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    x_train = x_train.reshape((60000, 28, 28, 1))
    x_test = x_test.reshape((10000, 28, 28, 1))
    x_train, x_test = x_train / 255.0, x_test / 255.0
    model = models.Sequential()
    model.add(layers.Convolution2D(20, (5,5), activation = 'relu', input_shape = INPUT_SHAPE))
    model.add(layers.MaxPooling2D(pool_size = (2,2), strides =(2,2)))
    model.add(layers.Convolution2D(50, (5,5), activation = 'relu'))
    model.add(layers.MaxPooling2D(pool_size = (2,2), strides = (2,2)))
    model.add(layers.Flatten())
    model.add(layers.Dense(500, activation = 'relu'))
    model.add(layers.Dense(num_classes, activation = "softmax"))

    model.compile(
        loss="sparse_categorical_crossentropy",
        optimizer=tf.keras.optimizers.SGD(
            lr=config["lr"], momentum=config["momentum"]),
        metrics=["accuracy"])
    callbacks = tf.keras.callbacks.TensorBoard(log_dir = '.logs')
    model.fit(
        x_train,
        y_train,
        batch_size=batch_size,
        epochs=epochs,
        verbose=0,
        validation_data=(x_test, y_test),
        callbacks=[TuneReporterCallback()])
%%time
if __name__ == "__main__":
    import ray
    from ray import tune
    from ray.tune.schedulers import AsyncHyperBandScheduler
    mnist.load_data()  # we do this on the driver because it's not threadsafe
    ray.init(address='auto')
       
    tune.run(
        train_mnist,
        name="exp",
        stop={
            "mean_accuracy": 0.99,
            "training_iteration": 5 if args.smoke_test else 300
        },
        num_samples=10,
        resources_per_trial={
            "cpu": 22,
            "gpu": 4
        },
        config={
            "threads": 2,
            "lr": tune.sample_from(lambda spec: np.random.uniform(0.001, 0.1)),
            "momentum": tune.sample_from(
                lambda spec: np.random.uniform(0.1, 0.9)),
            "hidden": tune.sample_from(
                lambda spec: np.random.randint(32, 512)),
        })
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='vinhdiesal' date='2020-07-15T19:32:27Z'>
		Your code as written works if you reduce the resources allocated to each trial.
Determining the optimal resources per trial is something of an art form. From your code, it looks like TensorFlow is not configured to make use of more than one GPU per trial. Thus, it would make sense to me to allocate one or two CPUs and one GPU per trial.
		</comment>
		<comment id='5' author='vinhdiesal' date='2020-07-15T19:36:37Z'>
		Also, you are passing a config parameter called threads which I assume is supposed to limit the number of threads TensorFlow uses. However, you never use that parameter in the code, so TensorFlow will end up using all cores. That's probably not an issue if you are using GPUs.
Also, if you are using GPUs for training then the IMAC cluster will not be used--nor should it be--since it has no GPUs attached.
		</comment>
	</comments>
</bug>