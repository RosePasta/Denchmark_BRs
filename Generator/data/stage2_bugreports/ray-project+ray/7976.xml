<bug id='7976' author='annaluo676' open_date='2020-04-11T05:21:34Z' closed_time='2020-04-16T23:06:59Z'>
	<summary>[core][rllib] Decreasing CPU utilization with Ray 0.8.4</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

Ray version: 0.8.4
TensorFlow version: 2.1.0
CPU utilization kept decreasing during an ARS job training.  See below for the metric on a ec2 p2.x16large instance.
&lt;denchmark-link:https://user-images.githubusercontent.com/45078924/79036017-b919f700-7b78-11ea-99d0-b08859d55a5c.png&gt;&lt;/denchmark-link&gt;

&gt; ray memory showed 31 workers pinned in memory, which was expected. There are 124 local references, mostly at ray/rllib/utils/filter_manager.py:&lt;listcomp&gt;:25, ray/rllib/agents/ars/ars.py:&lt;listcomp&gt;:193 and ray/rllib/agents/ars/ars.py:&lt;listcomp&gt;:314. Seems that discounted reference counting didn't cause objects to explode.
Besides, Ray dashboard showed almost all Worker instead of Worker.do_rollout().
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Script to reproduce:
&lt;denchmark-code&gt;import ray
import os
from ray.rllib.agents.trainer import with_common_config
from ray.rllib.agents.ars.ars import ARSTrainer

gpus = [str(i) for i in list(range(12,16))]
cpus = 32
os.environ["CUDA_VISIBLE_DEVICES"] = ','.join(gpus)

ray.init(
    num_cpus=cpus,
    num_gpus=len(gpus),
    object_store_memory=int(1.5e11))

ars_config = with_common_config({
    "noise_stdev": 0.02,
    "num_rollouts": 50,
    "rollouts_used": 25,
    "num_workers": 31,
    "sgd_stepsize": 0.01,
    "noise_size": 250000000,
    "eval_prob": 0.5,
    "model":{
        "fcnet_hiddens": []  # a linear policy
        }
    })

agent = ARSTrainer(env="CartPole-v0", 
                   config=ars_config)

n_iter = int(1e7)
for i in range(n_iter):    
    result = agent.train()
&lt;/denchmark-code&gt;

If we cannot run your script, we cannot fix your issue.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='annaluo676' date='2020-04-11T05:53:16Z'>
		0.8.3 also exhibits this lazy worker syndrome; Ray 0.8.2 doesn't.
		</comment>
		<comment id='2' author='annaluo676' date='2020-04-11T06:39:19Z'>
		Thanks anna!
		</comment>
		<comment id='3' author='annaluo676' date='2020-04-15T06:10:56Z'>
		I can also observe from RLlib's internal metrics, time per iteration gradually increases and CPU load decreases.
&lt;denchmark-link:https://user-images.githubusercontent.com/14922/79304002-285b5800-7ea5-11ea-9f44-e54fcb7c0b4c.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='annaluo676' date='2020-04-15T20:23:03Z'>
		perf top shows
&lt;denchmark-code&gt;Overhead  Shared Object                                      Symbol
  10.96%  _raylet.so                                         [.] ray::RayObject::IsInPlasmaError
   5.12%  _raylet.so                                         [.] ray::CoreWorkerMemoryStore::GetMemoryStoreStatisticalData
   4.85%  perf                                               [.] __symbols__insert
   3.17%  python3.6                                          [.] _PyEval_EvalFrameDefault
   2.86%  libstdc++.so.6.0.26                                [.] std::string::_M_copy
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='annaluo676' date='2020-04-15T20:24:11Z'>
		This is pointing to a leak in the number of objects in the memory store.
		</comment>
		<comment id='6' author='annaluo676' date='2020-04-15T22:57:11Z'>
		&lt;denchmark-link:https://github.com/annaluo676&gt;@annaluo676&lt;/denchmark-link&gt;
 just tested and this patch appears to resolve the slowdown problems.
		</comment>
		<comment id='7' author='annaluo676' date='2020-04-16T00:16:57Z'>
		Thanks &lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
. According to &lt;denchmark-link:https://ray.readthedocs.io/en/latest/development.html#compilation&gt;https://ray.readthedocs.io/en/latest/development.html#compilation&lt;/denchmark-link&gt;
 I will need to recompile the c++ code changes.  emitted the following error:
&lt;denchmark-code&gt;INFO: Writing tracer profile to '/home/ubuntu/.cache/bazel/_bazel_ubuntu/4694ec92a350130755ebd31627e14086/command.profile.gz'
ERROR: Skipping '//:ray_pkg': no such package '': error globbing [python/ray/*.py, python/ray/autoscaler/*.py, python/ray/autoscaler/aws/example-full.yaml, python/ray/autoscaler/azure/example-full.yaml, python/ray/autoscaler/gcp/example-full.yaml, python/ray/autoscaler/local/example-full.yaml, python/ray/cloudpickle/*.py, python/ray/core/__init__.py, python/ray/core/generated/__init__.py, python/ray/core/generated/ray/__init__.py, python/ray/core/generated/ray/protocol/__init__.py, python/ray/dashboard/*.py, python/ray/dashboard/metrics_exporter/*.py, python/ray/experimental/*.py, python/ray/util/*.py, python/ray/internal/*.py, python/ray/projects/*.py, python/ray/projects/schema.json, python/ray/projects/templates/cluster_template.yaml, python/ray/projects/templates/project_template.yaml, python/ray/projects/templates/requirements.txt, python/ray/workers/default_worker.py]: /efs/annaluo/ray/python/ray/autoscaler (Too many levels of symbolic links)
WARNING: Target pattern parsing failed.
ERROR: no such package '': error globbing [python/ray/*.py, python/ray/autoscaler/*.py, python/ray/autoscaler/aws/example-full.yaml, python/ray/autoscaler/azure/example-full.yaml, python/ray/autoscaler/gcp/example-full.yaml, python/ray/autoscaler/local/example-full.yaml, python/ray/cloudpickle/*.py, python/ray/core/__init__.py, python/ray/core/generated/__init__.py, python/ray/core/generated/ray/__init__.py, python/ray/core/generated/ray/protocol/__init__.py, python/ray/dashboard/*.py, python/ray/dashboard/metrics_exporter/*.py, python/ray/experimental/*.py, python/ray/util/*.py, python/ray/internal/*.py, python/ray/projects/*.py, python/ray/projects/schema.json, python/ray/projects/templates/cluster_template.yaml, python/ray/projects/templates/project_template.yaml, python/ray/projects/templates/requirements.txt, python/ray/workers/default_worker.py]: /efs/annaluo/ray/python/ray/autoscaler (Too many levels of symbolic links)
&lt;/denchmark-code&gt;

The error seems to be caused by the symlinks for Rllib. Is this an issue?
		</comment>
		<comment id='8' author='annaluo676' date='2020-04-16T00:18:45Z'>
		I think the right way is pip install -e . In the python directory, not
calling that script.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Wed, Apr 15, 2020, 5:17 PM Anna Luo ***@***.***&gt; wrote:
 Thanks @ericl &lt;https://github.com/ericl&gt;. According to
 https://ray.readthedocs.io/en/latest/development.html#compilation I will
 need to recompile the c++ code changes. bash build.sh emitted the
 following error:

 INFO: Writing tracer profile to '/home/ubuntu/.cache/bazel/_bazel_ubuntu/4694ec92a350130755ebd31627e14086/command.profile.gz'
 ERROR: Skipping '//:ray_pkg': no such package '': error globbing [python/ray/*.py, python/ray/autoscaler/*.py, python/ray/autoscaler/aws/example-full.yaml, python/ray/autoscaler/azure/example-full.yaml, python/ray/autoscaler/gcp/example-full.yaml, python/ray/autoscaler/local/example-full.yaml, python/ray/cloudpickle/*.py, python/ray/core/__init__.py, python/ray/core/generated/__init__.py, python/ray/core/generated/ray/__init__.py, python/ray/core/generated/ray/protocol/__init__.py, python/ray/dashboard/*.py, python/ray/dashboard/metrics_exporter/*.py, python/ray/experimental/*.py, python/ray/util/*.py, python/ray/internal/*.py, python/ray/projects/*.py, python/ray/projects/schema.json, python/ray/projects/templates/cluster_template.yaml, python/ray/projects/templates/project_template.yaml, python/ray/projects/templates/requirements.txt, python/ray/workers/default_worker.py]: /efs/annaluo/ray/python/ray/autoscaler (Too many levels of symbolic links)
 WARNING: Target pattern parsing failed.
 ERROR: no such package '': error globbing [python/ray/*.py, python/ray/autoscaler/*.py, python/ray/autoscaler/aws/example-full.yaml, python/ray/autoscaler/azure/example-full.yaml, python/ray/autoscaler/gcp/example-full.yaml, python/ray/autoscaler/local/example-full.yaml, python/ray/cloudpickle/*.py, python/ray/core/__init__.py, python/ray/core/generated/__init__.py, python/ray/core/generated/ray/__init__.py, python/ray/core/generated/ray/protocol/__init__.py, python/ray/dashboard/*.py, python/ray/dashboard/metrics_exporter/*.py, python/ray/experimental/*.py, python/ray/util/*.py, python/ray/internal/*.py, python/ray/projects/*.py, python/ray/projects/schema.json, python/ray/projects/templates/cluster_template.yaml, python/ray/projects/templates/project_template.yaml, python/ray/projects/templates/requirements.txt, python/ray/workers/default_worker.py]: /efs/annaluo/ray/python/ray/autoscaler (Too many levels of symbolic links)

 The error seems to be caused by the symlinks for Rllib. Is this an issue?

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#7976 (comment)&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/AAADUSVW6CZU7WZD3NAXOLLRMZFAPANCNFSM4MF3TSCQ&gt;
 .



		</comment>
		<comment id='9' author='annaluo676' date='2020-04-17T19:23:03Z'>
		Unfortunately I was not able to recompile the whole package.  Since the the changes were merged, I can test with the latest wheel. Thanks Eric for looking into this!
		</comment>
	</comments>
</bug>