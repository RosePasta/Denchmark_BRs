<bug id='4530' author='jacooba' open_date='2019-04-01T14:14:14Z' closed_time='2019-04-07T19:11:31Z'>
	<summary>[rllib] Value Error with long RNN seq in Single Env</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
Ray installed from (source or binary): source
Ray version: 0.6.2
Python version: 3.5.5
Exact command to reproduce: (running on custom env)

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

When running with a single environment, load_data() in multi_gpu_impl.py throws an error, since self._loaded_max_seq_len is larger than self.max_per_device_batch_size. self._loaded_max_seq_len is 496 (but varies) and self.max_per_device_batch_size is 201, which is set by sgd_minibatch_size in yaml file. However, I believe self._loaded_max_seq_len should be truncated to 200, since sample_batch_size is 200 in the yaml. When I increase the number of environments per worker to 2 or 4, self._loaded_max_seq_len is always truncated to 200. For 1 env, why are my sequences (length 496) longer than 200?
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

self.max_per_device_batch_size: 201
self._loaded_max_seq_len 496
seq_batch_size: 0
len(self.devices): 1
2019-04-01 14:53:10,232	ERROR trial_runner.py:412 -- Error processing event.
Traceback (most recent call last):
File "/home/jake/ray/python/ray/tune/trial_runner.py", line 378, in _process_events
result = self.trial_executor.fetch_result(trial)
File "/home/jake/ray/python/ray/tune/ray_trial_executor.py", line 228, in fetch_result
result = ray.get(trial_future[0])
File "/home/jake/ray/python/ray/worker.py", line 2210, in get
raise value
ray.worker.RayTaskError: ray_worker (pid=131044, host=jake-Virtual-Machine)
File "/home/jake/ray/python/ray/rllib/agents/agent.py", line 274, in train
result = Trainable.train(self)
File "/home/jake/ray/python/ray/tune/trainable.py", line 151, in train
result = self._train()
File "/home/jake/ray/python/ray/rllib/agents/ppo/ppo.py", line 101, in _train
fetches = self.optimizer.step()
File "/home/jake/ray/python/ray/rllib/optimizers/multi_gpu_optimizer.py", line 177, in step
[tuples[k] for k in state_keys]))
File "/home/jake/ray/python/ray/rllib/optimizers/multi_gpu_impl.py", line 169, in load_data
"Must load at least 1 tuple sequence per device. Try "
ValueError: Must load at least 1 tuple sequence per device. Try increasing sgd_minibatch_size or reducing max_seq_len to ensure that at least one sequence fits per device.
	</description>
	<comments>
		<comment id='1' author='jacooba' date='2019-04-01T18:19:10Z'>
		Could you provide a reproduction script on a gym env here?
I think you could also try reducing the max seq len of your LSTM model?
		</comment>
		<comment id='2' author='jacooba' date='2019-04-02T08:54:45Z'>
		Hey! Thanks for getting back so quickly.
I believe reducing the max seq len of your LSTM model will fix the issue, but I am more concerned about whether there is either an underlying issue that might cause problems or a misunderstanding of how the parameters are supposed to operate. It seems like this might be unintended/inconsitent behaviour.
Here is a way to reproduce on cartpole. I've created and uploaded a zip containing two configs where the only difference is the number of envs.
This command should have a problem:
python3 train.py -f cartpole-dqn-lstm-bug.yaml
Whereas, this one should not:
python3 train.py -f cartpole-dqn-lstm-NO-bug.yaml
		</comment>
		<comment id='3' author='jacooba' date='2019-04-02T08:57:46Z'>
		&lt;denchmark-link:https://github.com/ray-project/ray/files/3033078/cartpole-dqn-lstm-bug.-.Copy.zip&gt;cartpole-dqn-lstm-bug - Copy.zip&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='jacooba' date='2019-04-04T06:14:15Z'>
		This turns out to be a bug in the check. We previously assumed the seq len would always be smaller than a minibatch.
&lt;denchmark-link:https://github.com/ray-project/ray/pull/4557/files&gt;https://github.com/ray-project/ray/pull/4557/files&lt;/denchmark-link&gt;
 changes the behaviour to automatically increase the minibatch size to fit at least one sequence.
As for why it worked when num_envs &gt; 1: this is a separate bug where sometimes RNN sequences got concatenated together across sample batches. This could be unsafe, so in &lt;denchmark-link:https://github.com/ray-project/ray/pull/4557&gt;#4557&lt;/denchmark-link&gt;
 this is also disallowed.
		</comment>
		<comment id='5' author='jacooba' date='2019-04-04T08:43:01Z'>
		Thanks for following up on this! The concatenation could certainly be unsafe. However, with multiple envs, and in general, the downside to automatic SGD minibatch resizing is that the user may not realize and it could cause training issues. Perhaps this is better than the alternative though. A Boolean could be introduced for dynamic minibatch resizing, but this may be overkill.
		</comment>
	</comments>
</bug>