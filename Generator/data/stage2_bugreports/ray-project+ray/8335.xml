<bug id='8335' author='pmacalpine' open_date='2020-05-06T05:58:05Z' closed_time='2020-05-24T21:18:12Z'>
	<summary>[rllib] pytorch error when using gpu "TypeError: can't convert CUDA tensor to numpy" in torch_policy.py</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

When using a GPU with examples/custom_torch_policy.py I get the following error:
&lt;denchmark-code&gt;2020-05-06 05:40:47,676 ERROR trial_runner.py:519 -- Trial MyCustomTrainer_CartPole-v0_00000: Error processing event.
Traceback (most recent call last):
  File "/home/patmac/.conda/envs/rllib/lib/python3.7/site-packages/ray/tune/trial_runner.py", line 467, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File "/home/patmac/.conda/envs/rllib/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py", line 431, in fetch_result
    result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)
  File "/home/patmac/.conda/envs/rllib/lib/python3.7/site-packages/ray/worker.py", line 1516, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): ray::MyCustomTrainer.train() (pid=40405, ip=172.16.226.199)
  File "python/ray/_raylet.pyx", line 450, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 404, in ray._raylet.execute_task.function_executor
  File "/home/patmac/.conda/envs/rllib/lib/python3.7/site-packages/ray/rllib/agents/trainer.py", line 502, in train
    raise e
  File "/home/patmac/.conda/envs/rllib/lib/python3.7/site-packages/ray/rllib/agents/trainer.py", line 488, in train
    result = Trainable.train(self)
  File "/home/patmac/.conda/envs/rllib/lib/python3.7/site-packages/ray/tune/trainable.py", line 261, in train
    result = self._train()
  File "/home/patmac/.conda/envs/rllib/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py", line 147, in _train
    fetches = self.optimizer.step()
  File "/home/patmac/.conda/envs/rllib/lib/python3.7/site-packages/ray/rllib/optimizers/sync_samples_optimizer.py", line 71, in step
    self.standardize_fields)
  File "/home/patmac/.conda/envs/rllib/lib/python3.7/site-packages/ray/rllib/utils/sgd.py", line 117, in do_minibatch_sgd
    }, minibatch.count)))[policy_id]
  File "/home/patmac/.conda/envs/rllib/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py", line 654, in learn_on_batch
    info_out[pid] = policy.learn_on_batch(batch)
  File "/home/patmac/.conda/envs/rllib/lib/python3.7/site-packages/ray/rllib/policy/torch_policy.py", line 235, in learn_on_batch
    assert not any(np.isnan(l.detach().numpy()) for l in loss_out)
  File "/home/patmac/.conda/envs/rllib/lib/python3.7/site-packages/ray/rllib/policy/torch_policy.py", line 235, in &lt;genexpr&gt;
    assert not any(np.isnan(l.detach().numpy()) for l in loss_out)
TypeError: can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
&lt;/denchmark-code&gt;

This was caused by the operation within the assertion at 
 added as part of &lt;denchmark-link:https://github.com/ray-project/ray/commit/a00144f746bb2ba4da5b1ddaf4954d6f5320c953&gt;a00144f&lt;/denchmark-link&gt;
.
Ray version and other system information (Python version, TensorFlow version, OS):
ray: 0.9.0.dev0 (latest wheels installed on 5/5/20)
python: 3.7.6
tensorflow: 2.1
torch: 1.4.0
OS: Ubuntu 16.04.6 LTS
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
Run &lt;denchmark-link:https://github.com/ray-project/ray/blob/master/rllib/examples/custom_torch_policy.py&gt;examples/custom_torch_policy.py&lt;/denchmark-link&gt;
 but add the entry  to the end of  for  to run with a gpu.
If we cannot run your script, we cannot fix your issue.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='pmacalpine' date='2020-05-08T11:10:04Z'>
		I can confirm the issue. Replacing line 235 of ray/rllib/policy/torch_policy.py by
&lt;denchmark-code&gt;assert not any(torch.isnan(l) for l in loss_out)
&lt;/denchmark-code&gt;

fixes the issue on my setup. It should work both with or without gpu.
		</comment>
		<comment id='2' author='pmacalpine' date='2020-05-24T21:18:11Z'>
		Thanks a lot for filing this &lt;denchmark-link:https://github.com/pmacalpine&gt;@pmacalpine&lt;/denchmark-link&gt;
 and for your help &lt;denchmark-link:https://github.com/duburcqa&gt;@duburcqa&lt;/denchmark-link&gt;
 ! This line has been removed anyways by now b/c it broke some test cases using dummy losses. Closing this issue.
		</comment>
	</comments>
</bug>