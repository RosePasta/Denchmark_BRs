<bug id='10138' author='richardliaw' open_date='2020-08-15T22:14:11Z' closed_time='2020-10-06T14:10:58Z'>
	<summary>[tune] lazily generate configurations for grid search</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

Ray version and other system information (Python version, TensorFlow version, OS):
This currently breaks on most setups.
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
&lt;denchmark-code&gt;from ray import tune


def objective(step, alpha, beta):
    return (0.1 + alpha * step / 100)**(-1) + beta * 0.1


def training_function(config):
    # Hyperparameters
    alpha, beta = config["alpha"], config["beta"]
    for step in range(10):
        # Iterative training function - can be any arbitrary training procedure.
        intermediate_score = objective(step, alpha, beta)
        # Feed the score back back to Tune.
        tune.report(mean_loss=intermediate_score)


analysis = tune.run(
    training_function,
    config={
        "alpha": tune.grid_search([0.001, 0.01, 0.1]),
        "beta": tune.choice(list(range(10000)))
    },
    num_samples=1000000)
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://stackoverflow.com/questions/63395596/ray-tune-random-search-indefinitely-many-samples&gt;https://stackoverflow.com/questions/63395596/ray-tune-random-search-indefinitely-many-samples&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='richardliaw' date='2020-09-02T16:15:35Z'>
		So the problem here lies in two places. For one, the code above does request scheduling of 3 million runs in parallel. There is no easy way to limit concurrency here, see also &lt;denchmark-link:https://github.com/ray-project/ray/issues/10256&gt;#10256&lt;/denchmark-link&gt;
.
The other thing is that the basic variant generator generates all configurations at once, and creates trial objects for all of these. This could be fixed by batching the creation, like in this proof-of-concept commit:
&lt;denchmark-link:https://github.com/krfricke/ray/commit/74ef52d8cc7e5b99e567cfbba11699547a95e842&gt;krfricke@74ef52d&lt;/denchmark-link&gt;

This would however implicitly limit concurrency to . Thus it would make sense to combine this with &lt;denchmark-link:https://github.com/ray-project/ray/issues/10256&gt;#10256&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='2' author='richardliaw' date='2020-09-02T21:44:36Z'>
		This is pretty interesting - there's actually 3 different rate-limiting steps to be aware of.

Sampling concurrency (currently set to infinite, bounded by ConcurrencyLimiter): This is important because it affects the actual optimization.
Machine level concurrency (currently set to total_resources / resources_per_trial): This is important because it prevents resource exhaustion/overloading of trials
Autoscaling queue depth (defaults to 1): This is important (later on) because it will guide autoscaling decisions.

Another important consideration is "out of band" trials. That is, it is possible for sampling concurrency = 1, but the actual number of concurrent trials being evaluated = N &gt; 1. (See Repeater, or the Tune Client API).
Another important consideration is autoscaling. We want to support the case where we have 0 GPUs in our cluster (CPU-only head node, with autoscaling workers). This will require a trial to start even when the machine level concurrency = 0 (effectively).
Therefore, my recommendation is the following:

For search algs/grid search:

Only generate trials when asked, 1 by 1.
Return None if sampling concurrency is met.


For trial runner:

Do not greedily gather/collect trials - only query up to "queue depth + machine level concurrency (derived from resources_per_trial)" trials at once.



RE: Machine level concurrency: int - having a parameter for this only makes sense for fixed cluster sizes, but also not sure how many people actually need to set this. It most likely will be overriden by sampling concurrency in all cases.
		</comment>
		<comment id='3' author='richardliaw' date='2020-10-06T14:10:58Z'>
		Closed via &lt;denchmark-link:https://github.com/ray-project/ray/pull/10802&gt;#10802&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>