<bug id='4397' author='nikola-j' open_date='2019-03-18T12:31:55Z' closed_time='2019-03-29T20:19:43Z'>
	<summary>[rllib] Remote worker envs cause memory leak</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
Ray installed from (source or binary): source
Ray version: newest master
Python version: 3.6.7
Exact command to reproduce:

import ray
import ray.tune as tune

ray.init()
tune.run_experiments({
    "my_experiment": {
        "run": "PPO",
        "env": "CartPole-v0",
        'stop': {
            'training_iteration': 3,
        },
        'num_samples': 10,
        "config": {
            "num_gpus": 1,
            "num_workers": 4,
            "num_envs_per_worker": 8,
            "remote_worker_envs": True,
            "lr": tune.grid_search([0.01, 0.001, 0.0001]),
        },
    },
})
&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

When running rllib with remote worker envs and multiple tune trials the number of workers grows constantly between each trial until rllib crashes because of OOM.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;2019-03-18 12:27:07,252	ERROR worker.py:1752 -- WARNING: X workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.
&lt;/denchmark-code&gt;

Where X increases by 48 for each new trial until I run out of memory.
	</description>
	<comments>
		<comment id='1' author='nikola-j' date='2019-03-21T04:06:11Z'>
		It looks like this is since the remote env actors aren't cleaned up. They probably could be killed with an atexit  hook or something? agent._stop() could also send remote calls to all the policy evaluators to clean up remote envs.
		</comment>
	</comments>
</bug>