<bug id='3418' author='stephanie-wang' open_date='2018-11-27T19:47:26Z' closed_time='2019-02-26T20:52:56Z'>
	<summary>Raylet dies if driver exits before a worker returns from `ray.get`</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


OS Platform and Distribution (e.g., Linux Ubuntu 16.04): all
Ray installed from (source or binary): source

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;


A worker for a job is blocked in a ray.get.
The driver for the job exits.
The raylet learns about the driver removal, and cleans up local state. This removes all tasks for the job in the local queue and kills the blocked worker.
The raylet processes the blocked worker's disconnection. It calls HandleTaskUnblocked and then fails here because the task has already been removed in step 3.

There are two ways to fix the issue. One is to just skip the calls to  if the worker is already dead. This will work, but it's brittle since any accounting usually done in  will have to be duplicated in step 3. In fact, this is already partly true for task dependency management, which is currently done &lt;denchmark-link:https://github.com/ray-project/ray/blob/0d56fc10ccf0802ae7b335532698b4b1fc573fe4/src/ray/raylet/node_manager.cc#L564&gt;here&lt;/denchmark-link&gt;
, instead of relying on the cleanup for failed tasks in &lt;denchmark-link:https://github.com/ray-project/ray/blob/0d56fc10ccf0802ae7b335532698b4b1fc573fe4/src/ray/raylet/node_manager.cc#L1085&gt;NodeManager::TreatTaskAsFailed&lt;/denchmark-link&gt;
.
I think the better way to do this is to only remove tasks that are not RUNNING or BLOCKED when cleaning up tasks for a dead driver. The remaining tasks will then get cleaned up once their corresponding workers exit.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;F1127 19:24:45.846309 23444 scheduling_queue.cc:236]  Check failed: task_ids.size() == 0
*** Check failure stack trace: ***
    @           0x598438  google::LogMessage::Fail()
    @           0x59837c  google::LogMessage::SendToLog()
    @           0x597cbe  google::LogMessage::Flush()
    @           0x597ab9  google::LogMessage::~LogMessage()
    @           0x4e3c80  ray::RayLog::~RayLog()
    @           0x563164  ray::raylet::SchedulingQueue::RemoveTasks()
    @           0x563332  ray::raylet::SchedulingQueue::RemoveTask()
    @           0x51d15c  ray::raylet::NodeManager::HandleTaskUnblocked()
    @           0x527dde  ray::raylet::NodeManager::ProcessDisconnectClientMessage()
    @           0x5289a8  ray::raylet::NodeManager::ProcessClientMessage()
    @           0x4985ac  _ZNSt17_Function_handlerIFvSt10shared_ptrIN3ray16ClientConnectionIN5boost4asio5local15stream_protocolEEEElPKhEZNS1_6raylet6Raylet12HandleAcceptERKNS3_6system10error_codeEEUlS8_lSA_E0_E9_M_invokeERKSt9_Any_dataOS8_OlOSA_
    @           0x4ef2c0  ray::ClientConnection&lt;&gt;::ProcessMessage()
    @           0x4ef813  ray::ClientConnection&lt;&gt;::ProcessMessageHeader()
    @           0x4eadb8  boost::asio::detail::read_op&lt;&gt;::operator()()
    @           0x4eb4d6  boost::asio::detail::reactive_socket_recv_op&lt;&gt;::do_complete()
    @           0x490e08  boost::asio::detail::epoll_reactor::descriptor_state::do_complete()
    @           0x491768  boost::asio::detail::task_io_service::run()
    @           0x48854e  main
    @     0x7f2d0aa7b830  __libc_start_main
    @           0x48caa9  _start
    @              (nil)  (unknown)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='stephanie-wang' date='2019-02-26T20:35:36Z'>
		&lt;denchmark-link:https://github.com/stephanie-wang&gt;@stephanie-wang&lt;/denchmark-link&gt;
 this has been fixed, right?
		</comment>
		<comment id='2' author='stephanie-wang' date='2019-02-26T20:52:56Z'>
		I believe so.
		</comment>
	</comments>
</bug>