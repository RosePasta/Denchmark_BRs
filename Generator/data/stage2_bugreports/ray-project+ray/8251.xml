<bug id='8251' author='vrangaswamy' open_date='2020-04-30T16:42:14Z' closed_time='2020-11-19T22:07:01Z'>
	<summary>Ray actors marked as dead while large file are being written.</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

ray 0.8.2
I have the following setup: I have 2 nodes on my driver and many other nodes on remote machines. One of the nodes on my driver is periodically writing large files to disk. When the disk-writer writes the file, I get the following error:
&lt;denchmark-code&gt;2020-04-29 20:08:58,747#011WARNING worker.py:1058 -- The node with client ID 116cd6d7effde94a120374931760615a6acec931 has been marked dead because the monitor has missed too many heartbeats from it.
2020-04-29 20:08:58,767#011WARNING worker.py:1058 -- A worker died or was killed while executing task ffffffffffffffffee9d776a0100.
&lt;/denchmark-code&gt;

Shortly after, my job crashes.
Writing to disk was taking &gt; 30 seconds. I was able to work around this bug by adjusting the num_heartbeats_timeout parameter in _internal_config, so that time needed to mark a node as failed is &gt; 30 sec. But it seems a little strange to be modifying something marked “For testing purposes only”.
I have a few questions:

Why are workers marked dead when I’m writing to disk?
I eventually write the files to S3; if I upload the objects directly (they’re numpy arrays), will that avoid this issue altogether?
As long as disk writes take less time than the heartbeat timeout, am I guaranteed to avoid this problem?

&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Will update in a couple days with a script.
	</description>
	<comments>
		<comment id='1' author='vrangaswamy' date='2020-04-30T16:44:13Z'>
		Is there any way you can see if the same issue happens in 0.8.4?
		</comment>
		<comment id='2' author='vrangaswamy' date='2020-04-30T17:28:12Z'>
		To clarify, the disk writes are occuring in a task? (repro script would help here)
		</comment>
		<comment id='3' author='vrangaswamy' date='2020-04-30T18:14:09Z'>
		I'll attempt a repro script with 0.8.4--look out for that in the next few days.
		</comment>
		<comment id='4' author='vrangaswamy' date='2020-05-01T16:56:59Z'>
		I believe I ran into this issue when when I was downloading a pre-tuned torchvision model on a slow internet connection. When I re-ran the script it worked fine since it was using the cached weights.
		</comment>
		<comment id='5' author='vrangaswamy' date='2020-05-01T17:34:05Z'>
		I've pasted my attempt at a repro script below. The behavior I was trying to reproduce is the DummyActor crashing when the DiskActor writes its large file, but unfortunately in a devo environment I was only able to get this to crash nondeterministically.
On an EC2 p3.8xlarge, after ~ 10 minutes, this results in the following error message after successfully writing to disk many times.  However, at scale (running a PPO-like algorithm with 1000s of remote worker nodes), the "DummyActor" consistently crashes when the "DiskActor" takes &gt; 30 seconds to write, and never if it takes &lt; 30 seconds.
Error msg:
&lt;denchmark-code&gt;2020-05-01 17:17:43,584 WARNING worker.py:1072 -- The node with client ID 0cc474dda1d9ddc4c20d6966fefedfcd4dd08efb has been marked dead because the monitor has missed too many heartbeats from it.
(pid=raylet) F0501 17:17:43.579385 95650 node_manager.cc:523]  Check failed: node_id != self_node_id_ Exiting because this node manager has mistakenly been marked dead by the monitor.
(pid=raylet) *** Check failure stack trace: ***
(pid=raylet)     @     0x5623a9ab711d  google::LogMessage::Fail()
(pid=raylet)     @     0x5623a9ab858c  google::LogMessage::SendToLog()
(pid=raylet)     @     0x5623a9ab6df9  google::LogMessage::Flush()
(pid=raylet)     @     0x5623a9ab7011  google::LogMessage::~LogMessage()
(pid=raylet)     @     0x5623a977b8f9  ray::RayLog::~RayLog()
(pid=raylet)     @     0x5623a958979b  ray::raylet::NodeManager::NodeRemoved()
(pid=raylet)     @     0x5623a958998c  _ZNSt17_Function_handlerIFvRKN3ray8ClientIDERKNS0_3rpc11GcsNodeInfoEEZNS0_6raylet11NodeManager11RegisterGcsEvEUlS3_S7_E0_E9_M_invokeERKSt9_Any_dataS3_S7_
(pid=raylet)     @     0x5623a963ea82  ray::gcs::ClientTable::HandleNotification()
(pid=raylet)     @     0x5623a963ef6b  _ZNSt17_Function_handlerIFvPN3ray3gcs14RedisGcsClientERKNS0_8ClientIDERKSt6vectorINS0_3rpc11GcsNodeInfoESaIS9_EEEZNS1_11ClientTable21SubscribeToNodeChangeERKSt8functionIFvS6_RKS9_EERKSG_IFvNS0_6StatusEEEEUlS3_RKNS0_8UniqueIDESD_E_E9_M_invokeERKSt9_Any_dataS3_S6_SD_
(pid=raylet)     @     0x5623a9641a18  _ZNSt17_Function_handlerIFvPN3ray3gcs14RedisGcsClientERKNS0_8ClientIDENS0_3rpc13GcsChangeModeERKSt6vectorINS7_11GcsNodeInfoESaISA_EEEZNS1_3LogIS4_SA_E9SubscribeERKNS0_5JobIDES6_RKSt8functionIFvS3_S6_SE_EERKSL_IFvS3_EEEUlS3_S6_S8_SE_E_E9_M_invokeERKSt9_Any_dataS3_S6_S8_SE_
(pid=raylet)     @     0x5623a963f58e  _ZNSt17_Function_handlerIFvSt10shared_ptrIN3ray3gcs13CallbackReplyEEEZNS2_3LogINS1_8ClientIDENS1_3rpc11GcsNodeInfoEE9SubscribeERKNS1_5JobIDERKS7_RKSt8functionIFvPNS2_14RedisGcsClientESF_NS8_13GcsChangeModeERKSt6vectorIS9_SaIS9_EEEERKSG_IFvSI_EEEUlS4_E_E9_M_invokeERKSt9_Any_dataS4_
(pid=raylet)     @     0x5623a966f80b  _ZN5boost4asio6detail18completion_handlerIZN3ray3gcs20RedisCallbackManager12CallbackItem8DispatchERSt10shared_ptrINS4_13CallbackReplyEEEUlvE_E11do_completeEPvPNS1_19scheduler_operationERKNS_6system10error_codeEm
(pid=raylet)     @     0x5623a9a4a0af  boost::asio::detail::scheduler::do_run_one()
(pid=raylet)     @     0x5623a9a4b131  boost::asio::detail::scheduler::run()
(pid=raylet)     @     0x5623a9a4bfd2  boost::asio::io_context::run()
(pid=raylet)     @     0x5623a95026d4  main
(pid=raylet)     @     0x7fee8c6f0830  __libc_start_main
(pid=raylet)     @     0x5623a9512a31  (unknown)
(pid=95702) E0501 17:17:43.875625 95772 core_worker.cc:379] Raylet failed. Shutting down.
(pid=95712) E0501 17:17:43.956218 95779 core_worker.cc:379] Raylet failed. Shutting down.
(pid=95692) E0501 17:17:44.036082 95799 core_worker.cc:379] Raylet failed. Shutting down.
(pid=95700) E0501 17:17:44.033759 95797 core_worker.cc:379] Raylet failed. Shutting down.
(pid=95688) E0501 17:17:44.029695 95795 core_worker.cc:379] Raylet failed. Shutting down.
(pid=95713) E0501 17:17:44.050458 95805 core_worker.cc:379] Raylet failed. Shutting down.
&lt;/denchmark-code&gt;

At the same time as running this script, I'm running stress --cpu 31 --hdd 1 --hdd-bytes 100000000000 to simulate running at scale; I think this is important because I've never been able to get it to crash without doing this.
&lt;denchmark-code&gt;import json
import os
import subprocess
import tempfile

import ray
import numpy as np
import time

from ray.rllib.agents import Trainer
from ray.rllib.agents.trainer import COMMON_CONFIG
from ray.tune import register_trainable

from datetime import datetime


def log(msg):
    print(f"{datetime.now()}: {msg}")


class MyTrainer(Trainer):

    _default_config = COMMON_CONFIG

    def _init(self, config, env_creator):
        self.dummy_actor = DummyActor.remote()
        self.disk_actor = DiskWritingActor.remote()
        self.dummy_actor.run.remote()
        time.sleep(4)
        self.disk_actor.run.remote()

    def _train(self):
        time.sleep(100)
        return {}


@ray.remote
class DiskWritingActor:

    def run(self):
        log('running disk actor')
        while True:
            log('creating huge array')
            large_arr = np.random.rand(1024, 1024, 30, 100).astype(np.float32)
            log(f'saving array of size {large_arr.nbytes}')
            start = time.time()
            with tempfile.TemporaryDirectory() as tmpdir:
                np.save(os.path.join(tmpdir, 'foo.py'), large_arr)
            end = time.time()
            log(f"took {end-start} to save")
            time.sleep(10)


@ray.remote
class DummyActor:
    def run(self):
        log('running dummy actor')
        while True:
            time.sleep(10)
            log("dummy actor still running")


register_trainable('mytrainer', MyTrainer)

if __name__ == '__main__':
   # I changed the actor timeout to be 1 second to try to force the issue.
    ray.init(_internal_config=json.dumps({'num_heartbeats_timeout': 10}))
    config = dict(rllib_config={
        'env': 'Acrobot-v1',
        'run': MyTrainer
    })
    ray.tune.run_experiments(config)
&lt;/denchmark-code&gt;

Please let me know if you're able to reproduce this behavior on your end--thanks for your help.
		</comment>
		<comment id='6' author='vrangaswamy' date='2020-05-01T20:06:09Z'>
		Thanks for the detailed writeup! At a first glance, it seems like it may be possible to reproduce even if the disk actor is running outside of Ray (e.g., it's just triggered by high system load). Do you have an intuition of whether this is the case?
Also, I wonder if writing to /tmp (iirc this is stored in memory and might cause swapping) vs some other disk makes a difference.
		</comment>
		<comment id='7' author='vrangaswamy' date='2020-05-01T20:10:26Z'>
		I think you're right about the first thing (being triggered by high system load). On the instance that I was using, /tmp is not actually a tmpfs but an EBS volume, so I don't that was the issue.
		</comment>
		<comment id='8' author='vrangaswamy' date='2020-05-01T20:31:52Z'>
		Hmm weird, it's hard to imagine EBS / network writes starving the heartbeats from getting through for 30 seconds at a time.
1000s of remote worker nodes -- is this literally multiple thousands? I wonder if this was already at a tipping point for scalability or something like that. Could you attach ray logs for the head node? There might be some indications that heartbeats are falling behind even under normal load.
		</comment>
		<comment id='9' author='vrangaswamy' date='2020-05-12T22:00:44Z'>
		I was facing this issue, when I was using ray in a resource constraint setting, i.e. when the server is being used by other users and we are all sort of dumping jobs, more or less at the same time ... then one of the workers in your ray-cluster might get sidelined by the kernel and hence the parent worker might not be able to ping the child worker ... I also found out that if we increase the 'num_heartbeats_timeout' in _internal_config (as stated by &lt;denchmark-link:https://github.com/vrangaswamy&gt;@vrangaswamy&lt;/denchmark-link&gt;
 ) then we may be able to avoid this issue ...
_internal_config = '{"initial_reconstruction_timeout_milliseconds": 30000000, "num_heartbeats_timeout": 10000}' 
in ray.init()
ray.init(num_cpus = MAX_WORKERS,  memory = 30E9, object_store_memory = 5E9,  local_mode = not WITH_RAY,  log_to_driver = False,  include_webui = True, num_redis_shards = 3, _internal_config = '{"initial_reconstruction_timeout_milliseconds": 30000000, "num_heartbeats_timeout": 10000}')
		</comment>
		<comment id='10' author='vrangaswamy' date='2020-09-11T23:38:56Z'>
		cc &lt;denchmark-link:https://github.com/stephanie-wang&gt;@stephanie-wang&lt;/denchmark-link&gt;
 How was the status for our heartbeat lagging issue? Is it resolved?
		</comment>
		<comment id='11' author='vrangaswamy' date='2020-11-13T05:20:23Z'>
		&lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
 I think this is a commonly known issue (when there's large disk IO, heartbeat seems to fail). And if this hypothesis is true, it will impact object spilling, so I will increase the priority to P1.
		</comment>
		<comment id='12' author='vrangaswamy' date='2020-11-19T22:07:00Z'>
		Duplicates &lt;denchmark-link:https://github.com/ray-project/ray/issues/11624&gt;#11624&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>