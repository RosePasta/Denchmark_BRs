<bug id='6928' author='flying-mojo' open_date='2020-01-27T14:55:59Z' closed_time='2020-02-03T12:09:38Z'>
	<summary>[RLLib] CNN + LSTM error (ModelV2, Keras)</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

When running a custom V2 model that combines a CNN and an LSTM following error is encountered:
Traceback (most recent call last):
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 426, in _process_trial
result = self.trial_executor.fetch_result(trial)
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py", line 378, in fetch_result
result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/worker.py", line 1457, in get
raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(InvalidArgumentError): ray::PPO.init() (pid=39866, ip=172.17.2.49)
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1367, in _do_call
return fn(*args)
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1352, in _run_fn
target_list, run_metadata)
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1445, in _call_tf_sessionrun
run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Only one input size may be -1, not both 0 and 1
[[{{node default_policy/model_1/tf_op_layer_default_policy/Reshape/default_policy/Reshape}}]]
Original stack trace for 'default_policy/model_1/tf_op_layer_default_policy/Reshape/default_policy/Reshape':
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/workers/default_worker.py", line 118, in 
ray.worker.global_worker.main_loop()
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 90, in init
Trainer.init(self, config, env, logger_creator)
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 398, in init
Trainable.init(self, config, logger_creator)
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/tune/trainable.py", line 96, in init
self._setup(copy.deepcopy(self.config))
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 523, in _setup
self._init(self.config, self.env_creator)
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 109, in _init
self.config["num_workers"])
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 568, in _make_workers
logdir=self.logdir)
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/evaluation/worker_set.py", line 64, in init
RolloutWorker, env_creator, policy, 0, self._local_config)
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/evaluation/worker_set.py", line 220, in _make_worker
_fake_sampler=config.get("_fake_sampler", False))
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py", line 350, in init
self._build_policy_map(policy_dict, policy_config)
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py", line 768, in _build_policy_map
policy_map[name] = cls(obs_space, act_space, merged_conf)
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/policy/tf_policy_template.py", line 143, in init
obs_include_prev_action_reward=obs_include_prev_action_reward)
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/policy/dynamic_tf_policy.py", line 200, in init
self._initialize_loss()
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/policy/dynamic_tf_policy.py", line 294, in _initialize_loss
SampleBatch(dummy_batch))
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/policy/tf_policy_template.py", line 156, in postprocess_trajectory
episode)
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/agents/ppo/ppo_policy.py", line 191, in postprocess_ppo_gae
*next_state)
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/utils/tf_ops.py", line 88, in call
symbolic_out[0] = fn(*placeholders)
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/agents/ppo/ppo_policy.py", line 249, in value
tf.convert_to_tensor([1]))
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/models/modelv2.py", line 154, in call
res = self.forward(restored, state or [], seq_lens)
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/ray/rllib/models/tf/recurrent_tf_modelv2.py", line 65, in forward
seq_lens)
File "./cnn_lstm_error.py", line 103, in forward_rnn
state)
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 778, in call
outputs = call_fn(cast_inputs, *args, **kwargs)
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py", line 717, in call
convert_kwargs_to_constants=base_layer_utils.call_context().saving)
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py", line 891, in _run_internal_graph
output_tensors = layer(computed_tensors, **kwargs)
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 778, in call
outputs = call_fn(cast_inputs, *args, **kwargs)
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 2497, in call
return self._make_op(inputs)
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 2524, in _make_op
op = graph._create_op_from_tf_operation(c_op)
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 3347, in _create_op_from_tf_operation
ret = Operation(c_op, self)
File "/data/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 1756, in init
self._traceback = tf_stack.extract_stack()
Ray version and other system information (Python version, TensorFlow version, OS):
Ray==0.8.0, tensorflow-gpu==2.1.0, Linux
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
running below scrip shows the problem:
&lt;denchmark-h:h1&gt;script begin&lt;/denchmark-h&gt;

from future import absolute_import
from future import division
from future import print_function
import numpy as np
import gym
from ray.rllib.models import ModelCatalog
from ray.rllib.models.modelv2 import ModelV2
from ray.rllib.models.tf.tf_modelv2 import TFModelV2
from ray.rllib.models.tf.recurrent_tf_modelv2 import RecurrentTFModelV2
from gym.spaces import Discrete, Box
import ray
from ray import tune
from ray.rllib.utils import try_import_tf
from ray.rllib.utils.annotations import override
from keras.applications.mobilenet_v2 import MobileNetV2
tf = try_import_tf()
class RandomEnv(gym.Env):
"""
A randomly acting environment that can be instantiated with arbitrary
action and observation spaces.
"""
&lt;denchmark-code&gt;def __init__(self, config):
    # Action space.
    self.action_space = config["action_space"]
    # Observation space from which to sample.
    self.observation_space = config["observation_space"]
    # Reward space from which to sample.
    self.reward_space = config.get(
        "reward_space",
        gym.spaces.Box(low=-1.0, high=1.0, shape=(), dtype=np.float32))
    # Chance that an episode ends at any step.
    self.p_done = config.get("p_done", 0.1)

def reset(self):
    return self.observation_space.sample()

def step(self, action):
    return self.observation_space.sample(), \
        float(self.reward_space.sample()), \
        bool(np.random.choice(
            [True, False], p=[self.p_done, 1.0 - self.p_done]
        )), {}
&lt;/denchmark-code&gt;

class CustomModel(RecurrentTFModelV2):
&lt;denchmark-code&gt;def __init__(self, obs_space, action_space, num_outputs, model_config,
             name):
    super(CustomModel, self).__init__(obs_space, action_space, num_outputs,
                                      model_config, name)

    visual_size = 90 * 160 * 3
    self.cell_size = 256
    cnn_shape = (90, 160, 3,)

    state_in_h = tf.keras.layers.Input(shape=(self.cell_size, ), name="h")
    state_in_c = tf.keras.layers.Input(shape=(self.cell_size, ), name="c")
    seq_in = tf.keras.layers.Input(shape=(), name="seq_in", dtype=tf.int32)

    inputs = tf.keras.layers.Input(
        shape=(None, visual_size), name="visual_inputs")

    input_visual = inputs
    input_visual = tf.reshape(input_visual, [-1, -1, 90, 160, 3])
    cnn_input = tf.keras.layers.Input(
        shape=cnn_shape, name="cnn_input")

    cnn_model = tf.keras.applications.mobilenet_v2.MobileNetV2(alpha=1.0, include_top=True, weights=None, input_tensor=cnn_input, pooling=None)
    vision_out = tf.keras.layers.TimeDistributed(cnn_model)(input_visual)

    lstm_out, state_h, state_c = tf.keras.layers.LSTM(
        self.cell_size, return_sequences=True, return_state=True, name="lstm")(
            inputs=vision_out,
            mask=tf.sequence_mask(seq_in),
            initial_state=[state_in_h, state_in_c])
    
    # Postprocess LSTM output with another hidden layer and compute values
    logits = tf.keras.layers.Dense(
        self.num_outputs,
        activation=tf.keras.activations.linear,
        name="logits")(lstm_out)
    values = tf.keras.layers.Dense(
        1, activation=None, name="values")(lstm_out)

    # Create the RNN model
    self.rnn_model = tf.keras.Model(
        inputs=[inputs, seq_in, state_in_h, state_in_c],
        outputs=[logits, values, state_h, state_c])
    self.register_variables(self.rnn_model.variables)
    self.rnn_model.summary()

@override(RecurrentTFModelV2)
def forward_rnn(self, inputs, state, seq_lens):
    model_out, self._value_out, h, c = self.rnn_model([inputs, seq_lens] +
                                                      state)
    return model_out, [h, c]

@override(ModelV2)
def get_initial_state(self):
    return [
        np.zeros(self.cell_size, np.float32),
        np.zeros(self.cell_size, np.float32),
    ]

@override(ModelV2)
def value_function(self):
    return tf.reshape(self._value_out, [-1])
&lt;/denchmark-code&gt;

if name == "main":
ray.init()
ModelCatalog.register_custom_model("my_model", CustomModel)
tune.run(
"PPO",
stop={
"timesteps_total": 10000,
},
config={
"env": RandomEnv,
"model": {
"custom_model": "my_model",
},
"vf_share_layers": False,
"num_workers": 1,  # parallelism
"env_config": {
"action_space": Discrete(2),
# Test a simple Tuple observation space.
"observation_space": gym.spaces.Box(0.0, 1.0, shape=(90, 160, 3), dtype=np.float32)
},
},
)
&lt;denchmark-h:h1&gt;script end&lt;/denchmark-h&gt;

If we cannot run your script, we cannot fix your issue.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='flying-mojo' date='2020-01-28T07:46:59Z'>
		Ok, I can reproduce the issue. Taking a look ...
		</comment>
		<comment id='2' author='flying-mojo' date='2020-01-28T11:58:54Z'>
		Ok, your Keras model definition has a problem in how you define your input.
You are doing input = tf.keras.layers.Input(shape=(None, 43200) ...) (&lt;- here you should just say: Input(shape=(43200,)) no None for the time rank).
The problem is that after that, you are doing a reshape to (-1, -1, ...) and the graph won't know, what to assign to the two undefined dims. So you need to have some kind of max-length idea for your sequences and then do reshape(-1, max_len, ...).
The good thing is that after I fixed that, I found a couple more bugs in RLlib's eager Policy, which I'll fix now. Will try to get the PR through today. ...
		</comment>
		<comment id='3' author='flying-mojo' date='2020-01-28T16:11:54Z'>
		Thank you for looking into it!
It makes sense to set max_len to known value. I should be able to confirm
if it resolves the issue in my actual experiment today too!
Looking forward to running in the eager mode too;)
Cheers,
Jarek
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Tue, 28 Jan 2020 at 11:58, Sven Mika ***@***.***&gt; wrote:
 Ok, your Keras model definition has a problem in how you define your input.
 You are doing input = tf.keras.layers.Input(shape=(None, 43200) ...) (&lt;-
 here you should just say: Input(shape=(43200,)) no None for the time
 rank).
 The problem is that after that, you are doing a reshape to (-1, -1, ...)
 and the graph won't know, what to assign to the two undefined dims. So you
 need to have some kind of max-length idea for your sequences and then do reshape(-1,
 max_len, ...).
 The good thing is that after I fixed that, I found a couple more bugs in
 RLlib's eager Policy, which I'll fix now. Will try to get the PR through
 today. ...

 —
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub
 &lt;#6928?email_source=notifications&amp;email_token=AHRPS7QXLWMKUWH4JQREEZLRAAMX7A5CNFSM4KMCUPIKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEKDBFPA#issuecomment-579211964&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/AHRPS7WRA4QC62SO6YK6POTRAAMX7ANCNFSM4KMCUPIA&gt;
 .



		</comment>
		<comment id='4' author='flying-mojo' date='2020-02-03T12:09:28Z'>
		No problem at all. I figured it out. This was (given that you are not using tf-eager) a problem in your model. Here is a working version of it (see code below). The trick was to correctly fold the time-rank into the batch rank before pushing it through the CNN, then correctly unfolding it again before the LSTM pass. Your original input with None was correct, however, the reshape with (-1, -1) does not work as tf has no idea, what the batch/time sizes are.
For the eager case, there is actually a bug in RLlib, which I'll fix now (issue &lt;denchmark-link:https://github.com/ray-project/ray/issues/6732&gt;#6732&lt;/denchmark-link&gt;
).
&lt;denchmark-code&gt;import numpy as np

from ray.rllib.models import ModelCatalog
from ray.rllib.models.modelv2 import ModelV2
from ray.rllib.models.tf.recurrent_tf_modelv2 import RecurrentTFModelV2
from gym.spaces import Discrete, Box

from ray.rllib.agents.ppo import PPOTrainer
from ray.rllib.examples.random_env import RandomEnv
from ray.rllib.utils import try_import_tf
from ray.rllib.utils.annotations import override

tf = try_import_tf()

cnn_shape = (4, 4, 3)


class CustomModel(RecurrentTFModelV2):
    def __init__(self, obs_space, action_space, num_outputs, model_config,
                 name):
        super(CustomModel, self).__init__(obs_space, action_space, num_outputs,
                                          model_config, name)

        self.cell_size = 16
        visual_size = cnn_shape[0] * cnn_shape[1] * cnn_shape[2]

        state_in_h = tf.keras.layers.Input(shape=(self.cell_size,), name="h")
        state_in_c = tf.keras.layers.Input(shape=(self.cell_size,), name="c")
        seq_in = tf.keras.layers.Input(shape=(), name="seq_in", dtype=tf.int32)

        inputs = tf.keras.layers.Input(
            shape=(None, visual_size), name="visual_inputs")

        input_visual = inputs
        input_visual = tf.reshape(
            input_visual, [-1, cnn_shape[0], cnn_shape[1], cnn_shape[2]])
        cnn_input = tf.keras.layers.Input(shape=cnn_shape, name="cnn_input")

        cnn_model = tf.keras.applications.mobilenet_v2.MobileNetV2(
            alpha=1.0,
            include_top=True,
            weights=None,
            input_tensor=cnn_input,
            pooling=None)
        vision_out = cnn_model(input_visual)
        vision_out = tf.reshape(
            vision_out,
            [-1, tf.shape(inputs)[1], vision_out.shape.as_list()[-1]])

        lstm_out, state_h, state_c = tf.keras.layers.LSTM(
            self.cell_size,
            return_sequences=True,
            return_state=True,
            name="lstm")(
                inputs=vision_out,
                mask=tf.sequence_mask(seq_in),
                initial_state=[state_in_h, state_in_c])

        # Postprocess LSTM output with another hidden layer and compute values.
        logits = tf.keras.layers.Dense(
            self.num_outputs,
            activation=tf.keras.activations.linear,
            name="logits")(lstm_out)
        values = tf.keras.layers.Dense(
            1, activation=None, name="values")(lstm_out)

        # Create the RNN model
        self.rnn_model = tf.keras.Model(
            inputs=[inputs, seq_in, state_in_h, state_in_c],
            outputs=[logits, values, state_h, state_c])
        self.register_variables(self.rnn_model.variables)
        self.rnn_model.summary()

    @override(RecurrentTFModelV2)
    def forward_rnn(self, inputs, state, seq_lens):
        model_out, self._value_out, h, c = self.rnn_model([inputs, seq_lens] +
                                                          state)
        return model_out, [h, c]

    @override(ModelV2)
    def get_initial_state(self):
        return [
            np.zeros(self.cell_size, np.float32),
            np.zeros(self.cell_size, np.float32),
        ]

    @override(ModelV2)
    def value_function(self):
        return tf.reshape(self._value_out, [-1])


if __name__ == "__main__":
    ModelCatalog.register_custom_model("my_model", CustomModel)
    trainer = PPOTrainer(
        env=RandomEnv,
        config={
            #"eager": True,
            "model": {
                "custom_model": "my_model",
                "max_seq_len": 20,
            },
            "vf_share_layers": True,
            "num_workers": 0,  # no parallelism
            "env_config": {
                "action_space": Discrete(2),
                # Test a simple Tuple observation space.
                "observation_space": Box(
                    0.0, 1.0, shape=cnn_shape, dtype=np.float32)
            }
        })
    trainer.train()
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>