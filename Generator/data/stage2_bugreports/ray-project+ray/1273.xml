<bug id='1273' author='robertnishihara' open_date='2017-11-29T21:37:50Z' closed_time='2018-09-13T22:28:17Z'>
	<summary>Calling ray.wait inside of a remote function can cause an application to hang.</summary>
	<description>
When a worker calls ray.get inside a task, it will release its CPU resources to the local scheduler so that other tasks can be scheduled while it is blocked. ray.wait probably needs to do the same thing. You can see this problem as follows.
import ray
import time
ray.init(num_cpus=1)

@ray.remote
def f():
    return 1

@ray.remote
def g():
    # This line will hang. The f task won't be scheduled because g doesn't
    # release its resources.
    ray.wait([f.remote()])

ray.get(g.remote())  # This line will hang.
	</description>
	<comments>
		<comment id='1' author='robertnishihara' date='2018-09-12T15:10:20Z'>
		I have run across what is likely the same bug, but the symptom is underutilization of available resources because ray.wait is holding them. Here's an example that can be used to illustrate either the hanging or underutilization case.
import ray
import time

N, M = 5, 4

demonstrate_hang = False
demonstrate_underutilization = True

if demonstrate_hang:
    # Not enough CPUs. Outer tasks will hold them all while                                                                                
    # waiting and not let inners ever get any.                                                                                             
    num_cpus = N
elif demonstrate_underutilization:
    # Enough CPUs for all the outer tasks and one inner to                                                                                 
    # squeeze through so we make progress and eventually get                                                                               
    # everything done. But we could be more efficient and                                                                                  
    # run more inners at a time if the waiting outers released                                                                             
    # CPUs. Towards the end we are able to run more inners at                                                                              
    # once as there are fewer outers left.                                                                                                 
    num_cpus = N + 1
else:
    # Enough CPUs for all inner and outer tasks to claim one                                                                               
    # so everyone runs at once.                                                                                                            
    num_cpus = N * (M + 1)

ray.init(num_cpus=num_cpus)

start = time.time()

@ray.remote
def inner(a, b):
    t = time.time() - start
    print("Starting inner {}-{} at {:.3f}".format(a, b, t))
    time.sleep(2)
    t = time.time() - start
    print("Finished inner {}-{} at {:.3f}".format(a, b, t))

@ray.remote
def outer(a):
    t = time.time() - start
    print("Starting outer {} at {:.3f}".format(a, t))
    ray.wait([inner.remote(a, jj) for jj in range(M)], num_returns=M)
    t = time.time() - start
    print("Finished outer {} at {:.3f}".format(a, t))

t = time.time() - start
print("Starting all at {:.3f}".format(t))

ray.wait([outer.remote(ii) for ii in range(N)], num_returns=N)

t = time.time() - start
print("Finished all at {:.3f}".format(t))
		</comment>
		<comment id='2' author='robertnishihara' date='2018-09-12T20:05:25Z'>
		Thanks &lt;denchmark-link:https://github.com/vengroff&gt;@vengroff&lt;/denchmark-link&gt;
 for reporting this (and for the clean example), I can reproduce the hanging with your script.
I have a fix in &lt;denchmark-link:https://github.com/ray-project/ray/pull/2864&gt;#2864&lt;/denchmark-link&gt;
 which should address the issue in the new backend. Setting  and running with the environment variable  fixes the issue.
Note that this requires that environment variable to be set (in order to pick up the fix), but it is not turned on by default yet. However, it will be for 0.6.
		</comment>
		<comment id='3' author='robertnishihara' date='2018-09-17T21:40:13Z'>
		Thank you. I have verified the fix works in the latest wheel.
		</comment>
		<comment id='4' author='robertnishihara' date='2018-09-18T12:52:38Z'>
		As I run larger workloads, I have started to see warnings like the following:
&lt;denchmark-code&gt;W0918 12:46:01.291203  3699 node_manager.cc:1081] Resources oversubscribed: {GPU,0.000000}, {CPU,7.000000}, 
W0918 12:46:43.063521  3699 node_manager.cc:1081] Resources oversubscribed: {GPU,0.000000}, {CPU,8.000000}, 
W0918 12:46:47.504438  3699 node_manager.cc:1081] Resources oversubscribed: {GPU,0.000000}, {CPU,9.000000}, 
W0918 12:47:30.264201  3699 node_manager.cc:1081] Resources oversubscribed: {GPU,0.000000}, {CPU,10.000000}, 
W0918 12:47:31.554244  3699 node_manager.cc:1081] Resources oversubscribed: {GPU,0.000000}, {CPU,11.000000}, 
W0918 12:47:32.415546  3699 node_manager.cc:1081] Resources oversubscribed: {GPU,0.000000}, {CPU,12.000000}, 
W0918 12:47:33.160295  3699 node_manager.cc:1081] Resources oversubscribed: {GPU,0.000000}, {CPU,13.000000}, 
W0918 12:47:33.843848  3699 node_manager.cc:1081] Resources oversubscribed: {GPU,0.000000}, {CPU,14.000000}, 
W0918 12:47:34.280972  3699 node_manager.cc:1081] Resources oversubscribed: {GPU,0.000000}, {CPU,15.000000}, 
W0918 12:47:34.462929  3699 node_manager.cc:1081] Resources oversubscribed: {GPU,0.000000}, {CPU,15.000000}, 
W0918 12:47:35.035676  3699 node_manager.cc:1081] Resources oversubscribed: {GPU,0.000000}, {CPU,16.000000}, 
W0918 12:47:35.614987  3699 node_manager.cc:1081] Resources oversubscribed: {GPU,0.000000}, {CPU,17.000000}, 
&lt;/denchmark-code&gt;

I have not yet dug in to see if these are the result of an accounting hiccup when waiting tasks release resources or something more significant. Just thought I'd point it out in case it's obvious to you what is going on before I debug further.
		</comment>
		<comment id='5' author='robertnishihara' date='2018-09-18T21:13:59Z'>
		Thanks for bringing this up! It's an innocuous warning.
Suppose there is a total of 1 CPU available, and consider the following workload
&lt;denchmark-code&gt;@ray.remote
def f():
    pass

@ray.remote
def h():
    pass

@ray.remote
def g():
    ray.get(f.remote())

g.remote()
h.remote()
&lt;/denchmark-code&gt;

This is what could happen:

g and h are submitted
g begins executing (1 CPU is in use)
g submits f
g blocks in ray.get and returns its 1 CPU to the scheduler, (now 0 CPUs are in use)
f begins executing (1 CPU) is in use
f finishes and returns (now 0 CPUs in use)
h begins executing (1 CPU in use)
g gets unblocked and continues executing (now 2 CPUs in use temporarily)
h finishes (now 1 CPU in use)
g finishes (now 0 CPUs in use)

Something similar could happen with ray.wait.
We could probably prevent this, but it seemed ok to allow resources to be temporarily oversubscribed.
		</comment>
		<comment id='6' author='robertnishihara' date='2018-09-18T21:14:43Z'>
		It might be worse with ray.wait now that I think about it because ray.wait can return (if a timeout is use) before any tasks finish executing).
		</comment>
	</comments>
</bug>