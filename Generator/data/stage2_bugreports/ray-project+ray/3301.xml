<bug id='3301' author='arcelien' open_date='2018-11-11T22:28:20Z' closed_time='2018-11-21T20:26:23Z'>
	<summary>[tune] PBT causes task reconstruction messages</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
Ray installed from (source or binary): binary from latest installed 11/7
Ray version: 0.5.3
Python version: 2.7.15

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

Getting excessive ray logging to stdout about reconstructing task ... when running PBT. There seems to be relatively less spam early on, and more as training progresses.
An example of this line is: I1111 00:40:52.240785 29181 node_manager.cc:1422] Reconstructing task 00000000b85b590f361da6f18d3143dabf82655a on client 94396e7715cc5f95bfbccf545a7e14591dacc7c1
With 4 trials, 200 total epochs, and a perturb interval of 10 iterations:
At 0 epochs: 1 line per iteration per trial
At 10 epochs, 1 checkpoint, 0 perturb: now 2 lines / iter / trial
At 20 epochs: 3-5 lines
At 2X epochs: ~10 lines
At 190 epochs, 27 checkpoints, 25 perturbs: About 100 lines / iter / trial
The total log size is about 14MB, about 100k total lines.
With 8 trials running the same thing, size increases to about 48MB, 345K total lines. Also, for some reason there's 47 checkpoints, and 25 perturbs (4 trials had 27 checkpoints, 25 perturbs).
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

I can attach logs if they would be helpful, but they are very big.
Saving and restoring is done with a tf.Saver object, calling saver.save() and saver.restore()
Configs are:
train_spec = {
    "run": RayModel,
    "trial_resources": {
        "cpu": 8,
        "gpu": 1
    },
    "stop": {
        "training_iteration": hparams.num_epochs,
    },
    "config": hparams.values(),
    "local_dir": FLAGS.local_dir,
    "checkpoint_freq": FLAGS.checkpoint_freq,
    "num_samples": FLAGS.num_samples
}
ray.init()
pbt = PopulationBasedTraining(
    time_attr="training_iteration",
    reward_attr="val_acc",
    perturbation_interval=FLAGS.perturbation_interval,
    m_explore_fn=explore)
run_experiments({"autoaug_pbt": train_spec}, scheduler=pbt, verbose=False)
	</description>
	<comments>
		<comment id='1' author='arcelien' date='2018-11-11T22:59:58Z'>
		&lt;denchmark-link:https://github.com/richardliaw&gt;@richardliaw&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/stephanie-wang&gt;@stephanie-wang&lt;/denchmark-link&gt;
 we shouldn't ever see reconstructing since Tune uses only actor tasks right? Any idea how this could happen?
This might be an 0.6 bug.
		</comment>
		<comment id='2' author='arcelien' date='2018-11-11T23:23:38Z'>
		This is now a release blocker :), given that PBT shouldn't even be creating tasks.
		</comment>
		<comment id='3' author='arcelien' date='2018-11-12T07:36:44Z'>
		&lt;denchmark-link:https://github.com/arcelien&gt;@arcelien&lt;/denchmark-link&gt;
 could you provide some more details on how to reproduce (for example, does this always happen within a short amount of time? if you run on small scale is it ok?)
I tried running PBT locally (the pbt_ppo_example.py file), and did not see this.
		</comment>
		<comment id='4' author='arcelien' date='2018-11-12T08:06:05Z'>
		Looks like the problem always happens for me with num_workers &gt; 1, with the spam getting progressively more as the number of workers increases.
I will try an experiment with a toy model later, but the current one is a resnet model from the tensorflow models repo.
		</comment>
		<comment id='5' author='arcelien' date='2018-11-12T08:38:36Z'>
		It sometimes happens with the Tune Tutorial too:
&lt;denchmark-link:https://github.com/ray-project/tutorial/tree/master/tune_exercises&gt;https://github.com/ray-project/tutorial/tree/master/tune_exercises&lt;/denchmark-link&gt;
 in the
Solutions notebook in the section with AsyncHyperBand.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Mon, Nov 12, 2018 at 12:06 AM Daniel Ho ***@***.***&gt; wrote:
 Looks like the problem always happens for me with num_workers &gt; 1, with
 the spam getting progressively more as the number of workers increases.

 I will try an experiment with a toy model later, but the current one is a
 resnet model from the tensorflow models repo.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#3301 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AEUc5ccGeiS5tzyK9qO79kyMEfDV_QZvks5uuSvxgaJpZM4YYuP4&gt;
 .



		</comment>
		<comment id='6' author='arcelien' date='2018-11-17T21:16:26Z'>
		Ran an ablation test:

original tensorflow wide-resnet model on CIFAR10, using tf.saver to checkpoint, training with tf.optim (adam): reconstructing messages appear after 1st checkpoint
modified model to much smaller toy-resnet (~1GB, mostly image data) from WRN (8GB+): messages appear, no change
removed checkpointing (tf.saver calls) entirely, restore starts from random init, - no change, messages appear after 1st dummy "checkpoint"
changed model to only use CPU compute, no GPU in trial resources config - no change, (getting some GPU init errors?)
changed so that only 1 forward pass runs per ray iteration, instead of 1 epoch, and no validation: no change
Removed training step entirely - fixed, messages disappear

		</comment>
		<comment id='7' author='arcelien' date='2018-11-18T00:19:47Z'>
		FYI &lt;denchmark-link:https://github.com/stephanie-wang&gt;@stephanie-wang&lt;/denchmark-link&gt;
 I was not able to reproduce this on python 2.7 /  master. It could be an environment specific thing.
		</comment>
		<comment id='8' author='arcelien' date='2018-11-18T00:24:44Z'>
		I noticed that it doesn't happen when all trials fit inside compute resources and runs don't get paused.
		</comment>
		<comment id='9' author='arcelien' date='2018-11-18T00:44:44Z'>
		Ok I think I know the proximate cause of this:
In tune, when we PAUSE a trial, we call runner.save_to_object.remote(): 


ray/python/ray/tune/ray_trial_executor.py


         Line 282
      in
      65c27c7






 trial._checkpoint.value = trial.runner.save_to_object.remote() 





This is followed up by __ray_terminate__ing the runner.
Later on, we pass that future returned by save_to_object to restore_from_remote(): 


ray/python/ray/tune/ray_trial_executor.py


         Line 301
      in
      65c27c7






 ray.get(trial.runner.restore_from_object.remote(value)) 





However that seems to somehow trigger the reconstructing message.
The message goes away if you add a ray.get() after save_to_object.remote() (i.e. , which forces the ray terminate to occur after the method has completed. Though I'm not sure why that should matter other than there being some bug in the backend about getting items after the actor has terminated, even if the actor did put the object successfully. &lt;denchmark-link:https://github.com/stephanie-wang&gt;@stephanie-wang&lt;/denchmark-link&gt;
 any ideas ?
This issue is quite specific though and fairly harmless, so not a release blocker.
		</comment>
	</comments>
</bug>