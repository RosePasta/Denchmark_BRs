<bug id='8368' author='duburcqa' open_date='2020-05-08T13:04:03Z' closed_time='2020-05-23T17:54:19Z'>
	<summary>"lr_schedule" option ignored using torch framework and PPO algorithm</summary>
	<description>
Ray version and other system information (Python version, TensorFlow version, OS):

Ray: 0.9.0.dev0 (2c599dbf05e41e338920ee2fbe692658bcbec4dd)
CUDA: 10.1
Pytorch: 1.4.0 with GPU support
Ubuntu 18.04
Python 3.6

&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

Setting the hyperparameter "lr_schedule" as no effect when using PyTorch as backend framework and PPO learning algorithm.
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import ray 
from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG 
 
config = DEFAULT_CONFIG.copy() 
for key, val in { 
    "env": "CartPole-v0", 
    "num_workers": 0, 
    "use_pytorch": False, 
    "lr": 1.0e-5, 
    "lr_schedule": [ 
        [0, 1.0e-6], 
        [1, 1.0e-7], 
    ] 
}.items(): config[key] = val 
 
ray.init() 

for use_pytorch in [False, True]: 
    config["use_pytorch"] = use_pytorch 
    agent = PPOTrainer(config, "CartPole-v0") 
    for _ in range(2): 
        result = agent.train() 
        print(f"use_pytorch: {use_pytorch} - Current learning rate: "\
              f"{result['info']['learner']['default_policy']['cur_lr']}")
&lt;/denchmark-code&gt;


 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='duburcqa' date='2020-05-08T15:19:55Z'>
		Thanks for filing this issue &lt;denchmark-link:https://github.com/duburcqa&gt;@duburcqa&lt;/denchmark-link&gt;
 . Will take a look ...
		</comment>
		<comment id='2' author='duburcqa' date='2020-05-21T18:09:03Z'>
		cc &lt;denchmark-link:https://github.com/krfricke&gt;@krfricke&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='duburcqa' date='2020-05-23T09:06:28Z'>
		I can confirm this - would appreciate an update on it
		</comment>
		<comment id='4' author='duburcqa' date='2020-05-23T10:12:06Z'>
		Looks like the lr and entropy coeff schedule mixin was not added in the Torch PPO policy. This simple PR fixes it:
&lt;denchmark-code&gt;use_pytorch: False - Current learning rate: 9.999999747378752e-06
use_pytorch: False - Current learning rate: 1.0000000116860974e-07
use_pytorch: True - Current learning rate: 1e-05
use_pytorch: True - Current learning rate: 1e-07
&lt;/denchmark-code&gt;

Interesting though that the learning rates seem to differ slightly between torch and tf
		</comment>
		<comment id='5' author='duburcqa' date='2020-05-23T10:59:15Z'>
		Nice, thank you ! The learning rate using tensorflow comes from a conversion from float32 to float64 that must be done somewhere. If you want to check:
&lt;denchmark-code&gt;import numpy as np
print(np.float64(np.float32(1.0e-5)))
print(np.float64(np.float32(1.0e-7)))
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='duburcqa' date='2020-05-23T12:49:35Z'>
		Hmmm I made some more experiments and I am not convinced that the lr is actually properly updated... Is it possible that the learning rate in cur_lr is different from the actual learning rate?
		</comment>
	</comments>
</bug>