<bug id='4402' author='albarji' open_date='2019-03-18T21:21:08Z' closed_time='2019-03-24T03:17:02Z'>
	<summary>num_steps_sampled and num_steps_trained unbalanced in IMPALA</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


OS Platform and Distribution: Ubuntu 16.04.5 LTS
Ray installed from: binary
Ray version: 0.6.3
Python version: 3.6.8
Exact command to reproduce: rllib train -f config.yml
(config file provided below)

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

It seems like IMPALA drops samples when using either more than 1 num_gpus or more than 1 num_data_loader_buffers, or at the very least reports an unbalanced number of num_steps_sampled and num_steps_trained. This can be reproduced by running a test on Qbert with the following configuration files
Single GPU, single loader buffer
atari-impala:
    env: QbertNoFrameskip-v4
    run: IMPALA
    config:
        sample_batch_size: 50
        train_batch_size: 500
        num_workers: 20
        num_gpus: 1
        num_envs_per_worker: 64
        clip_rewards: True
        lr_schedule: [
            [0, 0.0005],
            [200000000, 0.000000000001],
        ]
        replay_proportion: 0
        num_data_loader_buffers: 1
        max_sample_requests_in_flight_per_worker: 1
This produces the expected results, num_steps_sampled is on par with num_steps_trained. My GPUs are old (Tesla K20m), so the learner queue fills pretty quickly and the workers slow down. Since this is suboptimal, it made sense to me to either use more GPUs or more load buffers.
Single GPU, two load buffers
atari-impala:
    env: QbertNoFrameskip-v4
    run: IMPALA
    config:
        sample_batch_size: 50
        train_batch_size: 500
        num_workers: 20
        num_gpus: 1
        num_envs_per_worker: 64
        clip_rewards: True
        lr_schedule: [
            [0, 0.0005],
            [200000000, 0.000000000001],
        ]
        replay_proportion: 0
        num_data_loader_buffers: 2
        max_sample_requests_in_flight_per_worker: 1
Just adding another buffer results in an empty queue, but the num_steps_trained falls way below num_steps_sampled, and I can't see a significant improvement in throughput.
Two GPUs, one load buffer
atari-impala:
    env: QbertNoFrameskip-v4
    run: IMPALA
    config:
        sample_batch_size: 50
        train_batch_size: 500
        num_workers: 20
        num_gpus: 2
        num_envs_per_worker: 64
        clip_rewards: True
        lr_schedule: [
            [0, 0.0005],
            [200000000, 0.000000000001],
        ]
        replay_proportion: 0
        num_data_loader_buffers: 1
        max_sample_requests_in_flight_per_worker: 1
Queue is still mostly full, and again the num_steps_trained falls way below num_steps_sampled.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;


Log for 1 GPU, 1 load buffer (first 10 iterations)
Log for 1 GPU, 2 load buffers (first 10 iterations)
Log for 2 GPUs, 1 load buffer (first 10 iterations)

	</description>
	<comments>
		<comment id='1' author='albarji' date='2019-03-19T01:54:13Z'>
		I have a fix for this -- it turns out to be just an accounting error when in multi-GPU mode. Basically, for reasons we use train_batch_size to count train timesteps instead of the true number of timesteps trained. This can sometimes vary from the configured batch size.
Btw just FYI,         num_envs_per_worker: 64 will cause your batch size to be 50 * 64, so the train batch size will become 3200. You can reduce the num envs per worker, or sample batch size, to avoid this.
		</comment>
	</comments>
</bug>