<bug id='9911' author='simon-mo' open_date='2020-08-04T16:43:22Z' closed_time='2020-09-28T21:06:26Z'>
	<summary>[Serve] Failure from Stress Test</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

&lt;denchmark-link:https://github.com/rkooo567&gt;@rkooo567&lt;/denchmark-link&gt;
 ran the high stress serve failure test and the test failed in about an hour. The last iteration has the following traceback:
&lt;denchmark-code&gt;E0803 20:47:56.177394  3690  4217 core_worker.cc:415] Will resubmit task after a 5000ms delay: Type=ACTOR_TASK, Language=PYTHON, Resources: {CPU: 1, }, function_descriptor={type=PythonFunctionDescriptor, module_name=ray.serve.controller, class_name=ServeController, function_name=create_backend, function_hash=}, task_id=6bd48a388a1ece6f45b95b1c0100, job_id=0100, num_args=6, num_returns=2, actor_task_spec={actor_id=45b95b1c0100, actor_caller_id=ffffffffffffffffffffffff0100, actor_counter=5926}
2020-08-03 20:47:56,188	WARNING worker.py:1134 -- A worker died or was killed while executing task ffffffffffffffff45b95b1c0100.
2020-08-03 20:47:57,190	WARNING worker.py:1134 -- A worker died or was killed while executing task ffffffffffffffffe065d7c00100.
2020-08-03 20:47:57,221	WARNING worker.py:1134 -- WARNING: 6 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.
2020-08-03 20:47:58,194	WARNING worker.py:1134 -- A worker died or was killed while executing task ffffffffffffffff7e41d8370100.
2020-08-03 20:47:59,198	WARNING worker.py:1134 -- A worker died or was killed while executing task ffffffffffffffff90530a270100.
2020-08-03 20:48:00,216	WARNING worker.py:1134 -- A worker died or was killed while executing task ffffffffffffffffb4ee6f6f0100.
2020-08-03 20:48:01,209	ERROR worker.py:1074 -- Possible unhandled error from worker: �[36mray::RayServeWorker_handler.__init__()�[39m (pid=1145, ip=172.31.16.196)
  File "python/ray/_raylet.pyx", line 479, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 429, in ray._raylet.execute_task.function_executor
  File "python/ray/_raylet.pyx", line 1265, in ray._raylet.CoreWorker.run_async_func_in_event_loop
  File "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/concurrent/futures/_base.py", line 425, in result
    return self.__get_result()
  File "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/async_compat.py", line 31, in wrapper
    return func(*args, **kwargs)
  File "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/serve/backend_worker.py", line 115, in __init__
    controller.get_metric_exporter.remote())
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
2020-08-03 20:48:01,217	WARNING worker.py:1134 -- A worker died or was killed while executing task ffffffffffffffff90530a270100.
2020-08-03 20:48:01,217	WARNING worker.py:1134 -- WARNING: 6 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.
2020-08-03 20:48:01,409	WARNING worker.py:1134 -- WARNING: 7 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.
Traceback (most recent call last):
  File "workloads/serve_failure.py", line 119, in &lt;module&gt;
    RandomTest(max_endpoints=(num_nodes * cpus_per_node) - 4).run()
  File "workloads/serve_failure.py", line 102, in run
    random.choices(actions, weights=weights)[0]()
  File "workloads/serve_failure.py", line 79, in create_endpoint
    serve.create_backend(new_endpoint, handler)
  File "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/serve/api.py", line 31, in check
    return f(*args, **kwargs)
  File "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/serve/api.py", line 281, in create_backend
    replica_config))
  File "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/worker.py", line 1538, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError: �[36mray::ServeController.create_backend()�[39m (pid=1148, ip=172.31.16.196)
  File "python/ray/_raylet.pyx", line 479, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 429, in ray._raylet.execute_task.function_executor
  File "python/ray/_raylet.pyx", line 1265, in ray._raylet.CoreWorker.run_async_func_in_event_loop
  File "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/concurrent/futures/_base.py", line 425, in result
    return self.__get_result()
  File "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/serve/controller.py", line 703, in create_backend
    await self.broadcast_backend_config(backend_tag)
  File "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/serve/controller.py", line 780, in broadcast_backend_config
    await asyncio.gather(*broadcast_futures)
ray.exceptions.RayTaskError: �[36mray::RayServeWorker_handler.update_config()�[39m (pid=1145, ip=172.31.16.196)
  File "python/ray/_raylet.pyx", line 439, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 474, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 478, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 479, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 429, in ray._raylet.execute_task.function_executor
  File "python/ray/_raylet.pyx", line 1265, in ray._raylet.CoreWorker.run_async_func_in_event_loop
  File "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/concurrent/futures/_base.py", line 425, in result
    return self.__get_result()
  File "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/async_compat.py", line 31, in wrapper
    return func(*args, **kwargs)
  File "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/serve/backend_worker.py", line 115, in __init__
    controller.get_metric_exporter.remote())
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
2020-08-03 20:48:02,222	WARNING worker.py:1134 -- A worker died or was killed while executing task fffffffffffffffff504de030100.
&lt;/denchmark-code&gt;

My suspicion is that there are some section of the code in controller that doesn't deal with crashes well.
Ray version and other system information (Python version, TensorFlow version, OS):
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
If we cannot run your script, we cannot fix your issue.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='simon-mo' date='2020-09-28T21:06:10Z'>
		Closing stale issue since the latest release passes. Please re-open if it is still a problem.
		</comment>
	</comments>
</bug>