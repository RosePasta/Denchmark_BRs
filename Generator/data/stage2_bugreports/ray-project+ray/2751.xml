<bug id='2751' author='atumanov' open_date='2018-08-27T22:16:09Z' closed_time='2018-08-29T06:53:41Z'>
	<summary>test/actor_test.py::ActorsWithGPUs::testActorMultipleGPUsFromMultipleTasks failing</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macosx
Ray installed from (source or binary): source
Ray version: upstream/master , atumanov/raylet-scheduling-simple
Python version: py2.7.12
Exact command to reproduce:
RAY_USE_XRAY=1 python -m pytest  -v  -s test/actor_test.py::ActorsWithGPUs

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

test/actor_test.py::ActorsWithGPUs::testActorMultipleGPUsFromMultipleTasks may intermittently fail when run on either macosx or linux. The first problematic output captured from the failing test so far:
&lt;denchmark-code&gt;sh: fork: Resource temporarily unavailable
sh: fork: Resource temporarily unavailable
sh: fork: Resource temporarily unavailable
sh: fork: Resource temporarily unavailable
sh: fork: Resource temporarily unavailable
sh: fork: Resource temporarily unavailable
sh: fork: Resource temporarily sh: fork: Resource temporarily unavailable
sh: fork: Resource temporarily unavailable
sh: fork: Resource temporarily unavailable
sh: fork: Resource temporarily unavailable
sh: fork: Resource temporarily unavailable
sh: fork: Resource temporarily unavailable
sh: fork: Resource temporarily unavailable
sh: fork: Resource temporarily unavailable
sh: fork: Resource temporarily unavailable
sh: fork: Resource temporarily unavailable
sh: fork: Resource temporarily unavailable
sh: fork: Resource temporarily unavailable
sh: fork: Resource temporarily unavailable
sh: fork: Resource temporarily unavailable
sh: fork: Resource temporarily unavailable
sh: fork: Resource temporarily unavailable
sh: fork: Resource temporarily unavailable
sh: fork: Resource temporarily unavailable
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='atumanov' date='2018-08-27T22:22:18Z'>
		I've seen this as well. The problem seems most acute on machines with few cores. It could be that we're starting up actors too quickly and that we should reduce the number of concurrent workers that can be started in



ray/src/ray/raylet/worker_pool.cc


        Lines 97 to 105
      in
      b4cba9a






 void WorkerPool::StartWorkerProcess(const Language &amp;language, bool force_start) { 



 // The first condition makes sure that we are always starting up to 



 // num_cpus_ number of processes in parallel. 



 if (static_cast&lt;int&gt;(starting_worker_processes_.size()) &gt;= num_cpus_ &amp;&amp; !force_start) { 



 // Workers have been started, but not registered. Force start disabled -- returning. 



 RAY_LOG(DEBUG) &lt;&lt; starting_worker_processes_.size() 



                    &lt;&lt; " worker processes pending registration"; 



 return; 



   } 





from num_cpus_ to something smaller.
EDIT: Although in this test we artificially pass in a much larger value of num_cpus.
		</comment>
	</comments>
</bug>