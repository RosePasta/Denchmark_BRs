<bug id='561' author='robertnishihara' open_date='2017-05-17T19:25:10Z' closed_time='2017-06-09T22:55:58Z'>
	<summary>Launching more tasks causes failure with retry messages.</summary>
	<description>
The following script fails.
@ray.remote
def f(x):
  return 1

def g(n):
  x = 1
  for i in range(n):
    x = f.remote(x)
  return x

ray.get([g(1000) for _ in range(100)])
It will indefinitely print a message like the following.
&lt;denchmark-code&gt;[WARN] (/Users/rkn/Workspace/ray/src/common/state/table.cc:85) retrying operation task_table_test_and_update, retry_count = -1
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='robertnishihara' date='2017-05-19T04:11:25Z'>
		I just tried this with 10 Redis shards, and the same problem occurs.
		</comment>
		<comment id='2' author='robertnishihara' date='2017-05-19T20:18:33Z'>
		I also tried removing the event log and removing retries (in addition to 10 Redis shards). A local scheduler still gets marked as dead.
		</comment>
		<comment id='3' author='robertnishihara' date='2017-05-26T19:31:05Z'>
		When I say "removing the event log", I mean using the change in &lt;denchmark-link:https://github.com/ray-project/ray/pull/572&gt;#572&lt;/denchmark-link&gt;
. Similarly, when I say "removing retries", I mean using the change in &lt;denchmark-link:https://github.com/ray-project/ray/pull/574&gt;#574&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='4' author='robertnishihara' date='2017-06-08T04:41:56Z'>
		When this happens, the local_scheduler process is stuck at 100% cpu, and appears to be spending most of its time in common/thirdparty/ae/ae.c:aeDeleteTimeEvent(). It looks like each call to aeDeleteTimeEvent() scans the entire event queue, so assuming you eventually want to delete each event, the overall CPU used for event processing is going to grow O(n^2) wrt the size of the queue.
Because of this n^2 thing, I think that launching enough tasks is making timer processing slow enough that the queue grows unboundedly. Indeed it seems that once the queue size hits &gt;6k events, it does start growing unboundedly and you get the retry messages.
TLDR: I think event processing is O(n^2) and can't keep up if there are too many tasks. Thoughts on angle of attack?
		</comment>
		<comment id='5' author='robertnishihara' date='2017-06-09T22:55:58Z'>
		Addressed by &lt;denchmark-link:https://github.com/ray-project/ray/pull/649&gt;#649&lt;/denchmark-link&gt;
.
		</comment>
	</comments>
</bug>