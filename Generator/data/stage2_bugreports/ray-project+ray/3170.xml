<bug id='3170' author='llan-ml' open_date='2018-10-31T13:53:06Z' closed_time='2018-11-10T06:36:24Z'>
	<summary>Trials always fail with RayGetError</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
Ray installed from (source or binary): binary
Ray version: a221f55
Python version: 3.6.5

I run experiments with a lot of trails, but trials fail after run for a while.
All failed trials raise ray.worker.RayGetError in different places of the code, but all are related to Actor.
Here are some samples:
Traceback (most recent call last):
  File "/home/lanlin/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 243, in _process_events
    result = self.trial_executor.fetch_result(trial)
  File "/home/lanlin/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py", line 200, in fetch_result
    result = ray.get(trial_future[0])
  File "/home/lanlin/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/worker.py", line 2257, in get
    raise RayGetError(object_ids, value)
ray.worker.RayGetError: Could not get objectid ObjectID(01000000b4d20acfc3d2a0957da8f94a483252c5). It was created by remote function train which failed with:

Remote function train failed with:

Traceback (most recent call last):
  File "/home/lanlin/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/worker.py", line 801, in _process_task
    *arguments)
  File "/home/lanlin/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/function_manager.py", line 481, in actor_method_executor
    method_returns = method(actor, *args)
  File "/home/lanlin/Workspaces/morrl/maml.py", line 159, in train
    return Agent.__base__.train(self)
  File "/home/lanlin/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/tune/trainable.py", line 146, in train
    result = self._train()
  File "/home/lanlin/Workspaces/morrl/maml.py", line 150, in _train
    fetches = self.optimizer.step()
  File "/home/lanlin/Workspaces/morrl/maml_optimizer.py", line 68, in step
    self.sync_weights()
  File "/home/lanlin/Workspaces/morrl/maml_optimizer.py", line 28, in sync_weights
    e.set_weights.remote(weights) for e in self.remote_evaluators])
  File "/home/lanlin/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/worker.py", line 2249, in get
    raise RayGetError(object_ids[i], value)
ray.worker.RayGetError: Could not get objectid ObjectID(01000000571c3d8c18772aaa96281fd5e490352c). It was created by remote function  which failed with:

Remote function  failed with:

Invalid return value: likely worker died or was killed while executing the task.

Traceback (most recent call last):
  File "/home/lanlin/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 243, in _process_events
    result = self.trial_executor.fetch_result(trial)
  File "/home/lanlin/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py", line 200, in fetch_result
    result = ray.get(trial_future[0])
  File "/home/lanlin/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/worker.py", line 2257, in get
    raise RayGetError(object_ids, value)
ray.worker.RayGetError: Could not get objectid ObjectID(0100000070a8f76287e7e97fdf04f6f3fea6ef14). It was created by remote function train which failed with:

Remote function train failed with:

Traceback (most recent call last):
  File "/llan/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/worker.py", line 801, in _process_task
    *arguments)
  File "/llan/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/function_manager.py", line 481, in actor_method_executor
    method_returns = method(actor, *args)
  File "/llan/Workspaces/morrl/maml.py", line 159, in train
    return Agent.__base__.train(self)
  File "/llan/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/tune/trainable.py", line 146, in train
    result = self._train()
  File "/llan/Workspaces/morrl/maml.py", line 150, in _train
    fetches = self.optimizer.step()
  File "/llan/Workspaces/morrl/maml_optimizer.py", line 48, in step
    for e in self.remote_evaluators]))
  File "/llan/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/worker.py", line 2249, in get
    raise RayGetError(object_ids[i], value)
ray.worker.RayGetError: Could not get objectid ObjectID(01000000b8ad0fd6f340492888c8cd34049722a0). It was created by remote function  which failed with:

Remote function  failed with:

Invalid return value: likely worker died or was killed while executing the task.

Traceback (most recent call last):
  File "/home/lanlin/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 243, in _process_events
    result = self.trial_executor.fetch_result(trial)
  File "/home/lanlin/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py", line 200, in fetch_result
    result = ray.get(trial_future[0])
  File "/home/lanlin/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/worker.py", line 2257, in get
    raise RayGetError(object_ids, value)
ray.worker.RayGetError: Could not get objectid ObjectID(01000000de3f51eb7835f543290efaecdf49b687). It was created by remote function train which failed with:

Remote function train failed with:

Traceback (most recent call last):
  File "/home/llan/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/worker.py", line 801, in _process_task
    *arguments)
  File "/home/llan/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/function_manager.py", line 481, in actor_method_executor
    method_returns = method(actor, *args)
  File "/home/llan/Workspaces/morrl/maml.py", line 159, in train
    return Agent.__base__.train(self)
  File "/home/llan/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/tune/trainable.py", line 146, in train
    result = self._train()
  File "/home/llan/Workspaces/morrl/maml.py", line 150, in _train
    fetches = self.optimizer.step()
  File "/home/llan/Workspaces/morrl/maml_optimizer.py", line 39, in step
    for e in self.remote_evaluators])
  File "/home/llan/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/worker.py", line 2249, in get
    raise RayGetError(object_ids[i], value)
ray.worker.RayGetError: Could not get objectid ObjectID(0100000014d521475a4d89587da8c214381aee91). It was created by remote function inner_update which failed with:

Remote function inner_update failed with:

Traceback (most recent call last):
  File "/home/llan/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/worker.py", line 801, in _process_task
    *arguments)
  File "/home/llan/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/function_manager.py", line 481, in actor_method_executor
    method_returns = method(actor, *args)
  File "/home/llan/Workspaces/morrl/maml_policy_evaluator.py", line 121, in inner_update
    inner_grad_values, inner_infos, samples = self._inner_update_once()
  File "/home/llan/Workspaces/morrl/maml_policy_evaluator.py", line 104, in _inner_update_once
    samples = self.sample()
  File "/home/llan/Workspaces/morrl/maml_policy_evaluator.py", line 100, in sample
    self.reset_sample()
  File "/home/llan/Workspaces/morrl/maml_policy_evaluator.py", line 84, in reset_sample
    async_env.new_obs = async_env.vector_env.vector_reset()
  File "/home/llan/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/rllib/env/vector_env.py", line 76, in vector_reset
    return [e.reset() for e in self.envs]
  File "/home/llan/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/rllib/env/vector_env.py", line 76, in 
    return [e.reset() for e in self.envs]
  File "/home/llan/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/gym/core.py", line 308, in reset
    observation = self.env.reset(**kwargs)
  File "/home/llan/Workspaces/morrl/reset_wrapper.py", line 36, in reset
    reset_args = ray.get(self.reset_args_holder.get.remote())
  File "/home/llan/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/worker.py", line 2257, in get
    raise RayGetError(object_ids, value)
ray.worker.RayGetError: Could not get objectid ObjectID(010000007409f067b5eda6f654084b741f365669). It was created by remote function  which failed with:

Remote function  failed with:

Invalid return value: likely worker died or was killed while executing the task.

	</description>
	<comments>
		<comment id='1' author='llan-ml' date='2018-10-31T17:18:51Z'>
		Invalid return value: likely worker died or was killed while executing the task.
Hey this usually means that the worker crashed unexpectedly without returning an exception, sometimes due to running out of memory. Some tips to debug this: check out /tmp/raylogs/worker-*, or run dmesg to see the kernel log.
This might also be relevant: &lt;denchmark-link:https://ray.readthedocs.io/en/latest/rllib.html#troubleshooting&gt;https://ray.readthedocs.io/en/latest/rllib.html#troubleshooting&lt;/denchmark-link&gt;

If this doesn't turn up anything, do you have a self-contained script that can reproduce this problem?
		</comment>
		<comment id='2' author='llan-ml' date='2018-11-01T03:59:33Z'>
		Ok the segfault is definitely the problem then. Any idea what causes that
to happen? A reproduction script would be great.

You can also capture a core dump by running ulimit -c unlimited to enable
that for your session, and looking for core files in the logs dir (I think?)

+pcmoritz
&lt;denchmark-link:#&gt;â€¦&lt;/denchmark-link&gt;


On Wed, Oct 31, 2018, 8:14 PM lanlin ***@***.***&gt; wrote:
 Here are some log files in /tmp/ray/session/.
 plasma_store_0.err

 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:1000: Allowing the Plasma store to use up to 108.145GB of memory.
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:1030: Starting object store with directory /dev/shm and huge page support disabled
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 13
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 17
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 16
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 14
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 15
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 11
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 12
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 9
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 10
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 50
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 48
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 47
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 43
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 42
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 37
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 39
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 64
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 53
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 46
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 44
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 45
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 51
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 54
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 63
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 38
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 29
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 33
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 28
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 36
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 40
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 73
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 72
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 25
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 32
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 35
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 26
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 34
 /ray/build/external/arrow/src/arrow_ep/cpp/src/plasma/store.cc:609: Disconnecting client on fd 30

 raylet_0.err

 ...
 ...
  store
 E1101 11:00:40.961925 15211 object_manager.cc:765] Create Chunk Failed index = 0: object already exists in the plasma store
 E1101 11:00:40.964740 15215 object_manager.cc:765] Create Chunk Failed index = 0: object already exists in the plasma store
 E1101 11:00:40.966769 15214 object_manager.cc:765] Create Chunk Failed index = 0: object already exists in the plasma store
 E1101 11:00:40.967262 15221 object_manager.cc:765] Create Chunk Failed index = 0: object already exists in the plasma store
 E1101 11:00:41.017674 15210 object_manager.cc:765] Create Chunk Failed index = 0: object already exists in the plasma store
 E1101 11:00:41.053932 15209 object_manager.cc:765] Create Chunk Failed index = 0: object already exists in the plasma store
 E1101 11:00:41.107113 15220 object_manager.cc:765] Create Chunk Failed index = 0: object already exists in the plasma store
 E1101 11:00:41.114253 15209 object_manager.cc:765] Create Chunk Failed index = 0: object already exists in the plasma store
 E1101 11:00:41.240339 15210 object_manager.cc:765] Create Chunk Failed index = 0: object already exists in the plasma store
 E1101 11:00:41.245818 15221 object_manager.cc:765] Create Chunk Failed index = 0: object already exists in the plasma store
 E1101 11:00:41.375005 15215 object_manager.cc:765] Create Chunk Failed index = 0: object already exists in the plasma store
 E1101 11:00:41.379787 15209 object_manager.cc:765] Create Chunk Failed index = 0: object already exists in the plasma store
 E1101 11:00:41.489735 15211 object_manager.cc:765] Create Chunk Failed index = 0: object already exists in the plasma store
 E1101 11:00:41.497563 15218 object_manager.cc:765] Create Chunk Failed index = 0: object already exists in the plasma store
 E1101 11:00:41.617103 15213 object_manager.cc:765] Create Chunk Failed index = 0: object already exists in the plasma store
 E1101 11:00:41.622661 15212 object_manager.cc:765] Create Chunk Failed index = 0: object already exists in the plasma store
 2018-11-01 11:00:42.743367: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA

 redis-shard_0.out

 15072:M 01 Nov 10:53:54.699 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.
 15072:M 01 Nov 10:53:54.699 # Server started, Redis version 3.9.102
 15072:M 01 Nov 10:53:54.699 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
 15072:M 01 Nov 10:53:54.699 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.

 There are no messages in all worker*.err files.

 Also, I check out the output of dmesg, and there is a new log when the
 error occurs:

 [75144.571872] python[33817]: segfault at 58 ip 00007efcc442226f sp 00007efca6de7710 error 4 in _message.cpython-36m-x86_64-linux-gnu.so[7efcc4379000+216000]

 â€”
 You are receiving this because you commented.
 Reply to this email directly, view it on GitHub
 &lt;#3170 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AAA6Stwb8Jtl_G0uOCiXvEojiAHMD0xmks5uqmcogaJpZM4YEH1_&gt;
 .



		</comment>
		<comment id='3' author='llan-ml' date='2018-11-01T04:52:55Z'>
		Sorry, I think I mistook the output of dmesg. The segfault error was generated previously, not when the RayGetError occurs.
For other observations, I confirm again.
What I am doing is to implement &lt;denchmark-link:https://arxiv.org/abs/1703.03400&gt;MAML&lt;/denchmark-link&gt;
 on Ray. For now, I can only reproduce the error using the whole code I have written, rather than a simple script.
If convenient, please check out the &lt;denchmark-link:https://github.com/llan-ml/maml-on-ray&gt;code&lt;/denchmark-link&gt;
, and run . The code works well in a single node mode (i.e., ).
But when distributed, (there are two nodes in my case), the error occurs after about 70 iterations for a trial. Also, when there are some failed trials, the console just stops printing anything.
		</comment>
		<comment id='4' author='llan-ml' date='2018-11-01T08:43:08Z'>
		In the single-node mode, after running about five hours, it shows the following error:
I1101 16:14:07.849241 78905 node_manager.cc:1391] Reconstructing task 000000007c633b8a176a1d419a77a803a16689d6 on client 17c52b458054390d5fc2ed03c9d5faf16776a9e0
W1101 16:14:11.769070 78905 task_dependency_manager.cc:255] Task lease to renew has already expired by -3623ms
W1101 16:14:11.769287 78905 task_dependency_manager.cc:255] Task lease to renew has already expired by -3623ms
W1101 16:14:11.769392 78905 task_dependency_manager.cc:255] Task lease to renew has already expired by -3623ms
W1101 16:14:11.769455 78905 task_dependency_manager.cc:255] Task lease to renew has already expired by -3623ms
W1101 16:14:11.769528 78905 task_dependency_manager.cc:255] Task lease to renew has already expired by -3622ms
W1101 16:14:11.769593 78905 task_dependency_manager.cc:255] Task lease to renew has already expired by -3622ms
W1101 16:14:11.769639 78905 task_dependency_manager.cc:255] Task lease to renew has already expired by -3622ms
W1101 16:14:11.769699 78905 task_dependency_manager.cc:255] Task lease to renew has already expired by -537ms
W1101 16:14:11.769778 78905 task_dependency_manager.cc:255] Task lease to renew has already expired by -509ms
W1101 16:14:11.769837 78905 task_dependency_manager.cc:255] Task lease to renew has already expired by -507ms
W1101 16:14:11.769891 78905 node_manager.cc:232] Last heartbeat was sent 3974 ms ago
F1101 16:14:22.579573 78905 node_manager.cc:342]  Check failed: client_id != gcs_client_-&gt;client_table().GetLocalClientId() Exiting because this node manager has mistakenly been marked dead by the monitor.
*** Check failure stack trace: ***
    @           0x55ac46  google::LogMessage::Fail()
    @           0x55ab92  google::LogMessage::SendToLog()
    @           0x55a516  google::LogMessage::Flush()
    @           0x55a325  google::LogMessage::~LogMessage()
    @           0x4cc0a8  ray::RayLog::~RayLog()
    @           0x4f98b2  ray::raylet::NodeManager::ClientRemoved()
    @           0x4a2621  ray::gcs::ClientTable::HandleNotification()
    @           0x4a2eab  _ZNSt17_Function_handlerIFvPN3ray3gcs14AsyncGcsClientERKNS0_8UniqueIDERKSt6vectorI16ClientTableDataTSaIS8_EEEZZNS1_11ClientTable7ConnectERKS8_ENKUlS3_S6_SG_E_clES3_S6_SG_EUlS3_S6_SC_E_E9_M_invokeERKSt9_Any_dataS3_S6_SC_
    @           0x4b0ebc  _ZZN3ray3gcs3LogINS_8UniqueIDE15ClientTableDataE9SubscribeERKS2_S6_RKSt8functionIFvPNS0_14AsyncGcsClientES6_RKSt6vectorI16ClientTableDataTSaISB_EEEERKS7_IFvS9_EEENKUlRKSsE_clESP_
    @           0x4c7c89  (anonymous namespace)::ProcessCallback()
    @           0x4c8ba8  ray::gcs::SubscribeRedisCallback()
    @           0x512a96  redisProcessCallbacks
    @           0x4cbb33  RedisAsioClient::handle_read()
    @           0x4cbfc1  boost::asio::detail::reactive_null_buffers_op&lt;&gt;::do_complete()
    @           0x48ca51  boost::asio::detail::task_io_service::run()
    @           0x48513f  main
    @     0x7fc369fc4830  __libc_start_main
    @           0x489811  (unknown)

Does this happen often?
		</comment>
		<comment id='5' author='llan-ml' date='2018-11-01T17:06:12Z'>
		This looks like a raylet task leases bug &lt;denchmark-link:https://github.com/stephanie-wang&gt;@stephanie-wang&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='llan-ml' date='2018-11-01T17:55:55Z'>
		Hmm actually it looks like the main issue might be that the raylet backend is falling behind. This line means that the node is sending heartbeats too far apart:
W1101 16:14:11.769891 78905 node_manager.cc:232] Last heartbeat was sent 3974 ms ago
If the heartbeats get delayed by too much, the node will be assumed to be dead, which is probably why you're not seeing any more output.
&lt;denchmark-link:https://github.com/llan-ml&gt;@llan-ml&lt;/denchmark-link&gt;
, are those the full logs from the node manager? Can you send the rest if not?
		</comment>
		<comment id='7' author='llan-ml' date='2018-11-02T01:32:53Z'>
		Thanks &lt;denchmark-link:https://github.com/llan-ml&gt;@llan-ml&lt;/denchmark-link&gt;
 . Btw, as a workaround you can use an older version of Ray (0.5.3) with pip install ray==0.5.3.
		</comment>
		<comment id='8' author='llan-ml' date='2018-11-02T04:14:28Z'>
		&lt;denchmark-link:https://github.com/stephanie-wang&gt;@stephanie-wang&lt;/denchmark-link&gt;
 The error happens suddenly just following the normal printing, and then the driver process is still hanging up without exit. I also check out the files in /tmp/ray/session/*, they are almost the same with the above log samples.
		</comment>
		<comment id='9' author='llan-ml' date='2018-11-02T07:37:36Z'>
		&lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
 I install ray==0.5.3 and then replace the files of Tune and RLlib with the latest ones. The distributed tuning works normally now.
		</comment>
		<comment id='10' author='llan-ml' date='2018-11-07T14:31:08Z'>
		Hi &lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
 I am just wondering which commit fixes this issue.
		</comment>
		<comment id='11' author='llan-ml' date='2018-11-07T17:07:29Z'>
		&lt;denchmark-link:https://github.com/ray-project/ray/pull/3217&gt;#3217&lt;/denchmark-link&gt;
 I believe
		</comment>
		<comment id='12' author='llan-ml' date='2018-11-08T02:25:01Z'>
		I ran a tune experiment again, and found several problems:

RayGetError still occurs.

Remote function ï¿½[31mtrainï¿½[39m failed with:

Traceback (most recent call last):
  File "/home1/lanlin/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/worker.py", line 801, in _process_task
    *arguments)
  File "/home1/lanlin/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/function_manager.py", line 481, in actor_method_executor
    method_returns = method(actor, *args)
  File "/home1/lanlin/Workspaces/morrl/maml.py", line 159, in train
    return Agent.__base__.train(self)
  File "/home1/lanlin/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/tune/trainable.py", line 146, in train
    result = self._train()
  File "/home1/lanlin/Workspaces/morrl/maml.py", line 150, in _train
    fetches = self.optimizer.step()
  File "/home1/lanlin/Workspaces/morrl/maml_optimizer.py", line 39, in step
    for e in self.remote_evaluators])
  File "/home1/lanlin/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/worker.py", line 2280, in get
    raise RayGetError(object_ids[i], value)
ray.worker.RayGetError: Could not get objectid ObjectID(0100000044679ec05fcb1f23b41fb96d582fe3ad). It was created by remote function ï¿½[31mï¿½[39m which failed with:

Remote function ï¿½[31mï¿½[39m failed with:

Invalid return value: likely worker died or was killed while executing the task.

Error processing event.
Traceback (most recent call last):
  File "/home/lanlin/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 243, in _process_events
    result = self.trial_executor.fetch_result(trial)
  File "/home/lanlin/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py", line 200, in fetch_result
    result = ray.get(trial_future[0])
  File "/home/lanlin/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/worker.py", line 2288, in get
    raise RayGetError(object_ids, value)
ray.worker.RayGetError: Could not get objectid ObjectID(0100000076c9076f672dbed028df3f9a8b7f8086). It was created by remote function ï¿½[31mtrainï¿½[39m which failed with:

Remote function ï¿½[31mtrainï¿½[39m failed with:

Traceback (most recent call last):
  File "/home1/lanlin/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/worker.py", line 801, in _process_task
    *arguments)
  File "/home1/lanlin/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/function_manager.py", line 481, in actor_method_executor
    method_returns = method(actor, *args)
  File "/home1/lanlin/Workspaces/morrl/maml.py", line 159, in train
    return Agent.__base__.train(self)
  File "/home1/lanlin/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/tune/trainable.py", line 146, in train
    result = self._train()
  File "/home1/lanlin/Workspaces/morrl/maml.py", line 150, in _train
    fetches = self.optimizer.step()
  File "/home1/lanlin/Workspaces/morrl/maml_optimizer.py", line 39, in step
    for e in self.remote_evaluators])
  File "/home1/lanlin/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/ray/worker.py", line 2280, in get
    raise RayGetError(object_ids[i], value)
ray.worker.RayGetError: Could not get objectid ObjectID(0100000044679ec05fcb1f23b41fb96d582fe3ad). It was created by remote function ï¿½[31mï¿½[39m which failed with:

Remote function ï¿½[31mï¿½[39m failed with:

Invalid return value: likely worker died or was killed while executing the task.

Log sync requires cluster to be setup with `ray create_or_update`.
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 84/112 CPUs, 0/4 GPUs
Result logdir: /ray_results/MetaSGD_PointEnv
ERROR trials:
 - MetaSGD_PointEnv_11_inner_grad_clip=20.0,random_seed=1,vf_loss_coeff=0.02:	ERROR, 1 failures: /ray_results/MetaSGD_PointEnv/MetaSGD_PointEnv_11_inner_grad_clip=20.0,random_seed=1,vf_loss_coeff=0.02_2018-11-08_00-35-41_ab8mnzf/error_2018-11-08_00-41-15.txt [pid=9444], 300 s, 55 iter
PENDING trials:
 - MetaSGD_PointEnv_15_inner_grad_clip=20.0,random_seed=3,vf_loss_coeff=0.02:	PENDING
 - MetaSGD_PointEnv_16_inner_grad_clip=10.0,random_seed=4,vf_loss_coeff=0.02:	PENDING
 - MetaSGD_PointEnv_17_inner_grad_clip=20.0,random_seed=4,vf_loss_coeff=0.02:	PENDING
 - MetaSGD_PointEnv_18_inner_grad_clip=10.0,random_seed=5,vf_loss_coeff=0.02:	PENDING
  ... 49977 not shown
 - MetaSGD_PointEnv_49996_inner_grad_clip=10.0,random_seed=4,vf_loss_coeff=0.2:	PENDING
 - MetaSGD_PointEnv_49997_inner_grad_clip=20.0,random_seed=4,vf_loss_coeff=0.2:	PENDING
 - MetaSGD_PointEnv_49998_inner_grad_clip=10.0,random_seed=5,vf_loss_coeff=0.2:	PENDING
 - MetaSGD_PointEnv_49999_inner_grad_clip=20.0,random_seed=5,vf_loss_coeff=0.2:	PENDING
RUNNING trials:
 - MetaSGD_PointEnv_10_inner_grad_clip=10.0,random_seed=1,vf_loss_coeff=0.02:	RUNNING [pid=7065], 1643 s, 2 iter
 - MetaSGD_PointEnv_12_inner_grad_clip=10.0,random_seed=2,vf_loss_coeff=0.02:	RUNNING [pid=15965], 253 s, 46 iter
 - MetaSGD_PointEnv_13_inner_grad_clip=20.0,random_seed=2,vf_loss_coeff=0.02:	RUNNING [pid=14681], 242 s, 42 iter
 - MetaSGD_PointEnv_14_inner_grad_clip=10.0,random_seed=3,vf_loss_coeff=0.02:	RUNNING [pid=21923], 229 s, 42 iter
TERMINATED trials:
 - MetaSGD_PointEnv_0_inner_grad_clip=10.0,random_seed=1,vf_loss_coeff=0.01:	TERMINATED [pid=4138], 1816 s, 300 iter
 - MetaSGD_PointEnv_1_inner_grad_clip=20.0,random_seed=1,vf_loss_coeff=0.01:	TERMINATED [pid=1332], 1816 s, 300 iter
 - MetaSGD_PointEnv_2_inner_grad_clip=10.0,random_seed=2,vf_loss_coeff=0.01:	TERMINATED [pid=4128], 1736 s, 300 iter
 - MetaSGD_PointEnv_3_inner_grad_clip=20.0,random_seed=2,vf_loss_coeff=0.01:	TERMINATED [pid=1326], 1820 s, 300 iter
  ... 2 not shown
 - MetaSGD_PointEnv_6_inner_grad_clip=10.0,random_seed=4,vf_loss_coeff=0.01:	TERMINATED [pid=7321], 3206 s, 300 iter
 - MetaSGD_PointEnv_7_inner_grad_clip=20.0,random_seed=4,vf_loss_coeff=0.01:	TERMINATED [pid=11478], 3222 s, 300 iter
 - MetaSGD_PointEnv_8_inner_grad_clip=10.0,random_seed=5,vf_loss_coeff=0.01:	TERMINATED [pid=10803], 3214 s, 300 iter
 - MetaSGD_PointEnv_9_inner_grad_clip=20.0,random_seed=5,vf_loss_coeff=0.01:	TERMINATED [pid=7082], 1579 s, 300 iter


Some trials were running slowly, mainly because a trial may consume a lot of time at some iteration.

RUNNING trials:
 - MetaSGD_PointEnv_69_inner_grad_clip=20.0,random_seed=5,vf_loss_coeff=0.02:	RUNNING [pid=23650], 5369 s, 187 iter
 - MetaSGD_PointEnv_70_inner_grad_clip=10.0,random_seed=1,vf_loss_coeff=0.05:	RUNNING [pid=23585], 5379 s, 187 iter
 - MetaSGD_PointEnv_73_inner_grad_clip=20.0,random_seed=2,vf_loss_coeff=0.05:	RUNNING [pid=24634], 743 s, 126 iter
 - MetaSGD_PointEnv_74_inner_grad_clip=10.0,random_seed=3,vf_loss_coeff=0.05:	RUNNING
 - MetaSGD_PointEnv_75_inner_grad_clip=20.0,random_seed=3,vf_loss_coeff=0.05:	RUNNING


The building of trials quite slowly. My experiment with 50 trials took several minutes to build all 50 trials.

It seems that the s3 version I used is &lt;denchmark-link:https://github.com/ray-project/ray/commit/80f63696ac250aa2b3342ddf26c7e94ade399bdb&gt;80f6369&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='llan-ml' date='2018-11-08T03:31:57Z'>
		If the cluster is still healthy, then the crash is probably in the application code. Usual things to check are memory usage, resource contention, etc?
The slow building is weird though, perhaps use ray stack to determine what is going on during the building.
&lt;denchmark-code&gt;  ... 49977 not shown
&lt;/denchmark-code&gt;

This is 50000 not 50 though right?
		</comment>
		<comment id='14' author='llan-ml' date='2018-11-08T04:46:26Z'>
		oh, sorry. I forgot to comment the argument "num_samples=1000", which I used with HyperOptSearch. I'll try again.
		</comment>
		<comment id='15' author='llan-ml' date='2018-11-10T06:36:24Z'>
		I installed the version &lt;denchmark-link:https://github.com/ray-project/ray/commit/9dd3eedbac31d93cc32e9e87d03e8d8da1507fa6&gt;9dd3eed&lt;/denchmark-link&gt;
, and the tuning works now without . However, &lt;denchmark-link:https://github.com/ray-project/ray/commit/80f63696ac250aa2b3342ddf26c7e94ade399bdb&gt;80f6369&lt;/denchmark-link&gt;
 still raises this error. Perhaps some commits between these two commits solve this issue.
		</comment>
	</comments>
</bug>