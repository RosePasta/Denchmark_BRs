<bug id='670' author='robertnishihara' open_date='2017-06-14T23:57:23Z' closed_time='2017-06-16T23:54:38Z'>
	<summary>Segfault in plasma manager when getting many objects on multiple machines.</summary>
	<description>
The plasma manager can be made to segfault as follows.


Start a large number of nodes (it may actually be possible to reproduce this on one or two nodes). However, on 100 m4.xlarge instances, this happens consistently.


Start Ray on all of the machines. You may want to start the Ray processes on the head node separately so that you can start the plasma manager in GDB. To do this, follow the instructions in #108 (comment).


Run the following workload (after initializing Ray).
@ray.remote
def f():
    return 1

%time l = ray.get([f.remote() for _ in range(1000000)])


This will eventually (maybe after a couple minutes) cause the plasma manager on the head node to segfault. A backtrace looks like the following.
&lt;denchmark-code&gt;Program received signal SIGSEGV, Segmentation fault.
sdscatlen (s=0xbfaaa60c00000018 &lt;error: Cannot access memory at address 0xbfaaa60c00000018&gt;, t=t@entry=0x2e7cbd38, 
    len=len@entry=103) at sds.c:239
239	    size_t curlen = sdslen(s);
(gdb) 
(gdb) bt
#0  sdscatlen (s=0xbfaaa60c00000018 &lt;error: Cannot access memory at address 0xbfaaa60c00000018&gt;, t=t@entry=0x2e7cbd38, 
    len=len@entry=103) at sds.c:239
#1  0x0000000000468280 in __redisAppendCommand (c=c@entry=0x6a78b0, 
    cmd=cmd@entry=0x2e7cbd38 "*3\r\n$38\r\nRAY.OBJECT_TABLE_REQUEST_NOTIFICATIONS\r\n$20\r\np", len=len@entry=103)
    at hiredis.c:910
#2  0x000000000046a643 in __redisAsyncCommand (ac=ac@entry=0x6a78b0, fn=fn@entry=
    0x441d07 &lt;redis_object_table_request_notifications_callback(redisAsyncContext*, void*, void*)&gt;, 
    privdata=privdata@entry=0x4fa7a, cmd=0x2e7cbd38 "*3\r\n$38\r\nRAY.OBJECT_TABLE_REQUEST_NOTIFICATIONS\r\n$20\r\np", 
    len=&lt;optimized out&gt;) at async.c:642
#3  0x000000000046bc36 in redisAsyncCommandArgv (ac=0x6a78b0, 
    fn=0x441d07 &lt;redis_object_table_request_notifications_callback(redisAsyncContext*, void*, void*)&gt;, privdata=0x4fa7a, 
    argc=&lt;optimized out&gt;, argv=&lt;optimized out&gt;, argvlen=&lt;optimized out&gt;) at async.c:679
#4  0x00000000004420a1 in redis_object_table_request_notifications (callback_data=0x2e7cbc80)
    at /home/ubuntu/ray/src/common/state/redis.cc:820
#5  0x000000000044afde in init_table_callback (db_handle=0x6a6180, id=..., 
    label=0x4722a0 &lt;object_table_request_notifications(DBHandle*, int, UniqueID*, RetryInfo*)::__func__&gt; "object_table_request_notifications", data=0x2ffe4590, retry=0x471e80 &lt;default_retry&gt;, done_callback=0x0, 
    retry_callback=0x441f1e &lt;redis_object_table_request_notifications(TableCallbackData*)&gt;, user_context=0x0)
    at /home/ubuntu/ray/src/common/state/table.cc:49
#6  0x000000000044d775 in object_table_request_notifications (db_handle=0x6a6180, num_object_ids=166503, object_ids=0x2cbccea0, 
    retry=0x0) at /home/ubuntu/ray/src/common/state/object_table.cc:86
#7  0x0000000000429603 in fetch_timeout_handler (loop=&lt;optimized out&gt;, id=&lt;optimized out&gt;, context=0x698c70)
    at /home/ubuntu/ray/src/plasma/plasma_manager.cc:1017
#8  0x00000000004508e5 in processTimeEvents (eventLoop=0x698cd0) at /home/ubuntu/ray/src/common/thirdparty/ae/ae.c:322
#9  0x0000000000450c2b in aeProcessEvents (eventLoop=0x698cd0, flags=3) at /home/ubuntu/ray/src/common/thirdparty/ae/ae.c:423
#10 0x0000000000450d7b in aeMain (eventLoop=0x698cd0) at /home/ubuntu/ray/src/common/thirdparty/ae/ae.c:455
#11 0x000000000042daa8 in event_loop_run (loop=0x698cd0) at /home/ubuntu/ray/src/common/event_loop.cc:58
#12 0x000000000042c567 in start_server (store_socket_name=store_socket_name@entry=0x7fffffffe78c "/tmp/s1", 
    manager_socket_name=manager_socket_name@entry=0x7fffffffe797 "/tmp/m1", 
    master_addr=master_addr@entry=0x7fffffffe7b6 "172.31.4.42", port=port@entry=23894, 
    redis_primary_addr=redis_primary_addr@entry=0x7fffffffe3a0 "172.31.4.42", redis_primary_port=6379)
    at /home/ubuntu/ray/src/plasma/plasma_manager.cc:1644
#13 0x0000000000424036 in main (argc=11, argv=0x7fffffffe4d8) at /home/ubuntu/ray/src/plasma/plasma_manager.cc:1725
&lt;/denchmark-code&gt;

Note that it is also common to see error messages like the following printed by the redis servers.
&lt;denchmark-code&gt;10223:M 11 Jun 05:43:55.992 # Client id=13 addr=172.31.8.8:51404 fd=10 name= age=215 idle=0 flags=N db=0 sub=1 psub=0 multi=-1 qbuf=0 qbuf-free=0 obl=0 oll=8182 omem=134217827 events=rw cmd=subscribe scheduled to be closed ASAP for overcoming of output buffer limits.
&lt;/denchmark-code&gt;

However, these seem more likely to be symptoms of the plasma manager dying than the cause of the problem itself (I think I only see these messages printed a little while after the manager has died).
This also causes the script to die with an error like the following.
&lt;denchmark-code&gt;/home/ubuntu/ray/src/plasma/plasma_extension.cc213 Check failed: _s.ok() Bad status: IOError: Broken pipe
&lt;/denchmark-code&gt;

Again, this is most likely a symptom of the plasma manager dying.
	</description>
	<comments>
		<comment id='1' author='robertnishihara' date='2017-06-15T20:08:29Z'>
		Running the workload again, I saw a SIGPIPE first before seeing the memory access problem.
&lt;denchmark-code&gt;Program received signal SIGPIPE, Broken pipe.
0x00007ffff7bca4a0 in __write_nocancel () at ../sysdeps/unix/syscall-template.S:84
84	../sysdeps/unix/syscall-template.S: No such file or directory.
(gdb) 
(gdb) bt
#0  0x00007ffff7bca4a0 in __write_nocancel () at ../sysdeps/unix/syscall-template.S:84
#1  0x00000000004680ab in redisBufferWrite (c=c@entry=0x6a78b0, done=done@entry=0x7fffffffd1f4) at hiredis.c:839
#2  0x000000000046b9d5 in redisAsyncHandleWrite (ac=0x6a78b0) at async.c:550
#3  0x000000000043b6c1 in redisAeWriteEvent (el=0x698cd0, fd=11, privdata=0x6a71b0, mask=3)
    at /home/ubuntu/ray/src/common/cmake/../thirdparty/hiredis/adapters/ae.h:56
#4  0x0000000000450c01 in aeProcessEvents (eventLoop=0x698cd0, flags=3) at /home/ubuntu/ray/src/common/thirdparty/ae/ae.c:416
#5  0x0000000000450d7b in aeMain (eventLoop=0x698cd0) at /home/ubuntu/ray/src/common/thirdparty/ae/ae.c:455
#6  0x000000000042daa8 in event_loop_run (loop=0x698cd0) at /home/ubuntu/ray/src/common/event_loop.cc:58
#7  0x000000000042c567 in start_server (store_socket_name=store_socket_name@entry=0x7fffffffe78c "/tmp/s1", 
    manager_socket_name=manager_socket_name@entry=0x7fffffffe797 "/tmp/m1", 
    master_addr=master_addr@entry=0x7fffffffe7b6 "172.31.4.42", port=port@entry=23894, 
    redis_primary_addr=redis_primary_addr@entry=0x7fffffffe3a0 "172.31.4.42", redis_primary_port=6379)
    at /home/ubuntu/ray/src/plasma/plasma_manager.cc:1644
#8  0x0000000000424036 in main (argc=11, argv=0x7fffffffe4d8) at /home/ubuntu/ray/src/plasma/plasma_manager.cc:1725
(gdb) c
Continuing.

Program received signal SIGSEGV, Segmentation fault.
sdscatlen (s=0x24b493200000018 &lt;error: Cannot access memory at address 0x24b493200000018&gt;, t=t@entry=0x3144d9f8, len=len@entry=103) at sds.c:239
239	    size_t curlen = sdslen(s);
&lt;/denchmark-code&gt;

One possibility is that Redis is actually killing the plasma manager connection first, which triggers a SIGPIPE, which then triggers a memory access problem.
Note that the manager ignores sigpipes due to the following line.
signal(SIGPIPE, SIG_IGN);
		</comment>
		<comment id='2' author='robertnishihara' date='2017-06-16T00:27:10Z'>
		Ok, I tried starting the Redis servers in separate terminal windows, and right before the segfault is caught in gdb in the plasma manager, the Redis server (not the primary shard), prints Killed.
it looks like the problem was the following. The non-primary Redis shard was using more and more memory (I checked this in top). In this case the head node had about 7GB memory (c4.xlarge). Eventually the Redis shard used too much memory and was killed.
This led to a sigpipe happening in the plasma manager (which we ignored because of the call to signal(SIGPIPE, SIG_IGN);, we need to do this because sigpipes happen whenever a client disconnects), which then led to a segfault in the manager.
Running the same workload with a larger head node (m4.16xlarge) finished getting the one million tasks in about 1.5 minutes (though it's possible that the larger node size changed the number of tasks that were executed locally in a meaningful way).
Note: I'm running the same workload with 10 million tasks instead of 1 million. It finished after 23 minutes, but it looks like at some point the non-primary Redis shard dropped a subscription client (presumably the plasma manager on the head node). The problem could be that the plasma manager really isn't able to keep up with Redis since it is requesting notifications about several hundred thousand objects every second and it's also doing a bunch of computation to receive objects and also to receive notifications from the local object store.
Note: When running the manager in gdb, I noticed that it was very slow to process notifications about newly arrived objects from the local object store (e.g., a workload would fetch 100000 objects, and the objects would arrive pretty quickly, but then the manager would continue requesting object notifications from Redis as the notifications from the store slowly trickled in to the manager. Eventually it would receive all of the notifications from the store and would stop sending requests to Redis.
		</comment>
		<comment id='3' author='robertnishihara' date='2017-06-16T23:54:38Z'>
		Closing this because the problem appears to be that a Redis server used up too much memory and got killed.
Note: It looks like we have terrible error messages when a Redis server dies (e.g., this shows up as a segfault in the plasma manager instead of a SIGPIPE or something more reasonable. We should figure out how to improve these error messages.
Note that while debugging this problem, I came across the bug in &lt;denchmark-link:https://github.com/ray-project/ray/issues/677&gt;#677&lt;/denchmark-link&gt;
.
		</comment>
	</comments>
</bug>