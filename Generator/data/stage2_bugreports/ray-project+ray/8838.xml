<bug id='8838' author='hartikainen' open_date='2020-06-08T17:25:39Z' closed_time='2020-06-30T07:55:10Z'>
	<summary>[tune] gpus unused in local mode (`CUDA_VISIBLE_DEVICES == ''`)</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

When running Tune in local mode with ray==0.8.5, the CUDA_VISIBLE_DEVICES are always set to empty string, thus making the gpus unused. This behavior is different from 0.8.4, in which the CUDA_VISIBLE_DEVICES are the same both with local_mode=True and local_mode=False. Same thing happens with the latest wheels.
Ray version and other system information (Python version, TensorFlow version, OS):
&lt;denchmark-code&gt;$ pip freeze | grep "tensorflow\|tf\|ray"
ray==0.8.5
tensorflow==2.1.0
tensorflow-estimator==2.1.0
tensorflow-probability==0.9.0
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

import os

import ray
from ray import tune
import sys

expected = os.getenv('CUDA_VISIBLE_DEVICES')


class DebugRunner(tune.Trainable):
    def _setup(self, config):
        pass

    def _train(self):
        import os
        import tensorflow as tf
        cuda_visible_devices = os.getenv('CUDA_VISIBLE_DEVICES')
        gpus = tf.config.experimental.list_physical_devices('GPU')
        print(gpus)
        assert cuda_visible_devices == expected, (
            expected, cuda_visible_devices)
        return {
            'done': True,
            'cuda_visible_devices': cuda_visible_devices,
            'gpus': gpus,
        }


ray.init(local_mode=bool(int(sys.argv[1])))
tune.run(DebugRunner, resources_per_trial={'gpu': 1})
Here's what the outputs look like for different ray version and command-line args passed to the snippet (Some lines omitted for clarity):
&lt;denchmark-h:h2&gt;With ray==0.8.5, local_mode=False&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;$ CUDA_VISIBLE_DEVICES=1 python ./scripts/ray_cuda_issue.py 0  # 0.8.5
...
== Status ==
Memory usage on this node: 307.0/503.8 GiB
Using FIFO scheduling algorithm.
Resources requested: 1/40 CPUs, 1/1 GPUs, 0.0/130.32 GiB heap, 0.0/41.26 GiB objects
Result logdir: /users/krinen/ray_results/DebugRunner
Number of trials: 1 (1 RUNNING)
+-------------------+----------+-------+
| Trial name        | status   | loc   |
|-------------------+----------+-------|
| DebugRunner_00000 | RUNNING  |       |
+-------------------+----------+-------+

...
(pid=11144) 2020-06-08 18:11:23.425322: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:
(pid=11144) pciBusID: 0000:05:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
(pid=11144) coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s
...
(pid=11144) [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
...
(pid=11144) 2020-06-08 18:11:23.447472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
...
Result for DebugRunner_00000:
  cuda_visible_devices: '1'
  date: 2020-06-08_18-11-23
  done: true
  experiment_id: 710f0e270d954de9b430b6ff6866bdfa
  experiment_tag: '0'
  gpus:
  - - /physical_device:GPU:0
    - GPU
  hostname: gandalf
  iterations_since_restore: 1
  node_ip: 163.1.88.121
  pid: 11144
  time_since_restore: 5.4869067668914795
  time_this_iter_s: 5.4869067668914795
  time_total_s: 5.4869067668914795
  timestamp: 1591636283
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: '00000'

== Status ==
Memory usage on this node: 307.9/503.8 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/40 CPUs, 0/1 GPUs, 0.0/130.32 GiB heap, 0.0/41.26 GiB objects
Result logdir: /users/krinen/ray_results/DebugRunner
Number of trials: 1 (1 TERMINATED)
+-------------------+------------+-------+--------+------------------+
| Trial name        | status     | loc   |   iter |   total time (s) |
|-------------------+------------+-------+--------+------------------|
| DebugRunner_00000 | TERMINATED |       |      1 |          5.48691 |
+-------------------+------------+-------+--------+------------------+
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;With ray==0.8.5, local_mode=True:&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;$ CUDA_VISIBLE_DEVICES=1 python ./scripts/ray_cuda_issue.py 1  # 0.8.5
...
2020-06-08 18:11:57.675648: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
...
[]

...
2020-06-08 18:11:57,694 ERROR trial_runner.py:519 -- Trial DebugRunner_00000: Error processing event.
Traceback (most recent call last):
  File "/users/krinen/conda/envs/softlearning-tf2/lib/python3.7/site-packages/ray/tune/trial_runner.py", line 467, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File "/users/krinen/conda/envs/softlearning-tf2/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py", line 431, in fetch_result
    result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)
  File "/users/krinen/conda/envs/softlearning-tf2/lib/python3.7/site-packages/ray/worker.py", line 1515, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(AssertionError): ray::DebugRunner.train() (pid=11658, ip=163.1.88.121)
  File "python/ray/_raylet.pyx", line 463, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 417, in ray._raylet.execute_task.function_executor
  File "/users/krinen/conda/envs/softlearning-tf2/lib/python3.7/site-packages/ray/tune/trainable.py", line 261, in train
    result = self._train()
  File "./scripts/ray_cuda_issue.py", line 21, in _train
AssertionError: ('1', '')
== Status ==
Memory usage on this node: 307.8/503.8 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/40 CPUs, 0/1 GPUs, 0.0/130.32 GiB heap, 0.0/41.26 GiB objects
Result logdir: /users/krinen/ray_results/DebugRunner
Number of trials: 1 (1 ERROR)
+-------------------+----------+-------+
| Trial name        | status   | loc   |
|-------------------+----------+-------|
| DebugRunner_00000 | ERROR    |       |
+-------------------+----------+-------+
Number of errored trials: 1
+-------------------+--------------+-------------------------------------------------------------------------------------------+
| Trial name        |   # failures | error file                                                                                |
|-------------------+--------------+-------------------------------------------------------------------------------------------|
| DebugRunner_00000 |            1 | /users/krinen/ray_results/DebugRunner/DebugRunner_0_2020-06-08_18-11-54uby7h2dg/error.txt |
+-------------------+--------------+-------------------------------------------------------------------------------------------+

Traceback (most recent call last):
  File "./scripts/ray_cuda_issue.py", line 30, in &lt;module&gt;
    tune.run(DebugRunner, resources_per_trial={'gpu': 1})
  File "/users/krinen/conda/envs/softlearning-tf2/lib/python3.7/site-packages/ray/tune/tune.py", line 347, in run
    raise TuneError("Trials did not complete", incomplete_trials)
ray.tune.error.TuneError: ('Trials did not complete', [DebugRunner_00000])
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;With ray==0.8.4, local_mode=True:&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;$ CUDA_VISIBLE_DEVICES=1 python ./scripts/ray_cuda_issue.py 1  # 0.8.4
...
pciBusID: 0000:05:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
...
2020-06-08 18:12:53.499990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
== Status ==
Memory usage on this node: 306.3/503.8 GiB
Using FIFO scheduling algorithm.
Resources requested: 1/40 CPUs, 1/1 GPUs, 0.0/130.27 GiB heap, 0.0/41.26 GiB objects
Result logdir: /users/krinen/ray_results/DebugRunner
Number of trials: 1 (1 RUNNING)
+-------------------+----------+-------+
| Trial name        | status   | loc   |
|-------------------+----------+-------|
| DebugRunner_00000 | RUNNING  |       |
+-------------------+----------+-------+

...
Result for DebugRunner_00000:
  cuda_visible_devices: '1'
  date: 2020-06-08_18-12-53
  done: true
  experiment_id: 119a71b9fb1940ef96834e7ec858d336
  experiment_tag: '0'
  gpus:
  - - /physical_device:GPU:0
    - GPU
  hostname: gandalf
  iterations_since_restore: 1
  node_ip: 163.1.88.121
  pid: 12268
  time_since_restore: 4.360386848449707
  time_this_iter_s: 4.360386848449707
  time_total_s: 4.360386848449707
  timestamp: 1591636373
  timesteps_since_restore: 0
  training_iteration: 1
  trial_id: '00000'

== Status ==
Memory usage on this node: 306.3/503.8 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/40 CPUs, 0/1 GPUs, 0.0/130.22 GiB heap, 0.0/41.26 GiB objects
Result logdir: /users/krinen/ray_results/DebugRunner
Number of trials: 1 (1 TERMINATED)
+-------------------+------------+-------+--------+------------------+
| Trial name        | status     | loc   |   iter |   total time (s) |
|-------------------+------------+-------+--------+------------------|
| DebugRunner_00000 | TERMINATED |       |      1 |          4.36039 |
+-------------------+------------+-------+--------+------------------+
&lt;/denchmark-code&gt;


 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='hartikainen' date='2020-06-08T17:30:27Z'>
		cc &lt;denchmark-link:https://github.com/ijrsvt&gt;@ijrsvt&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>