<bug id='7819' author='felixs8696' open_date='2020-03-30T22:52:12Z' closed_time='2020-11-26T20:22:24Z'>
	<summary>Ray[Rlllib] RNN Model does not work with multiple GPUs. Error: Cannot merge devices with incompatible ids: '/job:localhost/replica:0/task:0/device:GPU:0' and '/job:localhost/replica:0/task:0/device:GPU:1'</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

When I run the &lt;denchmark-link:https://github.com/ray-project/ray/blob/master/rllib/examples/custom_keras_rnn_model.py&gt;https://github.com/ray-project/ray/blob/master/rllib/examples/custom_keras_rnn_model.py&lt;/denchmark-link&gt;
 script using the reproduction script listed below, I get the error message "RuntimeError: GPUs were assigned to this worker by Ray, but TensorFlow reports GPU acceleration is disabled. This could be due to a bad CUDA or TF installation." How can I fix this issue. Is there a certain docker image you guys are using to run, what versions of these packages should I be using?
Ray version and other system information (Python version, TensorFlow version, OS):
Tensorflow 2.1.0
Ray 0.8.2
Rllib 0.8.2
CUDA 10.1
cuDNN 7.6.2
&lt;denchmark-code&gt;2020-03-30 22:44:51,630 ERROR trial_runner.py:513 -- Trial PPO_RepeatAfterMeEnv_0cb1ce6e: Error processing event.
Traceback (most recent call last):
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 459, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py", line 377, in fetch_result
    result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/worker.py", line 1504, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(RuntimeError): ray::PPO.__init__() (pid=120757, ip=172.31.69.1)
  File "python/ray/_raylet.pyx", line 437, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 449, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 450, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 452, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 430, in ray._raylet.execute_task.function_executor
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 86, in __init__
    Trainer.__init__(self, config, env, logger_creator)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 447, in __init__
    super().__init__(config, logger_creator)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/tune/trainable.py", line 172, in __init__
    self._setup(copy.deepcopy(self.config))
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 591, in _setup
    self._init(self.config, self.env_creator)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 105, in _init
    self.config["num_workers"])
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 658, in _make_workers
    logdir=self.logdir)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/rllib/evaluation/worker_set.py", line 60, in __init__
    RolloutWorker, env_creator, policy, 0, self._local_config)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/rllib/evaluation/worker_set.py", line 262, in _make_worker
    _fake_sampler=config.get("_fake_sampler", False))
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py", line 365, in __init__
    "GPUs were assigned to this worker by Ray, but "
RuntimeError: GPUs were assigned to this worker by Ray, but TensorFlow reports GPU acceleration is disabled. This could be due to a bad CUDA or TF installation.
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Reproduction&lt;/denchmark-h&gt;


[v] I have verified my script runs in a clean environment and reproduces the issue.
[v] I have verified the issue also occurs with the latest wheels.

Reproduction script
&lt;denchmark-code&gt;#!/usr/bin/env bash

python3 -m venv .env
source .env/bin/activate
pip install --upgrade pip setuptools wheel
pip install tensorflow-gpu==2.1.0
#Install [rllib] dependencies
pip install ray==0.8.2
pip install ray[rllib]==0.8.2
pip install pandas
sed -i '159i \            "num_gpus": 2,' .env/lib/python3.6/site-packages/ray/rllib/examples/custom_keras_rnn_model.py

sudo apt-get update &amp;&amp; apt-get install -y --no-install-recommends --allow-unauthenticated \
     cuda-command-line-tools-10-1 \
     cuda-cudart-dev-10-1 \
     cuda-cufft-dev-10-1 \
     cuda-curand-dev-10-1 \
     cuda-cusolver-dev-10-1 \
     cuda-cusparse-dev-10-1 \
     libcudnn7=7.6.2.24-1+cuda10.1 \
     libnccl2=2.4.7-1+cuda10.1 \
     libnccl-dev=2.4.7-1+cuda10.1 \
     &amp;&amp; apt-get update \
     &amp;&amp; apt-get install -y --no-install-recommends --allow-unauthenticated --allow-downgrades \
     libcublas10=10.1.0.105-1 \
     libcublas-dev=10.1.0.105-1

python .env/lib/python3.6/site-packages/ray/rllib/examples/custom_keras_rnn_model.py
&lt;/denchmark-code&gt;

Related Issue: &lt;denchmark-link:https://github.com/ray-project/ray/issues/7747&gt;#7747&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='felixs8696' date='2020-03-31T00:42:27Z'>
		Sorry, for that version I did not follow the TensorFlow guidelines on how to enable GPU support properly. After doing so this is the new issue I'm having.
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

When I run the &lt;denchmark-link:https://github.com/ray-project/ray/blob/master/rllib/examples/custom_keras_rnn_model.py&gt;https://github.com/ray-project/ray/blob/master/rllib/examples/custom_keras_rnn_model.py&lt;/denchmark-link&gt;
 script using the reproduction script listed below, I get the error message "Error: Cannot merge devices with incompatible ids: '/job:localhost/replica:0/task:0/device:GPU:0' and '/job:localhost/replica:0/task:0/device:GPU:1'" On other forums people have said that setting  in the  for the  can solve this problem. While you can set this as a top-level config parameter, I don't see option being passed through to the  that is being used in the TFPolicy class which is injected by this line &lt;denchmark-link:https://github.com/ray-project/ray/blob/releases/0.8.2/rllib/policy/dynamic_tf_policy.py#L183&gt;https://github.com/ray-project/ray/blob/releases/0.8.2/rllib/policy/dynamic_tf_policy.py#L183&lt;/denchmark-link&gt;
. Can someone look into this?
Ray version and other system information (Python version, TensorFlow version, OS):
Tensorflow 2.1.0
Ray 0.8.2
Rllib 0.8.2
CUDA 10.1
cuDNN 7.6.4
&lt;denchmark-code&gt;2020-03-31 00:27:50,543 ERROR trial_runner.py:513 -- Trial PPO_RepeatAfterMeEnv_69e12914: Error processing event.
Traceback (most recent call last):
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 459, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py", line 377, in fetch_result
    result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/worker.py", line 1504, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(InternalError): ray::PPO.__init__() (pid=49585, ip=172.31.69.1)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1367, in _do_call
    return fn(*args)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1350, in _run_fn
    self._extend_graph()
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1390, in _extend_graph
    tf_session.ExtendSession(self._session)
tensorflow.python.framework.errors_impl.InternalError: Constraining by assigned device should not cause an error. Original root's assigned device name: /job:localhost/replica:0/task:0/device:GPU:0 node's assigned device name "/job:localho
st/replica:0/task:0/device:GPU:1. Error: Cannot merge devices with incompatible ids: '/job:localhost/replica:0/task:0/device:GPU:0' and '/job:localhost/replica:0/task:0/device:GPU:1'

During handling of the above exception, another exception occurred:

ray::PPO.__init__() (pid=49585, ip=172.31.69.1)
  File "python/ray/_raylet.pyx", line 437, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 449, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 450, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 452, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 430, in ray._raylet.execute_task.function_executor
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 86, in __init__
    Trainer.__init__(self, config, env, logger_creator)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 447, in __init__
    super().__init__(config, logger_creator)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/tune/trainable.py", line 172, in __init__
    self._setup(copy.deepcopy(self.config))
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 591, in _setup
    self._init(self.config, self.env_creator)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 112, in _init
    self.optimizer = make_policy_optimizer(self.workers, config)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/rllib/agents/ppo/ppo.py", line 95, in choose_policy_optimizer
    shuffle_sequences=config["shuffle_sequences"])
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/rllib/optimizers/multi_gpu_optimizer.py", line 120, in __init__
    self.per_device_batch_size, policy.copy))
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/rllib/optimizers/multi_gpu_impl.py", line 91, in __init__
    len(input_placeholders)))
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/rllib/optimizers/multi_gpu_impl.py", line 291, in _setup_device
    graph_obj = self.build_graph(device_input_slices)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/rllib/policy/dynamic_tf_policy.py", line 256, in copy
    TFPolicy._initialize_loss(instance, loss, loss_inputs)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py", line 231, in _initialize_loss
    self._sess.run(tf.global_variables_initializer())
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 960, in run
    run_metadata_ptr)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1183, in _run
    feed_dict_tensor, options, run_metadata)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1361, in _do_run
    run_metadata)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1386, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Constraining by assigned device should not cause an error. Original root's assigned device name: /job:localhost/replica:0/task:0/device:GPU:0 node's assigned device name "/job:localho
st/replica:0/task:0/device:GPU:1. Error: Cannot merge devices with incompatible ids: '/job:localhost/replica:0/task:0/device:GPU:0' and '/job:localhost/replica:0/task:0/device:GPU:1'
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Reproduction&lt;/denchmark-h&gt;


[v] I have verified my script runs in a clean environment and reproduces the issue.
[v] I have verified the issue also occurs with the latest wheels.

Reproduction script
&lt;denchmark-code&gt;#!/usr/bin/env bash

deactivate
sudo rm -rf .env
python3 -m venv .env
source .env/bin/activate
pip install --upgrade pip setuptools wheel
pip install tensorflow==2.1.0

export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64

# Add NVIDIA package repositories
# Add HTTPS support for apt-key
sudo apt-get install -y gnupg-curl
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_10.1.243-1_amd64.deb
sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub
sudo dpkg -i cuda-repo-ubuntu1604_10.1.243-1_amd64.deb
sudo apt-get update
wget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64/nvidia-machine-learning-repo-ubuntu1604_1.0.0-1_amd64.deb
sudo apt install -y ./nvidia-machine-learning-repo-ubuntu1604_1.0.0-1_amd64.deb
sudo apt-get update

# Install NVIDIA driver
# Issue with driver install requires creating /usr/lib/nvidia
sudo mkdir /usr/lib/nvidia
sudo apt-get install -y --no-install-recommends nvidia-418
# Reboot. Check that GPUs are visible using the command: nvidia-smi

# Install development and runtime libraries (~4GB)
sudo apt-get install -y --no-install-recommends \
    cuda-10-1 \
    libcudnn7=7.6.4.38-1+cuda10.1  \
    libcudnn7-dev=7.6.4.38-1+cuda10.1


# Install TensorRT. Requires that libcudnn7 is installed above.
sudo apt-get install -y --no-install-recommends \
    libnvinfer6=6.0.1-1+cuda10.1 \
    libnvinfer-dev=6.0.1-1+cuda10.1 \
    libnvinfer-plugin6=6.0.1-1+cuda10.1

#Install [rllib] dependencies
pip install ray==0.8.2
pip install ray[rllib]==0.8.2
pip install pandas
sed -i '159i \            "num_gpus": 2,' .env/lib/python3.6/site-packages/ray/rllib/examples/custom_keras_rnn_model.py


# Reboot

python .env/lib/python3.6/site-packages/ray/rllib/examples/custom_keras_rnn_model.py
&lt;/denchmark-code&gt;

Related Issue: &lt;denchmark-link:https://github.com/ray-project/ray/issues/7747&gt;#7747&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='felixs8696' date='2020-03-31T10:34:45Z'>
		I'm having the same problem. Although this is not a solution: for now, downgrading tensorflow to 1.14 worked for me.
		</comment>
		<comment id='3' author='felixs8696' date='2020-03-31T23:15:48Z'>
		You can enable soft placement in the TF session options of the trainer config. It's on by default actually &lt;denchmark-link:https://github.com/ray-project/ray/blob/master/rllib/agents/trainer.py#L244&gt;https://github.com/ray-project/ray/blob/master/rllib/agents/trainer.py#L244&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='felixs8696' date='2020-03-31T23:54:48Z'>
		It’s failing even though it’s on. Also the allow soft placement parameter is not passed through to the tensorflow session object used in the TF policy from the dynamic tf policy. You can see from the line I linked.
Also I tried building ray from source and manually adding allow soft placement and it still did not fix the issue.
		</comment>
		<comment id='5' author='felixs8696' date='2020-04-01T00:04:32Z'>
		&lt;denchmark-link:https://github.com/macsmitty&gt;@macsmitty&lt;/denchmark-link&gt;
 when I downgraded to TF 1.14, I got this error:
&lt;denchmark-code&gt;2020-03-31 23:20:19,068 ERROR trial_runner.py:513 -- Trial PPO_RepeatAfterMeEnv_2ba0d328: Error processing event.
Traceback (most recent call last):
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 459, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py", line 377, in fetch_result
    result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/worker.py", line 1504, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): ray::PPO.__init__() (pid=19125, ip=172.31.69.1)
  File "python/ray/_raylet.pyx", line 437, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 449, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 450, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 452, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 430, in ray._raylet.execute_task.function_executor
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 86, in __init__
    Trainer.__init__(self, config, env, logger_creator)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 447, in __init__
    super().__init__(config, logger_creator)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/tune/trainable.py", line 172, in __init__
    self._setup(copy.deepcopy(self.config))
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 591, in _setup
    self._init(self.config, self.env_creator)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 105, in _init
    self.config["num_workers"])
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 658, in _make_workers
    logdir=self.logdir)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/rllib/evaluation/worker_set.py", line 60, in __init__
    RolloutWorker, env_creator, policy, 0, self._local_config)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/rllib/evaluation/worker_set.py", line 262, in _make_worker
    _fake_sampler=config.get("_fake_sampler", False))
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py", line 355, in __init__
    self._build_policy_map(policy_dict, policy_config)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py", line 820, in _build_policy_map
    policy_map[name] = cls(obs_space, act_space, merged_conf)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/rllib/policy/tf_policy_template.py", line 138, in __init__
    obs_include_prev_action_reward=obs_include_prev_action_reward)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/rllib/policy/dynamic_tf_policy.py", line 180, in __init__
    timestep=timestep)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/ray/rllib/utils/exploration/stochastic_sampling.py", line 74, in get_exploration_action
    return self._get_tf_exploration_action_op(action_dist, explore)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py", line 414, in __call__
    self._initialize(args, kwds, add_initializers_to=initializer_map)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py", line 357, in _initialize
    *args, **kwds))
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1349, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1652, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1545, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py", line 715, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py", line 307, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File "/home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py", line 705, in wrapper
    raise e.ag_error_metadata.to_exception(type(e))
TypeError: in converted code:
    relative to /home/ubuntu/dolphin/RAIRayUtils/src/rai_ray_utils/rllib/models/v2/.env/lib/python3.6/site-packages:

    ray/rllib/utils/exploration/stochastic_sampling.py:79 _get_tf_exploration_action_op
        if explore:
    tensorflow/python/framework/ops.py:690 __bool__
        raise TypeError("Using a `tf.Tensor` as a Python `bool` is not allowed. "

    TypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='felixs8696' date='2020-04-01T13:05:00Z'>
		Taking a look now ...
		</comment>
		<comment id='7' author='felixs8696' date='2020-04-06T18:49:43Z'>
		Any updates?
		</comment>
		<comment id='8' author='felixs8696' date='2020-04-08T13:36:34Z'>
		&lt;denchmark-link:https://github.com/felixs8696&gt;@felixs8696&lt;/denchmark-link&gt;
 Have you tried the following combo?
Tensorflow 2.0.0
Ray 0.8.0
CUDA 10.0
cuDNN 7.4
		</comment>
		<comment id='9' author='felixs8696' date='2020-04-26T07:43:50Z'>
		same error here.
Tensorflow 2.1.0
Ray 0.8.4
CUDA 10.1
cuDNN 7.6.3
		</comment>
		<comment id='10' author='felixs8696' date='2020-11-12T20:01:00Z'>
		Hi, I'm a bot from the Ray team :)
To help human contributors to focus on more relevant issues, I will automatically add the stale label to issues that have had no activity for more than 4 months.
If there is no further activity in the 14 days, the issue will be closed!

If you'd like to keep the issue open, just leave any comment, and the stale label will be removed!
If you'd like to get more attention to the issue, please tag one of Ray's contributors.

You can always ask for help on our &lt;denchmark-link:https://discuss.ray.io/&gt;discussion forum&lt;/denchmark-link&gt;
 or &lt;denchmark-link:https://github.com/ray-project/ray#getting-involved&gt;Ray's public slack channel&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='11' author='felixs8696' date='2020-11-26T20:22:19Z'>
		Hi again! The issue will be closed because there has been no more activity in the 14 days since the last message.
Please feel free to reopen or open a new issue if you'd still like it to be addressed.
Again, you can always ask for help on our &lt;denchmark-link:https://discuss.ray.io&gt;discussion forum&lt;/denchmark-link&gt;
 or &lt;denchmark-link:https://github.com/ray-project/ray#getting-involved&gt;Ray's public slack channel&lt;/denchmark-link&gt;
.
Thanks again for opening the issue!
		</comment>
	</comments>
</bug>