<bug id='7412' author='drozzy' open_date='2020-03-03T03:07:47Z' closed_time='2020-03-09T22:53:11Z'>
	<summary>"restore" deletes the restored from checkpoint</summary>
	<description>
If I specify a checkpoint in the tune.run(DQNTrainer, restore=X) - the checkpoint X gets deleted!
So if there is a bug or something during the initialization, I loose my checkpoint and can no longer restart from it!
	</description>
	<comments>
		<comment id='1' author='drozzy' date='2020-03-06T15:59:07Z'>
		When I restore from a checkpoint in the following way, the checkpoint is not deleted for me:
tune.run("DDPG", config=config, restore=checkpoint_path)
I'm on ray v0.8.2 and tensorflow 2.0.0
EDIT nvm also have this problem, didn't check/test properly
		</comment>
		<comment id='2' author='drozzy' date='2020-03-06T22:06:44Z'>
		Hm... are you setting check-pointing to be done during this trial? E.g.:
&lt;denchmark-code&gt;
tune.run(DQNTrainer, 
   restore=checkpoint,
   checkpoint_freq=5, checkpoint_at_end=True, checkpoint_score_attr='episode_reward_mean', keep_checkpoints_num=10)
&lt;/denchmark-code&gt;

Because when I restore it starts a new trial and starts saving checkpoints in a different trial directory (under ~/ray_results), but for some reason it deletes the last checkpoint from the old directory.
		</comment>
		<comment id='3' author='drozzy' date='2020-03-07T07:04:43Z'>
		cc &lt;denchmark-link:https://github.com/ujvl&gt;@ujvl&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='drozzy' date='2020-03-07T16:33:33Z'>
		&lt;denchmark-link:https://github.com/drozzy&gt;@drozzy&lt;/denchmark-link&gt;
 thanks for flagging this, will push a fix this weekend.
		</comment>
	</comments>
</bug>