<bug id='6189' author='zplizzi' open_date='2019-11-19T02:50:12Z' closed_time='2020-02-06T22:44:46Z'>
	<summary>Check failure in ray::raylet::NodeManager::HandleObjectMissing</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
Ray installed from (source or binary): binary
Ray version: 0.7.4
Python version: 3.6.4
Exact command to reproduce: can't consistently reproduce

I get the following errors sometimes when running a hyperparameter tuning experiment with Tune. I'm tuning a RL algorithm (which uses parts of RLlib also, such as nested actors for environment rollouts). I've never seen this issue when running the training directly (not through Tune).
Here's the contents of  on the head node: &lt;denchmark-link:https://github.com/ray-project/ray/files/3861680/ray_logs.zip&gt;ray_logs.zip&lt;/denchmark-link&gt;

Let me know what other info I can provide to debug.
&lt;denchmark-code&gt;(pid=271, ip=192.168.80.248) Traceback (most recent call last):
(pid=271, ip=192.168.80.248)   File "/usr/local/lib/python3.6/dist-packages/ray/workers/default_worker.py", line 98, in &lt;module&gt;
(pid=271, ip=192.168.80.248)     ray.worker.global_worker.main_loop()
(pid=271, ip=192.168.80.248)   File "/usr/local/lib/python3.6/dist-packages/ray/worker.py", line 1134, in main_loop
(pid=271, ip=192.168.80.248)     task = self._get_next_task_from_raylet()
(pid=271, ip=192.168.80.248)   File "/usr/local/lib/python3.6/dist-packages/ray/worker.py", line 1117, in _get_next_task_from_raylet
(pid=271, ip=192.168.80.248)     task = self.raylet_client.get_task()
(pid=271, ip=192.168.80.248)   File "python/ray/_raylet.pyx", line 247, in ray._raylet.RayletClient.get_task
(pid=271, ip=192.168.80.248)   File "python/ray/_raylet.pyx", line 61, in ray._raylet.check_status
(pid=271, ip=192.168.80.248) ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Raylet connection closed.
(pid=271, ip=192.168.80.248)
(pid=271, ip=192.168.80.248) During handling of the above exception, another exception occurred:
(pid=271, ip=192.168.80.248)
(pid=271, ip=192.168.80.248) Traceback (most recent call last):
(pid=271, ip=192.168.80.248)   File "/usr/local/lib/python3.6/dist-packages/ray/workers/default_worker.py", line 105, in &lt;module&gt;
(pid=271, ip=192.168.80.248)     job_id=None)
(pid=271, ip=192.168.80.248)   File "/usr/local/lib/python3.6/dist-packages/ray/utils.py", line 68, in push_error_to_driver
(pid=271, ip=192.168.80.248)     worker.raylet_client.push_error(job_id, error_type, message, time.time())
(pid=271, ip=192.168.80.248)   File "python/ray/_raylet.pyx", line 300, in ray._raylet.RayletClient.push_error
(pid=271, ip=192.168.80.248)   File "python/ray/_raylet.pyx", line 61, in ray._raylet.check_status
(pid=271, ip=192.168.80.248) ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Connection closed unexpectedly.
2019-11-19 02:31:15,861 ERROR worker.py:1791 -- The node with client ID 5308993854613ba26e1b2a1128a1d616538ff531 has been marked dead because the monitor has missed too many heartbeats from it.
2019-11-19 02:31:17,558 ERROR worker.py:1791 -- The node with client ID f6d51b8c28f60765ea6ddb54bf1d1acbd675890f has been marked dead because the monitor has missed too many heartbeats from it.
2019-11-19 02:31:30,706 ERROR trial_runner.py:552 -- Error processing event.
&lt;/denchmark-code&gt;

and
&lt;denchmark-code&gt;(pid=raylet, ip=192.168.66.223) F1119 02:31:03.937857    17 scheduling_queue.cc:318]  Check failed: task_ids.empty()
(pid=raylet, ip=192.168.66.223) *** Check failure stack trace: ***
(pid=raylet, ip=192.168.66.223)     @           0x6f6e3a  google::LogMessage::Fail()
(pid=raylet, ip=192.168.66.223)     @           0x6f8223  google::LogMessage::SendToLog()
(pid=raylet, ip=192.168.66.223)     @           0x6f6b62  google::LogMessage::Flush()
(pid=raylet, ip=192.168.66.223)     @           0x6f6d51  google::LogMessage::~LogMessage()
(pid=raylet, ip=192.168.66.223)     @           0x5293a2  ray::RayLog::~RayLog()
(pid=raylet, ip=192.168.66.223)     @           0x46d659  ray::raylet::SchedulingQueue::MoveTasks()
(pid=raylet, ip=192.168.66.223)     @           0x4608cc  ray::raylet::NodeManager::HandleObjectMissing()
(pid=raylet, ip=192.168.66.223)     @           0x49cdc0  ray::ObjectStoreNotificationManager::ProcessStoreRemove()
(pid=raylet, ip=192.168.66.223)     @           0x49e666  ray::ObjectStoreNotificationManager::ProcessStoreNotification()
(pid=raylet, ip=192.168.66.223)     @           0x49db1a  boost::asio::detail::read_op&lt;&gt;::operator()()
(pid=raylet, ip=192.168.66.223)     @           0x49dd12  boost::asio::detail::reactive_socket_recv_op&lt;&gt;::do_complete()
(pid=raylet, ip=192.168.66.223)     @           0x425bdd  boost::asio::detail::scheduler::run()
(pid=raylet, ip=192.168.66.223)     @           0x40fb20  main
(pid=raylet, ip=192.168.66.223)     @     0x7fe34f6b4b97  __libc_start_main
(pid=raylet, ip=192.168.66.223)     @           0x4207f1  (unknown)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='zplizzi' date='2019-11-20T02:24:43Z'>
		&lt;denchmark-link:https://github.com/zplizzi&gt;@zplizzi&lt;/denchmark-link&gt;
 I wanted to ask if your Wifi / Ethernet connection was turned off or you happened to change the Connection somehow? This could have occurred. Last night I had the same issue, as soon as I turned off the Wifi this crash occurred, since the IP address of the node is the one assigned by the router, it crashes.
		</comment>
		<comment id='2' author='zplizzi' date='2019-11-20T02:27:21Z'>
		Let me know if you can reproduce it by turning off your Wifi / Ethernet.
		</comment>
		<comment id='3' author='zplizzi' date='2019-11-20T02:30:17Z'>
		The whole cluster is in AWS, so the internet connection between nodes
should presumably be rock solid.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Tue, Nov 19, 2019, 6:27 PM ARUNAVO RAY ***@***.***&gt; wrote:
 Let me know if you can reproduce it by turning off your Wifi / Ethernet.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#6189?email_source=notifications&amp;email_token=ABKW56EZSMAOWC3ILSTKLPDQUSOBXA5CNFSM4JO5AUN2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEEQO6XA#issuecomment-555806556&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/ABKW56DPCTZS7UKFRFDRY2LQUSOBXANCNFSM4JO5AUNQ&gt;
 .



		</comment>
		<comment id='4' author='zplizzi' date='2019-11-20T06:19:39Z'>
		Can you try using the latest nightly build? &lt;denchmark-link:https://ray.readthedocs.io/en/latest/installation.html&gt;https://ray.readthedocs.io/en/latest/installation.html&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='zplizzi' date='2019-11-21T02:21:08Z'>
		It's not quickly or reliably reproducible, but I'll try the latest nightly next time I'm doing a few of these larger tuning runs - probably within the next week or two.
		</comment>
		<comment id='6' author='zplizzi' date='2019-11-21T06:57:14Z'>
		I get the exact same error and tried the nightly (0.8.0.dev6) without success.
		</comment>
		<comment id='7' author='zplizzi' date='2019-11-21T20:12:31Z'>
		It looks like somehow there is an internal inconsistency tracking the set of tasks that are in the WAITING state. I don't think this is a recent regression (perhaps &lt;denchmark-link:https://github.com/stephanie-wang&gt;@stephanie-wang&lt;/denchmark-link&gt;
 can confirm), so perhaps it shouldn't block the release.
Fortunately / unfortunately we're migrating Ray away from this code path. If you're feeling adventurous, you can try the env var RAY_FORCE_DIRECT=1 to try out the direct gRPC based actors. Note that you really need the very latest wheels for this to have any chance of working though :)
Otherwise, a script that can be run without external dependencies (i.e., training some dummy model or dummy env) would be needed for tracking the issue down, it would be very hard to debug otherwise.
		</comment>
		<comment id='8' author='zplizzi' date='2019-11-21T20:13:21Z'>
		By the way, can this error be reproduced with a single Ray node cluster?
		</comment>
		<comment id='9' author='zplizzi' date='2019-11-22T02:28:14Z'>
		I'lll try to get a reproducible example.  I was able to avoid the issue by reducing the number of tasks from ~100,000 to ~1,000.  With the large number of tasks it would happen every time, but with only 1000 tasks I never encountered it.
		</comment>
		<comment id='10' author='zplizzi' date='2020-02-06T22:44:46Z'>
		No reproducible error. We saw some issues like this in the release tests but resolved them, so closing for now. Feel free to re-open if the problem persists.
		</comment>
	</comments>
</bug>