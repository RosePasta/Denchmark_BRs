<bug id='10487' author='Patol75' open_date='2020-09-02T02:03:14Z' closed_time='2020-09-04T04:55:38Z'>
	<summary>[core] Potential memory leak with ActorPool</summary>
	<description>
Hi,
I have a Python program which uses the utility class ActorPool to distribute the execution of a class method over an arbitrarily large number of Actors, as many as there are CPUs available. The structure of the code is described in this StackOverflow &lt;denchmark-link:https://stackoverflow.com/a/63556897/10640534&gt;post&lt;/denchmark-link&gt;
. I have noticed that, as the number of executed tasks increases on a given Actor, the memory used by this Actor increases as well. In fact, it never stops. In the case of a local execution on my laptop (Python 3.6.9, Ray 0.8.7, Ubuntu 18.04.5 LTS), requiring 6 Actors/CPUs, I can readily see on the Machine View of the Ray Dashboard that all 6 processes are progressively using more and more RAM, synchronously. However, in the Memory tab, I can only see 6 LOCAL_REFERENCEs and 6 ACTOR_HANDLEs. The same behaviour also occurs during a 192 CPU execution on a university cluster (Python 3.7.4, Ray 0.8.7, CentOS Linux release 8.2.2004 (Core)). This time, as I reach the amount of memory available for the job, I hit a RayOutOfMemoryError:
&lt;denchmark-code&gt;ray.memory_monitor.RayOutOfMemoryError: More than 95% of the memory on node NodeName is used (179.24 / 188.58 GB). The top 10 memory consumers are:

PID     MEM     COMMAND
193020  3.62GiB ray::FooClass.Foo()
193009  3.61GiB ray::FooClass.Foo()
193004  3.61GiB ray::FooClass.Foo()
193027  3.61GiB ray::FooClass.Foo()
193011  3.61GiB ray::FooClass.Foo()
193030  3.6GiB  ray::FooClass.Foo()
193008  3.6GiB  ray::FooClass.Foo()
193015  3.6GiB  ray::FooClass.Foo()
193023  3.6GiB  ray::FooClass.Foo()
193032  3.6GiB  ray::FooClass.Foo()

In addition, up to 0.05 GiB of shared memory is currently being used by the Ray object store. You can set the object store size with the `object_store_memory` parameter when starting Ray.
---
--- Tip: Use the `ray memory` command to list active objects in the cluster.
---
&lt;/denchmark-code&gt;

In comparison, when I monitor the execution through Ray Dashboard on my laptop, each process starts with only 300 MB of memory.
Can anyone suggest what I can do to prevent this behaviour from occurring? I have had a look at garbage collector and atexit packages, but I feel like I do not know how to use them properly as I did not achieve any improvement. I have also thought that I could stop and restart the ActorPool while also deleting the ActorPool reference and the references to the results yielded though pool.map_unordered, but that did not help either. Additionally, I have tried to update the decorator applied to FooClass with @ray.remote(memory=350 * 1024 * 1024), but that only gave me the following warning
&lt;denchmark-code&gt;(pid=20594) 2020-09-02 11:58:35,171	WARNING memory_monitor.py:145 -- Heap memory usage for ray_FooClass_20594 is 0.3323 / 0.3418 GiB limit
&lt;/denchmark-code&gt;

EDIT: Releasing the Actors and re-instantiating them, with the caveat of executing __init__ again, allows to free the memory. But, surely, there has to be a better way?
	</description>
	<comments>
		<comment id='1' author='Patol75' date='2020-09-04T01:27:40Z'>
		Can you share the script in which we can reproduce the issue? The behavior looks definitely weird given you have only 6 objects that are referenced.
		</comment>
		<comment id='2' author='Patol75' date='2020-09-04T02:05:57Z'>
		&lt;denchmark-link:https://github.com/rkooo567&gt;@rkooo567&lt;/denchmark-link&gt;
 I have created this &lt;denchmark-link:https://github.com/Patol75/PotMemLeak.git&gt;repository&lt;/denchmark-link&gt;
 where you can find files required for a minimum reproduction scenario. You can start the script by typing . On my laptop, on 6 cores, this leads to an ever-increasing memory usage. Thank you for any help.
		</comment>
		<comment id='3' author='Patol75' date='2020-09-04T02:23:40Z'>
		Okay. It definitely seems like the memory is increasing
		</comment>
		<comment id='4' author='Patol75' date='2020-09-04T02:24:36Z'>
		I have one question. Is there any possibility you have some variables that keep accumulated over time inside FooClass ?
		</comment>
		<comment id='5' author='Patol75' date='2020-09-04T02:29:35Z'>
		That would be something like a list/dictionary/arrray to which entries are continuously appended? I do not think I have anything of the sort, unfortunately.
		</comment>
		<comment id='6' author='Patol75' date='2020-09-04T02:59:41Z'>
		I ran your class without Ray,
&lt;denchmark-code&gt;a = FooClass(args)
for v in zip(*np.asarray(indArr == 0).nonzero()):
    a.Foo(v)
&lt;/denchmark-code&gt;

And it seems like it also increases memory.
So, I assume there's some memory leak in your class?
		</comment>
		<comment id='7' author='Patol75' date='2020-09-04T03:12:37Z'>
		&lt;denchmark-link:https://github.com/rkooo567&gt;@rkooo567&lt;/denchmark-link&gt;
 You are right, I should have checked that myself, I am sorry. Well, I guess I will have to find out where it comes from then. Thank you for your input; feel free to close this thread.
		</comment>
		<comment id='8' author='Patol75' date='2020-09-04T04:01:21Z'>
		&lt;denchmark-link:https://github.com/rkooo567&gt;@rkooo567&lt;/denchmark-link&gt;
 The memory leak comes from the calls to the Scipy Interpolator objects. According to this &lt;denchmark-link:https://github.com/scipy/scipy/issues/9747&gt;thread&lt;/denchmark-link&gt;
, it is actually a Numpy issue. So definitely nothing to do with Ray. Thanks again for your help.
EDIT: It might even be fixed in the upcoming Numpy 1.19.2 if this &lt;denchmark-link:https://github.com/numpy/numpy/pull/16936&gt;PR&lt;/denchmark-link&gt;
 is included.
EDIT 2: Confirmed, I have compiled the branch associated to the PR, and the leak is gone.
		</comment>
		<comment id='9' author='Patol75' date='2020-10-05T13:27:22Z'>
		&lt;denchmark-link:https://github.com/Patol75&gt;@Patol75&lt;/denchmark-link&gt;
 I think I may have a similar issue and I was trying to follow this thread, but it appears that your github repo was taken down.  Can you share?
		</comment>
		<comment id='10' author='Patol75' date='2020-10-05T23:53:58Z'>
		&lt;denchmark-link:https://github.com/jeffreywillert&gt;@jeffreywillert&lt;/denchmark-link&gt;
 You can simply compile the Numpy branch associated to the PR I mentioned in my previous message and run your code again. It fixed the memory leak I had with Scipy interpolator objects.
		</comment>
	</comments>
</bug>