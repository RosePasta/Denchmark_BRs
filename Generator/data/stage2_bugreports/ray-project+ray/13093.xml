<bug id='13093' author='blueplastic' open_date='2020-12-28T23:30:20Z' closed_time='2020-12-30T17:28:50Z'>
	<summary>[core] Object Spilling to disk throwing ObjectStoreFullError</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

Ray version: 1.2.0.dev0
Python version: 3.7.8
OS: Debian GNU/Linux 10 (buster)
When I store a large object in memory, instead of it gracefully spilling to disk, I get a ObjectStoreFullError:
&lt;denchmark-code&gt;ObjectStoreFullError: Failed to put object ffffffffffffffffffffffffffffffffffffffff0100000001000000 in object store because it is full. Object size is 8388608252 bytes.
The local object store is full of objects that are still in scope and cannot be evicted. Tip: Use the `ray memory` command to list active objects in the cluster.
&lt;/denchmark-code&gt;

Also, perhaps related, after Ray successfully initializes, the Memory tab of the Dashboard is stuck on "Loading":
&lt;denchmark-link:https://user-images.githubusercontent.com/734379/103248710-c9227280-4963-11eb-8e5e-db56e81dcb7c.png&gt;&lt;/denchmark-link&gt;

However, once I place a small string object in Ray's memory, then the Memory page of the Dashboard automatically refreshes and correctly displays the new small object (it breaks out of the Loading page loop).
&lt;denchmark-h:h3&gt;Reproduction&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import ray
import numpy as np
import json
import ray

ray.init(dashboard_host="0.0.0.0", 
    _system_config={
        "automatic_object_spilling_enabled": True,
        "object_spilling_config": json.dumps(
            {"type": "filesystem", "params": {"directory_path": "/tmp/spill"}},
        )
    },)

# At this time, the Dashboard Memory tab is stuck in Loading.

# This works fine
small_name = ray.put("john")

# Now the Dashboard Memory page successfully automatically updates/refreshes, and shows the new tiny string.

# This throws an error
big_array = ray.put(np.zeros(1000 * 1024 * 1024))
&lt;/denchmark-code&gt;

The above ray.put cmd first shows this in the Jupyter cell output while the cmd is executing:
&lt;denchmark-code&gt;(pid=raylet) [2020-12-28 23:26:30,400 E 1849 1849] local_object_manager.cc:166: Spilled 0 MiB, 1 objects, write throughput 0 MiB/s
&lt;/denchmark-code&gt;

Then the Jupyter cell switches to this full error:
&lt;denchmark-code&gt;(pid=raylet) [2020-12-28 23:09:46,866 E 13012 13012] local_object_manager.cc:166: Spilled 0 MiB, 1 objects, write throughput 0 MiB/s
2020-12-28 23:09:56,960	INFO worker.py:1427 -- Put failed since the value was either too large or the store was full of pinned objects.

---------------------------------------------------------------------------
ObjectStoreFullError                      Traceback (most recent call last)
&lt;ipython-input-6-5378e212193d&gt; in &lt;module&gt;
----&gt; 1 big_array = ray.put(np.zeros(1000 * 1024 * 1024))

/opt/conda/lib/python3.7/site-packages/ray/_private/client_mode_hook.py in wrapper(*args, **kwargs)
     45         if client_mode_enabled and _client_hook_enabled:
     46             return getattr(ray, func.__name__)(*args, **kwargs)
---&gt; 47         return func(*args, **kwargs)
     48 
     49     return wrapper

/opt/conda/lib/python3.7/site-packages/ray/worker.py in put(value)
   1422     with profiling.profile("ray.put"):
   1423         try:
-&gt; 1424             object_ref = worker.put_object(value, pin_object=True)
   1425         except ObjectStoreFullError:
   1426             logger.info(

/opt/conda/lib/python3.7/site-packages/ray/worker.py in put_object(self, value, object_ref, pin_object)
    276             self.core_worker.put_serialized_object(
    277                 serialized_value, object_ref=object_ref,
--&gt; 278                 pin_object=pin_object))
    279 
    280     def deserialize_objects(self, data_metadata_pairs, object_refs):

python/ray/_raylet.pyx in ray._raylet.CoreWorker.put_serialized_object()

python/ray/_raylet.pyx in ray._raylet.CoreWorker._create_put_buffer()

python/ray/_raylet.pyx in ray._raylet.check_status()

ObjectStoreFullError: Failed to put object ffffffffffffffffffffffffffffffffffffffff0100000003000000 in object store because it is full. Object size is 8388608252 bytes.
The local object store is full of objects that are still in scope and cannot be evicted. Tip: Use the `ray memory` command to list active objects in the cluster.
&lt;/denchmark-code&gt;


[x ] I have verified my script runs in a clean environment and reproduces the issue.
[ x] I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='blueplastic' date='2020-12-30T04:35:09Z'>
		Could the object be larger than total available memory on the node? Ray doesn't support single objects that are too large, since the object is materialized in memory entirely on initial creation.
Btw just curious, do you have a use case for a single large object that cannot fit in memory?
		</comment>
		<comment id='2' author='blueplastic' date='2020-12-30T17:28:50Z'>
		Eric, you're correct that the object was larger than available memory.
I was able to place smaller np arrays in memory, the error only occurs if my array size is larger than the object store's memory.
I don't have a use case for caching very large objects, but I incorrectly assumed that the object would partially spill to disk like on Spark.
Also, I noticed that I can create and work with absurdly large ndarray objects on my local laptop (array_object.nbytes &gt; 80 TB). Maybe that object is represented in a very compact way in numpy and has to be fully serialized if exported out of numpy into Ray's memory.
By the way, that stuck "Loading" page still happens when I first open the Dashboard and go to the Memory tab, but it's not a big deal b/c it fixes itself after I put the first item in memory.
We can probably close this issue.
		</comment>
	</comments>
</bug>