<bug id='684' author='robertnishihara' open_date='2017-06-19T21:53:21Z' closed_time='2020-01-16T00:02:28Z'>
	<summary>Ray crashes under stressful workload.</summary>
	<description>
The following workload failed on a t2.medium instance.
@ray.remote
def f(x):
  return 1

def g(n):
  x = 1
  for i in range(n):
    x = f.remote(x)
  return x

ray.get([g(1000) for _ in range(1000)])
Note that this is the same workload as in &lt;denchmark-link:https://github.com/ray-project/ray/issues/561&gt;#561&lt;/denchmark-link&gt;
 except bigger. The workload produced the following output.
&lt;denchmark-code&gt;[WARN] (/home/ubuntu/ray/src/local_scheduler/local_scheduler_algorithm.cc:570) fetch_object_timeout_handler took 4445 milliseconds.
Disconnecting client on fd 5
/home/ubuntu/ray/src/plasma/plasma_extension.cc/home/ubuntu/ray/src/plasma/plasma_extension.cc213 Check failed: _s.ok() Bad status: IOError: Connection reset by peer
213 Check failed: _s.ok() Bad status: IOError: Connection reset by peer
Disconnecting client on fd 11
[INFO] (/home/ubuntu/ray/src/local_scheduler/local_scheduler.cc:967) Disconnecting client on fd 15
[INFO] (/home/ubuntu/ray/src/local_scheduler/local_scheduler.cc:117) Killed worker with pid 9977
[INFO] (/home/ubuntu/ray/src/local_scheduler/local_scheduler.cc:967) Disconnecting client on fd 14
[INFO] (/home/ubuntu/ray/src/local_scheduler/local_scheduler.cc:117) Killed worker with pid 9978
/home/ubuntu/ray/src/plasma/plasma_extension.cc213 Check failed: _s.ok() Bad status: IOError: Broken pipe
/home/ubuntu/ray/src/plasma/plasma_extension.cc213 Check failed: _s.ok() Bad status: IOError: Broken pipe
Disconnecting client on fd 10
[INFO] (/home/ubuntu/ray/src/local_scheduler/local_scheduler.cc:230) Started worker with pid 9992
[INFO] (/home/ubuntu/ray/src/local_scheduler/local_scheduler.cc:967) Disconnecting client on fd 17
[INFO] (/home/ubuntu/ray/src/local_scheduler/local_scheduler.cc:117) Killed worker with pid 9987
[INFO] (/home/ubuntu/ray/src/local_scheduler/local_scheduler.cc:967) Disconnecting client on fd 16
[INFO] (/home/ubuntu/ray/src/local_scheduler/local_scheduler.cc:117) Killed worker with pid 9985
Disconnecting client on fd 16
Disconnecting client on fd 14
/home/ubuntu/ray/src/local_scheduler/local_scheduler_algorithm.cc558 Check failed: _s.ok() Bad status: IOError: Broken pipe
Connection to socket failed for pathname /tmp/plasma_manager51139193
Could not connect to socket /tmp/plasma_manager51139193
Disconnecting client on fd 5
A worker died or was killed while executing a task.

You can inspect errors by running

    ray.error_info()

If this driver is hanging, start a new one with

    ray.init(redis_address="127.0.0.1:35323")

Disconnecting client on fd 7
A worker died or was killed while executing a task.

You can inspect errors by running

    ray.error_info()

If this driver is hanging, start a new one with

    ray.init(redis_address="127.0.0.1:35323")

A worker died or was killed while executing a task.

You can inspect errors by running

    ray.error_info()

If this driver is hanging, start a new one with

    ray.init(redis_address="127.0.0.1:35323")

A worker died or was killed while executing a task.

You can inspect errors by running

    ray.error_info()

If this driver is hanging, start a new one with

    ray.init(redis_address="127.0.0.1:35323")

WARNING:root:Timed out b'plasma_manager'
WARNING:root:Removed b'plasma_manager', client ID c9aeb1ec7000871cccbc44fbfbc0f6113f2f56bb
Traceback (most recent call last):
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/redis/connection.py", line 439, in connect
    sock = self._connect()
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/redis/connection.py", line 494, in _connect
    raise err
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/redis/connection.py", line 482, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/redis/client.py", line 572, in execute_command
    connection.send_command(*args)
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 111 connecting to b'127.0.0.1':58532. Connection refused.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/redis/connection.py", line 439, in connect
    sock = self._connect()
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/redis/connection.py", line 494, in _connect
    raise err
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/redis/connection.py", line 482, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/ray/python/ray/monitor.py", line 401, in &lt;module&gt;
    monitor.run()
  File "/home/ubuntu/ray/python/ray/monitor.py", line 361, in run
    self.cleanup_object_table()
  File "/home/ubuntu/ray/python/ray/monitor.py", line 131, in cleanup_object_table
    objects = self.state.object_table()
  File "/home/ubuntu/ray/python/ray/experimental/state.py", line 176, in object_table
    object_info_keys = self._keys(OBJECT_INFO_PREFIX + "*")
  File "/home/ubuntu/ray/python/ray/experimental/state.py", line 122, in _keys
    result.extend(client.keys(pattern))
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/redis/client.py", line 936, in keys
    return self.execute_command('KEYS', pattern)
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/redis/client.py", line 578, in execute_command
    connection.send_command(*args)
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/redis/connection.py", line 563, in send_command
    self.send_packed_command(self.pack_command(*args))
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/redis/connection.py", line 538, in send_packed_command
    self.connect()
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/redis/connection.py", line 442, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 111 connecting to b'127.0.0.1':58532. Connection refused.
[WARN] (/home/ubuntu/ray/src/global_scheduler/global_scheduler.cc:414) Missed too many heartbeats from local scheduler, marking as dead.
/home/ubuntu/ray/src/plasma/plasma_extension.cc213 Check failed: _s.ok() Bad status: IOError: Broken pipe
Disconnecting client on fd 9
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='robertnishihara' date='2017-06-25T20:55:56Z'>
		Closing this because the workload runs fine an a larger node (m4.16xlarge). The problem is most likely that there was not enough memory on the t2.medium instance for Redis.
		</comment>
		<comment id='2' author='robertnishihara' date='2017-06-25T20:56:22Z'>
		That said, we need a much better error message when that happens.
		</comment>
		<comment id='3' author='robertnishihara' date='2018-09-09T04:09:21Z'>
		I got this error after a few hundred rounds of communications between 10 workers on a relatively small machine (10 cores, 24gb ram). Is there a way to trace the memory footprint of Redis?
Edit: indeed increasing the memory size helps but I think would be good to have a better error message and to reduce the memory footprint.
		</comment>
	</comments>
</bug>