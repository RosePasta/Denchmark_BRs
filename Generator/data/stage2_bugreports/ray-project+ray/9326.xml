<bug id='9326' author='Nintorac' open_date='2020-07-07T05:13:17Z' closed_time='2020-09-01T20:14:36Z'>
	<summary>[autoscaler/docker] file_mounts fail when permissions required to create the host directory</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

Ray nightly.
When trying to create a file mount for the home directory of a container i.e. /root/, cluster creation fails because the host tries to write to /root/ which it does not have permission to do.
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Use the following cluster.yml
&lt;denchmark-code&gt;# An unique identifier for the head node and workers of this cluster.
cluster_name: default

# The minimum number of workers nodes to launch in addition to the head
# node. This number should be &gt;= 0.
min_workers: 0

# The maximum number of workers nodes to launch in addition to the head
# node. This takes precedence over min_workers.
max_workers: 1

# The initial number of worker nodes to launch in addition to the head
# node. When the cluster is first brought up (or when it is refreshed with a
# subsequent `ray up`) this number of nodes will be started.
initial_workers: 0

# Whether or not to autoscale aggressively. If this is enabled, if at any point
#   we would start more workers, we start at least enough to bring us to
#   initial_workers.
autoscaling_mode: default

# This executes all commands on all nodes in the docker container,
# and opens all the necessary ports to support the Ray cluster.
# Empty string means disabled.
docker:
    image: tensorflow/tensorflow:1.5.0-py3
    container_name: "ray_docker" # e.g. ray_docker
    # If true, pulls latest version of image. Otherwise, `docker run` will only pull the image
    # if no cached version is present.
    # pull_before_run: False
    pull_before_run: True

    run_options: []

    # # Example of running a GPU head with CPU workers
    # head_image: "tensorflow/tensorflow:1.13.1-gpu-py3"
    head_run_options:  []
    worker_run_options:
        - --runtime=nvidia

    # worker_image: "ubuntu:18.04"


    # run_options:
    #   - --runtime=nvidia

# The autoscaler will scale up the cluster to this target fraction of resource
# usage. For example, if a cluster of 10 nodes is 100% busy and
# target_utilization is 0.8, it would resize the cluster to 13. This fraction
# can be decreased to increase the aggressiveness of upscaling.
# This value must be less than 1.0 for scaling to happen.
target_utilization_fraction: 0.8

# If a node is idle for this many minutes, it will be removed.
idle_timeout_minutes: 5

# Cloud-provider specific configuration.
provider:
    type: gcp
    region: us-central1
    availability_zone: us-central1-a
    project_id: ray # Globally unique project id

# How Ray will authenticate with newly launched nodes.
auth:
    ssh_user: ubuntu
# By default Ray creates a new private keypair, but you can also use your own.
# If you do so, make sure to also set "KeyName" in the head and worker node
# configurations below. This requires that you have added the key into the
# project wide meta-data.
#    ssh_private_key: /path/to/your/key.pem

# Provider-specific config for the head node, e.g. instance type. By default
# Ray will auto-configure unspecified fields such as subnets and ssh-keys.
# For more documentation on available fields, see:
# https://cloud.google.com/compute/docs/reference/rest/v1/instances/insert
head_node:
    machineType: n1-standard-2
    disks:
      - boot: true
        autoDelete: true
        type: PERSISTENT
        initializeParams:
          diskSizeGb: 50
          # See https://cloud.google.com/compute/docs/images for more images
          diskType: https://www.googleapis.com/compute/v1/projects/ray/zones/us-central1-a/diskTypes/pd-ssd
          sourceImage: projects/deeplearning-platform-release/global/images/common-cu101-v20200630
    # Additional options can be found in in the compute docs at
    # https://cloud.google.com/compute/docs/reference/rest/v1/instances/insert

    # If the network interface is specified as below in both head and worker
    # nodes, the manual network config is used.  Otherwise an existing subnet is
    # used.  To use a shared subnet, ask the subnet owner to grant permission
    # for 'compute.subnetworks.use' to the ray autoscaler account...
    # networkInterfaces:
    #   - kind: compute#networkInterface
    #     subnetwork: path/to/subnet
    #     aliasIpRanges: []

worker_nodes:
    machineType: n1-standard-8
    disks:
      - boot: true
        autoDelete: true
        type: PERSISTENT
        initializeParams:
          diskSizeGb: 50
          # See https://cloud.google.com/compute/docs/images for more images
          sourceImage: projects/deeplearning-platform-release/global/images/common-cu101-v20200630
          diskType: https://www.googleapis.com/compute/v1/projects/ray/zones/us-west1-a/diskTypes/pd-ssd
    guestAccelerators:
      - acceleratorType: projects/ray/zones/us-central1-a/acceleratorTypes/nvidia-tesla-k80
        acceleratorCount: 1
    metadata:
      items:
        - key: install-nvidia-driver
          value: "True"
    # Run workers on preemtible instance by default.
    # Comment this out to use on-demand.
    scheduling:
      # - preemptible: true
      - onHostMaintenance: TERMINATE
    serviceAccounts:
      - email: ray-worker@ray.iam.gserviceaccount.com
        scopes: ["https://www.googleapis.com/auth/cloud-platform"]
      
    # Additional options can be found in in the compute docs at
    # https://cloud.google.com/compute/docs/reference/rest/v1/instances/insert

# Files or directories to copy to the head and worker nodes. The format is a
# dictionary from REMOTE_PATH: LOCAL_PATH, e.g.
file_mounts: {
    "/root/data/": "data/",
}

# List of commands that will be run before `setup_commands`. If docker is
# enabled, these commands will run outside the container and before docker
# is setup.
initialization_commands: []

# List of shell commands to run to set up nodes.
setup_commands:
  - pip install ray[tune]
    # Note: if you're developing Ray, you probably want to create an AMI that
    # has your Ray repo pre-cloned. Then, you can replace the pip installs
    # below with a git checkout &lt;your_sha&gt; (and possibly a recompile).
    # - echo 'export PATH="$HOME/anaconda3/envs/tensorflow_p36/bin:$PATH"' &gt;&gt; ~/.bashrc

    # Install Anaconda.
    # - &gt;-
    #   wget https://repo.continuum.io/archive/Anaconda3-5.0.1-Linux-x86_64.sh -O ~/anaconda3.sh
    #   || true
    #   &amp;&amp; bash ~/anaconda3.sh -b -p ~/anaconda3 || true
    #   &amp;&amp; rm ~/anaconda3.sh
    #   &amp;&amp; echo 'export PATH="$HOME/anaconda3/bin:$PATH"' &gt;&gt; ~/.profile

    # # Install ray
    # # - pip install -U https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.9.0.dev0-cp27-cp27mu-manylinux1_x86_64.whl
    # # - pip install -U https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.9.0.dev0-cp35-cp35m-manylinux1_x86_64.whl
    # - pip install -U https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.9.0.dev0-cp36-cp36m-manylinux1_x86_64.whl


# Custom commands that will be run on the head node after common setup.
head_setup_commands:
  - pip install google-api-python-client==1.7.8

# Custom commands that will be run on worker nodes after common setup.
worker_setup_commands: []


# Command to start ray on the head node. You don't need to change this.
head_start_ray_commands:
    - ray stop
    - &gt;-
      ulimit -n 65536;
      ray start
      --head
      --port=6379
      --object-manager-port=8076
      --autoscaling-config=~/ray_bootstrap_config.yaml

# Command to start ray on worker nodes. You don't need to change this.
worker_start_ray_commands:
    - ray stop
    - &gt;-
      ulimit -n 65536;
      ray start
      --address=$RAY_HEAD_IP:6379
      --object-manager-port=8076
&lt;/denchmark-code&gt;

results in the following error mkdir: cannot create directory ‘/root’: Permission denied
full error output here
&lt;denchmark-code&gt;2020-07-07 04:44:43,207 INFO updater.py:264 -- NodeUpdater: ray-default-head-7347dd6f: Running ssh -tt -i /home/nintorac/.ssh/ray-autoscaler_gcp_us-central1_ray-runner_ubuntu_0.pem -o ConnectTimeout=120s -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_9151b9c88a/c21f969b5f/%C -o ControlPersist=10s -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 ubuntu@34.66.223.160 bash --login -c -i 'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; mkdir -p /root/bistableGRU'
mkdir: cannot create directory ‘/root’: Permission denied
Shared connection to 34.66.223.160 closed.
2020-07-07 04:44:43,891 INFO log_timer.py:22 -- NodeUpdater: ray-default-head-7347dd6f: Synced bistableGRU/ to /root/bistableGRU/  [LogTimer=683ms]
2020-07-07 04:44:43,891 INFO log_timer.py:22 -- NodeUpdater: ray-default-head-7347dd6f: Applied config 9766e837de18583f489ebd562c12bb3d6ead4b5e  [LogTimer=13577ms]
2020-07-07 04:44:44,497 INFO node_provider.py:21 -- wait_for_compute_zone_operation: Waiting for operation operation-1594097084024-5a9d2a66e4854-99e9b008-3f76ae9f to finish...
2020-07-07 04:44:49,705 INFO node_provider.py:32 -- wait_for_compute_zone_operation: Operation operation-1594097084024-5a9d2a66e4854-99e9b008-3f76ae9f finished.
2020-07-07 04:44:49,706 ERROR updater.py:441 -- NodeUpdater: ray-default-head-7347dd6f: Error executing: SSH command Failed. See above for the output from the failure.2020-07-07 04:44:49,817 ERROR commands.py:367 -- get_or_create_head_node: Updating 34.66.223.160 failed
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Nintorac' date='2020-07-10T23:25:01Z'>
		&lt;denchmark-link:https://github.com/ijrsvt&gt;@ijrsvt&lt;/denchmark-link&gt;
 Is it supposed to be closed?
		</comment>
		<comment id='2' author='Nintorac' date='2020-07-11T01:08:50Z'>
		&lt;denchmark-link:https://github.com/rkooo567&gt;@rkooo567&lt;/denchmark-link&gt;
  The PR yes, this issue, no.
The best workaround for trying to do
&lt;denchmark-code&gt;file_mounts:
   "/root/item" : "/whatever/item"
&lt;/denchmark-code&gt;

is to do
&lt;denchmark-code&gt;file_mounts:
   "/home/item" : "/local_whatever/item"

docker:
   run_options:
       "-v /home/item:/root/item"
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='Nintorac' date='2020-08-31T12:53:38Z'>
		One way to allow this to happen would be to have initialization_commands run before file mounts. If that is not feasible for some reason, maybe you could introduce preinitialization_commands.
Inside these commands, the ray user could run whatever they want, such as sudo mkdir /whatever/directory, since presumably they would know whether or not the user running the ray commands has sudo access.
		</comment>
	</comments>
</bug>