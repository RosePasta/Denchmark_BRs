<bug id='11294' author='anqixu' open_date='2020-10-09T03:39:24Z' closed_time='2020-10-12T20:48:45Z'>
	<summary>[rllib] SAC (pytorch) log_alpha not optimizing with GPU enabled</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

The entropy regularization coefficient alpha (implemented as sac_model.log_alpha tensor) is not being updated by sac_policy.alpha_optim.
Verify by running the attached script, load tensorboard, and confirm that the alpha value does not change over training iterations.
&lt;denchmark-h:h3&gt;Suspected cause&lt;/denchmark-h&gt;

I believe the culprit is due to an incorrect order of execution:

sac_model.log_alpha is initially created as a tensor on CPU device
sac_policy.alpha_optim is initialized with the sac_model.log_alpha param
but then sac_model.log_alpha is CLONED to the CUDA device, and this copy is used during backprop

To verify, put a debugger breakpoint &lt;denchmark-link:https://github.com/ray-project/ray/blob/b450cb030a5d316f04d4f6322e152335853d1a88/rllib/policy/torch_policy.py#L402&gt;after opt.step()&lt;/denchmark-link&gt;
, then verify that  does not match :
self._optimizers[3].param_groups[0]["params"][0]
# DEBUGGER OUTPUT: tensor([-0.0003], requires_grad=True)

self.model.log_alpha
# DEBUGGER OUTPUT: tensor([0.], device='cuda:0', grad_fn=&lt;CopyBackwards&gt;)

self._optimizers[3].param_groups[0]["params"][0] is self.model.log_alpha
# DEBUGGER OUTPUT: False
Unfortunately, I don't know RLLib internals enough to know how to properly solve this issue.
&lt;denchmark-h:h3&gt;Related issue: unable to disable GPU usage in RLLib configs&lt;/denchmark-h&gt;

According to &lt;denchmark-link:https://docs.ray.io/en/master/rllib-algorithms.html#sac&gt;SAC configs&lt;/denchmark-link&gt;
, we should be able to disable GPU usage by the learner/driver by setting . I also set  just in case. But Ray still puts tensors onto CUDA. I don't know why this is the case.
To sidestep this issue, we can force Ray to start without GPUs: ray.init(num_gpus=0, ...).
Ray version and other system information (Python version, TensorFlow version, OS):

Tested with ray 1.0.0 (via pip) and nightly (ray-1.1.0.dev0)
Python 3.7.5 (default, Nov  7 2019, 10:50:52) [GCC 8.3.0] on linux
Ubuntu 18.04: Linux MIMIC-5530-UB1804 5.4.0-48-generic #52~18.04.1-Ubuntu SMP Thu Sep 10 12:50:22 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux

&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
&lt;denchmark-link:https://github.com/ray-project/ray/files/5352388/torch_sac_alpha_unchanged.py.txt&gt;torch_sac_alpha_unchanged.py.txt&lt;/denchmark-link&gt;

If we cannot run your script, we cannot fix your issue.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='anqixu' date='2020-10-09T06:33:22Z'>
		For a quick fix (PR to follow today):
Could try creating the sale.log_alpha property inside sac_torch_model.py like so?:
&lt;denchmark-code&gt;        log_alpha = nn.Parameter(
            torch.from_numpy(np.array([np.log(initial_alpha)])).float())
        self.register_parameter("log_alpha", log_alpha)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='anqixu' date='2020-10-09T06:41:24Z'>
		PR: &lt;denchmark-link:https://github.com/ray-project/ray/pull/11298&gt;#11298&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='anqixu' date='2020-10-09T07:57:04Z'>
		&lt;denchmark-link:https://github.com/anqixu&gt;@anqixu&lt;/denchmark-link&gt;

Thanks for filing this issue! :)
		</comment>
	</comments>
</bug>