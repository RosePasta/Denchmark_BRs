<bug id='3494' author='ericl' open_date='2018-12-07T21:04:44Z' closed_time='2019-01-05T06:30:35Z'>
	<summary>[rllib] Model self loss isn't included in all algorithms</summary>
	<description>
&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

&lt;denchmark-link:https://groups.google.com/forum/#!topic/ray-dev/dk0erEEnkFY&gt;https://groups.google.com/forum/#!topic/ray-dev/dk0erEEnkFY&lt;/denchmark-link&gt;

In DQN, DDPG, IMPALA, and A3C, the gradients() function for the tf policy graph is overriden, but does not include model.loss().
The fix is to reference self._loss in gradients(), since that is always the total loss passed to TFPolicyGraph.
We should also add a simple test that the model self loss is improving for each algorithm (for example by just adding custom model with a free TF variable that gets sent to zero).
	</description>
	<comments>
		<comment id='1' author='ericl' date='2018-12-07T21:33:33Z'>
		&lt;denchmark-link:https://github.com/megankawakami&gt;@megankawakami&lt;/denchmark-link&gt;
 it would be great if you could look at this.
		</comment>
		<comment id='2' author='ericl' date='2018-12-08T22:18:12Z'>
		Does it mean that for example A3C work wrongly?
		</comment>
		<comment id='3' author='ericl' date='2018-12-08T22:32:51Z'>
		Only if you are using self supervised losses.
		</comment>
		<comment id='4' author='ericl' date='2018-12-09T03:58:01Z'>
		I got it , thanks.
		</comment>
	</comments>
</bug>