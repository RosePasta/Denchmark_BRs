<bug id='10404' author='wuisawesome' open_date='2020-08-28T19:30:14Z' closed_time='2020-09-13T18:58:48Z'>
	<summary>[Autoscaler] Only recognizes workers it launches</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

Ray version and other system information (Python version, TensorFlow version, OS):
0.8.6+
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):

Start a ray cluster with gpu workers (non-resource demand autoscaler), head should not have gpu's.
Manually add a node (not using autoscaler), with a custom resource {"custom": 1}

&lt;denchmark-code&gt;@ray.remote(num_gpus=1)
def gpu_task():
    pass

@ray.remote(resources={"custom":1})
def launch_gpu():
    # This blocks and never starts a gpu worker.
    ray.get(gpu_task.remote())

ray.get(launch_gpu.remote())
&lt;/denchmark-code&gt;

If we cannot run your script, we cannot fix your issue.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='wuisawesome' date='2020-08-28T19:36:44Z'>
		The root cause of this is that StandardAutoscaler::workers() explicitely filters
&lt;denchmark-code&gt;    def workers(self):
        return self.provider.non_terminated_nodes(
            tag_filters={TAG_RAY_NODE_KIND: NODE_KIND_WORKER})

&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='wuisawesome' date='2020-08-28T21:07:15Z'>
		&lt;denchmark-link:https://github.com/wuisawesome&gt;@wuisawesome&lt;/denchmark-link&gt;
 Can you set the priority of this task?
		</comment>
		<comment id='3' author='wuisawesome' date='2020-08-29T12:21:50Z'>
		
The root cause of this is that StandardAutoscaler::workers() explicitely filters
    def workers(self):
        return self.provider.non_terminated_nodes(
            tag_filters={TAG_RAY_NODE_KIND: NODE_KIND_WORKER})


This is indeed the root cause. When this would be fixed, the autoscaler will however still try to update the manual node because the ray-launch-config and ray-runtime-config labels are not set on the manual node either.
Additionally, the script you posted looks very similar to the thing I'm trying to do, but is still a little different.
In my case the gpu_task task is not started from within an actor but simply from within python code that is running on the manually started worker. I don't know if that makes a difference or not...
		</comment>
	</comments>
</bug>