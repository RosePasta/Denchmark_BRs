<bug id='5940' author='sytelus' open_date='2019-10-17T01:27:03Z' closed_time='2019-10-18T20:53:34Z'>
	<summary>Ray is not finding GPU but TF, PyTorch and nvcc does</summary>
	<description>
I have two NVIDIA TitanX but Ray isn't seeing any:
&lt;denchmark-code&gt;ray.init(num_gpus=2)
print(ray.get_gpu_ids())
# prints []
&lt;/denchmark-code&gt;

Ray also prints below inicating no GPUs:
2019-10-16 18:20:17,954 INFO multi_gpu_optimizer.py:93 -- LocalMultiGPUOptimizer devices ['/cpu:0'] 
But TensorFlow sees all devices:
&lt;denchmark-code&gt;import tensorflow
from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())
&lt;/denchmark-code&gt;

That prints:
&lt;denchmark-code&gt;[name: "/device:CPU:0"
device_type: "CPU"
...
, name: "/device:XLA_CPU:0"
device_type: "XLA_CPU"
...
, name: "/device:GPU:0"
device_type: "GPU"
...
, name: "/device:GPU:1"
device_type: "GPU"
...
, name: "/device:XLA_GPU:0"
device_type: "XLA_GPU"
...
, name: "/device:XLA_GPU:1"
device_type: "XLA_GPU"
...
]
&lt;/denchmark-code&gt;

Similarly,
&lt;denchmark-code&gt;/usr/local/cuda/bin/nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Sun_Jul_28_19:07:16_PDT_2019
Cuda compilation tools, release 10.1, V10.1.243
&lt;/denchmark-code&gt;

Why Ray doesn't see my GPUs?
	</description>
	<comments>
		<comment id='1' author='sytelus' date='2019-10-17T05:01:14Z'>
		Just wondering what does Ray uses to find GPU? May be I can also look into this...
		</comment>
		<comment id='2' author='sytelus' date='2019-10-17T05:12:51Z'>
		Ray does not allocate GPUs unless you specify them as resources, please see: &lt;denchmark-link:https://ray.readthedocs.io/en/latest/rllib-training.html#specifying-resources&gt;https://ray.readthedocs.io/en/latest/rllib-training.html#specifying-resources&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='sytelus' date='2019-10-17T05:44:02Z'>
		Thanks, that was helpful although its confusing. This is what happens:
Even if I explicitly init ray with num_gpus=1, ray.get_gpu_ids() is [].
However, if I start PPOTrainer with explicit num_gpus=1 then ray gets GPU. If I don't set this in config then it doesn't.
I believe the confusing part is ray.get_gpu_ids() which I thought is the detected GPUs in the system. Instead, it's actually allocated gpus in the system. I think there should be a method, may be, detected_gpus() so one can test that ray indeed sees GPUs and things are good to go. It would also be great if Ray just allocated GPUs automatically to itself (which should be good perhaps 99% of the times) so we don't have to worry about this additional config.
		</comment>
		<comment id='4' author='sytelus' date='2019-10-17T08:26:07Z'>
		Ray does the following to check:
&lt;denchmark-code&gt;def _autodetect_num_gpus():
    """Attempt to detect the number of GPUs on this machine.

    TODO(rkn): This currently assumes Nvidia GPUs and Linux.

    Returns:
        The number of GPUs if any were detected, otherwise 0.
    """
    proc_gpus_path = "/proc/driver/nvidia/gpus"
    if os.path.isdir(proc_gpus_path):
        return len(os.listdir(proc_gpus_path))
    return 0
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='sytelus' date='2019-10-18T00:42:32Z'>
		I am having a similar issue. The number returned by the code in &lt;denchmark-link:https://github.com/ray-project/ray/issues/5940#issuecomment-543065188&gt;#5940 (comment)&lt;/denchmark-link&gt;
 is correct, but when starting training it's telling me the following:
&lt;denchmark-code&gt;Initializing Ray automatically.For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run`.
2019-10-17 20:39:19,979 WARNING worker.py:1426 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.
2019-10-17 20:39:19,981 WARNING resource_spec.py:163 -- Warning: Capping object memory store to 20.0GB. To increase this further, specify `object_store_memory` when calling ray.init() or ray start.
2019-10-17 20:39:19,981 INFO resource_spec.py:205 -- Starting Ray with 52.49 GiB memory available for workers and up to 18.63 GiB for objects. You can adjust these settings with ray.init(memory=&lt;bytes&gt;, object_store_memory=&lt;bytes&gt;).
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/20 CPUs, 0/2 GPUs, 0.0/52.49 GiB heap, 0.0/12.84 GiB objects
Memory usage on this node: 12.3/125.4 GiB

2019-10-17 20:39:22,108 WARNING logger.py:325 -- Could not instantiate tf2_compat_logger: cannot import name 'resnet'.
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 1/20 CPUs, 0/2 GPUs, 0.0/52.49 GiB heap, 0.0/12.84 GiB objects
Memory usage on this node: 13.7/125.4 GiB
Result logdir: /home/herzc/ray_results/main
Number of trials: 1 ({'RUNNING': 1})
RUNNING trials:
 - main_0_lr=0.01:      RUNNING

(pid=27890) Warning: There's no GPU available on this machine, training will be performed on CPU.
&lt;/denchmark-code&gt;

At first, it detects GPUs but then somehow the process doesn't.
What could be a quick fix for this?
		</comment>
		<comment id='6' author='sytelus' date='2019-10-18T00:55:23Z'>
		Ray is detecting the GPUs but not allocating them since your code didn't
request any for the trials (that's why it says 0/2 GPUs requested).

The Tune API provides a way to request GPUs per trial.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Thu, Oct 17, 2019, 5:42 PM Christian Herz ***@***.***&gt; wrote:
 I am having a similar issue. The number returned by the code in #5940
 (comment)
 &lt;#5940 (comment)&gt;
 is correct, but when starting training it's telling me the following:

 Initializing Ray automatically.For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run`.
 2019-10-17 20:39:19,979 WARNING worker.py:1426 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.
 2019-10-17 20:39:19,981 WARNING resource_spec.py:163 -- Warning: Capping object memory store to 20.0GB. To increase this further, specify `object_store_memory` when calling ray.init() or ray start.
 2019-10-17 20:39:19,981 INFO resource_spec.py:205 -- Starting Ray with 52.49 GiB memory available for workers and up to 18.63 GiB for objects. You can adjust these settings with ray.init(memory=&lt;bytes&gt;, object_store_memory=&lt;bytes&gt;).
 == Status ==
 Using FIFO scheduling algorithm.
 Resources requested: 0/20 CPUs, 0/2 GPUs, 0.0/52.49 GiB heap, 0.0/12.84 GiB objects
 Memory usage on this node: 12.3/125.4 GiB

 2019-10-17 20:39:22,108 WARNING logger.py:325 -- Could not instantiate tf2_compat_logger: cannot import name 'resnet'.
 == Status ==
 Using FIFO scheduling algorithm.
 Resources requested: 1/20 CPUs, 0/2 GPUs, 0.0/52.49 GiB heap, 0.0/12.84 GiB objects
 Memory usage on this node: 13.7/125.4 GiB
 Result logdir: /home/herzc/ray_results/main
 Number of trials: 1 ({'RUNNING': 1})
 RUNNING trials:
  - main_0_lr=0.01:      RUNNING

 (pid=27890) Warning: There's no GPU available on this machine, training will be performed on CPU.

 At first, it detects GPUs but then somehow the process doesn't.

 What could be a quick fix for this?

 —
 You are receiving this because you commented.
 Reply to this email directly, view it on GitHub
 &lt;#5940?email_source=notifications&amp;email_token=AAADUSQI7TSO65E7LDXFQLLQPEBATA5CNFSM4JBS5PL2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEBSA6ZY#issuecomment-543428455&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/AAADUSXXIC5G3YHKGHNWJRLQPEBATANCNFSM4JBS5PLQ&gt;
 .



		</comment>
		<comment id='7' author='sytelus' date='2019-10-18T00:58:02Z'>
		&lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
 Why wouldn't it automatically allocate all found GPU unless otherwise defined?
Is this the resource? &lt;denchmark-link:https://github.com/ray-project/ray/issues/5940#issuecomment-543005640&gt;#5940 (comment)&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='sytelus' date='2019-10-18T01:05:41Z'>
		How would it know how many GPUs to give to each trial?

Please see
&lt;denchmark-link:https://ray.readthedocs.io/en/latest/tune-usage.html#trial-parallelism&gt;https://ray.readthedocs.io/en/latest/tune-usage.html#trial-parallelism&lt;/denchmark-link&gt;


&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Thu, Oct 17, 2019, 5:58 PM Christian Herz ***@***.***&gt; wrote:
 @ericl &lt;https://github.com/ericl&gt; Why wouldn't it automatically allocate
 all found GPU unless otherwise defined?

 Is this the resource? #5940 (comment)
 &lt;#5940 (comment)&gt;

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#5940?email_source=notifications&amp;email_token=AAADUSXSJOOLAF4DQ5RLDK3QPEC2ZA5CNFSM4JBS5PL2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEBSCD4I#issuecomment-543433201&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/AAADUSUIPLFDNNQAEPEL6BTQPEC2ZANCNFSM4JBS5PLQ&gt;
 .



		</comment>
		<comment id='9' author='sytelus' date='2019-10-18T20:53:33Z'>
		Closing this now - thanks!
		</comment>
	</comments>
</bug>