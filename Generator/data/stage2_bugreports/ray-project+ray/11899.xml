<bug id='11899' author='simon-mo' open_date='2020-11-10T02:48:20Z' closed_time='2020-11-11T10:52:49Z'>
	<summary>[Autoscaler] Log error when using k8s provider</summary>
	<description>
Ray 1.0.1. I tried refactor the k8s yaml using resource demand scheduler but encountered this error when running ray up with it. It just log an error and carries on.
&lt;denchmark-code&gt;`2020-11-10 02:46:30,795	ERROR util.py:110 -- Failed to autodetect node resources
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.8/site-packages/ray/autoscaler/_private/util.py", line 106, in fillout_defaults
    defaults = _fillout_available_node_types_resources(defaults)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/ray/autoscaler/_private/util.py", line 122, in _fillout_available_node_types_resources
    return provider_cls.fillout_available_node_types_resources(
TypeError: fillout_available_node_types_resources() missing 1 required positional argument: 'cluster_config'
...AND the log continue normally.
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;cluster_name: default

min_workers: 0
max_workers: 5
initial_workers: 0
autoscaling_mode: default
target_utilization_fraction: 0.8
idle_timeout_minutes: 99999

provider:
  type: kubernetes
  use_internal_ips: true
  namespace: ray
  autoscaler_service_account:
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: autoscaler
  autoscaler_role:
    kind: Role
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
      name: autoscaler
    rules:
      - apiGroups:
          - ''
        resources:
          - pods
          - pods/status
          - pods/exec
        verbs:
          - get
          - watch
          - list
          - create
          - delete
          - patch
  autoscaler_role_binding:
    apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
      name: autoscaler
    subjects:
      - kind: ServiceAccount
        name: autoscaler
    roleRef:
      kind: Role
      name: autoscaler
      apiGroup: rbac.authorization.k8s.io
  services:
    - apiVersion: v1
      kind: Service
      metadata:
        name: ray-head
      spec:
        selector:
          component: ray-head
        ports:
          - protocol: TCP
            port: 8000
            targetPort: 8000
    - apiVersion: v1
      kind: Service
      metadata:
        name: ray-workers
      spec:
        selector:
          component: ray-worker
        ports:
          - protocol: TCP
            port: 8000
            targetPort: 8000

available_node_types:
  k8s_head_node:
    resources: {"CPU": 1}
    node_config:
      apiVersion: v1
      kind: Pod
      metadata:
        generateName: ray-head-
        labels:
          component: ray-head
      spec:
        serviceAccountName: autoscaler
        restartPolicy: Never
        volumes:
          - name: dshm
            emptyDir:
              medium: Memory
        containers:
          - name: ray-node
            image: 'rayproject/ray-ml:1.0.1'
            command:
              - /bin/bash
              - '-c'
              - '--'
            args:
              - 'trap : TERM INT; sleep infinity &amp; wait;'
            ports:
              - containerPort: 6379
              - containerPort: 6380
              - containerPort: 6381
              - containerPort: 12345
              - containerPort: 12346
            volumeMounts:
              - mountPath: /dev/shm
                name: dshm
            resources:
              requests:
                cpu: 1000m
                memory: 512Mi
              limits:
                memory: 2Gi
            env:
              - name: MY_CPU_REQUEST
                valueFrom:
                  resourceFieldRef:
                    resource: requests.cpu
  k8s_worker_nodes:
    node_config:
      apiVersion: v1
      kind: Pod
      metadata:
        generateName: ray-worker-
        labels:
          component: ray-worker
      spec:
        serviceAccountName: default
        restartPolicy: Never
        volumes:
          - name: dshm
            emptyDir:
              medium: Memory
        containers:
          - name: ray-node
            image: 'rayproject/ray-ml:1.0.1'
            command:
              - /bin/bash
              - '-c'
              - '--'
            args:
              - 'trap : TERM INT; sleep infinity &amp; wait;'
            ports:
              - containerPort: 12345
              - containerPort: 12346
            volumeMounts:
              - mountPath: /dev/shm
                name: dshm
            resources:
              requests:
                cpu: 1000m
                memory: 512Mi
              limits:
                memory: 2Gi
            env:
              - name: MY_CPU_REQUEST
                valueFrom:
                  resourceFieldRef:
                    resource: requests.cpu

head_node_type: k8s_head_node
worker_default_node_type: k8s_worker_nodes

file_mounts: {}
cluster_synced_files: []
file_mounts_sync_continuously: false
initialization_commands: []
setup_commands: []

head_setup_commands: []
worker_setup_commands:
  - echo "done sleeping"
head_start_ray_commands:
  - ray stop
  - &gt;-
    ulimit -n 65536; ray start --head --num-cpus=$MY_CPU_REQUEST --port=6379
    --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml
    --dashboard-host 0.0.0.0
worker_start_ray_commands:
  - ray stop
  - &gt;-
    ulimit -n 65536; ray start --num-cpus=$MY_CPU_REQUEST
    --address=$RAY_HEAD_IP:6379 --object-manager-port=8076
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='simon-mo' date='2020-11-10T02:53:42Z'>
		cc &lt;denchmark-link:https://github.com/AmeerHajAli&gt;@AmeerHajAli&lt;/denchmark-link&gt;
, this seems like it should be a simple fix. Btw little mistakes like this that are annoying to test are a good argument in favor of keeping this code wrapped in a try/catch + log instead of letting the error propagate.
		</comment>
		<comment id='2' author='simon-mo' date='2020-11-10T11:32:20Z'>
		&lt;denchmark-link:https://github.com/simon-mo&gt;@simon-mo&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/wuisawesome&gt;@wuisawesome&lt;/denchmark-link&gt;
 , this should have been fixed a few days ago right? Can you try the most recent nightly?
It is replaced with 


		</comment>
		<comment id='3' author='simon-mo' date='2020-11-10T13:26:19Z'>
		I am not sure what is the problem here "It just log an error and carries on"? &lt;denchmark-link:https://github.com/simon-mo&gt;@simon-mo&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='simon-mo' date='2020-11-10T16:32:01Z'>
		Sorry I was try to say that it's bad UX. Users don't know what's going on and will think that they did something wrong.
		</comment>
		<comment id='5' author='simon-mo' date='2020-11-10T19:22:20Z'>
		There also shouldn't be an exception to begin with. fillout_available_node_types_resources() missing 1 required positional argument: 'cluster_config' This is clearly a minor bug (we should still pass a cluster config and have it do nothing, not throw)
		</comment>
		<comment id='6' author='simon-mo' date='2020-11-11T10:43:00Z'>
		When I run this on the provided yaml, I get the following error:
jsonschema.exceptions.ValidationError: 'resources' is a required property
		</comment>
		<comment id='7' author='simon-mo' date='2020-11-11T10:52:49Z'>
		Closing this as it is resolved here:
&lt;denchmark-link:https://github.com/ray-project/ray/pull/11628&gt;#11628&lt;/denchmark-link&gt;

Note that we keep the logger.exception to figure out bugs like this and to warn the users if things are wrong.
For example, this should log an exception if someone is using staroid but it is not installed on his laptop.
		</comment>
	</comments>
</bug>