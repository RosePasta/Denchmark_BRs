<bug id='7387' author='janblumenkamp' open_date='2020-03-01T10:55:08Z' closed_time='2020-05-06T06:17:22Z'>
	<summary>multi-agent policy is using the trainer's config as the base, not the policy's config</summary>
	<description>
&lt;denchmark-h:h1&gt;What is the problem?&lt;/denchmark-h&gt;

I wanted to change the DQN Policy in the &lt;denchmark-link:https://github.com/ray-project/ray/blob/releases/0.8.1/rllib/examples/multiagent_two_trainers.py&gt;multiagent_two_trainers.py&lt;/denchmark-link&gt;
 example to a A3C policy, but an exception is thrown that I don't really understand.
Ray version and other system information (Python version, TensorFlow version, OS):

Ray version 0.8.1
TensorFlow version 2.1.0 (GPU)
Python version 3.6.9
Linux 5.3.0-28-generic #30~18.04.1-Ubuntu SMP x86_64 x86_64 x86_64 GNU/Linux

&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import argparse
import gym

import ray
from ray.rllib.agents.a3c.a3c import A3CTrainer
from ray.rllib.agents.a3c.a3c_tf_policy import A3CTFPolicy
from ray.rllib.agents.ppo.ppo import PPOTrainer
from ray.rllib.agents.ppo.ppo_policy import PPOTFPolicy
from ray.rllib.tests.test_multi_agent_env import MultiCartpole
from ray.tune.logger import pretty_print
from ray.tune.registry import register_env

parser = argparse.ArgumentParser()
parser.add_argument("--num-iters", type=int, default=20)

if __name__ == "__main__":
    args = parser.parse_args()
    ray.init()

    # Simple environment with 4 independent cartpole entities
    register_env("multi_cartpole", lambda _: MultiCartpole(4))
    single_env = gym.make("CartPole-v0")
    obs_space = single_env.observation_space
    act_space = single_env.action_space

    policies = {
        "ppo_policy": (PPOTFPolicy, obs_space, act_space, {}),
        "a3c_policy": (A3CTFPolicy, obs_space, act_space, {}),
    }

    def policy_mapping_fn(agent_id):
        if agent_id % 2 == 0:
            return "ppo_policy"
        else:
            return "a3c_policy"

    ppo_trainer = PPOTrainer(
        env="multi_cartpole",
        config={
            "multiagent": {
                "policies": policies,
                "policy_mapping_fn": policy_mapping_fn,
                "policies_to_train": ["ppo_policy"],
            },
            "observation_filter": "NoFilter",
    })

    a3c_trainer = A3CTrainer(
        env="multi_cartpole",
        config={
            "multiagent": {
                "policies": policies,
                "policy_mapping_fn": policy_mapping_fn,
                "policies_to_train": ["a3c_policy"],
            },
            "gamma": 0.95,
            "n_step": 3,
    })

    for i in range(args.num_iters):
        print("== Iteration", i, "==")

        print("-- A3C --")
        print(pretty_print(a3c_trainer.train()))

        print("-- PPO --")
        print(pretty_print(ppo_trainer.train()))

        # swap weights to synchronize
        a3c_trainer.set_weights(ppo_trainer.get_weights(["ppo_policy"]))
        ppo_trainer.set_weights(a3c_trainer.get_weights(["a3c_policy"]))
&lt;/denchmark-code&gt;

Error:
&lt;denchmark-code&gt;2020-03-01 10:49:37,911	WARNING services.py:1080 -- Failed to start the dashboard. The dashboard requires Python 3 as well as 'pip install aiohttp psutil setproctitle grpcio'.
/auto/homes/jan/master-project/venv_project/local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: �[33mWARN: Box bound precision lowered by casting to float32�[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
2020-03-01 10:49:38,488	INFO trainer.py:370 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
2020-03-01 10:49:38,532	INFO trainer.py:517 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
Traceback (most recent call last):
  File "multi_trainers_example.py", line 55, in &lt;module&gt;
    "observation_filter": "NoFilter",
  File "/auto/homes/jan/master-project/venv_project/local/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 83, in __init__
    Trainer.__init__(self, config, env, logger_creator)
  File "/auto/homes/jan/master-project/venv_project/local/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 397, in __init__
    Trainable.__init__(self, config, logger_creator)
  File "/auto/homes/jan/master-project/venv_project/local/lib/python3.6/site-packages/ray/tune/trainable.py", line 172, in __init__
    self._setup(copy.deepcopy(self.config))
  File "/auto/homes/jan/master-project/venv_project/local/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 528, in _setup
    self._init(self.config, self.env_creator)
  File "/auto/homes/jan/master-project/venv_project/local/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py", line 102, in _init
    self.config["num_workers"])
  File "/auto/homes/jan/master-project/venv_project/local/lib/python3.6/site-packages/ray/rllib/agents/trainer.py", line 573, in _make_workers
    logdir=self.logdir)
  File "/auto/homes/jan/master-project/venv_project/local/lib/python3.6/site-packages/ray/rllib/evaluation/worker_set.py", line 60, in __init__
    RolloutWorker, env_creator, policy, 0, self._local_config)
  File "/auto/homes/jan/master-project/venv_project/local/lib/python3.6/site-packages/ray/rllib/evaluation/worker_set.py", line 216, in _make_worker
    _fake_sampler=config.get("_fake_sampler", False))
  File "/auto/homes/jan/master-project/venv_project/local/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py", line 342, in __init__
    self._build_policy_map(policy_dict, policy_config)
  File "/auto/homes/jan/master-project/venv_project/local/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py", line 778, in _build_policy_map
    policy_map[name] = cls(obs_space, act_space, merged_conf)
  File "/auto/homes/jan/master-project/venv_project/local/lib/python3.6/site-packages/ray/rllib/policy/tf_policy_template.py", line 139, in __init__
    obs_include_prev_action_reward=obs_include_prev_action_reward)
  File "/auto/homes/jan/master-project/venv_project/local/lib/python3.6/site-packages/ray/rllib/policy/dynamic_tf_policy.py", line 197, in __init__
    self._initialize_loss()
  File "/auto/homes/jan/master-project/venv_project/local/lib/python3.6/site-packages/ray/rllib/policy/dynamic_tf_policy.py", line 343, in _initialize_loss
    TFPolicy._initialize_loss(self, loss, loss_inputs)
  File "/auto/homes/jan/master-project/venv_project/local/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py", line 193, in _initialize_loss
    (g, v) for (g, v) in self.gradients(self._optimizer, self._loss)
  File "/auto/homes/jan/master-project/venv_project/local/lib/python3.6/site-packages/ray/rllib/policy/tf_policy_template.py", line 164, in gradients
    return gradients_fn(self, optimizer, loss)
  File "/auto/homes/jan/master-project/venv_project/local/lib/python3.6/site-packages/ray/rllib/agents/a3c/a3c_tf_policy.py", line 111, in clip_gradients
    grads, _ = tf.clip_by_global_norm(grads, policy.config["grad_clip"])
  File "/auto/homes/jan/master-project/venv_project/local/lib/python3.6/site-packages/tensorflow_core/python/ops/clip_ops.py", line 296, in clip_by_global_norm
    constant_op.constant(1.0, dtype=use_norm.dtype) / clip_norm)
  File "/auto/homes/jan/master-project/venv_project/local/lib/python3.6/site-packages/tensorflow_core/python/ops/math_ops.py", line 906, in binary_op_wrapper
    y, dtype_hint=x.dtype.base_dtype, name="y")
  File "/auto/homes/jan/master-project/venv_project/local/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 1256, in convert_to_tensor_v2
    as_ref=False)
  File "/auto/homes/jan/master-project/venv_project/local/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 1314, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File "/auto/homes/jan/master-project/venv_project/local/lib/python3.6/site-packages/tensorflow_core/python/framework/constant_op.py", line 317, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File "/auto/homes/jan/master-project/venv_project/local/lib/python3.6/site-packages/tensorflow_core/python/framework/constant_op.py", line 258, in constant
    allow_broadcast=True)
  File "/auto/homes/jan/master-project/venv_project/local/lib/python3.6/site-packages/tensorflow_core/python/framework/constant_op.py", line 296, in _constant_impl
    allow_broadcast=allow_broadcast))
  File "/auto/homes/jan/master-project/venv_project/local/lib/python3.6/site-packages/tensorflow_core/python/framework/tensor_util.py", line 439, in make_tensor_proto
    raise ValueError("None values not supported.")
ValueError: None values not supported.
&lt;/denchmark-code&gt;


 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the [latest wheels] (https://ray.readthedocs.io/en/latest/installation.html). (This problem also occurs in the latest nightly build)

	</description>
	<comments>
		<comment id='1' author='janblumenkamp' date='2020-03-02T01:09:18Z'>
		This is since the PPO trainer is using the default config for PPO for the A3C policy, which has grad_clip: None. You can fix this by
&lt;denchmark-code&gt;        "a3c_policy": (A3CTFPolicy, obs_space, act_space, {"grad_clip": 40.0}),
&lt;/denchmark-code&gt;

You also have to remove the n_step etc. DQN configs from A3CTrainer to get the script to run.
Actually it should be a bug that it's using the wrong base config, I will look into that.
		</comment>
	</comments>
</bug>