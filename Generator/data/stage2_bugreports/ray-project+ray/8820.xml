<bug id='8820' author='NumberChiffre' open_date='2020-06-07T16:32:35Z' closed_time='2020-06-26T20:36:59Z'>
	<summary>Hyperparameters removed using PBT + HyperOpt + PPO for Rllib, Tune</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

I used PBT + HyperOpt for tuning PPO agents on a custom environment and I noticed issues with certain hyperparameters (num_sgd_iter, train_batch_size, sgd_minibatch_size) as they get removed from the hparam tab on tensorboard. This would not occur if HyperOpt is removed from the search algorithm, so PBT + PPO would work fine. I confirmed this issue using a simple gym environment (CartPole-v0) and the same error occured. Error shown here:

2020-06-07 16:27:07,588	ERROR trial_runner.py:519 -- Trial PPO_CartPole-v0_9f955854: Error processing event.
Traceback (most recent call last):
File "/usr/local/lib/python3.7/site-packages/ray/python/ray/tune/trial_runner.py", line 467, in _process_trial
result = self.trial_executor.fetch_result(trial)
File "/usr/local/lib/python3.7/site-packages/ray/python/ray/tune/ray_trial_executor.py", line 430, in fetch_result
result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)
File "/usr/local/lib/python3.7/site-packages/ray/python/ray/worker.py", line 1522, in get
raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(TypeError): ray::PPO.train() (pid=2636, ip=172.31.17.191)
File "python/ray/_raylet.pyx", line 460, in ray._raylet.execute_task
File "python/ray/_raylet.pyx", line 414, in ray._raylet.execute_task.function_executor
File "/usr/local/lib/python3.7/site-packages/ray/python/ray/rllib/agents/trainer.py", line 494, in train
raise e
File "/usr/local/lib/python3.7/site-packages/ray/python/ray/rllib/agents/trainer.py", line 480, in train
result = Trainable.train(self)
File "/usr/local/lib/python3.7/site-packages/ray/python/ray/tune/trainable.py", line 260, in train
result = self._train()
File "/usr/local/lib/python3.7/site-packages/ray/python/ray/rllib/agents/trainer_template.py", line 132, in _train
return self._train_exec_impl()
File "/usr/local/lib/python3.7/site-packages/ray/python/ray/rllib/agents/trainer_template.py", line 170, in _train_exec_impl
res = next(self.train_exec_impl)
File "/usr/local/lib/python3.7/site-packages/ray/python/ray/util/iter.py", line 731, in next
return next(self.built_iterator)
File "/usr/local/lib/python3.7/site-packages/ray/python/ray/util/iter.py", line 744, in apply_foreach
for item in it:
File "/usr/local/lib/python3.7/site-packages/ray/python/ray/util/iter.py", line 744, in apply_foreach
for item in it:
File "/usr/local/lib/python3.7/site-packages/ray/python/ray/util/iter.py", line 814, in apply_filter
for item in it:
File "/usr/local/lib/python3.7/site-packages/ray/python/ray/util/iter.py", line 814, in apply_filter
for item in it:
File "/usr/local/lib/python3.7/site-packages/ray/python/ray/util/iter.py", line 744, in apply_foreach
for item in it:
File "/usr/local/lib/python3.7/site-packages/ray/python/ray/util/iter.py", line 744, in apply_foreach
for item in it:
File "/usr/local/lib/python3.7/site-packages/ray/python/ray/util/iter.py", line 752, in apply_foreach
result = fn(item)
File "/usr/local/lib/python3.7/site-packages/ray/python/ray/rllib/execution/train_ops.py", line 65, in call
self.sgd_minibatch_size, [])
File "/usr/local/lib/python3.7/site-packages/ray/python/ray/rllib/utils/sgd.py", line 111, in do_minibatch_sgd
for i in range(num_sgd_iter):
TypeError: 'numpy.float64' object cannot be interpreted as an integer
2020-06-07 16:27:07,595	INFO logger.py:271 -- Removed the following hyperparameter values when logging to tensorboard: {'num_sgd_iter': 29, 'sgd_minibatch_size': 1642, 'train_batch_size': 9333}
s2020-06-07 16:34:34,854	INFO logger.py:271 -- Removed the following hyperparameter values when logging to tensorboard: {'num_sgd_iter': 59, 'sgd_minibatch_size': 1456, 'train_batch_size': 9918}
Traceback (most recent call last):
File "/home/ubuntu/tsizzle_clone/rl-trading/test_ppo_pbt.py", line 107, in 
File "/usr/local/lib/python3.7/site-packages/ray/python/ray/tune/tune.py", line 348, in run
raise TuneError("Trials did not complete", incomplete_trials)
ray.tune.error.TuneError: ('Trials did not complete', [PPO_CartPole-v0_9f955844, PPO_CartPole-v0_9f955845, PPO_CartPole-v0_9f955846, PPO_CartPole-v0_9f955847, PPO_CartPole-v0_9f955848, PPO_CartPole-v0_9f955849, PPO_CartPole-v0_9f95584a, PPO_CartPole-v0_9f95584b, PPO_CartPole-v0_9f95584c, PPO_CartPole-v0_9f95584e, PPO_CartPole-v0_9f95584f, PPO_CartPole-v0_9f955850, PPO_CartPole-v0_9f955851, PPO_CartPole-v0_9f955852, PPO_CartPole-v0_9f955853, PPO_CartPole-v0_9f955854, PPO_CartPole-v0_9f955855, PPO_CartPole-v0_9f955856, PPO_CartPole-v0_9f955857, PPO_CartPole-v0_9f955858, PPO_CartPole-v0_9f955859, PPO_CartPole-v0_9f95585a, PPO_CartPole-v0_9f95585b, PPO_CartPole-v0_9f95585c, PPO_CartPole-v0_9f95585d, PPO_CartPole-v0_9f95585e, PPO_CartPole-v0_9f95585f, PPO_CartPole-v0_9f955860, PPO_CartPole-v0_9f955861, PPO_CartPole-v0_9f955862, PPO_CartPole-v0_9f955863, PPO_CartPole-v0_9f955864, PPO_CartPole-v0_9f955865, PPO_CartPole-v0_9f955866, PPO_CartPole-v0_9f955867, PPO_CartPole-v0_9f955868, PPO_CartPole-v0_9f955869, PPO_CartPole-v0_9f95586a, PPO_CartPole-v0_9f95586b])


python 3.7
-e git+&lt;denchmark-link:https://github.com/ray-project/ray.git@38399c98855206c77410c3da81b64f426e8ffbde#egg=ray&amp;subdirectory=python&gt;https://github.com/ray-project/ray.git@38399c98855206c77410c3da81b64f426e8ffbde#egg=ray&amp;subdirectory=python&lt;/denchmark-link&gt;

torch==1.5.0
Ubuntu 16.04.6 LTS
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
import ray
from ray import tune
from ray.tune.logger import DEFAULT_LOGGERS
from hyperopt import hp
from ray.tune.schedulers import PopulationBasedTraining
from ray.tune.suggest.hyperopt import HyperOptSearch

import datetime
from pytz import timezone
import random

if __name__ == '__main__':
    ray.init(
        # local_mode=True,
        ignore_reinit_error=True,
    )

    # Postprocess the perturbed config to ensure it's still valid
    def explore(config):
        # ensure we collect enough timesteps to do sgd
        if config["train_batch_size"] &lt; config["sgd_minibatch_size"] * 2:
            config["train_batch_size"] = config["sgd_minibatch_size"] * 2
        # ensure we run at least one sgd iter
        if config["num_sgd_iter"] &lt; 1:
            config["num_sgd_iter"] = 1
        return config


    pbt = PopulationBasedTraining(
        time_attr="time_total_s",
        metric="episode_reward_mean",
        mode="max",
        perturbation_interval=2,
        resample_probability=0.25,
        # Specifies the mutations of these hyperparams
        hyperparam_mutations={
            "train_batch_size": lambda: random.randint(2000, 10000),
            "sgd_minibatch_size": lambda: random.randint(200, 1000),
            "num_sgd_iter": lambda: random.randint(20, 60),
            "lr": lambda: random.uniform(1e-5, 1e-2),
            "lambda": lambda: random.uniform(0.9, 1.0),
            "clip_param": lambda: random.uniform(0.1, 0.5),
        },
        custom_explore_fn=explore)

    algo = HyperOptSearch(
        space={
            "train_batch_size": hp.randint("train_batch_size", 2000, 10000),
            "sgd_minibatch_size": hp.randint("sgd_minibatch_size", 200, 2000),
            "num_sgd_iter": hp.randint("num_sgd_iter", 10, 60),
            "lr": hp.uniform("lr", 1e-5, 1e-2),
            "lambda": hp.uniform("lambda", 0.9, 1.0),
            "clip_param": hp.uniform("clip_param", 0.1, 0.5),
        },
        metric="episode_reward_mean",
        mode="max")

    cfg = {
        "lambda": 0.95,
        "clip_param": 0.2,
        "lr": 1e-4,
        "num_sgd_iter": tune.sample_from(
            lambda spec: random.choice([10, 20, 30])),
        "sgd_minibatch_size": tune.sample_from(
            lambda spec: random.choice([100, 200, 1000])),
        "train_batch_size": tune.sample_from(
            lambda spec: random.choice([2000, 5000, 10000]))
    }
    cfg['log_level'] = 'INFO'
    cfg['framework'] = 'torch'
    cfg['model'] = {
        'fcnet_hiddens': [32],
        'fcnet_activation': 'linear',
    }
    # cfg['env'] = "PongNoFrameskip-v4"
    cfg['env'] = "CartPole-v0"
    start_time = datetime.datetime.now(timezone('EST')).isoformat()
    current_date = start_time.split('T')[0]

    # tune experiments and return best params
    analysis = tune.run(
        run_or_experiment="PPO",
        scheduler=pbt,
        search_alg=algo,
        loggers=DEFAULT_LOGGERS,
        checkpoint_freq=2,
        verbose=False,
        stop={
            "training_iteration": 100,
        },
        num_samples=40,
        config=cfg,
    )
    best_trial = analysis.get_best_trial("episode_reward_mean")
    best_checkpoint = max(
        analysis.get_trial_checkpoints_paths(best_trial, 'episode_reward_mean'))
    print(
        f"Best config: {analysis.get_best_config(metric='episode_reward_mean')}")
    print(f"Best model: {best_checkpoint}")
    ray.shutdown()
If we cannot run your script, we cannot fix your issue.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='NumberChiffre' date='2020-06-26T13:36:04Z'>
		Works by changing hp.randint with hp.choice on ray 0.8.6
import ray
from ray import tune
from ray.tune.logger import DEFAULT_LOGGERS
from hyperopt import hp
from ray.tune.schedulers import PopulationBasedTraining
from ray.tune.suggest.hyperopt import HyperOptSearch

import datetime
from pytz import timezone
import random

if __name__ == '__main__':
    ray.init(
        # local_mode=True,
        ignore_reinit_error=True,
    )

    # Postprocess the perturbed config to ensure it's still valid
    def explore(config):
        # ensure we collect enough timesteps to do sgd
        if config["train_batch_size"] &lt; config["sgd_minibatch_size"] * 2:
            config["train_batch_size"] = config["sgd_minibatch_size"] * 2
        # ensure we run at least one sgd iter
        if config["num_sgd_iter"] &lt; 1:
            config["num_sgd_iter"] = 1
        return config


    pbt = PopulationBasedTraining(
        time_attr="time_total_s",
        metric="episode_reward_mean",
        mode="max",
        perturbation_interval=2,
        resample_probability=0.25,
        # Specifies the mutations of these hyperparams
        hyperparam_mutations={
            "train_batch_size": lambda: random.randint(2000, 10000),
            "sgd_minibatch_size": lambda: random.randint(200, 1000),
            "num_sgd_iter": lambda: random.randint(1, 60),
            "lr": lambda: random.uniform(1e-5, 1e-2),
            "lambda": lambda: random.uniform(0.9, 1.0),
            "clip_param": lambda: random.uniform(0.1, 0.5),
        },)
        # custom_explore_fn=explore)

    algo = HyperOptSearch(
        space={
            "train_batch_size": hp.choice("train_batch_size", range(2000, 10000)),
            "sgd_minibatch_size": hp.choice("sgd_minibatch_size", range(200, 1000)),
            "num_sgd_iter": hp.choice("num_sgd_iter", range(1, 60)),
            "lr": hp.uniform("lr", 1e-5, 1e-2),
            "lambda": hp.uniform("lambda", 0.9, 1.0),
            "clip_param": hp.uniform("clip_param", 0.1, 0.5),
        },
        metric="episode_reward_mean",
        mode="max")

    cfg = {
        "lambda": 0.95,
        "clip_param": 0.2,
        "lr": 1e-4,
        "num_sgd_iter": tune.sample_from(
            lambda spec: random.choice([10, 20, 30])),
        "sgd_minibatch_size": tune.sample_from(
            lambda spec: random.choice([100, 200, 1000])),
        "train_batch_size": tune.sample_from(
            lambda spec: random.choice([2000, 5000, 10000]))
    }
    cfg['log_level'] = 'INFO'
    cfg['framework'] = 'torch'
    cfg['model'] = {
        'fcnet_hiddens': [32],
        'fcnet_activation': 'linear',
    }
    # cfg['env'] = "PongNoFrameskip-v4"
    cfg['env'] = "CartPole-v0"
    start_time = datetime.datetime.now(timezone('EST')).isoformat()
    current_date = start_time.split('T')[0]

    # tune experiments and return best params
    analysis = tune.run(
        run_or_experiment="PPO",
        scheduler=pbt,
        search_alg=algo,
        loggers=DEFAULT_LOGGERS,
        checkpoint_freq=2,
        # verbose=False,
        stop={
            "training_iteration": 100,
        },
        num_samples=10,
        config=cfg,
    )
    best_trial = analysis.get_best_trial("episode_reward_mean")
    best_checkpoint = max(
        analysis.get_trial_checkpoints_paths(best_trial, 'episode_reward_mean'))
    print(
        f"Best config: {analysis.get_best_config(metric='episode_reward_mean')}")
    print(f"Best model: {best_checkpoint}")
    ray.shutdown()
		</comment>
	</comments>
</bug>