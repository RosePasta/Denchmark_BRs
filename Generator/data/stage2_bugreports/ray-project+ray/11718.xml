<bug id='11718' author='LianShuaiLong' open_date='2020-10-30T09:01:48Z' closed_time='2020-11-02T17:24:15Z'>
	<summary>[tune] final model saved is different from the best model?</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

when i test the pbt algorithm in ray, i replace the ConvNet in with my own model
and my test data is same with train data
but i find the final model saved is not the best model
&lt;denchmark-link:https://user-images.githubusercontent.com/23699530/97680009-fde38c00-1ad0-11eb-8ec7-06f891bf4ee3.png&gt;&lt;/denchmark-link&gt;

Ray version and other system information (Python version, TensorFlow version, OS):
ray version: 0.8.7
python:3.6.7
os:ubuntu 16.04
pytorch:1.6.0
codes:
#!/usr/bin/env python
&lt;denchmark-h:h1&gt;tutorial_imports_begin&lt;/denchmark-h&gt;

import argparse
import os
import numpy as np
import torch
import torch.optim as optim
from torchvision import datasets
from inceptionv4 import train, test, ConvNet,get_data_loaders
import ray
from ray import tune
from ray.tune.schedulers import PopulationBasedTraining
from ray.tune.utils import validate_save_restore
from ray.tune.trial import ExportFormat
&lt;denchmark-h:h1&gt;tutorial_imports_end&lt;/denchmark-h&gt;

os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'
os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'
&lt;denchmark-h:h1&gt;trainable_begin&lt;/denchmark-h&gt;

class PytorchTrainble(tune.Trainable):
"""Train a Pytorch ConvNet with Trainable and PopulationBasedTraining
scheduler. The example reuse some of the functions in mnist_pytorch,
and is a good demo for how to add the tuning function without
changing the original training code.
"""
&lt;denchmark-code&gt;def setup(self, config):
    self.train_loader, self.test_loader = get_data_loaders()
    self.model = ConvNet(3)
    self.optimizer = optim.SGD(
        self.model.parameters(),
        lr=config.get("lr", 0.01),
        momentum=config.get("momentum", 0.9))

def step(self):
    train(self.model, self.optimizer, self.train_loader)
    acc = test(self.model, self.test_loader)
    return {"mean_accuracy": acc}

def save_checkpoint(self, checkpoint_dir):
    checkpoint_path = os.path.join(checkpoint_dir, "model.pth")
    torch.save(self.model.state_dict(), checkpoint_path)
    return checkpoint_path

def load_checkpoint(self, checkpoint_path):
    self.model.load_state_dict(torch.load(checkpoint_path))

def _export_model(self, export_formats, export_dir):
    if export_formats == [ExportFormat.MODEL]:
        path = os.path.join(export_dir, "exported_convnet.pt")
        torch.save(self.model.state_dict(), path)
        return {export_formats[0]: path}
    else:
        raise ValueError("unexpected formats: " + str(export_formats))

def reset_config(self, new_config):
    for param_group in self.optimizer.param_groups:
        if "lr" in new_config:
            param_group["lr"] = new_config["lr"]
        if "momentum" in new_config:
            param_group["momentum"] = new_config["momentum"]

    self.config = new_config
    return True
&lt;/denchmark-code&gt;

&lt;denchmark-h:h1&gt;trainable_end&lt;/denchmark-h&gt;

if name == "main":
parser = argparse.ArgumentParser()
parser.add_argument(
"--smoke-test", action="store_true", help="Finish quickly for testing")
args, _ = parser.parse_known_args()
&lt;denchmark-code&gt;ray.init(num_cpus=6,num_gpus=2)#temp_dir='/workspace/lianshuailong_pbt/ray_dir/')#necessary!!!!!!!!!!!!!!

# check if PytorchTrainble will save/restore correctly before execution
validate_save_restore(PytorchTrainble)
validate_save_restore(PytorchTrainble, use_object_store=True)

# __pbt_begin__
scheduler = PopulationBasedTraining(
    time_attr="training_iteration",
    metric="mean_accuracy",
    mode="max",
    perturbation_interval=5,
    hyperparam_mutations={
        # distribution for resampling
        "lr": lambda: np.random.uniform(0.0001, 1),
        # allow perturbations within this set of categorical values
        "momentum": [0.8, 0.9, 0.99],
    })

# __pbt_end__

# __tune_begin__
class CustomStopper(tune.Stopper):
    def __init__(self):
        self.should_stop = False

    def __call__(self, trial_id, result):
        max_iter = 5 if args.smoke_test else 50
        if not self.should_stop and result["mean_accuracy"] &gt; 0.96:
            self.should_stop = True
        return self.should_stop or result["training_iteration"] &gt;= max_iter

    def stop_all(self):
        return self.should_stop

stopper = CustomStopper()

analysis = tune.run(
    PytorchTrainble,
    name="pbt_test",
    scheduler=scheduler,
    reuse_actors=True,
    verbose=1,
    stop=stopper,
    export_formats=[ExportFormat.MODEL],
    checkpoint_score_attr="mean_accuracy",
    checkpoint_freq=5,
    keep_checkpoints_num=4,
    num_samples=4,#num_samples = num_trials
    checkpoint_at_end = True,
    resources_per_trial={'cpu':2,'gpu':1},
    config={
        "lr": tune.choice([0.001,0.0001,0.00001]),#tune.uniform(0.001, 1),
        "momentum": tune.choice([0.999,0.99])#tune.uniform(0.001, 1),
    })
# __tune_end__

#print(type(analysis))
best_trial = analysis.get_best_trial("mean_accuracy")
#print(best_trial)
#print(type(best_trial))
#print(analysis.get_trial_checkpoints_paths(best_trial,'mean_accuracy'))
best_checkpoint = max(analysis.get_trial_checkpoints_paths(best_trial, "mean_accuracy"))
restored_trainable = PytorchTrainble()
restored_trainable.restore(best_checkpoint[0])
best_model = restored_trainable.model
# Note that test only runs on a small random set of the test data, thus the
# accuracy may be different from metrics shown in tuning process.
test_acc = test(best_model, get_data_loaders()[1])
print("best model accuracy: ", test_acc)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
If we cannot run your script, we cannot fix your issue.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='LianShuaiLong' date='2020-10-30T17:46:42Z'>
		&lt;denchmark-link:https://github.com/LianShuaiLong&gt;@LianShuaiLong&lt;/denchmark-link&gt;
 can you try upgrading to Ray 1.0?
		</comment>
		<comment id='2' author='LianShuaiLong' date='2020-11-02T06:58:08Z'>
		
@LianShuaiLong can you try upgrading to Ray 1.0?

thanks for your suggestion,
		</comment>
	</comments>
</bug>