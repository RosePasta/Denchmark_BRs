<bug id='11201' author='fshriver' open_date='2020-10-04T17:50:55Z' closed_time='2020-10-22T16:27:26Z'>
	<summary>Ray cannot connect to socket</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

I'm trying to run Ray Tune on my university's computing cluster, however apparently there's an issue with how Ray distributes jobs. Specifically, it seems that when I'm trying to run ray.init() inside of my script, since I just want to run on a single GPU-enabled node, the following error message is returned ([compute-directory] is a file system that our compute nodes have access to):
&lt;denchmark-code&gt;F1004 12:37:49.333159 129114 129114 raylet_client.cc:54] Could not connect to socket [compute-directory]/raytemp/session_2020-10-04_12-37-38_811569_129114/sockets/raylet
*** Check failure stack trace: ***
    @     0x2b73f9c608ad  google::LogMessage::Fail()
    @     0x2b73f9c61a0c  google::LogMessage::SendToLog()
    @     0x2b73f9c60589  google::LogMessage::Flush()
    @     0x2b73f9c607a1  google::LogMessage::~LogMessage()
    @     0x2b73f9c17949  ray::RayLog::~RayLog()
    @     0x2b73f993f255  ray::raylet::RayletConnection::RayletConnection()
    @     0x2b73f99405be  ray::raylet::RayletClient::RayletClient()
    @     0x2b73f98dec27  ray::CoreWorker::CoreWorker()
    @     0x2b73f98e2dd4  ray::CoreWorkerProcess::CreateWorker()
    @     0x2b73f98e4042  ray::CoreWorkerProcess::CoreWorkerProcess()
    @     0x2b73f98e4a0b  ray::CoreWorkerProcess::Initialize()
    @     0x2b73f981ebfe  __pyx_pw_3ray_7_raylet_10CoreWorker_1__cinit__()
    @     0x2b73f98203e5  __pyx_tp_new_3ray_7_raylet_CoreWorker()
    @     0x562bdc2f89f5  type_call
    @     0x562bdc32d79c  _PyObject_FastCallKeywords
    @     0x562bdc32e481  call_function
    @     0x562bdc35ccaa  _PyEval_EvalFrameDefault
    @     0x562bdc2a4ea8  _PyEval_EvalCodeWithName
    @     0x562bdc31f933  fast_function
    @     0x562bdc32e3af  call_function
    @     0x562bdc35dace  _PyEval_EvalFrameDefault
    @     0x562bdc2a4ea8  _PyEval_EvalCodeWithName
    @     0x562bdc31f933  fast_function
    @     0x562bdc32e3af  call_function
    @     0x562bdc35dace  _PyEval_EvalFrameDefault
    @     0x562bdc2a474b  _PyEval_EvalCodeWithName
    @     0x562bdc32f223  PyEval_EvalCode
    @     0x562bdc3b25b2  run_mod
    @     0x562bdc3b299e  PyRun_FileExFlags
    @     0x562bdc3b2b8b  PyRun_SimpleFileExFlags
    @     0x562bdc3b6887  Py_Main
    @     0x562bdc2826a7  main
Aborted (core dumped)
&lt;/denchmark-code&gt;

It appears that this issue may be due to the network settings, however from looking in past issues I also see some people reporting it's due to memory constraints or needing to pass in some other command. As an effort to fix that, in the example script snippet below I've set the temp directory to a location that I KNOW the compute nodes have access to, however the above error message still appears. In this scenario, since I'm trying to run on a single node in an interactive session, I'm not sure what could be causing the issue. I do know about the SLURM peculiarities that must be accounted for when using SLURM, documented at (&lt;denchmark-link:https://docs.ray.io/en/master/cluster/slurm.html&gt;https://docs.ray.io/en/master/cluster/slurm.html&lt;/denchmark-link&gt;
) however I don't think that should be necessary if I'm not actually trying to distribute jobs across nodes, as I'm only trying to run ray on the node I am currently logged in to. Any help on this would be appreciated.
I've attached some of the logs generated by Ray under [compute-directory]/raytemp/:
gcs_server.err:
&lt;denchmark-code&gt;I1004 12:37:39.176844 129128 129128 io_service_pool.cc:36] IOServicePool is running with 1 io_service.
I1004 12:37:39.209870 129128 129128 gcs_redis_failure_detector.cc:30] Starting redis failure detector.
I1004 12:37:39.209997 129128 129128 gcs_object_manager.cc:271] Loading initial data.
I1004 12:37:39.210038 129128 129128 gcs_node_manager.cc:424] Loading initial data.
I1004 12:37:39.210712 129128 129128 gcs_object_manager.cc:286] Finished loading initial data.
I1004 12:37:39.210819 129128 129128 gcs_node_manager.cc:445] Finished loading initial data.
I1004 12:37:39.210836 129128 129128 gcs_actor_manager.cc:913] Loading initial data.
I1004 12:37:39.210898 129128 129128 gcs_actor_manager.cc:976] Finished loading initial data.
I1004 12:37:39.211059 129128 129128 grpc_server.cc:74] GcsServer server started, listening on port 40224.
I1004 12:37:39.246960 129128 129128 gcs_server.cc:260] Gcs server address = 10.13.164.76:40224
I1004 12:37:39.246995 129128 129128 gcs_server.cc:264] Finished setting gcs server address: 10.13.164.76:40224
W1004 12:37:50.184828 129128 129130 metric_exporter.cc:211] Export metrics to agent failed: IOError: 14: Connection reset by peer
W1004 12:37:50.184828 129128 129130 metric_exporter.cc:211] Export metrics to agent failed: IOError: 14: Connection reset by peer
W1004 12:38:00.994897 129128 129130 metric_exporter.cc:211] Export metrics to agent failed: IOError: 14: failed to connect to all addresses
W1004 12:38:00.994897 129128 129130 metric_exporter.cc:211] Export metrics to agent failed: IOError: 14: failed to connect to all addresses
&lt;/denchmark-code&gt;

python-core-driver-01000000ffffffffffffffffffffffffffffffff.20201004-123739.129114.log:
&lt;denchmark-code&gt;I1004 12:37:39.263516 129114 129114 core_worker.cc:117] Constructing CoreWorkerProcess. pid: 129114
I1004 12:37:40.306079 129114 129114 client_connection.cc:53] Retrying to connect to socket for endpoint [compute-directory]/raytemp/session_2020-10-04_12-37-38_811569_129114/sockets/raylet (num_attempts = 1, num_retries = 10)
I1004 12:37:41.311228 129114 129114 client_connection.cc:53] Retrying to connect to socket for endpoint [compute-directory]/raytemp/session_2020-10-04_12-37-38_811569_129114/sockets/raylet (num_attempts = 2, num_retries = 10)
I1004 12:37:42.318224 129114 129114 client_connection.cc:53] Retrying to connect to socket for endpoint [compute-directory]/raytemp/session_2020-10-04_12-37-38_811569_129114/sockets/raylet (num_attempts = 3, num_retries = 10)
I1004 12:37:43.318521 129114 129114 client_connection.cc:53] Retrying to connect to socket for endpoint [compute-directory]/raytemp/session_2020-10-04_12-37-38_811569_129114/sockets/raylet (num_attempts = 4, num_retries = 10)
I1004 12:37:44.324218 129114 129114 client_connection.cc:53] Retrying to connect to socket for endpoint [compute-directory]/raytemp/session_2020-10-04_12-37-38_811569_129114/sockets/raylet (num_attempts = 5, num_retries = 10)
I1004 12:37:45.324478 129114 129114 client_connection.cc:53] Retrying to connect to socket for endpoint [compute-directory]/raytemp/session_2020-10-04_12-37-38_811569_129114/sockets/raylet (num_attempts = 6, num_retries = 10)
I1004 12:37:46.324734 129114 129114 client_connection.cc:53] Retrying to connect to socket for endpoint [compute-directory]/raytemp/session_2020-10-04_12-37-38_811569_129114/sockets/raylet (num_attempts = 7, num_retries = 10)
I1004 12:37:47.324980 129114 129114 client_connection.cc:53] Retrying to connect to socket for endpoint [compute-directory]/raytemp/session_2020-10-04_12-37-38_811569_129114/sockets/raylet (num_attempts = 8, num_retries = 10)
I1004 12:37:48.333040 129114 129114 client_connection.cc:53] Retrying to connect to socket for endpoint [compute-directory]/raytemp/session_2020-10-04_12-37-38_811569_129114/sockets/raylet (num_attempts = 9, num_retries = 10)
F1004 12:37:49.333159 129114 129114 raylet_client.cc:54] Could not connect to socket [compute-directory]/raytemp/session_2020-10-04_12-37-38_811569_129114/sockets/raylet
F1004 12:37:49.333159 129114 129114 raylet_client.cc:54] Could not connect to socket [compute-directory]/raytemp/session_2020-10-04_12-37-38_811569_129114/sockets/raylet
F1004 12:37:49.333159 129114 129114 raylet_client.cc:54] Could not connect to socket [compute-directory]/raytemp/session_2020-10-04_12-37-38_811569_129114/sockets/raylet
F1004 12:37:49.333159 129114 129114 raylet_client.cc:54] Could not connect to socket [compute-directory]/raytemp/session_2020-10-04_12-37-38_811569_129114/sockets/raylet
&lt;/denchmark-code&gt;

raylet.err (the below was just repeated multiple times):
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/home/joel397/anaconda3/envs/tf2.0-ray1.0.0/lib/python3.6/site-packages/redis/connection.py", line 552, in connect
    sock = self._connect()
  File "/home/joel397/anaconda3/envs/tf2.0-ray1.0.0/lib/python3.6/site-packages/redis/connection.py", line 609, in _connect
    raise err
  File "/home/joel397/anaconda3/envs/tf2.0-ray1.0.0/lib/python3.6/site-packages/redis/connection.py", line 597, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/joel397/anaconda3/envs/tf2.0-ray1.0.0/lib/python3.6/site-packages/ray/workers/default_worker.py", line 172, in &lt;module&gt;
    connect_only=True)
  File "/home/joel397/anaconda3/envs/tf2.0-ray1.0.0/lib/python3.6/site-packages/ray/node.py", line 142, in __init__
    redis_client.get("session_name"))
  File "/home/joel397/anaconda3/envs/tf2.0-ray1.0.0/lib/python3.6/site-packages/redis/client.py", line 1579, in get
    return self.execute_command('GET', name)
  File "/home/joel397/anaconda3/envs/tf2.0-ray1.0.0/lib/python3.6/site-packages/redis/client.py", line 875, in execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
  File "/home/joel397/anaconda3/envs/tf2.0-ray1.0.0/lib/python3.6/site-packages/redis/connection.py", line 1185, in get_connection
    connection.connect()
  File "/home/joel397/anaconda3/envs/tf2.0-ray1.0.0/lib/python3.6/site-packages/redis/connection.py", line 557, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 111 connecting to 10.13.164.76:6379. Connection refused.
&lt;/denchmark-code&gt;

raylet.out:
&lt;denchmark-code&gt;I1004 12:37:39.337460 129139 129139 io_service_pool.cc:36] IOServicePool is running with 1 io_service.
I1004 12:37:39.339325 129139 129139 store_runner.cc:30] Allowing the Plasma store to use up to 57.5308GB of memory.
I1004 12:37:39.339366 129139 129139 store_runner.cc:44] Starting object store with directory /dev/shm and huge page support disabled
I1004 12:37:40.343340 129139 129139 grpc_server.cc:74] ObjectManager server started, listening on port 43512.
W1004 12:37:50.348263 129139 129154 metric_exporter.cc:211] Export metrics to agent failed: IOError: 14: failed to connect to all addresses
W1004 12:37:50.348263 129139 129154 metric_exporter.cc:211] Export metrics to agent failed: IOError: 14: failed to connect to all addresses
W1004 12:38:07.144881 129139 129154 metric_exporter.cc:211] Export metrics to agent failed: IOError: 14: failed to connect to all addresses
W1004 12:38:07.144881 129139 129154 metric_exporter.cc:211] Export metrics to agent failed: IOError: 14: failed to connect to all addresses
&lt;/denchmark-code&gt;

redis.out:
&lt;denchmark-code&gt;129119:C 04 Oct 2020 12:37:38.850 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
129119:C 04 Oct 2020 12:37:38.851 # Redis version=5.0.9, bits=64, commit=00000000, modified=0, pid=129119, just started
129119:C 04 Oct 2020 12:37:38.851 # Configuration loaded
129119:M 04 Oct 2020 12:37:38.852 # Server initialized
129119:M 04 Oct 2020 12:37:38.852 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
&lt;/denchmark-code&gt;

redis-shard_0.out:
&lt;denchmark-code&gt;129123:C 04 Oct 2020 12:37:38.957 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
129123:C 04 Oct 2020 12:37:38.958 # Redis version=5.0.9, bits=64, commit=00000000, modified=0, pid=129123, just started
129123:C 04 Oct 2020 12:37:38.958 # Configuration loaded
129123:M 04 Oct 2020 12:37:38.959 # Could not create server TCP listening socket *:6379: bind: Address already in use
129124:C 04 Oct 2020 12:37:39.061 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
129124:C 04 Oct 2020 12:37:39.061 # Redis version=5.0.9, bits=64, commit=00000000, modified=0, pid=129124, just started
129124:C 04 Oct 2020 12:37:39.061 # Configuration loaded
129124:M 04 Oct 2020 12:37:39.061 # Server initialized
129124:M 04 Oct 2020 12:37:39.062 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
&lt;/denchmark-code&gt;

Ray version and other system information (Python version, TensorFlow version, OS):
OS: RHEL v7.7
Python package versions:
&lt;denchmark-code&gt;# Name                    Version                   Build  Channel
_libgcc_mutex             0.1                 conda_forge    conda-forge
_openmp_mutex             4.5                       1_gnu    conda-forge
absl-py                   0.10.0                   pypi_0    pypi
aiohttp                   3.6.2                    pypi_0    pypi
aiohttp-cors              0.7.0                    pypi_0    pypi
aioredis                  1.3.1                    pypi_0    pypi
astor                     0.8.1                    pypi_0    pypi
async-timeout             3.0.1                    pypi_0    pypi
attrs                     20.2.0                   pypi_0    pypi
beautifulsoup4            4.9.3                    pypi_0    pypi
blessings                 1.7                      pypi_0    pypi
ca-certificates           2020.6.20            hecda079_0    conda-forge
cachetools                4.1.1                    pypi_0    pypi
certifi                   2020.6.20        py36h9f0ad1d_0    conda-forge
chardet                   3.0.4                    pypi_0    pypi
click                     7.1.2                    pypi_0    pypi
cloudpickle               1.6.0                    pypi_0    pypi
colorama                  0.4.3                    pypi_0    pypi
colorful                  0.5.4                    pypi_0    pypi
contextvars               2.4                      pypi_0    pypi
dataclasses               0.7                      pypi_0    pypi
decorator                 4.4.2                    pypi_0    pypi
filelock                  3.0.12                   pypi_0    pypi
future                    0.18.2                   pypi_0    pypi
gast                      0.2.2                    pypi_0    pypi
google                    3.0.0                    pypi_0    pypi
google-api-core           1.22.3                   pypi_0    pypi
google-auth               1.22.0                   pypi_0    pypi
google-auth-oauthlib      0.4.1                    pypi_0    pypi
google-pasta              0.2.0                    pypi_0    pypi
googleapis-common-protos  1.52.0                   pypi_0    pypi
gpustat                   0.6.0                    pypi_0    pypi
grpcio                    1.32.0                   pypi_0    pypi
h5py                      2.10.0                   pypi_0    pypi
hiredis                   1.1.0                    pypi_0    pypi
hyperopt                  0.2.4                    pypi_0    pypi
idna                      2.10                     pypi_0    pypi
idna-ssl                  1.1.0                    pypi_0    pypi
immutables                0.14                     pypi_0    pypi
importlib-metadata        2.0.0                    pypi_0    pypi
joblib                    0.17.0                   pypi_0    pypi
jsonschema                3.2.0                    pypi_0    pypi
keras-applications        1.0.8                    pypi_0    pypi
keras-preprocessing       1.1.2                    pypi_0    pypi
ld_impl_linux-64          2.35                 h769bd43_9    conda-forge
libffi                    3.2.1             he1b5a44_1007    conda-forge
libgcc-ng                 9.3.0               h5dbcf3e_17    conda-forge
libgomp                   9.3.0               h5dbcf3e_17    conda-forge
libstdcxx-ng              9.3.0               h2ae2ef3_17    conda-forge
markdown                  3.2.2                    pypi_0    pypi
msgpack                   1.0.0                    pypi_0    pypi
multidict                 4.7.6                    pypi_0    pypi
ncurses                   6.2                  he1b5a44_1    conda-forge
networkx                  2.5                      pypi_0    pypi
numpy                     1.19.2                   pypi_0    pypi
nvidia-ml-py3             7.352.0                  pypi_0    pypi
oauthlib                  3.1.0                    pypi_0    pypi
opencensus                0.7.10                   pypi_0    pypi
opencensus-context        0.1.1                    pypi_0    pypi
openssl                   1.1.1h               h516909a_0    conda-forge
opt-einsum                3.3.0                    pypi_0    pypi
pandas                    1.1.2                    pypi_0    pypi
pip                       20.2.3                     py_0    conda-forge
prometheus-client         0.8.0                    pypi_0    pypi
protobuf                  3.13.0                   pypi_0    pypi
psutil                    5.7.2                    pypi_0    pypi
py-spy                    0.3.3                    pypi_0    pypi
pyasn1                    0.4.8                    pypi_0    pypi
pyasn1-modules            0.2.8                    pypi_0    pypi
pyrsistent                0.17.3                   pypi_0    pypi
python                    3.6.11          h4d41432_2_cpython    conda-forge
python-dateutil           2.8.1                    pypi_0    pypi
python_abi                3.6                     1_cp36m    conda-forge
pytz                      2020.1                   pypi_0    pypi
pyyaml                    5.3.1                    pypi_0    pypi
ray                       1.0.0                    pypi_0    pypi
readline                  8.0                  he28a2e2_2    conda-forge
redis                     3.4.1                    pypi_0    pypi
requests                  2.24.0                   pypi_0    pypi
requests-oauthlib         1.3.0                    pypi_0    pypi
rsa                       4.6                      pypi_0    pypi
scikit-learn              0.23.2                   pypi_0    pypi
scipy                     1.5.2                    pypi_0    pypi
setuptools                49.6.0           py36h9f0ad1d_1    conda-forge
shortuuid                 1.0.1                    pypi_0    pypi
six                       1.15.0                   pypi_0    pypi
sklearn                   0.0                      pypi_0    pypi
soupsieve                 2.0.1                    pypi_0    pypi
sqlite                    3.33.0               h4cf870e_0    conda-forge
tabulate                  0.8.7                    pypi_0    pypi
tensorboard               2.0.2                    pypi_0    pypi
tensorboardx              2.1                      pypi_0    pypi
tensorflow                2.0.0                    pypi_0    pypi
tensorflow-estimator      2.0.1                    pypi_0    pypi
tensorflow-gpu            2.0.0                    pypi_0    pypi
termcolor                 1.1.0                    pypi_0    pypi
threadpoolctl             2.1.0                    pypi_0    pypi
tk                        8.6.10               hed695b0_0    conda-forge
tqdm                      4.50.0                   pypi_0    pypi
typing-extensions         3.7.4.3                  pypi_0    pypi
urllib3                   1.25.10                  pypi_0    pypi
werkzeug                  1.0.1                    pypi_0    pypi
wheel                     0.35.1             pyh9f0ad1d_0    conda-forge
wrapt                     1.12.1                   pypi_0    pypi
xz                        5.2.5                h516909a_1    conda-forge
yarl                      1.6.0                    pypi_0    pypi
zipp                      3.3.0                    pypi_0    pypi
zlib                      1.2.11            h516909a_1009    conda-forge
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

import ray
ray.init(include_dashboard=False, _temp_dir='&lt;compute-directory&gt;/raytemp/')
If we cannot run your script, we cannot fix your issue.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='fshriver' date='2020-10-05T04:08:39Z'>
		I assume the error inside raylet.err is the cause. raylet has been probably crashed so that your worker cannot connect to through a socket file. (When ray communicates to a local raylet, it does through a raw TCP socket file). Can you make sure it is possible to connect to the Redis using some redis client? 10.13.164.76:6379
		</comment>
		<comment id='2' author='fshriver' date='2020-10-05T16:03:32Z'>
		I confirmed that I can connect to the ray server, when it's started externally (not from ray.init()). If I start ray external to the script (by running ray start --head --port=6379, for example) and run:
ray.init(address = 'auto', _redis_password='[whatever]', _temp_dir='[compute-directory]/raytemp/')
apparently the script CAN actually connect to the ray server, and starts putting trial information on my screen. However, within less than 30 seconds the script apparently crashes and simply reports "Killed". There doesn't appear to be any additional information, and apparently the [compute-directory]/raytemp directory I pass into the script no longer gets used to store log files. However, starting the ray server internally within my script reliably results in the logs saying they're unable to connect to the socket, same as before.
I guess I'd be willing to go with the slightly more tedious option of having to start the ray server external to the script, if I could figure out a way to get more information on what it means when it simply says "Killed". Is there a way to do that?
		</comment>
		<comment id='3' author='fshriver' date='2020-10-06T00:22:57Z'>
		Update on this: I'm working with my university's HPC support personnel to resolve the issue, and they're also running into issues when trying to use/connect to the Redis server. However, honestly the quickest and most convenient fix for me would probably be to externally start the Ray server, as the script then seems to have no problem connecting to it. HOWEVER, I've figured out that apparently the kernel doesn't like Ray when it starts trying to actually process jobs, and ends up killing the process due to out of memory constraints. As an example, I downloaded the hyperopt example (&lt;denchmark-link:https://github.com/ray-project/ray/blob/master/python/ray/tune/examples/hyperopt_example.py&gt;https://github.com/ray-project/ray/blob/master/python/ray/tune/examples/hyperopt_example.py&lt;/denchmark-link&gt;
) and modified it to point to the externally-started Ray server instead of one that's automatically started. It consistently fails after about 10 seconds (although the actual time it takes to fail is variable) and reports that it was killed by the Linux kernel. Below is the tail of the output from dmesg:
&lt;denchmark-code&gt;[1400495.941376] [ pid ]   uid  tgid total_vm      rss nr_ptes swapents oom_score_adj name
[1400495.949952] [ 1926]  4258  1926    31915     1048      18        0             0 bash
[1400495.958431] [ 4234]  4258  4234    41603     2373      35        0             0 redis-server
[1400495.967572] [ 4239]  4258  4239    41603     2316      33        0             0 redis-server
[1400495.976725] [ 4243]  4258  4243   195341     4857      45        0             0 gcs_server
[1400495.985734] [ 4244]  4258  4244   170171    12237      93        0             0 /home/joel397/a
[1400495.995191] [ 4259]  4258  4259     3896      363      12        0             0 plasma_store_se
[1400496.004609] [ 4260]  4258  4260 29476678    14973     118        0             0 raylet
[1400496.013269] [ 4261]  4258  4261   344066    14086     123        0             0 /home/joel397/a
[1400496.022704] [13843]  4258 13843    28797      278      13        0             0 tmux: client
[1400496.031885] [13845]  4258 13845    29557      571      14        0             0 tmux: server
[1400496.041056] [13984]  4258 13984    31874      973      17        0             0 bash
[1400496.049545] [14050]  4258 14050    32919      814      20        0             0 htop
[1400496.058024] [14176]  4258 14176   370411    18810     172        0             0 ray::ImplicitFu
[1400496.067467] [14239]  4258 14239   370282    18688     169        0             0 ray::ImplicitFu
[1400496.076927] [14640]  4258 14640    31900     1041      17        0             0 bash
[1400496.085419] [14887]  4258 14887 15019885    25950     291        0             0 python3 hyperop
[1400496.094860] [14927]  4258 14927 14778124    18430     171        0             0 ray::ImplicitFu
[1400496.104332] [14935]  4258 14935 14778124    18499     172        0             0 ray::ImplicitFu
[1400496.113796] [14937]  4258 14937 14730272    12315     111        0             0 ray::IDLE
[1400496.122730] [14938]  4258 14938 14747192    13652     140        0             0 ray::IDLE
[1400496.131647] [14956]  4258 14956 14753466    15026     156        0             0 ray::IDLE
[1400496.140535] Memory cgroup out of memory: Kill process 15044 (python3 hyperop) score 170 or sacrifice child
[1400496.150831] Killed process 14887 (python3 hyperop), UID 4258, total-vm:60079540kB, anon-rss:87140kB, file-rss:16652kB, shmem-rss:8kB
&lt;/denchmark-code&gt;

I'm confused as to what would be causing this kind of behavior however, since the example script shouldn't be consuming that much memory... is this a known issue? I can't find anything in documentation or issues reporting something like this happening before, other than this Github issue: &lt;denchmark-link:https://github.com/ray-project/ray/issues/10037&gt;#10037&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='fshriver' date='2020-10-06T01:05:23Z'>
		You can actually see the memory usage from the ray dashboard. Can you check how much memory was used when you ran the example? The error just looks like some of the processes used more memory than it could use in your group.
		</comment>
		<comment id='5' author='fshriver' date='2020-10-06T01:06:20Z'>
		Also, not sure if you have the access, but you can also try checking the memory constraint of cgroup and increase them if they are too low?
		</comment>
		<comment id='6' author='fshriver' date='2020-10-22T16:27:23Z'>
		Going to go ahead and close this, as the issues are stale and have been resolved (user error).
		</comment>
	</comments>
</bug>