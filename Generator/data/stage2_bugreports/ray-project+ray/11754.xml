<bug id='11754' author='pengzhenghao' open_date='2020-11-02T14:49:45Z' closed_time='2020-11-04T12:48:26Z'>
	<summary>[rllib] Behavior Cloning leaks memory</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

Ray version and other system information (Python version, TensorFlow version, OS):
Behavior Cloning leaks memory. Using timesteps_per_iteration will make leakage much faster.
I am using Ray 1.0.
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
&lt;denchmark-h:h4&gt;Setting&lt;/denchmark-h&gt;

First, I generate the dataset using the way in ray/rllib/tuned_examples/marwil/cartpole-bc.yaml, but changing the dataset size from 50,000 to 200,000:
rllib train --run=PPO --env=CartPole-v0 --stop='{"timesteps_total": 200000}' --config='{"output": "/tmp/out", "batch_mode": "complete_episodes"}'
Then I run the script test.yaml identical to the example yaml but with larger stopping criterion, via: rllib train -f test.yaml:
# test.yaml
cartpole-bc:
    env: CartPole-v0
    run: BC
    stop:
        timesteps_total: 5000000  # &lt;&lt;== Here
    config:
        # Works for both torch and tf.
        framework: tf
        # In order to evaluate on an actual environment, use these following
        # settings:
        evaluation_num_workers: 1
        evaluation_interval: 1
        evaluation_config:
            input: sampler
        # The historic (offline) data file from the PPO run (at the top).
        input: /tmp/out
        timesteps_per_iteration: 10000  # &lt;&lt;== Here!!!!
&lt;denchmark-h:h4&gt;With timesteps_per_iteration &lt;/denchmark-h&gt;

Then I find the memory increases linearly.
The first iteration, the memory cost is:
&lt;denchmark-link:https://user-images.githubusercontent.com/22206995/97874156-8a9d7c80-1d53-11eb-92c7-5a0f245041b3.png&gt;&lt;/denchmark-link&gt;

After 2.3M step, 180s (printed time, not exactly time), the memory goes to 6.3G.
&lt;denchmark-link:https://user-images.githubusercontent.com/22206995/97879436-eddedd00-1d5a-11eb-983b-be832f7c7140.png&gt;&lt;/denchmark-link&gt;

So if timesteps_per_iteration  is turn on, the memory goes from 4.9 G to 6.3 G in 2.3M steps in a linear manner. The memory is leaked in a very fast speed, if using larger dataset, until the OOM happens.
&lt;denchmark-h:h4&gt;Without timesteps_per_iteration &lt;/denchmark-h&gt;

If we close the option timesteps_per_iteration , the memory is gradually increased from 6.2G to 6.8G and will still goes to OOM, but the speed is much slower than with timesteps_per_iteration.
&lt;denchmark-link:https://user-images.githubusercontent.com/22206995/97880130-bd4b7300-1d5b-11eb-9180-1bf639ba8c2b.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/22206995/97880813-93df1700-1d5c-11eb-9ace-eafdeef905c6.png&gt;&lt;/denchmark-link&gt;

I have tried this on larger dataset and the memory leak is even more drastic, so the result here is only to reproduce the issue. For your information, I find that the incremental memory is exactly the size of the offline dataset.
&lt;denchmark-h:h4&gt;Another issue&lt;/denchmark-h&gt;

By the way, the printed time (time_total_s) is incorrect in BC. The real-world time elapses much much faster than the printed time. I am not sure why.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='pengzhenghao' date='2020-11-02T15:02:31Z'>
		I think the issue is related to &lt;denchmark-link:https://github.com/ray-project/ray/issues/9961&gt;#9961&lt;/denchmark-link&gt;
 .
The document say replay_buffer_size  refer to the steps, while in practice it refers to the number of batches. This is extremely misleading.
&lt;denchmark-code&gt;    # Number of steps max to keep in the batch replay buffer.
    "replay_buffer_size": 100000,
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='pengzhenghao' date='2020-11-04T12:48:26Z'>
		Closing this. This has been fixed here:
&lt;denchmark-link:https://github.com/ray-project/ray/pull/10957&gt;#10957&lt;/denchmark-link&gt;
 (not part of ray 1.0.0, only available in current master!).
A warning has been added and a counting bug related to returning the correct buffer size has been fixed.
		</comment>
	</comments>
</bug>