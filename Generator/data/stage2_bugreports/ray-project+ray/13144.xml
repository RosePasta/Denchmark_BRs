<bug id='13144' author='sahewat' open_date='2020-12-31T22:37:25Z' closed_time='2021-01-01T18:50:14Z'>
	<summary>RAY_PDB toggle causes actor died message</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

*Ray version and other system information (Python version, TensorFlow version, OS): Python 3.7, ray 1.2 nightly (have also reproduced w/ same issue on 1.1 release)
&lt;denchmark-link:https://github.com/richardliaw&gt;@richardliaw&lt;/denchmark-link&gt;
 I have ray set up on a GCP cluster using the instructions provided in the latest docs.
I’m having problems with ray SGD dying after throwing the error:
&lt;denchmark-code&gt;File "/opt/conda/lib/python3.7/site-packages/ray/util/sgd/torch/torch_trainer.py", line 266, in __init__
    self._start_workers(self.max_replicas)
  File "/opt/conda/lib/python3.7/site-packages/ray/util/sgd/torch/torch_trainer.py", line 326, in _start_workers
    self.worker_group.start_workers(num_workers)
  File "/opt/conda/lib/python3.7/site-packages/ray/util/sgd/torch/worker_group.py", line 231, in start_workers
    ray.get(self._setup_operator())
  File "/opt/conda/lib/python3.7/site-packages/ray/_private/client_mode_hook.py", line 47, in wrapper
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/ray/worker.py", line 1395, in get
    raise value
&lt;/denchmark-code&gt;

ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task. Check python-core-worker-*.log files for more information.
I’ve investigated the  files on the worker and see the attached output &lt;denchmark-link:https://github.com/ray-project/ray/files/5758394/out.txt&gt;out.txt&lt;/denchmark-link&gt;

Strangely, this doesn’t happen as long as I use the RAY_PDB=1 flag and runs normally.
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a short code snippet (less than 50 lines if possible) that can be copy-pasted to reproduce the issue. The snippet should have no external library dependencies (i.e., use fake or mock data / environments):
&lt;denchmark-code&gt;import ray
from ray.util.sgd import TorchTrainer
from ray.util.sgd.torch import TrainingOperator
from ray.util.sgd.torch.examples.train_example import LinearDataset

import torch
from torch.utils.data import DataLoader

class CustomTrainingOperator(TrainingOperator):
    def setup(self, config):
        # Load data.
        train_loader = DataLoader(LinearDataset(2, 5), config["batch_size"])
        val_loader = DataLoader(LinearDataset(2, 5), config["batch_size"])

        # Create model.
        model = torch.nn.Linear(1, 1)

        # Create optimizer.
        optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)

        # Create loss.
        loss = torch.nn.MSELoss()

        # Register model, optimizer, and loss.
        self.model, self.optimizer, self.criterion = self.register(
            models=model,
            optimizers=optimizer,
            criterion=loss)

        # Register data loaders.
        self.register_data(train_loader=train_loader, validation_loader=val_loader)


ray.init(address='auto')

trainer1 = TorchTrainer(
    training_operator_cls=CustomTrainingOperator,
    num_workers=2,
    use_gpu=True,
    config={"batch_size": 64})

stats = trainer1.train()
print(stats)
trainer1.shutdown()
print("success!")
&lt;/denchmark-code&gt;

If the code snippet cannot be run by itself, the issue will be closed with "needs-repro-script".

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='sahewat' date='2020-12-31T23:02:02Z'>
		&lt;denchmark-link:https://github.com/sahewat&gt;@sahewat&lt;/denchmark-link&gt;
 can you quickly measure roughly how often RAY_PDB=1 fails vs RAY_PDB=0 fails?
		</comment>
	</comments>
</bug>