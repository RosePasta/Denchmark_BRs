<bug id='1259' author='robertnishihara' open_date='2017-11-26T23:23:49Z' closed_time='2017-11-27T07:29:38Z'>
	<summary>Initial sets of tasks are slow on large cluster.</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
Ray installed from (source or binary): source
Ray version: Testing this PR #1257, but I'm pretty sure it happens with the current master at e583d5a.
Python version: Python 3.5.2, Anaconda 4.2.0 (64-bit)

I first start Ray on the head node with
&lt;denchmark-code&gt;ray start --head --redis-port=6379 --redis-max-clients=65504
&lt;/denchmark-code&gt;

Then I start Ray on the workers with
&lt;denchmark-code&gt;parallel-ssh -h workers.txt -P -I -p 200 &lt; start_worker.sh
&lt;/denchmark-code&gt;

using a start_worker.sh containing
&lt;denchmark-code&gt;export PATH=/home/ubuntu/anaconda3/bin/:$PATH
ray start --redis-address=&lt;head-ip-address&gt;:6379
&lt;/denchmark-code&gt;

Then I start a driver (on the head node) with
import ray
import time

ray.init(redis_address=&lt;head-node-ip&gt;)

@ray.remote
def f():
    time.sleep(0.01)
    return ray.services.get_node_ip_address()
Then I run
&lt;denchmark-code&gt;%time l = set(ray.get([f.remote() for _ in range(1000)]))
&lt;/denchmark-code&gt;

in the driver a number of times. The first couple times take on the order of 50 seconds. Subsequent runs take around 170 milliseconds.
In /tmp/raylogs, a number of processes contain lines like
&lt;denchmark-code&gt;[WARN] (/home/ubuntu/ray/src/common/state/db_client_table.cc:54) calling redis_get_cached_db_client in a loop in with 1 manager IDs took 1038 milliseconds.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='robertnishihara' date='2017-11-27T03:47:51Z'>
		The problem seems to be that the primary Redis shard is overwhelmed (it's CPU utilization seems to grow linearly with the number of workers, reaching 100% utilization with around 15000 workers) even when no tasks are running and the cluster isn't doing anything. The CPU utilization is a combination of two things.


Each worker has an import_thread, which calls
worker.import_pubsub_client.psubscribe("__keyspace@0__:Exports")



Local schedulers and plasma managers send frequent heartbeats to the primary redis shard.


I think the issue is that psubscribe is causing all of the local scheduler heartbeats to be more expensive than they should be (presumably the channel name is checked against all of the patterns). However, we aren't actually using patterns in these psubscribes, so the fix should be easy.
		</comment>
	</comments>
</bug>