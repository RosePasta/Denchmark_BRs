<bug id='7626' author='Jimpachnet' open_date='2020-03-16T16:39:36Z' closed_time='2020-03-30T21:58:41Z'>
	<summary>[Ray] Ray crashes when initializing a simple actor</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

When executing &lt;denchmark-link:https://gist.github.com/Jimpachnet/d3c283bf8d6dbafaf32dd391c63e9dc4&gt;this&lt;/denchmark-link&gt;
 fairly simple script on a single machine with 80 cores and 200Gb of RAM, Ray crashes. Depending on the used Ray version, I get different errors:
Python 3.5.2
Ray 0.8.2:
&lt;denchmark-code&gt;[...]
File "/home/siev_le/Documents/venvs/sdddpg/lib/python3.5/site-packages/ray/pickle5_files/pickle5/pickle.py", line 570, in save
    self.save_global(obj)
  File "/home/siev_le/Documents/venvs/sdddpg/lib/python3.5/site-packages/ray/pickle5_files/pickle5/pickle.py", line 1093, in save_global
    self.save(module_name)
  File "/home/siev_le/Documents/venvs/sdddpg/lib/python3.5/site-packages/ray/pickle5_files/pickle5/pickle.py", line 601, in save
    self.save_reduce(obj=obj, *rv)
  File "/home/siev_le/Documents/venvs/sdddpg/lib/python3.5/site-packages/ray/pickle5_files/pickle5/pickle.py", line 685, in save_reduce
    save(cls)
  File "/home/siev_le/Documents/venvs/sdddpg/lib/python3.5/site-packages/ray/pickle5_files/pickle5/pickle.py", line 570, in save
    self.save_global(obj)
  File "/home/siev_le/Documents/venvs/sdddpg/lib/python3.5/site-packages/ray/pickle5_files/pickle5/pickle.py", line 1093, in save_global
    self.save(module_name)
  File "/home/siev_le/Documents/venvs/sdddpg/lib/python3.5/site-packages/ray/pickle5_files/pickle5/pickle.py", line 601, in save
    self.save_reduce(obj=obj, *rv)
  File "/home/siev_le/Documents/venvs/sdddpg/lib/python3.5/site-packages/ray/pickle5_files/pickle5/pickle.py", line 685, in save_reduce
    save(cls)
  File "/home/siev_le/Documents/venvs/sdddpg/lib/python3.5/site-packages/ray/pickle5_files/pickle5/pickle.py", line 570, in save
    self.save_global(obj)
  File "/home/siev_le/Documents/venvs/sdddpg/lib/python3.5/site-packages/ray/pickle5_files/pickle5/pickle.py", line 1093, in save_global
    self.save(module_name)
  File "/home/siev_le/Documents/venvs/sdddpg/lib/python3.5/site-packages/ray/pickle5_files/pickle5/pickle.py", line 601, in save
    self.save_reduce(obj=obj, *rv)
  File "/home/siev_le/Documents/venvs/sdddpg/lib/python3.5/site-packages/ray/pickle5_files/pickle5/pickle.py", line 685, in save_reduce
    save(cls)
  File "/home/siev_le/Documents/venvs/sdddpg/lib/python3.5/site-packages/ray/pickle5_files/pickle5/pickle.py", line 570, in save
    self.save_global(obj)
  File "/home/siev_le/Documents/venvs/sdddpg/lib/python3.5/site-packages/ray/pickle5_files/pickle5/pickle.py", line 1093, in save_global
    self.save(module_name)
  File "/home/siev_le/Documents/venvs/sdddpg/lib/python3.5/site-packages/ray/pickle5_files/pickle5/pickle.py", line 601, in save
    self.save_reduce(obj=obj, *rv)
  File "/home/siev_le/Documents/venvs/sdddpg/lib/python3.5/site-packages/ray/pickle5_files/pickle5/pickle.py", line 685, in save_reduce
    save(cls)
  File "/home/siev_le/Documents/venvs/sdddpg/lib/python3.5/site-packages/ray/pickle5_files/pickle5/pickle.py", line 551, in save
    rv = reduce(obj)
  File "/home/siev_le/Documents/venvs/sdddpg/lib/python3.5/site-packages/ray/cloudpickle/cloudpickle_fast.py", line 557, in reducer_override
    return _class_reduce(obj)
  File "/home/siev_le/Documents/venvs/sdddpg/lib/python3.5/site-packages/ray/cloudpickle/cloudpickle_fast.py", line 358, in _class_reduce
    elif not _is_global(obj):
  File "/home/siev_le/Documents/venvs/sdddpg/lib/python3.5/site-packages/ray/cloudpickle/cloudpickle.py", line 198, in _is_global
    obj2, parent = _getattribute(module, name)
  File "/usr/lib/python3.5/pickle.py", line 263, in _getattribute
    if subpath == '&lt;locals&gt;':
RecursionError: maximum recursion depth exceeded in comparison

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main.py", line 32, in &lt;module&gt;
    learner = SDDDPG(parallel_settings, ddpg_settings, dddpg_settings)
  File "/home/siev_le/Documents/projects/sdddpg/sdddpg/sdddpg.py", line 40, in __init__
    plasma_directory="/tmp/", include_webui=False)
  File "/home/siev_le/Documents/venvs/sdddpg/lib/python3.5/site-packages/ray/worker.py", line 805, in init
    if _internal_config else {})
  File "/home/siev_le/Documents/venvs/sdddpg/lib/python3.5/site-packages/ray/worker.py", line 1259, in connect
    worker.put_object(1, object_id=temporary_object_id)
  File "/home/siev_le/Documents/venvs/sdddpg/lib/python3.5/site-packages/ray/worker.py", line 272, in put_object
    serialized_value = self.get_serialization_context().serialize(value)
  File "/home/siev_le/Documents/venvs/sdddpg/lib/python3.5/site-packages/ray/serialization.py", line 353, in serialize
    value, protocol=5, buffer_callback=writer.buffer_callback)
  File "/home/siev_le/Documents/venvs/sdddpg/lib/python3.5/site-packages/ray/cloudpickle/cloudpickle_fast.py", line 72, in dumps
    cp.dump(obj)
  File "/home/siev_le/Documents/venvs/sdddpg/lib/python3.5/site-packages/ray/cloudpickle/cloudpickle_fast.py", line 624, in dump
    raise pickle.PicklingError(msg)
pickle5.pickle.PicklingError: Could not pickle object as excessively deep recursion required.
&lt;/denchmark-code&gt;

With Ray 0.8.1 i get the following error:
&lt;denchmark-link:https://gist.github.com/Jimpachnet/71308735549da2adcacdba57ab0ae5cc&gt;error&lt;/denchmark-link&gt;
 . This error disappears when i reduce the number of used cores to around 50.
What are possible reasons?
Thank you in advance.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the [latest wheels]

	</description>
	<comments>
		<comment id='1' author='Jimpachnet' date='2020-03-18T16:48:09Z'>
		&lt;denchmark-link:https://github.com/suquark&gt;@suquark&lt;/denchmark-link&gt;
 This is a known issue I believe?
		</comment>
		<comment id='2' author='Jimpachnet' date='2020-03-19T03:30:03Z'>
		I suppose it is more likely related to cloudpickle. Let me reproduce this bug first.
&lt;denchmark-link:https://github.com/Jimpachnet&gt;@Jimpachnet&lt;/denchmark-link&gt;
 Just to make sure that only ray 0.8.1 will get the error when num of CPU cores &gt; 50, right?
If it is convenient for you, could you also try ray 0.8.2 with python3.6+? That could provide very valuable information, because some bugs could only specific to python3.5
		</comment>
		<comment id='3' author='Jimpachnet' date='2020-03-19T04:12:30Z'>
		When I run &lt;denchmark-link:https://github.com/Jimpachnet&gt;@Jimpachnet&lt;/denchmark-link&gt;
 's script with python3.7.4, tensorflow==1.15.2, I got this exception:
&lt;denchmark-code&gt;TypeError: can't pickle _thread.RLock objects
&lt;/denchmark-code&gt;

This exception is due to the fact that the return value of  () is not serializable. &lt;denchmark-link:https://github.com/simon-mo&gt;@simon-mo&lt;/denchmark-link&gt;
 I am not familiar with tensorflow serialization. Should  be serializable in this case?
		</comment>
		<comment id='4' author='Jimpachnet' date='2020-03-19T10:17:00Z'>
		Thanks for looking into it. &lt;denchmark-link:https://github.com/suquark&gt;@suquark&lt;/denchmark-link&gt;
 thanks for pointing out the problem with pickling , this was an artifact of my attempts to narrow down the problem. Updating to Python 3.6 resolves the pickling problem w/ ray 0.8.2 and now shows the same behavior as ray 0.8.1 with python 3.5. With both configurations, the error only occurs for  &gt; 46 .
		</comment>
		<comment id='5' author='Jimpachnet' date='2020-03-27T13:19:14Z'>
		Ray 0.8.3 does not change the behavior compared to ray 0.8.2.
		</comment>
		<comment id='6' author='Jimpachnet' date='2020-03-30T04:40:28Z'>
		If possible, updating to Python 3.5.4 and above should make the pickling error go away. We currently don't have enough bandwidth to fix this for 3.5.2. CPython interpreter changed quite a lot between those versions.
		</comment>
		<comment id='7' author='Jimpachnet' date='2020-03-30T04:44:58Z'>
		For the scaling problem, can you try forcing  environment variable?
This should be set by ray on startup but might as well to propagate this across. The reason is documented in &lt;denchmark-link:https://ray.readthedocs.io/en/latest/configure.html#cluster-resources&gt;https://ray.readthedocs.io/en/latest/configure.html#cluster-resources&lt;/denchmark-link&gt;

Other issue might be some resource ran out, what does ulimit -a shows? If any of the resource is too low, especially file descriptors or number of threads, you might want to increase them.
		</comment>
		<comment id='8' author='Jimpachnet' date='2020-03-30T09:30:40Z'>
		Thanks for your answer, OMP_NUM_THREADS=1 is explicitly set also returned by printenv.
ulimit shows a fairly standard
&lt;denchmark-code&gt;core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 805860
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 1024
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 805860
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited
&lt;/denchmark-code&gt;

With tensorflow (cpu) == 1.12.0 (so far i used 1.15.2) I can set Num_Workers=67 without crashing, otherwise I get
&lt;denchmark-code&gt;(pid=12078)   np_resource = np.dtype([("resource", np.ubyte, 1)])
(pid=12078) 2020-03-30 09:26:26.297750: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
(pid=12078) terminate called after throwing an instance of 'std::system_error'
(pid=12078)   what():  Resource temporarily unavailable
(pid=12078) *** Aborted at 1585560386 (unix time) try "date -d @1585560386" if you are using GNU date ***
(pid=12078) PC: @                0x0 (unknown)
(pid=12078) *** SIGABRT (@0x3e900002f2e) received by PID 12078 (TID 0x7f5fc2654700) from PID 12078; stack trace: ***
(pid=12078)     @     0x7f5fc223f390 (unknown)
(pid=12078)     @     0x7f5fc1346428 gsignal
(pid=12078)     @     0x7f5fc134802a abort
(pid=12078)     @     0x7f5fbfb8484d __gnu_cxx::__verbose_terminate_handler()
(pid=12078)     @     0x7f5fbfb826b6 (unknown)
(pid=12078)     @     0x7f5fbfb82701 std::terminate()
(pid=12078)     @     0x7f5fbfb82919 __cxa_throw
(pid=12078)     @     0x7f5fbfbab7fe std::__throw_system_error()
(pid=12078)     @     0x7f5fbfbade73 std::thread::_M_start_thread()
(pid=12078)     @     0x7f5fbfbadecd std::thread::_M_start_thread()
(pid=12078)     @     0x7f5fae9ce3e0 tensorflow::(anonymous namespace)::PosixEnv::StartThread()
(pid=12078)     @     0x7f5fae9a7187 tensorflow::thread::ThreadPool::ThreadPool()
(pid=12078)     @     0x7f5fae9a7430 tensorflow::thread::ThreadPool::ThreadPool()
(pid=12078)     @     0x7f5fae9746d3 tensorflow::NewThreadPoolFromSessionOptions()
(pid=12078)     @     0x7f50d71add75 tensorflow::DirectSession::DirectSession()
(pid=12078)     @     0x7f50d71adf03 tensorflow::DirectSessionFactory::NewSession()
(pid=12078)     @     0x7f5fae983289 tensorflow::NewSession()
(pid=12078)     @     0x7f50d5972895 TF_NewSession
E0330 09:26:26.366935 12198 task_manager.cc:194] Task failed: IOError: 14: Socket closed: Type=ACTOR_TASK, Language=PYTHON, function_descriptor={type=PythonFunctionDescriptor, module_name=sdddpg.rollout_worker, class_name=Rollout_Worker, function_name=create_actor_network, function_hash=}, task_id=2d32654833bf14b8a9aba8520100, job_id=0100, num_args=0, num_returns=2, actor_task_spec={actor_id=a9aba8520100, actor_caller_id=ffffffffffffffffffffffff0100, actor_counter=0}
2020-03-30 09:26:26,367 WARNING worker.py:1072 -- A worker died or was killed while executing task ffffffffffffffffa9aba8520100.
(pid=12078)     @     0x7f50d577d822 tensorflow::TF_NewSessionRef()
(pid=12078)     @     0x7f50d571bd61 _wrap_TF_NewSessionRef
(pid=12078)     @           0x52352e _PyCFunction_FastCallKeywords
(pid=12078)     @           0x57dbf9 (unknown)
(pid=12078)     @           0x57642f _PyEval_EvalFrameDefault
(pid=12078)     @           0x5754b7 (unknown)
(pid=12078)     @           0x57ed4b (unknown)
(pid=12078)     @           0x57dcdc (unknown)
(pid=12078)     @           0x5771ad _PyEval_EvalFrameDefault
(pid=12078)     @           0x5757ee (unknown)
(pid=12078)     @           0x57f65f _PyFunction_FastCallDict
(pid=12078)     @           0x4e839c _PyObject_Call_Prepend
(pid=12078)     @           0x4e7a2a PyObject_Call
(pid=12078)     @           0x53621f (unknown)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='Jimpachnet' date='2020-03-30T21:11:14Z'>
		&lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
 have you seem anything like this with tensorflow? This seems like tensorflow can't start new threads with their thread_pool even we set OMP_NUM_THREADS=1?
&lt;denchmark-link:https://github.com/Jimpachnet&gt;@Jimpachnet&lt;/denchmark-link&gt;
 the "number of open file" looks dangerously low. But from the traceback it looks like there's issue with tensorflow threading, which is not related to file descriptors.
Maybe consider this fix? &lt;denchmark-link:https://stackoverflow.com/questions/34389945/changing-the-number-of-threads-in-tensorflow-on-cifar10&gt;https://stackoverflow.com/questions/34389945/changing-the-number-of-threads-in-tensorflow-on-cifar10&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='Jimpachnet' date='2020-03-30T21:58:41Z'>
		I played a bit around and it worked on a fresh system and by removing the session from the member variables of the workers. The open file limit is no problem for such a small system. Thanks for the help.
		</comment>
		<comment id='11' author='Jimpachnet' date='2020-08-06T19:13:21Z'>
		&lt;denchmark-link:https://github.com/Jimpachnet&gt;@Jimpachnet&lt;/denchmark-link&gt;
 Will you please explain in more detail how to "worked on a fresh system and by removing the session from the member variables of the workers"?
		</comment>
		<comment id='12' author='Jimpachnet' date='2020-08-06T21:24:30Z'>
		Back then removing the Tensorflow session from the class member variables "fixed" the issue. With the current ray release the problem does not occur anymore.
		</comment>
	</comments>
</bug>