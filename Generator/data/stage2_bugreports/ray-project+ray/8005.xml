<bug id='8005' author='PovelikinRostislav' open_date='2020-04-13T14:48:35Z' closed_time='2021-01-05T03:41:33Z'>
	<summary>RaySGD benchmark fails to finish with more than one worker</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

Execution of benchmark.py with 2 workers on local cluster fails due to killed actors because of unhandled exception from PyTorch:
&lt;denchmark-code&gt;$ python benchmark.py --local --no-cuda
2020-04-13 17:35:22,720 INFO resource_spec.py:204 -- Starting Ray with 120.07 GiB memory available for workers and up to 55.47 GiB for objects. You can adjust these settings with ray.init(me
mory=&lt;bytes&gt;, object_store_memory=&lt;bytes&gt;).
2020-04-13 17:35:23,075 INFO services.py:1146 -- View the Ray dashboard at localhost:8265
Model: resnet50
Batch size: 32
Number of CPUs: 2
2020-04-13 17:35:26,601 WARNING torch_runner.py:102 -- TorchTrainer data_creator return values are no longer wrapped as DataLoaders. Users must return DataLoader(s) in data_creator. This war
ning will be removed in a future version of Ray.
(pid=76784) 2020-04-13 17:35:26,605     WARNING torch_runner.py:102 -- TorchTrainer data_creator return values are no longer wrapped as DataLoaders. Users must return DataLoader(s) in data_c
reator. This warning will be removed in a future version of Ray.
2020-04-13 17:36:05,455 WARNING torch_trainer.py:413 -- [/opt/conda/conda-bld/pytorch_1579022027171/work/third_party/gloo/gloo/transport/tcp/unbound_buffer.cc:84] Timed out waiting 10000ms f
or recv operation to complete
2020-04-13 17:36:05,841 WARNING worker.py:1072 -- A worker died or was killed while executing task fffffffffffffffff66d17ba0100.
2020-04-13 17:36:08,759 WARNING torch_runner.py:102 -- TorchTrainer data_creator return values are no longer wrapped as DataLoaders. Users must return DataLoader(s) in data_creator. This war
ning will be removed in a future version of Ray.
(pid=76787) 2020-04-13 17:36:08,780     WARNING torch_runner.py:102 -- TorchTrainer data_creator return values are no longer wrapped as DataLoaders. Users must return DataLoader(s) in data_c
reator. This warning will be removed in a future version of Ray.
2020-04-13 17:36:09,108 INFO torch_trainer.py:376 -- Retrying training step with 2 workers.
2020-04-13 17:36:11,273 ERROR worker.py:1011 -- Possible unhandled error from worker: ray::DistributedTorchRunner.train_epoch() (pid=76784, ip=10.125.21.189)
  File "python/ray/_raylet.pyx", line 452, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 407, in ray._raylet.execute_task.function_executor
  File "/rpovelik/installed/miniconda3/envs/ray/lib/python3.8/site-packages/ray/util/sgd/torch/distributed_torch_runner.py", line 138, in train_epoch
    return super(DistributedTorchRunner, self).train_epoch(**kwargs)
  File "/rpovelik/installed/miniconda3/envs/ray/lib/python3.8/site-packages/ray/util/sgd/torch/torch_runner.py", line 192, in train_epoch
    train_stats = self.training_operator.train_epoch(iterator, info)
  File "benchmark.py", line 81, in train_epoch
    timeit.timeit(benchmark, number=args.num_warmup_batches)
  File "/rpovelik/installed/miniconda3/envs/ray/lib/python3.8/timeit.py", line 232, in timeit
    return Timer(stmt, setup, timer, globals).timeit(number)
  File "/rpovelik/installed/miniconda3/envs/ray/lib/python3.8/timeit.py", line 176, in timeit
    timing = self.inner(it, self.timer)
  File "&lt;timeit-src&gt;", line 6, in inner
  File "benchmark.py", line 76, in benchmark
    loss.backward()
  File "/rpovelik/installed/miniconda3/envs/ray/lib/python3.8/site-packages/torch/tensor.py", line 195, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/rpovelik/installed/miniconda3/envs/ray/lib/python3.8/site-packages/torch/autograd/__init__.py", line 97, in backward
    Variable._execution_engine.run_backward(
RuntimeError: Expected isFloatingType(grads[i].type().scalarType()) to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch
.)
2020-04-13 17:36:28,319 WARNING torch_trainer.py:413 -- [/opt/conda/conda-bld/pytorch_1579022027171/work/third_party/gloo/gloo/transport/tcp/unbound_buffer.cc:84] Timed out waiting 10000ms f
or recv operation to complete
2020-04-13 17:36:28,721 WARNING worker.py:1072 -- A worker died or was killed while executing task ffffffffffffffff6170691e0100.
2020-04-13 17:36:30,674 WARNING torch_runner.py:102 -- TorchTrainer data_creator return values are no longer wrapped as DataLoaders. Users must return DataLoader(s) in data_creator. This war
ning will be removed in a future version of Ray.
(pid=76785) 2020-04-13 17:36:30,691     WARNING torch_runner.py:102 -- TorchTrainer data_creator return values are no longer wrapped as DataLoaders. Users must return DataLoader(s) in data_c
reator. This warning will be removed in a future version of Ray.
2020-04-13 17:36:31,026 INFO torch_trainer.py:376 -- Retrying training step with 2 workers.
2020-04-13 17:36:33,519 ERROR worker.py:1011 -- Possible unhandled error from worker: ray::DistributedTorchRunner.train_epoch() (pid=76787, ip=10.125.21.189)
  File "python/ray/_raylet.pyx", line 452, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 407, in ray._raylet.execute_task.function_executor
  File "/rpovelik/installed/miniconda3/envs/ray/lib/python3.8/site-packages/ray/util/sgd/torch/distributed_torch_runner.py", line 138, in train_epoch
    return super(DistributedTorchRunner, self).train_epoch(**kwargs)
  File "/rpovelik/installed/miniconda3/envs/ray/lib/python3.8/site-packages/ray/util/sgd/torch/torch_runner.py", line 192, in train_epoch
    train_stats = self.training_operator.train_epoch(iterator, info)
  File "benchmark.py", line 81, in train_epoch
    timeit.timeit(benchmark, number=args.num_warmup_batches)
  File "/rpovelik/installed/miniconda3/envs/ray/lib/python3.8/timeit.py", line 232, in timeit
    return Timer(stmt, setup, timer, globals).timeit(number)
  File "/rpovelik/installed/miniconda3/envs/ray/lib/python3.8/timeit.py", line 176, in timeit
    timing = self.inner(it, self.timer)
  File "&lt;timeit-src&gt;", line 6, in inner
  File "benchmark.py", line 76, in benchmark
    loss.backward()
  File "/rpovelik/installed/miniconda3/envs/ray/lib/python3.8/site-packages/torch/tensor.py", line 195, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/rpovelik/installed/miniconda3/envs/ray/lib/python3.8/site-packages/torch/autograd/__init__.py", line 97, in backward
    Variable._execution_engine.run_backward(
RuntimeError: Expected isFloatingType(grads[i].type().scalarType()) to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch
.)
2020-04-13 17:36:50,438 WARNING torch_trainer.py:413 -- [/opt/conda/conda-bld/pytorch_1579022027171/work/third_party/gloo/gloo/transport/tcp/unbound_buffer.cc:84] Timed out waiting 10000ms f
or recv operation to complete
2020-04-13 17:36:50,805 WARNING worker.py:1072 -- A worker died or was killed while executing task ffffffffffffffff15c675b20100.
2020-04-13 17:36:53,777 WARNING torch_runner.py:102 -- TorchTrainer data_creator return values are no longer wrapped as DataLoaders. Users must return DataLoader(s) in data_creator. This war
ning will be removed in a future version of Ray.
(pid=76759) 2020-04-13 17:36:53,792     WARNING torch_runner.py:102 -- TorchTrainer data_creator return values are no longer wrapped as DataLoaders. Users must return DataLoader(s) in data_c
reator. This warning will be removed in a future version of Ray.
2020-04-13 17:36:54,194 INFO torch_trainer.py:376 -- Retrying training step with 2 workers.
2020-04-13 17:36:56,283 ERROR worker.py:1011 -- Possible unhandled error from worker: ray::DistributedTorchRunner.train_epoch() (pid=76785, ip=10.125.21.189)
  File "python/ray/_raylet.pyx", line 452, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 407, in ray._raylet.execute_task.function_executor
  File "/rpovelik/installed/miniconda3/envs/ray/lib/python3.8/site-packages/ray/util/sgd/torch/distributed_torch_runner.py", line 138, in train_epoch
    return super(DistributedTorchRunner, self).train_epoch(**kwargs)
  File "/rpovelik/installed/miniconda3/envs/ray/lib/python3.8/site-packages/ray/util/sgd/torch/torch_runner.py", line 192, in train_epoch
    train_stats = self.training_operator.train_epoch(iterator, info)
  File "benchmark.py", line 81, in train_epoch
    timeit.timeit(benchmark, number=args.num_warmup_batches)
  File "/rpovelik/installed/miniconda3/envs/ray/lib/python3.8/timeit.py", line 232, in timeit
    return Timer(stmt, setup, timer, globals).timeit(number)
  File "/rpovelik/installed/miniconda3/envs/ray/lib/python3.8/timeit.py", line 176, in timeit
    timing = self.inner(it, self.timer)
  File "&lt;timeit-src&gt;", line 6, in inner
  File "benchmark.py", line 76, in benchmark
    loss.backward()
  File "/rpovelik/installed/miniconda3/envs/ray/lib/python3.8/site-packages/torch/tensor.py", line 195, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/rpovelik/installed/miniconda3/envs/ray/lib/python3.8/site-packages/torch/autograd/__init__.py", line 97, in backward
    Variable._execution_engine.run_backward(
RuntimeError: Expected isFloatingType(grads[i].type().scalarType()) to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch
.)
2020-04-13 17:37:14,169 WARNING torch_trainer.py:413 -- [/opt/conda/conda-bld/pytorch_1579022027171/work/third_party/gloo/gloo/transport/tcp/unbound_buffer.cc:84] Timed out waiting 10000ms f
or recv operation to complete
Traceback (most recent call last):
  File "benchmark.py", line 114, in &lt;module&gt;
    result = trainer.train()
  File "/rpovelik/installed/miniconda3/envs/ray/lib/python3.8/site-packages/ray/util/sgd/torch/torch_trainer.py", line 381, in train
    raise RuntimeError("Training run failed.")
RuntimeError: Training run failed.
Exception ignored in: &lt;function ActorHandle.__del__ at 0x7f9bdfa3b700&gt;
Traceback (most recent call last):
  File "/rpovelik/installed/miniconda3/envs/ray/lib/python3.8/site-packages/ray/actor.py", line 657, in __del__
AttributeError: 'NoneType' object has no attribute 'global_worker'
&lt;/denchmark-code&gt;

Note that with single worker on local cluster everything is OK:
&lt;denchmark-code&gt;python benchmark.py --local --no-cuda
2020-04-13 17:26:36,702 INFO resource_spec.py:204 -- Starting Ray with 120.12 GiB memory available for workers and up to 55.48 GiB for objects. You can adjust these settings with ray.init(me
mory=&lt;bytes&gt;, object_store_memory=&lt;bytes&gt;).
2020-04-13 17:26:37,055 INFO services.py:1146 -- View the Ray dashboard at localhost:8265
Model: resnet50
Batch size: 32
Number of CPUs: 1
2020-04-13 17:26:38,087 WARNING torch_runner.py:102 -- TorchTrainer data_creator return values are no longer wrapped as DataLoaders. Users must return DataLoader(s) in data_creator. This war
ning will be removed in a future version of Ray.
Iter #0: 13.4 img/sec per CPU
Iter #1: 14.2 img/sec per CPU
Iter #2: 15.1 img/sec per CPU
Iter #3: 14.7 img/sec per CPU
Iter #4: 14.3 img/sec per CPU
Iter #5: 14.5 img/sec per CPU
Iter #6: 14.0 img/sec per CPU
Iter #7: 13.9 img/sec per CPU
Iter #8: 13.7 img/sec per CPU
Iter #9: 14.8 img/sec per CPU
Img/sec per CPU: 14.3 +-1.0
Total img/sec on 1 CPU(s): 14.3 +-1.0
&lt;/denchmark-code&gt;

Ray version and other system information (Python version, TensorFlow version, OS):
ray 0.8.4
torch 1.4.0
Ubuntu 18.04
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
python benchmark.py --local --no-cuda
If we cannot run your script, we cannot fix your issue.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='PovelikinRostislav' date='2020-04-14T16:40:19Z'>
		This seems weird. Can you try installing the nightly wheels of the latest master? &lt;denchmark-link:https://ray.readthedocs.io/en/latest/installation.html#latest-snapshots-nightlies&gt;https://ray.readthedocs.io/en/latest/installation.html#latest-snapshots-nightlies&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='PovelikinRostislav' date='2020-05-07T13:12:45Z'>
		Has this issue been resolved? I am running my own program and encounter similar errors:
&lt;denchmark-code&gt;(pid=3769) 2020-05-07 21:07:16,707    WARNING torch_trainer.py:413 -- [/pytorch/third_party/gloo/gloo/transport/tcp/unbound_buffer.cc:72] Timed out waiting 10000ms for recv operation to complete
(pid=4075) WARNING: Logging before InitGoogleLogging() is written to STDERR
(pid=4075) I0507 21:07:18.378806  4075 logging.cc:148] Set ray log level from environment variable RAY_BACKEND_LOG_LEVEL to -1
(pid=3769) 2020-05-07 21:07:19,155    INFO torch_trainer.py:377 -- Retrying training step with 2 workers.
2020-05-07 21:07:20,152 WARNING worker.py:1072 -- A worker died or was killed while executing task ffffffffffffffff2512146c0100.
(pid=3769) E0507 21:07:20.134506  3788 task_manager.cc:254] Task failed: IOError: 14: Socket closed: Type=ACTOR_TASK, Language=PYTHON, function_descriptor={type=PythonFunctionDescriptor, module_name=ray.util.sgd.torch.distributed_torch_runner, class_name=DistributedTorchRunner, function_name=train_epoch, function_hash=}, task_id=93415054fcfa13882512146c0100, job_id=0100, num_args=6, num_returns=2, actor_task_spec={actor_id=2512146c0100, actor_caller_id=ffffffffffffffff45b95b1c0100, actor_counter=1}
&lt;/denchmark-code&gt;

But the program doesn't exit...
Any ideas on this warning: [/pytorch/third_party/gloo/gloo/transport/tcp/unbound_buffer.cc:72] Timed out waiting 10000ms for recv operation to complete
This happens when I run in cluster. When I run multiple workers on a single node, everything is fine.
		</comment>
		<comment id='3' author='PovelikinRostislav' date='2020-05-09T07:45:04Z'>
		Oh, seems my problem gets resolved after making timeout larger when init_process_group... Default only 10s.
		</comment>
		<comment id='4' author='PovelikinRostislav' date='2020-08-23T17:22:00Z'>
		&lt;denchmark-link:https://github.com/hkvision&gt;@hkvision&lt;/denchmark-link&gt;
 can you explain how you got it working, as it still fails for me even after setting timeout on 

python3.8, ubuntu20.04
		</comment>
		<comment id='5' author='PovelikinRostislav' date='2020-08-24T01:47:56Z'>
		&lt;denchmark-link:https://github.com/dokuboyejo&gt;@dokuboyejo&lt;/denchmark-link&gt;

If you are using TorchTrainer, you need to set this timeout through the environment variable: &lt;denchmark-link:https://github.com/ray-project/ray/blob/master/python/ray/util/sgd/torch/constants.py#L9&gt;https://github.com/ray-project/ray/blob/master/python/ray/util/sgd/torch/constants.py#L9&lt;/denchmark-link&gt;
 instead of the timeout in ray.get.
		</comment>
		<comment id='6' author='PovelikinRostislav' date='2020-12-22T02:46:37Z'>
		Hi, I'm a bot from the Ray team :)
To help human contributors to focus on more relevant issues, I will automatically add the stale label to issues that have had no activity for more than 4 months.
If there is no further activity in the 14 days, the issue will be closed!

If you'd like to keep the issue open, just leave any comment, and the stale label will be removed!
If you'd like to get more attention to the issue, please tag one of Ray's contributors.

You can always ask for help on our &lt;denchmark-link:https://discuss.ray.io/&gt;discussion forum&lt;/denchmark-link&gt;
 or &lt;denchmark-link:https://github.com/ray-project/ray#getting-involved&gt;Ray's public slack channel&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='7' author='PovelikinRostislav' date='2021-01-05T03:41:32Z'>
		Hi again! The issue will be closed because there has been no more activity in the 14 days since the last message.
Please feel free to reopen or open a new issue if you'd still like it to be addressed.
Again, you can always ask for help on our &lt;denchmark-link:https://discuss.ray.io&gt;discussion forum&lt;/denchmark-link&gt;
 or &lt;denchmark-link:https://github.com/ray-project/ray#getting-involved&gt;Ray's public slack channel&lt;/denchmark-link&gt;
.
Thanks again for opening the issue!
		</comment>
	</comments>
</bug>