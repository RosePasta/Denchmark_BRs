<bug id='7192' author='xavier-lr' open_date='2020-02-17T08:57:54Z' closed_time='2020-08-19T03:54:43Z'>
	<summary>Cancelling stale RPC</summary>
	<description>
I'm testing ray for data processing in python and I'm facing an error.
My script sets up an Actor that holds a Panda DataFrame, and then launch several processes with Ray; each one updates a different row of the Dataframe through the Actor. Everyone goes fines untill at some point it starts failing
My code could be summarized as follows (with symbolic variables):
&lt;denchmark-code&gt;@ray.remote
class MyActor(object):
    def __init__(self, size):
        self.pd = pd.DataFrame(columns=size, index=size)

    def set_value(self, x, y, v):
        self.pd.loc[x, y] = v

    def get(self):
        return self.pd

num_cpus = psutil.cpu_count()
avail_mem = psutil.virtual_memory()[1]
ray.init(logging_level=logging.WARN, num_cpus=num_cpus, object_store_memory=int(avail_mem*0.10), memory=int(avail_mem*0.85))

@ray.remote
def my_function(actor, my_list):
    for item in my_list:
        i, j, result = do_some_calculations(item)
        actor.set_value.remote(i, j, result)

    return

actor = MyActor.remote(size)

oids = list()
for ix, i in x:
    my_list = get_my_list(i)
    oids.append( my_function.remote(actor, my_list) )

    print(percentage_complete)

    if ix+1 &gt;= num_cpus:
        ray_ready, ray_pending = ray.wait(oids, num_returns=1)
        ray.get(ray_ready)
        oids = ray_pending

for oid in oids:
    ray.get(oid)

my_result = ray.get(actor.get.remote())
&lt;/denchmark-code&gt;

The output is:
&lt;denchmark-code&gt;5%
10%
15%
20%
25%
30%
[2m [36m(pid=11248) [0m E0117 07:40:02.136816 11248 direct_actor_transport.h:307] client skipping requests 0 to 583

[2m [36m(pid=11248) [0m E0117 07:40:02.137846 11248 direct_actor_transport.h:333] Cancelling stale RPC with seqno 0 &lt; 584
[2m [36m(pid=11248) [0m E0117 07:40:02.138236 11248 direct_actor_transport.h:333] Cancelling stale RPC with seqno 1 &lt; 584
[2m [36m(pid=11248) [0m E0117 07:40:02.143378 11248 direct_actor_transport.h:307] client skipping requests 584 to 584
[2m [36m(pid=11248) [0m E0117 07:40:02.143662 11248 direct_actor_transport.h:333] Cancelling stale RPC with seqno 2 &lt; 585
[2m [36m(pid=11248) [0m E0117 07:40:02.147694 11248 direct_actor_transport.h:307] client skipping requests 585 to 585
[2m [36m(pid=11248) [0m E0117 07:40:02.147997 11248 direct_actor_transport.h:333] Cancelling stale RPC with seqno 3 &lt; 586
[2m [36m(pid=11248) [0m E0117 07:40:02.149776 11248 direct_actor_transport.h:307] client skipping requests 586 to 586
[2m [36m(pid=11248) [0m E0117 07:40:02.149844 11248 direct_actor_transport.h:333] Cancelling stale RPC with seqno 4 &lt; 587
[2m [36m(pid=11685) [0m E0117 07:40:02.138691 12092 task_manager.cc:116] Task failed: IOError: 2: client cancelled stale rpc: Type=ACTOR_TASK, Language=PYTHON, function_descriptor=__main__,Propension2,set_value, task_id=d14b6d5ecf29de8902b03c520100, job_id=0100, num_args=6, num_returns=2, actor_task_spec={actor_id=02b03c520100, actor_caller_id=fb438af88d1fdbd9ffffffff0100, actor_counter=0}
[2m [36m(pid=11685) [0m E0117 07:40:02.139008 12092 task_manager.cc:116] Task failed: IOError: 2: client cancelled stale rpc: Type=ACTOR_TASK, Language=PYTHON, function_descriptor=__main__,Propension2,set_value, task_id=56a047b865e52b1102b03c520100, job_id=0100, num_args=6, num_returns=2, actor_task_spec={actor_id=02b03c520100, actor_caller_id=fb438af88d1fdbd9ffffffff0100, actor_counter=1}
[2m [36m(pid=11685) [0m E0117 07:40:02.143932 12092 task_manager.cc:116] Task failed: IOError: 2: client cancelled stale rpc: Type=ACTOR_TASK, Language=PYTHON, function_descriptor=__main__,Propension2,set_value, task_id=238e89cf787abcd102b03c520100, job_id=0100, num_args=6, num_returns=2, actor_task_spec={actor_id=02b03c520100, actor_caller_id=fb438af88d1fdbd9ffffffff0100, actor_counter=2}
[2m [36m(pid=11685) [0m E0117 07:40:02.148279 12092 task_manager.cc:116] Task failed: IOError: 2: client cancelled stale rpc: Type=ACTOR_TASK, Language=PYTHON, function_descriptor=__main__,Propension2,set_value, task_id=af78becd9695b6a302b03c520100, job_id=0100, num_args=6, num_returns=2, actor_task_spec={actor_id=02b03c520100, actor_caller_id=fb438af88d1fdbd9ffffffff0100, actor_counter=3}
[2m [36m(pid=11685) [0m E0117 07:40:02.150089 12092 task_manager.cc:116] Task failed: IOError: 2: client cancelled stale rpc: Type=ACTOR_TASK, Language=PYTHON, function_descriptor=__main__,Propension2,set_value, task_id=f16975ec8220390f02b03c520100, job_id=0100, num_args=6, num_returns=2, actor_task_spec={actor_id=02b03c520100, actor_caller_id=fb438af88d1fdbd9ffffffff0100, actor_counter=4}
[2m [36m(pid=11248) [0m E0117 07:40:08.774125 11248 direct_actor_transport.h:307] client skipping requests 587 to 587
[2m [36m(pid=11248) [0m E0117 07:40:08.774216 11248 direct_actor_transport.h:333] Cancelling stale RPC with seqno 5 &lt; 588
[2m [36m(pid=11248) [0m E0117 07:40:08.775399 11248 direct_actor_transport.h:333] Cancelling stale RPC with seqno 6 &lt; 588
....
&lt;/denchmark-code&gt;

It's running on a AWS EC2 instance with 64GB, looking at memory during execution it doesn't even reaches 50% usage.
	</description>
	<comments>
		<comment id='1' author='xavier-lr' date='2020-03-03T09:16:11Z'>
		Getting flooded with these message as well. did you find any way of solving this &lt;denchmark-link:https://github.com/xavier-lr&gt;@xavier-lr&lt;/denchmark-link&gt;
 ?
		</comment>
		<comment id='2' author='xavier-lr' date='2020-03-03T09:36:47Z'>
		
Getting flooded with these message as well. did you find any way of solving this @xavier-lr ?

No solution found, no answer... just moved out of ray.
		</comment>
		<comment id='3' author='xavier-lr' date='2020-03-06T01:42:47Z'>
		&lt;denchmark-link:https://github.com/xavier-lr&gt;@xavier-lr&lt;/denchmark-link&gt;
 or &lt;denchmark-link:https://github.com/pscholle&gt;@pscholle&lt;/denchmark-link&gt;
 could you provide a reproducible script that causes this issue and I'll look into it? Unfortunately the above isn't too helpful because there are some inputs/functions not defined.
		</comment>
		<comment id='4' author='xavier-lr' date='2020-03-08T06:49:36Z'>
		It doesn't happen all the time.
In my main program I am using an actor as a service which fires off tasks as its parameters are updated. I don't like the fact that I am using named_actors to grab a handle on the actor, but its the best method I know so far.
I am aware that I may be using ray incorrectly here so please point this out!
import ray

from ray.util import named_actors
import time


@ray.remote
def add_to_store(store):
    value = 12
    store.add_parameter.remote('b', [value])
    store.log_time.remote(time.time())


@ray.remote
class Store():
    def __init__(self, default_params: dict):
        self._params = default_params
        self._log_times = list()

    def add_parameter(self, key: str, value: list):
        self._params[key] += value

    def log_time(self, time: float):
        self._log_times.append(time)

    def get_parameter(self, key: str):
        return self._params[key]

    def run_store(self):
        store = named_actors.get_actor("store")
        futs = [add_to_store.remote(store) for _ in range(10)]


def test():

    ray.init()

    params = {
        'b': [11]
    }

    store = Store.remote(params)

    named_actors.register_actor("store", store)

    fut = store.run_store.remote()

    ray.get(fut)

    time.sleep(1)

    object_id = store.get_parameter.remote('b')
    print(len(ray.get(object_id)))


if __name__ == '__main__':
    test()
		</comment>
		<comment id='5' author='xavier-lr' date='2020-03-08T18:38:10Z'>
		Ok, I just retested this script and had to run it like 10 times until this error message showed up. very confused...
All the jobs still complete btw
		</comment>
		<comment id='6' author='xavier-lr' date='2020-05-19T18:21:24Z'>
		&lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
  I can take this over.
		</comment>
		<comment id='7' author='xavier-lr' date='2020-05-26T06:45:31Z'>
		&lt;denchmark-link:https://github.com/pscholle&gt;@pscholle&lt;/denchmark-link&gt;
 Could you test if this is reproducible in the latest master? I couldn't reproduce it in the latest master although I could do when I used ray 0.8.5. You can download the nightly wheel (&lt;denchmark-link:https://docs.ray.io/en/latest/installation.html#latest-snapshots-nightlies&gt;https://docs.ray.io/en/latest/installation.html#latest-snapshots-nightlies&lt;/denchmark-link&gt;
).
		</comment>
		<comment id='8' author='xavier-lr' date='2020-07-26T06:52:33Z'>
		Got same error message in different set of code.. Used ray Dev.0.9..
		</comment>
		<comment id='9' author='xavier-lr' date='2020-07-26T07:13:10Z'>
		&lt;denchmark-link:https://github.com/rightx2&gt;@rightx2&lt;/denchmark-link&gt;
 can you create an issue with reproducible script?
		</comment>
		<comment id='10' author='xavier-lr' date='2020-07-26T09:38:03Z'>
		&lt;denchmark-link:https://github.com/rkooo567&gt;@rkooo567&lt;/denchmark-link&gt;
 Mine is so complicated application so need to simplify it first. Then, I will try to reproduce it
		</comment>
		<comment id='11' author='xavier-lr' date='2020-08-19T03:54:43Z'>
		Close it since there's no reproduction (and last time when I checked it, it was resolved). Feel free to reopen if there's a reproduction script :)
		</comment>
		<comment id='12' author='xavier-lr' date='2020-09-02T00:05:36Z'>
		I just hit this when running the microbenchmarks on latest (&lt;denchmark-link:https://github.com/ray-project/ray/commit/3b10b67a15bb07cf3f918126dd3c5a7047b467df&gt;3b10b67&lt;/denchmark-link&gt;
) master.
&lt;denchmark-code&gt;1:n actor calls async per second 7072.62 +- 94.34
(pid=1502) E0901 23:50:48.679822  1502  1847 task_manager.cc:323] Task failed: IOError: 2: client cancelled stale rpc: Type=ACTOR_TASK, Language=PYTHON, Resources: {}, function_descriptor={type=PythonFunctionDescriptor, module_name=ray.ray_perf, class_name=Actor, function_name=small_value, function_hash=}, task_id=fed130b9491e78a882d56c4c01000000, job_id=01000000, num_args=0, num_returns=2, actor_task_spec={actor_id=82d56c4c01000000, actor_caller_id=47c6b4b8279ec9beffffffff01000000, ac
tor_counter=29}
(pid=10006) E0901 23:50:48.632827 10006 10006 direct_actor_transport.h:395] Cancelling stale RPC with seqno 28 &lt; 29
(pid=10028) E0901 23:50:48.641721 10028 10028 direct_actor_transport.h:395] Cancelling stale RPC with seqno 28 &lt; 29
(pid=1502) E0901 23:50:48.703166  1502  1847 task_manager.cc:323] Task failed: IOError: 2: client cancelled stale rpc: Type=ACTOR_TASK, Language=PYTHON, Resources: {}, function_descriptor={type=PythonFunctionDescriptor, module_name=ray.ray_perf, class_name=Actor, function_name=small_value, function_hash=}, task_id=2fc35ed34c5c73aad3d2ac7a01000000, job_id=01000000, num_args=0, num_returns=2, actor_task_spec={actor_id=d3d2ac7a01000000, actor_caller_id=47c6b4b8279ec9beffffffff01000000, ac
tor_counter=29}
Traceback (most recent call last):
  File "/home/clark/.local/bin/ray", line 11, in &lt;module&gt;
    load_entry_point('ray', 'console_scripts', 'ray')()
  File "/mnt/data/workspace/ray/python/ray/scripts/scripts.py", line 1599, in main
    return cli()
  File "/usr/lib/python3/dist-packages/click/core.py", line 764, in __call__
    return self.main(*args, **kwargs)
  File "/usr/lib/python3/dist-packages/click/core.py", line 717, in main
    rv = self.invoke(ctx)
  File "/usr/lib/python3/dist-packages/click/core.py", line 1137, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/usr/lib/python3/dist-packages/click/core.py", line 956, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/usr/lib/python3/dist-packages/click/core.py", line 555, in invoke
    return callback(*args, **kwargs)
  File "/mnt/data/workspace/ray/python/ray/scripts/scripts.py", line 1430, in microbenchmark
    main()
  File "/mnt/data/workspace/ray/python/ray/ray_perf.py", line 224, in main
    timeit("n:n actor calls async", actor_multi2, m * n)
  File "/mnt/data/workspace/ray/python/ray/ray_perf.py", line 80, in timeit
    fn()
  File "/mnt/data/workspace/ray/python/ray/ray_perf.py", line 222, in actor_multi2
    ray.get([work.remote(a) for _ in range(m)])
  File "/mnt/data/workspace/ray/python/ray/worker.py", line 1423, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError: ray::ray.ray_perf.work() (pid=1502, ip=10.128.0.88)
  File "python/ray/_raylet.pyx", line 480, in ray._raylet.execute_task
    with ray.worker._changeproctitle(title, next_title):
  File "python/ray/_raylet.pyx", line 481, in ray._raylet.execute_task
    outputs = function_executor(*args, **kwargs)
  File "/mnt/data/workspace/ray/python/ray/ray_perf.py", line 219, in work
    ray.get([actors[i % n_cpu].small_value.remote() for i in range(n)])
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
&lt;/denchmark-code&gt;

Happens in &lt;denchmark-link:https://github.com/ray-project/ray/blob/3b10b67a15bb07cf3f918126dd3c5a7047b467df/python/ray/ray_perf.py#L214-L224&gt;this test&lt;/denchmark-link&gt;
. I ran the microbenchmarks a few more times but haven't encountered this again.
		</comment>
	</comments>
</bug>