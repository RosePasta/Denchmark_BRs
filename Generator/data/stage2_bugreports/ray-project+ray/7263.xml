<bug id='7263' author='devin-petersohn' open_date='2020-02-21T20:58:25Z' closed_time='2020-02-25T07:22:50Z'>
	<summary>Tasks can exceed gRPC max message size (in modin)</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

Multiple Modin users have reported Raylet failures that present themselves in various ways.
&lt;denchmark-h:h3&gt;Reproduction&lt;/denchmark-h&gt;

Reproduction requires Modin and is only reproducible in certain environments. Scripts range from errors when chaining multiple operations together to setting environment variables. I will detail a few cases below.
Reproduction script 1 (most recent user) - &lt;denchmark-link:https://github.com/modin-project/modin/issues/1095&gt;modin-project/modin#1095&lt;/denchmark-link&gt;
:
import string
import numpy as np
import modin.pandas as mpd

chars = [i+j+k for i in string.ascii_letters for j in string.ascii_letters for k in string.ascii_letters]

mdf_chars = mpd.DataFrame({'Random_Char': chars, "Random_Int": np.random.randint(0, len(chars)), "Random_Float": np.random.random(len(chars))})

mdf_chars.sample(100000).reset_index()
mdf_chars.sample(10000).describe()
Error logs reported by user:
&lt;denchmark-code&gt;UserWarning: Distributing &lt;class 'dict'&gt; object. This may take some time.
F0220 22:58:56.524958 230891520 direct_task_transport.cc:154] Lost connection with local raylet. Error: IOError: 8: Received message larger than max (10899141 vs. 4194304)
*** Check failure stack trace: ***
    @        0x11d15fce2  google::LogMessage::~LogMessage()
    @        0x11ce44b35  ray::RayLog::~RayLog()
    @        0x11cd43564  ray::CoreWorkerDirectTaskSubmitter::RetryLeaseRequest()
    @        0x11cd48e09  std::__1::__function::__func&lt;&gt;::operator()()
    @        0x11cd6153f  ray::rpc::ClientCallImpl&lt;&gt;::OnReplyReceived()
    @        0x11cd02e85  _ZN5boost4asio19asio_handler_invokeIZN3ray3rpc17ClientCallManager29PollEventsFromCompletionQueueEiEUlvE_EEvRT_z
    @        0x11cd02dd4  _ZN5boost4asio6detail18completion_handlerIZN3ray3rpc17ClientCallManager29PollEventsFromCompletionQueueEiEUlvE_E11do_completeEPvPNS1_19scheduler_operationERKNS_6system10error_codeEm
    @        0x11cd07853  boost::asio::detail::scheduler::do_run_one()
    @        0x11cd07392  boost::asio::detail::scheduler::run()
    @        0x11ccf5bf9  ray::CoreWorker::RunIOService()
    @        0x11cd0e63e  std::__1::__thread_proxy&lt;&gt;()
    @     0x7fff7f4992eb  _pthread_body
    @     0x7fff7f49c249  _pthread_start
    @     0x7fff7f49840d  thread_start
Abort trap: 6
&lt;/denchmark-code&gt;

Reproduction script 2 - &lt;denchmark-link:https://github.com/modin-project/modin/issues/1068&gt;modin-project/modin#1068&lt;/denchmark-link&gt;

import ray
ray.init(huge_pages=True, plasma_directory="/mnt/hugepages")
import modin.pandas as pd

dtypes = {
    'object_id': 'int32',
    'mjd': 'float32',
    'passband': 'int32',
    'flux': 'float32',
    'flux_err': 'float32',
    'detected': 'int32'
}

PATH='/localdisk/benchmark_datasets/plasticc'
GPU_MEMORY = 16
TEST_ROWS = 453653104
OVERHEAD = 1.2
SKIP_ROWS = int((1 - GPU_MEMORY/(32.0*OVERHEAD))*TEST_ROWS)

test = pd.read_csv('%s/test_set_100000.csv'%PATH,
        # skiprows=range(1, 1+SKIP_ROWS),
        dtype=dtypes)
Output logs:
&lt;denchmark-code&gt;2020-02-07 11:16:24,545 INFO resource_spec.py:216 -- Starting Ray with 95.17 GiB memory available for workers and up to 18.63 GiB for objects. You can adjust these settings with ray.init(memory=&lt;bytes&gt;, object_store_memory=&lt;bytes&gt;).
2020-02-07 11:16:24,801 WARNING services.py:1365 -- WARNING: object_store_memory is not verified when plasma_directory is set.
F0207 11:16:26.652703 51019 direct_task_transport.cc:154] Lost connection with local raylet. Error: IOError: 14: Socket closed
*** Check failure stack trace: ***
    @     0x7f5ff966f98d  google::LogMessage::Fail()
    @     0x7f5ff9670dfc  google::LogMessage::SendToLog()
    @     0x7f5ff966f669  google::LogMessage::Flush()
    @     0x7f5ff966f881  google::LogMessage::~LogMessage()
    @     0x7f5ff94440e9  ray::RayLog::~RayLog()
    @     0x7f5ff93650c9  ray::CoreWorkerDirectTaskSubmitter::RetryLeaseRequest()
    @     0x7f5ff9367863  _ZNSt17_Function_handlerIFvRKN3ray6StatusERKNS0_3rpc16WorkerLeaseReplyEEZNS0_29CoreWorkerDirectTaskSubmitter24RequestNewWorkerIfNeededERKSt4pairIiSt6vectorINS0_8ObjectIDESaISC_EEEPKNS4_7AddressEEUlS3_S7_E_E9_M_invokeERKSt9_Any_dataS3_S7_
    @     0x7f5ff939ec3f  ray::rpc::ClientCallImpl&lt;&gt;::OnReplyReceived()
    @     0x7f5ff93419d3  _ZN5boost4asio6detail18completion_handlerIZN3ray3rpc17ClientCallManager29PollEventsFromCompletionQueueEiEUlvE_E11do_completeEPvPNS1_19scheduler_operationERKNS_6system10error_codeEm
    @     0x7f5ff93403a5  boost::asio::detail::scheduler::run()
    @     0x7f5ff93444a3  ray::CoreWorker::RunIOService()
    @     0x7f5ff905f302  (unknown)
    @     0x7f5ffa325669  start_thread
    @     0x7f5ffa461323  clone
Aborted (core dumped)
&lt;/denchmark-code&gt;

Other details can be found in the linked issues. This is happening on a variety of operating systems and versions.
	</description>
	<comments>
		<comment id='1' author='devin-petersohn' date='2020-02-22T01:40:53Z'>
		&lt;denchmark-link:https://github.com/edoakes&gt;@edoakes&lt;/denchmark-link&gt;
 it looks like the task is exceeding the max gRPC message size 
		</comment>
		<comment id='2' author='devin-petersohn' date='2020-02-25T07:23:28Z'>
		It looks like some of the errors might not be related to this one, so please open a new issue if they persist after 7269.
		</comment>
	</comments>
</bug>