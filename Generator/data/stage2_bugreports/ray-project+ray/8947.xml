<bug id='8947' author='maxpumperla' open_date='2020-06-15T11:04:14Z' closed_time='2020-06-16T13:33:58Z'>
	<summary>Issue in Ray RLlib when using action masking in MultiAgentEnv</summary>
	<description>
I have a running example of an action masking agent for a  following your &lt;denchmark-link:https://docs.ray.io/en/master/rllib-models.html#variable-length-parametric-action-spaces&gt;rough sketch in the docs&lt;/denchmark-link&gt;
, works fine (using MacOS, Python 3.7, latest available Ray).
As suggested, my observations space looks like this:
&lt;denchmark-code&gt;            self.observation_space = spaces.Dict({
                "action_mask": spaces.Box(0, 1, shape=(self.num_actions,)),
                "observations": observations
            })
&lt;/denchmark-code&gt;

which makes my observations a dict. All good so far. But when I change to an equivalent MultiAgentEnv for this task, this approach fails.
I think I found the reason for this behaviour, too. In the sampler you make an assumption about the raw data coming in:
&lt;denchmark-link:https://github.com/ray-project/ray/blob/master/rllib/evaluation/sampler.py#L577-L578&gt;https://github.com/ray-project/ray/blob/master/rllib/evaluation/sampler.py#L577-L578&lt;/denchmark-link&gt;

Namely, you assume that each entry corresponds to an environment, so you iterate through them. But in my case the incoming data comes from a single env and happens to belong to a single observation space. In any case, this eventually leads to an error being thrown here:
&lt;denchmark-link:https://github.com/ray-project/ray/blob/master/rllib/models/preprocessors.py#L59-L62&gt;https://github.com/ray-project/ray/blob/master/rllib/models/preprocessors.py#L59-L62&lt;/denchmark-link&gt;

since neither the action_mask nor the observations alone belong to the observation space.
It's fairly difficult for me to make this a standalone, reproducible example right now, as it contains a lot of confidential info. But maybe I can follow up with an extended, multi-env cartpole example for this.
In any case, I'd be interested in:

a quick fix / hack, because I'd really like to use this "feature" of rllib
ideas in how to properly handle this in the future. Especially hints on how we could reduce the number of assumptions, and how to produce informative error messages for the users would be appreciated.

I'm willing to contribute as well.
	</description>
	<comments>
		<comment id='1' author='maxpumperla' date='2020-06-16T00:59:40Z'>
		How are you defining the multiagent config? I think this should work (based on the test_multi_agent_complex_spaces unit test).
The multiagent config should be something like
&lt;denchmark-code&gt;                "multiagent": {
                    "policies": {
                        "p1": (
                            Policy, COMPLEX_SPACE1, act_space,
                            {"model": {"custom_model": "m1"}}),
                        "p2": (
                            Policy, COMPLEX_SPACE2, act_space,
                            {"model": {"custom_model": "m2"}}),
                    },
...
                },
&lt;/denchmark-code&gt;

then the env should return
&lt;denchmark-code&gt;{
   &lt;agent_id&gt;: &lt;complex obs&gt;,
   &lt;agent_id&gt;: &lt;complex obs&gt;,
   ...
}
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='maxpumperla' date='2020-06-16T07:08:32Z'>
		&lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
 if I don't use action masking in the multi agent env, the example runs seamlessly. so unless you're saying I should change my config for action masking, I think my setting should be fine.
		</comment>
		<comment id='3' author='maxpumperla' date='2020-06-16T07:40:47Z'>
		&lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
 Just to confirm, if you run the &lt;denchmark-link:https://github.com/ray-project/ray/blob/master/rllib/examples/parametric_actions_cartpole.py&gt;parametric cartpole example&lt;/denchmark-link&gt;
 from master, but change the definition of  to extend a , i.e. like so:
&lt;denchmark-code&gt;from ray import rllib

class ParametricActionsCartPole(rllib.env.MultiAgentEnv):
&lt;/denchmark-code&gt;

you'll get a reproducible example of my error, namely the following:
&lt;denchmark-code&gt;  File "/Users/maxpumperla/code/pathmind/factory-puzzle/venv/lib/python3.7/site-packages/ray/rllib/models/preprocessors.py", line 62, in check_shape
    self._obs_space, observation)
ValueError: ('Observation outside expected value range', Dict(action_mask:Box(10,), avail_actions:Box(10, 2), cart:Box(4,)), array([0., 0., 0., 1., 0., 1., 0., 0., 0., 0.]))
&lt;/denchmark-code&gt;

in which the system expects a dict observation with three items, but only gets the first entry, just like I reported. And obviously that examples also runs when not applying masking.
		</comment>
		<comment id='4' author='maxpumperla' date='2020-06-16T08:03:51Z'>
		Hey I think this is the expected behaviour: fundamentally, multi agent envs
return dicts of obses keyed by agent id, and requires some changes to get
your obs to work. I would start with a working multi agent env, then change
the obs space to a complex type with e.g., action mask, etc.

Eric
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Tue, Jun 16, 2020 at 12:41 AM Max Pumperla ***@***.***&gt; wrote:
 @ericl &lt;https://github.com/ericl&gt; Just to confirm, if you run the parametric
 cartpole example
 &lt;https://github.com/ray-project/ray/blob/master/rllib/examples/parametric_actions_cartpole.py&gt;
 from master, but change the definition of ParametricActionsCartPole to
 extend a MultiAgentEnv, i.e. like so:

 from ray import rllib

 class ParametricActionsCartPole(rllib.env.MultiAgentEnv):

 you'll get a reproducible example of my error, namely the following:

   File "/Users/maxpumperla/code/pathmind/factory-puzzle/venv/lib/python3.7/site-packages/ray/rllib/models/preprocessors.py", line 62, in check_shape
     self._obs_space, observation)
 ValueError: ('Observation outside expected value range', Dict(action_mask:Box(10,), avail_actions:Box(10, 2), cart:Box(4,)), array([0., 0., 0., 1., 0., 1., 0., 0., 0., 0.]))

 in which the system expects a dict observation with three items, but only
 gets the first entry, just like I reported. And obviously that examples
 also runs when not applying masking.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#8947 (comment)&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/AAADUSSNP2GYFHEBDCLV73DRW4OY5ANCNFSM4N6CCMXQ&gt;
 .



		</comment>
		<comment id='5' author='maxpumperla' date='2020-06-16T08:18:52Z'>
		&lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
 what you propose is exactly what I did already!
I'd appreciate if you guys could at least have a look at this issue before making this my problem. I'm here to figure out what "some changes" are.
Honestly, I'm not sure how this is supposed to be expected behaviour. the error does not happen on the "obs level", but deeply in sampler.py and preprocessor.py, where your system makes an invalid assumption about the shape of the observation space.
&lt;denchmark-code&gt;    # For each environment.
    for env_id, agent_obs in unfiltered_obs.items():
&lt;/denchmark-code&gt;

the reason you need this comment in sampler.py and many other places is because there's no type or basic validity checking, just all plain dicts and assumptions. Fundamentally, it doesn't fail early enough, and the error message is not helpful. It's pretty much impossible to infer what needs to be changed to make the example work. A hint would be nice. How would you build a multi agent env masking example?
Again, the config is fine. This is an example from your repo, following instructions from your docs.
		</comment>
		<comment id='6' author='maxpumperla' date='2020-06-16T08:34:24Z'>
		I understand the frustration with bad error messages, but I really think this is a misconfiguration. I took a stab at adapting the parametric cartpole env to run as a multi agent env with one agent, which works for me:
&lt;denchmark-code&gt;import argparse
import gym
from gym.spaces import Box, Dict, Discrete
import numpy as np
import random

import ray
from ray import tune
from ray.rllib.examples.models.parametric_actions_model import \
    ParametricActionsModel, TorchParametricActionsModel
from ray.rllib.env import MultiAgentEnv
from ray.rllib.models import ModelCatalog
from ray.rllib.utils.test_utils import check_learning_achieved
from ray.tune.registry import register_env

max_avail_actions = 10

AGENT_OBS_SPACE = Dict({
    "action_mask": Box(0, 1, shape=(max_avail_actions, )),
    "avail_actions": Box(-10, 10, shape=(max_avail_actions, 2)),
    "cart": gym.make("CartPole-v0").observation_space,
})

AGENT_ACTION_SPACE = Discrete(max_avail_actions)

# hacked up as a single agent multiagentenv
class ParametricActionsCartPole(MultiAgentEnv):

    def __init__(self, _):
        # Use simple random 2-unit action embeddings for [LEFT, RIGHT]
        self.left_action_embed = np.random.randn(2)
        self.right_action_embed = np.random.randn(2)
        self.wrapped = gym.make("CartPole-v0")

    def update_avail_actions(self):
        self.action_assignments = np.array([[0., 0.]] * AGENT_ACTION_SPACE.n)
        self.action_mask = np.array([0.] * AGENT_ACTION_SPACE.n)
        self.left_idx, self.right_idx = random.sample(
            range(AGENT_ACTION_SPACE.n), 2)
        self.action_assignments[self.left_idx] = self.left_action_embed
        self.action_assignments[self.right_idx] = self.right_action_embed
        self.action_mask[self.left_idx] = 1
        self.action_mask[self.right_idx] = 1

    def reset(self):
        self.update_avail_actions()
        return {"a": {
            "action_mask": self.action_mask,
            "avail_actions": self.action_assignments,
            "cart": self.wrapped.reset(),
        }}

    def step(self, action):
        action = action["a"]
        if action == self.left_idx:
            actual_action = 0
        elif action == self.right_idx:
            actual_action = 1
        else:
            raise ValueError(
                "Chosen action was not one of the non-zero action embeddings",
                action, self.action_assignments, self.action_mask,
                self.left_idx, self.right_idx)
        orig_obs, rew, done, info = self.wrapped.step(actual_action)
        self.update_avail_actions()
        obs = {
            "action_mask": self.action_mask,
            "avail_actions": self.action_assignments,
            "cart": orig_obs,
        }
        return {"a": obs}, {"a": rew}, {"__all__": done}, {}


parser = argparse.ArgumentParser()
parser.add_argument("--run", type=str, default="PPO")
parser.add_argument("--torch", action="store_true")
parser.add_argument("--as-test", action="store_true")
parser.add_argument("--stop-iters", type=int, default=200)
parser.add_argument("--stop-reward", type=float, default=150.0)
parser.add_argument("--stop-timesteps", type=int, default=100000)

if __name__ == "__main__":
    args = parser.parse_args()
    ray.init()

    register_env("pa_cartpole", lambda _: ParametricActionsCartPole(10))
    ModelCatalog.register_custom_model(
        "pa_model", TorchParametricActionsModel
        if args.torch else ParametricActionsModel)

    config = {
        "env": "pa_cartpole",
        "model": {
            "custom_model": "pa_model",
        },
        "multiagent": {
            "policies": {
                "p1": (None, AGENT_OBS_SPACE, AGENT_ACTION_SPACE, {}),
            },
            "policy_mapping_fn": lambda agent_id: "p1",
        },
        "num_workers": 0,
        "framework": "torch" if args.torch else "tf",
    }

    stop = {
        "training_iteration": args.stop_iters,
        "timesteps_total": args.stop_timesteps,
        "episode_reward_mean": args.stop_reward,
    }

    results = tune.run(args.run, stop=stop, config=config)

    if args.as_test:
        check_learning_achieved(results, args.stop_reward)

    ray.shutdown()
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='maxpumperla' date='2020-06-16T09:04:12Z'>
		&lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
 ok, got it. so you need to manually wrap your observation using a dummy key, here called . Thank you!
I didn't mean to complain, I'm genuinely interested in helping improve the system. The config is central to everything you do internally, but it's also as leaky an abstraction as possible. Essentially,  users adapting examples slightly need to have an understanding of the complete internal workings os the system, how everything comes together. Your hack / suggestion is far from obvious (I hope you agree) for someone learning your API, especially given that you don't need to modify it without masking. That's why I insisted this is not expected behaviour, it's simply not clear why you would have to dummy-wrap your obs in an example that worked before, just because you added masking.
Natural questions to ask would be:

how can we document this properly?
could we potentially do this wrapping you're proposing internally, i.e. without the user having to specify it explicitly?

		</comment>
		<comment id='8' author='maxpumperla' date='2020-06-16T09:11:55Z'>
		I guess what I'm saying is that everything and anything in ray comes down to masterfully tweaking a config object, but it's fundamentally the job of this project to provide a stable, useful API to users that works without them having to rely on knowledge about the codebase. I spend way more time on your GitHub than on your docs (and I know lots of other people who do the same), and this is usually not a good sign.
Anyway, if you have a suggestion for how to fix this issue properly, I'm happy to send a PR.
		</comment>
		<comment id='9' author='maxpumperla' date='2020-06-16T13:34:54Z'>
		ok, I dug a little deeper into this. thanks for your help &lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='10' author='maxpumperla' date='2020-06-16T16:59:24Z'>
		Where would be a good place to add this? Depending on what doc you read it might not have been clear. Always happy to get doc improvement prs. Fwiw it's at least in the doc string for multiagent env: &lt;denchmark-link:https://github.com/ray-project/ray/blob/master/rllib/env/multi_agent_env.py#L56&gt;https://github.com/ray-project/ray/blob/master/rllib/env/multi_agent_env.py#L56&lt;/denchmark-link&gt;

Btw it's not really a hack to wrap in a, fundamentally multiagent envs are about returning a dict mapping from agent IDs (here 'a') to their observations.
		</comment>
		<comment id='11' author='maxpumperla' date='2020-06-16T17:11:43Z'>
		&lt;denchmark-link:https://github.com/ericl&gt;@ericl&lt;/denchmark-link&gt;
 yeah I realized my problem was a little different than the cartpole one, I already had a proper multi-agent setup. I initially thought you were proposing to wrap everything in an extra "a" to make it work, hence I said hack. That was simply a misunderstanding as it turns out. I was applying the masking step for multi-agent envs incorrectly, as I figured out when applying your code snippet to my codebase.
Let me think about ways to help others with this. I should have a few PRs coming up anyway, so that might just fit right in.
		</comment>
		<comment id='12' author='maxpumperla' date='2020-06-16T17:13:09Z'>
		Sounds great! Btw, we're going to add type hints soon, which should help
out a bit here.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Tue, Jun 16, 2020, 10:11 AM Max Pumperla ***@***.***&gt; wrote:
 @ericl &lt;https://github.com/ericl&gt; yeah I realized my problem was a little
 different than the cartpole one, I already had a proper multi-agent setup.
 I initially thought you were proposing to wrap everything in an extra "a"
 to make it work, hence I said hack. That was simply a misunderstanding as
 it turns out. I was applying the masking step for multi-agent envs
 incorrectly, as I figured out when applying your code snippet to my
 codebase.

 Let me think about ways to help others with this. I should have a few PRs
 coming up anyway, so that might just fit right in.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#8947 (comment)&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/AAADUSTMNDX7IOY2Z5EOGOLRW6RV5ANCNFSM4N6CCMXQ&gt;
 .



		</comment>
		<comment id='13' author='maxpumperla' date='2020-06-16T17:18:03Z'>
		that's great! do you discuss stuff like this (type hinting etc.) in your slack? Maybe we can discuss here and there.
I'm likely presenting at the next Ray Summit Connect in July, so I guess I'll see you there (in zoom or so).
In any case, thanks again for the quick turnaround!
		</comment>
		<comment id='14' author='maxpumperla' date='2020-06-16T20:38:33Z'>
		Yea I think slack would be a good place to discuss improvements!

See you at ray summit.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Tue, Jun 16, 2020, 10:18 AM Max Pumperla ***@***.***&gt; wrote:
 that's great! do you discuss stuff like this (type hinting etc.) in your
 slack? Maybe we can discuss here and there.

 I'm likely presenting at the next Ray Summit Connect in July, so I guess
 I'll see you there (in zoom or so).

 In any case, thanks again for the quick turnaround!

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#8947 (comment)&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/AAADUSUGOJPTLF5GSNSTOODRW6SNTANCNFSM4N6CCMXQ&gt;
 .



		</comment>
	</comments>
</bug>