<bug id='11668' author='ljendele' open_date='2020-10-28T11:19:17Z' closed_time='2020-12-14T18:50:19Z'>
	<summary>[rllib] DDPG ApeX fails with PyTorch and GPU</summary>
	<description>
This is not a contribution.
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

I tried running this example (&lt;denchmark-link:https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ddpg/mountaincarcontinuous-apex-ddpg.yaml&gt;https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ddpg/mountaincarcontinuous-apex-ddpg.yaml&lt;/denchmark-link&gt;
) with PyTorch and GPU (Titan Xp).
However, it always fails due to this error:
&lt;denchmark-code&gt;Failure # 1 (occurred at 2020-10-28_10-12-48)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/ray/tune/trial_runner.py", line 515, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File "/usr/local/lib/python3.6/dist-packages/ray/tune/ray_trial_executor.py", line 488, in fetch_result
    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)
  File "/usr/local/lib/python3.6/dist-packages/ray/worker.py", line 1428, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(RuntimeError): ray::APEX_DDPG.train() (pid=39535, ip=XX.XX.XX.XX)
  File "python/ray/_raylet.pyx", line 484, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 438, in ray._raylet.execute_task.function_executor
  File "/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/trainer.py", line 519, in train
    raise e
  File "/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/trainer.py", line 505, in train
    result = Trainable.train(self)
  File "/usr/local/lib/python3.6/dist-packages/ray/tune/trainable.py", line 336, in train
    result = self.step()
  File "/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/trainer_template.py", line 134, in step
    res = next(self.train_exec_impl)
  File "/usr/local/lib/python3.6/dist-packages/ray/util/iter.py", line 756, in __next__
    return next(self.built_iterator)
  File "/usr/local/lib/python3.6/dist-packages/ray/util/iter.py", line 783, in apply_foreach
    for item in it:
  File "/usr/local/lib/python3.6/dist-packages/ray/util/iter.py", line 783, in apply_foreach
    for item in it:
  File "/usr/local/lib/python3.6/dist-packages/ray/util/iter.py", line 843, in apply_filter
    for item in it:
  File "/usr/local/lib/python3.6/dist-packages/ray/util/iter.py", line 843, in apply_filter
    for item in it:
  File "/usr/local/lib/python3.6/dist-packages/ray/util/iter.py", line 783, in apply_foreach
    for item in it:
  File "/usr/local/lib/python3.6/dist-packages/ray/util/iter.py", line 843, in apply_filter
    for item in it:
  File "/usr/local/lib/python3.6/dist-packages/ray/util/iter.py", line 1075, in build_union
    item = next(it)
  File "/usr/local/lib/python3.6/dist-packages/ray/util/iter.py", line 756, in __next__
    return next(self.built_iterator)
  File "/usr/local/lib/python3.6/dist-packages/ray/util/iter.py", line 783, in apply_foreach
    for item in it:
  File "/usr/local/lib/python3.6/dist-packages/ray/util/iter.py", line 783, in apply_foreach
    for item in it:
  File "/usr/local/lib/python3.6/dist-packages/ray/util/iter.py", line 783, in apply_foreach
    for item in it:
  File "/usr/local/lib/python3.6/dist-packages/ray/rllib/execution/concurrency_ops.py", line 132, in base_iterator
    raise RuntimeError("Error raised reading from queue")
RuntimeError: Error raised reading from queue
&lt;/denchmark-code&gt;

Ray version and other system information (Python version, TensorFlow version, OS):
Ray version: 1.0.0
Python version: 3.6.8
Tensorflow version: 1.15.0 (not running with tf though)
Pytorch version: 1..6.0+cu92
OS: Ubuntu 18.04 (in docker container)
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
To reproduce, run rllib train -f mountaincarcontinuous-apex-ddpg-pytorch.yaml
Here is the content of the file:
&lt;denchmark-code&gt;# This can be expected to reach 90 reward within ~1.5-2.5m timesteps / ~150-250 seconds on a K40 GPU
mountaincarcontinuous-apex-ddpg-pytorch:
    env: MountainCarContinuous-v0
    run: APEX_DDPG
    stop:
        episode_reward_mean: 90
    config:
        # Works for both torch and tf.
        framework: torch  # &lt;— This line is changed
        clip_rewards: False
        num_workers: 16
        num_gpus: 1  # &lt;— This line is changed
        exploration_config:
            ou_base_scale: 1.0
        n_step: 3
        target_network_update_freq: 50000
        tau: 1.0
        evaluation_interval: 5
        evaluation_num_episodes: 10
&lt;/denchmark-code&gt;


 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

Thank a lot for your help,
Lukas
	</description>
	<comments>
		<comment id='1' author='ljendele' date='2020-10-28T14:18:03Z'>
		I also face this issue when running the "custom_rnn_model" example (&lt;denchmark-link:https://github.com/ray-project/ray/blob/master/rllib/examples/custom_rnn_model.py&gt;https://github.com/ray-project/ray/blob/master/rllib/examples/custom_rnn_model.py&lt;/denchmark-link&gt;
), with APPO instead of PPO, both with Pytorch and Tensorflow. This issue has also been mentioned in &lt;denchmark-link:https://github.com/ray-project/ray/issues/9436&gt;#9436&lt;/denchmark-link&gt;
, it would be really helpful if someone found a solution.
&lt;denchmark-code&gt;"""Example of using a custom RNN keras model."""

import argparse

import ray
from ray import tune
from ray.tune.registry import register_env
from ray.rllib.examples.env.repeat_after_me_env import RepeatAfterMeEnv
from ray.rllib.examples.env.repeat_initial_obs_env import RepeatInitialObsEnv
from ray.rllib.examples.models.rnn_model import RNNModel, TorchRNNModel
from ray.rllib.models import ModelCatalog
from ray.rllib.utils.test_utils import check_learning_achieved

parser = argparse.ArgumentParser()
parser.add_argument("--run", type=str, default="APPO")
parser.add_argument("--env", type=str, default="RepeatAfterMeEnv")
parser.add_argument("--num-cpus", type=int, default=0)
parser.add_argument("--as-test", action="store_true")
parser.add_argument("--torch", default=True, action="store_true")
parser.add_argument("--stop-reward", type=float, default=90)
parser.add_argument("--stop-iters", type=int, default=100)
parser.add_argument("--stop-timesteps", type=int, default=100000)

if __name__ == "__main__":
    args = parser.parse_args()

    ray.init(num_cpus=args.num_cpus or None)

    ModelCatalog.register_custom_model(
        "rnn", TorchRNNModel if args.torch else RNNModel)
    register_env("RepeatAfterMeEnv", lambda c: RepeatAfterMeEnv(c))
    register_env("RepeatInitialObsEnv", lambda _: RepeatInitialObsEnv())

    config = {
        "env": args.env,
        "env_config": {
            "repeat_delay": 2,
        },
        "gamma": 0.9,
        "num_workers": 0,
        "num_envs_per_worker": 20,
        "entropy_coeff": 0.001,
        "num_sgd_iter": 5,
        "vf_loss_coeff": 1e-5,
        "model": {
            "custom_model": "rnn",
            "max_seq_len": 20,
        },
        "framework": "torch" if args.torch else "tf",
    }

    stop = {
        "training_iteration": args.stop_iters,
        "timesteps_total": args.stop_timesteps,
        "episode_reward_mean": args.stop_reward,
    }

    results = tune.run(args.run, config=config, stop=stop)

    if args.as_test:
        check_learning_achieved(results, args.stop_reward)
    ray.shutdown()
&lt;/denchmark-code&gt;

Ray version and other system information (Python version, TensorFlow version, OS):
Ray version: 0.8.6
Python version: 3.7.9
Tensorflow version: 1.15.0
Pytorch version: 1.4.0+cpu
OS: Windows 10 Entreprise, version 1809
		</comment>
		<comment id='2' author='ljendele' date='2020-11-02T08:42:44Z'>
		Could you try the latest master?
It seems to be fixed now, I can run the above example fine on my GPU machine using the exact same yaml file as you and being on the latest master branch.
Here is the PR that fixed this (presumably):
&lt;denchmark-link:https://github.com/ray-project/ray/pull/11033&gt;#11033&lt;/denchmark-link&gt;

Let me know, if this doesn't work.
There should be a more specific error message (something related to some torch tensor being on cuda device and another tensor being on cpu) in the output? The above stacktrace doesn't really tell much.
		</comment>
		<comment id='3' author='ljendele' date='2020-11-02T10:18:25Z'>
		Hi &lt;denchmark-link:https://github.com/sven1977&gt;@sven1977&lt;/denchmark-link&gt;
,
thanks for looking into this! I didn't see any more specific message or longer stack trace that what I have posted. However, I tried running with the latest wheels and the issue does seem to fixed. The PyTorch version does not seem to converge unlike the TF version. That's a different issue and most likely related to different initialization?
I will try ApeX execution plan with our custom environment. Hopefully, the errors disappear there too.
Thank you,
Lukas
		</comment>
		<comment id='4' author='ljendele' date='2020-11-03T20:26:23Z'>
		Thanks a lot for the pointers. I tried running with our custom environment with the latest wheels. The Error raised reading from queue is gone now and RLlib trains (and very fast). However, for some runs, I get the following error:
&lt;denchmark-code&gt;  File "/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/trainer.py", line 508, in train
    raise e
  File "/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/trainer.py", line 494, in train
    result = Trainable.train(self)
  File "/usr/local/lib/python3.6/dist-packages/ray/tune/trainable.py", line 336, in train
    result = self.step()
  File "/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/trainer_template.py", line 147, in step
    res = next(self.train_exec_impl)
  File "/usr/local/lib/python3.6/dist-packages/ray/util/iter.py", line 756, in __next__
    return next(self.built_iterator)
  File "/usr/local/lib/python3.6/dist-packages/ray/util/iter.py", line 783, in apply_foreach
    for item in it:
  File "/usr/local/lib/python3.6/dist-packages/ray/util/iter.py", line 783, in apply_foreach
    for item in it:
  File "/usr/local/lib/python3.6/dist-packages/ray/util/iter.py", line 843, in apply_filter
    for item in it:
  File "/usr/local/lib/python3.6/dist-packages/ray/util/iter.py", line 843, in apply_filter
    for item in it:
  File "/usr/local/lib/python3.6/dist-packages/ray/util/iter.py", line 783, in apply_foreach
    for item in it:
  File "/usr/local/lib/python3.6/dist-packages/ray/util/iter.py", line 843, in apply_filter
    for item in it:
  File "/usr/local/lib/python3.6/dist-packages/ray/util/iter.py", line 1075, in build_union
    item = next(it)
  File "/usr/local/lib/python3.6/dist-packages/ray/util/iter.py", line 756, in __next__
    return next(self.built_iterator)
  File "/usr/local/lib/python3.6/dist-packages/ray/util/iter.py", line 783, in apply_foreach
    for item in it:
  File "/usr/local/lib/python3.6/dist-packages/ray/util/iter.py", line 791, in apply_foreach
    result = fn(item)
  File "/usr/local/lib/python3.6/dist-packages/ray/rllib/execution/train_ops.py", line 388, in __call__
    lambda p, p_id: p_id in to_update and p.update_target())
  File "/usr/local/lib/python3.6/dist-packages/ray/rllib/evaluation/rollout_worker.py", line 923, in foreach_trainable_policy
    for pid, policy in self.policy_map.items()
  File "/usr/local/lib/python3.6/dist-packages/ray/rllib/evaluation/rollout_worker.py", line 924, in &lt;listcomp&gt;
    if pid in self.policies_to_train
  File "/usr/local/lib/python3.6/dist-packages/ray/rllib/execution/train_ops.py", line 388, in &lt;lambda&gt;
    lambda p, p_id: p_id in to_update and p.update_target())
  File "/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/ddpg/ddpg_torch_policy.py", line 259, in update_target
    (1.0 - tau) * var_target.data
RuntimeError: CUDA error: an illegal memory access was encountered
&lt;/denchmark-code&gt;

Is it possible that there is some race condition between the trainer and the TargetNetwork updater?
		</comment>
		<comment id='5' author='ljendele' date='2020-12-11T14:49:30Z'>
		Hmm, yeah, I see this error, too, sometimes on Windows.
		</comment>
		<comment id='6' author='ljendele' date='2020-12-14T18:50:15Z'>
		Does seem like a PyTorch/cuda/cudnn bug.
e.g. &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/21819&gt;pytorch/pytorch#21819&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>