<bug id='8212' author='sean-metzger' open_date='2020-04-28T10:09:17Z' closed_time='2020-04-28T16:50:02Z'>
	<summary>Error with Ray init [tune]</summary>
	<description>
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

Ray init wont work.
Note: I have a separate hyperopt script where I use ray tune to hyperopt a small nn on small dataset. I can call it now, and it works. I can run this exact script on a jupyter notebook with 3 gpus, and it works. But when I submit it as a server job on our gpu servers, it breaks. I have no idea why, especially since I can call ray init in the exact same way in different sets of code and it works completely fine.
Ray version and other system information (Python version, TensorFlow version, OS):
ray 0.8.4
python 3.6.6
torch 1.2.0
torchvision 0.4.0a0+6b959ee
Output files:
&lt;denchmark-link:https://github.com/ray-project/ray/files/4544842/debug_state.txt&gt;debug_state.txt&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
import ray
ray.init(
num_gpus=4,
temp_dir="/userdata/user/all_deepul_files/runs/tmp",
include_webui=False,
ignore_reinit_error=True
)
If we cannot run your script, we cannot fix your issue.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

Debug State:
&lt;denchmark-link:https://github.com/ray-project/ray/files/4544640/debug_state.txt&gt;debug_state.txt&lt;/denchmark-link&gt;

plasma_store.err:
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0428 02:21:15.315223 188310 store.cc:1244] Allowing the Plasma store to use up to 120.562GB of memory.
I0428 02:21:15.315352 188310 store.cc:1271] Starting object store with directory /dev/shm and huge page support disabled
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

worker.err:
Ray worker pid: 61869
2020-04-28 02:50:59,201	INFO trainable.py:217 -- Getting current IP.
Before I was getting messages similar to &lt;denchmark-link:https://github.com/ray-project/ray/issues/7932&gt;#7932&lt;/denchmark-link&gt;
: i.e.
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0428 01:48:39.800639 40672 store.cc:1244] Allowing the Plasma store to use up to 120.509GB of memory.
I0428 01:48:39.800767 40672 store.cc:1271] Starting object store with directory /dev/shm and huge page support disabled
E0428 01:48:39.800951 40672 io.cc:153] Socket pathname is too long.
F0428 01:48:39.800961 40672 store.cc:1167]  Check failed: socket &gt;= 0
*** Check failure stack trace: ***
*** Aborted at 1588063719 (unix time) try "date -d @1588063719" if you are using GNU date ***
PC: @                0x0 (unknown)
*** SIGABRT (@0x45400009ee0) received by PID 40672 (TID 0x2affe80b33c0) from PID 40672; stack trace: ***
@     0x2affe8938390 (unknown)
@     0x2affe8d8f428 gsignal
@     0x2affe8d9102a abort
@     0x55fa0e577499 google::logging_fail()
@     0x55fa0e5792bd google::LogMessage::Fail()
@     0x55fa0e57a64c google::LogMessage::SendToLog()
@     0x55fa0e578f99 google::LogMessage::Flush()
@     0x55fa0e5791b1 google::LogMessage::~LogMessage()
@     0x55fa0e56dd69 arrow::util::ArrowLog::~ArrowLog()
@     0x55fa0e548d90 plasma::StartServer()
@     0x55fa0e53bad9 main
@     0x2affe8d7a830 __libc_start_main
@     0x55fa0e53ddb1 (unknown)
	</description>
	<comments>
		<comment id='1' author='sean-metzger' date='2020-04-28T16:50:01Z'>
		I found the bug, I was requesting too many workers when I called my data loader. If a bug like this could be easier to find in the future that'd be sweet! Took me a while.
		</comment>
		<comment id='2' author='sean-metzger' date='2020-04-28T17:10:49Z'>
		Hm, awesome! &lt;denchmark-link:https://github.com/sean-metzger&gt;@sean-metzger&lt;/denchmark-link&gt;
 what was your debugging process for this?
Knowing that would help us raise a better warning.
		</comment>
		<comment id='3' author='sean-metzger' date='2020-04-28T17:35:11Z'>
		Sure,  and this is coming from a new Tune user (started last week).

Couldn't get any error messages, tried changing verbose to 0,1,2.
Looked in /ray_runs. It was confusing because it sometimes seemed to start a first round of experiments, but then nothing would happen, and there was no error message.
Went on github and found previous issues, tried modifying memory, where '/tmp' was, plasma_directory, etc. (was hard to interpret the .err files, and I didn't know they existed, and they looked similar to what people were getting)
Looked at https://rllib.readthedocs.io/en/latest/using-ray-with-pytorch.html for any clues about using a loader or model wrong or blowing up memory somehow.
Realized something could be wrong with dataloaders, and realized I was requesting a bunch of workers, since there had been a huge amount of worker files in the tmp directory I created. Looked at a reference doc (kakaobrain/fastautoaugment) for confirmation.

Hope that helps, again new to ray and parallelization. If you have tips on how to proceed next time that'd be great.
P.s. Ray is great! overhauled all my other hyperopt code with it.
		</comment>
		<comment id='4' author='sean-metzger' date='2020-04-28T23:47:01Z'>
		Hm, that sounds like a pretty bad experience (but definitely a really useful user journey) -- thanks for typing this all up. I'll look into making this better.
In any case, glad you like it! For next steps:

You might consider using an early stopping mechanism like "AsyncHyperBandScheduler".
Since you're using Pytorch, I'd look into using RaySGD for Pytorch (https://docs.ray.io/en/latest/raysgd/raysgd_pytorch.html)

Feel free to open issues/feature requests as you go!
		</comment>
		<comment id='5' author='sean-metzger' date='2020-04-29T05:44:56Z'>
		
Couldn't get any error messages, tried changing verbose to 0,1,2.

&lt;denchmark-link:https://github.com/richardliaw&gt;@richardliaw&lt;/denchmark-link&gt;
 Do we understand why there weren't any error messages? Is there a bug causing them to get suppressed?

... tried modifying memory, where '/tmp' was, plasma_directory, etc.

Maybe we should make these things a bit more hidden. It seems really unlikely that changing them will solve people's problems, but it will be frustrating.

(was hard to interpret the .err files, and I didn't know they existed, and they looked similar to what people were getting)

Maybe the default way to view these should just be through the dashboard, and they should be somewhat hidden except when we detect an error in which case we can show flag things as noteworthy? &lt;denchmark-link:https://github.com/mfitton&gt;@mfitton&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/sean-metzger&gt;@sean-metzger&lt;/denchmark-link&gt;
 thanks a lot for your help! Can you think of any information we could display in the Ray dashboard (or elsewhere) that would have made it obvious what the problem was from the start? Do you have thoughts about what the right error message to display would have been (and what should have triggered the error message)?
		</comment>
		<comment id='6' author='sean-metzger' date='2020-04-29T19:17:50Z'>
		Thanks for the follow ups!
I think just having error messages in the output would've been the most helpful - i was pretty lost for a while as a new user, just because there was nothing to go off in the main output file (our server prints to a txt file in our userdata), wheras other programs print a Traceback there etc. I didnt really know to go to a dashboard etc from the 3 tune tutorial notebooks on colab.
Best error message probably would have been that too many workers/subprocesses were requested, along the lines of 'exiting: too many subprocesses/workers requested' and where that happened in the code. It would have shown me it was something with my code.
		</comment>
	</comments>
</bug>