<bug id='7082' author='sherylwang' open_date='2020-02-07T06:57:17Z' closed_time='2020-02-08T02:13:11Z'>
	<summary>[tune] pytorch model warped in ray.tune.Trainable optimized abnormally</summary>
	<description>
Hi, I'm using tune PopulationBasedTraining with pytorch. I found after wrapping the pytorch model in ray.tune.Trainable api and train the model with ray.tune.run_experiments, The model's performance is much worse than the original pytorch's.  Even without the pbt scheduler, the performance is also worse than the original pytorch's. What would be the reason?
My ray[tune] version is 0.8.0, installed with pip, pytorch version is 1.2.0 cuda10.
	</description>
	<comments>
		<comment id='1' author='sherylwang' date='2020-02-07T07:04:53Z'>
		&lt;denchmark-link:https://github.com/sherylwang&gt;@sherylwang&lt;/denchmark-link&gt;
 Any numbers / code?
		</comment>
		<comment id='2' author='sherylwang' date='2020-02-07T07:14:23Z'>
		import os
import sys
import argparse
from termcolor import cprint

import ray
from ray.tune import run_experiments
from ray.tune import Trainable
import torch
from engine import train_class

class RayModel(Trainable):
    def _setup(self, config):
        """config is dict type config defined for tune.run api"""
        cprint("calling setup", 'green')
        args = argparse.Namespace()
        vars(args).update(**self.config)
        self.args = args
        self.trainer = train_class.MainWorker(args)

    def _train(self):
        cprint("training for one epoch", "green")
        # train, val, test
        train_acc = self.trainer.train(self._iteration)
        val_acc = self.trainer.validate(self._iteration)
        test_acc = self.trainer.test(self._iteration)
        return {
            "val_acc": val_acc,
            "train_acc": train_acc,
            "test_acc": test_acc
        }

    def _save(self, checkpoint_dir):
        """ray tune requires to save model within checkpoint_dir"""
        filename = self.trainer.save_model(self._iteration, checkpoint_dir)
        return filename

    def _restore(self, checkpoint):
        self.trainer.load_model(checkpoint)

    def reset_config(self, new_config):
        self.config = new_config
        args = argparse.Namespace()
        vars(args).update(**self.config)
        self.args = args
        self.trainer.reset_config(self.args)
        return True
This is the Trainable model.
import sys
import random
import numpy as np
from termcolor import cprint
import ray
from ray.tune import run_experiments
from ray.tune.schedulers import PopulationBasedTraining

import raymodel
import setup
from engine import parser

def main():
    args = parser.parse_args()
    setup.alter_args(args)
    # define the experiment object
    train_spec = {
        "run": raymodel.RayModel,
        "resources_per_trial": {
            "cpu": args.cpu,
            "gpu": args.ray_gpu
        },
        "stop": {
            "training_iteration": args.epochs,
        },
        "config": vars(args),
        "local_dir": args.local_dir,
        "checkpoint_freq": 0,
        "num_samples": args.num_samples
    }

    run_experiments(
        {
            args.name: train_spec
        } )


if __name__ == "__main__":
    cprint("start search", "green")
    main()
This is the training code with run_experiments api.
		</comment>
		<comment id='3' author='sherylwang' date='2020-02-07T07:43:17Z'>
		The training loss become around 27 after the first training epoch without tune. While with tune run_experiments and trainable, the loss is still 37 after 4 epochs. That's weird. &lt;denchmark-link:https://github.com/GarrisonD&gt;@GarrisonD&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='sherylwang' date='2020-02-07T09:07:06Z'>
		Can you try this and print the output?
&lt;denchmark-code&gt;trainable_model = RayModel()
print(trainable_model.train())
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='sherylwang' date='2020-02-07T11:17:08Z'>
		&lt;denchmark-code&gt;import sys
import random
import numpy as np
from termcolor import cprint
import ray
from ray.tune import run_experiments
from ray.tune.schedulers import PopulationBasedTraining

import raymodel
import setup
from engine import parser

def main():
    args = parser.parse_args()
    setup.alter_args(args)
    model = raymodel.RayModel(vars(args))
    print(model.train())

if __name__ == "__main__":
    cprint("start search", "green")
    main()
&lt;/denchmark-code&gt;

I've run the above code.
		</comment>
		<comment id='6' author='sherylwang' date='2020-02-07T13:06:06Z'>
		I found the gap of performance is not due to tune. The hyper-parameter setting(weight decay) was set different when training origin pytorch model. Thanks for the help. &lt;denchmark-link:https://github.com/richardliaw&gt;@richardliaw&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/GarrisonD&gt;@GarrisonD&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='sherylwang' date='2020-02-08T02:13:11Z'>
		Great! Will close this for now.
		</comment>
	</comments>
</bug>