<bug id='8701' author='jetjodh' open_date='2020-06-01T15:42:18Z' closed_time='2020-11-15T05:53:18Z'>
	<summary>[tune] TypeError: cannot serialize '_io.BufferedReader' object</summary>
	<description>
tune
&lt;denchmark-h:h3&gt;What is the problem?&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;2020-06-01 15:34:33,008	INFO resource_spec.py:212 -- Starting Ray with 7.42 GiB memory available for workers and up to 3.73 GiB for objects. You can adjust these settings with ray.init(memory=&lt;bytes&gt;, object_store_memory=&lt;bytes&gt;).
2020-06-01 15:34:34,527	INFO services.py:1170 -- View the Ray dashboard at 127.0.0.1:8265
/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead
  warnings.warn("pickle support for Storage will be removed in 1.5. Use `torch.save` instead", FutureWarning)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-25-1723726f32b8&gt; in &lt;module&gt;()
     21         },
     22         stop={
---&gt; 23             "roc_auc": 0.98})
     24 # model = train_model(model, criterion, optimizer, exp_lr_scheduler,
     25 #                        num_epochs=17)

5 frames
/usr/local/lib/python3.6/dist-packages/ray/cloudpickle/cloudpickle_fast.py in dump(self, obj)
    615     def dump(self, obj):
    616         try:
--&gt; 617             return Pickler.dump(self, obj)
    618         except RuntimeError as e:
    619             if "recursion" in e.args[0]:

TypeError: cannot serialize '_io.BufferedReader' object
&lt;/denchmark-code&gt;

Ray version and other system information (Python version, TensorFlow version, OS):
&lt;denchmark-h:h3&gt;Reproduction (REQUIRED)&lt;/denchmark-h&gt;

Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):
&lt;denchmark-code&gt;import time
from ray.tune import track
import ray
ray.shutdown()
ray.init(webui_host='127.0.0.1')
def train(config):
    optimizer = AdamW(model.parameters(), lr=config["lr"], weight_decay=config["weight_decay"], betas=config["betas"], eps=config["eps"])
    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.01)
    history = train_model(model, criterion, optimizer, exp_lr_scheduler,num_epochs=5)
    tune.report(roc_auc=history['roc_auc'])
    return {"roc_auc":history['roc_auc']}
sched = AsyncHyperBandScheduler(
        time_attr="training_iteration", metric="roc_auc")
analysis = tune.run(train,
        scheduler=sched,
        config={
            "lr": tune.sample_from(lambda spec: 10**(-10 * np.random.rand())),
            "weight_decay": tune.uniform(0.0001, 0.1),
            "betas": tune.uniform(0.6, 0.9),
            "eps": tune.uniform(0.000000008, 0.0000008)
        },
        stop={
            "roc_auc": 0.98})

&lt;/denchmark-code&gt;

If we cannot run your script, we cannot fix your issue.

 I have verified my script runs in a clean environment and reproduces the issue.
 I have verified the issue also occurs with the latest wheels.

	</description>
	<comments>
		<comment id='1' author='jetjodh' date='2020-06-01T16:19:22Z'>
		Can you instantiate the model inside the function?
		</comment>
		<comment id='2' author='jetjodh' date='2020-06-01T16:41:02Z'>
		&lt;denchmark-code&gt;import time
from ray.tune import track
import ray
ray.shutdown()
ray.init(webui_host='127.0.0.1')
def train(config):
    model = MyModel()
    torch.cuda.empty_cache()
    model.to('cuda')
    optimizer = AdamW(model.parameters(), lr=config["lr"], weight_decay=config["weight_decay"], betas=config["betas"], eps=config["eps"])
    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.01)
    history = train_model(model, criterion, optimizer, exp_lr_scheduler,num_epochs=5)
    tune.report(roc_auc=history['roc_auc'])
    return {"roc_auc":history['roc_auc']}
sched = AsyncHyperBandScheduler(
        time_attr="training_iteration", metric="roc_auc")
analysis = tune.run(train,
        scheduler=sched,
        config={
            "lr": tune.sample_from(lambda spec: 10**(-10 * np.random.rand())),
            "weight_decay": tune.uniform(0.0001, 0.1),
            "betas": tune.uniform(0.6, 0.9),
            "eps": tune.uniform(0.000000008, 0.0000008)
        },
        stop={
            "roc_auc": 0.98})
&lt;/denchmark-code&gt;

Gives the error:
&lt;denchmark-code&gt;2020-06-01 16:39:11,111	INFO resource_spec.py:212 -- Starting Ray with 7.13 GiB memory available for workers and up to 3.57 GiB for objects. You can adjust these settings with ray.init(memory=&lt;bytes&gt;, object_store_memory=&lt;bytes&gt;).
2020-06-01 16:39:11,964	INFO services.py:1170 -- View the Ray dashboard at 127.0.0.1:8265
/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead
  warnings.warn("pickle support for Storage will be removed in 1.5. Use `torch.save` instead", FutureWarning)
---------------------------------------------------------------------------
PicklingError                             Traceback (most recent call last)
&lt;ipython-input-14-fafb54790ffc&gt; in &lt;module&gt;()
     24         },
     25         stop={
---&gt; 26             "roc_auc": 0.98})
     27 # model = train_model(model, criterion, optimizer, exp_lr_scheduler,
     28 #                        num_epochs=17)

5 frames
/usr/local/lib/python3.6/dist-packages/ray/tune/tune.py in run(run_or_experiment, name, stop, config, resources_per_trial, num_samples, local_dir, upload_dir, trial_name_creator, loggers, sync_to_cloud, sync_to_driver, checkpoint_freq, checkpoint_at_end, sync_on_checkpoint, keep_checkpoints_num, checkpoint_score_attr, global_checkpoint_period, export_formats, max_failures, fail_fast, restore, search_alg, scheduler, with_server, server_port, verbose, progress_reporter, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, return_trials, ray_auto_init)
    245     for i, exp in enumerate(experiments):
    246         if not isinstance(exp, Experiment):
--&gt; 247             run_identifier = Experiment.register_if_needed(exp)
    248             experiments[i] = Experiment(
    249                 name=name,

/usr/local/lib/python3.6/dist-packages/ray/tune/experiment.py in register_if_needed(cls, run_object)
    201                 logger.warning(
    202                     "No name detected on trainable. Using {}.".format(name))
--&gt; 203             register_trainable(name, run_object)
    204             return name
    205         else:

/usr/local/lib/python3.6/dist-packages/ray/tune/registry.py in register_trainable(name, trainable)
     68         raise TypeError("Second argument must be convertable to Trainable",
     69                         trainable)
---&gt; 70     _global_registry.register(TRAINABLE_CLASS, name, trainable)
     71 
     72 

/usr/local/lib/python3.6/dist-packages/ray/tune/registry.py in register(self, category, key, value)
    110             raise TuneError("Unknown category {} not among {}".format(
    111                 category, KNOWN_CATEGORIES))
--&gt; 112         self._to_flush[(category, key)] = pickle.dumps(value)
    113         if _internal_kv_initialized():
    114             self.flush_values()

/usr/local/lib/python3.6/dist-packages/ray/cloudpickle/cloudpickle_fast.py in dumps(obj, protocol, buffer_callback)
     70         cp = CloudPickler(file, protocol=protocol,
     71                           buffer_callback=buffer_callback)
---&gt; 72         cp.dump(obj)
     73         return file.getvalue()
     74 

/usr/local/lib/python3.6/dist-packages/ray/cloudpickle/cloudpickle_fast.py in dump(self, obj)
    615     def dump(self, obj):
    616         try:
--&gt; 617             return Pickler.dump(self, obj)
    618         except RuntimeError as e:
    619             if "recursion" in e.args[0]:

PicklingError: args[0] from __newobj__ args has the wrong class
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='jetjodh' date='2020-06-01T17:03:56Z'>
		Can you also maybe also try moving the criterion inside too?
Further, can you make sure torch.cuda.is_initialized == False before tune.run?
		</comment>
		<comment id='4' author='jetjodh' date='2020-06-01T17:09:53Z'>
		Now the error is:
&lt;denchmark-code&gt;2020-06-01 17:07:19,582	INFO resource_spec.py:212 -- Starting Ray with 7.42 GiB memory available for workers and up to 3.72 GiB for objects. You can adjust these settings with ray.init(memory=&lt;bytes&gt;, object_store_memory=&lt;bytes&gt;).
2020-06-01 17:07:20,452	INFO services.py:1170 -- View the Ray dashboard at 127.0.0.1:8265
/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead
  warnings.warn("pickle support for Storage will be removed in 1.5. Use `torch.save` instead", FutureWarning)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-14-dfc5052317ee&gt; in &lt;module&gt;()
     26         },
     27         stop={
---&gt; 28             "roc_auc": 0.98})
     29 # model = train_model(model, criterion, optimizer, exp_lr_scheduler,
     30 #                        num_epochs=17)

5 frames
/usr/local/lib/python3.6/dist-packages/ray/tune/tune.py in run(run_or_experiment, name, stop, config, resources_per_trial, num_samples, local_dir, upload_dir, trial_name_creator, loggers, sync_to_cloud, sync_to_driver, checkpoint_freq, checkpoint_at_end, sync_on_checkpoint, keep_checkpoints_num, checkpoint_score_attr, global_checkpoint_period, export_formats, max_failures, fail_fast, restore, search_alg, scheduler, with_server, server_port, verbose, progress_reporter, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, return_trials, ray_auto_init)
    245     for i, exp in enumerate(experiments):
    246         if not isinstance(exp, Experiment):
--&gt; 247             run_identifier = Experiment.register_if_needed(exp)
    248             experiments[i] = Experiment(
    249                 name=name,

/usr/local/lib/python3.6/dist-packages/ray/tune/experiment.py in register_if_needed(cls, run_object)
    201                 logger.warning(
    202                     "No name detected on trainable. Using {}.".format(name))
--&gt; 203             register_trainable(name, run_object)
    204             return name
    205         else:

/usr/local/lib/python3.6/dist-packages/ray/tune/registry.py in register_trainable(name, trainable)
     68         raise TypeError("Second argument must be convertable to Trainable",
     69                         trainable)
---&gt; 70     _global_registry.register(TRAINABLE_CLASS, name, trainable)
     71 
     72 

/usr/local/lib/python3.6/dist-packages/ray/tune/registry.py in register(self, category, key, value)
    110             raise TuneError("Unknown category {} not among {}".format(
    111                 category, KNOWN_CATEGORIES))
--&gt; 112         self._to_flush[(category, key)] = pickle.dumps(value)
    113         if _internal_kv_initialized():
    114             self.flush_values()

/usr/local/lib/python3.6/dist-packages/ray/cloudpickle/cloudpickle_fast.py in dumps(obj, protocol, buffer_callback)
     70         cp = CloudPickler(file, protocol=protocol,
     71                           buffer_callback=buffer_callback)
---&gt; 72         cp.dump(obj)
     73         return file.getvalue()
     74 

/usr/local/lib/python3.6/dist-packages/ray/cloudpickle/cloudpickle_fast.py in dump(self, obj)
    615     def dump(self, obj):
    616         try:
--&gt; 617             return Pickler.dump(self, obj)
    618         except RuntimeError as e:
    619             if "recursion" in e.args[0]:

TypeError: can't pickle tensorflow.python._pywrap_file_io.WritableFile objects
&lt;/denchmark-code&gt;

I think the problem is in cloudpickle from the start.
		</comment>
		<comment id='5' author='jetjodh' date='2020-06-15T17:56:58Z'>
		Hm, you're calling torch.cuda.empty_cache() but you're now running into Tensorflow issues?
The last thing you should try is to move all of the imports (tensorflow, pytorch) into the Trainable object.
		</comment>
		<comment id='6' author='jetjodh' date='2020-06-16T10:36:26Z'>
		I used tf for padding the sequences in my dataset object. Is that causing these problems?
		</comment>
		<comment id='7' author='jetjodh' date='2020-11-11T20:24:48Z'>
		Hi, I'm a bot from the Ray team :)
To help human contributors to focus on more relevant issues, I will automatically add the stale label to issues that have had no activity for more than 4 months.
If there is no further activity in the 14 days, the issue will be closed!

If you'd like to keep the issue open, just leave any comment, and the stale label will be removed!
If you'd like to get more attention to the issue, please tag one of Ray's contributors.

You can always ask for help on our &lt;denchmark-link:https://discuss.ray.io/&gt;discussion forum&lt;/denchmark-link&gt;
 or &lt;denchmark-link:https://github.com/ray-project/ray#getting-involved&gt;Ray's public slack channel&lt;/denchmark-link&gt;
.
		</comment>
	</comments>
</bug>