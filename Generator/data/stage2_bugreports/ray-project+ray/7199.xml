<bug id='7199' author='yannbouteiller' open_date='2020-02-17T21:46:57Z' closed_time='2020-02-18T18:55:23Z'>
	<summary>[rllib] PPO: TF eager execution doesn't work with custom models</summary>
	<description>
I am trying to modify FullyConnectedNetwork(TFModelV2) to replace the last activation by a sigmoid (PPO keeps outputing saturated actions with the default model).
First, I set config["eager"] = True for debugging
Then, I copy-paste FullyConnectedNetwork(TFModelV2), rename it, register the renamed model and use it as a custom model.
Then, when executing
trainer = agents.ppo.PPOTrainer(env="my_multiagent_env", config=config)
I get the following error:

2020-02-17 16:43:30,693	ERROR worker.py:1003 -- Possible unhandled error from worker: ray::RolloutWorker.init() (pid=25478, ip=132.207.28.246)
File "python/ray/_raylet.pyx", line 643, in ray._raylet.execute_task
File "python/ray/_raylet.pyx", line 623, in function_executor
File "/home/yann/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py", line 357, in init
policy_dict, policy_config)
File "/home/yann/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py", line 778, in _build_policy_map
policy_map[name] = cls(obs_space, act_space, merged_conf)
File "/home/yann/anaconda3/lib/python3.7/site-packages/ray/rllib/policy/eager_tf_policy.py", line 234, in init
[_flatten_action(action_space.sample())]),
File "/home/yann/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py", line 1087, in convert_to_tensor
return convert_to_tensor_v2(value, dtype, preferred_dtype, name)
File "/home/yann/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py", line 1145, in convert_to_tensor_v2
as_ref=False)
File "/home/yann/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py", line 1199, in internal_convert_to_tensor
ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
File "/home/yann/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py", line 305, in _constant_tensor_conversion_function
return constant(v, dtype=dtype, name=name)
File "/home/yann/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py", line 246, in constant
allow_broadcast=True)
File "/home/yann/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py", line 284, in _constant_impl
allow_broadcast=allow_broadcast))
File "/home/yann/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py", line 482, in make_tensor_proto
_GetDenseDimensions(values)))
ValueError: Argument must be a dense tensor: [array([0., 1.], dtype=float32)] - got shape [1, 2], but wanted [1].
2020-02-17 16:43:30,696	ERROR worker.py:1003 -- Possible unhandled error from worker: ray::RolloutWorker.init() (pid=25476, ip=132.207.28.246)
File "python/ray/_raylet.pyx", line 643, in ray._raylet.execute_task
File "python/ray/_raylet.pyx", line 623, in function_executor
File "/home/yann/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py", line 357, in init
policy_dict, policy_config)
File "/home/yann/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py", line 778, in _build_policy_map
policy_map[name] = cls(obs_space, act_space, merged_conf)
File "/home/yann/anaconda3/lib/python3.7/site-packages/ray/rllib/policy/eager_tf_policy.py", line 234, in init
[_flatten_action(action_space.sample())]),
File "/home/yann/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py", line 1087, in convert_to_tensor
return convert_to_tensor_v2(value, dtype, preferred_dtype, name)
File "/home/yann/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py", line 1145, in convert_to_tensor_v2
as_ref=False)
File "/home/yann/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py", line 1199, in internal_convert_to_tensor
ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
File "/home/yann/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py", line 305, in _constant_tensor_conversion_function
return constant(v, dtype=dtype, name=name)
File "/home/yann/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py", line 246, in constant
allow_broadcast=True)
File "/home/yann/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py", line 284, in _constant_impl
allow_broadcast=allow_broadcast))
File "/home/yann/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py", line 482, in make_tensor_proto
_GetDenseDimensions(values)))
ValueError: Argument must be a dense tensor: [array([0., 1.], dtype=float32)] - got shape [1, 2], but wanted [1].

If I use SAC instead of PPO doing the same thing, it seems to work but I get a warning saying:

2020-02-17 16:45:23,965	WARNING sac_policy.py:27 -- Setting use_state_preprocessor=True since a custom model was specified.

ray version: 0.8.1
	</description>
	<comments>
		<comment id='1' author='yannbouteiller' date='2020-02-18T18:55:23Z'>
		Please provide a repro script (reopen this issue once you have it).
		</comment>
		<comment id='2' author='yannbouteiller' date='2020-02-18T23:51:57Z'>
		Sorry for wasting your time, it works actually, it was a bug in my environment.
For reference, the bug was caused by the fact that, with multiagent environments, rllib calls step() with an empty action_dict at the beginning for some reason (?) and in this case I replace missing actions by actions sampled in an action_space that had one extra dimension (nb_agents * nb_agents * nb_actions instead of nb_agents * nb_actions). My bug was only visible in this specific case, apparently.
		</comment>
		<comment id='3' author='yannbouteiller' date='2020-02-27T20:52:26Z'>
		&lt;denchmark-link:https://github.com/yannbouteiller&gt;@yannbouteiller&lt;/denchmark-link&gt;
  Could you explain this a bit more? I think I am having a similar bug, but I am not using a multi-agent environment. My detailed explanation of the issue is here: &lt;denchmark-link:https://github.com/ray-project/ray/issues/7359&gt;#7359&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='yannbouteiller' date='2020-02-27T22:56:45Z'>
		
@yannbouteiller Could you explain this a bit more? I think I am having a similar bug, but I am not using a multi-agent environment. My detailed explanation of the issue is here: #7359

Hi, I don't know if your issue is very similar, and I am not sure what happened in my case to be honest because I don't see how the issue I described was related to this bug although correcting the bug seems to have corrected it.
Essentially I was passing a list of lists instead of a list as action to my own step() function when rllib called step() with an empty action (?), and that somehow caused this in the end. Probably step() was then returning something weird. Sorry this is not very helpful...
		</comment>
		<comment id='5' author='yannbouteiller' date='2020-03-01T17:26:37Z'>
		Ok, thanks for your response! It was helpful and clarified some questions I had.
		</comment>
	</comments>
</bug>