<bug id='17824' author='StefanoD' open_date='2020-07-13T08:23:13Z' closed_time='2020-07-17T13:17:17Z'>
	<summary>Instabilities in cv::solve() on small numbers</summary>
	<description>
&lt;denchmark-link:https://github.com/opencv/opencv/files/4921238/signal_586_20x-full_sensor_0.17_fineFocus_.zip&gt;signal_586_20x-full_sensor_0.17_fineFocus_.zip&lt;/denchmark-link&gt;

&lt;denchmark-h:h5&gt;System information (version)&lt;/denchmark-h&gt;


OpenCV =&gt; 3.2.0  and 4.4 (master branch from yesterday)
Operating System / Platform =&gt; Ubuntu 18.04
Compiler =&gt; gcc

&lt;denchmark-h:h5&gt;Detailed description&lt;/denchmark-h&gt;

I'm trying to fit a signal to a parabola function and it works fine in most cases. But when my x-data has the size of 10^-8 and my y-data 10^3, then I get complete garbage while the GNU Scientific Library returns the correct fittet answer.
I try to replicate numpy.polyfit.
&lt;denchmark-h:h5&gt;Steps to reproduce&lt;/denchmark-h&gt;

Parsing the CSV-File. First column is the signal, second the position.
The position gets divided by 1'000'000'000 in order to convert it to nm.
#include &lt;opencv2/core/core.hpp&gt;
#include &lt;vector&gt;
#include &lt;fstream&gt;

struct MeasurementData
{
    std::vector&lt;double&gt; positionsNm;
    std::vector&lt;double&gt; signalData;
};

double parabolaModelFunction(double x, const double *p)
{
    return p[0] + p[1]* x + p[2]* x * x;
}

std::vector&lt;double&gt; getFittetValues(const std::vector&lt;double&gt; &amp;x_values, const double *params)
{
    std::vector&lt;double&gt; fittetYValues;
    fittetYValues.reserve(x_values.size());

    for (double x : x_values)
    {
        double fittetY = parabolaModelFunction(x, params);
        fittetYValues.emplace_back(fittetY);
    }

    return fittetYValues;
}

MeasurementData getData(const std::string &amp;filepath)
{
    std::ifstream infile(filepath);
    MeasurementData measurementData;

    std::string s;
    while (getline(infile, s))
    {
        int index = s.find(';');
        std::string strSignal = s.substr(0, index);
        std::string strXPos = s.substr(index + 1);
        measurementData.signalData.emplace_back(std::stod(strSignal));
        measurementData.positionsNm.emplace_back(std::stod(strXPos) / 1'000'000'000.0);
    }

    return measurementData;
}

int main()
{
    MeasurementData data = getData("signal_586_20x-full_sensor_0.17_fineFocus_.csv");

    double *x = data.positionsNm.data();
    double *y = data.signalData.data();
    const int size = data.signalData.size();

    /// Bad params with scale factor 1.0
    std::vector&lt;double&gt; fittetParams = polyFit(x, y, size, 2, 1.0);
    std::vector&lt;double&gt; fittetYValues = getFittetValues(data.positionsNm, fittetParams.data());
    const double meanSquareError = cv::norm(data.signalData, fittetYValues);

    /// Good params with scale factor 100000
    std::vector&lt;double&gt; fittetParams2 =  polyFit(x, y, size, 2, 100000);
    std::vector&lt;double&gt; fittetYValues2 = getFittetValues(data.positionsNm, fittetParams2.data());
    const double meanSquareError2 = cv::norm(data.signalData, fittetYValues2);

    return 0;
}
My polyfit function:
#include &lt;opencv2/core/core.hpp&gt;
#include &lt;vector&gt;
#include &lt;cmath&gt;

cv::Mat getVandermondeMatrix(double* xValues, size_t numberSamplingPoints, size_t polynomialDegree, double scaleFactor)
{
    /// See https://en.wikipedia.org/wiki/Vandermonde_matrix
    const size_t columns = polynomialDegree + 1;
    cv::Mat vandermondeMat(numberSamplingPoints, columns, CV_64F);

    for (size_t row = 0; row &lt; numberSamplingPoints; ++row)
    {
        /// First column is always 1
        vandermondeMat.at&lt;double&gt;(row, 0) = 1.0;
        const double x = xValues[row];

        for (size_t col = 1; col &lt; columns; ++col)
        {
            vandermondeMat.at&lt;double&gt;(row, col) = std::pow(x * scaleFactor, col);
        }
    }

    return vandermondeMat;
}

std::vector&lt;double&gt; polyFit(double* xValues, double* yValues, size_t numberSamplingPoints, size_t polynomialDegree,
                            double scaleFactorXValues) {

    cv::Mat X = getVandermondeMatrix(xValues, numberSamplingPoints, polynomialDegree, scaleFactorXValues);
    cv::Mat y(numberSamplingPoints, 1, CV_64F, yValues);

    cv::Mat b;
    cv::solve(X, y, b, cv::DECOMP_SVD);

    const size_t numberParameters = polynomialDegree + 1;
    assert(b.rows = numberParameters);

    std::vector&lt;double&gt; parameters;
    parameters.reserve(numberParameters);

    for (size_t row = 0; row &lt; numberParameters; ++row)
    {
        double param = b.at&lt;double&gt;(row, 0);
        /// Because we divided also by the scale factor (by the inverse X-Matrix), we have to scale back by
        /// multiplying the parameters by scale factor^(polynom degree).
        param *= std::pow(scaleFactorXValues, row);
        parameters.emplace_back(param);
    }

    return parameters;
}
The correct parameters should be:
f(x) = axÂ² + bx + c
a: -1.753501E+25
b: 5.052680E+17
c: -3.639784E+09
Blue is the signal and red the correct fitted parabola.
&lt;denchmark-link:https://user-images.githubusercontent.com/1634862/87297511-d7411b80-c508-11ea-9220-6170631fdda1.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='StefanoD' date='2020-07-13T17:58:20Z'>
		cv::solve(X, y, b, cv::DECOMP_SVD);
Possibly the issue comes from OpenCV using its own function for SVD decomposition? &lt;denchmark-link:https://github.com/opencv/opencv/blob/01b2c5a77ca6dbef3baef24ebc0a5984579231d9/modules/core/src/lapack.cpp#L1078&gt;Link to the source code&lt;/denchmark-link&gt;
 for  function.
Maybe related: &lt;denchmark-link:https://github.com/opencv/opencv/issues/10084&gt;#10084&lt;/denchmark-link&gt;

You can try to:

check with cv::getBuildInformation() if OpenCV is built with Lapack support, otherwise try to build OpenCV with Lapack support
use Eigen library to replace cv::solve()
use your own function for cv::solve() with direct call to Lapack routines

		</comment>
		<comment id='2' author='StefanoD' date='2020-07-14T06:01:28Z'>
		&lt;denchmark-link:https://github.com/catree&gt;@catree&lt;/denchmark-link&gt;

cv::getBuildInformation() delivers:
&lt;denchmark-code&gt;Use Lapack:                  NO
Use Eigen:                   YES (ver 3.3.4)
&lt;/denchmark-code&gt;

Does this bug get fixed?
BTW: The GNU Scientific Library can handle this signal without scaling. But I can't use that, because of the GPL License
		</comment>
		<comment id='3' author='StefanoD' date='2020-07-14T19:51:52Z'>
		Update:
This signal is for several libs a challenge.
On

OpenCV 3.2.0 with Eigen usage
Intel MKL
Eigen 3.3.4
I had to use the scale factor 10'000 or 10'000 on the x values and correct the scaling on the parameter again.
Only the GNU Scientific Library could handle x values with magnitude of 10^(-8).

BUT: The latest OpenCV version 4.4 pre-release from master until this commit could not handle at all this signal! Scaling didn't help!
Maybe, because I compiled without Eigen or Lapack usage:
See &lt;denchmark-link:https://github.com/opencv/opencv/issues/17824#issuecomment-658400479&gt;#17824 (comment)&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/opencv/opencv/issues/17824#issuecomment-658418032&gt;#17824 (comment)&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;Lapack:                      NO
Eigen:                       NO
&lt;/denchmark-code&gt;

I will try to compile with Eigen usage.
		</comment>
		<comment id='4' author='StefanoD' date='2020-07-14T20:34:59Z'>
		What I mentioned before is that since OpenCV uses internally its own function for SVD, there may be some issues compared to other libraries that use Lapack.

This signal is for several libs a challenge.

Now you said that, you should validate the results of the SVD of X with the different libraries. What I would do:

dump the values of X in full precision into text files
compare the SVD results with Numpy, Matlab, Eigen (or whatever reference scientific computing libraries) and OpenCV

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;


Only the GNU Scientific Library could handle x values with magnitude of 10^(-8).

I don't have experience with GSL but you should determine the source of your issue:


OpenCV SVD result is not correct, due to #10084, can be tested by comparing the results of SVD between Numpy, Matlab, Eigen and OpenCV built without Lapack and Eigen support


only GSL returns correct results compared to Lapack, or at least only GSL gives correct results regardless of the input range


The difference between 1) and 2) is important:

issue with 1) means that OpenCV should use Lapack instead of using its own function for SVD, see #11007
issue with 2) is not related to OpenCV

BTW, what about the same code with full Numpy/Python code? Does it give always correct results?
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Looking quickly at the source code, it seems that if OpenCV is built with Lapack support, Lapack is indeed used:



opencv/modules/core/src/hal_internal.cpp


        Lines 198 to 199
      in
      f0c411d






 template &lt;typename fptype&gt; static inline int 



 lapack_SVD(fptype* a, size_t a_step, fptype *w, fptype* u, size_t u_step, fptype* vt, size_t v_step, int m, int n, int flags, int* info) 





I don't think it is the case with Eigen.
This should be mentioned in the documentation. What is the impact of OpenCV built with Lapack? All related Lapack (SVD, ...) and BLAS (GEMM, GEMV,...) functions in OpenCV are using Lapack/BLAS functions?
What is the impact of OpenCV built with Eigen? Only conversion between OpenCV and Eigen types are available? Is there some code that will use Eigen functions?
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

This is what I got with OpenCV built with MKL:
&lt;denchmark-code&gt;param: 12497
param: 0.000174958
param: 2.44941e-12

param: 12497
param: 1.74958e+06
param: 2.44941e+08
&lt;/denchmark-code&gt;

Seems different to your "correct results":

a: -1,753501E+25
b: 5,052680E+17
c: -3,639784E+09

		</comment>
		<comment id='5' author='StefanoD' date='2020-07-14T20:37:27Z'>
		&lt;denchmark-link:https://github.com/catree&gt;@catree&lt;/denchmark-link&gt;

I just updated the CSV, because I stumbled into this error. At work we have german computer and 14,78 is interpreted as 14.78. Here, at my computer, this has been interpreted wrongly. So I updated the CSV and replaced all commas with dots.
Sorry, for this inconvenience!
EDIT: Using the english notation, the correct result should be:
&lt;denchmark-code&gt;a: -1.753501E+25
b: 5.052680E+17
c: -3.639784E+09
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='StefanoD' date='2020-07-14T20:50:46Z'>
		
What is the impact of OpenCV built with Eigen?

This can be checked with a global search of #ifdef HAVE_EIGEN
Only this function in the main OpenCV repository uses Eigen functions when OpenCV is built with Eigen support:



opencv/modules/core/src/lapack.cpp


         Line 1377
      in
      f0c411d






 bool cv::eigen( InputArray _src, OutputArray _evals, OutputArray _evects ) 





&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;


I will try to compile with Eigen usage.

My advice is to build OpenCV with Lapack and Eigen support:

with MKL:

OpenCV should now use Lapack routines for things like SVD and other related scientific computing functions
you will have better performance for things like big matrix-matrix multiplication


on Linux you can install MKL or liblapack-dev and libopenblas-dev on Ubuntu
seems like Eigen is not really used in OpenCV but it does not hurt to have Eigen installed on your computer

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

I have now with MKL:
&lt;denchmark-code&gt;lapack_SVD
b: [734040.0239747886, -50074965279034.33, -1443043.954262242]
param: 734040
param: -5.0075e+13
param: -1.44304e+06
lapack_SVD
b: [-3639783982.325067, 5052680460244.994, -1753500805619430]
param: -3.63978e+09
param: 5.05268e+17
param: -1.7535e+25
&lt;/denchmark-code&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

This is what I got by manually tweaking the code to use the internal SVD function in OpenCV:
&lt;denchmark-code&gt;JacobiSVDImpl_
b: [734040.0239744243, -50074965279009.09, -1443043.954261458]
param: 734040
param: -5.0075e+13
param: -1.44304e+06
JacobiSVDImpl_
b: [-3639783982.220085, 5052680460099.282, -1753500805568869]
param: -3.63978e+09
param: 5.05268e+17
param: -1.7535e+25
&lt;/denchmark-code&gt;

Both results match. Scientific computing is not my field.
So your issue is not related to OpenCV but rather to some kind of input range normalisation or numerical precision in scientific computing.
Most likely your code in full Matlab or Numpy/Python should give the same results than OpenCV.
Scientific computing is a whole field of expertise and I cannot help on it.
		</comment>
		<comment id='7' author='StefanoD' date='2020-07-14T21:04:14Z'>
		
I have now:
b: [734040.0239747886, -50074965279034.33, -1443043.954262242]
param: 734040
param: -5.0075e+13
param: -1.44304e+06
b: [-3639783982.325067, 5052680460244.994, -1753500805619430]
param: -3.63978e+09
param: 5.05268e+17
param: -1.7535e+25


That is exactly the error you could reproduce. The latter params are the correct one. If you would plot data.positionsNm and fittetYValues2 in one CSV file and data.positionsNm and data.signalData into another CSV file, you would plot the graph I have shown above.
The armadillo lib could also solve this without scaling.
// Solve this manually
std::vector&lt;double&gt; armaSolveManually(const MeasurementData &amp;data, size_t polynomialDegree)
{
    const size_t columns = polynomialDegree + 1;
    arma::dmat X(data.positionsNm.size(), columns);
    arma::dvec Y(data.signalData);

    for (size_t row = 0; row &lt; data.positionsNm.size(); ++row)
    {
        /// First column is always 1
        X.at(row, 0) = 1.0;
        const double x = data.positionsNm[row];

        for (size_t col = 1; col &lt; columns; ++col)
        {
            X.at(row, col) = std::pow(x, col);
        }
    }

    arma::dmat params = arma::solve(X, Y, arma::solve_opts::allow_ugly);
    std::vector&lt;double&gt; vParams;

    for (size_t i = 0; i &lt; params.size(); ++i)
    {
        vParams.emplace_back(params.at(i, 0));
    }

    return vParams;
}

// Solve by using polyfit of armadillo
std::vector&lt;double&gt; armaSolve(const MeasurementData &amp;data)
{
    arma::dvec X(data.positionsNm);
    arma::dvec Y(data.signalData);

    arma::dvec params = arma::polyfit(X, Y, 2);
    int i = 0;
    std::vector&lt;double&gt; vParams;

    for (int i = 0; i &lt; params.size(); ++i)
    {
        vParams.emplace_back(params.at(i));
    }


    return vParams;
}
		</comment>
		<comment id='8' author='StefanoD' date='2020-07-14T21:07:27Z'>
		
Most likely your code in full Matlab or Numpy/Python should give the same results than OpenCV.
Scientific computing is a whole field of expertise and I cannot help on it.

Numpy.polyfit() delivers the correct result without scaling.
		</comment>
		<comment id='9' author='StefanoD' date='2020-07-14T21:16:41Z'>
		New update:
Eigen or not Eigen, I get always this result:
&lt;denchmark-code&gt;Without scaling the x-values:
b: [734040.0239747886, -50074965279034.33, -1443043.954262242]
param: 734040
param: -5.0075e+13
param: -1.44304e+06

With scaling the x-values:
b: [-3639783982.325067, 5052680460244.994, -1753500805619430]
param: -3.63978e+09
param: 5.05268e+17
param: -1.7535e+25
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='StefanoD' date='2020-07-14T21:22:19Z'>
		I don't think your issue is related to OpenCV.

Eigen or not Eigen, I get always this result:

You should look into the OpenCV source code. Eigen is not used in OpenCV apart for the function I mentioned above.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

You are using only two OpenCV functions:

cv::norm()
cv::solve(X, y, b, cv::DECOMP_SVD);

For the latter, a quick check showed same results between MKL and internal SVD function.
Again, I would dump X values in full precision and compare the results between the different libraries.
Matlab, Numpy, all use Lapack for scientific routines. So normally all results should match. If it does not match, you have the possible source of your issue.
If the issue comes from OpenCV solve(), I don't see how it could be since it should just involves SVD and matrix multiplication. Maybe the other libs use some kind of data normalisation?
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Anyway, for scientific computing I am not qualified. These are my advices for debugging.
		</comment>
		<comment id='11' author='StefanoD' date='2020-07-14T21:34:10Z'>
		Btw: Just to mention: Actually OpenBLAS was installed, but not recognized on Ubuntu 20.04.
&lt;denchmark-code&gt;apt search libopenblas-dev
libopenblas-dev/focal-updates,now 0.3.8+ds-1ubuntu0.20.04.1 amd64  [installiert]
  Optimized BLAS (linear algebra) library (dev, meta)
&lt;/denchmark-code&gt;

installiert means "installed".
Compiling OpenCV printed following:
&lt;denchmark-code&gt;-- Could not find OpenBLAS include. Turning OpenBLAS_FOUND off
-- Could not find OpenBLAS lib. Turning OpenBLAS_FOUND off
&lt;/denchmark-code&gt;

Anyway: Thx for your help! I think this ticket can be closed.
		</comment>
		<comment id='12' author='StefanoD' date='2020-07-16T19:02:32Z'>
		&lt;denchmark-link:https://github.com/catree&gt;@catree&lt;/denchmark-link&gt;

Shall I close this ticket?
		</comment>
		<comment id='13' author='StefanoD' date='2020-07-17T08:29:05Z'>
		&lt;denchmark-link:https://github.com/StefanoD&gt;@StefanoD&lt;/denchmark-link&gt;

It is up to you.

Instabilities in cv::solve() on small numbers

There is an issue in OpenCV if cv::solve(X, y, b, cv::DECOMP_SVD); gives incorrect results while replacing the call by Eigen or GSL it gives correct results.
		</comment>
		<comment id='14' author='StefanoD' date='2020-07-17T13:17:17Z'>
		GSL and armadillo are delivering the correct result by default.
I will close this issue, because even Intel MKL and Eigen can't deliver the correct result without scaling and this is a kind of default pre-processing step which many libraries are expecting.
		</comment>
		<comment id='15' author='StefanoD' date='2020-07-17T17:20:11Z'>
		&lt;denchmark-link:https://github.com/StefanoD&gt;@StefanoD&lt;/denchmark-link&gt;

Can you add which functions you are using to replace  for GSL and armadillo?
		</comment>
		<comment id='16' author='StefanoD' date='2020-07-17T19:50:05Z'>
		&lt;denchmark-link:https://github.com/catree&gt;@catree&lt;/denchmark-link&gt;

For armadillo, see this &lt;denchmark-link:https://github.com/opencv/opencv/issues/17824#issuecomment-658412674&gt;comment&lt;/denchmark-link&gt;
 with two different ways.
For GSL I used . Maybe I post the GSL example on monday.
		</comment>
	</comments>
</bug>