<bug id='11809' author='svebert' open_date='2018-06-22T11:58:32Z' closed_time='2018-06-29T13:19:47Z'>
	<summary>DNN module: ConcatLayer: wrong axis attribute</summary>
	<description>
&lt;denchmark-h:h5&gt;System information (version)&lt;/denchmark-h&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/39772342/41774893-da9b2c9a-7621-11e8-825c-4ce3be8a5cf0.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/opencv/opencv/files/2127509/frozen_inference_graph.zip&gt;frozen_inference_graph.zip&lt;/denchmark-link&gt;


OpenCV =&gt;4
Operating System / Platform =&gt;Win10 64bit
Compiler =&gt; vc14

&lt;denchmark-h:h5&gt;Detailed description&lt;/denchmark-h&gt;

When I use the Concatenate-Layer in keras, which will lead to a tensorflow "ConcatV2"-Layer, the axis parameter is not read in correctly in OpenCV-DNN. I specify axis=1, and opencv reads in axis=2. As there is no axis 2, i get a "vector is out of bounds error".
When I debug opencv, then something goes wrong in line 1016 in tf_importer.cpp
int axis = getConstBlob(layer, value_id, axisId).int_val().Get(0);
layerParams.set("axis", 0 &lt;= axis &amp;&amp; axis &lt; 4 ? toNCHW(axis) : axis);
In line 1015, the axis gets read in correctly, but layerParams.set, fails to set this value to the layerParameters.
Therefore in the forward-step, the program crashes in the concat_layer.cpp at line 98:
axisSum += curShape[cAxis];
cAxis = 2, but it should be at max 1.
I think the LayerParams.blob is not stored correctly or something like that.
To reproduce my issue, please unzip my "frozen_inference-graph" and try
cv::Mat oImageIn = &lt;-load image with 128x128x3; or use cv::Mat::zeros(cv::Size(128,128),CV_8UC3)
cv::dnn::Net oNet = cv::dnn::readNetFromTensorflow(sPathToTensorflowModel_PB);
cv::Mat oBlob= cv::dnn::blobFromImage(oImageIn, 1. / 255, cv::Size(128, 128));
oNet .setInput(oBlob);
cv::Mat output = oNet.forward();
the program will crash in the forward-step. Please see the following code for producing my frozen inference graph.
&lt;denchmark-h:h5&gt;Steps to reproduce&lt;/denchmark-h&gt;

import tensorflow as tf
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import Input, Conv2D, Flatten, Concatenate
from tensorflow.python.keras import backend as K
from tensorflow.python.keras.models import load_model
from tensorflow.python.tools import optimize_for_inference_lib
from tensorflow.tools.graph_transforms import TransformGraph

import numpy as np
import os

def build_model():
	inputShape = (128, 128, 3)
	inp = Input(shape=inputShape)
	x=Conv2D(8, (3, 3), padding='same', name='conv1')(inp)
	y=Conv2D(8, (1, 1), padding='same', name='conv2')(inp)
	x=Flatten()(x)
	y=Flatten()(y)
	pred = Concatenate(axis=1)([x,y])
	return Model(inputs = inp, outputs=pred)

def keras2cv(outputPath='',inputModelPath='test.hd5',customObjects={}):
	model = load_model(inputModelPath,custom_objects=customObjects)
	if(len(outputPath)&gt;0):
		if(outputPath[len(outputPath)-1]!="/" or outputPath[len(outputPath)-1]!="\\"):
			outputPath+="/"
	convertForDeployment(model,bWriteIntermediate=True, outputPath = outputPath)

def loadModel(file='trained_model.h5',customObjects={}):
	model = load_model(file,custom_objects=customObjects)

def convertForDeployment(model, bWriteIntermediate = False, outputPath = '', bOptimize=False):
	#get session	
	K.set_learning_phase(0)
	sess = K.get_session()
	#get out/input names from model
	output_names =  [model.output.name.split(':')[0]]
	input_names = model.input_names
	input_shape = model.input.shape
	print(model.uses_learning_phase)
	#freeze graph
	graph_def = sess.graph.as_graph_def(add_shapes=True)
	graph_def = tf.graph_util.convert_variables_to_constants(sess, graph_def,output_names)

	# Print the graph nodes
	print('\n'.join(node.name for node in graph_def.node))

	transforms = ["fold_batch_norms(ignore_errors=True)","fold_old_batch_norms(ignore_errors=True)","remove_nodes(op=PlaceholderWithDefault)", "strip_unused_nodes(type=float, shape=\"1"+str(input_shape[1])+","+str(input_shape[2])+","+str(input_shape[3])+"\")", "sort_by_execution_order"]
	graph_def_transformed = TransformGraph(graph_def,input_names, output_names, transforms)
	#write resulting frozen graph
	tf.train.write_graph(graph_def_transformed, '', outputPath+'frozen_inference_graph.pb', as_text=False)

	shape = model.input.shape
	shape_np=[1]
	for i in range(1,len(shape)):
		shape_np.append(int(shape[i]))
	inp = np.random.standard_normal(shape_np).astype(np.float32)
	tf_out = model.predict(inp)	
	print(tf_out)

config = tf.ConfigProto()
config.gpu_options.allow_growth = True
sess = tf.Session(config=config)
K.set_session(sess)

namePrefix='anchorboxes'
model = build_model()
#model = train(model)
model.save(namePrefix+'_'+'test.hd5',True,False)

K.clear_session()
config = tf.ConfigProto()
config.gpu_options.allow_growth = True
sess = tf.Session(config=config)
K.set_session(sess)

K.set_learning_phase(0)

model = build_model()
model.load_weights(namePrefix+'_'+'test.hd5')
model.save(namePrefix+'_'+'test.hd5',True,False)

keras2cv('',namePrefix+'_'+'test.hd5')
	</description>
	<comments>
		<comment id='1' author='svebert' date='2018-06-25T05:59:14Z'>
		&lt;denchmark-link:https://github.com/svebert&gt;@svebert&lt;/denchmark-link&gt;
, As a workaround you may specify a negative axis value (-1). Current implementation uses it without remapping between data layouts.
		</comment>
		<comment id='2' author='svebert' date='2018-06-25T12:50:32Z'>
		&lt;denchmark-link:https://github.com/svebert&gt;@svebert&lt;/denchmark-link&gt;
, Thank you for pointing this issue! Can you test a PR &lt;denchmark-link:https://github.com/opencv/opencv/pull/11826&gt;#11826&lt;/denchmark-link&gt;
 which fixes it?
		</comment>
		<comment id='3' author='svebert' date='2018-06-28T09:38:36Z'>
		&lt;denchmark-link:https://github.com/dkurt&gt;@dkurt&lt;/denchmark-link&gt;
 , I tested your Code with the minimal example above, and it is working well.
But, unfortunatley a more complex model does not work..
For the more complex model in tf_importer.cpp at line 1085 data_layouts[name] == DATA_LAYOUT_NHWC is true for the Concatenate-Layer. But it should not?
Please find the "more complex model" in the atteched zip file.
&lt;denchmark-link:https://github.com/opencv/opencv/files/2144662/frozen_inference_graph.zip&gt;frozen_inference_graph.zip&lt;/denchmark-link&gt;

Edit: Can I somehow specify in the tensorflow/keras model the NCHW/NHWC type?
		</comment>
		<comment id='4' author='svebert' date='2018-06-28T10:33:12Z'>
		&lt;denchmark-link:https://github.com/svebert&gt;@svebert&lt;/denchmark-link&gt;
, thanks! Can confirm it. Please wait a PR.
		</comment>
		<comment id='5' author='svebert' date='2018-06-29T06:55:39Z'>
		&lt;denchmark-link:https://github.com/svebert&gt;@svebert&lt;/denchmark-link&gt;
, please check a PR &lt;denchmark-link:https://github.com/opencv/opencv/pull/11854&gt;#11854&lt;/denchmark-link&gt;
. This is my test:
import tensorflow as tf
import cv2 as cv
import numpy as np

graph = 'frozen_inference_graph.pb'
with tf.gfile.FastGFile(graph) as f:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(f.read())

np.random.seed(223)
inp = np.random.standard_normal([1, 224, 224, 3]).astype(np.float32)
with tf.Session() as sess:
    sess.graph.as_default()
    tf.import_graph_def(graph_def, name='')

    tfOut = sess.run(sess.graph.get_tensor_by_name('dense_2/BiasAdd:0'),
                   feed_dict={'input_1_1:0': inp})
    print tfOut.shape

cvNet = cv.dnn.readNet(graph)
cvNet.setInput(inp.transpose(0, 3, 1, 2))
cvNet.enableFusion(False)
cvOut = cvNet.forward('dense_2/MatMul')
print np.max(np.abs(cvOut - tfOut))

print cvOut
print tfOut
&lt;denchmark-code&gt;[[-65.80359   -27.910795  -54.921364  -27.604753  -27.709885   -2.723682
  -28.844164   -7.1437993 -25.040667  -18.016867  -27.615324  -25.195995
   -9.184336  -27.989994  -37.43037    -2.2433722 -21.976744  -23.972857
  -37.575256  -46.84014    -3.0007439 -37.980648  -13.265318  -16.91342
   -7.293052  -23.003624  -28.80726   -21.175913 ]]
[[-65.80357   -27.910793  -54.921356  -27.604755  -27.709877   -2.7236712
  -28.844166   -7.1437936 -25.04066   -18.016867  -27.61533   -25.195993
   -9.184328  -27.989996  -37.43037    -2.2433808 -21.976738  -23.97286
  -37.57524   -46.840137   -3.000749  -37.980644  -13.265319  -16.913416
   -7.2930493 -23.003618  -28.807251  -21.17591  ]]
&lt;/denchmark-code&gt;

This model is terminated by ReLU which makes all that numbers are zeros so it's output from the layer before it.
		</comment>
		<comment id='6' author='svebert' date='2018-06-29T13:19:47Z'>
		&lt;denchmark-link:https://github.com/dkurt&gt;@dkurt&lt;/denchmark-link&gt;
   Thanks again! The PR fixes my problem. Regarding the ReLU, you are right, I am still in the progress of designing my model...
However, now all parts I need (at the moment ;) ) are working.
		</comment>
	</comments>
</bug>