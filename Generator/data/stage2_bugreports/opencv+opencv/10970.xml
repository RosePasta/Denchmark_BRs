<bug id='10970' author='jstumpin' open_date='2018-03-01T10:01:33Z' closed_time='2018-03-09T10:07:15Z'>
	<summary>Wrong output when using DNN_TARGET_OPENCL with blobFromImages</summary>
	<description>
System information (version)

OpenCV =&gt; 3.4.1
Operating System / Platform =&gt; Windows 10 (x64)
Compiler =&gt; Visual Studio 2017 (v140)
iGPU =&gt; Intel Iris Plus Graphics 650 (23.20.16.4901)

Detailed description
DNN's  (using GoogleNet as a test case) produces wrong classification label and unbounded probability (i.e. &gt;1) when  is paired with . I'm uncertain whether or not it is a driver issue as pointed out &lt;denchmark-link:https://github.com/01org/caffe/issues/63#issuecomment-325092900&gt;here&lt;/denchmark-link&gt;
 because either  or  works fine in solitary (e.g.  +  or  + ). Since Intel's CaffeCL doesn't have a problem with batch processing, it's highly unlikely that driver is the issue.
Steps to reproduce
Code:
&lt;denchmark-code&gt;...
//using fine-tuned GoogleNet with 6 classes.
...
net.setPreferableTarget(DNN_TARGET_OPENCL);
inputBlob = blobFromImages(imgs, 1.0f, Size(), Scalar(), false); 
net.setInput(inputBlob, "data"); 
prob = net.forward("softmax");
for (int i = 0; i &lt; imgs.size(); ++i)
{
   int classId;
   double classProb;
   getMaxClass(prob.row(i), &amp;classId, &amp;classProb);
   std::vector&lt;String&gt; classNames = readClassNames();
   std::cout &lt;&lt; "---------- Prediction for " &lt;&lt; labels[i] &lt;&lt; " ----------" &lt;&lt; std::endl;
   std::cout &lt;&lt; "Best class: #" &lt;&lt; classId &lt;&lt; " '" &lt;&lt; classNames.at(classId) &lt;&lt; "'" &lt;&lt; std::endl;
   std::cout &lt;&lt; "Probability: " &lt;&lt; classProb * 100 &lt;&lt; "%" &lt;&lt; std::endl;
}
...
&lt;/denchmark-code&gt;

Output:

---------- Prediction for c6.jpg ----------
Best class: &lt;denchmark-link:https://github.com/opencv/opencv/pull/5&gt;#5&lt;/denchmark-link&gt;
 'c6'
Probability: 99.9999%
---------- Prediction for c6.jpg ----------
Best class: &lt;denchmark-link:https://github.com/opencv/opencv/pull/3&gt;#3&lt;/denchmark-link&gt;
 'c4'
Probability: 297.147%
---------- Prediction for c6.jpg ----------
Best class: #0 'c1'
Probability: 214.173%
---------- Prediction for c6.jpg ----------
Best class: &lt;denchmark-link:https://github.com/opencv/opencv/pull/3&gt;#3&lt;/denchmark-link&gt;
 'c4'
Probability: 246.445%
---------- Prediction for c6.jpg ----------
Best class: &lt;denchmark-link:https://github.com/opencv/opencv/pull/4&gt;#4&lt;/denchmark-link&gt;
 'c5'
Probability: 377.985%
---------- Prediction for c6.jpg ----------
Best class: #0 'c1'
Probability: 131.051%
---------- Prediction for c6.jpg ----------
Best class: &lt;denchmark-link:https://github.com/opencv/opencv/pull/5&gt;#5&lt;/denchmark-link&gt;
 'c6'
Probability: 150.404%
---------- Prediction for c6.jpg ----------
Best class: &lt;denchmark-link:https://github.com/opencv/opencv/pull/2&gt;#2&lt;/denchmark-link&gt;
 'c3'
Probability: 177.241%
---------- Prediction for c6.jpg ----------
Best class: &lt;denchmark-link:https://github.com/opencv/opencv/pull/4&gt;#4&lt;/denchmark-link&gt;
 'c5'
Probability: 81.1507%
---------- Prediction for c6.jpg ----------
Best class: &lt;denchmark-link:https://github.com/opencv/opencv/pull/5&gt;#5&lt;/denchmark-link&gt;
 'c6 '
Probability: 409.997%

---------- Prediction for c6.jpg ----------
Best class: &lt;denchmark-link:https://github.com/opencv/opencv/pull/5&gt;#5&lt;/denchmark-link&gt;
 'c6'
Probability: 99.9998%
---------- Prediction for c6.jpg ----------
Best class: &lt;denchmark-link:https://github.com/opencv/opencv/pull/5&gt;#5&lt;/denchmark-link&gt;
 'c6'
Probability: 100%
---------- Prediction for c6.jpg ----------
Best class: &lt;denchmark-link:https://github.com/opencv/opencv/pull/5&gt;#5&lt;/denchmark-link&gt;
 'c6'
Probability: 100%
---------- Prediction for c6.jpg ----------
Best class: &lt;denchmark-link:https://github.com/opencv/opencv/pull/5&gt;#5&lt;/denchmark-link&gt;
 'c6'
Probability: 100%
---------- Prediction for c6.jpg ----------
Best class: &lt;denchmark-link:https://github.com/opencv/opencv/pull/5&gt;#5&lt;/denchmark-link&gt;
 'c6'
Probability: 100%
---------- Prediction for c6.jpg ----------
Best class: &lt;denchmark-link:https://github.com/opencv/opencv/pull/5&gt;#5&lt;/denchmark-link&gt;
 'c6'
Probability: 99.9999%
---------- Prediction for c6.jpg ----------
Best class: &lt;denchmark-link:https://github.com/opencv/opencv/pull/5&gt;#5&lt;/denchmark-link&gt;
 'c6'
Probability: 100%
---------- Prediction for c6.jpg ----------
Best class: &lt;denchmark-link:https://github.com/opencv/opencv/pull/5&gt;#5&lt;/denchmark-link&gt;
 'c6'
Probability: 100%
---------- Prediction for c6.jpg ----------
Best class: &lt;denchmark-link:https://github.com/opencv/opencv/pull/5&gt;#5&lt;/denchmark-link&gt;
 'c6'
Probability: 100%
---------- Prediction for c6.jpg ----------
Best class: &lt;denchmark-link:https://github.com/opencv/opencv/pull/5&gt;#5&lt;/denchmark-link&gt;
 'c6'
Probability: 100%
	</description>
	<comments>
		<comment id='1' author='jstumpin' date='2018-03-06T12:46:24Z'>
		&lt;denchmark-link:https://github.com/jstumpin&gt;@jstumpin&lt;/denchmark-link&gt;
, Thank you! If I'm right this bug could be reproduced only if you run the same  object using blobs with different batch sizes. In example,
np.random.seed(2701)
img = np.random.standard_normal([224, 224, 3]).astype(np.float32)

net = cv.dnn.readNetFromCaffe('bvlc_googlenet.prototxt', 'bvlc_googlenet.caffemodel')
net.setPreferableTarget(cv.dnn.DNN_TARGET_OPENCL)

blob = cv.dnn.blobFromImage(img, 1.0, (224, 224), (0, 0, 0), False, False)
net.setInput(blob)
out = net.forward()
print out[:, 0:5]
produces [[ 0.00046398  0.00062906  0.00050327  0.0016055   0.00496788]] and that's OK. Then let's try to run network on batch size 2 from the beginning.
np.random.seed(2701)
img = np.random.standard_normal([224, 224, 3]).astype(np.float32)

net = cv.dnn.readNetFromCaffe('bvlc_googlenet.prototxt', 'bvlc_googlenet.caffemodel')
net.setPreferableTarget(cv.dnn.DNN_TARGET_OPENCL)

blob = cv.dnn.blobFromImages([img, img], 1.0, (224, 224), (0, 0, 0), False, False)
net.setInput(blob)
out = net.forward()
print out[:, 0:5]
output:
&lt;denchmark-code&gt;[[ 0.00046398  0.00062906  0.00050327  0.0016055   0.00496788]
 [ 0.00046398  0.00062906  0.00050327  0.0016055   0.00496788]]
&lt;/denchmark-code&gt;

But if we change a batch size after the first forward pass,
np.random.seed(2701)
img = np.random.standard_normal([224, 224, 3]).astype(np.float32)

net = cv.dnn.readNetFromCaffe('bvlc_googlenet.prototxt', 'bvlc_googlenet.caffemodel')
net.setPreferableTarget(cv.dnn.DNN_TARGET_OPENCL)

blob = cv.dnn.blobFromImage(img, 1.0, (224, 224), (0, 0, 0), False, False)
net.setInput(blob)
out = net.forward()

blob = cv.dnn.blobFromImages([img, img], 1.0, (224, 224), (0, 0, 0), False, False)
net.setInput(blob)
out = net.forward()
print out[:, 0:5]
Number of correct samples equals batch size from the first run (batch size 1 here):
&lt;denchmark-code&gt;[[ 0.00014901  0.00034072  0.00020369  0.00052105  0.00425998]
 [ 0.01349979  0.          0.          0.          0.        ]]
&lt;/denchmark-code&gt;

So the problem is that at the first  pass network was initialized correctly but when input shape changed and network reinitialized some of layers still works for batch size 1. There are two possible solutions: work with the same batch size all the time or apply small changes to SoftMax layer that can fix it: &lt;denchmark-link:https://github.com/opencv/opencv/pull/10992/files#diff-acc9c5de67036cf516ae66327919caf6R96&gt;https://github.com/opencv/opencv/pull/10992/files#diff-acc9c5de67036cf516ae66327919caf6R96&lt;/denchmark-link&gt;
 .
		</comment>
		<comment id='2' author='jstumpin' date='2018-03-07T04:45:05Z'>
		My bad for not elaborating on the code snippets. Indeed, I have a warm-up session prior to the actual forward pass which only consumed a single image. Wouldn't have known it to cause the problem since I did not face it with CaffeCL, albeit sharing the procedure. Although the first possible solution instantly fixed the issue, I can't say the same for the latter. The patch produces a constant pair of (mis)classification label and probability (e.g. "c2" &amp; "16.9916%"). Varying batch sizes should be a norm for real-time query-based applications.
		</comment>
		<comment id='3' author='jstumpin' date='2018-03-07T11:18:23Z'>
		&lt;denchmark-link:https://github.com/jstumpin&gt;@jstumpin&lt;/denchmark-link&gt;
, Let me disagree with you. You need to store output of each layer somewhere and varying batch size you change shapes of layers' outputs. In example, with batch size = 1 network produces output  but for batch size 2 it produces . Even with advanced memory management reallocations may happen. Moreover you need to reinitialize backends or recompile computation kernels if they depends on shape.
		</comment>
		<comment id='4' author='jstumpin' date='2018-03-07T14:15:57Z'>
		&lt;denchmark-link:https://github.com/jstumpin&gt;@jstumpin&lt;/denchmark-link&gt;
, Please check updated PR &lt;denchmark-link:https://github.com/opencv/opencv/pull/10992&gt;#10992&lt;/denchmark-link&gt;
. Now that works correct. Thank you!
		</comment>
		<comment id='5' author='jstumpin' date='2018-03-08T02:16:29Z'>
		Thanks for the quick fix. Good news is, it simply works. Bad news is, forward pass seems abit slower than before, ~5ms difference. Previously the performance is at par with CaffeCL (with no layer fusion and using floating point precision). However, performance for the detection network (SSD-VGG) is still ~40-50% slower (given the same handicap as the above comparison). I guess we need to bridge this performance gap first before implementing fp16 precision as what it is envisioned for OpenCV 4.
		</comment>
		<comment id='6' author='jstumpin' date='2018-03-08T05:55:40Z'>
		&lt;denchmark-link:https://github.com/jstumpin&gt;@jstumpin&lt;/denchmark-link&gt;
, can you share a code that measures efficiency to reproduce it?
		</comment>
		<comment id='7' author='jstumpin' date='2018-03-08T07:21:58Z'>
		Prior to the PR, there is no latency regardless using fixed or varied batch sizes. Do note that the latency only occurs when varied batch sizes is in place. Here's the code for measurement:
&lt;denchmark-code&gt;long long milliseconds_now() {
	static LARGE_INTEGER s_frequency;
	static BOOL s_use_qpc = QueryPerformanceFrequency(&amp;s_frequency);
	if (s_use_qpc) {
		LARGE_INTEGER now;
		QueryPerformanceCounter(&amp;now);
		return (1000LL * now.QuadPart) / s_frequency.QuadPart;
	}
	else {
		return GetTickCount();
	}
}
...
start = milliseconds_now();
prob = net.forward("softmax");
elapsed = milliseconds_now() - start;
&lt;/denchmark-code&gt;

Average forward pass for 220 images on GoogleNet:

Fixed batch size (10 for warmup, 10 for actual): ~0.010041s
Varied batch size (1 for warmup, 10 for actual): ~0.015214s

		</comment>
	</comments>
</bug>