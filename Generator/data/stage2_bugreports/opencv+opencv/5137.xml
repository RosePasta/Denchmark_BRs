<bug id='5137' author='LorenaGdL' open_date='2015-08-06T11:10:06Z' closed_time='2015-08-12T14:41:40Z'>
	<summary>SVM error when input data size is too large (exception in calc_non_rbf_base function)</summary>
	<description>
When using large training datasets (combination of number of samples and number of features per sample), the SVM train function throws and exception. Specifically, the exception arises in calc_non_rbf_base function from svm.cpp source file.
Major findings about the issue:

Caused when the used combination of large number of samples and large number of features per sample goes above some threshold.
Independent on input data values.
Dependent on the proportion of positive and negative samples in dataset.

More info about the issue, including tests, results and screenshots can be found in this Q&amp;A post: &lt;denchmark-link:http://answers.opencv.org/question/67137/svm-error-when-input-data-size-is-too-large-calc_non_rbf_base/&gt;http://answers.opencv.org/question/67137/svm-error-when-input-data-size-is-too-large-calc_non_rbf_base/&lt;/denchmark-link&gt;

System information: OpenCV 2.4.12 (build with CMake, basic options enabled), Visual Studio 2013, Windows 7 64 bits, 16GB RAM
Code to reproduce the issue can be found here: &lt;denchmark-link:https://gist.github.com/LorenaGdL/526bad96ea35c982eec4&gt;https://gist.github.com/LorenaGdL/526bad96ea35c982eec4&lt;/denchmark-link&gt;

If you need any other info, feel absolutely free to ask. Thank you all.
	</description>
	<comments>
		<comment id='1' author='LorenaGdL' date='2015-08-12T14:41:40Z'>
		Should be fixed, please check.
		</comment>
		<comment id='2' author='LorenaGdL' date='2015-08-13T09:50:53Z'>
		Seems it works like a charm! Thank you so so much :)
		</comment>
	</comments>
</bug>