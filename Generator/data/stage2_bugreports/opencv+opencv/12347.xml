<bug id='12347' author='hrasti' open_date='2018-08-30T11:36:33Z' closed_time='2018-09-04T03:27:07Z'>
	<summary>Number of neighbors in knn classification</summary>
	<description>
&lt;denchmark-h:h5&gt;System information (version)&lt;/denchmark-h&gt;


OpenCV =&gt; 3.4.2
Operating System / Platform =&gt; Windows 64 Bit
Python: 3.6.5

In KNN classification usually number of neighbors is considered to be less than the number of samples. However, if you use all samples as neighborhood size (k &gt;= number of patterns) the function will always return the first class as the label for new patterns.
&lt;denchmark-code&gt;#import needed libraries'
import cv2.cv2 as cv
import numpy as np
from matplotlib import pyplot as plt

#Feature Vectors: Class 1
trainSample1 = np.random.randint(10,100,(100,2))
trainSample1 = np.float32(trainSample1)
x = trainSample1[:,0]
y = trainSample1[:,1]
plt.scatter(x, y, 7, 'b','o')

#Feature Vectors: Class 2
trainSample2 = np.random.randint(60,150,(100,2))
trainSample2 = np.float32(trainSample2)
x = trainSample2[:,0]
y = trainSample2[:,1]
plt.scatter(x, y, 7, 'r','^')

#Feature Vectors: Class 3
trainSample3 = np.random.randint(100,200,(100,2))
trainSample3 = np.float32(trainSample3)
x = trainSample3[:,0]
y = trainSample3[:,1]
plt.scatter(x, y, 7, 'm','+')

#Append Samples
allSamplesFeatures = trainSample1
allSamplesFeatures = np.append(allSamplesFeatures,trainSample2,axis=0)
allSamplesFeatures = np.append(allSamplesFeatures,trainSample3,axis=0)

#Targets of All Classes
allTargetFeatures = np.ones((100,1))
allTargetFeatures = np.append(allTargetFeatures, np.ones((100,1))*2,axis=0)
allTargetFeatures = np.append(allTargetFeatures, np.ones((100,1))*3,axis=0)
allTargetFeatures = np.float32(allTargetFeatures)

#Generate New Commers
newPoints = np.float32(np.random.randint(10,200,(50,2)))
plt.scatter(newPoints[:,0], newPoints[:,1], 15, 'g','*')

#Create KNN Classification Model
knn = cv.ml.KNearest_create()
knn.train(allSamplesFeatures, cv.ml.ROW_SAMPLE, allTargetFeatures)

#Classifying New Commers
NeighborSize = 300
ret, results, neighbours, dist = knn.findNearest(newPoints, NeighborSize)

#Plot and Print the Results
plt.show()
print("  ----&gt;&gt;&gt;&gt;&gt;" , results , '\n')
&lt;/denchmark-code&gt;

I tested using different class numbers and features. The result was always the same for K&gt;=#samples
I think the number of neighborhoods should be limited smaller than the "len(patterns)" and adding a simple condition at the beginning of "findNearest" function is necessary.
	</description>
	<comments>
		<comment id='1' author='hrasti' date='2018-08-31T08:47:47Z'>
		i could reproduce your problem (from c++):
&lt;denchmark-code&gt;    Mat xTrainData = (Mat_&lt;float&gt;(5,2) &lt;&lt; 1, 1.1, 1.1, 1, 2, 2, 2.1, 2, 2.1, 2.1);
    Mat yTrainLabels = (Mat_&lt;float&gt;(5,1) &lt;&lt; 1, 1, 2, 2, 2);
    Ptr&lt;KNearest&gt; knnKdt = KNearest::create();
    knnKdt-&gt;train(xTrainData, ml::ROW_SAMPLE, yTrainLabels);

    Mat xTestData = (Mat_&lt;float&gt;(2,2) &lt;&lt; 1.1, 1.1, 2, 2.2);
    Mat zBestLabels, neighbours, dist;
    int K = 16; // but 5 train samples only !!
    knnKdt-&gt;findNearest(xTestData, K, zBestLabels, neighbours, dist);
    cout &lt;&lt; zBestLabels &lt;&lt; endl;
    cout &lt;&lt; neighbours &lt;&lt; endl;
    cout &lt;&lt; dist &lt;&lt; endl;

[0;
 0]
[1, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0;
 2, 2, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[0.010000004, 0.010000004, 1.6199999, 1.8099997, 1.9999995, 3.4028235e+038, 3.4028235e+038, 3.4028235e+038, 3.4028235e+038, 3.4028235e+038, 3.4028235e+038, 3.4028235e+0
38, 3.4028235e+038, 3.4028235e+038, 3.4028235e+038, 3.4028235e+038;
 0.020000011, 0.040000018, 0.049999997, 2.21, 2.25, 3.4028235e+038, 3.4028235e+038, 3.4028235e+038, 3.4028235e+038, 3.4028235e+038, 3.4028235e+038, 3.4028235e+038, 3.40
28235e+038, 3.4028235e+038, 3.4028235e+038, 3.4028235e+038]
&lt;/denchmark-code&gt;

and you can imagine, what the majority vote of the neighbours results into ...

I think the number of neighborhoods should be limited smaller than the "len(patterns)" and adding a simple condition at the beginning of "findNearest" function is necessary.

there already is a condition like this here:



opencv/modules/ml/src/knearest.cpp


         Line 149
      in
      90f47eb






 int k = std::min(k0, nsamples); 





but it's applied far too late, the neighbours and dists are already allocated with wrong sizes.
if we move the check here:
&lt;denchmark-link:https://github.com/opencv/opencv/blob/master/modules/ml/src/knearest.cpp#L309&gt;https://github.com/opencv/opencv/blob/master/modules/ml/src/knearest.cpp#L309&lt;/denchmark-link&gt;

(and add one for the (still broken!!) kdtree),
we get correct results (still with K=16, and the code above):
&lt;denchmark-code&gt;[2;
 2]
[1, 1, 2, 2, 2;
 2, 2, 2, 1, 1]
[0.010000004, 0.010000004, 1.6199999, 1.8099997, 1.9999995;
 0.020000011, 0.040000018, 0.049999997, 2.21, 2.25]
&lt;/denchmark-code&gt;

note, that this is still a "bad" , but "correct" result (due to the large number of "bad" neighbours).
for K=2, we get the correct [1,2] solution.
		</comment>
		<comment id='2' author='hrasti' date='2018-08-31T14:57:04Z'>
		Thank you &lt;denchmark-link:https://github.com/berak&gt;@berak&lt;/denchmark-link&gt;
 for answering. However, I am using python 3.6.5 and opencv 3.4.2. I checked the KNN classification using different Ks, and in the cases which k was bigger than or equal to the number of samples, the function always returned the first class as the label.
		</comment>
		<comment id='3' author='hrasti' date='2018-08-31T15:11:08Z'>
		&lt;denchmark-link:https://github.com/hrasti&gt;@hrasti&lt;/denchmark-link&gt;
, this pr went into 3.4, so you will need to update the opencv src and recompile your cv2
(or wait for the next 3.4.4 release, if you're using PIP).
apart from that: this pr only changed correct "clamping" of K to the number of train samples available, so no more bogus values go into the calculation.
however, you can only expect it to return a correct result as long as K is (significantly) smaller than the average sample count per class (100 in your example),
else, (even if all works correctly) there have to be samples of other classes in the majority voting.
let me put it this way:
IF i conviced you, that this pr fixes your problem, please kindly close the issue.
otherwise, please come back with a more detailled explanation of what you think is missing.
		</comment>
		<comment id='4' author='hrasti' date='2018-09-01T22:24:31Z'>
		Dear &lt;denchmark-link:https://github.com/berak&gt;@berak&lt;/denchmark-link&gt;

Thank you for the explanation. So the function is not modified KNN. I mistakenly expected the modified KNN which consider the distances as weights. I checked by different number of input samples and the majority voting in the function do great. However, the point is whenever the number of samples in the neighborhood from different classes are equal the function always return the first input class.
I change the order of the input classes and label was always the first input class. Considering any k size, for each new data, if the number of neighbors of different classes are equal, which means the majority voting can not make a logical decision, the first input class is always selected as the winner label?
In these cases, In my opinion,  it would be better if the class label could be chosen randomly. Otherwise, I am not sure, but I guess the results may be biased.
		</comment>
	</comments>
</bug>