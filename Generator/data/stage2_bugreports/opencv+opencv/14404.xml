<bug id='14404' author='shelkesagar29' open_date='2019-04-23T22:14:46Z' closed_time='2019-04-30T07:58:25Z'>
	<summary>Batch inference not working when InferenecEngine as backend</summary>
	<description>
&lt;denchmark-h:h5&gt;System information (version)&lt;/denchmark-h&gt;


OpenCV =&gt; 4.0.1
Operating System / Platform =&gt; Ubuntu 16.04
Compiler =&gt; gcc

&lt;denchmark-h:h5&gt;Detailed description&lt;/denchmark-h&gt;

Hi,
I am using OpenCV DNN module for NN inference. Inference on single image, i.e. batch size of 1 works fine but when I try to use more than one images in inference batch, it fails giving error
what(): OpenCV(4.0.1-openvino) /home/jenkins/workspace/OpenCV/OpenVINO/build/opencv/modules/dnn/src/op_inf_engine.cpp:553: error: (-215:Assertion failed) Failed to initialize Inference Engine backend: Input blob size is not equal network input size (1350000!=270000). in function 'initPlugin'
I am using blobFromImages instead of blobFromImage. Blob has 'batch_size' images in it but just inference doesn't work.
I know OpenVINO API has SetBatch() method.
Should I instead switch to OpenVINO API for inference?
Thanks for your time.
	</description>
	<comments>
		<comment id='1' author='shelkesagar29' date='2019-04-24T06:00:20Z'>
		Please provide more details. Do you use a model in native format or IR with .XML and .bin files?
		</comment>
		<comment id='2' author='shelkesagar29' date='2019-04-24T09:11:53Z'>
		&lt;denchmark-link:https://github.com/shelkesagar29&gt;@shelkesagar29&lt;/denchmark-link&gt;
, Thank you for the report! Please test the changes from PR &lt;denchmark-link:https://github.com/opencv/opencv/pull/14407&gt;#14407&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='3' author='shelkesagar29' date='2019-04-24T14:32:29Z'>
		&lt;denchmark-link:https://github.com/dkurt&gt;@dkurt&lt;/denchmark-link&gt;
 I was using IR format with .xml and .bin files.
Somehow, with normal OpenCV backend, batch inference works.
		</comment>
		<comment id='4' author='shelkesagar29' date='2019-04-24T14:38:10Z'>
		&lt;denchmark-link:https://github.com/shelkesagar29&gt;@shelkesagar29&lt;/denchmark-link&gt;
, please test the changes from PR or add a reference to IR files (if it's possible). For me only native object detection models with IE backend have similar error.
		</comment>
		<comment id='5' author='shelkesagar29' date='2019-04-24T14:43:18Z'>
		&lt;denchmark-link:https://github.com/dkurt&gt;@dkurt&lt;/denchmark-link&gt;
 I will test and let you know. Thanks for the quick response.
		</comment>
		<comment id='6' author='shelkesagar29' date='2019-04-24T21:00:23Z'>
		&lt;denchmark-link:https://github.com/dkurt&gt;@dkurt&lt;/denchmark-link&gt;
 After few experiments, I am still not able to run batch inference through OpenCV. I made the changes suggested by you and built opencv from source with inference engine.
This time to make things easy, I used SSD Mobilenet 300 IR files (shared here &lt;denchmark-link:https://drive.google.com/open?id=13NZtkv9HqdfCftA2w49LVFSttTUx5L8Q&gt;https://drive.google.com/open?id=13NZtkv9HqdfCftA2w49LVFSttTUx5L8Q&lt;/denchmark-link&gt;
) and following code snippet
`
&lt;denchmark-code&gt; #include "opencv2/core/core.hpp"
 #include "opencv2/highgui/highgui.hpp"
 #include "opencv2/imgproc/imgproc.hpp"
 #include "opencv2/dnn.hpp"

 using namespace cv;
 using namespace std;
 using namespace dnn;
Size inpSize(300,300);
Net net = readNet("./openvino_32/frozen_inference_graph.xml", 
                              "./openvino_32/frozen_inference_graph.bin");
net.setPreferableBackend(DNN_BACKEND_INFERENCE_ENGINE);
net.setPreferableTarget(DNN_TARGET_CPU);

vector&lt;Mat&gt; blob;
Mat dummy_image = imread("./openvino_32/000067.jpg");
vector&lt;Mat&gt;image_batch (3, dummy_image);
blobFromImages(image_batch, blob, 1, inpSize, Scalar(), true);
 // Run a model.
 net.setInput(blob);                                
std::vector&lt;cv::Mat&gt; detection;
net.forward(detection);
&lt;/denchmark-code&gt;

`
This code again fails with same error
what():  OpenCV(4.0.1-dev) /home/harmanx/opencv/modules/dnn/src/op_inf_engine.cpp:686: error: (-215:Assertion failed) Failed to initialize Inference Engine backend: Input blob size is not equal network input size (810000!=270000). in function 'initPlugin'
This error is from op_inf_engine.cpp file.
		</comment>
		<comment id='7' author='shelkesagar29' date='2019-04-29T08:01:26Z'>
		&lt;denchmark-link:https://github.com/shelkesagar29&gt;@shelkesagar29&lt;/denchmark-link&gt;
, It looks like the problem in the generated IR:
&lt;denchmark-code&gt;terminate called after throwing an instance of 'InferenceEngine::details::InferenceEngineException'
  what():  Failed to infer shapes for Reshape layer (BoxPredictor_5/ClassPredictor/BiasAdd/Reshape) with error: Invalid reshape mask (dim attribute): number of elements in input: [3,1,1,546] and output: [1,6,91] mismatch
&lt;/denchmark-code&gt;

So there are two options:

Try to use an origin model. OpenCV can build Inference Engine graph in runtime for it.
Modify IR:

For an every ""BoxPredictor_*/ClassPredictor/BiasAdd/Reshape"" replace target shape considering batch dimension. In example,
from
&lt;layer id="94" name="BoxPredictor_0/ClassPredictor/BiasAdd/Reshape" precision="FP32" type="Reshape"&gt;
  &lt;data dim="1,1083,91"/&gt;
to
&lt;layer id="94" name="BoxPredictor_0/ClassPredictor/BiasAdd/Reshape" precision="FP32" type="Reshape"&gt;
  &lt;data dim="-1,1083,91"/&gt;
There are 6 layers to be changed,
BoxPredictor_0/ClassPredictor/BiasAdd/Reshape
BoxPredictor_1/ClassPredictor/BiasAdd/Reshape
BoxPredictor_2/ClassPredictor/BiasAdd/Reshape
BoxPredictor_3/ClassPredictor/BiasAdd/Reshape
BoxPredictor_4/ClassPredictor/BiasAdd/Reshape
BoxPredictor_5/ClassPredictor/BiasAdd/Reshape
		</comment>
		<comment id='8' author='shelkesagar29' date='2019-04-29T20:27:37Z'>
		&lt;denchmark-link:https://github.com/dkurt&gt;@dkurt&lt;/denchmark-link&gt;
 That was the problem with IR generated with model optimizer.
Now there are two options,

Pass original .pb and .pbtxt file and let OpenCV build IR
Instead of modifying IR afterwards, we can also do one of the following
a. Pass batch size parameter (--batch N ) while generating IR using model optimizer.
b. OR pass input image size considering batch size i.e for SSD mobilenet, instated of passing [1, 300, 300, 3], we can pass [N, 300, 300, 3] to accommodate batch size N

I have tested second approach and it works fine. OpenVINO team should mention this in the documentation.
Thank yo for your time and efforts.
		</comment>
	</comments>
</bug>