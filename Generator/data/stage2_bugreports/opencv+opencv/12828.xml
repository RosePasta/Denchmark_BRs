<bug id='12828' author='tomoaki0705' open_date='2018-10-14T15:07:57Z' closed_time='2018-10-16T01:13:01Z'>
	<summary>video: kernel of OpenCL calcOpticalFlowPyrLK has race condition</summary>
	<description>
&lt;denchmark-h:h5&gt;System information (version)&lt;/denchmark-h&gt;


OpenCV =&gt; recent 3.4 branch
Operating System / Platform =&gt; Win10 64bit
Compiler =&gt; Visual Studio 2015
CUDA =&gt; 10.0
GPU =&gt; GTX 1060

related &lt;denchmark-link:https://github.com/opencv/opencv/issues/9529&gt;#9529&lt;/denchmark-link&gt;

relaetd &lt;denchmark-link:https://github.com/opencv/opencv/issues/5915&gt;#5915&lt;/denchmark-link&gt;

&lt;denchmark-h:h5&gt;Detailed description&lt;/denchmark-h&gt;

I confirmed some sporadic test failures in "PyrLKOpticalFlow.Mat"
I looked in the kernel, and realized there is a race condition.  Synchronization was inside the #if so in some condition, race happens.



opencv/modules/video/src/opencl/pyrlk.cl


        Lines 198 to 210
      in
      0101fa7






 if (tid &lt; 32) 



     { 



         smem1[tid] += smem1[tid + 32]; 



 #if WAVE_SIZE &lt; 32 



     } 



 barrier(CLK_LOCAL_MEM_FENCE); 



 if (tid &lt; 16) 



     { 



 #endif 



         smem1[tid] += smem1[tid + 16]; 



 #if WAVE_SIZE &lt;16 



     } 



 barrier(CLK_LOCAL_MEM_FENCE); 





Now, I faced similar situation in &lt;denchmark-link:https://github.com/opencv/opencv/pull/11409&gt;#11409&lt;/denchmark-link&gt;

I'm not sure where this  came from, but I removed it and modified the reduction part in more simple way.
And during the investigation, I think there is also something wrong during the upload.
Let me send my patch later.
&lt;denchmark-h:h5&gt;Steps to reproduce&lt;/denchmark-h&gt;

Run opencv_test_video on above configuration
	</description>
	<comments>
		<comment id='1' author='tomoaki0705' date='2018-10-15T00:06:45Z'>
		Additionally, it seems that there was another race condition in _prevPts
_prevPts was converted to UMat before the call to sparse (Third parameter)



opencv/modules/video/src/lkpyramid.cpp


         Line 1067
      in
      0101fa7






 return sparse(_prevImg.getUMat(), _nextImg.getUMat(), _prevPts.getUMat(), umatNextPts, umatStatus, umatErr); 





I looked in the implementation, and I think getUMat() just makes relation between the UMat and the actual memory.
When the accelarator tries to access the memory, probably a memory transfer occours, and I guess that this transfer happens asynchronously. (This is my guess)
So, the actual transfer starts when the run is called



opencv/modules/video/src/lkpyramid.cpp


         Line 1007
      in
      0101fa7






 return kernel.run(2, globalThreads, localThreads, true); // sync=true because ocl::Image2D lifetime is not handled well for temp UMat 





Now, during the test failure, I could see that some data get corrupted after kernel call.
But deleting a part of the code step by step, it seems that the when the test fails, prevPts was broken from the beginning.
So my guess is that

_prevPts is created as UMat, but the actualy memory exists on CPU
When the kernel starts, the transfer starts asynchronously
The read and write to the same memory has race condition, is read happens before the write, the data seems corrupted.

I don't have enough evidence for that, but the behavior looks so.
My patch tries copyTo before this, and that didn't cause a failure for 1000 iteration.
I'm happy to have discussion.
		</comment>
		<comment id='2' author='tomoaki0705' date='2018-10-15T00:25:06Z'>
		After all, it seems that the cause was the race condition in 
In another word, fix about the  seems not needed.
Though, it still has a race condition, so modified like &lt;denchmark-link:https://github.com/opencv/opencv/pull/11409&gt;#11409&lt;/denchmark-link&gt;

I'm happy to have discussion
		</comment>
		<comment id='3' author='tomoaki0705' date='2018-10-16T01:13:01Z'>
		resolved by &lt;denchmark-link:https://github.com/opencv/opencv/pull/12829&gt;#12829&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>