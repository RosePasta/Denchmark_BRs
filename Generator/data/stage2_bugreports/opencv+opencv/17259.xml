<bug id='17259' author='HaoLiuHust' open_date='2020-05-11T06:56:34Z' closed_time='2020-05-22T18:58:03Z'>
	<summary>same dnn model with different params, the speed is different</summary>
	<description>
I have two caffe models with same network construction but different param files. Curiously，they have very different inference speed, one is 2X times faster than the another. Why this could be happen?
	</description>
	<comments>
		<comment id='1' author='HaoLiuHust' date='2020-05-11T07:12:11Z'>
		Please provide a reproducer
		</comment>
		<comment id='2' author='HaoLiuHust' date='2020-05-11T07:33:59Z'>
		link：&lt;denchmark-link:https://pan.baidu.com/s/1aoxn6uxSolvsEYnkl2mgwQ&gt;https://pan.baidu.com/s/1aoxn6uxSolvsEYnkl2mgwQ&lt;/denchmark-link&gt;

download code：90ns
I have uploaded here, model_1.XX is the faster one. I was running it on Win10 with Win32 mode
		</comment>
		<comment id='3' author='HaoLiuHust' date='2020-05-11T07:43:49Z'>
		&lt;denchmark-link:https://github.com/HaoLiuHust&gt;@HaoLiuHust&lt;/denchmark-link&gt;
, sorry, cannot download from it. Please use Dropbox if you can.
		</comment>
		<comment id='4' author='HaoLiuHust' date='2020-05-11T07:49:49Z'>
		is google drive ok?
		</comment>
		<comment id='5' author='HaoLiuHust' date='2020-05-11T07:51:57Z'>
		&lt;denchmark-link:https://github.com/HaoLiuHust&gt;@HaoLiuHust&lt;/denchmark-link&gt;
, also good, thanks!
		</comment>
		<comment id='6' author='HaoLiuHust' date='2020-05-11T08:07:59Z'>
		&lt;denchmark-link:https://drive.google.com/open?id=1ZIrJ9oU0XneNJcl1C9QzCipSkRQHamc2&gt;https://drive.google.com/open?id=1ZIrJ9oU0XneNJcl1C9QzCipSkRQHamc2&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/dkurt&gt;@dkurt&lt;/denchmark-link&gt;
 thanks
		</comment>
		<comment id='7' author='HaoLiuHust' date='2020-05-11T08:50:46Z'>
		I am able to reproduce this issue with OCV CPU but not with CUDA. The CUDA backend gives identical timings for both models.
I have replaced the timing code with this:
&lt;denchmark-code&gt;using std::chrono::steady_clock;
auto start = steady_clock::now();
auto preds = net.forward();
auto end = steady_clock::now();
&lt;/denchmark-code&gt;

CUDA backend:

timings for model 1
Total time:303ms
Total time:14ms
Total time:16ms
Total time:15ms
Total time:15ms
Total time:16ms
Total time:16ms
Total time:15ms
Total time:15ms
Total time:16ms
Total time:15ms
Total time:16ms
Total time:17ms
Total time:13ms
Total time:15ms
Total time:15ms
Total time:15ms
Total time:14ms
Total time:15ms
Total time:14ms
test



timings for model 2
Total time:271ms
Total time:15ms
Total time:14ms
Total time:14ms
Total time:14ms
Total time:14ms
Total time:15ms
Total time:13ms
Total time:15ms
Total time:15ms
Total time:15ms
Total time:14ms
Total time:14ms
Total time:15ms
Total time:16ms
Total time:14ms
Total time:14ms
Total time:22ms
Total time:13ms
Total time:15ms
test


OCV CPU:

timings for model 1
Total time:219ms
Total time:69ms
Total time:69ms
Total time:68ms
Total time:69ms
Total time:68ms
Total time:70ms
Total time:70ms
Total time:71ms
Total time:73ms
Total time:72ms
Total time:72ms
Total time:73ms
Total time:72ms
Total time:72ms
Total time:73ms
Total time:72ms
Total time:72ms
Total time:72ms
Total time:72ms
test



timings for model 2
Total time:505ms
Total time:357ms
Total time:355ms
Total time:352ms
Total time:352ms
Total time:353ms
Total time:353ms
Total time:352ms
Total time:351ms
Total time:350ms
Total time:350ms
Total time:351ms
Total time:350ms
Total time:350ms
Total time:350ms
Total time:350ms
Total time:350ms
Total time:350ms
Total time:349ms
Total time:350ms
test


		</comment>
		<comment id='8' author='HaoLiuHust' date='2020-05-11T09:11:45Z'>
		&lt;denchmark-link:https://github.com/YashasSamaga&gt;@YashasSamaga&lt;/denchmark-link&gt;
 Thanks, I am using CPU.
		</comment>
		<comment id='9' author='HaoLiuHust' date='2020-05-11T12:06:19Z'>
		&lt;denchmark-code&gt;$ sha1sum.exe model_*
427859bbcc753a59fc856f0f83c7f4daa6459241 *model_1.caffemodel
181a9f6ade86d2aa8605db82e07a43c018758957 *model_2.caffemodel
b5d68a6126c632dea88174dd3deb8edde98d789f *model_1.prototxt
b5d68a6126c632dea88174dd3deb8edde98d789f *model_2.prototxt
&lt;/denchmark-code&gt;

Files are correct. Can also confirm that both models work with same efficiency with disabled fusion net.enableFusion(false). So we need to find a problem in fusion.
		</comment>
		<comment id='10' author='HaoLiuHust' date='2020-05-11T12:28:29Z'>
		&lt;denchmark-link:https://github.com/HaoLiuHust&gt;@HaoLiuHust&lt;/denchmark-link&gt;
, with Intel's Inference Engine backend (OpenVINO) both models work with the same efficiency (got about 32ms on Intel(R) Core(TM) i7-6700K CPU @ 4.00GHz). You may try to use OpenCV from &lt;denchmark-link:https://github.com/opencv/opencv/releases/download/4.3.0/opencv-4.3.0-dldt-2020.2-vc16-avx2.7z&gt;https://github.com/opencv/opencv/releases/download/4.3.0/opencv-4.3.0-dldt-2020.2-vc16-avx2.7z&lt;/denchmark-link&gt;
 while we work on solution for default backend.
		</comment>
		<comment id='11' author='HaoLiuHust' date='2020-05-11T12:53:49Z'>
		&lt;denchmark-link:https://github.com/dkurt&gt;@dkurt&lt;/denchmark-link&gt;
 Thanks for your warmly help， but I need to use OpenCV on x86, OpenVino only support x64...our customer's compute maybe 32bit OS. I will try it anyway
		</comment>
		<comment id='12' author='HaoLiuHust' date='2020-05-11T13:32:31Z'>
		&lt;denchmark-link:https://github.com/HaoLiuHust&gt;@HaoLiuHust&lt;/denchmark-link&gt;
, got it. So, OpenVINO does not support 32bit CPU inference, it's only for 64bit. We will try to figure out the solution for OpenCV path as well.
Thanks for highlighting the problem!
		</comment>
		<comment id='13' author='HaoLiuHust' date='2020-05-11T18:08:39Z'>
		&lt;denchmark-link:https://github.com/HaoLiuHust&gt;@HaoLiuHust&lt;/denchmark-link&gt;
, following recommendation from &lt;denchmark-link:https://github.com/alibaba/MNN/issues/786#issuecomment-626532797&gt;alibaba/MNN#786 (comment)&lt;/denchmark-link&gt;
 I recompiled OpenCV with  option and got similar efficiency for both weights. But note that this option is  by default due it's unsafe. In example, it broke 3 accuracy tests:
&lt;denchmark-code&gt;[  FAILED  ] Layer_LSTM_Test_Accuracy_with_.CaffeRecurrent
[  FAILED  ] Test_Caffe_nets.FasterRCNN_vgg16/0, where GetParam() = OCV/CPU
[  FAILED  ] Test_Caffe_nets.FasterRCNN_zf/0, where GetParam() = OCV/CPU
&lt;/denchmark-code&gt;

/cc &lt;denchmark-link:https://github.com/vpisarev&gt;@vpisarev&lt;/denchmark-link&gt;

		</comment>
		<comment id='14' author='HaoLiuHust' date='2020-05-12T01:17:03Z'>
		I have tested it on MNN, still different speed..From profiling, the slower one's convolution layer is much slower. Is  weights value can effect the speed of a layer? too  weird ...
		</comment>
		<comment id='15' author='HaoLiuHust' date='2020-05-12T05:52:41Z'>
		&lt;denchmark-link:https://github.com/HaoLiuHust&gt;@HaoLiuHust&lt;/denchmark-link&gt;
, we are not responsible for MNN, have you tested  for OpenCV?
		</comment>
		<comment id='16' author='HaoLiuHust' date='2020-05-12T05:54:07Z'>
		have not yet. ENABLE_FAST_MATH can not ensure the accuracy, is that right?
		</comment>
		<comment id='17' author='HaoLiuHust' date='2020-05-12T06:38:43Z'>
		&lt;denchmark-link:https://github.com/HaoLiuHust&gt;@HaoLiuHust&lt;/denchmark-link&gt;
, according to the tests, yes. So I'd like to ask just try it and check if both models produce accurate results and compare their efficiency. It's not a solution, but just an experiment.
		</comment>
		<comment id='18' author='HaoLiuHust' date='2020-05-14T17:08:46Z'>
		
I have tested it on MNN, still different speed..From profiling, the slower one's convolution layer is much slower. Is weights value can effect the speed of a layer? too weird ...

I zeroed all weights that were smaller than 1e-15 and both give the same efficiency. I suspect that the fusion process is leading to a lot of denormals by multiplying small numbers with small numbers. I have some doubts on my claim though because it's a bit unusual to have models filled with so many tiny weights to cause serious performance degradation.
Denormals have leading zeros in the mantissa which is not-so-normal representation. Normally, you would have leading zeros counted in the exponent to make room for having as many significant digits as possible in the mantissa. When the number becomes so small that you cannot make the exponent any smaller without an overflow, you will use leading zeros in the mantissa to store the value. Most hardware are optimized for handling normal numbers efficiently and often have alternate slow paths for dealing with denormals. When you enable fast math, you are also enabling flush-to-zero (treat denormals as zero). With FTZ, the hardware deals with them efficiently by simply ignoring them.
The CUDA backend didn't face this issue probably because the convolutions are largely implemented using fused multiply-add ops and the FMA pipeline can handle denormals. Only multi-instruction sequences like sqrt require special handling of denormals.
		</comment>
		<comment id='19' author='HaoLiuHust' date='2020-05-14T19:20:06Z'>
		&lt;denchmark-link:https://github.com/YashasSamaga&gt;@YashasSamaga&lt;/denchmark-link&gt;
, thanks for explanation!
Unfortunalety, despite the model from issue achieves similar efficiency with thresholded weights, there is a stable regression for some of the models in the opencv_perf_dnn on CPU:
&lt;denchmark-code&gt;                       Name of Test                         base     ftz      ftz    
                                                                               vs    
                                                                              base   
                                                                           (x-factor)
AlexNet::DNNTestNetwork::OCV/CPU                           14.110  14.181     1.00   
DenseNet_121::DNNTestNetwork::OCV/CPU                      38.680  39.131     0.99   
EAST_text_detection::DNNTestNetwork::OCV/CPU               68.929  68.890     1.00   
ENet::DNNTestNetwork::OCV/CPU                              44.435  26.954     1.65   &lt;&lt;&lt;&lt;&lt;
FastNeuralStyle_eccv16::DNNTestNetwork::OCV/CPU            119.535 124.284    0.96   
GoogLeNet::DNNTestNetwork::OCV/CPU                         15.213  15.050     1.01   
Inception_5h::DNNTestNetwork::OCV/CPU                      16.492  16.505     1.00   
Inception_v2_Faster_RCNN::DNNTestNetwork::OCV/CPU          286.843 288.193    1.00   
Inception_v2_SSD_TensorFlow::DNNTestNetwork::OCV/CPU       43.045  43.664     0.99   
MobileNet_SSD_Caffe::DNNTestNetwork::OCV/CPU               19.457  21.011     0.93   &lt;&lt;&lt;&lt;&lt;
MobileNet_SSD_v1_TensorFlow::DNNTestNetwork::OCV/CPU       21.112  22.336     0.95   &lt;&lt;&lt;&lt;&lt;
MobileNet_SSD_v2_TensorFlow::DNNTestNetwork::OCV/CPU       29.928  31.641     0.95   &lt;&lt;&lt;&lt;&lt;
OpenFace::DNNTestNetwork::OCV/CPU                           3.917   3.936     1.00   
OpenPose_pose_mpi_faster_4_stages::DNNTestNetwork::OCV/CPU 604.125 612.948    0.99   
ResNet_50::DNNTestNetwork::OCV/CPU                         35.706  35.647     1.00   
SSD::DNNTestNetwork::OCV/CPU                               268.489 266.141    1.01   
SqueezeNet_v1_1::DNNTestNetwork::OCV/CPU                    3.910   3.888     1.01   
YOLOv3::DNNTestNetwork::OCV/CPU                            209.808 209.162    1.00   
opencv_face_detector::DNNTestNetwork::OCV/CPU              13.539  14.016     0.97
&lt;/denchmark-code&gt;

@@ -414,6 +414,7 @@ public:
                 cv::multiply(originWeights.row(i), weightsMultipliers[i], weightsMat.row(i));
                 biasvec[i] *= wi;
             }
+            weightsMat.setTo(0, abs(weightsMat) &lt;= 1e-15f);  // Flush to zero (FTZ) denormal weights
         }
		</comment>
		<comment id='20' author='HaoLiuHust' date='2020-05-14T19:31:27Z'>
		That's crazy :) If we will keep just a signed zero (probably for ReLU?) the performance for target network is OK and there is no efficiency regressions:
&lt;denchmark-code&gt;                       Name of Test                         base     ftz      ftz    
                                                                               vs    
                                                                              base   
                                                                           (x-factor)
AlexNet::DNNTestNetwork::OCV/CPU                           14.110  14.064     1.00   
DenseNet_121::DNNTestNetwork::OCV/CPU                      38.680  38.555     1.00   
EAST_text_detection::DNNTestNetwork::OCV/CPU               68.929  68.397     1.01   
ENet::DNNTestNetwork::OCV/CPU                              44.435  26.273     1.69   &lt;&lt;&lt;&lt;&lt;
FastNeuralStyle_eccv16::DNNTestNetwork::OCV/CPU            119.535 118.005    1.01   
GoogLeNet::DNNTestNetwork::OCV/CPU                         15.213  15.202     1.00   
Inception_5h::DNNTestNetwork::OCV/CPU                      16.492  16.675     0.99   
Inception_v2_Faster_RCNN::DNNTestNetwork::OCV/CPU          286.843 288.505    0.99   
Inception_v2_SSD_TensorFlow::DNNTestNetwork::OCV/CPU       43.045  42.981     1.00   
MobileNet_SSD_Caffe::DNNTestNetwork::OCV/CPU               19.457  19.491     1.00   
MobileNet_SSD_v1_TensorFlow::DNNTestNetwork::OCV/CPU       21.112  21.173     1.00   
MobileNet_SSD_v2_TensorFlow::DNNTestNetwork::OCV/CPU       29.928  29.599     1.01   
OpenFace::DNNTestNetwork::OCV/CPU                           3.917   3.964     0.99   
OpenPose_pose_mpi_faster_4_stages::DNNTestNetwork::OCV/CPU 604.125 607.526    0.99   
ResNet_50::DNNTestNetwork::OCV/CPU                         35.706  35.471     1.01   
SSD::DNNTestNetwork::OCV/CPU                               268.489 266.976    1.01   
SqueezeNet_v1_1::DNNTestNetwork::OCV/CPU                    3.910   3.898     1.00   
YOLOv3::DNNTestNetwork::OCV/CPU                            209.808 206.514    1.02   
opencv_face_detector::DNNTestNetwork::OCV/CPU              13.539  13.453     1.01 
&lt;/denchmark-code&gt;

@@ -414,6 +414,10 @@ public:
                 cv::multiply(originWeights.row(i), weightsMultipliers[i], weightsMat.row(i));
                 biasvec[i] *= wi;
             }
+            Mat mask = (abs(weightsMat) &lt;= 1e-15f) &amp; (weightsMat &gt; 0);
+            weightsMat.setTo(0, mask);  // Flush to zero (FTZ) denormal weights
+            mask = (abs(weightsMat) &lt;= 1e-15f) &amp; (weightsMat &lt; 0);
+            weightsMat.setTo(-0, mask);  // Flush to zero (FTZ) denormal weights
         }
		</comment>
		<comment id='21' author='HaoLiuHust' date='2020-05-14T20:06:01Z'>
		&lt;denchmark-link:https://github.com/HaoLiuHust&gt;@HaoLiuHust&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/YashasSamaga&gt;@YashasSamaga&lt;/denchmark-link&gt;
, please take a look to proposed fix: &lt;denchmark-link:https://github.com/opencv/opencv/pull/17295&gt;#17295&lt;/denchmark-link&gt;

		</comment>
		<comment id='22' author='HaoLiuHust' date='2020-05-15T02:38:45Z'>
		Thanks for your work, I have tested it, speed is ok, and I made similar change in MNN, also OK. I will try more to test accuracy
		</comment>
		<comment id='23' author='HaoLiuHust' date='2020-05-15T08:24:13Z'>
		The accuracy is OK, Thanks
		</comment>
	</comments>
</bug>