<bug id='13895' author='mahehu' open_date='2019-02-22T18:04:26Z' closed_time='2019-02-25T14:08:28Z'>
	<summary>SSD object detector returns too large bboxes for non-square inputs</summary>
	<description>
&lt;denchmark-h:h5&gt;System information (version)&lt;/denchmark-h&gt;


OpenCV =&gt; 4.0.0.21 installed via pip
Tensorflow =&gt; 1.12.
Operating System / Platform =&gt; Ubuntu 18.04
Compiler =&gt; Installed via pip
Platform =&gt; Anaconda Python 3.6.8

&lt;denchmark-h:h5&gt;Detailed description&lt;/denchmark-h&gt;

An SSD object detector trained on Tensorflow object detection API gives incorrect bounding boxes with non-square images. With square images (in training and testing), the results are exactly the same as those of native Tensorflow. When training and testing images are not square, opencv returns larger bounding boxes than native TF.
For example, in the non-square &lt;denchmark-link:http://www.cs.tut.fi/~hehu/ttmmpp/out0.jpg&gt;image&lt;/denchmark-link&gt;
, of size  the bounding boxes are as follows (green = TF, yellow = opencv):
OpenCV    : Left 0.1078883, Top 0.0987248, Width 0.7832990, Height 0.6393267
TensorFlow: Left 0.2059162, Top 0.0987247, Width 0.5874741, Height 0.6393268
top and height coordinates are exactly the same, but left and width are not. Width appears to be exactly 33.3% larger with opencv than TF (which seems to be related to the input aspect ratio--verified with other models).
&lt;denchmark-h:h5&gt;Steps to reproduce&lt;/denchmark-h&gt;


Download our face detection model trained on TF object detection api.
Download the example image
Execute the following python code:

&lt;denchmark-code&gt;import cv2 as cv
import tensorflow as tf

cvNet = cv.dnn.readNetFromTensorflow('ssdmobv1depth0.75_240x180/frozen_inference_graph.pb',
                                     'ssdmobv1depth0.75_240x180/graph.pbtxt')

# Read the graph.
with tf.gfile.FastGFile('ssdmobv1depth0.75_240x180/frozen_inference_graph.pb', 'rb') as f:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(f.read())

with tf.Session() as sess:
    # Restore session
    sess.graph.as_default()
    tf.import_graph_def(graph_def, name='')

    img = cv.imread('0.jpg')
    rows = img.shape[0]
    cols = img.shape[1]

    # Run OpenCV
    cvNet.setInput(cv.dnn.blobFromImage(img, size=(240, 180), swapRB=True, crop=False))
    cvOut = cvNet.forward()

    # Run Tensorflow
    inp = cv.resize(img, (240, 180))
    inp = inp[:, :, [2, 1, 0]]  # BGR2RGB
    out = sess.run([sess.graph.get_tensor_by_name('num_detections:0'),
                    sess.graph.get_tensor_by_name('detection_scores:0'),
                    sess.graph.get_tensor_by_name('detection_boxes:0'),
                    sess.graph.get_tensor_by_name('detection_classes:0')],
                    feed_dict={'image_tensor:0': inp.reshape(1, inp.shape[0], inp.shape[1], 3)})

    # Visualize CV
    for detection in cvOut[0,0,:,:]:
        score = float(detection[2])
        if score &gt; 0.3:
            leftcv = detection[3]
            topcv = detection[4]
            widthcv = detection[5]-detection[3]
            heightcv = detection[6]-detection[4]
            rightcv = detection[5]

            left = detection[3] * cols
            top = detection[4] * rows
            right = detection[5] * cols
            bottom = detection[6] * rows
            cv.rectangle(img, (int(left), int(top)), (int(right), int(bottom)), (23, 230, 210), thickness=2)

    # Visualize TF
    num_detections = int(out[0][0])
    for i in range(num_detections):
        classId = int(out[3][0][i])
        score = float(out[1][0][i])
        bbox = [float(v) for v in out[2][0][i]]
        if score &gt; 0.3:
            lefttf = bbox[1]
            toptf = bbox[0]
            widthtf = bbox[3]-bbox[1]
            heighttf = bbox[2]-bbox[0]
            righttf = bbox[3]

            x = bbox[1] * cols
            y = bbox[0] * rows
            right = bbox[3] * cols
            bottom = bbox[2] * rows
            cv.rectangle(img, (int(x), int(y)), (int(right), int(bottom)), (125, 255, 51), thickness=2)

    print("Found boxes:")
    print("OpenCV    : Left {:.7f}, Top {:.7f}, Width {:.7f}, Height {:.7f}".format(leftcv, topcv, widthcv, heightcv))
    print("TensorFlow: Left {:.7f}, Top {:.7f}, Width {:.7f}, Height {:.7f}".format(lefttf, toptf, widthtf, heighttf))
    print("Width ratio:", widthcv/widthtf)

    try:
        cv.imshow('Yellow = OpenCV, Green = TensorFlow', img)
        cv.waitKey()
    except:
        cv.imwrite('out0.jpg', img)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='mahehu' date='2019-02-22T18:38:21Z'>
		Thank you! We also observed that issue. The problem must be in the prior boxes generation.
		</comment>
		<comment id='2' author='mahehu' date='2019-02-25T09:48:30Z'>
		&lt;denchmark-link:https://github.com/mahehu&gt;@mahehu&lt;/denchmark-link&gt;
, Thank you! The issue can be solved just modifying anchors generator: &lt;denchmark-link:https://github.com/opencv/opencv/pull/13906&gt;#13906&lt;/denchmark-link&gt;
.
OpenCV output:
&lt;denchmark-link:https://user-images.githubusercontent.com/25801568/53328693-99cff400-38fb-11e9-8822-3e39a7502895.jpg&gt;&lt;/denchmark-link&gt;

TensorFlow output:
&lt;denchmark-link:https://user-images.githubusercontent.com/25801568/53328708-a18f9880-38fb-11e9-8e0b-24f9d55f3600.jpg&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='mahehu' date='2019-02-25T14:08:28Z'>
		Thank you! Works like a charm.
		</comment>
	</comments>
</bug>