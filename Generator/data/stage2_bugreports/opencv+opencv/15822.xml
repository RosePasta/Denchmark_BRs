<bug id='15822' author='moberweger' open_date='2019-10-31T19:49:37Z' closed_time='2020-05-10T19:02:58Z'>
	<summary>3D CNN - Inconsistent shape for ConcatLayer in function 'getMemoryShapes'</summary>
	<description>
&lt;denchmark-h:h5&gt;System information (version)&lt;/denchmark-h&gt;


OpenCV == 4.1.2-dev
python == 3.6
Operating System / Platform == Ubuntu 18.04 x86_64
compiler == gcc 7.4.0
tensorflow == 1.14.0

&lt;denchmark-h:h5&gt;Detailed description&lt;/denchmark-h&gt;

After loading a 3D CNN model from tensorflow, OpenCV triggers an error when running net.forward(). The cause of the error is related to 3D convolutions and tensor concatenation. The equivalent code runs for 2D networks, but fails for 3D networks.
The code raises the following error:
cv2.error: OpenCV(4.1.2-dev) /opencv/modules/dnn/src/layers/concat_layer.cpp:101: error: (-201:Incorrect size of input array) Inconsistent shape for ConcatLayer in function 'getMemoryShapes'
&lt;denchmark-h:h5&gt;Steps to reproduce&lt;/denchmark-h&gt;

The following python script can be used to trigger the error.
#!/usr/bin/env python3
import os

os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = ""
import numpy
import cv2
import tensorflow as tf
from tensorflow.tools.graph_transforms import TransformGraph


def run2d():
    batch_size = 4
    input_shape = [batch_size, 32, 32, 2]
    features = tf.placeholder(tf.float32, input_shape, name='input')

    features0 = tf.layers.conv2d(inputs=features, filters=8, kernel_size=[3, 3], padding='same')
    features1 = tf.layers.conv2d(inputs=features, filters=16, kernel_size=[3, 3], padding='same')
    features2 = tf.concat([features0, features1], axis=-1)
    features3 = tf.layers.conv2d(inputs=features2, filters=8, kernel_size=[3, 3], padding='same')

    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        constant_graph = tf.graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(),
                                                                      ['conv2d_2/BiasAdd'])
        tf.train.write_graph(constant_graph, "", "graph_final.pb", as_text=False)

    # export
    with tf.gfile.FastGFile("graph_final.pb", 'rb') as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())
        graph_def = TransformGraph(graph_def, ['input'], ['conv2d_2/BiasAdd'], ['strip_unused_nodes'])
        with tf.gfile.FastGFile('saved_model.pb', 'wb') as f:
            f.write(graph_def.SerializeToString())

    # read model
    cvNet = cv2.dnn.readNetFromTensorflow('./saved_model.pb')
    img = numpy.zeros((batch_size, 2, 32, 32), dtype='float32')
    cvNet.setInput(img)
    cvOut = cvNet.forward()
    print(cvOut.shape)


def run3d():
    batch_size = 4
    input_shape = [batch_size, 32, 32, 32, 2]
    features = tf.placeholder(tf.float32, input_shape, name='input')

    features0 = tf.layers.conv3d(inputs=features, filters=8, kernel_size=[3, 3, 3], padding='same')
    features1 = tf.layers.conv3d(inputs=features, filters=16, kernel_size=[3, 3, 3], padding='same')
    features2 = tf.concat([features0, features1], axis=-1)
    features3 = tf.layers.conv3d(inputs=features2, filters=8, kernel_size=[3, 3, 3], padding='same')

    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        constant_graph = tf.graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(),
                                                                      ['conv3d_2/BiasAdd'])
        tf.train.write_graph(constant_graph, "", "graph_final.pb", as_text=False)

    # export
    with tf.gfile.FastGFile("graph_final.pb", 'rb') as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())
        graph_def = TransformGraph(graph_def, ['input'], ['conv3d_2/BiasAdd'], ['strip_unused_nodes'])
        with tf.gfile.FastGFile('saved_model.pb', 'wb') as f:
            f.write(graph_def.SerializeToString())

    # read model
    cvNet = cv2.dnn.readNetFromTensorflow('./saved_model.pb')
    img = numpy.zeros((batch_size, 2, 32, 32, 32), dtype='float32')
    cvNet.setInput(img)
    cvOut = cvNet.forward()
    print(cvOut.shape)


if __name__ == '__main__':
    print(cv2.__version__)
    print(tf.__version__)
    run2d()
    run3d()
	</description>
	<comments>
		<comment id='1' author='moberweger' date='2020-02-17T15:36:41Z'>
		I am verifying the bug on my system.
		</comment>
		<comment id='2' author='moberweger' date='2020-02-17T17:16:11Z'>
		Confirmed. I got the same error on my system with Tensorflow 1.14 and  branch of OpenCV.
&lt;denchmark-link:https://github.com/alalek&gt;@alalek&lt;/denchmark-link&gt;
 Can I work on it?
		</comment>
		<comment id='3' author='moberweger' date='2020-02-18T14:59:57Z'>
		I think I have figured out where the problem is.
The dimensions are re-ordered in OpenCV and don't match with the ones in Tensorflow but the axis being used is same in both, so, IMO, we have to canonicalize the shapes. For example, in OpenCV, the number of filters are placed at index 1 in the above case and in Tensorflow those are placed at the last index i.e, -1 which is leading to error.
I will try to submit a fix for this.
		</comment>
		<comment id='4' author='moberweger' date='2020-03-13T17:11:06Z'>
		Hey &lt;denchmark-link:https://github.com/czgdp1807&gt;@czgdp1807&lt;/denchmark-link&gt;
 did you come across a fix?
		</comment>
		<comment id='5' author='moberweger' date='2020-03-13T17:12:18Z'>
		No not as of now. If you want to make a PR to fix it then please feel free to do so.
		</comment>
		<comment id='6' author='moberweger' date='2020-05-10T19:02:58Z'>
		Please try &lt;denchmark-link:https://github.com/opencv/opencv/pull/17118&gt;#17118&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>