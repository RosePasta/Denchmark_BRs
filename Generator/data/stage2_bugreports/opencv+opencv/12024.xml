<bug id='12024' author='Triplesalt' open_date='2018-07-20T01:49:59Z' closed_time='2018-07-27T00:56:57Z'>
	<summary>tf_importer : Silent bug with a different input order for Mul+Maximum (ReLU)</summary>
	<description>
&lt;denchmark-h:h5&gt;System information (version)&lt;/denchmark-h&gt;


OpenCV =&gt; 3.4

&lt;denchmark-h:h5&gt;Detailed description&lt;/denchmark-h&gt;

Custom leaky ReLU implementations can have a different input order for the Maximum layer than the OpenCV import code expects, causing it to silently skip the ReLU layer in the network.
tf_importer.cpp expects the resulting graph to look like this (&lt;denchmark-link:https://github.com/opencv/opencv/blob/8b5f061dae9908222bf373e071ac3d6c314dafc1/modules/dnn/src/tensorflow/tf_importer.cpp#L1246&gt;tf_importer.cpp:1246-1258&lt;/denchmark-link&gt;
 ) :
// Try to match with a LeakyRelu:
// node {
//   name: "LeakyRelu/mul"
//   op: "Mul"
//   input: "LeakyRelu/alpha"
//   input: "input"
// }
// node {
//   name: "LeakyRelu/Maximum"
//   op: "Maximum"
//   input: "LeakyRelu/mul"
//   input: "input"
// }
In this (normal) case, the importer will combine these two to a single ReLU node and redirect any input from the LeakyRelu/Maximum node to the new ReLU node LeakyRelu/mul.
If the inputs of LeakyRelu/Maximum are the other way round, the importer will still combine the two nodes to a ReLU node named LeakyRelu/mul, but all inputs from LeakyRelu/Maximum will be redirected to input instead of LeakyRelu/mul.
The problem lies in the ExcludeLayer call (&lt;denchmark-link:https://github.com/opencv/opencv/blob/8b5f061dae9908222bf373e071ac3d6c314dafc1/modules/dnn/src/tensorflow/tf_importer.cpp#L1263&gt;tf_importer.cpp:1263&lt;/denchmark-link&gt;
) with input_blob_index set to 0, while LeakyRelu/mul could also be at input index 1.
I've seen more ExcludeLayer calls with input index 0, so this might not be the only instance of this issue.
&lt;denchmark-h:h5&gt;Steps to reproduce&lt;/denchmark-h&gt;

Load a graph with a ReLU as mentioned above using readNetFromTensorflow.
Works fine :
tf.maximum(0.01*input, input, name="LeakyRelu")
Does not work (is effectively skipped in the network) :
tf.maximum(input, 0.01*input, name="LeakyRelu")
	</description>
	<comments>
		<comment id='1' author='Triplesalt' date='2018-07-27T00:56:57Z'>
		Closing since the pull request has been merged.
		</comment>
	</comments>
</bug>