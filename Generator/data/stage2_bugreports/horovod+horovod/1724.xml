<bug id='1724' author='brandon-biggs' open_date='2020-02-17T15:31:24Z' closed_time='2020-03-25T18:03:46Z'>
	<summary>Horovod.run.run on multiple hosts kills jupyter notebook session</summary>
	<description>
Environment:

Framework: (TensorFlow, Keras, PyTorch, MXNet)
Framework version: 2.0
Horovod version: 0.19.0
MPI version:

&lt;denchmark-code&gt;mpirun --version
mpirun (Open MPI) 4.0.2
&lt;/denchmark-code&gt;


CUDA version: 10.2
NCCL version: 2.5.6
Python version: 2.7
OS and version: Centos 7,7
GCC version:

&lt;denchmark-code&gt;$ gcc --version
gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39)
&lt;/denchmark-code&gt;

Checklist:

Did you search issues to find if somebody asked this question before? No
If your question is about hang, did you read this doc? Don't think it's about hang
If your question is about docker, did you read this doc? Not about docker
Did you check if you question is answered in the troubleshooting guide? Couldn't find it

Bug report:
Please describe errorneous behavior you're observing and steps to reproduce it.
I have horovod.run.run working on 1 server with four GPUs finally after the issues I had in &lt;denchmark-link:https://github.com/horovod/horovod/issues/1691&gt;#1691&lt;/denchmark-link&gt;
. However, now when I try to run horovod.run.run on multiple servers with multiple GPUs, my Jupyter notebook runs for a few seconds and then crashes. The error message is:
&lt;denchmark-code&gt;The kernel appears to have died. It will restart automatically.
&lt;/denchmark-code&gt;

Here's my first cell:
&lt;denchmark-code&gt;import horovod
import horovod.run
import tensorflow as tf
import horovod.tensorflow as hvd

def test_main():
    # Horovod: initialize Horovod.
    hvd.init()
    # Horovod: pin GPU to be used to process local rank (one GPU per process)
    gpus = tf.config.experimental.list_physical_devices('GPU')
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)
    if gpus:
        tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')
    print(f"Hello world from {hvd.local_rank()}")
&lt;/denchmark-code&gt;

Then I make my horovod.run.run call in my second cell:
&lt;denchmark-code&gt;horovod.run.run(test_main, np=2, hosts="host1:4, host2:4", verbose=True, disable_cache=True)
&lt;/denchmark-code&gt;

I do get a little output before it crashes:

Filtering local host names.
Remote host found: host1
Checking ssh on all remote hosts.
SSH was successful into all the remote hosts.
Testing interfaces on all the hosts.
Launched horovodrun server.
Attempted to launch horovod task servers.
Waiting for the hosts to acknowledge.
Notified all the hosts that the registration is complete.
Waiting for hosts to perform host-to-host interface checking.

After this it immediately crashes. I figured there might be a brief wait, but there is not. Looking at the code, the crash appears to be occurring in [run.py)[https://github.com/horovod/horovod/blob/master/horovod/run/run.py#L252].
I can run this script with just mpi and it works on host1:4 and host2:4 just fine, so I don't think it's that. If y'all have any ideas, let me know.
I'd like to scale horovod.run.run up to a few more than just these two nodes if possible. :)
	</description>
	<comments>
		<comment id='1' author='brandon-biggs' date='2020-02-17T19:49:29Z'>
		Hey &lt;denchmark-link:https://github.com/brandon-biggs&gt;@brandon-biggs&lt;/denchmark-link&gt;
, does it also fail if you use  as opposed to ?  The code paths should be very similar.
		</comment>
		<comment id='2' author='brandon-biggs' date='2020-02-17T20:12:33Z'>
		Thanks &lt;denchmark-link:https://github.com/tgaddair&gt;@tgaddair&lt;/denchmark-link&gt;
 sorry for all the problems I'm causing. Hopefully these problems are more helpful to y'all than annoying :)
&lt;denchmark-code&gt;$ horovodrun -np 8 -H host1:4,host2:4 python horovod-run-run.py --verbose

Launching horovodrun task function was not successful:

File "../path/python3.7/site-packages/horovod/run/common/util/network.py", line 174, in __init__
    'Linux.'.format(service_name=service_name, addresses=addresses))
horovod.run.common.util.network.NoValidAddressesFound: Horovodrun was unable to connect to horovodrun task service #0 on any of the following addresses: {'lo': [('127.0.0.1', 12913)], '&lt;host 1 network interface1&gt;': [('&lt;host1 ip address here&gt;', 12913)]}.

'Linux.'.format(service_name=service_name, addresses=addresses))
horovod.run.common.util.network.NoValidAddressesFound: Horovodrun was unable to connect to horovodrun task service #1 on any of the following addresses: {'lo': [('127.0.0.1', 19370)], 'host 2 interface1': [('&lt;host2 ip address here', 19370)]}.

One possible cause of this problem is that horovodrun currently requires every host to have at least one routable network interface with the same name across all of the hosts. You can run "ifconfig -a" on every host and check for the common routable interface. To fix the problem, you can rename interfaces on Linux.
&lt;/denchmark-code&gt;

Hmm.. How interesting. Quite the issue. The interface names definitely aren't the same. It's not on purpose that they're different. When we set this system up, we never really had a purpose to having them be the same or different, it just happened.
According to the documentation, the equivalent should be the following mpirun command, which does work:
&lt;denchmark-code&gt;mpirun -np 8 -H host1:4,host2:4 -bind-to none -map-by slot -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -mca pml ob1 -mca btl ^openib python horovod-run-run.py
&lt;/denchmark-code&gt;

What's the purpose in requiring the interface names to be the same?
		</comment>
		<comment id='3' author='brandon-biggs' date='2020-02-18T17:53:40Z'>
		That's a good question, &lt;denchmark-link:https://github.com/brandon-biggs&gt;@brandon-biggs&lt;/denchmark-link&gt;
.  cc &lt;denchmark-link:https://github.com/abditag2&gt;@abditag2&lt;/denchmark-link&gt;
, who did the original implementation of .  Do you know the reasoning for including interfaces instead of excluding unroutable interfaces?  It may make sense to allow users to circumvent this behavior in some way.
		</comment>
		<comment id='4' author='brandon-biggs' date='2020-02-19T22:09:43Z'>
		After looking carefully into this, matching interface names is not something that is strictly necessary. If I remember correctly, we chose this because explicitly specifying what NIC to use rather than excluding some NICs, seemed simpler and compatible with most of the common naming scheme on cloud and on-prem machines.
I don't disagree with switching to excluding the unroutable interfaces. Note that if the naming of interfaces is not consistent across workers, we might still end up in a situation like the following where we will have to exclude both eth0 and eth1 and show different behavior than mpirun.
Host 1:
eth0: local virtual machine or container
eth1: physical eth NIC
Host 2:
eth0: physical eth NIC
eth1: local virtual machine or container
		</comment>
		<comment id='5' author='brandon-biggs' date='2020-02-20T14:39:37Z'>
		Hi &lt;denchmark-link:https://github.com/abditag2&gt;@abditag2&lt;/denchmark-link&gt;
 Thanks for the feedback. That makes sense as to why it may be an option, but it seems like it's being strictly enforced right now. I could be wrong, I just never saw an option as to where I could specify a way to ignore this.
Is there a way to ignore strict enforcement so that I can submit to multiple nodes or is there something in the code that's going to need to change?
		</comment>
		<comment id='6' author='brandon-biggs' date='2020-02-20T19:15:12Z'>
		we don't have an option right now. I will submit a fix for this asap. In the meanwhile, can you rename your NICs to get unblocked?
		</comment>
		<comment id='7' author='brandon-biggs' date='2020-02-20T21:28:33Z'>
		&lt;denchmark-link:https://github.com/abditag2&gt;@abditag2&lt;/denchmark-link&gt;
 I won't be able to rename my NICS. We're looking to put in some IB cards. We'll try to give these the same names. Having the fix would be great. Thanks for helping with this!
		</comment>
		<comment id='8' author='brandon-biggs' date='2020-03-19T17:45:29Z'>
		&lt;denchmark-link:https://github.com/abditag2&gt;@abditag2&lt;/denchmark-link&gt;
 anyway to bypass same name NIC requirement .
		</comment>
		<comment id='9' author='brandon-biggs' date='2020-03-25T03:02:39Z'>
		Hey &lt;denchmark-link:https://github.com/alokprasad&gt;@alokprasad&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/brandon-biggs&gt;@brandon-biggs&lt;/denchmark-link&gt;
, can you try out &lt;denchmark-link:https://github.com/horovod/horovod/pull/1808&gt;#1808&lt;/denchmark-link&gt;
 and check that it addresses your issue?
		</comment>
		<comment id='10' author='brandon-biggs' date='2020-03-25T13:25:52Z'>
		&lt;denchmark-link:https://github.com/tgaddair&gt;@tgaddair&lt;/denchmark-link&gt;
 I'd be happy to give it a shot! I'll try it out today and get back to you on it.
		</comment>
		<comment id='11' author='brandon-biggs' date='2020-03-25T14:57:08Z'>
		&lt;denchmark-link:https://github.com/tgaddair&gt;@tgaddair&lt;/denchmark-link&gt;
 I installed it, tested it on my "hello world" code that was in my original post, and it didn't crash! So that's exciting! I'm working on a more advanced test now. I'll keep y'all updated. Here's my horovod.run.run command:
horovod.run.run(train, np=2, hosts="host1:4,host2:4", verbose=True, disable_cache=True, network_interface="enp19s0f1.10,enp19s0f0.10", use_mpi=True, mpi_args="--oversubscribe")â€‹
		</comment>
	</comments>
</bug>