<bug id='1381' author='maxhgerlach' open_date='2019-09-05T09:20:02Z' closed_time='2020-01-20T15:55:53Z'>
	<summary>hvd.init() hangs with error message help-opal-shmem-mmap.txt</summary>
	<description>
Environment:

Framework: TensorFlow
Framework version: 1.13.1, custom built without XLA
Horovod version: 0.16.4
MPI version: Open MPI 4.0.1
CUDA version: 10.0
NCCL version: 2.3.7-1+cuda10.0
Python version: 2.7.12
OS and version: Ubuntu 16.04.5 LTS
GCC version: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609

Mellanox OFED 4.4-2.0.7.0 is installed.
Bug report:
Recently, our custom Horovod-based training code sometimes produces an error message after each worker rank called hvd.init(). The processes then hang without exiting. Here is the error message:
&lt;denchmark-code&gt;--------------------------------------------------------------------------
A system call failed during shared memory initialization that should
not have.  It is likely that your MPI job will now either abort or
experience performance degradation.

  Local host:  heinzel101
  System call: open(2)
  Error:       No such file or directory (errno 2)
--------------------------------------------------------------------------
[training_host:330371] 3 more processes have sent help message help-opal-shmem-mmap.txt / sys call fail
[training_host:330371] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
&lt;/denchmark-code&gt;

We start the training like this (5 local workers in this case): mpirun --prefix /opt/openmpi -np 5 -H training_host:5 -bind-to none -map-by slot -x NCCL_DEBUG=INFO -x NCCL_DEBUG_FILE=/dev/stderr -x LD_LIBRARY_PATH -x PATH -mca pml ob1 -mca btl ^openib --tag-output --timestamp-output python [training_script].
Here are gdb backtraces for all threads for one of the workers: &lt;denchmark-link:https://gist.github.com/maxhgerlach/7e7ccbd16714c7a070156f88fce7d291&gt;https://gist.github.com/maxhgerlach/7e7ccbd16714c7a070156f88fce7d291&lt;/denchmark-link&gt;
 It looks like the hang is inside  at line 929:
&lt;denchmark-code&gt;    // No ranks were given and no communicator provided to horovod_init() so use
    // MPI_COMM_WORLD
    MPI_Comm_dup(MPI_COMM_WORLD, &amp;(ctx.mpi_comm));
&lt;/denchmark-code&gt;

As in &lt;denchmark-link:https://github.com/horovod/horovod/issues/1102&gt;#1102&lt;/denchmark-link&gt;
 for each worker we still fork out a process via multiprocessing to do IO and preprocessing  initializing Horovod or MPI in the parent process. Does the error sound as if it might be related to this?
	</description>
	<comments>
		<comment id='1' author='maxhgerlach' date='2019-10-23T10:55:33Z'>
		We still observe this issue quite often. Might this be fixed in a more recent Horovod release?
		</comment>
		<comment id='2' author='maxhgerlach' date='2020-01-16T14:54:26Z'>
		In the mean time I have updated to Open MPI 4.0.2 and Horovod 0.19, but this issue still occurs with many of our training jobs. Unfortunately it is very hard to reproduce consistently.
I noticed that there are a couple of open issues regarding the Vader shared memory BTL in Open MPI, e.g. &lt;denchmark-link:https://github.com/open-mpi/ompi/issues/7308&gt;open-mpi/ompi#7308&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/alsrgv&gt;@alsrgv&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tgaddair&gt;@tgaddair&lt;/denchmark-link&gt;
 Would you recommend switching to  instead of ? Since expensive data transfers run over NCCL anyway, a slight decrease in MPI performance should not matter for us.
EDIT: I misread some old FAQs earlier. The vader BTL seems to have completely replaced the older sm for shared memory in Open MPI. I am considering to completely disable vader anyway, even if this means MPI communications would be restricted to TCP only.
		</comment>
		<comment id='3' author='maxhgerlach' date='2020-01-20T15:55:53Z'>
		After further investigation it looks like I misattributed this issue to Horovod or Open MPI.
A default setting in systemd, that we did not know about, causes shared memory files in  to be deleted while the Horovod training making use of them is still running. See &lt;denchmark-link:https://superuser.com/questions/1117764/why-are-the-contents-of-dev-shm-is-being-removed-automatically&gt;https://superuser.com/questions/1117764/why-are-the-contents-of-dev-shm-is-being-removed-automatically&lt;/denchmark-link&gt;

This was very hard to pinpoint because this only happens when the training is run by our automated custom system, but not while a login shell of the same Linux user is open on the same host.
		</comment>
	</comments>
</bug>