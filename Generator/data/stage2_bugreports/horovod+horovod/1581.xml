<bug id='1581' author='AHEADer' open_date='2019-12-11T01:51:55Z' closed_time='2019-12-19T06:52:16Z'>
	<summary>Retrain in Tensorflow will make the master rank exit earlier than other ranks</summary>
	<description>
Environment:

Framework: Tensorflow
Framework version: 1.14
Horovod version:
MPI version:
CUDA version: 2.4.7
NCCL version:
Python version: 3.6.8
OS and version: Ubuntu 16.04
GCC version: 4.9.3

Bug report:
After training in Tensorflow, I will get a trained model containing its training steps.
If I train with the same model_dir again, tf.estimator.train() will detect if maximum_step is equal or smaller than steps trained in model_dir. I do not change any config so in this situation, the maximum_step is equal to trained steps. Therefore train() method will end immediately and will not start a session, then the master rank ends.  I set only the master rank can read the model_dir. In this way, the master rank will not broadcast the tensor to other ranks because it does not run session(so hooks are useless). Other ranks start their own train() process but end with an error saying cannot connect the master rank.
I wonder if there's any way that the master rank can notify other ranks to stop their processes.
	</description>
	<comments>
		<comment id='1' author='AHEADer' date='2019-12-11T16:20:00Z'>
		Hey &lt;denchmark-link:https://github.com/AHEADer&gt;@AHEADer&lt;/denchmark-link&gt;
, do you have a small example that demonstrates the issue?  From your description, it sounds like you should be able to load the checkpointed model state on worker-0, then broadcast the number of trained steps before calling train().  If the number of trained steps is equal to the maximum_step, then all workers can exit.  But perhaps I'm misunderstanding something, please let me know if I am.
		</comment>
		<comment id='2' author='AHEADer' date='2019-12-12T01:37:31Z'>
		I check the source code, it seems that Horovod broadcast the global variables in TF Hook. However, before starting a session in estimator.train(), estimator will check if trained steps larger than maximum steps. I mean in retraining situation, estimator.train() will exit after checking and will not start a session. So other ranks who start their session and wait for the master rank to broadcast global variables will fail and produce an error.
I use docker to run Horovod and for each rank, I will assign a container. That's my experiment situation.
		</comment>
		<comment id='3' author='AHEADer' date='2019-12-13T19:09:12Z'>
		&lt;denchmark-link:https://github.com/AHEADer&gt;@AHEADer&lt;/denchmark-link&gt;
, you should be able to create a temporary session to broadcast this information before calling .  You can also use  to broadcast the trained steps without needing to create a session.
Fixing this within Horovod automatically would be tricky, because as you say, the estimator does not execute any hooks before returning when global step is greater than max steps.
In my opinion, the best workaround for this is to call the Estimator's _load_global_step_from_checkpoint_dir on rank 0, then broadcast this value to all workers and perform the check before calling train().
You may also want to try using steps instead of max_steps here.
In your opinion, what would be a good solution to this problem, given that this behavior is outside of Horovod's control, and we'll need to do extra work in the user script one way or another?
		</comment>
		<comment id='4' author='AHEADer' date='2019-12-19T06:52:16Z'>
		&lt;denchmark-link:https://github.com/tgaddair&gt;@tgaddair&lt;/denchmark-link&gt;
 Actually I run each rank in different pods inside one job. My solution is a little stupid by detecting if rank 0 exit normally. I think your solutions are much better. It is indeed a little tricky to do this in Horovod.
		</comment>
	</comments>
</bug>