<bug id='1204' author='pyotr777' open_date='2019-07-10T07:23:31Z' closed_time='2019-07-13T14:07:29Z'>
	<summary>MPI_ERR_OTHER: known error not in list</summary>
	<description>
Environment:

Framework: TensorFlow 1.12.0, Keras 2.2.4
Framework version: TensorFlow 1.12.0, Keras 2.2.4
Horovod version: 0.16.4
MPI version: Open MPI 2.1.1
CUDA version: 10.0
NCCL version: 2.4.7
Python version: 2.7.15
OS and version: Ubuntu 18.04.1 LTS
GCC version: 7.4.0
Hardware: Two NVIDIA Xaviers connected over Ethernet

Checklist:

 Did you search issues to find if somebody asked this question before?
 If your question is about hang, did you read this doc?
 If your question is about docker, did you read this doc?
 Did you check if you question is answered in the troubleshooting guide?

Bug report:
I have two identical NVIDIA Xaviers. I can SSH from one to another with
xavier$ ssh xavier2 and xavier2$ ssh xavier.
I can run the keras mnist sample from Xavier2:
&lt;denchmark-code&gt;xavier2$ mpirun -np 2 -H localhost:1,xavier:1 -mca btl_tcp_if_include eth0 -x NCCL_SOCKET_IFNAME=eth0 -bind-to none -map-by slot python keras_mnist.py
Using TensorFlow backend.
Using TensorFlow backend.
Keras 2.2.4
Keras 2.2.4
2019-07-10 16:16:42.987703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:924] ARM64 does not support NUMA - returning NUMA node zero
2019-07-10 16:16:42.987876: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:
name: Xavier major: 7 minor: 2 memoryClockRate(GHz): 1.5
...
&lt;/denchmark-code&gt;

But when running the same from another machine I get an error
&lt;denchmark-code&gt;xavier$ mpirun -np 2 -H localhost:1,xavier2:1 -mca btl_tcp_if_include eth0 -x NCCL_SOCKET_IFNAME=eth0 -bind-to none -map-by slot python keras_mnist.py
Warning: Permanently added 'xavier2.local,192.168.83.36' (ECDSA) to the list of known hosts.
Using TensorFlow backend.
Using TensorFlow backend.
Keras 2.2.4
Keras 2.2.4
[xavier2:7609] *** An error occurred in MPI_Allreduce
[xavier2:7609] *** reported by process [515244033,545460846593]
[xavier2:7609] *** on communicator MPI COMMUNICATOR 3 DUP FROM 0
[xavier2:7609] *** MPI_ERR_OTHER: known error not in list
[xavier2:7609] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[xavier2:7609] ***    and potentially your MPI job)
--------------------------------------------------------------------------
An MPI communication peer process has unexpectedly disconnected.  This
usually indicates a failure in the peer process (e.g., a crash or
otherwise exiting without calling MPI_FINALIZE first).

Although this local MPI process will likely now behave unpredictably
(it may even hang or crash), the root cause of this problem is the
failure of the peer -- that is what you need to investigate.  For
example, there may be a core file that you can examine.  More
generally: such peer hangups are frequently caused by application bugs
or other external events.

  Local host: xavier
  Local PID:  16335
  Peer host:  xavier2
--------------------------------------------------------------------------
...
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='pyotr777' date='2019-07-11T01:40:41Z'>
		&lt;denchmark-link:https://github.com/pyotr777&gt;@pyotr777&lt;/denchmark-link&gt;
, could you try this with Open MPI 4.0.1, just to make sure we're not running into some old MPI issue?
		</comment>
		<comment id='2' author='pyotr777' date='2019-07-13T14:07:29Z'>
		Don't know what it was, but it works again now both ways: from xavier and xavier2.
Sorry. Closing this issue.
		</comment>
	</comments>
</bug>