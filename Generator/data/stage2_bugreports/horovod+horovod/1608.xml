<bug id='1608' author='una-dinosauria' open_date='2019-12-23T20:45:44Z' closed_time='2020-01-09T21:03:49Z'>
	<summary>RuntimeError: For integral input tensors, argument alpha must not be a floating point number</summary>
	<description>
Environment:

Framework: (TensorFlow, Keras, PyTorch, MXNet): Pytorch
Framework version: 1.3.0
Horovod version: 0.16.1
MPI version: 3.1.2 (?)
CUDA version: 10.2
NCCL version: ? (probably not important)
Python version: 2.7
OS and version: Ubuntu 14.04.4 (probably not important)
GCC version: 4.8.4 (probably not important)

Bug report:
Horovod does not like it when my module has integer parameters, even if they do not require a gradient. It says
Traceback (most recent call last):
  File "/.../mwe.py", line 30, in &lt;module&gt;
    hvd.broadcast_optimizer_state(optimizer, root_rank=0)
  File "/.../python2.7/dist-packages/horovod-0.16.1-py2.7-linux-x86_64.egg/horovod/torch/__init__.py", line 261, in broadcast_optimizer_state
    super(optimizer.__class__, optimizer).step()
  File "/.../torch/optim/sgd.py", line 106, in step
    p.data.add_(-group['lr'], d_p)
RuntimeError: For integral input tensors, argument alpha must not be a floating point number.
MWE
import torch
import torch.nn as nn
import torch.optim as optim

import horovod.torch as hvd


class A(nn.Module):

    def __init__(self, a, b):
        super(A, self).__init__()
        self.a = nn.Parameter(a.int(), requires_grad=False)   # change to a.float() and hvd is happy
        self.b = nn.Parameter(b)

    def forward(self, x):
        return torch.index_select(self.b, 0, self.a.long()) * x


hvd.init()

a = torch.Tensor([1, 3])
b = torch.rand(4)

model = A(a, b).cuda()

optimizer = optim.SGD(model.parameters(), lr=0.01)
optimizer = hvd.DistributedOptimizer(optimizer, named_parameters=model.named_parameters())

hvd.broadcast_parameters(model.state_dict(), root_rank=0)
hvd.broadcast_optimizer_state(optimizer, root_rank=0)  # &lt;-- hvd is sad :(

model.train()

for i in range(1000):

    x = torch.Tensor(torch.rand(2)).cuda()

    optimizer.zero_grad()

    loss = torch.mean((model(x) - 0) ** 2)

    loss.backward()
    optimizer.step()

    if hvd.rank() == 0:
        print(i, loss.detach())
	</description>
	<comments>
		<comment id='1' author='una-dinosauria' date='2019-12-23T23:19:51Z'>
		Hey &lt;denchmark-link:https://github.com/una-dinosauria&gt;@una-dinosauria&lt;/denchmark-link&gt;
, thanks for reporting this.  And thanks even more for the reproducible example.  I'll take a look at this today and get back to you as soon as I have a fix.
		</comment>
		<comment id='2' author='una-dinosauria' date='2019-12-24T00:35:51Z'>
		Hey &lt;denchmark-link:https://github.com/una-dinosauria&gt;@una-dinosauria&lt;/denchmark-link&gt;
, please take a look at &lt;denchmark-link:https://github.com/horovod/horovod/pull/1609&gt;#1609&lt;/denchmark-link&gt;
 if you get a chance, it should address your issue.  Going forward, it will also provide a more generic  function that can be used to broadcast anything, for example, if  doesn't work in the future.
		</comment>
		<comment id='3' author='una-dinosauria' date='2019-12-24T05:40:40Z'>
		Left a comment in the PR. Looks awesome -- thanks.
		</comment>
	</comments>
</bug>