<bug id='1885' author='vycezhong' open_date='2020-04-18T09:23:15Z' closed_time='2020-04-25T08:16:53Z'>
	<summary>speed plunges when training locally</summary>
	<description>
Environment:

Framework:  mxnet-cu102
Framework version: 1.6.0
Horovod version: 0.19.1
MPI version: 3.1.0
CUDA version: 10.2
NCCL version: 2.5.6
Python version: 3.6
OS and version: ubuntu18.04lts
GCC version: 7.5.0

HW: p3.16xlarge
Bug report:
horovodrun -np 8 -H localhost:8 python ~/repos/horovod/examples/mxnet_imagenet_resnet50.py --model resnet50_v2 --mode gluon --rec-train ~/data/ILSVRC2012/train.rec --rec-train-idx ~/data/ILSVRC2012/train.idx --rec-val ~/data/ILSVRC2012/val.rec --rec-val-idx ~/data/ILSVRC2012/val.idx --use-rec --batch-size 64 --num-epochs 120 --data-nthreads 2 --warmup-epochs 5 --lr 0.05 --lr-mode step --log-interval 1
At first everything worked well. but later GPUs run so slow that i thought they did not run at all. I set log-interval to 1 and found it did run at an extremely slow speed, around 70img/s.
&lt;denchmark-link:https://user-images.githubusercontent.com/25879526/79633480-6d7ad500-8198-11ea-941c-36d03e8a5434.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/horovod/horovod/files/4496373/hvd.log&gt;full log&lt;/denchmark-link&gt;
 can be found here.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

BTW, I did not observe this phenomenon when training distributedly with 8 x p3.16xlarge. hvd can reach the speed of ~15000img/s. that's about 68% scaling efficiency. is that reasonable?
horovodrun -np 64 -H xxx:8 xxx:8 xxx:8 xxx:8 xxx:8 xxx:8 xxx:8 xxx:8  python ~/repos/horovod/examples/mxnet_imagenet_resnet50.py --model resnet50_v2 --mode gluon --rec-train ~/data/ILSVRC2012/train.rec --rec-train-idx ~/data/ILSVRC2012/train.idx --rec-val ~/data/ILSVRC2012/val.rec --rec-val-idx ~/data/ILSVRC2012/val.idx --use-rec --batch-size 64 --num-epochs 120 --data-nthreads 2 --warmup-epochs 5 --lr 0.05 --lr-mode step --log-interval 50
my installation:
HOROVOD_NCCL_INCLUDE=/usr/local/cuda-10.2/include HOROVOD_NCCL_LIB=/usr/local/cuda-10.2/lib HOROVOD_GPU_ALLREDUCE=NCCL HOROVOD_GPU_BROADCAST=NCCL pip install --no-cache-dir horovod
i found similar issue &lt;denchmark-link:https://github.com/horovod/horovod/issues/824&gt;#824&lt;/denchmark-link&gt;
 but in my case the data is just on each server's ssd.
ubuntu@ip-xxx:~$ df -h
Filesystem      Size  Used Avail Use% Mounted on
udev            241G     0  241G   0% /dev
tmpfs            49G  1.1M   49G   1% /run
/dev/xvda1      993G  383G  611G  39% /
tmpfs           241G  1.1G  240G   1% /dev/shm
tmpfs           5.0M     0  5.0M   0% /run/lock
tmpfs           241G     0  241G   0% /sys/fs/cgroup
/dev/loop0       18M   18M     0 100% /snap/amazon-ssm-agent/1480
/dev/loop2       94M   94M     0 100% /snap/core/8935
/dev/loop1       18M   18M     0 100% /snap/amazon-ssm-agent/1566
/dev/loop3       92M   92M     0 100% /snap/core/8689
tmpfs            49G     0   49G   0% /run/user/1000
the data path is /home/ubuntu/data, which is on /dev/xvda1.
	</description>
	<comments>
		<comment id='1' author='vycezhong' date='2020-04-24T19:34:00Z'>
		Hey &lt;denchmark-link:https://github.com/vycezhong&gt;@vycezhong&lt;/denchmark-link&gt;
, does this pattern happen consistently when training on a single machine, or does it happen differently each time?  If you could create a &lt;denchmark-link:https://github.com/horovod/horovod/blob/master/docs/timeline.rst&gt;timeline&lt;/denchmark-link&gt;
, that would help us to understand what's going on.
For the 8 machine case, 68% scaling efficiency is pretty good for 25Gbps network on the p3.16xlarge instances.  Normally I would suggest enabling fp16 compression to improve the scaling in this situation, but since you're using MXNet it doesn't look like we've implemented that feature in MXNet (&lt;denchmark-link:https://github.com/apeforest&gt;@apeforest&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/romerojosh&gt;@romerojosh&lt;/denchmark-link&gt;
, can you confirm if there's a way to do fp16 allreduce without doing fp16 forward/backward).
Certainly if you were to use the more expensive p3dn.24xlarge instances with 100Gbps NIC, you should expect to see closer to 90%+ scaling efficiency.
		</comment>
		<comment id='2' author='vycezhong' date='2020-04-25T08:16:53Z'>
		&lt;denchmark-link:https://github.com/tgaddair&gt;@tgaddair&lt;/denchmark-link&gt;
 it seems that this happened by accident. i cannot reproduce it now. everything's back to normal. if I encounter this situation again later, I will report in time.
Thanks for your explanation for scaling efficiency. I open a PR &lt;denchmark-link:https://github.com/horovod/horovod/pull/1908&gt;#1908&lt;/denchmark-link&gt;
 to add this feature.
		</comment>
	</comments>
</bug>