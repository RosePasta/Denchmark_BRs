<bug id='33' author='crizCraig' open_date='2017-10-10T20:12:06Z' closed_time='2017-10-12T00:59:51Z'>
	<summary>AttributeError: 'NoneType' object has no attribute 'dtype'</summary>
	<description>
Some models, like &lt;denchmark-link:https://github.com/tensorflow/models/tree/master/research/object_detection&gt;RFCN&lt;/denchmark-link&gt;
 can return variables without gradients
&lt;denchmark-code&gt;  File "train.py", line 249, in &lt;module&gt;
    tf.app.run()
  File "/home/csq/.cache/bazel/_bazel_csq/0a3580d8ecd2fafe9cc9970e974f5dc4/execroot/source/bazel-out/release_links/lib/python_env/tensorflow/python/platform/app.py", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File "train.py", line 229, in main
    worker_job_name, is_chief, FLAGS.train_dir, FLAGS.trace_every_n_steps)
  File "/home/csq/src/models/research/object_detection/trainer.py", line 271, in train
    for (gradient, var) in gradients]
  File "/home/csq/.cache/bazel/_bazel_csq/0a3580d8ecd2fafe9cc9970e974f5dc4/execroot/source/bazel-out/release_links/lib/python_env/horovod/tensorflow/__init__.py", line 76, in allreduce
    horovod_size = tf.cast(size(), tensor.dtype)
AttributeError: 'NoneType' object has no attribute 'dtype'
&lt;/denchmark-code&gt;

I just work around it by replacing compute_gradients() with
&lt;denchmark-code&gt;gradients = (super(hvd.DistributedOptimizer, training_optimizer)
             .compute_gradients(total_loss))
gradients = [(g, v) for g, v in gradients if g is not None]
if hvd.size() &gt; 1:
  with tf.name_scope(training_optimizer._name + "_Allreduce"):
    grads_and_vars = [(hvd.allreduce(gradient, device_dense=training_optimizer._device_dense,
                                                    device_sparse=training_optimizer._device_sparse), var)
                      for (gradient, var) in gradients]
&lt;/denchmark-code&gt;

but think adding
&lt;denchmark-code&gt;gradients = [(g, v) for g, v in gradients if g is not None]
&lt;/denchmark-code&gt;

after this line: &lt;denchmark-link:https://github.com/uber/horovod/blob/64317537436ae4225a004043acaa37a205a18dc3/horovod/tensorflow/__init__.py#L166&gt;https://github.com/uber/horovod/blob/64317537436ae4225a004043acaa37a205a18dc3/horovod/tensorflow/__init__.py#L166&lt;/denchmark-link&gt;

could fix it for others.
	</description>
	<comments>
		<comment id='1' author='crizCraig' date='2017-10-11T03:20:26Z'>
		Thanks &lt;denchmark-link:https://github.com/crizCraig&gt;@crizCraig&lt;/denchmark-link&gt;
 for reporting this issue!
I think the more transparent way could be to retain (None, var) in the list of computed gradients and just bypass all reduce phase for those gradients.  This seems more aligned with original optimizer interface.
Do you agree?
Thanks,
Alex
		</comment>
		<comment id='2' author='crizCraig' date='2017-10-11T21:59:46Z'>
		That sounds reasonable per &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/84e00bf64d89b8b39fe3b82fa305ee13b2a5613b/tensorflow/python/training/optimizer.py#L381-L383&gt;https://github.com/tensorflow/tensorflow/blob/84e00bf64d89b8b39fe3b82fa305ee13b2a5613b/tensorflow/python/training/optimizer.py#L381-L383&lt;/denchmark-link&gt;

The underlying optimizer removed the variables without a gradient in my case, so maybe that solution is more specific to me than I thought.
		</comment>
	</comments>
</bug>