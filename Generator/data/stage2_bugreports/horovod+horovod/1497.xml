<bug id='1497' author='alasdairm-gr' open_date='2019-11-05T09:28:15Z' closed_time='2019-11-20T21:29:59Z'>
	<summary>Memory usage issue when using Horovod with spark-submit and YARN</summary>
	<description>
Environment:

Framework: Keras (but it doesn't matter)
Framework version:
Horovod version: 0.18.2
MPI version: 4
CUDA version: N/A
NCCL version: N/A
Python version: 3.6
OS and version: N/A
GCC version: N/A

Checklist:

Did you search issues to find if somebody asked this question before? Yes
If your question is about hang, did you read this doc? N/A
If your question is about docker, did you read this doc?  N/A
Did you check if you question is answered in the troubleshooting guide? Yes

Bug report:
We have found an issue when using Horovod with YARN.  We are happy to fix this ourselves.  We would like some guidance from the Horovod community on what would be the best approach.
Scenario:

You have a YARN cluster of 4 nodes
You run horovod via spark-submit with 5 executors, 32GB per executor and 8 cores per executor

Result:

5 executors (YARN containers) are created
One node gets 2 executors (YARN containers)
That node gets 16 cores allocated
16 keras processes are started on that node through openmpi
The 16 keras processes all share 32GB memory because they are all started inside the same YARN container
Your process runs out of memory and crashes because you do not expect to have 16 keras processes sharing 32GB data; you expect them to be split into 2 separate containers with 8 processes sharing 32GB data

I think i found the code that causes this to happen:



horovod/horovod/spark/driver/mpirun_rsh.py


         Line 32
      in
      6cc9ca7






 # Since tasks with the same host hash have shared memory, we will run only 




 Horovod makes the assumption that all processes on the same host have shared memory. This is not a correct assumption when you are using YARN.
Possible solutions:
Below are the solutions I can think of.  I would appreciate some advice on other approaches.

Change the way we calculate host hashes, use one host hash per YARN container
When starting the MPI processes, somehow work out how many containers we have on each node and then start the process on the right container (not sure how this would work)

	</description>
	<comments>
		<comment id='1' author='alasdairm-gr' date='2019-11-11T16:35:45Z'>
		Hey &lt;denchmark-link:https://github.com/alasdairm-gr&gt;@alasdairm-gr&lt;/denchmark-link&gt;
, apologies for the late response.  Sounds like we need to modify the host_hash calculation in &lt;denchmark-link:https://github.com/horovod/horovod/blob/master/horovod/run/common/util/host_hash.py&gt;host_hash.py&lt;/denchmark-link&gt;
 for YARN.  If we add information about the YARN container ID (&lt;denchmark-link:https://stackoverflow.com/questions/41724300/how-do-i-get-the-yarn-containerid-from-inside-the-container&gt;example&lt;/denchmark-link&gt;
), it should make the individual containers uniquely identifiable.
		</comment>
		<comment id='2' author='alasdairm-gr' date='2019-11-20T09:50:46Z'>
		Hi &lt;denchmark-link:https://github.com/tgaddair&gt;@tgaddair&lt;/denchmark-link&gt;
, adding CONTAINER_ID environment variable to the host hash fixed this problem: &lt;denchmark-link:https://github.com/horovod/horovod/pull/1525&gt;#1525&lt;/denchmark-link&gt;
.
It took quite some effort to understand how Horovod uses MPI to launch the Python method inside the Spark executors. I have added a description of that process to the contributors.rst file in docs, together with some more comments in the code, that would have helped me understanding the mechanics faster.
Please have a look at the PR and let me know what you think.
		</comment>
		<comment id='3' author='alasdairm-gr' date='2019-11-20T21:29:59Z'>
		Thanks for the PR!  We'll go ahead and close the issue since &lt;denchmark-link:https://github.com/horovod/horovod/pull/1525&gt;#1525&lt;/denchmark-link&gt;
 has landed.
		</comment>
	</comments>
</bug>