<bug id='1680' author='sonNeturo' open_date='2020-01-21T16:31:50Z' closed_time='2020-01-21T16:37:08Z'>
	<summary>Help running the examples provided</summary>
	<description>
Environment:

Framework: pytorch
Framework version: 1.3.1
Horovod version: 0.19.0
MPI version: 4.0.1
CUDA version: V10.0.130
NCCL version: 2.3 I guess, since it's a google VM
Python version: Python 3.6.10
OS and version: Debian GNU/Linux 9 (stretch)
GCC version: g++ (Debian 6.3.0-18+deb9u1)

where running python horovod/examples/pytorch_mnist.py
I get the following error message:
&lt;denchmark-code&gt;--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --&gt; Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --&gt; Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --&gt; Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[rapids-instance:08553] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='sonNeturo' date='2020-01-21T16:38:28Z'>
		nvm it looks like it's because I had installed libopenmpi-dev.
		</comment>
		<comment id='2' author='sonNeturo' date='2020-04-22T15:54:12Z'>
		is there something wrong with having libopenmpi-dev installed?
		</comment>
	</comments>
</bug>