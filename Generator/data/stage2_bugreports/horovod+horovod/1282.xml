<bug id='1282' author='romerojosh' open_date='2019-08-07T19:23:15Z' closed_time='2019-08-10T18:09:38Z'>
	<summary>Autotuner causes unstable training behavior at startup due to possible race condition</summary>
	<description>
Environment:

Framework: Tensorflow
Framework version: 1.14.0
Horovod version: 0.16.4
MPI version: OpenMPI 3.1.4
CUDA version: 10.1
NCCL version: 2.4.8
Python version: 3.6.8
OS and version: Ubuntu 18.04.2
GCC version: 7.4.0

Bug report:
Recently, one of our developers has been running cases with TensorFlow + AMP and noticed some very unstable training behavior at startup while the autotuner is active, indicating some type of data corruption occurring with the gradients. The instability does not occur with the autotuner disabled.
We ran a lot of experiments and found that the issue appears to be related to modifications to HOROVOD_FUSION_THRESHOLD, as the training with autotuning stabilizes if we set that parameter to a fixed value.
I dug around in the code and I think it is possible that there may be a race condition which occurs when the fusion buffer size is updated by the autotuner. Since NCCL/CUDA operations are completed asynchronously w.r.t to the main Horovod thread, there seems to be a possibility that a subsequent op can trigger a reallocation of the fusion buffer before the previous NCCL/CUDA operation completes.
To test this hypothesis, I changed the  of the finalizer thread to  to force the main thread to wait until NCCL/CUDA operations complete before continuing (see  &lt;denchmark-link:https://github.com/romerojosh/horovod/commit/5983aa6115bdc2cc039c249546270d8ee7fd0160&gt;romerojosh/horovod@5983aa6&lt;/denchmark-link&gt;
) With this change, the training runs stably with the autotuner enabled and a variable fusion threshold. I don't think this is a proper fix as it introduces unnecessary serialization, however it does suggest that some type of sync is necessary when modifying the fusion threshold during training.
cc &lt;denchmark-link:https://github.com/DEKHTIARJonathan&gt;@DEKHTIARJonathan&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='romerojosh' date='2019-08-08T03:24:55Z'>
		Great catch!  &lt;denchmark-link:https://github.com/tgaddair&gt;@tgaddair&lt;/denchmark-link&gt;
, could you take a look?
		</comment>
		<comment id='2' author='romerojosh' date='2019-08-08T10:04:07Z'>
		&lt;denchmark-link:https://github.com/tgaddair&gt;@tgaddair&lt;/denchmark-link&gt;
 : In order to help you. I can run a stability test in 3-4hours. The only thing I need is a git repo and git commit hash and I'll be able to tell you if any change solved the issue ;)
		</comment>
		<comment id='3' author='romerojosh' date='2019-08-08T15:14:59Z'>
		Thanks, &lt;denchmark-link:https://github.com/DEKHTIARJonathan&gt;@DEKHTIARJonathan&lt;/denchmark-link&gt;
, I'll take a look at this hopefully some time today or tomorrow at the latest.
		</comment>
		<comment id='4' author='romerojosh' date='2019-08-10T03:12:56Z'>
		Hey &lt;denchmark-link:https://github.com/DEKHTIARJonathan&gt;@DEKHTIARJonathan&lt;/denchmark-link&gt;
, could you take a look at &lt;denchmark-link:https://github.com/horovod/horovod/pull/1288&gt;#1288&lt;/denchmark-link&gt;
?  My current hypothesis is that because the finalizer thread didn't have a handle to the fusion buffer pointer, that resetting the shared pointer in the fusion buffer manager led to the memory being reclaimed and used by something else.  With this change, we keep an extra reference count to that pointer for the duration of the finalization.
		</comment>
		<comment id='5' author='romerojosh' date='2019-08-10T10:15:51Z'>
		Testing your fix. I'm updating the PR
		</comment>
	</comments>
</bug>