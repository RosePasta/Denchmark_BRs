<bug id='1182' author='zrss' open_date='2019-06-30T04:16:26Z' closed_time='2019-07-01T07:52:45Z'>
	<summary>Horovod missing ranks (stuck at the MPI comm ?)</summary>
	<description>
Environment:

Framework: (TensorFlow, Keras, PyTorch, MXNet)
TensorFlow
Framework version:
1.8.0
Horovod version:
0.15.2
MPI version:
openmpi 4.0.0
CUDA version:
cuda 9.0.176-1
NCCL version:
2.4.2
Python version:
python 3.6
OS and version:
ubuntu 16.04

Bug report:
Please describe errorneous behavior you're observing and steps to reproduce it.
Hi all, currently, i have a case about missing ranks, and it seems stuck at the MPI comm from the stacktrace view of the process
rank 0 (partial backtrace)
&lt;denchmark-code&gt;Thread 2 (Thread 0x7fe9be0b1700 (LWP 442)):
#0  0x00007fe934790bce in mca_btl_smcuda_component_progress ()
   from /usr/local/openmpi/lib/openmpi/mca_btl_smcuda.so
#1  0x00007fe93edd3dac in opal_progress ()
   from /usr/local/openmpi/lib/libopen-pal.so.40
#2  0x00007fe934152a85 in mca_pml_ob1_recv ()
   from /usr/local/openmpi/lib/openmpi/mca_pml_ob1.so
#3  0x00007fe93f41b8ff in ompi_coll_base_gather_intra_basic_linear ()
   from /usr/local/openmpi/lib/libmpi.so.40
#4  0x00007fe93f3e518d in PMPI_Gather ()
   from /usr/local/openmpi/lib/libmpi.so.40
#5  0x00007fe93f6c4d46 in horovod::common::(anonymous namespace)::RunLoopOnce (
    state=..., is_coordinator=is_coordinator@entry=true)
    at horovod/common/operations.cc:1780
#6  0x00007fe93f6c7cab in horovod::common::(anonymous namespace)::BackgroundThreadLoop (state=...) at horovod/common/operations.cc:1629
&lt;/denchmark-code&gt;

rank X (partial backtrace)
&lt;denchmark-code&gt;Thread 2 (Thread 0x7fb7aa23a700 (LWP 439)):
#0  0x00007fb7331df76c in mca_btl_vader_component_progress ()
   from /usr/local/openmpi/lib/openmpi/mca_btl_vader.so
#1  0x00007fb7473d4dac in opal_progress ()
   from /usr/local/openmpi/lib/libopen-pal.so.40
#2  0x00007fb7479bcbd5 in ompi_request_default_wait ()
   from /usr/local/openmpi/lib/libmpi.so.40
#3  0x00007fb747a12fbb in ompi_coll_base_bcast_intra_generic ()
   from /usr/local/openmpi/lib/libmpi.so.40
#4  0x00007fb747a13532 in ompi_coll_base_bcast_intra_binomial ()
   from /usr/local/openmpi/lib/libmpi.so.40
#5  0x00007fb7329be343 in ompi_coll_tuned_bcast_intra_dec_fixed ()
   from /usr/local/openmpi/lib/openmpi/mca_coll_tuned.so
#6  0x00007fb7479d50a9 in PMPI_Bcast ()
   from /usr/local/openmpi/lib/libmpi.so.40
#7  0x00007fb747cc7700 in horovod::common::(anonymous namespace)::RunLoopOnce (
&lt;/denchmark-code&gt;

and part of the horovod training log
&lt;denchmark-code&gt;training_1/Adam/DistributedAdam_Allreduce/HorovodAllreduce_training_1_Adam_gradients_res5c_branch2c_convolution_grad_Conv2DBackpropFilter_0 [missing ranks: 3]
training_1/Adam/DistributedAdam_Allreduce/HorovodAllreduce_training_1_Adam_gradients_dense_class_8_BiasAdd_grad_BiasAddGrad_0 [missing ranks: 3]
training_1/Adam/DistributedAdam_Allreduce/HorovodAllreduce_training_1_Adam_gradients_dense_class_8_MatMul_grad_MatMul_1_0 [missing ranks: 3]
training_1/Adam/DistributedAdam_Allreduce/HorovodAllreduce_training_1_Adam_gradients_dense_regress_8_MatMul_grad_MatMul_1_0 [missing ranks: 3]
training_1/Adam/DistributedAdam_Allreduce/HorovodAllreduce_training_1_Adam_gradients_dense_regress_8_BiasAdd_grad_BiasAddGrad_0 [missing ranks: 3]
&lt;/denchmark-code&gt;

but it's very odd, rank 0 waits at the MPI_Gather and rank X waits at the MPI_Bcast ?
	</description>
	<comments>
		<comment id='1' author='zrss' date='2019-07-01T07:35:47Z'>
		&lt;denchmark-link:https://github.com/zrss&gt;@zrss&lt;/denchmark-link&gt;
, are these threads "stuck" in a sense that their stacktrace does not change over time?  Both Gather and Bcast are part of the coordination loop that runs even when nothing is being reduced, so it's possible that you captured their state at different times on the different servers.
Going back to your training log, did you paste just a subset of stalled tensors, or all of them?  If a subset, could you post full set somewhere?
		</comment>
		<comment id='2' author='zrss' date='2019-07-01T07:52:45Z'>
		thx for the reply. i get your point

Both Gather and Bcast are part of the coordination loop that runs even when nothing is being reduced

there are much stacktraces and some other stalled tensors i have not posted yet, but i re-run my job  it work fine now, thx
		</comment>
	</comments>
</bug>