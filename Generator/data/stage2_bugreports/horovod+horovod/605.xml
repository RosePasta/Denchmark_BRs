<bug id='605' author='jotterbach' open_date='2018-11-01T22:51:26Z' closed_time='2018-11-03T01:28:25Z'>
	<summary>pytorch + horovod 0.15.1 distributed optimizer not working anymore</summary>
	<description>
I just upgraded horovod 0.15.0 -&gt; 0.15.1 on a ubuntu image 4.4.0-137-generic #163-Ubuntu SMP Mon Sep 24 13:14:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. When using the DistributedOptimizer from horovod.torch I now encounter the error
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "train.py", line 641, in &lt;module&gt;
    train_images(hps)
  File "train.py", line 444, in train_images
    train_step(batch, batch_idx, epoch, hps, model, opt, train_logger)
  File "train.py", line 457, in train_step
    opt.step()
  File "/opt/conda/lib/python3.6/site-packages/horovod/torch/__init__.py", line 97, in step
    return super(self.__class__, self).step(closure)
  File "/opt/conda/lib/python3.6/site-packages/torch/optim/adamax.py", line 75, in step
    exp_avg.mul_(beta1).add_(1 - beta1, grad)
TypeError: mul_() received an invalid combination of arguments - got (numpy.float32), but expected one of:
 * (Tensor other)
      didn't match because some of the arguments have invalid types: (numpy.float32)
 * (float other)
      didn't match because some of the arguments have invalid types: (numpy.float32)
&lt;/denchmark-code&gt;

Downgrading to 0.15.0 fixes the issue. The behavior is independent of CPU, GPU or MultipleGPU training.
	</description>
	<comments>
		<comment id='1' author='jotterbach' date='2018-11-01T23:48:12Z'>
		Hey &lt;denchmark-link:https://github.com/jotterbach&gt;@jotterbach&lt;/denchmark-link&gt;
, what version of PyTorch are you using?
		</comment>
		<comment id='2' author='jotterbach' date='2018-11-02T00:27:49Z'>
		Sorry, I forgot to specify that. It's 0.4.0.
		</comment>
		<comment id='3' author='jotterbach' date='2018-11-02T16:20:06Z'>
		That's curious.  I haven't been able to repro yet, but it looks like the value of beta1 is being set to a numpy float32 when a pure Python float is expected.  Do you have a snippet of code that initializes the Adamax optimizer?
		</comment>
		<comment id='4' author='jotterbach' date='2018-11-02T19:51:14Z'>
		Sure thing &lt;denchmark-link:https://github.com/tgaddair&gt;@tgaddair&lt;/denchmark-link&gt;
 . The code is fairly straight forward:
&lt;denchmark-code&gt;if hps.cuda:
    model = model.cuda()
opt_s = torch.optim.Adamax(model.parameters(), hps.lr)
opt = hvd.DistributedOptimizer(opt_s,
                               named_parameters=model.named_parameters())

hvd.broadcast_parameters(model.state_dict(), root_rank=0)
hvd.broadcast_optimizer_state(opt, root_rank=0)
&lt;/denchmark-code&gt;

The relevant code in the training loop looks like:
&lt;denchmark-code&gt;model.train()
opt.zero_grad()
if hps.cuda:
    batch = batch.cuda()
loss = model(batch)

loss.backward()
opt.synchronize()
opt.step()
&lt;/denchmark-code&gt;

The optimizer is called nowhere else.
		</comment>
		<comment id='5' author='jotterbach' date='2018-11-02T20:45:18Z'>
		Ah, okay, I see what's going on here.  This is the result of a change we made to hvd.broadcast_optimizer_state() to broadcast the scalar options.  Looks like beta1 is getting coerced into a numpy.float32, which is causing your issue.
I'll start working on a fix for this.  Thanks for reporting!
		</comment>
	</comments>
</bug>