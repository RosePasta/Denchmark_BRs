<bug id='904' author='jaliyae' open_date='2019-03-12T08:18:58Z' closed_time='2019-03-14T07:32:40Z'>
	<summary>Latest Horovod with PyTorch error with NCCL</summary>
	<description>
Environment:

Framework: (PyTorch)
Framework version: Source
Horovod version: 0.16
MPI version: openmpi:1.10.3
CUDA version:90
NCCL version:2.3.7-1+cuda9.0
Python version:3.6
OS and version:Ubuntu 16

PyTorch with Horovod 0.16 produce the following error (repro all the time) on multi node setup with NCCL. Could not find a similar stack in previous questions. Is this a known issue? any workarounds, please let me know.
&lt;denchmark-code&gt;2019-03-12T01:47:15.000Z /      hvd.broadcast_parameters(model.state_dict(), root_rank=0)
2019-03-12T01:47:15.000Z /    File "/opt/conda/lib/python3.6/site-packages/horovod/torch/__init__.py", line 224, in broadcast_parameters
2019-03-12T01:47:15.000Z /      handle = broadcast_async_(p, root_rank, name)
2019-03-12T01:47:15.000Z /    File "/opt/conda/lib/python3.6/site-packages/horovod/torch/mpi_ops.py", line 380, in broadcast_async_
2019-03-12T01:47:15.000Z /      return _broadcast_async(tensor, tensor, root_rank, name)
2019-03-12T01:47:15.000Z /    File "/opt/conda/lib/python3.6/site-packages/horovod/torch/mpi_ops.py", line 290, in _broadcast_async
2019-03-12T01:47:15.000Z /      tensor, output, root_rank, name.encode() if name is not None else _NULL)
2019-03-12T01:47:15.000Z /  RuntimeError: CUDA error: an illegal memory access was encountered (copy_to_cpu at /opt/pytorch/aten/src/ATen/native/cuda/Copy.cu:189)
2019-03-12T01:47:15.000Z /  frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;) + 0x6a (0x7f72bcd5c88a in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10.so)
2019-03-12T01:47:15.000Z /  frame #1: (anonymous namespace)::copy_to_cpu(at::Tensor&amp;, at::Tensor const&amp;) + 0x433 (0x7f72c2544f63 in /opt/conda/lib/python3.6/site-packages/torch/lib/libcaffe2_gpu.so)
2019-03-12T01:47:15.000Z /  frame #2: void (anonymous namespace)::_copy__cuda&lt;float&gt;(at::Tensor&amp;, at::Tensor const&amp;, bool) + 0x944 (0x7f72c25d17c4 in /opt/conda/lib/python3.6/site-packages/torch/lib/libcaffe2_gpu.so)
2019-03-12T01:47:15.000Z /  frame #3: at::native::_s_copy__cuda(at::Tensor&amp;, at::Tensor const&amp;, bool) + 0x65 (0x7f72c2548325 in /opt/conda/lib/python3.6/site-packages/torch/lib/libcaffe2_gpu.so)
2019-03-12T01:47:15.000Z /  frame #4: at::native::_s_copy_from_cuda(at::Tensor const&amp;, at::Tensor const&amp;, bool) + 0x42 (0x7f72c25484e2 in /opt/conda/lib/python3.6/site-packages/torch/lib/libcaffe2_gpu.so)
2019-03-12T01:47:15.000Z /  frame #5: at::CUDAFloatType::_s_copy_from(at::Tensor const&amp;, at::Tensor const&amp;, bool) const + 0x8e (0x7f72c13066fe in /opt/conda/lib/python3.6/site-packages/torch/lib/libcaffe2_gpu.so)
2019-03-12T01:47:15.000Z /  frame #6: at::native::_s_copy__cpu(at::Tensor&amp;, at::Tensor const&amp;, bool) + 0x6d (0x7f72bd65bf0d in /opt/conda/lib/python3.6/site-packages/torch/lib/libcaffe2.so)
2019-03-12T01:47:15.000Z /  frame #7: &lt;unknown function&gt; + 0x9ca1c8 (0x7f72bd93a1c8 in /opt/conda/lib/python3.6/site-packages/torch/lib/libcaffe2.so)
2019-03-12T01:47:15.000Z /  frame #8: torch::autograd::VariableType::s_copy_(at::Tensor&amp;, at::Tensor const&amp;, bool) const + 0xcaa (0x7f72bc2b0a5a in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so.1)
2019-03-12T01:47:15.000Z /  frame #9: at::TypeDefault::copy_(at::Tensor&amp;, at::Tensor const&amp;, bool) const + 0x118 (0x7f72bda89458 in /opt/conda/lib/python3.6/site-packages/torch/lib/libcaffe2.so)
2019-03-12T01:47:15.000Z /  frame #10: at::TypeDefault::copy(at::Tensor const&amp;, bool, c10::optional&lt;c10::Device&gt;) const + 0x1db (0x7f72bda88edb in /opt/conda/lib/python3.6/site-packages/torch/lib/libcaffe2.so)
2019-03-12T01:47:15.000Z /  frame #11: at::native::to(at::Tensor const&amp;, c10::TensorOptions const&amp;, bool, bool) + 0x12fa (0x7f72bd7f48ca in /opt/conda/lib/python3.6/site-packages/torch/lib/libcaffe2.so)
2019-03-12T01:47:15.000Z /  frame #12: at::TypeDefault::to(at::Tensor const&amp;, c10::TensorOptions const&amp;, bool, bool) const + 0x2b (0x7f72bda5bb5b in /opt/conda/lib/python3.6/site-packages/torch/lib/libcaffe2.so)
2019-03-12T01:47:15.000Z /  frame #13: torch::autograd::VariableType::to(at::Tensor const&amp;, c10::TensorOptions const&amp;, bool, bool) const + 0x186 (0x7f72bc0ffc46 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so.1)
2019-03-12T01:47:15.000Z /  frame #14: &lt;unknown function&gt; + 0x79c61 (0x7f7280f50c61 in /opt/conda/lib/python3.6/site-packages/horovod/torch/mpi_lib_v2.cpython-36m-x86_64-linux-gnu.so)
2019-03-12T01:47:15.000Z /  frame #15: horovod::torch::DoBroadcastCudaOnCPU(at::Tensor, at::Tensor, int, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;) + 0xca (0x7f7280f4e4ea in /opt/conda/lib/python3.6/site-packages/horovod/torch/mpi_lib_v2.cpython-36m-x86_64-linux-gnu.so)
2019-03-12T01:47:15.000Z /  frame #16: &lt;unknown function&gt; + 0x85d98 (0x7f7280f5cd98 in /opt/conda/lib/python3.6/site-packages/horovod/torch/mpi_lib_v2.cpython-36m-x86_64-linux-gnu.so)
2019-03-12T01:47:15.000Z /  frame #17: &lt;unknown function&gt; + 0x85ece (0x7f7280f5cece in /opt/conda/lib/python3.6/site-packages/horovod/torch/mpi_lib_v2.cpython-36m-x86_64-linux-gnu.so)
2019-03-12T01:47:15.000Z /  frame #18: &lt;unknown function&gt; + 0x82ff7 (0x7f7280f59ff7 in /opt/conda/lib/python3.6/site-packages/horovod/torch/mpi_lib_v2.cpython-36m-x86_64-linux-gnu.so)


&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='jaliyae' date='2019-03-12T23:45:51Z'>
		&lt;denchmark-link:https://github.com/jaliyae&gt;@jaliyae&lt;/denchmark-link&gt;
, to help with reproduction, does this repro with simple &lt;denchmark-link:https://github.com/horovod/horovod/blob/master/examples/pytorch_mnist.py&gt;PyTorch MNIST&lt;/denchmark-link&gt;
 example?  Additionally, can you share the output of ?
		</comment>
		<comment id='2' author='jaliyae' date='2019-03-14T07:32:40Z'>
		&lt;denchmark-link:https://github.com/alsrgv&gt;@alsrgv&lt;/denchmark-link&gt;
, thank you for looking into this. I think this is a no issue, we found that only one of the machines in our cluster is causing this, probably it has a bad configruation. After removing it we no longer see this error. I will close the issue for the above reason.
		</comment>
	</comments>
</bug>