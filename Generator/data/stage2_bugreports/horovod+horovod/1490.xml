<bug id='1490' author='jasstionzyf' open_date='2019-11-01T06:43:29Z' closed_time='2019-11-01T07:02:17Z'>
	<summary>pytorch multi optimizer error</summary>
	<description>
i want to apply different learning rate at different layers within one model, so i write following code:
&lt;denchmark-code&gt;   optimizers=[]
    for layerName, lr in layersLrMap.items():
          layer = getattr(model, layerName)

          optimizer = optim.Adam(layer.parameters(), lr=lr * pt_hvd.size())
          optimizer = pt_hvd.DistributedOptimizer(optimizer, named_parameters=layer.named_parameters(),
                                                  compression=compression)
          optimizers.append(optimizer)
&lt;/denchmark-code&gt;

then
&lt;denchmark-code&gt; loss.backward()
             for optimizer in optimizers:
                 optimizer.step()
&lt;/denchmark-code&gt;

but throw error:
&lt;denchmark-code&gt;File "/data/conda/imageAI/lib/python3.7/site-packages/torch/tensor.py", line 118, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/data/conda/imageAI/lib/python3.7/site-packages/torch/autograd/__init__.py", line 93, in backward
    allow_unreachable=True)  # allow_unreachable flag
  File "/data/conda/imageAI/lib/python3.7/site-packages/horovod/torch/__init__.py", line 141, in hook
    handle, ctx = self._allreduce_grad_async(p)
  File "/data/conda/imageAI/lib/python3.7/site-packages/horovod/torch/__init__.py", line 124, in _allreduce_grad_async
    handle = allreduce_async_(tensor_compressed, average=True, name=name)
  File "/data/conda/imageAI/lib/python3.7/site-packages/horovod/torch/mpi_ops.py", line 183, in allreduce_async_
    return _allreduce_async(tensor, tensor, average, name)
  File "/data/conda/imageAI/lib/python3.7/site-packages/horovod/torch/mpi_ops.py", line 88, in _allreduce_async
    name.encode() if name is not None else _NULL)
ValueError: Requested to allreduce, allgather, or broadcast a tensor with the same name as another tensor that is currently being processed.  If you want to request another tensor, use a different tensor name.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='jasstionzyf' date='2019-11-01T07:04:24Z'>
		not bug.   i have used pytorch optimizer wrong.
should like following way to support  layer wise learning rate:
&lt;denchmark-code&gt;opt_params=[]
opt_params.append({"params": layer.parameters(), "lr": lr})
optimizer = optim.Adam(opt_params, lr=starter_learning_rate * pt_hvd.size())
optimizer = pt_hvd.DistributedOptimizer(optimizer, named_parameters=model.named_parameters(),
                                                compression=compression)
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>