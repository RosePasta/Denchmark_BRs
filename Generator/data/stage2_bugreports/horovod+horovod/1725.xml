<bug id='1725' author='kangp3' open_date='2020-02-17T15:36:45Z' closed_time='2020-02-18T01:55:06Z'>
	<summary>PID KeyError in broadcast_optimizer_state</summary>
	<description>
Environment:

Framework: PyTorch
Framework version: torch==1.4.0, torchvision==0.5.0
Horovod version: horovod==0.19.0
MPI version: Open MPI 4.0.0
CUDA version: 10.0
NCCL version: 2.4.2-1+cuda10.0
Python version: 3.7.6
OS and version: Ubuntu 18.04
GCC version: 7.4.0

Checklist:

Did you search issues to find if somebody asked this question before?
If your question is about hang, did you read this doc?
If your question is about docker, did you read this doc?
Did you check if you question is answered in the troubleshooting guide?

Bug report:
Please describe erroneous behavior you're observing and steps to reproduce it.
When calling broadcast_optimizer_state we're seeing KeyErrors when trying to access the state_dict for apparently nonexistent pids on all processes, example stack trace attached:
&lt;denchmark-code&gt;Traceback (most recent call last):
  ...
  File ************
    HVD.broadcast_optimizer_state(optimizer, root_rank=0)
  File "/usr/local/lib/python3.7/dist-packages/horovod/torch/__init__.py", line 572, in broadcast_optimizer_state
    param_state = state_dict['state'][pid]
KeyError: 140137585983888
&lt;/denchmark-code&gt;

This is running on a single multi-GPU instance, and all processes are failing the same way (although with different pids). Any pointers appreciated, thanks!
	</description>
	<comments>
		<comment id='1' author='kangp3' date='2020-02-17T15:56:44Z'>
		This script seems to consistently reproduce the error in our environment:
&lt;denchmark-code&gt;import torch
import torchvision

import horovod.torch as HVD

HVD.init()
torch.cuda.set_device(HVD.local_rank())
torch.cuda.manual_seed(20)

MODEL = torchvision.models.detection.keypointrcnn_resnet50_fpn()

optimizer = torch.optim.SGD(
    MODEL.parameters(), lr=0.001, weight_decay=1e-6, momentum=0.9, nesterov=True
)

HVD.broadcast_optimizer_state(optimizer, root_rank=0)
optimizer = HVD.DistributedOptimizer(
    optimizer, named_parameters=MODEL.named_parameters(), backward_passes_per_step=1
)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='kangp3' date='2020-02-17T21:38:40Z'>
		Thanks for raising this issue &lt;denchmark-link:https://github.com/kangp3&gt;@kangp3&lt;/denchmark-link&gt;
 and thanks for the repro &lt;denchmark-link:https://github.com/amnda-d&gt;@amnda-d&lt;/denchmark-link&gt;
.  Looks like this bug was introduced by &lt;denchmark-link:https://github.com/horovod/horovod/pull/1609&gt;#1609&lt;/denchmark-link&gt;
, where we fixed this method to skip setting state on params that do not require grads, but didn't fix the attempt to broadcast those params.
&lt;denchmark-link:https://github.com/horovod/horovod/pull/1726&gt;#1726&lt;/denchmark-link&gt;
 should fix the issue, feel free to try it out and let me know how it goes.
Thanks!
		</comment>
		<comment id='3' author='kangp3' date='2020-02-24T05:52:11Z'>
		what model are you training? I encountered with this problem when I was training faster-rcnn.
		</comment>
	</comments>
</bug>