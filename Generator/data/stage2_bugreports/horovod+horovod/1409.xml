<bug id='1409' author='yjxiong' open_date='2019-09-24T01:08:00Z' closed_time='2020-07-13T18:30:00Z'>
	<summary>SegFault when using horovod.mxnet.allgather</summary>
	<description>
Environment:

Framework: MXNet
Framework version: 1.5.0
Horovod version: 0.18.1
MPI version: 4.0.1
CUDA version: 10.0
NCCL version: 2.3.7
Python version: 3.6.8
OS and version: Linux 4.14.138-89.102.amzn1.x86_64 (EC2 instance on AWS)
GCC version: 4.8.5

Checklist:

Did you search issues to find if somebody asked this question before? Yes
If your question is about hang, did you read this doc? N/A
If your question is about docker, did you read this doc? N/A
Did you check if you question is answered in the troubleshooting guide? N/A

Bug report:
Please describe errorneous behavior you're observing and steps to reproduce it.
The function horovod.mxnet.allgather will lead to Segmentation fault: 11 when I use more than 1 worker. The minimal code block to reproduce the error is as below:
&lt;denchmark-code&gt;import horovod.mxnet as hvd
import mxnet as mx

hvd.init()

ctx = mx.gpu(hvd.local_rank())

v = mx.nd.ones((100, 50), ctx=ctx)

r = hvd.allgather(v)

print(r.shape)
print(r)
&lt;/denchmark-code&gt;

Running it with the command horovodrun -np 2 -H localhost:2 python3 test_allgather.py  will result in stack traces pasted below:
&lt;denchmark-code&gt;[1,1]&lt;stderr&gt;:
[1,1]&lt;stderr&gt;:Segmentation fault: 11
[1,1]&lt;stderr&gt;:
[1,0]&lt;stderr&gt;:
[1,0]&lt;stderr&gt;:Segmentation fault: 11
[1,0]&lt;stderr&gt;:
[1,1]&lt;stderr&gt;:Stack trace:
[1,1]&lt;stderr&gt;:  [bt] (0) /usr/local/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2e6b160) [0x7f5c59089160]
[1,1]&lt;stderr&gt;:  [bt] (1) /lib64/libc.so.6(+0x362f0) [0x7f5d3593b2f0]
[1,1]&lt;stderr&gt;:  [bt] (2) /usr/local/lib/python3.6/site-packages/mxnet/libmxnet.so(void mxnet::CopyFromToImpl&lt;mshadow::cpu, mshadow::gpu&gt;(mxnet::NDArray const&amp;, mxnet::NDArray const&amp;, mxnet::RunContext, std::vector&lt;mxnet::Resource, std::allocator&lt;mxnet::Resource&gt; &gt; const&amp;)+0x119) [0x7f5c589f7399]
[1,1]&lt;stderr&gt;:  [bt] (3) /usr/local/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x27d9895) [0x7f5c589f7895]
[1,1]&lt;stderr&gt;:  [bt] (4) /usr/local/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x25c1ce1) [0x7f5c587dfce1]
[1,1]&lt;stderr&gt;:  [bt] (5) /usr/local/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x25c80c0) [0x7f5c587e60c0]
[1,1]&lt;stderr&gt;:  [bt] (6) /usr/local/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x25c8356) [0x7f5c587e6356]
[1,1]&lt;stderr&gt;:  [bt] (7) /usr/local/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x25c0444) [0x7f5c587de444]
[1,1]&lt;stderr&gt;:  [bt] (8) /usr/lib64/libstdc++.so.6(+0xbb890) [0x7f5d29649890]
[1,0]&lt;stderr&gt;:Stack trace:
[1,0]&lt;stderr&gt;:  [bt] (0) /usr/local/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2e6b160) [0x7fd67db0b160]
[1,0]&lt;stderr&gt;:  [bt] (1) /lib64/libc.so.6(+0x362f0) [0x7fd75a3bd2f0]
[1,0]&lt;stderr&gt;:  [bt] (2) /usr/local/lib/python3.6/site-packages/mxnet/libmxnet.so(void mxnet::CopyFromToImpl&lt;mshadow::cpu, mshadow::gpu&gt;(mxnet::NDArray const&amp;, mxnet::NDArray const&amp;, mxnet::RunContext, std::vector&lt;mxnet::Resource, std::allocator&lt;mxnet::Resource&gt; &gt; const&amp;)+0x119) [0x7fd67d479399]
[1,0]&lt;stderr&gt;:  [bt] (3) /usr/local/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x27d9895) [0x7fd67d479895]
[1,0]&lt;stderr&gt;:  [bt] (4) /usr/local/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x25c1ce1) [0x7fd67d261ce1]
[1,0]&lt;stderr&gt;:  [bt] (5) /usr/local/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x25c80c0) [0x7fd67d2680c0]
[1,0]&lt;stderr&gt;:  [bt] (6) /usr/local/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x25c8356) [0x7fd67d268356]
[1,0]&lt;stderr&gt;:  [bt] (7) /usr/local/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x25c0444) [0x7fd67d260444]
[1,0]&lt;stderr&gt;:  [bt] (8) /usr/lib64/libstdc++.so.6(+0xbb890) [0x7fd74f242890]
[1,0]&lt;stderr&gt;:
[1,0]&lt;stderr&gt;:Segmentation fault: 11
[1,0]&lt;stderr&gt;:
[1,1]&lt;stderr&gt;:[ip-172-31-20-44:29671] *** Process received signal ***
[1,1]&lt;stderr&gt;:[ip-172-31-20-44:29671] Signal: Aborted (6)
[1,1]&lt;stderr&gt;:[ip-172-31-20-44:29671] Signal code:  (-6)
[1,1]&lt;stderr&gt;:[ip-172-31-20-44:29671] [ 0] /lib64/libpthread.so.0(+0xf5e0)[0x7f5d363ea5e0]
[1,1]&lt;stderr&gt;:[ip-172-31-20-44:29671] [ 1] [1,1]&lt;stderr&gt;:/lib64/libc.so.6(gsignal+0x37)[0x7f5d3593b277]
[1,1]&lt;stderr&gt;:[ip-172-31-20-44:29671] [ 2] [1,1]&lt;stderr&gt;:/lib64/libc.so.6(abort+0x148)[0x7f5d3593c968]
[1,1]&lt;stderr&gt;:[ip-172-31-20-44:29671] [ 3] [1,1]&lt;stderr&gt;:/lib64/libc.so.6(+0x78d97)[0x7f5d3597dd97]
[1,1]&lt;stderr&gt;:[ip-172-31-20-44:29671] [ 4] [1,1]&lt;stderr&gt;:/lib64/libc.so.6(+0x814f9)[0x7f5d359864f9]
[1,1]&lt;stderr&gt;:[ip-172-31-20-44:29671] [ 5] [1,1]&lt;stderr&gt;:/usr/lib64/python3.6/dist-packages/horovod/mxnet/mpi_lib.cpython-36m-x86_64-linux-gnu.so(_ZN7horovod6common11NCCLContext8ShutDownEv+0xf6)[0x7f5c174dd346]
[1,1]&lt;stderr&gt;:[ip-172-31-20-44:29671] [ 6] [1,1]&lt;stderr&gt;:/usr/lib64/python3.6/dist-packages/horovod/mxnet/mpi_lib.cpython-36m-x86_64-linux-gnu.so(+0x53f38)[0x7f5c1748bf38]
[1,1]&lt;stderr&gt;:[ip-172-31-20-44:29671] [ 7] [1,1]&lt;stderr&gt;:/usr/lib64/libstdc++.so.6(+0xbb890)[0x7f5d29649890]
[1,1]&lt;stderr&gt;:[ip-172-31-20-44:29671] [ 8] /lib64/libpthread.so.0(+0x7de5)[0x7f5d363e2de5]
[1,1]&lt;stderr&gt;:[ip-172-31-20-44:29671] [1,1]&lt;stderr&gt;:[ 9] [1,1]&lt;stderr&gt;:/lib64/libc.so.6(clone+0x6d)[0x7f5d35a02f1d]
[1,1]&lt;stderr&gt;:[ip-172-31-20-44:29671] *** End of error message ***
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun noticed that process rank 1 with PID 0 on node ip-172-31-20-44 exited on signal 6 (Aborted).
--------------------------------------------------------------------------
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='yjxiong' date='2019-09-25T21:39:48Z'>
		Hey &lt;denchmark-link:https://github.com/yuxihu&gt;@yuxihu&lt;/denchmark-link&gt;
, could you take a look at this MXNet issue when you get a chance?
		</comment>
		<comment id='2' author='yjxiong' date='2019-09-25T22:04:28Z'>
		&lt;denchmark-link:https://github.com/yjxiong&gt;@yjxiong&lt;/denchmark-link&gt;
 The allgather function is not well tested in mxnet-horovod. Currently the mxnet-horovod integration uses allreduce and broadcast functions. Any specific use case that you want to use allgather?
		</comment>
		<comment id='3' author='yjxiong' date='2019-09-26T20:35:15Z'>
		Many use cases in distributed training require Allgather, also AllScatter. For example, to gather output from all workers into each worker for model parallelism and testing. Really hoping to see this get fixed.
		</comment>
	</comments>
</bug>