<bug id='1920' author='sparticlesteve' open_date='2020-04-29T04:20:51Z' closed_time='2020-05-05T17:26:16Z'>
	<summary>hvd.load_model not properly wrapping optimizer in tf.keras 1.15</summary>
	<description>
Environment:

Framework: tf.keras
Framework version: TF 1.15.0
Horovod version: 0.19.0
MPI version: cray-mpich/7.7.10
CUDA version: n/a
NCCL version: n/a
Python version: 3.7.4
OS and version: Cray linux based on SLES 15
GCC version: 7.3.0

Checklist:

Did you search issues to find if somebody asked this question before? Oui
If your question is about hang, did you read this doc? n/a
If your question is about docker, did you read this doc? n/a
Did you check if you question is answered in the troubleshooting guide? Oui oui

Bug report:
Please describe erroneous behavior you're observing and steps to reproduce it.
I've been noticing bad training results when resuming from checkpoint with hvd.load_model. I tracked it down to diverging worker models, and noticed that my model optimizers from hvd.load_model were not properly wrapped in the horovod DistributedOptimizer. I believe it's a bug in this logic that determines all optimizer classes to wrap right here:



horovod/horovod/_keras/__init__.py


         Line 113
      in
      d1b13ec






 horovod_objects = { 





This part:
&lt;denchmark-code&gt;horovod_objects = {
    subclass.__name__.lower(): wrap_optimizer(subclass)
    for subclass in keras.optimizers.Optimizer.__subclasses__()
    if subclass.__module__ == keras.optimizers.Optimizer.__module__
}
&lt;/denchmark-code&gt;

This check on the subclass module matching Optimizer module doesn't work in TF 1.15. E.g., for the SGD optimizer, the class module (the LHS) is actually
&lt;denchmark-code&gt;In [9]: tensorflow.python.keras.optimizer_v2.gradient_descent.SGD.__module__
Out[9]: 'tensorflow.python.keras.optimizer_v2.gradient_descent'
&lt;/denchmark-code&gt;

whereas the RHS is
&lt;denchmark-code&gt;In [10]: tf.keras.optimizers.Optimizer.__module__
Out[10]: 'tensorflow.python.keras.optimizer_v2.optimizer_v2â€™
&lt;/denchmark-code&gt;

I have for now implemented a workaround in my code that just removes the module equality comparison and confirm that my optimizers are correctly being wrapped in DistributedOptimizer.
I don't have a suggestion for how to fix this in Horovod. Presumably the module paths are inconsistent across Keras and TF versions :(
	</description>
	<comments>
		<comment id='1' author='sparticlesteve' date='2020-05-01T22:07:33Z'>
		Thanks for raising this issue, &lt;denchmark-link:https://github.com/sparticlesteve&gt;@sparticlesteve&lt;/denchmark-link&gt;
.  I'll take a look this.  Presumably, this means our load_model test is not behaving correctly (or at least missing some edge cases).
		</comment>
		<comment id='2' author='sparticlesteve' date='2020-05-04T21:57:55Z'>
		Hey &lt;denchmark-link:https://github.com/sparticlesteve&gt;@sparticlesteve&lt;/denchmark-link&gt;
, I think &lt;denchmark-link:https://github.com/horovod/horovod/pull/1935&gt;#1935&lt;/denchmark-link&gt;
 should address this issue.  Please give it a shot when you have some time and let us know how it goes.  Thanks.
		</comment>
		<comment id='3' author='sparticlesteve' date='2020-05-04T22:08:28Z'>
		Awesome, &lt;denchmark-link:https://github.com/tgaddair&gt;@tgaddair&lt;/denchmark-link&gt;
 , thanks for the quick turnaround!
I will report back when I can.
		</comment>
	</comments>
</bug>