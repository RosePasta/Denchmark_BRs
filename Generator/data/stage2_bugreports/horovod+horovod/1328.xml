<bug id='1328' author='qingyu-wang' open_date='2019-08-21T02:48:01Z' closed_time='2019-08-22T08:20:59Z'>
	<summary>multi-node training error: div_cpu</summary>
	<description>
Environment:

Docker: horovod/horovod:0.16.4-tf1.14.0-torch1.1.0-mxnet1.4.1-py3.6

Bug report:
I want to use horovod and apex together to try fp16 training, but when I run the demo, I got following error.
&lt;denchmark-code&gt;terminate called after throwing an instance of 'c10::Error'
  what():  "div_cpu" not implemented for 'Half' (operator() at /pytorch/build/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp.AVX2.cpp:60)
frame #0: std::function&lt;std::string ()&gt;::operator()() const + 0x11 (0x7f35b27bb441 in /usr/local/lib/python3.6/dist-packages/torch/lib/libc10.so)
frame #1: c10::Error::Error(c10::SourceLocation, std::string const&amp;) + 0x2a (0x7f35b27bad7a in /usr/local/lib/python3.6/dist-packages/torch/lib/libc10.so)
frame #2: &lt;unknown function&gt; + 0x14d7862 (0x7f351cdae862 in /usr/local/lib/python3.6/dist-packages/torch/lib/libcaffe2.so)
frame #3: &lt;unknown function&gt; + 0x8b38d7 (0x7f351c18a8d7 in /usr/local/lib/python3.6/dist-packages/torch/lib/libcaffe2.so)
frame #4: at::native::div_out(at::Tensor&amp;, at::Tensor const&amp;, at::Tensor const&amp;) + 0x151 (0x7f351c1864a1 in /usr/local/lib/python3.6/dist-packages/torch/lib/libcaffe2.so)
frame #5: at::native::div_(at::Tensor&amp;, c10::Scalar) + 0x35 (0x7f351c186c85 in /usr/local/lib/python3.6/dist-packages/torch/lib/libcaffe2.so)
frame #6: at::TypeDefault::div_(at::Tensor&amp;, c10::Scalar) const + 0x60 (0x7f351c5c2380 in /usr/local/lib/python3.6/dist-packages/torch/lib/libcaffe2.so)
frame #7: torch::autograd::VariableType::div_(at::Tensor&amp;, c10::Scalar) const + 0x3e3 (0x7f35a35ce433 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch.so.1)
frame #8: &lt;unknown function&gt; + 0x892ef (0x7f35116e82ef in /usr/local/lib/python3.6/dist-packages/horovod/torch/mpi_lib_v2.cpython-36m-x86_64-linux-gnu.so)
frame #9: &lt;unknown function&gt; + 0x4370c (0x7f35116a270c in /usr/local/lib/python3.6/dist-packages/horovod/torch/mpi_lib_v2.cpython-36m-x86_64-linux-gnu.so)
frame #10: &lt;unknown function&gt; + 0x46292 (0x7f35116a5292 in /usr/local/lib/python3.6/dist-packages/horovod/torch/mpi_lib_v2.cpython-36m-x86_64-linux-gnu.so)
frame #11: &lt;unknown function&gt; + 0x4968e (0x7f35116a868e in /usr/local/lib/python3.6/dist-packages/horovod/torch/mpi_lib_v2.cpython-36m-x86_64-linux-gnu.so)
frame #12: &lt;unknown function&gt; + 0xbd9e0 (0x7f35b22be9e0 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #13: &lt;unknown function&gt; + 0x76db (0x7f35b78466db in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #14: clone + 0x3f (0x7f35b7b7f88f in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::Error'
  what():  "div_cpu" not implemented for 'Half' (operator() at /pytorch/build/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp.AVX2.cpp:60)
frame #0: std::function&lt;std::string ()&gt;::operator()() const + 0x11 (0x7fbd1b40b441 in /usr/local/lib/python3.6/dist-packages/torch/lib/libc10.so)
frame #1: c10::Error::Error(c10::SourceLocation, std::string const&amp;) + 0x2a (0x7fbd1b40ad7a in /usr/local/lib/python3.6/dist-packages/torch/lib/libc10.so)
frame #2: &lt;unknown function&gt; + 0x14d7862 (0x7fbc859fe862 in /usr/local/lib/python3.6/dist-packages/torch/lib/libcaffe2.so)
frame #3: &lt;unknown function&gt; + 0x8b38d7 (0x7fbc84dda8d7 in /usr/local/lib/python3.6/dist-packages/torch/lib/libcaffe2.so)
frame #4: at::native::div_out(at::Tensor&amp;, at::Tensor const&amp;, at::Tensor const&amp;) + 0x151 (0x7fbc84dd64a1 in /usr/local/lib/python3.6/dist-packages/torch/lib/libcaffe2.so)
frame #5: at::native::div_(at::Tensor&amp;, c10::Scalar) + 0x35 (0x7fbc84dd6c85 in /usr/local/lib/python3.6/dist-packages/torch/lib/libcaffe2.so)
frame #6: at::TypeDefault::div_(at::Tensor&amp;, c10::Scalar) const + 0x60 (0x7fbc85212380 in /usr/local/lib/python3.6/dist-packages/torch/lib/libcaffe2.so)
frame #7: torch::autograd::VariableType::div_(at::Tensor&amp;, c10::Scalar) const + 0x3e3 (0x7fbd19d81433 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch.so.1)
frame #8: &lt;unknown function&gt; + 0x892ef (0x7fbc7a3382ef in /usr/local/lib/python3.6/dist-packages/horovod/torch/mpi_lib_v2.cpython-36m-x86_64-linux-gnu.so)
frame #9: &lt;unknown function&gt; + 0x4370c (0x7fbc7a2f270c in /usr/local/lib/python3.6/dist-packages/horovod/torch/mpi_lib_v2.cpython-36m-x86_64-linux-gnu.so)
frame #10: &lt;unknown function&gt; + 0x46292 (0x7fbc7a2f5292 in /usr/local/lib/python3.6/dist-packages/horovod/torch/mpi_lib_v2.cpython-36m-x86_64-linux-gnu.so)
frame #11: &lt;unknown function&gt; + 0x4968e (0x7fbc7a2f868e in /usr/local/lib/python3.6/dist-packages/horovod/torch/mpi_lib_v2.cpython-36m-x86_64-linux-gnu.so)
frame #12: &lt;unknown function&gt; + 0xbd9e0 (0x7fbd1af0e9e0 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #13: &lt;unknown function&gt; + 0x76db (0x7fbd204966db in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #14: clone + 0x3f (0x7fbd207cf88f in /lib/x86_64-linux-gnu/libc.so.6)
&lt;/denchmark-code&gt;

here is my demo code, if APEX = False, multi-code trainning is fine.
&lt;denchmark-code&gt;import time

import torch
import torch.backends.cudnn as cudnn
import torch.nn as nn
import torch.optim as optim
import torchvision
# Horovod
import horovod.torch as hvd

APEX = True
# Apex
if APEX:
    import apex
    from apex import amp

hvd.init()

time.sleep(0.1 * hvd.rank())
print("global [%2s/%2s], local [%2s/%2s]" % (hvd.rank(), hvd.size(), hvd.local_rank(), hvd.local_size()))
time.sleep(0.1 * (hvd.size() - hvd.rank()))

torch.cuda.set_device(hvd.local_rank())
cudnn.benchmark = True

cudnn.deterministic = True

torch.manual_seed(4)
torch.cuda.manual_seed_all(4)

model = torchvision.models.resnet50()

optimizer = optim.SGD(model.parameters(), lr=0.001, weight_decay=1e-4)

# Horovod
FP16_ALLREDUCE = True
optimizer = hvd.DistributedOptimizer(
    optimizer,
    named_parameters=model.named_parameters(),
    compression=hvd.Compression.fp16 if APEX and FP16_ALLREDUCE else hvd.Compression.none
)

# Apex
SYNC_BN = True
if APEX and SYNC_BN:
    model = apex.parallel.convert_syncbn_model(model)

model.cuda()

# Horovod
hvd.broadcast_parameters(model.state_dict(), root_rank=0)
hvd.broadcast_optimizer_state(optimizer, root_rank=0)

# Apex
if APEX:
    OPT_LEVEL = "O1"
    KEEP_BATCHNORM_FP32 = None
    LOSS_SCALE = True
    model, optimizer = amp.initialize(model, optimizer,
        opt_level=OPT_LEVEL,
        keep_batchnorm_fp32=KEEP_BATCHNORM_FP32,
        loss_scale=LOSS_SCALE
    )

criterion = nn.CrossEntropyLoss().cuda()


_inputs = (torch.LongTensor(64, 3, 224, 224).random_().cuda() % 255).float().add_(-127.5).mul_(1/255)

targets = (torch.LongTensor(64).random_().cuda() % 1000)

epochs = 5
batches = 1
for epoch_idx in range(epochs):
    for batch_idx in range(batches):

        if hvd.rank() == 0:
            print("\n\nepoch: %s, batch: %s" % (epoch_idx, batch_idx))

        seed = epoch_idx * batches + batch_idx
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)

        inputs = _inputs + (torch.LongTensor(64, 3, 224, 224).random_().cuda() % 255).float().add_(-127.5).mul_(1/25500)
        if hvd.rank() == 0:
            print(inputs[0, :, 0, 0])

        outputs = model(inputs)
        loss = criterion(outputs, targets)

        optimizer.zero_grad()

        # Apex
        if APEX:
            with amp.scale_loss(loss, optimizer) as scaled_loss:
                scaled_loss.backward()
        else:
            loss.backward()

        optimizer.step()

        if hvd.rank() == 0:
            print(targets.detach().cpu().numpy())
            print(outputs.detach().cpu().numpy().argmax(axis=1))
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='qingyu-wang' date='2019-08-22T08:20:40Z'>
		&lt;denchmark-link:https://gist.github.com/alsrgv/0713add50fe49a409316832a31612dde&gt;https://gist.github.com/alsrgv/0713add50fe49a409316832a31612dde&lt;/denchmark-link&gt;

i try this code and solved the problem
		</comment>
		<comment id='2' author='qingyu-wang' date='2019-09-26T05:57:57Z'>
		How do you solve this problem?ï¼ŒI have the same problem. Can I parse the code?
Thank you!
		</comment>
	</comments>
</bug>