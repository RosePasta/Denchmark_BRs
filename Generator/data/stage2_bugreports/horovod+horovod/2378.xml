<bug id='2378' author='bureddy' open_date='2020-10-15T18:36:38Z' closed_time='2020-10-15T18:42:39Z'>
	<summary>Collectives performance</summary>
	<description>
Environment:

Framework: TensorFlow-CPU
Framework version: 2.3.1
Horovod version: 0.20.3
MPI version: OpenMPI 4.0.4rc3
CUDA version: N/A
NCCL version: N/A
Python version: 3.6.9
Spark / PySpark version:
OS and version: Ubuntu 18.04
GCC version:
CMake version:

Checklist:

Did you search issues to find if somebody asked this question before?
If your question is about hang, did you read this doc?
If your question is about docker, did you read this doc?
Did you check if you question is answered in the troubleshooting guide?

Bug report:
I'm seeing Collective performance is significantly lower with TF-CPU/HOROVOD compared to native MPI performance with OSU microbenchmarks. In this specific case, it is a simple 5MB all to all .
the below results are with 1 process per node, process bind to CPU socket, 8 nodes, message size:5MB.
MPI                      : ~ 3000 us
TF/HOROVOD      : ~30000 us
I have put the timers inside MPI library around the  MPI_Alltoallv to get accurate timings in both cases.
It seems the Issue looks like related to the noise coming from existing of many threads in TF.
is this expected behavior? is there any tining will improve the collective performance.
	</description>
	<comments>
		<comment id='1' author='bureddy' date='2020-10-15T18:42:52Z'>
		opened as question
		</comment>
	</comments>
</bug>