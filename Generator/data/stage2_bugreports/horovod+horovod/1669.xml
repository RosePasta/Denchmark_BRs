<bug id='1669' author='ahmadki' open_date='2020-01-14T21:15:07Z' closed_time='2020-07-13T18:30:00Z'>
	<summary>mxnet allgather is borken. it crashed when using GPU and produces wrong results when using CPU</summary>
	<description>
Environment:

Framework: MXNet
Framework version: 1.5.1
Horovod version: built from source
MPI version: 3.1.1
CUDA version: 10.2
NCCL version: 2.5.6
Python version: 3.6.9
OS and version: Linux Ubuntu 18.04.3
GCC version: 7.4.0

Checklist:

Did you search issues to find if somebody asked this question before?
If your question is about hang, did you read this doc? Yes
If your question is about docker, did you read this doc?
Did you check if you question is answered in the troubleshooting guide?


Following PR &lt;denchmark-link:https://github.com/horovod/horovod/pull/1639&gt;#1639&lt;/denchmark-link&gt;
,  in mxnet is still borken. It produces incorrect results when using CPU and crashes when using GPUs
I installed horovod from source:
git clone --recurse-submodules -j8 https://github.com/horovod/horovod.git
cd horovod
export HOROVOD_GPU_ALLREDUCE=NCCL
export HOROVOD_NCCL_INCLUDE=/usr/include
export HOROVOD_NCCL_LIB=/usr/lib/x86_64-linux-gnu
export HOROVOD_NCCL_LINK=SHARED
export HOROVOD_WITHOUT_PYTORCH=1
export HOROVOD_WITHOUT_TENSORFLOW=1
export HOROVOD_WITH_MXNET=1
export HOROVOD_WITH_MPI=1
ln -s /usr/local/cuda/lib64/stubs/libcuda.so ./libcuda.so.1
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$PWD
python setup.py install
Then ran the following sample code:
# CUDA_VISIBLE_DEVICES=0,1 mpiexec -n 2 --allow-run-as-root python all_gather_bug.py
import mxnet as mx
from mxnet import ndarray
import horovod.mxnet as hvd
from mpi4py import MPI

use_gpu = False
use_mpi4py = False

if use_mpi4py:
    comm = MPI.COMM_WORLD
    hvd.init(comm=comm)
    local_rank = comm.Get_rank()
else:
    hvd.init()
    local_rank = hvd.local_rank()

ctx = mx.gpu(local_rank) if use_gpu else mx.cpu(local_rank)
mx.random.seed(local_rank)

a = ndarray.random.uniform(shape=[10, 100, 100], ctx=ctx)
print("(before) a.shape = {}".format(a.shape))
print("(before) a[0]={}".format(a[0]))

a = hvd.allgather(a)
print("(after) a.shape = {}".format(a.shape))
print("(after) a[0]={}".format(a[0]))

mx.nd.waitall()
hvd.shutdown()
When using CPU, it looks like rank 0 is overwritting other ranks (ie broadcast instead of allgather):
&lt;denchmark-code&gt;(before) a.shape = (10, 100, 100)
(before) a.shape = (10, 100, 100)
(before) a[0]=
[[5.4881352e-01 5.9284461e-01 7.1518934e-01 ... 6.0783064e-01
  4.1702199e-01 9.9718481e-01]
 [7.2032452e-01 9.3255734e-01 1.1438108e-04 ... 1.8508208e-01
  2.5926229e-02 9.3154085e-01]
 [5.4966247e-01 9.4773060e-01 4.3532240e-01 ... 8.3994907e-01
  2.9090473e-01 1.2132858e-01]
 ...
 [2.0300540e-01 3.4745708e-01 6.7227858e-01 ... 9.6727759e-01
  4.6623814e-01 1.4771296e-01]
 [5.4340494e-01 6.7115563e-01 2.7836940e-01 ... 1.1400783e-02
  5.1639861e-01 5.5254018e-01]
 [5.7066756e-01 9.5268434e-01 2.8474227e-02 ... 3.9755744e-01
  6.7598689e-01 4.6260124e-01]]
&lt;NDArray 100x100 @cpu(0)&gt;
(after) a.shape = (10, 100, 100)
(before) a[0]=
[[0.52383333 0.05501367 0.03996297 ... 0.57466674 0.86663705 0.89201903]
 [0.26314485 0.8478666  0.13140848 ... 0.47916418 0.26825976 0.62353116]
 [0.60529524 0.56331843 0.08485638 ... 0.32175666 0.5547923  0.8856785 ]
 ...
 [0.8128833  0.50981677 0.89495724 ... 0.37793323 0.36698005 0.35633445]
 [0.12747145 0.10071229 0.580568   ... 0.38354123 0.4238108  0.21727636]
 [0.54174274 0.33404106 0.6885549  ... 0.9920475  0.7385572  0.32574102]]
&lt;NDArray 100x100 @cpu(1)&gt;
(after) a.shape = (10, 100, 100)
(after) a[0]=
[[5.4881352e-01 5.9284461e-01 7.1518934e-01 ... 6.0783064e-01
  4.1702199e-01 9.9718481e-01]
 [7.2032452e-01 9.3255734e-01 1.1438108e-04 ... 1.8508208e-01
  2.5926229e-02 9.3154085e-01]
 [5.4966247e-01 9.4773060e-01 4.3532240e-01 ... 8.3994907e-01
  2.9090473e-01 1.2132858e-01]
 ...
 [2.0300540e-01 3.4745708e-01 6.7227858e-01 ... 9.6727759e-01
  4.6623814e-01 1.4771296e-01]
 [5.4340494e-01 6.7115563e-01 2.7836940e-01 ... 1.1400783e-02
  5.1639861e-01 5.5254018e-01]
 [5.7066756e-01 9.5268434e-01 2.8474227e-02 ... 3.9755744e-01
  6.7598689e-01 4.6260124e-01]]
&lt;NDArray 100x100 @cpu(0)&gt;
(after) a[0]=
[[5.4881352e-01 5.9284461e-01 7.1518934e-01 ... 6.0783064e-01
  4.1702199e-01 9.9718481e-01]
 [7.2032452e-01 9.3255734e-01 1.1438108e-04 ... 1.8508208e-01
  2.5926229e-02 9.3154085e-01]
 [5.4966247e-01 9.4773060e-01 4.3532240e-01 ... 8.3994907e-01
  2.9090473e-01 1.2132858e-01]
 ...
 [2.0300540e-01 3.4745708e-01 6.7227858e-01 ... 9.6727759e-01
  4.6623814e-01 1.4771296e-01]
 [5.4340494e-01 6.7115563e-01 2.7836940e-01 ... 1.1400783e-02
  5.1639861e-01 5.5254018e-01]
 [5.7066756e-01 9.5268434e-01 2.8474227e-02 ... 3.9755744e-01
  6.7598689e-01 4.6260124e-01]]
&lt;NDArray 100x100 @cpu(1)&gt;
&lt;/denchmark-code&gt;

When using GPUs the operation fails:
&lt;denchmark-code&gt;(before) a.shape = (10, 100, 100)
(before) a[0]=
[[0.6686509  0.17409194 0.3850025  ... 0.43011498 0.0661214  0.2502998 ]
 [0.7005292  0.19000232 0.6673837  ... 0.27718288 0.16084558 0.223108  ]
 [0.96042585 0.81086403 0.54152083 ... 0.5650488  0.5196334  0.6767488 ]
 ...
 [0.96879214 0.9387428  0.04036242 ... 0.13176239 0.3436321  0.47154343]
 [0.8069018  0.91234195 0.01141495 ... 0.35816687 0.57390726 0.68393874]
 [0.72049534 0.67948174 0.44702923 ... 0.87448525 0.63809574 0.7006303 ]]
&lt;NDArray 100x100 @gpu(0)&gt;
(after) a.shape = (10, 100, 100)
(before) a.shape = (10, 100, 100)
(before) a[0]=
[[0.7685592  0.10232276 0.8685353  ... 0.93769354 0.62144864 0.21535844]
 [0.85973674 0.3420865  0.6202223  ... 0.5464046  0.41442537 0.32170743]
 [0.11786121 0.23281038 0.95843846 ... 0.17739207 0.5901362  0.28355032]
 ...
 [0.70749086 0.61171615 0.37854642 ... 0.3485002  0.29636437 0.7359518 ]
 [0.4345038  0.90834665 0.5242443  ... 0.0793817  0.40161872 0.6579807 ]
 [0.8531954  0.18177992 0.7053579  ... 0.5257004  0.24457276 0.74836564]]
&lt;NDArray 100x100 @gpu(1)&gt;
(after) a.shape = (10, 100, 100)
Traceback (most recent call last):
  File "all_gather_bug.py", line 29, in &lt;module&gt;
    print("(after) a[0]={}".format(a[0]))
  File "/opt/mxnet/python/mxnet/ndarray/ndarray.py", line 194, in __repr__
    return '\n%s\n&lt;%s %s @%s&gt;' % (str(self.asnumpy()),
  File "/opt/mxnet/python/mxnet/ndarray/ndarray.py", line 1996, in asnumpy
    ctypes.c_size_t(data.size)))
  File "/opt/mxnet/python/mxnet/base.py", line 252, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: MPI_Allgatherv failed, see MPI output for details.
Traceback (most recent call last):
  File "all_gather_bug.py", line 29, in &lt;module&gt;
    print("(after) a[0]={}".format(a[0]))
  File "/opt/mxnet/python/mxnet/ndarray/ndarray.py", line 194, in __repr__
    return '\n%s\n&lt;%s %s @%s&gt;' % (str(self.asnumpy()),
  File "/opt/mxnet/python/mxnet/ndarray/ndarray.py", line 1996, in asnumpy
    ctypes.c_size_t(data.size)))
  File "/opt/mxnet/python/mxnet/base.py", line 252, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: MPI_Allgatherv failed, see MPI output for details.
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[59002,1],1]
  Exit code:    1
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='ahmadki' date='2020-01-14T21:25:28Z'>
		There is a known issue with MXNet allgather. &lt;denchmark-link:https://github.com/horovod/horovod/issues/1409&gt;#1409&lt;/denchmark-link&gt;

We are currently working to resolve it. Will keep you posted. Thanks!
		</comment>
		<comment id='2' author='ahmadki' date='2020-01-15T00:19:44Z'>
		I believe I found the issue.
The operation receive buffer is defined to have the same size as the send buffer (instead of the sum of sizes from all ranks):
&lt;denchmark-link:https://github.com/horovod/horovod/blob/master/horovod/mxnet/mpi_ops.py#L155&gt;https://github.com/horovod/horovod/blob/master/horovod/mxnet/mpi_ops.py#L155&lt;/denchmark-link&gt;

This obviously results in truncated results on CPU and illegal memory access in CUDA.
I'm working on a PR.
		</comment>
		<comment id='3' author='ahmadki' date='2020-01-15T06:01:48Z'>
		Thanks. That'll be really great. I would love to review when it's ready.
		</comment>
		<comment id='4' author='ahmadki' date='2020-01-28T19:30:31Z'>
		&lt;denchmark-link:https://github.com/ahmadki&gt;@ahmadki&lt;/denchmark-link&gt;
 Do you need any help with this PR? Thanks!
		</comment>
		<comment id='5' author='ahmadki' date='2020-01-28T19:31:41Z'>
		There is another report of this issue (&lt;denchmark-link:https://github.com/horovod/horovod/issues/1519&gt;#1519&lt;/denchmark-link&gt;
). It would be great if you could provide an ETA for this fix. Thanks!
		</comment>
		<comment id='6' author='ahmadki' date='2020-01-30T23:10:48Z'>
		Sorry for going  AWOL. Incorrect output tensor shape was only the symptom, and the problem is much deeper than I originally though. I got tied up with other things at work and I won't be able to take a second crack at a solution before next week. A summary of my findings so far:
The allgather output nd-tensor is defined &lt;denchmark-link:https://github.com/horovod/horovod/blob/master/horovod/mxnet/mpi_ops.py#L155&gt;here&lt;/denchmark-link&gt;
, This by it self is not an issue since MPI wrappers  reshape and reallocate the output memory buffers as needed.
However, because hvd.allgather() is async, the original output tensor is returned immediately without modifications. We would expect subsequent calls to the output tensor to be blocking until it is re-evaluated but this is not the case.
The output tensor buffer is shallow copied &lt;denchmark-link:https://github.com/horovod/horovod/blob/master/horovod/mxnet/mpi_ops.cc#L127&gt;here&lt;/denchmark-link&gt;
 and changes made to it by &lt;denchmark-link:https://github.com/horovod/horovod/blob/master/horovod/common/ops/collective_operations.h#L91&gt;AllocateOutput&lt;/denchmark-link&gt;
 are not reflected on the original tensor.
So a solution (probably) should include the creation of a null tensor using ctypes to keep memory ownership at the python level ? and removing the shallow copies.
		</comment>
		<comment id='7' author='ahmadki' date='2020-06-24T04:13:22Z'>
		Any updates here ? Got the same problem recently.
		</comment>
	</comments>
</bug>