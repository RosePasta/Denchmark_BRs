<bug id='1921' author='sparticlesteve' open_date='2020-04-29T05:22:41Z' closed_time='2020-05-04T14:53:05Z'>
	<summary>Keras LR callbacks have unintended behavior when resuming from checkpoint</summary>
	<description>
Environment:

Framework: tf.keras
Framework version: TF 1.15.0
Horovod version: 0.19.0
MPI version: cray-mpich/7.7.10
CUDA version: n/a
NCCL version: n/a
Python version: 3.7.4
OS and version: Cray linux based on SLES 15
GCC version: 7.3.0

Checklist:

Did you search issues to find if somebody asked this question before? yes
If your question is about hang, did you read this doc? n/a
If your question is about docker, did you read this doc? n/a
Did you check if you question is answered in the troubleshooting guide? yes

Bug report:
Please describe erroneous behavior you're observing and steps to reproduce it.
There is a problem with the Keras learning rate callbacks (inheriting from ) as implemented when using checkpoints and resuming training. This class pulls the  from the model optimizer in :


All LR (and momentum) modifications are done with respect to that initial learning rate and the current epoch (or batch). However, if one is writing a checkpoint, the current modified learning rate and momentum are written to the checkpoint file. Then, upon loading that checkpoint and resuming training with the LR callback, it pulls the  LR as its new . Unless the user takes care to reset the optimizer's LR (and momentum) after loading from checkpoint and before training, the original schedule applied will not produce the intended schedule. The Keras Imagenet resnet50 example is affected by this, for instance:
&lt;denchmark-link:https://github.com/horovod/horovod/blob/master/examples/keras_imagenet_resnet50.py&gt;https://github.com/horovod/horovod/blob/master/examples/keras_imagenet_resnet50.py&lt;/denchmark-link&gt;

In contrast, the LR scheduler in Keras (and tf.keras) is implemented such that modifications depend on the  learning rate. This slightly different approach is therefore not affected by the checkpoint resume issue. It doesn't have any momentum correction, though.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/bab74a15d9ad6bb9066b3e31d601d6a45b1cb221/tensorflow/python/keras/callbacks.py#L1349&gt;https://github.com/tensorflow/tensorflow/blob/bab74a15d9ad6bb9066b3e31d601d6a45b1cb221/tensorflow/python/keras/callbacks.py#L1349&lt;/denchmark-link&gt;

I think there are a couple of possible reasonable solutions. One is to change the multiplier logic to match that of the Keras LR scheduler so that the new LR is a result of the multiplier times the current LR. This logic change would likely break folks' code, though. Another possible solution is to allow (or require!) the user to set the initial LR in the callback constructor. This way I can ensure that it is always set to the correct, intended value. Finally, as I alluded to above, the user can reset their optimizer LR (and appropriately scale the momentum) after loading from checkpoint and before training. However, I consider this a workaround rather than a solution.
I'm hoping I explained it clearly enough that I don't need a MWE, but I can provide one if required.
	</description>
	<comments>
		<comment id='1' author='sparticlesteve' date='2020-05-01T22:27:19Z'>
		Hey &lt;denchmark-link:https://github.com/sparticlesteve&gt;@sparticlesteve&lt;/denchmark-link&gt;
, thanks for the detailed report.  I am inclined towards letting the user optionally provide the  in the callback constructor, and only pulling it in  if it hasn't been set.  That way, we don't break the existing behavior, but provide a solution for for this scenario.
Curious if other contributors have any thoughts on this.
&lt;denchmark-link:https://github.com/alsrgv&gt;@alsrgv&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/nvcastet&gt;@nvcastet&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/romerojosh&gt;@romerojosh&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='sparticlesteve' date='2020-05-02T00:31:59Z'>
		+1 for optional initial_lr, wouldn't want to introduce a breaking change.
		</comment>
		<comment id='3' author='sparticlesteve' date='2020-05-02T01:57:10Z'>
		Should it really be optional though? It sounds like what &lt;denchmark-link:https://github.com/sparticlesteve&gt;@sparticlesteve&lt;/denchmark-link&gt;
 is describing here is erroneous behavior on restart, which really ought to be fixed. I can't imagine any user would want their LR schedule somehow dependent on if/when they restart.
		</comment>
		<comment id='4' author='sparticlesteve' date='2020-05-02T02:22:13Z'>
		Breaking existing scripts it not nice without major version bump, some people may not even be using restarts from checkpoints :-)
		</comment>
		<comment id='5' author='sparticlesteve' date='2020-05-02T14:49:40Z'>
		One option would be to add the initial_lr parameter as optional for now, but with a DeprecationWarning that we will remove the option to set initial_lr implicitly in a future version (e.g., 0.21.0).  Thoughts?
		</comment>
		<comment id='6' author='sparticlesteve' date='2020-05-04T14:11:06Z'>
		&lt;denchmark-link:https://github.com/tgaddair&gt;@tgaddair&lt;/denchmark-link&gt;
 yes, seems to be the best solution.
 in the next TF version seems to go from LR scheduler not taking any learning rate as input (only current epoch) to taking the current epoch and current LR: &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler?version=nightly&gt;https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler?version=nightly&lt;/denchmark-link&gt;

I am assuming customers can use  in a Horovod script with no problem if they want a scheduler based on current learning rate, correct?
		</comment>
		<comment id='7' author='sparticlesteve' date='2020-05-04T14:54:02Z'>
		Good point, &lt;denchmark-link:https://github.com/nvcastet&gt;@nvcastet&lt;/denchmark-link&gt;
.  Yes, users can use the Keras LR scheduler as an alternative.
		</comment>
		<comment id='8' author='sparticlesteve' date='2020-05-04T15:41:15Z'>
		Yeah, I mentioned the Keras implementations above, and this is my current workaround, but as I noted, one (perhaps minor) difference is that the Keras ones do not do any momentum correction.
		</comment>
		<comment id='9' author='sparticlesteve' date='2020-05-04T15:44:16Z'>
		Hey &lt;denchmark-link:https://github.com/sparticlesteve&gt;@sparticlesteve&lt;/denchmark-link&gt;
, can you take a look at &lt;denchmark-link:https://github.com/horovod/horovod/pull/1933&gt;#1933&lt;/denchmark-link&gt;
 and verify that it solves the issue for you?
		</comment>
		<comment id='10' author='sparticlesteve' date='2020-05-04T15:46:41Z'>
		Thanks. I'll give it a shot when I have time and report back.
		</comment>
	</comments>
</bug>