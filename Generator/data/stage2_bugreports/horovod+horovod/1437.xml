<bug id='1437' author='yselivonchyk' open_date='2019-10-09T23:17:32Z' closed_time='2019-10-13T20:55:10Z'>
	<summary>DistributedOptimzer is not compatible with keras.Optimizer</summary>
	<description>
Environment:

Framework: (TensorFlow, Keras, PyTorch, MXNet)
Framework version: 1.14
Horovod version: 0.16.4
Python version: 3.6.8

Checklist:

Did you search issues to find if somebody asked this question before?
If your question is about hang, did you read this doc?
If your question is about docker, did you read this doc?
Did you check if you question is answered in the troubleshooting guide?

Bug report:
Please describe errorneous behavior you're observing and steps to reproduce it.
Horovod DistributedOptimzer wrapper is not compatible with keras:
&lt;denchmark-code&gt;import tensorflow as tf
import horovod.tensorflow.keras as hvd

hvd.init()
opt = tf.keras.optimizers.Adam()
hopt = hvd.DistributedOptimizer(opt)
opt.get_config()
cfg = hopt.get_config()
opt_copy = opt.from_config(cfg)
opt_copy = opt.__class__.from_config(cfg)
hopt_copy = hopt.from_config(cfg) # TypeError: __init__() got an unexpected keyword argument 'learning_rate'
hopt_copy = hopt.__class__.from_config(cfg) # TypeError: __init__() got an unexpected keyword argument 'learning_rate'
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='yselivonchyk' date='2019-10-11T04:15:43Z'>
		Hey &lt;denchmark-link:https://github.com/yselivonchyk&gt;@yselivonchyk&lt;/denchmark-link&gt;
, for using  can you try importing  instead of ?
		</comment>
		<comment id='2' author='yselivonchyk' date='2019-10-11T18:05:21Z'>
		&lt;denchmark-link:https://github.com/tgaddair&gt;@tgaddair&lt;/denchmark-link&gt;
 Thank you for pointing that out. I updated the issue to reflect how hvd.tensorflow.keras is not compatible with tf.keras.optimizers.Optimizer.
		</comment>
		<comment id='3' author='yselivonchyk' date='2019-10-11T18:08:54Z'>
		To put it in context, this functionality is used by tensorflow.python.keras.models.clone_and_build_model which is called by tf.keras.experimental.export_saved_model
		</comment>
		<comment id='4' author='yselivonchyk' date='2019-10-11T18:19:57Z'>
		Hey &lt;denchmark-link:https://github.com/yselivonchyk&gt;@yselivonchyk&lt;/denchmark-link&gt;
, for the new example, can you try this instead:
&lt;denchmark-code&gt;import tensorflow as tf
import horovod.tensorflow.keras as hvd

hvd.init()
opt = tf.keras.optimizers.Adam()
hopt = hvd.DistributedOptimizer(opt)
opt.get_config()
cfg = hopt.get_config()
opt_copy = opt.from_config(cfg)
opt_copy = opt.__class__.from_config(cfg)
hopt_copy = hvd.DistributedOptimizer(opt_copy)
&lt;/denchmark-code&gt;

Basically, we never directly instantiate the DistributedOptimizer, we always create it by wrapping another optimizer.
Can you give me an example of how you're using tf.keras.experimental.export_saved_model?  Maybe we can workaround this by exporting the non-distributed optimizer instead.
		</comment>
		<comment id='5' author='yselivonchyk' date='2019-10-11T19:24:41Z'>
		&lt;denchmark-link:https://github.com/tgaddair&gt;@tgaddair&lt;/denchmark-link&gt;

I am using it with
&lt;denchmark-code&gt;model.fit(...)
if hvd.rank()==0:
    tf.contrib.saved_model.save_keras_model(model, args.model_dir)
&lt;/denchmark-code&gt;

Where tf.contrib.saved_model.save_keras_modelmaps into tf.keras.experimental.export_saved_model and than orig_optimizer.__class__.from_config(optimizer_config) inside clone_and_build_model() fails with the error # TypeError: __init__() got an unexpected keyword argument 'learning_rate'.
To get around it I had to rewrite clone_and_build_model adding next code snippet:
&lt;denchmark-code&gt;      if "horovod._keras" not in str(type(orig_optimizer)):
        optimizer = orig_optimizer.__class__.from_config(optimizer_config)
      else:
        optimizer = orig_optimizer.__class__.__bases__[0].from_config(optimizer_config)
&lt;/denchmark-code&gt;

I would assume, that for horovod optimzer a reasonable code update would be to add next method to DistributedOptimizer (&lt;denchmark-link:https://github.com/horovod/horovod/blob/master/horovod/_keras/__init__.py&gt;https://github.com/horovod/horovod/blob/master/horovod/_keras/__init__.py&lt;/denchmark-link&gt;
)
&lt;denchmark-code&gt;def from_config(cfg):
   return _DistributedOptimizer(optimizer.from_config(cfg))
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='yselivonchyk' date='2019-10-12T19:34:30Z'>
		I see, thanks for clarifying the usage &lt;denchmark-link:https://github.com/yselivonchyk&gt;@yselivonchyk&lt;/denchmark-link&gt;
.  That makes sense.  I put together a quick PR with your suggested fix (&lt;denchmark-link:https://github.com/horovod/horovod/pull/1444&gt;#1444&lt;/denchmark-link&gt;
).  Feel free to try it out and let me know if it solves the issue.
		</comment>
	</comments>
</bug>