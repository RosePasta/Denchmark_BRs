<bug id='1099' author='ildoonet' open_date='2019-05-28T01:10:09Z' closed_time='2019-05-30T01:58:02Z'>
	<summary>Duplicated Synchronization with Gradient Clipping?</summary>
	<description>
Environment:

Framework: pytorch
Framework version: 1.0
Horovod version: latest
MPI version: -
CUDA version: -
NCCL version: -
Python version: 3.6
OS and version: Ubuntu 16.04

&lt;denchmark-link:https://github.com/horovod/horovod/blob/master/horovod/torch/__init__.py#L174&gt;https://github.com/horovod/horovod/blob/master/horovod/torch/__init__.py#L174&lt;/denchmark-link&gt;

If you see this documentation, when using gradient clipping, there is duplicated(twice) synchronization. Once is when 'synchronize' is called and second one is when 'step' is called.
After I followed this documentation with gradient clipping, training speed is slower.
Any solusions?
	</description>
	<comments>
		<comment id='1' author='ildoonet' date='2019-05-29T03:34:08Z'>
		&lt;denchmark-link:https://github.com/ildoonet&gt;@ildoonet&lt;/denchmark-link&gt;
, thanks for raising this!  This regression has been introduced by &lt;denchmark-link:https://github.com/horovod/horovod/pull/597&gt;#597&lt;/denchmark-link&gt;
.
While we're thinking about a proper solution, can you specify  after  wraps original optimizer, &lt;denchmark-link:https://gist.github.com/alsrgv/0713add50fe49a409316832a31612dde&gt;like this&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='2' author='ildoonet' date='2019-05-29T05:08:47Z'>
		&lt;denchmark-link:https://github.com/alsrgv&gt;@alsrgv&lt;/denchmark-link&gt;
 Thanks for the tip. I will try with it and wait for the proper solutions.
		</comment>
		<comment id='3' author='ildoonet' date='2019-05-30T01:58:55Z'>
		&lt;denchmark-link:https://github.com/ildoonet&gt;@ildoonet&lt;/denchmark-link&gt;
, the fix was merged into master.  You can reinstall Horovod from master (or wait a bit for 0.16.3), and use , as new documentation prescribes.
		</comment>
	</comments>
</bug>