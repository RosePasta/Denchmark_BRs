<bug id='2126' author='shinleylee' open_date='2020-07-22T03:49:09Z' closed_time='2020-07-24T13:50:08Z'>
	<summary>Horovod deadlock in fork-path model</summary>
	<description>
Sagemaker: 5 * ml.m5.4xlarge， kernel: tensorflow_p36
Environment:

Framework: TensorFlow.keras
Framework version: 2.0.0
Horovod version: 0.18.2
MPI version: mpi4py 3.0.3
Python version: 3.6

Bug report:
When training a model with a fork path (in the attached graph, feature1 was embedded by two layer separately and multiplied at last after distinct processes), and a deadlock emerged right at the forked tensor, leading to a failure in training.
log：
&lt;denchmark-code&gt;[1,0]&lt;stderr&gt;:INFO:tensorflow:hvd.rank: 0, train_row_number: 1729685
[1,0]&lt;stderr&gt;:INFO:tensorflow:hvd.rank: 0, validation_row_number: 1235605
[1,1]&lt;stderr&gt;:INFO:tensorflow:hvd.rank: 1, train_row_number: 1729868
[1,1]&lt;stderr&gt;:INFO:tensorflow:hvd.rank: 1, validation_row_number: 1235605
[1,4]&lt;stderr&gt;:INFO:tensorflow:hvd.rank: 4, train_row_number: 1729977
[1,4]&lt;stderr&gt;:INFO:tensorflow:hvd.rank: 4, validation_row_number: 1235605
[1,2]&lt;stderr&gt;:INFO:tensorflow:hvd.rank: 2, train_row_number: 1729496
[1,2]&lt;stderr&gt;:INFO:tensorflow:hvd.rank: 2, validation_row_number: 1235605
[1,3]&lt;stderr&gt;:INFO:tensorflow:hvd.rank: 3, train_row_number: 1730209
[1,3]&lt;stderr&gt;:INFO:tensorflow:hvd.rank: 3, validation_row_number: 1235605
[1,0]&lt;stderr&gt;:INFO:tensorflow:hvd.rank: 0, steps per epoch: 844
[1,0]&lt;stderr&gt;:INFO:tensorflow:hvd.rank: 0, steps in validation: 604
[1,1]&lt;stderr&gt;:INFO:tensorflow:hvd.rank: 1, steps per epoch: 844
[1,1]&lt;stderr&gt;:INFO:tensorflow:hvd.rank: 1, steps in validation: 604
[1,4]&lt;stderr&gt;:INFO:tensorflow:hvd.rank: 4, steps per epoch: 844
[1,4]&lt;stderr&gt;:INFO:tensorflow:hvd.rank: 4, steps in validation: 604
[1,0]&lt;stdout&gt;:Epoch 1/6
[1,2]&lt;stderr&gt;:INFO:tensorflow:hvd.rank: 2, steps per epoch: 844
[1,2]&lt;stderr&gt;:INFO:tensorflow:hvd.rank: 2, steps in validation: 604
[1,3]&lt;stderr&gt;:INFO:tensorflow:hvd.rank: 3, steps per epoch: 844
[1,3]&lt;stderr&gt;:INFO:tensorflow:hvd.rank: 3, steps in validation: 604
[ip-10-0-222-34.ec2.internal:00114] 4 more processes have sent help message help-orte-odls-default.txt / memory not bound
[ip-10-0-222-34.ec2.internal:00114] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[1,0]&lt;stderr&gt;:[2020-07-15 08:18:52.425933: W horovod/common/stall_inspector.cc:105] One or more tensors were submitted to be reduced, gathered or 
    broadcasted by subset of ranks and are waiting for remainder of ranks for more than 60 seconds. This may indicate that different ranks are 
    trying to submit different tensors or that only subset of ranks is submitting tensors, which will cause deadlock. 
[1,0]&lt;stderr&gt;:Stalled ranks:
[1,0]&lt;stderr&gt;:0: [training/Adam_Allreduce/HorovodAllgather_training_Adam_gradients_gradients_feature1_factor_embedding_lookup_grad_Reshape_1_0]
[1,0]&lt;stderr&gt;:1: [training/Adam_Allreduce/HorovodAllgather_training_Adam_gradients_gradients_feature1_factor_embedding_lookup_grad_Reshape_1_0]
[1,0]&lt;stderr&gt;:2: [training/Adam_Allreduce/HorovodAllgather_training_Adam_gradients_gradients_feature1_factor_embedding_lookup_grad_Reshape_1_0]
[1,0]&lt;stderr&gt;:3: [training/Adam_Allreduce/HorovodAllgather_training_Adam_gradients_gradients_feature1_embedding_embedding_lookup_grad_Reshape_1_0]
[1,0]&lt;stderr&gt;:4: [training/Adam_Allreduce/HorovodAllgather_training_Adam_gradients_gradients_feature1_factor_embedding_lookup_grad_Reshape_1_0]
&lt;/denchmark-code&gt;

The steps were consistent between workers (batch size 2048).
CPU, Memory, Disk Utilization were healthy.
A minimun model to reproduce this deadlock is here:
&lt;denchmark-link:https://user-images.githubusercontent.com/25567460/88131605-f5e19980-cc0f-11ea-8a35-4305993d40e0.png&gt;&lt;/denchmark-link&gt;

The data is unlikely to be provided, but feature1 can be regarded as week_of_day indexes for instance. And feature2 can be regarded as is_weekend.
Does anyone have any ideas how this come?
(If more information needed pls let me know)
	</description>
	<comments>
		<comment id='1' author='shinleylee' date='2020-07-22T13:14:05Z'>
		Hey &lt;denchmark-link:https://github.com/shinleylee&gt;@shinleylee&lt;/denchmark-link&gt;
, seems the error is definitely coming from the fork path in the model.  4 of the workers are hitting the  path, and 1 is hitting .  Do you have a code sample I can use to reproduce the issue?  Is this running in eager mode or graph mode ()?
		</comment>
		<comment id='2' author='shinleylee' date='2020-07-23T06:56:44Z'>
		&lt;denchmark-link:https://github.com/tgaddair&gt;@tgaddair&lt;/denchmark-link&gt;
 Here is the core codes I used (which you may find a little different from the graph above):
&lt;denchmark-code&gt;def get_model(learning_rate):
    # Embedding Layers
    input_tensors = []
    embedding_tensors = []
    total_embedd_len = 0
    embedding_paras = [
        # embedding column name, distinct value count, embedding dimensionality
        ("feature1_id_encode", 200, 4),                    
        ("feature2_id_encode", 5000, 8),                      
        ("feature3_encode", 3000, 7),                
        ("feature4_id_encode", 100, 3),  
        ("is_weekend", 2, 1)
    ]
    # composing embedding layers
    for column_name, count_value, dimension_length in embedding_paras:
        total_embedd_len = total_embedd_len + dimension_length
        input_tensor = Input(name='{}'.format(column_name), shape=(1,), dtype='int32')
        embedding_tensor = Embedding(
            count_value,
            dimension_length,
            input_length=1,
            embeddings_initializer='glorot_normal',
            name='{}_embedding'.format(column_name)
        )(input_tensor)
        embedding_tensor = Flatten()(embedding_tensor)

        input_tensors.append(input_tensor)
        embedding_tensors.append(embedding_tensor)
    
    day_of_week_input = Input(name='day_of_week', shape=(1,), dtype='int32')
    day_of_week_embedding = Embedding(8, 2, input_length=1, embeddings_initializer='glorot_normal', name='day_of_week_embedding')(day_of_week_input)
    day_of_week_embedding = Flatten()(day_of_week_embedding)
    input_tensors.append(day_of_week_input)
    embedding_tensors.append(day_of_week_embedding)
    
    # WIDE component
    wide_tensor = Dense(7,
        kernel_initializer='glorot_normal',
        activation='relu',
        name='wide_dense'
    )(Concatenate()(embedding_tensors))
    
    # weekday factor
    day_of_week_factor = Embedding(8, 7, input_length=1, embeddings_initializer='glorot_normal', name='day_of_week_factor')(day_of_week_input)
    day_of_week_factor = Flatten()(day_of_week_factor)
    mul_tensor = Multiply()([wide_tensor, day_of_week_factor])
    
    output_tensor = Activation(activation='relu')(mul_tensor)
    
    model = Model(inputs=input_tensors, outputs=output_tensor)
    adam = optimizers.Adam(lr=learning_rate)
    optimizer = hvd.DistributedOptimizer(adam)
    model.compile(
        loss=MAE,
        optimizer=optimizer,
        metrics=[MSE, MSLE, MAPE, MAE],
        experimental_run_tf_function=False
    )

    if hvd.rank() == 0:
        print(model.summary())
    return model


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    # input data and model directories
    parser.add_argument('--model_dir', type=str)  # default /opt/ml/model
    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])
    parser.add_argument('--validation', type=str, default=os.environ['SM_CHANNEL_VALIDATION'])
    parser.add_argument('--current_host', type=str, default=os.environ.get('SM_CURRENT_HOST'))
    # hyperparameters sent by the client are passed as command-line arguments to the script.
    parser.add_argument('--epochs', type=int, required=True)
    parser.add_argument('--batch_size', type=int, required=True)
    parser.add_argument('--learning_rate', type=float, required=True)
    args, _ = parser.parse_known_args()

    # Horovod: initialize Horovod.
    hvd.init()

    COLUMN_NAMES = [
        "feature1_id_encode", "feature2_id_encode", "feature3_encode", "feature4_id_encode",
        "day_of_week", "is_weekend",
        "y"
    ]

    SELECT_COLUMNS = COLUMN_NAMES

    COLUMN_TYPES = [
        tf.int32, tf.int32, tf.int32, tf.int32,
        tf.int32, tf.int32,
        tf.string
    ]

    LABEL_COLUMN = 'y'

    epochs = args.epochs
    batch_size = args.batch_size
    learning_rate = args.learning_rate

    train_file_path = args.train + '/part*'
    validation_file_path = args.validation + '/part*'

    # get dataset
    raw_train_dataset = get_dataset(train_file_path, batch_size=batch_size, num_epochs=1, shuffle=True)
    raw_validation_dataset = get_dataset(validation_file_path, batch_size=batch_size, num_epochs=1, shuffle=False)
    train_dataset = raw_train_dataset.map(preprocess_fn)
    validation_dataset = raw_validation_dataset.map(preprocess_fn)
    # get row number
    train_row_number = get_row_count(train_file_path)
    validation_row_number = get_row_count(validation_file_path)

    logger.info("hvd.rank: {}, train_row_number: {}".format(hvd.rank(), train_row_number))
    logger.info("hvd.rank: {}, validation_row_number: {}".format(hvd.rank(), validation_row_number))

    base_model_dir = '/opt/ml/input/data/model'
    if os.path.isdir(base_model_dir):
        latest_model_path = get_latest_model_path(base_model_dir, base_job_name)
        logger.info("latest_model_path: {}".format(latest_model_path))
        os.system('tar -xvf {} -C {}'.format(latest_model_path, base_model_dir))
        model = tf.keras.models.load_model(os.path.join(base_model_dir, 'model.h5'))
    else:
        logger.info('Non-Incremental Training')
        model = get_model(learning_rate)

    callbacks = [
        hvd.callbacks.BroadcastGlobalVariablesCallback(0),
    ]
    if hvd.rank() == 0:
        callbacks.append(tf.keras.callbacks.ModelCheckpoint('./checkpoint-{epoch}.h5'))

    steps_per_epoch = int(train_row_number / batch_size)
    validation_steps = math.ceil(validation_row_number / batch_size)
    logger.info("hvd.rank: {}, steps per epoch: {}".format(hvd.rank(), steps_per_epoch))
    logger.info("hvd.rank: {}, steps in validation: {}".format(hvd.rank(), validation_steps))
    
    history = model.fit(
        train_dataset,
#         validation_data=validation_dataset,
        verbose=2 if hvd.rank() == 0 else 0,
        callbacks=callbacks,
        epochs=epochs
    )
&lt;/denchmark-code&gt;

some inessential components (like dataset pre-processing) are removed.
I didn't enable/disable eager mode explicitly, so I guess it executed in eager mode by default in tensorflow 2.0 (though my code is static graph style).
		</comment>
		<comment id='3' author='shinleylee' date='2020-07-23T22:10:05Z'>
		Hey &lt;denchmark-link:https://github.com/shinleylee&gt;@shinleylee&lt;/denchmark-link&gt;
, I tried reproing this locally using a small toy model with the same structure, but no luck.  One thing to try might be to upgrade to TF 2.2 and run without setting .  That way, graph mode will be used, which should allow the ops to be executed in parallel (no pre-determined order).
		</comment>
		<comment id='4' author='shinleylee' date='2020-07-24T03:08:18Z'>
		Yes, I just found that when I changed framework_version to 2.1.0, it could run smoothly. I guess there is some glitch between tensorflow 2.0.0 and horovod 0.18.2 in package compatibility in some special cases.
And so far this is acceptable for me as a workaround/solution though the root cause unknown. Any further insights are welcomed. I really appreciate your response and support, thank you!
		</comment>
		<comment id='5' author='shinleylee' date='2020-07-24T13:50:07Z'>
		Glad you got it working, &lt;denchmark-link:https://github.com/shinleylee&gt;@shinleylee&lt;/denchmark-link&gt;
.  I missed the fact that you were on Horovod 0.18.2, that may have had something to do with it.  But I think we can resolve this issue for now as the solution seems to be to upgrade TensorFlow.
		</comment>
	</comments>
</bug>