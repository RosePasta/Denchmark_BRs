<bug id='1616' author='hoangcuong2011' open_date='2019-12-26T13:07:12Z' closed_time='2020-02-21T10:58:58Z'>
	<summary>Horovodrun stops unexpectedly without any notifications</summary>
	<description>
Environment:

Framework: (TensorFlow, Keras, PyTorch, MXNet) TensorFlow Keras
Framework version:  1.15.0
Horovod version: 0.18.2
MPI version: openmpi-4.0.2
CUDA version: 10.0
NCCL version: 2.5.6
Python version: 3.6
OS and version: Ubuntu
GCC version: 5.4.0

I run my abc python file as:
horovodrun -np 4 -H localhost:4 python abc.py
Just in case: my model is a seq2seq model. It has a large amount of parameters (80Million). The batch size for training the model is 50.
I runs successfully, but then exists for no reason sometimes later (iterations 240 of 2667 from the third epoch) (see picture - the program just exists)
&lt;denchmark-link:https://user-images.githubusercontent.com/8759715/71476995-46c81580-281a-11ea-9ddd-14c535fe2d20.png&gt;&lt;/denchmark-link&gt;

I would also highlight that there is no warning, error. It just stops without any thing.
Also I run the code another time and got similar result. The only difference is that it stops somewhere in epoch 2, not epoch 3 as the above.
So I wonder whether it is a bug? Did you ever observe this before?
	</description>
	<comments>
		<comment id='1' author='hoangcuong2011' date='2019-12-26T16:57:52Z'>
		Hey &lt;denchmark-link:https://github.com/hoangcuong2011&gt;@hoangcuong2011&lt;/denchmark-link&gt;
, I've not seen this behavior before.  Does this also happen if you run without , or run with fewer processes?
The lack of any error message whatsoever implies that some other process is hard killing horovodrun.  If the worker processes were killed somehow, there would be an error message from MPI.
		</comment>
		<comment id='2' author='hoangcuong2011' date='2019-12-27T17:39:02Z'>
		&lt;denchmark-link:https://github.com/tgaddair&gt;@tgaddair&lt;/denchmark-link&gt;
: Thx for your reply. I think one of the possible reasons is that I set the steps_per_epoch wrongly. This is because If I do that properly (by dividing to hvd.size() as well) then I don't observe it happens. Modifying steps_per_epoch was the only thing I did if I recall correctly.
I will close this but when I know in details with more examples why it happens and how to fix that, I will reopen and let you know.
Thx.
		</comment>
		<comment id='3' author='hoangcuong2011' date='2019-12-28T07:31:46Z'>
		&lt;denchmark-link:https://github.com/tgaddair&gt;@tgaddair&lt;/denchmark-link&gt;
: Actually, I wanted to confirm that it happens again.
I have the same code, and I run it multiple times.
For the first time it runs OK. For the seconds it was killed around epoch 7. Because of this I reopen the issue, even though I don't know how to do next because the error seems mysterious.
So I will try runing without horovodrun, or run with fewer processes and let you know.
		</comment>
		<comment id='4' author='hoangcuong2011' date='2020-01-08T08:12:15Z'>
		&lt;denchmark-link:https://github.com/tgaddair&gt;@tgaddair&lt;/denchmark-link&gt;
: I have a Mac laptop, and I loggin GCP machines with the laptop. I run python with nohup command.
I noticed if I kept my mac access to the same session on GCP I run python all the time, I didn't have any of this problem. However, if I happened to exist the session for sometime the problem would happen at some points - horovod suddently stops unexpectedly.
Specially, It happened when turn off my laptop, or when I got some issues with the internet for a moment.
I don't think this is a bug with horovod, but rather a problem with the linux or MPI? I don't know. Any comments on this are more than appreciated. Meanwhile I will try to keep connecting to GCP machine all the time! :))
		</comment>
		<comment id='5' author='hoangcuong2011' date='2020-02-21T10:58:57Z'>
		I think I found a way to overcome this issue - instead of running with nohup, I can run horovod with &amp; command. For instance:
horovodrun -np 8 -H localhost:8 python -u mycode.py &gt; ./mylog_file 2&gt;&amp;1 &amp;
With this I did not find the issue.
		</comment>
	</comments>
</bug>