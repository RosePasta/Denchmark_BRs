<bug id='876' author='mx8435' open_date='2019-03-03T02:51:11Z' closed_time='2019-03-04T12:52:42Z'>
	<summary>Each Process requires GPU memory in TensorFlow 1.13.1</summary>
	<description>
Environment:

Framework: (TensorFlow, Keras, PyTorch, MXNet) Tensorflow
Framework version: 1.13.1
Horovod version: 0.6.0
MPI version: 4.0.0
CUDA version: 10.0
NCCL version: 2.4.2
Python version: 3.6.5
OS and version: OSX


Hi &lt;denchmark-link:https://github.com/alsrgv&gt;@alsrgv&lt;/denchmark-link&gt;
, I test  on localhost, but I find that each process occupy a gpu memoy. Is my openmpi install failed? Or NCCL install failed?
HOROVOD_TIMELINE=/tmp/timeline.json HOROVOD_GPU_ALLREDUCE=NCCL LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64:/usr/local/nccl_2.4.2/lib  mpirun --allow-run-as-root -np 8  -H localhost:8  -bind-to none -map-by slot  -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH  -x PATH  -mca pml ob1 -mca btl ^openib  python main.py
&lt;denchmark-link:https://user-images.githubusercontent.com/8417466/53690252-02a6e880-3da2-11e9-989d-e31c34588014.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='mx8435' date='2019-03-03T02:54:12Z'>
		Each horovod process (or as we call it a rank) runs on one GPU. The number of processes that you specify must match the total number of GPUs you have on all your nodes.
		</comment>
		<comment id='2' author='mx8435' date='2019-03-03T04:23:19Z'>
		&lt;denchmark-link:https://github.com/abditag2&gt;@abditag2&lt;/denchmark-link&gt;

Thanks for your reply. I've checked the number of GPUs (8) and the  option (np=8), it's match.  Is the openmpi or nccl  not install correct?
		</comment>
		<comment id='3' author='mx8435' date='2019-03-03T04:33:10Z'>
		Hey &lt;denchmark-link:https://github.com/mx8435&gt;@mx8435&lt;/denchmark-link&gt;
, is the issue here that every worker process is taking some memory from every GPU?
If so, did you remember to pin each process to a unique GPU?
&lt;denchmark-code&gt;config = tf.ConfigProto()
config.gpu_options.allow_growth = True
config.gpu_options.visible_device_list = str(hvd.local_rank())
&lt;/denchmark-code&gt;

Also, are you running with XLA enabled? XLA may cause issues with GPU memory being allocated across all processes.
		</comment>
		<comment id='4' author='mx8435' date='2019-03-03T06:25:43Z'>
		
Hey @mx8435, is the issue here that every worker process is taking some memory from every GPU?
If so, did you remember to pin each process to a unique GPU?
config = tf.ConfigProto()
config.gpu_options.allow_growth = True
config.gpu_options.visible_device_list = str(hvd.local_rank())

Also, are you running with XLA enabled? XLA may cause issues with GPU memory being allocated across all processes.

XLA is not  used in my program and config.gpu_options.visible_device_list = str(hvd.local_rank()) has been set.
		</comment>
		<comment id='5' author='mx8435' date='2019-03-03T16:10:23Z'>
		&lt;denchmark-link:https://github.com/mx8435&gt;@mx8435&lt;/denchmark-link&gt;
 do you find the solution yet?
		</comment>
		<comment id='6' author='mx8435' date='2019-03-03T16:45:57Z'>
		&lt;denchmark-link:https://github.com/mx8435&gt;@mx8435&lt;/denchmark-link&gt;
 does this problem also occur when you run standard Horovod examples?  For instance  or ?
		</comment>
		<comment id='7' author='mx8435' date='2019-03-03T21:34:52Z'>
		&lt;denchmark-link:https://github.com/mx8435&gt;@mx8435&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/xajxiang&gt;@xajxiang&lt;/denchmark-link&gt;
, this is &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/24461#issuecomment-467235703&gt;a known issue&lt;/denchmark-link&gt;
 in TF 1.13.1.  Unfortunately, the fix did not make it into TF 1.13.x branch.  You can either downgrade to TF 1.12.0 or try out .
		</comment>
		<comment id='8' author='mx8435' date='2019-03-04T00:21:14Z'>
		&lt;denchmark-link:https://github.com/alsrgv&gt;@alsrgv&lt;/denchmark-link&gt;
  but TF1.12.0 only support cuda 9.0. I see the issue releated a known issue in TF 1.13.1.
If I can close the XLA feature in 1.13.X,  then I can solve the problem
		</comment>
		<comment id='9' author='mx8435' date='2019-03-04T00:25:11Z'>
		&lt;denchmark-link:https://github.com/xajxiang&gt;@xajxiang&lt;/denchmark-link&gt;
, if you have to continue using CUDA 10, you can try . Alternatively, you can try to &lt;denchmark-link:https://www.tensorflow.org/install/source&gt;build TF from source&lt;/denchmark-link&gt;
 and disable XLA.  I’m not aware of a way to disable XLA without rebuilding TF 1.13.
		</comment>
		<comment id='10' author='mx8435' date='2019-03-04T00:32:02Z'>
		&lt;denchmark-link:https://github.com/alsrgv&gt;@alsrgv&lt;/denchmark-link&gt;
 thanks a lot,   I will try to build TF from soure and disable XLA. the solution to disable XLA is through  marco definiton or some thing?
		</comment>
		<comment id='11' author='mx8435' date='2019-03-04T01:06:18Z'>
		&lt;denchmark-link:https://github.com/xajxiang&gt;@xajxiang&lt;/denchmark-link&gt;
, the configuration script will ask whether you want XLA enabled and you can say “N”.
		</comment>
		<comment id='12' author='mx8435' date='2019-06-07T11:34:13Z'>
		Is this fixed in the upcoming TensorFlow 1.14?
		</comment>
		<comment id='13' author='mx8435' date='2019-06-08T09:12:59Z'>
		&lt;denchmark-link:https://github.com/maxhgerlach&gt;@maxhgerlach&lt;/denchmark-link&gt;
, yes, it is.
		</comment>
	</comments>
</bug>