<bug id='1677' author='brandon-biggs' open_date='2020-01-20T18:32:12Z' closed_time='2020-01-28T19:01:16Z'>
	<summary>Horovodrun can't run when translating to mpirun because of '-H'</summary>
	<description>
Environment:

Framework: (TensorFlow)
Framework version: 2.0 GPU
Horovod version: 0.19.0
MPI version: 1.10.7
CUDA version: 10.2
NCCL version: 2.5.6
Python version: 3.7
OS and version: Centos 7.7
GCC version: 4.8.5

Checklist:

Did you search issues to find if somebody asked this question before? I looked for it, couldn't find it.
If your question is about hang, did you read this doc? Not about hang
If your question is about docker, did you read this doc? Not about docker
Did you check if you question is answered in the troubleshooting guide? Not in the guide

Bug report:
Please describe erroneous behavior you're observing and steps to reproduce it.
When running horovodrun from the terminal, you can pass a hostfile with, -hostfile/--hostfile. If you use --verbose, you can see the equivalent Open MPI command. I noticed that horovodrun converts the --hostfile to -H for hosts. This isn't an exact conversion from horovodrun to mpirun, and for some reason is causing issues in my environment.
I installed openmpi with yum. There's some kind of weird bug that I haven't fully figured out yet where specifying just the host with -H causes the ORTE was unable to reliably start one or more daemons. error, but --hostfile does not cause this issue. However, because horovodrun translates both --hostfiles and -H to mpirun -H, horovodrun will not work. Because I installed openmpi from yum and didn't make any changes, I thought this may be an important bug to note for others who may also run into this issue.
Example from the &lt;denchmark-link:https://horovod.readthedocs.io/en/latest/mpirun.html&gt;documentation&lt;/denchmark-link&gt;
:

Equivalent:
&lt;denchmark-code&gt;mpirun -np 16 \
    -H server1:4,server2:4,server3:4,server4:4 \
    -bind-to none -map-by slot \
    -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH \
    -mca pml ob1 -mca btl ^openib \
    python train.py
&lt;/denchmark-code&gt;

However,
horovodrun -np 16 --hostfile hostfile python train.py
is not equivalent to:
&lt;denchmark-code&gt;mpirun -np 16 \
    --hostfile hostfile \
    -bind-to none -map-by slot \
    -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH \
    -mca pml ob1 -mca btl ^openib \
    python train.py
&lt;/denchmark-code&gt;

but is equivalent to the original, which in my case, causes problems, not sure why.
&lt;denchmark-code&gt;mpirun -np 16 \
    -H server1:4,server2:4,server3:4,server4:4 \
    -bind-to none -map-by slot \
    -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH \
    -mca pml ob1 -mca btl ^openib \
    python train.py
&lt;/denchmark-code&gt;

I'm recompiling mpi. Hopefully that will resolve my issues, but I wanted to bring this to everyone's attention. Let me know if there are any questions about this report. This may definitely be by design, or maybe there's an easy workaround that I'm just not aware of.
	</description>
	<comments>
		<comment id='1' author='brandon-biggs' date='2020-01-21T16:47:40Z'>
		Hey &lt;denchmark-link:https://github.com/brandon-biggs&gt;@brandon-biggs&lt;/denchmark-link&gt;
, were you able to resolve the issue by recompiling MPI?  I agree that the behavior is not identical to running , but I'm hesitant to add more complexity to the code without understanding where the difference in behavior is coming from.  &lt;denchmark-link:https://github.com/nvcastet&gt;@nvcastet&lt;/denchmark-link&gt;
 or &lt;denchmark-link:https://github.com/romerojosh&gt;@romerojosh&lt;/denchmark-link&gt;
, are you familiar enough with the underlying workings of MPI to know if there's a behavior difference between  and ?
		</comment>
		<comment id='2' author='brandon-biggs' date='2020-01-21T17:49:10Z'>
		&lt;denchmark-link:https://github.com/tgaddair&gt;@tgaddair&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/brandon-biggs&gt;@brandon-biggs&lt;/denchmark-link&gt;
 In the past, I have seen different behavior between  and  and saw that Open MPI had a separate logic for both code paths. I would not be surprised if for legacy reason, there is also a different behavior with  in some cases.
&lt;denchmark-link:https://github.com/brandon-biggs&gt;@brandon-biggs&lt;/denchmark-link&gt;
 We could probably call  with horovodrun but it would be nice if you could share with us a specific behavior issue you had by having the translation from  to ?
On the other hand, it is also nice to have a consistent behavior between  and  in  because it is what the user intuitively expects if he does not know the details of MPI.
		</comment>
		<comment id='3' author='brandon-biggs' date='2020-01-27T21:06:08Z'>
		&lt;denchmark-link:https://github.com/tgaddair&gt;@tgaddair&lt;/denchmark-link&gt;
 No, recompiling MPI didn't seem to resolve my issue. I understand the complexity issue. I wasn't really looking for you to solve my issue here, I just wanted to make everyone aware of this issue that I was running into. Maybe someone will stumble upon this in the future.
&lt;denchmark-link:https://github.com/nvcastet&gt;@nvcastet&lt;/denchmark-link&gt;
  doesn't work. I'm not sure why. Something to do with this version of openmpi, or a mix with Centos7 + openmpi, I really don't know. So I have to use  which means I can't use horovodrun as intended.
		</comment>
		<comment id='4' author='brandon-biggs' date='2020-01-28T14:08:24Z'>
		&lt;denchmark-link:https://github.com/brandon-biggs&gt;@brandon-biggs&lt;/denchmark-link&gt;
 I tried to google "behavior differences between --hostfile and -H MPI" but did not get anything. If you are interested in pursuing this mystery, it would be nice to get more info from the MPI guys by opening an issue at: &lt;denchmark-link:https://github.com/open-mpi/ompi/issues&gt;https://github.com/open-mpi/ompi/issues&lt;/denchmark-link&gt;
 (assuming you are using OpenMPI).
		</comment>
		<comment id='5' author='brandon-biggs' date='2020-01-28T14:31:23Z'>
		&lt;denchmark-link:https://github.com/nvcastet&gt;@nvcastet&lt;/denchmark-link&gt;
 It really was a weird issue. Doesn't really matter to me. I installed a different version from source and it's fine now, assuming I use horovodrun with that new openmpi loaded of course :)
Really just wanted to make a note of it in case someone else ran into this issue as this stumped me for quite some time while I was trying to get horovodrun to work in my environment, especially since it happened with the default yum install openmpi-devel package.
		</comment>
		<comment id='6' author='brandon-biggs' date='2020-01-28T14:47:57Z'>
		&lt;denchmark-link:https://github.com/brandon-biggs&gt;@brandon-biggs&lt;/denchmark-link&gt;
 Just for reference, which version of OpenMPI pulled by  did not work and which one did work (built from source)?
		</comment>
		<comment id='7' author='brandon-biggs' date='2020-01-28T14:54:27Z'>
		yum install openmpi-devel installed -
&lt;denchmark-code&gt;$ mpiCC --version
g++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39)
&lt;/denchmark-code&gt;

From source, looks like the same version, but with different configure flags.
&lt;denchmark-code&gt;$ mpiCC --version
g++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39)
&lt;/denchmark-code&gt;

The config flags - ./configure --prefix=./openmpi/openmpi-4.0.2/ --with-tm=/opt/pbs/ --enable-mpirun-prefix-by-default
		</comment>
		<comment id='8' author='brandon-biggs' date='2020-01-28T14:59:10Z'>
		&lt;denchmark-link:https://github.com/brandon-biggs&gt;@brandon-biggs&lt;/denchmark-link&gt;
 That is the GCC version. To get the MPI version, run .
		</comment>
		<comment id='9' author='brandon-biggs' date='2020-01-28T15:00:14Z'>
		That's embarrassing...
Sorry I was mistaken. The yum install openmpi-devel version is 1.10.7 and the source version is 4.0.2
		</comment>
		<comment id='10' author='brandon-biggs' date='2020-01-28T15:07:28Z'>
		&lt;denchmark-link:https://github.com/brandon-biggs&gt;@brandon-biggs&lt;/denchmark-link&gt;
 No problem. Just curious, can you try the default MPI with  with  being the absolute path of your MPI folder e.g. ?
		</comment>
		<comment id='11' author='brandon-biggs' date='2020-01-28T15:22:12Z'>
		&lt;denchmark-link:https://github.com/nvcastet&gt;@nvcastet&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;ssh: Could not resolve hostname localhost:4: Name or service not known
--------------------------------------------------------------------------
ORTE was unable to reliably start one or more daemons.
This usually is caused by:

* not finding the required libraries and/or binaries on
  one or more nodes. Please check your PATH and LD_LIBRARY_PATH
  settings, or configure OMPI with --enable-orterun-prefix-by-default

* lack of authority to execute on one or more specified nodes.
  Please verify your allocation and authorities.

* the inability to write startup files into /tmp (--tmpdir/orte_tmpdir_base).
  Please check with your sys admin to determine the correct location to use.

*  compilation of the orted with dynamic libraries when static are required
  (e.g., on Cray). Please check your configure cmd line and consider using
  one of the contrib/platform definitions for your system type.

* an inability to create a connection back to mpirun due to a
  lack of common network interfaces and/or no route found between
  them. Please check network connectivity (including firewalls
  and network routing requirements).
--------------------------------------------------------------------------
&lt;/denchmark-code&gt;

		</comment>
		<comment id='12' author='brandon-biggs' date='2020-01-28T15:29:28Z'>
		&lt;denchmark-link:https://github.com/brandon-biggs&gt;@brandon-biggs&lt;/denchmark-link&gt;
 Ok i found the issue: &lt;denchmark-link:https://www.mail-archive.com/devel@lists.open-mpi.org/msg18505.html&gt;https://www.mail-archive.com/devel@lists.open-mpi.org/msg18505.html&lt;/denchmark-link&gt;

In version , specifying the number of slots on the command line with  is not supported, so it considers the  as part of the hostname.
		</comment>
		<comment id='13' author='brandon-biggs' date='2020-01-28T17:23:16Z'>
		Nice find &lt;denchmark-link:https://github.com/nvcastet&gt;@nvcastet&lt;/denchmark-link&gt;
!  I think a sufficient solution to this issue is to note in our docs that  with Open MPI requires version &gt; 2.  What do you think?
		</comment>
		<comment id='14' author='brandon-biggs' date='2020-01-28T18:52:49Z'>
		&lt;denchmark-link:https://github.com/brandon-biggs&gt;@brandon-biggs&lt;/denchmark-link&gt;
  For RHEL/CentOS 7, the version 3 of MPI should be present in their repo:
 (notice the number  in the package name). But to get the latest version, you will need to compile.
		</comment>
		<comment id='15' author='brandon-biggs' date='2020-01-28T18:53:31Z'>
		&lt;denchmark-link:https://github.com/tgaddair&gt;@tgaddair&lt;/denchmark-link&gt;
  Yes that should be fine.
		</comment>
	</comments>
</bug>