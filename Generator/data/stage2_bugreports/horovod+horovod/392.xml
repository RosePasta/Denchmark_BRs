<bug id='392' author='andfoy' open_date='2018-07-20T19:05:11Z' closed_time='2018-07-23T19:04:28Z'>
	<summary>Error broadcasting Adam optimizer parameters on PyTorch</summary>
	<description>
Thanks for your great work on horovod, it is a great library that speeds up our training processes significatively.
I've tried to use the novel hvd.broadcast_optimizer_state function introduced on 0.13.10, however, it seems to fail on optimizers different from torch.optim.SGD, because they seem to define additional model parameters that are not necessarily of type torch.tensor.
For instance, if I try to use Adam as optimizer (As shown on the example snippet), the function will fail with the following traceback:
optimizer = optim.Adam(net.parameters(), lr=args.lr * args.nodes)
optimizer = hvd.DistributedOptimizer(
    optimizer, named_parameters=net.named_parameters())

if osp.exists(args.optim_snapshot) and args.rank == 0:
    optimizer.load_state_dict(torch.load(args.optim_snapshot))

hvd.broadcast_optimizer_state(optimizer, root_rank=0)
&lt;denchmark-code&gt;File "/media/SSD1/score-textseg/ref_score_net/train.py", line 327, in &lt;module&gt;
    hvd.broadcast_optimizer_state(optimizer, root_rank=0)
  File "/home/eamargffoy/anaconda3/envs/parallel/lib/python3.6/site-packages/horovod/torch/__init__.py", line 199, in broadcast_optimizer_state
    broadcast_parameters(params, root_rank)
  File "/home/eamargffoy/anaconda3/envs/parallel/lib/python3.6/site-packages/horovod/torch/__init__.py", line 152, in broadcast_parameters
    handle = broadcast_async_(p, root_rank, name)
  File "/home/eamargffoy/anaconda3/envs/parallel/lib/python3.6/site-packages/horovod/torch/mpi_ops.py", line 348, in broadcast_async_
    return _broadcast_async(tensor, tensor, root_rank, name)
  File "/home/eamargffoy/anaconda3/envs/parallel/lib/python3.6/site-packages/horovod/torch/mpi_ops.py", line 256, in _broadcast_async
    function = _check_function(_broadcast_function_factory, tensor)
  File "/home/eamargffoy/anaconda3/envs/parallel/lib/python3.6/site-packages/horovod/torch/mpi_ops.py", line 39, in _check_function
    function = function_factory(tensor)
  File "/home/eamargffoy/anaconda3/envs/parallel/lib/python3.6/site-packages/horovod/torch/mpi_ops.py", line 252, in _broadcast_function_factory
    return 'horovod_torch_broadcast_async_' + tensor.type().replace('.', '_')
AttributeError: 'int' object has no attribute 'type'
&lt;/denchmark-code&gt;

I would like to know if the above appreciation is the cause of such failure, and if it is, if I can contribute to the project by fixing it.
Thanks in advance.
	</description>
	<comments>
		<comment id='1' author='andfoy' date='2018-07-21T04:37:48Z'>
		&lt;denchmark-link:https://github.com/andfoy&gt;@andfoy&lt;/denchmark-link&gt;
, thanks for reporting this issue!
cc &lt;denchmark-link:https://github.com/tgaddair&gt;@tgaddair&lt;/denchmark-link&gt;
 - we should definitely fix it, and contributions are certainly welcome.
		</comment>
		<comment id='2' author='andfoy' date='2018-07-21T21:33:27Z'>
		Hey &lt;denchmark-link:https://github.com/andfoy&gt;@andfoy&lt;/denchmark-link&gt;
, thanks for raising this issue! Please take a look at &lt;denchmark-link:https://github.com/horovod/horovod/pull/395&gt;#395&lt;/denchmark-link&gt;
 if you get a chance and let me know if it fixes the issue for your model.
&lt;denchmark-link:https://github.com/alsrgv&gt;@alsrgv&lt;/denchmark-link&gt;
 Given the hackiness of dealing with optimizer state and all its exceptions, I think the best longterm solution may be to go back to simply serializing/deserializing the  into a blob with  once we drop support for v0.3.0.
		</comment>
		<comment id='3' author='andfoy' date='2018-07-22T19:03:21Z'>
		&lt;denchmark-link:https://github.com/tgaddair&gt;@tgaddair&lt;/denchmark-link&gt;
 I've tried PR &lt;denchmark-link:https://github.com/horovod/horovod/pull/395&gt;#395&lt;/denchmark-link&gt;
 and it seems to be working as expected! I think you can mark this issue as fixed
		</comment>
		<comment id='4' author='andfoy' date='2018-07-23T18:51:14Z'>
		&lt;denchmark-link:https://github.com/tgaddair&gt;@tgaddair&lt;/denchmark-link&gt;
, sounds good.  We'll just need to make sure broadcasting mechanics works for very large objects (vanilla MPI is limited to 2GB) and doesn't consume too much RAM.
		</comment>
		<comment id='5' author='andfoy' date='2018-07-23T19:04:28Z'>
		Thanks &lt;denchmark-link:https://github.com/andfoy&gt;@andfoy&lt;/denchmark-link&gt;
!  &lt;denchmark-link:https://github.com/horovod/horovod/pull/395&gt;#395&lt;/denchmark-link&gt;
 has been landed, so marking this as resolved.
		</comment>
	</comments>
</bug>