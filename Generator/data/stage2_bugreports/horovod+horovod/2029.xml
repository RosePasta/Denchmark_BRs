<bug id='2029' author='orwa-te' open_date='2020-06-15T21:19:32Z' closed_time='2020-06-19T16:32:49Z'>
	<summary>hvd.keras_estimator.fit() throws Py4JJavaError caused by java.io.IOException</summary>
	<description>
Environment:

Framework: (TensorFlow, Keras, PyTorch, MXNet): Tensorflow
Framework version: 2.1.0
Horovod version: 0.19.4
MPI version: 4.0.3
CUDA version: None
NCCL version: None
Python version: 3.7.0
OS and version: Ubuntu 20
GCC version: 9.3.0

Checklist:

Did you search issues to find if somebody asked this question before?
If your question is about hang, did you read this doc?
If your question is about docker, did you read this doc?
Did you check if you question is answered in the troubleshooting guide?


I am trying to execute the example linked at &lt;denchmark-link:https://github.com/horovod/horovod/blob/master/examples/keras_spark_mnist.py&gt;keras_spark_mnist.py&lt;/denchmark-link&gt;
 on my standalone Spark cluster which is an only single machine with 10 GB Ram and has one worker on the same machine.
After a few seconds of execution, I get an error at line 114 "" described below:
"Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob. : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 9, 192.168.198.131, executor 0): java.io.IOException: Cannot run program "python": error=2, No such file or directory"
What could be the problem? How do I solve it?
	</description>
	<comments>
		<comment id='1' author='orwa-te' date='2020-06-16T12:00:56Z'>
		Hi Orwa,
how do you launch that script (e.g. python keras_spark_mnist.py or spark-submit), can you give me the command line?
Did you try to set PYSPARK_PYTHON to the full path of the python binary that you are using for your keras example script?
Can you run the following pyspark connecting to your standalone cluster?
&lt;denchmark-code&gt;pyspark --master "..."
&gt;&gt;&gt; spark.range(5).show()
+---+
| id|
+---+
|  0|
|  1|
|  2|
|  3|
|  4|
+---+
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='orwa-te' date='2020-06-16T20:32:01Z'>
		
Hi Orwa,
how do you launch that script (e.g. python keras_spark_mnist.py or spark-submit), can you give me the command line?
Did you try to set PYSPARK_PYTHON to the full path of the python binary that you are using for your keras example script?
Can you run the following pyspark connecting to your standalone cluster?
pyspark --master "..."
&gt;&gt;&gt; spark.range(5).show()
+---+
| id|
+---+
|  0|
|  1|
|  2|
|  3|
|  4|
+---+


Hello EnricoMi, Thanks for your reply.
Command used to launch the program is spark-submit --master spark://192.168.198.131:7077 keras_spark_mnist.py, also tried using Jupyter Notebook and got the same result.
Anaconda is installed and I enter my virtual environment(python=3.7.0) before executing the program so that I guess PYSPRAK_PYTHON is not needed to be configured in spark-env.sh
I executed spark.range(5).show() and got the same output without any problem
		</comment>
		<comment id='3' author='orwa-te' date='2020-06-16T20:39:02Z'>
		Let me try to reproduce this.
		</comment>
		<comment id='4' author='orwa-te' date='2020-06-17T08:07:49Z'>
		Sorry, I cannot reproduce this with my standalone Spark cluster.
Can you please do the following things:

Run env PYSPRAK_PYTHON=PATH/TO/VENV/bin/python spark-submit --master spark://192.168.198.131:7077 keras_spark_mnist.py.
Change verbose=1 for hvd.KerasEstimator to verbose=3 in keras_spark_mnist.py and watch out for Task service executes command: log messages and the environment variables used for that command (probably in the executors' stderr log).

Thanks
		</comment>
		<comment id='5' author='orwa-te' date='2020-06-17T18:22:41Z'>
		
Sorry, I cannot reproduce this with my standalone Spark cluster.
Can you please do the following things:

Run env PYSPRAK_PYTHON=PATH/TO/VENV/bin/python spark-submit --master spark://192.168.198.131:7077 keras_spark_mnist.py.
Change verbose=1 for hvd.KerasEstimator to verbose=3 in keras_spark_mnist.py and watch out for Task service executes command: log messages and the environment variables used for that command (probably in the executors' stderr log).

Thanks

Well, there is a change in the output logs but unfortunately got another error....
I ran the following command env PYSPRAK_PYTHON=/home/orwa/anaconda3/envs/spark1/bin/python spark-submit --master spark://192.168.198.131:7077 keras_spark_mnist.py  where spark1 is my virtual environment and it looked that it worked but got another exception.
There were many logs in the terminal but I could exclude those that might help:
"[1,3]:Exception[1,3]:: [1,3]:apply_gradients() was called without a call to get_gradients() or _aggregate_gradients. If you're using TensorFlow 2.0, please specify experimental_run_tf_function=False in compile().[1,3]:
"Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
mpirun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:
Process name: [[57505,1],3]
Exit code:    1"
...
...
"py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job 3 cancelled part of cancelled job group horovod.spark.run.0"
		</comment>
		<comment id='6' author='orwa-te' date='2020-06-17T18:32:09Z'>
		Also, I tried setting the "PYSPARK_PYTHON" in spark-env.sh file, increasing the number of CPUs of the cluster to 4 , reducing the number of partitions, and still get the same result with the same error!
		</comment>
		<comment id='7' author='orwa-te' date='2020-06-19T04:43:05Z'>
		It looks like the  fixes the original issue for you and now surfaces an unrelated bug. &lt;denchmark-link:https://github.com/tgaddair&gt;@tgaddair&lt;/denchmark-link&gt;
 can you elaborate on the  exception?
		</comment>
		<comment id='8' author='orwa-te' date='2020-06-19T08:00:41Z'>
		&lt;denchmark-link:https://github.com/orwa-te&gt;@orwa-te&lt;/denchmark-link&gt;
 can you please provide the full stack trace / more log output of the original error where it complained about ?
		</comment>
		<comment id='9' author='orwa-te' date='2020-06-19T08:04:59Z'>
		And can you please run the following after undoing your spark-env.sh modifications from above?
&lt;denchmark-code&gt;pyspark
&gt;&gt;&gt; def fn(index, _):
...   return [index]
... 
&gt;&gt;&gt; spark.sparkContext.range(0, numSlices=10).mapPartitionsWithIndex(fn).collect()
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='orwa-te' date='2020-06-19T10:49:25Z'>
		
@orwa-te can you please provide the full stack trace / more log output of the original error where it complained about Cannot run program "python": error=2, No such file or directory"?

Check this link for the full stack trace of the logs produced by the command  : &lt;denchmark-link:https://gist.github.com/orwa-te/512674a1e43b52372f2c3f8ba96dca9b&gt;output logs&lt;/denchmark-link&gt;

Keep in mind that in this case  is removed from .
Setting the  and executing the same command mentioned above gives the result :&lt;denchmark-link:https://gist.github.com/orwa-te/94dd46c4571b7063de9425fd627ed53e&gt;output logs&lt;/denchmark-link&gt;
 (the full stack trace)
		</comment>
		<comment id='11' author='orwa-te' date='2020-06-19T10:59:50Z'>
		
And can you please run the following after undoing your spark-env.sh modifications from above?
pyspark
&gt;&gt;&gt; def fn(index, _):
...   return [index]
... 
&gt;&gt;&gt; spark.sparkContext.range(0, numSlices=10).mapPartitionsWithIndex(fn).collect()
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]


Whether PYSPARK_PYTHON is set or removed from spark-env.sh I get the same output for this example:
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
		</comment>
		<comment id='12' author='orwa-te' date='2020-06-19T13:15:44Z'>
		Do I understand this right? When you run spark-submit --master spark://192.168.198.131:7077 keras_spark_mnist.py without PYSPARK_PYTHON in spark-env.sh then you get the first provided output logs. When you add PYSPARK_PYTHON to spark-env.sh you get the apply_gradients() was called without a call to get_gradients() or _aggregate_gradients exception?
I would argue that PYSPARK_PYTHON needs to be set in spark-env.sh to enable your stand alone spark cluster to also run python code, which is a requirement of Horovod to run on your cluster. It looks like pyspark does this for you, but spark-submit does not. There is no Python installed on the spark workers machines, only in venvs. Hence PYSPARK_PYTHON need to be given to spark-submit.
		</comment>
		<comment id='13' author='orwa-te' date='2020-06-19T16:07:06Z'>
		
Do I understand this right? When you run spark-submit --master spark://192.168.198.131:7077 keras_spark_mnist.py without PYSPARK_PYTHON in spark-env.sh then you get the first provided output logs. When you add PYSPARK_PYTHON to spark-env.sh you get the apply_gradients() was called without a call to get_gradients() or _aggregate_gradients exception?
I would argue that PYSPARK_PYTHON needs to be set in spark-env.sh to enable your stand alone spark cluster to also run python code, which is a requirement of Horovod to run on your cluster. It looks like pyspark does this for you, but spark-submit does not. There is no Python installed on the spark workers machines, only in venvs. Hence PYSPARK_PYTHON need to be given to spark-submit.

You're right. I will always set PYSPARK_PYTHON path when I run the command, but now I am looking forward to seeing a solution to the problem where I got output logs 2...... that I linked above......
Thanks for your efforts
		</comment>
		<comment id='14' author='orwa-te' date='2020-06-19T16:32:48Z'>
		Ok, to keep bugs separated I will therefore close this issue. I have created &lt;denchmark-link:https://github.com/horovod/horovod/issues/2041&gt;#2041&lt;/denchmark-link&gt;
 to track the tensorflow exception.
		</comment>
	</comments>
</bug>