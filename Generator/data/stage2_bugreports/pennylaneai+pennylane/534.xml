<bug id='534' author='antalszava' open_date='2020-03-06T16:17:10Z' closed_time='2020-08-21T08:03:00Z'>
	<summary>Normalization tolerance checks not consistent between core and Qiskit plugin (np.isclose vs math.isclose)</summary>
	<description>
Specific problem
Most of the checks in PennyLane use the numpy.isclose function. In contrast to this, Qiskit has a normalization check using math.isclose.
The significance of the two functions is described more in details in the following issue:
&lt;denchmark-link:https://github.com/numpy/numpy/issues/10161&gt;numpy/numpy#10161&lt;/denchmark-link&gt;

The main issue with this situation in our case is that there might be certain cases where np.isclose will return a different result than math.isclose. In such cases, the Qiskit normalization would fail, whereas the PennyLane test suite would not pick this case up. This results in a QiskitError for any of the devices from the Qiskit plugin e.g. qiskit.aer.
Possible solutions
Overall, this issue could be resolved in two possible ways - a broader and a more problem-specific one:


Creating a generic test suite of test cases that can be parametrized based on the device. (Seems to be the more favourable case, in the long run, though would also create a potential overhead.)
Each test case in this test suite would test a specific circuit to be run on each plugin.
This test suite could be run potentially whenever there is a change in PennyLane core.
This test suite should potentially be independent of PennyLane core since it might be that a     change there would break an existing plugin. This is certainly fine in cases where there is a good reason for this and would simply be a breaking change between two releases. However, it might also be that such a break would not have any valid explanation, hinting that something needs to be investigated further.
The particular example of tolerance in Qiskit could be picked up in this latter case.


Changing to using math.isclose everywhere in our test suite. (definitely more problem-specific, might create engineering overhead in the short term)


	</description>
	<comments>
		<comment id='1' author='antalszava' date='2020-03-07T11:25:07Z'>
		Thanks &lt;denchmark-link:https://github.com/antalszava&gt;@antalszava&lt;/denchmark-link&gt;
 for catching this. I agree with your first point - it would be great to have an integration test suite package in PennyLane core, where integration test cases can be programmatically loaded and run. For example, maybe  (akin to ).
I might caution against changing to math.isclose() everywhere for two reasons:


np.isclose() works well with PyTorch and TF tensors


This doesn't so much affect the PL test suite directly


We may change to math.isclose(), and then find that another plugin was using np.isclose, and so there is a mismatch there ðŸ˜†


It would be good to identify the best places where this issue occurs --- I'm thinking
		</comment>
		<comment id='2' author='antalszava' date='2020-03-16T02:07:45Z'>
		Having given this another thought, one thing we could do is have math.isclose only for the cases where we are performing normalization in PennyLane (this is the case on the math_isclose branch). This way it will certainly pass the PL test suite (math.isclose being stricter than np.isclose) and will also avoid any plugins failing due to normalization taking place in PL core.
One thing to note is that if we changed to math.isclose everywhere, the same thing would hold. That is: even if the framework of a plugin uses np.isclose, the values coming from PennyLane core will pass such checks. This would, however, also include redefining np.allclose using math.isclose (the test suite is packed with cases for both).
When do PyTorch and TF tensors have problems with math.isclose?
Overall, perhaps it might be worth using  occasionally in PL tests as well. For example, as far as the &lt;denchmark-link:https://github.com/PennyLaneAI/pennylane/pull/520&gt;#520&lt;/denchmark-link&gt;
 PR goes (which would like to test that we have normalized with a tolerance value of , perhaps that would a good example for it:
In [1]: inputs                                                                                                                
Out[1]: 
array([0.25774962, 0.12075974, 0.17810621, 0.16814957, 0.02525843,
       0.01594345, 0.2235507 , 0.16466643, 0.23458277, 0.08877844,
       0.28940175, 0.2926239 , 0.06708705, 0.06413924, 0.27241975,
       0.16637318, 0.05604165, 0.28971237, 0.28985111, 0.01712505,
       0.21879687, 0.32273167, 0.16640956, 0.27358068, 0.11071106])

In [2]: np.sum(np.abs(inputs) ** 2)                                                                                           
Out[2]: 1.0000000087491003

In [3]: np.isclose(np.sum(np.abs(inputs) ** 2), 1, atol=10e-9)                                                                   
Out[3]: True

In [4]: np.isclose(np.sum(np.abs(inputs) ** 2), 1, atol=10e-10)                                                                  
Out[4]: True

In [5]: np.isclose(np.sum(np.abs(inputs) ** 2), 1, atol=10e-16)                                                                  
Out[5]: True

In [6]: math.isclose(np.sum(np.abs(inputs) ** 2), 1, abs_tol=10e-9)                                                              
Out[6]: True

In [7]: math.isclose(np.sum(np.abs(inputs) ** 2), 1, abs_tol=10e-10)                                                             
Out[7]: False

In [8]: math.isclose(np.sum(np.abs(inputs) ** 2), 1, abs_tol=10e-16)                                                             
Out[8]: False
		</comment>
		<comment id='3' author='antalszava' date='2020-03-16T03:56:23Z'>
		
When do PyTorch and TF tensors have problems with math.isclose?

More so this is an issue with array broadcasting I believe; np.isclose works on arrays of all shapes/dimensions, but math.isclose likely only works with scalars. Not too much of a issue, but requires probably maintaining our own array-supporting wrapped version of math.isclose().

Having given this another thought, one thing we could do is have math.isclose only for the cases where we are performing normalization in PennyLane (this is the case on the math_isclose branch).

I agree, I think this is the best solution at the moment. Let's use math.isclose() anywhere normalization is being validated, including in the codebase and in the tests.
		</comment>
		<comment id='4' author='antalszava' date='2020-08-20T06:47:17Z'>
		&lt;denchmark-link:https://github.com/antalszava&gt;@antalszava&lt;/denchmark-link&gt;
 can this issue now be closed?
		</comment>
		<comment id='5' author='antalszava' date='2020-08-21T01:25:25Z'>
		Hi &lt;denchmark-link:https://github.com/josh146&gt;@josh146&lt;/denchmark-link&gt;
,
Did a bit of more digging and came to the conclusion that this scenario indeed comes down to Qiskit specific normalization, which is independent from using . Therefore happy to close this issue and &lt;denchmark-link:https://github.com/PennyLaneAI/pennylane/pull/758&gt;#758&lt;/denchmark-link&gt;
 and revisit if we'd rethink checks overall in PennyLane. A simple difference in how the norm of the statevector is calculated seems to result in the difference for PennyLane and Qiskit.
PennyLane example
Consider the following example using AmplitudeEmbedding:
import pennylane as qml
import torch
from pennylane.templates.embeddings import AmplitudeEmbedding

device = torch.device("cpu")
inputs = torch.tensor([0.1569, 0.2157, 0.1843, 0.1098, 0.1216, 0.1490, 0.0784, 0.1137, 0.1412,
            0.2863, 0.3216, 0.2706, 0.1961, 0.2039, 0.2510, 0.1765, 0.2157, 0.2431,
            0.1843, 0.2275, 0.2157, 0.1451, 0.1490, 0.1686, 0.1216, 0.1529, 0.1647])

num_qubits = 5

dev = qml.device('qiskit.aer',wires=num_qubits)

@qml.qnode(dev, interface='torch')
def circuit(x=None, weights=None):
    AmplitudeEmbedding(x, wires=list(range(num_qubits)), pad=0., normalize=True)
    return qml.expval(qml.PauliZ(0))

print(circuit(x=inputs, weights=conv_params))
This raises the following QiskitError:
&lt;denchmark-code&gt;     56         if not math.isclose(sum(np.absolute(params) ** 2), 1.0,
     57                             abs_tol=_EPS):
---&gt; 58             raise QiskitError("Sum of amplitudes-squared does not equal one.")
&lt;/denchmark-code&gt;

However, using default.qubit, it executes fine:
&lt;denchmark-code&gt;tensor(0.2509, dtype=torch.float64)
&lt;/denchmark-code&gt;

What happens internally?
Turns out, when using qiskit.aer, this merely comes down to the following internally whenever normalization happens:
import numpy as np

n_amplitudes = 2 ** 5

inputs = torch.tensor([0.1569, 0.2157, 0.1843, 0.1098, 0.1216, 0.1490, 0.0784, 0.1137, 0.1412,
            0.2863, 0.3216, 0.2706, 0.1961, 0.2039, 0.2510, 0.1765, 0.2157, 0.2431,
            0.1843, 0.2275, 0.2157, 0.1451, 0.1490, 0.1686, 0.1216, 0.1529, 0.1647])

# Logic similar to how the input is being processed in AmplitudeEmbedding inside of PennyLane
# For pad=0. and normalize=True
padding = 0.
inputs = np.array(inputs)
features = np.pad(
    inputs, (0, n_amplitudes - len(inputs)), mode="constant", constant_values=padding
)

norm = np.linalg.norm(features, ord=2)
features = features / norm

pennylane_check = np.linalg.norm(features, ord=2)

# This is the type of check in Qiskit-Terra that fails
qiskit_check = sum(np.absolute(features) ** 2)
print(pennylane_check, qiskit_check)
&lt;denchmark-code&gt;1.0 1.0000000805594027
&lt;/denchmark-code&gt;

Both values are being checked against 1.0 with tolerance 1e-10 (consistent in PL and Qiskit). However, the Qiskit check deviates from 1 largely compared to the tolerance and fails the check.
Therefore the PennyLane normalization is passed whereas the Qiskit normalization check fails. This particular example turns out to be independent of the use of math.islose or np.isclose, as the example was constructed so that normalization would be performed with either of them.
		</comment>
		<comment id='6' author='antalszava' date='2020-08-21T08:03:00Z'>
		Ah, so the difference comes down to either
&gt;&gt;&gt; pennylane_check = np.linalg.norm(features, ord=2)
&gt;&gt;&gt; qiskit_check = sum(np.absolute(features) ** 2)
Interesting  Yes, lets close this issue and &lt;denchmark-link:https://github.com/PennyLaneAI/pennylane/pull/758&gt;#758&lt;/denchmark-link&gt;
, and we can consider what to do next
		</comment>
	</comments>
</bug>