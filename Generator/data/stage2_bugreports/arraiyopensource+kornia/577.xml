<bug id='577' author='connorlee77' open_date='2020-05-28T06:57:47Z' closed_time='2020-11-04T10:41:27Z'>
	<summary>[Bug] kornia.constants pi</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

It appears from my debugging that pi * indices.to(patch.dtype) in orientation.py causes some sort of error.
&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Switch GPU to torch.device('cuda:1')
Use laforienter in some way.
Run code using CUDA_LAUNCH_BLOCK=1 python xx.py

File "/home/x/anaconda3/envs/pt-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in call
result = self.forward(*input, **kwargs)
File "/home/x/kornia/kornia/feature/scale_space_detector.py", line 205, in forward
lafs = self.ori(lafs, img)
File "/home/x/anaconda3/envs/pt-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in call
result = self.forward(*input, **kwargs)
File "/home/x/kornia/kornia/feature/orientation.py", line 156, in forward
angles_radians: torch.Tensor = self.angle_detector(patches).view(B, N)
File "/home/x/anaconda3/envs/pt-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in call
result = self.forward(*input, **kwargs)
File "/home/x/kornia/kornia/feature/orientation.py", line 103, in forward
angle = -((2. * pi * indices.to(patch.dtype) / float(self.num_ang_bins)) - pi)
RuntimeError: CUDA error: an illegal memory access was encountered
&lt;denchmark-h:h2&gt;Expected behavior&lt;/denchmark-h&gt;

Multiplying pi and indices.to(...) should not throw an error when I'm using device 1 rather than device 0.
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;

PyTorch version: 1.5.0+cu101
Is debug build: No
CUDA used to build PyTorch: 10.1
OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
CMake version: Could not collect
Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration:
GPU 0: TITAN RTX
GPU 1: TITAN RTX
Nvidia driver version: 435.21
cuDNN version: /usr/local/cuda-10.0/lib64/libcudnn.so.7
Versions of relevant libraries:
[pip] numpy==1.18.1
[pip] numpydoc==0.9.2
[pip] torch==1.5.0+cu101
[pip] torchfile==0.1.0
[pip] torchsummary==1.5.1
[pip] torchvision==0.6.0+cu101
[conda] _tflow_select             2.3.0                       mkl
[conda] blas                      1.0                         mkl
[conda] cudatoolkit               10.1.243             h6bb024c_0
[conda] mkl                       2020.0                      166
[conda] mkl-service               2.3.0            py37he904b0f_0
[conda] mkl_fft                   1.0.15           py37ha843d7b_0
[conda] mkl_random                1.1.0            py37hd6b4f25_0
[conda] numpy                     1.18.1           py37h4f9e942_0
[conda] numpy-base                1.18.1           py37hde5b4d6_1
[conda] numpydoc                  0.9.2                      py_0
[conda] tensorflow                1.14.0          mkl_py37h45c423b_0
[conda] tensorflow-base           1.14.0          mkl_py37h7ce6ba3_0
[conda] torch                     1.5.0+cu101              pypi_0    pypi
[conda] torchfile                 0.1.0                    pypi_0    pypi
[conda] torchsummary              1.5.1                    pypi_0    pypi
[conda] torchvision               0.6.0+cu101              pypi_0    pypi
&lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;

This works as expected when I'm using device 0. Error occurs when using device 1.
	</description>
	<comments>
		<comment id='1' author='connorlee77' date='2020-08-29T09:19:53Z'>
		&lt;denchmark-link:https://github.com/connorlee77&gt;@connorlee77&lt;/denchmark-link&gt;
 is this fixed ? /cc  &lt;denchmark-link:https://github.com/ducha-aiki&gt;@ducha-aiki&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='connorlee77' date='2020-10-28T10:06:49Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions, and happy coding day üòé
		</comment>
	</comments>
</bug>