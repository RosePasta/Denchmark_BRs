<bug id='286' author='mrT23' open_date='2019-10-25T17:09:11Z' closed_time='2019-12-16T14:31:54Z'>
	<summary>[BUG] rotation isnt working on GPU</summary>
	<description>
i am trying to do a simple rotation of a batch on GPU and it fails.
on CPU the rotation works:
   transform = nn.Sequential( kornia.geometry.Rotate(angle=torch.Tensor([3]).to(device=input.device)) ); input=transform(input)
i am using pytorch 1.2, cuda10,  magma-cuda100
error messege for the GPU case:
CUDA runtime error: an illegal instruction was encountered (73) in magma_sgetri_outofplace_batched at /opt/conda/conda-bld/magma-cuda100_1564975479425/work/src/sgetri_outofplace_batched.cpp:137
CUDA runtime error: an illegal instruction was encountered (73) in magma_queue_destroy_internal at /opt/conda/conda-bld/magma-cuda100_1564975479425/work/interface_cuda/interface.cpp:944
CUDA runtime error: an illegal instruction was encountered (73) in magma_queue_destroy_internal at /opt/conda/conda-bld/magma-cuda100_1564975479425/work/interface_cuda/interface.cpp:945
CUDA runtime error: an illegal instruction was encountered (73) in magma_queue_destroy_internal at /opt/conda/conda-bld/magma-cuda100_1564975479425/work/interface_cuda/interface.cpp:946
10/25 05:05:21 PM Current run is terminating due to exception: inverse_cuda: For batch 0: U(977005056,977005056) is zero, singular U..
Traceback (most recent call last):
File "/opt/project/src/algo/Miil_engine/Engine.py", line 108, in _run_epoch
self._fire_event(Events.ITERATION_STARTED)
File "/opt/project/src/algo/Miil_engine/Engine.py", line 319, in _fire_event
is_stop = func(self, *(event_args + args), **kwargs) or is_stop
File "/opt/project/src/algo/Miil_engine/callbacks.py", line 106, in gpu_augmentations
input=transform(input)
File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 547, in call
result = self.forward(*input, **kwargs)
File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py", line 92, in forward
input = module(input)
File "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py", line 547, in call
result = self.forward(*input, **kwargs)
File "/opt/conda/lib/python3.7/site-packages/kornia/geometry/transform/affwarp.py", line 236, in forward
return rotate(input, self.angle, self.center)
File "/opt/conda/lib/python3.7/site-packages/kornia/geometry/transform/affwarp.py", line 142, in rotate
return affine(tensor, rotation_matrix[..., :2, :3])
File "/opt/conda/lib/python3.7/site-packages/kornia/geometry/transform/affwarp.py", line 100, in affine
warped: torch.Tensor = warp_affine(tensor, matrix, (height, width))
File "/opt/conda/lib/python3.7/site-packages/kornia/geometry/transform/imgwarp.py", line 165, in warp_affine
return transform_warp_impl(src, M_3x3, (src.shape[-2:]), dsize)
File "/opt/conda/lib/python3.7/site-packages/kornia/geometry/transform/imgwarp.py", line 63, in transform_warp_impl
dst_pix_trans_src_pix, dsize_src, dsize_dst)
File "/opt/conda/lib/python3.7/site-packages/kornia/geometry/transform/imgwarp.py", line 49, in dst_norm_to_dst_norm
src_pix_trans_src_norm = torch.inverse(src_norm_trans_src_pix)
RuntimeError: inverse_cuda: For batch 0: U(977005056,977005056) is zero, singular U.
	</description>
	<comments>
		<comment id='1' author='mrT23' date='2019-10-27T12:40:49Z'>
		&lt;denchmark-link:https://github.com/mrT23&gt;@mrT23&lt;/denchmark-link&gt;
 Is in our next roadmap to setup the infra to cover gpu tests
		</comment>
		<comment id='2' author='mrT23' date='2019-10-27T12:59:09Z'>
		ok.
I think your package has great potential, keep up the good work
some recommendations and advice, feel free to ignore :)


all the inverse 3x3 matrix operations would be faster, simpler and safer if done always on the CPU.
just the warping need to be done on GPU


i also think that the adjust_hue implementation is wrong, in several ways.


first, the line:
"
if not isinstance(hue_factor, float) and -0.5 &lt; hue_factor &lt; 0.5:
raise TypeError(f"The hue_factor should be a float number in the range between"
f" [-0.5, 0.5]. Got {type(hue_factor)}")
"
should be
"
if isinstance(hue_factor, float) and -0.5 &lt; hue_factor &lt; 0.5:
raise TypeError(f"The hue_factor should be a float number in the range between"
f" [-0.5, 0.5]. Got {type(hue_factor)}")
"


second, hue should be an additive factor, not multiplicative
h_out: torch.Tensor = torch.frac(h * hue_factor)
for additive, hueshould be with mean of 1, not zero.


also, for the far future, I would recommend trying to accelerate the GPU code, currently its quite slow. one example - try using inplace operations as much as possible (a+=b, not a=a+b, clamp_ instead of clamp,...)


		</comment>
		<comment id='3' author='mrT23' date='2019-10-27T13:23:04Z'>
		&lt;denchmark-link:https://github.com/mrT23&gt;@mrT23&lt;/denchmark-link&gt;
 thanks for the feedback ! very helpful ;D
We will go for the optimizations once we have settled down as a team.
regarding the hue issue check this: &lt;denchmark-link:https://github.com/kornia/kornia/issues/290&gt;#290&lt;/denchmark-link&gt;

as for inplace operations, I'm very skeptical to it since sometimes raises errors during backprop which is the main scope of this library. Very open to discuss and test that :D
		</comment>
		<comment id='4' author='mrT23' date='2019-10-27T13:58:37Z'>
		lets take for example the main contrast operation:
"
x_adjust: torch.Tensor = input + contrast_factor
out: torch.Tensor = torch.clamp(x_adjust, 0.0, 1.0)
"
there is no reason not to do it as inplace, without allocation anything:
"
input.add_(contrast_factor)
input.clamp_(0,1)
"
this will prevent allocating memory and will run faster.
if backward gradients are needed, you cant do inplace operations one after another,  so you might set it as:
"
input.add_(contrast_factor)
input=input.clamp(0,1)
"
still i am not allocating anything and the operations will be faster and more memory efficient.
b.t.w1: i think that this is not contrast but brightness implementation
contrast should be a multiplicative factor
b.t.w2: another important feature for GPU augmentations, and in general, might be to add the option to do them explicitly without backward gradients:
"with torch.no_grad(): "
this again will save memory and reduce runtime. sometimes you want backward gradients, but for augmentations, you usually don't.
		</comment>
		<comment id='5' author='mrT23' date='2019-11-02T08:12:39Z'>
		&lt;denchmark-link:https://github.com/mrT23&gt;@mrT23&lt;/denchmark-link&gt;
 we updated the roadmap and already working on some of the things you mention.
board meeting: &lt;denchmark-link:https://github.com/kornia/kornia/wiki/2019#2019-10-31&gt;https://github.com/kornia/kornia/wiki/2019#2019-10-31&lt;/denchmark-link&gt;

roadmap: &lt;denchmark-link:https://github.com/kornia/kornia/issues/312&gt;#312&lt;/denchmark-link&gt;

gpu tests [wip]: &lt;denchmark-link:https://github.com/kornia/kornia/pull/313&gt;#313&lt;/denchmark-link&gt;

regarding to the augmentation with no grads, I don't we will do that. The reason is because we might want users to backpropagate throw the parameters and leverage differentiable data augmentation. As you mention, with just one single line of code the user can already disable grads.
		</comment>
		<comment id='6' author='mrT23' date='2019-12-16T14:31:54Z'>
		&lt;denchmark-link:https://github.com/mrT23&gt;@mrT23&lt;/denchmark-link&gt;
 closing this issue since the original bug has (or should have) been solved in &lt;denchmark-link:https://github.com/kornia/kornia/pull/313&gt;#313&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>