<bug id='527' author='muupan' open_date='2019-08-23T07:38:09Z' closed_time='2019-08-26T04:27:13Z'>
	<summary>Bug in `batch_recurrent_experiences` regarding next_action</summary>
	<description>
batch_recurrent_experiences is expected to batch next_action of transitions into an array when they are available. However, because the current code only checks the last transition in every episode, it will result in error when there is an episode such that

it contains an intermediate transition that does not have next_action, and
its last transition has next_action.

This behavior of batch_recurrent_experiences is unexpected and should be fixed.
Such episodes themselves might seem weird, but they actually appear, because DQN and agents that inherit it only set next_action in stop_episode_and_train.
Thus, this problem can arise when

recurrent=True,
cupy is used (because numpy.asarray([1,None]) is ok while cupy.asarray([1,None]) is not), and
the non-batch interface is used.

This cannot be noticed by running examples/atari/train_drqn_ale.py  because it is very unlikely that all the sub-episodes in a batch you sample from a replay buffer end with terminal states on Atari, but tests/agents_tests/test_dqn.py::TestDQNOnDiscretePOABC::test_training_gpu actually fails because of it.
	</description>
	<comments>
	</comments>
</bug>