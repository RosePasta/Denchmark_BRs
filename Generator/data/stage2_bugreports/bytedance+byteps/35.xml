<bug id='35' author='boscotsang' open_date='2019-07-02T09:35:02Z' closed_time='2019-07-10T09:48:18Z'>
	<summary>AttributeError: module 'byteps.torch' has no attribute 'push_pull'</summary>
	<description>
I used hps.allreduce and an error was raised
AttributeError: module 'byteps.torch' has no attribute 'allreduce'
However, I replace hvd.allreduce with bps.push_pull, there was alos an error
AttributeError: module 'byteps.torch' has no attribute 'push_pull'
	</description>
	<comments>
		<comment id='2' author='boscotsang' date='2019-07-02T10:53:19Z'>
		When I use bps.byteps_push_pull to compute the average loss across multiple process.
&lt;denchmark-code&gt;def metric_average(val, name):
    tensor = torch.tensor(val)
    avg_tensor = bps.byteps_push_pull(tensor, name=name)
    return avg_tensor.item()

loss_val = train()
loss_val = metric_average(loss_val, name="loss_val")
&lt;/denchmark-code&gt;

It raised int object has no attribute item()
And after I remove .item(), the following error was raised
[2019-07-02 17:37:11.589984: F byteps/common/nccl_manager.cc:35] Check failed: e == cudaSuccess || e == cudaErrorCudartUnloadin
After remove the bps.byteps_push_pull, the error disapppeared.
		</comment>
		<comment id='3' author='boscotsang' date='2019-07-02T11:42:01Z'>
		&lt;denchmark-link:https://github.com/boscotsang&gt;@boscotsang&lt;/denchmark-link&gt;
 Could you provide the complete scripts (including the python file and commands) so that we can try to reproduce?
		</comment>
		<comment id='4' author='boscotsang' date='2019-07-02T17:41:48Z'>
		Using byteps_push_pull is not expected. It does not return a tensor; it returns a handle of the push_pull operation. The real problem was that we didn't expose the correct push_pull API in  torch/__init__.py
&lt;denchmark-link:https://github.com/boscotsang&gt;@boscotsang&lt;/denchmark-link&gt;
 I have a PR that addresses this (&lt;denchmark-link:https://github.com/bytedance/byteps/pull/36&gt;#36&lt;/denchmark-link&gt;
), We will do our test. Would you also try it in your environment and let us know whether it works? You can use the code in branch "torch_dev", and just use  in your code.
One minor difference from Horovod allreduce API is that you must specify a name
		</comment>
		<comment id='5' author='boscotsang' date='2019-07-03T04:46:07Z'>
		&lt;denchmark-link:https://github.com/ymjiang&gt;@ymjiang&lt;/denchmark-link&gt;
  example/pytorch/train_mnist_byteps.py could be a reproduction code since its function metric average used allreduce.
&lt;denchmark-link:https://github.com/bobzhuyb&gt;@bobzhuyb&lt;/denchmark-link&gt;
 I fetch the torch_dev branch and modify the allreduce with push_all in example/pytorch/train_mnist_byteps.py. Now I can call the push_pull function. However, the error

is still exist.
		</comment>
		<comment id='6' author='boscotsang' date='2019-07-03T04:56:33Z'>
		&lt;denchmark-link:https://github.com/boscotsang&gt;@boscotsang&lt;/denchmark-link&gt;
 Okay, for this kind of problem, we need to know your cuda version, nccl version, gcc version, and how you installed byteps (from source code, pip package, or docker). I am assuming you are running single-machine multi-gpu for now?
		</comment>
		<comment id='7' author='boscotsang' date='2019-07-03T05:04:18Z'>
		&lt;denchmark-link:https://github.com/bobzhuyb&gt;@bobzhuyb&lt;/denchmark-link&gt;

CUDA Version: 9.0
GCC Version: 4.9.3
NCCL Version: 2.4.8
I installed byteps from source code, and running single-machine multi-gpu.
		</comment>
		<comment id='8' author='boscotsang' date='2019-07-03T05:08:34Z'>
		Which GPU model are you using?
The only thing different from our tested cases is the NCCL version. We'll reproduce this environment.
		</comment>
		<comment id='9' author='boscotsang' date='2019-07-03T05:49:31Z'>
		&lt;denchmark-link:https://github.com/bobzhuyb&gt;@bobzhuyb&lt;/denchmark-link&gt;

NVIDIA P40
		</comment>
		<comment id='10' author='boscotsang' date='2019-07-03T05:56:13Z'>
		By the way, I have another machine with NVIDIA 1080ti and nccl 2.2.13 and I have also encountered the problem in this machine.
		</comment>
		<comment id='11' author='boscotsang' date='2019-07-03T08:01:31Z'>
		We can't reproduce the CUDA: an illegal memory access was encountered error.. We tried gcc 4.9, 5.4, nccl 2.4.8, 2.3.7, pytorch 1.0.1, 1.1.0. Still can't reproduce the error.
Would you paste your detailed commands, and how many GPUs do you have?
		</comment>
		<comment id='12' author='boscotsang' date='2019-07-03T08:15:41Z'>
		I have 8 gpus and the following is the commands
&lt;denchmark-code&gt;export NVIDIA_VISIBLE_DEVICES=4,5,6,7
export CUDA_VISIBLE_DEVICES=4,5,6,7 
export DMLC_WORKER_ID=0
export DMLC_NUM_WORKER=1
export DMLC_ROLE=worker

export DMLC_NUM_SERVER=1
export DMLC_PS_ROOT_URI=127.0.0.1
export DMLC_PS_ROOT_PORT=1234

python byteps/launcher/launch.py python example/pytorch/train_mnist_byteps.py
&lt;/denchmark-code&gt;

I commented the train() function in train_mnist_byteps.py in order to directly run the test() function which used metric_average
		</comment>
		<comment id='13' author='boscotsang' date='2019-07-03T08:26:03Z'>
		Ahhh... This could be due to that we have an assumption somewhere that the GPU list starts with 0,1,2... In fact, I already realized a few places we wrote cudaSetDevice(BytePSGlobal::GetLocalRank())
Would you try using NVIDIA_VISIBLE_DEVICES=0,1,2,3 ?
We will fix this.
		</comment>
		<comment id='14' author='boscotsang' date='2019-07-03T08:39:35Z'>
		I have try NVIDIA_VISIBLE_DEVICES=0,1,2,3 and the problem is still exist
		</comment>
		<comment id='15' author='boscotsang' date='2019-07-04T06:42:59Z'>
		I guess that in function metric_average, the tensor was constructed in cpu so nccl cannot handle the tensor correctly. I changed the torch.tensor(val) to torch.tensor(val, device=torch.device("cuda") and the error was gone.
		</comment>
		<comment id='16' author='boscotsang' date='2019-07-04T13:07:40Z'>
		By the way, the return value of push_pull is the sum of the tensors even if average=True
		</comment>
		<comment id='17' author='boscotsang' date='2019-07-04T13:13:16Z'>
		&lt;denchmark-link:https://github.com/boscotsang&gt;@boscotsang&lt;/denchmark-link&gt;
  Thanks a lot for the report! We will fix the averaging problem.
For CPU tensors, BytePS registers it with CUDA and maps it into GPU memory space. For GPUs that support this, NCCL can still handle it (though it would be slower than real GPU memory). We need to check whether P40 supports this.. We are using T100 and it seems to be fine.
		</comment>
		<comment id='18' author='boscotsang' date='2019-07-05T06:14:42Z'>
		&lt;denchmark-link:https://github.com/bobzhuyb&gt;@bobzhuyb&lt;/denchmark-link&gt;
 Does the average problem of push_pull influence  the gradient average of DsitributedOptimizer?
		</comment>
		<comment id='19' author='boscotsang' date='2019-07-05T07:26:10Z'>
		&lt;denchmark-link:https://github.com/boscotsang&gt;@boscotsang&lt;/denchmark-link&gt;
 The averaging is done here &lt;denchmark-link:https://github.com/bytedance/byteps/blob/master/byteps/torch/ops.cc#L89&gt;https://github.com/bytedance/byteps/blob/master/byteps/torch/ops.cc#L89&lt;/denchmark-link&gt;

We have tested the convergence of torch examples, and it seemed correct.
EDIT: this seems to be a serious bug. &lt;denchmark-link:https://github.com/bytedance/byteps/blob/master/byteps/torch/ops.cc#L89&gt;https://github.com/bytedance/byteps/blob/master/byteps/torch/ops.cc#L89&lt;/denchmark-link&gt;
 should be  instead of 
Fortunately, for in-place push_pull (used by DsitributedOptimizer), output is the same tensor as tensor, so it fortunately works. But bps.push_pull() uses out-of-place push_pull, so the averaging is not done correctly.
&lt;denchmark-link:https://github.com/ymjiang&gt;@ymjiang&lt;/denchmark-link&gt;
 We must fix this as soon as possible.
		</comment>
		<comment id='20' author='boscotsang' date='2019-07-05T07:43:24Z'>
		Fixed &lt;denchmark-link:https://github.com/bytedance/byteps/commit/a43391971b8da5e53988dc4dfd6affcfb7c2f88d&gt;a433919&lt;/denchmark-link&gt;
.
&lt;denchmark-link:https://github.com/boscotsang&gt;@boscotsang&lt;/denchmark-link&gt;
 Thank you very much for the reports. Please try and let us know if it works.
		</comment>
		<comment id='21' author='boscotsang' date='2019-07-10T09:48:17Z'>
		It works, thanks
		</comment>
	</comments>
</bug>