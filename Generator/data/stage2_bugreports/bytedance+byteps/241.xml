<bug id='241' author='zhenglin03' open_date='2020-04-14T18:19:33Z' closed_time='2020-05-21T05:23:13Z'>
	<summary>benchmark with cross barrier is much slower than that without cross barrier</summary>
	<description>
Describe the bug
I benchmarked the performance of BytePS with cross barrier using the script in /example/pytorch/benchmark_cross_barrier_byteps.py. The throughput in the case of resnet50-32bs with cross barrier (220+ img/sec) is much lower than that without cross barrier (280+ img/sec) which using the script in /example/pytorch/benchmark_byteps.py. The experiments are both on 2 servers and 2 workers with RDMA, each of worker has 8 V100 GPUs.
Expected behavior
I thought cross barrier with preempt communication would bring performance improvement, but it seems to be worse. I wonder if this is expected, or if I have wrong understanding.
Environment (please complete the following information):

OS: 16.04.2-Ubuntu
GCC version: 5.4.0
CUDA and NCCL version: CUDA 10.1
Framework : PyTorch

	</description>
	<comments>
		<comment id='1' author='zhenglin03' date='2020-04-15T01:08:59Z'>
		This is not expected. Although ResNet-50 is not communication-intensive, crossing the barrier should at least provide equal performance compared to the baseline. Can you paste the complete commands for reproduction?
Hi &lt;denchmark-link:https://github.com/pengyanghua&gt;@pengyanghua&lt;/denchmark-link&gt;
 , can you please take a look?
		</comment>
		<comment id='2' author='zhenglin03' date='2020-04-15T03:54:29Z'>
		&lt;denchmark-link:https://github.com/ymjiang&gt;@ymjiang&lt;/denchmark-link&gt;
 My commands are as follows:
For the scheduler:
docker run -it --net=host --device /dev/infiniband/rdma_cm --device /dev/infiniband/issm0 --device /dev/infiniband/ucm0 --device /dev/infiniband/umad0 --device /dev/infiniband/uverbs0 --cap-add IPC_LOCK bytepsimage/pytorch bash
export DMLC_ENABLE_RDMA=1
export DMLC_NUM_WORKER=2
export DMLC_ROLE=scheduler
export DMLC_NUM_SERVER=2
export DMLC_INTERFACE=eth0
export DMLC_PS_ROOT_URI=10.255.127.25
export DMLC_PS_ROOT_PORT=9007
export DMLC_NODE_HOST=10.255.127.25
bpslaunch
For the server-0:
docker run -it --net=host --device /dev/infiniband/rdma_cm --device /dev/infiniband/issm0 --device /dev/infiniband/ucm0 --device /dev/infiniband/umad0 --device /dev/infiniband/uverbs0 --cap-add IPC_LOCK bytepsimage/pytorch bash
export DMLC_ENABLE_RDMA=1
export DMLC_NUM_WORKER=2
export DMLC_ROLE=server
export DMLC_NUM_SERVER=2
export DMLC_INTERFACE=eth0
export DMLC_PS_ROOT_URI=10.255.127.25
export DMLC_PS_ROOT_PORT=9007
export DMLC_NODE_HOST=10.255.127.25
bpslaunch
For the server-1:
docker run -it --net=host --device /dev/infiniband/rdma_cm --device /dev/infiniband/issm0 --device /dev/infiniband/ucm0 --device /dev/infiniband/umad0 --device /dev/infiniband/uverbs0 --cap-add IPC_LOCK bytepsimage/pytorch bash
export DMLC_ENABLE_RDMA=1
export DMLC_NUM_WORKER=2
export DMLC_ROLE=server
export DMLC_NUM_SERVER=2
export DMLC_INTERFACE=eth0
export DMLC_PS_ROOT_URI=10.255.127.25
export DMLC_PS_ROOT_PORT=9007
export DMLC_NODE_HOST=10.255.130.19
bpslaunch
For worker-0:
export NVIDIA_SO=$(\ls /usr/lib/libnvidia* | xargs -I{} echo '-v {}:{}')
export CUDA_SO=$(\ls /usr/lib/libcuda* | xargs -I{} echo '-v {}:{}')
export DEVICES=$(\ls /dev/nvidia* | xargs -I{} echo '--device {}:{}')
docker run -it --net=host --shm-size=32768m $NVIDIA_SO  $CUDA_SO $DEVICES -v /home/opt:/home/opt  --device /dev/infiniband/rdma_cm --device /dev/infiniband/issm0 --device /dev/infiniband/ucm0 --device /dev/infiniband/umad0 --device /dev/infiniband/uverbs0 --cap-add IPC_LOCK bytepsimage/pytorch bash
export NVIDIA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
export LD_LIBRARY_PATH=/home/opt/nvidia_lib/:$LD_LIBRARY_PATH
export DMLC_ENABLE_RDMA=1
export DMLC_WORKER_ID=0
export DMLC_NUM_WORKER=2
export DMLC_ROLE=worker
export DMLC_NUM_SERVER=2
export DMLC_INTERFACE=eth0
export DMLC_PS_ROOT_URI=10.255.127.25
export DMLC_PS_ROOT_PORT=9007
export DMLC_NODE_HOST=10.255.127.23
export BYTEPS_SCHEDULING_CREDIT=1
bpslaunch python3 /usr/local/byteps/example/pytorch/benchmark_cross_barrier_byteps.py --model resnet50 --num-iters 10
For worker-1:
export NVIDIA_SO=$(\ls /usr/lib/libnvidia* | xargs -I{} echo '-v {}:{}')
export CUDA_SO=$(\ls /usr/lib/libcuda* | xargs -I{} echo '-v {}:{}')
export DEVICES=$(\ls /dev/nvidia* | xargs -I{} echo '--device {}:{}')
docker run -it --net=host --shm-size=32768m $NVIDIA_SO  $CUDA_SO $DEVICES -v /home/opt:/home/opt  --device /dev/infiniband/rdma_cm --device /dev/infiniband/issm0 --device /dev/infiniband/ucm0 --device /dev/infiniband/umad0 --device /dev/infiniband/uverbs0 --cap-add IPC_LOCK bytepsimage/pytorch bash
export NVIDIA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
export LD_LIBRARY_PATH=/home/opt/nvidia_lib/:$LD_LIBRARY_PATH
export DMLC_ENABLE_RDMA=1
export DMLC_WORKER_ID=1
export DMLC_NUM_WORKER=2
export DMLC_ROLE=worker
export DMLC_NUM_SERVER=2
export DMLC_INTERFACE=eth0
export DMLC_PS_ROOT_URI=10.255.127.25
export DMLC_PS_ROOT_PORT=9007
export DMLC_NODE_HOST=10.255.129.21
export BYTEPS_SCHEDULING_CREDIT=1
bpslaunch python3 /usr/local/byteps/example/pytorch/benchmark_cross_barrier_byteps.py --model resnet50 --num-iters 10
		</comment>
		<comment id='3' author='zhenglin03' date='2020-04-16T10:06:36Z'>
		&lt;denchmark-link:https://github.com/zhenglin03&gt;@zhenglin03&lt;/denchmark-link&gt;
 You may try tuning the credits and tensor partition size by setting BYTEPS_SCHEDULING_CREDIT and BYTEPS_PARTITION_BYTES.
		</comment>
		<comment id='4' author='zhenglin03' date='2020-04-20T04:34:01Z'>
		
@zhenglin03 You may try tuning the credits and tensor partition size by setting BYTEPS_SCHEDULING_CREDIT and BYTEPS_PARTITION_BYTES.

Is there a recommended value for the parameterï¼Ÿ
		</comment>
		<comment id='5' author='zhenglin03' date='2020-04-20T05:25:25Z'>
		&lt;denchmark-link:https://github.com/zhenglin03&gt;@zhenglin03&lt;/denchmark-link&gt;
 The best parameters depend on your hardware and training model. Typically if the partition size and credit size are larger, the bandwidth utilization will be higher but the performance gain of preemptive scheduling will be lower. Setting BYTEPS_SCHEDULING_CREDIT to 1 is not recommended as network bandwidth is not fully utilized. You can try a larger value, e.g., 2-4.
		</comment>
		<comment id='6' author='zhenglin03' date='2020-04-20T05:56:34Z'>
		&lt;denchmark-link:https://github.com/zhenglin03&gt;@zhenglin03&lt;/denchmark-link&gt;
 By default, BytePS disables scheduling because users may or may not use the cross-barrier version. If users don't use the cross-barrier version, there is no point enabling scheduling.
However, since you are explicitly testing the cross-barrier version, you can try setting BYTEPS_SCHEDULING_CREDIT as pengyanghua suggests to enable scheduling.
		</comment>
		<comment id='7' author='zhenglin03' date='2020-04-26T04:36:20Z'>
		&lt;denchmark-link:https://github.com/pengyanghua&gt;@pengyanghua&lt;/denchmark-link&gt;
 I set BYTEPS_SCHEDULING_CREDIT to 4, but the results did not improve. And I set BYTEPS_PARTITION_BYTES to 1m, 2m and 4m, the results even got a bit worse. Could you provide the performance comparison with and without cross barrier  in the case of training ResNet50 on 2 V100 workers and 2 servers, please?
		</comment>
		<comment id='8' author='zhenglin03' date='2020-04-27T03:14:14Z'>
		&lt;denchmark-link:https://github.com/zhenglin03&gt;@zhenglin03&lt;/denchmark-link&gt;
 Do you mean that the training speed is same as the baseline after setting BYTEPS_SCHEDULING_CREDIT to 4? So far we have not tested the scheduling feature in byteps completely and we do not have the performance data in your case.
The performance gain depends on how much we can overlap gradient communication with backward and forward computation. You can profile the time of the computation and communication and figure out the ideal overlapping performance gain. For ResNet50, the ideal gain may be very small as it is not communication-intensive if using RDMA network.
		</comment>
		<comment id='9' author='zhenglin03' date='2020-04-28T09:05:57Z'>
		&lt;denchmark-link:https://github.com/pengyanghua&gt;@pengyanghua&lt;/denchmark-link&gt;
 I mean the training speed is lower than the baseline after setting BYTEPS_SCHEDULING_CREDIT to 4 in my case. It is the same as my first experiment results.
By the way, I read the code recently and find out that the 'priority' passed to EnqueueTensor function in pytorch is seem not to be set if I use bps.DistributedOptimizer directly to do communication in training, right? There is byteps_push_pull being called in DistributedOptimizer.step, but no 'priority' passed. Then 'priority' is set to 0 in  push_pull_async_inplace in pytorch ops.
Does it obstruct benefitting from priority scheduling? I think cross barrier need to be used with priority scheduling, or it may not have benefit.
		</comment>
		<comment id='10' author='zhenglin03' date='2020-05-01T03:54:42Z'>
		&lt;denchmark-link:https://github.com/zhenglin03&gt;@zhenglin03&lt;/denchmark-link&gt;
 Thanks very much for figuring out the bug. It is supposed to be set in &lt;denchmark-link:https://github.com/bytedance/byteps/blob/master/byteps/torch/cross_barrier.py#L153&gt;https://github.com/bytedance/byteps/blob/master/byteps/torch/cross_barrier.py#L153&lt;/denchmark-link&gt;
, otherwise there is no priority scheduling benefit. You may refer to &lt;denchmark-link:https://github.com/bytedance/byteps/blob/bytescheduler/bytescheduler/bytescheduler/pytorch/horovod.py#L59&gt;https://github.com/bytedance/byteps/blob/bytescheduler/bytescheduler/bytescheduler/pytorch/horovod.py#L59&lt;/denchmark-link&gt;
 for priority implementation. We welcome your contribution and will fix this problem.
		</comment>
	</comments>
</bug>