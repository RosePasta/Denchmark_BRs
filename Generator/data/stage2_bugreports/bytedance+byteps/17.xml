<bug id='17' author='burness' open_date='2019-06-28T05:40:20Z' closed_time='2019-06-28T07:44:21Z'>
	<summary>"set_mempolicy: Operation not permitted" and performance degradation in 8GPU with single machine</summary>
	<description>
Describe the bug
I use 4GPUs(1080ti) in a single machine, It perform well, but when I use 8GPUs  and byteps get performance degradation: from 161.8 img/sec per GPU to 17.4 img/sec per GPU  and have some warning info "set_mempolicy: Operation not permitted".

Steps to reproduce the behavior:
Just change the gpu num in &lt;denchmark-link:https://github.com/bytedance/byteps/blob/master/docs/step-by-step-tutorials.md&gt;step-by-step toturial&lt;/denchmark-link&gt;

Environment (please complete the following information):

OS: ubuntu
GCC version: Ubuntu 5.4.0-6ubuntu1~16.04.11)
CUDA and NCCL version:  CUDA Version 9.0.176
Framework (TF, PyTorch, MXNet): TensorFlow
use the byteps docker

Additional context
Add any other context about the problem here.
	</description>
	<comments>
		<comment id='1' author='burness' date='2019-06-28T05:50:06Z'>
		Looks like Docker prevented numactl from setting mempolicy. We will need to confirm if this persists on our testbeds &lt;denchmark-link:https://github.com/ymjiang&gt;@ymjiang&lt;/denchmark-link&gt;
.
As a quick workaround, perhaps try  with , using the custom seccomp.json file like this: &lt;denchmark-link:https://gist.github.com/w1ndy/4aee49aa3a608c977a858542ed5f1ee5&gt;https://gist.github.com/w1ndy/4aee49aa3a608c977a858542ed5f1ee5&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='2' author='burness' date='2019-06-28T06:02:09Z'>
		&lt;denchmark-link:https://github.com/changlan&gt;@changlan&lt;/denchmark-link&gt;
  I  try  with --security-opt seccomp=seccomp.json， warn info like “set_mempolicy: Operation not permitted”  disappear， but the speed is still low
&lt;denchmark-link:https://user-images.githubusercontent.com/3112825/60320753-43f48100-99ad-11e9-8e44-62e82a398d4c.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='burness' date='2019-06-28T06:13:18Z'>
		It may have something to do with NUMA. Can you try export BYTEPS_PCIE_SWITCH_SIZE=8 before running? This may help us figure out the problem.
As far as we have tested, on our testbed there is no such problem. &lt;denchmark-link:https://github.com/changlan&gt;@changlan&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='burness' date='2019-06-28T07:26:00Z'>
		&lt;denchmark-link:https://github.com/ymjiang&gt;@ymjiang&lt;/denchmark-link&gt;
  will solve this problem, can you add it in byteps to auto set this val according the GPU num?
		</comment>
		<comment id='5' author='burness' date='2019-06-28T07:32:11Z'>
		&lt;denchmark-link:https://github.com/ymjiang&gt;@ymjiang&lt;/denchmark-link&gt;
 Let's change the default value to 8. Only very experienced BytePS users may know whether a &lt;8 value is better for their environment.
		</comment>
		<comment id='6' author='burness' date='2019-06-28T07:44:08Z'>
		Default value changed to 8 now (&lt;denchmark-link:https://github.com/bytedance/byteps/commit/7c4dd6733590fb59474f6d9e07b1af8c5905fd17&gt;7c4dd67&lt;/denchmark-link&gt;
). Closing this issue.
		</comment>
		<comment id='7' author='burness' date='2019-06-28T08:40:04Z'>
		&lt;denchmark-link:https://github.com/ymjiang&gt;@ymjiang&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/changlan&gt;@changlan&lt;/denchmark-link&gt;
   It seems that there is not "/usr/local/byteps/launcher/launch.py" in the server images "bytepsimage/byteps_server" . Anyone could help push the correct images?
		</comment>
		<comment id='8' author='burness' date='2019-06-28T08:50:08Z'>
		&lt;denchmark-link:https://github.com/burness&gt;@burness&lt;/denchmark-link&gt;
 Can you clean the cached image and try again now? We just pushed a new image.
		</comment>
		<comment id='9' author='burness' date='2019-06-29T01:55:54Z'>
		&lt;denchmark-link:https://github.com/ymjiang&gt;@ymjiang&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/changlan&gt;@changlan&lt;/denchmark-link&gt;
   I have change an new image, and  when i run  in scheduler docker container, it fails  and its error log :
&lt;denchmark-code&gt;BytePS launching scheduler
[01:53:31] src/./zmq_van.h:285: Start ZMQ recv thread
Traceback (most recent call last):
  File "/usr/local/byteps/launcher/launch.py", line 45, in &lt;module&gt;
    import mxnet
  File "/root/incubator-mxnet/python/mxnet/__init__.py", line 91, in &lt;module&gt;
    from . import kvstore_server
  File "/root/incubator-mxnet/python/mxnet/kvstore_server.py", line 85, in &lt;module&gt;
    _init_kvstore_server_module()
  File "/root/incubator-mxnet/python/mxnet/kvstore_server.py", line 82, in _init_kvstore_server_module
    server.run()
  File "/root/incubator-mxnet/python/mxnet/kvstore_server.py", line 73, in run
    check_call(_LIB.MXKVStoreRunServer(self.handle, _ctrl_proto(self._controller()), None))
  File "/root/incubator-mxnet/python/mxnet/base.py", line 252, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [01:53:31] src/van.cc:358: Check failed: (my_node_.port) != (-1) bind failed

Stack trace returned 10 entries:
[bt] (0) /root/incubator-mxnet/lib/libmxnet.so(dmlc::StackTrace[abi:cxx11]()+0x1bc) [0x7f400f4d1e5c]
[bt] (1) /root/incubator-mxnet/lib/libmxnet.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x28) [0x7f400f4d31d8]
[bt] (2) /root/incubator-mxnet/lib/libmxnet.so(ps::Van::Start(int)+0x962) [0x7f40129bb362]
[bt] (3) /root/incubator-mxnet/lib/libmxnet.so(ps::ZMQVan::Start(int)+0x1a6) [0x7f40129c7406]
[bt] (4) /root/incubator-mxnet/lib/libmxnet.so(ps::Postoffice::Start(int, char const*, bool)+0x7c) [0x7f40129b5fcc]
[bt] (5) /root/incubator-mxnet/lib/libmxnet.so(mxnet::kvstore::KVStoreDist::RunServer(std::function&lt;void (int, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;)&gt; const&amp;)+0x11e) [0x7f4012946c7e]
[bt] (6) /root/incubator-mxnet/lib/libmxnet.so(MXKVStoreRunServer+0x65) [0x7f40128aef15]
[bt] (7) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f40a1503e40]
[bt] (8) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x2eb) [0x7f40a15038ab]
[bt] (9) /usr/lib/python2.7/lib-dynload/_ctypes.x86_64-linux-gnu.so(_ctypes_callproc+0x48f) [0x7f40a17133df]
&lt;/denchmark-code&gt;

It seems port cause the error
		</comment>
		<comment id='10' author='burness' date='2019-06-29T02:05:05Z'>
		&lt;denchmark-link:https://github.com/ymjiang&gt;@ymjiang&lt;/denchmark-link&gt;
 another question,  I see the bt error infor in the log, How can it get the coredump file? I set  in your container, But I can't find any coredump files
		</comment>
		<comment id='11' author='burness' date='2019-06-29T02:13:31Z'>
		&lt;denchmark-link:https://github.com/ymjiang&gt;@ymjiang&lt;/denchmark-link&gt;
 Did you miss "--net=host" in all your commands in the tutorial?
		</comment>
		<comment id='12' author='burness' date='2019-06-29T02:25:35Z'>
		&lt;denchmark-link:https://github.com/burness&gt;@burness&lt;/denchmark-link&gt;
 Can you add  while you run docker? We will modify the tutorials.
		</comment>
		<comment id='13' author='burness' date='2019-06-29T02:44:19Z'>
		Fixed the tutorial: &lt;denchmark-link:https://github.com/bytedance/byteps/commit/ec55073427d89d411c78f90999d0d5487217a2c6&gt;ec55073&lt;/denchmark-link&gt;

		</comment>
		<comment id='14' author='burness' date='2019-06-29T03:29:53Z'>
		add  will solve this problem， and another advice :  you can delete the comment words
because may be export the val to be   that the
&lt;denchmark-link:https://user-images.githubusercontent.com/3112825/60379175-ee79ac00-9a60-11e9-8051-d046af436ac6.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='15' author='burness' date='2019-06-29T03:31:20Z'>
		&lt;denchmark-link:https://github.com/burness&gt;@burness&lt;/denchmark-link&gt;
 Thank you for reminding. On our platform there seems no such problem for . Did you notice that in your OS?
Anyway, we will fix this just in case.
		</comment>
	</comments>
</bug>