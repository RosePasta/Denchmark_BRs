<bug id='249' author='bhetherman' open_date='2020-04-28T08:18:29Z' closed_time='2021-01-16T05:54:40Z'>
	<summary>Unable to load tensorflow plugin for bytescheduler</summary>
	<description>
I can build TF 1.13.2 from source with the patch. However when I try to run the tf_cnn_benchmarks.py I get a error when tf.load_library('libplugin.so') is called.
[libprotobuf ERROR external/protobuf_archive/src/google/protobuf/descriptor_database.cc:58] File already exists in database: tensorflow/core/kernels/boosted_trees/boosted_trees.proto
[libprotobuf FATAL external/protobuf_archive/src/google/protobuf/descriptor.cc:1358] CHECK failed: GeneratedDatabase()-&gt;Add(encoded_file_descriptor, size):
terminate called after throwing an instance of 'google::protobuf::FatalException'
what():  CHECK failed: GeneratedDatabase()-&gt;Add(encoded_file_descriptor, size):
Aborted (core dumped)
Environment:

OS: Ubuntu 16.04.6 LTS (Xenial Xerus)
GCC version: gcc (Ubuntu 4.9.3-13ubuntu2) 4.9.3
CUDA and NCCL version: Cuda V10.0.130
TF 1.13.2 (installed from source with)

Any help would be appreciated. Thanks!
	</description>
	<comments>
		<comment id='1' author='bhetherman' date='2020-05-01T01:15:44Z'>
		Hi &lt;denchmark-link:https://github.com/pengyanghua&gt;@pengyanghua&lt;/denchmark-link&gt;
 , can you take a look?
		</comment>
		<comment id='2' author='bhetherman' date='2020-05-01T03:33:04Z'>
		&lt;denchmark-link:https://github.com/bhetherman&gt;@bhetherman&lt;/denchmark-link&gt;
 Can you import tensorflow and run tf_cnn_benchmarks.py without the libplugin.so plugin? There seems to be a similar problem in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/33746&gt;tensorflow/tensorflow#33746&lt;/denchmark-link&gt;
 .
		</comment>
		<comment id='3' author='bhetherman' date='2020-05-01T23:15:02Z'>
		&lt;denchmark-link:https://github.com/pengyanghua&gt;@pengyanghua&lt;/denchmark-link&gt;
  the script runs fine without trying to import libplugin.so. Heres the output in case that helps.
&lt;denchmark-link:https://github.com/bytedance/byteps/files/4566588/tf_cnn_benchmarks_output.txt&gt;tf_cnn_benchmarks_output.txt&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='bhetherman' date='2020-05-02T04:24:13Z'>
		&lt;denchmark-link:https://github.com/bhetherman&gt;@bhetherman&lt;/denchmark-link&gt;
 Which versions of protobuf and tf-cnn-benchmark do you use? could you paste all the output here when running the plugin?
		</comment>
		<comment id='5' author='bhetherman' date='2020-05-04T17:47:35Z'>
		pip show protobuf gives 3.11.3
protoc --version gives 3.11.4
So that could be the problem. As for tf-cnn-benchmark Im using the version in bytescheduler/tensorflow/benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py. And all the output when I load the libplugin is in the OP. There is only those few lines.
		</comment>
		<comment id='6' author='bhetherman' date='2020-05-04T20:05:09Z'>
		I updated the pip package to 3.11.4, that did not fix the problem. I still get the same output from the tf_cnn_benchmark script
		</comment>
		<comment id='7' author='bhetherman' date='2020-05-05T03:14:54Z'>
		&lt;denchmark-link:https://github.com/bhetherman&gt;@bhetherman&lt;/denchmark-link&gt;
  For tf_cnn_benchmarks.py, are you using the version &lt;denchmark-link:https://github.com/tensorflow/benchmarks/tree/cnn_tf_v1.13_compatible&gt;https://github.com/tensorflow/benchmarks/tree/cnn_tf_v1.13_compatible&lt;/denchmark-link&gt;
 and did you apply the patch?
		</comment>
		<comment id='8' author='bhetherman' date='2020-05-18T18:47:34Z'>
		&lt;denchmark-link:https://github.com/pengyanghua&gt;@pengyanghua&lt;/denchmark-link&gt;
 sorry for the delay. Other things came up at work. I made sure to use the tf1.13 compatible version and apply the patch. Results in the same output/error:
python tf_cnn_benchmarks.py
[libprotobuf ERROR external/protobuf_archive/src/google/protobuf/descriptor_database.cc:58] File already exists in database: tensorflow/core/kernels/boosted_trees/boosted_trees.proto
[libprotobuf FATAL external/protobuf_archive/src/google/protobuf/descriptor.cc:1358] CHECK failed: GeneratedDatabase()-&gt;Add(encoded_file_descriptor, size):
terminate called after throwing an instance of 'google::protobuf::FatalException'
what():  CHECK failed: GeneratedDatabase()-&gt;Add(encoded_file_descriptor, size):
Aborted (core dumped)
		</comment>
		<comment id='9' author='bhetherman' date='2020-05-19T12:08:58Z'>
		&lt;denchmark-link:https://github.com/bhetherman&gt;@bhetherman&lt;/denchmark-link&gt;
 It seems that the root cause is the protobuf version. You may try installing protobuf with a lower version, e.g., v3.7.0.
		</comment>
		<comment id='10' author='bhetherman' date='2020-05-20T07:42:04Z'>
		I tried protobuf 3.7.0 and 3.6.1, and got the same error. No additional help unfortunately. I also tried installing protobuf using pip and from source
		</comment>
		<comment id='11' author='bhetherman' date='2020-05-21T05:19:18Z'>
		&lt;denchmark-link:https://github.com/bhetherman&gt;@bhetherman&lt;/denchmark-link&gt;
 Would you try the solution here &lt;denchmark-link:https://stackoverflow.com/questions/37051635/several-shared-object-using-same-proto-leading-the-the-error-file-already-exist&gt;https://stackoverflow.com/questions/37051635/several-shared-object-using-same-proto-leading-the-the-error-file-already-exist&lt;/denchmark-link&gt;
 ? Compile the protobuf into libprotobuf.a instead of libprotobuf.so.
		</comment>
		<comment id='12' author='bhetherman' date='2020-05-26T21:04:30Z'>
		I'm not sure what parameters to give the configure/make files for protobuf to build the '.a' version, any idea?
		</comment>
		<comment id='13' author='bhetherman' date='2020-05-29T12:51:28Z'>
		&lt;denchmark-link:https://github.com/bhetherman&gt;@bhetherman&lt;/denchmark-link&gt;
 We plan to provide a Dockerfile for ByteScheduler tensorflow, stay tuned.
		</comment>
		<comment id='14' author='bhetherman' date='2020-05-29T14:58:30Z'>
		&lt;denchmark-link:https://github.com/pengyanghua&gt;@pengyanghua&lt;/denchmark-link&gt;
  oh awesome! That will be great! Thank you for all the help
		</comment>
		<comment id='15' author='bhetherman' date='2020-12-07T02:43:00Z'>
		Hi, I met Segmentation fault too.
I already recomplie TF 1.13.2 from source with the patch and build the libplugin.so.
Benchmark can run successfully without 'tf.load_library('libplugin.so')'.
But meet Segmentation fault with 'tf.load_library('libplugin.so')'.
Could help me with this? Thank you!
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Run commands:
&lt;denchmark-code&gt;# ps
TF_CPP_MIN_VLOG_LEVEL=2 CUDA_VISIBLE_DEVICES=2 python tf_cnn_benchmarks.py --batch_size=32 \
--model=vgg16 --variable_update=parameter_server \
--num_gpus=2 --job_name=ps --ps_hosts='localhost:3457' --worker_hosts='localhost:3458'
# worker
TF_CPP_MIN_VLOG_LEVEL=2 CUDA_VISIBLE_DEVICES=0,1 python tf_cnn_benchmarks.py --batch_size=32 \
--model=vgg16 --variable_update=parameter_server \
--num_gpus=2 --job_name=worker --ps_hosts='localhost:3457' --worker_hosts='localhost:3458' 
&lt;/denchmark-code&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

End of Logs:
&lt;denchmark-code&gt;2020-12-07 10:17:38.724819: I tensorflow/core/distributed_runtime/master_session.cc:760] RunPartitions step_id 87403373717659205 execution_count 0
2020-12-07 10:17:38.724965: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogStep { step_id: 87403373717659205 handle: "0000000000000001" }
2020-12-07 10:17:38.725201: I tensorflow/core/common_runtime/executor.cc:1661] Process node: 3 step 87403373717659205 {{node report_uninitialized_variables/boolean_mask/GatherV2_S1}} = RecvParameter[recv_device="/job:worker/replica:0/task:0/device:CPU:0", send_device="/job:ps/replica:0/task:0/device:CPU:0", send_device_incarnation=-5586968458083455864, tensor_name="edge_2933_report_uninitialized_variables/boolean_mask/GatherV2", tensor_type=DT_STRING]() device: /job:worker/replica:0/task:0/device:CPU:0
2020-12-07 10:17:38.725303: I tensorflow/core/common_runtime/executor.cc:1661] Process node: 0 step 87403373717659205 {{node _SOURCE}} = NoOp[]() device: /job:worker/replica:0/task:0/device:CPU:0
2020-12-07 10:17:38.725408: I tensorflow/core/common_runtime/executor.cc:1804] Synchronous kernel done: 0 step 87403373717659205 {{node _SOURCE}} = NoOp[]() device: /job:worker/replica:0/task:0/device:CPU:0
2020-12-07 10:17:38.725341: I tensorflow/core/distributed_runtime/base_rendezvous_mgr.cc:309] RemoteRendezvous Recv 0x7f44263b6da0 /job:ps/replica:0/task:0/device:CPU:0;b27719c9d49fac88;/job:worker/replica:0/task:0/device:CPU:0;edge_2933_report_uninitialized_variables/boolean_mask/GatherV2;0:0
2020-12-07 10:17:38.726962: I tensorflow/core/distributed_runtime/rpc/grpc_remote_worker.cc:191] RecvTensorAsync req: step_id: 87403373717659205
rendezvous_key: "/job:ps/replica:0/task:0/device:CPU:0;b27719c9d49fac88;/job:worker/replica:0/task:0/device:CPU:0;edge_2933_report_uninitialized_variables/boolean_mask/GatherV2;0:0"
request_id: 3737165821673222621

2020-12-07 10:17:39.002503: I tensorflow/core/distributed_runtime/rpc/grpc_remote_worker.cc:237] done callback, req: step_id: 87403373717659205
rendezvous_key: "/job:ps/replica:0/task:0/device:CPU:0;b27719c9d49fac88;/job:worker/replica:0/task:0/device:CPU:0;edge_2933_report_uninitialized_variables/boolean_mask/GatherV2;0:0"
request_id: 3737165821673222621
 response send_start_micros: 1607307459002113

Fatal Python error: Segmentation fault

Thread 0x00007f47c0fcc740 (most recent call first):
  File "/tensorflow/python/client/session.py", line 1407 in _call_tf_sessionrun
  File "/tensorflow/python/client/session.py", line 1319 in _run_fn
  File "/tensorflow/python/client/session.py", line 1334 in _do_call
  File "/tensorflow/python/client/session.py", line 1328 in _do_run
  File "/tensorflow/python/client/session.py", line 1152 in _run
  File "/tensorflow/python/client/session.py", line 929 in run
  File "/tensorflow/python/training/session_manager.py", line 518 in _ready
  File "/tensorflow/python/training/session_manager.py", line 474 in _model_ready_for_local_init
  File "/tensorflow/python/training/session_manager.py", line 489 in _try_run_local_init_op
  File "/tensorflow/python/training/session_manager.py", line 291 in prepare_session
  File "/tensorflow/python/training/supervisor.py", line 730 in prepare_or_wait_for_session
  File "/tensorflow/python/training/supervisor.py", line 993 in managed_session
  File "/python3.6/contextlib.py", line 81 in __enter__
  File "/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py", line 2286 in _benchmark_graph
  File "/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py", line 2088 in _benchmark_train
  File "/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py", line 1883 in run
  File "tf_cnn_benchmarks.py", line 69 in main
  File "/absl/app.py", line 251 in _run_main
  File "/absl/app.py", line 303 in run
  File "tf_cnn_benchmarks.py", line 74 in &lt;module&gt;
&lt;/denchmark-code&gt;

		</comment>
		<comment id='16' author='bhetherman' date='2020-12-08T14:34:28Z'>
		&lt;denchmark-link:https://github.com/lucasleesw&gt;@lucasleesw&lt;/denchmark-link&gt;
 Hi, could you give more details or commands about your GCC version, how you compiled TF after applying the patch, how you compiled the TF plugin?
		</comment>
		<comment id='17' author='bhetherman' date='2020-12-09T01:10:43Z'>
		
@lucasleesw Hi, could you give more details or commands about your GCC version, how you compiled TF after applying the patch, how you compiled the TF plugin?

Hi &lt;denchmark-link:https://github.com/pengyanghua&gt;@pengyanghua&lt;/denchmark-link&gt;
, my GCC version is 4.9, I complied TF with source(origin/r1.13) after applying the patch. The bazel version is 0.19.2.
Sorry I have no idea about what cause this error.
Everything is fine if I run with 'tf.load_library('libplugin.so')' like this:
&lt;denchmark-code&gt;python tf_cnn_benchmarks.py --batch_size=32 \
--model=vgg16 --variable_update=parameter_server \
--num_gpus=4
&lt;/denchmark-code&gt;

		</comment>
		<comment id='18' author='bhetherman' date='2020-12-11T11:44:35Z'>
		&lt;denchmark-link:https://github.com/lucasleesw&gt;@lucasleesw&lt;/denchmark-link&gt;
 So after changing the start command of worker, it works?
		</comment>
		<comment id='19' author='bhetherman' date='2020-12-11T11:52:13Z'>
		&lt;denchmark-link:https://github.com/pengyanghua&gt;@pengyanghua&lt;/denchmark-link&gt;
 Hi, I recompiled TF and plugin with default GCC in Ubuntu. This segment error in worker part was gone but fall in another segment fault error in ps part.
It may similar to segment fault mentioned in &lt;denchmark-link:https://github.com/bytedance/byteps/pull/105#issuecomment-557541104&gt;#105 (comment)&lt;/denchmark-link&gt;
, but I am not quite sure now. Thanks for your help~
		</comment>
		<comment id='20' author='bhetherman' date='2020-12-29T09:22:07Z'>
		Hi, I tried to build TF 1.13.2 from source but failed. The system information is as follows: Ubuntu 16.04/18.04, Bazel 0.19.2, GCC 4.9, Python 2.7, CUDA10.0, cudnn7, GPU T4.
The error is: ERROR: /home/cluster/tensorflow/tensorflow/core/kernels/BUILD:593:1: C++ compilation of rule '//tensorflow/core/kernels:eigen_contraction_kernel' failed (Exit 1)
Any help would be appreciated! Thanks!
		</comment>
		<comment id='21' author='bhetherman' date='2020-12-30T10:11:50Z'>
		
Hi, I tried to build TF 1.13.2 from source but failed. The system information is as follows: Ubuntu 16.04/18.04, Bazel 0.19.2, GCC 4.9, Python 2.7, CUDA10.0, cudnn7, GPU T4.
The error is: ERROR: /home/cluster/tensorflow/tensorflow/core/kernels/BUILD:593:1: C++ compilation of rule '//tensorflow/core/kernels:eigen_contraction_kernel' failed (Exit 1)
Any help would be appreciated! Thanks!

Solved. I downgrade GCC to 4.8.
		</comment>
	</comments>
</bug>