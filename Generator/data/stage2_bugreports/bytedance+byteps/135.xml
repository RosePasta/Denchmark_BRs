<bug id='135' author='jazka' open_date='2019-10-25T02:42:21Z' closed_time='2019-11-05T06:22:38Z'>
	<summary>Crash during MXNet distributed training</summary>
	<description>
As described in [https://github.com/bytedance/byteps/blob/master/docs/step-by-step-tutorial.md], for section "Distributed Training (TCP)" with MXNet, everything is fine at first, then crash after training a while.
&lt;denchmark-code&gt;2019-10-24T14:11:27.552522043Z INFO:root:Epoch[0] Batch [40000-40020]	Speed: 141.83 samples/sec	accuracy=1.000000
2019-10-24T14:11:27.552524672Z INFO:root:Epoch[0] Batch [40000-40020]	Speed: 141.85 samples/sec	accuracy=1.000000
2019-10-24T14:11:27.552527341Z INFO:root:Epoch[0] Batch [40000-40020]	Speed: 141.85 samples/sec	accuracy=1.000000
2019-10-24T14:11:27.552530088Z INFO:root:Epoch[0] Batch [40000-40020]	Speed: 141.83 samples/sec	accuracy=1.000000
2019-10-24T14:11:30.895411795Z INFO:root:Epoch[0] Train-accuracy=0.990960
2019-10-24T14:11:30.895451643Z INFO:root:Epoch[0] Time cost=8809.962
2019-10-24T14:11:30.895456768Z INFO:root:Epoch[0] Train-accuracy=0.990373
2019-10-24T14:11:30.895460990Z INFO:root:Epoch[0] Train-accuracy=0.990857
2019-10-24T14:11:30.895464996Z INFO:root:Epoch[0] Time cost=8809.948
2019-10-24T14:11:30.895469883Z INFO:root:Epoch[0] Time cost=8809.938
2019-10-24T14:11:30.895474126Z INFO:root:Epoch[0] Train-accuracy=0.990121
2019-10-24T14:11:30.895478329Z INFO:root:Epoch[0] Time cost=8809.948
2019-10-24T14:11:30.895482272Z INFO:root:Epoch[0] Train-accuracy=0.991191
2019-10-24T14:11:30.895502052Z INFO:root:Epoch[0] Time cost=8809.940
2019-10-24T14:11:30.895506735Z INFO:root:Epoch[0] Train-accuracy=0.991340
2019-10-24T14:11:30.895510343Z INFO:root:Epoch[0] Time cost=8809.942
2019-10-24T14:11:30.895514023Z INFO:root:Epoch[0] Train-accuracy=0.990433
2019-10-24T14:11:30.895517677Z INFO:root:Epoch[0] Time cost=8809.942
2019-10-24T14:11:30.906795173Z INFO:root:Epoch[0] Train-accuracy=0.989851
2019-10-24T14:11:30.906839797Z INFO:root:Epoch[0] Time cost=8809.934
2019-10-24T14:11:31.098516951Z 
2019-10-24T14:11:31.098545436Z Segmentation fault: 11
2019-10-24T14:11:31.098548428Z 
2019-10-24T14:11:31.098550789Z Stack trace returned 10 entries:
2019-10-24T14:11:31.098553259Z [bt] (0) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x40b29a) [0x7ff62ff5f29a]
2019-10-24T14:11:31.098555937Z [bt] (1) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x342be86) [0x7ff632f7fe86]
2019-10-24T14:11:31.098558416Z [bt] (2) /lib/x86_64-linux-gnu/libc.so.6(+0x354b0) [0x7ff6eb9ec4b0]
2019-10-24T14:11:31.098560609Z [bt] (3) /lib/x86_64-linux-gnu/libpthread.so.0(pthread_join+0x9) [0x7ff6ebd898d9]
2019-10-24T14:11:31.098562828Z [bt] (4) /usr/lib/x86_64-linux-gnu/libstdc++.so.6(std::thread::join()+0x27) [0x7ff6e71f5b97]
2019-10-24T14:11:31.098565128Z [bt] (5) /usr/local/lib/python2.7/dist-packages/byteps-0.1.0-py2.7-linux-x86_64.egg/byteps/mxnet/c_lib.so(byteps::common::BytePSCommSocket::~BytePSCommSocket()+0xe5) [0x7ff57401dc65]
2019-10-24T14:11:31.098567759Z [bt] (6) /usr/local/lib/python2.7/dist-packages/byteps-0.1.0-py2.7-linux-x86_64.egg/byteps/mxnet/c_lib.so(+0x29215) [0x7ff574002215]
2019-10-24T14:11:31.098570108Z [bt] (7) /usr/local/lib/python2.7/dist-packages/byteps-0.1.0-py2.7-linux-x86_64.egg/byteps/mxnet/c_lib.so(byteps::common::NcclManager::~NcclManager()+0x10c) [0x7ff57401e0ac]
2019-10-24T14:11:31.098572490Z [bt] (8) /usr/local/lib/python2.7/dist-packages/byteps-0.1.0-py2.7-linux-x86_64.egg/byteps/mxnet/c_lib.so(+0x29215) [0x7ff574002215]
2019-10-24T14:11:31.098574930Z [bt] (9) /usr/local/lib/python2.7/dist-packages/byteps-0.1.0-py2.7-linux-x86_64.egg/byteps/mxnet/c_lib.so(byteps::common::BytePSGlobal::Shutdown()+0x39a) [0x7ff57401499a]
2019-10-24T14:11:35.301298756Z Exception in thread Thread-5:
2019-10-24T14:11:35.301328223Z Traceback (most recent call last):
2019-10-24T14:11:35.301331125Z   File "/usr/lib/python2.7/threading.py", line 801, in __bootstrap_inner
2019-10-24T14:11:35.301333868Z     self.run()
2019-10-24T14:11:35.301336079Z   File "/usr/lib/python2.7/threading.py", line 754, in run
2019-10-24T14:11:35.301338565Z     self.__target(*self.__args, **self.__kwargs)
2019-10-24T14:11:35.301340997Z   File "/usr/local/byteps/launcher/launch.py", line 41, in worker
2019-10-24T14:11:35.301344944Z     subprocess.check_call(command, env=my_env, stdout=sys.stdout, stderr=sys.stderr, shell=True)
2019-10-24T14:11:35.301357489Z   File "/usr/lib/python2.7/subprocess.py", line 541, in check_call
2019-10-24T14:11:35.301360000Z     raise CalledProcessError(retcode, cmd)
2019-10-24T14:11:35.301362338Z CalledProcessError: Command '/usr/local/byteps/example/mxnet/start_mxnet_byteps.sh --benchmark 1 --batch-size=32' returned non-zero exit status 255
&lt;/denchmark-code&gt;

More Details:

Two machine with 8*GPU, "Scheduler" and "Work0" are running on one machine, "Server" and "Work1" are running on another one.
The config and script:

&lt;denchmark-code&gt;docker run -it --net=host bytepsimage/byteps_server bash
export DMLC_NUM_WORKER=2
export DMLC_ROLE=scheduler
export DMLC_NUM_SERVER=1
export DMLC_PS_ROOT_URI=172.31.60.11
export DMLC_PS_ROOT_PORT=9001
python /usr/local/byteps/launcher/launch.py
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;docker run -it --net=host bytepsimage/byteps_server bash
export DMLC_NUM_WORKER=2 
export DMLC_ROLE=server
export DMLC_NUM_SERVER=1 
export DMLC_PS_ROOT_URI=172.31.60.11
export DMLC_PS_ROOT_PORT=9001
export MXNET_OMP_MAX_THREADS=4
python /usr/local/byteps/launcher/launch.py
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;docker run --gpus all -it --net=host --shm-size=32768m bytepsimage/worker_mxnet bash
export NVIDIA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
export DMLC_WORKER_ID=0 # worker-0
export DMLC_NUM_WORKER=2 # 2 workers
export DMLC_ROLE=worker # your role is worker
export DMLC_NUM_SERVER=1 
export DMLC_PS_ROOT_URI=172.31.60.11
export DMLC_PS_ROOT_PORT=9001
export EVAL_TYPE=benchmark
python /usr/local/byteps/launcher/launch.py /usr/local/byteps/example/mxnet/start_mxnet_byteps.sh --benchmark 1 --batch-size=32
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;docker run --gpus all -it --net=host --shm-size=32768m bytepsimage/worker_mxnet bash
export NVIDIA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
export DMLC_WORKER_ID=1 # worker-1
export DMLC_NUM_WORKER=2 # 2 workers
export DMLC_ROLE=worker # your role is worker
export DMLC_NUM_SERVER=1 
export DMLC_PS_ROOT_URI=172.31.60.11
export DMLC_PS_ROOT_PORT=9001
export EVAL_TYPE=benchmark
python /usr/local/byteps/launcher/launch.py /usr/local/byteps/example/mxnet/start_mxnet_byteps.sh --benchmark 1 --batch-size=32
&lt;/denchmark-code&gt;


To speed up the test,  "num_epochs" is set with 1 in file "/usr/local/byteps/example/mxnet/train_imagenet_byteps.py"

	</description>
	<comments>
		<comment id='1' author='jazka' date='2019-10-25T02:45:30Z'>
		Thanks for reporting this. This is a known issue that BytePS may crash after the training is finished. Probably it does not release the resources well enough. We will work on this.
		</comment>
		<comment id='2' author='jazka' date='2019-10-25T02:49:29Z'>
		&lt;denchmark-link:https://github.com/bobzhuyb&gt;@bobzhuyb&lt;/denchmark-link&gt;
 Is there any workaround? I need the benchmark result to compare TCP and RDMA?
		</comment>
		<comment id='3' author='jazka' date='2019-10-25T02:55:06Z'>
		If you just want to compare the performance, you have the output "Speed: 141.83 samples/sec" in your log. If you train a real workload, you can save the model before exiting the program. It only crashes when the whole process terminates (when BytePSGlobal::Shutdown() is called)..
We shall fix this bug soon.
		</comment>
		<comment id='4' author='jazka' date='2019-10-25T03:12:16Z'>
		I see. After you public the new release with bug fix, I will close this issue. Thanks!
		</comment>
		<comment id='5' author='jazka' date='2019-10-29T05:49:29Z'>
		&lt;denchmark-link:https://github.com/jazka&gt;@jazka&lt;/denchmark-link&gt;
 The fix has been merged. Can you try again?
The docker image has not been updated. You may need to build the latest code yourself.
		</comment>
		<comment id='6' author='jazka' date='2019-10-30T08:24:57Z'>
		&lt;denchmark-link:https://github.com/jazka&gt;@jazka&lt;/denchmark-link&gt;
 The docker image has been updated now. You may use  to get the latest one.
		</comment>
		<comment id='7' author='jazka' date='2019-11-05T04:49:01Z'>
		Yes, crash has gone. A little issue is that there is no benchmark results after training.
		</comment>
		<comment id='8' author='jazka' date='2019-11-05T04:55:12Z'>
		&lt;denchmark-link:https://github.com/jazka&gt;@jazka&lt;/denchmark-link&gt;
 We often use the output "Speed: 141.83 samples/sec" as the benchmark result. Would you clarify what kind of result you may like to see?
		</comment>
		<comment id='9' author='jazka' date='2019-11-05T06:22:38Z'>
		Yes, the current output is enough, and it will be great to output the summary benchmark results like "Tensorflow" or "PyTorch".
&lt;denchmark-code&gt;Img/sec per GPU: 158.6 +-3.7
Total img/sec on 16 GPU(s): 2537.1 +-59.1
&lt;/denchmark-code&gt;

Anyway, the crash has been fixed, I will close the issue. Thanks for your update!
		</comment>
	</comments>
</bug>