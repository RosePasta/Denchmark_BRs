<bug id='278' author='jasonliu747' open_date='2020-07-29T03:18:26Z' closed_time='2020-08-22T12:27:01Z'>
	<summary>Different memory consumption among GPUs</summary>
	<description>
Describe the bug
When running a training, either on single or multiple nodes, the memory of first GPU card will always be consumed more than others. Looks like it only happened after upgrading BytePS version to v0.2.4.
To Reproduce
Steps to reproduce the behavior:
&lt;denchmark-code&gt;apiVersion: "kubeflow.org/v1beta1"
kind: "MXJob"
metadata:
  name: "byteps-mxnet-job"
spec:
  jobMode: MXTrain
  mxReplicaSpecs:
    Scheduler:
      replicas: 1
      restartPolicy: Never
      template:
        spec:
          containers:
            - name: mxnet
              image: bytepsimage/mxnet:v0.2.4
              command: ["bpslaunch"]
    Server:
      replicas: 1
      restartPolicy: Never
      template:
        spec:
          containers:
            - name: mxnet
              image: bytepsimage/mxnet:v0.2.4
              command: ["bpslaunch"]
              resources:
                requests:
                  cpu: 4
    Worker:
      replicas: 2
      restartPolicy: Never
      template:
        spec:
          containers:
            - name: mxnet
              image: bytepsimage/mxnet:v0.2.4
              command: ["bpslaunch"]
              args: ["python3", "/usr/local/byteps/example/mxnet/train_imagenet_byteps.py", "--benchmark", "1", "--batch-size=32", "--num-epochs=1"]
              volumeMounts:
              - mountPath: /dev/shm
                name: dshm
              resources:
                limits:
                  nvidia.com/gpu: 8
          volumes:
          - name: dshm
            emptyDir:
              medium: Memory
&lt;/denchmark-code&gt;


&lt;denchmark-link:https://user-images.githubusercontent.com/24452340/88752517-b7f2f100-d18c-11ea-8729-4dd6102498b9.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='jasonliu747' date='2020-07-30T01:39:17Z'>
		I am not sure why PID 35529-35546 also uses memory on GPU0. Is your  identical with this &lt;denchmark-link:https://github.com/bytedance/byteps/blob/master/example/mxnet/train_imagenet_byteps.py&gt;example&lt;/denchmark-link&gt;
?

Looks like it only happened after upgrading BytePS version to v0.2.4.

If it means v0.2.3 has no such issue, it looks weird. Because &lt;denchmark-link:https://github.com/bytedance/byteps/commit/c16efff5aa5722738978c93da4ca86b2f8c00634&gt;c16efff&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/bytedance/byteps/commit/9e3b46d3c5508f0cc1b19540564c124f3a2fc0eb&gt;9e3b46d&lt;/denchmark-link&gt;
 do not seem to be relevant to this problem..
		</comment>
		<comment id='2' author='jasonliu747' date='2020-07-30T02:43:06Z'>
		
I am not sure why PID 35529-35546 also uses memory on GPU0. Is your train_imagenet_byteps.py identical with this example?

Yes, it's totally identical.

If it means v0.2.3 has no such issue, it looks weird.

I'm not sure which version caused this issue. The last version I used before v0.2.4 was v0.2.0.  And I just double checked, there is indeed no such issue in v0.2.0.
&lt;denchmark-link:https://user-images.githubusercontent.com/24452340/88874323-62cce300-d251-11ea-9430-6aa230818811.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='jasonliu747' date='2020-07-30T04:03:05Z'>
		
I am not sure why PID 35529-35546 also uses memory on GPU0. Is your train_imagenet_byteps.py identical with this example?

Looks like it only happened after upgrading BytePS version to v0.2.4.

If it means v0.2.3 has no such issue, it looks weird. Because c16efff and 9e3b46d do not seem to be relevant to this problem..

Are u sure? I have been experiencing this problem since I started using byteps.
		</comment>
		<comment id='4' author='jasonliu747' date='2020-07-30T10:19:13Z'>
		It happens when training IMAGENET. Interestingly, &lt;denchmark-link:https://github.com/vycezhong/byteps/blob/gradient_compression/example/mxnet/train_gluon_imagenet_byteps_gc.py&gt;my script&lt;/denchmark-link&gt;
 using  also suffers this.
		</comment>
		<comment id='5' author='jasonliu747' date='2020-07-30T10:46:15Z'>
		Yes, I can reproduce this with v0.2.4 on MXNet.. But PyTorch and TF benchmarks are fine. So I think the problem is in the MXNet example code (or maybe the plugins). We should investigate this.
		</comment>
		<comment id='6' author='jasonliu747' date='2020-08-19T08:02:35Z'>
		&lt;denchmark-link:https://github.com/ymjiang&gt;@ymjiang&lt;/denchmark-link&gt;
 any update on this issue?
		</comment>
		<comment id='7' author='jasonliu747' date='2020-08-22T07:07:38Z'>
		&lt;denchmark-link:https://github.com/jasonliu747&gt;@jasonliu747&lt;/denchmark-link&gt;
  It comes from . After setting  the problem is gone.
		</comment>
		<comment id='8' author='jasonliu747' date='2020-08-22T12:27:57Z'>
		Fixed by &lt;denchmark-link:https://github.com/vycezhong&gt;@vycezhong&lt;/denchmark-link&gt;
. Closing this.
		</comment>
	</comments>
</bug>