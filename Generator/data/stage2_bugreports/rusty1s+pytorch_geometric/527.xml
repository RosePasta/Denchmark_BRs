<bug id='527' author='theswgong' open_date='2019-07-16T08:12:13Z' closed_time='2019-07-18T15:52:30Z'>
	<summary>GATConv costs huge gpu memory</summary>
	<description>
Is there any reason for GATConv costing huge GPU memory? My model has only 6 layers of GATConv with around 10k parameters, but it costs more than 32GB GPU memory with 16 batch size. If I replace two of layers with nn.Linear, it would have 4M parameters but only cost 27GB memory.
It seems there is no broadcasting issues in GATConv implementation. What's the possible issue?
	</description>
	<comments>
		<comment id='1' author='theswgong' date='2019-07-16T08:31:10Z'>
		I will look into it.
		</comment>
		<comment id='2' author='theswgong' date='2019-07-17T07:38:52Z'>
		I also faced similar issues, so I am curious of your thoughts &lt;denchmark-link:https://github.com/rusty1s&gt;@rusty1s&lt;/denchmark-link&gt;
 . But in my case, I suspect this is related to my setting where graphs are very dense. This could explain that the memory issue here is not related to the number of parameters, but more likely to the mapping of nodes with x_j/x_i in the message function ?
		</comment>
		<comment id='3' author='theswgong' date='2019-07-17T10:28:32Z'>
		I think that one source of the problem can be softmax implementation. If I am not wrong, the current pyg implementation of softmax is using autograd to do the backward propagation. As you know, autograd saves a lot of intermidiate values. I think if the deriviative of the softmax is used in the backpropagation will reduce a lot the memory usage.
		</comment>
		<comment id='4' author='theswgong' date='2019-07-18T14:28:52Z'>
		Hi, I finally did some tests concerning this issue. IMO, the softmax call is not the issue as memory usage stays nearly the same with and without the softmax call. This makes sense, as autograd only needs to hold an additional tensor of size [num_edges, num_heads] in memory. In addition, overall memory consumption scales linearly with the number of feature channels and the number of edges, so I do not think there is any problem with the current implementation.
To be honest, IMO the GATConv scales bad by design, as autograd needs to hold tensors of shape [num_edges, num_heads, 2*num_features] in memory. There is no real solution to prevent this, as we need to compute attention scores for each edge and each head, based on pair-wise node features. However, I can easily stack up 10 GAT layers with 8*64 features each on PubMed, and consume about 8GB of memory. For dense graphs, this can explode quite rapidly though.
		</comment>
		<comment id='5' author='theswgong' date='2019-07-18T14:34:29Z'>
		I see. For my case, it may be caused by large number of edges I guess (around 1.5M directed edges, is this too large?)?
		</comment>
		<comment id='6' author='theswgong' date='2019-07-18T14:35:00Z'>
		Yes! Reduce the batch-size ;)
		</comment>
		<comment id='7' author='theswgong' date='2019-07-18T14:37:33Z'>
		Do you know what else conv operators won't scale up by edges? It seems that all the graph convs requires to hold a tensor with the shape at least [num_edges, 2*num_features]?
		</comment>
		<comment id='8' author='theswgong' date='2019-07-18T14:40:46Z'>
		All conv operators will scale up by edges if they depend on pair-wise node features or edge features. However, something like GCNConv can be implemented much more memory-friendly as this operator does not require mapping node features into edge space, e.g., by using sparse-matrix multiplications (with slower runtimes though).
		</comment>
	</comments>
</bug>