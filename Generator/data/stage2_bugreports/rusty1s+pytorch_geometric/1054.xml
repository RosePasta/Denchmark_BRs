<bug id='1054' author='yanghtr' open_date='2020-03-23T05:38:11Z' closed_time='2020-03-30T03:42:33Z'>
	<summary>Example code: an illegal memory access was encountered</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Follows the instructions to install pytorch_geometric
download pytorch_geometric/examples/pointnet2_classification.py
run CUDA_VISIBLE_DEVICES=0  python pointnet2_classification.py

&lt;denchmark-h:h2&gt;Expected behavior&lt;/denchmark-h&gt;

The network should be trained successfully. However, it shows the error:
&lt;denchmark-code&gt;THCudaCheck FAIL file=/pytorch/aten/src/THC/THCReduceAll.cuh line=327 error=700 : an illegal memory access was encountered
Traceback (most recent call last):
  File "pointnet2_classification.py", line 115, in &lt;module&gt;
    train(epoch)
  File "pointnet2_classification.py", line 82, in train
    loss = F.nll_loss(model(data), data.y)
  File "/home/***/miniconda2/envs/pytorch14/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "pointnet2_classification.py", line 63, in forward
    sa1_out = self.sa1_module(*sa0_out)
  File "/home/***/miniconda2/envs/pytorch14/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "pointnet2_classification.py", line 22, in forward
    max_num_neighbors=64)
  File "/home/***/miniconda2/envs/pytorch14/lib/python3.6/site-packages/torch_geometric/nn/pool/__init__.py", line 159, in radius
    return torch_cluster.radius(x, y, r, batch_x, batch_y, max_num_neighbors)
  File "/home/***/miniconda2/envs/pytorch14/lib/python3.6/site-packages/torch_cluster/radius.py", line 54, in radius
    batch_size = int(batch_x.max()) + 1
RuntimeError: cuda runtime error (700) : an illegal memory access was encountered at /pytorch/aten/src/THC/THCReduceAll.cuh:327
&lt;/denchmark-code&gt;

I also try CUDA_LAUNCH_BLOCKING=1 to track where the error occur. The error message is:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "pointnet2_classification.py", line 115, in &lt;module&gt;
    train(epoch)
  File "pointnet2_classification.py", line 82, in train
    loss = F.nll_loss(model(data), data.y)
  File "/home/***/miniconda2/envs/pytorch14/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "pointnet2_classification.py", line 63, in forward
    sa1_out = self.sa1_module(*sa0_out)
  File "/home/***/miniconda2/envs/pytorch14/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "pointnet2_classification.py", line 21, in forward
    row, col = radius(pos, pos[idx], self.r, batch, batch[idx],
RuntimeError: CUDA error: an illegal memory access was encountered
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;


OS: Ubuntu 16.04
Python version: 3.6
PyTorch version: 1.4.0
CUDA/cuDNN version: CUDA 10.1, cudnn 7.5
GCC version: 5.4.0
Any other relevant information: I use miniconda, and create an environment of python3.6

&lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='yanghtr' date='2020-03-23T16:18:28Z'>
		Seems to be a problem with the new fps function of torch-cluster. I am working on it.
		</comment>
		<comment id='2' author='yanghtr' date='2020-03-24T08:28:39Z'>
		I fixed this issue in torch-cluster master and am now working on a new release.
		</comment>
		<comment id='3' author='yanghtr' date='2020-03-26T03:48:33Z'>
		&lt;denchmark-link:https://github.com/rusty1s&gt;@rusty1s&lt;/denchmark-link&gt;
 .Hi, I still have the bug. I update the  and the code could run several epochs. However, after several epochs, it runs into the same bug:
&lt;denchmark-code&gt;Epoch: 001, Test: 0.5099
Epoch: 002, Test: 0.7026
Epoch: 003, Test: 0.7313
Epoch: 004, Test: 0.7181
Epoch: 005, Test: 0.8007
Epoch: 006, Test: 0.8128
Epoch: 007, Test: 0.8436
THCudaCheck FAIL file=/pytorch/aten/src/THC/THCReduceAll.cuh line=327 error=700 : an illegal memory access was encountered
Traceback (most recent call last):
  File "pointnet2_classification.py", line 115, in &lt;module&gt;
    train(epoch)
  File "pointnet2_classification.py", line 82, in train
    loss = F.nll_loss(model(data), data.y)
  File "/home/***/Software/miniconda2/envs/pytorch14/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "pointnet2_classification.py", line 63, in forward
    sa1_out = self.sa1_module(*sa0_out)
  File "/home/***/Software/miniconda2/envs/pytorch14/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "pointnet2_classification.py", line 22, in forward
    max_num_neighbors=64)
  File "/home/***/Software/miniconda2/envs/pytorch14/lib/python3.6/site-packages/torch_geometric/nn/pool/__init__.py", line 159, in radius
    return torch_cluster.radius(x, y, r, batch_x, batch_y, max_num_neighbors)
  File "/home/***/Software/miniconda2/envs/pytorch14/lib/python3.6/site-packages/torch_cluster/radius.py", line 54, in radius
    batch_size = int(batch_x.max()) + 1
RuntimeError: cuda runtime error (700) : an illegal memory access was encountered at /pytorch/aten/src/THC/THCReduceAll.cuh:327
&lt;/denchmark-code&gt;

Could you help with this?
		</comment>
		<comment id='4' author='yanghtr' date='2020-03-26T08:44:56Z'>
		Mh, there seems to be still a bug going on. Let me look into it once more.
		</comment>
		<comment id='5' author='yanghtr' date='2020-03-26T16:44:35Z'>
		Add  before &lt;denchmark-link:https://github.com/rusty1s/pytorch_cluster/blob/9c33077e0bcc0da42dc66e62a34a5e88a921b1c3/csrc/cuda/fps_cuda.cu#L28&gt;this line&lt;/denchmark-link&gt;
 seems to work.
		</comment>
		<comment id='6' author='yanghtr' date='2020-03-26T16:46:07Z'>
		Yes, that's it! Thank you! I will fix it.
		</comment>
		<comment id='7' author='yanghtr' date='2020-03-30T06:31:12Z'>
		Has the binary been released yet?
		</comment>
		<comment id='8' author='yanghtr' date='2020-03-30T09:45:45Z'>
		Not yet, will work on it.
		</comment>
	</comments>
</bug>