<bug id='500' author='joeybose' open_date='2019-07-08T20:08:30Z' closed_time='2019-07-11T05:23:25Z'>
	<summary>Reparametrization in VGAE model can cause NAN's</summary>
	<description>
&lt;denchmark-h:h2&gt;❓ Questions &amp; Help&lt;/denchmark-h&gt;

While, the implementation of the reparametrization is clean and conventional it can potentially lead to  as the  term is not controlled. This behavior is dependent on the use case, and for the most part will not cause problems for many applications but I believe a better fix could be something like to add another line after : &lt;denchmark-link:https://github.com/rusty1s/pytorch_geometric/blob/master/torch_geometric/nn/models/autoencoder.py#L231&gt;https://github.com/rusty1s/pytorch_geometric/blob/master/torch_geometric/nn/models/autoencoder.py#L231&lt;/denchmark-link&gt;

&lt;denchmark-code&gt; self.__logvar__ = torch.clamp(self.__logvar__,min=LOG_VAR_MIN,max=LOG_VAR_MAX)
&lt;/denchmark-code&gt;

Where, LOG_VAR_MIN and LOG_VAR_MAX are suitable constants. This prevents numerical issues (underflow/overflow) that can happen during optimization.
	</description>
	<comments>
		<comment id='1' author='joeybose' date='2019-07-09T04:42:55Z'>
		Thank you. I didn‘t know this is common in practice. So i guess it is not the encoder producing NaNs but the exp calls afterwards. Wouldn‘t it be better to just filter out the NaNs after those calls?
		</comment>
		<comment id='2' author='joeybose' date='2019-07-09T07:17:25Z'>
		One can filter out NaNs after the fact as well, I've been using the above trick and it seems to work but maybe its more elegant to filter out NaN's? I think both methods should work.
		</comment>
		<comment id='3' author='joeybose' date='2019-07-09T07:18:50Z'>
		So what do you use for LOG_VAR_MIN and LOG_VAR_MAX?
		</comment>
		<comment id='4' author='joeybose' date='2019-07-09T07:24:54Z'>
		I've been using LOG_VAR_MIN=2 and LOG_VAR_MAX=20 and it was basically taken from another repo and it seems to work. There may be better constants, but for my use case, it solved the issue.
		</comment>
		<comment id='5' author='joeybose' date='2019-07-09T07:29:54Z'>
		But this would result in zero gradients if the encoder does not produce logvar in this interval. I can see the reasoning behind clamping high values, but why should we clamp values smaller than 2? There shouldn't be any numerical issues for low values if I am not mistaken.
		</comment>
		<comment id='6' author='joeybose' date='2019-07-09T08:35:39Z'>
		Well, most of the numerical instability comes from values being too close to 0, so there should be a clamp from below.  I agree however that 2 might be too high and can be lower.
		</comment>
		<comment id='7' author='joeybose' date='2019-07-09T09:55:53Z'>
		But why? There are no instabilities when taking exp(0)?
		</comment>
		<comment id='8' author='joeybose' date='2019-07-09T18:49:00Z'>
		Well, the output from a linear layer is unbounded so if you have a large negative number than exp() of that could be numerically unstable.
		</comment>
		<comment id='9' author='joeybose' date='2019-07-09T20:35:14Z'>
		Yes, it may underflow and result in zeros, but certainly not in NaNs, or what am I missing? Can you do me a favor and test if your model still works if you only clamp high values.
		</comment>
		<comment id='10' author='joeybose' date='2019-07-09T20:40:33Z'>
		Sure I'll update this tonight with the result.
		</comment>
		<comment id='11' author='joeybose' date='2019-07-10T21:39:36Z'>
		Seems like you're right, and you don't need LOG_VAR_MIN. Only having LOG_VAR_MAX solves the issue (at least in my use case)!
		</comment>
		<comment id='12' author='joeybose' date='2019-07-11T04:27:47Z'>
		Perfect, will add it :)
		</comment>
		<comment id='13' author='joeybose' date='2019-07-11T04:32:48Z'>
		Small correction LOG_VAR_MAX=20 blows up still so a lower value like 10 is more stable. But other than that everything is as you predicted.
		</comment>
		<comment id='14' author='joeybose' date='2019-07-11T05:23:24Z'>
		Added to master. Thank you!
		</comment>
	</comments>
</bug>