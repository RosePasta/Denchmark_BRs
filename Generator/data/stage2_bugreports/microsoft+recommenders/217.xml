<bug id='217' author='loomlike' open_date='2018-11-12T00:09:37Z' closed_time='2018-11-13T14:35:58Z'>
	<summary>Spark evaluator recall_at_k is not 'at k'</summary>
	<description>
Spark evaluator recall_at_k returns just recall, not 'at k'
	</description>
	<comments>
		<comment id='1' author='loomlike' date='2018-11-12T09:07:54Z'>
		&lt;denchmark-link:https://github.com/loomlike&gt;@loomlike&lt;/denchmark-link&gt;
 can you provide more details about this issue?
		</comment>
		<comment id='2' author='loomlike' date='2018-11-12T17:34:43Z'>
		&lt;denchmark-link:https://github.com/yueguoguo&gt;@yueguoguo&lt;/denchmark-link&gt;
 In python version, get_top_k_items does select top k items from recommendation list where 'at k' functions (such as ndcg, precision, recall) use the k-item lists.
In Spark version, we reuse Spark MLlib implementation for ndcgAt(k) and precisionAt(k), where k is passed as an argument to those function. Since spark select top-k items within each function, we don't select top k of the recommended items.
In recall at k function, however, we calculate recall by using map function, where it basically calculate the length of intersection of recommended list and true label list without selecting top k items from the recommended list beforehand.
As a result, if a user passes a recommended list bigger than K, let's say L, our pyspark implementation will give recall-at-L instead of K, where python implementation will return correct number, recall-at-K.
		</comment>
		<comment id='3' author='loomlike' date='2018-11-13T04:03:21Z'>
		&lt;denchmark-link:https://github.com/loomlike&gt;@loomlike&lt;/denchmark-link&gt;
 top-k for our Spark evaluation modules is guaranteed by the "relevancy functions". This is applied before the actual calculation of the metrics. See &lt;denchmark-link:https://github.com/Microsoft/Recommenders/blob/98a2ce1dde2cf6637a7982cec80cc700851f54b1/reco_utils/evaluation/spark_evaluation.py#L241&gt;line&lt;/denchmark-link&gt;
 here for details.
The deficiency of the current implementation, however, is that the relevancy functions may lead to duplication in calculations with the top-k within Spark metrics. This should be something we can avoid when DF methods are available in Spark.
		</comment>
		<comment id='4' author='loomlike' date='2018-11-13T14:35:50Z'>
		&lt;denchmark-link:https://github.com/yueguoguo&gt;@yueguoguo&lt;/denchmark-link&gt;
 oh, yeah you're right. thought it doesn't use the relevancy functions I will close this issue.
		</comment>
	</comments>
</bug>