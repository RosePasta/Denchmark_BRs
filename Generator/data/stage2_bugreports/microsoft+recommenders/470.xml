<bug id='470' author='miguelgfierro' open_date='2019-01-30T22:18:20Z' closed_time='2019-01-31T15:55:48Z'>
	<summary>[BUG] Problem with tests in Spark</summary>
	<description>
&lt;denchmark-h:h3&gt;What is affected by this bug?&lt;/denchmark-h&gt;

Spark unit tests are failing:
&lt;denchmark-code&gt;2019-01-30T20:22:44.8475536Z tests/unit/test_movielens.py ..                                          [ 11%]
2019-01-30T20:22:57.0368426Z tests/unit/test_spark_evaluation.py ......FFFF.                          [ 72%]
2019-01-30T20:23:12.7221736Z tests/unit/test_spark_splitter.py .....                                  [100%]
2019-01-30T20:23:12.7221885Z 
2019-01-30T20:23:12.7223863Z =================================== FAILURES ===================================
2019-01-30T20:23:12.7224283Z ______________________________ test_spark_recall _______________________________
2019-01-30T20:23:12.7224471Z 
2019-01-30T20:23:12.7224677Z spark_data = (DataFrame[userID: bigint, itemID: bigint, rating: bigint], DataFrame[userID: bigint, itemID: bigint, prediction: bigint])
2019-01-30T20:23:12.7225821Z target_metrics = {'exp_var': -6.4466 ± 6.4e-02, 'mae': 6.375 ± 6.4e-04, 'map': 0.23613 ± 2.4e-05, 'ndcg': 0.38172 ± 3.8e-05, ...}
2019-01-30T20:23:12.7225895Z 
2019-01-30T20:23:12.7226756Z     @pytest.mark.spark
2019-01-30T20:23:12.7226920Z     def test_spark_recall(spark_data, target_metrics):
2019-01-30T20:23:12.7227113Z         df_true, df_pred = spark_data
2019-01-30T20:23:12.7227286Z     
2019-01-30T20:23:12.7227363Z         evaluator = SparkRankingEvaluation(df_true, df_pred)
2019-01-30T20:23:12.7232980Z         assert evaluator.recall_at_k() == target_metrics["recall"]
2019-01-30T20:23:12.7233517Z     
2019-01-30T20:23:12.7233610Z         evaluator1 = SparkRankingEvaluation(
2019-01-30T20:23:12.7233917Z             df_true, df_pred, relevancy_method="by_threshold"
2019-01-30T20:23:12.7234061Z         )
2019-01-30T20:23:12.7234149Z &gt;       assert evaluator1.recall_at_k() == target_metrics["recall"]
2019-01-30T20:23:12.7234972Z E       assert 0.24444444444444444 == 0.37777 ± 3.8e-05
2019-01-30T20:23:12.7235056Z E        +  where 0.24444444444444444 = &lt;bound method SparkRankingEvaluation.recall_at_k of &lt;reco_utils.evaluation.spark_evaluation.SparkRankingEvaluation object at 0x7f20b6ae2b70&gt;&gt;()
2019-01-30T20:23:12.7237393Z E        +    where &lt;bound method SparkRankingEvaluation.recall_at_k of &lt;reco_utils.evaluation.spark_evaluation.SparkRankingEvaluation object at 0x7f20b6ae2b70&gt;&gt; = &lt;reco_utils.evaluation.spark_evaluation.SparkRankingEvaluation object at 0x7f20b6ae2b70&gt;.recall_at_k
2019-01-30T20:23:12.7237461Z 
2019-01-30T20:23:12.7237505Z tests/unit/test_spark_evaluation.py:154: AssertionError
2019-01-30T20:23:12.7237615Z _____________________________ test_spark_precision _____________________________
2019-01-30T20:23:12.7237668Z 
2019-01-30T20:23:12.7237720Z spark_data = (DataFrame[userID: bigint, itemID: bigint, rating: bigint], DataFrame[userID: bigint, itemID: bigint, prediction: bigint])
2019-01-30T20:23:12.7238525Z target_metrics = {'exp_var': -6.4466 ± 6.4e-02, 'mae': 6.375 ± 6.4e-04, 'map': 0.23613 ± 2.4e-05, 'ndcg': 0.38172 ± 3.8e-05, ...}
2019-01-30T20:23:12.7238777Z spark = &lt;pyspark.sql.session.SparkSession object at 0x7f20b56baa90&gt;
2019-01-30T20:23:12.7238811Z 
2019-01-30T20:23:12.7238912Z     @pytest.mark.spark
2019-01-30T20:23:12.7238956Z     def test_spark_precision(spark_data, target_metrics, spark):
2019-01-30T20:23:12.7239185Z         df_true, df_pred = spark_data
2019-01-30T20:23:12.7240496Z     
2019-01-30T20:23:12.7240619Z         evaluator = SparkRankingEvaluation(df_true, df_pred, k=10)
2019-01-30T20:23:12.7240672Z         assert evaluator.precision_at_k() == target_metrics["precision"]
2019-01-30T20:23:12.7240718Z     
2019-01-30T20:23:12.7240805Z         evaluator1 = SparkRankingEvaluation(
2019-01-30T20:23:12.7240852Z             df_true, df_pred, relevancy_method="by_threshold"
2019-01-30T20:23:12.7240896Z         )
2019-01-30T20:23:12.7240982Z &gt;       assert evaluator1.precision_at_k() == target_metrics["precision"]
2019-01-30T20:23:12.7241389Z E       assert 0.13333333333333333 == 0.26666 ± 2.7e-05
2019-01-30T20:23:12.7241469Z E        +  where 0.13333333333333333 = &lt;bound method SparkRankingEvaluation.precision_at_k of &lt;reco_utils.evaluation.spark_evaluation.SparkRankingEvaluation object at 0x7f20b561b4a8&gt;&gt;()
2019-01-30T20:23:12.7241625Z E        +    where &lt;bound method SparkRankingEvaluation.precision_at_k of &lt;reco_utils.evaluation.spark_evaluation.SparkRankingEvaluation object at 0x7f20b561b4a8&gt;&gt; = &lt;reco_utils.evaluation.spark_evaluation.SparkRankingEvaluation object at 0x7f20b561b4a8&gt;.precision_at_k
2019-01-30T20:23:12.7241740Z 
2019-01-30T20:23:12.7241790Z tests/unit/test_spark_evaluation.py:167: AssertionError
2019-01-30T20:23:12.7241842Z _______________________________ test_spark_ndcg ________________________________
2019-01-30T20:23:12.7241969Z 
2019-01-30T20:23:12.7242025Z spark_data = (DataFrame[userID: bigint, itemID: bigint, rating: bigint], DataFrame[userID: bigint, itemID: bigint, prediction: bigint])
2019-01-30T20:23:12.7242642Z target_metrics = {'exp_var': -6.4466 ± 6.4e-02, 'mae': 6.375 ± 6.4e-04, 'map': 0.23613 ± 2.4e-05, 'ndcg': 0.38172 ± 3.8e-05, ...}
2019-01-30T20:23:12.7247150Z 
2019-01-30T20:23:12.7247376Z     @pytest.mark.spark
2019-01-30T20:23:12.7247425Z     def test_spark_ndcg(spark_data, target_metrics):
2019-01-30T20:23:12.7247653Z         df_true, df_pred = spark_data
2019-01-30T20:23:12.7311256Z     
2019-01-30T20:23:12.7311938Z         evaluator0 = SparkRankingEvaluation(df_true, df_true, k=10, col_prediction="rating")
2019-01-30T20:23:12.7312044Z         assert evaluator0.ndcg_at_k() == 1.0
2019-01-30T20:23:12.7312186Z     
2019-01-30T20:23:12.7312233Z         evaluator = SparkRankingEvaluation(df_true, df_pred, k=10)
2019-01-30T20:23:12.7312283Z         assert evaluator.ndcg_at_k() == target_metrics["ndcg"]
2019-01-30T20:23:12.7312374Z     
2019-01-30T20:23:12.7312416Z         evaluator1 = SparkRankingEvaluation(
2019-01-30T20:23:12.7312462Z             df_true, df_pred, relevancy_method="by_threshold"
2019-01-30T20:23:12.7312733Z         )
2019-01-30T20:23:12.7312778Z &gt;       assert evaluator1.ndcg_at_k() == target_metrics["ndcg"]
2019-01-30T20:23:12.7313255Z E       assert 0.29083654563422323 == 0.38172 ± 3.8e-05
2019-01-30T20:23:12.7313329Z E        +  where 0.29083654563422323 = &lt;bound method SparkRankingEvaluation.ndcg_at_k of &lt;reco_utils.evaluation.spark_evaluation.SparkRankingEvaluation object at 0x7f20b5423160&gt;&gt;()
2019-01-30T20:23:12.7313416Z E        +    where &lt;bound method SparkRankingEvaluation.ndcg_at_k of &lt;reco_utils.evaluation.spark_evaluation.SparkRankingEvaluation object at 0x7f20b5423160&gt;&gt; = &lt;reco_utils.evaluation.spark_evaluation.SparkRankingEvaluation object at 0x7f20b5423160&gt;.ndcg_at_k
2019-01-30T20:23:12.7313553Z 
2019-01-30T20:23:12.7313601Z tests/unit/test_spark_evaluation.py:209: AssertionError
2019-01-30T20:23:12.7313741Z ________________________________ test_spark_map ________________________________
2019-01-30T20:23:12.7313777Z 
2019-01-30T20:23:12.7313885Z spark_data = (DataFrame[userID: bigint, itemID: bigint, rating: bigint], DataFrame[userID: bigint, itemID: bigint, prediction: bigint])
2019-01-30T20:23:12.7314365Z target_metrics = {'exp_var': -6.4466 ± 6.4e-02, 'mae': 6.375 ± 6.4e-04, 'map': 0.23613 ± 2.4e-05, 'ndcg': 0.38172 ± 3.8e-05, ...}
2019-01-30T20:23:12.7314409Z 
2019-01-30T20:23:12.7314500Z     @pytest.mark.spark
2019-01-30T20:23:12.7314544Z     def test_spark_map(spark_data, target_metrics):
2019-01-30T20:23:12.7314588Z         df_true, df_pred = spark_data
2019-01-30T20:23:12.7314714Z     
2019-01-30T20:23:12.7314756Z         evaluator1 = SparkRankingEvaluation(
2019-01-30T20:23:12.7314804Z             k=10, rating_true=df_true, rating_pred=df_true, col_prediction="rating"
2019-01-30T20:23:12.7314895Z         )
2019-01-30T20:23:12.7314937Z         assert evaluator1.map_at_k() == 1.0
2019-01-30T20:23:12.7314978Z     
2019-01-30T20:23:12.7315064Z         evaluator = SparkRankingEvaluation(df_true, df_pred, k=10)
2019-01-30T20:23:12.7315122Z         assert evaluator.map_at_k() == target_metrics["map"]
2019-01-30T20:23:12.7315165Z     
2019-01-30T20:23:12.7315265Z         evaluator1 = SparkRankingEvaluation(
2019-01-30T20:23:12.7315311Z             df_true, df_pred, relevancy_method="by_threshold"
2019-01-30T20:23:12.7315354Z         )
2019-01-30T20:23:12.7315437Z &gt;       assert evaluator1.map_at_k() == target_metrics["map"]
2019-01-30T20:23:12.7315655Z E       assert 0.16666666666666666 == 0.23613 ± 2.4e-05
2019-01-30T20:23:12.7315720Z E        +  where 0.16666666666666666 = &lt;bound method SparkRankingEvaluation.map_at_k of &lt;reco_utils.evaluation.spark_evaluation.SparkRankingEvaluation object at 0x7f20b538e550&gt;&gt;()
2019-01-30T20:23:12.7315857Z E        +    where &lt;bound method SparkRankingEvaluation.map_at_k of &lt;reco_utils.evaluation.spark_evaluation.SparkRankingEvaluation object at 0x7f20b538e550&gt;&gt; = &lt;reco_utils.evaluation.spark_evaluation.SparkRankingEvaluation object at 0x7f20b538e550&gt;.map_at_k
2019-01-30T20:23:12.7315959Z 
2019-01-30T20:23:12.7316086Z tests/unit/test_spark_evaluation.py:227: AssertionError
2019-01-30T20:23:12.7316138Z =============================== warnings summary ===============================
2019-01-30T20:23:12.7316463Z /anaconda/envs/reco_pyspark/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:75
2019-01-30T20:23:12.7316857Z   /anaconda/envs/reco_pyspark/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
2019-01-30T20:23:12.7316932Z     return _inspect.getargspec(target)
2019-01-30T20:23:12.7317313Z   /anaconda/envs/reco_pyspark/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
2019-01-30T20:23:12.7317431Z     return _inspect.getargspec(target)
2019-01-30T20:23:12.7317465Z 
2019-01-30T20:23:12.7317687Z /anaconda/envs/reco_pyspark/lib/python3.6/site-packages/nbconvert/exporters/exporter_locator.py:28
2019-01-30T20:23:12.7318087Z   /anaconda/envs/reco_pyspark/lib/python3.6/site-packages/nbconvert/exporters/exporter_locator.py:28: DeprecationWarning: `nbconvert.exporters.exporter_locator` is deprecated in favor of `nbconvert.exporters.base` since nbconvert 5.0.
2019-01-30T20:23:12.7318162Z     DeprecationWarning)
2019-01-30T20:23:12.7318239Z 
2019-01-30T20:23:12.7318441Z /anaconda/envs/reco_pyspark/lib/python3.6/site-packages/tornado/web.py:1747
2019-01-30T20:23:12.7318708Z   /anaconda/envs/reco_pyspark/lib/python3.6/site-packages/tornado/web.py:1747: DeprecationWarning: @asynchronous is deprecated, use coroutines instead
2019-01-30T20:23:12.7318825Z     DeprecationWarning)
2019-01-30T20:23:12.7318855Z 
2019-01-30T20:23:12.7318896Z tests/unit/test_spark_splitter.py::test_min_rating_filter
2019-01-30T20:23:12.7318999Z   /data/home/recocat/cicd/2/s/tests/unit/test_spark_splitter.py:62: DeprecationWarning: This function is deprecated. Please call randint(1, 5 + 1) instead
2019-01-30T20:23:12.7319062Z     1, 5, test_specs["number_of_rows"]
2019-01-30T20:23:12.7319118Z   /data/home/recocat/cicd/2/s/tests/unit/test_spark_splitter.py:65: DeprecationWarning: This function is deprecated. Please call randint(1, 15 + 1) instead
2019-01-30T20:23:12.7319285Z     1, 15, test_specs["number_of_rows"]
2019-01-30T20:23:12.7319341Z   /data/home/recocat/cicd/2/s/tests/unit/test_spark_splitter.py:68: DeprecationWarning: This function is deprecated. Please call randint(1, 5 + 1) instead
2019-01-30T20:23:12.7319467Z     1, 5, test_specs["number_of_rows"]
2019-01-30T20:23:12.7319499Z 
2019-01-30T20:23:12.7319706Z -- Docs: https://docs.pytest.org/en/latest/warnings.html
2019-01-30T20:23:12.7319911Z ---- generated xml file: /data/home/recocat/cicd/2/s/reports/test-unit.xml -----
2019-01-30T20:23:12.7320011Z ======= 4 failed, 14 passed, 65 deselected, 7 warnings in 71.41 seconds ========
2019-01-30T20:23:13.1068522Z ##[error]Bash exited with code '1'.

&lt;/denchmark-code&gt;

also, nightly build of master branch is failing. This is curious since we haven't touched master for a while
&lt;denchmark-code&gt;2019-01-30T22:11:08.8553852Z tests/smoke/test_notebooks_pyspark.py F                                  [100%]
2019-01-30T22:11:08.8554009Z 
2019-01-30T22:11:08.8555105Z =================================== FAILURES ===================================
2019-01-30T22:11:08.8555658Z ____________________________ test_als_pyspark_smoke ____________________________
2019-01-30T22:11:08.8555827Z 
2019-01-30T22:11:08.8558041Z notebooks = {'als_deep_dive': '/data/home/recocat/cicd/6/s/notebooks/02_model/als_deep_dive.ipynb', 'als_pyspark': '/data/home/rec...repare_data/data_split.ipynb', 'evaluation': '/data/home/recocat/cicd/6/s/notebooks/03_evaluate/evaluation.ipynb', ...}
2019-01-30T22:11:08.8558264Z 
2019-01-30T22:11:08.8558312Z     @pytest.mark.smoke
2019-01-30T22:11:08.8558355Z     @pytest.mark.spark
2019-01-30T22:11:08.8558438Z     def test_als_pyspark_smoke(notebooks):
2019-01-30T22:11:08.8558493Z         notebook_path = notebooks["als_pyspark"]
2019-01-30T22:11:08.8558537Z         pm.execute_notebook(
2019-01-30T22:11:08.8558624Z             notebook_path,
2019-01-30T22:11:08.8559081Z             OUTPUT_NOTEBOOK,
2019-01-30T22:11:08.8559261Z             kernel_name=KERNEL_NAME,
2019-01-30T22:11:08.8560027Z             parameters=dict(TOP_K=10, MOVIELENS_DATA_SIZE="100k"),
2019-01-30T22:11:08.8560329Z         )
2019-01-30T22:11:08.8560523Z         nb = pm.read_notebook(OUTPUT_NOTEBOOK)
2019-01-30T22:11:08.8560731Z         df = nb.dataframe
2019-01-30T22:11:08.8560973Z         result_map = df.loc[df["name"] == "map", "value"].values[0]
2019-01-30T22:11:08.8561102Z &gt;       assert result_map == pytest.approx(0.004745, TOL)
2019-01-30T22:11:08.8562438Z E       assert 0.004501150747208286 == 0.004745 ± 2.4e-04
2019-01-30T22:11:08.8562727Z E        +  where 0.004745 ± 2.4e-04 = &lt;function approx at 0x7f3916eaa0d0&gt;(0.004745, 0.05)
2019-01-30T22:11:08.8562826Z E        +    where &lt;function approx at 0x7f3916eaa0d0&gt; = pytest.approx
2019-01-30T22:11:08.8580346Z 
2019-01-30T22:11:08.8580540Z tests/smoke/test_notebooks_pyspark.py:26: AssertionError

&lt;/denchmark-code&gt;

another example of the same test on the same day, it looks that there is a high variability in ALS:
&lt;denchmark-code&gt;2019-01-30T18:39:10.0020645Z collected 4 items / 3 deselected
2019-01-30T18:39:10.0030617Z 
2019-01-30T18:40:02.8710069Z tests/smoke/test_notebooks_pyspark.py F                                  [100%]
2019-01-30T18:40:02.8710356Z 
2019-01-30T18:40:02.8711447Z =================================== FAILURES ===================================
2019-01-30T18:40:02.8711779Z ____________________________ test_als_pyspark_smoke ____________________________
2019-01-30T18:40:02.8712135Z 
2019-01-30T18:40:02.8714246Z notebooks = {'als_deep_dive': '/data/home/recocat/cicd/6/s/notebooks/02_model/als_deep_dive.ipynb', 'als_pyspark': '/data/home/rec...repare_data/data_split.ipynb', 'evaluation': '/data/home/recocat/cicd/6/s/notebooks/03_evaluate/evaluation.ipynb', ...}
2019-01-30T18:40:02.8714375Z 
2019-01-30T18:40:02.8714419Z     @pytest.mark.smoke
2019-01-30T18:40:02.8714544Z     @pytest.mark.spark
2019-01-30T18:40:02.8714586Z     def test_als_pyspark_smoke(notebooks):
2019-01-30T18:40:02.8715492Z         notebook_path = notebooks["als_pyspark"]
2019-01-30T18:40:02.8720322Z         pm.execute_notebook(
2019-01-30T18:40:02.8720463Z             notebook_path,
2019-01-30T18:40:02.8720932Z             OUTPUT_NOTEBOOK,
2019-01-30T18:40:02.8721154Z             kernel_name=KERNEL_NAME,
2019-01-30T18:40:02.8721273Z             parameters=dict(TOP_K=10, MOVIELENS_DATA_SIZE="100k"),
2019-01-30T18:40:02.8721525Z         )
2019-01-30T18:40:02.8722610Z         nb = pm.read_notebook(OUTPUT_NOTEBOOK)
2019-01-30T18:40:02.8722719Z         df = nb.dataframe
2019-01-30T18:40:02.8722768Z         result_map = df.loc[df["name"] == "map", "value"].values[0]
2019-01-30T18:40:02.8722816Z &gt;       assert result_map == pytest.approx(0.004745, TOL)
2019-01-30T18:40:02.8729149Z E       assert 0.003998755188036714 == 0.004745 ± 2.4e-04
2019-01-30T18:40:02.8729450Z E        +  where 0.004745 ± 2.4e-04 = &lt;function approx at 0x7fd545e520d0&gt;(0.004745, 0.05)
2019-01-30T18:40:02.8729582Z E        +    where &lt;function approx at 0x7fd545e520d0&gt; = pytest.approx
2019-01-30T18:40:02.8729629Z 
2019-01-30T18:40:02.8729674Z tests/smoke/test_notebooks_pyspark.py:26: AssertionError

&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;In which platform does it happen?&lt;/denchmark-h&gt;

For example:

Azure Data Science Virtual Machine.

	</description>
	<comments>
		<comment id='1' author='miguelgfierro' date='2019-01-31T14:22:51Z'>
		For the bug of unit tests in spark, there was a problem in the last change in the threshold value of SparkRankingEvaluation.
Initially we had a value of 3.5: &lt;denchmark-link:https://github.com/Microsoft/Recommenders/blob/0fb3f76ae5b02b6dd3921ecddedee414082ee2c3/reco_utils/evaluation/spark_evaluation.py#L371&gt;https://github.com/Microsoft/Recommenders/blob/0fb3f76ae5b02b6dd3921ecddedee414082ee2c3/reco_utils/evaluation/spark_evaluation.py#L371&lt;/denchmark-link&gt;
. Then we changed the threshold to 10, this can be seen here &lt;denchmark-link:https://github.com/Microsoft/Recommenders/blob/a35d2cba61bba060ad155baa228dbba1009c3c4a/reco_utils/evaluation/spark_evaluation.py#L381&gt;https://github.com/Microsoft/Recommenders/blob/a35d2cba61bba060ad155baa228dbba1009c3c4a/reco_utils/evaluation/spark_evaluation.py#L381&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/Microsoft/Recommenders/blob/0fb3f76ae5b02b6dd3921ecddedee414082ee2c3/reco_utils/common/constants.py#L13&gt;https://github.com/Microsoft/Recommenders/blob/0fb3f76ae5b02b6dd3921ecddedee414082ee2c3/reco_utils/common/constants.py#L13&lt;/denchmark-link&gt;
.
As of now, if we want to change the threshold in the evaluator, we can't:
&lt;denchmark-code&gt;class SparkRankingEvaluation:
    """SparkRankingEvaluation"""

    def __init__(
        self,
        rating_true,
        rating_pred,
        k=10,
        relevancy_method="top_k",
        col_user=DEFAULT_USER_COL,
        col_item=DEFAULT_ITEM_COL,
        col_rating=DEFAULT_RATING_COL,
        col_prediction=PREDICTION_COL,
    )
&lt;/denchmark-code&gt;

if we use relevancy_method="by_threshold", this will call this function:
&lt;denchmark-code&gt;def get_relevant_items_by_threshold(
    dataframe,
    col_user="customerID",
    col_item="itemID",
    col_rating="rating",
    threshold=3.5,
):
&lt;/denchmark-code&gt;

we don't have access of threshold from the SparkRankingEvaluation class.
FYI &lt;denchmark-link:https://github.com/yueguoguo&gt;@yueguoguo&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/gramhagen&gt;@gramhagen&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>