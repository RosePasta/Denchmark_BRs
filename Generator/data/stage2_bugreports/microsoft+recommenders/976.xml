<bug id='976' author='Mikelesc' open_date='2019-11-15T10:54:56Z' closed_time='2019-11-15T14:00:19Z'>
	<summary>NCFDataset not working properly with random split</summary>
	<description>
&lt;denchmark-h:h3&gt;Description&lt;/denchmark-h&gt;

When trying to feed the NCFDataset function obtainig train/test sets from the python_random_split function in reco_utils I've noticed that if you shrink the dataset to less than about 29K elements it throws this error:

TypeError: ("unsupported operand type(s) for -: 'float' and 'set'", 'occurred at index 122')

&lt;denchmark-h:h3&gt;How to replicate the issue&lt;/denchmark-h&gt;

Fill the dataset (100k):
&lt;denchmark-code&gt;df = movielens.load_pandas_df(
    size=MOVIELENS_DATA_SIZE,
    header=["userID", "itemID", "rating", "timestamp"]
)
&lt;/denchmark-code&gt;

Pick random elements for train-test-splitting
train, test = python_random_split(df.sample(999))
And then this step throws the error above:
data = NCFDataset(train=train, test=test, seed=SEED)
&lt;denchmark-h:h3&gt;Comments&lt;/denchmark-h&gt;

Everything seems to work fine for me if that sample number is higher than 29K , so I just wonder why is that happening and if it's supposed to work like that.
I'm using W10 with the Anaconda Python v 3.6.5 and I was learning with the ncf_deep_dive and the ncf_movielens notebooks.
	</description>
	<comments>
		<comment id='1' author='Mikelesc' date='2019-11-15T13:57:17Z'>
		I guess it's just because of the leave-one-out evaluation, so there has to be interactions from every user both in the train and the test set in order to make NCFDataset work properly.  Then I should change the way to prepare the dataset for the NCF if I want to try random splitting.
		</comment>
	</comments>
</bug>