<bug id='736' author='loomlike' open_date='2019-04-12T02:15:00Z' closed_time='2019-06-28T11:44:56Z'>
	<summary>[FEATURE] Use rand-seed for reproducibility</summary>
	<description>
&lt;denchmark-h:h3&gt;Description&lt;/denchmark-h&gt;

Use seed for NN-based models - both at notebooks and tests
&lt;denchmark-h:h3&gt;Expected behavior with the suggested feature&lt;/denchmark-h&gt;

Produce same results from notebooks
Assert exact value from tests
	</description>
	<comments>
		<comment id='1' author='loomlike' date='2019-04-15T11:13:38Z'>
		we are using seed in the algos: &lt;denchmark-link:https://github.com/Microsoft/Recommenders/blob/master/reco_utils/recommender/rbm/rbm.py#L74&gt;https://github.com/Microsoft/Recommenders/blob/master/reco_utils/recommender/rbm/rbm.py#L74&lt;/denchmark-link&gt;
. I'm not sure if there is anyone missing.
When trying to address reproducibility, &lt;denchmark-link:https://github.com/anargyri&gt;@anargyri&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/msalvaris&gt;@msalvaris&lt;/denchmark-link&gt;
  and I had some fun trying to achieve it. In DL is very difficult, since the optimization is stochastic and there are some asynchronous processes in the GPU that make difficult to have complete reproducibility.
		</comment>
		<comment id='2' author='loomlike' date='2019-04-15T17:25:13Z'>
		hmmm seems like there are many threads discussing about non-deterministic results from tf out there.
In fastai, you can set several seeds to make the results reproducible.
Have you tried no-multi-threading with all the seeds set including tf.random.set_random_seed? Still no luck?
		</comment>
		<comment id='3' author='loomlike' date='2019-04-15T21:23:57Z'>
		yeah we have that function in all the DL algos: &lt;denchmark-link:https://github.com/Microsoft/Recommenders/search?q=tf.random.set_random_seed&amp;unscoped_q=tf.random.set_random_seed&gt;https://github.com/Microsoft/Recommenders/search?q=tf.random.set_random_seed&amp;unscoped_q=tf.random.set_random_seed&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='loomlike' date='2019-04-17T05:33:18Z'>
		see this, just yesterday we got an error in the nightly builds for xdeepfm: &lt;denchmark-link:https://msdata.visualstudio.com/DefaultCollection/AlgorithmsAndDataScience/_build/results?buildId=2873512&gt;https://msdata.visualstudio.com/DefaultCollection/AlgorithmsAndDataScience/_build/results?buildId=2873512&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='loomlike' date='2019-04-17T17:00:36Z'>
		yeah... we should handle this randomness, but I couldn't find time to work on this issue. If this is urgent thing, can anybody take this item and fix? If not, we can temporally loose the tolerance value for NN algos and I will take care in the next few weeks.
		</comment>
		<comment id='6' author='loomlike' date='2019-04-18T14:50:03Z'>
		adding more folks to discuss this &lt;denchmark-link:https://github.com/anargyri&gt;@anargyri&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/yueguoguo&gt;@yueguoguo&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/gramhagen&gt;@gramhagen&lt;/denchmark-link&gt;
. I agree with Jun Ki, I think it is a very annoying issue. Many times our GPU nightly builds fails because of this
		</comment>
		<comment id='7' author='loomlike' date='2019-04-18T14:57:48Z'>
		One example of a nightly build that failed:
&lt;denchmark-code&gt;tests/smoke/test_deeprec_model.py ..                                     [ 25%]
tests/smoke/test_notebooks_gpu.py ...F..                                 [100%]

=================================== FAILURES ===================================
____________________________ test_notebook_xdeepfm _____________________________

notebooks = {'als_deep_dive': '/data/home/recocat/cicd/17/s/notebooks/02_model/als_deep_dive.ipynb', 'als_pyspark': '/data/home/re...aseline_deep_dive.ipynb', 'data_split': '/data/home/recocat/cicd/17/s/notebooks/01_prepare_data/data_split.ipynb', ...}

    @pytest.mark.smoke
    @pytest.mark.gpu
    @pytest.mark.deeprec
    def test_notebook_xdeepfm(notebooks):
        notebook_path = notebooks["xdeepfm_quickstart"]
        pm.execute_notebook(
            notebook_path,
            OUTPUT_NOTEBOOK,
            kernel_name=KERNEL_NAME,
            parameters=dict(
                EPOCHS_FOR_SYNTHETIC_RUN=20,
                EPOCHS_FOR_CRITEO_RUN=1,
                BATCH_SIZE_SYNTHETIC=128,
                BATCH_SIZE_CRITEO=2048,
            ),
        )
        results = pm.read_notebook(OUTPUT_NOTEBOOK).dataframe.set_index("name")["value"]
    
        assert results["res_syn"]["auc"] == pytest.approx(0.982, rel=TOL, abs=ABS_TOL)
&gt;       assert results["res_syn"]["logloss"] == pytest.approx(0.2306, rel=TOL, abs=ABS_TOL)
E       assert 0.103 == 0.2306 ± 1.2e-01
E        +  where 0.2306 ± 1.2e-01 = &lt;function approx at 0x7f8fa788b840&gt;(0.2306, rel=0.5, abs=0.05)
E        +    where &lt;function approx at 0x7f8fa788b840&gt; = pytest.approx

&lt;/denchmark-code&gt;

I think in this case it is safe to widen the tolerances because in the smoke tests we are doing a small number of iterations, so it's normal that the metrics change a lot. Probably in the integration tests we can be more strict, because there we have more iterations and the model should converge to certain metrics
		</comment>
	</comments>
</bug>