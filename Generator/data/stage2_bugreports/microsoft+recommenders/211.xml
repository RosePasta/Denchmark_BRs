<bug id='211' author='miguelgfierro' open_date='2018-11-09T09:26:04Z' closed_time='2019-02-06T19:27:23Z'>
	<summary>OOM in SAR PySpark metrics with Movielens &amp;gt;10M</summary>
	<description>
The OOM is not in the fit or test, but when we do the metrics.
&lt;denchmark-code&gt;rank_eval = SparkRankingEvaluation(test, top_k, k = TOP_K, col_user="UserId", col_item="MovieId",                                     col_rating="Rating", col_prediction="prediction",
                                    relevancy_method="top_k")

[Yesterday 18:25] Miguel Fierro
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
&lt;ipython-input-19-3af886e35859&gt; in &lt;module&gt;
      1 rank_eval = SparkRankingEvaluation(test, top_k, k = TOP_K, col_user="UserId", col_item="MovieId", 
      2                                     col_rating="Rating", col_prediction="prediction",
----&gt; 3                                     relevancy_method="top_k")

/data/home/recocat/notebooks/miguel/Recommenders/reco_utils/evaluation/spark_evaluation.py in __init__(self, rating_true, rating_pred, k, relevancy_method, col_user, col_item, col_rating, col_prediction)
    256         )
    257 
--&gt; 258         self._metrics = self._calculate_metrics()
    259 
    260     def _calculate_metrics(self):

/data/home/recocat/notebooks/miguel/Recommenders/reco_utils/evaluation/spark_evaluation.py in _calculate_metrics(self)
    273         ).drop(self.col_user)
    274 
--&gt; 275         return RankingMetrics(self._items_for_user_all.rdd)
    276 
    277     def precision_at_k(self):

/anaconda/envs/reco_pyspark/lib/python3.6/site-packages/pyspark/sql/dataframe.py in rdd(self)
     86         """
     87         if self._lazy_rdd is None:
---&gt; 88             jrdd = self._jdf.javaToPython()
     89             self._lazy_rdd = RDD(jrdd, self.sql_ctx._sc, BatchedSerializer(PickleSerializer()))
     90         return self._lazy_rdd

/anaconda/envs/reco_pyspark/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args)
   1255         answer = self.gateway_client.send_command(command)
   1256         return_value = get_return_value(
-&gt; 1257             answer, self.gateway_client, self.target_id, self.name)
   1258 
   1259         for temp_arg in temp_args:

/anaconda/envs/reco_pyspark/lib/python3.6/site-packages/pyspark/sql/utils.py in deco(*a, **kw)
     61     def deco(*a, **kw):
     62         try:
---&gt; 63             return f(*a, **kw)
     64         except py4j.protocol.Py4JJavaError as e:
     65             s = e.java_exception.toString()

/anaconda/envs/reco_pyspark/lib/python3.6/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    326                 raise Py4JJavaError(
    327                     "An error occurred while calling {0}{1}{2}.\n".
--&gt; 328                     format(target_id, ".", name), value)
    329             else:
    330                 raise Py4JError(

Py4JJavaError: An error occurred while calling o231432.javaToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 80.0 failed 1 times, most recent failure: Lost task 3.0 in stage 80.0 (TID 357, localhost, executor driver): org.apache.spark.util.TaskCompletionListenerException: null

Previous exception in task: Unable to acquire 16384 bytes of memory, got 0
	org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:157)
	org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:98)
	org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.reset(UnsafeInMemorySorter.java:186)
	org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:229)
	org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:204)
	org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:283)
	org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)
	org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:383)
	org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:407)
	org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray.add(ExternalAppendOnlyUnsafeRowArray.scala:142)


&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='miguelgfierro' date='2018-11-09T11:03:36Z'>
		Hi &lt;denchmark-link:https://github.com/miguelgfierro&gt;@miguelgfierro&lt;/denchmark-link&gt;
 can you share me the details such as machine size and Spark configurations? Also, codes to reproduce the same error.
		</comment>
		<comment id='2' author='miguelgfierro' date='2018-11-09T11:37:29Z'>
		the machine is the cicd machine -&gt; Standard NC6s_v2 (6 vcpus, 112 GB memory).
Spark configuration is in the notebook:
&lt;denchmark-code&gt;spark = SparkSession \
    .builder \
    .appName("SAR pySpark") \
    .master("local[*]") \
    .config("spark.driver.memory", "2g")\
    .config("spark.executor.cores", "32")\
    .config("spark.executor.memory", "8g")\
    .config("spark.yarn.executor.memoryOverhead", "3g")\
    .config("spark.memory.fraction", "0.9")\
    .config("spark.memory.stageFraction", "0.3")\
    .config("spark.executor.instances", 1)\
    .config("spark.executor.heartbeatInterval", "36000s")\
    .config("spark.network.timeout", "10000000s")\
    .config("spark.driver.maxResultSize", "50g")\
    .config("spark.sql.shuffle.partitions", "10")\
    .getOrCreate()
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='miguelgfierro' date='2018-11-09T19:35:32Z'>
		Is this happening at metrics instead of fit / test, because Spark's lazy evaluation?
Anyway we may need to describe SAR will require at least num_items * num_users * float (double) size of memory in the machine to run? &lt;denchmark-link:https://github.com/miguelgfierro&gt;@miguelgfierro&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/yueguoguo&gt;@yueguoguo&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='miguelgfierro' date='2018-11-13T04:04:28Z'>
		&lt;denchmark-link:https://github.com/loomlike&gt;@loomlike&lt;/denchmark-link&gt;
 it is a great idea to suggest minimal memory size for users - it is always issue for running large-scale analytics on single-node Spark environment.
		</comment>
		<comment id='5' author='miguelgfierro' date='2018-11-13T06:25:55Z'>
		agree
		</comment>
		<comment id='6' author='miguelgfierro' date='2018-11-14T15:26:39Z'>
		I agree with &lt;denchmark-link:https://github.com/loomlike&gt;@loomlike&lt;/denchmark-link&gt;
 about lazy evaluation - can you trigger the result of .predict with .count() and see if that produces an OOM? Same for structures computes in .fit
		</comment>
		<comment id='7' author='miguelgfierro' date='2019-02-06T19:27:23Z'>
		closing this for now, please feel free to reopen if needed
		</comment>
	</comments>
</bug>