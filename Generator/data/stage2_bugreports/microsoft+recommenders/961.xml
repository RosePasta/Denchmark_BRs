<bug id='961' author='atimesastudios' open_date='2019-10-24T13:57:03Z' closed_time='2019-10-29T13:26:25Z'>
	<summary>[BUG] xDeepFM output file length less than test file length</summary>
	<description>
&lt;denchmark-h:h3&gt;Description&lt;/denchmark-h&gt;

After fitting an xDeepFM model using CIN and DNN, I call model.predict() passing in the test file in FFM format with 12788 rows of test examples in the FFM format. When I examine the output file created I see that there are 12689 rows of predictions.
I've used different train, validate, test splits and find that I keep seeing fewer rows of predictions than test examples.
The way the model is implemented, the output file rows do not have information such as userID, itemID. If this were the case, at least I could see which examples did not have a prediction returned for them.
What might be the cause for this discrepancy? Is there any way to write out the userID, itemID associated with the prediction?
&lt;denchmark-h:h3&gt;In which platform does it happen?&lt;/denchmark-h&gt;

Windows 10 running Anaconda. Currently using the master branch from recommenders
&lt;denchmark-h:h3&gt;How do we replicate the issue?&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Expected behavior (i.e. solution)&lt;/denchmark-h&gt;

The number of rows in the output file should match the number of rows in the test file.
Ideally, the output file should have the userID, productID pair and the prediction, or some way of indexing the prediction back to the example presented. It would be nice if this was returned from the model.predict call as a unified dataframe rather than writing outputs
&lt;denchmark-h:h3&gt;Other Comments&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='atimesastudios' date='2019-10-25T09:22:46Z'>
		FYI &lt;denchmark-link:https://github.com/yueguoguo&gt;@yueguoguo&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/Leavingseason&gt;@Leavingseason&lt;/denchmark-link&gt;
 any idea for this?
		</comment>
		<comment id='2' author='atimesastudios' date='2019-10-25T11:25:45Z'>
		train:
batch_size: 128
total examples 255762
split is 0.9, 0.05, 0.05
train shape is (230186, 5)
validate shape is (12788, 5)
test shape is (12788, 5)
&lt;denchmark-code&gt;rating	userID	itemID	feat1	feat2
&lt;/denchmark-code&gt;

227840	4	1:1:2249	2:2:65178	3:3:1	4:112:1
204339	3	1:1:1847	2:2:62676	3:4:1	4:253:1
73644	4	1:1:1689	2:2:44864	3:3:1	4:141:1
68849	3	1:1:2650	2:2:44697	3:5:1	4:142:1
176296	5	1:1:2653	2:2:58389	3:3:1	4:387:1
fields 4
features 486
[('DNN_FIELD_NUM', None), ('FEATURE_COUNT', 486), ('FIELD_COUNT', 4), ('MODEL_DIR', './outputs'), ('PAIR_NUM', None), ('SUMMARIES_DIR', './outputs'), ('activation', ['relu', 'relu']), ('attention_activation', None), ('attention_dropout', 0.0), ('attention_layer_sizes', None), ('batch_size', 128), ('cross_activation', 'identity'), ('cross_l1', 0.0), ('cross_l2', 0.0001), ('cross_layer_sizes', [100, 100, 50]), ('cross_layers', None), ('data_format', 'ffm'), ('dim', 10), ('doc_size', None), ('dropout', [0.0, 0.0]), ('dtype', 32), ('embed_l1', 0.0), ('embed_l2', 0.0001), ('enable_BN', False), ('entityEmb_file', None), ('entity_dim', None), ('entity_embedding_method', None), ('entity_size', None), ('epochs', 7), ('fast_CIN_d', 0), ('filter_sizes', None), ('init_method', 'tnormal'), ('init_value', 0.01), ('is_clip_norm', 0), ('iterator_type', None), ('kg_file', None), ('kg_training_interval', 5), ('layer_l1', 0.0), ('layer_l2', 0.0), ('layer_sizes', [100, 100]), ('learning_rate', 0.001), ('load_model_name', None), ('load_saved_model', False), ('loss', 'square_loss'), ('lr_kg', 0.5), ('lr_rs', 1), ('max_grad_norm', 2), ('method', 'regression'), ('metrics', ['rmse']), ('model_type', 'xDeepFM'), ('mu', None), ('n_item', None), ('n_item_attr', None), ('n_user', None), ('n_user_attr', None), ('num_filters', None), ('optimizer', 'adam'), ('reg_kg', 0.0), ('save_epoch', 2), ('save_model', True), ('show_step', 20), ('train_ratio', None), ('transform', None), ('use_CIN_part', True), ('use_DNN_part', True), ('use_FM_part', False), ('use_Linear_part', False), ('user_clicks', None), ('user_dropout', False), ('wordEmb_file', None), ('word_size', None), ('write_tfevents', True)]
It uses CIN and DNN only
Output file has 12689 rows
		</comment>
		<comment id='3' author='atimesastudios' date='2019-10-25T12:03:53Z'>
		Running the same setup with batch_size = 256 gives me an output file with 12739 predictions versus the test file which has 12788 examples.
		</comment>
		<comment id='4' author='atimesastudios' date='2019-10-25T12:27:29Z'>
		I went back to batch_size 128 and made my test file a multiple of 128, so the number of examples is now 12544, (128 * 98 = 12544). Ran it again and this time my output file has 12447 rows, a difference of 97 predictions.
		</comment>
		<comment id='5' author='atimesastudios' date='2019-10-25T12:56:08Z'>
		Noticed that &lt;denchmark-link:https://github.com/microsoft/recommenders/blob/master/tests/smoke/test_deeprec_model.py&gt;https://github.com/microsoft/recommenders/blob/master/tests/smoke/test_deeprec_model.py&lt;/denchmark-link&gt;
, the smoke test for deeprec does not check to see if the number of predictions returned matches the number of examples presented.
		</comment>
		<comment id='6' author='atimesastudios' date='2019-10-25T13:23:37Z'>
		Capturing this link here in case it is relevant:
&lt;denchmark-link:https://stackoverflow.com/questions/48551158/keras-predict-generator-not-returning-correct-number-of-samples&gt;https://stackoverflow.com/questions/48551158/keras-predict-generator-not-returning-correct-number-of-samples&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='atimesastudios' date='2019-10-25T13:27:44Z'>
		&lt;denchmark-link:https://user-images.githubusercontent.com/3003461/67574917-6a32ef00-f709-11e9-818d-113270b51b18.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/miguelgfierro&gt;@miguelgfierro&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/yueguoguo&gt;@yueguoguo&lt;/denchmark-link&gt;
  and &lt;denchmark-link:https://github.com/Leavingseason&gt;@Leavingseason&lt;/denchmark-link&gt;

I carefully examined the output file. It looks like there are some lines that have more than one prediction.
		</comment>
		<comment id='8' author='atimesastudios' date='2019-10-25T14:05:35Z'>
		&lt;denchmark-link:https://user-images.githubusercontent.com/3003461/67575325-53d96300-f70a-11e9-918e-ffe7c459760d.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/3003461/67575358-69e72380-f70a-11e9-8de2-710363db0d52.png&gt;&lt;/denchmark-link&gt;

The bug manifests itself exactly at my batch_size boundary which is 128.
I'm going to instrument base_model.py in BaseModel's predict() to see how many times it puts a blank line
		</comment>
		<comment id='9' author='atimesastudios' date='2019-10-25T14:24:00Z'>
		Is BaseModel predict() expecting each input file to be exactly batch_size number of examples?
		</comment>
		<comment id='10' author='atimesastudios' date='2019-10-25T16:13:43Z'>
		&lt;denchmark-link:https://github.com/miguelgfierro&gt;@miguelgfierro&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/yueguoguo&gt;@yueguoguo&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/Leavingseason&gt;@Leavingseason&lt;/denchmark-link&gt;
 - I think this change fixes the problem.
&lt;denchmark-link:https://user-images.githubusercontent.com/3003461/67586823-868e5600-f720-11e9-89fd-1d78afa337c7.png&gt;&lt;/denchmark-link&gt;

After making this change, my output files do not have more than one prediction per line and the number of predictions ties out with the number of examples. I'd like to create a pull request for this fix.
		</comment>
		<comment id='11' author='atimesastudios' date='2019-10-26T08:51:53Z'>
		&lt;denchmark-link:https://github.com/atimesastudios&gt;@atimesastudios&lt;/denchmark-link&gt;
 sorry for not replying in time but you are right. We have spotted the bug before in &lt;denchmark-link:https://github.com/microsoft/recommenders/issues/782&gt;#782&lt;/denchmark-link&gt;
 and fixed it in &lt;denchmark-link:https://github.com/microsoft/recommenders/pull/833&gt;#833&lt;/denchmark-link&gt;
. Are you using the new version of the repo utils?
		</comment>
		<comment id='12' author='atimesastudios' date='2019-10-26T12:06:38Z'>
		I will check to make sure I have the latest version.
		</comment>
		<comment id='13' author='atimesastudios' date='2019-10-26T12:11:24Z'>
		&lt;denchmark-link:https://github.com/yueguoguo&gt;@yueguoguo&lt;/denchmark-link&gt;
 - I just looked at the codebase on the recommender master branch. It still seems to have the bug in the codebase.
&lt;denchmark-code&gt;    load_sess = self.sess
    with tf.gfile.GFile(outfile_name, "w") as wt:
        for batch_data_input in self.iterator.load_data_from_file(infile_name):
            step_pred = self.infer(load_sess, batch_data_input)
            step_pred = np.reshape(step_pred, -1)
            wt.write("\n".join(map(str, step_pred)))

        # line break after each batch.
        wt.write("\n")
    return self
&lt;/denchmark-code&gt;

		</comment>
		<comment id='14' author='atimesastudios' date='2019-10-26T12:20:26Z'>
		&lt;denchmark-link:https://github.com/yueguoguo&gt;@yueguoguo&lt;/denchmark-link&gt;
 - It appears that the fix made in &lt;denchmark-link:https://github.com/microsoft/recommenders/pull/833&gt;#833&lt;/denchmark-link&gt;
 was overwritten by the very next commit ("small fix"). Would it be possible to re-apply the fix into the latest codebase?
		</comment>
		<comment id='15' author='atimesastudios' date='2019-10-29T03:15:43Z'>
		Thanks &lt;denchmark-link:https://github.com/atimesastudios&gt;@atimesastudios&lt;/denchmark-link&gt;

Also thanks for @elogicsal for creating PR to fix the issue.
		</comment>
	</comments>
</bug>