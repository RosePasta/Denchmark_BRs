<bug id='911' author='rogerrojur' open_date='2019-09-03T09:45:46Z' closed_time='2019-10-29T03:43:12Z'>
	<summary>[BUG]</summary>
	<description>
&lt;denchmark-h:h3&gt;Description&lt;/denchmark-h&gt;

Py4JJavaError                             Traceback (most recent call last)
 in 
9 )
10
---&gt; 11 data = movielens.load_spark_df(spark, size=MOVIELENS_DATA_SIZE, schema=schema)
12 data.show()
D:\tuijian\Recommenders\reco_utils\dataset\movielens.py in load_spark_df(spark, size, header, schema, local_cache_path, dbutils, title_col, genres_col, year_col)
451         # Cache and force trigger action since data-file might be removed.
452         df.cache()
--&gt; 453         df.count()
454
455     return df
&lt;denchmark-h:h3&gt;In which platform does it happen?&lt;/denchmark-h&gt;

windows 10
&lt;denchmark-h:h3&gt;How do we replicate the issue?&lt;/denchmark-h&gt;

I follow the setup.md to create the environment reco_pyspark. When I start the test for "als_movielens.ipynb", this error occur, and there is no detail description for this bug, so confusing.
	</description>
	<comments>
		<comment id='1' author='rogerrojur' date='2019-09-03T09:56:34Z'>
		Is this the entire error message generated? Usually there's a section indication the type of error. Running a single node spark instance on Windows can be a little tricky so there could be a few things going on. Any additional output will help Barrow it down.
		</comment>
		<comment id='2' author='rogerrojur' date='2019-10-29T03:43:12Z'>
		Reopen if needed.
		</comment>
	</comments>
</bug>