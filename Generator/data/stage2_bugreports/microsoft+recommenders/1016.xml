<bug id='1016' author='elogicaadith' open_date='2019-12-18T18:00:34Z' closed_time='2020-07-30T13:46:56Z'>
	<summary>[BUG] Not able to restore xDeepFM model</summary>
	<description>
&lt;denchmark-h:h3&gt;Description&lt;/denchmark-h&gt;

Not able to restore trained xDeepFM models from TensorFlow save files.
The "create_hparams" function, present in the "deeprec_utils.py" script takes two variables related to model restoration:

load_saved_model - a Boolean, set to False by default in the YAML file for xDeepFM.
load_model_name - the model path.

Out of these two variables read from the YAML file, I could only see load_saved_model being used in the "load_model" function present in the base_model.py file.
The following is the code for the load_model function:
&lt;denchmark-link:https://user-images.githubusercontent.com/58820992/71109271-d9b9dd80-2192-11ea-8640-546e60c0b75a.png&gt;&lt;/denchmark-link&gt;

Observations:

In the statement act_path = self.hparams.load_saved_model ; act_path is assigned with the Boolean variable &amp; and not the path string variable - this will cause the self.saver.restore(self.sess, act_path) statement to fail.
Most importantly, the load_model function is not being called by xDeepFM.py. Therefore, I believe that xDeepFM does not support loading trained models at the moment.

tried restoring the model with tensorflow 1.14:
&lt;denchmark-link:https://user-images.githubusercontent.com/58820992/71110922-3e2a6c00-2196-11ea-8774-6aff86b95dd6.png&gt;&lt;/denchmark-link&gt;

I've had no luck restoring it this way as well.
Any help with this is much appreciated - thank you!
&lt;denchmark-h:h3&gt;In which platform does it happen?&lt;/denchmark-h&gt;

All platforms
&lt;denchmark-h:h3&gt;How do we replicate the issue?&lt;/denchmark-h&gt;

Try restoring a saved xDeepFM model
&lt;denchmark-h:h3&gt;Expected behavior (i.e. solution)&lt;/denchmark-h&gt;

The model should be restored from files; making it usable in the run_eval &amp; predict functions.
	</description>
	<comments>
		<comment id='1' author='elogicaadith' date='2019-12-19T15:49:31Z'>
		FYI &lt;denchmark-link:https://github.com/Leavingseason&gt;@Leavingseason&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='elogicaadith' date='2020-01-27T16:45:50Z'>
		&lt;denchmark-link:https://github.com/elogicaadith&gt;@elogicaadith&lt;/denchmark-link&gt;
 - I was able to use load_model from base_model.py to load a saved SLi_Rec (Sequential Recommender) model that inherits from BaseModel via SequentialBaseModel.
I ran SLi_Rec on Azure Machine Learning Service and saved the model and summary directories from my best model run to ./outputs/model and ./outputs/summary on my local computer. The model directory has the checkpoint file, data, index and meta files for each epoch.
In my case the best epoch was epoch 8 with information in "epoch_8.data-00000-of-00001", "epoch_8.index" and "epoch_8.meta"
In the case of SLi_Rec I had to use prepare_hparams to recreate the hparams information and an iterator called SequentialIterator. Using these objects I was able to instantiate the model. This will probably be similar for xDeepFM, if I recall correctly xDeepFM uses an FFMTextIterator.
In the case of SLi_Rec:
hparams = prepare_hparams(yaml, ...)
input_creator = SequentialIterator
model = SLI_RECModel(hparams, input_creator)
Once I had a model object, I was able to call load_model as follows:
(ellipses used to redact the actual path)
model_dir = os.path.join(..., "outputs", "model")
model.load_model(model_path=os.path.join(model_dir, "epoch_8")
Once the model was restored I was able to successfully call predict() by passing in a test file path and output file path.
In the case of xDeepFM, the XDeepFMModel class directly inherits from BaseModel.
So you will most likely have the same sequence: use prepare_hparams to get the hparam object, instantiate an FFMTextIterator and create an XDeepFMModel instance.
Identify the epoch that you want to load and follow the pattern above.
See if that works for you.
I do see that act_path is instantiated to be boolean, but I don't think that's what is creating the issue.
		</comment>
	</comments>
</bug>