<bug id='984' author='ghost(ghost)' open_date='2019-11-25T03:38:36Z' closed_time='2020-07-30T13:51:05Z'>
	<summary>[BUG] Memory error in map_at_k (using Cornac BPR)</summary>
	<description>
&lt;denchmark-h:h3&gt;Description&lt;/denchmark-h&gt;

I am running Cornac BPR on a problem with 13K users and 25K items on an Standard NC12 system with 118GB of RAM. I used the Cornac BPR example and adapted it to do hyperparameter optimization in Azure. The model is fit and all_predictions run OK. Now the program starts to evaluate the model and experiences a Memory Error in map_at_k.
&lt;denchmark-code&gt;eval_map = map_at_k(test, all_predictions, col_prediction="prediction", k=TOP_K)
&lt;/denchmark-code&gt;

File /reco_utils/evaluation/python_evaluation.py", line 618, in map_at_k
threshold=threshold,
File reco_utils/evaluation/python_evaluation.py", line 81, in check_column_dtypes_wrapper
**kwargs
File reco_utils/dataset/pandas_df_utils.py", line 493, in wrapper
return cached_wrapper(*args, **kwargs)
File reco_utils/dataset/pandas_df_utils.py", line 460, in hash
hashable += tuple(self.pandas_object.columns)
MemoryError
I am able to run a different, smaller problem with under 550 users and 24K items with no problems.
What is the best way to run the evaluation functions for large datasets?
Is there a way to batch feed the predictions to the evaluation functions map_at_k, ndcg_at_k etc.?
&lt;denchmark-h:h3&gt;In which platform does it happen?&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;How do we replicate the issue?&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Expected behavior (i.e. solution)&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Other Comments&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='ghost(ghost)' date='2019-11-25T13:05:00Z'>
		The exact same problem occurs when I run SVD on the same dataset on the same Standard NC12 instance (112 GB of RAM)
File "reco_utils/azureml/svd_training.py", line 119, in 
main()
File "reco_utils/azureml/svd_training.py", line 112, in main
svd = svd_training(args)
File "reco_utils/azureml/svd_training.py", line 59, in svd_training
result = eval(metric)(validation_data, all_predictions, col_prediction='prediction', k=k)
File reco_utils/evaluation/python_evaluation.py", line 451, in precision_at_k
threshold=threshold,
File reco_utils/evaluation/python_evaluation.py", line 81, in check_column_dtypes_wrapper
**kwargs
File reco_utils/dataset/pandas_df_utils.py", line 493, in wrapper
return cached_wrapper(*args, **kwargs)
File reco_utils/dataset/pandas_df_utils.py", line 460, in hash
hashable += tuple(self.pandas_object.columns)
MemoryError
		</comment>
		<comment id='2' author='ghost(ghost)' date='2019-11-25T15:57:53Z'>
		the quickest answer would be to use Spark, that should just work.
Now, the dataset you are using is kind of the same size of movielens20M (20 million ratings from 138000 users on 27000 movies). In the past (correct me if I'm wrong) &lt;denchmark-link:https://github.com/yueguoguo&gt;@yueguoguo&lt;/denchmark-link&gt;
 optimized the code for the metrics. Maybe there is a way to optimize this further. @elogicsal any help on this would be beneficial
Finally, if you don't want to use spark, there is a way to optimize cpu workloads through several machines using dask library, however implementing this is not easy.
		</comment>
		<comment id='3' author='ghost(ghost)' date='2019-11-29T07:09:51Z'>
		It looks like the issue is from &lt;denchmark-link:https://github.com/microsoft/recommenders/blob/7c0a6a3e23dee047b9afbf8564bd236a8300454e/reco_utils/dataset/pandas_df_utils.py#L458&gt;here&lt;/denchmark-link&gt;
 where the pandas dataframe columns are put into a hashable object. The hashable object is produced from . Not sure which part is the root cause of the problem though...
		</comment>
		<comment id='4' author='ghost(ghost)' date='2019-12-26T03:28:53Z'>
		Hi @elogicsal have you worked out a solution to the problem?
		</comment>
		<comment id='5' author='ghost(ghost)' date='2020-01-02T15:36:46Z'>
		&lt;denchmark-link:https://github.com/yueguoguo&gt;@yueguoguo&lt;/denchmark-link&gt;
 - Not as yet, will keep looking.
		</comment>
		<comment id='6' author='ghost(ghost)' date='2020-01-21T14:17:16Z'>
		I had to bump up to the next Azure ML Compute size to get enough RAM (200GB+) for the scale of my problem.
		</comment>
	</comments>
</bug>