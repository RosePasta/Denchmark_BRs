<bug id='792' author='sakinaljana' open_date='2018-05-11T02:54:57Z' closed_time='2018-05-22T04:31:20Z'>
	<summary>*Help* No model output yielded when running example using Cloud ML Engine</summary>
	<description>
Hi guys,
I'm just trying to running example from here:
&lt;denchmark-link:https://tensorflow.github.io/tensor2tensor/cloud_mlengine.html&gt;https://tensorflow.github.io/tensor2tensor/cloud_mlengine.html&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;# Note that both the data dir and output dir have to be on GCS
DATA_DIR=gs://my-bucket/data
OUTPUT_DIR=gs://my-bucket/train
t2t-trainer \
  --problem=translate_ende_wmt32k \
  --model=transformer \
  --hparams_set=transformer_base \
  --data_dir=$DATA_DIR \
  --output_dir=$OUTPUT_DIR \
  --cloud_mlengine
&lt;/denchmark-code&gt;

turns out the execution is successful but the model files (e.g.: .pbtxt, checkpoint, *.meta, *.index, ckpt, hparams.json) is not written into my $OUTPUT_DIR. I suspect the worker is not running because the logging only throw master instance, not worker instance. This is came conclusion the training is not yet started. Is there any steps that i'm missing to perform this on ML engine?
Thanks
I'm using latest version of T2T: v1.6.2 and Tensorflow 1.8
Appendix:
Sample cloud ML logs output:
&lt;denchmark-code&gt;T master-replica-0 gapic-google-cloud-logging-v2 0.91.3 has requirement google-gax&lt;0.16dev,&gt;=0.15.7, but you'll have google-gax 0.12.5 which is incompatible.
master-replica-0 Installing collected packages: MarkupSafe, Jinja2, itsdangerous, click, flask, gunicorn, bz2file, pyglet, gym, h5py, greenlet, gevent, tensor2tensor, DummyT2TPackage
master-replica-0 The script flask is installed in '/root/.local/bin' which is not on PATH.
master-replica-0 Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
master-replica-0 The scripts gunicorn and gunicorn_paster are installed in '/root/.local/bin' which is not on PATH.

I  master-replica-0 Successfully built DummyUsrDirPackage master-replica-0 
E  master-replica-0 gapic-google-cloud-logging-v2 0.91.3 has requirement google-gax&lt;0.16dev,&gt;=0.15.7, but you'll have google-gax 0.12.5 which is incompatible. master-replica-0 
I  master-replica-0 Installing collected packages: DummyUsrDirPackage master-replica-0 
I  master-replica-0   Found existing installation: DummyUsrDirPackage 0.1 master-replica-0 
I  master-replica-0     Uninstalling DummyUsrDirPackage-0.1: master-replica-0 
I  master-replica-0       Successfully uninstalled DummyUsrDirPackage-0.1 master-replica-0 
I  master-replica-0 Successfully installed DummyUsrDirPackage-0.1 master-replica-0 
I  master-replica-0 Running command: python -m tensor2tensor.bin.t2t_trainer --eval_steps 100 --cloud_tpu False --decode_hparams  --sync False --eval_run_autoregressive False --eval_use_test_set False --worker_id 0 --eval_early_stopping_metric_minimize True --worker_replicas 1 --agent_policy_path  --random_seed 1234 --worker_gpu_memory_fraction 0.95 --train_steps 7500 --cloud_tpu_name test-tpu --locally_shard_to_cpu False --iterations_per_loop 100 --registry_help False --worker_gpu 1 --keep_checkpoint_max 20 --save_checkpoints_secs 0 --gpu_order  --master  --generate_data False --intra_op_parallelism_threads 0 --enable_graph_rewriter False --eval_early_stopping_metric loss --autoencoder_path  --output_dir gs://test_t2t/poetry/model --profile False --ps_job /job:ps --tmp_dir /tmp/t2t_datagen --schedule continuous_train_and_eval --inter_op_parallelism_threads 0 --hparams  --use_tpu False --eval_early_stopping_metric_delta 0.1 --ps_gpu 0 --tfdbg False --local_eval_frequency 1000 --data_dir gs://test_t2t/poetry/subset --ps_replicas 0 --export_saved_model False --problem poetry_line_problem --log_device_placement False --hparams_set transformer_poetry --dbgprofile False --timit_paths  --cloud_skip_confirmation False --cloud_delete_on_done False --tpu_num_shards 8 --cloud_vm_name test-vm --parsing_path  --worker_job /job:localhost --model transformer --keep_checkpoint_every_n_hours 10000 --t2t_usr_dir t2t_usr_dir_internal --job-dir gs://test_t2t/poetry/model master-replica-0 
I  master-replica-0 
Registry contents:
------------------

  Models:
    aligned:
      * aligned
    attention:
      * attention_lm
      * attention_lm_moe
    autoencoder:
      * autoencoder_autoregressive
      * autoencoder_basic_discrete
      * autoencoder_ordered_discrete
      * autoencoder_residual
      * autoencoder_residual_discrete
      * autoencoder_stacked
    basic:
      * basic_autoencoder
      * basic_conv_gen
      * basic_fc_relu
    byte:
      * byte_net
    cycle:
      * cycle_gan
    diagonal:
      * diagonal_neural_gpu
    distillation:
      * distillation
    gene:
      * gene_expression_conv
    imagetransformer:
      * imagetransformer
    imagetransformer2d:
      * imagetransformer2d
    imagetransformer:
      * imagetransformer_moe
    img2img:
      * img2img_transformer
    lstm:
      * lstm_encoder
      * lstm_seq2seq
      * lstm_seq2seq_attention
      * lstm_seq2seq_attention_bidirectional_encoder
      * lstm_seq2seq_bidirectional_encoder
    multi:
      * multi_model
    neural:
      * neural_gpu
    r:
      * r_transformer
      * r_transformer_encoder
    resnet:
      * resnet
    revnet:
      * revnet
    shake:
      * shake_shake
    slice:
      * slice_net
    super:
      * super_lm
    transformer:
      * transformer
      * transformer_ae
      * transformer_encoder
      * transformer_moe
      * transformer_revnet
      * transformer_scorer
      * transformer_sketch
      * transformer_symshard
    vanilla:
      * vanilla_gan
    xception:
      * xception

  HParams:
    afx:
      * afx_adafactor
      * afx_adam
      * afx_base
      * afx_clip
      * afx_clip2
      * afx_clip_factored
      * afx_factored
      * afx_fast
      * afx_mimic_adam
      * afx_pow05
      * afx_pow08
      * afx_pow08_clip
      * afx_pow10
      * afx_relative
      * afx_small
      * afx_small_bfloat16
      * afx_small_p10
      * afx_small_p11
      * afx_small_p12
      * afx_small_p16
      * afx_small_p8
      * afx_unscale
      * afx_unscale_relative
    aligned:
      * aligned_8k
      * aligned_8k_grouped
      * aligned_base
      * aligned_grouped
      * aligned_local
      * aligned_local_1k
      * aligned_local_expert
      * aligned_lsh
      * aligned_memory_efficient
      * aligned_moe
      * aligned_no_att
      * aligned_no_timing
      * aligned_pos_emb
      * aligned_pseudolocal
      * aligned_pseudolocal_256
    atari:
      * atari_base
    attention:
      * attention_lm_11k
      * attention_lm_12k
      * attention_lm_16k
      * attention_lm_ae_extended
      * attention_lm_attention_moe_tiny
      * attention_lm_base
      * attention_lm_hybrid_v2
      * attention_lm_moe_24b_diet
      * attention_lm_moe_32b_diet
      * attention_lm_moe_base
      * attention_lm_moe_base_ae
      * attention_lm_moe_base_hybrid
      * attention_lm_moe_base_local
      * attention_lm_moe_base_long_seq
      * attention_lm_moe_base_memeff
      * attention_lm_moe_large
      * attention_lm_moe_large_diet
      * attention_lm_moe_memory_efficient
      * attention_lm_moe_small
      * attention_lm_moe_tiny
      * attention_lm_moe_translation
      * attention_lm_moe_unscramble_base
      * attention_lm_no_moe_small
      * attention_lm_small
      * attention_lm_translation
      * attention_lm_translation_full_attention
      * attention_lm_translation_l12
    autoencoder:
      * autoencoder_autoregressive
      * autoencoder_basic_discrete
      * autoencoder_discrete_pong
      * autoencoder_ordered_discrete
      * autoencoder_residual
      * autoencoder_residual_discrete
      * autoencoder_residual_discrete_big
      * autoencoder_stacked
    basic:
      * basic_1
      * basic_autoencoder
      * basic_conv
      * basic_conv_l1
      * basic_conv_l2
      * basic_conv_small
      * basic_fc_small
    bytenet:
      * bytenet_base
    continuous:
      * continuous_action_base
    cycle:
      * cycle_gan_small
    discrete:
      * discrete_action_base
    distill:
      * distill_resnet_32_to_15_cifar20x5
    gene:
      * gene_expression_conv_base
    image:
      * image_transformer2d_base
      * image_transformer_base
    imagetransformer1d:
      * imagetransformer1d_base_12l_64by64
      * imagetransformer1d_base_8l_64by64
    imagetransformer2d:
      * imagetransformer2d_base
      * imagetransformer2d_base_12l_8_16_big
      * imagetransformer2d_base_12l_8_64_64by64
      * imagetransformer2d_base_14l_8_16_big
      * imagetransformer2d_base_14l_8_16_big_uncond
      * imagetransformer2d_base_8l_8_16
      * imagetransformer2d_base_8l_8_16_big
      * imagetransformer2d_base_8l_8_16_big_16k
      * imagetransformer2d_base_8l_8_16_ls
      * imagetransformer2d_base_8l_8_32_big
      * imagetransformer2d_base_8l_8_64_64by64
      * imagetransformer2d_tiny
    imagetransformer:
      * imagetransformer_ae_cifar
      * imagetransformer_b10l_4h_big_uncond_dr01_tpu
      * imagetransformer_b10l_4h_big_uncond_dr03_lr025_tpu
      * imagetransformer_b10l_4h_big_uncond_dr03_lr05_tpu
      * imagetransformer_b10l_4h_big_uncond_dr03_tpu
      * imagetransformer_b10l_dr03_moe_tpu
      * imagetransformer_b12l_4h_b128_h512_uncond_dr03_im
      * imagetransformer_b12l_4h_b128_h512_uncond_dr03_tpu
      * imagetransformer_b12l_4h_b128_uncond_dr03_tpu
      * imagetransformer_b12l_4h_b256_uncond_dr03_lr025_tpu
      * imagetransformer_b12l_4h_b256_uncond_dr03_tpu
      * imagetransformer_b12l_4h_big_uncond_dr03_lr025_tpu
      * imagetransformer_b12l_4h_big_uncond_dr03_tpu
      * imagetransformer_b12l_4h_small_uncond_dr03_tpu
      * imagetransformer_b12l_8h_b256_uncond_dr03_tpu
      * imagetransformer_bas8l_8h_big_uncond_dr03_imgnet
      * imagetransformer_base
      * imagetransformer_base_10l_16h_big_dr01_imgnet
      * imagetransformer_base_10l_16h_big_dr01_moe_imgnet
      * imagetransformer_base_10l_16h_big_uncond_dr01_imgnet
      * imagetransformer_base_10l_8h_big_cond_dr03_dan
      * imagetransformer_base_10l_8h_big_uncond_dr03_dan
      * imagetransformer_base_10l_8h_big_uncond_dr03_dan_64
      * imagetransformer_base_10l_8h_big_uncond_dr03_dan_64_2d
      * imagetransformer_base_12l_8h_big
      * imagetransformer_base_12l_8h_big_uncond
      * imagetransformer_base_14l_8h_big
      * imagetransformer_base_14l_8h_big_dr01
      * imagetransformer_base_14l_8h_big_uncond
      * imagetransformer_base_14l_8h_big_uncond_dr01
      * imagetransformer_base_8l_8h_big_cond_dr03_dan
      * imagetransformer_base_8l_8h_big_cond_dr03_dan_128
      * imagetransformer_base_8l_8h_big_cond_dr03_dan_dilated
      * imagetransformer_base_8l_8h_big_cond_dr03_dan_dilated_b
      * imagetransformer_base_8l_8h_big_cond_dr03_dan_dilated_c
      * imagetransformer_base_8l_8h_big_cond_dr03_dan_dilated_d
      * imagetransformer_base_tpu
      * imagetransformer_moe_tiny
      * imagetransformer_sep_channels
      * imagetransformer_sep_channels_10l_8h
      * imagetransformer_sep_channels_12l_16h_imagenet_large
      * imagetransformer_sep_channels_12l_8h
      * imagetransformer_sep_channels_12l_8h_4k
      * imagetransformer_sep_channels_12l_8h_nda
      * imagetransformer_sep_channels_12l_8h_sep_rgb
      * imagetransformer_sep_channels_16l_16h_imgnet_lrg_loc
      * imagetransformer_sep_channels_16l_16h_imgnet_lrg_loc_128
      * imagetransformer_sep_channels_8l
      * imagetransformer_sep_channels_8l_8h
      * imagetransformer_sep_channels_8l_8h_local_and_global_att
      * imagetransformer_sep_channels_8l_glu_ffn
      * imagetransformer_sep_channels_8l_multipos3
      * imagetransformer_sep_channels_8l_self_att_ffn
      * imagetransformer_sep_channels_8l_tpu
      * imagetransformer_sep_output_channels_8l
      * imagetransformer_sep_output_channels_8l_local_and_global_att
      * imagetransformer_tiny
      * imagetransformer_tiny_tpu
    img2img:
      * img2img_transformer2d_base
      * img2img_transformer2d_n103
      * img2img_transformer2d_n24
      * img2img_transformer2d_n3
      * img2img_transformer2d_n31
      * img2img_transformer2d_n44
      * img2img_transformer2d_q1
      * img2img_transformer2d_q2
      * img2img_transformer2d_q3
      * img2img_transformer2d_tiny
      * img2img_transformer_b1
      * img2img_transformer_b2
      * img2img_transformer_b3
      * img2img_transformer_base
      * img2img_transformer_base_tpu
      * img2img_transformer_dilated
      * img2img_transformer_tiny
      * img2img_transformer_tiny_tpu
    lmx:
      * lmx_base
      * lmx_h1k_f4k
      * lmx_h1k_f64k
      * lmx_h2k_f8k
      * lmx_h3k_f12k
      * lmx_h4k_f16k
      * lmx_moe
      * lmx_moe_h1k_f4k_x32
      * lmx_moe_h1k_f8k_x16
      * lmx_relative
      * lmx_relative_nopos
    lstm:
      * lstm_asr_v1
      * lstm_attention
      * lstm_bahdanau_attention
      * lstm_bahdanau_attention_multi
      * lstm_luong_attention
      * lstm_luong_attention_multi
      * lstm_seq2seq
    multimodel:
      * multimodel_base
      * multimodel_tiny
    neural:
      * neural_gpu
    ppo:
      * ppo_base_v1
    r:
      * r_mix_transformer_act_step_position_random_timing_base
      * r_mix_transformer_act_step_position_timing_base
      * r_mix_transformer_base
      * r_transformer_act_accumulated_base
      * r_transformer_act_accumulated_tiny
      * r_transformer_act_base
      * r_transformer_act_base_d03
      * r_transformer_act_base_sb
      * r_transformer_act_big
      * r_transformer_act_big_d03
      * r_transformer_act_global_base
      * r_transformer_act_global_tiny
      * r_transformer_act_large
      * r_transformer_act_position_random_timing_tiny
      * r_transformer_act_position_step_timing_tiny
      * r_transformer_act_position_timing_tiny
      * r_transformer_act_random_base
      * r_transformer_act_random_tiny
      * r_transformer_act_step_position_random_timing_base
      * r_transformer_act_step_position_random_timing_tiny
      * r_transformer_act_step_position_timing_base
      * r_transformer_act_step_position_timing_tiny
      * r_transformer_act_step_sinusoid_position_random_timing_tiny
      * r_transformer_act_step_sinusoid_position_timing_tiny
      * r_transformer_act_step_timing_tiny
      * r_transformer_act_tall
      * r_transformer_act_tall_actlossw0
      * r_transformer_act_tall_actlossw001
      * r_transformer_act_tiny
      * r_transformer_act_tiny_d02
      * r_transformer_act_tiny_d02_sb
      * r_transformer_act_tiny_d05
      * r_transformer_act_tiny_sb
      * r_transformer_base
      * r_transformer_base_dropconnect
      * r_transformer_base_sb
      * r_transformer_big
      * r_transformer_dwa_base
      * r_transformer_dwa_tiny
      * r_transformer_dwa_tiny_test
      * r_transformer_gru_base
      * r_transformer_highway_base
      * r_transformer_highway_tiny
      * r_transformer_lstm_base
      * r_transformer_position_random_timing_base
      * r_transformer_position_random_timing_tiny
      * r_transformer_position_step_timing_tiny
      * r_transformer_position_timing_tiny
      * r_transformer_rnn_base
      * r_transformer_skip_base
      * r_transformer_skip_tiny
      * r_transformer_step_position_random_timing_tiny
      * r_transformer_step_position_timing_base
      * r_transformer_step_position_timing_tiny
      * r_transformer_step_sinusoid_timing_tiny
      * r_transformer_step_timing_tiny
      * r_transformer_teeny
      * r_transformer_tiny
    resnet:
      * resnet_101
      * resnet_152
      * resnet_18
      * resnet_200
      * resnet_34
      * resnet_50
      * resnet_cifar_15
      * resnet_cifar_32
      * resnet_imagenet_102
      * resnet_imagenet_34
    revnet:
      * revnet_104
      * revnet_110_cifar
      * revnet_164_cifar
      * revnet_38_cifar
    shake:
      * shake_shake_quick
    shakeshake:
      * shakeshake_big
      * shakeshake_small
      * shakeshake_tpu
    slicenet:
      * slicenet_1
      * slicenet_1noam
      * slicenet_1tiny
    super:
      * super_lm_b8k
      * super_lm_base
      * super_lm_big
      * super_lm_big_tpu
      * super_lm_conv
      * super_lm_high_mix
      * super_lm_low_mix
      * super_lm_moe
      * super_lm_moe_4b_diet
      * super_lm_moe_h4
      * super_lm_tpu
      * super_lm_tpu_memtest
    transformer:
      * transformer_ae_a3
      * transformer_ae_a6
      * transformer_ae_a8
      * transformer_ae_base
      * transformer_ae_base_tpu
      * transformer_ae_small
      * transformer_base
      * transformer_base_single_gpu
      * transformer_base_sketch
      * transformer_base_v1
      * transformer_base_v2
      * transformer_big
      * transformer_big_dr1
      * transformer_big_dr2
      * transformer_big_enfr
      * transformer_big_enfr_tpu
      * transformer_big_single_gpu
      * transformer_big_tpu
      * transformer_clean
      * transformer_clean_big
      * transformer_clean_big_tpu
      * transformer_dr0
      * transformer_dr2
      * transformer_ff1024
      * transformer_ff4096
      * transformer_h1
      * transformer_h16
      * transformer_h32
      * transformer_h4
      * transformer_hs1024
      * transformer_hs256
      * transformer_k128
      * transformer_k256
      * transformer_l10
      * transformer_l2
      * transformer_l4
      * transformer_l8
      * transformer_librispeech
      * transformer_librispeech_tpu
      * transformer_librispeech_tpu_v1
      * transformer_librispeech_tpu_v2
      * transformer_librispeech_v1
      * transformer_librispeech_v2
      * transformer_lm_tpu_0
      * transformer_lm_tpu_1
      * transformer_ls0
      * transformer_ls2
      * transformer_moe_12k
      * transformer_moe_2k
      * transformer_moe_8k
      * transformer_moe_8k_lm
      * transformer_moe_base
      * transformer_moe_prepend_8k
      * transformer_opt
      * transformer_packed_tpu
      * transformer_parameter_attention_a
      * transformer_parameter_attention_b
      * transformer_parsing_base
      * transformer_parsing_big
      * transformer_parsing_ice
      * transformer_poetry
      * transformer_prepend
      * transformer_prepend_v1
      * transformer_prepend_v2
      * transformer_relative
      * transformer_relative_big
      * transformer_relative_tiny
      * transformer_revnet_base
      * transformer_revnet_big
      * transformer_sketch
      * transformer_sketch_2layer
      * transformer_sketch_4layer
      * transformer_sketch_6layer
      * transformer_small
      * transformer_small_sketch
      * transformer_small_tpu
      * transformer_supervised_attention
      * transformer_symshard_base
      * transformer_symshard_h4
      * transformer_symshard_lm_0
      * transformer_symshard_sh4
      * transformer_teeny
      * transformer_test
      * transformer_tiny
      * transformer_tiny_tpu
      * transformer_tpu
      * transformer_tpu_1b
      * transformer_tpu_bf16_activation
      * transformer_tpu_with_conv
    vanilla:
      * vanilla_gan
    xception:
      * xception_base
      * xception_tiny
      * xception_tiny_tpu

  RangedHParams:
    basic1:
      * basic1
    revnet:
      * revnet_range
    slicenet1:
      * slicenet1
    transformer:
      * transformer_base_range
      * transformer_poetry_range
      * transformer_sketch_ranged
      * transformer_tiny_tpu_range
      * transformer_tpu_range

  Modalities:
    audio:audio:
      * audio:audio_spectral_modality
    audio:default:
      * audio:default
    audio:identity:
      * audio:identity
    audio:speech:
      * audio:speech_recognition_modality
    class:
      * class_label:default
      * class_label:identity
      * class_label:sigmoid
      * class_label:sigmoid_max_pooling
    generic:default:
      * generic:default
    image:channel:
      * image:channel_embeddings_bottom
    image:default:
      * image:default
    image:identity:
      * image:identity
    image:image:
      * image:image_channel_compress
    real:default:
      * real:default
    real:identity:
      * real:identity
    real:l2:
      * real:l2_loss
    real:log:
      * real:log_poisson_loss
    symbol:ctc:
      * symbol:ctc
    symbol:default:
      * symbol:default
    symbol:identity:
      * symbol:identity
    video:default:
      * video:default
    video:identity:
      * video:identity
    video:l1:
      * video:l1
    video:l2:
      * video:l2

  Problems:
    algorithmic:
      * algorithmic_addition_binary40
      * algorithmic_addition_decimal40
      * algorithmic_cipher_shift200
      * algorithmic_cipher_shift5
      * algorithmic_cipher_vigenere200
      * algorithmic_cipher_vigenere5
      * algorithmic_identity_binary40
      * algorithmic_identity_decimal40
      * algorithmic_multiplication_binary40
      * algorithmic_multiplication_decimal40
      * algorithmic_reverse_binary40
      * algorithmic_reverse_binary40_test
      * algorithmic_reverse_decimal40
      * algorithmic_reverse_nlplike32k
      * algorithmic_reverse_nlplike8k
      * algorithmic_shift_decimal40
    audio:
      * audio_timit_characters_tune
      * audio_timit_tokens8k_test
      * audio_timit_tokens8k_tune
    genomics:
      * genomics_expression_cage10
      * genomics_expression_gm12878
      * genomics_expression_l262k
    gym:
      * gym_discrete_problem_with_agent_on_freeway
      * gym_discrete_problem_with_agent_on_pong
      * gym_freeway_random50k
      * gym_freeway_random5k
      * gym_pong_random50k
      * gym_pong_random5k
      * gym_simulated_discrete_problem_with_agent
      * gym_simulated_discrete_problem_with_agent_on_freeway
      * gym_simulated_discrete_problem_with_agent_on_pong
    image:
      * image_celeba
      * image_celeba_multi_resolution
      * image_cifar10
      * image_cifar100
      * image_cifar100_plain
      * image_cifar100_plain8
      * image_cifar100_plain_gen
      * image_cifar100_tune
      * image_cifar10_plain
      * image_cifar10_plain8
      * image_cifar10_plain_gen
      * image_cifar10_tune
      * image_cifar20
      * image_cifar20_plain
      * image_cifar20_plain8
      * image_cifar20_plain_gen
      * image_cifar20_tune
      * image_fashion_mnist
      * image_fsns
      * image_imagenet
      * image_imagenet224
      * image_imagenet32
      * image_imagenet32_small
      * image_imagenet64
      * image_imagenet64_gen
      * image_imagenet_multi_resolution_gen
      * image_mnist
      * image_mnist_tune
      * image_ms_coco_characters
      * image_ms_coco_tokens32k
      * image_text_ms_coco
      * image_text_ms_coco_multi_resolution
    img2img:
      * img2img_celeba
      * img2img_celeba64
      * img2img_cifar10
      * img2img_cifar100
      * img2img_imagenet
    lambada:
      * lambada_lm
      * lambada_lm_control
      * lambada_rc
      * lambada_rc_control
    languagemodel:
      * languagemodel_lm1b32k
      * languagemodel_lm1b32k_packed
      * languagemodel_lm1b8k_packed
      * languagemodel_lm1b_characters
      * languagemodel_lm1b_characters_packed
      * languagemodel_ptb10k
      * languagemodel_ptb_characters
      * languagemodel_wiki_noref_v128k_l1k
      * languagemodel_wiki_noref_v32k_l1k
      * languagemodel_wiki_noref_v8k_l16k
      * languagemodel_wiki_noref_v8k_l1k
      * languagemodel_wiki_scramble_l128
      * languagemodel_wiki_scramble_l1k
      * languagemodel_wiki_xml_v8k_l1k
      * languagemodel_wiki_xml_v8k_l4k
      * languagemodel_wikitext103
      * languagemodel_wikitext103_characters
    librispeech:
      * librispeech
      * librispeech_clean
      * librispeech_clean_small
      * librispeech_noisy
      * librispeech_train_full_test_clean
    multinli:
      * multinli_matched
      * multinli_mismatched
    ocr:
      * ocr_test
    parsing:
      * parsing_english_ptb16k
      * parsing_english_ptb8k
      * parsing_icelandic16k
    poetry:
      * poetry_line_problem
    programming:
      * programming_desc2code_cpp
      * programming_desc2code_py
    sentiment:
      * sentiment_imdb
    squad:
      * squad
      * squad_concat
      * squad_concat_positioned
    summarize:
      * summarize_cnn_dailymail32k
    sva:
      * sva_language_modeling
      * sva_number_prediction
    text2text:
      * text2text_tmpdir
    translate:
      * translate_encs_wmt32k
      * translate_encs_wmt_characters
      * translate_ende_wmt32k
      * translate_ende_wmt32k_packed
      * translate_ende_wmt8k
      * translate_ende_wmt8k_packed
      * translate_ende_wmt_bpe32k
      * translate_ende_wmt_characters
      * translate_enet_wmt32k
      * translate_enet_wmt_characters
      * translate_enfr_wmt32k
      * translate_enfr_wmt32k_packed
      * translate_enfr_wmt8k
      * translate_enfr_wmt_characters
      * translate_enfr_wmt_small32k
      * translate_enfr_wmt_small8k
      * translate_enfr_wmt_small_characters
      * translate_enmk_setimes32k
      * translate_enmk_setimes_characters
      * translate_envi_iwslt32k
      * translate_enzh_wmt32k
      * translate_enzh_wmt8k
    video:
      * video_twentybn
    wikisum:
      * wikisum_commoncrawl
      * wikisum_commoncrawl_lead_section
      * wikisum_web
      * wikisum_web_lead_section
   master-replica-0 
I  master-replica-0 Module completed; cleaning up. master-replica-0 
I  master-replica-0 Clean up finished. master-replica-0 
I  master-replica-0 Task completed successfully. master-replica-0 
I  Job completed successfully. 
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='sakinaljana' date='2018-05-16T04:17:13Z'>
		Guys, please help &lt;denchmark-link:https://github.com/rsepassi&gt;@rsepassi&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/lukaszkaiser&gt;@lukaszkaiser&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='sakinaljana' date='2018-05-21T18:13:02Z'>
		Yes, this is indeed a bug. Will be fixed shortly.
		</comment>
		<comment id='3' author='sakinaljana' date='2018-05-21T18:13:08Z'>
		Thanks for reporting!
		</comment>
		<comment id='4' author='sakinaljana' date='2018-05-22T01:58:50Z'>
		Thanks &lt;denchmark-link:https://github.com/rsepassi&gt;@rsepassi&lt;/denchmark-link&gt;
 for the response. Is this known problem? If possible, could you share which part is bug? is there any workaround to solve this issue? Thanks
		</comment>
		<comment id='5' author='sakinaljana' date='2018-05-22T03:04:16Z'>
		The latest release should now work.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Mon, May 21, 2018 at 6:58 PM dyngts ***@***.***&gt; wrote:
 Thanks @rsepassi &lt;https://github.com/rsepassi&gt; for the response. Is this
 known problem? If possible, could you share which part is bug? is there any
 workaround to solve this issue? Thanks

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#792 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABEGW2yZWFRlf_UzafR-GUBmzT7UeSxgks5t03DdgaJpZM4T61Fe&gt;
 .



		</comment>
	</comments>
</bug>