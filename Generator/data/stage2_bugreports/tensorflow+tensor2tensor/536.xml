<bug id='536' author='wachaong' open_date='2018-01-23T02:10:30Z' closed_time='2018-02-09T01:22:45Z'>
	<summary>decode using attention-lm</summary>
	<description>
my script
1 #!/bin/sh
2
3 PROBLEM=languagemodel_ptb10k
4 #MODEL=transformer
5 #HPARAMS=transformer_base
6 MODEL=attention_lm
7 HPARAMS=attention_lm_small
8
9 DATA_DIR=t2t_data
10 TMP_DIR=t2t_datagen_tmp
11 TRAIN_DIR=t2t_train/$PROBLEM/$MODEL-$HPARAMS
12
13 mkdir -p $DATA_DIR $TMP_DIR $TRAIN_DIR
14
15 ## Generate data
16 #t2t-datagen
17 # --data_dir=$DATA_DIR
18 # --tmp_dir=$TMP_DIR
19 # --problem=$PROBLEM
20
21 t2t-trainer
22 --data_dir=$DATA_DIR
23 --problems=$PROBLEM
24 --model=$MODEL
25 --hparams_set=$HPARAMS
26 --output_dir=$TRAIN_DIR
27 --worker_gpu=8
28 --train_steps=1000
29 --eval_steps=10
30
31 t2t-decoder
32 --data_dir=$DATA_DIR
33 --problems=$PROBLEM
34 --model=$MODEL
35 --hparams_set=$HPARAMS
36 --output_dir=$TRAIN_DIR
37 --decode_from_file=$TMP_DIR/simple-examples/data/small_test
my decode result seems empty:
INFO:tensorflow:Inference results INPUT: it was the first time in seven years that moscow has n't joined efforts led by
INFO:tensorflow:Inference results OUTPUT:
INFO:tensorflow:Inference results INPUT: for a while
INFO:tensorflow:Inference results OUTPUT:
INFO:tensorflow:Inference results INPUT: we 've been spending a lot of time in los angeles talking to tv production people says mike parks president of call interactive which supplied technology for both abc sports and nbc 's consumer minutes
INFO:tensorflow:Inference results OUTPUT:
INFO:tensorflow:Inference results INPUT: from the fee the local phone company and the long-distance carrier extract their costs to carry the call passing the rest of the money to the
INFO:tensorflow:Inference results OUTPUT:
	</description>
	<comments>
		<comment id='1' author='wachaong' date='2018-02-09T01:22:45Z'>
		Closing in favor of &lt;denchmark-link:https://github.com/tensorflow/tensor2tensor/issues/439&gt;#439&lt;/denchmark-link&gt;
 which reports the same issue.
		</comment>
	</comments>
</bug>