<bug id='401' author='smcdufff' open_date='2017-11-04T12:30:47Z' closed_time='2017-12-22T17:47:26Z'>
	<summary>Crashing -- Memory leak ?</summary>
	<description>
I upgraded to 1.2.6 (tried also 1.2.7) from 1.2.2 and my training is always crashing after 4K steps (system A with 3 GPU)  or 60K (system B with 1 GPU).
If I go back to 1.2.2, it works again on both system (500 millions steps).
Here my configuration
--model=transformer
--hparams_set=transformer_n_da
Here the stacktrace:
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Ran out of GPU memory when allocating 329294592 bytes for
[[Node: symbol_modality_21472_512_2/parallel_0_1/symbol_modality_21472_512/padded_cross_entropy/smoothing_cross_entropy/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/gpu:0"](symbol_modality_21472_512_2/parallel_0_1/symbol_modality_21472_512/padded_cross_entropy/smoothing_cross_entropy/Reshape, symbol_modality_21472_512_2/parallel_0_1/symbol_modality_21472_512/padded_cross_entropy/smoothing_cross_entropy/Reshape_1)]]
[[Node: total_loss/_5453 = _Recv&lt;denchmark-link:&gt;client_terminated=false, recv_device="/job:localhost/replica:0/task:0/cpu:0", send_device="/job:localhost/replica:0/task:0/gpu:0", send_device_incarnation=1, tensor_name="edge_23728_total_loss", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/cpu:0"&lt;/denchmark-link&gt;
]]
Caused by op 'symbol_modality_21472_512_2/parallel_0_1/symbol_modality_21472_512/padded_cross_entropy/smoothing_cross_entropy/SoftmaxCrossEntropyWithLogits', defined at:
File "C:\Users\smcdu\git\tensor2tensor\tensor2tensor\bin\t2t-trainer", line 96, in 
tf.app.run()
File "C:\Users\smcdu\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\platform\app.py", line 48, in run
_sys.exit(main(_sys.argv[:1] + flags_passthrough))
File "C:\Users\smcdu\git\tensor2tensor\tensor2tensor\bin\t2t-trainer", line 92, in main
schedule=FLAGS.schedule)
File "C:\Users\smcdu\git\tensor2tensor\tensor2tensor\utils\trainer_utils.py", line 378, in run
hparams=hparams)
File "C:\Users\smcdu\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\contrib\learn\python\learn\learn_runner.py", line 209, in run
return _execute_schedule(experiment, schedule)
File "C:\Users\smcdu\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\contrib\learn\python\learn\learn_runner.py", line 46, in _execute_schedule
return task()
File "C:\Users\smcdu\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\contrib\learn\python\learn\experiment.py", line 502, in train_and_evaluate
self.train(delay_secs=0)
File "C:\Users\smcdu\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\contrib\learn\python\learn\experiment.py", line 280, in train
hooks=self._train_monitors + extra_hooks)
File "C:\Users\smcdu\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\contrib\learn\python\learn\experiment.py", line 672, in _call_train
hooks=hooks)
File "C:\Users\smcdu\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\estimator\estimator.py", line 241, in train
loss = self._train_model(input_fn=input_fn, hooks=hooks)
File "C:\Users\smcdu\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\estimator\estimator.py", line 630, in _train_model
model_fn_lib.ModeKeys.TRAIN)
File "C:\Users\smcdu\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\estimator\estimator.py", line 615, in _call_model_fn
model_fn_results = self._model_fn(features=features, **kwargs)
File "C:\Users\smcdu\git\tensor2tensor\tensor2tensor\utils\model_builder.py", line 349, in wrapping_model_fn
return model_fn(model, features, mode, hparams, **kwargs)
File "C:\Users\smcdu\git\tensor2tensor\tensor2tensor\utils\model_builder.py", line 164, in model_fn
max_idx=len(hparams.problems) - 1)
File "C:\Users\smcdu\git\tensor2tensor\tensor2tensor\utils\input_fn_builder.py", line 182, in cond_on_index
return fn(cur_idx)
File "C:\Users\smcdu\git\tensor2tensor\tensor2tensor\utils\model_builder.py", line 132, in nth_model
features, skip=(skipping_is_on and skip_this_one))
File "C:\Users\smcdu\git\tensor2tensor\tensor2tensor\utils\t2t_model.py", line 598, in model_fn
sharded_logits, sharded_features["targets"], dp)
File "C:\Users\smcdu\git\tensor2tensor\tensor2tensor\utils\modality.py", line 161, in loss_sharded
self.loss, sharded_top_out, sharded_targets)
File "C:\Users\smcdu\git\tensor2tensor\tensor2tensor\utils\expert_utils.py", line 222, in call
outputs.append(fns[i](*my_args[i], **my_kwargs[i]))
File "C:\Users\smcdu\git\tensor2tensor\tensor2tensor\utils\modality.py", line 156, in loss
weights_fn=weights_fn)
File "C:\Users\smcdu\git\tensor2tensor\tensor2tensor\layers\common_layers.py", line 1473, in padded_cross_entropy
confidence)
File "C:\Users\smcdu\git\tensor2tensor\tensor2tensor\layers\common_layers.py", line 1521, in smoothing_cross_entropy
logits=logits, labels=soft_targets)
File "C:\Users\smcdu\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\nn_ops.py", line 1597, in softmax_cross_entropy_with_logits
precise_logits, labels, name=name)
File "C:\Users\smcdu\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py", line 2385, in _softmax_cross_entropy_with_logits
features=features, labels=labels, name=name)
File "C:\Users\smcdu\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\op_def_library.py", line 767, in apply_op
op_def=op_def)
File "C:\Users\smcdu\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\ops.py", line 2630, in create_op
original_op=self._default_original_op, op_def=op_def)
File "C:\Users\smcdu\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\ops.py", line 1204, in init
self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access
ResourceExhaustedError (see above for traceback): Ran out of GPU memory when allocating 329294592 bytes for
[[Node: symbol_modality_21472_512_2/parallel_0_1/symbol_modality_21472_512/padded_cross_entropy/smoothing_cross_entropy/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/gpu:0"](symbol_modality_21472_512_2/parallel_0_1/symbol_modality_21472_512/padded_cross_entropy/smoothing_cross_entropy/Reshape, symbol_modality_21472_512_2/parallel_0_1/symbol_modality_21472_512/padded_cross_entropy/smoothing_cross_entropy/Reshape_1)]]
[[Node: total_loss/_5453 = _Recv&lt;denchmark-link:&gt;client_terminated=false, recv_device="/job:localhost/replica:0/task:0/cpu:0", send_device="/job:localhost/replica:0/task:0/gpu:0", send_device_incarnation=1, tensor_name="edge_23728_total_loss", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/cpu:0"&lt;/denchmark-link&gt;
]]
2017-11-04 07:22:21.186625: E C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\stream_executor\event.cc:33] error destroying CUDA event in context 000001E6F21B9320: CUDA_ERROR_ILLEGAL_ADDRESS
	</description>
	<comments>
		<comment id='1' author='smcdufff' date='2017-11-04T13:54:04Z'>
		I confirm a similar experience. The memory requirements of the same model are gradually increasing with newer T2T versions (I remember one increase with the adoption of TF Dataset records and another now between 1.2.3 and 1.2.6).
Lowering the batch size helps. What is important is the BLEU vs. training time curve - I plan to finish this experiment (comparing different T2T versions). I know the default hyperparams has been changed (and maybe the training is faster), so I hope the curve of newer T2T versions will be better in the end.
		</comment>
		<comment id='2' author='smcdufff' date='2017-11-04T14:16:21Z'>
		I'm glad I was not the only one.
Thank you for your answer.
I'm trying upgrading to TF 1.4.0. If doesn't work I will lower my batch size. I will report here when I will have some results.
Although, I find it strange that it failed after 60K steps with batch size 4096 on a GPU with 8 gig RAM.
Let me know where you will publish your experiment.
		</comment>
		<comment id='3' author='smcdufff' date='2017-11-04T17:39:16Z'>
		I tried T2T 1.2.6 with TF 1.4.0 and it failed with batch size 1500 on a GPU with 11 GB (transformer_big_single_gpu).
In older T2T versions I was able to use batch size 2048 with the same configuration.
		</comment>
		<comment id='4' author='smcdufff' date='2017-11-04T17:54:50Z'>
		Thank you!
OK I will stop my test with TF4 and try to see what causes the problem.
batch size of 1500 seems very reasonable with GPU with 11 GB.
After how many steps it crashes ?
		</comment>
		<comment id='5' author='smcdufff' date='2017-11-04T19:30:36Z'>
		With --hparams=batch_size=2048, it crashes usually after 1600 steps (sometimes 500, sometimes 2500 - the behavior is non-deterministic).
		</comment>
		<comment id='6' author='smcdufff' date='2017-11-04T20:13:56Z'>
		Also, I don't know if you noticed but with new default  parameters (ie lr=0.2 n da) if you do a single gpu run with a batch size of 2048 it's not good at all, you really need 4096
(not sure if for big you noticed the same)
		</comment>
		<comment id='7' author='smcdufff' date='2017-11-04T22:00:09Z'>
		I don't think the problem is related to this...
Do you know why batch size = 2048 is not good for 1 GPU ?
What are good values for 2, 3 and 16 GPU ?
		</comment>
		<comment id='8' author='smcdufff' date='2017-11-04T22:28:02Z'>
		I am not saying it is related, just saying that if you go OOM, then you may have to decrease your batch_size but then you may get worse results.
I don't know why, I just observed it.
		</comment>
		<comment id='9' author='smcdufff' date='2017-11-04T22:52:06Z'>
		&lt;denchmark-link:https://github.com/vince62s&gt;@vince62s&lt;/denchmark-link&gt;
  thank you for sharing!
This OOM is very strange. Pretty sure it is a bug somewhere between 1.23 and 1.26. I will try every version to determine which one introduce the problem.
		</comment>
		<comment id='10' author='smcdufff' date='2017-11-13T10:54:07Z'>
		I can also confirm the same error even on DGX-1 even with v1.28 which is insane.
		</comment>
		<comment id='11' author='smcdufff' date='2017-11-13T13:43:30Z'>
		I agree with you. Still looking for a fix... currently my knowledge in t2t is very small... it could take a while.
I'm surprised that this wasn't fix yet since it make t2t unusable for translation.
		</comment>
		<comment id='12' author='smcdufff' date='2017-12-14T19:53:09Z'>
		Maybe related to this... &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/14107&gt;tensorflow/tensorflow#14107&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='smcdufff' date='2017-12-22T17:28:09Z'>
		Were you running training and eval in the same job? It may have been because of the experiment schedule being set to train_and_evaluate which uses more memory. We now use continuous_train_and_eval which shouldn't blow out memory. Have you been able to repro with v1.4+?
		</comment>
		<comment id='14' author='smcdufff' date='2017-12-22T17:37:41Z'>
		I was using only train.
The OUT OF MEMORY is gone from 1.3.2. But it is slow by around 50% to train. Didn't try to decode.
Now the slowness could be because of  &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/14107&gt;tensorflow/tensorflow#14107&lt;/denchmark-link&gt;
 ?
Do you want me to close this one and open a new about being slow ?
		</comment>
		<comment id='15' author='smcdufff' date='2017-12-22T17:47:26Z'>
		Ah, ok, that's good. Yes, if you'd like you can open a new one about speed. Thanks!
		</comment>
	</comments>
</bug>