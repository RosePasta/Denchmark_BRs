<bug id='809' author='martinpopel' open_date='2018-05-16T17:06:32Z' closed_time='2018-06-08T03:08:16Z'>
	<summary>*bug* checkpoints are incompatible wrt training/learning_rate</summary>
	<description>
When training with learning_rate_schedule=constant, the checkpoints contain a key called training/learning_rate.
When training with learning_rate_schedule=rsqrt_decay, the checkpoints do not contain the key.
Thus one cannot continue training with a constant LR with a model pre-trained with rsqrt_decay, we get the Key training/learning_rate not found in checkpoint error.
Should the key be there and why?
tensor2tensor==1.6.0
tensorflow==1.6.0
	</description>
	<comments>
		<comment id='1' author='martinpopel' date='2018-05-29T05:13:01Z'>
		&lt;denchmark-link:https://github.com/martinpopel&gt;@martinpopel&lt;/denchmark-link&gt;
 I sent a &lt;denchmark-link:https://github.com/tensorflow/tensor2tensor/pull/842&gt;PR&lt;/denchmark-link&gt;
 to fix this issue, wish it can help you. Thanks.
		</comment>
		<comment id='2' author='martinpopel' date='2018-06-08T03:08:16Z'>
		After merging &lt;denchmark-link:https://github.com/tensorflow/tensor2tensor/pull/842&gt;#842&lt;/denchmark-link&gt;
 this should be ok, right? Closing, but feel free to reopen if needed of course!
		</comment>
	</comments>
</bug>