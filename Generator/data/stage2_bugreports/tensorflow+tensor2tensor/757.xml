<bug id='757' author='f-lng' open_date='2018-04-30T09:24:06Z' closed_time='2018-05-24T20:54:24Z'>
	<summary>Tracking bug for issues in Wikisum</summary>
	<description>
&lt;denchmark-h:h3&gt;Description&lt;/denchmark-h&gt;

I can currently not sign the Contributor License Agreement, so I will not do a pull request, sorry about this.
If you are unable to process any of these patches without a pull-request, I will make one at the end of the week. But for the sake of getting these notes out as soon as possible, for now, this is all I can offer.
&lt;denchmark-h:h2&gt;Notes on shell commands&lt;/denchmark-h&gt;

(&lt;denchmark-link:https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/wikisum#commands-to-generate-wikisumweb&gt;https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/wikisum#commands-to-generate-wikisumweb&lt;/denchmark-link&gt;
)


Command "python -m tensor2tensor.data_generators.wikisum.parallel_launch" needs to go without '.py'


--command_prefix can't find the scripts, needs to do a cd first (will break --code_dir feature, but this seems to have no effect on the other commands anyway, as they directly call the python module)
--command_prefix="cd ~/.local/lib/python3.5/site-packages/tensor2tensor/data_generators/ ; python3 -m tensor2tensor.data_generators.wikisum.get_references_web --out_dir={{BUCKET}}/wiki_references --shard_id"


If "Cloud Storage JSON API" is not enabled yet in the gcloud account, it will fail silently on the cloud worker. Should be checked in parallel_launch.py first


Vocab-generation on Python3 throws an re exception



   File "/usr/local/lib/python3.5/dist-packages/tensor2tensor/data_generators/wikisum/wikisum.py", line 361, in _normalize_text
    text = re.sub("[%s]" % re.escape(string.punctuation), r" \g&lt;0&gt; ", text)
  File "/usr/lib/python3.5/re.py", line 182, in sub
    return _compile(pattern, flags).sub(repl, string, count)
TypeError: cannot use a string pattern on a bytes-like object



Vocab-generation does not work on windows, as it can not access gs:// files, so it should be done in the cloud as well (explitictly using python2 to work around the re exception mentioned above)

&lt;denchmark-code&gt;python2 -m tensor2tensor.data_generators.wikisum.parallel_launch \
  --num_instances=1 \
  --cpu=4 --mem=16 \
  --name=wikisum-vocab-gen \  
  --setup_command="pip install tensor2tensor tensorflow -U -q --user" \
  --command_prefix="python2 -m tensor2tensor.data_generators.wikisum.generate_vocab  --out_dir=$BUCKET/data  --refs_dir=$BUCKET/wiki_references ; echo Done shard
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Changes for parallel_launch.py&lt;/denchmark-h&gt;

Here is the full parallel_launch.py file &lt;denchmark-link:https://pastebin.com/UUDjA9jL&gt;https://pastebin.com/UUDjA9jL&lt;/denchmark-link&gt;

Below are the changes I made:
&lt;denchmark-link:https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/wikisum/parallel_launch.py#L46&gt;https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/wikisum/parallel_launch.py#L46&lt;/denchmark-link&gt;
 - Cleaning output
&lt;denchmark-code&gt;#FIX prevent a ton of future warnings from h5py
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/wikisum/parallel_launch.py#L88&gt;https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/wikisum/parallel_launch.py#L88&lt;/denchmark-link&gt;
 - Some changes to the shell commands
&lt;denchmark-code&gt;#FIX added username 'wsumuser', otherwise got an error as my windows-username is root
COPY_CODE = "gcloud compute scp --recurse {local_dir} wsumuser@{instance_name}:~/"
SSH = "gcloud compute ssh wsumuser@{instance_name} --command"
DEFAULT_ZONE = "gcloud config get-value compute/zone"
#FIX use screens logging functionality to get rid of escaping problems for window's popen and to get more log coverage
SCREEN = "screen -L ~/logs-XXX.txt -dmS test bash -c \"{command}\""
#FIX no need for piping anymore, using screen -L
LOGS = "; gsutil cp ~/logs-XXX.txt {bucket}logs-{task_id}.txt"
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/wikisum/parallel_launch.py#L105&gt;https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/wikisum/parallel_launch.py#L105&lt;/denchmark-link&gt;
 - Find gcloud on windows
&lt;denchmark-code&gt;     #FIX try to find gcloud on windows
      try:
        return sp.check_call(args)
      except FileNotFoundError:
        if args[0] == "gcloud": args[0] = os.getenv('LOCALAPPDATA') + "/Google/Cloud SDK/google-cloud-sdk/bin/gcloud.cmd"
        return sp.check_call(args)
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/wikisum/parallel_launch.py#L115&gt;https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/wikisum/parallel_launch.py#L115&lt;/denchmark-link&gt;
 - Use python instead of netcat to wait for SSH
&lt;denchmark-code&gt;def wait_for_ssh(ip):
  """Wait for SSH to be available at given IP address."""
  # FIX don't use netcat, but python to test for open SSH port
  import socket
  for i in range(12):
    s = socket.socket()
    s.settimeout(2)

    try:
      s.connect((ip, 22))
      s.close()
      return True
    except:
      s.close()
    time.sleep(10)
  return False
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/wikisum/parallel_launch.py#L209&gt;https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/wikisum/parallel_launch.py#L209&lt;/denchmark-link&gt;
 - Python3 compatibility

&lt;denchmark-link:https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/wikisum/parallel_launch.py#L216&gt;https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/wikisum/parallel_launch.py#L216&lt;/denchmark-link&gt;
 - Log-Dir handling on windows
&lt;denchmark-code&gt;    #FIX window's \ leads to an error on the cloud-server afterwards
    log_dir = log_dir.replace("\\","/")

    #FIX on windows there is no gs:// , so give the user the opportunity to create the directories
    from tensorflow.python.framework.errors_impl import UnimplementedError
    try:
      tf.gfile.MakeDirs(log_dir)
    except UnimplementedError:
      input("[!] Use http://console.cloud.google.com and manually create '" + log_dir + "', then press return.................")
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/wikisum/parallel_launch.py#L251&gt;https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/wikisum/parallel_launch.py#L251&lt;/denchmark-link&gt;
 - Don't fail silently (the zip-exception, for example, was silenced)
&lt;denchmark-code&gt;
    # FIX Don't fail silently
    except Exception as e:  # pylint: disable=bare-except
      failed.append(i)
      tf.logging.error("Failed to launch task %d due to exception %s", i, str(e))
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Changes for cloud_tpu.py&lt;/denchmark-h&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/utils/cloud_tpu.py#L229&gt;https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/utils/cloud_tpu.py#L229&lt;/denchmark-link&gt;
 - Find gcloud on windows
&lt;denchmark-code&gt;def shell_output(cmd_, **kwargs):
  try:
    return text_encoder.to_unicode(sp.check_output(format_cmd(cmd_, **kwargs)))
  except FileNotFoundError:
    return text_encoder.to_unicode(sp.check_output(format_cmd_win(cmd_, **kwargs)))

def shell_run(cmd_, **kwargs):
  try:
    return sp.check_call(format_cmd(cmd_, **kwargs))
  except FileNotFoundError:
    return sp.check_call(format_cmd_win(cmd_, **kwargs))

def format_cmd_win(cmd_, **kwargs):
  ret = cmd_.format(**kwargs).strip().split()
  if ret[0] == "gcloud": ret[0] = os.getenv('LOCALAPPDATA') + "/Google/Cloud SDK/google-cloud-sdk/bin/gcloud.cmd"
  elif ret[0] == "gsutil": ret[0] = os.getenv('LOCALAPPDATA') + "/Google/Cloud SDK/google-cloud-sdk/bin/gsutil.cmd"
  return ret
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;TensorFlow and tensor2tensor versions&lt;/denchmark-h&gt;

tensor2tensor 1.6.1
tensorflow-gpu 1.7.0
	</description>
	<comments>
		<comment id='1' author='f-lng' date='2018-05-02T02:57:14Z'>
		Thanks very much &lt;denchmark-link:https://github.com/f-lng&gt;@f-lng&lt;/denchmark-link&gt;
!
I'll address some of the bugs, though I'm not keen on adding Windows support currently. I'll have to think more about it since it adds a bit of complexity.
Were you able to generate the dataset in the end? Web or Commoncrawl or both? Were the time and cost estimates reasonably accurate?
		</comment>
		<comment id='2' author='f-lng' date='2018-05-02T07:38:47Z'>
		&lt;denchmark-link:https://github.com/rsepassi&gt;@rsepassi&lt;/denchmark-link&gt;
 Yes, I figured that windows is not very high on the list, and it's no problem for me, but perhaps someone will find the notes usefull anyway. However, as most stuff happens in the cloud, it is not that much of a problem to adjust it accordingly. This is no "run and forget" generator anyway.
I am getting the wikisumWeb dataset.
I did not create it yet, I had to wait until yesterday to get my quota increase, so right now I am only as far as downloading all the references.
However, I did test the other parts on a small subset already, and I am positive it will work out in the end.
Tough, my version of wikisum is now a bit different from yours (due to changes because of windows, but also due to changes in how I want my dataset to be generated [using a bigram overlap per sentence, instead of tfidf per paragraph, for example]).
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Additional notes:
&lt;denchmark-h:h2&gt;Reliability of SSH commands&lt;/denchmark-h&gt;

As soon as I try to spawn all 1k instances at once, even tough I reduced the threads to 10, I do get immediate and frequent SSH timeouts (they are retried, and mostly work out, but still).
What works better is to have a loop that calls parallel_launch.py with chunks of 10 using --instance_ids
&lt;denchmark-h:h2&gt;Checks&lt;/denchmark-h&gt;

Also somehow related, I think there is the need for some additional checks after the download has been completed, in order to check if every shard has been downloaded correctly.
I will write one that parses the logs in the google bucket, but it will look quite messy (on windows I have to download all of the logs first, for example). Should I still post it here?
FYI: 24 instances failed for some reason in my run
&lt;denchmark-h:h2&gt;Text normalization&lt;/denchmark-h&gt;

Still not sure if it was a neccessary decision because of the capabilities of the transformer, but I think it should not include lowering the text, as - as far as I know - lowering the text on a 32k bilingual corpus is not necessary, so I think it is not necessary here as well. What do you think?
&lt;denchmark-h:h2&gt;code_dir upload&lt;/denchmark-h&gt;

I think it should be uploaded directly into "~/.local/lib/python{python_version}/site-packages/tensor2tensor/data_generators/" , to make sure all imports use the new version and the "python -m" command work out in the same way, too
&lt;denchmark-h:h2&gt;Timeout&lt;/denchmark-h&gt;

Reducing the timeout to 10 seconds saves 20% of computational costs, with neglectable decrease of coverage.
(Note: I only tested coverage on the first shard for both timeouts.)
(Here &lt;denchmark-link:https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/wikisum/get_references_web_single_group.py#L209&gt;https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/wikisum/get_references_web_single_group.py#L209&lt;/denchmark-link&gt;
 )
&lt;denchmark-h:h2&gt;Costs&lt;/denchmark-h&gt;

The costs for downloading the web-references were ~500€ (including the timeout reduction, having most workers only run for 4 hours)

Compute Engine | Custom instance Core running in Americas | 658,50 Day | 429,86 €
Compute Engine | Custom instance Ram running in Americas | 659,19 Gibibyte-day | 57,67 €
Compute Engine | Network Internet Egress from Americas to Americas | 106,40 Gibibyte | 10,37 €

You might add to the cost estimate that the 10TB of data will cost ~160€ per month to store
		</comment>
		<comment id='3' author='f-lng' date='2018-05-03T09:28:16Z'>
		&lt;denchmark-h:h2&gt;Time estimates for final generation step&lt;/denchmark-h&gt;

&lt;denchmark-link:https://github.com/rsepassi&gt;@rsepassi&lt;/denchmark-link&gt;

For me it does not take 8 hours but only 16 minutes to generate the final traindata.
I tested on the first 10 shards, here is the log of the first 3 : &lt;denchmark-link:https://pastebin.com/K3YxjuXk&gt;https://pastebin.com/K3YxjuXk&lt;/denchmark-link&gt;

The generated datafiles are ~200mb, so the 200gb for the final dataset seems about right.
They hold about 2.2k datapoints (ref-&gt;wiki) each, so in total I expect 2.2mio datapoints. Does that seem about right as well?
		</comment>
		<comment id='4' author='f-lng' date='2018-05-03T11:28:04Z'>
		&lt;denchmark-h:h2&gt;Deletion bug&lt;/denchmark-h&gt;

If you start instances with --instance_ids , the deletion-on-failure routine mixes up the instance's index with its actual ID/Name, and tries to delete the wrong instances
Here you can see the problematic code &lt;denchmark-link:https://pastebin.com/fuL2UZFw&gt;https://pastebin.com/fuL2UZFw&lt;/denchmark-link&gt;
 (see the comments)
		</comment>
		<comment id='5' author='f-lng' date='2018-05-03T18:51:36Z'>
		Thank you so much for the further reports &lt;denchmark-link:https://github.com/f-lng&gt;@f-lng&lt;/denchmark-link&gt;
. Keep them coming!
For the SSH reliability: I think the best thing to do is to try a full launch of the 1k and then do a relaunch with --instance_ids for the ones where the SSH command failed.
Checks: I tried to make the scripts that are launched on the remote machines as robust as possible so if some of your jobs failed after launch, please report the error messages here.
Text normalization: it may not be strictly necessary, but it's what we did for the paper so we kept it the same.
code_dir upload: If you use the code_dir feature, you'll want to update the setup command. For example, you can make code_dir the entire local tensor2tensor directory and change the setup command to pip install -e . --user so that it makes all the from tensor2tensor... imports use the local copy.
Timeout: that's great. I'll reduce it to 10 and update the cost.
Time and costs: Yes, it took &lt;30m for the final step for me as well. I must have based the initial estimate on a shard running on my local machine instead of on GCP so the data transfer was the bottleneck. Will update the time and cost estimates provided.
Deletion bug: Good find! Will update.
		</comment>
		<comment id='6' author='f-lng' date='2018-05-03T19:07:05Z'>
		&lt;denchmark-link:https://github.com/rsepassi&gt;@rsepassi&lt;/denchmark-link&gt;
 Sure thing, but I think these were all the bugs I will find, as I am currently generating the final dataset and it looks like it all works without problems now.
I will give you an update as soon as I have trained a model with it, tough.
SSH reliability: Indeed that might have been a better approach, it takes a lot of time to spawn them in chunks of 10.
Checks: I think the problems came from time-outed SSH commands, so nothing you can fix I guess. As a very simple check, perhaps this snippet is enough:
&lt;denchmark-code&gt;rerun_shards = set()
ret = shell_output("gsutil ls {bucket}", bucket=BUCKET + "/wiki_references/process_0")

for i in range(1000):
    if ret.find(".gz-"+str(i).rjust(5, '0')+"-of-01000") == -1:
        rerun_shards.add(i)

print("INSTANCES_REDO =" , list(rerun_shards) )
&lt;/denchmark-code&gt;

Text normalization: Alright, good to know, thanks!
code_dir upload: I fixed it by uploading the wikisum/-directory directly into the pip installation, so I did not have to upload the entire tensor2tensor-directory.
Time and costs: Good to know. So all in all, the costs were pretty ok, especially given the 250€ you get with a new google-cloud account.
		</comment>
		<comment id='7' author='f-lng' date='2018-05-03T21:06:58Z'>
		Good luck on your experiments!
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Thu, May 3, 2018 at 12:07 PM Fabian Langer ***@***.***&gt; wrote:
 @rsepassi &lt;https://github.com/rsepassi&gt; Sure thing, but I think these
 were all the bugs I will find, as I am currently generating the final
 dataset. I will give you an update as soon as I have trained a model with
 it, tough.

 SSH reliability: Indeed that might have been a better approach, it takes a
 lot of time to spawn them in chunks of 10.

 Checks: I think the problems came from time-outed SSH commands, so nothing
 you can fix I guess. As a very simple check, perhaps this snippet is enough:

 rerun_shards = set()
 ret = shell_output("gsutil ls {bucket}", bucket=BUCKET + "/wiki_references/process_0")

 for i in range(1000):
     if ret.find(".gz-"+str(i).rjust(5, '0')+"-of-01000") == -1:
         rerun_shards.add(i)

 print("INSTANCES_REDO =" , list(rerun_shards) )

 Text normalization: Alright, good to know, thanks!

 code_dir upload: I fixed it by uploading the wikisum/-directory directly
 into the pip installation, so I did not have to upload the entire
 tensor2tensor-directory.

 Time and costs: Good to know. So all in all, the costs were pretty ok,
 especially given the 250€ you get with a new google-cloud account.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#757 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABEGW560mOcKfxdbOSPL4NnodgjK-ebHks5tu1VbgaJpZM4TsUiW&gt;
 .



		</comment>
		<comment id='8' author='f-lng' date='2018-05-04T02:54:53Z'>
		For others that see this bug, the fixes will be out with the next T2T release sometime next week.
		</comment>
		<comment id='9' author='f-lng' date='2018-05-04T09:23:58Z'>
		Just started the first training run &lt;denchmark-link:https://github.com/rsepassi&gt;@rsepassi&lt;/denchmark-link&gt;
 and noticed the following things
&lt;denchmark-h:h2&gt;eval_steps&lt;/denchmark-h&gt;


--eval_steps=100

Seems a bit low, especially given the huge amount of evaluation data. Looks like you would spend most time on evaluation with that setting.
&lt;denchmark-h:h2&gt;hparams&lt;/denchmark-h&gt;

There should be a note on setting the L parameter, as the paper mentions that the base transformer is not able to learn for L ~ 2000 (and also mentions the best range for L on lead section generation).
So perhaps a custom hparam set for wikisum would be a good idea.
&lt;denchmark-code&gt;@registry.register_hparams
def transformer_big_wsum_v1():
  hparams = transformer_big_single_gpu() # for best results
  hparams.batch_size = 2500 # maximum for 1080TI
  hparams.max_input_seq_length = 500  # setting L
  hparams.max_target_seq_length = 0 # never truncate EOS

  return hparams
&lt;/denchmark-code&gt;

(Is that the right parameter to set?)
		</comment>
		<comment id='10' author='f-lng' date='2018-05-04T16:33:30Z'>
		Yes, I’ll add a note on the length parameter. Hopefully we’ll have better
repro instructions on the next push.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Fri, May 4, 2018 at 2:24 AM Fabian Langer ***@***.***&gt; wrote:
 Just started the first training run @rsepassi
 &lt;https://github.com/rsepassi&gt; and noticed the following things
 eval_steps

 --eval_steps=100
 Seems a bit low, especially given the huge amount of evaluation data.
 Looks like you would spend most time on evaluation with that setting.

 hparams

 There should be a note on setting the L parameter, as the paper mentions
 that the base transformer is not able to learn for L ~ 2000 (and also
 mentions the best range for L on lead section generation)

   hparams.max_input_seq_length = 500
   hparams.max_target_seq_length = 0

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#757 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABEGW0sm84fTkss6nBcYK6fScT2aQA4Pks5tvB4wgaJpZM4TsUiW&gt;
 .



		</comment>
		<comment id='11' author='f-lng' date='2018-05-05T07:38:09Z'>
		Deleted because I found something odd, now investigating
		</comment>
		<comment id='12' author='f-lng' date='2018-05-05T10:05:59Z'>
		&lt;denchmark-link:https://github.com/rsepassi&gt;@rsepassi&lt;/denchmark-link&gt;
 I found a problem that will prevent training on wikisum data at all.
&lt;denchmark-h:h3&gt;debug_keep_up&lt;/denchmark-h&gt;

If the flag is set, the script checks for num_instances instead of the actual requested instance_ids to be 1. However, using --instance_ids you could set one specific instance to debug and still have num_instances at 1000, so len(instances_ids) should be checked.
So &lt;denchmark-link:https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/wikisum/parallel_launch.py#L248&gt;https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/wikisum/parallel_launch.py#L248&lt;/denchmark-link&gt;
 should be

&lt;denchmark-h:h2&gt;Failing generation due to out-of-memory&lt;/denchmark-h&gt;

About 25 instances fail during example generation due to running out of memory.
I did not find the bug previously, as the according failure message only shows up in the log sometimes (mostly the process is killed silently).
But due to the next bug, I had to look  into it and found that, in order to make sure all datafiles are generated, you need 3gb of RAM instead of 2.
&lt;denchmark-h:h2&gt;Silent failing if not all datafiles are there&lt;/denchmark-h&gt;

Deleted, assumption was wrong and it is working as expected
		</comment>
		<comment id='13' author='f-lng' date='2018-05-05T12:48:16Z'>
		Update:
Ok, it seems that - while the bug of not enough RAM exists - my conclusion of its impact was wrong. The reason for the incomplete evaluation was that I had still set "eval_drop_long_sequences=true".
The problem is, that might not help me determine why my first training run did overfit so badly. Will investigate further and report back.
		</comment>
		<comment id='14' author='f-lng' date='2018-05-07T08:34:38Z'>
		&lt;denchmark-h:h2&gt;Training behaviour&lt;/denchmark-h&gt;

&lt;denchmark-link:https://github.com/rsepassi&gt;@rsepassi&lt;/denchmark-link&gt;

Alright, overfitting is no longer happening and the training behaviour looks good to me with:
&lt;denchmark-code&gt;@registry.register_hparams
def transformer_base_wsum_v1():
  hparams = transformer_base_v1()

  hparams.max_input_seq_length = 500
  hparams.max_target_seq_length = 0
  hparams.max_length = 500

  return hparams
&lt;/denchmark-code&gt;

However, I am not sure if the metrics are within expected ranges, could you check against your own training runs? &lt;denchmark-link:https://pasteboard.co/Hk2M9A9.png&gt;https://pasteboard.co/Hk2M9A9.png&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;Collaps of model output&lt;/denchmark-h&gt;

And while the model still seems to learn nicely I found something odd - and I am not sure if that is expected during early training (20k steps and 1.5 days in) or a sign for a problem: model output collapsed to one "best guess", and the model (4 beams, alpha 0.6) always outputs (more or less) the same sentence, regardless of the input.
Here you find an example decode from the dataset:
&lt;denchmark-link:https://pastebin.com/a540WiZ2&gt;https://pastebin.com/a540WiZ2&lt;/denchmark-link&gt;

		</comment>
		<comment id='15' author='f-lng' date='2018-05-07T23:16:34Z'>
		Thanks for the further notes &lt;denchmark-link:https://github.com/f-lng&gt;@f-lng&lt;/denchmark-link&gt;
.
I'll fix the debug_keep_up bug and up produce_examples to 3G of memory.
I'll also add a data validation script and instructions for running it to the README.
On the issue of training stopping when a file is missing, I don't understand where that behavior is coming from and so am skeptical that that's indeed what you're seeing. To find files to read, it uses a &lt;denchmark-link:https://github.com/tensorflow/tensor2tensor/blob/4ebb860e7846818011b0fa77702edf04fb34011c/tensor2tensor/data_generators/problem.py#L541&gt;filepattern&lt;/denchmark-link&gt;
, so it shouldn't matter if there are missing ones. Could you provide some more of your observations that leads you to believe that it's stopping because of a missing file?
		</comment>
		<comment id='16' author='f-lng' date='2018-05-08T07:37:15Z'>
		&lt;denchmark-link:https://github.com/rsepassi&gt;@rsepassi&lt;/denchmark-link&gt;
 No you are right, it was just an assumption that I made based on the fact that evaluation stopped at 60/100 and the first missing file was in that range as well.
But the assumption was wrong, it did not stop because a file was missing. It simply skipped the data due to "eval_drop_long_sequences=true"
--
&lt;denchmark-link:https://github.com/rsepassi&gt;@rsepassi&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/lukaszkaiser&gt;@lukaszkaiser&lt;/denchmark-link&gt;

What is more of an issue now is the actual training.
Can one of you say anything about the expected training behaviour, my hyperparameters and the model collaps that I am seeing? Is it expected after 2.5 days or is something broken?
I am now 2.5 days in, and output seems to have collapsed to a single sentence, no matter what input the network gets.
Metrics are still looking OK I guess : &lt;denchmark-link:https://pasteboard.co/Hkc3lcQ.png&gt;https://pasteboard.co/Hkc3lcQ.png&lt;/denchmark-link&gt;

For completeness, here are the used hparams again (learning the lead section problem)

@registry.register_hparams
def transformer_base_wsum_v1():
hparams = transformer_base_v1()
hparams.max_input_seq_length = 500
hparams.max_target_seq_length = 0
hparams.max_length = 500
return hparams

		</comment>
		<comment id='17' author='f-lng' date='2018-05-18T00:14:21Z'>
		Hi &lt;denchmark-link:https://github.com/f-lng&gt;@f-lng&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/rsepassi&gt;@rsepassi&lt;/denchmark-link&gt;
,
I am trying to use the Transformer-Decoder-Only model from the "Generating Wikipedia..." paper for my own dataset.
I see that this thread points to instructions for generating the WikiSum dataset but I didn't see instructions for running the model as described in "Generating Wikipedia..." ?
Also, are there any guidelines on how I should format my dataset so that the T2T Transformer-Decoder-Only model can consume it?
		</comment>
		<comment id='18' author='f-lng' date='2018-05-24T20:54:23Z'>
		Closing this bug as data generation now works. Thanks &lt;denchmark-link:https://github.com/f-lng&gt;@f-lng&lt;/denchmark-link&gt;
 for the bug finding!
For training, I believe these settings produce reasonable results. We usually trained on 8 GPUs or a TPU:
model = 'transformer'
hparams = "max_input_seq_length=$MAX_LEN"
hparams_set = 'transformer_base'
decode_hparams = 'beam_size=4,alpha=0.6,batch_size=1'
		</comment>
	</comments>
</bug>