<bug id='496' author='lnabergall' open_date='2017-12-30T04:26:37Z' closed_time='2018-02-09T01:23:25Z'>
	<summary>Language model decoding problems</summary>
	<description>
There seem to be problems with decoding from a language model, in particular a (very small) attention_lm and a custom lstm model. When t2t-decoder is run in interactive mode:
python t2t-decoder --data_dir="C:\t2t\t2t_data\lm1b_baseline" --tmp_dir="C:\t2t\t2t_temp" --t2t_usr_dir="C:\t2t\multi_lm" --output_dir="C:\t2t\t2t_training\lm1b_baseline\attention_lm-attention_lm_tiny" --problem=languagemodel_lm1b8k --model=attention_lm --hparams_set=attention_lm_tiny --decode_interactive
it produces the same output every time, regardless of the input:

INFO:tensorflow:Restoring parameters from C:\t2t\t2t_training\lm1b_baseline\attention_lm-attention_lm_tiny\model.ckpt-108581
2017-12-29 23:16:23.449223: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\kernels\logging_ops.cc:79] beam_decode batch_size=[1]
INFO:tensorflow:" I 'm not going to be able to make it , " he said .
INTERACTIVE MODE  num_samples=1  decode_length=100
it=&lt;input_type&gt;     ('text' or 'image' or 'label', default: text)
pr=&lt;problem_num&gt;    (set the problem number, default: 0)
in=&lt;input_problem&gt;  (set the input problem number)
ou=&lt;output_problem&gt; (set the output problem number)
ns=&lt;num_samples&gt;    (changes number of samples, default: 1)
dl=&lt;decode_length&gt;  (changes decode length, default: 100)
&lt;target_prefix&gt;                (decode)
q                   (quit)
&gt;What if
2017-12-29 23:16:29.480916: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\kernels\logging_ops.cc:79] beam_decode batch_size=[1]
INFO:tensorflow:" I 'm not going to be able to make it , " he said .
INTERACTIVE MODE  num_samples=1  decode_length=100
it=&lt;input_type&gt;     ('text' or 'image' or 'label', default: text)
pr=&lt;problem_num&gt;    (set the problem number, default: 0)
in=&lt;input_problem&gt;  (set the input problem number)
ou=&lt;output_problem&gt; (set the output problem number)
ns=&lt;num_samples&gt;    (changes number of samples, default: 1)
dl=&lt;decode_length&gt;  (changes decode length, default: 100)
&lt;target_prefix&gt;                (decode)
q                   (quit)
&gt;Let's try this instead
2017-12-29 23:16:49.811394: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\kernels\logging_ops.cc:79] beam_decode batch_size=[1]
INFO:tensorflow:" I 'm not going to be able to make it , " he said .
INTERACTIVE MODE  num_samples=1  decode_length=100
it=&lt;input_type&gt;     ('text' or 'image' or 'label', default: text)
pr=&lt;problem_num&gt;    (set the problem number, default: 0)
in=&lt;input_problem&gt;  (set the input problem number)
ou=&lt;output_problem&gt; (set the output problem number)
ns=&lt;num_samples&gt;    (changes number of samples, default: 1)
dl=&lt;decode_length&gt;  (changes decode length, default: 100)
&lt;target_prefix&gt;                (decode)
q                   (quit)
&gt;asdfasfdas
2017-12-29 23:16:58.642319: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\kernels\logging_ops.cc:79] beam_decode batch_size=[1]
INFO:tensorflow:" I 'm not going to be able to make it , " he said .
INTERACTIVE MODE  num_samples=1  decode_length=100
it=&lt;input_type&gt;     ('text' or 'image' or 'label', default: text)
pr=&lt;problem_num&gt;    (set the problem number, default: 0)
in=&lt;input_problem&gt;  (set the input problem number)
ou=&lt;output_problem&gt; (set the output problem number)
ns=&lt;num_samples&gt;    (changes number of samples, default: 1)
dl=&lt;decode_length&gt;  (changes decode length, default: 100)
&lt;target_prefix&gt;                (decode)
q                   (quit)
&gt;

This problem also occurs for a LSTM model:
&lt;denchmark-code&gt;@registry.register_model
class LSTMLm(t2t_model.T2TModel):
    """Basic LSTM recurrent neural network."""

    def model_fn_body(self, features):
        if self._hparams.initializer == "orthogonal":
            raise ValueError("LSTM models fail with orthogonal initializer.")
        train = self._hparams.mode == tf.estimator.ModeKeys.TRAIN
        with tf.variable_scope("lstm_lm"):
            # Flatten and shift inputs.
            shifted_targets = common_layers.shift_right(features.get("targets"))
            inputs = common_layers.flatten4d3d(shifted_targets)
            outputs, _ = lstm.lstm(inputs, self._hparams, train, "lstm")
            return tf.expand_dims(outputs, axis=2)
&lt;/denchmark-code&gt;

python t2t-decoder --data_dir="C:\t2t\t2t_data\lm1b_baseline" --tmp_dir="C:\t2t\t2t_temp" --t2t_usr_dir="C:\t2t\multi_lm" --output_dir="C:\t2t\t2t_training\lm1b_baseline\lstm-lstm_tiny" --problem=languagemodel_lm1b8k --model=lstm_lm --hparams_set=lstm_tiny --decode_interactive
Note that both models were trained on the 1 billion word dataset with a vocab size of 8192
&lt;denchmark-code&gt;@registry.register_problem("languagemodel_lm1b8k")
class LanguagemodelLm1b8k(LanguagemodelLm1b32k):

    @property
    def targeted_vocab_size(self):
        return 2**13  # 8192
&lt;/denchmark-code&gt;

for over 100,000 steps and reached decent negative log perplexities, under -4.
Although I have not done a complete review of the source code, this seems to occur for two reasons: (1) the model isn't receiving the input sequence and conditioning the output on it, and (2) there isn't any non-deterministic sampling from the output probabilities.
Is this correct? And if so, is this the intended behavior or a bug? Both (1) and (2) are a problem for decoding from language models. Others have raised similar issues with decoding, e.g. &lt;denchmark-link:https://github.com/tensorflow/tensor2tensor/issues/439&gt;#439&lt;/denchmark-link&gt;
.
I'm running TensorFlow 1.3.0 and Tensor2Tensor 1.3.0. Any help would be greatly appreciated.
	</description>
	<comments>
		<comment id='1' author='lnabergall' date='2017-12-30T09:46:14Z'>
		I have not tested again with last versions, but it would be great if you could test it with TF 1.4 and T2T 1.4.1
They made significant changes since.
		</comment>
		<comment id='2' author='lnabergall' date='2018-01-01T23:52:16Z'>
		I'm having some problems using T2T 1.4.1 and TF 1.4, it looks like T2T now requires TF Eager, but it isn't yet included in the official TF releases?
Also, I tried the nightly TensorFlow releases, but encountered one of several problems (depending on the version): (1) TF eager imports not working, (2) T2T hanging before training as in &lt;denchmark-link:https://github.com/tensorflow/tensor2tensor/issues/494&gt;#494&lt;/denchmark-link&gt;
, or (3) the TensorFlow runtime not starting. I thought the last problem could be caused by an improper install or missing environment variables, but it seems that all the CUDA and cuDNN files were placed in the right locations and the environment variables were there.
		</comment>
		<comment id='3' author='lnabergall' date='2018-01-16T09:25:01Z'>
		i meet the same result, my script
1 #!/bin/sh
2
3 PROBLEM=languagemodel_ptb10k
4 #MODEL=transformer
5 #HPARAMS=transformer_base
6 MODEL=attention_lm
7 HPARAMS=attention_lm_small
8
9 DATA_DIR=t2t_data
10 TMP_DIR=t2t_datagen_tmp
11 TRAIN_DIR=t2t_train/$PROBLEM/$MODEL-$HPARAMS
12
13 mkdir -p $DATA_DIR $TMP_DIR $TRAIN_DIR
14
15 ## Generate data
16 #t2t-datagen 
17 #    --data_dir=$DATA_DIR 
18 #    --tmp_dir=$TMP_DIR 
19 #    --problem=$PROBLEM
20
21 t2t-trainer 
22     --data_dir=$DATA_DIR 
23     --problems=$PROBLEM 
24     --model=$MODEL 
25     --hparams_set=$HPARAMS 
26     --output_dir=$TRAIN_DIR 
27     --worker_gpu=8 
28     --train_steps=1000 
29     --eval_steps=10
30
31 t2t-decoder 
32     --data_dir=$DATA_DIR 
33     --problems=$PROBLEM 
34     --model=$MODEL 
35     --hparams_set=$HPARAMS 
36     --output_dir=$TRAIN_DIR 
37     --decode_from_file=$TMP_DIR/simple-examples/data/small_test
my decode result seems empty:
INFO:tensorflow:Inference results INPUT: it was the first time in seven years that moscow has n't joined efforts led by
INFO:tensorflow:Inference results OUTPUT:
INFO:tensorflow:Inference results INPUT: for a while
INFO:tensorflow:Inference results OUTPUT:
INFO:tensorflow:Inference results INPUT: we 've been spending a lot of time in los angeles talking to tv production people says mike parks president of call interactive which supplied technology for both abc sports and nbc 's consumer minutes
INFO:tensorflow:Inference results OUTPUT:
INFO:tensorflow:Inference results INPUT: from the fee the local phone company and the long-distance carrier extract their costs to carry the call passing the rest of the money to the
INFO:tensorflow:Inference results OUTPUT:
		</comment>
		<comment id='4' author='lnabergall' date='2018-02-09T01:23:25Z'>
		Closing in favor of &lt;denchmark-link:https://github.com/tensorflow/tensor2tensor/issues/439&gt;#439&lt;/denchmark-link&gt;
 which reports the same issue.
		</comment>
	</comments>
</bug>