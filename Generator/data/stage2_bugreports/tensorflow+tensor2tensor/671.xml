<bug id='671' author='nimasnjb' open_date='2018-03-29T02:44:43Z' closed_time='2018-05-04T02:32:23Z'>
	<summary>Incorrect metrics or bad decodding or both?</summary>
	<description>
In text2text problems:
I have the last versions of tensorflow and tensor2tensor as 28th of March. I had a problem with the previous version like 1.4. I knew there is something wrong with the metrics like ROUGE.
In CNN-Dailymail problem:
I got extraordinary high scores for rouge, like 0.95. Accuracy is above 65% and loss is under 0.35. It must be a master-model. I give it a test-set to decode. The decoded text and the input are almost identical. there might be just 25% difference in lengths. I don't understand why I got those brilliant metrics for this output.
So far we might say the decoder is generating bad outputs and metrics are correct.
In line_poetry_problem:
I used a dataset that contains duplicated instances. The metric for accuracy_per_sequence must relatively high, but it is zero! When I remove the duplicates and train the model with the new dataset, the accuracy_per_sequence rise with a good slop to 6%!!
These two observations (from poetry problem) suggest that the metrics are incorrect. I don't understand why the quality of decoded outputs is not consistent with the model metrics. The poetry model gives me a long string of a duplicated words as output, while the model shows great metrics in the evaluation.
Have anyone got good results from CNN-Dailymail problem?
&lt;denchmark-h:h3&gt;TensorFlow and tensor2tensor versions&lt;/denchmark-h&gt;

1.6, 1.5.5
	</description>
	<comments>
		<comment id='1' author='nimasnjb' date='2018-03-29T06:23:50Z'>
		Note that the ROUGE (all its variants) implementation in T2T does not glue subwords together before computing the score, so it is just an approximation
&lt;denchmark-link:https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/utils/rouge.py#L220-L221&gt;https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/utils/rouge.py#L220-L221&lt;/denchmark-link&gt;

I have no experience with ROUGE, but with BLEU the approx_bleu is about 1.5 higher than it should be (this may be related to the fact that there are about 1.5 subwords per word in my dataset).
		</comment>
		<comment id='2' author='nimasnjb' date='2018-03-29T06:55:05Z'>
		Thank you. Just in the past hours, I changed the hparams for poetry problem.
attention_dropout = 0.6, layer_prepostprocess_dropout = 0.6, max_length = 256
Now the outputs are just as they should be. So dropouts and max_lengths are very important. I think max lengths that are not multiplications of 2 (e.g 200, 500) might result in confusion in the sequencing! It's a guess.
It's been said that "Longer sequences (than max_length) are dropped and shorter ones are padded."
Is it true that the longers are dropped, or in other words are ignored, or they will be just truncated?
		</comment>
		<comment id='3' author='nimasnjb' date='2018-03-29T07:42:41Z'>
		
Is it true that the longers are dropped, or in other words are ignored, or they will be just truncated?

They are dropped, see
&lt;denchmark-link:https://github.com/tensorflow/tensor2tensor/blob/346a27b/tensor2tensor/data_generators/problem.py#L789&gt;https://github.com/tensorflow/tensor2tensor/blob/346a27b/tensor2tensor/data_generators/problem.py#L789&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensor2tensor/blob/346a27b/tensor2tensor/utils/data_reader.py#L51&gt;https://github.com/tensorflow/tensor2tensor/blob/346a27b/tensor2tensor/utils/data_reader.py#L51&lt;/denchmark-link&gt;


I think max lengths that are not multiplications of 2 (e.g 200, 500) might result in confusion in the sequencing! It's a guess.

That would be really strange. But actually, there is something strange (at least in MT), see &lt;denchmark-link:https://github.com/tensorflow/tensor2tensor/issues/582&gt;#582&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='4' author='nimasnjb' date='2018-03-29T07:57:45Z'>
		I will let the CNNDMto train for 100000 steps. with fixed lengths for input and targets.
So far I noticed that the test outputs are either empty or duplicated of a single word like "foo foo foo ...". I will put an update after training completed.
		</comment>
		<comment id='5' author='nimasnjb' date='2018-03-29T08:03:14Z'>
		
So far I noticed that the test outputs are either empty or duplicated of a single word like "foo foo foo ...".

If the dev-set score is close to zero and does not improve, it is usually a sign that the training diverged and probably will never get OK. Empty output or repeating one subword all the time are typical for diverged training. There may be multiple reasons for the divergence and multiple possible solutions, see e.g. &lt;denchmark-link:http://ufallab.ms.mff.cuni.cz/~popel/training-tips-transformer.pdf&gt;this paper&lt;/denchmark-link&gt;
 (about MT, but I guess most of it applies to any text2text).
		</comment>
		<comment id='6' author='nimasnjb' date='2018-03-29T16:50:25Z'>
		This is what I'm trying to say. All the metrics in the evaluation are getting better step by step, while the outputs are the same, the repetition of a single word. Do you have any idea? Do you think that the model is ok, but I'm doing the decoding wrong?
MODEL=transformer
HPARAMS=transformer_prepend
t2t-trainer 
--t2t_usr_dir=$USER_DIR 
--model=$MODEL 
--problems=$PROBLEM 
--hparams_set=$HPARAMS 
--hparams="batch_size=1024,attention_dropout = 0.3, layer_prepostprocess_dropout = 0.3, learning_rate = 0.05, max_input_seq_length=1024, max_target_seq_length=128" 
--data_dir=$DATA_DIR 
--output_dir=$TRAIN_DIR 
--worker_gpu=4 
--train_steps=500000 
--eval_steps=50
t2t-decoder 
--t2t_usr_dir=$USER_DIR 
--data_dir=$DATA_DIR 
--problems=$PROBLEM 
--model=$MODEL 
--hparams_set=$HPARAMS 
--hparams="batch_size=1024,attention_dropout = 0.3, layer_prepostprocess_dropout = 0.3, learning_rate = 0.05, max_input_seq_length=1024, max_target_seq_length=128" 
--decode_hparams="beam_size=$BEAM_SIZE,alpha=$ALPHA,batch_size=4" 
--output_dir=$TRAIN_DIR 
--worker_gpu=1 
--eval_use_test_set=False 
--schedule=eval 
--decode_from_file=$DECODE_FILE \
--decode_to_file=$OUTPUT_FILE
		</comment>
		<comment id='7' author='nimasnjb' date='2018-03-30T11:29:18Z'>
		I found the culprit. When I remove the "prepend_inputs_masked_attention" in hparams, the outputs are fine. But the performance is almost 40% lower. I think training works well with attention mask, but decoder has a bug when we train the model with attention. This is a bad bug, paralyzed the model from reaching its best performance.
		</comment>
		<comment id='8' author='nimasnjb' date='2018-05-04T02:31:57Z'>
		The prepend_inputs_masked_attention bug should now be fixed. Try again and let us know how it goes.
		</comment>
		<comment id='9' author='nimasnjb' date='2018-05-04T02:32:23Z'>
		Closing in favor of &lt;denchmark-link:https://github.com/tensorflow/tensor2tensor/issues/680&gt;#680&lt;/denchmark-link&gt;
.
		</comment>
	</comments>
</bug>