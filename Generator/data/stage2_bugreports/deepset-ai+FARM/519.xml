<bug id='519' author='PhilipMay' open_date='2020-09-04T16:43:36Z' closed_time='2020-10-21T15:15:04Z'>
	<summary>Getting different predictions on different runs with same ELECTRA model.</summary>
	<description>
I trained an Electra Model on text classification. 3 Classes.
Saved it with
&lt;denchmark-code&gt;    save_dir = Path("saved_models/bert-german-doc-tutorial")
    model.save(save_dir)
    processor.save(save_dir)
&lt;/denchmark-code&gt;

Now I load Data I want to predict on and getting different results:
&lt;denchmark-code&gt;1st run:
labal_n          5055
label_i           855
label_e          8

2nd run
labal_n          4609
label_e          990
label_i           319

3rd run
label_i        3355
labal_n       2510
label_e       53
&lt;/denchmark-code&gt;

How on earth can that be? Even if the model would be a total crap the prediction should be deterministic - right?
	</description>
	<comments>
		<comment id='1' author='PhilipMay' date='2020-09-04T16:55:13Z'>
		How do you load it? Its Friday evening, this might also affect the preds or at least the code ;)
		</comment>
		<comment id='2' author='PhilipMay' date='2020-09-04T18:47:33Z'>
		Well - I did use torch 1.6.0. Might that be the reason?
And transformers 3.1.0
		</comment>
		<comment id='3' author='PhilipMay' date='2020-09-04T19:12:29Z'>
		Is it possible that there is a bug in transformers that does not let me load saved delectra models from local disk?
		</comment>
		<comment id='4' author='PhilipMay' date='2020-09-05T08:00:01Z'>
		
How do you load it?

&lt;denchmark-code&gt;    model = Inferencer.load(model_path,
                        num_processes=4,
                        batch_size=batch_size,
                        gpu=True,
                        )
    result = model.inference_from_dicts(dicts=unlabeled_text_dict, multiprocessing_chunksize=4000)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='PhilipMay' date='2020-09-05T08:03:33Z'>
		I tested it with a normal BERT model.
There I get 100% stable results.
Only with electra I get unstable things...
Loading / saving electra models with FARM / transformers has a bug is my current hypothesis.
Maybe some layers are not saved / loaded and so are random initialized...
Do you have any idea how and where I could start a debugging?
This might also effect your "future release plans" if you know what I mean... &lt;denchmark-link:https://github.com/Timoeller&gt;@Timoeller&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/stefan-it&gt;@stefan-it&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='PhilipMay' date='2020-09-05T08:06:40Z'>
		Mhh, I believe the culprit might be hidden somewhere else : )

save_dir = Path("saved_models/bert-german-doc-tutorial")

in FARM we still have string based model loading, so if you name your model "bert" it will be treated as such. Could that be the problem?
		</comment>
		<comment id='7' author='PhilipMay' date='2020-09-05T08:15:28Z'>
		
in FARM we still have string based model loading, so if you name your model "bert" it will be treated as such. Could that be the problem?

It is "./models/german-nlp-group-electra-base-german-uncased"
		</comment>
		<comment id='8' author='PhilipMay' date='2020-09-05T08:44:35Z'>
		How about posting the code you actually used so we can find potential bugs? :p
As of debugging, I would step into the debugger when loading the model and see if the weights are assigned correctly and are the same across multiple restarts.
		</comment>
		<comment id='9' author='PhilipMay' date='2020-09-05T10:55:47Z'>
		
How about posting the code you actually used so we can find potential bugs? :p

Yes - I will try to build an example with an open dataset.
&lt;denchmark-link:https://github.com/Timoeller&gt;@Timoeller&lt;/denchmark-link&gt;
 I am doing EarlyStopping which saves the model and loads the best after stopping to evaluate against test.
That seems to work. Maybe there is a difference in loading it while training and loading it for Inference?
I will check that tonight...
		</comment>
		<comment id='10' author='PhilipMay' date='2020-09-05T22:56:48Z'>
		Very strange, because the Inferencer constructor calls set_all_seeds ðŸ¤”
		</comment>
		<comment id='11' author='PhilipMay' date='2020-09-06T04:36:21Z'>
		I always thought that the Trainer (callback) saving and loading for test evaluation is working correctly and that there is only a bug in the Interferer. But that is not the case. See here &lt;denchmark-link:https://github.com/deepset-ai/FARM/issues/522&gt;#522&lt;/denchmark-link&gt;
.
When training, the model is not loaded from disk when doing test set evaluation but the old is used.
		</comment>
		<comment id='12' author='PhilipMay' date='2020-09-06T04:45:02Z'>
		
Very strange, because the Inferencer constructor calls set_all_seeds thinking

&lt;denchmark-link:https://github.com/stefan-it&gt;@stefan-it&lt;/denchmark-link&gt;
 do you think this might reset important parts of the neutwork?
		</comment>
		<comment id='13' author='PhilipMay' date='2020-09-07T07:33:00Z'>
		I doule checked this. With "normal" BERT models there is no problem. Seems to be electra specific.
		</comment>
		<comment id='14' author='PhilipMay' date='2020-09-07T08:57:43Z'>
		So early stopping is unrelated to Inferencer loading, lets not try to mix bugs : )
Can you create a minimal script reproducing the ELECTRA Inferencer error?
		</comment>
		<comment id='15' author='PhilipMay' date='2020-09-07T10:01:39Z'>
		Example code to reproduce: &lt;denchmark-link:https://gist.github.com/PhilipMay/bd250cba591b3252b8da2f3d31ee5b64&gt;https://gist.github.com/PhilipMay/bd250cba591b3252b8da2f3d31ee5b64&lt;/denchmark-link&gt;

I am using the master branch from FARM, transformers 3.1.0 and torch 1.6.0.
First run it with lang_model = "bert-base-german-dbmdz-uncased". After that run with lang_model = "german-nlp-group/electra-base-german-uncased". Compare the results as I did below:
&lt;denchmark-h:h2&gt;Output for the bert model&lt;/denchmark-h&gt;

lang_model = "bert-base-german-dbmdz-uncased":
&lt;denchmark-code&gt;result from early stopping (on dev set) 0.7936361061278443
result from test set (with best loaded trial) 0.7892217833682655
Please compare result from early stopping (on dev set) and result from test set (with best loaded trial)...
&lt;/denchmark-code&gt;

Best early stopping result on dev set is 0.79 while the test set result is 0.78. This is the expected result.
... note that the trainer stored the model after each evaluation (when it was better) and then loads and evaluates it vs. test.
Test set evaluation is a little bit lower but in the same range. Everything is ok...
&lt;denchmark-h:h2&gt;Output for the bert model&lt;/denchmark-h&gt;

lang_model = "german-nlp-group/electra-base-german-uncased":
&lt;denchmark-code&gt;result from early stopping (on dev set) 0.7837095866567335
result from test set (with best loaded trial) 0.47395779141955396
&lt;/denchmark-code&gt;

Best early stopping result on dev set is 0.78 while the test set result is 0.47.
The test evaluation is much lower which is not normal.
F1 Macro of 0.47 should be close to the naive baseline (random prediction) on this dataset.
My guess is that there is a bug in saving or loading electra models.
		</comment>
		<comment id='16' author='PhilipMay' date='2020-09-07T10:35:57Z'>
		Are you sure of this code:
&lt;denchmark-link:https://github.com/deepset-ai/FARM/blob/master/farm/modeling/language_model.py#L1255-L1265&gt;https://github.com/deepset-ai/FARM/blob/master/farm/modeling/language_model.py#L1255-L1265&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/stefan-it&gt;@stefan-it&lt;/denchmark-link&gt;
 you wrote it 4 months ago. Could you check that?
		</comment>
		<comment id='17' author='PhilipMay' date='2020-09-07T12:17:26Z'>
		&lt;denchmark-link:https://github.com/PhilipMay&gt;@PhilipMay&lt;/denchmark-link&gt;
 I thought we were talking about different inferencer results.
How about we break this down. Load the inferencer once, make preds1, load it another time, make preds2. If preds1 != preds2 we should fix something.
Including crossval and early stopping adds too much complexity on this issue. Is that correct or am I missing something?
		</comment>
		<comment id='18' author='PhilipMay' date='2020-09-07T14:09:25Z'>
		Ok &lt;denchmark-link:https://github.com/Timoeller&gt;@Timoeller&lt;/denchmark-link&gt;
 - sorry for the bad example. You are right.
Here is a more streamlined example without crossvalidaton: &lt;denchmark-link:https://gist.github.com/PhilipMay/2e42eeb7174cf0a122036a26ab38ceba#file-electra_load_test-py&gt;https://gist.github.com/PhilipMay/2e42eeb7174cf0a122036a26ab38ceba#file-electra_load_test-py&lt;/denchmark-link&gt;

I load a model, train it on germeval18 wait for automatic test set evaluation and write down the f1 macro.
Then the model is saved and I press return to continue.
Then model is loaded to inferencer and evaluated on test set again. Then f1 macro is calculated and printed.
For lang_model = "bert-base-german-dbmdz-uncased"
Test F1 macro on training: 0.7699
Test F1 macro on inference: 0.7686296419156032
For lang_model = "german-nlp-group/electra-base-german-uncased"
Test F1 macro on training: 0.7628
Test F1 macro on inference: 0.4487975932065501
The difference in case of electra shows the bug. The numbers should be that same!
&lt;denchmark-link:https://github.com/stefan-it&gt;@stefan-it&lt;/denchmark-link&gt;
 could you maybe have a look on this? You were the nice guy who added electra support to FARM.
Please make sure to install the current transformers version 3.1.0 to run this. Otherwise the tokenizer will drop umlauts.
		</comment>
		<comment id='19' author='PhilipMay' date='2020-09-07T15:16:53Z'>
		The issue comes from the class weights, those are not properly loaded and not specifying task_type="classification" in the inferencer.
And I also observe the label mapping is inverted, but only sometimes. Unsure why this happens...
		</comment>
		<comment id='20' author='PhilipMay' date='2020-09-07T18:44:11Z'>
		&lt;denchmark-link:https://github.com/Timoeller&gt;@Timoeller&lt;/denchmark-link&gt;
 do you know more details?

Are you sure that the problem is coming from loading and not from saving?
Is it the language model that is not correctly loaded or is it the head?
Is the bug located in HF or Farm?

		</comment>
		<comment id='21' author='PhilipMay' date='2020-09-08T07:55:27Z'>
		
Very strange, because the Inferencer constructor calls set_all_seeds thinking

I removed that line. Bug still persists.
		</comment>
		<comment id='22' author='PhilipMay' date='2020-09-08T08:49:50Z'>
		Ok, I did some further testing as well because there are apparently weird things happening.
There seems to be a difference when running the script

with model training + inferencing in one go vs
model training, saving, stopping the script. Running a new script with inference on the saved model only

Could you please verify this &lt;denchmark-link:https://github.com/PhilipMay&gt;@PhilipMay&lt;/denchmark-link&gt;

		</comment>
		<comment id='23' author='PhilipMay' date='2020-09-08T09:19:22Z'>
		&lt;denchmark-link:https://github.com/Timoeller&gt;@Timoeller&lt;/denchmark-link&gt;
 I can approve this. Running training, final test set evaluation and saving in one script and then loading and inference in other script the bug does  and we have the .
		</comment>
		<comment id='24' author='PhilipMay' date='2020-09-08T09:21:32Z'>
		All GPUs are currently occupied for EACL, but I'll try to run your script soon :)
		</comment>
		<comment id='25' author='PhilipMay' date='2020-09-08T09:22:42Z'>
		
All GPUs are currently occupied for EACL, but I'll try to run your script soon :)

&lt;denchmark-link:https://github.com/stefan-it&gt;@stefan-it&lt;/denchmark-link&gt;
 that would be awsome!
		</comment>
		<comment id='26' author='PhilipMay' date='2020-09-08T09:54:26Z'>
		
Running training, final test set evaluation and saving in one script and then loading and inference in other script the bug does not happen and we have the same F1 macro on test set on both runs.

Ok, so now we have a hook on the mystery. It obviously must be that variables do not get GCed and that affects loading or predictions somehow... I rather believe this is a pytorch GC issue on the computation graph and some variables being not deleted.
		</comment>
		<comment id='27' author='PhilipMay' date='2020-09-08T10:00:42Z'>
		

Running training, final test set evaluation and saving in one script and then loading and inference in other script the bug does not happen and we have the same F1 macro on test set on both runs.

Ok, so now we have a hook on the mystery. It obviously must be that variables do not get GCed and that affects loading or predictions somehow... I rather believe this is a pytorch GC issue on the computation graph and some variables being not deleted.

The strange thing is: Why does it happen with Electra but not with BERT? The most important difference I see is here:
&lt;denchmark-link:https://github.com/deepset-ai/FARM/blob/master/farm/modeling/language_model.py#L1255-L1265&gt;https://github.com/deepset-ai/FARM/blob/master/farm/modeling/language_model.py#L1255-L1265&lt;/denchmark-link&gt;

		</comment>
		<comment id='28' author='PhilipMay' date='2020-09-09T06:46:18Z'>
		

Running training, final test set evaluation and saving in one script and then loading and inference in other script the bug does not happen and we have the same F1 macro on test set on both runs.

Ok, so now we have a hook on the mystery. It obviously must be that variables do not get GCed and that affects loading or predictions somehow... I rather believe this is a pytorch GC issue on the computation graph and some variables being not deleted.

Yes and no...
For this example this is right. But I have a case where I use early stopping. The electra model that is saved by early stopping is even not working correctly when loaded and evaluated in an other script. But maybe that is a 2nd bug...
		</comment>
		<comment id='29' author='PhilipMay' date='2020-09-09T10:36:34Z'>
		I would also separate these cases for simplicity.
How do you want to proceed with the pytorch GC issue?
		</comment>
		<comment id='30' author='PhilipMay' date='2020-09-09T10:40:27Z'>
		
How do you want to proceed with the pytorch GC issue?

Well - to be honest: I hope &lt;denchmark-link:https://github.com/stefan-it&gt;@stefan-it&lt;/denchmark-link&gt;
 checks and fixes it.
I made some experiments with things I thought would be the reason but did not come to a solution. I maybe check the recent torch issues in github. But since it works with BERT but not with Electra I still think it might be a bug in FARM.
		</comment>
		<comment id='31' author='PhilipMay' date='2020-09-09T20:00:49Z'>
		On it...
		</comment>
		<comment id='32' author='PhilipMay' date='2020-09-09T21:10:57Z'>
		It's very strange, because the root cause is the language_model = LanguageModel.load(lang_model) call. And it's working for DistilBERT (that also uses the SequenceSummary pooling stuff)...
		</comment>
		<comment id='33' author='PhilipMay' date='2020-09-10T10:10:39Z'>
		Yes. I tested it with the german Distilbert. And it was all ok. No Bug.
		</comment>
		<comment id='34' author='PhilipMay' date='2020-09-12T16:03:41Z'>
		Are there any new insights in this topic?
&lt;denchmark-link:https://github.com/stefan-it&gt;@stefan-it&lt;/denchmark-link&gt;
 would it help to have a look at transformers and how they added the head on top of electra?
&lt;denchmark-link:https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_electra.py#L424&gt;https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_electra.py#L424&lt;/denchmark-link&gt;

		</comment>
		<comment id='35' author='PhilipMay' date='2020-10-02T16:45:15Z'>
		Hi, just wanted to bunp this. Any idea how to continue with this?
Thanks
Philip
		</comment>
		<comment id='36' author='PhilipMay' date='2020-10-04T11:11:55Z'>
		Since we (stefan-it and deepset) will be releasing our joint German Electra model soon we will then also look into this issue.
We will most likely be able to include checking this issue in our next sprint.
		</comment>
		<comment id='37' author='PhilipMay' date='2020-10-21T08:22:18Z'>
		We are boiling down this strange model re loading behaviour. Our next steps will be to determine when the error happens. Then we will deep dive into why:
&lt;denchmark-code&gt;â€¢ Producing bug
    â—¦ Single script training and loading again
    â—¦ Single script model loading twice without training
    â—¦ CPU vs CUDA
    â—¦ Inferencer vs Evaluator (Evaluator also needs to be loaded from disk as in our Trainner.train fct during early stopping)
â€¢ Not producing bug (sanity check)
    â—¦ Separate scripts training + saving + loading
â€¢ What is going wrong in weight initialization
    â—¦ Is LM initialized wrong
    â—¦ Or just PH
    â—¦ Or both?
&lt;/denchmark-code&gt;

		</comment>
		<comment id='38' author='PhilipMay' date='2020-10-21T12:06:29Z'>
		Great &lt;denchmark-link:https://github.com/Timoeller&gt;@Timoeller&lt;/denchmark-link&gt;
 ! Many thanks.
		</comment>
	</comments>
</bug>