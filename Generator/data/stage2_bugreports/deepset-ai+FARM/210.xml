<bug id='210' author='tholor' open_date='2020-01-23T15:40:32Z' closed_time='2020-01-23T17:06:41Z'>
	<summary>AMP mode "O0" not working on multi GPUs due to DataParallel</summary>
	<description>

Training with automatix mixed precision is crashing in the very first forward pass of the model.
Parameters are not all on the same device. This seems to be related to recent changes on modifying the model dimensions after converting the model for amp (e.g. &lt;denchmark-link:https://github.com/deepset-ai/FARM/pull/195&gt;#195&lt;/denchmark-link&gt;
) .
Error message
&lt;denchmark-code&gt;Train epoch 1/1 (Cur. train loss: 0.0000):   0%|        | 0/141 [00:00&lt;?, ?it/s]Traceback (most recent call last):
  File "/home/ubuntu/pycharm/FARM/examples/doc_classification.py", line 118, in &lt;module&gt;
    doc_classifcation()
  File "/home/ubuntu/pycharm/FARM/examples/doc_classification.py", line 100, in doc_classifcation
    trainer.train()
  File "/home/ubuntu/pycharm/FARM/farm/train.py", line 409, in train
    logits = self.model.forward(**batch)
  File "/home/ubuntu/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 152, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/ubuntu/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 162, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/ubuntu/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 85, in parallel_apply
    output.reraise()
  File "/home/ubuntu/miniconda3/envs/py37/lib/python3.7/site-packages/torch/_utils.py", line 394, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in replica 1 on device 1.
Original Traceback (most recent call last):
  File "/home/ubuntu/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 60, in _worker
    output = module(*input, **kwargs)
  File "/home/ubuntu/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/ubuntu/miniconda3/envs/py37/lib/python3.7/site-packages/apex/amp/_initialize.py", line 197, in new_fwd
    **applier(kwargs, input_caster))
  File "/home/ubuntu/pycharm/FARM/farm/modeling/adaptive_model.py", line 222, in forward
    **kwargs, output_all_encoded_layers=False
  File "/home/ubuntu/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/ubuntu/pycharm/FARM/farm/modeling/language_model.py", line 355, in forward
    attention_mask=padding_mask,
  File "/home/ubuntu/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/ubuntu/miniconda3/envs/py37/lib/python3.7/site-packages/transformers/modeling_bert.py", line 735, in forward
    embedding_output = self.embeddings(input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds)
  File "/home/ubuntu/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/ubuntu/miniconda3/envs/py37/lib/python3.7/site-packages/transformers/modeling_bert.py", line 186, in forward
    inputs_embeds = self.word_embeddings(input_ids)
  File "/home/ubuntu/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/ubuntu/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/sparse.py", line 114, in forward
    self.norm_type, self.scale_grad_by_freq, self.sparse)
  File "/home/ubuntu/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/functional.py", line 1484, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: arguments are located on different GPUs at /opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/THC/generic/THCTensorIndex.cu:400
&lt;/denchmark-code&gt;

To Reproduce
Run examples/doc_classification.py with use_amp= "O0"
System:

OS: Ubuntu 18.04
GPU/CPU: 4 x v100
FARM version: latest master e6224bd

	</description>
	<comments>
		<comment id='1' author='tholor' date='2020-01-23T17:06:31Z'>
		Ok, narrowed it down. It actually only seems to be a problem with the combination of amp level "O0" and multi GPU. "O1" works like a charm. This seems to be a known issue in apex (&lt;denchmark-link:https://github.com/NVIDIA/apex/issues/503&gt;NVIDIA/apex#503&lt;/denchmark-link&gt;
). As mode "O0" is regular fp32 (for which we don't really need to use apex) this bug is not really essential to us. However, we should keep an eye on if it gets fixed upstream within apex.
		</comment>
	</comments>
</bug>