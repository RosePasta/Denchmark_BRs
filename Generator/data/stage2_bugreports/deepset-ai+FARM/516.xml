<bug id='516' author='tholor' open_date='2020-09-04T09:57:54Z' closed_time='2020-12-02T14:11:32Z'>
	<summary>FastTokenizer gives different output for sequence starting with a subword token</summary>
	<description>
Describe the bug
In QA we split long texts into multiple passages with a sliding window after tokenization.
This means a long word with multiple tokens might be splitted across two passages (start of word in passage 1, rest in passage 2). For these cases, the current implementation of fast tokenizer will produce different results than the slow tokenizer.
&lt;denchmark-code&gt;FastTokenizer:          ['When', 'were', 'the', 'first', 'traces', 'of', 'Human', 'life', 'found', 'in', 'France', '?', 'trained', '-', 'be', '##rt', ')', 'provides', 'general', '-', 'purpose', 'architecture', '##s', '(', 'B', '##ER', '##T', ',', 'GP', '##T', '-', '2', ',', 'R', '##o', '##BE', '##RT', '##a', ',', 'X', '##LM', ',', 'Di', '##st', '##il', '##B', '##ert', ',', 'X', '##L', '##Net', '…', ')', 'for', 'Natural', 'Language', 'Understanding', '(', 'NL', '##U', ')', 'and', 'Natural', 'Language', 'Generation', '(', 'NL', '##G', ')', 'with', 'over', '32', '+', 'pre', '##tra', '##ined', 'models', 'in', '100', '+', 'languages', 'and', 'deep', 'inter', '##oper', '##ability', 'between', 'Ten', '##sor', '##F', '##low', '2', '.', '0', 'and', 'P', '##y', '##T', '##or', '##ch', '.', 'Transformers', '(', 'formerly', 'known', 'as', 'p', '##yt', '##or', '##ch', '-', 'transform', '##ers', 'and', 'p', '##yt', '##or', '##ch', '-', 'pre', '##tra', '##ined', '-', 'be', '##rt', ')', 'provides', 'general', '-', 'purpose', 'architecture', '##s', '(', 'B', '##ER', '##T', ',', 'GP', '##T', '-', '2', ',', 'R', '##o', '##BE', '##RT', '##a', ',', 'X', '##LM', ',', 'Di', '##st', '##il', '##B', '##ert', ',', 'X', '##L', '##Net', '…', ')', 'for', 'Natural', 'Language', 'Understanding', '(', 'NL', '##U', ')', 'and', 'Natural', 'Language', 'Generation', '(', 'NL', '##G', ')', 'with', 'over', '32', '+', 'pre', '##tra', '##ined', 'models', 'in', '100', '+', 'languages', 'and', 'deep', 'inter', '##oper', '##ability', 'between', 'Ten', '##sor', '##F', '##low', '2', '.', '0', 'and', 'P', '##y', '##T', '##or', '##ch', '.', 'Transformers', '(', 'formerly', 'known', 'as', 'p', '##yt', '##or', '##ch', '-', 'transform', '##ers', 'and', 'p', '##yt', '##or', '##ch', '-', 'pre', '##tra', '##ined', '-', 'be', '##rt', ')', 'provides', 'general', '-', 'purpose', 'architecture', '##s', '(', 'B', '##ER', '##T', ',', 'GP', '##T', '-', '2', ',', 'R', '##o', '##BE', '##RT', '##a', ',', 'X', '##LM', ',', 'Di', '##st', '##il', '##B', '##ert', ',', 'X', '##L', '##Net', '…', ')', 'for', 'Natural', 'Language', 'Understanding', '(', 'NL', '##U', ')', 'and', 'Natural', 'Language', 'Generation', '(', 'NL', '##G', ')', 'with', 'over', '32', '+', 'pre', '##tra', '##ined', 'models', 'in', '100', '+', 'languages', 'and', 'deep', 'inter', '##oper', '##ability', 'between', 'Ten', '##sor', '##F', '##low', '2', '.', '0', 'and', 'P', '##y', '##T', '##or', '##ch', '.', 'Transformers', '(', 'formerly', 'known', 'as', 'p', '##yt', '##or', '##ch', '-', 'transform', '##ers', 'and', 'p', '##yt', '##or', '##ch', '-', 'pre', '##tra', '##ined', '-', 'be', '##rt', ')', 'provides', 'general', '-', 'purpose', 'architecture', '##s', '(', 'B', '##ER', '##T', ',', 'GP', '##T', '-', '2', ',', 'R', '##o', '##BE', '##RT', '##a', ',', 'X', '##LM', ',', 'Di', '##st', '##il', '##B', '##ert', ',', 'X', '##L', '##Net', '…', ')'] 
tokenize_with_metadata: ['When', 'were', 'the', 'first', 'traces', 'of', 'Human', 'life', 'found', 'in', 'France', '?', '##tra', '##ined', '-', 'be', '##rt', ')', 'provides', 'general', '-', 'purpose', 'architecture', '##s', '(', 'B', '##ER', '##T', ',', 'GP', '##T', '-', '2', ',', 'R', '##o', '##BE', '##RT', '##a', ',', 'X', '##LM', ',', 'Di', '##st', '##il', '##B', '##ert', ',', 'X', '##L', '##Net', '…', ')', 'for', 'Natural', 'Language', 'Understanding', '(', 'NL', '##U', ')', 'and', 'Natural', 'Language', 'Generation', '(', 'NL', '##G', ')', 'with', 'over', '32', '+', 'pre', '##tra', '##ined', 'models', 'in', '100', '+', 'languages', 'and', 'deep', 'inter', '##oper', '##ability', 'between', 'Ten', '##sor', '##F', '##low', '2', '.', '0', 'and', 'P', '##y', '##T', '##or', '##ch', '.', 'Transformers', '(', 'formerly', 'known', 'as', 'p', '##yt', '##or', '##ch', '-', 'transform', '##ers', 'and', 'p', '##yt', '##or', '##ch', '-', 'pre', '##tra', '##ined', '-', 'be', '##rt', ')', 'provides', 'general', '-', 'purpose', 'architecture', '##s', '(', 'B', '##ER', '##T', ',', 'GP', '##T', '-', '2', ',', 'R', '##o', '##BE', '##RT', '##a', ',', 'X', '##LM', ',', 'Di', '##st', '##il', '##B', '##ert', ',', 'X', '##L', '##Net', '…', ')', 'for', 'Natural', 'Language', 'Understanding', '(', 'NL', '##U', ')', 'and', 'Natural', 'Language', 'Generation', '(', 'NL', '##G', ')', 'with', 'over', '32', '+', 'pre', '##tra', '##ined', 'models', 'in', '100', '+', 'languages', 'and', 'deep', 'inter', '##oper', '##ability', 'between', 'Ten', '##sor', '##F', '##low', '2', '.', '0', 'and', 'P', '##y', '##T', '##or', '##ch', '.', 'Transformers', '(', 'formerly', 'known', 'as', 'p', '##yt', '##or', '##ch', '-', 'transform', '##ers', 'and', 'p', '##yt', '##or', '##ch', '-', 'pre', '##tra', '##ined', '-', 'be', '##rt', ')', 'provides', 'general', '-', 'purpose', 'architecture', '##s', '(', 'B', '##ER', '##T', ',', 'GP', '##T', '-', '2', ',', 'R', '##o', '##BE', '##RT', '##a', ',', 'X', '##LM', ',', 'Di', '##st', '##il', '##B', '##ert', ',', 'X', '##L', '##Net', '…', ')', 'for', 'Natural', 'Language', 'Understanding', '(', 'NL', '##U', ')', 'and', 'Natural', 'Language', 'Generation', '(', 'NL', '##G', ')', 'with', 'over', '32', '+', 'pre', '##tra', '##ined', 'models', 'in', '100', '+', 'languages', 'and', 'deep', 'inter', '##oper', '##ability', 'between', 'Ten', '##sor', '##F', '##low', '2', '.', '0', 'and', 'P', '##y', '##T', '##or', '##ch', '.', 'Transformers', '(', 'formerly', 'known', 'as', 'p', '##yt', '##or', '##ch', '-', 'transform', '##ers', 'and', 'p', '##yt', '##or', '##ch', '-', 'pre', '##tra', '##ined', '-', 'be', '##rt', ')', 'provides', 'general', '-', 'purpose', 'architecture', '##s', '(', 'B', '##ER', '##T', ',', 'GP', '##T', '-', '2', ',', 'R', '##o', '##BE', '##RT', '##a', ',', 'X', '##LM', ',', 'Di', '##st', '##il', '##B', '##ert', ',', 'X', '##L', '##Net', '…', ')']

&lt;/denchmark-code&gt;

Expected behavior
Same tokens / input_ids for Slow and fast tokenizers
Additional context
We could either change the way we split passages or adjust the FastTokenizer processing in sample_to_features_qa()
System:

OS: Ubuntu 18.04
GPU/CPU: CPU
FARM version: latest master (33afbd3)

	</description>
	<comments>
		<comment id='1' author='tholor' date='2020-12-02T14:11:31Z'>
		Will be resolved in &lt;denchmark-link:https://github.com/deepset-ai/FARM/pull/624&gt;#624&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>