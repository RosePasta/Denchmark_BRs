<bug id='385' author='PhilipMay' open_date='2020-05-30T13:12:45Z' closed_time='2020-07-27T10:02:52Z'>
	<summary>CI Pipeline seems to be broken</summary>
	<description>
For some reason the CI errors out. See here:
&lt;denchmark-link:https://dev.azure.com/deepset/FARM/_build/results?buildId=285&amp;view=logs&amp;j=5f32d555-91de-5b47-aa96-4f80d9326a6b&amp;t=813bfe30-cb15-55d3-e33c-c7c8b5f23c95&amp;l=81&gt;https://dev.azure.com/deepset/FARM/_build/results?buildId=285&amp;view=logs&amp;j=5f32d555-91de-5b47-aa96-4f80d9326a6b&amp;t=813bfe30-cb15-55d3-e33c-c7c8b5f23c95&amp;l=81&lt;/denchmark-link&gt;

This is from the last commit: &lt;denchmark-link:https://github.com/deepset-ai/FARM/commit/258a76fface7652248530bdd9c841d6022c89af5&gt;258a76f&lt;/denchmark-link&gt;

In the PR (before merging) the CI was all green: &lt;denchmark-link:https://github.com/deepset-ai/FARM/pull/382&gt;#382&lt;/denchmark-link&gt;

What is the reason here?
	</description>
	<comments>
		<comment id='1' author='PhilipMay' date='2020-05-31T14:36:41Z'>
		So the error:  has to do with memory issues. We recently changed from Travis CI to Azure pipelines because we thought this fixes OOM issues. &lt;denchmark-link:https://github.com/tanaysoni&gt;@tanaysoni&lt;/denchmark-link&gt;
 could you please have a look at it?
		</comment>
		<comment id='2' author='PhilipMay' date='2020-05-31T19:44:18Z'>
		We should reduce the memory usage of the test_question_answering.py  test. Maybe use a smaller model then distilbert-base-uncased?
		</comment>
		<comment id='3' author='PhilipMay' date='2020-06-01T14:10:56Z'>
		Fixed by: &lt;denchmark-link:https://github.com/deepset-ai/FARM/commit/32fe61ba17870e0f43b42b3e3d179dfa8fb4d910&gt;32fe61b&lt;/denchmark-link&gt;

Closing this.
		</comment>
		<comment id='4' author='PhilipMay' date='2020-06-01T18:47:58Z'>
		Well, there still seems to be a problem which does not always happen but just from time to time. See here: &lt;denchmark-link:https://dev.azure.com/deepset/FARM/_build/results?buildId=303&amp;view=logs&amp;j=5f32d555-91de-5b47-aa96-4f80d9326a6b&amp;t=813bfe30-cb15-55d3-e33c-c7c8b5f23c95&amp;l=87&gt;https://dev.azure.com/deepset/FARM/_build/results?buildId=303&amp;view=logs&amp;j=5f32d555-91de-5b47-aa96-4f80d9326a6b&amp;t=813bfe30-cb15-55d3-e33c-c7c8b5f23c95&amp;l=87&lt;/denchmark-link&gt;

This happened after I rebased master and got the Update to pytorch 1.5.0.
Could you please investigate this?
I will reopen this. Issue.
Thanks
Philip
		</comment>
		<comment id='5' author='PhilipMay' date='2020-06-02T05:32:26Z'>
		More strange things happening:

test_ner.py crash: https://dev.azure.com/deepset/FARM/_build/results?buildId=311&amp;view=logs&amp;j=5f32d555-91de-5b47-aa96-4f80d9326a6b&amp;t=813bfe30-cb15-55d3-e33c-c7c8b5f23c95&amp;l=81
complete f*up: https://github.com/deepset-ai/FARM/runs/728689459

		</comment>
		<comment id='6' author='PhilipMay' date='2020-06-02T09:50:56Z'>
		:D jap, confirming there is no such thing as a free lunch and our azure CI pipeline is on a free plan for now...
I spoke to our engineer and the good news is the CI issues - especially the OOM issue - popped up a while ago and might not be related to our PRs. Fixing these issues properly will require some more time on our end.
		</comment>
		<comment id='7' author='PhilipMay' date='2020-06-02T15:49:51Z'>
		Do you see any possibility to add more debug code so we can see in detail what is happening? Do we have memory leaks somewhere? PyTorch, Huggingfaces, ...
Are we even sure if it is an OOM problem?
		</comment>
		<comment id='8' author='PhilipMay' date='2020-06-05T14:05:00Z'>
		Any news on this topic? Did you come to new insights? Is there anything I can help with?
		</comment>
		<comment id='9' author='PhilipMay' date='2020-06-06T16:31:49Z'>
		The test_lm_finetuning.py test(-s) seem to spawn many threads (subprocesses in this case to be exact) (see screenshot).
&lt;denchmark-link:https://user-images.githubusercontent.com/229382/83949403-de974a00-a823-11ea-8351-d1fd17615552.png&gt;&lt;/denchmark-link&gt;

Each subprocesses seems to use 3 GB ram or more. Might that be part of the problem? Could it help to reduce the number of subprocesses?
Edit: Is is test_inference.py that spawns the subprocesses before test_lm_finetuning.py is executed.
		</comment>
		<comment id='10' author='PhilipMay' date='2020-06-06T18:00:29Z'>
		I observed that when test_inference.py is executed it spawns several subprocesses using about 3 GB memory. When executing all tests (with just pytest) the spawned subprocesses are not stopped after test_inference.py has been executed. That is strange.
I think the @pytest.mark.parametrize("num_processes", [2, 0, None], scope="session") command in  somehow the reason.
My guess is that somewhere in
    def _inference_with_multiprocessing(
        self, dicts, return_json, aggregate_preds, multiprocessing_chunksize
    ):
&lt;denchmark-link:https://github.com/deepset-ai/FARM/blob/master/farm/infer.py#L423&gt;https://github.com/deepset-ai/FARM/blob/master/farm/infer.py#L423&lt;/denchmark-link&gt;

is a bug that does not close opened subprocesses. Might that be the case?
		</comment>
		<comment id='11' author='PhilipMay' date='2020-06-06T18:14:42Z'>
		
Warning
multiprocessing.pool objects have internal resources that need to be properly managed (like any other resource) by using the pool as a context manager or by calling close() and terminate() manually. Failure to do this can lead to the process hanging on finalization.
Note that is not correct to rely on the garbage colletor to destroy the pool as CPython does not assure that the finalizer of the pool will be called (see object.del() for more information).

See here: &lt;denchmark-link:https://docs.python.org/3.7/library/multiprocessing.html#module-multiprocessing.pool&gt;https://docs.python.org/3.7/library/multiprocessing.html#module-multiprocessing.pool&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='PhilipMay' date='2020-06-06T19:11:45Z'>
		Here is the PR that fixes the CI problems by fixing the unclosed worker pool problem which was found above: &lt;denchmark-link:https://github.com/deepset-ai/FARM/pull/403&gt;#403&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>