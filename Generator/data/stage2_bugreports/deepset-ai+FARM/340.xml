<bug id='340' author='malteos' open_date='2020-04-28T13:02:00Z' closed_time='2020-04-28T14:04:53Z'>
	<summary>Cannot use F1-micro for multi-label classification</summary>
	<description>
Describe the bug
Using F1-micro as metric for a multi-label classification causes the Evaluator to throw an error.
Task
&lt;denchmark-code&gt;processor.add_task(name='text_classification', 
                   metric='acc_f1',
                   label_list=label_list, 
                   label_column_name=label_col,
                   task_type='multilabel_classification',
                  )
&lt;/denchmark-code&gt;

Error message
&lt;denchmark-code&gt;ValueError                                Traceback (most recent call last)
&lt;ipython-input-20-6e6854edb652&gt; in &lt;module&gt;
      6 
      7 evaluator_test = Evaluator(data_loader=test_data_loader, tasks=processor.tasks, device=device)
----&gt; 8 result = evaluator_test.eval(model)
      9 
     10 evaluator_test.log_results(result, "Test", 0)

.../lib/python3.7/site-packages/farm/eval.py in eval(self, model, return_preds_and_labels)
    101                       "task_name": head.task_name}
    102             result.update(
--&gt; 103                 compute_metrics(metric=head.metric, preds=preds_all[head_num], labels=label_all[head_num]
    104                 )
    105             )

.../lib/python3.7/site-packages/farm/evaluation/metrics.py in compute_metrics(metric, preds, labels)
     51         return simple_accuracy(preds, labels)
     52     elif metric == "acc_f1":
---&gt; 53         return acc_and_f1(preds, labels)
     54     elif metric == "pear_spear":
     55         return pearson_and_spearman(preds, labels)

..lib/python3.7/site-packages/farm/evaluation/metrics.py in acc_and_f1(preds, labels)
     27 def acc_and_f1(preds, labels):
     28     acc = simple_accuracy(preds, labels)
---&gt; 29     f1 = f1_score(y_true=labels, y_pred=preds)
     30     return {"acc": acc, "f1": f1, "acc_and_f1": (acc + f1) / 2}
     31 

../lib/python3.7/site-packages/sklearn/metrics/classification.py in f1_score(y_true, y_pred, labels, pos_label, average, sample_weight)
   1057     return fbeta_score(y_true, y_pred, 1, labels=labels,
   1058                        pos_label=pos_label, average=average,
-&gt; 1059                        sample_weight=sample_weight)
   1060 
   1061 

../lib/python3.7/site-packages/sklearn/metrics/classification.py in fbeta_score(y_true, y_pred, beta, labels, pos_label, average, sample_weight)
   1180                                                  average=average,
   1181                                                  warn_for=('f-score',),
-&gt; 1182                                                  sample_weight=sample_weight)
   1183     return f
   1184 

../lib/python3.7/site-packages/sklearn/metrics/classification.py in precision_recall_fscore_support(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)
   1413         raise ValueError("beta should be &gt;0 in the F-beta score")
   1414     labels = _check_set_wise_labels(y_true, y_pred, average, labels,
-&gt; 1415                                     pos_label)
   1416 
   1417     # Calculate tp_sum, pred_sum, true_sum ###

../lib/python3.7/site-packages/sklearn/metrics/classification.py in _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
   1252             raise ValueError("Target is %s but average='binary'. Please "
   1253                              "choose another average setting, one of %r."
-&gt; 1254                              % (y_type, average_options))
   1255     elif pos_label not in (None, 1):
   1256         warnings.warn("Note that pos_label (set to %r) is ignored when "

ValueError: Target is multilabel-indicator but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted', 'samples'].
&lt;/denchmark-code&gt;

My best guess is that the average argument must be set to micro: 


FARM/farm/evaluation/metrics.py


         Line 57
      in
      5398d6f






 f1 = f1_score(y_true=labels, y_pred=preds) 





System:

OS: Ubuntu
GPU/CPU: GPU
FARM version: 0.4.2

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

PS: The inconsistent naming of acc_and_f1 and acc_f1 is a bit confusing.
	</description>
	<comments>
		<comment id='1' author='malteos' date='2020-04-28T13:29:03Z'>
		Hey &lt;denchmark-link:https://github.com/malteos&gt;@malteos&lt;/denchmark-link&gt;
 ,
Yep, I think you are right: the mentioned metric will currently only work for binary classification.
Do you want to create a PR for that?
As an alternative, you can also easily define your own metric in FARM:
&lt;denchmark-code&gt;    from farm.evaluation.metrics import register_metrics
    from sklearn.metrics import f1_score

    def micro_f1(preds, labels):
        f1 = f1_score(y_true=labels, y_pred=preds, average="micro")
        return {"f1": f1}

    register_metrics(name="my_metric", implementation=micro_f1)

    processor = TextClassificationProcessor(..., metric="my_metric", ...)
&lt;/denchmark-code&gt;

Maybe that's an option for you? It would allow you quite some more customization as you can pass in any function as long as it takes preds+labels and returns a  dict with the metric(s) .
		</comment>
		<comment id='2' author='malteos' date='2020-04-28T14:04:53Z'>
		Thanks for the quick reply!
A PR is currently out of scope but maybe later when I'm more familiar with FARM. Your proposed solution will probably work for now.
		</comment>
	</comments>
</bug>