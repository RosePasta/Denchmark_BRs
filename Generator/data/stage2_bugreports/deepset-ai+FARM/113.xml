<bug id='113' author='tholor' open_date='2019-10-11T15:31:43Z' closed_time='2019-10-14T17:11:19Z'>
	<summary>Empty datasets due to random_split_ConcatDataset</summary>
	<description>
Describe the bug
DataSilo is Crashing during the attempt of splitting a dev set from a "small" train set.
It seems that random_split_ConcatDataset() doesn't work if there's only a single chunk (= 1 dataset). idx_dataset is 0 in that case and thus creates an empty ConcatDataset for train
Error message
Error that was thrown (if available)
&lt;denchmark-code&gt;10/11/2019 17:12:47 - INFO - farm.data_handler.data_silo -   Loading dev set as a slice of train set
Traceback (most recent call last):
  File ".../train.py", line 436, in &lt;module&gt;
    augmentation=True)
  File ".../train.py", line 348, in continue_finetuning
    data_silo = DataSilo(processor=processor, batch_size=batch_size, multiprocessing_chunk_size=2000)
  File "/.../farm/data_handler/data_silo.py", line 49, in __init__
    self._load_data()
  File ".../farm/data_handler/data_silo.py", line 104, in _load_data
    self._create_dev_from_train()
  File ".../farm/data_handler/data_silo.py", line 175, in _create_dev_from_train
    train_dataset, dev_dataset = self.random_split_ConcatDataset(self.data["train"], lengths=[n_train, n_dev])
  File ".../farm/data_handler/data_silo.py", line 200, in random_split_ConcatDataset
    train = ConcatDataset(ds.datasets[:idx_dataset])
  File ".../torch/utils/data/dataset.py", line 68, in __init__
    assert len(datasets) &gt; 0, 'datasets should not be an empty iterable'
AssertionError: datasets should not be an empty iterable
&lt;/denchmark-code&gt;

Expected behavior
A portion of the train set should be splitted apart into a dev set

Related function:
&lt;denchmark-link:https://github.com/deepset-ai/FARM/blob/master/farm/data_handler/data_silo.py#L186&gt;https://github.com/deepset-ai/FARM/blob/master/farm/data_handler/data_silo.py#L186&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;    def random_split_ConcatDataset(self, ds, lengths):
...
        if sum(lengths) != len(ds):
            raise ValueError("Sum of input lengths does not equal the length of the input dataset!")

        idx_dataset = np.where(np.array(ds.cumulative_sizes) &gt; lengths[0])[0][0]

        train = ConcatDataset(ds.datasets[:idx_dataset])
        test = ConcatDataset(ds.datasets[idx_dataset:])
        return train, test
&lt;/denchmark-code&gt;

To Reproduce

Provide no dev_filename but instead set dev_split
Have a trainfile that has less samples that multiprocessing_chunksize

System:

OS: Ubuntu 18.04
GPU/CPU: CPU
FARM version: 0.2.1

	</description>
	<comments>
		<comment id='1' author='tholor' date='2019-10-14T17:11:19Z'>
		Resolved by &lt;denchmark-link:https://github.com/deepset-ai/FARM/pull/114&gt;#114&lt;/denchmark-link&gt;

Closing this now
		</comment>
	</comments>
</bug>