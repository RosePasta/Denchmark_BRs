<bug id='191' author='Hahny' open_date='2020-01-15T13:19:44Z' closed_time='2020-01-16T08:34:23Z'>
	<summary>Error in lm_finetuning.py on own dataset</summary>
	<description>
Hello and thanks for this wonderful package :)
When i try to customize a bert model (following examples/lm_finetuning.py) with my own dataset i get an error during loading the data into the Data Silo.
&lt;denchmark-code&gt;tokenizer = Tokenizer.load(
    pretrained_model_name_or_path="bert-base-german-cased", 
    do_lower_case=False
)

processor = BertStyleLMProcessor(
    data_dir="data/additional_tweets/bert_tweet_corpus", 
    tokenizer=tokenizer, 
    max_seq_len=128, 
    train_filename="test.txt",
    dev_filename="test.txt",
    test_filename="test.txt",
    next_sent_pred=False,
    max_docs=None
)

data_silo = DataSilo(processor=processor, 
                     batch_size=8, 
                     max_multiprocessing_chunksize=5)
&lt;/denchmark-code&gt;

Error looks like this:
&lt;denchmark-code&gt;
reprocessing Dataset:  17%|█▋        | 2613/15687 [01:34&lt;09:30, 22.93 Dicts/s]Exception in thread Thread-17:
Traceback (most recent call last):
  File "/usr/lib64/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/usr/lib64/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib64/python3.6/multiprocessing/pool.py", line 463, in _handle_results
    task = get()
  File "/usr/lib64/python3.6/multiprocessing/connection.py", line 251, in recv
    return _ForkingPickler.loads(buf.getbuffer())
  File "/home/ec2-user/env/lib64/python3.6/dist-packages/torch/multiprocessing/reductions.py", line 294, in rebuild_storage_fd
    fd = df.detach()
  File "/usr/lib64/python3.6/multiprocessing/resource_sharer.py", line 58, in detach
    return reduction.recv_handle(conn)
  File "/usr/lib64/python3.6/multiprocessing/reduction.py", line 182, in recv_handle
    return recvfds(s, 1)[0]
  File "/usr/lib64/python3.6/multiprocessing/reduction.py", line 161, in recvfds
    len(ancdata))
RuntimeError: received 0 items of ancdata
&lt;/denchmark-code&gt;

The error seems to appear when i increase the data size (eg. 40000 tweets in blocks of 4)
The datafile looks like this (one tweet per row, "\n" between documents):
lesenswert tomasz konicz zu hintergründen der us sanktionen gg . russland und nord stream
 korruption ist es nur nur wenn geld geflossen ist . aber eine regierung mit einer stimmen
  das ist sehr nett aber so schlimm steht es garnicht um mich . war eher ein kleiner
mitschnitt eines radiointerviews zu den fibronil funden in eiern . 
 es währe nicht schlecht , wenn mal weniger autos auf der straße rum fahren würden .
 konstruktiv wäre , wenn wir als de das geld in sinnvolle aufbauhilfe in den ländern
 wir werden alle kollektiv unserem recht auf saubere luft beraubt und sie reden über
zu aktuellen north stream  debatte   
	</description>
	<comments>
		<comment id='1' author='Hahny' date='2020-01-15T13:56:52Z'>
		Hi &lt;denchmark-link:https://github.com/Hahny&gt;@Hahny&lt;/denchmark-link&gt;
, thank you for raising the issue!
I suspect the error is due to PyTorch keeping open a large number of file descriptors.
Could you share more details of the run-time environment, especially number of CPU cores and the output of ulimit -Hn?
If indeed the file descriptor limit is the issue, you can try changing the CUDA tensors sharing strategy to . You can find more documentation about it here: &lt;denchmark-link:https://pytorch.org/docs/stable/multiprocessing.html#sharing-strategies&gt;https://pytorch.org/docs/stable/multiprocessing.html#sharing-strategies&lt;/denchmark-link&gt;

To do that in FARM, replace the following code



FARM/farm/__init__.py


        Lines 15 to 22
      in
      1d30237






 if "file_descriptor" in mp.get_all_sharing_strategies(): 



 import resource 



 



 mp.set_sharing_strategy("file_descriptor") 



 



 rlimit = resource.getrlimit(resource.RLIMIT_NOFILE) 



 # seting soft limit to hard limit (=rlimit[1]) minus a small amount to be safe 



 resource.setrlimit(resource.RLIMIT_NOFILE, (rlimit[1]-512, rlimit[1])) 





with:
if "file_system" in mp.get_all_sharing_strategies():
    mp.set_sharing_strategy("file_system")
		</comment>
		<comment id='2' author='Hahny' date='2020-01-16T08:34:15Z'>
		Thanks for the fast response. I resolved the issue by increasing ax_multiprocessing_chunksize to the default parameter. Anyway, this error seems not related to FARM. But thanks for your help i think your proposed solution will work too.
		</comment>
	</comments>
</bug>