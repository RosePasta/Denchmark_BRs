<bug id='526' author='PhilipMay' open_date='2020-09-07T07:31:56Z' closed_time='2020-10-04T11:15:14Z'>
	<summary>Batch size ignored in Inferencer</summary>
	<description>
I tried to speed optimze the Inferencer. While doing that I noticed that this works without problem:
    model = Inferencer.load(model_path,
                        num_processes=4,
                        batch_size=1_000_000,
                        gpu=True,
                        )
Which is very strange...
Could you please have a look into this? Might be a bug?
Maybe you could explain the effect of:

num_processes in Inferencer.load
batch_size in Inferencer.load
multiprocessing_chunksize in model.inference_from_dicts

	</description>
	<comments>
		<comment id='1' author='PhilipMay' date='2020-09-07T08:53:15Z'>
		batch size behavior is unlikely a bug in FARM.
We observed non obvious ram consumption and running times with increasing batch sizes in pytorch.
To test FARM components you could set a breakpoint where the inferencer feeds in a batch and see how large the input is.
Num_processes ist related to preprocessing, the number of processes used.
mp_chunksize is the datasize for each of the processes. This datasize is highly dependent on your data and the processing involved, so it is difficult to automatically determine the optimal value. It should not be too large but also not too small. The set value should be fine in most cases.
		</comment>
		<comment id='2' author='PhilipMay' date='2020-09-18T08:43:52Z'>
		The docu sais: :param batch_size: Number of samples computed once per batch
See 


FARM/farm/infer.py


         Line 185
      in
      3685035






         :param batch_size: Number of samples computed once per batch 





&lt;denchmark-link:https://github.com/Timoeller&gt;@Timoeller&lt;/denchmark-link&gt;
 How can it be that my GPU can compute 1 million samples in one batch? Must be a bug in code or documentation.
I will have a look on it.
		</comment>
		<comment id='3' author='PhilipMay' date='2020-09-18T09:47:27Z'>
		I agree. Increasing batch size should lead to increased memory usage and than to OOM, so something is not straight forward.
I did a bit of testing myself. When using the Evaluator with a very large batch size everything is put into one batch and if there is enough data we get a cuda OOM.
When using the inferencer and multiprocessing the data is split into chunks. These chunks limit what can be put into one batch, so even if batch size is 1 million there is not enough data for it. If you disable mp you also OOM in the inferencer.
Why increasing batch sizes do not lead to improved speed (across a certain batch size threshold) has to do that data transfer is the limiting factor in GPUs, not compute. So that makes sense, too.
		</comment>
		<comment id='4' author='PhilipMay' date='2020-09-18T09:59:13Z'>
		Awsome. Thanks for the analysis &lt;denchmark-link:https://github.com/Timoeller&gt;@Timoeller&lt;/denchmark-link&gt;

So when using the inferencer and multiprocessing the batch size is effectually the same as num_processes if num_processes &lt; batch_size  ?
		</comment>
		<comment id='5' author='PhilipMay' date='2020-09-18T10:51:52Z'>
		no, the data is split into chunks that are related to num_processes.
in chunk one there is 312 samples, so the batch size is filled up to 312
in second chunk there might be 354 samples, so batch size is filled up to 354
if you would have enough data in one chunk it would be filled up until it hits the batch size.
		</comment>
		<comment id='6' author='PhilipMay' date='2020-10-04T11:15:14Z'>
		Seems resolved, closing now. Feel free to reopen.
		</comment>
	</comments>
</bug>