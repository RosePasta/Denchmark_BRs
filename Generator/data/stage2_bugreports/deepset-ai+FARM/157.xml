<bug id='157' author='tholor' open_date='2019-11-20T17:34:10Z' closed_time='2020-06-20T02:30:14Z'>
	<summary>Slow Tokenizer if custom vocab was added</summary>
	<description>
Describe the bug
Tokenizer becomes very slow with large custom vocab.
Additional context
This was introduced after switching to the Tokenizers from the transformers repo
There are related issues reported in the transformers repo:

huggingface/transformers#1830
huggingface/transformers#1621
huggingface/transformers#1881

To Reproduce

Add custom vocab to tokenizer via tokenizer.add_tokens()
Load some data into the data silo, e.g. run examples/lm_finetuning.py

System:

OS: Ubuntu 18.04
GPU/CPU: Both
FARM version: master @ 484d26c

	</description>
	<comments>
		<comment id='1' author='tholor' date='2020-01-06T10:33:52Z'>
		There are new fast tokenizers for BERT implemented in rust: &lt;denchmark-link:https://github.com/huggingface/transformers/pull/2211&gt;huggingface/transformers#2211&lt;/denchmark-link&gt;

We should have a look if they are compatible and solve the issue with custom vocab here.
		</comment>
		<comment id='2' author='tholor' date='2020-01-13T17:13:05Z'>
		Yes, please check it out and let us know here.
Repo is at &lt;denchmark-link:https://github.com/huggingface/tokenizers&gt;https://github.com/huggingface/tokenizers&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='tholor' date='2020-01-22T08:31:53Z'>
		Hey &lt;denchmark-link:https://github.com/julien-c&gt;@julien-c&lt;/denchmark-link&gt;
,
I finally found some time to test. Seems really promising! Great work!
 Same tokenization behaviour as BertTokenizer (see &lt;denchmark-link:https://github.com/deepset-ai/FARM/blob/8066ee6c718cd0818df2735c10ec67f61c9987b4/test/test_tokenization.py#L92&gt;test&lt;/denchmark-link&gt;
)
 Speed: ~ 7.8 x faster! (Tested via tokenizing SQuAD train set with 42 Mio chars)
 Speed remains the same with custom vocab &lt; 300. Somehow it's about 4x slower for custom vocab = 400 (using add_tokens())
The only blocker for us right now:
 The Tokenizer objects  and are therefore not usable with python's multiprocessing. As we make heavy use of multiprocessing during preprocessing, we can't really use them right now. Seems that others have a similar &lt;denchmark-link:https://github.com/huggingface/tokenizers/issues/87&gt;issue&lt;/denchmark-link&gt;
. Not sure how much of work is needed for fixing this, but for the XLM-R python tokenizer it was a very easy &lt;denchmark-link:https://github.com/huggingface/transformers/pull/2414&gt;fix&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='4' author='tholor' date='2020-01-22T14:57:09Z'>
		Hi &lt;denchmark-link:https://github.com/tholor&gt;@tholor&lt;/denchmark-link&gt;
, mind opening an issue on  too, cross-referencing this one? cc &lt;denchmark-link:https://github.com/n1t0&gt;@n1t0&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='tholor' date='2020-02-27T21:47:10Z'>
		In the add_tokens method, why don't we simply integrate new_tokens into the self.vocab? We are using the following CustomVocabBertTokenizer and it does not slow down when new_tokens are added:
&lt;denchmark-code&gt;from transformers import BertTokenizer, WordpieceTokenizer
from collections import OrderedDict


class CustomVocabBertTokenizer(BertTokenizer):
    def add_tokens(self, new_tokens):
        new_tokens = [token for token in tokens if not (token in self.vocab or token in self.all_special_tokens)]

        self.vocab = OrderedDict([
            *self.vocab.items(),
            *[
                (token, i + len(self.vocab))
                for i, token in enumerate(new_tokens)
            ]
        ])

        self.ids_to_tokens = OrderedDict([(ids, tok) for tok, ids in self.vocab.items()])
        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab, unk_token=self.unk_token)

        return len(new_tokens)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='tholor' date='2020-03-09T13:56:25Z'>
		Hi &lt;denchmark-link:https://github.com/salmanmashayekh&gt;@salmanmashayekh&lt;/denchmark-link&gt;
, sorry for the late reply. I haven't seen your comment until now.
This seems like a simple, scalable workaround. However, I am not sure if it has any unintended side effects (e.g. on saving/loading) in Transformers. Have you investigated the behavior in Transformers?
It could make sense to raise a PR there, since this is something useful to everybody and not only FARM users.
		</comment>
		<comment id='7' author='tholor' date='2020-06-06T01:44:46Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 14 days if no further activity occurs.
		</comment>
	</comments>
</bug>