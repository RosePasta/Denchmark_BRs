<bug id='544' author='Timoeller' open_date='2020-09-16T17:43:35Z' closed_time='2020-10-14T16:50:49Z'>
	<summary>Memory Leak across folds in Cross Validation (for QA)</summary>
	<description>
Describe the bug
Memory leak in cross validation
Error message
Cuda OOM
Additional context
The results variable, more precisely the predictions and labels, are kept between cross validation folds. This result variable can occupy RAM on GPU.
To Reproduce
Run question answering crossvalidation example.
So we should either delete the result variable properly or explicitly collect results to cpu in the Evaluator.
	</description>
	<comments>
		<comment id='1' author='Timoeller' date='2020-10-14T16:16:28Z'>
		I could not reproduce this issue, neither on a single V100 GPU machine nor on a multi GPU machine (4x V100). Maybe you had this issue because of a different PyTorch version or because of the dataset you were using.
		</comment>
		<comment id='2' author='Timoeller' date='2020-10-14T16:50:49Z'>
		Hey &lt;denchmark-link:https://github.com/bogdankostic&gt;@bogdankostic&lt;/denchmark-link&gt;
 thanks for looking into this issue.
I agree the error must come either from my virtualenv setting or a very specific combination of XLM-R large and my QA dataset, which was quite huge.
Anyways, closing this issue for now.
If anyone stumbles upon the same problem here is what I did to fix the memory leak on my end:
When using the Question_answering_crossvalidation example script, I did not aggregate the variables all_results, all_preds and all_labels and deleted the result variable after emptying the cuda cache. May the memory be with you.
		</comment>
	</comments>
</bug>