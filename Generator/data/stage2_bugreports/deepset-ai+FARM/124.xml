<bug id='124' author='neufang' open_date='2019-10-21T10:02:39Z' closed_time='2019-10-22T11:36:26Z'>
	<summary>Segmentation fault when running inference for QA models</summary>
	<description>
Describe the bug
When running examples/question_answering.py. Training finished.
line 107 result = model.inference_from_dicts(dicts=QA_input) throws Segmentation fault.
Error message
10/19/2019 20:57:29 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../saved_models/bert-english-qa-tutorial/language_model.bin 10/19/2019 20:57:30 - INFO - farm.modeling.adaptive_model -   Found files for loading 1 prediction heads 10/19/2019 20:57:30 - INFO - farm.modeling.prediction_head -   Loading prediction head from ../saved_models/bert-english-qa-tutorial/prediction_head_0.bin 10/19/2019 20:57:30 - WARNING - farm.modeling.adaptive_model -   ML logging didn't work: INVALID_PARAMETER_VALUE: Changing param value is not allowed. Param with key='lm_name' was already logged with value='bert-base-cased' for run ID='cd27bbe37acd434fb1792857e8d8da38. Attempted logging new value '../saved_models/bert-english-qa-tutorial'. 10/19/2019 20:57:30 - INFO - pytorch_transformers.tokenization_utils -   Model name '../saved_models/bert-english-qa-tutorial' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '../saved_models/bert-english-qa-tutorial' is a path or url to a directory containing tokenizer files. 10/19/2019 20:57:30 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../saved_models/bert-english-qa-tutorial/added_tokens.json. We won't load it. 10/19/2019 20:57:30 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../saved_models/bert-english-qa-tutorial/special_tokens_map.json. We won't load it. 10/19/2019 20:57:30 - INFO - pytorch_transformers.tokenization_utils -   loading file None 10/19/2019 20:57:30 - INFO - pytorch_transformers.tokenization_utils -   loading file None 10/19/2019 20:57:30 - INFO - pytorch_transformers.tokenization_utils -   loading file ../saved_models/bert-english-qa-tutorial/vocab.txt 10/19/2019 20:57:31 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, 16-bits training: False Segmentation fault
Expected behavior
Inferencer extracts the answer from the paragraph.
Additional context
Inference on text classification and NER works fine.
To Reproduce
Steps to reproduce the behavior
System:

OS: Ubuntu 17.10
GPU/CPU:  Tesla V100-SXM2-16GB
FARM version: 0.2.0

	</description>
	<comments>
		<comment id='1' author='neufang' date='2019-10-21T11:38:28Z'>
		Do you have the same issue with the most recent FARM version (master branch)?
The related test  looks good and doesn't throw this error (&lt;denchmark-link:https://github.com/deepset-ai/FARM/blob/master/test/test_question_answering.py&gt;https://github.com/deepset-ai/FARM/blob/master/test/test_question_answering.py&lt;/denchmark-link&gt;
).
Can you also post the processor_config.json from your stored model? I guess it's in '../saved_models/bert-english-qa-tutorial . It might help to understand possible problems related to the recent name changes from &lt;denchmark-link:https://github.com/deepset-ai/FARM/issues/120&gt;#120&lt;/denchmark-link&gt;

It should somehow look similar to this (created from the test):
&lt;denchmark-code&gt;{
  "baskets": [],
  "data_dir": "samples/qa",
  "dev_filename": "dev-sample.json",
  "dev_split": 0,
  "doc_stride": 128,
  "max_query_length": 64,
  "max_seq_len": 256,
  "ph_output_type": "per_token_squad",
  "target": "classification",
  "tasks": {
    "question_answering": {
      "label_list": [
        "start_token",
        "end_token"
      ],
      "metric": "squad",
      "label_tensor_name": "question_answering_label_ids",
      "label_name": "question_answering_label",
      "label_column_name": null,
      "task_type": null
    }
  },
  "test_filename": null,
  "train_filename": "train-sample.json",
  "tokenizer": "BertTokenizer",
  "lower_case": false,
  "never_split_chars": null,
  "processor": "SquadProcessor"
}
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='neufang' date='2019-10-22T11:36:09Z'>
		Hi, the latest version 0.2.2 works fine without Segmentation fault.
		</comment>
		<comment id='3' author='neufang' date='2019-10-22T11:48:17Z'>
		Hey &lt;denchmark-link:https://github.com/neufang&gt;@neufang&lt;/denchmark-link&gt;

Since we are currently improving the QA part in FARM (also the inference) I wanted to attached to this conversation.
Right now, the current version wont be correct when working with long documents. There will be correct model predictions but projecting them back to strings will break.
Please wait for the next release, if you plan to use the Inferencer on long documents.
		</comment>
	</comments>
</bug>