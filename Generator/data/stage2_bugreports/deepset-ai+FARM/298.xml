<bug id='298' author='cregouby' open_date='2020-03-27T17:10:46Z' closed_time='2020-04-01T12:19:54Z'>
	<summary>[data_handler/utils.py] read_docs_from_txt() is not robust to sentence made of Out-of-vocabulary characters</summary>
	<description>
Describe the bug
When a text file with one sentence per line is read by read_docs_from_txt() with the sentence being made only on UTF-8  characters, pipeline end-up silently with a tokeniser exception. Such sentences may occur when text comes out of an automatic extraction workflow.
Error message
&lt;denchmark-code&gt;      .--.        _____                       _
    .'_\/_'.     / ____|                     | |
    '. /\ .'    | (___   __ _ _ __ ___  _ __ | | ___
      "||"       \___ \ / _` | '_ ` _ \| '_ \| |/ _ \
       || /\     ____) | (_| | | | | | | |_) | |  __/
    /\ ||//\)   |_____/ \__,_|_| |_| |_| .__/|_|\___|
   (/\||/                             |_|
______\||/___________________________________________

ID: train-2283-0
Clear Text:
        doc_id: 02789c9e-a975-3e0a-be08-bd6d8fa1f864
        sent_id: 2
        text: ��������������������������������������������������������������������������������������������������������������������������������
Tokenized:
        tokens: []
        offsets: []
        start_of_word: []
Features:
        None
&lt;/denchmark-code&gt;

Expected behavior
You could maybe add an explicit error message, and a fallback to [UNK] token
To Reproduce
Create a syntetic dataset with sentences made of utf-8 space characters
System:

OS: Linux
GPU/CPU: GPU
FARM version: 4ab81bd

	</description>
	<comments>
		<comment id='1' author='cregouby' date='2020-04-01T09:52:26Z'>
		Hi, this pull request (&lt;denchmark-link:https://github.com/deepset-ai/FARM/pull/308&gt;#308&lt;/denchmark-link&gt;
) should fix the issue. Could you try it out and let us know if it solves it?
		</comment>
	</comments>
</bug>