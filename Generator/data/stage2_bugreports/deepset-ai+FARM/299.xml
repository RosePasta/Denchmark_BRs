<bug id='299' author='RobKnop' open_date='2020-03-28T21:37:37Z' closed_time='2020-06-20T10:01:49Z'>
	<summary>Cannot load model from local dir</summary>
	<description>
Describe the bug
I want to do this with Haystack:
&lt;denchmark-code&gt;### Inference ############

# Load model
reader = FARMReader(model_name_or_path="../../saved_models/twmkn9/albert-base-v2-squad2", use_gpu=False)
&lt;/denchmark-code&gt;

I finetuned the model before and saved it to my local dir.
Here the code:
&lt;denchmark-code&gt;### TRAINING #############
# Let's take a reader as a base model
reader = FARMReader(model_name_or_path="twmkn9/albert-base-v2-squad2", max_seq_len=512, use_gpu=False)

# and fine-tune it on your own custom dataset (should be in SQuAD like format)
train_data = "training_data"
reader.train(data_dir=train_data, train_filename="2020-02-23_answers.json", test_file_name='TEST_answers.json', use_gpu=False, n_epochs=1, dev_split=0.1)
&lt;/denchmark-code&gt;

Error message
&lt;denchmark-code&gt;03/28/2020 22:25:07 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None
03/28/2020 22:25:07 - INFO - farm.modeling.adaptive_model -   Found files for loading 1 prediction heads
03/28/2020 22:25:07 - WARNING - farm.modeling.prediction_head -   Some unused parameters are passed to the QuestionAnsweringHead. Might not be a problem. Params: {"training": true, "num_labels": 2, "ph_output_type": "per_token_squad", "model_type": "span_classification", "name": "QuestionAnsweringHead"}
03/28/2020 22:25:07 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 2]
03/28/2020 22:25:07 - INFO - farm.modeling.prediction_head -   Loading prediction head from ../../saved_models/twmkn9/albert-base-v2-squad2/prediction_head_0.bin
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
~/Documents/CodingProjects/NLPofTimFerrissShow/QnA_with_Tim_Haystack.py in 
      51 
      52 # Load model
----&gt; 53 reader = FARMReader(model_name_or_path="../../saved_models/twmkn9/albert-base-v2-squad2", use_gpu=False)
      54 # A retriever identifies the k most promising chunks of text that might contain the answer for our question
      55 # Retrievers use some simple but fast algorithm, here: TF-IDF

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/haystack/reader/farm.py in __init__(self, model_name_or_path, context_window_size, batch_size, use_gpu, no_ans_boost, top_k_per_candidate, top_k_per_sample, max_processes, max_seq_len, doc_stride)
     79         self.inferencer = Inferencer.load(model_name_or_path, batch_size=batch_size, gpu=use_gpu,
     80                                           task_type="question_answering", max_seq_len=max_seq_len,
---&gt; 81                                           doc_stride=doc_stride)
     82         self.inferencer.model.prediction_heads[0].context_window_size = context_window_size
     83         self.inferencer.model.prediction_heads[0].no_ans_boost = no_ans_boost

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/farm/infer.py in load(cls, model_name_or_path, batch_size, gpu, task_type, return_class_probs, strict, max_seq_len, doc_stride)
    139                 processor = InferenceProcessor.load_from_dir(model_name_or_path)
    140             else:
--&gt; 141                 processor = Processor.load_from_dir(model_name_or_path)
    142 
    143         # b) or from remote transformers model hub

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/farm/data_handler/processor.py in load_from_dir(cls, load_dir)
    189         del config["tokenizer"]
    190 
--&gt; 191         processor = cls.load(tokenizer=tokenizer, processor_name=config["processor"], **config)
    192 
    193         for task_name, task in config["tasks"].items():

TypeError: load() missing 1 required positional argument: 'data_dir'
&lt;/denchmark-code&gt;

Expected behavior
There is no error.
Additional context
I use Haystack
To Reproduce
Steps to reproduce the behavior
System:

OS: Mac OS 10.14.6 (mojave)
GPU/CPU: CPU Intel core i5
FARM version: 0.4.1

	</description>
	<comments>
		<comment id='1' author='RobKnop' date='2020-03-28T22:28:09Z'>
		Seems like the processor doesn't have the data_dir parameter set.
Did you fine-tune the model with farm or huggingface transformers?
Please also post the content of the processor config in here.
		</comment>
		<comment id='2' author='RobKnop' date='2020-03-28T22:33:54Z'>
		FARMReader(model_name_or_path="twmkn9/albert-base-v2-squad2
I used this model: &lt;denchmark-link:https://huggingface.co/twmkn9/albert-base-v2-squad2&gt;twmkn9/albert-base-v2-squad2&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='RobKnop' date='2020-03-28T22:34:57Z'>
		I don't have any processor config. :/
		</comment>
		<comment id='4' author='RobKnop' date='2020-03-28T22:39:16Z'>
		There are conversion scripts to convert from transformers to farm. Please try these.
If that doesn't work I will look into it tomorrow.
		</comment>
		<comment id='5' author='RobKnop' date='2020-03-29T09:59:44Z'>
		Do you thought of this &lt;denchmark-link:https://github.com/deepset-ai/FARM/blob/cbbef064e3f5234bae9212e8f0e6c00640fbcb78/examples/conversion_huggingface_models.py&gt;script&lt;/denchmark-link&gt;
?
I used Case 2:
reader = Inferencer.load("../../saved_models/twmkn9/albert-base-v2-squad2", task_type="question_answering")
I get the same error:
&lt;denchmark-code&gt;03/29/2020 11:51:55 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None
03/29/2020 11:51:56 - INFO - farm.modeling.adaptive_model -   Found files for loading 1 prediction heads
03/29/2020 11:51:56 - WARNING - farm.modeling.prediction_head -   Some unused parameters are passed to the QuestionAnsweringHead. Might not be a problem. Params: {"training": true, "num_labels": 2, "ph_output_type": "per_token_squad", "model_type": "span_classification", "name": "QuestionAnsweringHead"}
03/29/2020 11:51:56 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 2]
03/29/2020 11:51:56 - INFO - farm.modeling.prediction_head -   Loading prediction head from ../../saved_models/twmkn9/albert-base-v2-squad2/prediction_head_0.bin
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
~/Documents/CodingProjects/NLPofTimFerrissShow/QnA_with_Tim_Haystack.py in 
----&gt; 60 reader = Inferencer.load("../../saved_models/twmkn9/albert-base-v2-squad2", task_type="question_answering")

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/farm/infer.py in load(cls, model_name_or_path, batch_size, gpu, task_type, return_class_probs, strict, max_seq_len, doc_stride)
    139                 processor = InferenceProcessor.load_from_dir(model_name_or_path)
    140             else:
--&gt; 141                 processor = Processor.load_from_dir(model_name_or_path)
    142 
    143         # b) or from remote transformers model hub

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/farm/data_handler/processor.py in load_from_dir(cls, load_dir)
    189         del config["tokenizer"]
    190 
--&gt; 191         processor = cls.load(tokenizer=tokenizer, processor_name=config["processor"], **config)
    192 
    193         for task_name, task in config["tasks"].items():

TypeError: load() missing 1 required positional argument: 'data_dir'
&lt;/denchmark-code&gt;

As you said, in class Processor method "load()" there is an argument missing.
		</comment>
		<comment id='6' author='RobKnop' date='2020-03-29T15:18:25Z'>
		Hey &lt;denchmark-link:https://github.com/RobKnop&gt;@RobKnop&lt;/denchmark-link&gt;
 thanks for trying out the conversion script. Though that might have been bad advice. The conversion should happen under the hood.
I could reproduce your bug with minimal code inside haystack by doing:
&lt;denchmark-code&gt;reader = FARMReader(model_name_or_path="twmkn9/albert-base-v2-squad2", use_gpu=False)
reader.save("data/albert-temp") 
reader2 = FARMReader(model_name_or_path="data/albert-temp", use_gpu=False)
&lt;/denchmark-code&gt;

So I opened an issue there &lt;denchmark-link:https://github.com/deepset-ai/haystack/issues/49&gt;deepset-ai/haystack/issues/49&lt;/denchmark-link&gt;
 and we will fix it. Thanks for reporting!
As a quick work-around (where you might need internet access) you could load the model by using the Huggingface model name "twmkn9/albert-base-v2-squad2" as before and continue training with this model. The model should be cached if you downloaded it before on the same machine.
Hope that helps.
		</comment>
		<comment id='7' author='RobKnop' date='2020-03-30T15:48:27Z'>
		Fixed by &lt;denchmark-link:https://github.com/deepset-ai/FARM/pull/300&gt;#300&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='RobKnop' date='2020-03-31T22:55:23Z'>
		&lt;denchmark-link:https://github.com/tholor&gt;@tholor&lt;/denchmark-link&gt;

Still having a related problem after installing the latest FARM &amp; Haystack versions.  Unable to load my candidate models from local directories.  Following is an example from your "playbook".
Loading from transformers' model storage works as always:
reader = FARMReader(model_name_or_path="deepset/roberta-base-squad2", ...) 
Loading the same model stored in a local directory does not:
reader = FARMReader(model_name_or_path="/runs/roberta_base_squad2", ... ) 
Error message FileNotFoundError:  No such file or directory:   processor_config.json:
&lt;denchmark-code&gt;~/anaconda3/envs/nlp/lib/python3.7/site-packages/farm/infer.py in load(cls, model_name_or_path, batch_size, gpu, task_type, return_class_probs, strict, max_seq_len, doc_stride, extraction_layer, extraction_strategy)
    162                 processor = InferenceProcessor.load_from_dir(model_name_or_path)
    163             else:
--&gt; 164                 processor = Processor.load_from_dir(model_name_or_path)
    165 
    166         # b) or from remote transformers model hub

~/anaconda3/envs/nlp/lib/python3.7/site-packages/farm/data_handler/processor.py in load_from_dir(cls, load_dir)
    177         # read config
    178         processor_config_file = Path(load_dir) / "processor_config.json"
--&gt; 179         config = json.load(open(processor_config_file))
    180         # init tokenizer
    181         if "lower_case" in config.keys():

FileNotFoundError: [Errno 2] No such file or directory: /runs/roberta_base_squad2/processor_config.json'
&lt;/denchmark-code&gt;

Platform: Linux-4.15.0-91-generic-x86_64-with-debian-buster-sid
Python version:  3.7.7
PyTorch version (GPU?):  1.4.0 (True)
Tensorflow version (GPU?):  2.1.0 (True)
transformers version:  2.7.0
farm version:  0.4.2
farm-haystack:  0.1.3
		</comment>
		<comment id='9' author='RobKnop' date='2020-04-01T07:59:43Z'>
		Hey &lt;denchmark-link:https://github.com/ahotrod&gt;@ahotrod&lt;/denchmark-link&gt;
 ,
Right now we only support two options to load a FARMReader:
a) Local FARM model
b) Remote Transformers model
I guess your error comes up when trying to load a local model in Transformers format?
I think it makes totally sense to support this and we will put it in the backlog. However, it might take some days as we are currently quite busy with a few other features.
		</comment>
		<comment id='10' author='RobKnop' date='2020-04-04T20:08:06Z'>
		I still have the issue.
I updated to the latest FARM version (0.4.2) also tried the code of the current master branch.
I get the same error as stated above.
		</comment>
		<comment id='11' author='RobKnop' date='2020-04-04T21:39:02Z'>
		one more addition:
reader.save() works
reader.train(save_dir='xxxx') doesn’t work
		</comment>
		<comment id='12' author='RobKnop' date='2020-04-07T08:39:21Z'>
		I am currently exploring this, but have trouble reproducing the issue.
Running the following script with latest haystack (&lt;denchmark-link:https://github.com/deepset-ai/haystack/commit/5932aa01c3f1d84e030c38849fa89e1f5f2770c2&gt;deepset-ai/haystack@5932aa0&lt;/denchmark-link&gt;
) and FARM == 0.4.2 saves the trained model correctly in  and loads it back again into the reader (Ubuntu 18.04, GPU).
&lt;denchmark-code&gt;from haystack.reader.farm import FARMReader

reader = FARMReader(model_name_or_path="distilbert-base-uncased-distilled-squad", use_gpu=True)
train_data = "data/squad_small"
reader.train(data_dir=train_data, train_filename="train.json", batch_size=4, n_epochs=1, save_dir="mymodel/debug")
reader = FARMReader(model_name_or_path="mymodel/debug", use_gpu=True)
&lt;/denchmark-code&gt;

The resulting directory with the saved model looks like this:
&lt;denchmark-link:https://user-images.githubusercontent.com/1563902/78647406-d91c9280-78ba-11ea-9afb-81ed565d2625.png&gt;&lt;/denchmark-link&gt;

A few questions to narrow this down together:

Is that the situation that throws an error in your environment or did I misunderstood it?
Can you run my snippet successfully (I am using this tiny squad dataset for debugging)?
Have you re-run reader.train() with FARM 0.4.2 or is that an older model?
Is the error message you get the same as in your first comment of this issue (TypeError: load() missing 1 required positional argument: 'data_dir')?
Can you verify that your are really using FARM 0.4.2?
Does your directory with the saved model contain all files from the screenshot?

		</comment>
		<comment id='13' author='RobKnop' date='2020-06-06T09:20:51Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 14 days if no further activity occurs.
		</comment>
		<comment id='14' author='RobKnop' date='2020-11-27T20:28:26Z'>
		&lt;denchmark-link:https://github.com/tholor&gt;@tholor&lt;/denchmark-link&gt;

Hey! I am trying to load a locally saved Transformers model into the FARMReader. This is important to my project since the FARMReader seems to perform significantly better than the TransformersReader. Has there been any progress in allowing local Transformers models to be compatible with the FARMReader? Or are there any workarounds?
		</comment>
		<comment id='15' author='RobKnop' date='2020-11-30T12:05:26Z'>
		The only way I see is to load your model into FARM, then save it as a FARMmodel, then load it in haystack as a FARMReader.
You should be able to do all this to your local transformers model with (Please adjust the parameters accordingly):
&lt;denchmark-code&gt;    model = AdaptiveModel.convert_from_transformers(model_name_or_path, device=device, task_type="question_answering")

    tokenizer = Tokenizer.load(pretrained_model_name_or_path=model_name_or_path,do_lower_case=do_lower_case)
    processor = SquadProcessor(
        tokenizer=tokenizer,
        max_seq_len=256,
        label_list= ["start_token", "end_token"],
        metric="squad",
        train_filename=None,
        dev_filename=None,
        dev_split=0,
        test_filename=evaluation_filename,
        data_dir=data_dir,
        doc_stride=128,
    )
    model.connect_heads_with_processor(data_silo.processor.tasks, require_labels=True)
    
    model.save(save_dir)
    processor.save(save_dir)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='16' author='RobKnop' date='2020-12-02T17:00:09Z'>
		That did the trick, thanks!
		</comment>
		<comment id='17' author='RobKnop' date='2020-12-02T17:43:53Z'>
		Nice! Always happy to help, thanks for reporting back here.
		</comment>
	</comments>
</bug>