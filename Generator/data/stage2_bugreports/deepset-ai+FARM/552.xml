<bug id='552' author='Timoeller' open_date='2020-09-22T15:59:05Z' closed_time='2020-11-18T13:57:42Z'>
	<summary>Cannot extract QA answer at beginning of document</summary>
	<description>
Describe the bug
Answer at beginning of documents cannot be extracted.
Error message
No errors, just the predictions are wrong.
Additional context
I suspect the logit verification is not correct. We do check weather a start/end logit is within the question tokens, this is possibly flawed.
Doesnt predict correctly neither for bert nor for roberta type QA models.
To Reproduce
compare
&lt;denchmark-code&gt;inferencer = Inferencer.load(model_name_or_path="deepset/roberta-base-squad2", task_type="question_answering")

    qa_input = [
        {
            "qas": ["What is the largest city in Germany?"],
            "context": "Berlin is the capital and largest city of Germany by both area and population.",
        }
    ]

    results = inferencer.inference_from_dicts(qa_input)
    print(results)


vs 
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;qa_input = [
    {
        "qas": ["What is the largest city in Germany?"],
        "context": "Document testing. With short text before Berlin it is still no answer. Berlin is the capital and largest city of Germany by both area and population.",
    }
]
&lt;/denchmark-code&gt;

vs
&lt;denchmark-code&gt;    qa_input = [
        {
            "qas": ["What is the largest city in Germany?"],
            "context": "Document testing this weird behaviour for all kinds of cases. With short text before Berlin it is still no answer. Berlin is the capital and largest city of Germany by both area and population.",
        }
    ]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Timoeller' date='2020-09-23T13:51:15Z'>
		I had the same feeling in some test I made, I noticed that a suspected percentage of un-predicted answers was located at the beginning of the documents.
		</comment>
		<comment id='2' author='Timoeller' date='2020-10-14T15:42:18Z'>
		Comment from &lt;denchmark-link:https://github.com/deepset-ai/haystack/issues/476#issuecomment-708447639&gt;deepset-ai/haystack#476 (comment)&lt;/denchmark-link&gt;
:
&lt;denchmark-link:https://github.com/Timoeller&gt;@Timoeller&lt;/denchmark-link&gt;
 however, even though you have written that QA doesn't predict correctly neither for BERT nor for RoBERTa type QA models, I've noted that for BERT there are no problems. The problems come up . Even with RoBERTa-Large and ELECTRA-Large is ok.
from haystack.reader.farm import FARMReader
models = ["deepset/roberta-base-squad2","deepset/electra-base-squad2", "twmkn9/bert-base-uncased-squad2","twmkn9/albert-base-v2-squad2","deepset/minilm-uncased-squad2","ahotrod/electra_large_discriminator_squad2_512","phiyodr/roberta-large-finetuned-squad2"]
readers = []
for model in models:
  reader = FARMReader(model_name_or_path = model, use_gpu = True)
  readers.append(reader)

question = "What is the largest city in Germany?"
context = "Berlin is the capital and largest city of Germany by both area and population."
for model,reader in zip(models,readers):
  prediction = reader.predict_on_texts(question,[context],5)
  print(f"Model {model}: {[answer['answer'] for answer in prediction['answers']]}")
Output:
&lt;denchmark-code&gt;Model deepset/roberta-base-squad2: ['Germany by both area and population.']
Model deepset/electra-base-squad2: ['capital']
Model twmkn9/bert-base-uncased-squad2: ['Berlin']
Model twmkn9/albert-base-v2-squad2: ['Berlin']
Model deepset/minilm-uncased-squad2: ['Berlin']
Model ahotrod/electra_large_discriminator_squad2_512: ['Berlin']
Model phiyodr/roberta-large-finetuned-squad2: ['Berlin']
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='Timoeller' date='2020-10-14T16:26:55Z'>
		&lt;denchmark-link:https://github.com/brandenchan&gt;@brandenchan&lt;/denchmark-link&gt;
 the analysis over different model types might help you in debugging that bugger.
It is quite weird that other roberta or electra models (from ahotrod or phiyodr) can predict that first token as answer.
It might be that our deepset models wont predict the first token, since they have never seen such training examples, because of the bug we address in &lt;denchmark-link:https://github.com/deepset-ai/FARM/pull/564&gt;#564&lt;/denchmark-link&gt;

looking forward seeing this fixed
		</comment>
		<comment id='4' author='Timoeller' date='2020-10-14T16:32:11Z'>
		&lt;denchmark-link:https://github.com/Timoeller&gt;@Timoeller&lt;/denchmark-link&gt;
  but consider that  is also one of your models and, in this case, it correctly predicts .
If you've trained this model using the same strategy of  and  it's even weirder.
		</comment>
		<comment id='5' author='Timoeller' date='2020-11-02T16:17:24Z'>
		So thank you &lt;denchmark-link:https://github.com/antoniolanza1996&gt;@antoniolanza1996&lt;/denchmark-link&gt;
 for all the analysis and experiments!
I've just uploaded deepset/roberta-base-squad2-v2 which was trained after fixing the &lt;denchmark-link:https://github.com/deepset-ai/FARM/issues/558&gt;train-time first token issue &lt;/denchmark-link&gt;
. The performance is better than v1 and we have made sure that it can in fact predict first token. Could you try this out and see if it also fixes the problem on your side?
It is also odd to us that deepset/minilm-uncased-squad2 does not have the first token problem. Perhaps it has learned to focus more on word semantics and less on word position.
		</comment>
		<comment id='6' author='Timoeller' date='2020-11-02T16:29:54Z'>
		Hi &lt;denchmark-link:https://github.com/brandenchan&gt;@brandenchan&lt;/denchmark-link&gt;
 , thank you too for working on this! I'll try ASAP (hopefully later this week) and I'll let you know on my original issue on Haystack: &lt;denchmark-link:https://github.com/deepset-ai/haystack/issues/476&gt;deepset-ai/haystack#476&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='Timoeller' date='2020-11-18T13:57:42Z'>
		The model update of deepset/roberta-base-squad2 has fixed the problem. Confirmed in &lt;denchmark-link:https://github.com/deepset-ai/FARM/pull/476&gt;#476&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>