<bug id='120' author='neufang' open_date='2019-10-17T20:58:42Z' closed_time='2019-10-18T10:23:08Z'>
	<summary>run_all_experiments.py QA throws keyError</summary>
	<description>
When I run run_all_experiments.py, the last question answering stops with keyError
&lt;denchmark-code&gt;10/17/2019 20:16:51 - INFO - pytorch_transformers.modeling_utils -   load
ing weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert
-base-cased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_tr
ansformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67
e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
10/17/2019 20:16:54 - WARNING - farm.modeling.language_model -   Could no
t automatically detect from language model name what language it is.
We guess it's an *ENGLISH* model ...
If not: Init the language model by supplying the 'language' param.
Example: Bert.load('my_mysterious_model_name', language='de')
10/17/2019 20:16:58 - INFO - farm.modeling.optimization -   Number of opt
imization steps: 12220
Traceback (most recent call last):
  File "run_all_experiments.py", line 36, in &lt;module&gt;
    main()
  File "run_all_experiments.py", line 33, in main
    run_experiment(experiment)
  File "/home/user/farm/experiment.py", line 147, in run_experiment
    model = trainer.train(model)
  File "/home/user/farm/train.py", line 129, in train
    model.connect_heads_with_processor(self.data_silo.processor.tasks)
  File "/home/user/farm/modeling/adaptive_model.py", line 233, in connect
_heads_with_processor
    head.label_tensor_name = tasks[head.task_name]["label_tensor_name"]
KeyError: 'question_answering'
&lt;/denchmark-code&gt;

System:

OS: Ubuntu 17.10
GPU/CPU:  Tesla V100-SXM2-16GB
FARM version: 0.2.0

	</description>
	<comments>
		<comment id='1' author='neufang' date='2019-10-18T10:23:08Z'>
		Hi &lt;denchmark-link:https://github.com/neufang&gt;@neufang&lt;/denchmark-link&gt;
,
thanks for reporting! I had a look and the error was due to an outdated naming in the experiment config  ("label_list" instead of "labels")
This lead to initializing a processor without any tasks. Therefore, the connection with the prediction head failed, when it was looking fo the task with name "question_answering".
I have added now also a info message to inform for such cases, where labels or metric are not provided to the processor.
The fix is already merged into the master branch. So executing run_all_experiments.py with the latest code from there should work now.
		</comment>
	</comments>
</bug>