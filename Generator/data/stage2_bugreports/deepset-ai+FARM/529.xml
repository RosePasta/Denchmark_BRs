<bug id='529' author='ftesser' open_date='2020-09-07T11:48:55Z' closed_time='2020-10-27T15:44:30Z'>
	<summary>`examples/natural_questions.py` evaluation not computed for yes/no classes</summary>
	<description>
Describe the bug
Running the examples/natural_questions.py script I obtain the following evalution:
&lt;denchmark-code&gt;\\|//       \\|//      \\|//       \\|//     \\|//
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
***************************************************
***** EVALUATION | DEV SET | AFTER 2000 BATCHES *****
***************************************************
\\|//       \\|//      \\|//       \\|//     \\|//
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

09/07/2020 11:56:01 - INFO - farm.eval -   
 _________ question_answering _________
09/07/2020 11:56:01 - INFO - farm.eval -   loss: 1.1128250570618547
09/07/2020 11:56:01 - INFO - farm.eval -   task_name: question_answering
09/07/2020 11:56:01 - INFO - farm.eval -   EM: 0.632
09/07/2020 11:56:01 - INFO - farm.eval -   f1: 0.6685447088801921
09/07/2020 11:56:01 - INFO - farm.eval -   top_n_accuracy: 0.918
09/07/2020 11:56:01 - INFO - farm.eval -   report: 
 Not Implemented
09/07/2020 11:56:01 - INFO - farm.eval -   
 _________ text_classification _________
09/07/2020 11:56:01 - INFO - farm.eval -   loss: 0.3979932257842103
09/07/2020 11:56:01 - INFO - farm.eval -   task_name: text_classification
09/07/2020 11:56:01 - INFO - farm.eval -   f1_macro: 0.8045831308172903
09/07/2020 11:56:01 - INFO - farm.eval -   report: 
               precision    recall  f1-score   support

   no_answer     0.8355    0.8806    0.8575       871
        span     0.7878    0.7188    0.7517       537
         yes     0.0000    0.0000    0.0000         0
          no     0.0000    0.0000    0.0000         0

   micro avg     0.8189    0.8189    0.8189      1408
   macro avg     0.4058    0.3999    0.4023      1408
weighted avg     0.8173    0.8189    0.8171      1408
&lt;/denchmark-code&gt;

In the text_classification part yes and no classes are reported as 0 for all the scores also support, but I checked that in the dev-set some yes/no classes are present.
Expected behavior
Compute the right scores for yes/no classes.
To Reproduce
run examples/natural_questions.py
System:

OS: Linux Ubuntu 20.04.1 LTS
GPU/CPU: both
FARM version: 0.4.7

	</description>
	<comments>
		<comment id='1' author='ftesser' date='2020-09-07T12:04:34Z'>
		We will look nto it, the yes no cases might have been filtered out during a change in the data processing. The model that we uploaded to s3 is capable of answering yes/no questions (albeit not very well...)
		</comment>
		<comment id='2' author='ftesser' date='2020-09-14T14:06:09Z'>
		Hey &lt;denchmark-link:https://github.com/ftesser&gt;@ftesser&lt;/denchmark-link&gt;
, this PR (&lt;denchmark-link:https://github.com/deepset-ai/FARM/pull/540&gt;#540&lt;/denchmark-link&gt;
) should fix the problem. Could you confirm that it works on your side too?
		</comment>
		<comment id='3' author='ftesser' date='2020-09-14T15:45:32Z'>
		Hi &lt;denchmark-link:https://github.com/brandenchan&gt;@brandenchan&lt;/denchmark-link&gt;
, now I have this output:
&lt;denchmark-code&gt;^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
***************************************************
***** EVALUATION | DEV SET | AFTER 500 BATCHES *****
***************************************************
\\|//       \\|//      \\|//       \\|//     \\|//
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

09/14/2020 17:22:40 - INFO - farm.eval -   
 _________ question_answering _________
09/14/2020 17:22:40 - INFO - farm.eval -   loss: 1.4187173030383156
09/14/2020 17:22:40 - INFO - farm.eval -   task_name: question_answering
09/14/2020 17:22:40 - INFO - farm.eval -   EM: 0.564
09/14/2020 17:22:40 - INFO - farm.eval -   f1: 0.6097777428459533
09/14/2020 17:22:40 - INFO - farm.eval -   top_n_accuracy: 0.896
09/14/2020 17:22:40 - INFO - farm.eval -   report: 
 Not Implemented
09/14/2020 17:22:40 - INFO - farm.eval -   
 _________ text_classification _________
09/14/2020 17:22:40 - INFO - farm.eval -   loss: 0.5549181935399078
09/14/2020 17:22:40 - INFO - farm.eval -   task_name: text_classification
09/14/2020 17:22:41 - INFO - farm.eval -   f1_macro: 0.3754131570529907
09/14/2020 17:22:41 - INFO - farm.eval -   report: 
               precision    recall  f1-score   support

   no_answer     0.8370    0.7281    0.7788       846
        span     0.6589    0.8007    0.7229       562
         yes     0.0000    0.0000    0.0000         4
          no     0.0000    0.0000    0.0000         7

    accuracy                         0.7512      1419
   macro avg     0.3740    0.3822    0.3754      1419
weighted avg     0.7599    0.7512    0.7506      1419

&lt;/denchmark-code&gt;

So now there are some yes/no questions/answers here.
I have just two additional questions:


in the dev_medium.jsonl I counted 4  "YES"  answers and  5 "NO" answers, on a total of 1000 lines (examples) in the file, why the "no" answers here are 7 and why the sum  no_answer+ span + yes + no is not 100?  Is because of the stride + merging multiple disjoint short answers + downsampling samples of no_answer?


This dev_medium.jsonl that is automatically downloaded is a part of the original natural question dataset? Why there are so few yes/no examples?


		</comment>
		<comment id='4' author='ftesser' date='2020-09-14T19:33:25Z'>
		&lt;denchmark-link:https://github.com/brandenchan&gt;@brandenchan&lt;/denchmark-link&gt;
 I have analyzed the changes of PR (&lt;denchmark-link:https://github.com/deepset-ai/FARM/pull/540&gt;#540&lt;/denchmark-link&gt;
)  and I have just made a PR &lt;denchmark-link:https://github.com/deepset-ai/FARM/pull/542&gt;#542&lt;/denchmark-link&gt;
 that  take care of the generic cases where there are more than one answers for each question.
what do you think?
		</comment>
		<comment id='5' author='ftesser' date='2020-09-16T08:23:05Z'>
		So in answer to your questions:


The 1419 that you’re seeing in the text_classification summary refers to the number of passages (i.e. post document chunking). Some of the yes/no documents were perhaps long enough to be split into different passages and the yes/no span might occur in their overlapping section, hence why we have more than 4 yes and 5 no answers.  This is an artifact of how our nq system is designed - there is a QA prediction that returns one answer per document and a text classification head which returns one answer per passage. I can explain why it is like this is you’re interested.


dev_medium.jsonl is in fact a slice of the original natural questions dev set. In that set, yes/no questions actually only make up about 2% of the samples so dev_medium.jsonl approximately matches that.


I have written you a response in the PR regarding our Natural Questions code.
		</comment>
		<comment id='6' author='ftesser' date='2020-09-16T14:25:01Z'>
		Thanks for your explanations.

This is an artifact of how our nq system is designed - there is a QA prediction that returns one answer per document and a text classification head which returns one answer per passage. I can explain why it is like this is you’re interested.

Yes please I am interested in why you designed like this the NQ system.
I also write to you in the PR, why I am interested in having multiple answers.
		</comment>
		<comment id='7' author='ftesser' date='2020-09-18T12:11:31Z'>
		So our NQ system is built using two prediction heads. One is a QA Prediction Head which can extract answer spans or return no answers. The other is an Answer Type Classification Head which can mark each passage as [span, no_answer, yes, no]. This is inspired by the design of other models e.g. this (c.f. &lt;denchmark-link:https://arxiv.org/pdf/1901.08634v3.pdf&gt;https://arxiv.org/pdf/1901.08634v3.pdf&lt;/denchmark-link&gt;
)
Incoming documents are split into passages and the QA head extracts a span or no_answer from each passage. But question answering is a task that is performed on a document level. And so the model needs to choose which span to pick if it makes multiple span predictions in different passages. It also needs to know when to pick no_answer as a document level prediction. We refer to this step as “aggregation".
Once we have a document level span prediction, we need to know the model’s answer_type prediction. The answer_type head has created one prediction for each passage. The “merge” process links a span prediction to an answer_type prediction. To do so requires knowing which passage the span prediction comes from, and finding the answer_type prediction for that very passage. This is how we arrive at a final document level NQ prediction.
So to return to the reason why we have more yes / no support, evaluation happens before we “merge” answer_type prediction with span prediction. That’s why the summary shows stats in terms of number of passages instead of documents.
		</comment>
		<comment id='8' author='ftesser' date='2020-09-21T13:07:08Z'>
		Thanks for all the interesting information &lt;denchmark-link:https://github.com/brandenchan&gt;@brandenchan&lt;/denchmark-link&gt;

In the meantime, I have noticed another dis-alignment  between squad and NQ. I will open a separate issue later on.
		</comment>
	</comments>
</bug>