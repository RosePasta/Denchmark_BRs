<bug id='446' author='kolk' open_date='2020-07-07T14:20:47Z' closed_time='2020-07-07T14:35:37Z'>
	<summary>transformer 2.11.0 compatibility issues with ElectraConfig</summary>
	<description>
Describe the bug
Language model does not support ElectraConfig
Error message
&lt;denchmark-code&gt;ValueError: Unrecognized configuration class &lt;class 'transformers.configuration_electra.ElectraConfig'&gt; for this kind of AutoModel: AutoModelForQuestionAnswering.
Model type should be one of DistilBertConfig, AlbertConfig, LongformerConfig, RobertaConfig, BertConfig, XLNetConfig, FlaubertConfig, XLMConfig.
&lt;/denchmark-code&gt;

Expected behavior
Successful pipleline object creation
Additional context
Add any other context about the problem here, like type of downstream task, part of  etc..
To Reproduce
&lt;denchmark-code&gt;from transformers.modeling_auto import AutoModelForQuestionAnswering 
from transformers.tokenization_auto import AutoTokenizer 
model_name = "deepset/electra-base-squad2"
nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)
&lt;/denchmark-code&gt;

System:

OS: ubuntu 18
GPU/CPU: GPU
FARM version:
Transformer vers

	</description>
	<comments>
	</comments>
</bug>