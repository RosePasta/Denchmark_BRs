<bug id='215' author='rapwag01' open_date='2019-01-25T16:00:09Z' closed_time='2019-02-19T08:59:43Z'>
	<summary>Nonuniform quantization function with quantile initalization maps 0.0 to 0.0 instead of choosing the smallest cluster point.</summary>
	<description>
Describe the bug
The function __build_norm_quant_point() in /learners/nonuniform_quantization/utils.py takes the normalized weight tensor and maps 0.0 to 0.0 instead of the smallest value present in the initialized cluster tensor (which will be non-zero after quantile initialization).
The mapping happens at line 304 in the util.py file:
qx = tf.gather(c, min_index) * tf.abs(tf.sign(x_normalized))
where
c: the initialization of quantization points
x_normalized: the normalized weights
min_index: indexes where distance between weight value and cluster point is smallest
tf.sign(x) returns 0 for 0, +1 for x&gt;0 and -1 for x&lt;0
This means that there will be k+1 unique non-zero cluster values in the final weight tensor after inverse scaling has been applied. Because 0.0 in qx gets mapped to the minimal weight value (beta) when calling __inv_scale().
To Reproduce
Functional bug
Expected behavior
Shouldn't it be
qx = tf.gather(c, min_index)
instead? 0.0 in x_normalized should be mapped onto the minimal value in c so that there are k unique non-zero cluster values in the final non-uniformly quantized weight tensor.
PocketFlow Version
Pulled on Dec 13th 2018
	</description>
	<comments>
		<comment id='1' author='rapwag01' date='2019-01-31T11:01:40Z'>
		Hi, can I ask you to please have a look at this? Thank you very much!
&lt;denchmark-link:https://github.com/jiaxiang-wu&gt;@jiaxiang-wu&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='rapwag01' date='2019-02-08T00:58:41Z'>
		&lt;denchmark-link:https://github.com/haolibai&gt;@haolibai&lt;/denchmark-link&gt;
 Can you take a look at this issue?
		</comment>
		<comment id='3' author='rapwag01' date='2019-02-09T05:02:12Z'>
		&lt;denchmark-link:https://github.com/rapwag01&gt;@rapwag01&lt;/denchmark-link&gt;

Hi,
Thanks for suggestion. The problem will happen if weights are initialised to 0s. Since the weights are randomly initialised, it is not likely to happen for initialising at 0s.
The reason why we use gradient_over_map here is to enable the straight-through-estimation. If gradient_over_map is removed, the computation flow will not be able to track the weights.
		</comment>
		<comment id='4' author='rapwag01' date='2019-02-18T14:47:54Z'>
		&lt;denchmark-link:https://github.com/haolibai&gt;@haolibai&lt;/denchmark-link&gt;
  But __build_norm_quant_point() takes x_normalized as an input which is the weight tensor after scaling. It will always contain 0.0., because the minimal weight value will get mapped to 0.0 in __scale(self, w, mode).
Hence  "tf.abs(tf.sign(x_normalized))" on line 304 in util.py will be zero in some cases, adding an additional cluster point which has not been initialized before.
Hope you understand my problem.
Thanks.
		</comment>
		<comment id='5' author='rapwag01' date='2019-02-19T08:58:45Z'>
		Yes you are right. Thanks for the suggestion. We have fixed the bug.
		</comment>
		<comment id='6' author='rapwag01' date='2019-02-19T17:51:49Z'>
		Oh great news, thank you &lt;denchmark-link:https://github.com/haolibai&gt;@haolibai&lt;/denchmark-link&gt;
!
Although can you confirm that accuracy remains the same?
		</comment>
		<comment id='7' author='rapwag01' date='2019-02-20T02:56:08Z'>
		&lt;denchmark-link:https://github.com/rapwag01&gt;@rapwag01&lt;/denchmark-link&gt;
 Currently I only test the performance of non-uniform quantization on Cifar10, and the performance is nearly the same.
		</comment>
		<comment id='8' author='rapwag01' date='2019-02-20T16:16:11Z'>
		Ok, thanks.
		</comment>
	</comments>
</bug>