<bug id='115' author='brisker' open_date='2018-11-30T09:08:06Z' closed_time='2019-01-21T04:41:35Z'>
	<summary>Did you have the performance summary of the RL-based quantization performance on MobileNet?</summary>
	<description>
Did you have the performance summary of the RL based quantization performance on MobileNet? Given that you have described about this here :  &lt;denchmark-link:https://pocketflow.github.io/reinforcement_learning/&gt;https://pocketflow.github.io/reinforcement_learning/&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='brisker' date='2018-11-30T13:09:04Z'>
		Yes. We have some preliminary results of MobileNet on Imagenet dataset, with uql_evquivalent_bits=4 or nuql_evquivalent_bits=4. ( 8 bit quantization does not hurt the performance, while 2 bit degrade the performance significantly).
For uql_evquivalent_bits=4, the top-1 acc is 47.1%~53.0%.
For nuql_evquivalent_bits=4, the top-1 acc is 64.5%~65.9%.
The accuracy on validation set is measured at a fixed interval. Note that the variance of acc could be large sometimes. The above results may be improved if hyper-parameters of the RL agent are more carefully designed.
		</comment>
		<comment id='2' author='brisker' date='2018-12-02T13:48:07Z'>
		&lt;denchmark-link:https://github.com/haolibai&gt;@haolibai&lt;/denchmark-link&gt;

Why is the non-uniform quantization performance much better than uniform quantization? In standard quantization methods, it seems to be not like this.
		</comment>
		<comment id='3' author='brisker' date='2018-12-02T14:08:59Z'>
		Generally non-uniform quantization could have larger model capacity and thereon can outperform uniform quantization, especially in low-bit settings.
		</comment>
		<comment id='4' author='brisker' date='2018-12-03T07:30:48Z'>
		How to reproduce the RL-based quantization performance you have claimed?
Besides, your claimed performace is more like a range, "47.1%~53.0%." So what is the difference between the experiment settings of 47.1 and 53.0?
&lt;denchmark-link:https://github.com/haolibai&gt;@haolibai&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='brisker' date='2018-12-03T08:11:26Z'>
		The model is evaluated every 10000 steps, so there could be some variance on the accuracy. We found that for mobilenet, the variance is a little bit large (e.g., 47.1%~53.0%).
Remain the default parameters for the RL agent, i.e., run
./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py 
--learner uniform 
--uql_enbl_rl_agent
		</comment>
		<comment id='6' author='brisker' date='2018-12-03T11:23:42Z'>
		&lt;denchmark-link:https://github.com/haolibai&gt;@haolibai&lt;/denchmark-link&gt;


the performance of MobileNet you mentioned above, is it Mobilenet v1 or v2?
What is the 32-bit precision performance of the v1 and v2 mobilenet model, downloaded from https://api.ai.tencent.com/pocketflow  ??

		</comment>
		<comment id='7' author='brisker' date='2018-12-03T11:35:51Z'>
		
Mobilenet v1
The full precision results are in https://pocketflow.github.io/pre_trained_models/
Downloaded from: https://api.ai.tencent.com/pocketflow/models_mobilenet_v1_at_ilsvrc_12.tar.gz

		</comment>
		<comment id='8' author='brisker' date='2018-12-03T12:15:43Z'>
		&lt;denchmark-link:https://github.com/haolibai&gt;@haolibai&lt;/denchmark-link&gt;

I have run this command:(with default settings)
, and in the first several iterations, the acc is so low(see the pic below). Is this normal?
&lt;denchmark-link:https://user-images.githubusercontent.com/13804492/49373190-1063b800-f738-11e8-90eb-f7616abd6d8c.png&gt;&lt;/denchmark-link&gt;

(the log says "rostoring parameters from ./models/model.ckpt-166818", indicating it has already loaded the full-precision parameters)
		</comment>
		<comment id='9' author='brisker' date='2018-12-03T13:13:21Z'>
		&lt;denchmark-link:https://github.com/jiaxiang-wu&gt;@jiaxiang-wu&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/haolibai&gt;@haolibai&lt;/denchmark-link&gt;

it has already run for over 6 roll-out, the acc is still very low. Is this normal? I just follow the default settings.
		</comment>
		<comment id='10' author='brisker' date='2018-12-03T13:23:28Z'>
		This could be possible in initial roll-outs. You may increase the learning rate a little so as to effectively verify the current strategy.
Every time the model will be restored from a full-precision one and quantize it given the current strategy to see how good it performs.
The best performance throughout 200 roll-outs will be recorded, which is the best strategy correspondingly.
		</comment>
		<comment id='11' author='brisker' date='2018-12-03T13:34:44Z'>
		&lt;denchmark-link:https://github.com/haolibai&gt;@haolibai&lt;/denchmark-link&gt;

But I just follow the default settings. ...
Did you mean the default settings have bugs?....
Now it has enters the 10th roll-out, still nearly 0 acc.
		</comment>
		<comment id='12' author='brisker' date='2018-12-03T13:42:41Z'>
		The first 25% epochs are just for collecting training data for the buffer of DDPG, while the RL agent has not started training. The searching space is a little bit large, and MobileNet-v1 is indeed sensitive to low-bit quantization, so in most cases if the strategy is not good enough, the result will be pretty poor.
If I remember correctly, I generate the above results using the default settings. Nevertheless, the default setting may be not the best setting for the training.
		</comment>
		<comment id='13' author='brisker' date='2018-12-03T13:47:57Z'>
		&lt;denchmark-link:https://github.com/haolibai&gt;@haolibai&lt;/denchmark-link&gt;

Besides, I have used 4 gpus, but when running , the  of the last 3 gpus is actually 0%. So does the  actually support multi-gpu training?
		</comment>
		<comment id='14' author='brisker' date='2018-12-04T01:54:47Z'>
		The training of RL agent will use only one GPU, while for training the network parameters will use 4 GPUs.
Please check if you successfully use 4 GPUs when using other learners?
		</comment>
		<comment id='15' author='brisker' date='2018-12-04T07:58:02Z'>
		&lt;denchmark-link:https://github.com/haolibai&gt;@haolibai&lt;/denchmark-link&gt;

Currently I am only using the RL-based UniformQuantization learner. The mobilenet-v1 RL-based model has run over 120 roll-out, and the acc is still nearly 0. I just follow the default settings with the following command:

I am wondering if the version of the code has some changes, given that you mentioned you have got a 47.1%~53.0% performance with mobilenet-v1 default settings.
(I got successfully run a resnet20-cifar model with RL-based 4-bit quantization training, with around 74.6% top-1 acc, which is still poor but not nearly 0, with the following command:
bash ./scripts/run_local.sh nets/resnet_at_cifar10_run.py --learner=uniform --uql_enbl_rl_agent=True --uql_equivalent_bits=4)
		</comment>
		<comment id='16' author='brisker' date='2018-12-04T08:34:24Z'>
		Did you set enbl_warm_start=True? Since the quantized network is based on a pretrained model.
Sorry this should be added in the tutorial.
		</comment>
		<comment id='17' author='brisker' date='2018-12-04T08:40:15Z'>
		previously no. But just a moment ago, as you have mentioned, I added this flag into the command and run it, but still, in the first several iterations, the acc is still nearly 0, . It seems to be no difference. So I assume that there must be  other possible mistakes in the default settings.
&lt;denchmark-link:https://github.com/haolibai&gt;@haolibai&lt;/denchmark-link&gt;

		</comment>
		<comment id='18' author='brisker' date='2018-12-04T08:47:12Z'>
		I tried a moment ago, using the following configurations, and there are effective accs.
./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py -n=8 
--learner uniform \
--enbl_warm_start 
--uql_enbl_rl_agent 
--uql_equivalent_bits 4 \
--uql_w_bit_min 2 
--uql_w_bit_max 8
&lt;denchmark-link:https://user-images.githubusercontent.com/24502378/49429764-51190b00-f7e4-11e8-8589-793f6d40472e.png&gt;&lt;/denchmark-link&gt;

(seven is just a distributed system, you can replace it with your own GPUs)
Please make sure that you have successfully restored a pretrained MobileNet-v1, and correct hyper-parameters.
		</comment>
		<comment id='19' author='brisker' date='2018-12-04T08:48:54Z'>
		&lt;denchmark-link:https://github.com/haolibai&gt;@haolibai&lt;/denchmark-link&gt;

but what I refer to is mobilenet_v1, and the command I run is

		</comment>
		<comment id='20' author='brisker' date='2018-12-04T08:50:34Z'>
		&lt;denchmark-link:https://github.com/brisker&gt;@brisker&lt;/denchmark-link&gt;
 The key is that you need to enable warm-start by specifying .
		</comment>
		<comment id='21' author='brisker' date='2018-12-04T08:51:38Z'>
		The following are just the default settings, you can ignore them:
--uql_w_bit_min 2
--uql_w_bit_max 8
		</comment>
		<comment id='22' author='brisker' date='2018-12-04T11:58:21Z'>
		&lt;denchmark-link:https://github.com/haolibai&gt;@haolibai&lt;/denchmark-link&gt;

The problem seems not to be totally solved. I noticed that even though the allocated layer bits is a lot higher than 2-bit or 3-bit, the acc is still nearly 0. Here is my log. Could you please check it ?
I just can not figure out what is going wrong, given so many 0 acc.
&lt;denchmark-link:https://github.com/Tencent/PocketFlow/files/2643850/resnet18_imagenet_4bit_log.2.log&gt;resnet18_imagenet_4bit_log(2).log&lt;/denchmark-link&gt;

		</comment>
		<comment id='23' author='brisker' date='2018-12-05T03:08:08Z'>
		&lt;denchmark-link:https://github.com/brisker&gt;@brisker&lt;/denchmark-link&gt;
 Potential bug. We are looking at this issue.
		</comment>
		<comment id='24' author='brisker' date='2018-12-05T03:12:50Z'>
		besides, If I run the resnet_cifar experiments（still RL-based quantization）, the acc_top1 is normal, but the acc_top5 is all 0.
I assume this is a minor bug.
&lt;denchmark-link:https://github.com/haolibai&gt;@haolibai&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/jiaxiang-wu&gt;@jiaxiang-wu&lt;/denchmark-link&gt;

		</comment>
		<comment id='25' author='brisker' date='2018-12-05T03:54:14Z'>
		It is an earlier version before the release. Let us reproduce the result and inform you later.
		</comment>
		<comment id='26' author='brisker' date='2018-12-05T06:32:35Z'>
		&lt;denchmark-link:https://github.com/haolibai&gt;@haolibai&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/jiaxiang-wu&gt;@jiaxiang-wu&lt;/denchmark-link&gt;

what is the difference between ?
what is this type in the experiments you have reproduced(the mobilenet 47.1%~53.0%. performance )?
		</comment>
		<comment id='27' author='brisker' date='2018-12-05T06:54:40Z'>
		Without bucketing.
		</comment>
		<comment id='28' author='brisker' date='2018-12-05T10:23:18Z'>
		given a conv layer with weights float values , and  represents float values,
if without bucketing, all the  values are quantized using only one  alpha and beta.
if with bucketing with , all the values are quantized using  alphas and  betas?
Have I understood this right?
&lt;denchmark-link:https://github.com/haolibai&gt;@haolibai&lt;/denchmark-link&gt;

		</comment>
		<comment id='29' author='brisker' date='2018-12-05T11:46:24Z'>
		Yes correct.
		</comment>
		<comment id='30' author='brisker' date='2018-12-05T12:02:49Z'>
		&lt;denchmark-link:https://github.com/haolibai&gt;@haolibai&lt;/denchmark-link&gt;

If   the weight quantization bits is learned by rl agent, but what about the activation bits?
? or ?
		</comment>
		<comment id='31' author='brisker' date='2018-12-05T13:40:44Z'>
		activation_bits remains 32 bits, since quantization with different bits across layers can not be accelerated, which poses no necessity for activation quantization.
		</comment>
		<comment id='32' author='brisker' date='2018-12-06T02:43:22Z'>
		I do not quite understand what you mean.
Why the different activation bit across layers can not be accelerated?
here &lt;denchmark-link:https://github.com/haolibai/PocketFlow/blob/master/learners/uniform_quantization/bit_optimizer.py#L52&gt;https://github.com/haolibai/PocketFlow/blob/master/learners/uniform_quantization/bit_optimizer.py#L52&lt;/denchmark-link&gt;

it says Currently only weight bits are inferred via RL. Activations later.
So what is your plan in the future, regarding the activation-RL?
&lt;denchmark-link:https://github.com/haolibai&gt;@haolibai&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/jiaxiang-wu&gt;@jiaxiang-wu&lt;/denchmark-link&gt;

		</comment>
		<comment id='33' author='brisker' date='2018-12-06T09:59:12Z'>
		A basic requirement for low-bit inference on most current deep learning framework is that you need to maintain the same precision of bits for all the data. If weights are quantized with different bits, the requirement cannot be satisfied, and thereon cannot be accelerated. Nevertheless, weight quantization still can reduce the model size. However, for activation quantization, it can neither reduce the model size nor be accelerated, which brings no gain.
Of course we can do RL based quantization on activations theoretically, but only when it can bring some practical advantages will we implement it.
		</comment>
		<comment id='34' author='brisker' date='2018-12-06T10:47:29Z'>
		&lt;denchmark-link:https://github.com/haolibai&gt;@haolibai&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/jiaxiang-wu&gt;@jiaxiang-wu&lt;/denchmark-link&gt;

Will  Pocketflow support RL-agent reward signal directly from hardware, like lantency or energy consumption?
		</comment>
		<comment id='35' author='brisker' date='2018-12-06T11:52:45Z'>
		I am not sure.
		</comment>
		<comment id='36' author='brisker' date='2018-12-06T12:18:24Z'>
		&lt;denchmark-link:https://github.com/haolibai&gt;@haolibai&lt;/denchmark-link&gt;

so until now, is there bugs found in the quantization code, regarding the RL-based method?
		</comment>
		<comment id='37' author='brisker' date='2018-12-06T12:24:49Z'>
		Not yet, we are still working on it. BTW, you're also welcomed to help us debug the RL optimization part.
		</comment>
		<comment id='38' author='brisker' date='2018-12-06T12:44:52Z'>
		&lt;denchmark-link:https://github.com/haolibai&gt;@haolibai&lt;/denchmark-link&gt;

It seems that there is no corresponding paper about the RL-based quantization part. Could you please provide any documention about the RL-based quantization algorithm? Or any related paper?
		</comment>
		<comment id='39' author='brisker' date='2018-12-06T12:49:36Z'>
		We refer to &lt;denchmark-link:http://openaccess.thecvf.com/content_ECCV_2018/papers/Yihui_He_AMC_Automated_Model_ECCV_2018_paper.pdf&gt;http://openaccess.thecvf.com/content_ECCV_2018/papers/Yihui_He_AMC_Automated_Model_ECCV_2018_paper.pdf&lt;/denchmark-link&gt;
 to realize the RL based quantization. Please check.
Edit:
Although this paper uses reinforcement learning to optimize each layer's pruning ratio for channel pruning algorithm, rather than uniform quantization, its idea of training an RL agent to optimize layer-wise hyper-parameters is adopted in our RL-based quantization algorithm.
		</comment>
		<comment id='40' author='brisker' date='2018-12-07T01:34:29Z'>
		&lt;denchmark-link:https://github.com/haolibai&gt;@haolibai&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/jiaxiang-wu&gt;@jiaxiang-wu&lt;/denchmark-link&gt;



Is this pic (from  https://www.jiqizhixin.com/articles/2018-09-17-6 )corresponding to the RL-based quantization performance for imagenet-resnet18?



Did this pic mean that w4a32 quantization model with no RL only has about 63% top1-accuracy on imagenet, and with RL the acc can rise up to 68?


Are these performance with channel bucket?


		</comment>
		<comment id='41' author='brisker' date='2018-12-07T02:00:44Z'>
		
Yes.
In later experiment we found that w4a32 without RL can also achieve acc up to 66~67 by decreasing the learning rate properly. The RL based results have some variance, but can generally achieve comparable or better results.
Without bucketing.

		</comment>
		<comment id='42' author='brisker' date='2018-12-07T02:06:30Z'>
		&lt;denchmark-link:https://github.com/haolibai&gt;@haolibai&lt;/denchmark-link&gt;

why is the lr always very very small(usually) , no matter it is RL-based method or not.  In the log you sent me &lt;denchmark-link:https://github.com/Tencent/PocketFlow/issues/126#issuecomment-444805789&gt;here&lt;/denchmark-link&gt;
, the learning rate start from 2e-6. Is it necessary to set it as small as 2e-6 ? Or the printed  is not the "real " learning rate?
		</comment>
		<comment id='43' author='brisker' date='2018-12-07T02:29:33Z'>
		Experimentally, for higher learning rate the fine-tuning of quantization will not be stable, and sometimes the loss will turn to be NaN. You are also welcome to help us find how to tune the quantized network with larger learning rate.
		</comment>
		<comment id='44' author='brisker' date='2018-12-07T03:52:38Z'>
		&lt;denchmark-link:https://github.com/haolibai&gt;@haolibai&lt;/denchmark-link&gt;

In resnet-imagenet RL experiment, the acc may be normal. But when it comes to mobilenet, the reward got too many 0 in the roll-out process. I think it is due to the uniform quantization algorithm itself ( it is not a state-of-art model quantization paper. Is it a paper from Google?). So here I am curious about the performance you mentioned &lt;denchmark-link:https://github.com/Tencent/PocketFlow/issues/115#issuecomment-443198659&gt;here&lt;/denchmark-link&gt;
. Is there also many 0 acc during the roll-out process  in the experiment there?
		</comment>
		<comment id='45' author='brisker' date='2018-12-08T05:40:13Z'>
		&lt;denchmark-link:https://github.com/haolibai&gt;@haolibai&lt;/denchmark-link&gt;

Now I can also reproduce the resnet-18-4bit rl-based experiment performance, around 67% top1-acc, but the mobilenet-rl-based experiment just still got too many 0 accs in the roll-out training process. Have you reproduce the mobilenet-imagenet-rl-experiment , except the resnet experiment &lt;denchmark-link:https://github.com/Tencent/PocketFlow/issues/115#issuecomment-443198659&gt;here&lt;/denchmark-link&gt;
  ?
		</comment>
		<comment id='46' author='brisker' date='2018-12-10T01:48:44Z'>
		Hi. Not yet. We are recently short of GPUs. We will inform you immediately once we get the result. You can also try to adjust some parameters, such as the learning rate for RL global training, and narrow down the gap between w_bit_min and w_bit_max so as to avoid too many 2 bit quantizations.
		</comment>
		<comment id='47' author='brisker' date='2018-12-25T03:57:02Z'>
		&lt;denchmark-link:https://github.com/haolibai&gt;@haolibai&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/jiaxiang-wu&gt;@jiaxiang-wu&lt;/denchmark-link&gt;

Is some bug found now, regarding to the RL-quantization code?
BTW,

I found a bug in the code here: https://github.com/Tencent/PocketFlow/blob/master/learners/uniform_quantization/learner.py#L279
I think it should be
images.set_shape((FLAGS.batch_size_eval, images.shape[1], images.shape[2],images.shape[3]))
not
images.set_shape((FLAGS.batch_size, images.shape[1], images.shape[2],images.shape[3]))

		</comment>
		<comment id='48' author='brisker' date='2019-01-02T00:15:17Z'>
		Yes, it should be  instead of . Thanks for pointing it out.
For the potential bug in the RL-quantizaiton code, our team does not have enough people to cover that, at least for the moment. If it is urgent, you can try to debug it by yourself.
&lt;denchmark-link:https://github.com/brisker&gt;@brisker&lt;/denchmark-link&gt;

		</comment>
		<comment id='49' author='brisker' date='2019-01-03T02:38:18Z'>
		&lt;denchmark-link:https://github.com/jiaxiang-wu&gt;@jiaxiang-wu&lt;/denchmark-link&gt;

when will the 2.0 version be released? what will be new features?
		</comment>
		<comment id='50' author='brisker' date='2019-01-03T03:07:51Z'>
		&lt;denchmark-link:https://github.com/brisker&gt;@brisker&lt;/denchmark-link&gt;

Possibly at the end of 2019Q1. We are now adding support for object detection models ( SSD) and RNN/LSTM models.
		</comment>
		<comment id='51' author='brisker' date='2019-04-03T02:54:12Z'>
		&lt;denchmark-link:https://github.com/brisker&gt;@brisker&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/haolibai&gt;@haolibai&lt;/denchmark-link&gt;
  Have you reproduce the mobilenet-imagenet-rl-experiment , as reported &lt;denchmark-link:https://github.com/Tencent/PocketFlow/issues/115#issuecomment-443198659&gt;here&lt;/denchmark-link&gt;
,  I folowed the default scripts and encountered the same acc 0 issue as &lt;denchmark-link:https://github.com/brisker&gt;@brisker&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>