<bug id='117' author='darnox' open_date='2017-10-28T18:02:31Z' closed_time='2018-05-17T03:54:38Z'>
	<summary>NaN values in reward</summary>
	<description>
Hi,
I'm training my model but at some point, a strange thing happens. The reward returned by PPO sometimes has 'nan' value. It occurs in different places (not necessarily on specific epoch) and after a few steps, it can get back to correct float values and then once again return 'nan' in a future. It doesn't apply only to Mean Reward, the cumulative_reward shown in tensorboard is broken at this epochs as well. I'm attaching part of an output from training process. Maybe someone knows why it's happening.
Thanks

Mean Reward: -82.99281905388821
Saved Model
Mean Reward: -48.54445689334988
Saved Model
Mean Reward: -37.32348710034999
Saved Model
D:\ProgramData\Miniconda3\envs\unity\lib\site-packages\numpy\core\fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.
out=out, **kwargs)
D:\ProgramData\Miniconda3\envs\unity\lib\site-packages\numpy\core_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars
ret = ret.dtype.type(ret / rcount)
Mean Reward: nan
Saved Model
Mean Reward: nan
Saved Model
Mean Reward: nan
Saved Model

	</description>
	<comments>
		<comment id='1' author='darnox' date='2017-10-29T00:00:24Z'>
		Hi, it is possible that you ask for a report on the mean reward but no data point was collected yet, hence a division by 0. Can you try increasing summary-freq and save-freq in the ppo arguments and see if it solves your problem ?
		</comment>
		<comment id='2' author='darnox' date='2017-10-29T09:17:36Z'>
		I had nan error when all episodes were having exactly one step (every AgentStep call had "done"  property  set to true)
		</comment>
		<comment id='3' author='darnox' date='2017-11-09T04:16:49Z'>
		Can I work on this?
		</comment>
		<comment id='4' author='darnox' date='2017-11-11T13:36:28Z'>
		I got this error randomly when MaxStep was set to 0 in my agent component!
(ok still get the error sometimes... don't no why though)
		</comment>
		<comment id='5' author='darnox' date='2017-11-28T12:32:49Z'>
		I also have this problem, I encountered it when changing my action state vector, which initially seemed weird. But then I noticed I had changed the probability of my scripts setting academy.done to true, so I guess the episodes got to small to capture this. It seems I get this problem less often (less nan results) by setting max time steps in the academy object, and setting the summary frequency to five times that number, I still get some nan mean results though, and I think I always set a reward for each agent.
		</comment>
		<comment id='6' author='darnox' date='2017-12-15T22:50:57Z'>
		I'm getting the same problem here.  Does anyone know what causes this?  I even tried assigning a static reward each agent step but still it comes back as NaN even though I have MaxStep set at 10000 on the agent and summary frequency set to 50000.  I don't think the agent is getting done state priot to that, because I have reset on done set and reset causes a visible reset that I'm not seeing.  Really stumped on this one.
		</comment>
		<comment id='7' author='darnox' date='2017-12-15T23:27:29Z'>
		Well, it looks like setting Max Steps on the agent isn't doing anything.  Only setting Max Steps on the academy seems to force the agent to done. :(
		</comment>
		<comment id='8' author='darnox' date='2017-12-16T03:10:46Z'>
		So, this is still messing me up.  I've been stepping through the code in Heuristic mode and noticed that my reward is being reset by...
&lt;denchmark-code&gt;    foreach (Brain brain in brains)
    {
        brain.ResetDoneAndReward();
    }
&lt;/denchmark-code&gt;

In the Academy Step() method before the reward is being added by the call to SetCumulativeReward() inside CollectStates() which is called from DecideAction().  Tracing this back to RunMdp() it seems that step() is called, which resets the reward BEFORE the call to DecideAction() which gets the states and adds the reward...
&lt;denchmark-code&gt;            else
            {
                Step();
            }
        }

        DecideAction();
&lt;/denchmark-code&gt;

Why on earth would it be resetting the reward before collecting the states and adding the reward to the cumulative reward?!
In fact, does this piece of code even make sense...?
&lt;denchmark-code&gt;public void ResetDoneAndReward()
{
    foreach (Agent agent in agents.Values)
    {
        if (!agent.done || agent.resetOnDone)
        {
            agent.ResetReward();
            agent.done = false;
        }
    }
}
&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='darnox' date='2018-02-08T12:38:27Z'>
		I get the nan values when I increase the Time Scale together with decreasing the Quality Level, Time Scale = 100 and Quality Level = 0. If I used Quality Level as 5 and Time Scale as 1 everything works fine.
		</comment>
		<comment id='10' author='darnox' date='2018-03-28T23:05:25Z'>
		Hi, we released version 0.3 of ml-agents, please let us know if you are still facing this issue.
		</comment>
		<comment id='11' author='darnox' date='2020-01-03T10:20:39Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>