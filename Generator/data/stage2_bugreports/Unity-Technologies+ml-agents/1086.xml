<bug id='1086' author='choconamu' open_date='2018-08-13T10:07:59Z' closed_time='2018-12-18T00:05:20Z'>
	<summary>I want to know the reason of stop learning</summary>
	<description>
Hello. I want to run 3DBall in ml-agents of examples.
so I followed the introduction.
I installed all needs and build the example file(3DBall) - Brain----&gt;external
after all process, I entered the jupyter notebook .  opend the Basics.ipynb and run.
but, suddenly stopped the learning so I can't save the model file.
I'm trying to find the reason, but I can't find it.
so I did another way. in anaconda prompt- I typed "python learn.py ./3DBall --run-id=r1 --train".
and the result is  File "learn.py", line 28, in 
''')
UnicodeEncodeError: 'cp949' codec can't encode character '\u2584' in position 28: illegal multibyte sequence
I don't know what should I do. I search in google. but I couldn't find the reason.
please let me know....
here is the jupyter notebook
Unity ML-Agents Toolkit
Environment Basics
This notebook contains a walkthrough of the basic functions of the Python API for the Unity ML-Agents toolkit. For instructions on building a Unity environment, see here.

Set environment parameters
Be sure to set env_name to the name of the Unity environment file you want to launch. Ensure that the environment build is in the python/ directory.
**In [8]:

env_name = "3DBall"  # Name of the Unity environment binary to launch
train_mode = True  # Whether to run the environment in training or inference mode**

Load dependencies
The following loads the necessary dependencies and checks the Python version (at runtime). ML-Agents Toolkit (v0.3 onwards) requires Python 3.
**In [9]:

import matplotlib.pyplot as plt
import numpy as np
import sys
​
from unityagents import UnityEnvironment
​
%matplotlib inline
​
print("Python version:")
print(sys.version)
​
&lt;denchmark-h:h1&gt;check Python version&lt;/denchmark-h&gt;

if (sys.version_info[0] &lt; 3):
raise Exception("ERROR: ML-Agents Toolkit (v0.3 onwards) requires Python 3")**
Python version:
3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]

Start the environment
UnityEnvironment launches and begins communication with the environment when instantiated.
Environments contain brains which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python.
**In [10]:

env = UnityEnvironment(file_name=env_name)
​
&lt;denchmark-h:h1&gt;Examine environment parameters&lt;/denchmark-h&gt;

print(str(env))
​
&lt;denchmark-h:h1&gt;Set the default brain to work with&lt;/denchmark-h&gt;

default_brain = env.brain_names[0]
brain = env.brains[default_brain]
INFO:unityagents:
'Ball3DAcademy' started successfully!
Unity Academy name: Ball3DAcademy
Number of Brains: 1
Number of External Brains : 1
Lesson number : 0
Reset Parameters :
Unity brain name: Ball3DBrain
Number of Visual Observations (per agent): 0
Vector Observation space type: continuous
Vector Observation space size (per agent): 8
Number of stacked Vector Observation: 1
Vector Action space type: continuous
Vector Action space size (per agent): 2
Vector Action descriptions: ,
Unity Academy name: Ball3DAcademy
Number of Brains: 1
Number of External Brains : 1
Lesson number : 0
Reset Parameters :
Unity brain name: Ball3DBrain
Number of Visual Observations (per agent): 0
Vector Observation space type: continuous
Vector Observation space size (per agent): 8
Number of stacked Vector Observation: 1
Vector Action space type: continuous
Vector Action space size (per agent): 2
Vector Action descriptions: ,**

Examine the observation and state spaces
We can reset the environment to be provided with an initial set of observations and states for all the agents within the environment. In ML-Agents, states refer to a vector of variables corresponding to relevant aspects of the environment for an agent. Likewise, observations refer to a set of relevant pixel-wise visuals for an agent.
**In [11]:

&lt;denchmark-h:h1&gt;Reset the environment&lt;/denchmark-h&gt;

env_info = env.reset(train_mode=train_mode)[default_brain]
​
&lt;denchmark-h:h1&gt;Examine the state space for the default brain&lt;/denchmark-h&gt;

print("Agent state looks like: \n{}".format(env_info.vector_observations[0]))
​
&lt;denchmark-h:h1&gt;Examine the observation space for the default brain&lt;/denchmark-h&gt;

for observation in env_info.visual_observations:
print("Agent observations look like:")
if observation.shape[3] == 3:
plt.imshow(observation[0,:,:,:])
else:
plt.imshow(observation[0,:,:,0])**
Agent state looks like:
[-0.01467304 -0.01468306 -0.52082086  4.         -0.79952097  0.
0.          0.        ]

Take random actions in the environment
Once we restart an environment, we can step the environment forward and provide actions to all of the agents within the environment. Here we simply choose random actions based on the action_space_type of the default brain.
Once this cell is executed, 10 messages will be printed that detail how much reward will be accumulated for the next 10 episodes. The Unity environment will then pause, waiting for further signals telling it what to do next. Thus, not seeing any animation is expected when running this cell.
**In [12]:

for episode in range(10):
env_info = env.reset(train_mode=train_mode)[default_brain]
done = False
episode_rewards = 0
while not done:
if brain.vector_action_space_type == 'continuous':
env_info = env.step(np.random.randn(len(env_info.agents),
brain.vector_action_space_size))[default_brain]
else:
env_info = env.step(np.random.randint(0, brain.vector_action_space_size,
size=(len(env_info.agents))))[default_brain]
episode_rewards += env_info.rewards[0]
done = env_info.local_done[0]
print("Total reward this episode: {}".format(episode_rewards))**
Total reward this episode: 1.3000000342726707
Total reward this episode: 1.4000000357627869
Total reward this episode: 1.2000000327825546
Total reward this episode: 0.8000000268220901
Total reward this episode: 0.8000000268220901
Total reward this episode: 1.500000037252903
Total reward this episode: 1.500000037252903
Total reward this episode: 1.9000000432133675
Total reward this episode: 0.700000025331974
Total reward this episode: 1.500000037252903

Close the environment when finished
When we are finished using an environment, we can close it with the function below.
**In [13]:

env.close()**
	</description>
	<comments>
		<comment id='1' author='choconamu' date='2018-08-14T18:21:41Z'>
		Hi &lt;denchmark-link:https://github.com/choconamu&gt;@choconamu&lt;/denchmark-link&gt;
,
The issue with learn.py seems to be due to the Unity logo we print when the script is launched. Could you try removing these lines?
&lt;denchmark-code&gt;print('''
    
                    ▄▄▄▓▓▓▓
               ╓▓▓▓▓▓▓█▓▓▓▓▓
          ,▄▄▄m▀▀▀'  ,▓▓▓▀▓▓▄                           ▓▓▓  ▓▓▌
        ▄▓▓▓▀'      ▄▓▓▀  ▓▓▓      ▄▄     ▄▄ ,▄▄ ▄▄▄▄   ,▄▄ ▄▓▓▌▄ ▄▄▄    ,▄▄
      ▄▓▓▓▀        ▄▓▓▀   ▐▓▓▌     ▓▓▌   ▐▓▓ ▐▓▓▓▀▀▀▓▓▌ ▓▓▓ ▀▓▓▌▀ ^▓▓▌  ╒▓▓▌
    ▄▓▓▓▓▓▄▄▄▄▄▄▄▄▓▓▓      ▓▀      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌   ▐▓▓▄ ▓▓▌
    ▀▓▓▓▓▀▀▀▀▀▀▀▀▀▀▓▓▄     ▓▓      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌    ▐▓▓▐▓▓
      ^█▓▓▓        ▀▓▓▄   ▐▓▓▌     ▓▓▓▓▄▓▓▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▓▄    ▓▓▓▓`
        '▀▓▓▓▄      ^▓▓▓  ▓▓▓       └▀▀▀▀ ▀▀ ^▀▀    `▀▀ `▀▀   '▀▀    ▐▓▓▌
           ▀▀▀▀▓▄▄▄   ▓▓▓▓▓▓,                                      ▓▓▓▓▀
               `▀█▓▓▓▓▓▓▓▓▓▌
                    ¬`▀▀▀█▓
                    
''')
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='choconamu' date='2018-08-20T01:55:30Z'>
		I'm so sorry that It doesn't work...
same in jupyter notebook..
But, In anacoda prompt, I succeeded.Thanks!
		</comment>
		<comment id='3' author='choconamu' date='2018-08-30T19:06:34Z'>
		Dear choconamu, I face the same problem  can u help me how to solve it.
		</comment>
		<comment id='4' author='choconamu' date='2018-08-31T12:16:56Z'>
		Oh.. I'm so sorry... well, I don't know about your problem exactly...
In my case, I followed the document but the problem was happend.
Just erase some codes... as &lt;denchmark-link:https://github.com/awjuliani&gt;@awjuliani&lt;/denchmark-link&gt;
 's says~~
And I did- following the explanation- again.
But you know, I did two ways, the first one is using jupyter notebook.
Of course I failed...
And the second(using the prompt) is worked!!
That's all.
		</comment>
		<comment id='5' author='choconamu' date='2018-08-31T13:20:48Z'>
		Thanks a lot, the second one works for me too.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Friday, August 31, 2018, choconamu ***@***.***&gt; wrote:
 Oh.. I'm so sorry... well, I don't know about your problem exactly...
 In my case, I followed the document but the problem was happend.

 Just erase some codes... as @awjuliani &lt;https://github.com/awjuliani&gt; 's
 says~~
 And I did- following the explanation- again.

 But you know, I did two ways, the first one is using jupyter notebook.
 Of course I failed...

 And the second(using the prompt) is worked!!
 That's all.

 —
 You are receiving this because you commented.
 Reply to this email directly, view it on GitHub
 &lt;#1086 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AomTzr_I5bAbcXvSFv-QsXPJYfwSnB45ks5uWSlFgaJpZM4V6Ori&gt;
 .



		</comment>
		<comment id='6' author='choconamu' date='2018-12-18T00:05:20Z'>
		I'm closing this issue because it seems the problem has been resolved. Feel free to reopen if you need to continue to discuss this.
		</comment>
		<comment id='7' author='choconamu' date='2020-01-02T21:41:28Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>