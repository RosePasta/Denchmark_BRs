<bug id='1410' author='zako42' open_date='2018-11-07T18:00:08Z' closed_time='2019-04-30T17:39:06Z'>
	<summary>Documentation suggestion: Decision Frequency and SetReward vs AddReward</summary>
	<description>
In the documentation on designing agents, examples show using AddReward() and SetReward().  The difference between the two is not super clear, but my understanding is that SetReward sets an absolute reward for the current step, while AddReward accumulates instead.  Either way the step's total reward is added to the agent's cumulative reward each step.
I ran into an issue where I had Decision Frequency &gt; 1 and rewards were not assigned as I thought they intuitively were going to be.  I think what happened was I was using SetReward(-0.005f) every step, to give the agents some urgency.  Then in the agent logic, if the agent touched a target it did a SetReward(0.5f) which overrides the earlier -0.005f.  This works fine if Decision Frequency == 1.  However when &gt; 1, it seems like the "decision step" assigns reward.  So unless the agent touched the target on that step, it receives the -0.005 punishment even though it might have touched the target in an earlier step.
For example if Decision Frequency is 1000, and the agent touches 100 targets during steps 0-999.  I intuitively thought it's supposed to get 500 in rewards cash and some small punishment from the -0.005 * the 900 steps when the agent didn't touch targets.  But the reward seems to be set to -0.005 due to the decision step getting rewards from SetReward(-0.005f).  If I change both calls to use AddReward instead, everything works intuitively.  I'm guessing that the rewards are accumulated over the 1000 steps and then added on the decision step.  And of course if I change to Decision Frequency 1, it also works.
Anyway I think this is what is happening.  I was confused (probably still confused).  My suggestion is just to help others avoid my confusion  ðŸ˜ƒ
	</description>
	<comments>
		<comment id='1' author='zako42' date='2018-11-10T03:32:36Z'>
		Hi &lt;denchmark-link:https://github.com/zako42&gt;@zako42&lt;/denchmark-link&gt;
 , yes your understanding is correct. The  variable will be sent to python and reset to 0 only on the decision frame(the frame that action information is from python side model, instead of the stale action), on other action frames, the  variable can be changed by SetReward(), and only the last SetReward() before the send will matter.
I agree this is kind of confusing from a user's point, but from a implementation perspective it kind of make sense. Maybe we should sum all of the reward we get during the action frames, and use that to push to python side.
		</comment>
		<comment id='2' author='zako42' date='2019-04-30T17:39:06Z'>
		We've updated our doc to explain this issue in this PR. So I'm closing the issue for now. Please reopen this if you feel the doc is still not enough and what improvement you wish to make. &lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/pull/1996&gt;#1996&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='zako42' date='2020-05-05T21:58:54Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>