<bug id='1519' author='BerkayDrsn' open_date='2018-12-20T22:12:04Z' closed_time='2019-04-03T22:41:53Z'>
	<summary>Brain's discrete action output exceeds given value range after a number of episodes</summary>
	<description>
&lt;denchmark-link:https://user-images.githubusercontent.com/29129751/50313637-13db9b00-04bd-11e9-9131-639f4fefab74.PNG&gt;&lt;/denchmark-link&gt;

I am training an agent brain to make my agents find a goal object (green cube) in the environment. (Much like the pyramid and banana examples). They need to learn path-finding to find the goal in the center while avoiding the walls.
This is my current observation collection code for now. (I probably need to fix this. Will mention it below)
&lt;denchmark-link:https://user-images.githubusercontent.com/29129751/50313399-3d47f700-04bc-11e9-9640-0845a69b98c6.PNG&gt;&lt;/denchmark-link&gt;

However while training the discrete action space exceeds the given range parameters after a number of episode is completed, making my code throw an exception.
My training parameters are given below.
&lt;denchmark-link:https://user-images.githubusercontent.com/29129751/50312862-c1997a80-04ba-11e9-93d4-c568cc33367e.PNG&gt;&lt;/denchmark-link&gt;

As you can see the Vector Action space size (per agent) is [3, 3, 3, 2].
First two action is for the movement. [no action, forward, backward] and [no action, right, left]. Meaning the output value range is [0, 2]. It works fine for the first 14 episode with 1000 steps each. However in the 15th episode, it begins to output "3" for the both movement action. Since 3 is not a defined action value, it throws an exception.
The problem is, in 15th episode, it ONLY outputs 3. It doesn't even produce other results, it's fixed to 3. So I can't define a 3rd action and work-around this problem since the movement completely stops.
This is my action function right now.
&lt;denchmark-link:https://user-images.githubusercontent.com/29129751/50313532-b8a9a880-04bc-11e9-8cf6-1f4be14e7452.PNG&gt;&lt;/denchmark-link&gt;

I know the observation is probably defined bad right now. They can't train them self to find the goal. As you can see the mean reward is in negative range and bounces back and forth. In overall, brain doesn't converge into any behavior and making it impossible to train right now. I will fix the observation vector in future.
I wonder if it is possible that because the training doesn't converge into any result, the neural network learns to produce values that exceeds to vector action space range defined before so maybe it can find a different approach to solution? Or maybe it is just a bug, or some feature that I don't know?
	</description>
	<comments>
		<comment id='1' author='BerkayDrsn' date='2018-12-20T22:17:20Z'>
		Are you by any chance using LSTM?
If so, it is a known problem and we are working on a fix for it.
		</comment>
		<comment id='2' author='BerkayDrsn' date='2018-12-20T22:24:25Z'>
		
Are you by any chance using LSTM?
If so, it is a known problem and we are working on a fix for it.

It shouldn't be LSTM as I haven't configured for it explicitly but I am not sure.
"use-recurrent" flag is set to false and "stacked vector size" is set to 1. I don't know any other possible configuration for LSTM.
		</comment>
		<comment id='3' author='BerkayDrsn' date='2018-12-21T01:22:33Z'>
		Hi &lt;denchmark-link:https://github.com/BerkayDrsn&gt;@BerkayDrsn&lt;/denchmark-link&gt;
, we've pushed an experimental fix for an related issue.
If you want to try it, check out the hotfix-0.6.0a branch, and see if that fixes your problem. Let me know if it helps!
Also, try turning on normalize in your hyperparameters - that might also help, as your observations (velocity and magnitude) can be potentially quite large.
		</comment>
		<comment id='4' author='BerkayDrsn' date='2018-12-22T16:20:07Z'>
		Hi &lt;denchmark-link:https://github.com/ervteng&gt;@ervteng&lt;/denchmark-link&gt;
,
I've run some tests and I'm afraid I've run into the same issue. However It took significantly longer to produce the [3, 3] output. I am in 37000th step (37th episode) right now. Good news is that I could train the model a little bit. At the very least the agents begun to converge into a desired behavior. (they moved to goal)
I wonder why would any other example doesn't run into that issue. The problem is probably in my way of approaching to the solution. I am trying to replicate the Pyramid and Banana examples, but maybe I am missing something? As far as I know, they don't stuck just like I do.
		</comment>
		<comment id='5' author='BerkayDrsn' date='2019-01-02T18:13:21Z'>
		Hi &lt;denchmark-link:https://github.com/BerkayDrsn&gt;@BerkayDrsn&lt;/denchmark-link&gt;
, this issue happens when a NaN is introduced in the model weights on the TensorFlow side. This can happen due to a divide-by-zero or a bad gradient descent update. We fixed a divide-by-zero in the hot fix branch, though the second could still happen.
Generally we try to keep both observations and rewards between -1 and 1 to reduce the probability of the second one happening. In your C# code, is there any place where a divide-by-zero could happen? Oftentimes this is when a vector has 0 magnitude. Otherwise, are you using the normalize parameter and keeping the reward to a reasonable magnitude?
Thanks!
		</comment>
		<comment id='6' author='BerkayDrsn' date='2019-01-24T02:42:31Z'>
		The problem has resolved after normalizing the observation and reward value space into [-1, 1] range.
Thank you!
		</comment>
		<comment id='7' author='BerkayDrsn' date='2019-04-03T22:41:53Z'>
		Due to inactivity, I am closing this issue for now. Please feel free to re-open if you deem it necessary.
		</comment>
		<comment id='8' author='BerkayDrsn' date='2020-04-02T23:35:23Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>