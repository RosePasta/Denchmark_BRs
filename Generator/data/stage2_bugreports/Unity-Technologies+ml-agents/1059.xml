<bug id='1059' author='arconederveen' open_date='2018-08-03T09:21:31Z' closed_time='2018-08-22T07:43:24Z'>
	<summary>Crash while training</summary>
	<description>
Hi all,
I hope you can help with the following. While training I keep getting the following error:
&lt;denchmark-code&gt;  File "learn.py", line 84, in &lt;module&gt;
    tc.start_learning()
  File "D:\data\******\Documents\src\ml-agents-0.4.0b\python\unitytrainers\trainer_controller.py", line 260, in start_learning
    trainer.add_experiences(curr_info, new_info, take_action_outputs[brain_name])
  File "D:\data\******\Documents\src\ml-agents-0.4.0b\python\unitytrainers\ppo\trainer.py", line 308, in add_experiences
    intrinsic_rewards = self.generate_intrinsic_rewards(curr_info, next_info)
  File "D:\data\******\Documents\src\ml-agents-0.4.0b\python\unitytrainers\ppo\trainer.py", line 266, in generate_intrinsic_rewards
    feed_dict=feed_dict) * float(self.has_updated)
  File "C:\Users\******\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py", line 889, in run
    run_metadata_ptr)
  File "C:\Users\******\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py", line 1096, in _run
    % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))
ValueError: Cannot feed value of shape (0,) for Tensor 'PyramidBrain/vector_observation:0', which has shape '(?, 194)'
&lt;/denchmark-code&gt;

My environment is an adoption of the pyramids environment. But with a few changes. I train two brains in the environment which are coupled to two agents. The agents are instantiated every episode. During an episode one agent can die. It should remain dead until the episode ends. So it sets itself to done and destroys itself with a slight delay.
The error does not appear after a fixed number of steps.
I can train successfully with two agents which do not instantiate every new episode.
Using:
Unity 2017.2.0f3
ml-agents v0.4.0b
tensforflow 1.4.0
Agent settings:
Reset On Done: unchecked
Max Step 5000
Acadamy
Max Step 5000
The acadamy fires an event on reset
&lt;denchmark-code&gt;    public override void AcademyReset()
    {
        AcadamyResetHandlerE.Invoke();
    }
&lt;/denchmark-code&gt;

One of the functions called in response to this event is:
&lt;denchmark-code&gt;    void InstantiateAgents()
    {
        if(Agent1 != null &amp;&amp; !Agent1.IsDone())
        {
            Agent1.Done();
        }

        if (Agent2 != null &amp;&amp; !Agent2.IsDone())
        {
            Agent2.Done();
        }

        var agent1GO = Instantiate(AgentPrefab.gameObject);
        Agent1 = agent1GO.GetComponent&lt;MyPyramidAgent&gt;();
        Agent1.GiveBrain(Brain1);

        Agent1.GoalReached += OnGoalReached;
        SwitchLogic.SwitchStateChanged += Agent1.OnSwitchStateChanged;
        FireControl.AddAgent(Agent1);

        var agent2GO = Instantiate(AgentPrefab.gameObject);
        Agent2 = agent2GO.GetComponent&lt;MyPyramidAgent&gt;();
        Agent2.GiveBrain(Brain2);

        Agent2.GoalReached += OnGoalReached;
        SwitchLogic.SwitchStateChanged += Agent2.OnSwitchStateChanged;
        FireControl.AddAgent(Agent2);
    }
&lt;/denchmark-code&gt;

The agent destroys itself thusly:
&lt;denchmark-code&gt;    public override void AgentOnDone()
    {
        agentRb.velocity = Vector3.zero;
        agentRb.isKinematic = true;
        agentRb.detectCollisions = false;
        transform.GetComponent&lt;BoxCollider&gt;().enabled = false;

        Destroy(gameObject, 200f);
    }
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://drive.google.com/open?id=1N0sOsnZyrRnoH7CXXK0cKR5S4bCTFcjp&gt;Complete project&lt;/denchmark-link&gt;

Please let me know if you need any further information.
Kind regards,
Arco
	</description>
	<comments>
		<comment id='1' author='arconederveen' date='2018-08-03T17:41:36Z'>
		hi &lt;denchmark-link:https://github.com/arconederveen&gt;@arconederveen&lt;/denchmark-link&gt;
 - thanks for reaching out.  I had trouble downloading your project.  can you copy/paste your agent script?
		</comment>
		<comment id='2' author='arconederveen' date='2018-08-03T18:53:23Z'>
		Thanks for your response! Here you go:
P.s. I also uploaded a &lt;denchmark-link:https://drive.google.com/open?id=1qPU4rtSE1e7-KVe0RSEcg9E7NXGHDAUb&gt;cleaned up version&lt;/denchmark-link&gt;
. To save space the ml-agents folder is empty.
&lt;denchmark-code&gt;using System;
using System.Collections;
using System.Collections.Generic;
using System.Linq;
using UnityEngine;
using Random = UnityEngine.Random;
using MLAgents;

public class MyPyramidAgent : Agent
{
    private Rigidbody agentRb;
    private RayPerception rayPer;

    private int shotsTotal = 50;
    private int shotsFired = 0;

    public bool useVectorObs;

    public delegate void FireRequestHandler(MyPyramidAgent agent);
    public event FireRequestHandler FireRequest = delegate { };

    public delegate void GoalReachedHandler(MyPyramidAgent agent);
    public event GoalReachedHandler GoalReached = delegate { };

    bool switchState = false;

    public override void InitializeAgent()
    {
        base.InitializeAgent();
        agentRb = GetComponent&lt;Rigidbody&gt;();
        rayPer = GetComponent&lt;RayPerception&gt;();
    }

    public override void CollectObservations()
    {
        if (useVectorObs)
        {
            const float rayDistance = 35f;
            float[] rayAngles = { 20f, 90f, 160f, 45f, 135f, 70f, 110f };
            float[] rayAngles1 = { 25f, 95f, 165f, 50f, 140f, 75f, 115f };
            float[] rayAngles2 = { 15f, 85f, 155f, 40f, 130f, 65f, 105f };

            string[] detectableObjects = { "block", "wall", "goal", "switchOff", "switchOn", "stone", "agent" };

            // disable self detection
            transform.GetComponent&lt;BoxCollider&gt;().enabled = false;

            AddVectorObs(rayPer.Perceive(rayDistance, rayAngles, detectableObjects, 0f, 0f));
            AddVectorObs(rayPer.Perceive(rayDistance, rayAngles1, detectableObjects, 0f, 5f));
            AddVectorObs(rayPer.Perceive(rayDistance, rayAngles2, detectableObjects, 0f, 10f));
            AddVectorObs(switchState);
            AddVectorObs(transform.InverseTransformDirection(agentRb.velocity));
            AddVectorObs(1f - (float)shotsFired / (float)shotsTotal);

            // eanble detection
            transform.GetComponent&lt;BoxCollider&gt;().enabled = true;
        }
    }

    public void HitByBullet()
    {
        Debug.Log("Hit by bullet");
        SetReward(-1f);
        Done();
    }

    public void MoveAgent(float[] act)
    {
        var dirToGo = Vector3.zero;
        var rotateDir = Vector3.zero;

        if (brain.brainParameters.vectorActionSpaceType == SpaceType.continuous)
        {
            dirToGo = transform.forward * Mathf.Clamp(act[0], -1f, 1f);
            rotateDir = transform.up * Mathf.Clamp(act[1], -1f, 1f);
        }
        else
        {
            var action = Mathf.FloorToInt(act[0]);
            switch (action)
            {
                case 0:
                    dirToGo = transform.forward * 1f;
                    break;
                case 1:
                    dirToGo = transform.forward * -1f;
                    break;
                case 2:
                    rotateDir = transform.up * 1f;
                    break;
                case 3:
                    rotateDir = transform.up * -1f;
                    break;
                case 4:
                    if (shotsFired &lt; shotsTotal)
                    {
                        shotsFired++;
                        FireRequest.Invoke(this);
                    }
                    break;
            }
        }
        transform.Rotate(rotateDir, Time.deltaTime * 200f);
        agentRb.AddForce(dirToGo * 2f, ForceMode.VelocityChange);
    }

    public override void AgentAction(float[] vectorAction, string textAction)
    {
        AddReward(-1f / agentParameters.maxStep);
        MoveAgent(vectorAction);
    }

    public override void AgentReset()
    {
        //var enumerable = Enumerable.Range(0, 9).OrderBy(x =&gt; Guid.NewGuid()).Take(9);
        //var items = enumerable.ToArray();

        //myArea.CleanPyramidArea();

        agentRb.velocity = Vector3.zero;
        //myArea.PlaceObject(gameObject, items[0]);
        //transform.rotation = Quaternion.Euler(new Vector3(0f, Random.Range(0, 360)));

        //switchLogic.ResetSwitch(items[1], items[2]);
        //myArea.CreateStonePyramid(1, items[3]);
        //myArea.CreateStonePyramid(1, items[4]);
        //myArea.CreateStonePyramid(1, items[5]);
        //myArea.CreateStonePyramid(1, items[6]);
        //myArea.CreateStonePyramid(1, items[7]);
        //myArea.CreateStonePyramid(1, items[8]);
    }

    private void OnCollisionEnter(Collision collision)
    {
        if (collision.gameObject.CompareTag("goal"))
        {
            Debug.Log("Goal reached");
            SetReward(2f);
            Done();

            GoalReached.Invoke(this);
        }
    }

    public override void AgentOnDone()
    {
        agentRb.velocity = Vector3.zero;
        agentRb.isKinematic = true;
        agentRb.detectCollisions = false;
        transform.GetComponent&lt;BoxCollider&gt;().enabled = false;

        Destroy(gameObject, 200f);
    }

    public void OnSwitchStateChanged(bool state)
    {
        //Debug.Log("Switch Changed: " + state);
        switchState = state;
    }
}
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='arconederveen' date='2018-08-06T11:41:16Z'>
		Tried to see which observation vector has the wrong length and added the following debug code on the python side:
&lt;denchmark-code&gt;    def log_obs_vec(self, obs_vec, format_str):
        if isinstance(obs_vec, list):
            logger.debug('obs_vec is list: length %d: ', len(obs_vec))
            for v in obs_vec:
                 logger.debug(format_str, str(v.shape))
        else:
            logger.debug(format_str, str(obs_vec.shape))

    def generate_intrinsic_rewards(self, curr_info, next_info):
        """
        Generates intrinsic reward used for Curiosity-based training.
        :BrainInfo curr_info: Current BrainInfo.
        :BrainInfo next_info: Next BrainInfo.
        :return: Intrinsic rewards for all agents.
        """
        if self.use_curiosity:
            feed_dict = {self.model.batch_size: len(next_info.vector_observations), self.model.sequence_length: 1}
            if self.is_continuous_action:
                feed_dict[self.model.output] = next_info.previous_vector_actions
            else:
                feed_dict[self.model.action_holder] = next_info.previous_vector_actions.flatten()

            if curr_info.agents != next_info.agents:
                curr_info = self.construct_curr_info(next_info)

            if self.use_visual_obs:
                for i in range(len(curr_info.visual_observations)):
                    feed_dict[self.model.visual_in[i]] = curr_info.visual_observations[i]
                    feed_dict[self.model.next_visual_in[i]] = next_info.visual_observations[i]
            if self.use_vector_obs:
                self.log_obs_vec(curr_info.vector_observations, 'Shape vec obs curr: %s')                
                feed_dict[self.model.vector_in] = curr_info.vector_observations
                self.log_obs_vec(next_info.vector_observations, 'Shape vec obs next: %s')
                feed_dict[self.model.next_vector_in] = next_info.vector_observations
            if self.use_recurrent:
                if curr_info.memories.shape[1] == 0:
                    curr_info.memories = np.zeros((len(curr_info.agents), self.m_size))
                feed_dict[self.model.memory_in] = curr_info.memories
            intrinsic_rewards = self.sess.run(self.model.intrinsic_reward,
                                              feed_dict=feed_dict) * float(self.has_updated)
            return intrinsic_rewards
        else:
            return None
&lt;/denchmark-code&gt;

Error seems to occur when next_info.vector_observations is a list of length one with a numpy array of shape (0,)
&lt;denchmark-code&gt;obs_vec is list: length 0: 
Shape vec obs next: (0,)
&lt;/denchmark-code&gt;

Hope this helps.
		</comment>
		<comment id='4' author='arconederveen' date='2018-08-07T08:13:20Z'>
		&lt;denchmark-link:https://github.com/unityjeffrey&gt;@unityjeffrey&lt;/denchmark-link&gt;
 The error is triggered when calling  in . I tried to move the Done() to  and  but the error is still triggered.
		</comment>
		<comment id='5' author='arconederveen' date='2018-08-07T16:26:27Z'>
		&lt;denchmark-link:https://github.com/arconederveen&gt;@arconederveen&lt;/denchmark-link&gt;
 Hi, can you try without curiosity and tell us if the error still appears ?
		</comment>
		<comment id='6' author='arconederveen' date='2018-08-08T08:24:11Z'>
		&lt;denchmark-link:https://github.com/vincentpierre&gt;@vincentpierre&lt;/denchmark-link&gt;
 Hi, without curiosity the crash does not happen.
		</comment>
		<comment id='7' author='arconederveen' date='2018-08-13T20:54:28Z'>
		Hi &lt;denchmark-link:https://github.com/arconederveen&gt;@arconederveen&lt;/denchmark-link&gt;
 I was not able to download your project but I tested curiosity in a context similar to the one you described. Could you try out the fix on the  branch and tell us if you still see this error ?
		</comment>
		<comment id='8' author='arconederveen' date='2018-08-14T11:49:15Z'>
		Hi &lt;denchmark-link:https://github.com/vincentpierre&gt;@vincentpierre&lt;/denchmark-link&gt;
 I have patched my version of learn.py with your change and still get the same error.  When I use the develop-cold.. branch I run into issue &lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/issues/1041&gt;#1041&lt;/denchmark-link&gt;
.
P.s. I noticed is that an observation is still requested on the unity side from the agent even after it is set to done.
I don't understand why the download does not work but here is another try through &lt;denchmark-link:https://we.tl/iAgFQ15Us8&gt;another provider&lt;/denchmark-link&gt;

&lt;denchmark-code&gt; File "learn.py", line 102, in &lt;module&gt;
    tc.start_learning()
  File "D:\repos\mlrepos\ml-agents-0.4.0b\python\unitytrainers\trainer_controller.py", line 260, in start_learning
    trainer.add_experiences(curr_info, new_info, take_action_outputs[brain_name])
  File "D:\repos\mlrepos\ml-agents-0.4.0b\python\unitytrainers\ppo\trainer.py", line 323, in add_experiences
    intrinsic_rewards = self.generate_intrinsic_rewards(curr_info, next_info)
  File "D:\repos\mlrepos\ml-agents-0.4.0b\python\unitytrainers\ppo\trainer.py", line 281, in generate_intrinsic_rewards
    feed_dict=feed_dict) * float(self.has_updated)
  File "C:\Users\******\AppData\Local\Continuum\anaconda3\envs\unitymlagents\lib\site-packages\tensorflow\python\client\session.py", line 900, in run
    run_metadata_ptr)
  File "C:\Users\******\AppData\Local\Continuum\anaconda3\envs\unitymlagents\lib\site-packages\tensorflow\python\client\session.py", line 1111, in _run
    str(subfeed_t.get_shape())))
ValueError: Cannot feed value of shape (0,) for Tensor 'PyramidBrain/vector_observation:0', which has shape '(?, 194)'
&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='arconederveen' date='2018-08-20T16:55:54Z'>
		&lt;denchmark-link:https://github.com/arconederveen&gt;@arconederveen&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/awjuliani&gt;@awjuliani&lt;/denchmark-link&gt;

I was able to reproduce your error. I did not look to deep into the code but it seems the error is due to the number of agents dropping to 0 during one step. The curiosity module seems unable to handle the case where no agents are present in the scene (even temporarily)
Can you try this fix and tell us if it helped ?
line 250 of , replace
        if self.use_curiosity:
            feed_dict = {self.model.batch_size: len(next_info.vector_observations), self.model.sequence_length: 1}
            if self.is_continuous_action:
                feed_dict[self.model.output] = next_info.previous_vector_actions
            else:
                feed_dict[self.model.action_holder] = next_info.previous_vector_actions.flatten()

            if curr_info.agents != next_info.agents:
                curr_info = self.construct_curr_info(next_info)

            if self.use_visual_obs:
                for i in range(len(curr_info.visual_observations)):
with
        if self.use_curiosity:
            feed_dict = {self.model.batch_size: len(next_info.vector_observations), self.model.sequence_length: 1}
            if self.is_continuous_action:
                feed_dict[self.model.output] = next_info.previous_vector_actions
            else:
                feed_dict[self.model.action_holder] = next_info.previous_vector_actions.flatten()

            if curr_info.agents != next_info.agents:
                curr_info = self.construct_curr_info(next_info)

            ### Change here
            if len(curr_info.agents) == 0:
                return []
            ### End of change

            if self.use_visual_obs:
                for i in range(len(curr_info.visual_observations)):
		</comment>
		<comment id='10' author='arconederveen' date='2018-08-21T12:01:28Z'>
		&lt;denchmark-link:https://github.com/vincentpierre&gt;@vincentpierre&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/awjuliani&gt;@awjuliani&lt;/denchmark-link&gt;

This seems to work! I can no longer trigger the error. Thanks!
		</comment>
		<comment id='11' author='arconederveen' date='2020-01-03T05:03:29Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>