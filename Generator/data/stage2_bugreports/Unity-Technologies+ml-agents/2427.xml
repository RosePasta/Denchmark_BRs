<bug id='2427' author='sunrui19941128' open_date='2019-08-12T01:11:19Z' closed_time='2019-12-17T02:17:34Z'>
	<summary>KEY ERROR</summary>
	<description>
I got this error when I started Unity while imitating learning!!!!!!Help Me  Thanks!ML-Agents0.9.0
Traceback (most recent call last):
File "D:\Anaconda3\envs\ml-agents-tutorial\Scripts\mlagents-learn-script.py", line 11, in 
load_entry_point('mlagents', 'console_scripts', 'mlagents-learn')()
File "e:\githubproject\ml-agents\ml-agents\mlagents\trainers\learn.py", line 319, in main
run_training(0, run_seed, options, Queue())
File "e:\githubproject\ml-agents\ml-agents\mlagents\trainers\learn.py", line 118, in run_training
tc.start_learning(env, trainer_config)
File "e:\githubproject\ml-agents\ml-agents\mlagents\trainers\trainer_controller.py", line 297, in start_learning
n_steps = self.advance(env_manager)
File "e:\githubproject\ml-agents\ml-agents-envs\mlagents\envs\timers.py", line 210, in wrapped
return func(*args, **kwargs)
File "e:\githubproject\ml-agents\ml-agents\mlagents\trainers\trainer_controller.py", line 364, in advance
new_step_infos = env.step()
File "e:\githubproject\ml-agents\ml-agents-envs\mlagents\envs\subprocess_env_manager.py", line 166, in step
self._queue_steps()
File "e:\githubproject\ml-agents\ml-agents-envs\mlagents\envs\subprocess_env_manager.py", line 159, in _queue_steps
env_action_info = self._take_step(env_worker.previous_step)
File "e:\githubproject\ml-agents\ml-agents-envs\mlagents\envs\timers.py", line 210, in wrapped
return func(*args, **kwargs)
File "e:\githubproject\ml-agents\ml-agents-envs\mlagents\envs\subprocess_env_manager.py", line 253, in _take_step
all_action_info[brain_name] = self.policies[brain_name].get_action(
KeyError: 'RollerBallPlayer'
&lt;denchmark-link:https://user-images.githubusercontent.com/34094384/62842010-1f483480-bce1-11e9-8c0e-71f0627f677f.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='sunrui19941128' date='2019-08-12T15:41:23Z'>
		did you find a solution? I am hitting the same problem (suspect it is something simple) same setup trains offline but will not train online
		</comment>
		<comment id='2' author='sunrui19941128' date='2019-08-12T16:59:29Z'>
		Hi &lt;denchmark-link:https://github.com/sunrui19941128&gt;@sunrui19941128&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/mnsmuts&gt;@mnsmuts&lt;/denchmark-link&gt;
, can you post your yaml config file?
		</comment>
		<comment id='3' author='sunrui19941128' date='2019-08-13T01:21:32Z'>
		Ok  Immediately submitted I am copy  trainer_config.yaml  ,and modify config default.See the pictures for the details!
&lt;denchmark-link:https://user-images.githubusercontent.com/34094384/62908661-b8438200-bdab-11e9-932d-05d3d7b19e11.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='sunrui19941128' date='2019-08-13T01:24:33Z'>
		I didn't have this problem in version 0.8.1,But this problem was encountered in 0.9.0,Now I am using 0.8.1, so I can train the Demo normally.I tried a lot in 0.9.0 but nothing worked!Later it was reloaded with a 0.8.1 environment to train normally!

Hi @sunrui19941128, @mnsmuts, can you post your yaml config file?

I didn't have this problem in version 0.8.1,But this problem was encountered in 0.9.0,Now I am using 0.8.1, so I can train the Demo normally.I tried a lot in 0.9.0 but nothing worked!Later it was reloaded with a 0.8.1 environment to train normally!
		</comment>
		<comment id='5' author='sunrui19941128' date='2019-08-13T01:25:20Z'>
		
Hi @sunrui19941128, @mnsmuts, can you post your yaml config file?

&lt;denchmark-link:https://user-images.githubusercontent.com/34094384/62908763-40c22280-bdac-11e9-931c-793ea5de523d.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='sunrui19941128' date='2019-08-13T06:37:39Z'>
		Can you check if RollerBallPlayer exist in Broadcast Hub in the inspector window of your  Academy?
If it is TRUE, please try to remove RollerBallPlayer from the inspector window of the Academy.
		</comment>
		<comment id='7' author='sunrui19941128' date='2019-08-13T06:44:48Z'>
		any issues with it?
		</comment>
		<comment id='8' author='sunrui19941128' date='2019-08-13T06:50:06Z'>
		I'm not sure if issue is around there.  However, I faced the same issue. And I found it as workaround.
		</comment>
		<comment id='9' author='sunrui19941128' date='2019-08-13T06:54:09Z'>
		
我不确定问题是否存在。但是，我遇到了同样的问题。我发现它是一种解决方法。
ml-agents whitch version  are you using now?

		</comment>
		<comment id='10' author='sunrui19941128' date='2019-08-13T06:59:13Z'>
		I'm using 0.9.0.
		</comment>
		<comment id='11' author='sunrui19941128' date='2019-08-13T07:02:29Z'>
		I tried it and it didn't work with 0.9.0!!  Then it worked normally in 0.8.1!!It's really frustrating
		</comment>
		<comment id='12' author='sunrui19941128' date='2019-08-13T07:16:37Z'>
		I understand your frustration.
Do you mean that "KeyError: 'RollerBall Player'" is still shown in the error message even if you remove it from the inspector window in 0.9.0?
		</comment>
		<comment id='13' author='sunrui19941128' date='2019-08-13T09:27:54Z'>
		Hi, I can train in v0.8.2 but get the key error in v0.9.X. I am currently trying to roll back my environments to get back to a working system. removing the player brain made no difference, yaml file
&lt;denchmark-link:https://user-images.githubusercontent.com/22680369/62930751-ebd6da00-bdb4-11e9-80e9-ea75359750f4.png&gt;&lt;/denchmark-link&gt;

pretty much nothing in it except the default - unless that is not allowed?
		</comment>
		<comment id='14' author='sunrui19941128' date='2019-08-13T09:33:22Z'>
		In my case I get :
&lt;denchmark-link:https://user-images.githubusercontent.com/22680369/62930978-66075e80-bdb5-11e9-8c33-7020c38752ac.png&gt;&lt;/denchmark-link&gt;

then if I remove the player brain:
&lt;denchmark-link:https://user-images.githubusercontent.com/22680369/62931165-c3031480-bdb5-11e9-96df-aa8925e760ee.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='15' author='sunrui19941128' date='2019-08-13T09:34:56Z'>
		this is version v0.9.1 and I am removing the player brain from the academy.
		</comment>
		<comment id='16' author='sunrui19941128' date='2019-08-13T09:52:35Z'>
		Just in case it helps - here is the anaconda prompt when trying exactly the same training (imitation 3DBall) in v0.8.2
&lt;denchmark-link:https://user-images.githubusercontent.com/22680369/62932617-62290b80-bdb8-11e9-8682-38397baa789d.png&gt;&lt;/denchmark-link&gt;

Seems to work well, which is why it feels like v0.9.1 has a bug
		</comment>
		<comment id='17' author='sunrui19941128' date='2019-08-13T12:29:07Z'>
		
Can you check if RollerBallPlayer exist in Broadcast Hub in the inspector window of your Academy?
If it is TRUE, please try to remove RollerBallPlayer from the inspector window of the Academy.

This removes the keyError but the model does not train
		</comment>
		<comment id='18' author='sunrui19941128' date='2019-08-13T13:27:24Z'>
		&lt;denchmark-link:https://github.com/sunrui19941128&gt;@sunrui19941128&lt;/denchmark-link&gt;
 Can you share the screen capture of your inspector window of the Academy?
		</comment>
		<comment id='19' author='sunrui19941128' date='2019-08-14T00:51:59Z'>
		
@ sunrui19941128你可以分享学院督察窗口的截屏吗？

Ok,this 0.9.0version screenshort  with Acadamy !  Picture one is No delete PlayerBrain .  Picture Two is deleted PlayerBrain! I tried your idea!but There are key errors!I suspect a version problem!sam time 0.8.1 no Problem!
&lt;denchmark-link:https://user-images.githubusercontent.com/34094384/62987037-9e1fa780-be70-11e9-85d8-16dbc24b0768.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/34094384/62987060-b42d6800-be70-11e9-8a95-9b8914cee955.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='20' author='sunrui19941128' date='2019-08-14T07:34:48Z'>
		Thank you for sharing it.
As you said, there seems to be bug around there since the behavior differs between the two versions
To train RollerBallBrain (not RollerBallPlayer), we have to drag the RollerBallBrain asset to the RollerAgent GameObject Brain field to the learning brain.
Can you check your Brain field of RollerAgent?
		</comment>
		<comment id='21' author='sunrui19941128' date='2019-08-14T09:47:25Z'>
		Hi Yes all correct, the agent has a learning brain, the academy has a learning brain without control when creating a build for offline training (.demo), or if I am online training,  both a learning brain with control and a player brain.
		</comment>
		<comment id='22' author='sunrui19941128' date='2019-08-14T19:38:02Z'>
		Hey &lt;denchmark-link:https://github.com/mnsmuts&gt;@mnsmuts&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/yamashin0922&gt;@yamashin0922&lt;/denchmark-link&gt;
, I can confirm that there is a bug in v0.9.1 that is preventing PlayerBrains from being used. I've pushed a fix to the branch  - try checking out that branch and see if it fixes your issue.
If you want to edit yourself, add if brain_name in self.policies: at line 253 in subprocess_env_manager.py. Thank you for bringing this up; it will be fixed in the next release.
		</comment>
		<comment id='23' author='sunrui19941128' date='2019-08-15T07:10:42Z'>
		I had the same problem moving up to vs 0.9. However, it seems removing the player brain from the academy hub solves the issue and the agent is training when control is checked. And at the same time, you can still use the player brain any way when unchecking control from the learning brain.
		</comment>
		<comment id='24' author='sunrui19941128' date='2019-08-15T13:21:27Z'>
		
Hey @mnsmuts, @yamashin0922, I can confirm that there is a bug in v0.9.1 that is preventing PlayerBrains from being used. I've pushed a fix to the branch hotfix-onlinebc - try checking out that branch and see if it fixes your issue.
If you want to edit yourself, add if brain_name in self.policies: at line 253 in subprocess_env_manager.py. Thank you for bringing this up; it will be fixed in the next release.

Many thanks
		</comment>
		<comment id='25' author='sunrui19941128' date='2019-09-12T10:53:50Z'>
		I am also having a problem with KeyError: 'reward_signals'. This is Ubuntu 18.04 with Unity 2019.1.14f1 on anaconda python 3.6
pip list and conda info shows:
mlagents           0.9.3     /home/jubei/.local/lib/python3.6/site-packages
mlagents-envs      0.9.3     /home/jubei/local/lib/python3.6/site-packages
Here is my log:
INFO:mlagents.trainers:{'--base-port': '5005',
'--curriculum': 'None',
'--debug': False,
'--docker-target-name': 'None',
'--env': 'None',
'--help': False,
'--keep-checkpoints': '5',
'--lesson': '0',
'--load': False,
'--multi-gpu': False,
'--no-graphics': False,
'--num-envs': '1',
'--num-runs': '1',
'--run-id': 'test99',
'--sampler': 'None',
'--save-freq': '50000',
'--seed': '-1',
'--slow': False,
'--train': True,
'': 'configs/maze_config.yaml'}
INFO:mlagents.envs:Start training by pressing the Play button in the Unity Editor.
INFO:mlagents.envs:
'Academy' started successfully!
Unity Academy name: Academy
Number of Brains: 1
Number of Training Brains : 1
Reset Parameters :
Unity brain name: MazeLearningBrain
Number of Visual Observations (per agent): 0
Vector Observation space size (per agent): 80
Number of stacked Vector Observation: 1
Vector Action space type: continuous
Vector Action space size (per agent): [5]
Vector Action descriptions: , , , ,
Traceback (most recent call last):
File "/home/jubei/.local/bin/mlagents-learn", line 11, in 
load_entry_point('mlagents', 'console_scripts', 'mlagents-learn')()
File "/home/jubei/coding/ml-agents/ml-agents/mlagents/trainers/learn.py", line 337, in main
run_training(0, run_seed, options, Queue())
File "/home/jubei/coding/ml-agents/ml-agents/mlagents/trainers/learn.py", line 110, in run_training
multi_gpu,
File "/home/jubei/coding/ml-agents/ml-agents/mlagents/trainers/trainer_util.py", line 89, in initialize_trainers
multi_gpu,
File "/home/jubei/coding/ml-agents/ml-agents/mlagents/trainers/ppo/trainer.py", line 47, in init
brain, trainer_parameters, training, run_id, reward_buff_cap
File "/home/jubei/coding/ml-agents/ml-agents/mlagents/trainers/rl_trainer.py", line 41, in init
if not self.trainer_parameters["reward_signals"]:
KeyError: 'reward_signals'
This project of ours was made to run with an older branch "barracuda-test-0.2.0". The error above showed up when we tried to migrate that project to the latest version of mlagents. So essentially we moved that project from barracuda-test-0.2.0 to the latest stable mlagents (0.9.3)
Could it be that the brains we created as "assets" inside the unity project were old? From that older version?
		</comment>
		<comment id='26' author='sunrui19941128' date='2019-09-12T17:06:55Z'>
		Hi &lt;denchmark-link:https://github.com/joobei&gt;@joobei&lt;/denchmark-link&gt;
, you need to define at least one reward signal in your  file. Something like:
&lt;denchmark-code&gt;reward_signal:
    extrinsic:
            strength: 1.0
            gamma: 0.99
&lt;/denchmark-code&gt;

In the examples, it works because there is a default config provided at the top of the YAML. You could also copy/paste this default config to the top of your yaml.
		</comment>
		<comment id='27' author='sunrui19941128' date='2019-09-12T17:23:13Z'>
		&lt;denchmark-link:https://github.com/ervteng&gt;@ervteng&lt;/denchmark-link&gt;
 that solved the problem. We were using an older config file. Many thanks for the quick support!
		</comment>
		<comment id='28' author='sunrui19941128' date='2019-09-19T09:41:04Z'>
		Hi I have more or less the same problem but with KeyError: 'default'.
Its my first time so im not sure the issue, im using mlagents 0.9.2
Traceback (most recent call last):
File "d:\users\danin\anaconda3\envs\ml-agents\lib\runpy.py", line 193, in _run_module_as_main
"main", mod_spec)
File "d:\users\danin\anaconda3\envs\ml-agents\lib\runpy.py", line 85, in run_code
exec(code, run_globals)
File "D:\Users\danin\Anaconda3\envs\ml-agents\Scripts\mlagents-learn.exe_main.py", line 9, in 
File "d:\users\danin\anaconda3\envs\ml-agents\lib\site-packages\mlagents\trainers\learn.py", line 337, in main
run_training(0, run_seed, options, Queue())
File "d:\users\danin\anaconda3\envs\ml-agents\lib\site-packages\mlagents\trainers\learn.py", line 110, in run_training
multi_gpu,
File "d:\users\danin\anaconda3\envs\ml-agents\lib\site-packages\mlagents\trainers\trainer_util.py", line 45, in initialize_trainers
trainer_parameters = trainer_config["default"].copy()
KeyError: 'default'
		</comment>
		<comment id='29' author='sunrui19941128' date='2019-11-30T13:27:45Z'>
		Hey. I am having a similar issue but with the newer versions a lot has changed and could not figure it out. Here is the console output:
&lt;denchmark-link:https://user-images.githubusercontent.com/41434155/69901105-71d75a80-137d-11ea-9843-3c2a18cf084a.png&gt;&lt;/denchmark-link&gt;

Any help would be appreciated!
PS. I am a noob so please forgive any stupidity :)
Ayden
		</comment>
		<comment id='30' author='sunrui19941128' date='2019-11-30T18:16:52Z'>
		&lt;denchmark-link:https://github.com/Numan4221&gt;@Numan4221&lt;/denchmark-link&gt;
 You need to have a "default" section in your trainer config's yaml file. See &lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/blob/master/config/trainer_config.yaml#L1&gt;https://github.com/Unity-Technologies/ml-agents/blob/master/config/trainer_config.yaml#L1&lt;/denchmark-link&gt;
 for an example
&lt;denchmark-link:https://github.com/AydenWasTaken&gt;@AydenWasTaken&lt;/denchmark-link&gt;
 Can you post the contents of the curriculum folder and file? I think it's expecting a "Chameleon Behavior.json" file there, but you're probably going to have an easier time if you remove the space from the behavior name.
		</comment>
		<comment id='31' author='sunrui19941128' date='2019-11-30T18:25:02Z'>
		&lt;denchmark-link:https://user-images.githubusercontent.com/41434155/69904374-3ef58c80-13a6-11ea-911e-0f230731413d.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/41434155/69904383-4fa60280-13a6-11ea-85c8-bbaf4c7d0c03.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/chriselion&gt;@chriselion&lt;/denchmark-link&gt;
 This is the second ml agents project im trying and the first had "My behavior" (the default) and it was fine when the folder was called "penguin" and the file "PenguinLearning.json" so i don't see how the two are related. Could you clarify on what exactly this behavior name is used for?
		</comment>
		<comment id='32' author='sunrui19941128' date='2019-11-30T19:02:46Z'>
		Here's where we parse the curriculum directory: &lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/blob/master/ml-agents/mlagents/trainers/meta_curriculum.py#L56&gt;https://github.com/Unity-Technologies/ml-agents/blob/master/ml-agents/mlagents/trainers/meta_curriculum.py#L56&lt;/denchmark-link&gt;

Basically the key for the dictionary is the filename.
The behavior name is used as the key to that dictionary here: &lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/blob/master/ml-agents/mlagents/trainers/trainer_util.py#L107-L109&gt;https://github.com/Unity-Technologies/ml-agents/blob/master/ml-agents/mlagents/trainers/trainer_util.py#L107-L109&lt;/denchmark-link&gt;

It sounds like the dictionary has key "ChameleonLearning" (no space) but it's trying to look up "Chameleon Learning" (with a space).
We definitely need to make the error handling better here. and maybe handle unknown behavior names.
		</comment>
		<comment id='33' author='sunrui19941128' date='2019-12-01T10:45:33Z'>
		&lt;denchmark-link:https://github.com/chriselion&gt;@chriselion&lt;/denchmark-link&gt;
 Changing the behavior name to match the filename worked wonders, thank you!
Have a very good day
		</comment>
	</comments>
</bug>