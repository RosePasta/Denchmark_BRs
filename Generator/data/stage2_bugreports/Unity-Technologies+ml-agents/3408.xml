<bug id='3408' author='adverley' open_date='2020-02-11T12:35:10Z' closed_time='2020-03-06T23:56:45Z'>
	<summary>Single agent environments contain two agents when episode is ended due to Agent maxStep</summary>
	<description>

The method get_step_result() from the &lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/blob/master/ml-agents-envs/mlagents_envs/environment.py&gt;UnityEnvironment&lt;/denchmark-link&gt;
 identifies two active agents in a single-agent environment when the env is marked as  because maxStep of the Agent is reached. This crashes the Unity gym openai wrapper when running in single agent mode.
To Reproduce
Steps to reproduce the behavior:

Go to Unity ML agents example: Basic
Change BasicAgent script parameters to Max Step = 50 and Time between decisions = 0.02
Build project
Open the introductory notebook
Change part 5 of the notebook ("5. Take random actions in the environment") to take zig-zag actions and printing the amount of agents during the episode interactions.

For example, I added three lines:
&lt;denchmark-code&gt;actions = [1, 2]

for episode in range(10):
    steps_in_episode = 0
    env.reset()
    step_result = env.get_step_result(group_name)
    done = False
    episode_rewards = 0
    while not done:
        action_size = group_spec.action_size
        if group_spec.is_action_continuous():
            action = np.random.randn(step_result.n_agents(), group_spec.action_size)
            
        if group_spec.is_action_discrete():
            branch_size = group_spec.discrete_action_branches
            action = np.column_stack([np.random.randint(0, branch_size[i], size=(step_result.n_agents())) for i in range(len(branch_size))])
            

        action = actions[int(steps_in_episode % 2)]
        action = np.array(action).reshape((1, 1))
        env.set_actions(group_name, action)
        env.step()
        step_result = env.get_step_result(group_name)
        episode_rewards += step_result.reward[0]
        done = step_result.done[0]
        steps_in_episode += 1
        
        print(f"n agents: {step_result.n_agents()}")
        if done:
            print(f'Episode is done. N agents = {step_result.n_agents()} and n steps = {steps_in_episode}')
    print("Total reward this episode: {}".format(episode_rewards))
&lt;/denchmark-code&gt;

Running this notebook cell, will show that n_agents becomes 2 when the environment is reset (due to maxSteps reached). However, once the agent has successfully completed an episode before maxSteps is reached, this problem disappears. You can reproduce this by commenting out the added lines
&lt;denchmark-code&gt;action = actions[int(steps_in_episode % 2)]
action = np.array(action).reshape((1, 1))
&lt;/denchmark-code&gt;

and running the environment again. After uncommenting these lines of code again (hence taking the zig-zag actions), n_agents will remain 1, even at the end of the episode caused by maxSteps. This is problematic when creating a custom environment which is wrapped in the Unity ml agents OpenAI gym. The latter checks at each step whether there is only one agent in the environment (see &lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/blob/master/gym-unity/gym_unity/envs/__init__.py#L110&gt;here&lt;/denchmark-link&gt;
).
Environment:

OS + version: Ubuntu 19.10
ML-Agents version: latest master branch
Environment: Basic

	</description>
	<comments>
		<comment id='1' author='adverley' date='2020-02-11T19:33:48Z'>
		Hi &lt;denchmark-link:https://github.com/adverley&gt;@adverley&lt;/denchmark-link&gt;
, thanks for reporting this and we're working on a fix. In the meantime, checkout the previous release of ML-Agents (0.13.1) which shouldn't have this issue.
		</comment>
		<comment id='2' author='adverley' date='2020-03-06T23:56:45Z'>
		This issue has been fixed in version 14.1 and in the latest Master. Closing this issue for now - feel free to reopen if you're still having problems.
		</comment>
	</comments>
</bug>