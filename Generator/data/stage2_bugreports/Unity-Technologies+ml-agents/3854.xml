<bug id='3854' author='FlimFlamm' open_date='2020-04-25T22:52:11Z' closed_time='2020-04-28T00:43:27Z'>
	<summary>Multi-Agent Env: terminal fails to load multiple behaviors; only one agent trains</summary>
	<description>
Under certain conditions, the terminal will fail to properly load multiple agent behaviors in multi-agent environments. I have not determined the exact cause of this bug, but I have discovered reliable reproduction steps:
To Reproduce
1: Add a single 3DBallHardNew agent to the 3DBall scene
2: Make Offset-step = true on the Hard agent
3: Set decision period to 6 (from 5) on the hard agent
4: initiate training
5: see that the hardball agent is not actually training, and the behavior has not loaded
Environment:
Multiple ML-Agents versions, including 14, 15, and 16dev
Note:
This bug has something to do with the nature of decision interval settings and the first requested decision. A primitive but minimalistic fix for this bug is to actually place a single RequestDecision() within the start function of each agent script, which ensures the first decision tick includes both agent classes.
	</description>
	<comments>
		<comment id='1' author='FlimFlamm' date='2020-04-26T02:37:53Z'>
		Hi &lt;denchmark-link:https://github.com/FlimFlamm&gt;@FlimFlamm&lt;/denchmark-link&gt;
,
We have removed the offset-step functionality on master and will be available in the next release.  If you'd like to have custom stepping timings for your agents, you will be able to subscribe to the  event and you can decide to call  from there.  Thank you for your feedback.  Please reopen this issue if you feel the problem was not addressed. Cheers.
		</comment>
		<comment id='2' author='FlimFlamm' date='2020-04-26T17:55:49Z'>
		Hi &lt;denchmark-link:https://github.com/FlimFlamm&gt;@FlimFlamm&lt;/denchmark-link&gt;
,
We have logged this issue internally as MLA-995 and will update this thread with any information pertaining to the issue you've reported.
		</comment>
		<comment id='3' author='FlimFlamm' date='2020-04-27T17:33:00Z'>
		I tried to reproduce on 0.14.1 by following the described steps, but I worked for me. Since I only had one Hard agent, I had to wait longer to see the summary in the console, but it trained :
&lt;denchmark-code&gt;ml-agents (release-0.14.1) $ mlagents-learn --train config/trainer_config.yaml 
WARNING:tensorflow:From /Users/vincentpierre/Documents/ml-agents/ml-agents/mlagents/tf_utils/tf.py:33: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.



                        ▄▄▄▓▓▓▓
                   ╓▓▓▓▓▓▓█▓▓▓▓▓
              ,▄▄▄m▀▀▀'  ,▓▓▓▀▓▓▄                           ▓▓▓  ▓▓▌
            ▄▓▓▓▀'      ▄▓▓▀  ▓▓▓      ▄▄     ▄▄ ,▄▄ ▄▄▄▄   ,▄▄ ▄▓▓▌▄ ▄▄▄    ,▄▄
          ▄▓▓▓▀        ▄▓▓▀   ▐▓▓▌     ▓▓▌   ▐▓▓ ▐▓▓▓▀▀▀▓▓▌ ▓▓▓ ▀▓▓▌▀ ^▓▓▌  ╒▓▓▌
        ▄▓▓▓▓▓▄▄▄▄▄▄▄▄▓▓▓      ▓▀      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌   ▐▓▓▄ ▓▓▌
        ▀▓▓▓▓▀▀▀▀▀▀▀▀▀▀▓▓▄     ▓▓      ▓▓▌   ▐▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▌    ▐▓▓▐▓▓
          ^█▓▓▓        ▀▓▓▄   ▐▓▓▌     ▓▓▓▓▄▓▓▓▓ ▐▓▓    ▓▓▓ ▓▓▓  ▓▓▓▄    ▓▓▓▓`
            '▀▓▓▓▄      ^▓▓▓  ▓▓▓       └▀▀▀▀ ▀▀ ^▀▀    `▀▀ `▀▀   '▀▀    ▐▓▓▌
               ▀▀▀▀▓▄▄▄   ▓▓▓▓▓▓,                                      ▓▓▓▓▀
                   `▀█▓▓▓▓▓▓▓▓▓▌
                        ¬`▀▀▀█▓

        
 Version information:
  ml-agents: 0.14.1,
  ml-agents-envs: 0.14.1,
  Communicator API: API-14,
  TensorFlow: 1.15.0
INFO:mlagents_envs:Listening on port 5004. Start training by pressing the Play button in the Unity Editor.
INFO:mlagents_envs:Connected new brain:
3DBallHard?team=0
INFO:mlagents_envs:Connected new brain:
3DBall?team=0
INFO:mlagents.trainers:Hyperparameters for the PPOTrainer of brain 3DBall: 
	trainer:	ppo
	batch_size:	64
	beta:	0.001
	buffer_size:	12000
	epsilon:	0.2
	hidden_units:	128
	lambd:	0.99
	learning_rate:	0.0003
	learning_rate_schedule:	linear
	max_steps:	5.0e5
	memory_size:	256
	normalize:	True
	num_epoch:	3
	num_layers:	2
	time_horizon:	1000
	sequence_length:	64
	summary_freq:	12000
	use_recurrent:	False
	vis_encode_type:	simple
	reward_signals:	
	  extrinsic:	
	    strength:	1.0
	    gamma:	0.99
	summary_path:	ppo_3DBall
	model_path:	./models/ppo/3DBall
	keep_checkpoints:	5
2020-04-27 10:09:48.988420: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-04-27 10:09:49.022876: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fd8ca6174c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-04-27 10:09:49.022910: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
INFO:mlagents.trainers:Hyperparameters for the PPOTrainer of brain 3DBallHard: 
	trainer:	ppo
	batch_size:	1200
	beta:	0.001
	buffer_size:	12000
	epsilon:	0.2
	hidden_units:	128
	lambd:	0.95
	learning_rate:	0.0003
	learning_rate_schedule:	linear
	max_steps:	5.0e5
	memory_size:	256
	normalize:	True
	num_epoch:	3
	num_layers:	2
	time_horizon:	1000
	sequence_length:	64
	summary_freq:	12000
	use_recurrent:	False
	vis_encode_type:	simple
	reward_signals:	
	  extrinsic:	
	    strength:	1.0
	    gamma:	0.995
	summary_path:	ppo_3DBallHard
	model_path:	./models/ppo/3DBallHard
	keep_checkpoints:	5
INFO:mlagents.trainers: ppo: 3DBall: Step: 12000. Time Elapsed: 38.702 s Mean Reward: 1.135. Std of Reward: 0.748. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 24000. Time Elapsed: 73.358 s Mean Reward: 1.344. Std of Reward: 0.804. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 36000. Time Elapsed: 102.274 s Mean Reward: 1.712. Std of Reward: 1.060. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 48000. Time Elapsed: 132.748 s Mean Reward: 2.479. Std of Reward: 1.648. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 60000. Time Elapsed: 163.205 s Mean Reward: 3.836. Std of Reward: 3.055. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 72000. Time Elapsed: 195.402 s Mean Reward: 6.639. Std of Reward: 5.883. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 84000. Time Elapsed: 224.050 s Mean Reward: 14.521. Std of Reward: 12.335. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 96000. Time Elapsed: 251.838 s Mean Reward: 42.504. Std of Reward: 36.762. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 108000. Time Elapsed: 275.822 s Mean Reward: 47.755. Std of Reward: 50.392. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 120000. Time Elapsed: 301.871 s Mean Reward: 67.688. Std of Reward: 50.472. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 132000. Time Elapsed: 322.468 s Mean Reward: 107.900. Std of Reward: 0.000. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 144000. No episode was completed since last summary. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 156000. Time Elapsed: 378.165 s Mean Reward: 264.400. Std of Reward: 0.000. Training.
INFO:mlagents.trainers: ppo: 3DBallHard: Step: 12000. Time Elapsed: 401.258 s Mean Reward: 0.748. Std of Reward: 0.550. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 168000. Time Elapsed: 407.692 s Mean Reward: 678.100. Std of Reward: 0.000. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 180000. No episode was completed since last summary. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 192000. No episode was completed since last summary. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 204000. No episode was completed since last summary. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 216000. No episode was completed since last summary. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 228000. No episode was completed since last summary. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 240000. No episode was completed since last summary. Training.
INFO:mlagents.trainers: ppo: 3DBall: Step: 252000. No episode was completed since last summary. Training.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='FlimFlamm' date='2020-04-27T21:06:56Z'>
		&lt;denchmark-link:https://github.com/vincentpierre&gt;@vincentpierre&lt;/denchmark-link&gt;

Can you try increasing the decision interval of the hard ball agent to 7? (this seems to be a way to trigger it)
When the bug occurs, it definitely does not train or load the second behavior at any point.
		</comment>
		<comment id='5' author='FlimFlamm' date='2020-04-27T22:14:48Z'>
		I am unable to reproduce the bug. I followed the instructions you gave. Is there anything else you changed at all? Do you have errors in Unity? If you do in editor training, are there any errors? Maybe past the player logs? If you use an executable, can you check "development build" to see potential errors?
I tried changing the decision period to 7 and 8, but it still worked. I tried to set it to an insanely large number. The agent did not train then but I still saw the hyperparameters in the console :
&lt;denchmark-code&gt;INFO:mlagents.trainers:Hyperparameters for the PPOTrainer of brain 3DBallHard: 
	trainer:	ppo
	batch_size:	1200
	beta:	0.001
	buffer_size:	12000
	epsilon:	0.2
	hidden_units:	128
	lambd:	0.95
	learning_rate:	0.0003
	learning_rate_schedule:	linear
	max_steps:	5.0e5
	memory_size:	256
	normalize:	True
	num_epoch:	3
	num_layers:	2
	time_horizon:	1000
	sequence_length:	64
	summary_freq:	12000
	use_recurrent:	False
	vis_encode_type:	simple
	reward_signals:	
	  extrinsic:	
	    strength:	1.0
	    gamma:	0.995
	summary_path:	ppo_3DBallHard
	model_path:	./models/ppo/3DBallHard
	keep_checkpoints:	5
&lt;/denchmark-code&gt;

So it connected to the trainer...
Did you modify any code besides the scene ? git status does not show any changed files ?
		</comment>
		<comment id='6' author='FlimFlamm' date='2020-04-27T22:35:12Z'>
		&lt;denchmark-link:https://github.com/vincentpierre&gt;@vincentpierre&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/24987395/80426704-9f88e700-88bc-11ea-885c-e1589c3c7728.gif&gt;&lt;/denchmark-link&gt;

The fact that today a decision interval of 5 is causing the bug but a decision interval of 6 does not is really confusing me... (I could swear that last night a decision interval of 6 was broken. This makes me have wild theories, like somehow the system calendar is related to this problem)....
Here is a gif from another ml-agents user showing the same issue
&lt;denchmark-link:https://gyazo.com/e3986c887ee855d979dd3d11bc407ec5&gt;https://gyazo.com/e3986c887ee855d979dd3d11bc407ec5&lt;/denchmark-link&gt;

The only code that I modified in the project settings override, but I have reverted those changes (and even removed the project settings override object entirely) when i was looking for the cause.
I would recommend trying all of the decision intervals from 1-10. The interval that causes the bug seems to be unreliable... At the moment (as seen in the gif, an interval of 5 with offset on the hard agent is causing the issue, and 6 works....
		</comment>
		<comment id='7' author='FlimFlamm' date='2020-04-28T00:43:27Z'>
		I finally got to reproduce the error in 0.15.0
This is a very elusive bug but basically, C# wants to send information about the two behaviors to Python. Problem is : The information is incomplete because no decisions have been requested yet (Python ignores the incomplete data received). So in the end, C# thinks it sent the data and does not need to try again but Python ignores all data sent by the second behavior because it does not recognize it.
This bug was fixed on master (commit) &lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/commit/1dd6cb6eeb7f4a50c920047f235f3a0a0f50db5c&gt;1dd6cb6&lt;/denchmark-link&gt;
 because it caused another issue that we caught.
Thank you so much for your patience in this matter.
		</comment>
	</comments>
</bug>