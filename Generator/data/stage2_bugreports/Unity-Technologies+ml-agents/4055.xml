<bug id='4055' author='hridoylego' open_date='2020-06-02T16:58:08Z' closed_time='2020-06-19T20:05:58Z'>
	<summary>mlagents_envs can't connect to unity editor</summary>
	<description>

Tried to make a gym environment using the instruction from the gym_unity README.md from here:
&lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/tree/master/gym-unity&gt;https://github.com/Unity-Technologies/ml-agents/tree/master/gym-unity&lt;/denchmark-link&gt;

Unity editor can't seem to connect with the python API. I used the grid world environment with a single agent (deleted the other ones). The unity environment is opened and grid world runs for a while before it closes and throws the error.
The environment does not need interaction to run. With the new version of mlagents, there is a behavior parameter component in the agent. The only thing I do not know how to check is if the python interface is compatible with the environment.
To Reproduce
Simply load the mlagents project folder in unity, select the grid world scene, delete additional areas and their agents, build the unity environment, copy-paste the code provided in gym_unity with appropriate changes to the directory, and run.
Console logs / stack traces
&lt;denchmark-code&gt;(ml-agents) C:\Users\hridoy\Google Drive\reinforcement research\Project\env&gt;python -m  train_unity
C:\Users\hridoy\Anaconda3\envs\ml-agents\lib\site-packages\tensorflow\python\framework\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
C:\Users\hridoy\Anaconda3\envs\ml-agents\lib\site-packages\tensorflow\python\framework\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
C:\Users\hridoy\Anaconda3\envs\ml-agents\lib\site-packages\tensorflow\python\framework\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
C:\Users\hridoy\Anaconda3\envs\ml-agents\lib\site-packages\tensorflow\python\framework\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
C:\Users\hridoy\Anaconda3\envs\ml-agents\lib\site-packages\tensorflow\python\framework\dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
C:\Users\hridoy\Anaconda3\envs\ml-agents\lib\site-packages\tensorflow\python\framework\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
2020-06-02 12:29:25 INFO [environment.py:500] Environment timed out shutting down. Killing...
Traceback (most recent call last):
  File "C:\Users\hridoy\Anaconda3\envs\ml-agents\lib\runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "C:\Users\hridoy\Anaconda3\envs\ml-agents\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "C:\Users\hridoy\Google Drive\reinforcement research\Project\env\train_unity.py", line 35, in &lt;module&gt;
    main()
  File "C:\Users\hridoy\Google Drive\reinforcement research\Project\env\train_unity.py", line 10, in main
    unity_env = UnityEnvironment("./Unity Environment")
  File "C:\Users\hridoy\Anaconda3\envs\ml-agents\lib\site-packages\mlagents_envs\environment.py", line 213, in __init__
    aca_output = self.send_academy_parameters(rl_init_parameters_in)
  File "C:\Users\hridoy\Anaconda3\envs\ml-agents\lib\site-packages\mlagents_envs\environment.py", line 601, in send_academy_parameters
    return self.communicator.initialize(inputs)
  File "C:\Users\hridoy\Anaconda3\envs\ml-agents\lib\site-packages\mlagents_envs\rpc_communicator.py", line 99, in initialize
    self.poll_for_timeout()
  File "C:\Users\hridoy\Anaconda3\envs\ml-agents\lib\site-packages\mlagents_envs\rpc_communicator.py", line 92, in poll_for_timeout
    "The Unity environment took too long to respond. Make sure that :\n"
mlagents_envs.exception.UnityTimeOutException: The Unity environment took too long to respond. Make sure that :
         The environment does not need user interaction to launch
         The Agents are linked to the appropriate Brains
         The environment and the Python interface have compatible versions.
&lt;/denchmark-code&gt;

Environment (please complete the following information):

Unity Version: [Unity 2019.3.5f1]
OS + version: [Windows 10]
ML-Agents version: ML-Agents v0.14.1
TensorFlow version: 1.7.1
Environment: Gridworld

	</description>
	<comments>
		<comment id='1' author='hridoylego' date='2020-06-08T18:42:48Z'>
		Hi &lt;denchmark-link:https://github.com/hridoylego&gt;@hridoylego&lt;/denchmark-link&gt;

It seems you're reading docs from master but using version 0.14 of ML-Agents. Can you try upgrading to the most recent release and also using the corresponding docs?
		</comment>
		<comment id='2' author='hridoylego' date='2020-06-08T19:50:49Z'>
		Sorry, I forgot to update it. Also previously forgot to clarify I used the baseline deepq algorithm as shown in the first example on gym_unity's READMD.md saved as train_unity.py . I used the updated ml-agents library on python and the ml-agents package. I install using pip install -e ./ from the updated ml-agents directory. The current master's repo has been labeled unstable.  I get the following error on cmd:
&lt;denchmark-code&gt;(ml-agents) C:\Users\hridoy\ml-agents\Project\env&gt;python -m train_unity
2020-06-08 15:40:45 INFO [environment.py:108] Connected to Unity environment with package version 1.1.0-preview and communication version 1.0.0
2020-06-08 15:40:47 INFO [environment.py:265] Connected new brain:
GridWorld?team=0
Logging to ./logs
Traceback (most recent call last):
  File "C:\Users\hridoy\Anaconda3\envs\ml-agents\lib\runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "C:\Users\hridoy\Anaconda3\envs\ml-agents\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "C:\Users\hridoy\ml-agents\Project\env\train_unity.py", line 35, in &lt;module&gt;
    main()
  File "C:\Users\hridoy\ml-agents\Project\env\train_unity.py", line 29, in main
    dueling=True
  File "c:\users\hridoy\documents\baselines\baselines\deepq\deepq.py", line 204, in learn
    num_actions=env.action_space.n,
AttributeError: 'MultiDiscrete' object has no attribute 'n'
2020-06-08 15:40:48 INFO [environment.py:418] Environment shut down with return code 0 (CTRL_C_EVENT).

&lt;/denchmark-code&gt;

Here is the unity setup from which I built the environment:
&lt;denchmark-link:https://user-images.githubusercontent.com/59928531/84073795-78cfcd00-a99f-11ea-9397-9af4205bf208.png&gt;&lt;/denchmark-link&gt;

I had to delete the agent view area or I get the error there is more than 1 agent in the scene.
Here are the files in my env directory:
&lt;denchmark-link:https://user-images.githubusercontent.com/59928531/84073920-b2a0d380-a99f-11ea-9016-ee7e0d1965bc.png&gt;&lt;/denchmark-link&gt;

Update: I repeated the procedure with the latest stable release: "release 2" and got the same multi discrete error.
		</comment>
		<comment id='3' author='hridoylego' date='2020-06-09T16:52:17Z'>
		Ah, that looks to be an issue on the gym side since its trying to access an attribute n of the action space.  Perhaps the version of gym doesn't align with the version of baselines?
		</comment>
		<comment id='4' author='hridoylego' date='2020-06-18T16:09:50Z'>
		The gym and baseline versions are compatible according to pip. I also posted the issue in the baseline repo. The deepq function of baseline takes in the environment variable which is set equal to UnityToGymWrapper() function. Ie, env = UnityToGymWrapper(args**).
It is when deepq calls on env.action_space.n do I get the error. Could it be possible the error is due to something that happens in the UnityToGymWrapper function?
my gym version: 0.15.4
baselines version: 0.1.6
		</comment>
		<comment id='5' author='hridoylego' date='2020-06-19T20:05:58Z'>
		The flatten_branched flag needed to be set to True so the space type would be Discrete instead of Multidiscrete.
Simply change:
env = UnityToGymWrapper(unity_env, 0, uint8_visual=True)
With:
env = UnityToGymWrapper(unity_env, flatten_branched = True, uint8_visual=True)
The README.md should be updated
		</comment>
	</comments>
</bug>