<bug id='2738' author='andymule' open_date='2019-10-16T07:44:53Z' closed_time='2019-11-06T00:27:51Z'>
	<summary>SAC ValueError: Cannot feed value of shape (0,) for Tensor 'policy_1/Tanh_1:0', which has shape '(?, 39)'</summary>
	<description>
To Reproduce
Run the WalkerLearning training but with SAC instead of PPO. I used the attached settings.
Console logs / stack traces
&lt;denchmark-code&gt;INFO:mlagents.envs:Hyperparameters for the SACTrainer of brain WalkerLearning:
        trainer:        sac
        batch_size:     2048
        beta:   0.005
        buffer_size:    20480
        epsilon:        0.2
        hidden_units:   512
        lambd:  0.95
        learning_rate:  0.0003
        learning_rate_schedule: constant
        max_steps:      2000000
        memory_size:    256
        normalize:      True
        num_epoch:      3
        num_layers:     3
        time_horizon:   1000
        sequence_length:        64
        summary_freq:   3000
        use_recurrent:  False
        vis_encode_type:        simple
        reward_signals:
          extrinsic:
            strength:   1.0
            gamma:      0.995
          curiosity:
            strength:   0.1
            gamma:      0.9
            encoding_size:      128
        summary_path:   ./summaries/WalkerSAC_WalkerLearning
        model_path:     ./models/WalkerSAC-0/WalkerLearning
        keep_checkpoints:       5
        buffer_init_steps:      10000
        init_entcoef:   0.9
        num_update:     1
        tau:    0.005
2019-10-16 09:35:18.531843: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1423] Adding visible gpu devices: 0
2019-10-16 09:35:18.535685: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-10-16 09:35:18.541685: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:917]      0
2019-10-16 09:35:18.544988: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:930] 0:   N
2019-10-16 09:35:18.548602: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6318 MB memory) -&gt; physical GPU (device: 0, name: GeForce RTX 2080, pci bus id: 0000:21:00.0, compute capability: 7.5)
INFO:mlagents.trainers: WalkerSAC: WalkerLearning: Step: 3000. Time Elapsed: 51.759 s Mean Reward: -7.456. Std of Reward: 3.937. Training.
INFO:mlagents.trainers: WalkerSAC: WalkerLearning: Step: 6000. Time Elapsed: 102.145 s Mean Reward: -7.426. Std of Reward: 3.899. Training.
INFO:mlagents.trainers: WalkerSAC: WalkerLearning: Step: 9000. Time Elapsed: 153.148 s Mean Reward: -7.541. Std of Reward: 4.003. Training.
Traceback (most recent call last):
  File "c:\programdata\miniconda3\envs\ml-agents\lib\runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "c:\programdata\miniconda3\envs\ml-agents\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "C:\ProgramData\Miniconda3\envs\ml-agents\Scripts\mlagents-learn.exe\__main__.py", line 9, in &lt;module&gt;
  File "c:\programdata\miniconda3\envs\ml-agents\lib\site-packages\mlagents\trainers\learn.py", line 417, in main
    run_training(0, run_seed, options, Queue())
  File "c:\programdata\miniconda3\envs\ml-agents\lib\site-packages\mlagents\trainers\learn.py", line 255, in run_training
    tc.start_learning(env)
  File "c:\programdata\miniconda3\envs\ml-agents\lib\site-packages\mlagents\trainers\trainer_controller.py", line 202, in start_learning
    n_steps = self.advance(env_manager)
  File "c:\programdata\miniconda3\envs\ml-agents\lib\site-packages\mlagents\envs\timers.py", line 263, in wrapped
    return func(*args, **kwargs)
  File "c:\programdata\miniconda3\envs\ml-agents\lib\site-packages\mlagents\trainers\trainer_controller.py", line 292, in advance
    trainer.update_policy()
  File "c:\programdata\miniconda3\envs\ml-agents\lib\site-packages\mlagents\envs\timers.py", line 263, in wrapped
    return func(*args, **kwargs)
  File "c:\programdata\miniconda3\envs\ml-agents\lib\site-packages\mlagents\trainers\sac\trainer.py", line 246, in update_policy
    self.update_reward_signals()
  File "c:\programdata\miniconda3\envs\ml-agents\lib\site-packages\mlagents\trainers\sac\trainer.py", line 334, in update_reward_signals
    reward_signal_minibatches, n_sequences
  File "c:\programdata\miniconda3\envs\ml-agents\lib\site-packages\mlagents\trainers\sac\policy.py", line 236, in update_reward_signals
    update_vals = self._execute_model(feed_dict, update_dict)
  File "c:\programdata\miniconda3\envs\ml-agents\lib\site-packages\mlagents\trainers\tf_policy.py", line 151, in _execute_model
    network_out = self.sess.run(list(out_dict.values()), feed_dict=feed_dict)
  File "c:\programdata\miniconda3\envs\ml-agents\lib\site-packages\tensorflow\python\client\session.py", line 905, in run
    run_metadata_ptr)
  File "c:\programdata\miniconda3\envs\ml-agents\lib\site-packages\tensorflow\python\client\session.py", line 1116, in _run
    str(subfeed_t.get_shape())))
ValueError: Cannot feed value of shape (0,) for Tensor 'policy_1/Tanh_1:0', which has shape '(?, 39)'
&lt;/denchmark-code&gt;

Environment (please complete the following information):

OS + version: Windows 10 1809 Build 17763.805
ML-Agents version: v0.10, commit hash 056b4d
Environment: Walker with SAC

	</description>
	<comments>
		<comment id='1' author='andymule' date='2019-10-16T16:31:08Z'>
		HI &lt;denchmark-link:https://github.com/andymule&gt;@andymule&lt;/denchmark-link&gt;
 - this might be one of the bugs that was fixed in 0.10.1, but I'll try to reproduce it and confirm.
		</comment>
		<comment id='2' author='andymule' date='2019-10-16T17:19:59Z'>
		&lt;denchmark-link:https://github.com/andymule&gt;@andymule&lt;/denchmark-link&gt;
 Thanks, I can reproduce the error here; it appears to be related to the combination of continuous action space, SAC, and curiosity. We'll try to get a fix soon, but in the meantime, you probably don't want to enable curiosity on the Walker scene.
		</comment>
		<comment id='3' author='andymule' date='2019-11-06T00:27:51Z'>
		Sorry for the delay on this. This issue was fixed in &lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/issues/2738&gt;#2738&lt;/denchmark-link&gt;
, and the fix is in the 0.11 release.
		</comment>
	</comments>
</bug>