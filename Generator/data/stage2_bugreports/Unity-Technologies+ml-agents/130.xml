<bug id='130' author='ConorZAM' open_date='2017-11-04T22:56:39Z' closed_time='2018-01-03T21:01:51Z'>
	<summary>The model seems to be changing rewards to strings somehow</summary>
	<description>
I keep getting this error during training:
TypeError                                 Traceback (most recent call last)
 in ()
38             info = env.reset(train_mode=train_model)[brain_name]
39         # Decide and take an action
---&gt; 40         new_info = trainer.take_action(info, env, brain_name)
41         info = new_info
42         trainer.process_experiences(info, time_horizon, gamma, lambd)
~\Desktop\Machine Learning\ml-agents\python\ppo\trainer.py in take_action(self, info, env, brain_name)
54         self.stats['learning_rate'].append(learn_rate)
55         new_info = env.step(actions, value={brain_name: value})[brain_name]
---&gt; 56         self.add_experiences(info, new_info, epsi, actions, a_dist, value)
57         return new_info
58
~\Desktop\Machine Learning\ml-agents\python\ppo\trainer.py in add_experiences(self, info, next_info, epsi, actions, a_dist, value)
81                     history['action_probs'].append(a_dist[idx])
82                     history['value_estimates'].append(value[idx][0])
---&gt; 83                     history['cumulative_reward'] += next_info.rewards[idx]
84                     history['episode_steps'] += 1
85
TypeError: unsupported operand type(s) for +=: 'float' and 'str'
	</description>
	<comments>
		<comment id='1' author='ConorZAM' date='2017-11-05T19:48:30Z'>
		Did you try to debug it, like
print(next_info.rewards[idx])
to see whats inside ?
		</comment>
		<comment id='2' author='ConorZAM' date='2017-11-06T21:30:56Z'>
		Can you give us more details ? Like &lt;denchmark-link:https://github.com/surferau&gt;@surferau&lt;/denchmark-link&gt;
 suggested, I think printing the value of the reward will help. Also, does this happen right after you start training or does it happen randomly after some time? What kind of environment are you running ? How many agents / brains? Are you using the ppo script or are you using python directly?
		</comment>
		<comment id='3' author='ConorZAM' date='2017-11-06T21:38:44Z'>
		I'm using the ppo script, I'm sorry but I don't know where I can access the next_into.rewards[idx] from, it's not available to the agents class and that's all I've been working with so far.
It's usually a good hour or so into training that it crops up. I'm running a continuous state and action environment with 6 agents all sharing the same brain. It's nothing especially far from the balance ball example.
		</comment>
		<comment id='4' author='ConorZAM' date='2017-11-06T21:46:42Z'>
		Interesting, Could you go into ppo/trainer.py line 83 and replace the line 83
history['cumulative_reward'] += next_info.rewards[idx]
with
try:
     history['cumulative_reward'] += next_info.rewards[idx]
except:
     print(next_info.rewards[idx])
     print(next_info.rewards)
This way, when the error occurs, the content of reward will be displayed.
Did you modify code on the python side ?
What is the maximum value for the reward per agent ?
		</comment>
		<comment id='5' author='ConorZAM' date='2017-11-06T21:59:28Z'>
		I've added the code in and will update you next time it happens.
I've not changed anything on the python side.
The maximum value for reward over an agent's lifetime would be difficult to determine as it reflects their distance travelled from a point, however, my agents never manage to get very far and the mean reward has never gone above 10. (yet!)
		</comment>
		<comment id='6' author='ConorZAM' date='2017-11-15T08:50:43Z'>
		I got this error too. It's because the reward is 'NaN' so not a number. I guess it's either you have some invalid math operations in your reward calculation (e.g. division by zero) or some unknown issue of Unity engine (I personally got this without any invalid math operations so...)
To avoid this, I use "if type(reward) is float:" statement to skip these invalid rewards.
Hope this helps.
		</comment>
		<comment id='7' author='ConorZAM' date='2017-11-15T12:22:55Z'>
		Thanks for the advice, I've actually been making a lot of progress recently and the rewards have been behaving themselves since I posted this issue. I think the NaNs causing it were mainly a result of my environment being a mess and the agents being unable to find anything within them. If I do catch the string I'll let you know.
		</comment>
		<comment id='8' author='ConorZAM' date='2020-01-04T20:17:46Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>