<bug id='4605' author='Gr1nS1de' open_date='2020-10-26T15:33:29Z' closed_time='2020-12-01T18:26:54Z'>
	<summary>NaN exceptions when steps becomes over 2 billions</summary>
	<description>
I'm trying to train my 2d ragdoll character to take the right position on 14 points - 1 for each bone.
I use PPO and can see my entropy slowly goes down from 1.5 to 0.26 until now.
Here i've met some problem: when training reached 2147400000+ step - it stopped with NaN relative exceptions. So when i'm trying to resume the training - it's just starts from: -2147483625 step (from negative step) and immediately stops.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Here 2 logs:
First exceptions when steps count became too large
&lt;denchmark-code&gt;2020-10-07 21:44:14 INFO [stats.py:111] PatientPose: Step: 2147200000. Time Elapsed: 68309.679 s Mean Reward: 439.154. Std of Reward: 77.079. Training.
2020-10-07 21:45:40 INFO [stats.py:111] PatientPose: Step: 2147300000. Time Elapsed: 68395.498 s Mean Reward: 432.955. Std of Reward: 89.715. Training.
2020-10-07 21:46:55 INFO [stats.py:111] PatientPose: Step: 2147400000. Time Elapsed: 68470.389 s Mean Reward: 448.907. Std of Reward: 69.031. Training.
2020-10-07 21:47:50 INFO [environment.py:418] Environment shut down with return code 0.
Process Process-6:
2020-10-07 21:47:50 INFO [environment.py:418] Environment shut down with return code 0.
Process Process-1:
2020-10-07 21:47:50 INFO [environment.py:418] Environment shut down with return code 0.
Process Process-4:
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py", line 297, in _bootstrap
    self.run()
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py", line 297, in _bootstrap
    self.run()
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/subprocess_env_manager.py", line 157, in worker
    env.step()
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/m  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/subprocess_env_manager.py", line 157, in worker
    env.step()
ultiprocessing/process.py", line 297, in _bootstrap
    self.run()
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/environment.py", line 319, in step
    self._update_state(rl_output)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/environment.py", line 319, in step
    self._update_state(rl_output)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/subprocess_env_manager.py", line 157, in worker
    env.step()
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/environment.py", line 275, in _update_state
    agent_info_list, self._env_specs[brain_name]
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/environment.py", line 275, in _update_state
    agent_info_list, self._env_specs[brain_name]
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/environment.py", line 319, in step
    self._update_state(rl_output)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/rpc_utils.py", line 188, in steps_from_proto
    obs_index, obs_shape, decision_agent_info_list
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/rpc_utils.py", line 188, in steps_from_proto
    obs_index, obs_shape, decision_agent_info_list
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/environment.py", line 275, in _update_state
    agent_info_list, self._env_specs[brain_name]
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/rpc_utils.py", line 152, in _process_vector_observation
    _raise_on_nan_and_inf(np_obs, "observations")
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/rpc_utils.py", line 152, in _process_vector_observation
    _raise_on_nan_and_inf(np_obs, "observations")
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/rpc_utils.py", line 130, in _raise_on_nan_and_inf
    raise RuntimeError(f"The {source} provided had NaN values.")
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/rpc_utils.py", line 130, in _raise_on_nan_and_inf  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)

    raise RuntimeError(f"The {source} provided had NaN values.")
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/rpc_utils.py", line 188, in steps_from_proto
    obs_index, obs_shape, decision_agent_info_list
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/rpc_utils.py", line 152, in _process_vector_observation
    _raise_on_nan_and_inf(np_obs, "observations")
RuntimeError: The observations provided had NaN values.
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/rpc_utils.py", line 130, in _raise_on_nan_and_inf
    raise RuntimeError(f"The {source} provided had NaN values.")
RuntimeError: The observations provided had NaN values.
RuntimeError: The observations provided had NaN values.
2020-10-07 21:47:50 INFO [trainer_controller.py:234] Learning was interrupted. Please wait while the graph is generated.
2020-10-07 21:47:50 INFO [environment.py:418] Environment shut down with return code 0.
Process Process-5:
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py", line 297, in _bootstrap
    self.run()
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/subprocess_env_manager.py", line 157, in worker
    env.step()
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/environment.py", line 319, in step
    self._update_state(rl_output)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/environment.py", line 275, in _update_state
    agent_info_list, self._env_specs[brain_name]
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/rpc_utils.py", line 188, in steps_from_proto
    obs_index, obs_shape, decision_agent_info_list
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/rpc_utils.py", line 152, in _process_vector_observation
    _raise_on_nan_and_inf(np_obs, "observations")
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/rpc_utils.py", line 130, in _raise_on_nan_and_inf
    raise RuntimeError(f"The {source} provided had NaN values.")
RuntimeError: The observations provided had NaN values.
2020-10-07 21:47:50 INFO [environment.py:418] Environment shut down with return code 0.
Process Process-2:
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py", line 297, in _bootstrap
    self.run()
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/subprocess_env_manager.py", line 157, in worker
    env.step()
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/environment.py", line 319, in step
    self._update_state(rl_output)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/environment.py", line 275, in _update_state
    agent_info_list, self._env_specs[brain_name]
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/rpc_utils.py", line 188, in steps_from_proto
    obs_index, obs_shape, decision_agent_info_list
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/rpc_utils.py", line 152, in _process_vector_observation
    _raise_on_nan_and_inf(np_obs, "observations")
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/rpc_utils.py", line 130, in _raise_on_nan_and_inf
    raise RuntimeError(f"The {source} provided had NaN values.")
RuntimeError: The observations provided had NaN values.
2020-10-07 21:47:50 INFO [trainer_controller.py:108] Saved Model
2020-10-07 21:47:50 INFO [model_serialization.py:203] List of nodes to export for brain :patientPose?team=0
2020-10-07 21:47:50 INFO [model_serialization.py:205]     is_continuous_control
2020-10-07 21:47:50 INFO [model_serialization.py:205]     trainer_major_version
2020-10-07 21:47:50 INFO [model_serialization.py:205]     trainer_minor_version
2020-10-07 21:47:50 INFO [model_serialization.py:205]     trainer_patch_version
2020-10-07 21:47:50 INFO [model_serialization.py:205]     version_number
2020-10-07 21:47:50 INFO [model_serialization.py:205]     memory_size
2020-10-07 21:47:50 INFO [model_serialization.py:205]     action_output_shape
2020-10-07 21:47:50 INFO [model_serialization.py:205]     action
2020-10-07 21:47:50 INFO [model_serialization.py:205]     action_probs
Converting results/393_Patient_Pose_Train/PatientPose/frozen_graph_def.pb to results/393_Patient_Pose_Train/PatientPose.nn
IGNORED: Cast unknown layer
IGNORED: Shape unknown layer
IGNORED: StopGradient unknown layer
GLOBALS: 'is_continuous_control', 'trainer_major_version', 'trainer_minor_version', 'trainer_patch_version', 'version_number', 'memory_size', 'action_output_shape'
IN: 'vector_observation': [-1, 1, 1, 153] =&gt; 'sub_2'
OUT: 'action', 'action_probs'
DONE: wrote results/393_Patient_Pose_Train/PatientPose.nn file.
2020-10-07 21:47:51 INFO [model_serialization.py:83] Exported results/393_Patient_Pose_Train/PatientPose.nn file
2020-10-07 21:47:52 INFO [environment.py:418] Environment shut down with return code 0.
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/subprocess_env_manager.py", line 89, in recv
    response: EnvironmentResponse = self.conn.recv()
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py", line 407, in _recv_bytes
    buf = self._recv(4)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py", line 383, in _recv
    raise EOFError
EOFError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.7/bin/mlagents-learn", line 8, in &lt;module&gt;
    sys.exit(main())
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/learn.py", line 322, in main
    run_cli(parse_command_line())
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/learn.py", line 318, in run_cli
    run_training(run_seed, options)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/learn.py", line 163, in run_training
    tc.start_learning(env_manager)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/trainer_controller.py", line 243, in start_learning
    raise ex
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/trainer_controller.py", line 216, in start_learning
    external_brain_behavior_ids = set(env_manager.external_brains.keys())
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/subprocess_env_manager.py", line 301, in external_brains
    return self.env_workers[0].recv().payload
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/subprocess_env_manager.py", line 95, in recv
    raise UnityCommunicationException("UnityEnvironment worker: recv failed.")
mlagents_envs.exception.UnityCommunicationException: UnityEnvironment worker: recv failed
&lt;/denchmark-code&gt;

Exceptions when triying to resume this training - negative steps appearance
&lt;denchmark-code&gt;2020-10-07 21:57:04 INFO [tf_policy.py:165] Loading model for brain PatientPose?team=0 from results/393_Patient_Pose_Train/PatientPose.
2020-10-07 21:57:04 INFO [tf_policy.py:196] Resuming training from step -2147483625.
2020-10-07 21:57:06 INFO [environment.py:418] Environment shut down with return code 0.
Process Process-6:
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py", line 297, in _bootstrap
    self.run()
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/subprocess_env_manager.py", line 157, in worker
    env.step()
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/environment.py", line 319, in step
    self._update_state(rl_output)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/environment.py", line 275, in _update_state
    agent_info_list, self._env_specs[brain_name]
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/rpc_utils.py", line 188, in steps_from_proto
    obs_index, obs_shape, decision_agent_info_list
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/rpc_utils.py", line 152, in _process_vector_observation
    _raise_on_nan_and_inf(np_obs, "observations")
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/rpc_utils.py", line 130, in _raise_on_nan_and_inf
    raise RuntimeError(f"The {source} provided had NaN values.")
RuntimeError: The observations provided had NaN values.
2020-10-07 21:57:06 INFO [environment.py:418] Environment shut down with return code 0.
Process Process-2:
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py", line 297, in _bootstrap
    self.run()
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/subprocess_env_manager.py", line 157, in worker
    env.step()
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/environment.py", line 319, in step
    self._update_state(rl_output)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/environment.py", line 275, in _update_state
    agent_info_list, self._env_specs[brain_name]
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
2020-10-07 21:57:06 INFO [environment.py:418] Environment shut down with return code 0.
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/rpc_utils.py", line 188, in steps_from_proto
    obs_index, obs_shape, decision_agent_info_list
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/rpc_utils.py", line 152, in _process_vector_observation
    _raise_on_nan_and_inf(np_obs, "observations")
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/rpc_utils.py", line 130, in _raise_on_nan_and_inf
    raise RuntimeError(f"The {source} provided had NaN values.")
RuntimeError: The observations provided had NaN values.
Process Process-4:
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py", line 297, in _bootstrap
    self.run()
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/subprocess_env_manager.py", line 157, in worker
    env.step()
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/environment.py", line 319, in step
    self._update_state(rl_output)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/environment.py", line 275, in _update_state
    agent_info_list, self._env_specs[brain_name]
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/rpc_utils.py", line 188, in steps_from_proto
    obs_index, obs_shape, decision_agent_info_list
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/rpc_utils.py", line 152, in _process_vector_observation
    _raise_on_nan_and_inf(np_obs, "observations")
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/rpc_utils.py", line 130, in _raise_on_nan_and_inf
    raise RuntimeError(f"The {source} provided had NaN values.")
RuntimeError: The observations provided had NaN values.
2020-10-07 21:57:06 INFO [environment.py:418] Environment shut down with return code 0.
Process Process-3:
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py", line 297, in _bootstrap
    self.run()
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/subprocess_env_manager.py", line 157, in worker
    env.step()
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/environment.py", line 319, in step
    self._update_state(rl_output)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/environment.py", line 275, in _update_state
    agent_info_list, self._env_specs[brain_name]
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/rpc_utils.py", line 188, in steps_from_proto
    obs_index, obs_shape, decision_agent_info_list
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/rpc_utils.py", line 152, in _process_vector_observation
    _raise_on_nan_and_inf(np_obs, "observations")
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/rpc_utils.py", line 130, in _raise_on_nan_and_inf
    raise RuntimeError(f"The {source} provided had NaN values.")
RuntimeError: The observations provided had NaN values.
2020-10-07 21:57:06 INFO [environment.py:418] Environment shut down with return code 0.
Process Process-5:
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py", line 297, in _bootstrap
    self.run()
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/subprocess_env_manager.py", line 157, in worker
    env.step()
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/environment.py", line 319, in step
    self._update_state(rl_output)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/environment.py", line 275, in _update_state
    agent_info_list, self._env_specs[brain_name]
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/rpc_utils.py", line 188, in steps_from_proto
    obs_index, obs_shape, decision_agent_info_list
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/rpc_utils.py", line 152, in _process_vector_observation
    _raise_on_nan_and_inf(np_obs, "observations")
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/rpc_utils.py", line 130, in _raise_on_nan_and_inf
    raise RuntimeError(f"The {source} provided had NaN values.")
RuntimeError: The observations provided had NaN values.
2020-10-07 21:57:06 INFO [environment.py:418] Environment shut down with return code 0.
Process Process-1:
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py", line 297, in _bootstrap
    self.run()
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/subprocess_env_manager.py", line 157, in worker
    env.step()
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/environment.py", line 319, in step
    self._update_state(rl_output)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/environment.py", line 275, in _update_state
    agent_info_list, self._env_specs[brain_name]
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/rpc_utils.py", line 188, in steps_from_proto
    obs_index, obs_shape, decision_agent_info_list
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/rpc_utils.py", line 152, in _process_vector_observation
    _raise_on_nan_and_inf(np_obs, "observations")
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/rpc_utils.py", line 130, in _raise_on_nan_and_inf
    raise RuntimeError(f"The {source} provided had NaN values.")
RuntimeError: The observations provided had NaN values.
^C2020-10-07 21:58:17 INFO [trainer_controller.py:234] Learning was interrupted. Please wait while the graph is generated.
2020-10-07 21:58:17 INFO [trainer_controller.py:108] Saved Model
2020-10-07 21:58:17 INFO [model_serialization.py:203] List of nodes to export for brain :patientPose?team=0
2020-10-07 21:58:17 INFO [model_serialization.py:205]    is_continuous_control
2020-10-07 21:58:17 INFO [model_serialization.py:205]    trainer_major_version
2020-10-07 21:58:17 INFO [model_serialization.py:205]    trainer_minor_version
2020-10-07 21:58:17 INFO [model_serialization.py:205]    trainer_patch_version
2020-10-07 21:58:17 INFO [model_serialization.py:205]    version_number
2020-10-07 21:58:17 INFO [model_serialization.py:205]    memory_size
2020-10-07 21:58:17 INFO [model_serialization.py:205]    action_output_shape
2020-10-07 21:58:17 INFO [model_serialization.py:205]    action
2020-10-07 21:58:17 INFO [model_serialization.py:205]    action_probs
Converting results/393_Patient_Pose_Train/PatientPose/frozen_graph_def.pb to results/393_Patient_Pose_Train/PatientPose.nn
IGNORED: Cast unknown layer
IGNORED: Shape unknown layer
IGNORED: StopGradient unknown layer
GLOBALS: 'is_continuous_control', 'trainer_major_version', 'trainer_minor_version', 'trainer_patch_version', 'version_number', 'memory_size', 'action_output_shape'
IN: 'vector_observation': [-1, 1, 1, 153] =&gt; 'sub_2'
OUT: 'action', 'action_probs'
DONE: wrote results/393_Patient_Pose_Train/PatientPose.nn file.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Gr1nS1de' date='2020-10-26T21:44:08Z'>
		Thanks for the report, this definitely looks like a bug. At first glance, we need to change these



ml-agents/ml-agents/mlagents/trainers/tf/models.py


        Lines 44 to 49
      in
      a14730f






 global_step = tf.Variable( 



 0, name="global_step", trainable=False, dtype=tf.int32 



 ) 



 steps_to_increment = tf.placeholder( 



 shape=[], dtype=tf.int32, name="steps_to_increment" 



 ) 








ml-agents/ml-agents/mlagents/trainers/tf/models.py


        Lines 194 to 200
      in
      a14730f






 steps = tf.get_variable( 



 "normalization_steps", 



     [], 



 trainable=False, 



 dtype=tf.int32, 



 initializer=tf.zeros_initializer(), 



 ) 





to int64s.
I'm going to try to reproduce this (by hacking some initial values, not running for 2B steps) and confirm that changing the type fixes the issue, but if you're blocked, you can try it out locally too.
		</comment>
		<comment id='2' author='Gr1nS1de' date='2020-10-26T21:48:41Z'>
		Internal tracking ID for this is MLA-1503
		</comment>
		<comment id='3' author='Gr1nS1de' date='2020-10-26T23:52:34Z'>
		I haven't been able to reproduce the problem here yet.
The error message indicate that there are NaN values in the observations(RuntimeError: The observations provided had NaN values.). I still believe it's probably the int32 overflow resulting in NaN values in the actions, and then stepping Unity with NaN actions results in NaN observations. But it's also possible that the NaNs are coming directly from your observation calculations.
I made some changes based off of the release_8 branch (that's the latest one; you didn't say which version you're running) that will:

use int64 for the step values
raise an error sooner if the trainer produces NaNs

Could you grab the &lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/tree/MLA-1503-int-overflow-nan&gt;MLA-1503-int-overflow-nan&lt;/denchmark-link&gt;
 branch and see if that helps? I think either:

Everything will work fine
You'll see the "The observations provided had NaN values" exception again, in which case the error is in your environment
You'll see a "NaN action detected" exception, in which case it's still something in the trainer that isn't fixed with the int64 steps.

		</comment>
		<comment id='4' author='Gr1nS1de' date='2020-10-27T00:53:05Z'>
		Hi. Very appreciate for your quick response.
Sorry for missing version. Here is it:
Unity: 2020.1.9f1
ML Agents: 1.0.5
ml-agents-envs: 0.17.0
It's excluded - about NaN values in observations. I tested it few times with loading from checkpoint. The same behavior every time.
As much i remember - i can't just update my ml-agents package and continue training. So i can see your branch changes tomorrow, will apply them on my version and then i'll say the results.
		</comment>
		<comment id='5' author='Gr1nS1de' date='2020-10-28T00:55:15Z'>
		I've got exceptions when tried to resume training from checkpoint.
I suppose i can't just resume it with ur changes.
&lt;denchmark-code&gt;2020-10-28 03:51:08 INFO [tf_policy.py:165] Loading model for brain PatientPose?team=0 from results/393_Patient_Pose_Train/PatientPose.
2020-10-28 03:51:08.636221: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Invalid argument: tensor_name = global_step; expected dtype int64 does not equal original dtype int32
tensor_name = normalization_steps; expected dtype int64 does not equal original dtype int32
2020-10-28 03:51:08 INFO [trainer_controller.py:108] Saved Model
2020-10-28 03:51:09 INFO [environment.py:418] Environment shut down with return code 0.
2020-10-28 03:51:09 INFO [environment.py:418] Environment shut down with return code 0.
2020-10-28 03:51:10 INFO [environment.py:418] Environment shut down with return code 0.
2020-10-28 03:51:10 INFO [environment.py:418] Environment shut down with return code 0.
2020-10-28 03:51:11 INFO [environment.py:418] Environment shut down with return code 0.
2020-10-28 03:51:11 INFO [environment.py:418] Environment shut down with return code 0.
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/client/session.py", line 1365, in _do_call
    return fn(*args)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/client/session.py", line 1350, in _run_fn
    target_list, run_metadata)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/client/session.py", line 1443, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: tensor_name = global_step; expected dtype int64 does not equal original dtype int32
tensor_name = normalization_steps; expected dtype int64 does not equal original dtype int32
	 [[{{node save_1/RestoreV2}}]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py", line 1290, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/client/session.py", line 956, in run
    run_metadata_ptr)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/client/session.py", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/client/session.py", line 1359, in _do_run
    run_metadata)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/client/session.py", line 1384, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: tensor_name = global_step; expected dtype int64 does not equal original dtype int32
tensor_name = normalization_steps; expected dtype int64 does not equal original dtype int32
	 [[node save_1/RestoreV2 (defined at /lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]

Original stack trace for 'save_1/RestoreV2':
  File "/bin/mlagents-learn", line 8, in &lt;module&gt;
    sys.exit(main())
  File "/lib/python3.7/site-packages/mlagents/trainers/learn.py", line 322, in main
    run_cli(parse_command_line())
  File "/lib/python3.7/site-packages/mlagents/trainers/learn.py", line 318, in run_cli
    run_training(run_seed, options)
  File "/lib/python3.7/site-packages/mlagents/trainers/learn.py", line 163, in run_training
    tc.start_learning(env_manager)
  File "/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "/lib/python3.7/site-packages/mlagents/trainers/trainer_controller.py", line 218, in start_learning
    self._create_trainers_and_managers(env_manager, new_behavior_ids)
  File "/lib/python3.7/site-packages/mlagents/trainers/trainer_controller.py", line 204, in _create_trainers_and_managers
    self._create_trainer_and_manager(env_manager, behavior_id)
  File "/lib/python3.7/site-packages/mlagents/trainers/trainer_controller.py", line 180, in _create_trainer_and_manager
    trainer.add_policy(parsed_behavior_id, policy)
  File "/lib/python3.7/site-packages/mlagents/trainers/ppo/trainer.py", line 232, in add_policy
    self.optimizer = PPOOptimizer(self.policy, self.trainer_settings)
  File "/lib/python3.7/site-packages/mlagents/trainers/ppo/optimizer.py", line 99, in __init__
    self.policy.initialize_or_load()
  File "/lib/python3.7/site-packages/mlagents/trainers/policy/tf_policy.py", line 207, in initialize_or_load
    self._load_graph(self.model_path, reset_global_steps=reset_steps)
  File "/lib/python3.7/site-packages/mlagents/trainers/policy/tf_policy.py", line 162, in _load_graph
    self.saver = tf.train.Saver(max_to_keep=self.keep_checkpoints)
  File "/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py", line 828, in __init__
    self.build()
  File "/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py", line 840, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File "/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py", line 878, in _build
    build_restore=build_restore)
  File "/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py", line 508, in _build_internal
    restore_sequentially, reshape)
  File "/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py", line 328, in _AddRestoreOps
    restore_sequentially)
  File "/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py", line 575, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File "/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_io_ops.py", line 1696, in restore_v2
    name=name)
  File "/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py", line 794, in _apply_op_helper
    op_def=op_def)
  File "/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py", line 3357, in create_op
    attrs, op_def, compute_device)
  File "/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py", line 3426, in _create_op_internal
    op_def=op_def)
  File "/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.7/bin/mlagents-learn", line 8, in &lt;module&gt;
    sys.exit(main())
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/learn.py", line 322, in main
    run_cli(parse_command_line())
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/learn.py", line 318, in run_cli
    run_training(run_seed, options)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/learn.py", line 163, in run_training
    tc.start_learning(env_manager)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/trainer_controller.py", line 218, in start_learning
    self._create_trainers_and_managers(env_manager, new_behavior_ids)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/trainer_controller.py", line 204, in _create_trainers_and_managers
    self._create_trainer_and_manager(env_manager, behavior_id)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/trainer_controller.py", line 180, in _create_trainer_and_manager
    trainer.add_policy(parsed_behavior_id, policy)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/ppo/trainer.py", line 232, in add_policy
    self.optimizer = PPOOptimizer(self.policy, self.trainer_settings)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/ppo/optimizer.py", line 99, in __init__
    self.policy.initialize_or_load()
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/policy/tf_policy.py", line 207, in initialize_or_load
    self._load_graph(self.model_path, reset_global_steps=reset_steps)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/mlagents/trainers/policy/tf_policy.py", line 177, in _load_graph
    self.saver.restore(self.sess, ckpt.model_checkpoint_path)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py", line 1326, in restore
    err, "a mismatch between the current graph and the graph")
tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

tensor_name = global_step; expected dtype int64 does not equal original dtype int32
tensor_name = normalization_steps; expected dtype int64 does not equal original dtype int32
	 [[node save_1/RestoreV2 (defined at /lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]

Original stack trace for 'save_1/RestoreV2':
  File "/bin/mlagents-learn", line 8, in &lt;module&gt;
    sys.exit(main())
  File "/lib/python3.7/site-packages/mlagents/trainers/learn.py", line 322, in main
    run_cli(parse_command_line())
  File "/lib/python3.7/site-packages/mlagents/trainers/learn.py", line 318, in run_cli
    run_training(run_seed, options)
  File "/lib/python3.7/site-packages/mlagents/trainers/learn.py", line 163, in run_training
    tc.start_learning(env_manager)
  File "/lib/python3.7/site-packages/mlagents_envs/timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "/lib/python3.7/site-packages/mlagents/trainers/trainer_controller.py", line 218, in start_learning
    self._create_trainers_and_managers(env_manager, new_behavior_ids)
  File "/lib/python3.7/site-packages/mlagents/trainers/trainer_controller.py", line 204, in _create_trainers_and_managers
    self._create_trainer_and_manager(env_manager, behavior_id)
  File "/lib/python3.7/site-packages/mlagents/trainers/trainer_controller.py", line 180, in _create_trainer_and_manager
    trainer.add_policy(parsed_behavior_id, policy)
  File "/lib/python3.7/site-packages/mlagents/trainers/ppo/trainer.py", line 232, in add_policy
    self.optimizer = PPOOptimizer(self.policy, self.trainer_settings)
  File "/lib/python3.7/site-packages/mlagents/trainers/ppo/optimizer.py", line 99, in __init__
    self.policy.initialize_or_load()
  File "/lib/python3.7/site-packages/mlagents/trainers/policy/tf_policy.py", line 207, in initialize_or_load
    self._load_graph(self.model_path, reset_global_steps=reset_steps)
  File "/lib/python3.7/site-packages/mlagents/trainers/policy/tf_policy.py", line 162, in _load_graph
    self.saver = tf.train.Saver(max_to_keep=self.keep_checkpoints)
  File "/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py", line 828, in __init__
    self.build()
  File "/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py", line 840, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File "/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py", line 878, in _build
    build_restore=build_restore)
  File "/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py", line 508, in _build_internal
    restore_sequentially, reshape)
  File "/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py", line 328, in _AddRestoreOps
    restore_sequentially)
  File "/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py", line 575, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File "/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_io_ops.py", line 1696, in restore_v2
    name=name)
  File "/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py", line 794, in _apply_op_helper
    op_def=op_def)
  File "/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py", line 3357, in create_op
    attrs, op_def, compute_device)
  File "/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py", line 3426, in _create_op_internal
    op_def=op_def)
  File "/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='Gr1nS1de' date='2020-10-28T21:38:48Z'>
		I was afraid of that. I don't know if there's any good way to reuse your checkpoints with the changes.
If you don't want to try out the patch, you can hopefully recover your existing data using the --initialize-from option and pass the previous run-id (I think 393_Patient_Pose_Train based on your logs). This will start training over again from step 0 but using the existing weights. However, if there are NaNs in your latest checkpoint, you may need to delete it (or move it) in order to load from a previous checkpoint.
Edit: you can also modify the results/&lt;run_id&gt;/&lt;behavior_name&gt;/checkpoint file to control which checkpoint is loaded.
		</comment>
		<comment id='7' author='Gr1nS1de' date='2020-10-28T22:13:41Z'>
		I tried --initialize-from and it didn't help me. When steps from origin training + steps from new "initialized-from" becomes more than int32.Max - the same exception thrown.
It feels like some int var stored inside origin brain.
Well, ok, thanks anyway. Maybe i'll try to start new training with your changes if my new config wouldn't help me... Will try to say results here maybe in a month...
		</comment>
		<comment id='8' author='Gr1nS1de' date='2020-12-01T17:50:23Z'>
		Hi there :)
It seems to be fine right now on your branch MLA-1503-int-overflow-nan.
My new training now is on 2180000000 step and it's continue. Thank you for help.
		</comment>
		<comment id='9' author='Gr1nS1de' date='2020-12-01T18:26:54Z'>
		Great, we applied those fixes for the latest release (Release 10), so you don't need to keep using the branch.
I'll close this issue.
		</comment>
	</comments>
</bug>