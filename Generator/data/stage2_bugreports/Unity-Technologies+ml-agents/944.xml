<bug id='944' author='ADulian' open_date='2018-06-29T16:30:51Z' closed_time='2018-07-03T09:16:18Z'>
	<summary>Training multiple agent with single brain</summary>
	<description>
Hi,
Quick question regarding training multiple agents with a single brain. I'm quite new when it comes to using Ml-Agents btw, went over the documentation and one of many things that seemed really interesting was the idea of training a single brain using multiple (same) agents to gather more data in a single step. So I went over and created a new environment with a ball rolling towards the square box (example project from documentation), training single agent went quite well so I've modified few things and decided to go for multi-agent single-brain approach to compare the results. All good, brain seems to be getting smarter much faster apart from one annoying detail, whilst using single agent it learns to move towards the box with higher speed (penalty reward for time taken) and when I use it in the simulation it looks brilliant, however when I train the brain using multiple agents and use the brain in the simulation the agents seem to move towards the box really slowly despite having a constant speed that is used to increase the force. Went over few different combinations of rewards etc, and results are always the same.
So in short: Single agent = longer training time but agent moves quickly towards the target, Multiple Agents = much shorter training time, however, agents speed is utterly slow.
I'm guessing it must be some misunderstanding of training single brain with multiple agents from my side so any help would be appreciated.
Cheers
	</description>
	<comments>
		<comment id='1' author='ADulian' date='2018-06-29T16:38:04Z'>
		When you add multiple agents, are you adding more balls to the same platform or adding more instances of the ball + platform?
My guess is that the cause is not multiple agents, though. When developing that example, I found it very easy to end up with agents that moved very slowly or not at all (or the opposite where the agents immediately shot off to their deaths).
		</comment>
		<comment id='2' author='ADulian' date='2018-06-29T16:40:23Z'>
		Forgot to mention that, so by saying multiple agents i mean multiple instances of floor + ball + box all seperated
		</comment>
		<comment id='3' author='ADulian' date='2018-06-29T16:53:21Z'>
		Maybe &lt;denchmark-link:https://github.com/awjuliani&gt;@awjuliani&lt;/denchmark-link&gt;
 or &lt;denchmark-link:https://github.com/vincentpierre&gt;@vincentpierre&lt;/denchmark-link&gt;
 can comment on how collecting data from multiple agent instances is different than collecting it from a single instance.
		</comment>
		<comment id='4' author='ADulian' date='2018-06-29T17:37:42Z'>
		Hi @Gaduu,
Could you possibly share your TensorBoard (&lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Using-Tensorboard.md&gt;https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Using-Tensorboard.md&lt;/denchmark-link&gt;
) learning graphs for the two different scenarios, so we could get a better idea of how the training process is happening in both cases?
		</comment>
		<comment id='5' author='ADulian' date='2018-06-30T11:20:38Z'>
		Multi-Agent
&lt;denchmark-link:https://user-images.githubusercontent.com/9085529/42124707-f3f8d036-7c5f-11e8-9f1f-82e2201513a2.png&gt;&lt;/denchmark-link&gt;

Single-Agent
&lt;denchmark-link:https://user-images.githubusercontent.com/9085529/42124709-fd9f9a34-7c5f-11e8-84a2-74a58ddc3450.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='ADulian' date='2018-07-02T10:37:50Z'>
		Hi @Gaduu,
Thanks for sharing these. They make clear what I suspected. The multi-agent scenario is actually doing "better" in that the agents have learned to get a higher cumulative reward per episode. This is because the way the reward function is written for this example, the agent can actually get more reward for this weird looking behavior than they can get for going straight to the goal. I would recommend simplifying the reward function, with a simple +1 for reaching goal, and seeing how the multi-agent training works there.
It seems that this is the part of the reward function which can be exploited.
&lt;denchmark-code&gt;    // Getting closer
    if (distanceToTarget &lt; previousDistance)
    {
        AddReward(0.1f);
    }
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='ADulian' date='2018-07-03T09:16:18Z'>
		Hi &lt;denchmark-link:https://github.com/awjuliani&gt;@awjuliani&lt;/denchmark-link&gt;

Absolutely right, changed the function to first of all reward the agent with smaller amount and also punish the agent with small negative reward if current distance is bigger than previous.
if (distanceToTarget &lt; mPreviousDistance) AddReward(0.01f); else AddReward(-0.0025f); 
Many thanks for help, also about good time to start paying more attention to graphs :),
Cheers
		</comment>
		<comment id='8' author='ADulian' date='2020-01-03T07:10:29Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>