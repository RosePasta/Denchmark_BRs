<bug id='3597' author='JohnBergago' open_date='2020-03-09T16:06:41Z' closed_time='2020-05-15T22:34:54Z'>
	<summary>max_step not correctly propagated to gym_unity</summary>
	<description>
Describe the bug
It seems like the max_step flag in an agents info dict that is returned from an env.step() never becomes true.
To Reproduce
Set up example env

Open the Project project from ml-agents folder.
Open Basic Scene from Project Explorer Assets &gt; ML-Agents &gt; Examples &gt; Basic &gt; Scenes &gt; Basic
In Hierarchy click on Basic &gt; Basic Agent
In Inspector set Max Step in Agent section to 5
Build this scene and save to envs folder

Test with python
Run this code:
import numpy as np
from gym_unity.envs import UnityEnv
from mlagents_envs.environment import UnityEnvironment
from mlagents_envs.side_channel.engine_configuration_channel import EngineConfig, EngineConfigurationChannel

env_file_name = "path/to/envs/basicbuild/Basic"

# --- First test gym environment
env = UnityEnv(env_file_name, 2, no_graphics=False, flatten_branched=False, multiagent=False)

for e in range(2):
    print("Episode ", e)
    o, d = env.reset(), False
    steps = 0
    while not d:
        o, r, d, info = env.step([[0]] * env.number_agents)
        steps += 1
        if (isinstance(d, list)):
            d = any(d)
            print(steps, d)
        else:
            print("Steps: {} \tReward: {:6.3f} \tDone: {}\tMaxStep: {}".format(steps, r, d, info["batched_step_result"].max_step))

env.close()


engine_configuration_channel = EngineConfigurationChannel()
env = UnityEnvironment(file_name=env_file_name, base_port=5005, seed=1, side_channels=[engine_configuration_channel])

# --- No gym environment according to getting started notebook------------------------------------------
#Reset the environment
env.reset()

# Set the default brain to work with
group_name = env.get_agent_groups()[0]
group_spec = env.get_agent_group_spec(group_name)

# Set the time scale of the engine
engine_configuration_channel.set_configuration_parameters(time_scale = 1.0)

for e in range(2):
    print("No Gym Episode ", e)
    env.reset()
    step_result = env.get_step_result(group_name)
    d = False
    steps = 0
    while not d:
        action_size = group_spec.action_size
        action = np.column_stack([0]*step_result.n_agents())
        env.set_actions(group_name, action)
        env.step()
        step_result = env.get_step_result(group_name)

        r = step_result.reward[0]
        d = step_result.done[0]
        max_step = step_result.max_step[0]

        steps += 1
        if (isinstance(d, list)):
            d = any(d)
            print(steps, d)
        else:
            print("Steps: {}\tReward: {:5.3f}\tDone: {}\tMaxStep: {}".format(steps, r, d, max_step))
env.close()
Replace path/to/envs with the correct path to the built executable.
The output will look like:
&lt;denchmark-code&gt;Episode  0
Steps: 1        Reward: -0.010  Done: False     MaxStep: [False]
Steps: 2        Reward: -0.010  Done: False     MaxStep: [False]
Steps: 3        Reward: -0.010  Done: False     MaxStep: [False]
Steps: 4        Reward: -0.010  Done: False     MaxStep: [False]
Steps: 5        Reward:  0.000  Done: True      MaxStep: [False]
Episode  1
Steps: 1        Reward: -0.010  Done: False     MaxStep: [False]
Steps: 2        Reward: -0.010  Done: False     MaxStep: [False]
Steps: 3        Reward: -0.010  Done: False     MaxStep: [False]
Steps: 4        Reward: -0.010  Done: False     MaxStep: [False]
Steps: 5        Reward:  0.000  Done: True      MaxStep: [False]
INFO:mlagents_envs:Environment shut down with return code 0.
INFO:mlagents_envs:Connected to Unity environment with package version 0.14.1-preview and communication version 0.15.0
INFO:mlagents_envs:Connected new brain:
My Behavior?team=0
No Gym Episode  0
Steps: 1        Reward: -0.010  Done: False     MaxStep: False
Steps: 2        Reward: -0.010  Done: False     MaxStep: False
Steps: 3        Reward: -0.010  Done: False     MaxStep: False
Steps: 4        Reward: -0.010  Done: False     MaxStep: False
Steps: 5        Reward: 0.000   Done: True      MaxStep: True
No Gym Episode  1
Steps: 1        Reward: -0.010  Done: False     MaxStep: False
Steps: 2        Reward: -0.010  Done: False     MaxStep: False
Steps: 3        Reward: -0.010  Done: False     MaxStep: False
Steps: 4        Reward: -0.010  Done: False     MaxStep: False
Steps: 5        Reward: 0.000   Done: True      MaxStep: True
&lt;/denchmark-code&gt;

The first two episode are from the gym environment which returns done True, but MaxStep False. However it should be True, as the agent didn't solve the task (as can be seen at the reward). However, it works without gym. The last two episode are from a BaseEnv where everything seems to be as it should.
Environment (please complete the following information):

Ubuntu 18.04
latest master branch from source
TensorFlow version: 2.0
Environment: Basic with max steps

	</description>
	<comments>
		<comment id='1' author='JohnBergago' date='2020-03-09T18:17:26Z'>
		Thanks for sharing the steps for reproducing this issue and the bug report. I'll share with the team and address the issue accordingly!
		</comment>
		<comment id='2' author='JohnBergago' date='2020-04-14T22:15:24Z'>
		Hi &lt;denchmark-link:https://github.com/JohnBergago&gt;@JohnBergago&lt;/denchmark-link&gt;
 ,
Thank you for reporting this bug.
Our implementation of the  wrapper changed on  since the 0.15.1 release and I think your issue has been resolved. Can you try to reproduce your error on  so I can make sure this is the case?
		</comment>
		<comment id='3' author='JohnBergago' date='2020-04-18T14:17:25Z'>
		Hi &lt;denchmark-link:https://github.com/vincentpierre&gt;@vincentpierre&lt;/denchmark-link&gt;
,
I tested your new implementation of the environment interfaces. At first it was a little bit confusing, that max_step is only part of the info dict, when a terminal step happened. However, that one was easy to fix. But since you are not supporting multiagent environment anymore (which I was using a lot) I will have to figure out how to write my own wrapper anyway.
By the way, I just recognized that the LLAPI docs say, that the DecisionStep and TerminalStep contain a done field, but this seems not to be the case.
Thanks for your effort.
		</comment>
		<comment id='4' author='JohnBergago' date='2020-04-20T18:24:54Z'>
		Hi &lt;denchmark-link:https://github.com/JohnBergago&gt;@JohnBergago&lt;/denchmark-link&gt;

Thank you for your feedback, the done filed in the LLAPI doc is a mistake, I made a pull request to correct it.
We deprecated our multi-agent gym wrapper because it was very hard to maintain and did not work for a lot of environments. We encourage users to use the LLAPI directly or write their own wrappers for their specific needs.
If there is a functionality that you need in the LLAPI that does not exist, please let us know.
		</comment>
	</comments>
</bug>