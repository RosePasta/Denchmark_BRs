<bug id='4198' author='robinerd' open_date='2020-07-08T13:46:24Z' closed_time='2020-07-08T18:30:19Z'>
	<summary>RuntimeError: dictionary changed size during iteration</summary>
	<description>
Describe the bug
Sometimes during training it crashes in StatsReporter with message "RuntimeError: dictionary changed size during iteration" (see full stack trace below).
It happens rarely, I can sometimes train a whole night without any crash but I've also seen it crash after "only" a few million steps. In the below log it crashed after 30M steps.
I was training with the following command:
mlagents-learn train_monsters.yaml --env MachineLearning-Linux/S_of_M.x86_64 --width 800 --height 600 --time-scale 5 --no-graphics --run-id monsters-walk-14envs --num-envs 14 --resume
I have also gotten the same behaviour on my Windows machine (training with tensorflow-gpu).
To Reproduce
Not sure exactly how to reproduce, but I believe it is related to that we're adding custom charts in tensorboard by modifying Agent.cs.
We have added the following code in Agent.cs, in the SendInfo() function. We also have a custom wrapper "AddRewardDebug" around AddReward to help us balance rewards and understand training results better.
&lt;denchmark-code&gt;if (Academy.Instance.IsCommunicatorOn) // Only when training
{
      var statsRecorder = Academy.Instance.StatsRecorder;
      foreach (var key in rewardDebugs.Keys)
      {
            statsRecorder.Add("Reward Debugging/" + key, rewardDebugs[key]);
      }
}
&lt;/denchmark-code&gt;

Console logs / stack traces
The full crash stack trace, usually after many millions of steps.
&lt;denchmark-code&gt;Exception in thread Thread-2:
Traceback (most recent call last):
  File "/usr/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.6/dist-packages/mlagents/trainers/trainer_controller.py", line 340, in trainer_update_func
    trainer.advance()
  File "/usr/local/lib/python3.6/dist-packages/mlagents/trainers/trainer/rl_trainer.py", line 169, in advance
    self._process_trajectory(t)
  File "/usr/local/lib/python3.6/dist-packages/mlagents/trainers/ppo/trainer.py", line 63, in _process_trajectory
    super()._process_trajectory(trajectory)
  File "/usr/local/lib/python3.6/dist-packages/mlagents/trainers/trainer/rl_trainer.py", line 125, in _process_trajectory
    self._maybe_write_summary(self.get_step + len(trajectory.steps))
  File "/usr/local/lib/python3.6/dist-packages/mlagents/trainers/trainer/rl_trainer.py", line 138, in _maybe_write_summary
    self._write_summary(self._next_summary_step)
  File "/usr/local/lib/python3.6/dist-packages/mlagents/trainers/trainer/rl_trainer.py", line 117, in _write_summary
    self.stats_reporter.write_stats(int(step))
  File "/usr/local/lib/python3.6/dist-packages/mlagents/trainers/stats.py", line 344, in write_stats
    for key in StatsReporter.stats_dict[self.category]:
RuntimeError: dictionary changed size during iteration
&lt;/denchmark-code&gt;

Environment (please complete the following information):

Unity Version: 2019.4.1f1
OS + version: Compiled on Windows 10. Crash happens both when training on my Windows 10 desktop computer and in the cloud on Digital Ocean on Ubuntu.
ML-Agents version: python package from pip mlagents 0.17.0. Unity package com.unity.ml-agents version 1.1.0-preview via git, plus custom reward debugging code in Agent.cs.
TensorFlow version:  1.14.0
Environment: custom (I unfortunately don't have time for extensive testing right now, but still wanted to file a bug in case someone else has seen this behavior)

NOTE: We are unable to help reproduce bugs with custom environments.  Please attempt to reproduce your issue with one of the example environments, or provide a minimal patch to one of the environments needed to reproduce the issue.
	</description>
	<comments>
		<comment id='1' author='robinerd' date='2020-07-08T17:13:37Z'>
		Hi &lt;denchmark-link:https://github.com/robinerd&gt;@robinerd&lt;/denchmark-link&gt;
,
This was reported on the &lt;denchmark-link:https://forum.unity.com/threads/training-stops-with-runtimeerror-dictionary-changed-size-during-iteration.926297/#post-6064934&gt;forums&lt;/denchmark-link&gt;
 too. It's due to a conflict in multithreading on the python side, and I just got a repro last night (adding a sleep(1) in that loop and reducing the summary_freq to 1 makes it happen almost immediately).
I'll hopefully have a fix today.
		</comment>
		<comment id='2' author='robinerd' date='2020-07-08T18:14:39Z'>
		&lt;denchmark-link:https://github.com/ervteng&gt;@ervteng&lt;/denchmark-link&gt;
 beat me to it: &lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/pull/4201&gt;#4201&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='robinerd' date='2020-07-08T18:30:09Z'>
		Thanks a lot for the fast response! I will try out the fix during our upcoming trainings. Good to hear that you found a reliable way of reproducing it.
		</comment>
		<comment id='4' author='robinerd' date='2020-07-08T18:31:10Z'>
		(I'll reopen if I get any issues)
		</comment>
		<comment id='5' author='robinerd' date='2020-07-08T19:59:39Z'>
		Hey &lt;denchmark-link:https://github.com/robinerd&gt;@robinerd&lt;/denchmark-link&gt;
, the fix has been merged to  - quick way to workaround it on old releases of ML-Agents is to add  to your YAML file.
		</comment>
	</comments>
</bug>