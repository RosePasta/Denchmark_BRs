<bug id='1090' author='timdhoffmann' open_date='2018-08-14T10:25:51Z' closed_time='2019-02-01T06:04:28Z'>
	<summary>Agent.AddVectorObs(float observation) and Agent.AddVectorObs(List&amp;lt;float&amp;gt; observation) lead to different training resluts</summary>
	<description>
Adding float observations to a list and calling a single AddVectorObs(), passing that list as argument did not work in my training scenario (read: did not lead to expected results).
Replacing the single call with multiple AddVectorObs(), passing one float observation with each as argument, did lead to the expected training results.
Is this expected behaviour?
Best,
Tim
See commented code below:
&lt;denchmark-code&gt;public override void CollectObservations()
    {
        // Define observations.
        
        var raycastDistance = 20.0f;

        var distanceUp = 100f;
        // Raycasts up.
        RaycastHit2D hit2D = Physics2D.Raycast(transform.position, Vector2.up, raycastDistance);
        if (hit2D.collider)
        {
            distanceUp = hit2D.distance; 
        }

        var distanceDown = 100f;
        // Raycasts down.
        hit2D = Physics2D.Raycast(transform.position, Vector2.down, raycastDistance);
        if (hit2D.collider)
        {
            distanceDown = hit2D.distance;
        }

        var distanceLeft = 100f;
        // Raycasts left.
        hit2D = Physics2D.Raycast(transform.position, Vector2.left, raycastDistance);
        if (hit2D.collider)
        {
            distanceLeft = hit2D.distance;
        }

        var distanceRight = 100f;
        // Raycasts right.
        hit2D = Physics2D.Raycast(transform.position, Vector2.right, raycastDistance);
        if (hit2D.collider)
        {
            distanceRight = hit2D.distance;
        }

        // Submit observations.

        /// [IMPORTANT!] 
        /// Submitting observations in a single list seems to lead
        /// to undesired results!!!
        //var state = new List&lt;float&gt;()
        //{
        //    distanceUp,
        //    distanceDown,
        //    distanceLeft,
        //    distanceRight,
        //    _rigidBody2D.velocity.x,
        //    _rigidBody2D.velocity.y,
        //};
        //AddVectorObs(state);

        /// Therefore, submitting each observation separately.
        AddVectorObs(distanceUp);
        AddVectorObs(distanceDown);
        AddVectorObs(distanceLeft);
        AddVectorObs(distanceRight);
        AddVectorObs(_rigidBody2D.velocity.x);
        AddVectorObs(_rigidBody2D.velocity.y);
    }
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='timdhoffmann' date='2018-08-14T18:28:03Z'>
		Hi &lt;denchmark-link:https://github.com/timdhoffmann&gt;@timdhoffmann&lt;/denchmark-link&gt;
,
Thanks for bringing this us. Behind the scenes the difference is between repeatedly calling .Add(item) vs calling .AddRange(List&lt;item&gt;). On the surface I don't see a reason why these would lead to different behaviors. I can look into this more though.
Would it be possible to use the Basic notebook (&lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/blob/master/python/Basics.ipynb&gt;https://github.com/Unity-Technologies/ml-agents/blob/master/python/Basics.ipynb&lt;/denchmark-link&gt;
) to print out what the observation vector looks like in both cases?
		</comment>
		<comment id='2' author='timdhoffmann' date='2018-08-14T22:21:46Z'>
		Hi &lt;denchmark-link:https://github.com/awjuliani&gt;@awjuliani&lt;/denchmark-link&gt;
 ,
I thought the same, when looking up the definition in Agent.cs, that's what made me wonder.
I'd be happy to help with the notebook, however I'm using it for the first time and ran into an issue. Some help might be needed to getting it running in the first place. Training with learn.py from the Anaconda Promt seems to work fine.
Here's some info on my setup:

Anaconda 3 on Win 10
Unity 2018.2.3f1
working on forked ml-agents repository (created a new scene inside the provided unity-environments project)
python3 is not recognized by Anaconda Promt
python is recognized

I can also share a link to the repo I'm working on, if that helps.
Here's the downloaded notebook after attempting to run:
&lt;denchmark-h:h1&gt;Unity ML-Agents Toolkit&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;Environment Basics&lt;/denchmark-h&gt;

This notebook contains a walkthrough of the basic functions of the Python API for the Unity ML-Agents toolkit. For instructions on building a Unity environment, see &lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Getting-Started-with-Balance-Ball.md&gt;here&lt;/denchmark-link&gt;
.
&lt;denchmark-h:h3&gt;1. Set environment parameters&lt;/denchmark-h&gt;

Be sure to set env_name to the name of the Unity environment file you want to launch. Ensure that the environment build is in the python/ directory.
env_name = "None"  # Name of the Unity environment binary to launch
train_mode = True  # Whether to run the environment in training or inference mode
&lt;denchmark-h:h3&gt;2. Load dependencies&lt;/denchmark-h&gt;

The following loads the necessary dependencies and checks the Python version (at runtime). ML-Agents Toolkit (v0.3 onwards) requires Python 3.
import matplotlib.pyplot as plt
import numpy as np
import sys

from unityagents import UnityEnvironment

%matplotlib inline

print("Python version:")
print(sys.version)

# check Python version
if (sys.version_info[0] &lt; 3):
    raise Exception("ERROR: ML-Agents Toolkit (v0.3 onwards) requires Python 3")
&lt;denchmark-code&gt;---------------------------------------------------------------------------

ModuleNotFoundError                       Traceback (most recent call last)

&lt;ipython-input-2-e89f3de0a8fb&gt; in &lt;module&gt;()
      3 import sys
      4 
----&gt; 5 from unityagents import UnityEnvironment
      6 
      7 get_ipython().run_line_magic('matplotlib', 'inline')


D:\Tim\Documents\Github\ml-agents\python\unityagents\__init__.py in &lt;module&gt;()
----&gt; 1 from .environment import *
      2 from .brain import *
      3 from .exception import *
      4 from .curriculum import *


D:\Tim\Documents\Github\ml-agents\python\unityagents\environment.py in &lt;module&gt;()
     11 from .curriculum import Curriculum
     12 
---&gt; 13 from communicator_objects import UnityRLInput, UnityRLOutput, AgentActionProto,\
     14     EnvironmentParametersProto, UnityRLInitializationInput, UnityRLInitializationOutput,\
     15     UnityInput, UnityOutput


D:\Tim\Documents\Github\ml-agents\python\communicator_objects\__init__.py in &lt;module&gt;()
----&gt; 1 from .agent_action_proto_pb2 import *
      2 from .agent_info_proto_pb2 import *
      3 from .brain_parameters_proto_pb2 import *
      4 from .brain_type_proto_pb2 import *
      5 from .command_proto_pb2 import *


D:\Tim\Documents\Github\ml-agents\python\communicator_objects\agent_action_proto_pb2.py in &lt;module&gt;()
      4 import sys
      5 _b=sys.version_info[0]&lt;3 and (lambda x:x) or (lambda x:x.encode('latin1'))
----&gt; 6 from google.protobuf import descriptor as _descriptor
      7 from google.protobuf import message as _message
      8 from google.protobuf import reflection as _reflection


ModuleNotFoundError: No module named 'google'
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;3. Start the environment&lt;/denchmark-h&gt;

UnityEnvironment launches and begins communication with the environment when instantiated.
Environments contain brains which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python.
env = UnityEnvironment(file_name=env_name)

# Examine environment parameters
print(str(env))

# Set the default brain to work with
default_brain = env.brain_names[0]
brain = env.brains[default_brain]
&lt;denchmark-code&gt;---------------------------------------------------------------------------

NameError                                 Traceback (most recent call last)

&lt;ipython-input-3-19bd9c6b2f7f&gt; in &lt;module&gt;()
----&gt; 1 env = UnityEnvironment(file_name=env_name)
      2 
      3 # Examine environment parameters
      4 print(str(env))
      5 


NameError: name 'UnityEnvironment' is not defined
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;4. Examine the observation and state spaces&lt;/denchmark-h&gt;

We can reset the environment to be provided with an initial set of observations and states for all the agents within the environment. In ML-Agents, states refer to a vector of variables corresponding to relevant aspects of the environment for an agent. Likewise, observations refer to a set of relevant pixel-wise visuals for an agent.
# Reset the environment
env_info = env.reset(train_mode=train_mode)[default_brain]

# Examine the state space for the default brain
print("Agent state looks like: \n{}".format(env_info.vector_observations[0]))

# Examine the observation space for the default brain
for observation in env_info.visual_observations:
    print("Agent observations look like:")
    if observation.shape[3] == 3:
        plt.imshow(observation[0,:,:,:])
    else:
        plt.imshow(observation[0,:,:,0])
&lt;denchmark-code&gt;---------------------------------------------------------------------------

NameError                                 Traceback (most recent call last)

&lt;ipython-input-4-960e284ef7df&gt; in &lt;module&gt;()
      1 # Reset the environment
----&gt; 2 env_info = env.reset(train_mode=train_mode)[default_brain]
      3 
      4 # Examine the state space for the default brain
      5 print("Agent state looks like: \n{}".format(env_info.vector_observations[0]))


NameError: name 'env' is not defined
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;5. Take random actions in the environment&lt;/denchmark-h&gt;

Once we restart an environment, we can step the environment forward and provide actions to all of the agents within the environment. Here we simply choose random actions based on the action_space_type of the default brain.
Once this cell is executed, 10 messages will be printed that detail how much reward will be accumulated for the next 10 episodes. The Unity environment will then pause, waiting for further signals telling it what to do next. Thus, not seeing any animation is expected when running this cell.
for episode in range(10):
    env_info = env.reset(train_mode=train_mode)[default_brain]
    done = False
    episode_rewards = 0
    while not done:
        if brain.vector_action_space_type == 'continuous':
            env_info = env.step(np.random.randn(len(env_info.agents), 
                                                brain.vector_action_space_size))[default_brain]
        else:
            env_info = env.step(np.random.randint(0, brain.vector_action_space_size, 
                                                  size=(len(env_info.agents))))[default_brain]
        episode_rewards += env_info.rewards[0]
        done = env_info.local_done[0]
    print("Total reward this episode: {}".format(episode_rewards))
&lt;denchmark-code&gt;---------------------------------------------------------------------------

NameError                                 Traceback (most recent call last)

&lt;ipython-input-5-61e63e39d236&gt; in &lt;module&gt;()
      1 for episode in range(10):
----&gt; 2     env_info = env.reset(train_mode=train_mode)[default_brain]
      3     done = False
      4     episode_rewards = 0
      5     while not done:


NameError: name 'env' is not defined
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;6. Close the environment when finished&lt;/denchmark-h&gt;

When we are finished using an environment, we can close it with the function below.
env.close()
&lt;denchmark-code&gt;---------------------------------------------------------------------------

NameError                                 Traceback (most recent call last)

&lt;ipython-input-6-1baceacf4cb1&gt; in &lt;module&gt;()
----&gt; 1 env.close()


NameError: name 'env' is not defined
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='timdhoffmann' date='2018-08-14T23:09:37Z'>
		Ok, I figured out how to run the notebook. Initially I followed the official instructions for running it on windows, but I had to activate the ml-agents environment in Anaconda Prompt first and then run jupyter notebook from the environment...
Anyways, these are the two exported outputs (the "Total rewards this episode:..." outputs seem to be the only difference):
&lt;denchmark-h:h1&gt;a) When using multiple AddVectorObs(float):&lt;/denchmark-h&gt;

&lt;denchmark-h:h1&gt;Unity ML-Agents Toolkit&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;Environment Basics&lt;/denchmark-h&gt;

This notebook contains a walkthrough of the basic functions of the Python API for the Unity ML-Agents toolkit. For instructions on building a Unity environment, see &lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Getting-Started-with-Balance-Ball.md&gt;here&lt;/denchmark-link&gt;
.
&lt;denchmark-h:h3&gt;1. Set environment parameters&lt;/denchmark-h&gt;

Be sure to set env_name to the name of the Unity environment file you want to launch. Ensure that the environment build is in the python/ directory.
env_name = "MultipleFloats/Unity Environment"  # Name of the Unity environment binary to launch
train_mode = True  # Whether to run the environment in training or inference mode
&lt;denchmark-h:h3&gt;2. Load dependencies&lt;/denchmark-h&gt;

The following loads the necessary dependencies and checks the Python version (at runtime). ML-Agents Toolkit (v0.3 onwards) requires Python 3.
import matplotlib.pyplot as plt
import numpy as np
import sys

from unityagents import UnityEnvironment

%matplotlib inline

print("Python version:")
print(sys.version)

# check Python version
if (sys.version_info[0] &lt; 3):
    raise Exception("ERROR: ML-Agents Toolkit (v0.3 onwards) requires Python 3")
&lt;denchmark-code&gt;Python version:
3.6.6 |Anaconda, Inc.| (default, Jun 28 2018, 11:27:44) [MSC v.1900 64 bit (AMD64)]
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;3. Start the environment&lt;/denchmark-h&gt;

UnityEnvironment launches and begins communication with the environment when instantiated.
Environments contain brains which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python.
env = UnityEnvironment(file_name=env_name)

# Examine environment parameters
print(str(env))

# Set the default brain to work with
default_brain = env.brain_names[0]
brain = env.brains[default_brain]
&lt;denchmark-code&gt;INFO:unityagents:
'Academy' started successfully!
Unity Academy name: Academy
        Number of Brains: 1
        Number of External Brains : 1
        Lesson number : 0
        Reset Parameters :
		
Unity brain name: AvoidingMLAgentBrain
        Number of Visual Observations (per agent): 0
        Vector Observation space type: continuous
        Vector Observation space size (per agent): 6
        Number of stacked Vector Observation: 1
        Vector Action space type: continuous
        Vector Action space size (per agent): 4
        Vector Action descriptions: , , , 


Unity Academy name: Academy
        Number of Brains: 1
        Number of External Brains : 1
        Lesson number : 0
        Reset Parameters :
		
Unity brain name: AvoidingMLAgentBrain
        Number of Visual Observations (per agent): 0
        Vector Observation space type: continuous
        Vector Observation space size (per agent): 6
        Number of stacked Vector Observation: 1
        Vector Action space type: continuous
        Vector Action space size (per agent): 4
        Vector Action descriptions: , , , 
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;4. Examine the observation and state spaces&lt;/denchmark-h&gt;

We can reset the environment to be provided with an initial set of observations and states for all the agents within the environment. In ML-Agents, states refer to a vector of variables corresponding to relevant aspects of the environment for an agent. Likewise, observations refer to a set of relevant pixel-wise visuals for an agent.
# Reset the environment
env_info = env.reset(train_mode=train_mode)[default_brain]

# Examine the state space for the default brain
print("Agent state looks like: \n{}".format(env_info.vector_observations[0]))

# Examine the observation space for the default brain
for observation in env_info.visual_observations:
    print("Agent observations look like:")
    if observation.shape[3] == 3:
        plt.imshow(observation[0,:,:,:])
    else:
        plt.imshow(observation[0,:,:,0])
&lt;denchmark-code&gt;Agent state looks like: 
[4.5        4.5        8.39999962 8.39999962 0.         0.        ]
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;5. Take random actions in the environment&lt;/denchmark-h&gt;

Once we restart an environment, we can step the environment forward and provide actions to all of the agents within the environment. Here we simply choose random actions based on the action_space_type of the default brain.
Once this cell is executed, 10 messages will be printed that detail how much reward will be accumulated for the next 10 episodes. The Unity environment will then pause, waiting for further signals telling it what to do next. Thus, not seeing any animation is expected when running this cell.
for episode in range(10):
    env_info = env.reset(train_mode=train_mode)[default_brain]
    done = False
    episode_rewards = 0
    while not done:
        if brain.vector_action_space_type == 'continuous':
            env_info = env.step(np.random.randn(len(env_info.agents), 
                                                brain.vector_action_space_size))[default_brain]
        else:
            env_info = env.step(np.random.randint(0, brain.vector_action_space_size, 
                                                  size=(len(env_info.agents))))[default_brain]
        episode_rewards += env_info.rewards[0]
        done = env_info.local_done[0]
    print("Total reward this episode: {}".format(episode_rewards))
&lt;denchmark-code&gt;Total reward this episode: 18.80000029504299
Total reward this episode: 12.90000020712614
Total reward this episode: 17.700000278651714
Total reward this episode: 50.100000746548176
Total reward this episode: 17.30000027269125
Total reward this episode: 21.100000329315662
Total reward this episode: 50.100000746548176
Total reward this episode: 33.1000005081296
Total reward this episode: 14.70000023394823
Total reward this episode: 13.70000021904707
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;6. Close the environment when finished&lt;/denchmark-h&gt;

When we are finished using an environment, we can close it with the function below.
env.close()
&lt;denchmark-h:h1&gt;b) When using a single AddVectorObs(List&lt;float&gt;):&lt;/denchmark-h&gt;

&lt;denchmark-h:h1&gt;Unity ML-Agents Toolkit&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;Environment Basics&lt;/denchmark-h&gt;

This notebook contains a walkthrough of the basic functions of the Python API for the Unity ML-Agents toolkit. For instructions on building a Unity environment, see &lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Getting-Started-with-Balance-Ball.md&gt;here&lt;/denchmark-link&gt;
.
&lt;denchmark-h:h3&gt;1. Set environment parameters&lt;/denchmark-h&gt;

Be sure to set env_name to the name of the Unity environment file you want to launch. Ensure that the environment build is in the python/ directory.
env_name = "SingleFloatList/Unity Environment"  # Name of the Unity environment binary to launch
train_mode = True  # Whether to run the environment in training or inference mode
&lt;denchmark-h:h3&gt;2. Load dependencies&lt;/denchmark-h&gt;

The following loads the necessary dependencies and checks the Python version (at runtime). ML-Agents Toolkit (v0.3 onwards) requires Python 3.
import matplotlib.pyplot as plt
import numpy as np
import sys

from unityagents import UnityEnvironment

%matplotlib inline

print("Python version:")
print(sys.version)

# check Python version
if (sys.version_info[0] &lt; 3):
    raise Exception("ERROR: ML-Agents Toolkit (v0.3 onwards) requires Python 3")
&lt;denchmark-code&gt;Python version:
3.6.6 |Anaconda, Inc.| (default, Jun 28 2018, 11:27:44) [MSC v.1900 64 bit (AMD64)]
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;3. Start the environment&lt;/denchmark-h&gt;

UnityEnvironment launches and begins communication with the environment when instantiated.
Environments contain brains which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python.
env = UnityEnvironment(file_name=env_name)

# Examine environment parameters
print(str(env))

# Set the default brain to work with
default_brain = env.brain_names[0]
brain = env.brains[default_brain]
&lt;denchmark-code&gt;INFO:unityagents:
'Academy' started successfully!
Unity Academy name: Academy
        Number of Brains: 1
        Number of External Brains : 1
        Lesson number : 0
        Reset Parameters :
		
Unity brain name: AvoidingMLAgentBrain
        Number of Visual Observations (per agent): 0
        Vector Observation space type: continuous
        Vector Observation space size (per agent): 6
        Number of stacked Vector Observation: 1
        Vector Action space type: continuous
        Vector Action space size (per agent): 4
        Vector Action descriptions: , , , 


Unity Academy name: Academy
        Number of Brains: 1
        Number of External Brains : 1
        Lesson number : 0
        Reset Parameters :
		
Unity brain name: AvoidingMLAgentBrain
        Number of Visual Observations (per agent): 0
        Vector Observation space type: continuous
        Vector Observation space size (per agent): 6
        Number of stacked Vector Observation: 1
        Vector Action space type: continuous
        Vector Action space size (per agent): 4
        Vector Action descriptions: , , , 
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;4. Examine the observation and state spaces&lt;/denchmark-h&gt;

We can reset the environment to be provided with an initial set of observations and states for all the agents within the environment. In ML-Agents, states refer to a vector of variables corresponding to relevant aspects of the environment for an agent. Likewise, observations refer to a set of relevant pixel-wise visuals for an agent.
# Reset the environment
env_info = env.reset(train_mode=train_mode)[default_brain]

# Examine the state space for the default brain
print("Agent state looks like: \n{}".format(env_info.vector_observations[0]))

# Examine the observation space for the default brain
for observation in env_info.visual_observations:
    print("Agent observations look like:")
    if observation.shape[3] == 3:
        plt.imshow(observation[0,:,:,:])
    else:
        plt.imshow(observation[0,:,:,0])
&lt;denchmark-code&gt;Agent state looks like: 
[4.5        4.5        8.39999962 8.39999962 0.         0.        ]
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;5. Take random actions in the environment&lt;/denchmark-h&gt;

Once we restart an environment, we can step the environment forward and provide actions to all of the agents within the environment. Here we simply choose random actions based on the action_space_type of the default brain.
Once this cell is executed, 10 messages will be printed that detail how much reward will be accumulated for the next 10 episodes. The Unity environment will then pause, waiting for further signals telling it what to do next. Thus, not seeing any animation is expected when running this cell.
for episode in range(10):
    env_info = env.reset(train_mode=train_mode)[default_brain]
    done = False
    episode_rewards = 0
    while not done:
        if brain.vector_action_space_type == 'continuous':
            env_info = env.step(np.random.randn(len(env_info.agents), 
                                                brain.vector_action_space_size))[default_brain]
        else:
            env_info = env.step(np.random.randint(0, brain.vector_action_space_size, 
                                                  size=(len(env_info.agents))))[default_brain]
        episode_rewards += env_info.rewards[0]
        done = env_info.local_done[0]
    print("Total reward this episode: {}".format(episode_rewards))
&lt;denchmark-code&gt;Total reward this episode: 26.10000040382147
Total reward this episode: 28.40000043809414
Total reward this episode: 18.700000293552876
Total reward this episode: 16.200000256299973
Total reward this episode: 12.300000198185444
Total reward this episode: 19.40000030398369
Total reward this episode: 17.500000275671482
Total reward this episode: 24.600000381469727
Total reward this episode: 17.900000281631947
Total reward this episode: 28.700000442564487
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;6. Close the environment when finished&lt;/denchmark-h&gt;

When we are finished using an environment, we can close it with the function below.
env.close()
		</comment>
		<comment id='4' author='timdhoffmann' date='2018-08-15T15:46:12Z'>
		Thanks for sharing the output from Jupyter, &lt;denchmark-link:https://github.com/timdhoffmann&gt;@timdhoffmann&lt;/denchmark-link&gt;
. Strangely, it seems that the observation in both cases is exactly the same:
[4.5        4.5        8.39999962 8.39999962 0.         0.        ]
vs
[4.5        4.5        8.39999962 8.39999962 0.         0.        ]
Can you describe a little more about how the training and learned behavior is different between the two methods?
		</comment>
		<comment id='5' author='timdhoffmann' date='2018-08-15T16:13:40Z'>
		That's weird. Or maybe it's not. I wouldn't rule out the possibility that the mistake is on my end ;)
I might have done something wrong when I initially experienced the problem, or when building the two different project versions for the notebook testing.
Don't have the time right now, but I will try to do more testing in order to reproduce the issue.
Anyways, here is a brief description of the setup and my observations:

Setup:


It is a simple 2D test project, where the agent with a Rigidbody2D and 0 gravity should learn to float while avoiding to hit one of the four borders around it.
Observations are up/down/left/right distances to the four borders from raycasting and its own x/y velocity.
Actions should be applying x/y force to its own Rigidbody2D.


My observations:


When using a single list of floats as argument with AddVectorObs(), the agent constantly bumped into the borders. I tried a few (3 &lt; few &lt; 10) training passes with no success.
When I switched to using multiple calls to AddVectorObs() with a single float as argument per call, the agent was able to float around and avoid the borders right after the first training.

I don't know if it might have been just by chance that i had been unlucky with the first trainings and suddenly had success with the other method...
		</comment>
		<comment id='6' author='timdhoffmann' date='2018-08-21T18:17:29Z'>
		I am wondering about this same issue. My project is a 2D project as well, and with very similiar expectations and similar problems. My 2d agents keep running into borders when they should know better, and I'm passing a List of floats.... I will currently test to see if it makes a difference to pass them as separate floats and post back.
		</comment>
		<comment id='7' author='timdhoffmann' date='2018-09-27T21:40:35Z'>
		&lt;denchmark-link:https://github.com/awjuliani&gt;@awjuliani&lt;/denchmark-link&gt;
 Can I close this?
		</comment>
		<comment id='8' author='timdhoffmann' date='2019-02-01T06:04:28Z'>
		This issue has been inactive for some time (and we've since released a newer version of ML-Agents), so I'm going to close it.  Please feel free to re-open if you continue to see this problem.
		</comment>
		<comment id='9' author='timdhoffmann' date='2020-02-01T11:59:06Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>