<bug id='3058' author='Phong13' open_date='2019-12-09T18:24:45Z' closed_time='2020-03-12T23:09:48Z'>
	<summary>Academy does not handle an agent becoming active after the academy has initialized</summary>
	<description>
Describe the bug
My situation: I would like to stagger the start of training for my agents, because some of my agents are Done very quickly (agent has fallen down) and return low scores while the successful agents are not Done until much later and return high scores (often reaching maxSteps). The failing agents reset very frequently and fill the episode with low scores. This means that when I restart training, the initial episode has nothing but very low scores then the high scoring successfully agents are all done at the same time (maxSteps). Until the phase of the scoring gets randomized the reward comes in waves (low ... high ... low ... high). This is terrible for the training and the training takes a looooonnnnnggg time to recover.
My simple idea to fix this is to stagger the start of training by starting with all the agents but one initially disabled, then enabling them one at a time. Unfortunately the Academy is not written to handle agents being activated after the Academy has started. It expects all the agents to be active at the start. The agent does not "Reset" before being used.
Steps to reproduce the behavior:
Take one of the example scenes:
Add an _isInitialized variable that is set to true on InitializeAgent. Add checks to CollectObservations and AgentAction to ensure that the agent was initialized before these were called. Do the same with AgentReset. Check that the agent was reset before CollectObservations and AgentAction is called.
Add a script to enable the agents one at a time over a period of ten seconds.
Console logs / stack traces
The Academy does not call AgentReset before calling CollectObservations and AgentAction.
It is easy to fix. Add m_Done = true at the end of Agent.OnEnabledHelper.
Environment (please complete the following information):

Windows 10
ML-Agents v0.10
_TensorFlow 1.7
Environment: Doesn't matter.

NOTE: We are unable to help reproduce bugs with custom environments.  Please attempt to reproduce your issue with one of the example environments, or provide a minimal patch to one of the environments needed to reproduce the issue.
	</description>
	<comments>
		<comment id='1' author='Phong13' date='2020-03-06T23:24:05Z'>
		HI &lt;denchmark-link:https://github.com/Phong13&gt;@Phong13&lt;/denchmark-link&gt;
 - sorry that this one slipped through the cracks.
Some of this code has changed recently, but I think the core issue (Agents that are added after the Academy starts donâ€™t get reset) is still a problem.
I have this logged in our internal tracker as MLA-743 - hopefully we'll have a resolution for it soon.
		</comment>
		<comment id='2' author='Phong13' date='2020-03-12T23:09:48Z'>
		Just a heads up that this was fixed in &lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/pull/3605&gt;#3605&lt;/denchmark-link&gt;
 and will be in the 0.15.0 release (tentatively next week).
		</comment>
	</comments>
</bug>