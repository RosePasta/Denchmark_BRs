<bug id='3754' author='niskander' open_date='2020-04-08T15:56:13Z' closed_time='2020-08-02T21:03:10Z'>
	<summary>Cumulative Reward and Extrinsic Rewards graphs are different when only extrinsic rewards are used</summary>
	<description>
Hi, I've brought up this issue before in &lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/issues/3563&gt;#3563&lt;/denchmark-link&gt;
 but as mentioned there even though this changed behaviour/bug was introduced in the same update as self-play, it happens all the time.
The issue is that the Cumulative Rewards graph and the Extrinsic Rewards graph no longer look the same even though I'm only using extrinsic rewards. (Before the update, they were always the same.) Example screenshot attached. Zero-sum rewards are used so the cumulative rewards graph is correct; I'm not sure how to interpret the other one.
&lt;denchmark-link:https://user-images.githubusercontent.com/1988272/78805564-8d87e880-798f-11ea-87ce-0b5ce6f30e74.jpg&gt;&lt;/denchmark-link&gt;

In the migration notes it was mentioned that steps are now counted per-agent rather than as environment steps. I thought maybe that's the reason but the x-axes are the same for both graphs and they appear to have the same number of data points.
Could you please verify whether this changed behavior is a bug or not?
	</description>
	<comments>
		<comment id='1' author='niskander' date='2020-04-08T22:50:56Z'>
		Hi &lt;denchmark-link:https://github.com/niskander&gt;@niskander&lt;/denchmark-link&gt;

The reason for this is that in release v0.15, rewards are averaged over all agents which share a behavior name. So, in zero sum games, this results in the figure you see.  Your concern (raised in the issue &lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/issues/3563&gt;3563&lt;/denchmark-link&gt;
) led us to modify this so that only trajectories collected by the learning agent will be logged in tensorboard.  This is currently on master and will be part of the next release.
Thank you very much for using this feature and helping us refine it!
		</comment>
		<comment id='2' author='niskander' date='2020-04-09T18:28:48Z'>
		What do Cumulative Rewards and Extrinsic Rewards represent when we have multiple agents in the same instance?
		</comment>
		<comment id='3' author='niskander' date='2020-04-10T14:57:45Z'>
		&lt;denchmark-link:https://github.com/andrewcoh&gt;@andrewcoh&lt;/denchmark-link&gt;
 That's great to hear. However, I think there has to be another issue there, because the graphs are not the same even when self-play is turned off and all agents have the same brain.
Here's an example graph:
&lt;denchmark-link:https://user-images.githubusercontent.com/1988272/78999918-9dcbcf00-7b19-11ea-80a4-7103e9e2502b.png&gt;&lt;/denchmark-link&gt;

In this scenario there are rewards and penalties but the rewards are always higher in magnitude.
It looks like the cumulative environment rewards graph is summing the rewards while the extrinsic rewards graph is displaying each one as a separate data point. Again this behaviour is different from what it used to be. Could that be it?
		</comment>
		<comment id='4' author='niskander' date='2020-04-10T21:10:32Z'>
		&lt;denchmark-link:https://github.com/niskander&gt;@niskander&lt;/denchmark-link&gt;
 Do you have the same issue with PPO?
		</comment>
		<comment id='5' author='niskander' date='2020-04-12T14:30:56Z'>
		I am using PPO with self-play. My environment has fixed episode length, +1 for winning, -1 for losing, and 0 for a draw. The cumulative reward is correct, always zero, while the extrinsic reward shows some spikes.
&lt;denchmark-link:https://user-images.githubusercontent.com/32360377/79071223-d8c93080-7cda-11ea-984b-856078c9d545.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/32360377/79071224-d961c700-7cda-11ea-9115-6557fc56fb9d.png&gt;&lt;/denchmark-link&gt;

I am not sure I understand the extrinsic reward. Does It show only the reward of the learning team?
		</comment>
		<comment id='6' author='niskander' date='2020-07-06T19:11:34Z'>
		&lt;denchmark-link:https://github.com/andrewcoh&gt;@andrewcoh&lt;/denchmark-link&gt;
 Since &lt;denchmark-link:https://github.com/fedetask&gt;@fedetask&lt;/denchmark-link&gt;
 confirmed it for PPO I didn't try. Do you think it's a bug?
		</comment>
		<comment id='7' author='niskander' date='2020-08-02T21:03:10Z'>
		Issue doesn't exist in more recent versions, closing.
		</comment>
	</comments>
</bug>