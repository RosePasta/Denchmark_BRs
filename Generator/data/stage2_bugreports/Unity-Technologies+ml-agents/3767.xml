<bug id='3767' author='ugurkanates' open_date='2020-04-10T11:55:09Z' closed_time='2020-05-15T19:15:20Z'>
	<summary>Basic Environment has a problem with last state(before "done" state)</summary>
	<description>
Describe the bug
On Example environments the "Basic" one has problem with last state being suddenly concatenate last state and a random state( if I understood correctly) .
Normally all states has (1,20) shape but when an environment is close to be done it returns (2,20) shaped NumPy array so it brakes any code that was dependent on it.

Steps to reproduce the behavior:
Just using this example notebook provided
"&lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/blob/release-0.15.0/notebooks/getting-started.ipynb&gt;https://github.com/Unity-Technologies/ml-agents/blob/release-0.15.0/notebooks/getting-started.ipynb&lt;/denchmark-link&gt;
"
Along with Example Project built (or played from unity)
I built environment on 2019 version of Unity on Linux x86_64.  It is visual build ( not serverless build)
Though I also built that can test if anything going to change.
If you print state shapes on notebook at 5th step with
print(step_result.obs[0].shape)
You will see last "next_state - which I interpreted as much"  will be ( 2,20 ) np array instead of (1,20) like all steps taken in environment
This breaks my algorithm due I store states for experience replay.
&lt;denchmark-link:https://camo.githubusercontent.com/1180ca89ede59211eb1560cd90cdeed9505f85405aca9c093ab81ea4a7e1e67b/68747470733a2f2f692e6962622e636f2f4332426b427a472f53637265656e73686f742d436170747572652d323032302d30342d31302d31342d35322d31302e706e67&gt;&lt;/denchmark-link&gt;

Screenshots
Environment (please complete the following information):

OS + version:  Ubuntu 18.04 LTS
ML-Agents version:  0.15.1
Environment: (which example environment you used to reproduce the error) = Basic , Visual build

	</description>
	<comments>
		<comment id='1' author='ugurkanates' date='2020-04-13T19:14:22Z'>
		Hi &lt;denchmark-link:https://github.com/ugurkanates&gt;@ugurkanates&lt;/denchmark-link&gt;

The reason for this is that when an Agent reaches a terminal state, the environment returns the final state as well as a new starting state.
Can you explain your issue a bit more? I'm not sure if you mean there is an issue with the notebook example, or with something you're trying to do on your own.  Either way, let me know and I'll try to help.
		</comment>
		<comment id='2' author='ugurkanates' date='2020-04-13T19:37:37Z'>
		&lt;denchmark-link:https://github.com/andrewcoh&gt;@andrewcoh&lt;/denchmark-link&gt;
  Hi ,
The algorithm I'm using is a Rainbow DQN but it doesn't matter since any algorithm with Experience Replay would face with same problem.
I'm keeping all states in history to later sample and train from them(off -policy) . On my "step" function which basically stores environment results after taking calling environments own step function.  But since all  state shapes added before to batch/memory is (1,20) shaped when terminal state arrives code basically breaks down. I dont think this is correct behaviour from environment.
Just like you said It shouldn't new starting state because that literally breaks all existing implementations  since all shapes must be same.
Problem may be solved if step doesn't return 2 states. I have hacked one of the baseline algorithms to work like this but it's not ideal.
&lt;denchmark-code&gt;def step(self, action: np.ndarray) -&gt; Tuple[np.ndarray, np.float64, bool]:
        self.env.set_actions(group_name,np.atleast_2d(action))
        self.env.step()
        step_result = self.env.get_step_result(group_name)

        next_state = step_result.obs[0]
        reward = step_result.reward[0]
        done = step_result.done[0]
        if(next_state.shape[0] == 2):
            print(next_state.shape)
            next_state = np.d def step(self, action: np.ndarray) -&gt; Tuple[np.ndarray, np.float64, bool]:
        self.env.set_actions(group_name,np.atleast_2d(action))
        self.env.step()
        step_result = self.env.get_step_result(group_name)

        next_state = step_result.obs[0]
        reward = step_result.reward[0]
        done = step_result.done[0]
        if(next_state.shape[0] == 2):
            print(next_state.shape)
            next_state = np.delete(next_state,1,0)
        if not self.is_test:
            self.transition += [reward, next_state, done]
            
            # N-step transition
            if self.use_n_step:
                one_step_transition = self.memory_n.store(*self.transition)
            # 1-step transition
            else:
                one_step_transition = self.transition

            # add a single step transition
            if one_step_transition:
                self.memory.store(*one_step_transition)
    
        return next_state, reward, doneelete(next_state,1,0)
        if not self.is_test:
            self.transition += [reward, next_state, done]
            
            # N-step transition
            if self.use_n_step:
                one_step_transition = self.memory_n.store(*self.transition)
            # 1-step transition
            else:
                one_step_transition = self.transition

            # add a single step transition
            if one_step_transition:
                self.memory.store(*one_step_transition)
    
        return next_state, reward, done
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='ugurkanates' date='2020-05-12T12:33:02Z'>
		Is there any update on this? Experiencing the same problem.
		</comment>
		<comment id='4' author='ugurkanates' date='2020-05-15T19:15:19Z'>
		This is solved with 1.0.0 release considering they revamped Python API and terminal state held in another object type entirely.
		</comment>
	</comments>
</bug>