<bug id='2864' author='ghk71' open_date='2019-11-07T04:50:29Z' closed_time='2019-11-07T19:33:06Z'>
	<summary>Learning will not proceed.</summary>
	<description>
Describe the bug
I have followed the instructions in the manual and have not learned. Pressing the Play button will not proceed.
To Reproduce
Steps to reproduce the behavior:

Follow the manual as it is.
In the anaconda prompt use
'''mlagents-learn Config/trainer_config.yaml --run-id=TEST --train'''
However, even if you press the Play button, the agent does not move in the scene.

Console logs / stack traces
&lt;denchmark-code&gt;Process Process-1:
Traceback (most recent call last):
  File "C:\Users\DVL\Anaconda3\envs\ml-agents\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
  File "C:\Users\DVL\Anaconda3\envs\ml-agents\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
  File "C:\Users\DVL\Anaconda3\envs\ml-agents\lib\site-packages\mlagents\envs\subprocess_env_manager.py", line 82, in worker
    env = env_factory(worker_id)
  File "c:\users\dvl\ml-agents\ml-agents\mlagents\trainers\learn.py", line 368, in create_unity_environment
    args=env_args,
  File "C:\Users\DVL\Anaconda3\envs\ml-agents\lib\site-packages\mlagents\envs\environment.py", line 103, in __init__
    aca_params = self.send_academy_parameters(rl_init_parameters_in)
  File "C:\Users\DVL\Anaconda3\envs\ml-agents\lib\site-packages\mlagents\envs\environment.py", line 671, in send_academy_parameters
    return self.communicator.initialize(inputs).rl_initialization_output
  File "C:\Users\DVL\Anaconda3\envs\ml-agents\lib\site-packages\mlagents\envs\rpc_communicator.py", line 86, in initialize
    "The Unity environment took too long to respond. Make sure that :\n"
mlagents.envs.exception.UnityTimeOutException: The Unity environment took too long to respond. Make sure that :
         The environment does not need user interaction to launch
         The Academy's Broadcast Hub is configured correctly
         The Agents are linked to the appropriate Brains
         The environment and the Python interface have compatible versions.
Traceback (most recent call last):
  File "C:\Users\DVL\Anaconda3\envs\ml-agents\lib\multiprocessing\connection.py", line 312, in _recv_bytes
    nread, err = ov.GetOverlappedResult(True)
BrokenPipeError: [WinError 109] 파이프가 끝났습니다

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\DVL\Anaconda3\envs\ml-agents\lib\site-packages\mlagents\envs\subprocess_env_manager.py", line 59, in recv
    response: EnvironmentResponse = self.conn.recv()
  File "C:\Users\DVL\Anaconda3\envs\ml-agents\lib\multiprocessing\connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "C:\Users\DVL\Anaconda3\envs\ml-agents\lib\multiprocessing\connection.py", line 321, in _recv_bytes
    raise EOFError
EOFError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\DVL\Anaconda3\envs\ml-agents\Scripts\mlagents-learn-script.py", line 11, in &lt;module&gt;
    load_entry_point('mlagents', 'console_scripts', 'mlagents-learn')()
  File "c:\users\dvl\ml-agents\ml-agents\mlagents\trainers\learn.py", line 417, in main
    run_training(0, run_seed, options, Queue())
  File "c:\users\dvl\ml-agents\ml-agents\mlagents\trainers\learn.py", line 219, in run_training
    options.sampler_file_path, env.reset_parameters, run_seed
  File "C:\Users\DVL\Anaconda3\envs\ml-agents\lib\site-packages\mlagents\envs\subprocess_env_manager.py", line 225, in reset_parameters
    return self.env_workers[0].recv().payload
  File "C:\Users\DVL\Anaconda3\envs\ml-agents\lib\site-packages\mlagents\envs\subprocess_env_manager.py", line 62, in recv
    raise UnityCommunicationException("UnityEnvironment worker: recv failed.")
mlagents.envs.exception.UnityCommunicationException: UnityEnvironment worker: recv failed.
&lt;/denchmark-code&gt;


:Agent Inspector
&lt;denchmark-link:https://user-images.githubusercontent.com/38937802/68361060-ba786b00-0165-11ea-87da-ad37cf2a0d88.png&gt;&lt;/denchmark-link&gt;

:Academy Inspector
&lt;denchmark-link:https://user-images.githubusercontent.com/38937802/68361155-2b1f8780-0166-11ea-848e-280a67a54dfe.png&gt;&lt;/denchmark-link&gt;

:Roller Academy
&lt;denchmark-code&gt;using System.Collections;
using System.Collections.Generic;
using UnityEngine;
using MLAgents;

public class RollerAcademy : Academy
{
}

&lt;/denchmark-code&gt;

:Roller Agent
&lt;denchmark-code&gt;using System.Collections;
using System.Collections.Generic;
using UnityEngine;
using MLAgents;

public class RollerAgent : Agent
{
    Rigidbody rBody;

    private void Start()
    {
        rBody = GetComponent&lt;Rigidbody&gt;();
    }

    public Transform Target;
    public override void AgentReset()
    {
        if(this.transform.position.y &lt; 0)
        {
            this.rBody.angularVelocity = Vector3.zero;
            this.rBody.velocity = Vector3.zero;
            this.transform.position = new Vector3(0, 0.5f, 0);
        }

        Target.position = new Vector3(Random.value * 8 - 4, 0.5f, Random.value * 8 - 4);
    }

    public override void CollectObservations()
    {
        AddVectorObs(Target.position);
        AddVectorObs(this.transform.position);

        AddVectorObs(rBody.velocity.x);
        AddVectorObs(rBody.velocity.z);
    }

    public float speed = 10;
    public override void AgentAction(float[] vectorAction, string textAction)
    {
        Vector3 controlSignal = Vector3.zero;
        controlSignal.x = vectorAction[0];
        controlSignal.z = vectorAction[1];

        rBody.AddForce(controlSignal * speed);

        float distanceToTarget = Vector3.Distance(this.transform.position, Target.position);

        if(distanceToTarget &lt; 1.42f)
        {
            SetReward(1.0f);
            Done();
        }

        if(this.transform.position.y &lt;0)
        {
            Done();
        }
    }

    public override float[] Heuristic()
    {
        var action = new float[2];
        action[0] = Input.GetAxis("Horizontal");
        action[1] = Input.GetAxis("Vertical");
        return action;
    }
}

&lt;/denchmark-code&gt;

Environment (please complete the following information):

OS: Window 10
ML-Agents: 0.11.0
TensorFlow: 0.15.0
Unity: 2019.1.14f1
Python: 3.7
Anaconda: latest version
Use NetFrameWork 4.x

	</description>
	<comments>
		<comment id='1' author='ghk71' date='2019-11-07T11:47:27Z'>
		I have exactly the same issues when trying to train the Example "3DBall" environment, or just when trying to access the environment executable I previously built (following the documentation instructions), i.e. by running
&lt;denchmark-code&gt;from mlagents.envs.environment import UnityEnvironment
env = UnityEnvironment(file_name=&lt;env_name&gt;)
&lt;/denchmark-code&gt;

Error messages fully match the ones &lt;denchmark-link:https://github.com/ghk71&gt;@ghk71&lt;/denchmark-link&gt;
 posted above.
My set-up:
Windows 10
ML-Agents:  0.11.0
TensorFlow: 1.7.1 (GPU)
Unity: I tried both 2017.4.34.f1 and 2019.3.0b7
Python: I tried 3.8.x, 3.7.5 and 3.6.9
Anaconda: latest version
		</comment>
		<comment id='2' author='ghk71' date='2019-11-07T14:55:37Z'>
		Hi, please make sure your ML-Agents python package and UnitySDK are from the 0.11.0 release.  The definition of the gRPC service changed and it is not backwards compatible.
		</comment>
		<comment id='3' author='ghk71' date='2019-11-07T17:38:27Z'>
		
Hi, please make sure your ML-Agents python package and UnitySDK are from the 0.11.0 release. The definition of the gRPC service changed and it is not backwards compatible.

How I Can  check that??
		</comment>
		<comment id='4' author='ghk71' date='2019-11-07T18:09:22Z'>
		Oh... sorry I'm downloaded last version on git clone
but this version download to zip file and not set on virtual env
		</comment>
		<comment id='5' author='ghk71' date='2019-11-07T19:33:06Z'>
		Looks like you have resolved the issue. Closing it for now. Feel free to re-open if needed. Thanks.
		</comment>
		<comment id='6' author='ghk71' date='2019-11-12T11:17:28Z'>
		Hey again,
unfortunately I was not able to resolve the issue. I could successfully install and run ML-Agents on my personal computer, however on my PC at work there seems to remain the exact same issue as posted above. I cannot re-open the issue, since I didn't create it, I thought I'll just try and comment anyways.
I don't know how to ensure that the ML-Agents python package and UnitySDK are both from the latest release 0.11.0. However, I did the following steps and keep running into these error messages, not being able to connect to my executable or train at any point (I am working on Windows 10):

Install latest version of anaconda
Create and activate new python anaconda environment (I tried both python 3.7.4 and 3.6.9)
install tensorflow (the issue occures with both tf version 1.7.1 and 1.14)
with the anaconda environment activated, I cloned the git repo from git clone https://github.com/Unity-Technologies/ml-agents.git
I ran the following steps from the installation manual to install ml-agents:

&lt;denchmark-code&gt;cd ml-agents-envs
pip3 install -e ./
cd ..
cd ml-agents
pip3 install -e ./
&lt;/denchmark-code&gt;


I am currently working with Unity 2019.3.0b7, but I also tried running ml-agents with 2017.4.34f1
I tried both training the 3DBall example environment from the Unity editor itself (--env=None) and from an executable, as well as trying to train the RollerBall example.

Maybe there is some obvious mistake or step I am missing here, but I am unable to find it myself, and as already said I was able to successfully install ML-Agents on another computer.
Any help is very much appreciated.
Thanks and kind regards.
		</comment>
		<comment id='7' author='ghk71' date='2019-11-12T18:22:54Z'>
		Please execute pip show mlagents to see the version of ml-agents you have installed. In case you are not using 0.11.0 please execute pip install --upgrade mlagents. Also do the same with mlagents-envs. Hope this helps.
		</comment>
	</comments>
</bug>