<bug id='2335' author='FlimFlamm' open_date='2019-07-25T07:48:15Z' closed_time='2020-03-11T20:00:33Z'>
	<summary>Multiple brain + "reccurence" issue (ValueError exception when training)</summary>
	<description>
edit 3
bug is reproduced here &lt;denchmark-link:https://github.com/FlimFlamm/ml-agents&gt;https://github.com/FlimFlamm/ml-agents&lt;/denchmark-link&gt;

the standard pushblock scene is configured with a crawler agent, and it has use_recurrent set to t rue
edit 2
I have duplicated this error using default examples.
By putting a crawler into a push-block environment, and giving the push-block and crawler brains "recurrence", the same error occurs
edit
So it turns out that RECURRENCE is the problem. Something about any of these 5 brains being an RNN throws the crash-causing exception. I'm still looking for a proper fix though...
Bug Description:
When training a multi-brain setup where each brain has the same observation quantity and output space, there are no issues, but when I add an agent and brain that is dissimilar from the others, It throws an error. I've tested the agents individually and they work fine. The agent codes also run fine together as long as only one sized brain makes decisions.
A bit more info about the situation:
I have built a spider with 8 semi-autonomous legs, where each leg is an agent (8 leg agents total). Each pair of leg agents (four pairs total) get decisions from one of four identical but separate learning brains (one brain for the front legs, one for the back legs, etc...), and they all use the same agent script because their observation and action spaces are identical.
The problem I'm having occurred once I attempted to add an agent+brain to control the "abdomen". The "abdomen brain" has fewer moving parts and requires smaller observation and output spaces.
I've done testing and narrowed down the cause to any instance where the "abdomen brain" makes decisions at any point after any of the 8 leg agents /w 4 leg brains have made a decision, or vice versa.
Is there something about training multiple brains simultaneously that I've missed? (In older bug reports I've read that brains should be somehow organized into dictionaries, but i think this was depreciated information from 2017). I'm using on-demand decisions and a separate script to actually handle the triggering of decisions: when an individual agent enters conditions requiring a decision, it starts a counter in another script that is meant to prevent individual agents from making multiple decisions per FixedUpdate
I would post the code, but there's so much of it (and it is poorly organized at the moment), so I'm not sure what is needed. Any help or advice would be appreciated (I can share the project if needed, but it is quite large, messy, and convoluted)
Here is the error (and the loading data):
&lt;denchmark-code&gt; '--curriculum': 'None',
 '--debug': False,
 '--docker-target-name': 'None',
 '--env': 'None',
 '--help': False,
 '--keep-checkpoints': '5',
 '--lesson': '0',
 '--load': False,
 '--no-graphics': False,
 '--num-envs': '1',
 '--num-runs': '1',
 '--run-id': 'ppo',
 '--save-freq': '50000',
 '--seed': '-1',
 '--slow': False,
 '--train': True,
 '&lt;trainer-config-path&gt;': 'config\\trainer_config.yaml'}
INFO:mlagents.envs:Start training by pressing the Play button in the Unity Editor.
INFO:mlagents.envs:
'SpiderGoAcademy' started successfully!
Unity Academy name: SpiderGoAcademy
        Number of Brains: 5
        Number of Training Brains : 5
        Reset Parameters :

Unity brain name: SpiderAbdomenBrain
        Number of Visual Observations (per agent): 0
        Vector Observation space size (per agent): 1
        Number of stacked Vector Observation: 3
        Vector Action space type: discrete
        Vector Action space size (per agent): [3, 3, 3]
        Vector Action descriptions: , ,
Unity brain name: SpiderLegBrain
        Number of Visual Observations (per agent): 0
        Vector Observation space size (per agent): 25
        Number of stacked Vector Observation: 3
        Vector Action space type: discrete
        Vector Action space size (per agent): [3, 3, 3, 3, 3, 3, 3, 3]
        Vector Action descriptions: , , , , , , ,
Unity brain name: SpiderLegBrain2
        Number of Visual Observations (per agent): 0
        Vector Observation space size (per agent): 25
        Number of stacked Vector Observation: 3
        Vector Action space type: discrete
        Vector Action space size (per agent): [3, 3, 3, 3, 3, 3, 3, 3]
        Vector Action descriptions: , , , , , , ,
Unity brain name: SpiderLegBrain3
        Number of Visual Observations (per agent): 0
        Vector Observation space size (per agent): 25
        Number of stacked Vector Observation: 3
        Vector Action space type: discrete
        Vector Action space size (per agent): [3, 3, 3, 3, 3, 3, 3, 3]
        Vector Action descriptions: , , , , , , ,
Unity brain name: SpiderLegBrain4
        Number of Visual Observations (per agent): 0
        Vector Observation space size (per agent): 25
        Number of stacked Vector Observation: 3
        Vector Action space type: discrete
        Vector Action space size (per agent): [3, 3, 3, 3, 3, 3, 3, 3]
        Vector Action descriptions: , , , , , , ,
INFO:mlagents.envs:Hyperparameters for the PPO Trainer of brain SpiderAbdomenBrain:
        batch_size:     1024
        beta:   0.001
        buffer_size:    4096
        epsilon:        0.1
        gamma:  0.8
        hidden_units:   256
        lambd:  0.9
        learning_rate:  0.0003
        max_steps:      3000000
        normalize:      False
        num_epoch:      3
        num_layers:     4
        time_horizon:   64
        sequence_length:        16
        summary_freq:   10000
        use_recurrent:  True
        summary_path:   ./summaries/ppo-0_SpiderAbdomenBrain
        memory_size:    32
        use_curiosity:  False
        curiosity_strength:     0.001
        curiosity_enc_size:     128
        model_path:     ./models/ppo-0/SpiderAbdomenBrain
INFO:mlagents.envs:Hyperparameters for the PPO Trainer of brain SpiderLegBrain:
        batch_size:     1024
        beta:   0.001
        buffer_size:    4096
        epsilon:        0.1
        gamma:  0.8
        hidden_units:   256
        lambd:  0.9
        learning_rate:  0.0003
        max_steps:      3000000
        normalize:      False
        num_epoch:      3
        num_layers:     4
        time_horizon:   64
        sequence_length:        16
        summary_freq:   10000
        use_recurrent:  True
        summary_path:   ./summaries/ppo-0_SpiderLegBrain
        memory_size:    32
        use_curiosity:  False
        curiosity_strength:     0.001
        curiosity_enc_size:     128
        model_path:     ./models/ppo-0/SpiderLegBrain
INFO:mlagents.envs:Hyperparameters for the PPO Trainer of brain SpiderLegBrain2:
        batch_size:     1024
        beta:   0.001
        buffer_size:    4096
        epsilon:        0.1
        gamma:  0.8
        hidden_units:   256
        lambd:  0.9
        learning_rate:  0.0003
        max_steps:      3000000
        normalize:      False
        num_epoch:      3
        num_layers:     4
        time_horizon:   64
        sequence_length:        16
        summary_freq:   10000
        use_recurrent:  True
        summary_path:   ./summaries/ppo-0_SpiderLegBrain2
        memory_size:    32
        use_curiosity:  False
        curiosity_strength:     0.001
        curiosity_enc_size:     128
        model_path:     ./models/ppo-0/SpiderLegBrain2
INFO:mlagents.envs:Hyperparameters for the PPO Trainer of brain SpiderLegBrain3:
        batch_size:     1024
        beta:   0.001
        buffer_size:    4096
        epsilon:        0.1
        gamma:  0.8
        hidden_units:   256
        lambd:  0.9
        learning_rate:  0.0003
        max_steps:      3000000
        normalize:      False
        num_epoch:      3
        num_layers:     4
        time_horizon:   64
        sequence_length:        16
        summary_freq:   10000
        use_recurrent:  True
        summary_path:   ./summaries/ppo-0_SpiderLegBrain3
        memory_size:    32
        use_curiosity:  False
        curiosity_strength:     0.001
        curiosity_enc_size:     128
        model_path:     ./models/ppo-0/SpiderLegBrain3
INFO:mlagents.envs:Hyperparameters for the PPO Trainer of brain SpiderLegBrain4:
        batch_size:     1024
        beta:   0.001
        buffer_size:    4096
        epsilon:        0.1
        gamma:  0.8
        hidden_units:   256
        lambd:  0.9
        learning_rate:  0.0003
        max_steps:      3000000
        normalize:      False
        num_epoch:      3
        num_layers:     4
        time_horizon:   64
        sequence_length:        16
        summary_freq:   10000
        use_recurrent:  True
        summary_path:   ./summaries/ppo-0_SpiderLegBrain4
        memory_size:    32
        use_curiosity:  False
        curiosity_strength:     0.001
        curiosity_enc_size:     128
        model_path:     ./models/ppo-0/SpiderLegBrain4
Traceback (most recent call last):
  File "C:\Users\billyjoe\Anaconda3\envs\ml-agents\Scripts\mlagents-learn-script.py", line 11, in &lt;module&gt;
    load_entry_point('mlagents', 'console_scripts', 'mlagents-learn')()
  File "c:\users\billyjoe\documents\unity projects\ml-agents-master\ml-agents-master\ml-agents\mlagents\trainers\learn.py", line 285, in main
    run_training(0, run_seed, options, Queue())
  File "c:\users\billyjoe\documents\unity projects\ml-agents-master\ml-agents-master\ml-agents\mlagents\trainers\learn.py", line 107, in run_training
    tc.start_learning(env, trainer_config)
  File "c:\users\billyjoe\documents\unity projects\ml-agents-master\ml-agents-master\ml-agents\mlagents\trainers\trainer_controller.py", line 244, in start_learning
    new_info = self.take_step(env, curr_info)
  File "c:\users\billyjoe\documents\unity projects\ml-agents-master\ml-agents-master\ml-agents\mlagents\trainers\trainer_controller.py", line 315, in take_step
    curr_info, new_info, take_action_outputs[brain_name]
  File "c:\users\billyjoe\documents\unity projects\ml-agents-master\ml-agents-master\ml-agents\mlagents\trainers\ppo\trainer.py", line 236, in add_experiences
    curr_to_use = self.construct_curr_info(next_info)
  File "c:\users\billyjoe\documents\unity projects\ml-agents-master\ml-agents-master\ml-agents\mlagents\trainers\ppo\trainer.py", line 188, in construct_curr_info
    memories = np.vstack(memories)
  File "C:\Users\billyjoe\Anaconda3\envs\ml-agents\lib\site-packages\numpy\core\shape_base.py", line 234, in vstack
    return _nx.concatenate([atleast_2d(_m) for _m in tup], 0)
**ValueError: need at least one array to concatenate**
UnityEnvironment worker: keyboard interrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "C:\Users\billyjoe\Anaconda3\envs\ml-agents\lib\multiprocessing\util.py", line 319, in _exit_function
    p.join()
  File "C:\Users\billyjoe\Anaconda3\envs\ml-agents\lib\multiprocessing\process.py", line 124, in join
    res = self._popen.wait(timeout)
  File "C:\Users\billyjoe\Anaconda3\envs\ml-agents\lib\multiprocessing\popen_spawn_win32.py", line 80, in wait
    res = _winapi.WaitForSingleObject(int(self._handle), msecs)```


**Environment (please complete the following information):**
- Windows 10
- _ML-Agents version_: v8.2
- _Environment_: custom environment

**NOTE:** We are unable to help reproduce bugs with custom environments.  Please attempt to reproduce your issue with one of the example environments, or provide a minimal patch to one of the environments needed to reproduce the issue.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='FlimFlamm' date='2019-07-25T21:19:28Z'>
		So it turns out that RECURRENCE is the problem. Something about any of these 5 brains being an RNN throws the crash-causing exception.
		</comment>
		<comment id='2' author='FlimFlamm' date='2019-07-25T21:35:49Z'>
		I have duplicated this error using default examples.
By putting a crawler into a push-block environment, and giving the push-block and crawler brains "recurrence", the same error occurs
		</comment>
		<comment id='3' author='FlimFlamm' date='2019-07-25T22:30:40Z'>
		Hi &lt;denchmark-link:https://github.com/FlimFlamm&gt;@FlimFlamm&lt;/denchmark-link&gt;
 -- could you share the example where you changed the Crawler / PushBlock environments to reproduce?  Maybe on a fork of this repo?
		</comment>
		<comment id='4' author='FlimFlamm' date='2019-07-25T23:15:01Z'>
		
Hi @FlimFlamm -- could you share the example where you changed the Crawler / PushBlock environments to reproduce? Maybe on a fork of this repo?

I exported the crawler + push block + dependencies into a package (and zipped it). Please let me know if that is not a suitable format:  &lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/files/3433541/crawler-pushblock.bug.zip&gt;crawler-pushblock bug.zip&lt;/denchmark-link&gt;

To trigger the bug, the following trainer config file can be used (it has recurrent set to true)
&lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/files/3433548/trainer_config.zip&gt;trainer_config.zip&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='FlimFlamm' date='2019-07-25T23:23:59Z'>
		&lt;denchmark-link:https://github.com/harperj&gt;@harperj&lt;/denchmark-link&gt;
 I forgot to mention that it is the first push block scene that has the crawler added.
		</comment>
		<comment id='6' author='FlimFlamm' date='2019-07-25T23:52:22Z'>
		&lt;denchmark-link:https://github.com/harperj&gt;@harperj&lt;/denchmark-link&gt;
 I just updated a fork, so any formatting or import problems should be resolved by using this ml-agents repo &lt;denchmark-link:https://github.com/FlimFlamm/ml-agents&gt;https://github.com/FlimFlamm/ml-agents&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='FlimFlamm' date='2019-07-26T00:02:29Z'>
		The crawler agent has a bit of configuring required (you need to add its body parts to the agent script manually) so my apologies if you had any difficulties getting it running. The forked repository has it set correctly. (if you have any trouble, please let me know)
IF either the pushblock or the crawler brain has recurrence, it breaks the training process. At first I assumed it was my own spaghetti code, but I've narrowed the issue down to recurrence.
In cases where the observations and actions spaces match (and the agent scripts are all the same), I do not believe that this is actually a problem, as I've had no issue with 4 unique but identical brains training across multiple agents simultaneously. It's also possible that the problem pertains to using more than one sort of AgentScript. The 4 unique but identical brains I've been using also use the same agent script, so that might be why I've not had the problem until now.
		</comment>
		<comment id='8' author='FlimFlamm' date='2019-07-29T20:07:37Z'>
		Hi &lt;denchmark-link:https://github.com/FlimFlamm&gt;@FlimFlamm&lt;/denchmark-link&gt;
 -- looks like the repo you shared is missing a Prefab with the updated agents and training areas, so I can't run it.
		</comment>
		<comment id='9' author='FlimFlamm' date='2019-07-29T21:14:04Z'>
		&lt;denchmark-link:https://github.com/harperj&gt;@harperj&lt;/denchmark-link&gt;
 Thanks for the response. I just downloaded and tested it, and it definitely throws the error.
I did not make any new prefabs or scenes, I merely added a crawler agent to the PushBlock scene, and then gave the crawler "recurrent: true" in the trainer_config.yaml.
The relevant scene is at ML-Agents\Examples\PushBlock\Scenes\PushBlock
What was the exact issue that you were having? Let me know and I will do whatever is needed to fix it.
But essentially, the error happens whenever there is more than one type of learning agent in the scene, and where at least one of the agents has recurrence. I just put a crawler into the push-block scene, gave it recurrence, and the error occurs. (I did set it up in the academy and configure the crawler agent itself, and it works when recurrence is turned back off)
		</comment>
		<comment id='10' author='FlimFlamm' date='2019-07-29T21:15:33Z'>
		Eeverything should be properly set up, so all you need to do is open the SDK folder from the unity hub, and then initiate training with "mlagents-learn config\trainer_config.yaml --train" from the ml-agents root
		</comment>
		<comment id='11' author='FlimFlamm' date='2019-08-07T02:44:20Z'>
		&lt;denchmark-link:https://github.com/harperj&gt;@harperj&lt;/denchmark-link&gt;

Hello Again Harper, were you able to reproduce the bug? The version is no longer up to date since the 9.0 release (I think). Should I update to make sure the error persists?
Just to reiterate the problem, using multiple agents and brains with recurrence simply does not work (if you add one of the basic agents to the environment of another, and give one of the two  trainer_config profiles in use "use_recurrent: true", then it will throw a "value error" while unity itself hangs. Turning off recurrence allows both types of agents to train in the same scene at the same time, so I am positive that something about recurrence itself is the problem (i believe it is related to the way the engine packages memory data. if the memory arrays are differently shaped or different in number, and the handler doesn't know how to deal with unique instances, that would explain the "need at least one array to concatenate" issue, as the agent with the smaller memory buffer would lack enough inputs.
The only exception to reproducing this error as i have described is when only one agent file is used, and all the different brains have the same settings and input/output shape.
		</comment>
		<comment id='12' author='FlimFlamm' date='2020-03-11T20:00:33Z'>
		Hi &lt;denchmark-link:https://github.com/FlimFlamm&gt;@FlimFlamm&lt;/denchmark-link&gt;
 -- I was never able to reproduce this issue at the time.  I've tried again in the latest releases (including v0.14, the most recent release) and found that training works correctly.  We've also managed to combine various other agent types together with no issue.
Since it has been a long time and this is resolved in recent releases I'm going to go ahead and close the issue.
		</comment>
	</comments>
</bug>