<bug id='2153' author='nikola-j' open_date='2019-06-20T14:54:34Z' closed_time='2020-03-11T02:07:55Z'>
	<summary>Vector observation size limit</summary>
	<description>
Hi guys, I'm trying to use multiple agents in a single env, where each agent is using a vector of size 65536.
I'm using the Python API and when I try to restart the env with 15 agents it works, but when I try the same thing with 16 agents it hangs indefinitely.
Do you maybe know the reason why this would happen? Is there a limit with obs size in the Python API?
	</description>
	<comments>
		<comment id='1' author='nikola-j' date='2019-06-24T13:43:21Z'>
		Here is a way to reproduce the behavior:

Open Ball3D example scene
Remove NN model from 3DBallLearningBrain
Set observation size to 88000
Change Ball3DAgent.cs as follows:

    private List&lt;float&gt; a;

    public override void InitializeAgent()
    {
        ballRb = ball.GetComponent&lt;Rigidbody&gt;();
        a = new List&lt;float&gt;(88000 - 8);
        for (var i = 0; i &lt; a.Capacity; i++)
        {
            a.Add(0f);
        }
    }

    public override void CollectObservations()
    {
        AddVectorObs(gameObject.transform.rotation.z);
        AddVectorObs(gameObject.transform.rotation.x);
        AddVectorObs(ball.transform.position - gameObject.transform.position);
        AddVectorObs(ballRb.velocity);
        AddVectorObs(a);
    }

Set control flat for brain in Ball3DAcademy
Build it as a server build and save it somewhere
Run the env from python using this script:

import random
import time
from mlagents.envs import UnityEnvironment

unity_env = UnityEnvironment("3dball.x86_64")

unity_env.reset()

t1 = time.time()

for i in range(1000):
    if i%100==0:
        print(i,"/",1000)
    unity_env.step([random.uniform(-1,1)]*24)

print(time.time()-t1)
unity_env.close()

The script will stop on reset() and run indefinitely

Another weird thing to note is that changing the observation size will dramatically impact how fast we can iterate through the environment.
These are the results I got:

Observation size 8: 5 seconds to run for 1000 steps
Observation size 2000: 11 seconds to run for 1000 steps
Observation size 10000: 46 seconds to run for 1000 steps
Observation size 87000: 400 seconds to run for 1000 steps

		</comment>
		<comment id='2' author='nikola-j' date='2019-06-24T23:30:54Z'>
		Hi &lt;denchmark-link:https://github.com/nikola-j&gt;@nikola-j&lt;/denchmark-link&gt;
, thanks for reporting this bug. I was also able to reproduce the bug on my mac using a mac build. When the vector observation is 88000, and when I do unity_env.reset(), the unity executable will crash. Which when the vector observation is 10000, it doesn't crash.
Also when the observation size is larger, the simulation time increase is expected, because we need to serialize and send this data from C# side to python side and then back. Larger array will take larger amount of time to serialize and deserialize.
		</comment>
		<comment id='3' author='nikola-j' date='2019-06-25T11:02:46Z'>
		Hi &lt;denchmark-link:https://github.com/xiaomaogy&gt;@xiaomaogy&lt;/denchmark-link&gt;
, no problem.
Do you think there is room to improve the communication speed between C# and python?
		</comment>
		<comment id='4' author='nikola-j' date='2019-06-25T18:45:57Z'>
		&lt;denchmark-link:https://github.com/nikola-j&gt;@nikola-j&lt;/denchmark-link&gt;
 There definitely is. We are working on this.
		</comment>
		<comment id='5' author='nikola-j' date='2019-07-30T22:17:41Z'>
		Hi all -- this issue has been inactive for some time so I'm going to close it.  Feel free to reopen or create a new issue if you have more to discuss.
		</comment>
		<comment id='6' author='nikola-j' date='2019-07-31T19:44:05Z'>
		Hi &lt;denchmark-link:https://github.com/harperj&gt;@harperj&lt;/denchmark-link&gt;
, I know it has been inactive, but shouldn't it be closed once the bug is fixed? Or do you guys use something internal to track issues?
		</comment>
		<comment id='7' author='nikola-j' date='2019-08-01T21:44:39Z'>
		Hi &lt;denchmark-link:https://github.com/nikola-j&gt;@nikola-j&lt;/denchmark-link&gt;
 -- thanks for the comment.  It was a mistake on my part to close this issue, we will keep it open until the bug is fixed and I'll also make sure it's captured on our internal bug tracker.
		</comment>
		<comment id='8' author='nikola-j' date='2019-10-02T17:56:27Z'>
		Hi &lt;denchmark-link:https://github.com/nikola-j&gt;@nikola-j&lt;/denchmark-link&gt;
,
Thanks for reporting this issue.  We have logged the bug internally as MLA-84 and will update this issue once we have more information.
		</comment>
		<comment id='9' author='nikola-j' date='2019-10-10T16:40:00Z'>
		HI &lt;denchmark-link:https://github.com/nikola-j&gt;@nikola-j&lt;/denchmark-link&gt;

A small update on this. While doing some profiling yesterday, I noticed this this block was especially slow for large observations:


Some alternate approaches are discussed here: &lt;denchmark-link:https://stackoverflow.com/questions/6736590/fast-check-for-nan-in-numpy&gt;https://stackoverflow.com/questions/6736590/fast-check-for-nan-in-numpy&lt;/denchmark-link&gt;
 . I'm probably going to switch to the  approach soon. I'm reluctant to drop the check entirely, but maybe we can add an option to disable it...
		</comment>
		<comment id='10' author='nikola-j' date='2019-10-24T00:01:31Z'>
		&lt;denchmark-link:https://github.com/nikola-j&gt;@nikola-j&lt;/denchmark-link&gt;
 Hard to say that this is "fixed", but we sped up one part of the code that slowed down a lot with large numbers of vector observations: &lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/pull/2717&gt;#2717&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='nikola-j' date='2019-10-24T09:14:53Z'>
		Hi &lt;denchmark-link:https://github.com/chriselion&gt;@chriselion&lt;/denchmark-link&gt;
, thanks, that's great! Maybe you could run the check every 100 steps or something similar? How much of an improvement do you get if you disable it entirely?
		</comment>
		<comment id='12' author='nikola-j' date='2020-03-11T02:07:55Z'>
		This issue has been fixed in the latest version, so I'm going to close it for now. If you find this continues to be an issue, feel free to reopen it.
		</comment>
	</comments>
</bug>