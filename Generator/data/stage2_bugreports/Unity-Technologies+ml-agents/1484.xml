<bug id='1484' author='atapley' open_date='2018-12-13T19:40:07Z' closed_time='2019-09-16T17:40:01Z'>
	<summary>Unity Crashing on Windows</summary>
	<description>
I am currently running the PPO algorithm to train a network on a Windows machine. The program runs for a while (usually between 6 hrs to 13 hrs) but then I get a popup saying "Unity.exe has stopped working" which then stops the algorithm and sometimes requires the machine to reboot.
I am assuming this is a memory leak issue which saturates the PC's memory and causes it to crash. I cannot confirm this, however, since it usually crashes many hours into the training and I do not have a set idea of when it will happen without me constantly monitoring it. When I watch the memory usage in the beginning stages of training, I can see it slowly rising, but I have not watched it enough to the point where it completely saturates.
I have slightly modified the algorithm to save an additional field into the buffer for training purposes, but those values should be erased when the buffer is reset by the ml-agents code, so I do not think that is the issue.
	</description>
	<comments>
		<comment id='1' author='atapley' date='2018-12-13T19:42:17Z'>
		Edit: I am using ml-agents 0.4b.0 and Unity 2018.2.7 when this issue occurs. The environment is built on a Mac for Windows, then used on a Windows 10 machine.
		</comment>
		<comment id='2' author='atapley' date='2018-12-13T20:41:18Z'>
		What process has the memory rising: Is it the Unity executable or the Python training process ?
Can you give us more details about the environment you are using: Does it have visual observations ? How many agents are present in the scene ? Do you observe the same issue when you use our code (without your added fields to the buffer)? Is there a way we could reproduce this bug ?
		</comment>
		<comment id='3' author='atapley' date='2018-12-13T20:59:36Z'>
		When looking in the Task Manager, it is the Unity Environment process which has the rising memory usage.
The environment has one agent which is using solely RGB visual observations. The textures involved are simply colors (colored materials) as opposed to images used for textures.
This issue only arises when the code is being run for extended periods of time, so I have not tried it for the same length of time with the original code. The memory rises even with the original code, but I am not sure if it crashes the environment or not. I will run it overnight and get back to you with the results.
		</comment>
		<comment id='4' author='atapley' date='2018-12-13T22:41:15Z'>
		I do not quite understand what you mean by "The textures involved are simply colors (colored materials) as opposed to images used for textures."
What is the resolution ?
I am wondering if the issue is due to the visual observations, could you try a training without visual observations and see if the memory is still rising?
Thank you.
		</comment>
		<comment id='5' author='atapley' date='2018-12-14T00:54:01Z'>
		By that I just mean that the textures are just plain colors as opposed to something like brick/stone. I am not sure if the complexity of the texture can play a part in it or not, but I figured I would bring it up.
The visual observations are 256 x 256. They're quite big, but the higher resolution is necessary. It trains for a decent amount of time before crashing as well, so I am not sure if that would be the issue. I am currently running a program with the unaltered Unity PPO code to see if it crashes or not. Like I said, the memory was rising last I checked, but I am waiting to see if it will crash or not.
I will run a program with vector observations once that is finished and see if the same results are achieved.
		</comment>
		<comment id='6' author='atapley' date='2018-12-14T13:11:34Z'>
		Update: Unity crashed with the original code as well after about 5.5 hrs on the same environment.
		</comment>
		<comment id='7' author='atapley' date='2018-12-14T13:30:59Z'>
		Update: I am running the 3D Ball environment to test the issue with vector observations. The memory does not appear to be rising in this case, so it must be some issue with the visual observations as you were expecting.
		</comment>
		<comment id='8' author='atapley' date='2018-12-14T18:07:42Z'>
		I will label this as a bug for now. It could be that the textures are not properly deallocated. Maybe calling Resources.UnloadUnusedAssets(); regularly or calling GC would help (it would slow down the simulation but maybe the memory would not become saturated)
		</comment>
		<comment id='9' author='atapley' date='2019-09-16T17:40:01Z'>
		Hi &lt;denchmark-link:https://github.com/atapley&gt;@atapley&lt;/denchmark-link&gt;
,
In the past few months we have made a number of fixes to training to address lingering memory leaks. Please try the latest version of ML-Agents, and let us know if the issue is persisting. (Closing this issue for now)
		</comment>
	</comments>
</bug>