<bug id='926' author='tcmxx' open_date='2018-06-27T14:30:48Z' closed_time='2019-04-03T22:20:18Z'>
	<summary>potential memory leak from TFTensor</summary>
	<description>
I see in CoreBrainInternal, all TFTensors are not disposed by calling Dispose() (including datas feeding to network that are implicitly converted to TFTensor).
I think this might cause memory leak problem, at least according to Tensorflowsharp it is necessary to Dispose it, and previously this has encountered to me.
Also, Texture2D that are not destroyed might also cause memory leak (not very sure.).
	</description>
	<comments>
		<comment id='1' author='tcmxx' date='2018-06-28T19:19:23Z'>
		Hi &lt;denchmark-link:https://github.com/tcmxx&gt;@tcmxx&lt;/denchmark-link&gt;
,
Thanks for pointing this out. We currently have a PR addressing memory issues caused by the  function: &lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/pull/935&gt;#935&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='tcmxx' date='2018-06-29T08:20:59Z'>
		Hello &lt;denchmark-link:https://github.com/awjuliani&gt;@awjuliani&lt;/denchmark-link&gt;
 .
The Texture is just one issue. Other issue is with the TFTensor when using the internal brain.
Seems like TFTensor needs to be disposed after feeding as input and obtained from output. &lt;denchmark-link:https://github.com/migueldeicaza/TensorFlowSharp/issues/137&gt;migueldeicaza/TensorFlowSharp#137&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='tcmxx' date='2018-07-26T12:16:44Z'>
		can you try to dispose the result as well?
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Thu, Jul 26, 2018 at 2:27 PM huangkui ***@***.***&gt; wrote:
 Hi @tcmxx &lt;https://github.com/tcmxx&gt;
 I tried to call dispose every time, but the memory is not released...

  var graph = new TFGraph();
             var x1 = graph.Placeholder(TFDataType.Float,new TFShape(-1,10));
             var x2 = graph.Mul(x1, graph.Const(0.1f));
             var session = new TFSession(graph);
             for(int i=0;i&lt;500000;i++)            {
                 TFTensor tensor = TFTensor.FromBuffer(new TFShape(1, 10), new float[] { 0.03f, 0.03f, 0.03f, 0.03f, 0.03f, 0.03f, 0.03f, 0.03f, 0.03f, 0.03f }, 0, 10);
                  var resutl = session.GetRunner().AddInput(x1, tensor).Fetch(x2).Run() ;
                 var s = resutl[0].GetValue();
                 tensor.Dispose();
             };
             graph.Dispose();
             session.Dispose();

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#926 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AGL4iMHAO3R28pGMq_0aup4ZbvfrnZPPks5uKaedgaJpZM4U506Z&gt;
 .



		</comment>
		<comment id='4' author='tcmxx' date='2018-09-06T21:44:35Z'>
		&lt;denchmark-link:https://github.com/vincentpierre&gt;@vincentpierre&lt;/denchmark-link&gt;
 Does this seems to be the memory leak issue you had before?
		</comment>
		<comment id='5' author='tcmxx' date='2019-04-03T22:20:18Z'>
		Hi all. We have switched to Barracuda for inference. Closing this issue for now. Please feel free to re-open if you deem it necessary.
		</comment>
		<comment id='6' author='tcmxx' date='2020-04-04T08:21:36Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>