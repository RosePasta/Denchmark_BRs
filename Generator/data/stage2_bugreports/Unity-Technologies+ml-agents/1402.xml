<bug id='1402' author='atapley' open_date='2018-11-05T18:44:35Z' closed_time='2019-09-24T20:08:17Z'>
	<summary>Memory Leak on Linux</summary>
	<description>
Hi,
I have been using Unity ml-agents on my Mac to create environments. I then build the environment for Linux and upload it to git in order to pull the environment down on a Linux machine. When I use the environment on the Linux machine, I run the learn.py algorithm, but it continues to use more and more memory until it runs out and it crashes.
I am running on Ubuntu 16.04 with ml-agents 0.4.0b and Unity 2017.2.0f1.
I know this was an issue with textures in the past, but the version of ml-agents that I have is after this issue was fixed.
	</description>
	<comments>
		<comment id='1' author='atapley' date='2018-11-13T19:56:26Z'>
		My ML version 0.5 is also has this problem in Linux. The memory increase, increase ....
		</comment>
		<comment id='2' author='atapley' date='2018-11-15T05:13:54Z'>
		This is a known issue that is on our to fix bug list.
		</comment>
		<comment id='3' author='atapley' date='2018-11-17T19:01:34Z'>
		Resources.UnloadUnusedAssets();
		</comment>
		<comment id='4' author='atapley' date='2018-11-30T08:32:51Z'>
		&lt;denchmark-link:https://github.com/xiaomaogy&gt;@xiaomaogy&lt;/denchmark-link&gt;
  has fix bug on Version 0.6  ?
		</comment>
		<comment id='5' author='atapley' date='2018-12-03T19:10:14Z'>
		&lt;denchmark-link:https://github.com/arixlin&gt;@arixlin&lt;/denchmark-link&gt;
 Not yet.
		</comment>
		<comment id='6' author='atapley' date='2018-12-05T01:09:58Z'>
		&lt;denchmark-link:https://github.com/xiaomaogy&gt;@xiaomaogy&lt;/denchmark-link&gt;
  Thanks!
		</comment>
		<comment id='7' author='atapley' date='2019-05-13T23:10:09Z'>
		any pointers to what the cause of this is ?
		</comment>
		<comment id='8' author='atapley' date='2019-05-13T23:15:11Z'>
		In my opinion it could either be an issue on Unity's end with the visual observations, or an issue with Tensorflow itself. I have seen a few things saying that Tensorflow has a memory leak like this. I don't have the issue with vector observations though, so I'm not sure.
		</comment>
		<comment id='9' author='atapley' date='2019-05-14T07:20:32Z'>
		&lt;denchmark-link:https://github.com/atapley&gt;@atapley&lt;/denchmark-link&gt;
 Thanks for the fast reply. Yes, I suspected TF too, but will need to do some profiling to confirm that. The unity process (executable) itself seems stable, the python side is doing horrible things. Will post updates.
		</comment>
		<comment id='10' author='atapley' date='2019-05-14T09:49:03Z'>
		Using mlagents 0.7 with TF 1.13.1 results with the same memory leak. Could be a TF usage error - not freeing memory when it should be freed from the session?
Onwards...
		</comment>
		<comment id='11' author='atapley' date='2019-05-14T09:56:13Z'>
		Why I say it may be a usage error, is that utilizing rainbow (dopamine) with the same environment does not result in the memory leak. So it can't be TF alone - well it could, but not likely ?
		</comment>
		<comment id='12' author='atapley' date='2019-05-14T10:34:39Z'>
		&lt;denchmark-link:https://github.com/xiaomaogy&gt;@xiaomaogy&lt;/denchmark-link&gt;
 let me in on any knowns - perhaps including where this bug is being managed please-, would like to keep eye on progress and provide input.
		</comment>
		<comment id='13' author='atapley' date='2019-05-14T13:52:06Z'>
		From what I can tell, it's got to do with the trainer's accumulation of experiences. The trainer is the only thing that I see that is holding onto the memory - I haven't located its limit yet, or where it maintains the amount of memory it uses.
It doesn't matter what dimensions the observations are, that just determines how quickly you'll run out of memory.
		</comment>
		<comment id='14' author='atapley' date='2019-05-14T14:38:25Z'>
		Located the problem. Will see if I can get to the bottom of this and submit a PR.
		</comment>
		<comment id='15' author='atapley' date='2019-05-15T18:37:14Z'>
		This bug is mainly being tracked on this issue. We also have a internal Trello board, which just links to this issue.
		</comment>
		<comment id='16' author='atapley' date='2019-05-25T18:51:46Z'>
		&lt;denchmark-link:https://github.com/xiaomaogy&gt;@xiaomaogy&lt;/denchmark-link&gt;
 It's not the most desirable fix, but it does the job.
Ideally the training and eval would be completely decoupled, but that's a larger task than have time for at present.
This PR ultimately stops the buffer from being filled at all during evaluation, hence no leak. I believe this is desirable for efficiency.
Alternatively control flow could have been added as to clear the buffer during eval after it fills, would be fruitless though.
		</comment>
		<comment id='17' author='atapley' date='2019-05-25T18:55:57Z'>
		Ah, ignore the first commit (&lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/commit/be5e0ea8a18090325a29d3e851f850f4a91bfcec&gt;be5e0ea&lt;/denchmark-link&gt;
). I split some coupled changes, the patch is in &lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/commit/6fc56f7e73c5ff6a89fa36f479a857921d126ec1&gt;6fc56f7&lt;/denchmark-link&gt;

		</comment>
		<comment id='18' author='atapley' date='2019-05-29T00:26:59Z'>
		Hence no leak during the evaluation. But the training will still have memory leak, am I correct? &lt;denchmark-link:https://github.com/tjad&gt;@tjad&lt;/denchmark-link&gt;

		</comment>
		<comment id='19' author='atapley' date='2019-08-08T22:17:21Z'>
		The training wont have a leak - as far as I remember, I'll confirm that. The way in which the training works clears the buffer ,but during eval it wasn't training, and hence not clearing the buffer.
		</comment>
		<comment id='20' author='atapley' date='2019-08-08T22:20:33Z'>
		Probably why it's not been such a major issue :-)
		</comment>
		<comment id='21' author='atapley' date='2019-08-08T22:24:59Z'>
		I suspected when I looked into this issue that there may have been lack of direction where the actual memory leak was. I trained fine for hours on end. Evaluation wasn't happening as easily.
		</comment>
		<comment id='22' author='atapley' date='2019-09-24T20:08:17Z'>
		It looks like this issue has been solved, so I'm going to close it.  Thanks, and feel free to reopen if needed.
		</comment>
	</comments>
</bug>