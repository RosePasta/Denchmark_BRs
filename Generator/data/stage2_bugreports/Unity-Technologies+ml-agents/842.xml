<bug id='842' author='m-ronchi' open_date='2018-06-12T17:03:28Z' closed_time='2018-12-17T23:35:19Z'>
	<summary>Python exception when agents are done on first action</summary>
	<description>
code:
&lt;denchmark-code&gt;public class BugAcademy : Academy {

    public BugAgent[] agents;

    public int turn;

    public override void InitializeAcademy ()
    {
        base.InitializeAcademy ();
        turn = Random.Range (0, agents.Length);
    }
}

public class BugAgent : Agent {

    public BugAcademy academy;

    private void FixedUpdate ()
    {
        if (academy.turn == System.Array.IndexOf (academy.agents, this)) {
            RequestDecision ();
            academy.turn = -1;
        }
    }

    public override void CollectObservations ()
    {
        AddVectorObs (0);
    }

    public override void AgentAction (float[] vectorAction, string textAction)
    {
        foreach (var a in academy.agents) a.Done ();
        academy.turn = Random.Range (0, academy.agents.Length);
    }
}
&lt;/denchmark-code&gt;

scene setup:

Academy (BugAcademy, has reference to all the agents)

BugBrain (Brain, external, observation space = 1)


Agent 1 (BugAgent, brain set to BugBrain)
Agent 2 (BugAgent, brain set to BugBrain)

environment setup:

python with docker
connects to editor

expected result: training session runs to completion
actual result: after a couple hundred actions, python aborts with exception and editor stops playing
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "python/learn.py", line 81, in &lt;module&gt;
    tc.start_learning()
  File "/execute/python/unitytrainers/trainer_controller.py", line 259, in start_learning
    trainer.process_experiences(curr_info, new_info)
  File "/execute/python/unitytrainers/ppo/trainer.py", line 368, in process_experiences
    self.stats['episode_length'].append(self.episode_steps[agent_id])
KeyError: -718928
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='m-ronchi' date='2018-06-14T22:10:38Z'>
		&lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/pull/849&gt;#849&lt;/denchmark-link&gt;
 should have hidden the error, but your agent will not be able to learn at all if it is set to done right after it resets. Reinforcement Learning uses the current state of the agent as well as the state following the action taken in order to learn. If the agent resets at every step, it will not learn at all.
		</comment>
		<comment id='2' author='m-ronchi' date='2018-06-15T09:30:36Z'>
		Hi,
my training setup is as follows: when an agent makes an action (discrete), I validate that action: there are 40 possible actions, of which only 3 are valid in a given game state.
the example I posted was a simplification of what happens when the first action the agent does is invalid, and in the early stages (when agents play randomly) there is a 37/40 chance that it is
if the agent makes an invalid action, what should I do?

punish him (negative reward), then abort the game (reset/done all agents)
punish him, make a random valid action, continue the game
right now, I abort the game after 3 consecutive invalid actions (3 is the lower number that doesn't cause the exception)
there is (or will be) some way to filter invalid actions inside the NN

also, I am calling Done() as soon as the game is ended, does it mean that the agent is not learning from the last move made? should I call an additional RequestDecision() and only reset after that?
		</comment>
		<comment id='3' author='m-ronchi' date='2018-11-29T00:37:16Z'>
		Hi &lt;denchmark-link:https://github.com/m-ronchi&gt;@m-ronchi&lt;/denchmark-link&gt;


You could use Action Masking.
The fact that 3 is the lowest number of actions the network needs seems odd to me. What are the hyperparameters you are using ?
The agent will learn from the last move if you call done when the game ends as long as the episode contains more that one experience point.

I hope this helps.
		</comment>
		<comment id='4' author='m-ronchi' date='2018-11-29T10:08:18Z'>
		Hi,
I stopped working with ml-agents currently, and have instead taken a Montecarlo approach for my games (with c# jobs), due to the limitations of it at the time (i.e. no action masking and broken tensorflow with IL2CPP).
I may eventually retry implementing ml-agents in the future (by the way, do you plan to make it compatible with ECS/c# job system? currently is class-based)
for your second point, I think I was using the default hyperparameters at the time.
I have this leftover in training-config.yaml, but I don't remember if I set them before or after this issue:
&lt;denchmark-code&gt;default:
    trainer: ppo
    batch_size: 1024
    beta: 5.0e-3
    buffer_size: 10240
    epsilon: 0.2
    gamma: 0.99
    hidden_units: 128
    lambd: 0.95
    learning_rate: 3.0e-4
    max_steps: 5.0e4
    memory_size: 256
    normalize: false
    num_epoch: 3
    num_layers: 2
    time_horizon: 64
    sequence_length: 64
    summary_freq: 1000
    use_recurrent: false
    use_curiosity: false
    curiosity_strength: 0.01
    curiosity_enc_size: 128

MyBrain:
    hidden_units: 256
    max_steps: 5.0e5
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='m-ronchi' date='2018-11-29T17:44:09Z'>
		Ok, thank you for your answer. I was hoping our implementation of action masking would solve the problem. Our inference solution with TensorFlowSharp is still very experimental and there are a lot of limitations with it like the one you pointed out.
		</comment>
		<comment id='6' author='m-ronchi' date='2020-01-02T22:43:39Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>