<bug id='2312' author='Junggy' open_date='2019-07-22T20:58:04Z' closed_time='2019-10-01T18:05:06Z'>
	<summary>Issue with barracuda with some layers.</summary>
	<description>
Hello,
Currently I am doing visit related project with ML agents with extra layers.
But  while I am trying to convert my model into *.nn file,
I found issues with barracuda.py for certain layers.


While converting tf.layers.batch_normalization(), I have error.
I found out that when function get_tensor_data() is called from process_layer(), batch_norms's  "bn/Const" is rather empty (when printed, it prints b'') so that one of variable in the function (called data) is not created. Thus it ends up with complaining that local variable 'data' is not found.


So, I removed all the batch norm layer and tried once more. Then I have assertion error
Then I faced assertion error from cleanup_layers(),


&lt;denchmark-code&gt;def cleanup_layers(layers):
        all_layers = {l.name for l in layers}
        all_inputs = {i for l in layers for i in l.inputs}

        def is_unconnected_identity(layer):
            if layer.class_name == 'Activation' and layer.activation == 0: # Identity
                assert(len(layer.inputs) == 1)
                if layer.inputs[0] not in all_layers and layer.name not in all_inputs:
                    return True;
            return False;
&lt;/denchmark-code&gt;

I want to know what caused this problem.

Anyways, cleanup_layers seems just for optimization. So, I commented all of them and re-run the code. Then I have same local variable 'data' is not found error. Now it is coming from "crop_to_bounding_box/assert_positive/assert_less/Assert/Assert/data_0".

I think it generally has problem when data inside the tensor is empty(?), especially when layer contains multiple things (i.e. Const, Assert etc).
Are they going to be fixed soon?
Or should I just use tensorflow sharp plug in from previous ml-agent ?
(at least frozen_graph seems to be generated properly without error.. so I'd assume it would work)
	</description>
	<comments>
		<comment id='1' author='Junggy' date='2019-07-23T14:04:49Z'>
		I am also waiting out on the answer to the last question. Not sure what &lt;denchmark-link:https://github.com/mantasp&gt;@mantasp&lt;/denchmark-link&gt;
  and unity think about it. I need to get a prototype working using an externally trained tensorflow graph soon.
``
from  import print_function, division
import scipy
import tensorflow as tf
import keras
from keras.layers import Input, Dense, Flatten
from keras.models import Model
from keras.optimizers import Adam
from keras.engine import InputSpec
from keras.layers import Conv2D, UpSampling2D
import datetime
import sys
import os
def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):
graph = session.graph
with graph.as_default():
freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))
output_names = output_names or []
output_names += [v.op.name for v in tf.global_variables()]
input_graph_def = graph.as_graph_def()
if clear_devices:
for node in input_graph_def.node:
node.device = ''
frozen_graph = tf.graph_util.convert_variables_to_constants(
session, input_graph_def, output_names, freeze_var_names)
return frozen_graph
keras.backend.set_learning_phase(0)
_input = Input(shape=(32,32,1))
d1 = Conv2D(16, kernel_size=4, strides=2, padding='same')(_input)
d2 = Conv2D(16, kernel_size=4, strides=2, padding='same')(d1)
d3 = Conv2D(16, kernel_size=4, strides=2, padding='same')(d2)
d3 = UpSampling2D(size=(2,2))(d3)
flat = Flatten()(d3)
_output = Dense(32)(flat)
model = Model(_input, _output)
model.compile(loss=['mae'], loss_weights=[100], optimizer=Adam(0.0002, 0.5))
print(model.summary())
with keras.backend.get_session() as sess:
graph = sess.graph
with graph.as_default():
graphdef_inf = tf.graph_util.remove_training_nodes(graph.as_graph_def())
graphdef_frozen = tf.graph_util.convert_variables_to_constants(sess, graphdef_inf, [out.op.name for out in model.outputs])
tf.train.write_graph(graphdef_frozen, './dummy_model', 'conv_graph.pb', as_text=False)
`
Graphs trained using that code get converted without any error. I do not understand why though.``
		</comment>
		<comment id='2' author='Junggy' date='2019-08-27T21:40:10Z'>
		&lt;denchmark-link:https://github.com/mantasp&gt;@mantasp&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='Junggy' date='2019-08-30T15:30:18Z'>
		&lt;denchmark-link:https://github.com/Junggy&gt;@Junggy&lt;/denchmark-link&gt;
 I added your report to our issue tracking system.
&lt;denchmark-link:https://github.com/AkhilRaja&gt;@AkhilRaja&lt;/denchmark-link&gt;
 I don't think your issue is related to BatchNormalization import, could you please make separate report?
		</comment>
		<comment id='4' author='Junggy' date='2019-10-01T18:05:06Z'>
		Thank you for the discussion. We are closing this issue due to inactivity. Feel free to reopen it if youâ€™d like to continue the discussion.
		</comment>
	</comments>
</bug>