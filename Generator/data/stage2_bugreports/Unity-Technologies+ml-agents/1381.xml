<bug id='1381' author='bestpx' open_date='2018-10-27T06:34:06Z' closed_time='2018-11-19T20:31:47Z'>
	<summary>AgentAction not respecting mask during a long run</summary>
	<description>
Hi &lt;denchmark-link:https://github.com/awjuliani&gt;@awjuliani&lt;/denchmark-link&gt;
, after applying your suggestion, result got a lot better but I found that the agent tries to take actions that are masked out sometimes. This happens when using external brain during a long training session - doesn't happen until many steps into the session and seem to happen sooner if there are more agents running simultaneously.
Here are screenshots from 2 long runs I got today. Got same result many times on my PC as well. This test run has only 1 agent running at timescale 100.
&lt;denchmark-link:https://user-images.githubusercontent.com/3673086/47600428-c7208b80-d975-11e8-825f-da5f2c9fb5ef.png&gt;&lt;/denchmark-link&gt;

Here the  passed in is 
&lt;denchmark-link:https://user-images.githubusercontent.com/3673086/47600429-c7208b80-d975-11e8-82f9-8532ff43b0bd.png&gt;&lt;/denchmark-link&gt;

Which was masked
&lt;denchmark-link:https://user-images.githubusercontent.com/3673086/47600430-c7208b80-d975-11e8-9640-0e3d13c412c4.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/3673086/47600431-c7208b80-d975-11e8-8ff4-2b823a6e2994.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/3673086/47600432-c7208b80-d975-11e8-95bc-4e5e4544f3f1.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/3673086/47600433-c7208b80-d975-11e8-9f4f-60adc76bb930.png&gt;&lt;/denchmark-link&gt;

The agent code is here: &lt;denchmark-link:https://github.com/bestpx/YahtzeeUnityTrainer/blob/master/UnitySDK/Assets/Yahtzee/Game/MLAgent/YahtzeePlayerAgent.cs&gt;https://github.com/bestpx/YahtzeeUnityTrainer/blob/master/UnitySDK/Assets/Yahtzee/Game/MLAgent/YahtzeePlayerAgent.cs&lt;/denchmark-link&gt;

It's always running smoothly until the training session gets long... In order to test this I made a simple test which has the identical setup of my training environment except it doesn't execute any game logic: &lt;denchmark-link:https://github.com/bestpx/YahtzeeUnityTrainer/blob/master/UnitySDK/Assets/Test/TestAgent.cs&gt;https://github.com/bestpx/YahtzeeUnityTrainer/blob/master/UnitySDK/Assets/Test/TestAgent.cs&lt;/denchmark-link&gt;

but this  is getting "No episode was completed since last summary" message from step 2000. So maybe there is something I didn't setup properly in my environment?
	</description>
	<comments>
		<comment id='1' author='bestpx' date='2018-11-02T19:11:29Z'>
		&lt;denchmark-link:https://github.com/vincentpierre&gt;@vincentpierre&lt;/denchmark-link&gt;
 this seems to be your area? A test for action masking.
		</comment>
		<comment id='2' author='bestpx' date='2018-11-02T21:00:58Z'>
		Hi,
During training, the masked action's probabilities are not set to 0 but to an extremely small value. It is still possible this action is picked at random.  We set the masked probabilities to 10^-10 in &lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/blob/master/ml-agents/mlagents/trainers/models.py#L156&gt;here&lt;/denchmark-link&gt;
 and then renormalize. It could be that the exploration of your algorithm is too low so the agent is ultra confident the masked action is the right one (the masked action is set to 10^-10 but all other actions are even smaller than this.
Does the agent know the action is impossible? If the agent does not have that information and the masked action is usually the right one, the agent try to pick it.
		</comment>
		<comment id='3' author='bestpx' date='2018-11-02T21:57:12Z'>
		HI &lt;denchmark-link:https://github.com/vincentpierre&gt;@vincentpierre&lt;/denchmark-link&gt;
, thank you for the information!
I'm using this to train a board game and the mask is used to define game rules - invalid actions at certain game states. Can I set this value to 0 and expect external brain to never pick it up? Or is there another reliable way to absolutely enforce game rules?
If I ignore this issue and continue to train my agent, it gets better over time but still not good enough - it tries to do invalid actions sometimes and will score very low when they do so.
Also can you explain why the test script doesn't generate new episodes? It has the same setup - discrete action space, on demand decision making and ResetOnDone for agents.
		</comment>
		<comment id='4' author='bestpx' date='2018-11-02T22:01:57Z'>
		Using masks to define rules is tricky because the agent does not know when there is a mask or not. You need to give your agents the rules in some form. Give the agent a timer if there is a cooldown to some actions. For the test script, I suppose your agents do not terminate episodes at all after step 2000. This could be because Done is never set or because the agent have been destroyed an are no longer present in the scene.
		</comment>
		<comment id='5' author='bestpx' date='2018-11-02T22:21:23Z'>
		The problem is that I don't have control over what decision can possibly be made by the brain if I don't mask actions. Yes my agent can simply not execute the action that came in from external brain if the game state doesn't allow it (indeed my code already does that). Again I think what I need is a way to tell the brain that only a subset of action space is available at certain game state. For example if I want to make an agent to play Go, there needs to be a way to make sure we are not trying to play a move on a coordinate that is already occupied.
Do you mean Agent.Done() or Academy.Done()? Agent.Done() is for sure called many many times with ResetOnDone turned on.
		</comment>
		<comment id='6' author='bestpx' date='2018-11-15T22:10:58Z'>
		Let me try to rephrase "You need to give your agents the rules in some form." I meant giving the mask as observation to your agent so that it "knows" what not to try. Action Masking does exactly what you are describing in your previous comment, you are just facing a bug that I think is due to the lack of knowledge of the agent on what it can/cannot do. If given the same observations an action can either be masked or not, the agent will have no way to tell. I am still unable to reproduce the bug given the code you sent, I think it will be very hard given the amount of time needed to run the experiment.
For your test agent, you forgot to reset the action count to 0 after reset line 77.
Here is the modified TestAgent:
using System;
using System.Collections.Generic;
using CommonUtil;
using MLAgents;
using UnityEngine;
using Yahtzee.Game.Client;
using Yahtzee.Game.Client.GameActions;
using ILogger = CommonUtil.ILogger;
using Random = System.Random;

namespace Test
{
    public class TestAgent : Agent
    {
        public float timeBetweenDecisionsAtInference = 0.1f;
        private float timeSinceDecision;

        private TestAcademy _academy;
        private int _actionCount;
        private static int _gameCount;

        private List&lt;GameAction&gt; _actionTable;

        private ILogger Logger
        {
            get { return ServiceFactory.GetService&lt;ILogger&gt;(); }
        }
            
        public override void InitializeAgent()
        {
            Logger.Log(LogLevel.Info, "Initialize Agent");
            _academy = FindObjectOfType&lt;TestAcademy&gt;();

            _actionTable = new List&lt;GameAction&gt;();
            for (int i = 0; i &lt; 45; i++)
            {
                _actionTable.Add(new GameActionRoll());
            }
        }
    
        public override void CollectObservations()
        {
            // dummy
            for (int i = 0; i &lt; 70; i++)
            {
                AddVectorObs(1);
            }
            
            // mask actions
            _mask = new List&lt;int&gt;();
            // mask some random actions
            for (int i = 0; i &lt; _actionTable.Count; i++)
            {
                if (UnityEngine.Random.value &gt; 0.5f)
                {
                    _mask.Add(i);
                }
            }

            _mask[0] = 0;

            SetActionMask(0, _mask);
        }
        
        private List&lt;int&gt; _mask;
    
        public override void AgentAction(float[] vectorAction, string textAction)
        {
            int actionIndex = Mathf.FloorToInt(vectorAction[0]);
            var gameAction = _actionTable[actionIndex];
            if (brain.brainParameters.vectorActionSpaceType == SpaceType.discrete)
            {
                // ignore invalid actions
                if (_mask.Contains(actionIndex))
                {
                    Debug.Log("Invalid Game action attempted! " + gameAction.GetType());
                    SetReward(-20f);
                }
                else
                {
                    SetReward(0.1f);
                }
            }
            
            // pretend GameAction was taken
            _actionCount++;
            
            if (_actionCount &gt; 78) // gameover
            {
                EndTraining();
                _actionCount = 0;
            }
        }

        private void EndTraining()
        {
            Done();
        }
    
        public override void AgentReset()
        {
            
        }
        
    }
}
I unchecked On Demand Decision for simplicity
		</comment>
		<comment id='7' author='bestpx' date='2018-11-15T23:29:46Z'>
		I am making experiments but have not yet seen the error appear. I made a modified version of the training algorithm that should enforce that a masked action is never picked (instead of improbable) but it could lead to instabilities in training that are hard to test for. If you want to try it out, you can replace &lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/blob/master/ml-agents/mlagents/trainers/models.py&gt;https://github.com/Unity-Technologies/ml-agents/blob/master/ml-agents/mlagents/trainers/models.py&lt;/denchmark-link&gt;
 line 144 to 162 with
    @staticmethod
    def create_discrete_action_masking_layer(all_logits, action_masks, action_size):
        """
        Creates a masking layer for the discrete actions
        :param all_logits: The concatenated unnormalized action probabilities for all branches
        :param action_masks: The mask for the logits. Must be of dimension [None x total_number_of_action]
        :param action_size: A list containing the number of possible actions for each branch
        :return: The action output dimension [batch_size, num_branches] and the concatenated normalized logits
        """
        action_idx = [0] + list(np.cumsum(action_size))
        branches_logits = [all_logits[:, action_idx[i]:action_idx[i + 1]] for i in range(len(action_size))]
        branch_masks = [action_masks[:, action_idx[i]:action_idx[i + 1]] for i in range(len(action_size))]
        raw_probs = [tf.multiply(tf.nn.softmax(branches_logits[k]), branch_masks[k]) #different line here
                     for k in range(len(action_size))]
        normalized_probs = [
            tf.divide(raw_probs[k], tf.reduce_sum(raw_probs[k], axis=1, keepdims=True)) #different line here
                            for k in range(len(action_size))]
        output = tf.concat([tf.multinomial(tf.log(normalized_probs[k]), 1) for k in range(len(action_size))], axis=1)
        return output, tf.concat([tf.log(normalized_probs[k] + 1.0e-10) for k in range(len(action_size))], axis=1) #different line here
		</comment>
		<comment id='8' author='bestpx' date='2018-11-16T01:40:31Z'>
		Hi &lt;denchmark-link:https://github.com/vincentpierre&gt;@vincentpierre&lt;/denchmark-link&gt;
 thank you for getting back to me. I can't reproduce the issue on the TestAgent either but I'm not surprised. From your previous comment, the agent would only try to choose a masked action if all the available ones would not yield good reward based on previous training/observation.
I can consistently repro on the YahzeeAgent, the following log is from running 8 agents simultaneously. It could be that my reward function or my training environment is not optimal. I'll try your script and see what happens.
Unity log:
&lt;denchmark-code&gt;Invalid Game action attempted! Yahtzee.Game.Client.GameActions.GameActionRoll
UnityEngine.Debug:LogError(Object)
CommonUtil.Logger:Log(LogLevel, String) (at Assets/CommonUtil/Logger.cs:36)
Yahtzee.Game.MLAgent.YahtzeePlayerAgent:AgentAction(Single[], String) (at Assets/Yahtzee/Game/MLAgent/YahtzeePlayerAgent.cs:161)
MLAgents.Agent:AgentStep() (at Assets/ML-Agents/Scripts/Agent.cs:965)
MLAgents.Academy:EnvironmentStep() (at Assets/ML-Agents/Scripts/Academy.cs:588)
MLAgents.Academy:FixedUpdate() (at Assets/ML-Agents/Scripts/Academy.cs:610)
&lt;/denchmark-code&gt;

Console:
&lt;denchmark-code&gt;INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 1000. Mean Reward: 1.122. Std of Reward: 0.401. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 2000. Mean Reward: 1.091. Std of Reward: 0.410. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 3000. Mean Reward: 1.142. Std of Reward: 0.436. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 4000. Mean Reward: 1.171. Std of Reward: 0.439. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 5000. Mean Reward: 1.083. Std of Reward: 0.398. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 6000. Mean Reward: 1.102. Std of Reward: 0.406. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 7000. Mean Reward: 1.113. Std of Reward: 0.379. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 8000. Mean Reward: 1.108. Std of Reward: 0.384. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 9000. Mean Reward: 1.115. Std of Reward: 0.393. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 10000. Mean Reward: 1.153. Std of Reward: 0.416. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 11000. Mean Reward: 1.135. Std of Reward: 0.422. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 12000. Mean Reward: 1.121. Std of Reward: 0.371. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 13000. Mean Reward: 1.141. Std of Reward: 0.406. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 14000. Mean Reward: 1.076. Std of Reward: 0.372. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 15000. Mean Reward: 1.143. Std of Reward: 0.442. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 16000. Mean Reward: 1.132. Std of Reward: 0.409. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 17000. Mean Reward: 1.151. Std of Reward: 0.407. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 18000. Mean Reward: 1.134. Std of Reward: 0.415. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 19000. Mean Reward: 1.127. Std of Reward: 0.391. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 20000. Mean Reward: 1.133. Std of Reward: 0.401. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 21000. Mean Reward: 1.194. Std of Reward: 0.422. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 22000. Mean Reward: 1.203. Std of Reward: 0.461. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 23000. Mean Reward: 1.236. Std of Reward: 0.387. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 24000. Mean Reward: 1.236. Std of Reward: 0.394. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 25000. Mean Reward: 1.239. Std of Reward: 0.393. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 26000. Mean Reward: 1.339. Std of Reward: 0.450. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 27000. Mean Reward: 1.326. Std of Reward: 0.490. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 28000. Mean Reward: 1.343. Std of Reward: 0.414. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 29000. Mean Reward: 1.416. Std of Reward: 0.447. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 30000. Mean Reward: 1.434. Std of Reward: 0.427. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 31000. Mean Reward: 1.526. Std of Reward: 0.457. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 32000. Mean Reward: 1.498. Std of Reward: 0.421. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 33000. Mean Reward: 1.522. Std of Reward: 0.439. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 34000. Mean Reward: 1.527. Std of Reward: 0.369. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 35000. Mean Reward: 1.528. Std of Reward: 0.443. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 36000. Mean Reward: 1.639. Std of Reward: 0.458. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 37000. Mean Reward: 1.597. Std of Reward: 0.401. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 38000. Mean Reward: 1.711. Std of Reward: 0.443. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 39000. Mean Reward: 1.599. Std of Reward: 0.442. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 40000. Mean Reward: 1.754. Std of Reward: 0.472. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 41000. Mean Reward: 1.698. Std of Reward: 0.429. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 42000. Mean Reward: 1.712. Std of Reward: 0.434. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 43000. Mean Reward: 1.743. Std of Reward: 0.449. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 44000. Mean Reward: 1.773. Std of Reward: 0.421. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 45000. Mean Reward: 1.782. Std of Reward: 0.440. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 46000. Mean Reward: 1.772. Std of Reward: 0.454. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 47000. Mean Reward: 1.844. Std of Reward: 0.453. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 48000. Mean Reward: 1.831. Std of Reward: 0.417. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 49000. Mean Reward: 1.834. Std of Reward: 0.438. Training.
INFO:mlagents.envs:Saved Model
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 50000. Mean Reward: 1.826. Std of Reward: 0.428. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 51000. Mean Reward: 1.835. Std of Reward: 0.436. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 52000. Mean Reward: 1.845. Std of Reward: 0.396. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 53000. Mean Reward: 1.851. Std of Reward: 0.413. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 54000. Mean Reward: 1.931. Std of Reward: 0.420. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 55000. Mean Reward: 1.863. Std of Reward: 0.414. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 56000. Mean Reward: 1.935. Std of Reward: 0.419. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 57000. Mean Reward: 1.932. Std of Reward: 0.446. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 58000. Mean Reward: 1.985. Std of Reward: 0.485. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 59000. Mean Reward: 1.977. Std of Reward: 0.431. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 60000. Mean Reward: 1.889. Std of Reward: 0.431. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 61000. Mean Reward: 1.986. Std of Reward: 0.471. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 62000. Mean Reward: 2.006. Std of Reward: 0.486. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 63000. Mean Reward: 1.991. Std of Reward: 0.500. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 64000. Mean Reward: 1.980. Std of Reward: 0.460. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 65000. Mean Reward: 2.040. Std of Reward: 0.457. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 66000. Mean Reward: 1.939. Std of Reward: 0.475. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 67000. Mean Reward: 2.017. Std of Reward: 0.437. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 68000. Mean Reward: 2.060. Std of Reward: 0.438. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 69000. Mean Reward: 2.024. Std of Reward: 0.433. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 70000. Mean Reward: 2.110. Std of Reward: 0.451. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 71000. Mean Reward: 1.989. Std of Reward: 0.435. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 72000. Mean Reward: 2.093. Std of Reward: 0.488. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 73000. Mean Reward: 2.110. Std of Reward: 0.466. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 74000. Mean Reward: 2.158. Std of Reward: 0.500. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 75000. Mean Reward: 2.057. Std of Reward: 0.443. Training.
INFO:mlagents.trainers: test-yahtzee-0: YahtzeeBrain: Step: 76000. Mean Reward: 2.100. Std of Reward: 0.440. Training.
^C--------------------------Now saving model-------------------------
INFO:mlagents.envs:Learning was interrupted. Please wait while the graph is generated.
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/popen_fork.py", line 28, in poll
    pid, sts = os.waitpid(self.pid, flag)
KeyboardInterrupt
INFO:mlagents.envs:Saved Model
INFO:mlagents.envs:List of nodes to export :
INFO:mlagents.envs:	action
INFO:mlagents.envs:	value_estimate
INFO:mlagents.envs:	action_probs
INFO:mlagents.envs:	value_estimate
Scopely-10654:yahtzee-ml-agents dennispan$ INFO:tensorflow:Restoring parameters from ./models/test-yahtzee-0/model-76629.cptk
INFO:tensorflow:Froze 7 variables.
Converted 7 variables to const ops.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='bestpx' date='2018-11-16T01:44:18Z'>
		I also tried to add the mask to the observations :
            //Added to test if the mask can be learned
            float[] maskObs = new float[_actionTable.Count];
            for (var maskIndex = 0; maskIndex &lt; _mask.Count; maskIndex++)
            {
                maskObs[maskIndex] = 1f;
            }
            AddVectorObs(maskObs);
            // End of code added to test if the mask can be learned
I still have not encountered the bug so far and I am at 900k steps.
		</comment>
		<comment id='10' author='bestpx' date='2018-11-16T02:50:43Z'>
		Did you test this in isolation from your python code change? I just tried it - without python change and am still getting the problem at 100k steps
		</comment>
		<comment id='11' author='bestpx' date='2018-11-16T05:56:31Z'>
		I'm still getting this issue with both of your suggestions applied. Running with more agents seems to make it happen faster.
INFO:mlagents.trainers: test-observe-mask-and-python-0: YahtzeeBrain: Step: 179000. Mean Reward: 2.660. Std of Reward: 0.515. Training.
&lt;denchmark-link:https://user-images.githubusercontent.com/3673086/48599732-ddd75400-e91d-11e8-9c05-32dca1c0be8b.png&gt;&lt;/denchmark-link&gt;

I've uploaded everything I have here: &lt;denchmark-link:https://github.com/bestpx/yahtzee-ml-agents&gt;https://github.com/bestpx/yahtzee-ml-agents&lt;/denchmark-link&gt;

Notice it's a new repo since my previous repo wasn't forked properly.
		</comment>
		<comment id='12' author='bestpx' date='2018-11-16T17:27:45Z'>
		I have been using &lt;denchmark-link:https://github.com/bestpx/YahtzeeUnityTrainer/&gt;https://github.com/bestpx/YahtzeeUnityTrainer/&lt;/denchmark-link&gt;
 and I can only reproduce the bug with your configuration after 700k steps. When I feed the mask as observation, I don't have the bug (I am over 1M steps. When I modify the python code, I don't have the bug anymore, I get to another error due to instabilities in the network around (again) 700k steps:
&lt;denchmark-code&gt;INFO:mlagents.trainers: vanilla_2-0: YahtzeeBrain: Step: 690000. Mean Reward: 3.243. Std of Reward: 0.522. Training.
INFO:mlagents.trainers: vanilla_2-0: YahtzeeBrain: Step: 691000. Mean Reward: 3.243. Std of Reward: 0.538. Training.
INFO:mlagents.trainers: vanilla_2-0: YahtzeeBrain: Step: 692000. Mean Reward: 3.332. Std of Reward: 0.572. Training.
INFO:mlagents.trainers: vanilla_2-0: YahtzeeBrain: Step: 693000. Mean Reward: 3.211. Std of Reward: 0.465. Training.
INFO:mlagents.trainers: vanilla_2-0: YahtzeeBrain: Step: 694000. No episode was completed since last summary. Training.
INFO:mlagents.trainers: vanilla_2-0: YahtzeeBrain: Step: 695000. No episode was completed since last summary. Training.
INFO:mlagents.trainers: vanilla_2-0: YahtzeeBrain: Step: 696000. No episode was completed since last summary. Training.
INFO:mlagents.trainers: vanilla_2-0: YahtzeeBrain: Step: 697000. No episode was completed since last summary. Training.
INFO:mlagents.trainers: vanilla_2-0: YahtzeeBrain: Step: 698000. No episode was completed since last summary. Training.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='13' author='bestpx' date='2018-11-16T17:42:29Z'>
		Side note : I just realized that changing the code in the trainer will not take effect in your setup because of the way pip works. If you want the trainer code changes to take effect, you will need to do pip install -e .. The -e means that the local changes will be used.
		</comment>
		<comment id='14' author='bestpx' date='2018-11-16T22:18:48Z'>
		Update: The previous suggestion I made for the modification of the trainer code leads to instabilities according to my experiments. I think the solution bellow would work better :
def create_discrete_action_masking_layer(all_logits, action_masks, action_size):
    """
    Creates a masking layer for the discrete actions
    :param all_logits: The concatenated unnormalized action probabilities for all branches
    :param action_masks: The mask for the logits. Must be of dimension [None x total_number_of_action]
    :param action_size: A list containing the number of possible actions for each branch
    :return: The action output dimension [batch_size, num_branches] and the concatenated normalized logits
    """
    action_idx = [0] + list(np.cumsum(action_size))
    branches_logits = [all_logits[:, action_idx[i]:action_idx[i + 1]] for i in range(len(action_size))]
    branch_masks = [action_masks[:, action_idx[i]:action_idx[i + 1]] for i in range(len(action_size))]
    raw_probs = [tf.multiply(tf.nn.softmax(branches_logits[k]) + 1.0e-10, branch_masks[k]) 
                 for k in range(len(action_size))]
    normalized_probs = [
        tf.divide(raw_probs[k], tf.reduce_sum(raw_probs[k], axis=1, keepdims=True))
                        for k in range(len(action_size))]
    output = tf.concat([tf.multinomial(tf.log(normalized_probs[k]), 1) for k in range(len(action_size))], axis=1)
    return output, tf.concat([tf.log(normalized_probs[k] + 1.0e-10) for k in range(len(action_size))], axis=1)
I tested it for instabilities and it seems robust. If you want to try it out, don't forget to re-run the pip install command.
I am unable to train using  &lt;denchmark-link:https://github.com/bestpx/yahtzee-ml-agents&gt;https://github.com/bestpx/yahtzee-ml-agents&lt;/denchmark-link&gt;
 but I get to 3.5 reward on &lt;denchmark-link:https://github.com/bestpx/YahtzeeUnityTrainer&gt;https://github.com/bestpx/YahtzeeUnityTrainer&lt;/denchmark-link&gt;
 I am currently at 1M step and no invalid moves or collapse of the network so far.
I am facing another issue in which the log probabilities for the actions generated by the neural network grow too small or too large which can cause other issues, but I think it should not pose problem in your scenario.
		</comment>
		<comment id='15' author='bestpx' date='2018-11-19T18:26:38Z'>
		This fix works for me. I was able to train the agent for 10+ hours without picking masked action or "no episode completed". You can feel free to close this one.
Originally I was suspecting this issue lead to the suboptimal training result(~3.5). For reference, good human players should average 4.0 - 4.5. Can you work with me on trouble shooting the training?
		</comment>
		<comment id='16' author='bestpx' date='2018-11-19T20:31:46Z'>
		When I trained the agent, I got it around 3.9. It is possible your agent does not have enough information to solve the problem or that some parts of the state space are harder to explore. I think you should open a new issue as I will close this one.
		</comment>
		<comment id='17' author='bestpx' date='2020-01-03T00:51:02Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>