<bug id='2459' author='ChaiKnight' open_date='2019-08-22T13:08:53Z' closed_time='2019-08-28T10:54:37Z'>
	<summary>Simple CNN converted with Barracuda returns constant values with RenderTexture as Tensor</summary>
	<description>
Describe the bug
I exported a simple tensorflow-based CNN model and converted the .pb to a .nn-file. Loading it up as a model and feeding it a RenderTexture as Tensor (both are channel-last), I receive my 100 values as constant near-0 floats. Somewhere, the input loses its influence and I can't figure out where.

Use the example code in the Barracuda.md to feed a RenderTexture to a compute worker. Use a simple CNN (mine is 3 blocks of Conv2D and MaxPool and 2 Dense layers). I trained on CIFAR100 but I suppose it doesn't matter. Use &lt;denchmark-link:https://www.dlology.com/blog/how-to-convert-trained-keras-model-to-tensorflow-and-make-prediction/&gt;this&lt;/denchmark-link&gt;
 guide to export as .pb (none of the other guides produced a Graph that Barracuda would accept due to unknown input layers).

&lt;denchmark-link:https://imgur.com/a/rZ5uoLh&gt;This&lt;/denchmark-link&gt;
 imgur link demonstrates the input and output values. Note that the outputs are exactly the same between the two frames.
Environment (please complete the following information):

OS + version: Android 9, Kernel version 4.14.85 (is this relevant?)
ML-Agents version: Latest master branch.
Environment: Using an otherwise empty scene with the ml-agents Unity project and ARFoundation 1.5 to get the camera texture.

	</description>
	<comments>
		<comment id='1' author='ChaiKnight' date='2019-08-22T18:11:12Z'>
		I think this is a barracuda specific issue, so &lt;denchmark-link:https://github.com/mantasp&gt;@mantasp&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='ChaiKnight' date='2019-08-23T04:36:23Z'>
		So I decided to test a little more. Apparently, if I wait 2 seconds before calling the inference function for the first time (to make sure the camera is running), the results remain the same but the app lags out for about a second.
It seems like the inference sequence is never run again (if running it is supposed to lag it out) but I am not catching any exceptions which I would assume would happen if any of the method calls did not work.
Any idea how to further debug this?
		</comment>
		<comment id='3' author='ChaiKnight' date='2019-08-27T21:00:19Z'>
		Each time you update content of the RenderTexture you have to re-create input Tensor and pass new instance to the Barracuda.
		</comment>
		<comment id='4' author='ChaiKnight' date='2019-08-28T07:50:47Z'>
		&lt;denchmark-link:https://github.com/mantasp&gt;@mantasp&lt;/denchmark-link&gt;
 I'm already doing that. I create a new Tensor(with RenderTexture and channels) and pass it to my worker. My inference code is below if that helps:
&lt;denchmark-code&gt;var tensor = new Tensor(intermediateRendTex, 3);
            
worker.Execute(tensor);
worker.WaitForCompletion(); //this doesn't make a difference, it's just here to show I've tried it

var results = worker.Fetch(); //peek is the exact same

//code to print results exists here but is not shown

//we are done processing the results so we dispose
results.Dispose();
tensor.Dispose();
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='ChaiKnight' date='2019-08-28T10:54:37Z'>
		Update: I found a solution to this problem. Neither Compute nor ComputePrecompiled worked, so I tested ComputeRef. I still have no idea about the difference, but Ref seems to work.
In the meantime I've also enabled "Require ES3.1" and "Require ES3.1+AEP" and I've fiddled with texture settings and Project Settings (specifically related to graphics, enabling only GLES3.1 and Vulkan) so that might have had an effect.
I still don't know exactly why this works or what separates ComputeRef from Compute (I can't find any documentation), so if someone (&lt;denchmark-link:https://github.com/mantasp&gt;@mantasp&lt;/denchmark-link&gt;
 ?) can enlighten me that would be swell. For now though I guess the issue is closed.
		</comment>
		<comment id='6' author='ChaiKnight' date='2019-08-28T12:01:29Z'>
		
In the meantime I've also enabled "Require ES3.1" and "Require ES3.1+AEP" and I've fiddled with texture settings and Project Settings (specifically related to graphics, enabling only GLES3.1 and Vulkan) so that might have had an effect.

This is expected, though we should better document it.

I still don't know exactly why this works or what separates ComputeRef from Compute (I can't find any documentation), so if someone (@mantasp ?) can enlighten me that would be swell. For now though I guess the issue is closed.

This actually sounds like a bug. Could you share your network (random weights are fine) for us to test?
		</comment>
		<comment id='7' author='ChaiKnight' date='2019-08-28T13:24:05Z'>
		Yeah sure. Here's a &lt;denchmark-link:https://gofile.io/?c=caxd3J&gt;link&lt;/denchmark-link&gt;

The .nn file is the actual file used in my project currently, the .pb file is a very similar network I've just been working on, accidentally overwriting the original one. It shouldn't make a difference though, it's just a few extra conv layers.
		</comment>
		<comment id='8' author='ChaiKnight' date='2019-08-30T15:03:09Z'>
		Thank you!
		</comment>
	</comments>
</bug>