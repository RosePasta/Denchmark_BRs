<bug id='4125' author='ChristianCoenen' open_date='2020-06-15T18:34:44Z' closed_time='2020-06-24T18:27:35Z'>
	<summary>Academy.Instance.MaxStepCount != Python console output</summary>
	<description>
Describe the bug
Printing 'Academy.Instance.MaxStepCount' during training differs from the Python console output.
When the MaxStepCount is at around 5-6k in Unity, it prints the 12k step message on the console.
Interestingly, changing the decision period affects the difference too. If I change the decision period from 5 to 1 the MaxStepCount is at around 1k in Unity when it prints the 12k step message on the console. My understanding is that the MaxStepCount is not related to the decision period if "Take actions between decisions" is ticked and that the python connector uses the TotalStepCount value.
To Reproduce
Steps to reproduce the behavior:

loading the 3DBall example environment
creating an empty game object
Adding a script to the game object
Adding two lines to the script
4.1 using Unity.MLAgents;
4.2 print(Academy.Instance.TotalStepCount); // in the update loop

Environment (please complete the following information):

Unity Version: 2019.3.12f1
OS + version: Windows 10 Version 2004
ML-Agents version: Tested with Release 1 and Release 3
TensorFlow version: Release 1: 2.1 and Release 3: 2.2
Environment: 3DBall

	</description>
	<comments>
		<comment id='1' author='ChristianCoenen' date='2020-06-16T00:59:42Z'>
		Hi &lt;denchmark-link:https://github.com/ChristianCoenen&gt;@ChristianCoenen&lt;/denchmark-link&gt;
, the MaxStepCount in Unity is related to the number of steps for the environment, while the step count in console is related to the step number for the trainer, so they are different.
		</comment>
		<comment id='2' author='ChristianCoenen' date='2020-06-16T02:47:02Z'>
		Hi &lt;denchmark-link:https://github.com/xiaomaogy&gt;@xiaomaogy&lt;/denchmark-link&gt;
, so can you explain when the step count (which is printed on the console) is updated (like after a set of observations, actions, rewards are collected and then it is updated?). And can I access this value in Unity using the ML-Agents runtime scripts?
		</comment>
		<comment id='3' author='ChristianCoenen' date='2020-06-16T19:50:13Z'>
		&lt;denchmark-link:https://github.com/xiaomaogy&gt;@xiaomaogy&lt;/denchmark-link&gt;
 thanks for your answer!
I did some more research and would like to know if I am on the right track.

A DecisionPeriod of 5 means that the Agent will request a decision every 5 Academy steps.

found in the decision requester script as a comment


AgentScript -&gt; max_step == Academy steps

Tested


Tensorboard -&gt; Episode Length == Decision steps

Tested



Conclusion:
When I have 30 environments (1 agent in each environment) in one scene, then I can get 0-30 trainer steps during 1 Academy step? And the trainer step counter is increased by 1 each time a decision is requested from one of the 30 agents right?
That would be the only way which explains why the Academy MaxStepCount is lower than the trainer count and that that gap increases the lower the decision period / the more agents in a scene.

I think this is quite hard for many users to grasp (I got no answer in the Forum despite quite a few views before opening it here as an issue). The best I could find about this in the ML Agents doc was &lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Design.md&gt;The Simulation and Training Process&lt;/denchmark-link&gt;
. But that didn't answer many of my questions. I think some additional documentation might help some users.
		</comment>
		<comment id='4' author='ChristianCoenen' date='2020-06-24T17:34:34Z'>
		&lt;denchmark-link:https://github.com/ChristianCoenen&gt;@ChristianCoenen&lt;/denchmark-link&gt;
 Yes you are on the right track. When you have 30 environment, you can get 0 - 30 agent step during 1 academy step (since some of the agent might not finish during that academy step). And the ratio between agent step vs trainer step is determined by the decision period. So if the decision period is 1, you can get 0 - 30 trainer steps.
I do agree that this is quite hard for many user to understand, and we are working on a new code refactor to make it more understandable. Hopefully that will help.
		</comment>
	</comments>
</bug>