<bug id='1832' author='chagmgang' open_date='2019-03-17T13:34:31Z' closed_time='2019-10-01T21:10:38Z'>
	<summary>crawler print warning with observation and reward nan</summary>
	<description>
&lt;denchmark-code&gt;WARNING:mlagents.envs:An agent had a NaN reward for brain CrawlerStaticLearning
WARNING:mlagents.envs:An agent had a NaN observation for brain CrawlerStaticLearning
&lt;/denchmark-code&gt;

An error occurs on the training.
This error is printed with tensorflow training.
Tensorflow-gpu 1.12.0 version, python 3.6 is used.
	</description>
	<comments>
		<comment id='1' author='chagmgang' date='2019-03-19T01:34:17Z'>
		Hi &lt;denchmark-link:https://github.com/chagmgang&gt;@chagmgang&lt;/denchmark-link&gt;
, this is a known bug with the Crawler environment. We believe there is a divide-by-zero somewhere in the game code.
Did it stop learning, or did your crawler learn how to walk anyways?
		</comment>
		<comment id='2' author='chagmgang' date='2019-03-19T03:38:37Z'>
		The crawler environment is not working well. So in order to prove my algorithm, I did my ppo2 algorithm to walker. The result with tensorboard is here.
&lt;denchmark-link:https://user-images.githubusercontent.com/37325825/54578601-a7ccec80-4a43-11e9-839d-a4fc91e057af.png&gt;&lt;/denchmark-link&gt;

My tensorflow repository including my algorithms is &lt;denchmark-link:https://github.com/RLOpensource/tensorflow_RL&gt;https://github.com/RLOpensource/tensorflow_RL&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='chagmgang' date='2019-03-19T18:25:16Z'>
		Hi &lt;denchmark-link:https://github.com/chagmgang&gt;@chagmgang&lt;/denchmark-link&gt;
, I see. You're using your own trainer code w/ ml-agents, and it looks like at least it is training with walker. Was your walker able to walk?
As a side note, we mitigate this issue by setting all NaNs to 0 inside the Python mlagents.envs code. Usually this prevents the NaNs from propagating into the network. This is true whether or not you're using our trainer code.
If you're interested in a bunch of similar environments with different walker/crawler configurations, check out the Marathon Envs: &lt;denchmark-link:https://github.com/Unity-Technologies/marathon-envs&gt;https://github.com/Unity-Technologies/marathon-envs&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='chagmgang' date='2019-03-20T00:37:45Z'>
		My walker may be walk well because same algorithm to bipedalwalker makes good walk mechanism.
So Thanks for answering my question about environment.
		</comment>
		<comment id='5' author='chagmgang' date='2019-03-21T00:45:26Z'>
		Hey &lt;denchmark-link:https://github.com/chagmgang&gt;@chagmgang&lt;/denchmark-link&gt;
, I'm trying to replicate your issue with Crawler. How did you manage to get NaNs out? Was it using the ML-Agents trainer, or your trainer?
Thanks!
		</comment>
		<comment id='6' author='chagmgang' date='2019-03-26T20:21:47Z'>
		Hi &lt;denchmark-link:https://github.com/chagmgang&gt;@chagmgang&lt;/denchmark-link&gt;
, we believe it is an issue with Unity's Vector3.normalize function in the C# code. On our build of Unity (2017.4) no NaNs are outputted. Which version of Unity are you using?
Thanks
		</comment>
		<comment id='7' author='chagmgang' date='2019-10-01T21:10:38Z'>
		This issue should be fixed in the new Crawler environment in v0.10. Closing this bug.
		</comment>
	</comments>
</bug>