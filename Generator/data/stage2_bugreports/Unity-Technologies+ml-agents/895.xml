<bug id='895' author='alexchacon2341' open_date='2018-06-20T07:29:21Z' closed_time='2019-01-29T18:23:12Z'>
	<summary>Reward thresholding not working</summary>
	<description>
I was able to get curriculum to work using a test with the WallJump environment. For some reason, however, the training doesn't seem to register when the thresholds have been reached when using a nearly identical .json file in a GridWorld environment.
Has anyone else experienced a similar issue? Could it have something to do with using Visual Observations or On Demand Decisions?
	</description>
	<comments>
		<comment id='1' author='alexchacon2341' date='2018-06-20T21:41:24Z'>
		Could you give more detail on how "the training doesn't seem to register" please?
		</comment>
		<comment id='2' author='alexchacon2341' date='2018-06-20T22:32:31Z'>
		Yes, of course.

When I use curriculum training with the WallJump environment, I enter the
following command in my terminal using this wall.json file:





Everything works perfectly -- I am notified in the terminal once the first
threshold of 3000 steps has been reached and the curriculum proceeds to
Lesson 1. The wall begins to appear in the training, exactly as expected.



When I attempt to do the same with GridWorld, however, the curriculum seems
to be ignored entirely. Here are my commands and json file for that
environment:





Please note that while this particular json file uses rewards as its
measure instead of progress, I have also attempted to train using progress
as my measure with identical thresholds and max steps as I used in the
WallJump environment. Even then, the curriculum did not register in the
GridWorld environment.

It seems that, regardless of how I set up my json file, the training skips
over the thresholds as if no curriculum is being used. In the following
training, you can see that, regardless of whether the json file is using
3000 steps or two appearances of a mean reward greater than 0.5 as a
threshold, the criteria has been met. For some reason, however, the lesson
is not increased:



I am at a loss as to what could be causing this difference. As I mentioned
in my post, I am using Visual Observations and On Demand Decisions in the
GridWorld environment. I can't imagine these would affect the curriculum,
but also can't think of any other factors that would disrupt the training.

Thanks in advance for your help, I greatly appreciate it.

Sincerely,
Alex Chacon
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Wed, Jun 20, 2018 at 5:41 PM, Vincent(Yuan) Gao ***@***.*** &gt; wrote:
 Could you give more detail on how "the training doesn't seem to register"
 please?

 —
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub
 &lt;#895 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/Adg71wwG0CV-cF4p1OQ5My6vIVd0gzGNks5t-sGRgaJpZM4Uuvu9&gt;
 .



		</comment>
		<comment id='3' author='alexchacon2341' date='2018-06-20T22:39:42Z'>
		&lt;denchmark-link:https://github.com/alexchacon2341&gt;@alexchacon2341&lt;/denchmark-link&gt;
 It seems that I cannot see the pictures you attached.
		</comment>
		<comment id='4' author='alexchacon2341' date='2018-06-20T22:41:23Z'>
		Sorry, I responded via email. I'll try replying in GitHub itself with the
images.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Wed, Jun 20, 2018 at 6:39 PM, Vincent(Yuan) Gao ***@***.*** &gt; wrote:
 @alexchacon2341 &lt;https://github.com/alexchacon2341&gt; It seems that I
 cannot see the pictures you attached.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#895 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/Adg71z6hR7a7ZwFBrAhhTijNqHVAZTCaks5t-s86gaJpZM4Uuvu9&gt;
 .



		</comment>
		<comment id='5' author='alexchacon2341' date='2018-06-20T22:48:28Z'>
		Yes, of course.
When I use curriculum training with the WallJump environment, I enter the
following command in my terminal using this wall.json file:
&lt;denchmark-link:https://user-images.githubusercontent.com/30948311/41688495-5704a34c-74ba-11e8-9f4f-161370aa6efe.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/30948311/41688498-5ac39556-74ba-11e8-8b71-5e7cf5dd7d16.png&gt;&lt;/denchmark-link&gt;

Everything works perfectly -- I am notified in the terminal once the first
threshold of 3000 steps has been reached and the curriculum proceeds to
Lesson 1. The wall begins to appear in the training, exactly as expected.
&lt;denchmark-link:https://user-images.githubusercontent.com/30948311/41688447-2baaece2-74ba-11e8-8be5-cc4210663448.png&gt;&lt;/denchmark-link&gt;

When I attempt to do the same with GridWorld, however, the curriculum seems
to be ignored entirely. Here are my commands and json file for that
environment:
&lt;denchmark-link:https://user-images.githubusercontent.com/30948311/41688508-64fe36f2-74ba-11e8-8739-285bbbf51e7a.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/30948311/41688513-67eec94e-74ba-11e8-8aec-2f6a3844a07b.png&gt;&lt;/denchmark-link&gt;

Please note that while this particular json file uses rewards as its
measure instead of progress, I have also attempted to train using progress
as my measure with identical thresholds and max steps as I used in the
WallJump environment. Even then, the curriculum did not register in the
GridWorld environment.
It seems that, regardless of how I set up my json file, the training skips
over the thresholds as if no curriculum is being used. In the following
training, you can see that, regardless of whether the json file is using
3000 steps or two appearances of a mean reward greater than 0.5 as a
threshold, the criteria has been met. For some reason, however, the lesson
is not increased:
&lt;denchmark-link:https://user-images.githubusercontent.com/30948311/41688474-3cd7d764-74ba-11e8-8a4e-5ad04b4f7159.png&gt;&lt;/denchmark-link&gt;

I am at a loss as to what could be causing this difference. As I mentioned
in my post, I am using Visual Observations and On Demand Decisions in the
GridWorld environment. I can't imagine these would affect the curriculum,
but also can't think of any other factors that would disrupt the training.
Thanks in advance for your help, I greatly appreciate it.
Sincerely,
Alex Chacon
		</comment>
		<comment id='6' author='alexchacon2341' date='2018-06-21T08:12:52Z'>
		I think the problem is that the grid world academy does not reset (only the agent does). Try setting an academy max step in the grid world. Please let us know if this helped.
		</comment>
		<comment id='7' author='alexchacon2341' date='2018-06-21T08:17:02Z'>
		Are you sure the GridWorld academy doesn't reset? This is the agent reset
function from GridAgent.cs:

    public override void AgentReset()
    {
        academy.AcademyReset();
    }

Is this not causing the academy to reset?

Thanks,
Alex
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Thu, Jun 21, 2018 at 4:13 AM, vincentpierre ***@***.***&gt; wrote:
 I think the problem is that the grid world academy does not reset (only
 the agent does). Try setting an academy max step in the grid world. Please
 let us know if this helped.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#895 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/Adg714HKUjhMsSHqd72fwrwykLXlQ5ZTks5t-1WZgaJpZM4Uuvu9&gt;
 .



		</comment>
		<comment id='8' author='alexchacon2341' date='2018-06-21T08:28:10Z'>
		Can you try with setting a max step. There is a possibility that the academy resets without being set to done. This means that python might not realize that the academy is done and has reset.
		</comment>
		<comment id='9' author='alexchacon2341' date='2018-06-21T08:31:31Z'>
		Sure. I'm running a training session at the moment, but I'll run another one with a max step after. Just to be sure we're on the same page, this is where I should be setting the max step, correct?
&lt;denchmark-link:https://user-images.githubusercontent.com/30948311/41707378-f75dc584-750b-11e8-9470-0051f6aadfe7.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='alexchacon2341' date='2018-06-21T08:32:09Z'>
		Yes
		</comment>
		<comment id='11' author='alexchacon2341' date='2018-06-21T08:33:42Z'>
		OK, great. I'll run the training and get back to you ASAP. Thanks again!
		</comment>
		<comment id='12' author='alexchacon2341' date='2018-06-21T08:57:23Z'>
		No luck, I'm afraid. With a max step of 30,000 and the first threshold at 0.1, the lesson should have increased at 3,000. Here is my data:
&lt;denchmark-link:https://user-images.githubusercontent.com/30948311/41708718-5cd234f6-750f-11e8-8e8c-18d86c910497.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/30948311/41708784-8a526fea-750f-11e8-943d-20aa797bc8f5.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/30948311/41708789-8d4112ec-750f-11e8-869f-ab497241c400.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/30948311/41708796-90a07824-750f-11e8-8597-4cb9a471a950.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/30948311/41708802-935f26f0-750f-11e8-8604-4f9ade17c1fe.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='alexchacon2341' date='2018-07-01T14:28:21Z'>
		Same problem here. Curriculum does not jump to next step. It is possible to start every lesson by handy by using --lessong=[0,1,2...], but there is no next lesson by the curriculum.
Reward and progress mesure do not work.
		</comment>
		<comment id='14' author='alexchacon2341' date='2018-07-01T14:38:42Z'>
		Yeah, I’ve tried using another self-made environment and still can’t get
curricula to work. I ended up creating a variable that tracks steps and can
recreate the effects of progress-based curricula using it, but still have
no way of using a mean-based curriculum (which would be far more useful).

Is there a way of accessing the mean reward that is printed in the console?
I’ve tried using GetReward(), but that data comes in much more frequent
intervals that are difficult to work with.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Sun, Jul 1, 2018 at 10:28 AM marcelwessely ***@***.***&gt; wrote:
 Same problem here. Curriculum does not jump to next step. It is possible
 to start every lesson by handy by using --lessong=[0,1,2...], but there
 is no next lesson by the curriculum.
 Reward and progress mesure do not work.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#895 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/Adg712jl9R2fkSlkdqXRHkVJFXOc6XJEks5uCNyPgaJpZM4Uuvu9&gt;
 .



		</comment>
		<comment id='15' author='alexchacon2341' date='2018-08-14T18:11:29Z'>
		Hi &lt;denchmark-link:https://github.com/alexchacon2341&gt;@alexchacon2341&lt;/denchmark-link&gt;
,
&lt;denchmark-link:https://github.com/dericp&gt;@dericp&lt;/denchmark-link&gt;
 is working on improving the curriculum feature for the v0.5 release. I'll let him know to take a look at this thread.
		</comment>
		<comment id='16' author='alexchacon2341' date='2018-08-14T18:23:36Z'>
		&lt;denchmark-link:https://github.com/awjuliani&gt;@awjuliani&lt;/denchmark-link&gt;
 Great, thanks Arthur!
&lt;denchmark-link:https://github.com/dericp&gt;@dericp&lt;/denchmark-link&gt;
 Please let me know if there is any way I can be of assistance!
		</comment>
		<comment id='17' author='alexchacon2341' date='2018-08-14T21:34:39Z'>
		Hi &lt;denchmark-link:https://github.com/alexchacon2341&gt;@alexchacon2341&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/marcelwessely&gt;@marcelwessely&lt;/denchmark-link&gt;
,
Thanks for getting in contact. I believe the issue is related to Academy Max Steps like Vince mentioned. Let me explain a bit more.
There are three max steps parameters---one for agents, one for academies, and one for the trainer. The max steps that you set above was for the academy.
An environment will not reset and increment the lesson number unless the academy is "done" or has reached its max steps.
The max steps which is related to the "progress" that you define in a curriculum is the trainer max step.
Here is what I did to get curriculum learning working in GridWorld.
My curriculum is defined as:
{
    "measure" : "progress",
    "thresholds" : [0.1],
    "min_lesson_length" : 2,
    "signal_smoothing" : true, 
    "parameters" : 
    {
        "gridSize" : [5, 7]
    }
}
I set the max steps of the academy to 1000:
&lt;denchmark-link:https://user-images.githubusercontent.com/13430946/44119863-5d784112-9fcf-11e8-9488-76cf6915c208.png&gt;&lt;/denchmark-link&gt;

In python/trainer_config.yaml, I have edited max_steps under GridWorldBrain to be 5.0e4. This means that the trainer will run for 50,000 steps total instead of the default 500,000 that is set for GridWorldBrain.
This means we expect the lesson to change at step 5000.
Here is my output:
&lt;denchmark-code&gt;$ python python/learn.py --train \
--curriculum=python/curricula/grid.json \
--run-id=grid-world
. . .
INFO:unityagents: GridWorldBrain: Step: 2000. Mean Reward: -0.154. Std of Reward: 0.984.
INFO:unityagents: GridWorldBrain: Step: 4000. Mean Reward: -0.342. Std of Reward: 1.002.
INFO:unityagents:
Lesson changed. Now in Lesson 1 : 	gridSize -&gt; 7
INFO:unityagents: GridWorldBrain: Step: 6000. Mean Reward: 0.031. Std of Reward: 0.960.
&lt;/denchmark-code&gt;

Let me know if this works for you! If anything doesn't make sense feel free to reach out.
Note: Curriculum learning has been revamped and will be different in v0.5. If you're curious, check out the develop branch.
		</comment>
		<comment id='18' author='alexchacon2341' date='2018-08-15T23:18:02Z'>
		&lt;denchmark-link:https://github.com/dericp&gt;@dericp&lt;/denchmark-link&gt;
 Ah, I see my error. The Max Steps on the Grid Academy script was so high (30,000) that the AcademyReset() function was never called. Good to know the difference between this value and the max steps value in trainer_config.yaml. I have updated everything and the curriculum now works for both "progress" and "reward" measures.
I did notice something a little strange, which is that the lesson was increased before the mean reward reached each threshold. As you can see below, the curriculum entered Lesson 1 when a mean reward of -0.045 was reported even though the "reward" measure threshold was 0.1:
&lt;denchmark-link:https://user-images.githubusercontent.com/30948311/44178134-f8035b00-a0be-11e8-94ba-c45ddbf90925.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/30948311/44178060-94792d80-a0be-11e8-9ffb-ad1f02657c6c.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

And later, the curriculum entered Lesson 2 with a mean reward of 0.189 even though the next "reward" measure was 0.2:
&lt;denchmark-link:https://user-images.githubusercontent.com/30948311/44178086-ae1a7500-a0be-11e8-8361-b1bc4adeb7a8.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

I'm guessing this means the "reward" measure looks at individually reported reward values instead of the mean reward -- is that correct?
		</comment>
		<comment id='19' author='alexchacon2341' date='2018-08-16T22:42:09Z'>
		&lt;denchmark-link:https://github.com/alexchacon2341&gt;@alexchacon2341&lt;/denchmark-link&gt;
 That is definitely strange and could be a bug. Let me look into it.
		</comment>
		<comment id='20' author='alexchacon2341' date='2018-08-16T22:45:09Z'>
		Sounds good, thanks very much!
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Thu, Aug 16, 2018 at 6:42 PM Deric Pang ***@***.***&gt; wrote:
 @alexchacon2341 &lt;https://github.com/alexchacon2341&gt; That is definitely
 strange and could be a bug. Let me look into it.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#895 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/Adg716beKHRAO_p_8b2ULlSArkYiYC1eks5uRfVKgaJpZM4Uuvu9&gt;
 .



		</comment>
		<comment id='21' author='alexchacon2341' date='2018-08-24T15:03:16Z'>
		Encountering the same issue,
In GridAcademy (for sample reference) resetParameter doesn't have any "Write Access" according to the IDE, the default resetParameter seems only initialize in editor inspector?
&lt;denchmark-link:https://user-images.githubusercontent.com/14128307/44592007-db1f0400-a7f1-11e8-9a8d-6b699f695d3b.PNG&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='22' author='alexchacon2341' date='2018-09-05T16:23:59Z'>
		Hey everyone,
This bug should be fixed by &lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/pull/1141&gt;#1141&lt;/denchmark-link&gt;
. You can try it out on the release branch &lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/tree/release-v0.5&gt;https://github.com/Unity-Technologies/ml-agents/tree/release-v0.5&lt;/denchmark-link&gt;
. Note that ml-agents has evolved quite a bit on this branch and if you're interested in trying out the fix, read the migration doc &lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/blob/release-v0.5/docs/Migrating.md&gt;https://github.com/Unity-Technologies/ml-agents/blob/release-v0.5/docs/Migrating.md&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='23' author='alexchacon2341' date='2018-09-07T02:10:34Z'>
		Bug fix &lt;denchmark-link:https://github.com/Unity-Technologies/ml-agents/pull/1141&gt;#1141&lt;/denchmark-link&gt;
 looks good, thanks! In the current implementation, will changing "buffer_size" in trainer_config.yaml also change the size of the "reward_buffer" from which trainer_controller.py samples mean reward? Also, will the "Mean Reward" reported in the terminal (or equivalent) now be the same as the mean reward sampled from "buffer_size"?
		</comment>
		<comment id='24' author='alexchacon2341' date='2018-09-07T21:11:07Z'>
		buffer_size in the trainer configuration file refers to "the number of experiences to collect before updating the policy model" which is completely separate from the size of the reward_buffer. The size of the reward_buffer will be the min_lesson_length defined in the curriculum. Unfortunately, the mean reward reported in the console will not be the same as the reward sampled from the reward_buffer.
Copying from the CL docs:
min_lesson_length (int) - The minimum number of episodes that should be completed before the lesson can change. If measure is set to reward, the average cumulative reward of the last min_lesson_length episodes will be used to determine if the lesson should change. Must be nonnegative.
Important: the average reward that is compared to the thresholds is different than the mean reward that is logged to the console. For example, if min_lesson_length is 100, the lesson will increment after the average cumulative reward of the last 100 episodes exceeds the current threshold. The mean reward logged to the console is dictated by the summary_freq parameter in the trainer configuration file.
		</comment>
		<comment id='25' author='alexchacon2341' date='2018-09-08T02:44:38Z'>
		Got it. So, if desired, I could set "min_lesson_length" and "summary_freq" to the same value in order to make the output to the console essentially monitor the mean used by the curriculum, correct? And this would not effect the actual training of the agent?
		</comment>
		<comment id='26' author='alexchacon2341' date='2018-09-08T11:03:39Z'>
		Yes and yes, that sounds right.
		</comment>
		<comment id='27' author='alexchacon2341' date='2018-09-08T19:14:22Z'>
		Perfect, thanks so much!
		</comment>
		<comment id='28' author='alexchacon2341' date='2019-01-29T18:23:12Z'>
		Thanks for reaching out to us. Hopefully you were able to resolve your issue.  We are closing this due to inactivity, but if you need additional assistance, feel free to reopen the issue.
		</comment>
		<comment id='29' author='alexchacon2341' date='2019-01-29T18:25:56Z'>
		I was able to resolve the issue, thanks very much!

Best,
Alex
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Tue, Jan 29, 2019 at 1:23 PM esh ***@***.***&gt; wrote:
 Thanks for reaching out to us. Hopefully you were able to resolve your
 issue. We are closing this due to inactivity, but if you need additional
 assistance, feel free to reopen the issue.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#895 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/Adg717O7a8khnPEAE8IUPtVfFcVpK95Mks5vIJGegaJpZM4Uuvu9&gt;
 .



		</comment>
		<comment id='30' author='alexchacon2341' date='2020-01-30T12:33:48Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>