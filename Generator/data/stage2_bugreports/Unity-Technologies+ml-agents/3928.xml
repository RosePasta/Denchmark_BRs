<bug id='3928' author='SoyGema' open_date='2020-05-06T15:20:13Z' closed_time='2020-05-08T11:42:15Z'>
	<summary>GAIL OPTIMIZER  in discrete environments ValueError: Cannot feed value of shape (128, 0) for Tensor 'optimizer/Placeholder_3:0', which has shape '(?, 1)'</summary>
	<description>
Hello there!
Congratulations on the release . Been using the library for a while and is getting more straight forward and user friendly! üëç Thanks in advance for the time taken on trackling this challenge.
Im currently struggling with this issue regarding training in GAIL with discrete environment with ml-agents: 0.15.0
Bug description

The training process starts correctly , but fails during optimizer update .

ValueError: Cannot feed value of shape (128, 0) for Tensor 'optimizer/Placeholder_3:0', which has shape '(?, 1)' 
The assumption stands for:

(128,0 ) . 128 as the batch size
(?,1) . 1 as space size

As far as I can understand , (?,...) appears when there is a mismatch in between the shape of what I am feeding and what Tensorflow is expecting, and is normally fixed with np.reshape() method in NNs design, but not sure how/where to trackle this exaclty or how to move from this point.
Environment :
ml-agents: 0.15.0,
ml-agents-envs: 0.15.0,
Communicator API: 0.15.0,
TensorFlow: 2.0.1
gail_config.yaml config file:
&lt;denchmark-code&gt;	batch_size:	128
	beta:	0.01
	buffer_size:	2048
	epsilon:	0.2
	hidden_units:	512
	lambd:	0.95
	learning_rate:	0.0003
	max_steps:	1.0e7
	memory_size:	256
	normalize:	False
	num_epoch:	3
	num_layers:	2
	time_horizon:	128
	sequence_length:	64
	summary_freq:	30000
	use_recurrent:	False
	reward_signals:	
	  gail:	
	    strength:	0.01
	    gamma:	0.99
	    encoding_size:	128
	    demo_path:	/Users/me/Desktop/ml-agents/demos/ImitationLibra.demo
	summary_path:	Mempathy_Mempathy
	model_path:	./models/Mempathy/Mempathy
	keep_checkpoints:	5
&lt;/denchmark-code&gt;

Console logs / stack traces
&lt;denchmark-code&gt;    sys.exit(main())
  File "/Users/me/.pyenv/versions/3.7.0/lib/python3.7/site-packages/mlagents/trainers/learn.py", line 495, in main
    run_cli(parse_command_line())
  File "/Users/me/.pyenv/versions/3.7.0/lib/python3.7/site-packages/mlagents/trainers/learn.py", line 491, in run_cli
    run_training(run_seed, options)
  File "/Users/me/.pyenv/versions/3.7.0/lib/python3.7/site-packages/mlagents/trainers/learn.py", line 329, in run_training
    tc.start_learning(env_manager)
  File "/Users/me/.pyenv/versions/3.7.0/lib/python3.7/site-packages/mlagents_envs/timers.py", line 258, in wrapped
    return func(*args, **kwargs)
  File "/Users/me/.pyenv/versions/3.7.0/lib/python3.7/site-packages/mlagents/trainers/trainer_controller.py", line 207, in start_learning
    n_steps = self.advance(env_manager)
  File "/Users/me/.pyenv/versions/3.7.0/lib/python3.7/site-packages/mlagents_envs/timers.py", line 258, in wrapped
    return func(*args, **kwargs)
  File "/Users/me/.pyenv/versions/3.7.0/lib/python3.7/site-packages/mlagents/trainers/trainer_controller.py", line 280, in advance
    trainer.advance()
  File "/Users/me/.pyenv/versions/3.7.0/lib/python3.7/site-packages/mlagents/trainers/trainer/rl_trainer.py", line 73, in advance
    super().advance()
  File "/Users/me/.pyenv/versions/3.7.0/lib/python3.7/site-packages/mlagents/trainers/trainer/trainer.py", line 305, in advance
    self._update_policy()
  File "/Users/me/.pyenv/versions/3.7.0/lib/python3.7/site-packages/mlagents/trainers/ppo/trainer.py", line 211, in _update_policy
    buffer.make_mini_batch(l, l + batch_size), n_sequences
  File "/Users/me/.pyenv/versions/3.7.0/lib/python3.7/site-packages/mlagents_envs/timers.py", line 258, in wrapped
    return func(*args, **kwargs)
  File "/Users/me/.pyenv/versions/3.7.0/lib/python3.7/site-packages/mlagents/trainers/ppo/optimizer.py", line 305, in update
    update_vals = self._execute_model(feed_dict, self.update_dict)
  File "/Users/me/.pyenv/versions/3.7.0/lib/python3.7/site-packages/mlagents/trainers/optimizer/tf_optimizer.py", line 152, in _execute_model
    network_out = self.sess.run(list(out_dict.values()), feed_dict=feed_dict)
  File "/Users/me/.pyenv/versions/3.7.0/lib/python3.7/site-packages/tensorflow_core/python/client/session.py", line 956, in run
    run_metadata_ptr)
  File "/Users/me/.pyenv/versions/3.7.0/lib/python3.7/site-packages/tensorflow_core/python/client/session.py", line 1156, in _run
    (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))
ValueError: Cannot feed value of shape (128, 0) for Tensor 'optimizer/Placeholder_3:0', which has shape '(?, 1)'
&lt;/denchmark-code&gt;

Screenshots
This screenshot provides relevant information to Vector Observation Size and Vector Action Size from the demonstration recorded, feeded into the demo_path.
&lt;denchmark-link:https://user-images.githubusercontent.com/24204714/81192897-0db76300-8fbb-11ea-8bc0-b1efbb553f48.png&gt;&lt;/denchmark-link&gt;

This screenshot provides relevant information regarding Behavior parameters
&lt;denchmark-link:https://user-images.githubusercontent.com/24204714/81193025-30e21280-8fbb-11ea-959c-6b0ad352461c.png&gt;&lt;/denchmark-link&gt;

Thanks again for the time dedicated to this. Please, don't hesitate on underline if Im  having any kind of dimensionality misunderstanding in the discrete action space design and/or make any follow up questions of any kind relevant for this due to something that I might miss out :)
Thank you! üíØ
	</description>
	<comments>
		<comment id='1' author='SoyGema' date='2020-05-07T23:22:00Z'>
		Hi &lt;denchmark-link:https://github.com/SoyGema&gt;@SoyGema&lt;/denchmark-link&gt;
, does the same happen with v0.16 (the release 1 version of the Python trainers?) We added some better checking of demo file action/observation spaces that should throw a more verbose error if there's a size mismatch.
		</comment>
		<comment id='2' author='SoyGema' date='2020-05-08T11:42:15Z'>
		Hi :)
Thank you for taking the time to help.
I've updated to the v0.16 release and transform the data structure Agent.Heuristic() as the migration documentation says .
And then I found this
RuntimeError: The action dimensions [1] in demonstration do not match the policy's [6]. 
So the issue was due to coherence in between action dimensions in recording and training
&lt;denchmark-link:https://user-images.githubusercontent.com/24204714/81402178-33667880-9131-11ea-8281-4d982903c14f.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/24204714/81402182-37929600-9131-11ea-8a04-24ded51e6ea7.png&gt;&lt;/denchmark-link&gt;

It's working now! üíØ
Thanks for your support, and for making this change, it definetly makes the backtracking easier and describes the approach in a very semantic way ! üëç
Keep up the good work ü•á
		</comment>
		<comment id='3' author='SoyGema' date='2020-05-08T17:43:59Z'>
		Awesome, great you've got it working!
		</comment>
	</comments>
</bug>