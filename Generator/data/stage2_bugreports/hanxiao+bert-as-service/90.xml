<bug id='90' author='iamadog3333' open_date='2018-12-05T09:47:50Z' closed_time='2018-12-20T11:38:14Z'>
	<summary>the server seems hangs</summary>
	<description>
I started a server, it seems ok.
When I run a client to encode a demo sentence, the server will log a new job, but no more output after than. The server hangs. (The server log a new job only once, when I try to run a client second time, the server has no new job logged.)
When I use ctr+z to stop the server, the linux shows segment fault.
How to debug this problem? Thank you.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

server log______________
usage: app.py -model_dir ./models/chinese_L-12_H-768_A-12/ -num_worker=1
ARG   VALUE
gpu_memory_fraction = 0.5
max_batch_size = 256
max_seq_len = 25
model_dir = ./models/chinese_L-12_H-768_A-12/
num_worker = 1
pooling_layer = [-2]
pooling_strategy = REDUCE_MEAN
port = 5555
port_out = 5556
I:VENTILATOR:[ser:__i: 79]:frontend-sink ipc: ipc://tmpt7XjXt/socket
W:VENTILATOR:[ser:run:100]: only 0 GPU(s) is available, but ask for 1
I:SINK:[ser:run:230]:new job b'9be1e3dd-88f6-47c4-a51b-61b74bd6e65e#ea138d3d-ac8c-479e-ba3d-18b417fd8caa' size: 2 is registered!
--------client code -------------------
from service.client import BertClient
bc = BertClient()
print('client inited.')
x = ['hey you', 'whats up?']
print(bc.encode(x))  # [2, 25, 768]
print(bc.encode(x)[0])  # [1, 25, 768], sentence embeddings for hey you
print(bc.encode(x)[0][0])  # [1, 1, 768], word embedding for [CLS]
-----------client side log-------------------
you should NOT see this message multiple times! if you see it appears repeatedly, consider moving "BertClient()" out of the loop.
client inited.
	</description>
	<comments>
		<comment id='1' author='iamadog3333' date='2018-12-05T09:58:50Z'>
		seems related to &lt;denchmark-link:https://github.com/hanxiao/bert-as-service/issues/81&gt;#81&lt;/denchmark-link&gt;

maybe a bug on CPU-only machine, looks like the bert-worker on the server never starts?
		</comment>
		<comment id='2' author='iamadog3333' date='2018-12-05T11:23:52Z'>
		
seems related to #81
maybe a bug on CPU-only machine, looks like the bert-worker on the server never starts?

Find the reason.
The server did not started correctly.
For my machine, it should add some environmental path for python.
Like this:
/opt/compiler/gcc-xxx/lib/ld-linux-x86-64.so.x --library-path /opt/compiler/gcc-xxx/lib  ../../bin/python3.6  app.py -model_dir ./models/chinese_L-12_H-768_A-12/ -num_worker=1
		</comment>
		<comment id='3' author='iamadog3333' date='2018-12-10T06:17:31Z'>
		if you encounter such problem on a CPU-only machine, please try the latest version 1.5.0 by
pip install -U bert-serving-server bert-serving-client
When start the server side, remember to add a -cpu flag, e.g.
&lt;denchmark-code&gt;bert-serving-start -model_dir /your/model/path -cpu
&lt;/denchmark-code&gt;

For better performance on CPU, please give a smaller , e.g. 16, 32 as suggested in &lt;denchmark-link:https://github.com/hanxiao/bert-as-service#q-can-i-run-the-server-side-on-cpu-only-machine&gt;https://github.com/hanxiao/bert-as-service#q-can-i-run-the-server-side-on-cpu-only-machine&lt;/denchmark-link&gt;

Finally, &lt;denchmark-link:https://github.com/hanxiao/bert-as-service/pull/111&gt;#111&lt;/denchmark-link&gt;
 was the PR for fixing this issue.
		</comment>
		<comment id='4' author='iamadog3333' date='2018-12-11T02:55:18Z'>
		I've run into the same issue with a CPU-only Docker instance. I've upgraded to 1.5.0 but no dice.
Both the server and client are instantiated and running:
&lt;denchmark-code&gt;                ARG   VALUE
__________________________________________________
                 cpu = True
 gpu_memory_fraction = 0.5
      max_batch_size = 8
         max_seq_len = 25
           model_dir = /app/model/cased_L-12_H-768_A-12/
          num_worker = 1
       pooling_layer = [-2]
    pooling_strategy = REDUCE_MEAN
                port = 5555
            port_out = 5556

I:VENTILATOR:[__i:run:122]:device_map:
                worker  0 -&gt; gpu -1
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpmleudcse
WARNING:tensorflow:Estimator's model_fn (&lt;function model_fn_builder.&lt;locals&gt;.model_fn at 0x7ff9e5d5f598&gt;) includes params argument, but params are not passed to Estimator.
2018-12-11 02:52:10.841796: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2018-12-11 02:52:10.855842: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: UNKNOWN ERROR (-1)
2018-12-11 02:52:10.856818: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:150] kernel driver does not appear to be running on this host (76b3a55b3fff): /proc/driver/nvidia/version does not exist
2018-12-11 02:52:11.669188: W tensorflow/core/framework/allocator.cc:122] Allocation of 89075712 exceeds 10% of system memory.
I:WORKER-0:[__i:gen:312]:ready and listening!
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;I:VENTILATOR:[__i:run:133]:new config request   req id: 0       client: b'e991674e-68b8-4887-88fc-e4fe739d4fcd'
I:SINK:[__i:run:250]:send config        client b'e991674e-68b8-4887-88fc-e4fe739d4fcd'
server config:
                max_batch_size  =       8
                   max_seq_len  =       25
                          port  =       5555
          ventilator -&gt; worker  =       ipc://tmplIlyWd/socket
                   num_request  =       0
                worker -&gt; sink  =       ipc://tmpKHJTL4/socket
           server_current_time  =       2018-12-11 02:53:40.172891
                num_subprocess  =       2
             server_start_time  =       2018-12-11 02:53:06.977076
            tensorflow_version  =       1.12.0
           ventilator &lt;-&gt; sink  =       ipc://tmp1RPUtK/socket
                 pooling_layer  =       [-2]
                        client  =       e991674e-68b8-4887-88fc-e4fe739d4fcd
                     model_dir  =       /app/model/cased_L-12_H-768_A-12/
              pooling_strategy  =       2
                python_version  =       3.5.2 (default, Nov 23 2017, 16:37:01)
[GCC 5.4.0 20160609]
                server_version  =       1.5.0
                    run_on_gpu  =       False
                      port_out  =       5556
                    num_worker  =       1
I:VENTILATOR:[__i:run:133]:new config request   req id: 0       client: b'4288bb38-18b9-477c-86b7-180568dfd4ab'
I:SINK:[__i:run:250]:send config        client b'4288bb38-18b9-477c-86b7-180568dfd4ab'
server config:
                max_batch_size  =       8
                   max_seq_len  =       25
                          port  =       5555
          ventilator -&gt; worker  =       ipc://tmplIlyWd/socket
                   num_request  =       0
                worker -&gt; sink  =       ipc://tmpKHJTL4/socket
           server_current_time  =       2018-12-11 02:53:40.307398
                num_subprocess  =       2
             server_start_time  =       2018-12-11 02:53:06.977076
            tensorflow_version  =       1.12.0
           ventilator &lt;-&gt; sink  =       ipc://tmp1RPUtK/socket
                 pooling_layer  =       [-2]
                        client  =       4288bb38-18b9-477c-86b7-180568dfd4ab
                     model_dir  =       /app/model/cased_L-12_H-768_A-12/
              pooling_strategy  =       2
                python_version  =       3.5.2 (default, Nov 23 2017, 16:37:01)
[GCC 5.4.0 20160609]
                server_version  =       1.5.0
                    run_on_gpu  =       False
                      port_out  =       5556
                    num_worker  =       1
&lt;/denchmark-code&gt;

But when I try to encode, it looks like the server hangs:
&lt;denchmark-code&gt;I:VENTILATOR:[__i:run:147]:new encode request   req id: 1       client: b'e991674e-68b8-4887-88fc-e4fe739d4fcd'
I:SINK:[__i:run:247]:job register       size: 2 job id: b'e991674e-68b8-4887-88fc-e4fe739d4fcd#1'
I:WORKER-0:[__i:gen:316]:new job        size: 2 client: b'e991674e-68b8-4887-88fc-e4fe739d4fcd#1'
&lt;/denchmark-code&gt;

It just sits there and never returns. Occasionally, the first call to encode() will work, but never any subsequent calls.
		</comment>
		<comment id='5' author='iamadog3333' date='2018-12-11T03:02:46Z'>
		hmm, I'm reopening this issue for now to remind me.
		</comment>
		<comment id='6' author='iamadog3333' date='2018-12-11T04:53:29Z'>
		If I manually raise StopIteration at this line:



bert-as-service/server/bert_serving/server/__init__.py


         Line 324
      in
      6ad6882






 





I can at least guarantee a single response (though of course, nothing will work after that as the worker has exited).
Not sure if this is related to the tf.data.Dataset or the zmq worker.recv_multipart()
		</comment>
		<comment id='7' author='iamadog3333' date='2018-12-11T05:11:07Z'>
		Removing Dataset.prefetch() seems to fix the issue.



bert-as-service/server/bert_serving/server/__init__.py


         Line 336
      in
      6ad6882






 'input_type_ids': (None, self.max_seq_len)}).prefetch(self.prefetch_factor)) 





Not super familiar with the tf.data API so I'm not exactly sure what's going on, or why this only seems to be an issue on the Docker/CPU-only instance.
		</comment>
		<comment id='8' author='iamadog3333' date='2018-12-11T05:33:14Z'>
		Thanks &lt;denchmark-link:https://github.com/nmfisher&gt;@nmfisher&lt;/denchmark-link&gt;
 for digging the problem. I also had a vague feeling that the problem may due to the  but can't really tell for sure.
I didn't test with docker container, but w/o container both CPU and GPU work fine with prefetch(). FYI, prefetch() can improve about 20% inference speed, as it overlaps the CPU time (for preparing the data) and GPU time (for model computing).
Anyway, I can at least add a argument to CLI for controlling whether to use prefetch.
		</comment>
		<comment id='9' author='iamadog3333' date='2018-12-14T13:45:50Z'>
		&lt;denchmark-link:https://github.com/nmfisher&gt;@nmfisher&lt;/denchmark-link&gt;
 Just bump to 1.5.5, with more "multi-process friendly" BertWorker. I do think this fix is related to your problem. Please kindly  to upgrade and try. Thanks!
		</comment>
		<comment id='10' author='iamadog3333' date='2018-12-19T10:44:51Z'>
		Is upgrading to the latest version solve the problem?
pip install -U bert-serving-server bert-serving-client
		</comment>
		<comment id='11' author='iamadog3333' date='2018-12-20T11:38:10Z'>
		fyi, this issue is fixed in &lt;denchmark-link:https://github.com/hanxiao/bert-as-service/pull/155&gt;#155&lt;/denchmark-link&gt;
 and is available since 1.6.2, please do
pip install -U bert-serving-server bert-serving-client
for the update.
		</comment>
		<comment id='12' author='iamadog3333' date='2020-08-06T08:11:00Z'>
		&lt;denchmark-link:https://user-images.githubusercontent.com/16528112/89507572-f875ee80-d7cc-11ea-977f-b19a6104a57d.jpg&gt;&lt;/denchmark-link&gt;

why it takes long time when get into freeze step?
		</comment>
	</comments>
</bug>