<bug id='152' author='xiaobengou01' open_date='2018-12-20T02:18:46Z' closed_time='2018-12-20T11:33:44Z'>
	<summary>the bert-serving service does not work normaly</summary>
	<description>
the server reponse anything and hungs when running code bc.encode(['first do it']),i start the service with"bert-serving-start -model_dir /root/nlp/bert/models/chinese_L-12_H-768_A-12 -num_worker=1 -cpu" .I hava alraedy update to the lastest version 1.6.1.my python version is 3.6.3ã€‚system is ubuntu-18.04. CPU only.what can I do to solve this problem.
	</description>
	<comments>
		<comment id='1' author='xiaobengou01' date='2018-12-20T02:23:52Z'>
		
please fill the issue form, save time for both of us.
please provide detailed server logs.

		</comment>
		<comment id='2' author='xiaobengou01' date='2018-12-20T02:29:34Z'>
		&lt;denchmark-link:https://github.com/hanxiao&gt;@hanxiao&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/44760283/50260154-1f3db080-0442-11e9-9e6f-88bf298fcb08.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/44760283/50260162-2369ce00-0442-11e9-8424-bcdcaf1c3bcd.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='xiaobengou01' date='2018-12-20T02:33:23Z'>
		got it, may relate to &lt;denchmark-link:https://github.com/hanxiao/bert-as-service/issues/90#issuecomment-446074747&gt;#90 (comment)&lt;/denchmark-link&gt;

could you do bc.server_status and bc.status and paste the result here? remember to remove sensitive info for the sake of your privacy.
		</comment>
		<comment id='4' author='xiaobengou01' date='2018-12-20T02:37:10Z'>
		&lt;denchmark-link:https://user-images.githubusercontent.com/44760283/50260502-30d38800-0443-11e9-9fab-ecb99abb2e4b.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='xiaobengou01' date='2018-12-20T02:41:02Z'>
		ok got it. i think it's same as &lt;denchmark-link:https://github.com/hanxiao/bert-as-service/issues/90&gt;#90&lt;/denchmark-link&gt;
  which happens only on tf 1.12 CPU version. will do a fix.
		</comment>
		<comment id='6' author='xiaobengou01' date='2018-12-20T11:37:05Z'>
		fyi, this issue is fixed in &lt;denchmark-link:https://github.com/hanxiao/bert-as-service/pull/155&gt;#155&lt;/denchmark-link&gt;
 and is available since 1.6.2, please do
pip install -U bert-serving-server bert-serving-client
for the update.
		</comment>
		<comment id='7' author='xiaobengou01' date='2018-12-21T06:55:14Z'>
		after upgrade the version.it works fine now .thank you very much
		</comment>
		<comment id='8' author='xiaobengou01' date='2019-03-23T12:32:38Z'>
		I am still having this issue, even after setting prefetch_size to 0 and using latest version of Bert.
Bert 1.8.6
Python 3.6.8
Ubuntu 16.04, cpu only
Below is the server log
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;$ bert-serving-start -model_dir ./models/cased_L-24_H-1024_A-16 -num_worker=2 -cpu -prefetch_size 0
usage: /home/sphatik/miniconda3/envs/textmining/bin/bert-serving-start -model_dir ./models/cased_L-24_H-1024_A-16 -num_worker=2 -cpu -prefetch_size 0
                 ARG   VALUE
__________________________________________________
           ckpt_name = bert_model.ckpt
         config_name = bert_config.json
                cors = *
                 cpu = True
          device_map = []
  fixed_embed_length = False
                fp16 = False
 gpu_memory_fraction = 0.5
       graph_tmp_dir = None
    http_max_connect = 10
           http_port = None
        mask_cls_sep = False
      max_batch_size = 256
         max_seq_len = 25
           model_dir = ./models/cased_L-24_H-1024_A-16
          num_worker = 2
       pooling_layer = [-2]
    pooling_strategy = REDUCE_MEAN
                port = 5555
            port_out = 5556
       prefetch_size = 0
 priority_batch_size = 16
show_tokens_to_client = False
     tuned_model_dir = None
             verbose = False
                 xla = False

I:VENTILATOR:[__i:__i: 66]:freeze, optimize and export graph, could take a while...
I:GRAPHOPT:[gra:opt: 52]:model config: ./models/cased_L-24_H-1024_A-16/bert_config.json
I:GRAPHOPT:[gra:opt: 55]:checkpoint: ./models/cased_L-24_H-1024_A-16/bert_model.ckpt
I:GRAPHOPT:[gra:opt: 59]:build graph...
I:GRAPHOPT:[gra:opt:128]:load parameters from checkpoint...
I:GRAPHOPT:[gra:opt:132]:optimize...
I:GRAPHOPT:[gra:opt:140]:freeze...
I:GRAPHOPT:[gra:opt:145]:write graph to a tmp file: /tmp/tmpsn_ot94b
I:VENTILATOR:[__i:__i: 74]:optimized graph is stored at: /tmp/tmpsn_ot94b
I:VENTILATOR:[__i:_ru:118]:bind all sockets
I:VENTILATOR:[__i:_ru:122]:open 8 ventilator-worker sockets
I:VENTILATOR:[__i:_ru:125]:start the sink
I:SINK:[__i:_ru:284]:ready
I:VENTILATOR:[__i:_ge:202]:get devices
I:VENTILATOR:[__i:_ge:235]:device map: 
		worker  0 -&gt; cpu
		worker  1 -&gt; cpu
I:WORKER-0:[__i:_ru:492]:use device cpu, load graph from /tmp/tmpsn_ot94b
I:WORKER-1:[__i:_ru:492]:use device cpu, load graph from /tmp/tmpsn_ot94b
I:VENTILATOR:[__i:_ru:176]:new encode request	req id: 3	size: 1	client: b'688a67a7-4a06-4430-9b50-b77d2def7744'
I:SINK:[__i:_ru:331]:job register	size: 1	job id: b'688a67a7-4a06-4430-9b50-b77d2def7744#3'

&lt;/denchmark-code&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Client side:
&lt;denchmark-code&gt;&gt;&gt;&gt; from bert_serving.client import BertClient
&gt;&gt;&gt; bc = BertClient()
&gt;&gt;&gt; bc.encode(['hello there friend'])

&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='xiaobengou01' date='2019-03-27T08:10:13Z'>
		Hi Han - Thanks for your great work. Really appreciate it :)
Though I could install all the requirements,  I am still facing the same as &lt;denchmark-link:https://github.com/kvdesai&gt;@kvdesai&lt;/denchmark-link&gt;
 for the following system config:
Ubuntu v16.04 (CPU)
Python v3.5.2
TensorFlow 1.13.1
bert 1.8.6
The server does not go beyond
I:WORKER-0:[__i:_ru:492]:use device cpu, load graph from 
Below is my full log:
&lt;denchmark-link:https://user-images.githubusercontent.com/16071833/55059548-1c68e200-5067-11e9-86d4-ced5c3147c50.png&gt;&lt;/denchmark-link&gt;

Any help on this is greatly appreciated. Thanks :)
		</comment>
		<comment id='10' author='xiaobengou01' date='2019-11-23T10:21:48Z'>
		Hi Han,
I'm still facing the same issue, even though i'm on latest package of BERT.
Could you check if the problem is really resolved on a CPU only machine ?
		</comment>
	</comments>
</bug>