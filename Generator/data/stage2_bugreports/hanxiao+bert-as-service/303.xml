<bug id='303' author='crazyofapple' open_date='2019-04-01T09:09:31Z' closed_time='2019-04-02T12:12:54Z'>
	<summary>Version 1.8.8 Can't start http server with default args</summary>
	<description>
Prerequisites

Please fill in by replacing [ ] with [x].


 Are you running the latest bert-as-service?
 Did you follow the installation and the usage instructions in README.md?
 Did you check the FAQ list in README.md?
 Did you perform a cursory search on existing issues?

System information

Some of this information can be collected via this script.


OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
TensorFlow installed from (source or binary):
TensorFlow version:
Python version:
bert-as-service version:
GPU model and memory:
CPU model and memory:

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Description&lt;/denchmark-h&gt;

Using a default args in README, the http server is failed. I degrade to v1.8.2, the problem is solved.
bert-serving-start -model_dir=/MY_MODEL -http_port 8125
curl -X POST &lt;denchmark-link:http://0.0.0.0:8125/encode&gt;http://0.0.0.0:8125/encode&lt;/denchmark-link&gt;

-H 'content-type: application/json'
-d '{"id": 123,"texts": ["hello world"], "is_tokenized": false}'
with lsof -i :8125, it's nothing.
...
	</description>
	<comments>
		<comment id='1' author='crazyofapple' date='2019-04-01T13:34:48Z'>
		1.8.8 should work see &lt;denchmark-link:https://github.com/hanxiao/bert-as-service/issues/296&gt;#296&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='crazyofapple' date='2019-04-02T11:28:50Z'>
		same problem here even with 1.8.8
➜  query-embedding curl -X POST &lt;denchmark-link:http://127.0.0.1:8125/encode&gt;http://127.0.0.1:8125/encode&lt;/denchmark-link&gt;
 
-H 'content-type: application/json' 
-d '{"id": 123,"texts": ["hello world"], "is_tokenized": false}'
curl: (7) Failed to connect to 127.0.0.1 port 8125: Connection refused
here is my setup:
(query-embedding) ➜  query-embedding pip freeze
absl-py==0.7.1
astor==0.7.1
bert-serving-client==1.8.8
bert-serving-server==1.8.8
Click==7.0
Flask==1.0.2
Flask-Compress==1.4.0
Flask-Cors==3.0.7
Flask-JSON==0.3.3
gast==0.2.2
GPUtil==1.4.0
grpcio==1.19.0
h5py==2.9.0
itsdangerous==1.1.0
Jinja2==2.10
Keras-Applications==1.0.7
Keras-Preprocessing==1.0.9
Markdown==3.1
MarkupSafe==1.1.1
mock==2.0.0
numpy==1.16.2
pbr==5.1.3
protobuf==3.7.1
pyzmq==18.0.1
six==1.12.0
tensorboard==1.13.1
tensorflow==1.13.1
tensorflow-estimator==1.13.0
termcolor==1.1.0
Werkzeug==0.15.1
You are using pip version 10.0.1, however version 19.0.3 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
(query-embedding) ➜  query-embedding bert-serving-start -model_dir `pwd`/uncased_L-12_H-768_A-12/ -num_worker=2 -http_port 8125
usage: /Users/gla/py/annotateimages/query-embedding/bin/bert-serving-start -model_dir /Users/gla/git/query-embedding/uncased_L-12_H-768_A-12/ -num_worker=2 -http_port 8125
ARG   VALUE
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;       ckpt_name = bert_model.ckpt
     config_name = bert_config.json
            cors = *
             cpu = False
      device_map = []
&lt;/denchmark-code&gt;

fixed_embed_length = False
fp16 = False
gpu_memory_fraction = 0.5
graph_tmp_dir = None
http_max_connect = 10
http_port = 8125
mask_cls_sep = False
max_batch_size = 256
max_seq_len = 25
model_dir = /Users/gla/git/query-embedding/uncased_L-12_H-768_A-12/
num_worker = 2
pooling_layer = [-2]
pooling_strategy = REDUCE_MEAN
port = 5555
port_out = 5556
prefetch_size = 10
priority_batch_size = 16
show_tokens_to_client = False
tuned_model_dir = None
verbose = False
xla = False
I:VENTILATOR:[__i:__i: 66]:freeze, optimize and export graph, could take a while...
I:GRAPHOPT:[gra:opt: 52]:model config: /Users/gla/git/query-embedding/uncased_L-12_H-768_A-12/bert_config.json
I:GRAPHOPT:[gra:opt: 55]:checkpoint: /Users/gla/git/query-embedding/uncased_L-12_H-768_A-12/bert_model.ckpt
I:GRAPHOPT:[gra:opt: 59]:build graph...
WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:

https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
https://github.com/tensorflow/addons
If you depend on functionality not listed there, please file an issue.

I:GRAPHOPT:[gra:opt:128]:load parameters from checkpoint...
I:GRAPHOPT:[gra:opt:132]:optimize...
I:GRAPHOPT:[gra:opt:140]:freeze...
I:GRAPHOPT:[gra:opt:145]:write graph to a tmp file: /var/folders/fp/zb8y8hzn371dwkp2xtgs_8w9tvxd3s/T/tmpdwmqwkgk
I:VENTILATOR:[__i:__i: 74]:optimized graph is stored at: /var/folders/fp/zb8y8hzn371dwkp2xtgs_8w9tvxd3s/T/tmpdwmqwkgk
I:VENTILATOR:[__i:_ru:128]:bind all sockets
I:VENTILATOR:[__i:_ru:132]:open 8 ventilator-worker sockets
I:VENTILATOR:[__i:_ru:135]:start the sink
I:SINK:[__i:_ru:303]:ready
I:VENTILATOR:[__i:_ge:219]:get devices
W:VENTILATOR:[__i:_ge:243]:no GPU available, fall back to CPU
I:VENTILATOR:[__i:_ge:252]:device map:
worker  0 -&gt; cpu
worker  1 -&gt; cpu
I:VENTILATOR:[__i:_ru:151]:start http proxy
I:WORKER-0:[__i:_ru:514]:use device cpu, load graph from /var/folders/fp/zb8y8hzn371dwkp2xtgs_8w9tvxd3s/T/tmpdwmqwkgk
I:WORKER-1:[__i:_ru:514]:use device cpu, load graph from /var/folders/fp/zb8y8hzn371dwkp2xtgs_8w9tvxd3s/T/tmpdwmqwkgk
I:WORKER-0:[__i:gen:542]:ready and listening!
I:WORKER-1:[__i:gen:542]:ready and listening!
		</comment>
		<comment id='3' author='crazyofapple' date='2019-04-02T11:59:19Z'>
		I confirm it's a bug, related to &lt;denchmark-link:https://github.com/hanxiao/bert-as-service/issues/296&gt;#296&lt;/denchmark-link&gt;
 fixing it now
		</comment>
		<comment id='4' author='crazyofapple' date='2019-04-02T12:18:47Z'>
		fyi, this issue is fixed in &lt;denchmark-link:https://github.com/hanxiao/bert-as-service/pull/305&gt;#305&lt;/denchmark-link&gt;
 and the new feature is available since 1.8.9, please do
&lt;denchmark-code&gt;pip install -U bert-serving-server bert-serving-client
&lt;/denchmark-code&gt;

for the update.
When you start the server with -http_port, you should see something like this
&lt;denchmark-link:https://user-images.githubusercontent.com/2041322/55401775-5749a900-5584-11e9-82b3-ee0a924bbf59.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;I:VENTILATOR:[__i:_ru:163]:all set, ready to serve request!
&lt;/denchmark-code&gt;

this means the server is ready.
		</comment>
		<comment id='5' author='crazyofapple' date='2020-05-12T23:04:34Z'>
		Same issue here via docker setup. Using:
bert-serving-server==1.8.9
bert-serving-client==1.8.9
(I did get it working from a non-docker install).
Weird thing is that everything looks good. Health check is running... just can't see the http port.
&lt;denchmark-link:https://user-images.githubusercontent.com/19869387/81755010-721b6c00-946c-11ea-9bfc-4734f52478f5.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/19869387/81754128-3da6b080-946a-11ea-8912-21bf22b72a89.png&gt;&lt;/denchmark-link&gt;

I don't see that "*Serving Flask app" message though...?
If I sh into the docker container, the port responds... hm.
Tried exposing the port in Dockerfile
Also running with: docker run MYID -p localhost:8125:8125
		</comment>
		<comment id='6' author='crazyofapple' date='2020-05-13T00:07:08Z'>
		ooo, i just got the flask message. but still, port is not found...
&lt;denchmark-link:https://user-images.githubusercontent.com/19869387/81757214-fe309200-9472-11ea-9f32-2063131ba612.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='crazyofapple' date='2020-05-13T15:10:26Z'>
		Of course, after many hours of trying I got it working right after posting with:
docker run -p 8000:8125 MYID
		</comment>
	</comments>
</bug>