<bug id='40' author='lucyboll' open_date='2018-11-22T07:56:58Z' closed_time='2018-11-23T14:06:57Z'>
	<summary>character embedding or word embedding?</summary>
	<description>
Hello, thanks for your service, it is very useful. I notice that the word embedding is obtained for character 'h' rather than word 'hey' as follows. It seems like doesn't match with bert tokenizer.
`bc = BertClient()
x = ['hey you', 'whats up']
bc.encode(x)  # [2, 25, 768]
bc.encode(x)[0]  # [1, 25, 768], word embeddings for hey you
bc.encode(x)[0][0]  # [1, 1, 768], word embedding for [CLS]
bc.encode(x)[0][1]  # [1, 1, 768], word embedding for h
bc.encode(x)[0][8]  # [1, 1, 768], word embedding for [SEP]
bc.encode(x)[0][9]  # [1, 1, 768], word embedding for 0_PAD, meaningless
bc.encode(x)[0][25]  # error, out of index!`
	</description>
	<comments>
		<comment id='1' author='lucyboll' date='2018-11-22T08:40:25Z'>
		I am curious, how did you know the embedding correspond to which word / character ?
		</comment>
		<comment id='2' author='lucyboll' date='2018-11-22T09:09:29Z'>
		I want to know why the 0_PAD matrix is meaningless
		</comment>
		<comment id='3' author='lucyboll' date='2018-11-22T10:27:08Z'>
		How can I obtain word embedding instead of character embedding please?
		</comment>
		<comment id='4' author='lucyboll' date='2018-11-22T12:52:59Z'>
		I have a question. Where to set max_seq_len and pooling_strategy?
		</comment>
		<comment id='5' author='lucyboll' date='2018-11-22T15:19:54Z'>
		&lt;denchmark-link:https://github.com/lucyboll&gt;@lucyboll&lt;/denchmark-link&gt;
 thanks for pointing it out, i believe it's a bug. will fix it
		</comment>
		<comment id='6' author='lucyboll' date='2018-11-23T13:42:34Z'>
		&lt;denchmark-link:https://github.com/hanxiao&gt;@hanxiao&lt;/denchmark-link&gt;
 may this issue be related to &lt;denchmark-link:https://github.com/google-research/bert/issues/164&gt;google-research/bert#164&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='lucyboll' date='2018-11-23T14:19:42Z'>
		&lt;denchmark-link:https://github.com/lucyboll&gt;@lucyboll&lt;/denchmark-link&gt;
 I fix readme by providing more explanation for tokenization, see
&lt;denchmark-link:https://github.com/hanxiao/bert-as-service#q-how-can-i-get-word-embedding-instead-of-sentence-embedding&gt;https://github.com/hanxiao/bert-as-service#q-how-can-i-get-word-embedding-instead-of-sentence-embedding&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/hanxiao/bert-as-service#q-do-i-need-to-do-segmentation-for-chinese&gt;https://github.com/hanxiao/bert-as-service#q-do-i-need-to-do-segmentation-for-chinese&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='lucyboll' date='2018-11-23T14:23:39Z'>
		&lt;denchmark-link:https://github.com/RuiMao1988&gt;@RuiMao1988&lt;/denchmark-link&gt;
 If you are using Chinese BERT released by Google,  word embedding is character embedding, as this Chinese BERT is a character-based model.
		</comment>
	</comments>
</bug>