<bug id='2272' author='YiKuanLiu' open_date='2020-04-06T10:14:17Z' closed_time='2020-04-24T06:16:24Z'>
	<summary>ProxylessNAS: Getting "None" of grad value of arch_path_wb in ZeroLayer.</summary>
	<description>
Short summary about the issue/question: ProxylessNAS: Getting "None" of grad value of arch_path_wb in ZeroLayer.
Brief what process you are following:
I'm trying to customize a Neural Networks with ProxylessNAS to deal with a classification problem.
So I copy the example code and set num_classes=2 and other small details, but none of the main structure are verified.
However, I get the error msg: AttributeError: 'NoneType' object has no attribute 'data', that happened while trying to call the method "self.mutator.set_arch_param_grad()" in trainer.py.
Interestingly, I found that the it's because the issue happened when the mutator.ap_path_wb is [0,0,0,0,0,0,1], and it's grad is None.  It means the problem is related to the ZeroLayer.
Please tell me how to fix the bug!  Many thanks.
How to reproduce it:
nni Environment:

nni version: 1.4
nni mode(local|pai|remote): local
OS: linux, ubuntu 18.04
python version: 3.6.10
is conda or virtualenv used?:  conda used
is running in docker?: no

need to update document(yes/no): yes
Anything else we need to know: no
	</description>
	<comments>
		<comment id='1' author='YiKuanLiu' date='2020-04-07T12:18:19Z'>
		&lt;denchmark-link:https://github.com/YiKuanLiu&gt;@YiKuanLiu&lt;/denchmark-link&gt;
 thanks for reporting this issue. Could you provide detailed error message, such as, in which line this error message is raised. I guess it is in this line &lt;denchmark-link:https://github.com/microsoft/nni/blob/master/src/sdk/pynni/nni/nas/pytorch/proxylessnas/mutator.py#L246&gt;https://github.com/microsoft/nni/blob/master/src/sdk/pynni/nni/nas/pytorch/proxylessnas/mutator.py#L246&lt;/denchmark-link&gt;
?
And, it would be very helpful if you could share the code that can reproduce this problem.
		</comment>
		<comment id='2' author='YiKuanLiu' date='2020-04-09T05:24:45Z'>
		&lt;denchmark-link:https://github.com/QuanluZhang&gt;@QuanluZhang&lt;/denchmark-link&gt;
 Thanks for replying.  Exactly the line you've mentioned:
In the mutator.py

Actually, I've just define my own model, data provider, main and other minor adjustments.
So if you consider the problem is not in the code framework, I'll check again my dataset definition.
BTW, I've checked that if I comment the line in model.py:
op_candidates += [ops.OPS['Zero'](input_channel, width, stride)]
The program is doing well, but that will be not the ProxylessNAS anymore.
A batch(size=64) of my data is like:
img: torch.Size([64, 3, 224, 224])
label: tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0])
The error message is here:
Traceback (most recent call last):
File "/home/iis/NNI/main.py", line 95, in 
trainer.train()
File "/home/iis/.local/lib/python3.6/site-packages/nni/nas/pytorch/proxylessnas/trainer.py", line 485, in train
self._train()
File "/home/iis/.local/lib/python3.6/site-packages/nni/nas/pytorch/proxylessnas/trainer.py", line 358, in _train
arch_loss, exp_value = self._gradient_step()
File "/home/iis/.local/lib/python3.6/site-packages/nni/nas/pytorch/proxylessnas/trainer.py", line 438, in _gradient_step
self.mutator.set_arch_param_grad()
File "/home/iis/.local/lib/python3.6/site-packages/nni/nas/pytorch/proxylessnas/mutator.py", line 382, in set_arch_param_grad
mutable.registered_module.set_arch_param_grad(mutable)
File "/home/iis/.local/lib/python3.6/site-packages/nni/nas/pytorch/proxylessnas/mutator.py", line 251, in set_arch_param_grad
binary_grads = self.ap_path_wb.grad.data
AttributeError: 'NoneType' object has no attribute 'data'
My codes are below:
trainer.py
&lt;denchmark-h:h1&gt;Copyright (c) Microsoft Corporation.&lt;/denchmark-h&gt;

&lt;denchmark-h:h1&gt;Licensed under the MIT license.&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import math
import time
import json
import logging

import torch
from torch import nn as nn

from nni.nas.pytorch.base_trainer import BaseTrainer
from nni.nas.pytorch.trainer import TorchTensorEncoder
from nni.nas.pytorch.utils import AverageMeter
from .mutator import ProxylessNasMutator
from .utils import cross_entropy_with_label_smoothing, accuracy

logger = logging.getLogger(__name__)

class ProxylessNasTrainer(BaseTrainer):
    def __init__(self, model, model_optim, device,
                 train_loader, valid_loader, label_smoothing=0.1,
                 n_epochs=120, init_lr=0.025, binary_mode='full_v2',
                 arch_init_type='normal', arch_init_ratio=1e-3,
                 arch_optim_lr=1e-3, arch_weight_decay=0,
                 grad_update_arch_param_every=5, grad_update_steps=1,
                 warmup=True, warmup_epochs=25,
                 arch_valid_frequency=1,
                 load_ckpt=False, ckpt_path=None, arch_path=None):
        """
        Parameters
        ----------
        model : pytorch model
            the user model, which has mutables
        model_optim : pytorch optimizer
            the user defined optimizer
        device : pytorch device
            the devices to train/search the model
        train_loader : pytorch data loader
            data loader for the training set
        valid_loader : pytorch data loader
            data loader for the validation set
        label_smoothing : float
            for label smoothing
        n_epochs : int
            number of epochs to train/search
        init_lr : float
            init learning rate for training the model
        binary_mode : str
            the forward/backward mode for the binary weights in mutator
        arch_init_type : str
            the way to init architecture parameters
        arch_init_ratio : float
            the ratio to init architecture parameters
        arch_optim_lr : float
            learning rate of the architecture parameters optimizer
        arch_weight_decay : float
            weight decay of the architecture parameters optimizer
        grad_update_arch_param_every : int
            update architecture weights every this number of minibatches
        grad_update_steps : int
            during each update of architecture weights, the number of steps to train
        warmup : bool
            whether to do warmup
        warmup_epochs : int
            the number of epochs to do during warmup
        arch_valid_frequency : int
            frequency of printing validation result
        load_ckpt : bool
            whether load checkpoint
        ckpt_path : str
            checkpoint path, if load_ckpt is True, ckpt_path cannot be None
        arch_path : str
            the path to store chosen architecture
        """
        self.model = model
        self.model_optim = model_optim
        self.train_loader = train_loader
        self.valid_loader = valid_loader
        self.device = device
        self.n_epochs = n_epochs
        self.init_lr = init_lr
        self.warmup = warmup
        self.warmup_epochs = warmup_epochs
        self.arch_valid_frequency = arch_valid_frequency
        self.label_smoothing = label_smoothing

        self.train_batch_size = train_loader.batch_sampler.batch_size
        self.valid_batch_size = valid_loader.batch_sampler.batch_size
        # update architecture parameters every this number of minibatches
        self.grad_update_arch_param_every = grad_update_arch_param_every
        # the number of steps per architecture parameter update
        self.grad_update_steps = grad_update_steps
        self.binary_mode = binary_mode

        self.load_ckpt = load_ckpt
        self.ckpt_path = ckpt_path
        self.arch_path = arch_path

        # init mutator
        self.mutator = ProxylessNasMutator(model)

        # DataParallel should be put behind the init of mutator
        self.model = torch.nn.DataParallel(self.model)
        self.model.to(self.device)

        # iter of valid dataset for training architecture weights
        self._valid_iter = None
        # init architecture weights
        self._init_arch_params(arch_init_type, arch_init_ratio)
        # build architecture optimizer
        self.arch_optimizer = torch.optim.Adam(self.mutator.get_architecture_parameters(),
                                               arch_optim_lr,
                                               weight_decay=arch_weight_decay,
                                               betas=(0, 0.999),
                                               eps=1e-8)

        self.criterion = nn.CrossEntropyLoss()
        self.warmup_curr_epoch = 0
        self.train_curr_epoch = 0
        self.k = 2 # edit here, k means the top k accuracy of classes are recorded, should &lt; classes

    def _init_arch_params(self, init_type='normal', init_ratio=1e-3):
        """
        Initialize architecture weights
        """
        for param in self.mutator.get_architecture_parameters():
            if init_type == 'normal':
                param.data.normal_(0, init_ratio)
            elif init_type == 'uniform':
                param.data.uniform_(-init_ratio, init_ratio)
            else:
                raise NotImplementedError

    def _validate(self):
        """
        Do validation. During validation, LayerChoices use the chosen active op.

        Returns
        -------
        float, float, float
            average loss, average top1 accuracy, average top5 accuracy
        """
        self.valid_loader.batch_sampler.batch_size = self.valid_batch_size
        self.valid_loader.batch_sampler.drop_last = False

        self.mutator.set_chosen_op_active()
        # remove unused modules to save memory
        self.mutator.unused_modules_off()
        # test on validation set under train mode
        self.model.train()
        batch_time = AverageMeter('batch_time')
        losses = AverageMeter('losses')
        top1 = AverageMeter('top1')
        top_k = AverageMeter('top_k')
        end = time.time()
        with torch.no_grad():
            for i, (images, labels) in enumerate(self.valid_loader):
                images, labels = images.to(self.device), labels.to(self.device)
                output = self.model(images)
                loss = self.criterion(output, labels)
                acc1, acck = accuracy(output, labels, topk=(1, self.k))
                losses.update(loss, images.size(0))
                top1.update(acc1[0], images.size(0))
                top_k.update(acck[0], images.size(0))
                # measure elapsed time
                batch_time.update(time.time() - end)
                end = time.time()

                if i % 10 == 0 or i + 1 == len(self.valid_loader):
                    test_log = 'Valid' + ': [{0}/{1}]\t'\
                                        'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'\
                                        'Loss {loss.val:.4f} ({loss.avg:.4f})\t'\
                                        'Top-1 acc {top1.val:.3f} ({top1.avg:.3f})'.\
                        format(i, len(self.valid_loader) - 1, batch_time=batch_time, loss=losses, top1=top1)
                    # return top5:
                    test_log += '\tTop-k acc {top_k.val:.3f} ({top_k.avg:.3f})'.format(top_k=top_k)
                    logger.info(test_log)
        self.mutator.unused_modules_back()
        return losses.avg, top1.avg, top_k.avg

    def _warm_up(self):
        """
        Warm up the model, during warm up, architecture weights are not trained.
        """
        lr_max = 0.05
        data_loader = self.train_loader
        nBatch = len(data_loader)
        T_total = self.warmup_epochs * nBatch # total num of batches

        for epoch in range(self.warmup_curr_epoch, self.warmup_epochs):
            logger.info('\n--------Warmup epoch: %d--------\n', epoch + 1)
            batch_time = AverageMeter('batch_time')
            data_time = AverageMeter('data_time')
            losses = AverageMeter('losses')
            top1 = AverageMeter('top1')
            top_k = AverageMeter('top_k')
            # switch to train mode
            self.model.train()

            end = time.time()
            logger.info('warm_up epoch: %d', epoch)
            for i, (images, labels) in enumerate(data_loader):
                data_time.update(time.time() - end)
                # lr
                T_cur = epoch * nBatch + i
                warmup_lr = 0.5 * lr_max * (1 + math.cos(math.pi * T_cur / T_total))
                for param_group in self.model_optim.param_groups:
                    param_group['lr'] = warmup_lr
                images, labels = images.to(self.device), labels.to(self.device)
                # compute output
                self.mutator.reset_binary_gates() # random sample binary gates
                self.mutator.unused_modules_off() # remove unused module for speedup
                output = self.model(images)
                if self.label_smoothing &gt; 0:
                    loss = cross_entropy_with_label_smoothing(output, labels, self.label_smoothing)
                else:
                    loss = self.criterion(output, labels)
                # measure accuracy and record loss
                acc1, acck = accuracy(output, labels, topk=(1, self.k))
                losses.update(loss, images.size(0))
                top1.update(acc1[0], images.size(0))
                top_k.update(acck[0], images.size(0))
                # compute gradient and do SGD step
                self.model.zero_grad()
                loss.backward()
                self.model_optim.step()
                # unused modules back
                self.mutator.unused_modules_back()
                # measure elapsed time
                batch_time.update(time.time() - end)
                end = time.time()

                if i % 10 == 0 or i + 1 == nBatch:
                    batch_log = 'Warmup Train [{0}][{1}/{2}]\t' \
                                'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t' \
                                'Data {data_time.val:.3f} ({data_time.avg:.3f})\t' \
                                'Loss {losses.val:.4f} ({losses.avg:.4f})\t' \
                                'Top-1 acc {top1.val:.3f} ({top1.avg:.3f})\t' \
                                'Top-k acc {top_k.val:.3f} ({top_k.avg:.3f})\tlr {lr:.5f}'. \
                        format(epoch + 1, i, nBatch - 1, batch_time=batch_time, data_time=data_time,
                               losses=losses, top1=top1, top_k=top_k, lr=warmup_lr)
                    logger.info(batch_log)
            val_loss, val_top1, val_top_k = self._validate()
            val_log = 'Warmup Valid [{0}/{1}]\tloss {2:.3f}\ttop-1 acc {3:.3f}\ttop-k acc {4:.3f}\t' \
                      'Train top-1 {top1.avg:.3f}\ttop-k {top_k.avg:.3f}M'. \
                format(epoch + 1, self.warmup_epochs, val_loss, val_top1, val_top_k, top1=top1, top_k=top_k)
            logger.info(val_log)
            self.save_checkpoint()
            self.warmup_curr_epoch += 1

    def _get_update_schedule(self, nBatch):
        """
        Generate schedule for training architecture weights. Key means after which minibatch
        to update architecture weights, value means how many steps for the update.

        Parameters
        ----------
        nBatch : int
            the total number of minibatches in one epoch

        Returns
        -------
        dict
            the schedule for updating architecture weights
        """
        schedule = {}
        for i in range(nBatch):
            if (i + 1) % self.grad_update_arch_param_every == 0:
                schedule[i] = self.grad_update_steps
        return schedule

    def _calc_learning_rate(self, epoch, batch=0, nBatch=None):
        """
        Update learning rate.
        """
        T_total = self.n_epochs * nBatch
        T_cur = epoch * nBatch + batch
        lr = 0.5 * self.init_lr * (1 + math.cos(math.pi * T_cur / T_total))
        return lr

    def _adjust_learning_rate(self, optimizer, epoch, batch=0, nBatch=None):
        """
        Adjust learning of a given optimizer and return the new learning rate

        Parameters
        ----------
        optimizer : pytorch optimizer
            the used optimizer
        epoch : int
            the current epoch number
        batch : int
            the current minibatch
        nBatch : int
            the total number of minibatches in one epoch

        Returns
        -------
        float
            the adjusted learning rate
        """
        new_lr = self._calc_learning_rate(epoch, batch, nBatch)
        for param_group in optimizer.param_groups:
            param_group['lr'] = new_lr
        return new_lr

    def _train(self):
        """
        Train the model, it trains model weights and architecute weights.
        Architecture weights are trained according to the schedule.
        Before updating architecture weights, ```requires_grad``` is enabled.
        Then, it is disabled after the updating, in order not to update
        architecture weights when training model weights.
        """
        nBatch = len(self.train_loader)
        arch_param_num = self.mutator.num_arch_params()
        binary_gates_num = self.mutator.num_arch_params()
        logger.info('#arch_params: %d\t#binary_gates: %d', arch_param_num, binary_gates_num)

        update_schedule = self._get_update_schedule(nBatch)

        for epoch in range(self.train_curr_epoch, self.n_epochs):
            logger.info('\n--------Train epoch: %d--------\n', epoch + 1)
            batch_time = AverageMeter('batch_time')
            data_time = AverageMeter('data_time')
            losses = AverageMeter('losses')
            top1 = AverageMeter('top1')
            top_k = AverageMeter('top_k')
            # switch to train mode
            self.model.train()

            end = time.time()
            for i, (images, labels) in enumerate(self.train_loader):
                data_time.update(time.time() - end)
                lr = self._adjust_learning_rate(self.model_optim, epoch, batch=i, nBatch=nBatch)
                # train weight parameters
                images, labels = images.to(self.device), labels.to(self.device)
                self.mutator.reset_binary_gates()
                self.mutator.unused_modules_off()
                output = self.model(images)
                if self.label_smoothing &gt; 0:
                    loss = cross_entropy_with_label_smoothing(output, labels, self.label_smoothing)
                else:
                    loss = self.criterion(output, labels)
                acc1, acck = accuracy(output, labels, topk=(1, self.k))
                losses.update(loss, images.size(0))
                top1.update(acc1[0], images.size(0))
                top_k.update(acck[0], images.size(0))
                self.model.zero_grad()
                loss.backward()
                self.model_optim.step()
                self.mutator.unused_modules_back()
                if epoch &gt; 0:
                    for _ in range(update_schedule.get(i, 0)):
                        start_time = time.time()
                        # GradientArchSearchConfig
                        self.mutator.arch_requires_grad()
                        arch_loss, exp_value = self._gradient_step()
                        self.mutator.arch_disable_grad()
                        used_time = time.time() - start_time
                        log_str = 'Architecture [%d-%d]\t Time %.4f\t Loss %.4f\t null %s' % \
                                    (epoch + 1, i, used_time, arch_loss, exp_value)
                        logger.info(log_str)
                batch_time.update(time.time() - end)
                end = time.time()
                # training log
                if i % 10 == 0 or i + 1 == nBatch:
                    batch_log = 'Train [{0}][{1}/{2}]\t' \
                                'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t' \
                                'Data Time {data_time.val:.3f} ({data_time.avg:.3f})\t' \
                                'Loss {losses.val:.4f} ({losses.avg:.4f})\t' \
                                'Top-1 acc {top1.val:.3f} ({top1.avg:.3f})\t' \
                                'Top-k acc {top_k.val:.3f} ({top_k.avg:.3f})\tlr {lr:.5f}'. \
                        format(epoch + 1, i, nBatch - 1, batch_time=batch_time, data_time=data_time,
                               losses=losses, top1=top1, top_k=top_k, lr=lr)
                    logger.info(batch_log)
            # validate
            if (epoch + 1) % self.arch_valid_frequency == 0:
                val_loss, val_top1, val_top_k = self._validate()
                val_log = 'Valid [{0}]\tloss {1:.3f}\ttop-1 acc {2:.3f} \ttop-k acc {3:.3f}\t' \
                          'Train top-1 {top1.avg:.3f}\ttop-k {top_k.avg:.3f}'. \
                    format(epoch + 1, val_loss, val_top1, val_top_k, top1=top1, top_k=top_k)
                logger.info(val_log)
            self.save_checkpoint()
            self.train_curr_epoch += 1

    def _valid_next_batch(self):
        """
        Get next one minibatch from validation set

        Returns
        -------
        (tensor, tensor)
            the tuple of images and labels
        """
        if self._valid_iter is None:
            self._valid_iter = iter(self.valid_loader)
        try:
            data = next(self._valid_iter)
        except StopIteration:
            self._valid_iter = iter(self.valid_loader)
            data = next(self._valid_iter)
        return data

    def _gradient_step(self):
        """
        This gradient step is for updating architecture weights.
        Mutator is intensively used in this function to operate on
        architecture weights.

        Returns
        -------
        float, None
            loss of the model, None
        """
        # use the same batch size as train batch size for architecture weights
        self.valid_loader.batch_sampler.batch_size = self.train_batch_size
        self.valid_loader.batch_sampler.drop_last = True
        self.model.train()
        self.mutator.change_forward_mode(self.binary_mode)
        time1 = time.time()  # time
        # sample a batch of data from validation set
        images, labels = self._valid_next_batch()
        images, labels = images.to(self.device), labels.to(self.device)
        time2 = time.time()  # time
        self.mutator.reset_binary_gates()
        self.mutator.unused_modules_off()
        output = self.model(images)
        time3 = time.time()
        ce_loss = self.criterion(output, labels)
        expected_value = None
        loss = ce_loss
        self.model.zero_grad()
        print("trainer, loss.backward()--------------------------------------------------------1")
        loss.backward()
        print("trainer, loss.backward()--------------------------------------------------------2")

        self.mutator.set_arch_param_grad()
        self.arch_optimizer.step()
        if self.mutator.get_forward_mode() == 'two':
            self.mutator.rescale_updated_arch_param()
        self.mutator.unused_modules_back()
        self.mutator.change_forward_mode(None)
        time4 = time.time()
        logger.info('(%.4f, %.4f, %.4f)', time2 - time1, time3 - time2, time4 - time3)
        return loss.data.item(), expected_value.item() if expected_value is not None else None

    def save_checkpoint(self):
        """
        Save checkpoint of the whole model. Saving model weights and architecture weights in
        ```ckpt_path```, and saving currently chosen architecture in ```arch_path```.
        """
        if self.ckpt_path:
            state = {
                'warmup_curr_epoch': self.warmup_curr_epoch,
                'train_curr_epoch': self.train_curr_epoch,
                'model': self.model.state_dict(),
                'optim': self.model_optim.state_dict(),
                'arch_optim': self.arch_optimizer.state_dict()
            }
            torch.save(state, self.ckpt_path)
        if self.arch_path:
            self.export(self.arch_path)

    def load_checkpoint(self):
        """
        Load the checkpoint from ```ckpt_path```.
        """
        assert self.ckpt_path is not None, "If load_ckpt is not None, ckpt_path should not be None"
        ckpt = torch.load(self.ckpt_path)
        self.warmup_curr_epoch = ckpt['warmup_curr_epoch']
        self.train_curr_epoch = ckpt['train_curr_epoch']
        self.model.load_state_dict(ckpt['model'])
        self.model_optim.load_state_dict(ckpt['optim'])
        self.arch_optimizer.load_state_dict(ckpt['arch_optim'])

    def train(self):
        """
        Train the whole model.
        """
        if self.load_ckpt:
            self.load_checkpoint()
        if self.warmup:
            self._warm_up()
        self._train()

    def export(self, file_name):
        """
        Export the chosen architecture into a file

        Parameters
        ----------
        file_name : str
            the file that stores exported chosen architecture
        """
        exported_arch = self.mutator.sample_final()
        with open(file_name, 'w') as f:
            json.dump(exported_arch, f, indent=2, sort_keys=True, cls=TorchTensorEncoder)

    def validate(self):
        raise NotImplementedError

    def checkpoint(self):
        raise NotImplementedError
&lt;/denchmark-code&gt;

mutator.py
&lt;denchmark-h:h1&gt;Copyright (c) Microsoft Corporation.&lt;/denchmark-h&gt;

&lt;denchmark-h:h1&gt;Licensed under the MIT license.&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import math
import torch
from torch import nn as nn
from torch.nn import functional as F
import numpy as np

from nni.nas.pytorch.base_mutator import BaseMutator
from nni.nas.pytorch.mutables import LayerChoice
from .utils import detach_variable

class ArchGradientFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, binary_gates, run_func, backward_func):
        ctx.run_func = run_func
        ctx.backward_func = backward_func

        detached_x = detach_variable(x)
        with torch.enable_grad():
            output = run_func(detached_x)
        ctx.save_for_backward(detached_x, output)
        return output.data

    @staticmethod
    def backward(ctx, grad_output):
        detached_x, output = ctx.saved_tensors

        grad_x = torch.autograd.grad(output, detached_x, grad_output, only_inputs=True)
        # compute gradients w.r.t. binary_gates
        binary_grads = ctx.backward_func(detached_x.data, output.data, grad_output.data)
        print("backward: binary gards=",binary_grads)
        return grad_x[0], binary_grads, None, None

class MixedOp(nn.Module):
    """
    This class is to instantiate and manage info of one LayerChoice.
    It includes architecture weights, binary weights, and member functions
    operating the weights.

    forward_mode:
        forward/backward mode for LayerChoice: None, two, full, and full_v2.
        For training architecture weights, we use full_v2 by default, and for training
        model weights, we use None.
    """
    forward_mode = None
    def __init__(self, mutable):
        """
        Parameters
        ----------
        mutable : LayerChoice
            A LayerChoice in user model
        """
        super(MixedOp, self).__init__()
        self.ap_path_alpha = nn.Parameter(torch.Tensor(mutable.length))
        self.ap_path_wb = nn.Parameter(torch.Tensor(mutable.length))
        self.ap_path_alpha.requires_grad = False
        self.ap_path_wb.requires_grad = False
        self.active_index = [0]
        self.inactive_index = None
        self.log_prob = None
        self.current_prob_over_ops = None
        self.n_choices = mutable.length

    def get_ap_path_alpha(self):
        return self.ap_path_alpha

    def to_requires_grad(self):
        self.ap_path_alpha.requires_grad = True
        self.ap_path_wb.requires_grad = True

    def to_disable_grad(self):
        self.ap_path_alpha.requires_grad = False
        self.ap_path_wb.requires_grad = False

    def forward(self, mutable, x):
        """
        Define forward of LayerChoice. For 'full_v2', backward is also defined.
        The 'two' mode is explained in section 3.2.1 in the paper.
        The 'full_v2' mode is explained in Appendix D in the paper.

        Parameters
        ----------
        mutable : LayerChoice
            this layer's mutable
        x : tensor
            inputs of this layer, only support one input

        Returns
        -------
        output: tensor
            output of this layer
        """
        if MixedOp.forward_mode == 'full' or MixedOp.forward_mode == 'two':
            output = 0
            for _i in self.active_index:
                oi = self.candidate_ops[_i](x)
                output = output + self.ap_path_wb[_i] * oi
            for _i in self.inactive_index:
                oi = self.candidate_ops[_i](x)
                output = output + self.ap_path_wb[_i] * oi.detach()
        elif MixedOp.forward_mode == 'full_v2':
            def run_function(key, candidate_ops, active_id):
                def forward(_x):
                    return candidate_ops[active_id](_x)
                return forward

            def backward_function(key, candidate_ops, active_id, binary_gates):
                def backward(_x, _output, grad_output):
                    binary_grads = torch.zeros_like(binary_gates.data)
                    # print("backward: active id = ", active_id, ", binary gate=", binary_gates)
                    with torch.no_grad():
                        for k in range(len(candidate_ops)):
                            if k != active_id:
                                out_k = candidate_ops[k](_x.data)
                            else:
                                out_k = _output.data
                            grad_k = torch.sum(out_k * grad_output)
                            # print("grad_k:", grad_k)
                            binary_grads[k] = grad_k
                            # print("binary_grads:", binary_grads)
                    return binary_grads
                return backward
            output = ArchGradientFunction.apply(
                x, self.ap_path_wb, run_function(mutable.key, mutable.choices, self.active_index[0]),
                backward_function(mutable.key, mutable.choices, self.active_index[0], self.ap_path_wb))
        else:
            output = self.active_op(mutable)(x)
        return output

    @property
    def probs_over_ops(self):
        """
        Apply softmax on alpha to generate probability distribution

        Returns
        -------
        pytorch tensor
            probability distribution
        """
        probs = F.softmax(self.ap_path_alpha, dim=0)  # softmax to probability
        return probs

    @property
    def chosen_index(self):
        """
        choose the op with max prob

        Returns
        -------
        int
            index of the chosen one
        numpy.float32
            prob of the chosen one
        """
        probs = self.probs_over_ops.data.cpu().numpy()
        index = int(np.argmax(probs))
        return index, probs[index]

    def active_op(self, mutable):
        """
        assume only one path is active

        Returns
        -------
        PyTorch module
            the chosen operation
        """
        return mutable.choices[self.active_index[0]]

    @property
    def active_op_index(self):
        """
        return active op's index, the active op is sampled

        Returns
        -------
        int
            index of the active op
        """
        return self.active_index[0]

    def set_chosen_op_active(self):
        """
        set chosen index, active and inactive indexes
        """
        chosen_idx, _ = self.chosen_index
        self.active_index = [chosen_idx]
        self.inactive_index = [_i for _i in range(0, chosen_idx)] + \
                              [_i for _i in range(chosen_idx + 1, self.n_choices)]

    def binarize(self, mutable):
        """
        Sample based on alpha, and set binary weights accordingly.
        ap_path_wb is set in this function, which is called binarize.

        Parameters
        ----------
        mutable : LayerChoice
            this layer's mutable
        """
        self.log_prob = None
        # reset binary gates
        self.ap_path_wb.data.zero_()
        probs = self.probs_over_ops
        if MixedOp.forward_mode == 'two':
            # sample two ops according to probs
            sample_op = torch.multinomial(probs.data, 2, replacement=False)
            probs_slice = F.softmax(torch.stack([
                self.ap_path_alpha[idx] for idx in sample_op
            ]), dim=0)
            self.current_prob_over_ops = torch.zeros_like(probs)
            for i, idx in enumerate(sample_op):
                self.current_prob_over_ops[idx] = probs_slice[i]
            # choose one to be active and the other to be inactive according to probs_slice
            c = torch.multinomial(probs_slice.data, 1)[0] # 0 or 1
            active_op = sample_op[c].item()
            inactive_op = sample_op[1-c].item()
            self.active_index = [active_op]
            self.inactive_index = [inactive_op]
            # set binary gate
            self.ap_path_wb.data[active_op] = 1.0
        else:
            sample = torch.multinomial(probs, 1)[0].item()
            self.active_index = [sample]
            self.inactive_index = [_i for _i in range(0, sample)] + \
                                [_i for _i in range(sample + 1, len(mutable.choices))]
            self.log_prob = torch.log(probs[sample])
            self.current_prob_over_ops = probs
            self.ap_path_wb.data[sample] = 1.0
        # avoid over-regularization
        for choice in mutable.choices:
            for _, param in choice.named_parameters():
                param.grad = None

    @staticmethod
    def delta_ij(i, j):
        if i == j:
            return 1
        else:
            return 0

    def set_arch_param_grad(self, mutable):
        """
        Calculate alpha gradient for this LayerChoice.
        It is calculated using gradient of binary gate, probs of ops.
        """
        # print("set_arch_param_grad: the ap path wb = ", self.ap_path_wb)
        # print("set_arch_param_grad: the ap path wb.grad = ", self.ap_path_wb.grad)
        binary_grads = self.ap_path_wb.grad.data
        if self.active_op(mutable).is_zero_layer():
            self.ap_path_alpha.grad = None
            return
        if self.ap_path_alpha.grad is None:
            self.ap_path_alpha.grad = torch.zeros_like(self.ap_path_alpha.data)
        if MixedOp.forward_mode == 'two':
            involved_idx = self.active_index + self.inactive_index
            probs_slice = F.softmax(torch.stack([
                self.ap_path_alpha[idx] for idx in involved_idx
            ]), dim=0).data
            for i in range(2):
                for j in range(2):
                    origin_i = involved_idx[i]
                    origin_j = involved_idx[j]
                    self.ap_path_alpha.grad.data[origin_i] += \
                        binary_grads[origin_j] * probs_slice[j] * (MixedOp.delta_ij(i, j) - probs_slice[i])
            for _i, idx in enumerate(self.active_index):
                self.active_index[_i] = (idx, self.ap_path_alpha.data[idx].item())
            for _i, idx in enumerate(self.inactive_index):
                self.inactive_index[_i] = (idx, self.ap_path_alpha.data[idx].item())
        else:
            probs = self.probs_over_ops.data
            for i in range(self.n_choices):
                for j in range(self.n_choices):
                    self.ap_path_alpha.grad.data[i] += binary_grads[j] * probs[j] * (MixedOp.delta_ij(i, j) - probs[i])
        return

    def rescale_updated_arch_param(self):
        """
        rescale architecture weights for the 'two' mode.
        """
        if not isinstance(self.active_index[0], tuple):
            assert self.active_op.is_zero_layer()
            return
        involved_idx = [idx for idx, _ in (self.active_index + self.inactive_index)]
        old_alphas = [alpha for _, alpha in (self.active_index + self.inactive_index)]
        new_alphas = [self.ap_path_alpha.data[idx] for idx in involved_idx]

        offset = math.log(
            sum([math.exp(alpha) for alpha in new_alphas]) / sum([math.exp(alpha) for alpha in old_alphas])
        )

        for idx in involved_idx:
            self.ap_path_alpha.data[idx] -= offset


class ProxylessNasMutator(BaseMutator):
    """
    This mutator initializes and operates all the LayerChoices of the input model.
    It is for the corresponding trainer to control the training process of LayerChoices,
    coordinating with whole training process.
    """
    def __init__(self, model):
        """
        Init a MixedOp instance for each mutable i.e., LayerChoice.
        And register the instantiated MixedOp in corresponding LayerChoice.
        If does not register it in LayerChoice, DataParallel does not work then,
        because architecture weights are not included in the DataParallel model.
        When MixedOPs are registered, we use ```requires_grad``` to control
        whether calculate gradients of architecture weights.

        Parameters
        ----------
        model : pytorch model
            The model that users want to tune, it includes search space defined with nni nas apis
        """
        super(ProxylessNasMutator, self).__init__(model)
        self._unused_modules = None
        self.mutable_list = []
        for mutable in self.undedup_mutables:
            self.mutable_list.append(mutable)
            mutable.registered_module = MixedOp(mutable)

    def on_forward_layer_choice(self, mutable, *inputs):
        """
        Callback of layer choice forward. This function defines the forward
        logic of the input mutable. So mutable is only interface, its real
        implementation is defined in mutator.

        Parameters
        ----------
        mutable: LayerChoice
            forward logic of this input mutable
        inputs: list of torch.Tensor
            inputs of this mutable

        Returns
        -------
        torch.Tensor
            output of this mutable, i.e., LayerChoice
        int
            index of the chosen op
        """
        # FIXME: return mask, to be consistent with other algorithms
        idx = mutable.registered_module.active_op_index
        return mutable.registered_module(mutable, *inputs), idx

    def reset_binary_gates(self):
        """
        For each LayerChoice, binarize binary weights
        based on alpha to only activate one op.
        It traverses all the mutables in the model to do this.
        """
        for mutable in self.undedup_mutables:
            mutable.registered_module.binarize(mutable)

    def set_chosen_op_active(self):
        """
        For each LayerChoice, set the op with highest alpha as the chosen op.
        Usually used for validation.
        """
        for mutable in self.undedup_mutables:
            mutable.registered_module.set_chosen_op_active()

    def num_arch_params(self):
        """
        The number of mutables, i.e., LayerChoice

        Returns
        -------
        int
            the number of LayerChoice in user model
        """
        return len(self.mutable_list)

    def set_arch_param_grad(self):
        """
        For each LayerChoice, calculate gradients for architecture weights, i.e., alpha
        """
        for mutable in self.undedup_mutables:
            mutable.registered_module.set_arch_param_grad(mutable)

    def get_architecture_parameters(self):
        """
        Get all the architecture parameters.

        yield
        -----
        PyTorch Parameter
            Return ap_path_alpha of the traversed mutable
        """
        for mutable in self.undedup_mutables:
            yield mutable.registered_module.get_ap_path_alpha()

    def change_forward_mode(self, mode):
        """
        Update forward mode of MixedOps, as training architecture weights and
        model weights use different forward modes.
        """
        MixedOp.forward_mode = mode

    def get_forward_mode(self):
        """
        Get forward mode of MixedOp

        Returns
        -------
        string
            the current forward mode of MixedOp
        """
        return MixedOp.forward_mode

    def rescale_updated_arch_param(self):
        """
        Rescale architecture weights in 'two' mode.
        """
        for mutable in self.undedup_mutables:
            mutable.registered_module.rescale_updated_arch_param()

    def unused_modules_off(self):
        """
        Remove unused modules for each mutables.
        The removed modules are kept in ```self._unused_modules``` for resume later.
        """
        self._unused_modules = []
        for mutable in self.undedup_mutables:
            mixed_op = mutable.registered_module
            unused = {}
            if self.get_forward_mode() in ['full', 'two', 'full_v2']:
                involved_index = mixed_op.active_index + mixed_op.inactive_index
            else:
                involved_index = mixed_op.active_index
            for i in range(mixed_op.n_choices):
                if i not in involved_index:
                    unused[i] = mutable.choices[i]
                    mutable.choices[i] = None
            self._unused_modules.append(unused)

    def unused_modules_back(self):
        """
        Resume the removed modules back.
        """
        if self._unused_modules is None:
            return
        for m, unused in zip(self.mutable_list, self._unused_modules):
            for i in unused:
                m.choices[i] = unused[i]
        self._unused_modules = None

    def arch_requires_grad(self):
        """
        Make architecture weights require gradient
        """
        for mutable in self.undedup_mutables:
            mutable.registered_module.to_requires_grad()

    def arch_disable_grad(self):
        """
        Disable gradient of architecture weights, i.e., does not
        calcuate gradient for them.
        """
        for mutable in self.undedup_mutables:
            mutable.registered_module.to_disable_grad()

    def sample_final(self):
        """
        Generate the final chosen architecture.

        Returns
        -------
        dict
            the choice of each mutable, i.e., LayerChoice
        """
        result = dict()
        for mutable in self.undedup_mutables:
            assert isinstance(mutable, LayerChoice)
            index, _ = mutable.registered_module.chosen_index
            # pylint: disable=not-callable
            result[mutable.key] = F.one_hot(torch.tensor(index), num_classes=mutable.length).view(-1).bool()
        return result

&lt;/denchmark-code&gt;

I defined my own model, which is almost as same as the original one.
in model.py
&lt;denchmark-code&gt;class SearchMobileNet_NTU(nn.Module):
    def __init__(self,
                 width_stages=[24,40,80,96,192,320],
                 n_cell_stages=[4,4,4,4,4,1],
                 stride_stages=[2,2,2,1,2,1],
                 width_mult=1, n_classes=2,
                 dropout_rate=0, bn_param=(0.1, 1e-3)):
        """
        Parameters
        ----------
        width_stages: str
            width (output channels) of each cell stage in the block
        n_cell_stages: str
            number of cells in each cell stage
        stride_strages: str
            stride of each cell stage in the block
        width_mult : int
            the scale factor of width
        """
        super(SearchMobileNet_NTU, self).__init__()

        input_channel = putils.make_divisible(32 * width_mult, 8)  # 32
        first_cell_width = putils.make_divisible(16 * width_mult, 8) # 16
        for i in range(len(width_stages)):
            width_stages[i] = putils.make_divisible(width_stages[i] * width_mult, 8) #[24,40,80,96,192,320]
        # first conv
        first_conv = ops.ConvLayer(3, input_channel, kernel_size=3, stride=2, use_bn=True, act_func='relu6', ops_order='weight_bn_act')
        # first block
        first_block_conv = ops.OPS['3x3_MBConv1'](input_channel, first_cell_width, 1)
        first_block = first_block_conv

        input_channel = first_cell_width # now is 16

        blocks = [first_block]

        stage_cnt = 0
        for width, n_cell, s in zip(width_stages, n_cell_stages, stride_stages): # each block
            for i in range(n_cell): # each cell in a block
                if i == 0:
                    stride = s # first cell to downsampling
                else:
                    stride = 1

                op_candidates = [ops.OPS['3x3_MBConv3'](input_channel, width, stride),
                                 ops.OPS['3x3_MBConv6'](input_channel, width, stride),
                                 ops.OPS['5x5_MBConv3'](input_channel, width, stride),
                                 ops.OPS['5x5_MBConv6'](input_channel, width, stride),
                                 ops.OPS['7x7_MBConv3'](input_channel, width, stride),
                                 ops.OPS['7x7_MBConv6'](input_channel, width, stride)]

                if stride == 1 and input_channel == width: 
                    # if it is not the first one, cause first cell take C-in to width
                    op_candidates += [ops.OPS['Zero'](input_channel, width, stride)] # allow block be skipped
                    conv_op = nas.mutables.LayerChoice(op_candidates,
                                                       return_mask=True,
                                                       key="s{}_c{}".format(stage_cnt, i))
                else:
                    conv_op = nas.mutables.LayerChoice(op_candidates,
                                                       return_mask=True,
                                                       key="s{}_c{}".format(stage_cnt, i))
                # shortcut
                if stride == 1 and input_channel == width:
                    # if not first cell
                    shortcut = ops.IdentityLayer(input_channel, input_channel)
                else:
                    shortcut = None
                inverted_residual_block = ops.MobileInvertedResidualBlock(conv_op, shortcut, op_candidates)
                blocks.append(inverted_residual_block)
                input_channel = width
            stage_cnt += 1

        # feature mix layer
        last_channel = putils.make_devisible(1280 * width_mult, 8) if width_mult &gt; 1.0 else 1280
        feature_mix_layer = ops.ConvLayer(input_channel, last_channel, kernel_size=1, use_bn=True, act_func='relu6', ops_order='weight_bn_act', ) # 320-&gt;1280
        classifier = ops.LinearLayer(last_channel, n_classes, dropout_rate=dropout_rate) # for output, 1x1x1280-&gt;view-&gt;2

        self.first_conv = first_conv
        self.blocks = nn.ModuleList(blocks)
        self.feature_mix_layer = feature_mix_layer
        self.global_avg_pooling = nn.AdaptiveAvgPool2d(1)
        self.classifier = classifier

        # set bn param
        self.set_bn_param(momentum=bn_param[0], eps=bn_param[1])

    def forward(self, x):
        x = self.first_conv(x)
        for block in self.blocks:
            x = block(x)
        x = self.feature_mix_layer(x)
        x = self.global_avg_pooling(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

    def set_bn_param(self, momentum, eps):
        for m in self.modules():
            if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):
                m.momentum = momentum
                m.eps = eps
        return

    def init_model(self, model_init='he_fout', init_div_groups=False):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                if model_init == 'he_fout':
                    n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                    if init_div_groups:
                        n /= m.groups
                    m.weight.data.normal_(0, math.sqrt(2. / n))
                elif model_init == 'he_fin':
                    n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels
                    if init_div_groups:
                        n /= m.groups
                    m.weight.data.normal_(0, math.sqrt(2. / n))
                else:
                    raise NotImplementedError
            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()
            elif isinstance(m, nn.Linear):
                stdv = 1. / math.sqrt(m.weight.size(1))
                m.weight.data.uniform_(-stdv, stdv)
                if m.bias is not None:
                    m.bias.data.zero_()
&lt;/denchmark-code&gt;

And finally it's main.py.
Sorry, I can't provide my data due to the confidentiality agreement.
main.py:
&lt;denchmark-code&gt;import os
import sys
import logging
from argparse import ArgumentParser
import torch
from proxylessnas_example import datasets
from proxylessnas_example.putils import get_parameters
from proxylessnas_example.model import SearchMobileNet_NTU
from nni.nas.pytorch.proxylessnas import ProxylessNasTrainer
from proxylessnas_example.retrain import Retrain

logger = logging.getLogger('nni_proxylessnas')

if __name__ == "__main__":
    parser = ArgumentParser("proxylessnas")
    # configurations of the model
    parser.add_argument("--n_cell_stages", default='4,4,4,4,4,1', type=str)
    parser.add_argument("--stride_stages", default='2,2,2,1,2,1', type=str)
    parser.add_argument("--width_stages", default='24,40,80,96,192,320', type=str)
    parser.add_argument("--bn_momentum", default=0.1, type=float)
    parser.add_argument("--bn_eps", default=1e-3, type=float)
    parser.add_argument("--dropout_rate", default=0, type=float)
    parser.add_argument("--no_decay_keys", default='bn', type=str, choices=[None, 'bn', 'bn#bias'])
    # configurations of imagenet dataset
    parser.add_argument("--data_path", default='/home/iis/data/Capstone/train_test_set_03b.pkl', type=str) # edit the dir
    parser.add_argument("--train_batch_size", default=64, type=int)
    parser.add_argument("--test_batch_size", default=64, type=int)
    parser.add_argument("--n_worker", default=8, type=int)
    parser.add_argument("--resize_scale", default=0.08, type=float)
    parser.add_argument("--distort_color", default='normal', type=str, choices=['normal', 'strong', 'None'])
    # configurations for training mode
    parser.add_argument("--train_mode", default='search', type=str, choices=['search', 'retrain'])
    # configurations for search
    parser.add_argument("--checkpoint_path", default='./search_mobile_net.pt', type=str)
    parser.add_argument("--arch_path", default='./arch_path.pt', type=str)
    parser.add_argument("--no-warmup", dest='warmup', action='store_false')
    # configurations for retrain
    parser.add_argument("--exported_arch_path", default=None, type=str)

    args = parser.parse_args()
    if args.train_mode == 'retrain' and args.exported_arch_path is None:
        logger.error('When --train_mode is retrain, --exported_arch_path must be specified.')
        sys.exit(-1)

    model = SearchMobileNet_NTU(width_stages=[int(i) for i in args.width_stages.split(',')],
                                n_cell_stages=[int(i) for i in args.n_cell_stages.split(',')],
                                stride_stages=[int(i) for i in args.stride_stages.split(',')],
                                n_classes=2,
                                dropout_rate=args.dropout_rate,
                                bn_param=(args.bn_momentum, args.bn_eps))
    print('model module:', model.blocks)
    logger.info('SearchMobileNet_NTU model create done')
    model.init_model() # initial some parameters in the blocks
    logger.info('SearchMobileNet_NTU model init done')

    # move network to GPU if available
    if torch.cuda.is_available():
        device = torch.device('cuda')
    else:
        device = torch.device('cpu')

    logger.info('Creating data provider...')
    data_provider = datasets.NTUDataProvider(save_path=args.data_path,
                                                  train_batch_size=args.train_batch_size,
                                                  test_batch_size=args.test_batch_size,
                                                  n_worker=args.n_worker,
                                                  resize_scale=args.resize_scale,
                                                  mul_hw=4)
    logger.info('Creating data provider done')

    if args.no_decay_keys:
        keys = args.no_decay_keys
        momentum, nesterov = 0.9, True
        optimizer = torch.optim.SGD([
            {'params': get_parameters(model, keys, mode='exclude'), 'weight_decay': 4e-5},
            {'params': get_parameters(model, keys, mode='include'), 'weight_decay': 0},
        ], lr=0.05, momentum=momentum, nesterov=nesterov)
    else:
        optimizer = torch.optim.SGD(get_parameters(model), lr=0.05, momentum=momentum, nesterov=nesterov, weight_decay=4e-5)

    if args.train_mode == 'search':
        # this is architecture search
        logger.info('Creating ProxylessNasTrainer...')
        trainer = ProxylessNasTrainer(model,
                                      model_optim=optimizer,
                                      train_loader=data_provider.train,
                                      valid_loader=data_provider.valid,
                                      device=device,
                                      warmup=False,#args.warmup
                                      ckpt_path=args.checkpoint_path,
                                      arch_path=args.arch_path)

        logger.info('Start to train with ProxylessNasTrainer...')
        trainer.train()
        logger.info('Training done')
        trainer.export(args.arch_path)
        logger.info('Best architecture exported in %s', args.arch_path)
    elif args.train_mode == 'retrain':
        # this is retrain
        from nni.nas.pytorch.fixed import apply_fixed_architecture
        assert os.path.isfile(args.exported_arch_path), \
            "exported_arch_path {} should be a file.".format(args.exported_arch_path)
        apply_fixed_architecture(model, args.exported_arch_path)
        trainer = Retrain(model, optimizer, device, data_provider, n_epochs=300)
        trainer.run()
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='YiKuanLiu' date='2020-04-09T12:53:50Z'>
		&lt;denchmark-link:https://github.com/YiKuanLiu&gt;@YiKuanLiu&lt;/denchmark-link&gt;
 thanks for providing the code. I ed your code with the original code in NNI, there is almost no difference except comments, n_classes, and datasets. So I cannot figure out the reason from the provided code.
According to the error message,  is None, which means  is not calculated or not kept. Could you double check the functions  and  (&lt;denchmark-link:https://github.com/microsoft/nni/blob/master/src/sdk/pynni/nni/nas/pytorch/proxylessnas/mutator.py#L69&gt;https://github.com/microsoft/nni/blob/master/src/sdk/pynni/nni/nas/pytorch/proxylessnas/mutator.py#L69&lt;/denchmark-link&gt;
) are invoked in proper places in your code. To make sure  is True when &lt;denchmark-link:https://github.com/microsoft/nni/blob/master/src/sdk/pynni/nni/nas/pytorch/proxylessnas/mutator.py#L246&gt;this line&lt;/denchmark-link&gt;
 is executed. The easiest way is print  before this line.
		</comment>
		<comment id='4' author='YiKuanLiu' date='2020-04-10T05:20:44Z'>
		&lt;denchmark-link:https://github.com/QuanluZhang&gt;@QuanluZhang&lt;/denchmark-link&gt;
 , thanks for the kind suggestions.  I printed the  before the .
The context code is below:
&lt;denchmark-code&gt;        print("set_arch_param_grad: the ap path wb = ", self.ap_path_wb)
        print("set_arch_param_grad: the ap path wb.grad = ", self.ap_path_wb.grad)
        print(self.ap_path_wb.requires_grad)
        binary_grads = self.ap_path_wb.grad.data
        print("set binary_grads: DONE")
&lt;/denchmark-code&gt;

And the log is like:

....
set_arch_param_grad: the ap path wb =  Parameter containing:
tensor([0., 0., 0., 0., 1., 0., 0.], device='cuda:0', requires_grad=True)
set_arch_param_grad: the ap path wb.grad =  tensor([ 0.0442, -0.0843, -0.0136, -0.0026,  0.0201, -0.0413,  0.0000],
device='cuda:0')
True
set binary_grads: DONE
set_arch_param_grad: the ap path wb =  Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 1.], device='cuda:0', requires_grad=True)
set_arch_param_grad: the ap path wb.grad =  None
True

and the error was then invoked.
So the ap_path_wb.requires_grad is normal(True), but the grad value is None.
Do you think it's caused by the virtual environment issue, maybe?
If it's the case, can you provide your environment to me for double check?  Thanks!
		</comment>
		<comment id='5' author='YiKuanLiu' date='2020-04-11T13:49:54Z'>
		&lt;denchmark-link:https://github.com/YiKuanLiu&gt;@YiKuanLiu&lt;/denchmark-link&gt;
 thanks for the info, I don't think it is related to virtual environment.
my environment:
nni version: 1.4
nni mode(local|pai|remote): local
OS: linux, ubuntu 16.04
python version: 3.5.2
is conda or virtualenv used?: no
is running in docker?: no
Could you successfully run the original &lt;denchmark-link:https://github.com/microsoft/nni/tree/master/examples/nas/proxylessnas&gt;proxylessnas example&lt;/denchmark-link&gt;
? If it can, I don't think it is about the environment.
&lt;denchmark-link:https://github.com/microsoft/nni/blob/master/src/sdk/pynni/nni/nas/pytorch/proxylessnas/mutator.py#L110&gt;This function&lt;/denchmark-link&gt;
 calculate the gradient of . Could you add print to check that this function is invoked and the return value  is not None.
		</comment>
		<comment id='6' author='YiKuanLiu' date='2020-04-13T07:53:59Z'>
		&lt;denchmark-link:https://github.com/QuanluZhang&gt;@QuanluZhang&lt;/denchmark-link&gt;
 thanks for providing the info. It should not be caused by the environment.
And I've printed out the key and binary_grads when the function backward is invoked.
The context code is below:
&lt;denchmark-code&gt;def backward_function(key, candidate_ops, active_id, binary_gates):
                def backward(_x, _output, grad_output):
                    binary_grads = torch.zeros_like(binary_gates.data)
                    print("backward: key:", key)
                    with torch.no_grad():
                        for k in range(len(candidate_ops)):
                            if k != active_id:
                                out_k = candidate_ops[k](_x.data)
                            else:
                                out_k = _output.data
                            grad_k = torch.sum(out_k * grad_output)
                            binary_grads[k] = grad_k
                        print("binary_grads :", binary_grads)
                    return binary_grads
                return backward
&lt;/denchmark-code&gt;

Then in the log info:

backward: key: s5_c0
binary_grads : tensor([ 2.0258e-05, -1.5336e-02, -3.2752e-02, -4.6675e-03, -1.3672e-02,
-4.5278e-03], device='cuda:0')
backward: key: s4_c3
binary_grads : tensor([ 0.0008,  0.0113,  0.0033, -0.0114,  0.0053,  0.0026,  0.0000],
device='cuda:0')
backward: key: s4_c2
binary_grads : tensor([ 0.0068,  0.0025,  0.0001, -0.0090,  0.0023, -0.0123,  0.0000],
device='cuda:0')
backward: key: s4_c1
binary_grads : tensor([ 0.0156,  0.0125, -0.0088, -0.0162, -0.0105, -0.0050,  0.0000],
device='cuda:0')
backward: key: s4_c0
binary_grads : tensor([ 0.0363, -0.0204, -0.0092, -0.0399,  0.0015, -0.0273], device='cuda:0')
backward: key: s3_c3
binary_grads : tensor([ 0.0040,  0.0047, -0.0059, -0.0009, -0.0014, -0.0051,  0.0000],
device='cuda:0')
backward: key: s3_c2
binary_grads : tensor([ 0.0103,  0.0183, -0.0202, -0.0072,  0.0043,  0.0082,  0.0000],
device='cuda:0')
backward: key: s3_c0
binary_grads : tensor([-0.0484,  0.0237,  0.0217, -0.0029, -0.0220, -0.0012], device='cuda:0')
backward: key: s2_c3
binary_grads : tensor([ 0.0087, -0.0050,  0.0163, -0.0060,  0.0400,  0.0002,  0.0000],
device='cuda:0')
backward: key: s2_c1
binary_grads : tensor([ 0.0461,  0.0444, -0.0076,  0.0006,  0.0355,  0.0495,  0.0000],
device='cuda:0')
backward: key: s2_c0
binary_grads : tensor([-0.0248,  0.0234,  0.0051,  0.0054,  0.0332,  0.0389], device='cuda:0')
backward: key: s1_c2
binary_grads : tensor([-0.0004,  0.0866,  0.0444, -0.0119, -0.0808, -0.0687,  0.0000],
device='cuda:0')
backward: key: s1_c1
binary_grads : tensor([ 0.0528, -0.1417, -0.0284,  0.0025,  0.0393,  0.0081,  0.0000],
device='cuda:0')
backward: key: s1_c0
binary_grads : tensor([-0.0441,  0.0094, -0.0323,  0.0641,  0.0680, -0.0703], device='cuda:0')
backward: key: s0_c3
binary_grads : tensor([-0.0051, -0.0341,  0.0273,  0.0324,  0.1170,  0.0209,  0.0000],
device='cuda:0')
backward: key: s0_c2
binary_grads : tensor([ 0.0586,  0.0048, -0.0092,  0.0580,  0.0714,  0.0415,  0.0000],
device='cuda:0')
backward: key: s0_c1
binary_grads : tensor([ 0.1001,  0.0476, -0.0691,  0.0034, -0.0415,  0.0296,  0.0000],
device='cuda:0')
backward: key: s0_c0
binary_grads : tensor([-0.1552, -0.1085,  0.3534, -0.1200,  0.0579,  0.2437], device='cuda:0')

The interesting thing is: when it go back through blocks, it skipped the blocks with ops = ZeroLayer, for examples, the s3_c1, s2_c2, s1_c3 above (won't be the same every time).
Then the function set_arch_param_grad went through forward, it encountered the None value in the first ZeroLayer(8th block, s1_c3 in this case).
The log is like:

[...7 blocks info (s0_c0 to s1_c2)...]
[below is the 8th block(s1_c3) info]
set_arch_param_grad: the ap_path_wb =  Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 1.], device='cuda:0', requires_grad=True)
set_arch_param_grad: the ap_path_wb.grad =  None

So my next question is why the backward is not invoked in those zerolayers, and when(which line) it should be invoked?  Sorry that I don't have much experiences with deep learning.
Thanks!
		</comment>
		<comment id='7' author='YiKuanLiu' date='2020-04-20T07:29:49Z'>
		I  also encountered the same problem, is it solved? And I want to use proxylessNAS in SR field.
Thanks very much!
		</comment>
		<comment id='8' author='YiKuanLiu' date='2020-04-21T15:46:22Z'>
		&lt;denchmark-link:https://github.com/YiKuanLiu&gt;@YiKuanLiu&lt;/denchmark-link&gt;
 @gqq99 . I reproduced the problem. Here is a quick hotfix: move this line (&lt;denchmark-link:https://github.com/microsoft/nni/blob/master/src/sdk/pynni/nni/nas/pytorch/proxylessnas/mutator.py#L246&gt;https://github.com/microsoft/nni/blob/master/src/sdk/pynni/nni/nas/pytorch/proxylessnas/mutator.py#L246&lt;/denchmark-link&gt;
) to between L249 and L250.
		</comment>
		<comment id='9' author='YiKuanLiu' date='2020-04-23T07:17:24Z'>
		&lt;denchmark-link:https://github.com/QuanluZhang&gt;@QuanluZhang&lt;/denchmark-link&gt;
 Yeah，you're right! After debugging I also found that if you  move this line backward, when it selects zero_layer, it will return directly.
		</comment>
		<comment id='10' author='YiKuanLiu' date='2020-04-24T06:15:32Z'>
		&lt;denchmark-link:https://github.com/QuanluZhang&gt;@QuanluZhang&lt;/denchmark-link&gt;
 Thanks for the debugging and kind responses! The program is running smoothly now. :)
		</comment>
	</comments>
</bug>