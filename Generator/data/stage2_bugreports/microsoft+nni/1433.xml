<bug id='1433' author='martsalz' open_date='2019-08-07T11:30:12Z' closed_time='2019-08-21T07:17:43Z'>
	<summary>STATUS: Training service error: No such file</summary>
	<description>
First of all, thanks for this great library.
Short summary about the issue/question:
NNI is started on server 1 and connects to server 2 via SSH. The MNIST example runs successfully.
The same scenario is repeated between server 1 and server 3, but the experiment fails. The following error message appears in the web interface: STATUS: Training service error: No such file
On server 1-3 NNI, Tensorflow etc. is installed in a virtualenv . This virtualenv is started automatically after the SSH login (.bashrc entry).
nnimanager.log (Server 3): [2019-8-7 11:19:30] ERROR [ 'Error: No such file\n    at SFTPStream._transform (/home/msalz/venv_nni/nni/node_modules/ssh2-streams/lib/sftp.js:412:27)\n    at SFTPStream.Transform._read (_stream_transform.js:190:10)\n    at SFTPStream._read (/home/msalz/venv_nni/nni/node_modules/ssh2-streams/lib/sftp.js:183:15)\n    at SFTPStream.Transform._write (_stream_transform.js:178:12)\n    at doWrite (_stream_writable.js:410:12)\n    at writeOrBuffer (_stream_writable.js:394:5)\n    at SFTPStream.Writable.write (_stream_writable.js:294:11)\n    at Channel.ondata (_stream_readable.js:666:20)\n    at Channel.emit (events.js:182:13)\n    at addChunk (_stream_readable.js:283:12)' ]
[2019-8-7 11:19:30] INFO [ 'Change NNIManager status from: RUNNING to: ERROR' ]
The error occurs with different NNI versions (tested with 0.9 and 0.7).
The default port for the web interface is 8080. The NNI participants connect via SSH. Are any other ports used in the background by NNI?
What exactly does the error message mean and what is the reason for this error message?
Brief what process you are following:
How to reproduce it:
On all servers I have executed the following setup:
&lt;denchmark-code&gt;git clone https://github.com/microsoft/nni.git
python3.5 -m venv venv_nni
source venv_nni/bin/activate
cd nni
pip install tensorflow==1.5.0
pip install keras
python -m pip install --upgrade nni
pip install numpy==1.16.4
cd ..
&lt;/denchmark-code&gt;

This command only executes on server 1:
nnictl create --config nni/examples/trials/mnist/config.yml 
nni Environment:

nni version: 0.9.1.1
nni mode(local|pai|remote): remote
OS: CentOS Linux 7
python version: Python 3.5
is conda or virtualenv used?: virtualenv
is running in docker?: no

need to update document(yes/no): no
Anything else we need to know:
Here is the config.yml file from server 1:
&lt;denchmark-code&gt;authorName: default
experimentName: example_mnist
trialConcurrency: 1
maxExecDuration: 1h
maxTrialNum: 10
#choice: local, remote, pai
trainingServicePlatform: remote
searchSpacePath: search_space.json
#choice: true, false
useAnnotation: false
tuner:
  #choice: TPE, Random, Anneal, Evolution, BatchTuner, MetisTuner, GPTuner
  #SMAC (SMAC should be installed through nnictl)
  builtinTunerName: TPE
  classArgs:
    #choice: maximize, minimize
    optimize_mode: maximize
trial:
  command: python3 mnist.py
  codeDir: .
  gpuNum: 0
machineList:
  - ip: &lt;Server_3_IP&gt;
    username: &lt;username&gt;
    passwd: &lt;passwd&gt;
    #port can be skip if using default ssh port 22
    #port: 22
nniManagerIp: &lt;Server_1_IP&gt;
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='martsalz' date='2019-08-08T06:55:23Z'>
		The above scenarios refer to Tensorflow CPU. If Tensorflow GPU is installed on server 3, the following error message appears in nnimanager.log:
[2019-8-8 08:49:57] WARNING [ 'Scheduler: trialJob id PG5pY, no machine can be scheduled, return TMP_NO_AVAILABLE_GPU ' ] [2019-8-8 08:49:57] INFO [ 'Right now no available GPU can be allocated for trial PG5pY, will try to schedule later' ]
(Tensorflow GPU on server 3 works perfectly independent from NNI)
		</comment>
		<comment id='2' author='martsalz' date='2019-08-08T08:36:00Z'>
		&lt;denchmark-link:https://github.com/martsalz&gt;@martsalz&lt;/denchmark-link&gt;
 Can you check if  is available on your server 3?
		</comment>
		<comment id='3' author='martsalz' date='2019-08-08T08:40:25Z'>
		Yes, nvidia-smi is available on server 3
&lt;denchmark-link:https://user-images.githubusercontent.com/12211578/62688292-db93b900-b9c8-11e9-907a-338d6b56d066.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='martsalz' date='2019-08-08T09:34:54Z'>
		&lt;denchmark-link:https://github.com/martsalz&gt;@martsalz&lt;/denchmark-link&gt;

Please check the permission of  on server 3 and  it to  if it's not.
Maybe you have run NNI in local mode on server 3 before? This would create the directory with bad permission.
		</comment>
		<comment id='5' author='martsalz' date='2019-08-08T11:47:57Z'>
		The permission of /tmp/nni is now set to 777 and I can see that NNI creates subfolders when running. The above experiment now runs with Tensorflow CPU. The scenario with TF GPU on server 3 doesn't work - the question remains why the error message
TMP_NO_AVAILABLE_GPU
appears (in config.yml gpuNum is set to 1)
(my user account on server 3 has no admin permissions)
		</comment>
		<comment id='6' author='martsalz' date='2019-08-12T07:58:55Z'>
		Any ideas?
		</comment>
		<comment id='7' author='martsalz' date='2019-08-12T08:10:50Z'>
		&lt;denchmark-link:https://github.com/martsalz&gt;@martsalz&lt;/denchmark-link&gt;
 Is there a  folder? If yes,  it and everything inside to  as well.
And if there is a  process running, kill it.
		</comment>
		<comment id='8' author='martsalz' date='2019-08-12T08:20:35Z'>
		On server 3 there is no /tmp/nni/script folder and no python3 -m nni_gpu_tool.gpu_metrics_collector process.
		</comment>
		<comment id='9' author='martsalz' date='2019-08-12T08:26:01Z'>
		Please run METRIC_OUTPUT_DIR=/tmp/nni/script python3 -m nni_gpu_tool.gpu_metrics_collector manually and check the output.
		</comment>
		<comment id='10' author='martsalz' date='2019-08-12T08:34:35Z'>
		After executing the command, there is no output and the /tmp/nni/script folder is not created under server 3.
&lt;denchmark-link:https://user-images.githubusercontent.com/12211578/62853312-3c363500-bcec-11e9-80d1-2ee1bc1f1f81.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='martsalz' date='2019-08-12T08:55:50Z'>
		Could you please use mkdir -p /tmp/nni/script to create folder firstly, and then use METRIC_OUTPUT_DIR=/tmp/nni/script python3 -m nni_gpu_tool.gpu_metrics_collector to test the command? Before exec the command, please make sure there is no active nni_gpu_tool.gpu_metrics_collector process backend.
		</comment>
		<comment id='12' author='martsalz' date='2019-08-12T09:02:35Z'>
		Same as before: No output in the console and the  /tmp/nni/script folder is empty
		</comment>
		<comment id='13' author='martsalz' date='2019-08-12T09:18:20Z'>
		&lt;denchmark-link:https://github.com/martsalz&gt;@martsalz&lt;/denchmark-link&gt;
 Please try &lt;denchmark-link:https://gist.github.com/liuzhe-lz/756cc734d0aa96c3c733b3d86ee57396&gt;this script&lt;/denchmark-link&gt;
 with :
		</comment>
		<comment id='14' author='martsalz' date='2019-08-12T09:32:47Z'>
		Now gpu_metrics was created in /tmp/nni/script:
{"Timestamp": "Mon Aug 12 11:29:18 2019", "gpuCount": 2, "gpuInfos": [{"activeProcessNum": 2, "gpuMemUtil": "0", "gpuUtil": "0", "index": 0}, {"activeProcessNum": 2, "gpuMemUtil": "0", "gpuUtil": "0", "index": 1}]} {"Timestamp": "Mon Aug 12 11:29:23 2019", "gpuCount": 2, "gpuInfos": [{"activeProcessNum": 2, "gpuMemUtil": "3", "gpuUtil": "30", "index": 0}, {"activeProcessNum": 2, "gpuMemUtil": "0", "gpuUtil": "0", "index": 1}]} {"Timestamp": "Mon Aug 12 11:29:28 2019", "gpuCount": 2, "gpuInfos": [{"activeProcessNum": 2, "gpuMemUtil": "0", "gpuUtil": "0", "index": 0}, {"activeProcessNum": 2, "gpuMemUtil": "0", "gpuUtil": "0", "index": 1}]} {"Timestamp": "Mon Aug 12 11:29:33 2019", "gpuCount": 2, "gpuInfos": [{"activeProcessNum": 2, "gpuMemUtil": "0", "gpuUtil": "0", "index": 0}, {"activeProcessNum": 2, "gpuMemUtil": "10", "gpuUtil": "24", "index": 1}]} 
&lt;denchmark-link:https://user-images.githubusercontent.com/12211578/62856228-9f2bca00-bcf4-11e9-980d-b66af8be2603.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='15' author='martsalz' date='2019-08-12T09:45:42Z'>
		Seems there is already a gpu metric collector running. The pid is 21315.
Please show us the output of following commands: (in your venv)

ps aux | grep gpu_metric
echo $METRIC_OUTPUT_DIR
If the variable is not empty, head ${METRIC_OUTPUT_DIR}/*

		</comment>
		<comment id='16' author='martsalz' date='2019-08-12T10:29:30Z'>
		No output for echo $METRIC_OUTPUT_DIR
&lt;denchmark-link:https://user-images.githubusercontent.com/12211578/62859178-abb42080-bcfc-11e9-9c58-3c83eb46d6ba.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='17' author='martsalz' date='2019-08-12T10:35:33Z'>
		&lt;denchmark-link:https://github.com/martsalz&gt;@martsalz&lt;/denchmark-link&gt;
 OK. The problem is that a failed "zombie" collector is preventing new ones to start.
Please kill all related processes (, , , and ) and try running NNI again.
		</comment>
		<comment id='18' author='martsalz' date='2019-08-12T10:52:29Z'>
		I've killed the processes.

ps aux | grep gpu_metric
nnictl  create --config nni/examples/trials/mnist/config.yml (start NNI on S1 remote S3)
ps aux | grep gpu_metric

&lt;denchmark-link:https://user-images.githubusercontent.com/12211578/62859943-00589b00-bcff-11e9-9436-e16831cc6dd4.png&gt;&lt;/denchmark-link&gt;

nnimanager.log on server 1: TMP_NO_AVAILABLE_GPU
		</comment>
		<comment id='19' author='martsalz' date='2019-08-12T11:01:25Z'>
		I don't know if this is helpful: the NNI tool can't be started on server 3 (local) either. Remote (server 1 -&gt; server 3), NNI works on server 3 with TF CPU
&lt;denchmark-link:https://user-images.githubusercontent.com/12211578/62860535-bbcdff00-bd00-11e9-83ab-34354263e1f7.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='20' author='martsalz' date='2019-08-14T01:34:57Z'>
		It means the GPU metric collector is still not working...
Please kill all related processes, remove /tmp/nni, and try METRIC_OUTPUT_DIR=/tmp/nni/script python3 -m nni_gpu_tool.gpu_metrics_collector again.
		</comment>
		<comment id='21' author='martsalz' date='2019-08-16T04:07:18Z'>
		After several attempts, I cannot make any progress with the solutions proposed. Therefore I installed and executed NNI locally on another PC:
Tensorflow GPU is installed and the GPU is used / available (see ). The GPU is used successfully e.g. for this MNIST example: &lt;denchmark-link:https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py&gt;https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py&lt;/denchmark-link&gt;

NNI works with TF CPU - the application switches to RUNNING mode after a few seconds.
NNI does NOT work with TF GPU - in this case no "experiments" folder is created and no error message is logged. The web interface says WAITING. This state remains permanent until the process is aborted.
My user account has admin privileges on this PC. I also executed the python3 gpu_metrics_collector.py script and killed individual processes (ps aux | grep gpu_metric). The same NNI version is installed in a venv  with Python3 as in the scenario described above.
See the following details:
&lt;denchmark-link:https://user-images.githubusercontent.com/12211578/63142509-5c1a7100-bfea-11e9-91d8-1e41b8a22090.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/12211578/63142498-53c23600-bfea-11e9-9990-63d3b955d4d4.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/12211578/63142501-56249000-bfea-11e9-8216-42d1bcbf1748.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/12211578/63142512-60df2500-bfea-11e9-886a-e399cf95ef72.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/12211578/63142514-62a8e880-bfea-11e9-9cba-4bac20880ba8.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/12211578/63142517-650b4280-bfea-11e9-80ea-4ba6eb5408d3.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/12211578/63142872-f29b6200-bfeb-11e9-9cb7-c0fc3e72c457.png&gt;&lt;/denchmark-link&gt;

config.yml:
authorName: default
experimentName: example_mnist-keras
trialConcurrency: 1
maxExecDuration: 1h
maxTrialNum: 10
#choice: local, remote, pai
trainingServicePlatform: local
searchSpacePath: search_space.json
#choice: true, false
useAnnotation: false
tuner:
#choice: TPE, Random, Anneal, Evolution, BatchTuner, MetisTuner
#SMAC (SMAC should be installed through nnictl)
builtinTunerName: TPE
classArgs:
#choice: maximize, minimize
optimize_mode: maximize
trial:
command: python mnist-keras.py
codeDir: .
gpuNum: 1
		</comment>
		<comment id='22' author='martsalz' date='2019-08-20T07:41:13Z'>
		And if I use the developer version: v0.8-319-g19173aa (local on server 3), then the WAITING mode remains permanent. No error message, no folder is created under "experiments" (GPU).
If the CPU is selected, everything works fine...
		</comment>
		<comment id='23' author='martsalz' date='2019-08-20T08:52:01Z'>
		Try these steps:

Clear the environment (kill processes and remove directories)
Create directory /tmp/nni/script
Run command METRIC_OUTPUT_DIR=/tmp/nni/script python3 -m nni_gpu_tool.gpu_metrics_collector

The script should create /tmp/nni/script/gpu_metrics and write GPU info to it. (a JSON object per line)
If it does not, we can tell the bug is caused by this script.
If the script works well, keep it running and start NNI in another terminal, with gpuNum set to 1 in config file.
		</comment>
		<comment id='24' author='martsalz' date='2019-08-20T09:26:51Z'>
		The gpu_metrics script is created in the /tmp/nni/script folder. The status in the web interface still remains permanently on WAITING.
(nvidia-smi indicates that memory is available but no python script (tensorflow gpu) has reserved memory)
$ ps aux | grep gpu_metric msalz    15709  0.0  0.0 112716   976 pts/5    S+   11:26   0:00 grep --color=auto gpu_metric 
gpu_metrics:
{"Timestamp": "Tue Aug 20 11:24:25 2019", "gpuCount": 2, "gpuInfos": [{"activeProcessNum": 2, "gpuMemUtil": "2", "gpuUtil": "18", "index": 0}, {"activeProcessNum": 4, "gpuMemUtil": "0", "gpuUtil": "0", "index": 1}]}
		</comment>
		<comment id='25' author='martsalz' date='2019-08-21T06:32:20Z'>
		Your GPUs are already in use. By default NNI will not use active GPUs.
You may want to set  to change this behaviour.
Please refer to &lt;denchmark-link:https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/ExperimentConfig.md&gt;the doc&lt;/denchmark-link&gt;
 for details.
		</comment>
		<comment id='26' author='martsalz' date='2019-08-21T07:07:50Z'>
		The GPUs on server 3 are still being used by other users at the same time. After specifying useActiveGpu the GPU is now used.
Thank you very much for your help. :-)
		</comment>
	</comments>
</bug>