<bug id='1719' author='UnnnU' open_date='2018-11-16T14:16:16Z' closed_time='2018-12-07T09:10:16Z'>
	<summary>OpenPAI page don't show the utilization of the GPU.</summary>
	<description>
&lt;denchmark-link:https://user-images.githubusercontent.com/10185324/48626437-cfa43900-e9ec-11e8-87da-fa26abefdc0a.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/10185324/48626444-d29f2980-e9ec-11e8-9806-3f19f0c10236.png&gt;&lt;/denchmark-link&gt;

I run the example job 
The config show below:
`{
"jobName": "tensorflow-cifar10",
"image": "openpai/pai.example.tensorflow:stable",
"dataDir": "/lwy/datadir",
"outputDir": "/lwy/outputdata",
"taskRoles": [
&lt;denchmark-code&gt;{

  "name": "cifar_train",

  "taskNumber": 1,

  "cpuNumber": 1,

  "memoryMB": 32768,

  "gpuNumber": 2,

  "command": "git clone https://github.com/tensorflow/models &amp;&amp; cd models/research/slim &amp;&amp; python download_and_convert_data.py --dataset_name=cifar10 --dataset_dir=$PAI_DATA_DIR &amp;&amp; python train_image_classifier.py --batch_size=64 --model_name=inception_v3 --dataset_name=cifar10 --dataset_split_name=train --dataset_dir=$PAI_DATA_DIR --train_dir=$PAI_OUTPUT_DIR"

}
&lt;/denchmark-code&gt;

]
}`
But when it is running,the page do not show the utilization of GPU.
	</description>
	<comments>
		<comment id='1' author='UnnnU' date='2018-11-16T14:18:24Z'>
		Below is my datadir :  in the hdfs webportal
&lt;denchmark-link:https://user-images.githubusercontent.com/10185324/48626699-86a0b480-e9ed-11e8-9f5e-3321eb2eff53.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/10185324/48626673-7b4d8900-e9ed-11e8-92a9-0d6ed305c4f4.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='UnnnU' date='2018-11-17T01:51:38Z'>
		&lt;denchmark-link:https://user-images.githubusercontent.com/10185324/48655126-563d3280-ea4e-11e8-9f33-fdbf76c4125f.png&gt;&lt;/denchmark-link&gt;

Is there a problem with the configuration?
		</comment>
		<comment id='3' author='UnnnU' date='2018-11-19T03:00:28Z'>
		Could you make sure this job is running with GPU but CPU? You could either ssh or docker exec into it, then run nvidia-smi to verify.
It is very helpful if log is available.
		</comment>
		<comment id='4' author='UnnnU' date='2018-11-19T03:15:34Z'>
		&lt;denchmark-link:https://user-images.githubusercontent.com/10185324/48684358-452a2800-ebec-11e8-87cb-853a8d66bd65.png&gt;&lt;/denchmark-link&gt;

I think it is using GPU. I run the cifa-10 example
The log:
&lt;denchmark-link:https://user-images.githubusercontent.com/10185324/48684387-625ef680-ebec-11e8-92a0-9561127f9c74.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/10185324/48684401-6d198b80-ebec-11e8-99a6-8a0036746beb.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='UnnnU' date='2018-11-19T03:29:21Z'>
		&lt;denchmark-link:https://github.com/xudifsd&gt;@xudifsd&lt;/denchmark-link&gt;
 Seems exporter doesn't collect GPU metrics?
		</comment>
		<comment id='6' author='UnnnU' date='2018-11-19T04:47:01Z'>
		
@xudifsd Seems exporter doesn't collect GPU metrics?

&lt;denchmark-link:https://user-images.githubusercontent.com/10185324/48686843-2e3e0280-ebf9-11e8-8e7d-4a84b033733c.png&gt;&lt;/denchmark-link&gt;

K8s has this filed
		</comment>
		<comment id='7' author='UnnnU' date='2018-11-20T02:32:02Z'>
		what's your job-exporter container's version? 0.8.0?
		</comment>
		<comment id='8' author='UnnnU' date='2018-11-20T02:39:29Z'>
		Also, can you provide logs of job-exporter?
		</comment>
		<comment id='9' author='UnnnU' date='2018-11-20T02:49:16Z'>
		Oh, I suspect you got bite by &lt;denchmark-link:https://github.com/microsoft/pai/issues/1704&gt;#1704&lt;/denchmark-link&gt;
 . job-exporter will hangs on iftop call, which will cause job-exporter failed to create job_exporter.prom file, and readiness probe will throw error  you mentioned.
Can you check this?
You can ssh into your node and then docker exec into job-exporter container and call iftop directly to see if it uses virtual bridge. You should use  instead of  because we enforced &lt;denchmark-link:https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#denyescalatingexec&gt;DenyEscalatingExec&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='10' author='UnnnU' date='2018-11-20T03:20:10Z'>
		
what's your job-exporter container's version? 0.8.0?

&lt;denchmark-link:https://user-images.githubusercontent.com/10185324/48749350-7884b980-ecb5-11e8-9ddc-1c95ec0d3bdb.png&gt;&lt;/denchmark-link&gt;

Yes,v0.8.0
The logs of the job-exporter:
&lt;denchmark-link:https://user-images.githubusercontent.com/10185324/48749382-b1bd2980-ecb5-11e8-8713-db2872e813fd.png&gt;&lt;/denchmark-link&gt;

I exec the job-exporter container and call iftop.I get this
&lt;denchmark-link:https://user-images.githubusercontent.com/10185324/48749511-3b6cf700-ecb6-11e8-8a7e-7994801f9850.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='UnnnU' date='2018-11-20T03:43:33Z'>
		ok, what's eno1 interface? physical network interface? Could you please execute this command iftop -t -P -s 1 -L 10000 -B -n -N in problematic job-exporter container? Is it hangs?
		</comment>
		<comment id='12' author='UnnnU' date='2018-11-20T06:32:51Z'>
		I use single box deploy and enter the job-container container.
&lt;denchmark-link:https://user-images.githubusercontent.com/10185324/48755686-25206480-ecd1-11e8-85c9-119c5d3a6b0a.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='UnnnU' date='2018-11-20T06:53:07Z'>
		Can you list all your network interface? ifconfig should output that.
		</comment>
		<comment id='14' author='UnnnU' date='2018-11-20T08:04:20Z'>
		
Can you list all your network interface? ifconfig should output that.

&lt;denchmark-link:https://user-images.githubusercontent.com/10185324/48759396-ea70f900-ecdd-11e8-9e2d-f91347339052.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/10185324/48759402-ef35ad00-ecdd-11e8-87e9-8a0be291d207.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='15' author='UnnnU' date='2018-11-20T08:57:04Z'>
		@lllllwwwwyyyy Ok, it's because node-exporter hangs on iftop commands. Later we'll add several commits to address this issue.
To mitigate your problem, you can delete problematic node-exporter pod via kubectl or k8s dashboard, iftop will select interface randomly, and hopefully it can listen on the interface . Or you can modify source code of &lt;denchmark-link:https://github.com/Microsoft/pai/blob/master/src/job-exporter/src/network.py#L65&gt;job-exporter&lt;/denchmark-link&gt;
 directly, and add  arguments to iftop command, and push images to registry.
		</comment>
		<comment id='16' author='UnnnU' date='2018-11-21T11:08:06Z'>
		
@lllllwwwwyyyy Ok, it's because node-exporter hangs on iftop commands. Later we'll add several commits to address this issue.
To mitigate your problem, you can delete problematic node-exporter pod via kubectl or k8s dashboard, iftop will select interface randomly, and hopefully it can listen on the interface eno2. Or you can modify source code of job-exporter directly, and add -i eno2 arguments to iftop command, and push images to registry.

If dev-box:v0.8.1 has fixed this problem?
		</comment>
		<comment id='17' author='UnnnU' date='2018-11-22T01:55:49Z'>
		No, it was scheduled to be fixed in 0.9.x
		</comment>
		<comment id='18' author='UnnnU' date='2018-12-07T09:10:15Z'>
		close and track in &lt;denchmark-link:https://github.com/microsoft/pai/issues/1764&gt;#1764&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>