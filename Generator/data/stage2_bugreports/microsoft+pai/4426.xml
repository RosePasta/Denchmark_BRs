<bug id='4426' author='victorming' open_date='2020-04-22T06:17:01Z' closed_time='2020-07-09T03:52:20Z'>
	<summary>SparkPi task failed with v0.14.0</summary>
	<description>
Organization Name:
CloudEdu
Short summary about the issue/question:
SparkPi task failed with v0.14.0
Brief what process you are following:
I runned the sparkpi task according to the spark example, but it failed with following error:
/pai/bootstrap/docker_bootstrap.sh: line 206: -Dspark.history.fs.logDirectory=hdfs://192.168.101.241:9000/shared/spark-logs: No such file or directory
Job config:
protocolVersion: 2
name: admin_sparkpi_2020042204_753a345
type: job
jobRetryCount: 0
prerequisites:

type: dockerimage
uri: openpai/spark-example
name: docker_image_0
taskRoles:
spark_submitter:
instances: 1
completion:
minFailedInstances: 1
minSucceededInstances: 1
dockerImage: docker_image_0
resourcePerInstance:
gpu: 0
cpu: 2
memoryMB: 2048
commands:







spark-submit --conf spark.eventLog.enabled=true --conf
spark.history.fs.logDirectory=hdfs://192.168.101.241:9000/shared/spark-logs
--conf spark.eventLog.dir=hdfs://192.168.101.241:9000/shared/spark-logs
--class org.apache.spark.examples.SparkPi --master yarn --deploy-mode
cluster --driver-memory 1g --executor-memory 1g --executor-cores 1
--queue default ${SPARK_HOME}/examples/jars/spark-examples*.jar 10
taskRetryCount: 0
spark_history_server:
instances: 1
completion:
minFailedInstances: 1
minSucceededInstances: 1
dockerImage: docker_image_0
resourcePerInstance:
gpu: 0
cpu: 2
memoryMB: 1024
commands:






URL=http://${PAI_CURRENT_CONTAINER_IP}:${PAI_CONTAINER_HOST_history_server_PORT_LIST}/
&amp;&amp; echo Please visit spark histroy server: ${URL} &amp;&amp;
SPARK_DAEMON_JAVA_OPTS="-Dspark.history.ui.port=${PAI_CONTAINER_HOST_history_server_PORT_LIST}
-Dspark.history.fs.logDirectory=hdfs://192.168.101.241:9000/shared/spark-logs
-Dspark.eventLog.enabled=true
-Dspark.eventLog.dir=hdfs://192.168.101.241:9000/shared/spark-logs"
spark-class org.apache.spark.deploy.history.HistoryServer
taskRetryCount: 0
defaults:
virtualCluster: default



How to reproduce it:
OpenPAI Environment:

OpenPAI version:
Cloud provider or hardware configuration:
OS (e.g. from /etc/os-release):
Kernel (e.g. uname -a):
Hardware (e.g. core number, memory size, storage size, GPU type etc.):
Others:

Anything else we need to know:
	</description>
	<comments>
		<comment id='1' author='victorming' date='2020-04-22T08:16:31Z'>
		I did some exploration and found the image 'openpai/spark-example' might have some bugs. The env viariable HADOOP_CONF_DIR=/etc/hadoop doesn't exist in the container instantiated by this image. And I added "ENV ln -s /usr/local/hadoop-2.7.2/etc/hadoop /etc/hadoop" in the dockerfile and rebuilt the image. Then this task succeeded.
		</comment>
		<comment id='2' author='victorming' date='2020-04-27T07:08:22Z'>
		
I did some exploration and found the image 'openpai/spark-example' might have some bugs. The env viariable HADOOP_CONF_DIR=/etc/hadoop doesn't exist in the container instantiated by this image. And I added "ENV ln -s /usr/local/hadoop-2.7.2/etc/hadoop /etc/hadoop" in the dockerfile and rebuilt the image. Then this task succeeded.

Thanks &lt;denchmark-link:https://github.com/victorming&gt;@victorming&lt;/denchmark-link&gt;
 for raising this up and even debug into it. Would you like to submit an PR for it so that others can benefit from your fix? thanks.
		</comment>
		<comment id='3' author='victorming' date='2020-07-09T03:52:20Z'>
		close as no further response.
		</comment>
	</comments>
</bug>