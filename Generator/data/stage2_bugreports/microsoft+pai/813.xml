<bug id='813' author='dxysun' open_date='2018-07-05T02:02:27Z' closed_time='2018-08-28T11:42:27Z'>
	<summary>Single Box System Deployment Problem:The connection to the server 10.4.20.100:8080 was refused</summary>
	<description>
When I deployed this system according to A Guide For Single Box Bootstrap, I meet a problem in the step 3.When I use paictl tool to Boot up Kubernetes,the errors are follows:
The connection to the server 10.4.20.100:8080 was refused - did you specify the right host or port?
2018-07-05 01:51:00,537 [ERROR] - k8sPaiLibrary.maintainlib.common : There will be a delay after installing, please wait.
The connection to the server 10.4.20.100:8080 was refused - did you specify the right host or port?
2018-07-05 01:51:00,591 [ERROR] - k8sPaiLibrary.maintainlib.common : There will be a delay after installing, please wait.
The connection to the server 10.4.20.100:8080 was refused - did you specify the right host or port?
2018-07-05 01:51:00,649 [ERROR] - k8sPaiLibrary.maintainlib.common : There will be a delay after installing, please wait.
The connection to the server 10.4.20.100:8080 was refused - did you specify the right host or port?
2018-07-05 01:51:00,711 [ERROR] - k8sPaiLibrary.maintainlib.common : There will be a delay after installing, please wait.
The connection to the server 10.4.20.100:8080 was refused - did you specify the right host or port?
2018-07-05 01:51:00,762 [ERROR] - k8sPaiLibrary.maintainlib.common : There will be a delay after installing, please wait.
The connection to the server 10.4.20.100:8080 was refused - did you specify the right host or port?
2018-07-05 01:51:00,838 [ERROR] - k8sPaiLibrary.maintainlib.common : There will be a delay after installing, please wait.
How can I solve this problem?
	</description>
	<comments>
		<comment id='1' author='dxysun' date='2018-07-05T02:45:32Z'>
		Some Question.

Try to run this command to check there won't be any issue of networking accessing problems.

&lt;denchmark-code&gt;curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
&lt;/denchmark-code&gt;


SSH to your master node, and running following command. and paste the result.

&lt;denchmark-code&gt;sudo docker ps -a
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='dxysun' date='2018-07-05T03:55:19Z'>
		my network is ok, run the first command,This file was successfully downloaded.
&lt;denchmark-link:https://user-images.githubusercontent.com/16951972/42301804-e48c3c94-8049-11e8-9598-152f5fc78a9a.png&gt;&lt;/denchmark-link&gt;

the second command result
&lt;denchmark-link:https://user-images.githubusercontent.com/16951972/42301844-1c79f5ce-804a-11e8-9765-3cad93489635.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='dxysun' date='2018-07-05T04:10:18Z'>
		&lt;denchmark-link:https://github.com/dxysun&gt;@dxysun&lt;/denchmark-link&gt;
 Could you also share the output of  in the second step?
		</comment>
		<comment id='4' author='dxysun' date='2018-07-05T04:29:09Z'>
		The log is too much,I paste the main log about errors.
&lt;denchmark-link:https://user-images.githubusercontent.com/16951972/42302654-d11feb92-804e-11e8-8ef7-48a510f2223e.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/16951972/42302659-d4a4b8f6-804e-11e8-8ab3-b90decc42277.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/16951972/42302661-d820b296-804e-11e8-9c8d-f1de93a5fcbf.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/16951972/42302665-dbaa70dc-804e-11e8-9edb-5124e02ef457.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/16951972/42302670-deee46ce-804e-11e8-8118-db8e5194bc10.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='dxysun' date='2018-07-05T05:47:46Z'>
		Did you use the latest code in the master branch?
		</comment>
		<comment id='6' author='dxysun' date='2018-07-05T05:59:05Z'>
		I used the official prebuild dev-box image and didn't use the latest code to build the image.
		</comment>
		<comment id='7' author='dxysun' date='2018-07-10T08:17:58Z'>
		please verify if the error is kubepods related. You can search in you log if error message as below presents.
"Failed to start ContainerManager failed to initialize top level QOS containers: root container /kubepods doesn't exist"
If this is the case, you can try this workaround. Add
--cgroups-per-qos=false --enforce-node-allocatable="" to the kubelet command in file /pai/pai-management/k8sPaiLibrary/template/kubelet.sh.template and reboot the cluster.
		</comment>
		<comment id='8' author='dxysun' date='2018-07-10T13:37:10Z'>
		In the kubelet log,I can't find the error "Failed to start ContainerManager failed to initialize top level QOS containers: root container /kubepods doesn't exist".Here is the main error
&lt;denchmark-link:https://user-images.githubusercontent.com/16951972/42513365-5243e5d2-8489-11e8-8aae-af0eb53c1cdc.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='dxysun' date='2018-07-11T07:33:31Z'>
		I also meet the problem when use 'paictl.py cluster k8s-bootup -p /path/to/cluster-configuration/dir' cmd.
		</comment>
		<comment id='10' author='dxysun' date='2018-07-13T12:29:15Z'>
		&lt;denchmark-link:https://github.com/dxysun&gt;@dxysun&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/feichaohao&gt;@feichaohao&lt;/denchmark-link&gt;
 Could you share the full kubelet logs or the first and last 1000 lines? You can paste here or use something like &lt;denchmark-link:https://paste.ubuntu.com/&gt;Ubuntu Pastebin&lt;/denchmark-link&gt;
. Please replace your ip with a fake ip before that.
Or could you just find something like the following lines in your kubelet logs?
&lt;denchmark-code&gt;RunPodSandbox from runtime service failed: rpc error: code = Unknown desc = failed pulling image "gcr.io/google_containers/pause-amd64:3.0": Error response from daemon: Get https://gcr.io/v1/_ping: dial tcp 108.177.97.82:443: i/o timeout
CreatePodSandbox for pod "etcd-server-ip_default(hash)" failed: rpc error: code = Unknown desc = failed pulling image "gcr.io/google_containers/pause-amd64:3.0": Error response from daemon: Get https://gcr.io/v1/_ping: dial tcp 108.177.97.82:443: i/o timeout
&lt;/denchmark-code&gt;

I think there is a network issue (accessing to gcr.io instead of googleapis.com) inside kubelet container.
&lt;denchmark-link:https://github.com/ydye&gt;@ydye&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/fanyangCS&gt;@fanyangCS&lt;/denchmark-link&gt;
 any sugguestion?
		</comment>
		<comment id='11' author='dxysun' date='2018-07-13T13:08:13Z'>
		the full kubelet logs is here.
&lt;denchmark-link:https://paste.ubuntu.com/p/hHysSkX7Df/&gt;https://paste.ubuntu.com/p/hHysSkX7Df/&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='dxysun' date='2018-07-13T13:32:54Z'>
		&lt;denchmark-link:https://github.com/dxysun&gt;@dxysun&lt;/denchmark-link&gt;
 From line 687 in your logs:
&lt;denchmark-code&gt;I0713 13:00:37.303730     670 kube_docker_client.go:345] Pulling image "gcr.io/google_containers/etcd:3.2.17": "8ce4b3ffb18c: Downloading [===================================&gt;               ]  39.71MB/56.47MB"
&lt;/denchmark-code&gt;

It's still pulling image gcr.io/google_containers/etcd:3.2.17 which has not been finished. You can wait for a while.
		</comment>
		<comment id='13' author='dxysun' date='2018-07-13T14:33:00Z'>
		After I run the "python paictl.py cluster k8s-bootup -p ~/pai-config" command,I wait for 20 mininus.This error "The connection to the server 10.4.20.100:8080 was refused - did you specify the right host or port"  lasted for 20 minutes.Here is the full kubelet logs.
&lt;denchmark-link:https://paste.ubuntu.com/p/dWpZYShz9R/&gt;https://paste.ubuntu.com/p/dWpZYShz9R/&lt;/denchmark-link&gt;

		</comment>
		<comment id='14' author='dxysun' date='2018-07-13T15:17:08Z'>
		&lt;denchmark-link:https://github.com/dxysun&gt;@dxysun&lt;/denchmark-link&gt;
 From your new logs, image pulling was slow in kubelet but all images have been pulled at around 10 min. From line 1322 to 1352, there was a disk pressure and it started to delete containers and images.
&lt;denchmark-code&gt;W0713 13:44:02.853907   16105 eviction_manager.go:332] eviction manager: attempting to reclaim imagefs
I0713 13:44:02.853948   16105 helpers.go:1074] eviction manager: attempting to delete unused containers
I0713 13:44:02.881492   16105 helpers.go:1084] eviction manager: attempting to delete unused images
...
W0713 13:44:02.909109   16105 eviction_manager.go:435] eviction manager: unexpected error when attempting to reduce imagefs pressure: wanted to free 9223372036854775807 bytes, but freed 0 bytes space with errors in image deletion: [rpc error: code = Unknown desc = Error response from daemon: conflict: unable to delete 5e8b97a2a082 (cannot be forced) - image has dependent child images, rpc error: code = Unknown desc = Error response from daemon: conflict: unable to delete 1bb38d61d261 (must be forced) - image is being used by stopped container dde24636cc47, rpc error: code = Unknown desc = Error response from daemon: conflict: unable to delete 4ca1d30fcd9b (must be forced) - image is being used by stopped container 8df1f36c133c, rpc error: code = Unknown desc = Error response from daemon: conflict: unable to delete d1404d62e9f2 (cannot be forced) - image is being used by running container 98fb7a177d32, rpc error: code = Unknown desc = Error response from daemon: conflict: unable to delete ad2645f368aa (cannot be forced) - image is being used by running container 3cd0676cbf04, rpc error: code = Unknown desc = Error response from daemon: conflict: unable to delete 6c95437f475f (must be forced) - image is being used by stopped container 15cf1bcd8243, rpc error: code = Unknown desc = Error response from daemon: conflict: unable to delete ee036c407479 (must be forced) - image is being used by stopped container 41025ead5db1, rpc error: code = Unknown desc = Error response from daemon: conflict: unable to delete e38bc07ac18e (must be forced) - image is being used by stopped container bb8f96423064, rpc error: code = Unknown desc = Error response from daemon: conflict: unable to delete 99e59f495ffa (cannot be forced) - image is being used by running container 9ba2925108ad]
I0713 13:44:02.909131   16105 eviction_manager.go:346] eviction manager: must evict pod(s) to reclaim imagefs
&lt;/denchmark-code&gt;

Could you also check the disk space please?
		</comment>
		<comment id='15' author='dxysun' date='2018-07-13T15:52:56Z'>
		my disk space is enough.
Filesystem      Size  Used Avail Use% Mounted on
udev             16G     0   16G   0% /dev
tmpfs           3.2G   50M  3.1G   2% /run
/dev/nvme0n1p2  198G  179G  8.5G  96% /
tmpfs            16G  172K   16G   1% /dev/shm
tmpfs           5.0M  4.0K  5.0M   1% /run/lock
tmpfs            16G     0   16G   0% /sys/fs/cgroup
/dev/loop0       85M   85M     0 100% /snap/simplescreenrecorder-mardy/4
/dev/loop1       87M   87M     0 100% /snap/core/4917
/dev/loop2       87M   87M     0 100% /snap/core/4830
/dev/loop3       87M   87M     0 100% /snap/core/4650
/dev/nvme0n1p1  511M  3.5M  508M   1% /boot/efi
tmpfs           3.2G   84K  3.2G   1% /run/user/1000
/dev/sda1       1.8T  246G  1.5T  15% /mnt
tmpfs           3.2G     0  3.2G   0% /run/user/1001
		</comment>
		<comment id='16' author='dxysun' date='2018-07-20T12:01:21Z'>
		&lt;denchmark-link:https://github.com/dxysun&gt;@dxysun&lt;/denchmark-link&gt;

It looks like "/" has used up to 96% storage space. This will trigger k8s disk clean up action.

Please reserve a larger space or clean up enough space.
		</comment>
		<comment id='17' author='dxysun' date='2018-07-20T12:11:13Z'>
		8.5G is not enough for k8s?How much space should I allocate for k8s?
		</comment>
		<comment id='18' author='dxysun' date='2018-07-20T12:30:40Z'>
		k8s will monitor disk pressure. currently &gt;85% disk usage will trigger the disk cleanup action: k8s will kill some pod at its discretion. therefore, 96% is not enough.
Also, k8s will need to download many docker images to start PAI services, it would cost at least 10s of GB space.
In any case, please make sure the disk usage falls far below 85% to ensure safety.
		</comment>
		<comment id='19' author='dxysun' date='2018-07-20T14:33:39Z'>
		I use another computer to deploy. The disk space of this computer is enough, and I used the latest code  to build the image. Then the system is deployed according to A Guide For Single Box Bootstrap. The previous problem has been solved, but in the step 4 I meet another problem. When I ran this command "python paictl.py service start  -p ~/pai-config" to start all services, I encountered the following problems.

&lt;denchmark-link:https://user-images.githubusercontent.com/16951972/43007944-5dbf041a-8c6c-11e8-97a4-0be19843f693.png&gt;&lt;/denchmark-link&gt;

I waited for nearly an hour, it still hasn't changed.
What should I do to solve this problem?
		</comment>
		<comment id='20' author='dxysun' date='2018-07-23T11:19:35Z'>
		&lt;denchmark-link:https://github.com/dxysun&gt;@dxysun&lt;/denchmark-link&gt;
 Does the computer has GPUs? Please share the log of  pod, you can use  to find its name then use  to print the log.
		</comment>
		<comment id='21' author='dxysun' date='2018-07-24T03:39:19Z'>
		&lt;denchmark-link:https://github.com/dxysun&gt;@dxysun&lt;/denchmark-link&gt;
 the image size is large. And can you ssh to the target node. And check if the drivers image exists or not.
		</comment>
		<comment id='22' author='dxysun' date='2018-08-15T07:42:19Z'>
		&lt;denchmark-link:https://github.com/dxysun&gt;@dxysun&lt;/denchmark-link&gt;
 - any feedback on &lt;denchmark-link:https://github.com/abuccts&gt;@abuccts&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/ydye&gt;@ydye&lt;/denchmark-link&gt;
 's questions?
		</comment>
		<comment id='23' author='dxysun' date='2018-08-28T11:42:27Z'>
		no update in 1+ mon. closed
		</comment>
		<comment id='24' author='dxysun' date='2019-04-28T07:52:14Z'>
		
kubectl get pods

&lt;denchmark-code&gt;+ export MLNX_PREFIX=/var/drivers/mellanox/MLNX_OFED_LINUX-4.2-1.2.0.0-ubuntu16.04-x86_64/usermode
+ MLNX_PREFIX=/var/drivers/mellanox/MLNX_OFED_LINUX-4.2-1.2.0.0-ubuntu16.04-x86_64/usermode
+ export NV_DRIVER=/var/drivers/nvidia/384.111
+ NV_DRIVER=/var/drivers/nvidia/384.111
+ export LIBRARY_PATH=/usr/local/cuda/lib64/stubs:/var/drivers/mellanox/MLNX_OFED_LINUX-4.2-1.2.0.0-ubuntu16.04-x86_64/usermode/lib
+ LIBRARY_PATH=/usr/local/cuda/lib64/stubs:/var/drivers/mellanox/MLNX_OFED_LINUX-4.2-1.2.0.0-ubuntu16.04-x86_64/usermode/lib
+ export LD_LIBRARY_PATH=/var/drivers/mellanox/MLNX_OFED_LINUX-4.2-1.2.0.0-ubuntu16.04-x86_64/usermode/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/var/drivers/nvidia/384.111/lib:/var/drivers/nvidia/384.111/lib64:/usr/local/cuda/lib64
+ LD_LIBRARY_PATH=/var/drivers/mellanox/MLNX_OFED_LINUX-4.2-1.2.0.0-ubuntu16.04-x86_64/usermode/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/var/drivers/nvidia/384.111/lib:/var/drivers/nvidia/384.111/lib64:/usr/local/cuda/lib64
+ export PATH=/var/drivers/mellanox/MLNX_OFED_LINUX-4.2-1.2.0.0-ubuntu16.04-x86_64/usermode/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/var/drivers/nvidia/384.111/bin
+ PATH=/var/drivers/mellanox/MLNX_OFED_LINUX-4.2-1.2.0.0-ubuntu16.04-x86_64/usermode/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/var/drivers/nvidia/384.111/bin
+ export C_INCLUDE_PATH=/var/drivers/mellanox/MLNX_OFED_LINUX-4.2-1.2.0.0-ubuntu16.04-x86_64/usermode/include:/var/drivers/mellanox/MLNX_OFED_LINUX-4.2-1.2.0.0-ubuntu16.04-x86_64/usermode/include/infiniband
+ C_INCLUDE_PATH=/var/drivers/mellanox/MLNX_OFED_LINUX-4.2-1.2.0.0-ubuntu16.04-x86_64/usermode/include:/var/drivers/mellanox/MLNX_OFED_LINUX-4.2-1.2.0.0-ubuntu16.04-x86_64/usermode/include/infiniband
+ export CPLUS_INCLUDE_PATH=/var/drivers/mellanox/MLNX_OFED_LINUX-4.2-1.2.0.0-ubuntu16.04-x86_64/usermode/include:/var/drivers/mellanox/MLNX_OFED_LINUX-4.2-1.2.0.0-ubuntu16.04-x86_64/usermode/include/infiniband
+ CPLUS_INCLUDE_PATH=/var/drivers/mellanox/MLNX_OFED_LINUX-4.2-1.2.0.0-ubuntu16.04-x86_64/usermode/include:/var/drivers/mellanox/MLNX_OFED_LINUX-4.2-1.2.0.0-ubuntu16.04-x86_64/usermode/include/infiniband
+ lspci
+ grep -qE '[0-9a-fA-F][0-9a-fA-F]:[0-9a-fA-F][0-9a-fA-F].[0-9] (3D|VGA compatible) controller: NVIDIA Corporation.*'
+ '[' -f /usr/local/nvidia/bin/nvidia-smi ']'
+ /bin/bash -x install-nvidia-drivers
++ uname -r
+ KERNEL_FULL_VERSION=4.15.0-48-generic
+ CURRENT_DRIVER=/var/drivers/nvidia/current
+ echo ======== If NVIDIA present exit early =========
+ nvidiaPresent
+ [[ -f /proc/driver/nvidia/version ]]
+ grep -q 384.111 /proc/driver/nvidia/version
======== If NVIDIA present exit early =========
+ return 2
======== If NVIDIA driver already running uninstall it =========
+ '[' 2 == 0 ']'
+ echo ======== If NVIDIA driver already running uninstall it =========
+ lsmod
+ grep -qE '^nvidia'
++ lsmod
++ cut -f 4 -d ' '
++ tr -s ' '
++ grep -E '^nvidia'
+ DEP_MODS='

nvidia_drm
nvidia_uvm,nvidia_modeset'
+ for mod in '${DEP_MODS//,/ }'
+ rmmod nvidia_drm
rmmod: ERROR: Module nvidia_drm is in use
+ echo 'The driver nvidia_drm is still in use, can'\''t unload it.'
The driver nvidia_drm is still in use, can't unload it.
+ exit 1
+ exit 1

&lt;/denchmark-code&gt;

how can i solve this problem
		</comment>
		<comment id='25' author='dxysun' date='2019-05-28T07:26:05Z'>
		
please verify if the error is kubepods related. You can search in you log if error message as below presents.
"Failed to start ContainerManager failed to initialize top level QOS containers: root container /kubepods doesn't exist"
If this is the case, you can try this workaround. Add
--cgroups-per-qos=false --enforce-node-allocatable="" to the kubelet command in file /pai/pai-management/k8sPaiLibrary/template/kubelet.sh.template and reboot the cluster.

This workaround works on my deployment. Thanks!
A related issue: &lt;denchmark-link:https://github.com/kubernetes/kubernetes/issues/43704&gt;kubernetes/kubernetes#43704&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>