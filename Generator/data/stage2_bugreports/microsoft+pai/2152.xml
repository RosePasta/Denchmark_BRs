<bug id='2152' author='hirolsj' open_date='2019-02-18T01:56:00Z' closed_time='2019-07-26T05:25:44Z'>
	<summary>[bug]  Containers cannot exit normally, but frameworks exit and reallocate the resources they occupy。</summary>
	<description>
Organization Name:
NEL-BITA
Short summary about the issue/question:
Due to hardware, OS level or docker problems, containers cannot exit normally when the framework exits.
The key problem is that the resources occupied by these containers (gpu, port and memory) are reallocated, resulting in frequent errors in user submission tasks. For example, GPU oom and occupied ports.
Therefore, our requirement is to improve the framework exit mechanism, that is, to detect containers that do not exit normally and isolate the resources they occupy.
	</description>
	<comments>
		<comment id='1' author='hirolsj' date='2019-02-18T02:36:52Z'>
		what is the version you are using? we have fixed many of the issues in 0.9.y.
		</comment>
		<comment id='2' author='hirolsj' date='2019-02-18T02:45:12Z'>
		0.8.3
		</comment>
		<comment id='3' author='hirolsj' date='2019-02-18T03:21:56Z'>
		I remember NM will detect unmanged GPU and Port usage, and avoid to allocated resource on them. &lt;denchmark-link:https://github.com/mzmssg&gt;@mzmssg&lt;/denchmark-link&gt;
 for the YARN related investigation.
		</comment>
		<comment id='4' author='hirolsj' date='2019-02-18T03:27:39Z'>
		&lt;denchmark-link:https://github.com/hirolsj&gt;@hirolsj&lt;/denchmark-link&gt;
, can we sync it privately. we'd like to have the first hand information (e.g., ssh to the servers) to understand what is going on.
		</comment>
		<comment id='5' author='hirolsj' date='2019-02-18T03:41:45Z'>
		Ok, contact me &lt;denchmark-link:mailto:shengjun@leinao.ai&gt;shengjun@leinao.ai&lt;/denchmark-link&gt;
 by email.
		</comment>
		<comment id='6' author='hirolsj' date='2019-02-18T05:51:57Z'>
		&lt;denchmark-link:https://github.com/hirolsj&gt;@hirolsj&lt;/denchmark-link&gt;
 - 关于问题的在线讨论，请加入&lt;denchmark-link:https://gitter.im/Microsoft/pai%EF%BC%8C&gt;https://gitter.im/Microsoft/pai，&lt;/denchmark-link&gt;
 Gitter有iOS and Android客户端，国内也能访问。PAI Dev会在Gitter Chat Room answer questions.
		</comment>
		<comment id='7' author='hirolsj' date='2019-02-18T06:19:58Z'>
		&lt;denchmark-link:https://github.com/microsoft/pai/pull/1646&gt;#1646&lt;/denchmark-link&gt;

this is the PR addressing zombie container issue.
		</comment>
		<comment id='8' author='hirolsj' date='2019-02-18T07:28:40Z'>
		the customer reports the process in a job container goes to D state because of uninterruptible-sleep due to I/O ops, probably due to NFS issue. &lt;denchmark-link:https://eklitzke.org/uninterruptible-sleep&gt;https://eklitzke.org/uninterruptible-sleep&lt;/denchmark-link&gt;
. When this happened, the Docker container cannot exit, and Docke itself is in unhealthy state. Admin has to reboot the server to fix the issue.
		</comment>
		<comment id='9' author='hirolsj' date='2019-02-18T09:34:48Z'>
		Thanks！
		</comment>
		<comment id='10' author='hirolsj' date='2019-02-18T10:14:57Z'>
		A workaround to avoid rebooting is killing the shimd process(The parent process of container root process), then host initd will adopt the D processes and container can exit.
Of course it's not recommanded
		</comment>
		<comment id='11' author='hirolsj' date='2019-02-18T10:21:12Z'>
		Containers should be in some status like stopping in this case. But currently it finished directly.
&lt;denchmark-link:https://github.com/yqwang-ms&gt;@yqwang-ms&lt;/denchmark-link&gt;

GPU conflict is because NM only detect memory leak, no other detection.
Port conflict is a known bug to fix.&lt;denchmark-link:https://github.com/microsoft/pai/issues/1983&gt;#1983&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='hirolsj' date='2019-02-18T11:54:46Z'>
		&lt;denchmark-link:https://github.com/mzmssg&gt;@mzmssg&lt;/denchmark-link&gt;

I synced with Qingcha who implement GPU and ported the Port codes, he said NM has the GPU and Port detection.
And for the Port issue, it should be easy to fix, please take a look.
		</comment>
		<comment id='13' author='hirolsj' date='2019-02-18T12:44:40Z'>
		Double synced with Qingcha, we confirmed that current unmanaged GPU detection is not work.
		</comment>
		<comment id='14' author='hirolsj' date='2019-02-21T09:20:36Z'>
		&lt;denchmark-link:https://github.com/hirolsj&gt;@hirolsj&lt;/denchmark-link&gt;

More details to help you understand this topic:
Currently, we have 2 docker cotaniners associated to 1 yarn container. One is yarn container script(contaienr A), the other is the real job container(container B).
When killing job, YARN only care about A and kill it, then B could detect it, suicide and release resource. The issue happens because  B's suicide is blocked by kernal mode processes.
Back to your question: why can't we keep container  before resource released.
Because we rely on YARN, but YARN only best-effort kill container with SIGKILL. As you know, it doesn't work For 'D' processes.
		</comment>
		<comment id='15' author='hirolsj' date='2019-02-21T13:44:37Z'>
		According to &lt;denchmark-link:https://github.com/mzmssg&gt;@mzmssg&lt;/denchmark-link&gt;
, here is a possible fix.


in Yarn NM, do not change the state to container-completed, until NM makes sure the Yarn container quits.
Currently NM sends a SIGKILL and it believes the YarnContainer must exit, so it changes the state without making sure the YarnContainer state. (that's why it is best-effort). Normally, this is ok but not in your case.
A possible solution is to modify NM's state machine and let NM waits in a state until the YarnContainer exits.


in YarnContainer, do not quit until it makes sure DockerContainer quit.
Unfortunately, YarnContainer will be terminated if NM sends a SIGKILL. There is no way to ignore it . One way around is to modify NM so that it does not send SIGKILL. (this is dangerous in general but ok if you only run YarnContainerScript in PAI) .


For 1), we need to be careful to ensure the fix on state machine of NM is correct. This requires time. For 2), the fix will reduce the generality of PAI: the fix implies the container managed by NM will honor other SIGNAL (e.g., SIGTERM). but malicious container can exploit this to become a rogue container: it can hold on the resource infinitely.
My suggestion is to fix the uninterrupted-sleep issue, rather than applying 1 and 2. If you cannot fix that, 1 and 2 can be a workaround, but we cannot take it as a general solution at current stage. The recommended solution is to have an alert for such zombie container. Once you receive the alert, you decommission the node, reboot and recommission it. Although there is a chance for YARN to mistakenly schedule a job on that node before you decommission it, the job will get retried by system automatically (without burning the retrycount).
Does that make sense?
		</comment>
		<comment id='16' author='hirolsj' date='2019-02-21T14:17:12Z'>
		BTW, NM is NOT so "best effort", if NM does not restart, NM will wait until it has confirmed that the root process (i.e. the container A mentioned by mzmssg above) is exited, then tell RM to release its resource.
Another option is to detect the unmanaged resource which is not used by YARN, and RM will not allocate container for these resource.
		</comment>
		<comment id='17' author='hirolsj' date='2019-02-22T02:27:01Z'>
		&lt;denchmark-link:https://github.com/fanyangCS&gt;@fanyangCS&lt;/denchmark-link&gt;
 Ok. It's a  reasonable solution. It would be nice to have a alert and a tool (decommission/recommission nodes) for such cases.
Thanks!
		</comment>
		<comment id='18' author='hirolsj' date='2019-02-22T02:58:25Z'>
		&lt;denchmark-link:https://github.com/hirolsj&gt;@hirolsj&lt;/denchmark-link&gt;

The decommission tool will be added in v0.10. We are focusing on alert now.
&lt;denchmark-link:https://github.com/yqwang-ms&gt;@yqwang-ms&lt;/denchmark-link&gt;

It's an option, but there is a latency for the hardware detection, so jobs still might be scheduled to this node.
		</comment>
		<comment id='19' author='hirolsj' date='2019-02-22T03:11:18Z'>
		I have not dived into code, but seems not really, the total utilized resource are always unchanged, and RM should not always allocate resource on it. So, I cannot see latency or race condition here from my first sight, you can dive into code if you have time, and point out where is the latency you mentioned.
		</comment>
		<comment id='20' author='hirolsj' date='2019-02-22T04:09:20Z'>
		
the total utilized resource are always unchanged, and RM should not always allocate resource on it

The total utilized resource means utilized by external processes, such as zombie container or host processes? If so, it should be changed by hardware detection.
Or I misunderstand something?
		</comment>
		<comment id='21' author='hirolsj' date='2019-02-22T04:10:49Z'>
		The total utilized resource means the machine total utilization, including all utilizations on the machine, such as container utilization + YARN external utilization + YARN itself utilization, etc.
		</comment>
		<comment id='22' author='hirolsj' date='2019-02-22T04:28:14Z'>
		The "always" word, I mean, the duration that a managed YARN container becomes a unmanaged external processes.
And for this case, before NM wrongly release container, Total Used (1) = Container GPU Used (1) + External Used (0), after NM wrongly release container, Total Used (1) = Container GPU Used (0) + External Used (1). You can see, the Total Used (1) is unchanged, and across all the time, RM should not allocated new container on the already "Total Used (1)".
		</comment>
		<comment id='23' author='hirolsj' date='2019-02-22T05:46:17Z'>
		&lt;denchmark-link:https://github.com/yqwang-ms&gt;@yqwang-ms&lt;/denchmark-link&gt;

That's the point, after NM wrongly release container, we need the transistion is
&lt;denchmark-code&gt;(container[1], external[0]) -&gt;(container[0], external[1])
&lt;/denchmark-code&gt;

But actually it might be
&lt;denchmark-code&gt;(container[1], external[0]) -&gt; (container[0], external[0]) -&gt; (container[0], external[1])
&lt;/denchmark-code&gt;

The fisrt -&gt; is NM release container, the second -&gt; is NM detect the zombie container.
I remember that NM implementaion is:
One thread to detect hardware and store in memory.
Another thread to heart-beat the info to RM.
If the heart-beat thread runs after releasing container, but before detection thread, it would be in the intermedia state (container[0], external[0]).
		</comment>
		<comment id='24' author='hirolsj' date='2019-02-22T06:28:02Z'>
		Note that you always directly get Total Used from OS system, then External Used is got from Total Used - Container Used. Anyway, the Total Used is unchanged.
The intermedia state (container[0], external[0]). can be avoided, because you can send Total Used to RM, and in any time, the Total Used is unchanged.
		</comment>
		<comment id='25' author='hirolsj' date='2019-02-22T09:19:50Z'>
		I get you, so NM only report utilized GPU, RM union it with allocated GPU. Then the supplementary is available for scheduler?
		</comment>
		<comment id='26' author='hirolsj' date='2019-02-22T09:32:03Z'>
		Yes
		</comment>
		<comment id='27' author='hirolsj' date='2019-02-22T09:34:53Z'>
		BTW, I do not know you "only report" mean, suggest to keep current report info and add new if required.
		</comment>
		<comment id='28' author='hirolsj' date='2019-07-26T05:25:44Z'>
		close as no response for a long time.
		</comment>
	</comments>
</bug>