<bug id='636' author='jwdink' open_date='2020-05-13T16:39:41Z' closed_time='2020-06-06T18:44:13Z'>
	<summary>Issue with LBGFS optimizer: `optimizer_.zero_grad` no longer called inside of `train_step_single`</summary>
	<description>
It looks like in the following commit, the call to optimizer_.zero_grad was removed from train_step_single, and instead put inside of train_step:
&lt;denchmark-link:https://github.com/skorch-dev/skorch/commit/a62e4190d38bf698a314c4283ca7a9dd7cfbe428#diff-5d1f8e13ab8b822b72e9b295cf7301c5L596&gt;a62e419#diff-5d1f8e13ab8b822b72e9b295cf7301c5L596&lt;/denchmark-link&gt;

As far as I can tell, this isn't appropriate if you're using the LBFGS optimizer -- for that optimizer you need to zero the gradient on every call to the closure.
I ran into this because nets that had previously worked on skorch 0.60 now quickly hit  loss on skorch 0.80. I reverted versions and hunted down this commit for the cause (see &lt;denchmark-link:https://github.com/strongio/strong-glm/blob/master/examples/ceiling_weibull.py&gt;here&lt;/denchmark-link&gt;
 for an example).
	</description>
	<comments>
		<comment id='1' author='jwdink' date='2020-05-13T18:36:20Z'>
		&lt;denchmark-link:https://github.com/jwdink&gt;@jwdink&lt;/denchmark-link&gt;
 Thank you a lot for reporting this error.
You are right, the commit you identified is the reason for the trouble.
This is actually a difficult problem to solve. The idea of moving the zero_grad out of each individual call of optimizer.step was to allow the user to have more control over it, e.g. to implement gradient accumulation. This is in conflict with optimizers that want to call the closure multiple times and need zero_grad after each of it.
As a quick fix for you, I would propose to either restore the previous state or move the call to zero_grad inside the step_fn closure here:



skorch/skorch/net.py


        Lines 655 to 660
      in
      1f6b542






 def step_fn(): 



 step = self.train_step_single(Xi, yi, **fit_params) 



 step_accumulator.store_step(step) 



 return step['loss'] 



 self.optimizer_.step(step_fn) 



 self.optimizer_.zero_grad() 





I believe it should fix your issue.
Now what could be done in the long run? Hard to tell. Reverting the change removes the mentioned benefit. We could, hypothetically, add a parameter to NeuralNet that allows to switch between the two. But that still requires the user of LBFGS or similar optimizers to actively remember that. It's thus not a very nice solution.
If we could somehow know what optimizers actually need to call zero_grad, we could control the behavior dynamically. But I don't think we can know that. So this is also not a solution.
I never use optimizers that require that, thus have almost no experience. Do you have any good idea?
		</comment>
		<comment id='2' author='jwdink' date='2020-05-13T20:35:05Z'>
		
Now what could be done in the long run? Hard to tell. Reverting the change removes the mentioned benefit. We could, hypothetically, add a parameter to NeuralNet that allows to switch between the two. But that still requires the user of LBFGS or similar optimizers to actively remember that. It's thus not a very nice solution.

Since the change was made to accommodate additional control, would it make sense to add that argument to NeuralNet, but its default value causes the old behavior (i.e. calling zero_grad inside)? This way, only users looking for additional control (e.g. accumulating gradients) have to override this default argument. For everyone else, all the optimizers from the torch.optim module would work with NeuralNet out of the box.
		</comment>
		<comment id='3' author='jwdink' date='2020-05-14T20:19:41Z'>
		
but its default value causes the old behavior

Unfortunately, the changed behavior is part of our last release. Therefore, if we changed it back to the old behavior, we would create yet another breakage. But there is an argument to be made that if possible, skorch should work with the optimizers provided by PyTorch, and those include LBFGS.
So we could introduce a parameter like perform_zero_grad_each_step=True or so (not a very nice name) and if a user wants to do their custom stuff with gradients, they need to turn it off.
&lt;denchmark-link:https://github.com/ottonemo&gt;@ottonemo&lt;/denchmark-link&gt;
 do you have a better idea?
		</comment>
		<comment id='4' author='jwdink' date='2020-05-15T14:54:33Z'>
		
So we could introduce a parameter like perform_zero_grad_each_step=True or so (not a very nice name) and if a user wants to do their custom stuff with gradients, they need to turn it off.

Yes sorry if I was being unclear -- this is exactly what I was proposing.
		</comment>
		<comment id='5' author='jwdink' date='2020-05-15T19:26:34Z'>
		No, you were not unclear, I was just elaborating on what you said :)
		</comment>
		<comment id='6' author='jwdink' date='2020-05-16T19:51:56Z'>
		&lt;denchmark-link:https://github.com/jwdink&gt;@jwdink&lt;/denchmark-link&gt;
 I created a PR (&lt;denchmark-link:https://github.com/skorch-dev/skorch/pull/639&gt;#639&lt;/denchmark-link&gt;
) to fix this issue. Could you please check if your issue is solved when you use skorch from that branch?
		</comment>
		<comment id='7' author='jwdink' date='2020-05-18T14:54:33Z'>
		Yep, that fixes it. Thanks for the speed response!
		</comment>
		<comment id='8' author='jwdink' date='2020-06-06T18:44:13Z'>
		Fixed by &lt;denchmark-link:https://github.com/skorch-dev/skorch/pull/639&gt;#639&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>