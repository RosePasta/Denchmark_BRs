<bug id='93' author='ottonemo' open_date='2017-10-19T14:54:45Z' closed_time='2017-10-26T07:40:51Z'>
	<summary>Disable `use_cuda` after loading a model on a non-CUDA machine</summary>
	<description>
Currently we warn that CUDA is not supported but the model still has use_cuda=True.
	</description>
	<comments>
		<comment id='1' author='ottonemo' date='2017-10-19T19:08:46Z'>
		It actually works fine for me with CUDA  including GridSearchCV and RandomizedSearchCV - would be disappointing to discontinue since it is valuable for model selection.
There are some pitfalls - see example below - it requires Python 3.6.1, Anaconda numpy 1.13.1 (not pip installed) - you have to use "import multiprocessing and if name == 'main':  .... " as below to address the CUDA / Python incompatibility regarding fork and you have to define the PyTorch net in a separate python file to address the following:

https://stackoverflow.com/questions/36533134/cant-get-attribute-abc-on-module-main-from-abc-h-py
If you declare the pool prior to declaring the function you are trying to use in parallel it will throw this error. Reverse the order and it will no longer throw this error.

test.py:
&lt;denchmark-code&gt;import multiprocessing

if __name__ == '__main__':
    multiprocessing.set_start_method('spawn')

    import numpy as np

    import torch
    from torch import nn
    import torch.nn.functional as F

    from skorch.net import NeuralNetClassifier

    from sklearn.datasets import make_classification
    from sklearn import metrics
    from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

    X, y = make_classification(1000, 20, n_informative=10, random_state=None)
    X, y = X.astype(np.float32), y.astype(np.int64)

    from mymodule import *

    estimator = NeuralNetClassifier(
        MyModule,
        max_epochs=10,
        verbose=0,
        use_cuda = torch.cuda.is_available()
    )

    estimator.fit(X, y)
    y_proba = estimator.predict_proba(X)
    y_proba

    param_grid = {
        'module__num_units' : [10, 20, 30],
        'max_epochs': [10, 50, 100],
        'lr' : [0.001, 0.01, 0.1],
        'batch_size' : [40, 60, 80, 100],
        'optim' : [torch.optim.SGD, torch.optim.RMSprop],
        'optim__momentum' : [0.0,  0.6, 0.8, 0.9],
        'module__nonlin': [F.relu, F.tanh]
    }

    gs = RandomizedSearchCV(estimator, param_grid, cv=3, scoring='accuracy', verbose=1, n_jobs=-1)
    gs.fit(X, y)
    print(gs.best_score_, gs.best_params_)

    y_pred = gs.predict(X)
    print ("Generalization Accuracy: %.6f" %metrics.accuracy_score(y,y_pred))

**mymodule.py:**

import torch
from torch import nn
import torch.nn.functional as F

class MyModule(torch.nn.Module):
    def __init__(self, num_units=10, nonlin=F.relu):
        super(MyModule, self).__init__()

        self.dense0 = nn.Linear(20, num_units)
        self.nonlin = nonlin
        self.dropout = nn.Dropout(0.5)
        self.dense1 = nn.Linear(num_units, 10)
        self.output = nn.Linear(10, 2)

    def forward(self, X, **kwargs):
        X = self.nonlin(self.dense0(X))
        X = self.dropout(X)
        X = F.relu(self.dense1(X))
        X = F.softmax(self.output(X))
        return X

&lt;/denchmark-code&gt;

Results for the above

Fitting 3 folds for each of 10 candidates, totalling 30 fits
[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  2.1min finished
0.935 {'optim__momentum': 0.8, 'optim': &lt;class 'torch.optim.sgd.SGD'&gt;, 'module__num_units': 30, 'module__nonlin': &lt;function relu at 0x12d980a60&gt;, 'max_epochs': 50, 'lr': 0.1, 'batch_size': 100}
Generalization Accuracy: 0.966000

I run it on a Mac Pro with an external TitanXP using all 8 cores and it is very helpful.
&lt;denchmark-code&gt;&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='ottonemo' date='2017-10-20T08:27:50Z'>
		I don't believe the suggested change would affect your setup, so don't worry. It should only affect the case where someone trains a model on cuda and then loads it on a machine without access to cuda. Then you currently get an error if you continue using the model.
Btw., we renamed the optim argument to optimizer. If you update skorch, remember to change this.
		</comment>
	</comments>
</bug>