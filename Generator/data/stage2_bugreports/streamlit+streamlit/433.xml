<bug id='433' author='dnuske' open_date='2019-10-16T19:05:56Z' closed_time='2020-07-20T22:59:42Z'>
	<summary>st.write throws value is null when string is too big</summary>
	<description>
&lt;denchmark-h:h1&gt;Summary&lt;/denchmark-h&gt;

calling st.write with a string that is too big will result on an error message on the front end with message "value is null"
&lt;denchmark-link:https://user-images.githubusercontent.com/934511/66950384-a17d0e00-f02e-11e9-87bb-1df7158794b5.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-h:h1&gt;Steps to reproduce&lt;/denchmark-h&gt;


create a script with a variable that holds a string that's more than weights more than 50mb
streamlit run yourscript.py
message will appear on the front end

&lt;denchmark-h:h2&gt;Expected behavior:&lt;/denchmark-h&gt;

The error message should be clearer.
&lt;denchmark-h:h2&gt;Actual behavior:&lt;/denchmark-h&gt;

Error message does not point to the string length constrain of st.write.
	</description>
	<comments>
		<comment id='1' author='dnuske' date='2019-10-17T08:32:34Z'>
		This occurs with video files too, with st.video
		</comment>
		<comment id='2' author='dnuske' date='2019-10-23T23:36:23Z'>
		It's occurring with st.dataframe as well when the dataframe is big.
		</comment>
		<comment id='3' author='dnuske' date='2020-01-21T18:48:21Z'>
		Running into the same problem with st.video(), will try to reproduce later.
		</comment>
		<comment id='4' author='dnuske' date='2020-02-14T00:56:38Z'>
		Confirmed: st.write issue still outstanding.
&lt;denchmark-link:https://github.com/streamlit/streamlit/issues/415&gt;#415&lt;/denchmark-link&gt;
 (merged but not yet released) will clear up the st.video issue.
&lt;denchmark-link:https://github.com/streamlit/streamlit/pull/1021&gt;#1021&lt;/denchmark-link&gt;
 (in progress) might clear up the dataframe issue.
		</comment>
		<comment id='5' author='dnuske' date='2020-06-29T13:16:26Z'>
		Hi All, I have, from what I gather, the same problem loading large Datasets for visualisation (see also &lt;denchmark-link:https://discuss.streamlit.io/t/very-large-datasets/3168?u=brey&gt;https://discuss.streamlit.io/t/very-large-datasets/3168?u=brey&lt;/denchmark-link&gt;
).
Any possibility to fix it soon?
		</comment>
		<comment id='6' author='dnuske' date='2020-07-07T19:28:56Z'>
		Hi &lt;denchmark-link:https://github.com/brey&gt;@brey&lt;/denchmark-link&gt;
 -- Loading large datasets isn't exactly the same issue.  We're still working on a major overhaul of the dataframe processing code.  See &lt;denchmark-link:https://github.com/streamlit/streamlit/pull/1021&gt;#1021&lt;/denchmark-link&gt;
 for updates.
		</comment>
	</comments>
</bug>