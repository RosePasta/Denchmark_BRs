<bug id='1200' author='NiklasWilke' open_date='2020-03-09T14:07:52Z' closed_time='2020-06-01T17:44:43Z'>
	<summary>Big memory consumption when displaying dataframes</summary>
	<description>
&lt;denchmark-h:h1&gt;Summary&lt;/denchmark-h&gt;

When i start up a streamlit application and use st.dataframe or st.write on the full dataset it keeps increasing and increasing the used memory untill it overpasses my 16GB memory.
Edit:
When loading a 3KB .csv file and using st.write(df) on that loaded data, it results in the application consuming 87MB. (x29.000)
If i apply that factor, to the 300MB .csv file im trying to display. This results in 8700 GB allocated memory. (theoretically)
Is this the expected behaviour ?
&lt;denchmark-h:h1&gt;Steps to reproduce&lt;/denchmark-h&gt;

Run a python file as a streamlit application.
use :
st.dataframe(df)
st.write(df)
&lt;denchmark-h:h2&gt;Actual behavior:&lt;/denchmark-h&gt;

It just "Runs" forever untill i have to close the application.
&lt;denchmark-h:h1&gt;Debug info&lt;/denchmark-h&gt;


Windows 10
Streamlit version:  0.56.0
Python version: 3.8.1
Conda
OS version: Win10
Browser version: Chrome Version 80.0.3987.132

&lt;denchmark-link:https://user-images.githubusercontent.com/32451154/76247723-4d263500-6240-11ea-82ef-77093321cd06.PNG&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='NiklasWilke' date='2020-04-05T17:00:14Z'>
		Hi &lt;denchmark-link:https://github.com/NiklasWilke&gt;@NiklasWilke&lt;/denchmark-link&gt;


When loading a 3KB .csv file and using st.write(df) on that loaded data, it results in the application consuming 87MB. (x29.000)

Did you record how much memory the application consumes if you run your app without loading and displaying any data? Or in other words, can you do a test run with a 6KB .csv and record memory usage? I'm assuming it will jump incrementally, for ex. to 88MB, not double.

If i apply that factor, to the 300MB .csv file im trying to display

Why are you trying to display so much data at once?
Can you workaround this, perhaps using one of &lt;denchmark-link:https://discuss.streamlit.io/t/scalabilty-of-streamlit-for-pandas/131/4&gt;these techniques&lt;/denchmark-link&gt;
?

it keeps increasing and increasing the used memory untill it overpasses my 16GB memory.

We're working on switching from protobuf to arrow for data serialization which should alleviate some of the high memory usage you're seeing on the python side.
		</comment>
		<comment id='2' author='NiklasWilke' date='2020-06-01T17:44:43Z'>
		Closing due to lack of a response from the user. Feel free to re-open.
		</comment>
	</comments>
</bug>