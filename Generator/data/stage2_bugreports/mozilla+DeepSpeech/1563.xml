<bug id='1563' author='reuben' open_date='2018-09-19T16:57:29Z' closed_time='2018-09-28T07:14:38Z'>
	<summary>Non-deterministic output with the streaming model</summary>
	<description>
Example:
&lt;denchmark-code&gt;./deepspeech --model ~/Development/DeepSpeech/1300_export/output_graph.pbmm --alphabet ~/Development/DeepSpeech/data/alphabet.txt --lm ~/Development/DeepSpeech/data/lm/lm.binary --trie ~/Development/DeepSpeech/data/lm/trie --audio audio/repeat
TensorFlow: v1.6.0-18-g50214731ea
DeepSpeech: unknown
2018-09-19 13:54:04.607023: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Running on directory audio/repeat
&gt; audio/repeat/16.wav
why should one hall on the dway
&gt; audio/repeat/17.wav
why should one hall on the dway
&gt; audio/repeat/15.wav
why should one hall on the dway
&gt; audio/repeat/29.wav
why should one hall on the dway
&gt; audio/repeat/28.wav
why should one hall on the dway
&gt; audio/repeat/14.wav
why should one hall on the dway
&gt; audio/repeat/10.wav
why should one hall on the dway
&gt; audio/repeat/11.wav
why should one hall on the dway
&gt; audio/repeat/13.wav
why should one hall on the dway
&gt; audio/repeat/12.wav
why should one hall to on the way'
&gt; audio/repeat/9.wav
why should one hall on the dway
&gt; audio/repeat/8.wav
why should one hall on the dway
&gt; audio/repeat/6.wav
why should one hall on the dway
&gt; audio/repeat/7.wav
why should one hall on the dway
&gt; audio/repeat/5.wav
why should one hall on the dway
&gt; audio/repeat/4.wav
why should one halt on the way'
&gt; audio/repeat/1.wav
why should one hall on the dway
&gt; audio/repeat/3.wav
why should one hall on the dway
&gt; audio/repeat/2.wav
why should one hall on the way'
&gt; audio/repeat/23.wav
why should one hall on the dway
&gt; audio/repeat/22.wav
why should one hall on the dway
&gt; audio/repeat/20.wav
why should one hall on the dway
&gt; audio/repeat/21.wav
why should one hall on the dway
&gt; audio/repeat/19.wav
why should one hall on the dway
&gt; audio/repeat/25.wav
why should one halt on the way'
&gt; audio/repeat/24.wav
why should one hall on the dway
&gt; audio/repeat/30.wav
why should one halt on the way'
&gt; audio/repeat/18.wav
why should one hall on the dway
&gt; audio/repeat/26.wav
why should one hall to on the way'
&gt; audio/repeat/27.wav
why should one hall on the dway
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='reuben' date='2018-09-19T23:28:32Z'>
		./deepspeech --model ~/Development/DeepSpeech/1300_export/output_graph.pbmm --alphabet ~/Development/DeepSpeech/data/alphabet.txt --lm ~/Development/DeepSpeech/data/lm/lm.binary --trie ~/Development/DeepSpeech/data/lm/trie --audio audio/repeat
TensorFlow: v1.6.0-18-g50214731ea
DeepSpeech: unknown
2018-09-19 13:54:04.607023: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Running on directory audio/repeat

audio/repeat/16.wav
why should one hall on the dway
audio/repeat/17.wav
why should one hall on the dway
audio/repeat/15.wav
why should one hall on the dway
audio/repeat/29.wav
why should one hall on the dway
audio/repeat/28.wav
why should one hall on the dway
audio/repeat/14.wav
why should one hall on the dway
audio/repeat/10.wav
why should one hall on the dway
audio/repeat/11.wav
why should one hall on the dway
audio/repeat/13.wav
why should one hall on the dway
audio/repeat/12.wav
why should one hall to on the way'
audio/repeat/9.wav
why should one hall on the dway
audio/repeat/8.wav
why should one hall on the dway
audio/repeat/6.wav
why should one hall on the dway
audio/repeat/7.wav
why should one hall on the dway
audio/repeat/5.wav
why should one hall on the dway
audio/repeat/4.wav
why should one halt on the way'
audio/repeat/1.wav
why should one hall on the dway
audio/repeat/3.wav
why should one hall on the dway
audio/repeat/2.wav
why should one hall on the way'
audio/repeat/23.wav
why should one hall on the dway
audio/repeat/22.wav
why should one hall on the dway
audio/repeat/20.wav
why should one hall on the dway
audio/repeat/21.wav
why should one hall on the dway
audio/repeat/19.wav
why should one hall on the dway
audio/repeat/25.wav
why should one halt on the way'
audio/repeat/24.wav
why should one hall on the dway
audio/repeat/30.wav
why should one halt on the way'
audio/repeat/18.wav
why should one hall on the dway
audio/repeat/26.wav
why should one hall to on the way'
audio/repeat/7.htc
why should one hall on the dway

		</comment>
		<comment id='2' author='reuben' date='2018-09-20T05:15:53Z'>
		Another data point, but this time in the opposite direction.
Compiled v0.2.0 on Ubuntu 16.04, Deep Learning base image from AWS + Dockerfile, and ran the native client 80 times over the 8455-210777-0068.wav example audio, each time obtaining the same result.
So maybe it's the wrapping of the native code in Python, Node, or Rust? Or maybe it's some mismatch between the target platform and the platform the code was compiled for?
		</comment>
		<comment id='3' author='reuben' date='2018-09-20T13:50:13Z'>
		I've disabled using the LM decoder in the prod tests as we were getting intermittent failures, we should go back to using the LM there when this is fixed.
		</comment>
		<comment id='4' author='reuben' date='2018-09-25T18:14:18Z'>
		
I've disabled using the LM decoder in the prod tests as we were getting intermittent failures, we should go back to using the LM there when this is fixed.

FTR, there is still some intermittence even after disabling the LM. It looks less frequent, though.
		</comment>
		<comment id='5' author='reuben' date='2018-09-26T10:43:36Z'>
		Update: looking at retriggers failures on &lt;denchmark-link:https://tools.taskcluster.net/groups/UgiKCb1TSYuMCy0waX5o-g&gt;https://tools.taskcluster.net/groups/UgiKCb1TSYuMCy0waX5o-g&lt;/denchmark-link&gt;
, it would looks like only NodeJS and Python prod are impacted.
C++, NodeJS and Python prod tests are all using the LM through the same run_prod_inference_tests function. Though, as much as I can tell, in all of the failures reported in the above link, it is when we test the inference on the 44kHz to 16kHz downsampling.
C++ links directly to libsox while NodeJS and Python more or less relies on stdout of sox binary. Maybe there is a slight variation and it triggers intermittence. At that point, I'm starting to think that the TaskCluster intermittence is unrelated to the one reported here because of the LM ?
		</comment>
		<comment id='6' author='reuben' date='2018-09-26T16:34:54Z'>
		&lt;denchmark-link:https://github.com/kdavis-mozilla&gt;@kdavis-mozilla&lt;/denchmark-link&gt;
 I have been able to reproduce locally with v0.2.0 C++ binaries and the use of LM, so if there's anything, it's unrelated to Python, Node or Rust and it's lower-level.
		</comment>
		<comment id='7' author='reuben' date='2018-09-26T17:41:46Z'>
		&lt;denchmark-link:https://github.com/lissyx&gt;@lissyx&lt;/denchmark-link&gt;
 As an addendum to my comment. I ran the native client 80 times. But each time I restarted the client. I did not pass a directory full of wav files to the client.
&lt;denchmark-link:https://github.com/reuben&gt;@reuben&lt;/denchmark-link&gt;
 on the other hand passed a directory to the client so his client was never restarted. So it seems there is some language model state that is not properly "re-set" when multiple wav file are decoded in a single client run.
		</comment>
		<comment id='8' author='reuben' date='2018-09-27T04:49:07Z'>
		
@lissyx As an addendum to my comment. I ran the native client 80 times. But each time I restarted the client. I did not pass a directory full of wav files to the client.

Yep, that could be consistent, I did reproduce on a full directory pass, and this was after the 95th iteration.
i'll go and play with RR now that the TaskCluster issue has been addressed and we know it's unrelated.
		</comment>
		<comment id='9' author='reuben' date='2018-09-27T09:29:11Z'>
		&lt;denchmark-link:https://github.com/mozilla/DeepSpeech/files/2423393/deepspeech.valgrind.log&gt;deepspeech.valgrind.log&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='reuben' date='2018-09-27T11:45:42Z'>
		I'm still reproducing even with the patches from &lt;denchmark-link:https://github.com/mozilla/DeepSpeech/pull/1600&gt;#1600&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='reuben' date='2018-09-27T12:50:36Z'>
		looks like &lt;denchmark-link:https://github.com/mozilla/DeepSpeech/issues/1602&gt;#1602&lt;/denchmark-link&gt;
 will do it :)
		</comment>
		<comment id='12' author='reuben' date='2018-09-28T07:14:38Z'>
		Fixed by &lt;denchmark-link:https://github.com/mozilla/DeepSpeech/pull/1603&gt;#1603&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='reuben' date='2018-09-28T10:18:53Z'>
		Thanks for the fix!
		</comment>
		<comment id='14' author='reuben' date='2019-01-02T16:45:48Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>