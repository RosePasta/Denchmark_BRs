<bug id='2403' author='carlfm01' open_date='2019-10-07T08:58:32Z' closed_time='2019-10-19T09:06:25Z'>
	<summary>Investigate memory leak</summary>
	<description>

Have I written custom code (as opposed to running examples on an unmodified clone of the repository): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Enterprise
TensorFlow version of the Nuget: v1.13.1-13-g174b4760eb
DeepSpeech version of the Nuget:v0.6.0-alpha.1-0-g90e7980
Python version: NA
Bazel version (if compiling from source): NA
GCC/Compiler version (if compiling from source): NA
CUDA/cuDNN version: NA
GPU model and memory: NA
Exact command to reproduce: NA. Using C# client
Nuget version: :0.6.0-alpha.1

Description:
Memory not being released on model destroy when using the language model.
The issue is coming from 0.6.0-alpha.1, 0.6.0-alpha.0 works just fine.
&lt;denchmark-link:https://github.com/lissyx&gt;@lissyx&lt;/denchmark-link&gt;
 the LAZY config change is not the source of the issue, lazy change: &lt;denchmark-link:https://github.com/mozilla/DeepSpeech/pull/2385&gt;#2385&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='carlfm01' date='2019-10-07T08:59:31Z'>
		
The issue is coming from 0.6.0-alpha.1, 0.6.0-alpha.0 works just fine.

Thanks for that. Do you reproduce on Linux as well?
		</comment>
		<comment id='2' author='carlfm01' date='2019-10-07T09:00:31Z'>
		
Thanks for that. Do you reproduce on Linux as well?

Not yet, I was testing the Nugets first
		</comment>
		<comment id='3' author='carlfm01' date='2019-10-07T09:01:28Z'>
		The only big changes in that window are indeed related to the LM: &lt;denchmark-link:https://github.com/mozilla/DeepSpeech/commit/31afc6811f1cacc7eba1943e27931de3d7b8a8dc&gt;31afc68&lt;/denchmark-link&gt;
 cc &lt;denchmark-link:https://github.com/reuben&gt;@reuben&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='carlfm01' date='2019-10-07T09:08:03Z'>
		In your tests are you specifying an empty trie path and relying on the Scorer to create it dynamically? In other words, is Scorer::fill_dictionary being called?
		</comment>
		<comment id='5' author='carlfm01' date='2019-10-07T09:13:44Z'>
		
In your tests are you specifying an empty trie path and relying on the Scorer to create it dynamicall

No, I'm using a trie file path, let me see without it.
		</comment>
		<comment id='6' author='carlfm01' date='2019-10-07T10:10:36Z'>
		Without the trie, it keeps in the range from 2GB to 4GB every run, without a noticeable difference in resource usage from the first one and the last one.
		</comment>
		<comment id='7' author='carlfm01' date='2019-10-07T15:09:32Z'>
		So far I have a hard time reproducing the leak. &lt;denchmark-link:https://github.com/carlfm01&gt;@carlfm01&lt;/denchmark-link&gt;
 do you have specific code to share for reproducing ? amounts of memory leaked ?
		</comment>
		<comment id='8' author='carlfm01' date='2019-10-07T16:10:29Z'>
		&lt;denchmark-link:https://github.com/carlfm01&gt;@carlfm01&lt;/denchmark-link&gt;
 Your previous message mentionned growing from 200MB to 700MB over 20 iterations, that would mean we are loosing 25MB per run.
So far, I can only account for ~2MB at best:
&lt;denchmark-code&gt;==22384== LEAK SUMMARY:
==22384==    definitely lost: 24 bytes in 1 blocks
==22384==    indirectly lost: 0 bytes in 0 blocks
==22384==      possibly lost: 332,124 bytes in 1,521 blocks
==22384==    still reachable: 2,655,432 bytes in 33,076 blocks
==22384==                       of which reachable via heuristic:
==22384==                         newarray           : 52,576 bytes in 196 blocks
==22384==         suppressed: 0 bytes in 0 blocks
&lt;/denchmark-code&gt;

And most of it is from TensorFlow itself, and this might be false-positive from valgrind.
The only items that would connect to language model / decoder would account only for a few bytes (10 occurrences, between 24 bytes and 32 bytes, so at worst it's &lt; 320 bytes per run). Obivously, very far away from what you experience. I do fear this might be Windows-specific. Or at least, not reproductible on Linux / under valgrind.
		</comment>
		<comment id='9' author='carlfm01' date='2019-10-07T20:42:10Z'>
		
do you have specific code to share for reproducing?

Just the console example using a for over the same file::&lt;denchmark-link:https://gist.github.com/carlfm01/fd69a8ca2784837dabf9375d35258953#file-test-cs-L59&gt;https://gist.github.com/carlfm01/fd69a8ca2784837dabf9375d35258953#file-test-cs-L59&lt;/denchmark-link&gt;

To see the memory usage I'm using the VS profiler(poor details of the unmanaged side)
Now I want to compile the console client for Linux to perform the same test, or that was exactly what you did?
		</comment>
		<comment id='10' author='carlfm01' date='2019-10-08T01:45:07Z'>
		Finally compiled and now testing:
Versions:
&lt;denchmark-code&gt;TensorFlow: v1.14.0-16-g3b4ce374f5
DeepSpeech: v0.6.0-alpha.8-16-gfb611ef
&lt;/denchmark-code&gt;

Valgrind report for 1 run:
&lt;denchmark-code&gt;==32502== LEAK SUMMARY:
==32502==    definitely lost: 24 bytes in 1 blocks
==32502==    indirectly lost: 214 bytes in 5 blocks
==32502==      possibly lost: 331,620 bytes in 1,500 blocks
==32502==    still reachable: 2,115,668 bytes in 39,762 blocks
==32502==                       of which reachable via heuristic:
==32502==                         stdstring          : 465,999 bytes in 11,883 blocks
==32502==                         newarray           : 42,880 bytes in 194 blocks
==32502==         suppressed: 0 bytes in 0 blocks
&lt;/denchmark-code&gt;

20 runs:
&lt;denchmark-code&gt;==45309==
==45309== LEAK SUMMARY:
==45309==    definitely lost: 7,520 bytes in 42 blocks
==45309==    indirectly lost: 7,959,270 bytes in 102,263 blocks
==45309==      possibly lost: 2,973,857 bytes in 34,726 blocks
==45309==    still reachable: 2,154,783 bytes in 40,201 blocks
==45309==                       of which reachable via heuristic:
==45309==                         stdstring          : 938,728 bytes in 17,033 blocks
==45309==                         newarray           : 208,224 bytes in 988 blocks
&lt;/denchmark-code&gt;


I do fear this might be Windows-specific.

Given the results looks you are correct.
Now looking :&lt;denchmark-link:https://github.com/kkm000/openfst/issues/8&gt;kkm000/openfst#8&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='carlfm01' date='2019-10-08T06:41:51Z'>
		
Now looking :kkm000/openfst#8

So it would mean ConstFst is not really doing mmap() on windows, and thus we leak from there?
		</comment>
		<comment id='12' author='carlfm01' date='2019-10-08T16:46:57Z'>
		
not really doing mmap() on windows

Instead is using a custom implementation using a buffer: &lt;denchmark-link:https://github.com/kkm000/openfst/blob/989affd3043b6357e6047a395565c3e0d979c01f/src/lib/mapped-file.cc#L48&gt;https://github.com/kkm000/openfst/blob/989affd3043b6357e6047a395565c3e0d979c01f/src/lib/mapped-file.cc#L48&lt;/denchmark-link&gt;

I'll compile and debug that file, I also want to test a few things with &lt;denchmark-link:https://code.google.com/archive/p/mman-win32/&gt;https://code.google.com/archive/p/mman-win32/&lt;/denchmark-link&gt;
 and see if we can get rid of the buffer implementation.
		</comment>
		<comment id='13' author='carlfm01' date='2019-10-09T06:12:44Z'>
		The destructor of  MappedFile is never called:



DeepSpeech/native_client/ctcdecode/third_party/openfst-1.6.9-win/src/lib/mapped-file.cc


         Line 26
      in
      031479d






 MappedFile::~MappedFile() { 





then never executes:



DeepSpeech/native_client/ctcdecode/third_party/openfst-1.6.9-win/src/lib/mapped-file.cc


         Line 38
      in
      031479d






 operator delete(static_cast&lt;char *&gt;(region_.data) - region_.offset); 





Before I found that the deconstructor is not called, I tried to replicate with the python client on windows and turns out that the python client does no contains , why &lt;denchmark-link:https://github.com/lissyx&gt;@lissyx&lt;/denchmark-link&gt;
 ? I'm missing something?
Windows Python version : 
		</comment>
		<comment id='14' author='carlfm01' date='2019-10-09T06:58:04Z'>
		
turns out that the python client does no contains freeModel

It is handled by the wrapper: 


DeepSpeech/native_client/python/__init__.py


        Lines 42 to 45
      in
      031479d






 def __del__(self): 



 if self._impl: 



 deepspeech.impl.FreeModel(self._impl) 



 self._impl = None 





		</comment>
		<comment id='15' author='carlfm01' date='2019-10-09T07:10:17Z'>
		
The deconstructor of MappedFile is never called

Just to make sure, this is not only when using the Python code, this is always ?
		</comment>
		<comment id='16' author='carlfm01' date='2019-10-09T07:30:10Z'>
		
Just to make sure, this is not only when using the Python code, this is always ?

Yes always, with both, python client and the c# client.
		</comment>
		<comment id='17' author='carlfm01' date='2019-10-09T07:31:46Z'>
		
c# client.

Can you replicate with the C++ basic client ? Just to see if the .Net bindings could have a play in the equation.
		</comment>
		<comment id='18' author='carlfm01' date='2019-10-09T08:26:23Z'>
		MappedFile as much as I can read in the windows part is all std::unique_ptr&lt;&gt; scoped, is it possible we are missing something at a upper level?
		</comment>
		<comment id='19' author='carlfm01' date='2019-10-09T18:20:53Z'>
		
Can you replicate with the C++ basic client ?

yes I'll test the basic C++ client, allow me some time to complete my builds and switch back to r1.14
		</comment>
		<comment id='20' author='carlfm01' date='2019-10-10T09:21:13Z'>
		
yes I'll test the basic C++ client,

Dealing with make: \bin\amd64\cl.exe: Command not found, I'll read the cluster examples again and try carefully.
		</comment>
		<comment id='21' author='carlfm01' date='2019-10-14T15:43:29Z'>
		

yes I'll test the basic C++ client,

Dealing with make: \bin\amd64\cl.exe: Command not found, I'll read the cluster examples again and try carefully.

Have you been able to sort this out?
		</comment>
		<comment id='22' author='carlfm01' date='2019-10-15T03:26:36Z'>
		Hello &lt;denchmark-link:https://github.com/lissyx&gt;@lissyx&lt;/denchmark-link&gt;
, unfortunately not, last week was a busy week working on TTS. I tried with short time windows but did not get any luck.
I'm back to it :)
		</comment>
		<comment id='23' author='carlfm01' date='2019-10-16T01:12:11Z'>
		Just realized I wasted my time, Bazel is not detecting changes inside header files :), manually removed the fst.obj under _objs and now I see the execution path printed (With this I was trying to see where is mapped-file allocated but not released).
Bazel version: 0.24.1
At this point I don't know which changes were applied for the tests, testing again... :/
Now about the C++ basic client:

Can you replicate with the C++ basic client ?

Yes, is eating the same amount of memory as the .Net client.

make: \bin\amd64\cl.exe: Command not found

solved this by replacing :



DeepSpeech/native_client/definitions.mk


         Line 39
      in
      5fa6d23






 TOOLCHAIN := '$(VCINSTALLDIR)\bin\amd64\' 





with my full path to the cl.exe of my VS and then running vcvars64 before the make command
		</comment>
		<comment id='24' author='carlfm01' date='2019-10-16T13:43:12Z'>
		
with my full path to the cl.exe of my VS and then running vcvars64 before the make command

Right, reminds me of things I had to do on TaskCluster. I assumed that on developper systems, this should be already dealt with.


Can you replicate with the C++ basic client ?

Yes, is eating the same amount of memory as the .Net client.

Good, at least confirms it's not coming from the bindings. Do you think you can investigate why the destructor is not called ?
We have some code that triggers some lost but still reachable memory under valgrind on linux, and it deals with what calls this, so I'm wondering if this is not the root cause indeed, and we are just more lucky / going through another path on linux to free ?
		</comment>
		<comment id='25' author='carlfm01' date='2019-10-17T07:29:40Z'>
		
Do you think you can investigate why the destructor is not called ?

Yes, I'm already trying to spot the issue, but due to my newbie eyes for C++ I'm not making any significant progress.
The only thing that seems wrong apart from the destructor of MappedFile  never called is the destructor of ConstFstImpl only called for the first run, then never again. I want to see if this happens also on Linux.

scoped, is it possible we are missing something at a upper level?

Upper ConstFstImpl is ConstFst which is used for PathTrie as FstType I sort of feel the issue is coming from PathTrie and the usage of share_ptr:



DeepSpeech/native_client/ctcdecode/path_trie.cpp


         Line 83
      in
      336daa1






 new_path-&gt;matcher_ = matcher_; 





¿What do you think?
		</comment>
		<comment id='26' author='carlfm01' date='2019-10-17T16:18:33Z'>
		&lt;denchmark-link:https://github.com/carlfm01&gt;@carlfm01&lt;/denchmark-link&gt;
 Long-shot, but doing so does not seems to trigger issues here:
&lt;denchmark-code&gt;diff --git a/native_client/ctcdecode/path_trie.cpp b/native_client/ctcdecode/path_trie.cpp
index 51f75ff..dee792d 100644
--- a/native_client/ctcdecode/path_trie.cpp
+++ b/native_client/ctcdecode/path_trie.cpp
@@ -33,6 +33,8 @@ PathTrie::~PathTrie() {
   for (auto child : children_) {
     delete child.second;
   }
+
+  matcher_ = nullptr;
 }
 
 PathTrie* PathTrie::get_path_trie(int new_char, int new_timestep, float cur_log_prob_c, bool reset) {
&lt;/denchmark-code&gt;

This should make sure that any PathTrie destruction frees the matching allocation of matcher_.
		</comment>
		<comment id='27' author='carlfm01' date='2019-10-17T17:26:31Z'>
		&lt;denchmark-link:https://github.com/carlfm01&gt;@carlfm01&lt;/denchmark-link&gt;
 I'm having more doubts (and &lt;denchmark-link:https://github.com/reuben&gt;@reuben&lt;/denchmark-link&gt;
 shares this as well) against  there, which is  in  and that we  in . This  call triggers a  behind.
		</comment>
		<comment id='28' author='carlfm01' date='2019-10-18T10:15:21Z'>
		i'm keeping that open until &lt;denchmark-link:https://github.com/carlfm01&gt;@carlfm01&lt;/denchmark-link&gt;
 can confirm this is fixed
		</comment>
		<comment id='29' author='carlfm01' date='2019-10-19T04:03:02Z'>
		Thanks &lt;denchmark-link:https://github.com/lissyx&gt;@lissyx&lt;/denchmark-link&gt;

Using :
TensorFlow: v1.14.0-16-g3b4ce374f5
DeepSpeech: v0.6.0-alpha.10-2-g469ddd2
First run 53MB, last run 45MB.(.Net client)
I can confirm that issue is fixed for the .Net client.
Testing with Python and C++ client, looks like the C++ client is not releasing completely.
		</comment>
		<comment id='30' author='carlfm01' date='2019-10-19T09:06:22Z'>
		I'm going to close it then, we can still fix the C++ client but if the leak in the lib is fixed it's the most important :)
		</comment>
		<comment id='31' author='carlfm01' date='2019-11-18T09:56:43Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>