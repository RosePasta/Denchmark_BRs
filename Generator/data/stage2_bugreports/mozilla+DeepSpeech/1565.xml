<bug id='1565' author='tilmankamp' open_date='2018-09-20T11:09:51Z' closed_time='2020-07-24T09:26:56Z'>
	<summary>Parameters for limiting and skipping samples in data-sets not working anymore</summary>
	<description>
Regression: &lt;denchmark-link:https://github.com/mozilla/DeepSpeech/pull/1532/files#diff-2f5b069cc3a96ce123ef7356642acb29L99&gt;https://github.com/mozilla/DeepSpeech/pull/1532/files#diff-2f5b069cc3a96ce123ef7356642acb29L99&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='tilmankamp' date='2020-03-31T13:58:51Z'>
		Dupe of &lt;denchmark-link:https://github.com/mozilla/DeepSpeech/issues/2777&gt;#2777&lt;/denchmark-link&gt;
 ?
		</comment>
		<comment id='2' author='tilmankamp' date='2020-06-01T00:51:03Z'>
		with reference to &lt;denchmark-link:https://github.com/mozilla/DeepSpeech/pull/2851&gt;#2851&lt;/denchmark-link&gt;
, I tried 0.8.0a2 but still not able to get these flags working. Will these be available with final release?
INSTALL
pip list
deepspeech-training      0.8.0a2         /content/DeepSpeech/training
		</comment>
		<comment id='3' author='tilmankamp' date='2020-06-03T09:06:57Z'>
		
with reference to #2851, I tried 0.8.0a2 but still not able to get these flags working. Will these be available with final release?
INSTALL
pip list
deepspeech-training 0.8.0a2 /content/DeepSpeech/training

This PR was not merged
		</comment>
		<comment id='4' author='tilmankamp' date='2020-06-03T20:20:26Z'>
		This could be handy for running experiments quickly. Till 0.8.0 will have to limit data manually. Anyways, Looking forward to it.
		</comment>
		<comment id='5' author='tilmankamp' date='2020-07-01T07:54:27Z'>
		&lt;denchmark-link:https://github.com/tilmankamp&gt;@tilmankamp&lt;/denchmark-link&gt;
 Is this going to be able to land in 0.8.0?
		</comment>
		<comment id='6' author='tilmankamp' date='2020-07-22T15:15:37Z'>
		Limiting should be no problem to implement with the new data-set approach.
A bigger and most probably insufficient change would be required for true skipping, as the trivial approach of "reading all samples until reaching skip offset" would not help on "running experiments quickly". Background: The current data-set approach interleaves all data-sets passed on the command line on the fly while reading them from start to end during training. Skipping forward would require reading all indices and sample sizes up front, sorting them, interleaving them and then skipping to the required position.
A common problem that we once solved by skipping was determining the maximum batch-size on a GPU. This could also be done by reversing all data-sets and the interleaving-code (training longest samples first). This turns out to be cheap to implement.
So I consider implementing (fixing) limiting, dropping skip support altogether and adding --reverse_train, --reverse_dev, --reverse_test flags to be able to test for batch sizes.
		</comment>
	</comments>
</bug>