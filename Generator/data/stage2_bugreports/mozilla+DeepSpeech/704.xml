<bug id='704' author='pecastro' open_date='2017-07-09T09:35:57Z' closed_time='2019-04-02T13:48:59Z'>
	<summary>Cluster coordinator hangs at the end of training</summary>
	<description>
This does not happen in a clusterless scenario.
But in a simple 2 node (PS + Worker) setup or bigger the Coordinator will always hang at the end of training.
PS:
&lt;denchmark-code&gt;bin/run-ldc93s1.sh      --ps_hosts=host0:2233      --worker_hosts=host0:2222      --job_name=ps --task_index=0 --log_level 0 |&amp; tee -a ps_train.log
&lt;/denchmark-code&gt;

PS Output:
&lt;denchmark-code&gt;+ '[' '!' -f DeepSpeech.py ']'
+ '[' '!' -f data/ldc93s1/ldc93s1.csv ']'
+ '[' -d data/ldc93s1/test/ps ']'
+ checkpoint_dir=data/ldc93s1/test/ps
+ python3 -u DeepSpeech.py --train_files data/ldc93s1/ldc93s1.csv --dev_files data/ldc93s1/ldc93s1.csv --test_files data/ldc93s1/ldc93s1.csv --train_batch_size 1 --dev_batch_size 1 --test_batch_size 1 --n_hidden 494 --epoch 2 --checkpoint_dir data/ldc93s1/test/ps --export_dir data/ldc93s1/test/ps/export --fulltrace True --display_step 1 --log_level 0 --ps_hosts=host0:2233 --worker_hosts=host0:2222 --job_name=ps --task_index=0 --log_level 0
2017-07-09 09:12:43.287531: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-07-09 09:12:43.287572: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-07-09 09:12:43.301837: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; localhost:2233}
2017-07-09 09:12:43.301879: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; host0:2222}
2017-07-09 09:12:43.302765: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2233
D Waiting for stop token...
2017-07-09 09:12:45.991076: I tensorflow/core/distributed_runtime/master_session.cc:995] Start master session 32dce54fc71a4d98 with config:

D Got a stop token from worker 0.
D Session closed.
D Server stopped.
&lt;/denchmark-code&gt;

Worker:
&lt;denchmark-code&gt;bin/run-ldc93s1.sh      --ps_hosts=host0:2233      --worker_hosts=host0:2222      --job_name=worker --task_index=0 --log_traffic True --coord_host host0 --coord_port 2501 --log_level 0 |&amp; tee -a worker0_train.log
&lt;/denchmark-code&gt;

Worker Output:
&lt;denchmark-code&gt;+ '[' '!' -f DeepSpeech.py ']'
+ '[' '!' -f data/ldc93s1/ldc93s1.csv ']'
+ '[' -d data/ldc93s1/test/worker0 ']'
+ checkpoint_dir=data/ldc93s1/test/worker0
+ python3 -u DeepSpeech.py --train_files data/ldc93s1/ldc93s1.csv --dev_files data/ldc93s1/ldc93s1.csv --test_files data/ldc93s1/ldc93s1.csv --train_batch_size 1 --dev_batch_size 1 --test_batch_size 1 --n_hidden 494 --epoch 2 --checkpoint_dir data/ldc93s1/test/worker0 --export_dir data/ldc93s1/test/worker0/export --fulltrace True --display_step 1 --log_level 0 --ps_hosts=host0:2233 --worker_hosts=host0:2222 --job_name=worker --task_index=0 --log_traffic True --coord_host host0 --coord_port 2501 --log_level 0
D Starting coordinator...
D Coordinator started.
2017-07-09 09:12:45.360117: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-07-09 09:12:45.360147: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-07-09 09:12:45.375372: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; host0:2233}
2017-07-09 09:12:45.375431: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:2222}
2017-07-09 09:12:45.376382: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2222
W Parameter --validation_step needs to be &gt;0 for early stopping to work
2017-07-09 09:12:47.869413: I tensorflow/core/distributed_runtime/master_session.cc:995] Start master session 95bc524f41cc8c0b with config:
allow_soft_placement: true

D Starting queue runners...
------------------------------------------------------------------------
WARNING: libdeepspeech failed to load, resorting to deprecated code
         Refer to README.md for instructions on installing libdeepspeech
------------------------------------------------------------------------
D Queue runners started.
I STARTING Optimization
D step: 0
D epoch: 0
D target epoch: 2
D steps per epoch: 1
D number of batches in train set: 1
D batches per job: 1
D batches per step: 1
D number of jobs in train set: 1
D number of jobs already trained in first epoch: 0
D Got new Job (ID: 2, worker: 0, epoch: 0, set_name: train)
D Computing Job (ID: 2, worker: 0, epoch: 0, set_name: train)...
D Starting batch...
D Finished batch step 0.
D Sending Job (ID: 2, worker: 0, epoch: 0, set_name: train)...
D Training of Epoch 0 - Moved Job (ID: 2, worker: 0, epoch: 0, set_name: train) from running to done.
I Training of Epoch 0 - WER: 2.454545, loss: 376.378387451, mean edit distance: 1.269231
I --------------------------------------------------------------------------------
I WER: 2.454545, loss: 376.378387, mean edit distance: 1.269231
I  - src: "she had your dark suit in greasy wash water all year"
I  - res: "and deep p p m i p p p d d m p m m m p i p i d philip p p p p pypypn"
I --------------------------------------------------------------------------------
D Computing Job (ID: 4, worker: 0, epoch: 1, set_name: train)...
D Starting batch...
D Finished batch step 1.
D Sending Job (ID: 4, worker: 0, epoch: 1, set_name: train)...
D Training of Epoch 1 - Moved Job (ID: 4, worker: 0, epoch: 1, set_name: train) from running to done.
I Training of Epoch 1 - WER: 1.000000, loss: 203.96522522, mean edit distance: 0.961538
I --------------------------------------------------------------------------------
I WER: 1.000000, loss: 203.965225, mean edit distance: 0.961538
I  - src: "she had your dark suit in greasy wash water all year"
I  - res: ""
I --------------------------------------------------------------------------------
I FINISHED Optimization - training time: 0:00:07
D Computing Job (ID: 6, worker: 0, epoch: 2, set_name: test)...
D Starting batch...
D Finished batch step 1.
D Sending Job (ID: 6, worker: 0, epoch: 2, set_name: test)...
D Test of Epoch 2 - Moved Job (ID: 6, worker: 0, epoch: 2, set_name: test) from running to done.
I Test of Epoch 2 - WER: 1.000000, loss: 203.1690979, mean edit distance: 0.961538
I --------------------------------------------------------------------------------
I WER: 1.000000, loss: 203.169098, mean edit distance: 0.961538
I  - src: "she had your dark suit in greasy wash water all year"
I  - res: ""
I --------------------------------------------------------------------------------
D No jobs left for worker 0.
D Epochs - running: 0, done: 3
D Closing queues...
D Queues closed.
D Sending stop token to ps 0...
D Sent stop token to ps 0.
&lt;/denchmark-code&gt;

Worker will be hanging in this line 


DeepSpeech/DeepSpeech.py


         Line 1508
      in
      3ac527c






 while job and not session.should_stop(): 




 apparently still trying to communicate with a PS which is gone.
Using Tensorflow v1.2.0 but the same also happens with v1.1.0.
Using Python3 but the same also happens with Python2.
	</description>
	<comments>
		<comment id='1' author='pecastro' date='2017-07-10T09:29:25Z'>
		Thanks for the detailed report. I'll look into this soon.
		</comment>
		<comment id='2' author='pecastro' date='2017-07-10T21:37:06Z'>
		Interestingly ... If I restart the already stopped PS server then I get ...
&lt;denchmark-code&gt;2017-07-10 21:18:09.467354: I tensorflow/core/distributed_runtime/master_session.cc:885] DeregisterGraph error: Aborted: Graph handle is not found: 000000000000001e. Possibly, this worker just restarted.
2017-07-10 21:18:09.468548: I tensorflow/core/distributed_runtime/master_session.cc:885] DeregisterGraph error: Aborted: Graph handle is not found: 000000000000001b. Possibly, this worker just restarted.
2017-07-10 21:18:09.468650: I tensorflow/core/distributed_runtime/master_session.cc:885] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000013. Possibly, this worker just restarted.
2017-07-10 21:18:09.468778: I tensorflow/core/distributed_runtime/master_session.cc:885] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000011. Possibly, this worker just restarted.
2017-07-10 21:18:09.478118: I tensorflow/core/distributed_runtime/master_session.cc:885] DeregisterGraph error: Aborted: Graph handle is not found: 000000000000000f. Possibly, this worker just restarted.
2017-07-10 21:18:09.482981: I tensorflow/core/distributed_runtime/master_session.cc:885] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000015. Possibly, this worker just restarted.
2017-07-10 21:18:09.483253: I tensorflow/core/distributed_runtime/master_session.cc:885] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000007. Possibly, this worker just restarted.
2017-07-10 21:18:09.484214: I tensorflow/core/distributed_runtime/master_session.cc:885] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000019. Possibly, this worker just restarted.
2017-07-10 21:18:09.486352: I tensorflow/core/distributed_runtime/master_session.cc:885] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000009. Possibly, this worker just restarted.
2017-07-10 21:18:09.486588: I tensorflow/core/distributed_runtime/master_session.cc:885] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000020. Possibly, this worker just restarted.
2017-07-10 21:18:09.486855: I tensorflow/core/distributed_runtime/master_session.cc:885] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000005. Possibly, this worker just restarted.
2017-07-10 21:18:09.487259: I tensorflow/core/distributed_runtime/master_session.cc:885] DeregisterGraph error: Aborted: Graph handle is not found: 000000000000000b. Possibly, this worker just restarted.
2017-07-10 21:18:09.487427: I tensorflow/core/distributed_runtime/master_session.cc:885] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000003. Possibly, this worker just restarted.
2017-07-10 21:18:09.489184: I tensorflow/core/distributed_runtime/master_session.cc:885] DeregisterGraph error: Aborted: Graph handle is not found: 000000000000000d. Possibly, this worker just restarted.
D Session closed.
D Server stopped.
D Stopping coordinator...
D Coordinator stopped.
&lt;/denchmark-code&gt;

And now the PS will be left waiting on account of no stop token.
		</comment>
		<comment id='3' author='pecastro' date='2017-07-14T11:08:05Z'>
		I've added some extra debugging with TF_CPP_MIN_VLOG_LEVEL=2 and I can see the following behaviour ...
PS:
&lt;denchmark-code&gt;2017-07-14 10:49:16.458550: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
D Session closed.
D Server stopped.
2017-07-14 10:49:16.460205: I tensorflow/core/framework/rendezvous.cc:226] Recv 0x7f4e0c0281a0 18140624914059168632 /job:ps/replica:0/task:0/cpu:0;522d70f515e89df0;/job:worker/replica:0/task:0/cpu:0;edge_4_global_step;0:0
2017-07-14 10:49:16.463763: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogStep { step_id: 37 handle: "0000000000000015" }
.......
&lt;/denchmark-code&gt;

Worker:
&lt;denchmark-code&gt;2017-07-14 10:49:17.041277: I tensorflow/core/distributed_runtime/master_session.cc:412] Register node {
  name: "dummy_queue"
  op: "FIFOQueueV2"
  device: "/job:ps/replica:0/task:0/cpu:0"
  attr {
    key: "capacity"
    value {
      i: 1
    }
  }
  attr {
    key: "component_types"
    value {
      list {
        type: DT_INT32
      }
    }
  }
  attr {
    key: "container"
    value {
      s: ""
    }
  }
  attr {
    key: "shapes"
    value {
      list {
        shape {
        }
      }
    }
  }
  attr {
    key: "shared_name"
    value {
      s: "dummy_queue"
    }
  }
}
node {
  name: "dummy_queue_Close_1"
  op: "QueueCloseV2"
  input: "dummy_queue"
  device: "/job:ps/replica:0/task:0/cpu:0"
  attr {
    key: "cancel_pending_enqueues"
    value {
      b: true
    }
  }
}
library {
}
versions {
  producer: 24
}

2017-07-14 10:49:17.048462: I tensorflow/core/util/tensor_bundle/tensor_bundle.cc:136] Appending 1952288 bytes to file
.....
&lt;/denchmark-code&gt;

It seems the worker isn't really quite finished yet and still has ops to send to PS, namely this dummy_queue which I think comes from &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/sync_replicas_optimizer.py#L308&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/sync_replicas_optimizer.py#L308&lt;/denchmark-link&gt;

If I comment &lt;denchmark-link:https://github.com/mozilla/DeepSpeech/blob/master/DeepSpeech.py#L1484&gt;https://github.com/mozilla/DeepSpeech/blob/master/DeepSpeech.py#L1484&lt;/denchmark-link&gt;
 and run again ...
PS:
&lt;denchmark-code&gt;2017-07-14 11:01:09.604923: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
2017-07-14 11:01:09.604938: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
2017-07-14 11:01:09.604952: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
&lt;/denchmark-code&gt;

Will be left waiting for a termination command.
Worker:
&lt;denchmark-code&gt;2017-07-14 10:59:09.161241: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
2017-07-14 10:59:09.161259: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
2017-07-14 10:59:09.161298: I tensorflow/core/common_runtime/executor.cc:1557] Process node: 51 step 55 save/control_dependency = Identity[T=DT_STRING, _class=["loc:@save/Const"], _device="/job:worker/replica:0/task:0/cpu:0"](_recv_save/Const_0, ^save/SaveV2) is dead: 0
2017-07-14 10:59:09.161353: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorOutput { step_id: 55 kernel_name: "save/control_dependency" tensor { dtype: DT_STRING shape { } allocation_description { requested_bytes: 32 allocator_name: "cpu" ptr: 140040049115520 } } }
2017-07-14 10:59:09.161392: I tensorflow/core/common_runtime/executor.cc:1557] Process node: 52 step 55 _send_save/control_dependency_0 = _Send[T=DT_STRING, client_terminated=true, recv_device="/job:worker/replica:0/task:0/cpu:0", send_device="/job:worker/replica:0/task:0/cpu:0", send_device_incarnation=-2016048749053448315, tensor_name="save/control_dependency:0", _device="/job:worker/replica:0/task:0/cpu:0"](save/control_dependency) is dead: 0
2017-07-14 10:59:09.161411: I tensorflow/core/distributed_runtime/base_rendezvous_mgr.cc:188] BaseRemoteRendezvous Send 0x7f5d4c001700 /job:worker/replica:0/task:0/cpu:0;e4058e581ecc6f85;/job:worker/replica:0/task:0/cpu:0;save/control_dependency:0;0:0
2017-07-14 10:59:09.161474: I tensorflow/core/distributed_runtime/base_rendezvous_mgr.cc:287] RemoteRendezvous Recv 0x7f5d4c001700 /job:worker/replica:0/task:0/cpu:0;e4058e581ecc6f85;/job:worker/replica:0/task:0/cpu:0;save/control_dependency:0;0:0
2017-07-14 10:59:09.161859: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
2017-07-14 10:59:09.439435: I tensorflow/core/distributed_runtime/master_session.cc:1182] Unseen hash 4121888047455924127 for 
 TN: dummy_queue_Close_1

 is_partial = 0

2017-07-14 10:59:09.439506: I tensorflow/core/common_runtime/simple_graph_execution_state.cc:344] BuildGraph
2017-07-14 10:59:09.449729: I tensorflow/core/common_runtime/optimization_registry.cc:37] Running optimization phase 0
2017-07-14 10:59:09.449952: I tensorflow/core/distributed_runtime/master_session.cc:75] Created ReffedClientGraph for node with 4
2017-07-14 10:59:09.451581: I tensorflow/core/distributed_runtime/master_session.cc:1193] Preparing to execute new graph
2017-07-14 10:59:09.451716: I tensorflow/core/graph/graph_partition.cc:1045] Added send/recv: controls=0, data=0
2017-07-14 11:01:09.558367: I tensorflow/core/distributed_runtime/master_session.cc:1203] Discarding all reffed graphs
2017-07-14 11:01:09.558423: I tensorflow/core/distributed_runtime/master_session.cc:1203] Discarding all reffed graphs
2017-07-14 11:01:09.559065: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
#__PREVIOUS_REPEATS__
2017-07-14 11:01:09.560658: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
Exception in thread Thread-9:
Traceback (most recent call last):
  File "/usr/lib64/python3.5/threading.py", line 914, in _bootstrap_inner
    self.run()
  File "/usr/lib64/python3.5/threading.py", line 862, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3.5/site-packages/tensorflow/python/training/queue_runner_impl.py", line 254, in _run
    coord.request_stop(e)
  File "/usr/lib/python3.5/site-packages/tensorflow/python/training/coordinator.py", line 211, in request_stop
    six.reraise(*sys.exc_info())
  File "/usr/lib/python3.5/site-packages/six.py", line 686, in reraise
    raise value
  File "/usr/lib/python3.5/site-packages/tensorflow/python/training/queue_runner_impl.py", line 238, in _run
    enqueue_callable()
  File "/usr/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 1193, in _single_operation_run
    target_list_as_strings, status, None)
  File "/usr/lib64/python3.5/contextlib.py", line 66, in __exit__
    next(self.gen)
  File "/usr/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.CancelledError: Step was cancelled by an explicit call to `Session::Close()`.

2017-07-14 11:01:09.562404: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
#__PREVIOUS_REPEATS__
2017-07-14 11:01:09.596122: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
2017-07-14 11:01:09.600037: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogRawDeallocation { step_id: -3 operation: "TensorFlow C Api" allocator_name: "cpu" }
2017-07-14 11:01:09.600117: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogRawDeallocation { step_id: -3 operation: "TensorFlow C Api" allocator_name: "cpu" }
2017-07-14 11:01:09.600143: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogRawDeallocation { step_id: -3 operation: "TensorFlow C Api" allocator_name: "cpu" }
2017-07-14 11:01:09.600162: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogRawDeallocation { step_id: -3 operation: "TensorFlow C Api" allocator_name: "cpu" }
2017-07-14 11:01:09.600180: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogRawDeallocation { step_id: -3 operation: "TensorFlow C Api" allocator_name: "cpu" }
2017-07-14 11:01:09.600201: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
2017-07-14 11:01:09.601075: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogRawDeallocation { step_id: -3 operation: "TensorFlow C Api" allocator_name: "cpu" }
2017-07-14 11:01:09.601144: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogRawDeallocation { step_id: -3 operation: "TensorFlow C Api" allocator_name: "cpu" }
2017-07-14 11:01:09.601170: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogRawDeallocation { step_id: -3 operation: "TensorFlow C Api" allocator_name: "cpu" }
2017-07-14 11:01:09.601190: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogRawDeallocation { step_id: -3 operation: "TensorFlow C Api" allocator_name: "cpu" }
2017-07-14 11:01:09.601215: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
2017-07-14 11:01:09.601269: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogRawDeallocation { step_id: -3 operation: "TensorFlow C Api" allocator_name: "cpu" }
2017-07-14 11:01:09.601296: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogRawDeallocation { step_id: -3 operation: "TensorFlow C Api" allocator_name: "cpu" }
2017-07-14 11:01:09.601314: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogRawDeallocation { step_id: -3 operation: "TensorFlow C Api" allocator_name: "cpu" }
2017-07-14 11:01:09.601330: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogRawDeallocation { step_id: -3 operation: "TensorFlow C Api" allocator_name: "cpu" }
2017-07-14 11:01:09.601345: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogRawDeallocation { step_id: -3 operation: "TensorFlow C Api" allocator_name: "cpu" }
2017-07-14 11:01:09.601359: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogRawDeallocation { step_id: -3 operation: "TensorFlow C Api" allocator_name: "cpu" }
2017-07-14 11:01:09.601379: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
D Session closed.
2017-07-14 11:01:09.622072: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
2017-07-14 11:01:09.622127: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
2017-07-14 11:01:09.622145: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
2017-07-14 11:01:09.622158: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
2017-07-14 11:01:09.622174: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
2017-07-14 11:01:09.622187: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
2017-07-14 11:01:09.622201: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }
D Server stopped.
D Stopping coordinator...
D Coordinator stopped.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='pecastro' date='2017-07-24T18:35:33Z'>
		&lt;denchmark-link:https://github.com/tilmankamp&gt;@tilmankamp&lt;/denchmark-link&gt;
 Any news on this ? Can you confirm if this is a behaviour you're currently witnessing ?
		</comment>
		<comment id='5' author='pecastro' date='2017-07-25T08:55:43Z'>
		I am not witnessing this behaviour, but others did.
Currently I am doing some experiment that is about to simplify this overly complex orchestration of components. Therefore I put this aside for the moment.
However: If you got an idea on how to solve this easily, feel free to put up a PR and I'll look at it ASAP.
		</comment>
		<comment id='6' author='pecastro' date='2018-07-11T12:29:07Z'>
		Please &lt;denchmark-link:https://github.com/tilmankamp&gt;@tilmankamp&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/pecastro&gt;@pecastro&lt;/denchmark-link&gt;
 , do you guys have any news on this? Situation is exactly the same as described above, worker0 hangs at the end of the training, after "Sending stop token to ps". I had no success in finding a solution here or in mozilla discourse site. I'm using tensorflow 1.8.0, deepspeech 0.1.1, training goes fine in a single process. Worked hard to get this nice project running in a cluster with several machines, now I need to overcome this in order to proceed to training with real data in my own language. Thanks!
		</comment>
		<comment id='7' author='pecastro' date='2018-07-11T12:41:12Z'>
		&lt;denchmark-link:https://github.com/yash-sam&gt;@yash-sam&lt;/denchmark-link&gt;
 Care to test with TensorFlow 1.6.0, as we specify in the requirements file ? I don't know very well the cluster setup, but changing versions might not be a good idea, especially when debugging.
		</comment>
		<comment id='8' author='pecastro' date='2018-07-11T13:00:12Z'>
		&lt;denchmark-link:https://github.com/yash-sam&gt;@yash-sam&lt;/denchmark-link&gt;
 Could you check what happens if you comment &lt;denchmark-link:https://github.com/mozilla/DeepSpeech/blob/ca3e5cd91335d0232a1664ac391ae18de8adbb15/DeepSpeech.py#L1536&gt;this line&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='9' author='pecastro' date='2018-07-12T12:52:47Z'>
		Thank you &lt;denchmark-link:https://github.com/lissyx&gt;@lissyx&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/tilmankamp&gt;@tilmankamp&lt;/denchmark-link&gt;
 for such fast reply.
I've managed to do what follows.

With TF 1.6.0:
-- uncommented send_token_to_ps(session)

&lt;denchmark-code&gt;Using script ./run_mini-eng.sh...
Starting cluster with 1 parameter servers and 2 workers with 0 GPUs each...
Started ps 0
Started worker 0
[ps 0] + '[' '!' -f DeepSpeech.py ']'
[ps 0] + python -u DeepSpeech.py --decoder_library_path /home/userys/stt/DeepSpeech/native_client/libctc_decoder_with_kenlm.so --train_files /home/userys/stt/cv_eng-mini/other-dev/training/training.csv --dev_files /home/userys/stt/cv_eng-mini/other-dev/dev/dev.csv --test_files /home/userys/stt/cv_eng-mini/other-dev/test/test.csv --train_batch_size 15 --dev_batch_size 5 --test_batch_size 5 --epoch 4 --n_hidden 200 --display_step 1 --validation_step 1 --dropout_rate 0.07 --learning_rate 0.001 --valid_word_count_weight 3 --report_count 5 --export_dir /home/userys/stt/cv_eng-mini/other-dev/result/model_export/ --checkpoint_dir /home/userys/stt/cv_eng-mini/other-dev/result/checkout/ --alphabet_config_path /home/userys/stt/eng-mini-models/alphabet.txt --lm_binary_path /home/userys/stt/eng-mini-models/lm.binary --lm_trie_path /home/userys/stt/eng-mini-models/trie --log_level 0 --ps_hosts localhost:2000 --worker_hosts localhost:3000,localhost:3001 --job_name=ps --task_index=0
Started worker 1
[worker 0] + '[' '!' -f DeepSpeech.py ']'
[worker 0] + python -u DeepSpeech.py --decoder_library_path /home/userys/stt/DeepSpeech/native_client/libctc_decoder_with_kenlm.so --train_files /home/userys/stt/cv_eng-mini/other-dev/training/training.csv --dev_files /home/userys/stt/cv_eng-mini/other-dev/dev/dev.csv --test_files /home/userys/stt/cv_eng-mini/other-dev/test/test.csv --train_batch_size 15 --dev_batch_size 5 --test_batch_size 5 --epoch 4 --n_hidden 200 --display_step 1 --validation_step 1 --dropout_rate 0.07 --learning_rate 0.001 --valid_word_count_weight 3 --report_count 5 --export_dir /home/userys/stt/cv_eng-mini/other-dev/result/model_export/ --checkpoint_dir /home/userys/stt/cv_eng-mini/other-dev/result/checkout/ --alphabet_config_path /home/userys/stt/eng-mini-models/alphabet.txt --lm_binary_path /home/userys/stt/eng-mini-models/lm.binary --lm_trie_path /home/userys/stt/eng-mini-models/trie --log_level 0 --ps_hosts localhost:2000 --worker_hosts localhost:3000,localhost:3001 --job_name=worker --task_index=0
[worker 1] + '[' '!' -f DeepSpeech.py ']'
[worker 1] + python -u DeepSpeech.py --decoder_library_path /home/userys/stt/DeepSpeech/native_client/libctc_decoder_with_kenlm.so --train_files /home/userys/stt/cv_eng-mini/other-dev/training/training.csv --dev_files /home/userys/stt/cv_eng-mini/other-dev/dev/dev.csv --test_files /home/userys/stt/cv_eng-mini/other-dev/test/test.csv --train_batch_size 15 --dev_batch_size 5 --test_batch_size 5 --epoch 4 --n_hidden 200 --display_step 1 --validation_step 1 --dropout_rate 0.07 --learning_rate 0.001 --valid_word_count_weight 3 --report_count 5 --export_dir /home/userys/stt/cv_eng-mini/other-dev/result/model_export/ --checkpoint_dir /home/userys/stt/cv_eng-mini/other-dev/result/checkout/ --alphabet_config_path /home/userys/stt/eng-mini-models/alphabet.txt --lm_binary_path /home/userys/stt/eng-mini-models/lm.binary --lm_trie_path /home/userys/stt/eng-mini-models/trie --log_level 0 --ps_hosts localhost:2000 --worker_hosts localhost:3000,localhost:3001 --job_name=worker --task_index=1
[ps 0] 2018-07-11 16:42:15.680453: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; localhost:2000}
[ps 0] 2018-07-11 16:42:15.680489: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:3000, 1 -&gt; localhost:3001}
[ps 0] 2018-07-11 16:42:15.681120: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:332] Started server with target: grpc://localhost:2000
[ps 0] D Waiting for stop token...
[worker 0] D Starting coordinator...
[worker 0] D Coordinator started.
[worker 0] 2018-07-11 16:42:15.753550: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; localhost:2000}
[worker 0] 2018-07-11 16:42:15.753574: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:3000, 1 -&gt; localhost:3001}
[worker 1] 2018-07-11 16:42:15.754604: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; localhost:2000}
[worker 1] 2018-07-11 16:42:15.754625: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:3000, 1 -&gt; localhost:3001}
[worker 0] 2018-07-11 16:42:15.757254: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:332] Started server with target: grpc://localhost:3000
[worker 1] 2018-07-11 16:42:15.758096: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:332] Started server with target: grpc://localhost:3001
[ps 0] 2018-07-11 16:42:16.684595: I tensorflow/core/distributed_runtime/master_session.cc:1136] Start master session ef27a9c585d8a796 with config: 
[worker 0] 2018-07-11 16:42:18.243983: I tensorflow/core/distributed_runtime/master_session.cc:1136] Start master session b817adfcb04eb81a with config: allow_soft_placement: true
[worker 1] 2018-07-11 16:42:18.296797: I tensorflow/core/distributed_runtime/master_session.cc:1136] Start master session 1afe39756f04c240 with config: allow_soft_placement: true
[worker 0] D Starting queue runners...
[worker 0] D Queue runners started.
[worker 0] I STARTING Optimization
[worker 0] D step: 0
[worker 0] D epoch: 0
[worker 0] D target epoch: 4
[worker 0] D steps per epoch: 1
[worker 0] D number of batches in train set: 3
[worker 0] D batches per job: 1
[worker 0] D batches per step: 2
[worker 0] D number of jobs in train set: 3
[worker 0] D number of jobs already trained in first epoch: 0
[worker 0] D Computing Job (ID: 2, worker: 0, epoch: 0, set_name: train)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 0.
[worker 0] D Sending Job (ID: 2, worker: 0, epoch: 0, set_name: train)...
[worker 0] D Computing Job (ID: 3, worker: 0, epoch: 0, set_name: train)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 0.
[worker 0] D Sending Job (ID: 3, worker: 0, epoch: 0, set_name: train)...
[worker 0] D Computing Job (ID: 4, worker: 0, epoch: 0, set_name: train)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 1.
[worker 0] D Sending Job (ID: 4, worker: 0, epoch: 0, set_name: train)...
[worker 0] I Training of Epoch 0 - WER: 9.324362, loss: 521.654042561849, mean edit distance: 3.785765
[worker 0] I [RESULTS]
[worker 0] D Computing Job (ID: 6, worker: 0, epoch: 0, set_name: dev)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 1.
[worker 0] D Sending Job (ID: 6, worker: 0, epoch: 0, set_name: dev)...
[worker 0] D Computing Job (ID: 7, worker: 0, epoch: 0, set_name: dev)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 1.
[worker 0] D Sending Job (ID: 7, worker: 0, epoch: 0, set_name: dev)...
[worker 0] I Validation of Epoch 0 - WER: 9.574074, loss: 422.3068542480469, mean edit distance: 4.182127
[worker 0] I [RESULTS]
[worker 0] D Computing Job (ID: 9, worker: 0, epoch: 1, set_name: train)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 1.
[worker 0] D Sending Job (ID: 9, worker: 0, epoch: 1, set_name: train)...
[worker 0] D Computing Job (ID: 10, worker: 0, epoch: 1, set_name: train)...
[worker 0] D Starting batch...
[worker 1] 2018-07-11 16:42:48.461734: I tensorflow/core/distributed_runtime/master_session.cc:1136] Start master session e23e4e2896da8a20 with config: allow_soft_placement: true
[worker 1] D Starting queue runners...
[worker 1] D Queue runners started.
[worker 1] D Computing Job (ID: 11, worker: 1, epoch: 1, set_name: train)...
[worker 1] D Starting batch...
[worker 0] D Finished batch step 1.
[worker 0] D Sending Job (ID: 10, worker: 0, epoch: 1, set_name: train)...
[worker 0] D Computing Job (ID: 13, worker: 0, epoch: 1, set_name: dev)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 2.
[worker 0] D Sending Job (ID: 13, worker: 0, epoch: 1, set_name: dev)...
[worker 0] D Computing Job (ID: 14, worker: 0, epoch: 1, set_name: dev)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 2.
[worker 0] D Sending Job (ID: 14, worker: 0, epoch: 1, set_name: dev)...
[worker 0] I Validation of Epoch 1 - WER: 8.200000, loss: 339.6593017578125, mean edit distance: 3.579429
[worker 0] I [RESULTS]
[worker 0] D Computing Job (ID: 16, worker: 0, epoch: 2, set_name: train)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 2.
[worker 0] D Sending Job (ID: 16, worker: 0, epoch: 2, set_name: train)...
[worker 0] D Computing Job (ID: 17, worker: 0, epoch: 2, set_name: train)...
[worker 0] D Starting batch...
[worker 1] D Finished batch step 2.
[worker 1] D Sending Job (ID: 11, worker: 1, epoch: 1, set_name: train)...
[worker 0] I Training of Epoch 1 - WER: 9.164713, loss: 490.5286051432292, mean edit distance: 3.772048
[worker 0] I [RESULTS]
[worker 1] D Computing Job (ID: 18, worker: 1, epoch: 2, set_name: train)...
[worker 1] D Starting batch...
[worker 0] D Finished batch step 3.
[worker 0] D Sending Job (ID: 17, worker: 0, epoch: 2, set_name: train)...
[worker 0] D Computing Job (ID: 20, worker: 0, epoch: 2, set_name: dev)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 3.
[worker 0] D Sending Job (ID: 20, worker: 0, epoch: 2, set_name: dev)...
[worker 0] D Computing Job (ID: 21, worker: 0, epoch: 2, set_name: dev)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 3.
[worker 0] D Sending Job (ID: 21, worker: 0, epoch: 2, set_name: dev)...
[worker 0] I Validation of Epoch 2 - WER: 0.947222, loss: 227.6219940185547, mean edit distance: 0.911823
[worker 0] I [RESULTS]
[worker 1] D Computing Job (ID: 24, worker: 1, epoch: 3, set_name: train)...
[worker 1] D Starting batch...
[worker 0] D Finished batch step 3.
[worker 0] D Sending Job (ID: 23, worker: 0, epoch: 3, set_name: train)...
[worker 0] D Computing Job (ID: 25, worker: 0, epoch: 3, set_name: train)...
[worker 0] D Starting batch...
[worker 1] D Finished batch step 4.
[worker 1] D Sending Job (ID: 24, worker: 1, epoch: 3, set_name: train)...
[worker 1] D Computing Job (ID: 27, worker: 1, epoch: 3, set_name: dev)...
[worker 1] D Starting batch...
[worker 1] D Finished batch step 5.
[worker 1] D Sending Job (ID: 27, worker: 1, epoch: 3, set_name: dev)...
[worker 1] D Computing Job (ID: 28, worker: 1, epoch: 3, set_name: dev)...
[worker 1] D Starting batch...
[worker 0] D Finished batch step 4.
[worker 0] D Sending Job (ID: 25, worker: 0, epoch: 3, set_name: train)...
[worker 0] I Training of Epoch 3 - WER: 1.021267, loss: 218.6768595377604, mean edit distance: 0.892002
[worker 0] I [RESULTS]
[worker 0] I FINISHED Optimization - training time: 0:01:46
[worker 0] D Computing Job (ID: 30, worker: 0, epoch: 4, set_name: test)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 5.
[worker 0] D Sending Job (ID: 30, worker: 0, epoch: 4, set_name: test)...
[worker 0] D Computing Job (ID: 31, worker: 0, epoch: 4, set_name: test)...
[worker 0] D Starting batch...
[worker 1] D Finished batch step 5.
[worker 1] D Sending Job (ID: 28, worker: 1, epoch: 3, set_name: dev)...
[worker 0] I Validation of Epoch 3 - WER: 0.989130, loss: 198.5041732788086, mean edit distance: 0.892190
[worker 0] I [RESULTS]
[worker 0] D Checking for early stopping (last 4 steps) validation loss: 198.504173, with standard deviation: 79.781069 and mean: 329.862717
[worker 0] D Epochs - running: 0, done: 9
[worker 0] D Closing queues...
[worker 0] D Queues closed.
[worker 0] D Sending stop token to ps 0...
[ps 0] D Got a stop token from worker 0.
[worker 0] D Sent stop token to ps 0.
[ps 0] D Session closed.
[ps 0] D Server stopped.
[worker 1] D Session closed.
[worker 1] D Server stopped.
&lt;/denchmark-code&gt;

-- commented send_token_to_ps(session)
&lt;denchmark-code&gt;Using script ./run_mini-eng.sh...
Starting cluster with 1 parameter servers and 2 workers with 0 GPUs each...
Started ps 0
Started worker 0
[ps 0] + '[' '!' -f DeepSpeech.py ']'
[ps 0] + python -u DeepSpeech.py --decoder_library_path /home/userys/stt/DeepSpeech/native_client/libctc_decoder_with_kenlm.so --train_files /home/userys/stt/cv_eng-mini/other-dev/training/training.csv --dev_files /home/userys/stt/cv_eng-mini/other-dev/dev/dev.csv --test_files /home/userys/stt/cv_eng-mini/other-dev/test/test.csv --train_batch_size 15 --dev_batch_size 5 --test_batch_size 5 --epoch 4 --n_hidden 200 --display_step 1 --validation_step 1 --dropout_rate 0.07 --learning_rate 0.001 --valid_word_count_weight 3 --report_count 5 --export_dir /home/userys/stt/cv_eng-mini/other-dev/result/model_export/ --checkpoint_dir /home/userys/stt/cv_eng-mini/other-dev/result/checkout/ --alphabet_config_path /home/userys/stt/eng-mini-models/alphabet.txt --lm_binary_path /home/userys/stt/eng-mini-models/lm.binary --lm_trie_path /home/userys/stt/eng-mini-models/trie --log_level 0 --ps_hosts localhost:2000 --worker_hosts localhost:3000,localhost:3001 --job_name=ps --task_index=0
Started worker 1
[worker 0] + '[' '!' -f DeepSpeech.py ']'
[worker 0] + python -u DeepSpeech.py --decoder_library_path /home/userys/stt/DeepSpeech/native_client/libctc_decoder_with_kenlm.so --train_files /home/userys/stt/cv_eng-mini/other-dev/training/training.csv --dev_files /home/userys/stt/cv_eng-mini/other-dev/dev/dev.csv --test_files /home/userys/stt/cv_eng-mini/other-dev/test/test.csv --train_batch_size 15 --dev_batch_size 5 --test_batch_size 5 --epoch 4 --n_hidden 200 --display_step 1 --validation_step 1 --dropout_rate 0.07 --learning_rate 0.001 --valid_word_count_weight 3 --report_count 5 --export_dir /home/userys/stt/cv_eng-mini/other-dev/result/model_export/ --checkpoint_dir /home/userys/stt/cv_eng-mini/other-dev/result/checkout/ --alphabet_config_path /home/userys/stt/eng-mini-models/alphabet.txt --lm_binary_path /home/userys/stt/eng-mini-models/lm.binary --lm_trie_path /home/userys/stt/eng-mini-models/trie --log_level 0 --ps_hosts localhost:2000 --worker_hosts localhost:3000,localhost:3001 --job_name=worker --task_index=0
[worker 1] + '[' '!' -f DeepSpeech.py ']'
[worker 1] + python -u DeepSpeech.py --decoder_library_path /home/userys/stt/DeepSpeech/native_client/libctc_decoder_with_kenlm.so --train_files /home/userys/stt/cv_eng-mini/other-dev/training/training.csv --dev_files /home/userys/stt/cv_eng-mini/other-dev/dev/dev.csv --test_files /home/userys/stt/cv_eng-mini/other-dev/test/test.csv --train_batch_size 15 --dev_batch_size 5 --test_batch_size 5 --epoch 4 --n_hidden 200 --display_step 1 --validation_step 1 --dropout_rate 0.07 --learning_rate 0.001 --valid_word_count_weight 3 --report_count 5 --export_dir /home/userys/stt/cv_eng-mini/other-dev/result/model_export/ --checkpoint_dir /home/userys/stt/cv_eng-mini/other-dev/result/checkout/ --alphabet_config_path /home/userys/stt/eng-mini-models/alphabet.txt --lm_binary_path /home/userys/stt/eng-mini-models/lm.binary --lm_trie_path /home/userys/stt/eng-mini-models/trie --log_level 0 --ps_hosts localhost:2000 --worker_hosts localhost:3000,localhost:3001 --job_name=worker --task_index=1
[worker 0] D Starting coordinator...
[worker 0] D Coordinator started.
[ps 0] 2018-07-11 17:04:57.655700: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; localhost:2000}
[ps 0] 2018-07-11 17:04:57.655726: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:3000, 1 -&gt; localhost:3001}
[ps 0] 2018-07-11 17:04:57.656788: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2000
[ps 0] D Waiting for stop token...
[worker 0] 2018-07-11 17:04:57.668512: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; localhost:2000}
[worker 0] 2018-07-11 17:04:57.668540: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:3000, 1 -&gt; localhost:3001}
[worker 0] 2018-07-11 17:04:57.669416: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:3000
[worker 1] 2018-07-11 17:04:57.764706: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; localhost:2000}
[worker 1] 2018-07-11 17:04:57.764755: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:3000, 1 -&gt; localhost:3001}
[worker 1] 2018-07-11 17:04:57.766337: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:3001
[ps 0] 2018-07-11 17:04:58.662441: I tensorflow/core/distributed_runtime/master_session.cc:1024] Start master session 23324e9c87669953 with config: 
[worker 0] 2018-07-11 17:04:59.768437: I tensorflow/core/distributed_runtime/master_session.cc:1024] Start master session 170e2d9fd61d055f with config: allow_soft_placement: true
[worker 1] 2018-07-11 17:04:59.811822: I tensorflow/core/distributed_runtime/master_session.cc:1024] Start master session a78ca3aa883c97fd with config: allow_soft_placement: true
[worker 0] D Starting queue runners...
[worker 0] D Queue runners started.
[worker 0] I STARTING Optimization
[worker 0] D step: 0
[worker 0] D epoch: 0
[worker 0] D target epoch: 4
[worker 0] D steps per epoch: 1
[worker 0] D number of batches in train set: 3
[worker 0] D batches per job: 1
[worker 0] D batches per step: 2
[worker 0] D number of jobs in train set: 3
[worker 0] D number of jobs already trained in first epoch: 0
[worker 0] D Computing Job (ID: 2, worker: 0, epoch: 0, set_name: train)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 0.
[worker 0] D Sending Job (ID: 2, worker: 0, epoch: 0, set_name: train)...
[worker 0] D Computing Job (ID: 3, worker: 0, epoch: 0, set_name: train)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 0.
[worker 0] D Sending Job (ID: 3, worker: 0, epoch: 0, set_name: train)...
[worker 0] D Computing Job (ID: 4, worker: 0, epoch: 0, set_name: train)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 1.
[worker 0] D Sending Job (ID: 4, worker: 0, epoch: 0, set_name: train)...
[worker 0] I Training of Epoch 0 - WER: 8.797280, loss: 475.37054443359375, mean edit distance: 3.610869
[worker 0] I [RESULTS]
[worker 0] D Computing Job (ID: 6, worker: 0, epoch: 0, set_name: dev)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 1.
[worker 0] D Sending Job (ID: 6, worker: 0, epoch: 0, set_name: dev)...
[worker 0] D Computing Job (ID: 7, worker: 0, epoch: 0, set_name: dev)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 1.
[worker 0] D Sending Job (ID: 7, worker: 0, epoch: 0, set_name: dev)...
[worker 0] I Validation of Epoch 0 - WER: 8.228125, loss: 315.00770568847656, mean edit distance: 3.549095
[worker 0] I [RESULTS]
[worker 0] D Computing Job (ID: 9, worker: 0, epoch: 1, set_name: train)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 1.
[worker 0] D Sending Job (ID: 9, worker: 0, epoch: 1, set_name: train)...
[worker 0] D Computing Job (ID: 10, worker: 0, epoch: 1, set_name: train)...
[worker 0] D Starting batch...
[worker 1] 2018-07-11 17:05:30.015363: I tensorflow/core/distributed_runtime/master_session.cc:1024] Start master session 5acad9ccd13fc989 with config: allow_soft_placement: true
[worker 1] D Starting queue runners...
[worker 1] D Queue runners started.
[worker 1] D Computing Job (ID: 11, worker: 1, epoch: 1, set_name: train)...
[worker 1] D Starting batch...
[worker 0] D Finished batch step 1.
[worker 0] D Sending Job (ID: 10, worker: 0, epoch: 1, set_name: train)...
[worker 0] D Computing Job (ID: 13, worker: 0, epoch: 1, set_name: dev)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 2.
[worker 0] D Sending Job (ID: 13, worker: 0, epoch: 1, set_name: dev)...
[worker 0] D Computing Job (ID: 14, worker: 0, epoch: 1, set_name: dev)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 2.
[worker 0] D Sending Job (ID: 14, worker: 0, epoch: 1, set_name: dev)...
[worker 0] I Validation of Epoch 1 - WER: 3.585714, loss: 145.6614227294922, mean edit distance: 1.385377
[worker 0] I [RESULTS]
[worker 0] D Computing Job (ID: 16, worker: 0, epoch: 2, set_name: train)...
[worker 0] D Starting batch...
[worker 1] D Finished batch step 2.
[worker 1] D Sending Job (ID: 11, worker: 1, epoch: 1, set_name: train)...
[worker 0] I Training of Epoch 1 - WER: 6.674432, loss: 322.67418416341144, mean edit distance: 2.627363
[worker 0] I [RESULTS]
[worker 1] D Computing Job (ID: 17, worker: 1, epoch: 2, set_name: train)...
[worker 1] D Starting batch...
[worker 0] D Finished batch step 2.
[worker 0] D Sending Job (ID: 16, worker: 0, epoch: 2, set_name: train)...
[worker 0] D Computing Job (ID: 18, worker: 0, epoch: 2, set_name: train)...
[worker 0] D Starting batch...
[worker 1] D Finished batch step 3.
[worker 1] D Sending Job (ID: 17, worker: 1, epoch: 2, set_name: train)...
[worker 1] D Computing Job (ID: 20, worker: 1, epoch: 2, set_name: dev)...
[worker 1] D Starting batch...
[worker 1] D Finished batch step 3.
[worker 1] D Sending Job (ID: 20, worker: 1, epoch: 2, set_name: dev)...
[worker 1] D Computing Job (ID: 21, worker: 1, epoch: 2, set_name: dev)...
[worker 1] D Starting batch...
[worker 1] D Finished batch step 3.
[worker 1] D Sending Job (ID: 21, worker: 1, epoch: 2, set_name: dev)...
[worker 0] I Validation of Epoch 2 - WER: 0.922705, loss: 127.16676330566406, mean edit distance: 0.780348
[worker 0] I [RESULTS]
[worker 0] D Computing Job (ID: 24, worker: 0, epoch: 3, set_name: train)...
[worker 0] D Starting batch...
[worker 1] D Finished batch step 3.
[worker 1] D Sending Job (ID: 23, worker: 1, epoch: 3, set_name: train)...
[worker 1] D Computing Job (ID: 25, worker: 1, epoch: 3, set_name: train)...
[worker 1] D Starting batch...
[worker 0] D Finished batch step 4.
[worker 0] D Sending Job (ID: 24, worker: 0, epoch: 3, set_name: train)...
[worker 0] D Computing Job (ID: 27, worker: 0, epoch: 3, set_name: dev)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 5.
[worker 0] D Sending Job (ID: 27, worker: 0, epoch: 3, set_name: dev)...
[worker 0] D Computing Job (ID: 28, worker: 0, epoch: 3, set_name: dev)...
[worker 0] D Starting batch...
[worker 1] D Finished batch step 4.
[worker 1] D Sending Job (ID: 25, worker: 1, epoch: 3, set_name: train)...
[worker 0] I Training of Epoch 3 - WER: 0.996644, loss: 170.38519287109375, mean edit distance: 0.866425
[worker 0] I [RESULTS]
[worker 0] D Computing Job (ID: 31, worker: 0, epoch: 4, set_name: test)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 5.
[worker 0] D Sending Job (ID: 31, worker: 0, epoch: 4, set_name: test)...
[worker 0] D Checking for early stopping (last 4 steps) validation loss: 157.678299, with standard deviation: 84.527731 and mean: 195.945297
[worker 0] D Epochs - running: 1, done: 8
[worker 0] D - running: Test of Epoch 4 - jobs open: 0, jobs running: 1, jobs done: 1
[worker 0] D Closing queues...
[worker 1] D Finished batch step 5.
[worker 1] D Sending Job (ID: 30, worker: 1, epoch: 4, set_name: test)...
[worker 1] D Closing queues...
[worker 0] I Test of Epoch 4 - WER: 0.923851, loss: 91.361083984375, mean edit distance: 0.872381
[worker 0] I [RESULTS]
[worker 0] D Checking for early stopping (last 4 steps) validation loss: 157.678299, with standard deviation: 84.527731 and mean: 195.945297
[worker 0] D Epochs - running: 0, done: 9
[worker 0] D Queues closed.
[worker 1] D Queues closed.
[worker 1] D Session closed.
[worker 1] D Server stopped.
[worker 0] Exception in thread QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany:
[worker 0] Traceback (most recent call last):
[worker 0] File "/usr/lib64/python3.6/threading.py", line 916, in _bootstrap_inner
[worker 0] self.run()
[worker 0] File "/usr/lib64/python3.6/threading.py", line 864, in run
[worker 0] self._target(*self._args, **self._kwargs)
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py", line 268, in _run
[worker 0] coord.request_stop(e)
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py", line 213, in request_stop
[worker 0] six.reraise(*sys.exc_info())
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/six.py", line 693, in reraise
[worker 0] raise value
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py", line 252, in _run
[worker 0] enqueue_callable()
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1259, in _single_operation_run
[worker 0] None)
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py", line 516, in __exit__
[worker 0] c_api.TF_GetCode(self.status.status))
[worker 0] tensorflow.python.framework.errors_impl.CancelledError: Step was cancelled by an explicit call to `Session::Close()`.
[worker 0] 
[worker 0] D Session closed.
[worker 0] D Server stopped.
[worker 0] I Exporting the model...
[worker 0] Traceback (most recent call last):
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1361, in _do_call
[worker 0] return fn(*args)
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1332, in _run_fn
[worker 0] self._extend_graph()
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1392, in _extend_graph
[worker 0] graph_def.SerializeToString(), status)
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py", line 516, in __exit__
[worker 0] c_api.TF_GetCode(self.status.status))
[worker 0] tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'h6_1': Operation was explicitly assigned to /job:ps/task:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.
[worker 0] [[Node: h6_1 = VariableV2[_class=["loc:@h6_1"], container="", dtype=DT_FLOAT, shape=[200,29], shared_name="", _device="/job:ps/task:0"]()]]
[worker 0] 
[worker 0] During handling of the above exception, another exception occurred:
[worker 0] 
[worker 0] Traceback (most recent call last):
[worker 0] File "DeepSpeech.py", line 1838, in &lt;module&gt;
[worker 0] tf.app.run()
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/platform/app.py", line 126, in run
[worker 0] _sys.exit(main(argv))
[worker 0] File "DeepSpeech.py", line 1829, in main
[worker 0] export()
[worker 0] File "DeepSpeech.py", line 1751, in export
[worker 0] initializer_nodes='')
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py", line 106, in freeze_graph_with_def_protos
[worker 0] saver.restore(sess, input_checkpoint)
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/training/saver.py", line 1755, in restore
[worker 0] {self.saver_def.filename_tensor_name: save_path})
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 905, in run
[worker 0] run_metadata_ptr)
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1137, in _run
[worker 0] feed_dict_tensor, options, run_metadata)
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1355, in _do_run
[worker 0] options, run_metadata)
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1374, in _do_call
[worker 0] raise type(e)(node_def, op, message)
[worker 0] tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'h6_1': Operation was explicitly assigned to /job:ps/task:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.
[worker 0] [[Node: h6_1 = VariableV2[_class=["loc:@h6_1"], container="", dtype=DT_FLOAT, shape=[200,29], shared_name="", _device="/job:ps/task:0"]()]]
[worker 0] 
[worker 0] Caused by op 'h6_1', defined at:
[worker 0] File "DeepSpeech.py", line 1838, in &lt;module&gt;
[worker 0] tf.app.run()
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/platform/app.py", line 126, in run
[worker 0] _sys.exit(main(argv))
[worker 0] File "DeepSpeech.py", line 1829, in main
[worker 0] export()
[worker 0] File "DeepSpeech.py", line 1751, in export
[worker 0] initializer_nodes='')
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py", line 101, in freeze_graph_with_def_protos
[worker 0] _ = importer.import_graph_def(input_graph_def, name="")
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py", line 432, in new_func
[worker 0] return func(*args, **kwargs)
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/framework/importer.py", line 553, in import_graph_def
[worker 0] op_def=op_def)
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 3271, in create_op
[worker 0] op_def=op_def)
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1650, in __init__
[worker 0] self._traceback = self._graph._extract_stack() # pylint: disable=protected-access
[worker 0]
[worker 0] InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'h6_1': Operation was explicitly assigned to /job:ps/task:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.
[worker 0] [[Node: h6_1 = VariableV2[_class=["loc:@h6_1"], container="", dtype=DT_FLOAT, shape=[200,29], shared_name="", _device="/job:ps/task:0"]()]]
[worker 0]
&lt;/denchmark-code&gt;


With TF 1.8.0:
-- uncommented send_token_to_ps(session)

&lt;denchmark-code&gt;Using script ./run_mini-eng.sh...
Starting cluster with 1 parameter servers and 2 workers with 0 GPUs each...
Started ps 0
Started worker 0
[ps 0] + '[' '!' -f DeepSpeech.py ']'
[ps 0] + python -u DeepSpeech.py --decoder_library_path /home/userys/stt/DeepSpeech/native_client/libctc_decoder_with_kenlm.so --train_files /home/userys/stt/cv_eng-mini/other-dev/training/training.csv --dev_files /home/userys/stt/cv_eng-mini/other-dev/dev/dev.csv --test_files /home/userys/stt/cv_eng-mini/other-dev/test/test.csv --train_batch_size 15 --dev_batch_size 5 --test_batch_size 5 --epoch 4 --n_hidden 200 --display_step 1 --validation_step 1 --dropout_rate 0.07 --learning_rate 0.001 --valid_word_count_weight 3 --report_count 5 --export_dir /home/userys/stt/cv_eng-mini/other-dev/result/model_export/ --checkpoint_dir /home/userys/stt/cv_eng-mini/other-dev/result/checkout/ --alphabet_config_path /home/userys/stt/eng-mini-models/alphabet.txt --lm_binary_path /home/userys/stt/eng-mini-models/lm.binary --lm_trie_path /home/userys/stt/eng-mini-models/trie --log_level 0 --ps_hosts localhost:2000 --worker_hosts localhost:3000,localhost:3001 --job_name=ps --task_index=0
Started worker 1
[worker 0] + '[' '!' -f DeepSpeech.py ']'
[worker 0] + python -u DeepSpeech.py --decoder_library_path /home/userys/stt/DeepSpeech/native_client/libctc_decoder_with_kenlm.so --train_files /home/userys/stt/cv_eng-mini/other-dev/training/training.csv --dev_files /home/userys/stt/cv_eng-mini/other-dev/dev/dev.csv --test_files /home/userys/stt/cv_eng-mini/other-dev/test/test.csv --train_batch_size 15 --dev_batch_size 5 --test_batch_size 5 --epoch 4 --n_hidden 200 --display_step 1 --validation_step 1 --dropout_rate 0.07 --learning_rate 0.001 --valid_word_count_weight 3 --report_count 5 --export_dir /home/userys/stt/cv_eng-mini/other-dev/result/model_export/ --checkpoint_dir /home/userys/stt/cv_eng-mini/other-dev/result/checkout/ --alphabet_config_path /home/userys/stt/eng-mini-models/alphabet.txt --lm_binary_path /home/userys/stt/eng-mini-models/lm.binary --lm_trie_path /home/userys/stt/eng-mini-models/trie --log_level 0 --ps_hosts localhost:2000 --worker_hosts localhost:3000,localhost:3001 --job_name=worker --task_index=0
[worker 1] + '[' '!' -f DeepSpeech.py ']'
[worker 1] + python -u DeepSpeech.py --decoder_library_path /home/userys/stt/DeepSpeech/native_client/libctc_decoder_with_kenlm.so --train_files /home/userys/stt/cv_eng-mini/other-dev/training/training.csv --dev_files /home/userys/stt/cv_eng-mini/other-dev/dev/dev.csv --test_files /home/userys/stt/cv_eng-mini/other-dev/test/test.csv --train_batch_size 15 --dev_batch_size 5 --test_batch_size 5 --epoch 4 --n_hidden 200 --display_step 1 --validation_step 1 --dropout_rate 0.07 --learning_rate 0.001 --valid_word_count_weight 3 --report_count 5 --export_dir /home/userys/stt/cv_eng-mini/other-dev/result/model_export/ --checkpoint_dir /home/userys/stt/cv_eng-mini/other-dev/result/checkout/ --alphabet_config_path /home/userys/stt/eng-mini-models/alphabet.txt --lm_binary_path /home/userys/stt/eng-mini-models/lm.binary --lm_trie_path /home/userys/stt/eng-mini-models/trie --log_level 0 --ps_hosts localhost:2000 --worker_hosts localhost:3000,localhost:3001 --job_name=worker --task_index=1
[ps 0] 2018-07-11 16:42:15.680453: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; localhost:2000}
[ps 0] 2018-07-11 16:42:15.680489: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:3000, 1 -&gt; localhost:3001}
[ps 0] 2018-07-11 16:42:15.681120: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:332] Started server with target: grpc://localhost:2000
[ps 0] D Waiting for stop token...
[worker 0] D Starting coordinator...
[worker 0] D Coordinator started.
[worker 0] 2018-07-11 16:42:15.753550: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; localhost:2000}
[worker 0] 2018-07-11 16:42:15.753574: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:3000, 1 -&gt; localhost:3001}
[worker 1] 2018-07-11 16:42:15.754604: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; localhost:2000}
[worker 1] 2018-07-11 16:42:15.754625: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:3000, 1 -&gt; localhost:3001}
[worker 0] 2018-07-11 16:42:15.757254: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:332] Started server with target: grpc://localhost:3000
[worker 1] 2018-07-11 16:42:15.758096: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:332] Started server with target: grpc://localhost:3001
[ps 0] 2018-07-11 16:42:16.684595: I tensorflow/core/distributed_runtime/master_session.cc:1136] Start master session ef27a9c585d8a796 with config: 
[worker 0] 2018-07-11 16:42:18.243983: I tensorflow/core/distributed_runtime/master_session.cc:1136] Start master session b817adfcb04eb81a with config: allow_soft_placement: true
[worker 1] 2018-07-11 16:42:18.296797: I tensorflow/core/distributed_runtime/master_session.cc:1136] Start master session 1afe39756f04c240 with config: allow_soft_placement: true
[worker 0] D Starting queue runners...
[worker 0] D Queue runners started.
[worker 0] I STARTING Optimization
[worker 0] D step: 0
[worker 0] D epoch: 0
[worker 0] D target epoch: 4
[worker 0] D steps per epoch: 1
[worker 0] D number of batches in train set: 3
[worker 0] D batches per job: 1
[worker 0] D batches per step: 2
[worker 0] D number of jobs in train set: 3
[worker 0] D number of jobs already trained in first epoch: 0
[worker 0] D Computing Job (ID: 2, worker: 0, epoch: 0, set_name: train)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 0.
[worker 0] D Sending Job (ID: 2, worker: 0, epoch: 0, set_name: train)...
[worker 0] D Computing Job (ID: 3, worker: 0, epoch: 0, set_name: train)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 0.
[worker 0] D Sending Job (ID: 3, worker: 0, epoch: 0, set_name: train)...
[worker 0] D Computing Job (ID: 4, worker: 0, epoch: 0, set_name: train)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 1.
[worker 0] D Sending Job (ID: 4, worker: 0, epoch: 0, set_name: train)...
[worker 0] I Training of Epoch 0 - WER: 9.324362, loss: 521.654042561849, mean edit distance: 3.785765
[worker 0] I [RESULTS]
[worker 0] D Computing Job (ID: 6, worker: 0, epoch: 0, set_name: dev)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 1.
[worker 0] D Sending Job (ID: 6, worker: 0, epoch: 0, set_name: dev)...
[worker 0] D Computing Job (ID: 7, worker: 0, epoch: 0, set_name: dev)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 1.
[worker 0] D Sending Job (ID: 7, worker: 0, epoch: 0, set_name: dev)...
[worker 0] I Validation of Epoch 0 - WER: 9.574074, loss: 422.3068542480469, mean edit distance: 4.182127
[worker 0] I [RESULTS]
[worker 0] D Computing Job (ID: 9, worker: 0, epoch: 1, set_name: train)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 1.
[worker 0] D Sending Job (ID: 9, worker: 0, epoch: 1, set_name: train)...
[worker 0] D Computing Job (ID: 10, worker: 0, epoch: 1, set_name: train)...
[worker 0] D Starting batch...
[worker 1] 2018-07-11 16:42:48.461734: I tensorflow/core/distributed_runtime/master_session.cc:1136] Start master session e23e4e2896da8a20 with config: allow_soft_placement: true
[worker 1] D Starting queue runners...
[worker 1] D Queue runners started.
[worker 1] D Computing Job (ID: 11, worker: 1, epoch: 1, set_name: train)...
[worker 1] D Starting batch...
[worker 0] D Finished batch step 1.
[worker 0] D Sending Job (ID: 10, worker: 0, epoch: 1, set_name: train)...
[worker 0] D Computing Job (ID: 13, worker: 0, epoch: 1, set_name: dev)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 2.
[worker 0] D Sending Job (ID: 13, worker: 0, epoch: 1, set_name: dev)...
[worker 0] D Computing Job (ID: 14, worker: 0, epoch: 1, set_name: dev)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 2.
[worker 0] D Sending Job (ID: 14, worker: 0, epoch: 1, set_name: dev)...
[worker 0] I Validation of Epoch 1 - WER: 8.200000, loss: 339.6593017578125, mean edit distance: 3.579429
[worker 0] I [RESULTS]
[worker 0] D Computing Job (ID: 16, worker: 0, epoch: 2, set_name: train)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 2.
[worker 0] D Sending Job (ID: 16, worker: 0, epoch: 2, set_name: train)...
[worker 0] D Computing Job (ID: 17, worker: 0, epoch: 2, set_name: train)...
[worker 0] D Starting batch...
[worker 1] D Finished batch step 2.
[worker 1] D Sending Job (ID: 11, worker: 1, epoch: 1, set_name: train)...
[worker 0] I Training of Epoch 1 - WER: 9.164713, loss: 490.5286051432292, mean edit distance: 3.772048
[worker 0] I [RESULTS]
[worker 1] D Computing Job (ID: 18, worker: 1, epoch: 2, set_name: train)...
[worker 1] D Starting batch...
[worker 0] D Finished batch step 3.
[worker 0] D Sending Job (ID: 17, worker: 0, epoch: 2, set_name: train)...
[worker 0] D Computing Job (ID: 20, worker: 0, epoch: 2, set_name: dev)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 3.
[worker 0] D Sending Job (ID: 20, worker: 0, epoch: 2, set_name: dev)...
[worker 0] D Computing Job (ID: 21, worker: 0, epoch: 2, set_name: dev)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 3.
[worker 0] D Sending Job (ID: 21, worker: 0, epoch: 2, set_name: dev)...
[worker 0] I Validation of Epoch 2 - WER: 0.947222, loss: 227.6219940185547, mean edit distance: 0.911823
[worker 0] I [RESULTS]
[worker 0] D Computing Job (ID: 23, worker: 0, epoch: 3, set_name: train)...
[worker 0] D Starting batch...
[worker 1] D Finished batch step 3.
[worker 1] D Sending Job (ID: 18, worker: 1, epoch: 2, set_name: train)...
[worker 0] I Training of Epoch 2 - WER: 2.908756, loss: 238.54271952311197, mean edit distance: 1.532408
[worker 0] I [RESULTS]
[worker 1] D Computing Job (ID: 24, worker: 1, epoch: 3, set_name: train)...
[worker 1] D Starting batch...
[worker 0] D Finished batch step 3.
[worker 0] D Sending Job (ID: 23, worker: 0, epoch: 3, set_name: train)...
[worker 0] D Computing Job (ID: 25, worker: 0, epoch: 3, set_name: train)...
[worker 0] D Starting batch...
[worker 1] D Finished batch step 4.
[worker 1] D Sending Job (ID: 24, worker: 1, epoch: 3, set_name: train)...
[worker 1] D Computing Job (ID: 27, worker: 1, epoch: 3, set_name: dev)...
[worker 1] D Starting batch...
[worker 1] D Finished batch step 5.
[worker 1] D Sending Job (ID: 27, worker: 1, epoch: 3, set_name: dev)...
[worker 1] D Computing Job (ID: 28, worker: 1, epoch: 3, set_name: dev)...
[worker 1] D Starting batch...
[worker 0] D Finished batch step 4.
[worker 0] D Sending Job (ID: 25, worker: 0, epoch: 3, set_name: train)...
[worker 0] I Training of Epoch 3 - WER: 1.021267, loss: 218.6768595377604, mean edit distance: 0.892002
[worker 0] I [RESULTS]
[worker 0] I FINISHED Optimization - training time: 0:01:46
[worker 0] D Computing Job (ID: 30, worker: 0, epoch: 4, set_name: test)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 5.
[worker 0] D Sending Job (ID: 30, worker: 0, epoch: 4, set_name: test)...
[worker 0] D Computing Job (ID: 31, worker: 0, epoch: 4, set_name: test)...
[worker 0] D Starting batch...
[worker 1] D Finished batch step 5.
[worker 1] D Sending Job (ID: 28, worker: 1, epoch: 3, set_name: dev)...
[worker 0] I Validation of Epoch 3 - WER: 0.989130, loss: 198.5041732788086, mean edit distance: 0.892190
[worker 0] I [RESULTS]
[worker 0] D Checking for early stopping (last 4 steps) validation loss: 198.504173, with standard deviation: 79.781069 and mean: 329.862717
[worker 0] D Epochs - running: 1, done: 8
[worker 0] D - running: Test of Epoch 4 - jobs open: 0, jobs running: 1, jobs done: 1
[worker 1] D Closing queues...
[worker 1] D Queues closed.
[worker 1] D Sending stop token to ps 0...
[ps 0] D Got a stop token from worker 1.
[ps 0] D Waiting for stop token...
[worker 1] D Sent stop token to ps 0.
[worker 0] D Finished batch step 5.
[worker 0] D Sending Job (ID: 31, worker: 0, epoch: 4, set_name: test)...
[worker 0] I Test of Epoch 4 - WER: 1.000000, loss: 175.42423248291016, mean edit distance: 0.904292
[worker 0] I [RESULTS]
[worker 0] D Checking for early stopping (last 4 steps) validation loss: 198.504173, with standard deviation: 79.781069 and mean: 329.862717
[worker 0] D Epochs - running: 0, done: 9
[worker 0] D Closing queues...
[worker 0] D Queues closed.
[worker 0] D Sending stop token to ps 0...
[ps 0] D Got a stop token from worker 0.
[worker 0] D Sent stop token to ps 0.
[ps 0] D Session closed.
[ps 0] D Server stopped.
[worker 1] D Session closed.
[worker 1] D Server stopped.
&lt;/denchmark-code&gt;

-- commented send_token_to_ps(session)
&lt;denchmark-code&gt;Starting cluster with 1 parameter servers and 2 workers with 0 GPUs each...
Started ps 0
[ps 0] + '[' '!' -f DeepSpeech.py ']'
[ps 0] + python -u DeepSpeech.py --decoder_library_path /home/userys/stt/DeepSpeech/native_client/libctc_decoder_with_kenlm.so --train_files /home/userys/stt/cv_eng-mini/other-dev/training/training.csv --dev_files /home/userys/stt/cv_eng-mini/other-dev/dev/dev.csv --test_files /home/userys/stt/cv_eng-mini/other-dev/test/test.csv --train_batch_size 15 --dev_batch_size 5 --test_batch_size 5 --epoch 4 --n_hidden 200 --display_step 1 --validation_step 1 --dropout_rate 0.07 --learning_rate 0.001 --valid_word_count_weight 3 --report_count 5 --export_dir /home/userys/stt/cv_eng-mini/other-dev/result/model_export/ --checkpoint_dir /home/userys/stt/cv_eng-mini/other-dev/result/checkout/ --alphabet_config_path /home/userys/stt/eng-mini-models/alphabet.txt --lm_binary_path /home/userys/stt/eng-mini-models/lm.binary --lm_trie_path /home/userys/stt/eng-mini-models/trie --log_level 0 --ps_hosts localhost:2000 --worker_hosts localhost:3000,localhost:3001 --job_name=ps --task_index=0
Started worker 0
Started worker 1
[worker 0] + '[' '!' -f DeepSpeech.py ']'
[worker 1] + '[' '!' -f DeepSpeech.py ']'
[worker 0] + python -u DeepSpeech.py --decoder_library_path /home/userys/stt/DeepSpeech/native_client/libctc_decoder_with_kenlm.so --train_files /home/userys/stt/cv_eng-mini/other-dev/training/training.csv --dev_files /home/userys/stt/cv_eng-mini/other-dev/dev/dev.csv --test_files /home/userys/stt/cv_eng-mini/other-dev/test/test.csv --train_batch_size 15 --dev_batch_size 5 --test_batch_size 5 --epoch 4 --n_hidden 200 --display_step 1 --validation_step 1 --dropout_rate 0.07 --learning_rate 0.001 --valid_word_count_weight 3 --report_count 5 --export_dir /home/userys/stt/cv_eng-mini/other-dev/result/model_export/ --checkpoint_dir /home/userys/stt/cv_eng-mini/other-dev/result/checkout/ --alphabet_config_path /home/userys/stt/eng-mini-models/alphabet.txt --lm_binary_path /home/userys/stt/eng-mini-models/lm.binary --lm_trie_path /home/userys/stt/eng-mini-models/trie --log_level 0 --ps_hosts localhost:2000 --worker_hosts localhost:3000,localhost:3001 --job_name=worker --task_index=0
[worker 1] + python -u DeepSpeech.py --decoder_library_path /home/userys/stt/DeepSpeech/native_client/libctc_decoder_with_kenlm.so --train_files /home/userys/stt/cv_eng-mini/other-dev/training/training.csv --dev_files /home/userys/stt/cv_eng-mini/other-dev/dev/dev.csv --test_files /home/userys/stt/cv_eng-mini/other-dev/test/test.csv --train_batch_size 15 --dev_batch_size 5 --test_batch_size 5 --epoch 4 --n_hidden 200 --display_step 1 --validation_step 1 --dropout_rate 0.07 --learning_rate 0.001 --valid_word_count_weight 3 --report_count 5 --export_dir /home/userys/stt/cv_eng-mini/other-dev/result/model_export/ --checkpoint_dir /home/userys/stt/cv_eng-mini/other-dev/result/checkout/ --alphabet_config_path /home/userys/stt/eng-mini-models/alphabet.txt --lm_binary_path /home/userys/stt/eng-mini-models/lm.binary --lm_trie_path /home/userys/stt/eng-mini-models/trie --log_level 0 --ps_hosts localhost:2000 --worker_hosts localhost:3000,localhost:3001 --job_name=worker --task_index=1
[worker 0] D Starting coordinator...
[worker 0] D Coordinator started.
[worker 1] 2018-07-11 16:50:21.704450: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; localhost:2000}
[worker 1] 2018-07-11 16:50:21.704511: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:3000, 1 -&gt; localhost:3001}
[worker 0] 2018-07-11 16:50:21.704425: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; localhost:2000}
[worker 0] 2018-07-11 16:50:21.704480: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:3000, 1 -&gt; localhost:3001}
[ps 0] 2018-07-11 16:50:21.705124: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; localhost:2000}
[ps 0] 2018-07-11 16:50:21.705150: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:3000, 1 -&gt; localhost:3001}
[ps 0] 2018-07-11 16:50:21.708588: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:332] Started server with target: grpc://localhost:2000
[ps 0] D Waiting for stop token...
[worker 0] 2018-07-11 16:50:21.708790: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:332] Started server with target: grpc://localhost:3000
[worker 1] 2018-07-11 16:50:21.709005: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:332] Started server with target: grpc://localhost:3001
[ps 0] 2018-07-11 16:50:21.802474: I tensorflow/core/distributed_runtime/master_session.cc:1136] Start master session 0cdbd239b87bbe75 with config: 
[worker 0] 2018-07-11 16:50:24.223502: I tensorflow/core/distributed_runtime/master_session.cc:1136] Start master session 545eda9ad6647500 with config: allow_soft_placement: true
[worker 1] 2018-07-11 16:50:24.252122: I tensorflow/core/distributed_runtime/master_session.cc:1136] Start master session be8cfd37b4030ed2 with config: allow_soft_placement: true
[worker 0] D Starting queue runners...
[worker 0] D Queue runners started.
[worker 0] I STARTING Optimization
[worker 0] D step: 0
[worker 0] D epoch: 0
[worker 0] D target epoch: 4
[worker 0] D steps per epoch: 1
[worker 0] D number of batches in train set: 3
[worker 0] D batches per job: 1
[worker 0] D batches per step: 2
[worker 0] D number of jobs in train set: 3
[worker 0] D number of jobs already trained in first epoch: 0
[worker 0] D Computing Job (ID: 2, worker: 0, epoch: 0, set_name: train)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 0.
[worker 0] D Sending Job (ID: 2, worker: 0, epoch: 0, set_name: train)...
[worker 0] D Computing Job (ID: 3, worker: 0, epoch: 0, set_name: train)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 0.
[worker 0] D Sending Job (ID: 3, worker: 0, epoch: 0, set_name: train)...
[worker 0] D Computing Job (ID: 4, worker: 0, epoch: 0, set_name: train)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 1.
[worker 0] D Sending Job (ID: 4, worker: 0, epoch: 0, set_name: train)...
[worker 0] I Training of Epoch 0 - WER: 9.105985, loss: 500.3532206217448, mean edit distance: 3.754815
[worker 0] I [RESULTS]
[worker 0] D Computing Job (ID: 6, worker: 0, epoch: 0, set_name: dev)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 1.
[worker 0] D Sending Job (ID: 6, worker: 0, epoch: 0, set_name: dev)...
[worker 0] D Computing Job (ID: 7, worker: 0, epoch: 0, set_name: dev)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 1.
[worker 0] D Sending Job (ID: 7, worker: 0, epoch: 0, set_name: dev)...
[worker 0] I Validation of Epoch 0 - WER: 8.719643, loss: 376.02671813964844, mean edit distance: 3.781292
[worker 0] I [RESULTS]
[worker 0] D Computing Job (ID: 9, worker: 0, epoch: 1, set_name: train)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 1.
[worker 0] D Sending Job (ID: 9, worker: 0, epoch: 1, set_name: train)...
[worker 0] D Computing Job (ID: 10, worker: 0, epoch: 1, set_name: train)...
[worker 0] D Starting batch...
[worker 1] 2018-07-11 16:50:54.453482: I tensorflow/core/distributed_runtime/master_session.cc:1136] Start master session bed956680e5261b0 with config: allow_soft_placement: true
[worker 1] D Starting queue runners...
[worker 1] D Queue runners started.
[worker 1] D Computing Job (ID: 11, worker: 1, epoch: 1, set_name: train)...
[worker 1] D Starting batch...
[worker 0] D Finished batch step 1.
[worker 0] D Sending Job (ID: 10, worker: 0, epoch: 1, set_name: train)...
[worker 0] D Computing Job (ID: 13, worker: 0, epoch: 1, set_name: dev)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 2.
[worker 0] D Sending Job (ID: 13, worker: 0, epoch: 1, set_name: dev)...
[worker 0] D Computing Job (ID: 14, worker: 0, epoch: 1, set_name: dev)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 2.
[worker 0] D Sending Job (ID: 14, worker: 0, epoch: 1, set_name: dev)...
[worker 0] I Validation of Epoch 1 - WER: 5.057143, loss: 212.5933609008789, mean edit distance: 2.036126
[worker 0] I [RESULTS]
[worker 0] D Computing Job (ID: 16, worker: 0, epoch: 2, set_name: train)...
[worker 0] D Starting batch...
[worker 1] D Finished batch step 2.
[worker 1] D Sending Job (ID: 11, worker: 1, epoch: 1, set_name: train)...
[worker 0] I Training of Epoch 1 - WER: 7.593889, loss: 403.73203531901044, mean edit distance: 2.989009
[worker 0] I [RESULTS]
[worker 1] D Computing Job (ID: 17, worker: 1, epoch: 2, set_name: train)...
[worker 1] D Starting batch...
[worker 0] D Finished batch step 2.
[worker 0] D Sending Job (ID: 16, worker: 0, epoch: 2, set_name: train)...
[worker 0] D Computing Job (ID: 18, worker: 0, epoch: 2, set_name: train)...
[worker 0] D Starting batch...
[worker 1] D Finished batch step 2.
[worker 1] D Sending Job (ID: 17, worker: 1, epoch: 2, set_name: train)...
[worker 1] D Computing Job (ID: 20, worker: 1, epoch: 2, set_name: dev)...
[worker 1] D Starting batch...
[worker 1] D Finished batch step 3.
[worker 1] D Sending Job (ID: 20, worker: 1, epoch: 2, set_name: dev)...
[worker 1] D Computing Job (ID: 21, worker: 1, epoch: 2, set_name: dev)...
[worker 1] D Starting batch...
[worker 1] D Finished batch step 3.
[worker 1] D Sending Job (ID: 21, worker: 1, epoch: 2, set_name: dev)...
[worker 0] I Validation of Epoch 2 - WER: 1.192432, loss: 118.36197280883789, mean edit distance: 0.735087
[worker 0] I [RESULTS]
[worker 1] D Computing Job (ID: 23, worker: 1, epoch: 3, set_name: train)...
[worker 1] D Starting batch...
[worker 0] D Finished batch step 3.
[worker 0] D Sending Job (ID: 18, worker: 0, epoch: 2, set_name: train)...
[worker 0] I Training of Epoch 2 - WER: 3.223596, loss: 172.06224568684897, mean edit distance: 1.382320
[worker 0] I [RESULTS]
[worker 0] D Computing Job (ID: 24, worker: 0, epoch: 3, set_name: train)...
[worker 0] D Starting batch...
[worker 1] D Finished batch step 3.
[worker 1] D Sending Job (ID: 23, worker: 1, epoch: 3, set_name: train)...
[worker 1] D Computing Job (ID: 25, worker: 1, epoch: 3, set_name: train)...
[worker 1] D Starting batch...
[worker 0] D Finished batch step 4.
[worker 0] D Sending Job (ID: 24, worker: 0, epoch: 3, set_name: train)...
[worker 0] D Computing Job (ID: 27, worker: 0, epoch: 3, set_name: dev)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 5.
[worker 0] D Sending Job (ID: 27, worker: 0, epoch: 3, set_name: dev)...
[worker 0] D Computing Job (ID: 28, worker: 0, epoch: 3, set_name: dev)...
[worker 0] D Starting batch...
[worker 0] D Finished batch step 5.
[worker 0] D Sending Job (ID: 28, worker: 0, epoch: 3, set_name: dev)...
[worker 0] I Validation of Epoch 3 - WER: 0.934722, loss: 150.45843505859375, mean edit distance: 0.881838
[worker 0] I [RESULTS]
[worker 0] D Checking for early stopping (last 4 steps) validation loss: 150.458435, with standard deviation: 106.448286 and mean: 235.660684
[worker 0] I FINISHED Optimization - training time: 0:01:49
[worker 0] D Computing Job (ID: 30, worker: 0, epoch: 4, set_name: test)...
[worker 0] D Starting batch...
[worker 1] D Finished batch step 4.
[worker 1] D Sending Job (ID: 25, worker: 1, epoch: 3, set_name: train)...
[worker 0] I Training of Epoch 3 - WER: 1.060930, loss: 154.43622334798178, mean edit distance: 0.864035
[worker 0] I [RESULTS]
[worker 1] D Computing Job (ID: 31, worker: 1, epoch: 4, set_name: test)...
[worker 1] D Starting batch...
[worker 0] D Finished batch step 5.
[worker 0] D Sending Job (ID: 30, worker: 0, epoch: 4, set_name: test)...
[worker 0] D Checking for early stopping (last 4 steps) validation loss: 150.458435, with standard deviation: 106.448286 and mean: 235.660684
[worker 0] D Epochs - running: 1, done: 8
[worker 0] D - running: Test of Epoch 4 - jobs open: 0, jobs running: 1, jobs done: 1
[worker 0] D Closing queues...
[worker 0] D Queues closed.
[worker 1] D Finished batch step 5.
[worker 1] D Sending Job (ID: 31, worker: 1, epoch: 4, set_name: test)...
[worker 0] I Test of Epoch 4 - WER: 0.965517, loss: 104.44120025634766, mean edit distance: 0.924921
[worker 0] I [RESULTS]
[worker 0] D Checking for early stopping (last 4 steps) validation loss: 150.458435, with standard deviation: 106.448286 and mean: 235.660684
[worker 0] D Epochs - running: 0, done: 9
[worker 1] D Closing queues...
[worker 1] D Queues closed.
[worker 1] D Session closed.
[worker 1] D Server stopped.
[worker 0] Exception in thread QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany:
[worker 0] Traceback (most recent call last):
[worker 0] File "/usr/lib64/python3.6/threading.py", line 916, in _bootstrap_inner
[worker 0] self.run()
[worker 0] File "/usr/lib64/python3.6/threading.py", line 864, in run
[worker 0] self._target(*self._args, **self._kwargs)
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py", line 268, in _run
[worker 0] coord.request_stop(e)
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py", line 213, in request_stop
[worker 0] six.reraise(*sys.exc_info())
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/six.py", line 693, in reraise
[worker 0] raise value
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py", line 252, in _run
[worker 0] enqueue_callable()
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1244, in _single_operation_run
[worker 0] self._call_tf_sessionrun(None, {}, [], target_list, None)
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1409, in _call_tf_sessionrun
[worker 0] run_metadata)
[worker 0] tensorflow.python.framework.errors_impl.CancelledError: Step was cancelled by an explicit call to `Session::Close()`.
[worker 0] 
[worker 0] D Session closed.
[worker 0] D Server stopped.
[worker 0] I Exporting the model...
[worker 0] Traceback (most recent call last):
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1322, in _do_call
[worker 0] return fn(*args)
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1305, in _run_fn
[worker 0] self._extend_graph()
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1340, in _extend_graph
[worker 0] tf_session.ExtendSession(self._session)
[worker 0] tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'b1/Initializer/random_normal/RandomStandardNormal_1': Operation was explicitly assigned to /job:ps/task:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.
[worker 0] [[Node: b1/Initializer/random_normal/RandomStandardNormal_1 = RandomStandardNormal[T=DT_INT32, _class=["loc:@b1_1"], dtype=DT_FLOAT, seed=0, seed2=0, _device="/job:ps/task:0"](b1/Initializer/random_normal/shape_1:0)]]
[worker 0] 
[worker 0] During handling of the above exception, another exception occurred:
[worker 0] 
[worker 0] Traceback (most recent call last):
[worker 0] File "DeepSpeech.py", line 1838, in &lt;module&gt;
[worker 0] tf.app.run()
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/platform/app.py", line 126, in run
[worker 0] _sys.exit(main(argv))
[worker 0] File "DeepSpeech.py", line 1829, in main
[worker 0] export()
[worker 0] File "DeepSpeech.py", line 1751, in export
[worker 0] initializer_nodes='')
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py", line 104, in freeze_graph_with_def_protos
[worker 0] saver.restore(sess, input_checkpoint)
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/training/saver.py", line 1802, in restore
[worker 0] {self.saver_def.filename_tensor_name: save_path})
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 900, in run
[worker 0] run_metadata_ptr)
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1135, in _run
[worker 0] feed_dict_tensor, options, run_metadata)
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1316, in _do_run
[worker 0] run_metadata)
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1335, in _do_call
[worker 0] raise type(e)(node_def, op, message)
[worker 0] tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'b1/Initializer/random_normal/RandomStandardNormal_1': Operation was explicitly assigned to /job:ps/task:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.
[worker 0] [[Node: b1/Initializer/random_normal/RandomStandardNormal_1 = RandomStandardNormal[T=DT_INT32, _class=["loc:@b1_1"], dtype=DT_FLOAT, seed=0, seed2=0, _device="/job:ps/task:0"](b1/Initializer/random_normal/shape_1:0)]]
[worker 0] 
[worker 0] Caused by op 'b1/Initializer/random_normal/RandomStandardNormal_1', defined at:
[worker 0] File "DeepSpeech.py", line 1838, in &lt;module&gt;
[worker 0] tf.app.run()
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/platform/app.py", line 126, in run
[worker 0] _sys.exit(main(argv))
[worker 0] File "DeepSpeech.py", line 1829, in main
[worker 0] export()
[worker 0] File "DeepSpeech.py", line 1751, in export
[worker 0] initializer_nodes='')
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py", line 99, in freeze_graph_with_def_protos
[worker 0] _ = importer.import_graph_def(input_graph_def, name="")
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py", line 432, in new_func
[worker 0] return func(*args, **kwargs)
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/framework/importer.py", line 513, in import_graph_def
[worker 0] _ProcessNewOps(graph)
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/framework/importer.py", line 303, in _ProcessNewOps
[worker 0] for new_op in graph._add_new_tf_operations(compute_devices=False): # pylint: disable=protected-access
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 3540, in _add_new_tf_operations
[worker 0] for c_op in c_api_util.new_tf_operations(self)
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 3540, in &lt;listcomp&gt;
[worker 0] for c_op in c_api_util.new_tf_operations(self)
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 3428, in _create_op_from_tf_operation
[worker 0] ret = Operation(c_op, self)
[worker 0] File "/home/userys/stt/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1718, in __init__
[worker 0] self._traceback = self._graph._extract_stack() # pylint: disable=protected-access
[worker 0] 
[worker 0] InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'b1/Initializer/random_normal/RandomStandardNormal_1': Operation was explicitly assigned to /job:ps/task:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.
[worker 0] [[Node: b1/Initializer/random_normal/RandomStandardNormal_1 = RandomStandardNormal[T=DT_INT32, _class=["loc:@b1_1"], dtype=DT_FLOAT, seed=0, seed2=0, _device="/job:ps/task:0"](b1/Initializer/random_normal/shape_1:0)]]
[worker 0]
&lt;/denchmark-code&gt;

pip freeze (at start):
&lt;denchmark-code&gt;absl-py==0.2.2
asn1crypto==0.24.0
astor==0.6.2
bcrypt==3.1.4
beautifulsoup4==4.6.0
bleach==1.5.0
bs4==0.0.1
certifi==2018.4.16
cffi==1.11.5
chardet==3.0.4
cryptography==2.2.2
cycler==0.10.0
deepspeech==0.1.1
gast==0.2.0
grpcio==1.12.1
html5lib==0.9999999
idna==2.7
kiwisolver==1.0.1
Markdown==2.6.11
matplotlib==2.2.2
numpy==1.14.5
pandas==0.23.1
paramiko==2.4.1
progressbar2==3.38.0
protobuf==3.6.0
pyasn1==0.4.3
pycparser==2.18
PyNaCl==1.2.1
pyparsing==2.2.0
pysftp==0.2.9
python-dateutil==2.7.3
python-speech-features==0.6
python-utils==2.3.0
pytz==2018.4
pyxdg==0.26
requests==2.19.1
scipy==1.1.0
six==1.11.0
sox==1.3.3
tensorboard==1.8.0
tensorflow==1.8.0
termcolor==1.1.0
urllib3==1.23
Werkzeug==0.14.1
&lt;/denchmark-code&gt;

after pip uninstall tensorflow/tensorboard, pip install tensorflow==1.6.0:
&lt;denchmark-code&gt;absl-py==0.2.2
asn1crypto==0.24.0
astor==0.6.2
bcrypt==3.1.4
beautifulsoup4==4.6.0
bleach==1.5.0
bs4==0.0.1
certifi==2018.4.16
cffi==1.11.5
chardet==3.0.4
cryptography==2.2.2
cycler==0.10.0
deepspeech==0.1.1
gast==0.2.0
grpcio==1.12.1
html5lib==0.9999999
idna==2.7
kiwisolver==1.0.1
Markdown==2.6.11
matplotlib==2.2.2
numpy==1.14.5
pandas==0.23.1
paramiko==2.4.1
progressbar2==3.38.0
protobuf==3.6.0
pyasn1==0.4.3
pycparser==2.18
PyNaCl==1.2.1
pyparsing==2.2.0
pysftp==0.2.9
python-dateutil==2.7.3
python-speech-features==0.6
python-utils==2.3.0
pytz==2018.4
pyxdg==0.26
requests==2.19.1
scipy==1.1.0
six==1.11.0
sox==1.3.3
tensorboard==1.6.0
tensorflow==1.6.0
termcolor==1.1.0
urllib3==1.23
Werkzeug==0.14.1
&lt;/denchmark-code&gt;

Here I've used run_cluster.sh, but when using actual machines as nodes, worker also hangs at the end of the training.
Can you please give some advise?
		</comment>
		<comment id='10' author='pecastro' date='2018-11-20T14:28:17Z'>
		Same problem here.
		</comment>
		<comment id='11' author='pecastro' date='2019-04-02T13:48:59Z'>
		The training coordinator got removed through &lt;denchmark-link:https://github.com/mozilla/DeepSpeech/pull/1988&gt;#1988&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='12' author='pecastro' date='2019-05-02T13:59:50Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>