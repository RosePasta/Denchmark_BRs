<bug id='1568' author='tilmankamp' open_date='2018-09-20T15:15:53Z' closed_time='2019-04-02T13:51:37Z'>
	<summary>Missing allocation of a port and passing it to DeepSpeech.py</summary>
	<description>
The internal step/job coordinator of DeepSpeech.py requires a free port on the host. If more than one DeepSpeech instance runs on one machine, the assigned standard port(s) collide and the second instance will fail and exit.
Things to do:

Add a .pitrequest.txt to the root of the project that allocates 8 GPUs plus one port ([8:txp,port]).
Within .compute script: Pass Snakepit's HOST_GROUP0_PROCESS0_PORT0 environment variable to the --coord_port parameter of DeepSpeech.py.
Side-task: Remove legacy /bin/job-template.sbatch.

	</description>
	<comments>
		<comment id='1' author='tilmankamp' date='2018-09-20T15:21:46Z'>
		Could this be worked around by just binding to port 0 and letting the OS give us a free port, by default?
		</comment>
		<comment id='2' author='tilmankamp' date='2018-09-20T18:31:39Z'>
		It would do it (preventing collision), but it's not the "correct" future-proof and fair way.
Imagine a competing job (not necessarily DeepSpeech.py) on the same node that asks for a series of ports. As ports have to be announced to all job-tasks on creation, the reserving daemon (Snakepit) cannot already bind to them (and by that reserve them on system level). You can see that this can easily end up in a big mess.
An ideal solution would be complete virtualisation like with Kubernetes where the network within a "pod" is completely independent of the outside world and all pod tasks bind to ports of the same virtual localhost without any concurrency from outside.
		</comment>
		<comment id='3' author='tilmankamp' date='2018-09-20T19:03:37Z'>
		Got it.
		</comment>
		<comment id='4' author='tilmankamp' date='2019-04-02T13:51:37Z'>
		Obsolete as LXD based Snakepit service creates virtual network interfaces that are under full control of the tasks.
		</comment>
		<comment id='5' author='tilmankamp' date='2019-05-02T13:59:47Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>