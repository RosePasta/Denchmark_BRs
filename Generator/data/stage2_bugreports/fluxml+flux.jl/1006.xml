<bug id='1006' author='tanhevg' open_date='2020-01-27T13:37:14Z' closed_time='2020-09-24T10:26:47Z'>
	<summary>OneHotMatrix causes a 'scalar getindex disallowed' error on GPU</summary>
	<description>
I think this issue is causing many reported bugs that complain about "slowness" of OneHotMatrix
I suspect the underlying issue is that  is not properly adapted for GPU storage, and  stores an vector of .  A workaround would be to change  to store a  and a size, as proposed by &lt;denchmark-link:https://github.com/FluxML/Flux.jl/pull/578&gt;#578&lt;/denchmark-link&gt;
 . It would then be easy to adapt the vector for GPU storage.
MWE:
&lt;denchmark-code&gt;using Flux, CuArrays
CuArrays.allowscalar(false)
using Flux: onehotbatch

ohb = onehotbatch(rand(1:10, 100), 1:10) |&gt; gpu;
dl = Dense(10, 5) |&gt; gpu;

dl(ohb);

ERROR: scalar getindex is disallowed
Stacktrace:
 [1] assertscalar(::String) at /data/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:14
 [2] getindex at /data/.julia/packages/GPUArrays/1wgPO/src/indexing.jl:54 [inlined]
 [3] iterate at ./abstractarray.jl:914 [inlined]
 [4] iterate at ./abstractarray.jl:912 [inlined]
 [5] checkindex at ./abstractarray.jl:572 [inlined]
 [6] checkbounds_indices at ./abstractarray.jl:529 [inlined] (repeats 2 times)
 [7] checkbounds at ./abstractarray.jl:482 [inlined]
 [8] checkbounds at ./abstractarray.jl:503 [inlined]
 [9] _getindex at ./multidimensional.jl:669 [inlined]
 [10] getindex at ./abstractarray.jl:981 [inlined]
 [11] *(::CuArray{Float32,2,Nothing}, ::Flux.OneHotMatrix{CuArray{Flux.OneHotVector,1,Nothing}}) at /data/.julia/packages/Flux/2i5P1/src/onehot.jl:30
 [12] (::Dense{typeof(identity),CuArray{Float32,2,Nothing},CuArray{Float32,1,Nothing}})(::Flux.OneHotMatrix{CuArray{Flux.OneHotVector,1,Nothing}}) at /data/.julia/packages/Flux/2i5P1/src/layers/basic.jl:102
 [13] top-level scope at none:0
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='tanhevg' date='2020-01-27T14:56:09Z'>
		Could you try with &lt;denchmark-link:https://github.com/FluxML/Flux.jl/pull/764&gt;#764&lt;/denchmark-link&gt;
 ?
		</comment>
		<comment id='2' author='tanhevg' date='2020-01-27T15:56:52Z'>
		@dhairyagandhi96, &lt;denchmark-link:https://github.com/FluxML/Flux.jl/pull/764&gt;#764&lt;/denchmark-link&gt;
 makes no difference, it makes some changes to  which is not called in this example.
		</comment>
		<comment id='3' author='tanhevg' date='2020-02-03T11:20:02Z'>
		julia&gt; ohb = float.(onehotbatch(rand(1:10, 100), 1:10))  |&gt; gpu
10×100 CuArray{Float32,2,Nothing}:
....
or
julia&gt; ohb = cu.(onehotbatch(rand(1:10, 100), 1:10))  |&gt; gpu
10×100 CuArray{Float32,2,Nothing}:
....
julia&gt; dl(ohb)
5×100 CuArray{Float32,2,Nothing}:
....
		</comment>
		<comment id='4' author='tanhevg' date='2020-02-03T11:44:25Z'>
		Right, I misread that, apologies for that. Minimizing shouldn't be hard, it's probably in the matmul, but would need to cross check.
		</comment>
		<comment id='5' author='tanhevg' date='2020-02-03T13:39:48Z'>
		As far as I understand, the point of having  as a separate type is to replace the expensive matmul with inexpensive indexing operation. This optimisation is lost in the workaround proposed by &lt;denchmark-link:https://github.com/mrchaos&gt;@mrchaos&lt;/denchmark-link&gt;
 .
It also does not currently work properly with CuArrays, as demonstrated in the OP. I suppose this code from onehot.jl is supposed to make it work, but evidently it does not.
&lt;denchmark-code&gt;adapt_structure(T, xs::OneHotMatrix) = OneHotMatrix(xs.height, adapt(T, xs.data))

import .CuArrays: CuArray, cudaconvert
import Base.Broadcast: BroadcastStyle, ArrayStyle
BroadcastStyle(::Type{&lt;:OneHotMatrix{&lt;:CuArray}}) = ArrayStyle{CuArray}()
cudaconvert(x::OneHotMatrix{&lt;:CuArray}) = OneHotMatrix(x.height, cudaconvert(x.data))
&lt;/denchmark-code&gt;

I am not 100% sure of the correct way &lt;denchmark-link:https://github.com/JuliaGPU/Adapt.jl&gt;Adapt.jl&lt;/denchmark-link&gt;
 functions should be used to achieve the desired behaviour (docs are a bit scarce). One workaround I can think of is that instead of holding a  of  inside , it should hold an array of indices ().
		</comment>
		<comment id='6' author='tanhevg' date='2020-02-29T11:10:43Z'>
		In any case, we don't have this problem when computing crossentropies (which is the main reason why OneHotMatrix is there).
using Flux, CuArrays
CuArrays.allowscalar(false)
using Flux: onehotbatch

ohb = onehotbatch(rand(1:10, 100), 1:10) |&gt; gpu;
ŷ = CuArrays.rand(size(ohb)...)
Flux.crossentropy(ŷ, ohb)
I'm not sure wether OneHotMatrix was ever meant to be a fully-fledged AbstractMatrix to use as an input to a model
		</comment>
		<comment id='7' author='tanhevg' date='2020-03-02T11:48:12Z'>
		
I'm not sure wether OneHotMatrix was ever meant to be a fully-fledged AbstractMatrix to use as an input to a model

One-hot encoding is a standard technique used in language modelling and other applications of deep learning. When training these models on GPU with Flux, one currently has to revert to dense arrays with zeros and ones for that purpose. This is not ideal; either OneHotMatrix needs to be fixed to support one-hot encoding (seems like a natural fit), or a separate type should be provided.
		</comment>
		<comment id='8' author='tanhevg' date='2020-03-12T15:20:10Z'>
		&lt;denchmark-link:https://github.com/FluxML/Flux.jl/issues/958&gt;#958&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='tanhevg' date='2020-09-24T10:26:47Z'>
		Fixed by &lt;denchmark-link:https://github.com/JuliaGPU/CUDA.jl/issues/90&gt;JuliaGPU/CUDA.jl#90&lt;/denchmark-link&gt;
. Make sure Julia 1.5 is used, as well as  instead of .
		</comment>
	</comments>
</bug>