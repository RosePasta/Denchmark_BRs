<bug id='9' author='davidjamespritchard' open_date='2019-09-16T18:35:32Z' closed_time='2019-09-18T12:01:53Z'>
	<summary>Error when fitting the model: ValueError: need at least one array to concatenate</summary>
	<description>
Hi.
When fitting the model (StackedDenseNet) using the following code:
model.fit(batch_size=3,
validation_batch_size=10,
callbacks=callbacks,
epochs=1000,
n_workers=8,
steps_per_epoch=500)
I get the error message below. I have tried to attached the annotated data set, but it won't let me so I'll email it. Everything else seems to work.
Cheers
David
&lt;denchmark-h:h2&gt;Epoch 1/1000
500/500 [==============================] - 1194s 2s/step - loss: 22.9040 - output_0_loss: 11.7045 - output_1_loss: 11.1995&lt;/denchmark-h&gt;

ValueError                                Traceback (most recent call last)
 in ()
4           epochs=1000,
5           n_workers=8,
----&gt; 6           steps_per_epoch=500)
7 frames
/usr/local/lib/python3.6/dist-packages/deepposekit/models/engine.py in fit(self, batch_size, validation_batch_size, callbacks, epochs, use_multiprocessing, n_workers, steps_per_epoch, **kwargs)
143                                        validation_data=validation_generator,
144                                        validation_steps=len(validation_generator),
--&gt; 145                                        **kwargs)
146
147     def evaluate(self, batch_size):
/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py in wrapper(*args, **kwargs)
89                 warnings.warn('Update your ' + object_name + ' call to the ' +
90                               'Keras 2 API: ' + signature, stacklevel=2)
---&gt; 91             return func(*args, **kwargs)
92         wrapper._original_function = func
93         return wrapper
/usr/local/lib/python3.6/dist-packages/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
1656             use_multiprocessing=use_multiprocessing,
1657             shuffle=shuffle,
-&gt; 1658             initial_epoch=initial_epoch)
1659
1660     @interfaces.legacy_generator_methods_support
/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py in fit_generator(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
253                     break
254
--&gt; 255             callbacks.on_epoch_end(epoch, epoch_logs)
256             epoch += 1
257             if callbacks.model.stop_training:
/usr/local/lib/python3.6/dist-packages/keras/callbacks.py in on_epoch_end(self, epoch, logs)
150         logs = logs or {}
151         for callback in self.callbacks:
--&gt; 152             callback.on_epoch_end(epoch, logs)
153
154     def on_train_batch_begin(self, batch, logs=None):
/usr/local/lib/python3.6/dist-packages/deepposekit/callbacks.py in on_epoch_end(self, epoch, logs)
89
90     def on_epoch_end(self, epoch, logs={}):
---&gt; 91         evaluation_dict = self.evaluation_model.evaluate(self.batch_size)
92         y_pred = evaluation_dict['y_pred']
93         y_error = evaluation_dict['y_error']
/usr/local/lib/python3.6/dist-packages/deepposekit/models/engine.py in evaluate(self, batch_size)
162             keypoints.append([y_pred, y_error])
163
--&gt; 164         metrics = np.hstack(metrics)
165         keypoints = np.hstack(keypoints)
166
/usr/local/lib/python3.6/dist-packages/numpy/core/shape_base.py in hstack(tup)
338         return _nx.concatenate(arrs, 0)
339     else:
--&gt; 340         return _nx.concatenate(arrs, 1)
341
342
ValueError: need at least one array to concatenate
	</description>
	<comments>
		<comment id='1' author='davidjamespritchard' date='2019-09-16T18:39:01Z'>
		Just to add, it seems to happen at the end of the first epoch, regardless of the number of steps.
		</comment>
		<comment id='2' author='davidjamespritchard' date='2019-09-17T04:38:31Z'>
		If you make sure you have validation_split set to some value &gt;0 for the TrainingGenerator and set validation_batch_size=1 for Logger, or if you just remove Logger from your list of callbacks during training then it should work. This is caused by the validation set from TrainingGenerator being too small and/or the validation_batch_size being too big. A fix is already in the works.
		</comment>
		<comment id='3' author='davidjamespritchard' date='2019-09-18T12:01:53Z'>
		Thank you Jake.
Changing validation_split to 0.5 and validation_batch_size to 1 worked. It seems fine now.
		</comment>
		<comment id='4' author='davidjamespritchard' date='2019-09-19T15:56:14Z'>
		I'll reopen this until the bug is actually fixed
		</comment>
		<comment id='5' author='davidjamespritchard' date='2019-09-23T11:56:55Z'>
		This should now be fixed the latest version 0.1.1, but please reopen this issue if you're still having problems.
		</comment>
	</comments>
</bug>