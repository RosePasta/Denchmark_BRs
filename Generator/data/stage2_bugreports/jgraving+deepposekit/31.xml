<bug id='31' author='DenisPolygalov' open_date='2019-11-19T06:11:30Z' closed_time='2019-12-12T15:13:30Z'>
	<summary>`load_model` takes 5 minutes to run</summary>
	<description>
TR;DL
I'm seeing this problem on Windows, can someone confirm that this is "typical" time or Ubuntu/Linux is faster/slower?
&lt;denchmark-code&gt;deepposekit.models.load_model("best_model_densenet.h5")
&lt;/denchmark-code&gt;

takes 5 minutes to execute on Windows + Conda + tensorflow-gpu 2.0.0 + cudatoolkit 10.0.130 + cudnn 7.6.4. Most of the time spet in this call:
&lt;denchmark-code&gt;train_model = saving.load_model(filepath, custom_objects=custom_objects)
&lt;/denchmark-code&gt;

Which is a part of TensorFlow:
&lt;denchmark-code&gt;from tensorflow.python.keras.engine import saving
&lt;/denchmark-code&gt;

I'm only using DeepPoseKit-Data\datasets\fly\best_model_densenet.h5
model file without any modifications.
Full log of a single load_model("best_model_densenet.h5") call provided below. The "DEBUG" lines are mine, just time measurement statements.
&lt;denchmark-code&gt;2019-11-19 14:39:56.344360: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
DEBUG: load_model() IMPORTED in 7.264695 sec
2019-11-19 14:40:03.324369: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2019-11-19 14:40:03.464370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:05:00.0
2019-11-19 14:40:03.474370: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-11-19 14:40:03.494370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-11-19 14:40:03.504370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:05:00.0
2019-11-19 14:40:03.514370: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-11-19 14:40:03.534370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-11-19 14:40:05.114372: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-11-19 14:40:05.124372: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0
2019-11-19 14:40:05.134372: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N
2019-11-19 14:40:05.144372: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 wit
h 9604 MB memory) -&gt; physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:05:00.0, compute capability: 7.5)
DEBUG: saving.load_model(): CALLED in 264.017428 sec
DEBUG: model prepared in 9.466326 sec
DEBUG: load_model() CALLED in 273.486011 sec
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='DenisPolygalov' date='2019-11-19T11:06:05Z'>
		Yes, this seems to be abnormally long. Just tested on my local Ubuntu 18.04 machine this takes 32s and on a colab instance this takes 38s (both with TF 2.0). Having said that it seems the majority of the wall time is from tf.keras.models.load_model, which I have no control over. So I'll close this issue.
		</comment>
		<comment id='2' author='DenisPolygalov' date='2019-11-19T14:54:51Z'>
		I see. &lt;denchmark-link:https://github.com/jgraving&gt;@jgraving&lt;/denchmark-link&gt;
 could you please clarify that Windows support for DeepPoseKit is not of your interest so I won't spend my time publishing fixes for that and won't spend your time reviewing them? Doing Ubuntu-only project is completely OK, you is the author anyway, but according to your top README.md there is some Windows support. The problem is that these instructions are misleading and deadly outdated, so anyone who will try to use DeepPoseKit on Windows will be quite disappointing. The examples/Jupyter notebooks are full of Linux-specific stuff. And the 5 minutes loading time is just one of the problems.
It would be better if you remove all mentions of Windows from repository and state clearly "This is for Linux only" if you don't have time/intent of dealing with it.
P.S. you don't have control over  but you do have control over what is pushed into it.
		</comment>
		<comment id='3' author='DenisPolygalov' date='2019-11-19T15:40:36Z'>
		I'm not a paid software developer, and it is not my job to provide support for this or any software. Any time I spend on this project is a donation of my free time, so I don't appreciate the hostile tone of your message (whether you intended it as hostile or not). Support for Windows is limited because I don't use Windows, not because I have no interest in supporting it. There are multiple people in our group successfully using DeepPoseKit on Windows, but they have not complained of any major issues. The included notebooks are meant to be examples of how to use the code, not coverage for all use cases.
The README also says that contributions from the community are welcome, so if you have comments or suggestions for how to improve the code for Windows then please open an issue or PR to do so.
Otherwise, I'm unsure what you would like me to do in this case. The issue you opened here seems to be out of my control, as all the code used within deepposekit.models.load_model is standard to TF/Keras for saving/loading Keras models and there is no reason it should take an extraordinarily long time to run. If you've found a fix for the problem then you should open a PR. Otherwise this issue should be closed, as there is nothing further I can do at this point in time.
		</comment>
		<comment id='4' author='DenisPolygalov' date='2019-11-19T19:06:12Z'>
		Adding compile=False to saving.load_model at 


DeepPoseKit/deepposekit/models/loading.py


         Line 79
      in
      bbad8e6






 train_model = saving.load_model(filepath, custom_objects=custom_objects) 




 seems to help in load speed.
		</comment>
		<comment id='5' author='DenisPolygalov' date='2019-11-20T05:11:37Z'>
		&lt;denchmark-link:https://github.com/eduramiba&gt;@eduramiba&lt;/denchmark-link&gt;
 Thank you very much for your suggestion. I've tested it and it takes ~15 seconds now. I also tested whole pipeline (examples/step*.py scripts) and it seems like result is the same as when . My changes available in the &lt;denchmark-link:https://github.com/jgraving/DeepPoseKit/pull/30&gt;#30&lt;/denchmark-link&gt;
 Here is full log of the  call:
&lt;denchmark-code&gt;2019-11-20 09:52:59.546804: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
DEBUG: load_model() IMPORTED in 6.970643 sec
2019-11-20 09:53:06.317216: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2019-11-20 09:53:06.457616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:05:00.0
2019-11-20 09:53:06.473216: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-11-20 09:53:06.488816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-11-20 09:53:06.504417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:05:00.0
2019-11-20 09:53:06.520017: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-11-20 09:53:06.535617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-11-20 09:53:07.986419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-11-20 09:53:08.002019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0
2019-11-20 09:53:08.002019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N
2019-11-20 09:53:08.017619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 wit
h 9972 MB memory) -&gt; physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:05:00.0, compute capability: 7.5)
DEBUG: saving.load_model(): CALLED in 14.340591 sec
DEBUG: model prepared in 9.493438 sec
DEBUG: load_model() CALLED in 23.836108 sec
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='DenisPolygalov' date='2019-11-20T05:55:30Z'>
		&lt;denchmark-link:https://github.com/jgraving&gt;@jgraving&lt;/denchmark-link&gt;
 I'm sorry if my comment looks hostile for you. It is probably was my overreaction to immediate closing of this issue. To my knowledge the common practice in collaborative open source software development is to wait if anyone else has information regarding the issue (as you can see this is exactly what happened) and/or let person who opened the issue to take responsibility and close it by him/herself.
Currently I'm being paid for adapting DeepPoseKit for our needs. So I offered improvements (see my PR) and ask for the help in the area I'm currently not familiar with. The disadvantage is that we need DeepPoseKit to be used on Windows. So I have to choose - try to adapt DeepPoseKit in the way compatible with both Windows and Ubuntu, or make a hard fork. I prefer the former way so I just wanted to understand which way do you prefer.

There are multiple people in our group successfully using DeepPoseKit on Windows, but they have not complained of any major issues.

That looks wonderful if all these users update their environments regularly. What I see really often in scientific programming field is "project installed once by user A, - environment never get updated, - few month later conda/pip packages get updated at the upstream, - user B try to install the same thing, - newer packages get installed, - everything exploded due to version mismatch. In the worst case BS like Docker if used as the “solution”.

The included notebooks are meant to be examples of how to use the code, not coverage for all use cases.

The examples of how to use the code is not enough for painless multi-platform development. First thing new user need to do is to make sure that his/her new environment is correct and the code is producing expected output. For that - a testing suite (a set of plain Python scripts used for testing environment after upstream updates and/or making big changes in the local project code) is absolutely necessary.
		</comment>
		<comment id='7' author='DenisPolygalov' date='2019-11-22T14:07:27Z'>
		Sorry about that, I should have left the issue open. I did not see any possible solution, so I closed it prematurely. Obviously I was wrong. I will reopen this until an option to set compile=False is merged into the master branch.
The provided notebooks are meant to be examples, not test scripts. Obviously test scripts are essential to better support different environments, but we have not implemented them yet. Thank you for the suggestions, and I will review your PR when I'm back to work next week.
		</comment>
		<comment id='8' author='DenisPolygalov' date='2019-11-26T03:49:27Z'>
		I did some search on what compile flag actually does, maybe it will be useful for someone. The documentation says:

If an optimizer was found as part of the saved model, the model is already compiled. Otherwise, the model is uncompiled and a warning will be displayed. When compile is set to False, the compilation is omitted without any warning.
https://www.tensorflow.org/api_docs/python/tf/keras/models/load_model

The DeepPoseKit-Data/datasets/fly/annotation_data_release.h5 contain model that is already compiled. As far as I understand the compilation of the model actually occurs during/after the model training.
It seems like when executed on Linux - TensorFlow detects that the model to be loaded is already compiled and omit compilation automatically. On Windows however, the model will be compiled in any case. So this is not a bug of DeepPoseKit per se, but rather ambiguous behavior of TensorFlow.
Enforcing compile=False on Windows (as in my PR) is safe as long as the model to be loaded was already compiled in advance.
Here are also people encountering similar issue:
&lt;denchmark-link:https://stackoverflow.com/questions/53636759/keras-importing-load-model-takes-too-long&gt;https://stackoverflow.com/questions/53636759/keras-importing-load-model-takes-too-long&lt;/denchmark-link&gt;

Note that the loading method described above is not directly applicable to DeepPoseKit because:
&lt;denchmark-code&gt;    model_json = model.to_json()
AttributeError: 'StackedDenseNet' object has no attribute 'to_json'
&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='DenisPolygalov' date='2019-12-12T15:13:30Z'>
		Fixed with &lt;denchmark-link:https://github.com/jgraving/DeepPoseKit/pull/30&gt;#30&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>