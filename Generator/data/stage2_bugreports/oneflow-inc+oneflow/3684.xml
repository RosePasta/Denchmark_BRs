<bug id='3684' author='MARD1NO' open_date='2020-10-15T07:46:39Z' closed_time='2020-10-16T09:36:27Z'>
	<summary>Whether to check the batch axis?</summary>
	<description>
&lt;denchmark-h:h1&gt;问题引入&lt;/denchmark-h&gt;

当张量与张量之间进行操作，需要对 batch_axis 进行一系列检查以及推导
以 oneflow.math.add 为例子
&lt;denchmark-code&gt;@oneflow_export("math.add")
def add(
    x: Union[int, float, remote_blob_util.BlobDef],
    y: Union[int, float, remote_blob_util.BlobDef],
    name: Optional[str] = None,
) -&gt; remote_blob_util.BlobDef:
    if isinstance(x, (int, float)):
        return scalar_add(y, x, name)
    elif isinstance(y, (int, float)):
        return scalar_add(x, y, name)
    elif x.shape == y.shape and x.batch_axis == y.batch_axis:
        return element_wise_add(x, y, name)
    elif x.shape == (1,):
        return scalar_add_by_tensor(y, x, name)
    elif y.shape == (1,):
        return scalar_add_by_tensor(x, y, name)
    else:
        return broadcast_add(x, y, name)
&lt;/denchmark-code&gt;

当进入到最后一个else语句后执行 broadcast_add 会调用底层算子 


oneflow/oneflow/user/ops/math_binary_broadcast_ops.cpp


         Line 174
      in
      ff1dcbe






 .SetBatchAxisInferFn(user_op::BatchAxisInferFnUtil::NaiveInferBatchAxis) \ 





算子使用 NaiveInferBatchAxis 进行BatchAxis推导
具体问题在这句话



oneflow/oneflow/core/framework/batch_axis_context.cpp


         Line 46
      in
      4d84afb






 if (cur_in_batch_axis-&gt;has_value() == false) { continue; } 





这句话continue跳过后面的语句，导致没有检查 cur_in_batch_axis 和 batch_axis
&lt;denchmark-h:h1&gt;问题示例&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;# 假设有个 Tensor A ，其 batchaxis = None


# 另外有个 Tensor B，其 batchaxis = 0

Tensor A = oneflow.math.add(Tensor A, Tensor B) 
&lt;/denchmark-code&gt;

由于 


oneflow/oneflow/core/framework/batch_axis_context.cpp


         Line 46
      in
      4d84afb






 if (cur_in_batch_axis-&gt;has_value() == false) { continue; } 




 的存在，导致 Tensor A 被跳过了
而最终 Tensor A 的 batch_axis 来自于 Tensor B的。这一过程是不合理的
和文骁的初步讨论是删除 if (cur_in_batch_axis-&gt;has_value() == false) { continue; },不知道是否会引发其他问题。
	</description>
	<comments>
		<comment id='1' author='MARD1NO' date='2020-10-15T07:50:19Z'>
		broadcast add op 可以不用NaiveInferBatchAxis这个接口吧，自己再实现一个不check batch axis的版本就行了。
		</comment>
		<comment id='2' author='MARD1NO' date='2020-10-15T07:50:43Z'>
		你理解错我的意思 😂，不是删除
&lt;denchmark-code&gt;if (cur_in_batch_axis-&gt;has_value() == false) { continue; }
&lt;/denchmark-code&gt;

而是删除检查
&lt;denchmark-code&gt;CHECK_EQ_OR_RETURN(batch_axis-&gt;value(), cur_in_batch_axis-&gt;value());
&lt;/denchmark-code&gt;

而且在这之前还需要确定下是哪个 op infer batch axis 触发了这个问题。
		</comment>
		<comment id='3' author='MARD1NO' date='2020-10-15T07:52:47Z'>
		我的建议是不要改NaiveInferBatchAxis里的逻辑，而是再实现一个弱化版本的InferBatchAxis的函数，里面不做该CHECK。
		</comment>
		<comment id='4' author='MARD1NO' date='2020-10-15T07:53:58Z'>
		
我的建议是不要改NaiveInferBatchAxis里的逻辑，而是再实现一个弱化版本的InferBatchAxis的函数，里面不做该CHECK。

先调试看看是哪个 op 出了这个问题，再看一下这是个普遍性问题还是特例。
		</comment>
	</comments>
</bug>