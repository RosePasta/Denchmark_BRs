<bug id='652' author='hollowgalaxy' open_date='2020-01-11T03:09:52Z' closed_time='2020-01-13T00:35:33Z'>
	<summary>Experiment Groups with Distributed Training</summary>
	<description>
&lt;denchmark-h:h3&gt;Unclear documentation&lt;/denchmark-h&gt;

No documentation that touches upon how to specify resources (for distributed with horovod backend)
when doing hyperparamter optimization. Is that even possible?
&lt;denchmark-link:https://docs.polyaxon.com/concepts/experiment-groups-hyperparameters-optimization/&gt;https://docs.polyaxon.com/concepts/experiment-groups-hyperparameters-optimization/&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='hollowgalaxy' date='2020-01-12T06:03:42Z'>
		I got this. when using horovod framework and replicas from the experiment example.
Reason: ["[ErrorDetail(string="Received non valid specification config. Extra arguments passed for environment: ['replicas']", code='invalid')]"]
		</comment>
		<comment id='2' author='hollowgalaxy' date='2020-01-12T18:24:52Z'>
		Can you put a the yaml file you used for running into this issue?
		</comment>
		<comment id='3' author='hollowgalaxy' date='2020-01-12T22:12:22Z'>
		&lt;denchmark-code&gt;---
version: 1

kind: group
framework: horovod

inputs:
  - {name: batch_size, type: int, is_optional: true, default: 8}
  - {name: max_epochs, type: int, is_optional: true, default: 100}
  - {name: learning_policy, type: str, is_optional: true, default: reduce}
  - {name: min_learning_rate, type: float, is_optional: true, default: 0.00001}


environment:
  resources:
    gpu:
      requests: 1
      limits: 1
  replicas:
    n_workers: 1
    default_worker:
      resources:
        gpu:
          requests: 1
          limits: 1

hptuning:
  concurrency: 1
  bo:
    n_iterations: 15
    n_initial_trials: 2
    metric:
      name: loss
      optimization: minimize
    utility_function:
      n_warmup: 1
      n_iter: 2
      acquisition_function: ucb
      kappa: 2.576
      gaussian_process:
        kernel: matern
        length_scale: 1.0
        nu: 1.9
        n_restarts_optimizer: 0

  matrix:
    max_learning_rate:
      uniform: [0.001, 0.01]
    momentum:
      values: [0.85, 0.9, 0.95]
    weight_decay:
      uniform: [0.00001, 0.01]


build:
  dockerfile: polyaxon/Dockerfile

run:
  cmd: python polyaxon/train.py \
    --batch_size={{ batch_size }} \
    --max_epochs={{ max_epochs }} \
    --learning_policy={{ learning_policy }} \
    --max_learning_rate={{ max_learning_rate }} \
    --min_learning_rate={{ min_learning_rate }} \
    --momentum={{ momentum }} \
    --weight_decay={{ weight_decay }}
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='hollowgalaxy' date='2020-01-12T22:22:22Z'>
		I see where the error is coming from, I will attempt a fix for the 0.6 release, if the fix works, it should be available tomorrow once the release is finalized.
		</comment>
		<comment id='5' author='hollowgalaxy' date='2020-01-13T00:35:32Z'>
		This is fixed and should be part of the release. Closing the issue for now.
		</comment>
	</comments>
</bug>