<bug id='156' author='maver1ck' open_date='2018-07-30T14:41:40Z' closed_time='2018-08-03T10:24:42Z'>
	<summary>Removing helm chart should remove all tensorboards and notebook</summary>
	<description>
&lt;denchmark-h:h3&gt;Describe the bug&lt;/denchmark-h&gt;

After removing helm chart (--purge) there are still active pods from polyaxon:
&lt;denchmark-code&gt;NAME                                                              READY     STATUS        RESTARTS   AGE
plx-notebook-654ecad3d8cc46de9337ebb5fe04c7b9-fb6cdcb95-4btkr     1/1       Running       0          2d
plx-tensorboard-05158fc55d0a46e1b2ed20b6eb3e951a-5d5cbb947ljfwn   1/1       Running       0          2d
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;


create polyaxon deployment with helm
use tensorboard or jupyter
delete deployment

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

All pods will be deleted
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

Polyaxon 0.1.5
	</description>
	<comments>
		<comment id='1' author='maver1ck' date='2018-07-30T17:44:51Z'>
		I think the problem is with hooks
&lt;denchmark-code&gt;kubectl describe pod brawny-unicorn-clean-experiments-vqnsf

Events:
  Type     Reason                 Age                 From                                               Message
  ----     ------                 ----                ----                                               -------
  Normal   Scheduled              54m                 default-scheduler                                  Successfully assigned brawny-unicorn-clean-experiments-vqnsf to gke-concourse-default-pool-2cd18e9b-rmtg
  Normal   SuccessfulMountVolume  54m                 kubelet, gke-concourse-default-pool-2cd18e9b-rmtg  MountVolume.SetUp succeeded for volume "default-token-vm248"
  Normal   SandboxChanged         54m (x2 over 54m)   kubelet, gke-concourse-default-pool-2cd18e9b-rmtg  Pod sandbox changed, it will be killed and re-created.
  Warning  Failed                 52m (x11 over 54m)  kubelet, gke-concourse-default-pool-2cd18e9b-rmtg  Error: configmaps "brawny-unicorn-polyaxon-config" not found
  Normal   Pulled                 4m (x228 over 54m)  kubelet, gke-concourse-default-pool-2cd18e9b-rmtg  Container image "polyaxon/polyaxon-manage:0.1.6" already present on machine
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='maver1ck' date='2018-07-30T18:20:40Z'>
		&lt;denchmark-link:https://github.com/maver1ck&gt;@maver1ck&lt;/denchmark-link&gt;
 sorry for the late answer.
So the hook failed? I will try to do a couple more tests on the hooks. The thing is that I deactivated hooks retries because if the deployment fails, the command helm del --purge gets stuck in the hooks retrying indefinitely.
		</comment>
		<comment id='3' author='maver1ck' date='2018-07-30T18:22:53Z'>
		Yep. The workaround was to use helm del --no-hooks.
But you can see there was a lot of retries in my sample.
		</comment>
		<comment id='4' author='maver1ck' date='2018-07-30T18:34:15Z'>
		Yes that's something that I also added to the &lt;denchmark-link:https://docs.polyaxon.com/reference_polyaxon_helm/#uninstalling-the-chart&gt;delete section in the docs&lt;/denchmark-link&gt;
.
Thank you for reporting, I will look more into it.
		</comment>
		<comment id='5' author='maver1ck' date='2018-07-30T18:38:06Z'>
		Maybe .spec.activeDeadlineSeconds in job definition can be a solution ?
		</comment>
		<comment id='6' author='maver1ck' date='2018-07-30T18:40:31Z'>
		Thanks for the suggestion, I will try it and report back.
		</comment>
		<comment id='7' author='maver1ck' date='2018-08-03T10:24:41Z'>
		This was solved in the latest version 0.1.7. Please feel free to reopen.
		</comment>
	</comments>
</bug>