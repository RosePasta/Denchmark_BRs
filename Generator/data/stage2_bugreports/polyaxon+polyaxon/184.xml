<bug id='184' author='mouradmourafiq' open_date='2018-09-21T12:24:41Z' closed_time='2018-09-27T15:58:52Z'>
	<summary>Job's status is scheduled although it's running</summary>
	<description>
&lt;denchmark-h:h3&gt;Describe the bug&lt;/denchmark-h&gt;


I'm facing the issues of the experiments being stuck with the status 'Scheduled' meaning that the experiment didn't even start executing, but when I look at the metrics, I see the metrics plotted (which definitely suggests that the experiment has been executed, and even that it has finished). Actually, in the logs of the experiment I can see that the experiment has finished since it has reached the maximum number of epochs I set. This happens when I run the experiment group making the all other experiments in the group being stuck with the status 'Created' since they are waiting for this mentioned experiment to finish. Also, when I check the pods in the k8s, the pod which was running that experiment has the status 'Completed', but is never deleted (the pods are deleted when experiment finishes).

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Running an experiment group
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Experiment should reflect the current pod(s) state.
	</description>
	<comments>
		<comment id='1' author='mouradmourafiq' date='2018-09-27T15:58:51Z'>
		There was a major regression, a lot of events were dropped. A fix was made in the v0.2.3. Hopefully the platform is more stable now.
		</comment>
		<comment id='2' author='mouradmourafiq' date='2019-09-05T13:30:22Z'>
		I am experiencing a similar issue with experiments in version 0.5.6. The status says it is starting although the experiment is running.
&lt;denchmark-link:https://user-images.githubusercontent.com/54358486/64346233-0b4ad680-cff2-11e9-859e-d5c50c0dd83e.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='mouradmourafiq' date='2019-11-15T03:12:36Z'>
		We are seeing this problem too, with 0.5.6
		</comment>
		<comment id='4' author='mouradmourafiq' date='2019-11-15T09:26:38Z'>
		&lt;denchmark-link:https://github.com/stratospark&gt;@stratospark&lt;/denchmark-link&gt;
 are you still seeing errors mentioned in &lt;denchmark-link:https://github.com/polyaxon/polyaxon/issues/603&gt;#603&lt;/denchmark-link&gt;
, if yes then this is definitely can happen. One to solve it is to flush/reset the queue. The reason you might be seeing this is that the other error is making the scheduler unstable.
		</comment>
		<comment id='5' author='mouradmourafiq' date='2019-11-15T21:17:12Z'>
		&lt;denchmark-link:https://github.com/mouradmourafiq&gt;@mouradmourafiq&lt;/denchmark-link&gt;
 yes, we are still seeing those errors. Our experiments now jump from  to , without a  status in between.
Could you share the procedure of how to manually reset the queue? We are running a standard version of Polyaxon launched from the latest helm file. Rabbitmq and Redis are running in the cluster, but we aren't sure what touch to fix this. Thanks!
&lt;denchmark-link:https://user-images.githubusercontent.com/93366/68976147-fd5fc180-07a9-11ea-8cfe-ead4ed1b88bf.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='mouradmourafiq' date='2019-11-16T20:41:14Z'>
		Did not try this, if you have one scheduler replica you can try this command:
kubectl exec -it -n polyaxon  $(kubectl get pods -n polyaxon | grep schdueler | awk '{print $1}') -- python3 polyaxon/manage.py celeryctl purge
		</comment>
		<comment id='7' author='mouradmourafiq' date='2019-11-18T23:07:56Z'>
		&lt;denchmark-link:https://github.com/mouradmourafiq&gt;@mouradmourafiq&lt;/denchmark-link&gt;

I got a terminal into the scheduler pod and tried it without any luck:
&lt;denchmark-link:https://user-images.githubusercontent.com/93366/69101689-259f2880-0a15-11ea-9556-9045061e6eec.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='mouradmourafiq' date='2019-11-19T19:29:50Z'>
		&lt;denchmark-link:https://github.com/stratospark&gt;@stratospark&lt;/denchmark-link&gt;
 I will update this thread with the correct command to execute tomorrow or the day after.
		</comment>
		<comment id='9' author='mouradmourafiq' date='2020-02-06T20:03:51Z'>
		Hi, still running into this problem where it skips from scheduled to
completed state without running state in between.

It’s an issue for us because we poll polyaxon to see when experiments are
finished in order to launch the next stage. It used to be faster when, but
now it seems to wait for a timeout before switching into completed, which
adds a lot of latency to our entire multistage pipeline.
On Tue, Nov 19, 2019 at 11:29 AM Mourad ***@***.***&gt; wrote:
 &lt;denchmark-link:https://github.com/stratospark&gt;@stratospark&lt;/denchmark-link&gt;
 &lt;&lt;denchmark-link:https://github.com/stratospark&gt;https://github.com/stratospark&lt;/denchmark-link&gt;
&gt; I will update this thread
 with the correct command to execute tomorrow or the day after.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;&lt;denchmark-link:https://github.com/polyaxon/polyaxon/issues/184&gt;#184&lt;/denchmark-link&gt;
?email_source=notifications&amp;email_token=AAAWZNVYQI6FNMJXWXOQ5WDQUQ5DFA5CNFSM4FWQGIB2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEEPN5LQ#issuecomment-555671214&gt;,
 or unsubscribe
 &lt;&lt;denchmark-link:https://github.com/notifications/unsubscribe-auth/AAAWZNW6EYXQZ7QHYFVE36LQUQ5DFANCNFSM4FWQGIBQ&gt;https://github.com/notifications/unsubscribe-auth/AAAWZNW6EYXQZ7QHYFVE36LQUQ5DFANCNFSM4FWQGIBQ&lt;/denchmark-link&gt;
&gt;
 .

-- 
-pr
		</comment>
		<comment id='10' author='mouradmourafiq' date='2020-08-24T20:33:42Z'>
		We're encountering this on 0.6.0 and don't see any celeryctl purge command either
		</comment>
		<comment id='11' author='mouradmourafiq' date='2020-08-24T20:40:16Z'>
		It should be I think this instead:
&lt;denchmark-code&gt;cd /polyaxon/polyaxon
celery -A polyaxon purge 
&lt;/denchmark-code&gt;

		</comment>
		<comment id='12' author='mouradmourafiq' date='2020-08-24T20:54:36Z'>
		That command is valid, thanks!
		</comment>
		<comment id='13' author='mouradmourafiq' date='2020-08-24T21:08:00Z'>
		I will mention that this purge doesn't seem to actually do anything since the workers are still live:
&lt;denchmark-code&gt;No messages purged from 2 queues
&lt;/denchmark-code&gt;

I plan to scale down the scheduler pods to 0 then purge the queue
		</comment>
	</comments>
</bug>