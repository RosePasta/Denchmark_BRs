<bug id='296' author='NHZlX' open_date='2019-12-25T05:24:05Z' closed_time='2020-01-25T23:45:37Z'>
	<summary>There might be memory leak in instanceNormalizationPlugin</summary>
	<description>
There are two places that can cause leaks
First, every enqueue calls cudamalloc without releasing



TensorRT/plugin/instanceNormalizationPlugin/instanceNormalizationPlugin.cpp


         Line 211
      in
      572d54f






 CHECK_CUDA(cudaMalloc((void**) &amp;_d_bias, n * nchan_bytes)); 





Second,  cudnn_handle will also be created every time



TensorRT/plugin/instanceNormalizationPlugin/instanceNormalizationPlugin.cpp


         Line 217
      in
      572d54f






 CHECK_CUDNN(cudnnCreate(&amp;_cudnn_handle)); 





	</description>
	<comments>
		<comment id='1' author='NHZlX' date='2019-12-29T14:44:54Z'>
		May be related: &lt;denchmark-link:https://devtalk.nvidia.com/default/topic/1068998/memory-leak-in-tensorrt-instancenormalization/?offset=2&gt;https://devtalk.nvidia.com/default/topic/1068998/memory-leak-in-tensorrt-instancenormalization/?offset=2&lt;/denchmark-link&gt;

CC &lt;denchmark-link:https://github.com/rajeevsrao&gt;@rajeevsrao&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='NHZlX' date='2020-01-06T11:18:49Z'>
		Not sure if this is the same, but I have observed a memory leak in OnnxRuntime using TensorRT as can be seen &lt;denchmark-link:https://github.com/microsoft/onnxruntime/issues/2773&gt;microsoft/onnxruntime#2773&lt;/denchmark-link&gt;

Although this model uses BatchNormalization and not InstanceNormalization.
		</comment>
		<comment id='3' author='NHZlX' date='2020-01-25T23:45:37Z'>
		Fixed per &lt;denchmark-link:https://github.com/NVIDIA/TensorRT/pull/315&gt;#315&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>