<bug id='304' author='tiancai13579' open_date='2019-12-30T06:20:25Z' closed_time='2020-01-28T01:57:28Z'>
	<summary>tensort7 use 3d convolution get error</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

When I use 3d convolution and shuffle in tensorrt7, I get the flowing error:
&lt;denchmark-code&gt;[TensorRT] INTERNAL ERROR: Assertion failed: t-&gt;start.d[i] + t-&gt;extent.d[i] &lt;= r.dims.d[i]
../builder/cudnnBuilderGraph.cpp:862
Aborting...

[TensorRT] ERROR: ../builder/cudnnBuilderGraph.cpp (862) - Assertion Error in checkSanity: 0 (t-&gt;start.d[i] + t-&gt;extent.d[i] &lt;= r.dims.d[i])
Traceback (most recent call last):
  File "3d-test.py", line 84, in &lt;module&gt;
    context = engine.create_execution_context()
AttributeError: 'NoneType' object has no attribute 'create_execution_context'

&lt;/denchmark-code&gt;

here is my sample code
import tensorrt as trt
import numpy as np
import pycuda.driver as cuda
import pycuda.autoinit

class HostDeviceMem(object):
    def __init__(self, host_mem, device_mem):
        self.host = host_mem
        self.device = device_mem

    def __str__(self):
        return "Host:\n" + str(self.host) + "\nDevice:\n" + str(self.device)

    def __repr__(self):
        return self.__str__()

def allocate_buffers(engine):
    inputs = []
    outputs = []
    bindings = []
    stream = cuda.Stream()
    for binding in engine:
        size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size
        dtype = trt.nptype(engine.get_binding_dtype(binding))
        # Allocate host and device buffers
        host_mem = cuda.pagelocked_empty(size, dtype)
        device_mem = cuda.mem_alloc(host_mem.nbytes)
        # Append the device buffer to device bindings.
        bindings.append(int(device_mem))
        # Append to the appropriate list.
        if engine.binding_is_input(binding):
            inputs.append(HostDeviceMem(host_mem, device_mem))
        else:
            outputs.append(HostDeviceMem(host_mem, device_mem))
    return inputs, outputs, bindings, stream

def do_inference(context, bindings, inputs, outputs, stream, batch_size=1):
    # Transfer input data to the GPU.
    [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]
    # Run inference.
    context.execute_async(batch_size=batch_size, bindings=bindings, stream_handle=stream.handle)
    # Transfer predictions back from the GPU.
    [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]
    # Synchronize the stream
    stream.synchronize()
    # Return only the host outputs.
    return [out.host for out in outputs]

if __name__ == '__main__':
    logger = trt.Logger(trt.Logger.WARNING)
    builder = trt.Builder(logger)
    network = builder.create_network()
    builder.max_batch_size = 1
    builder.max_workspace_size = 1 &lt;&lt; 30

    data = network.add_input("data", trt.DataType.FLOAT, (1, 9, 32, 32))

    weight0 = np.ones(shape=(6, 1, 3, 3, 3), dtype=np.float32).reshape(-1)
    bias0 = np.zeros(shape=(6), dtype=np.float32).reshape(-1)

    layer0 = network.add_convolution_nd(data, 6, trt.Dims([3, 3, 3]), weight0, bias0)

    layer0 = network.add_shuffle(layer0.get_output(0))
    layer0.reshape_dims = trt.Dims([1, 42, 30, 30])


    weight1 = np.ones(shape=(6, 1, 3, 3, 3), dtype=np.float32).reshape(-1)
    bias1 = np.zeros(shape=(6), dtype=np.float32).reshape(-1)

    layer1 = network.add_convolution_nd(data, 6, trt.Dims([3, 3, 3]), weight1, bias1)
    layer1 = network.add_scale_nd(layer1.get_output(0),trt.ScaleMode.CHANNEL,shift=None,scale=None,power=None,channel_axis=1)

    layer1 = network.add_shuffle(layer1.get_output(0))
    layer1.reshape_dims = trt.Dims([1, 42, 30, 30])


    add_layer = network.add_elementwise(layer0.get_output(0), layer1.get_output(0),
                                        trt.ElementWiseOperation.SUM)

    output = add_layer.get_output(0)
    network.mark_output(output)

    engine = builder.build_cuda_engine(network)
    context = engine.create_execution_context()

    inputs, outputs, bindings, stream = allocate_buffers(engine)

    data = np.ones((1, 1, 9, 32, 32), dtype=np.float32)

    inputs[0].host = data
    trt_outputs = do_inference(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)
    output = trt_outputs[0]

    print("output\n{}".format(output.shape))
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;

TensorRT Version: 7.0.0.11
GPU Type: gtx1080
Nvidia Driver Version: 418.88
CUDA Version: 9.0.176
CUDNN Version: 7.6.4
Operating System + Version: Ubuntu 18.04.2 LTS
Python Version (if applicable): 3.6.7
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):
&lt;denchmark-h:h2&gt;Relevant Files&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;Steps To Reproduce&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='tiancai13579' date='2019-12-30T06:23:44Z'>
		if I block shuffle layer, it will work:
import tensorrt as trt
import numpy as np
import pycuda.driver as cuda
import pycuda.autoinit

class HostDeviceMem(object):
    def __init__(self, host_mem, device_mem):
        self.host = host_mem
        self.device = device_mem

    def __str__(self):
        return "Host:\n" + str(self.host) + "\nDevice:\n" + str(self.device)

    def __repr__(self):
        return self.__str__()

def allocate_buffers(engine):
    inputs = []
    outputs = []
    bindings = []
    stream = cuda.Stream()
    for binding in engine:
        size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size
        dtype = trt.nptype(engine.get_binding_dtype(binding))
        # Allocate host and device buffers
        host_mem = cuda.pagelocked_empty(size, dtype)
        device_mem = cuda.mem_alloc(host_mem.nbytes)
        # Append the device buffer to device bindings.
        bindings.append(int(device_mem))
        # Append to the appropriate list.
        if engine.binding_is_input(binding):
            inputs.append(HostDeviceMem(host_mem, device_mem))
        else:
            outputs.append(HostDeviceMem(host_mem, device_mem))
    return inputs, outputs, bindings, stream

def do_inference(context, bindings, inputs, outputs, stream, batch_size=1):
    # Transfer input data to the GPU.
    [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]
    # Run inference.
    context.execute_async(batch_size=batch_size, bindings=bindings, stream_handle=stream.handle)
    # Transfer predictions back from the GPU.
    [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]
    # Synchronize the stream
    stream.synchronize()
    # Return only the host outputs.
    return [out.host for out in outputs]

if __name__ == '__main__':
    logger = trt.Logger(trt.Logger.WARNING)
    builder = trt.Builder(logger)
    network = builder.create_network()
    builder.max_batch_size = 1
    builder.max_workspace_size = 1 &lt;&lt; 30

    data = network.add_input("data", trt.DataType.FLOAT, (1, 9, 32, 32))

    weight0 = np.ones(shape=(6, 1, 3, 3, 3), dtype=np.float32).reshape(-1)
    bias0 = np.zeros(shape=(6), dtype=np.float32).reshape(-1)

    layer0 = network.add_convolution_nd(data, 6, trt.Dims([3, 3, 3]), weight0, bias0)

    # layer0 = network.add_shuffle(layer0.get_output(0))
    # layer0.reshape_dims = trt.Dims([1, 42, 30, 30])


    weight1 = np.ones(shape=(6, 1, 3, 3, 3), dtype=np.float32).reshape(-1)
    bias1 = np.zeros(shape=(6), dtype=np.float32).reshape(-1)

    layer1 = network.add_convolution_nd(data, 6, trt.Dims([3, 3, 3]), weight1, bias1)
    layer1 = network.add_scale_nd(layer1.get_output(0),trt.ScaleMode.CHANNEL,shift=None,scale=None,power=None,channel_axis=1)

    # layer1 = network.add_shuffle(layer1.get_output(0))
    # layer1.reshape_dims = trt.Dims([1, 42, 30, 30])


    add_layer = network.add_elementwise(layer0.get_output(0), layer1.get_output(0),
                                        trt.ElementWiseOperation.SUM)

    output = add_layer.get_output(0)
    network.mark_output(output)

    engine = builder.build_cuda_engine(network)
    context = engine.create_execution_context()

    inputs, outputs, bindings, stream = allocate_buffers(engine)

    data = np.ones((1, 1, 9, 32, 32), dtype=np.float32)

    inputs[0].host = data
    trt_outputs = do_inference(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)
    output = trt_outputs[0]

    print("output\n{}".format(output.shape))
in terminal it will print:
&lt;denchmark-code&gt;[TensorRT] WARNING: Setting layouts of network and plugin input/output tensors to linear, as 3D operators are found and 3D non-linear IO formats are not supported, yet.
[TensorRT] WARNING: Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles
output
(37800,)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='tiancai13579' date='2020-01-28T01:57:28Z'>
		Hi &lt;denchmark-link:https://github.com/tiancai13579&gt;@tiancai13579&lt;/denchmark-link&gt;
,
This has been fixed and will be in the next release, thanks for your patience.
		</comment>
	</comments>
</bug>