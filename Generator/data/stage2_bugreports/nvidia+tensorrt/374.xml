<bug id='374' author='RizhaoCai' open_date='2020-02-10T11:42:45Z' closed_time='2021-01-12T14:13:31Z'>
	<summary>Jetson TX2, TensorRT,  Cudnn Error in execute: 8 (CUDNN_STATUS_EXECUTION_FAILED)</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

I am using TX2+TensorRT to deploy an object detection model (RFB-NET). When I upgraded the Jetpack to v4.3 for having TensroRT 6.0 and ran the experiment, I got such the error when executing
success_flag = context.execute(batch_size=batch_size, bindings=bindings) or context.execute_async(batch_size=batch_size, bindings=bindings, stream_handle=stream.handle)
Both of this two failed and the error would be below reported.

[TensorRT] ERROR: ../rtSafe/cuda/cudaConvolutionRunner.cpp (303) - Cudnn Error in execute: 7 (CUDNN_STATUS_MAPPING_ERROR)
[TensorRT] ERROR: FAILED_EXECUTION: std::exception

Everytime the context of the engine got the data of batch_size 1. Although the program can still move on for the next inference, the results were inaccurate. What is more, every time the error reported is slightly different: Cudnn Error in execute: 7 or 8
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;

Jetson TX2
Jetpack: 4.3
TensorRT Version:  6.0.1.10
GPU Type: Jetson TX2
Nvidia Driver Version:
CUDA Version: 10.0
CUDNN Version: 7.6.3
Operating System + Version: ubuntu 18.04
Python Version (if applicable): 3.6.9
PyTorch Version (if applicable): 1.1.0a0+b457266
&lt;denchmark-h:h2&gt;Relevant Files&lt;/denchmark-h&gt;

&lt;denchmark-link:https://drive.google.com/file/d/1NY0VM85dHwsYRDcCR0CGgeFOb9qKAGYj/view?usp=sharing&gt;ONNX model (opset7)&lt;/denchmark-link&gt;

&lt;denchmark-link:https://drive.google.com/open?id=1O0CaV6WVksemm7wqY07HZhvoE1alXJxG&gt;reproduce_error.py&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;Steps To Reproduce&lt;/denchmark-h&gt;

I really try out all the possible solutions, including 1) change work_space_size; 2) toggle torch.backends.cudnn.enabled=False/True; 3) Try different onnx opset versions;
but nothing works. Please help.
	</description>
	<comments>
		<comment id='1' author='RizhaoCai' date='2020-02-11T00:49:12Z'>
		Hi &lt;denchmark-link:https://github.com/RizhaoCai&gt;@RizhaoCai&lt;/denchmark-link&gt;

Are you doing inference on multiple threads? If so, are you using a separate execution context for each thread?
Are you sure your bindings are correct / valid pointers?
&lt;denchmark-link:https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#perform_inference_python&gt;https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#perform_inference_python&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='RizhaoCai' date='2020-02-11T03:09:53Z'>
		
Hi @RizhaoCai
Are you doing inference on multiple threads? If so, are you using a separate execution context for each thread?
Are you sure your bindings are correct / valid pointers?
https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#perform_inference_python

Thank you.
I think I just ran one thread, and I am sure I have the correct bindings
I am writing a script to reproduce the error. But very strangely, when I wrote the code that I thought would reproduce the error, the program can run successfully (using the same engine). So weird.
I am still working on writing a script to reproduce the error. It will be soon.
		</comment>
		<comment id='3' author='RizhaoCai' date='2020-02-11T03:52:17Z'>
		
Hi @RizhaoCai
Are you doing inference on multiple threads? If so, are you using a separate execution context for each thread?
Are you sure your bindings are correct / valid pointers?
https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#perform_inference_python

Hi &lt;denchmark-link:https://github.com/rmccorm4&gt;@rmccorm4&lt;/denchmark-link&gt;
,
I have found out where is the problem and how to figure it out.
However, I think it desires further discussion because it is really weird.
For reproducing the error, I have supplemented the code and ONNX model to the issue description. Please check &lt;denchmark-link:https://github.com/NVIDIA/TensorRT/issues/374#issue-562506131&gt;there&lt;/denchmark-link&gt;

For reproducing the error, just  and you will find the error will occur.
&lt;denchmark-link:https://user-images.githubusercontent.com/37952703/74209757-1225f580-4cc4-11ea-9e6d-207bf12d2427.png&gt;&lt;/denchmark-link&gt;

Let look at line 114 and 115 at the file reproduce_error.py:
 engine = get_engine(batch_size, onnx_file_path,engine_file_path, fp16_mode=False,int8_mode=False) just_for_producing_error_by_calling_cuda = torch.rand((3,600,260)).cuda()
If swap these two lines like:

And run  again, the error gone.
&lt;denchmark-link:https://user-images.githubusercontent.com/37952703/74209914-b019c000-4cc4-11ea-9fc8-e7735e13be5a.png&gt;&lt;/denchmark-link&gt;

It is really strange, isn't it?
Although I can figure out the problem by calling torch.tensor.cuda() first and then generating the engine, I really want to find out the reason behind such error.
Any discussion and help will be appreciated.
		</comment>
		<comment id='4' author='RizhaoCai' date='2020-10-27T20:55:02Z'>
		&lt;denchmark-link:https://github.com/RizhaoCai&gt;@RizhaoCai&lt;/denchmark-link&gt;
 thank for sharing your observation. Do you know if this behavior is specific to Jetson (aarch64) or have you been able to reproduce this on x86 platforms as well? Thanks.
		</comment>
		<comment id='5' author='RizhaoCai' date='2021-01-12T14:13:31Z'>
		I will close this since no response for more than 3 weeks, please reopen if you still have question, thanks!
		</comment>
	</comments>
</bug>