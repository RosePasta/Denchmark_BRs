<bug id='150' author='tedgoddard' open_date='2019-10-08T18:03:15Z' closed_time='2019-11-12T06:03:48Z'>
	<summary>Crash in BERT demo with second inference</summary>
	<description>
The first run of inference() succeeds, but the second crashes.  This is from invocation without the -q argument. The TensorRT-6.0.1 release runs repeated inferences without crashing.
&lt;denchmark-code&gt;Running Inference...
CONTEXT &lt;tensorrt.tensorrt.IExecutionContext object at 0x7f752cbe5650&gt; [&lt;pycuda._driver.DeviceAllocation object at 0x7f752cc633a0&gt;, &lt;pycuda._driver.DeviceAllocation object at 0x7f752cc633f0&gt;, &lt;pycuda._driver.DeviceAllocation object at 0x7f752cc63440&gt;] &lt;pycuda._driver.DeviceAllocation object at 0x7f752cc63530&gt; 1841740176
------------------------
Running inference in 268.401 Sentences/Sec
------------------------
Processing output 0 in batch
Answer: 'a high performance deep learning inference platform'
With probability: 63.411
Question (to exit, type one of ['exit', 'quit']): What is TensorRT?

Running Inference...
CONTEXT &lt;tensorrt.tensorrt.IExecutionContext object at 0x7f752cbe5650&gt; [&lt;pycuda._driver.DeviceAllocation object at 0x7f752cc633a0&gt;, &lt;pycuda._driver.DeviceAllocation object at 0x7f752cc633f0&gt;, &lt;pycuda._driver.DeviceAllocation object at 0x7f752cc63440&gt;] &lt;pycuda._driver.DeviceAllocation object at 0x7f752cc63530&gt; 1841740176
[TensorRT] ERROR: engine.cpp (165) - Cuda Error in ~ExecutionContext: 700 (an illegal memory access was encountered)
[TensorRT] ERROR: INTERNAL_ERROR: std::exception
[TensorRT] ERROR: Parameter check failed at: ../rtSafe/safeContext.cpp::terminateCommonContext::165, condition: cudaEventDestroy(context.start) failure.
[TensorRT] ERROR: Parameter check failed at: ../rtSafe/safeContext.cpp::terminateCommonContext::170, condition: cudaEventDestroy(context.stop) failure.
[TensorRT] ERROR: ../rtSafe/safeRuntime.cpp (32) - Cuda Error in free: 700 (an illegal memory access was encountered)
terminate called after throwing an instance of 'nvinfer1::CudaError'
  what():  std::exception
Aborted (core dumped)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='tedgoddard' date='2019-10-30T02:59:51Z'>
		Hi &lt;denchmark-link:https://github.com/tedgoddard&gt;@tedgoddard&lt;/denchmark-link&gt;
,

The TensorRT-6.0.1 release runs repeated inferences without crashing.

Sorry if I misunderstood, but are you saying this error happens in an earlier TensorRT version but works fine for TensorRT 6.0.1?

Can you share more info about the environment that produces this issue?

Provide details on the platforms you are using:
Linux distro and version
GPU type
Nvidia driver version
CUDA version
CUDNN version
Python version [if using python]
Tensorflow version
TensorRT version
If Jetson, OS + hardware versions

Can you also share the exact commands you ran to reproduce this issue?

		</comment>
		<comment id='2' author='tedgoddard' date='2019-11-12T06:03:48Z'>
		Closing since no response, but feel free to open a new issue or ask to re-open this one.
		</comment>
		<comment id='3' author='tedgoddard' date='2021-01-06T02:48:49Z'>
		&lt;denchmark-link:https://github.com/tedgoddard&gt;@tedgoddard&lt;/denchmark-link&gt;
   got the same problem, maybe you get two output in engine, how do you fix it?
		</comment>
	</comments>
</bug>