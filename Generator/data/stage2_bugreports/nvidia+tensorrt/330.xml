<bug id='330' author='xonobo' open_date='2020-01-13T08:57:55Z' closed_time='2020-01-13T21:54:10Z'>
	<summary>onnx model problem</summary>
	<description>
&lt;denchmark-link:https://devtalk.nvidia.com/default/topic/1067980/tensorrt/onnx-model-problem-cudaactivationrunner-cpp-96-/post/5419068/#5419068&gt;this&lt;/denchmark-link&gt;
 is the link to onnx model and the problems I face in loading to tensorrt 6. Strangely I can run it on tensorrt 5.
Since I got no response from the devtalk, I want to try here.
	</description>
	<comments>
		<comment id='1' author='xonobo' date='2020-01-13T21:54:09Z'>
		Hi &lt;denchmark-link:https://github.com/xonobo&gt;@xonobo&lt;/denchmark-link&gt;
,
Just tried your model with TensorRT 7.0 and it worked for me:
&lt;denchmark-code&gt;$ dpkg -l | grep nvinfer
...
ii  libnvinfer7    7.0.0-1+cuda10.2   amd64    TensorRT runtime libraries


$  trtexec --onnx=model0.onnx --explicitBatch
...
&amp;&amp;&amp;&amp; PASSED TensorRT.trtexec # trtexec --onnx=model0.onnx --explicitBatch
&lt;/denchmark-code&gt;

But it failed for TensorrRT 6.0:
&lt;denchmark-code&gt;$ nvidia-docker run -it -v ${PWD}:/mnt --workdir=/mnt nvcr.io/nvidia/tensorrt:19.12-py3
$ trtexec --onnx=model0.onnx --explicitBatch
...
[00/13/2020-21:46:09] [E] [TRT] (Unnamed Layer* 0) [Convolution]: at least 4 dimensions are required for input
While parsing node number 1 [BatchNormalization]:
ERROR: builtin_op_importers.cpp:695 In function importBatchNormalization:
[6] Assertion failed: scale_weights.shape == weights_shape
[00/13/2020-21:46:09] [E] Failed to parse onnx file
[00/13/2020-21:46:09] [E] Parsing model failed
[00/13/2020-21:46:09] [E] Engine could not be created
&amp;&amp;&amp;&amp; FAILED TensorRT.trtexec # trtexec --onnx=model0.onnx --explicitBatch

&lt;/denchmark-code&gt;

But if you build the OSS ONNX parser 19.12 tag: &lt;denchmark-link:https://github.com/NVIDIA/TensorRT/releases/tag/19.12&gt;https://github.com/NVIDIA/TensorRT/releases/tag/19.12&lt;/denchmark-link&gt;
) on top of TensorRT 6, it works, so this was likely found and fixed in upstream ONNX parser after the release:
&lt;denchmark-code&gt;$ nvidia-docker run -it -v ${PWD}:/mnt --workdir=/mnt nvcr.io/nvidia/tensorrt:19.12-py3
$ bash /usr/src/tensorrt/install_opensource.sh
...
Done!
Removing outdated ONNX python samples...
Done!
Copying over prerequisite files for building OSS samples to /workspace/tensorrt...
Done!
Replacing samples with open source versions and copying back python samples...
Done!
Cleaning up OSS directories...
Done!


$ trtexec --onnx=model0.onnx --explicitBatch
...
&amp;&amp;&amp;&amp; PASSED TensorRT.trtexec # trtexec --onnx=model0.onnx --explicitBatch
&lt;/denchmark-code&gt;

I believe this script (/usr/src/tensorrt/install_opensource.sh) comes with the NGC containers as of 19.12-py3, I don't think it comes with TRT6 release code.
You can copy the script from the container, or if you don't have docker installed then you can find a similar script here: &lt;denchmark-link:https://github.com/rmccorm4/tensorrt-utils/tree/master/OSS&gt;https://github.com/rmccorm4/tensorrt-utils/tree/master/OSS&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='xonobo' date='2020-02-05T15:43:05Z'>
		Hi &lt;denchmark-link:https://github.com/rmccorm4&gt;@rmccorm4&lt;/denchmark-link&gt;
 ,
While running the sampleOnnxMNIST.cpp in TensorRT-6.0.1.5/samples/sampleOnnxMNIST, I meet this problem too,

Because I am using TensorRT6.0 by downloading the NVIDIA TensorRT in &lt;denchmark-link:https://developer.nvidia.com/nvidia-tensorrt-6x-download&gt;https://developer.nvidia.com/nvidia-tensorrt-6x-download&lt;/denchmark-link&gt;
 and untar it directly, I can not know how to slove this problem by the way you given above.
Can you tell me how can I slove this problem?
		</comment>
		<comment id='3' author='xonobo' date='2020-02-05T21:00:26Z'>
		&lt;denchmark-link:https://github.com/wavesCHJ&gt;@wavesCHJ&lt;/denchmark-link&gt;
 something like this should work for building OSS components on top of TensorRT 6 on the host:
&lt;denchmark-code&gt;wget https://raw.githubusercontent.com/rmccorm4/tensorrt-utils/master/OSS/build_OSS.sh
source build_OSS.sh 19.12
trtexec --explicitBatch --onnx=model0.onnx
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;$ trtexec --explicitBatch --onnx=model0.onnx
...
&amp;&amp;&amp;&amp; PASSED TensorRT.trtexec # trtexec --explicitBatch --onnx=model0.onnx
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='xonobo' date='2020-02-06T08:31:15Z'>
		Hi &lt;denchmark-link:https://github.com/rmccorm4&gt;@rmccorm4&lt;/denchmark-link&gt;
 , thanks for your reply.
I think the build_OSS.sh script has some mistakes, such as the line 11

I think it should be

Anyway, I follow the steps in &lt;denchmark-link:https://github.com/NVIDIA/TensorRT/tree/release/6.0&gt;https://github.com/NVIDIA/TensorRT/tree/release/6.0&lt;/denchmark-link&gt;
, and when git clone the TensorRT, I change it to

Finally, I build the TensorRT successfully, and run the follow command successfully too:
&lt;denchmark-code&gt;$ ./trtexec --explicitBatch --onnx=model0.onnx  
...  
&amp;&amp;&amp;&amp; PASSED TensorRT.trtexec # ./trtexec --explicitBatch --onnx=model0.onnx  
&lt;/denchmark-code&gt;

But when I test my own onnx model, it fails again:
&lt;denchmark-code&gt;$./trtexec --explicitBatch --onnx=my_model.onnx
...
ERROR: /home/chenhj/TensorRT-6.0-rebuild/TensorRT/parsers/onnx/builtin_op_importers.cpp:689 In function importGather:
[8] Assertion failed: !(data-&gt;getType() == nvinfer1::DataType::kINT32 &amp;&amp; nbDims == 1 &amp;&amp; inputs.at(0).is_tensor()) &amp;&amp; "Cannot perform gather on a shape tensor!"
[01/06/2020-16:22:14] [E] Failed to parse onnx file
[01/06/2020-16:22:14] [E] Parsing model failed
[01/06/2020-16:22:14] [E] Engine creation failed
[01/06/2020-16:22:14] [E] Engine set up failed
&amp;&amp;&amp;&amp; FAILED TensorRT.trtexec # ./trtexec --explicitBatch --onnx=my_model.onnx
&lt;/denchmark-code&gt;

Why this happen? My model can be downloaded here: &lt;denchmark-link:https://github.com/wavesCHJ/temp/blob/master/my_model.onnx&gt;https://github.com/wavesCHJ/temp/blob/master/my_model.onnx&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='xonobo' date='2020-02-06T21:04:51Z'>
		&lt;denchmark-link:https://github.com/wavesCHJ&gt;@wavesCHJ&lt;/denchmark-link&gt;


I think the build_OSS.sh script has some mistakes, such as the line 11
git submodule update --init --recursive --progress
I think it should be
git submodule update --init --recursive

 seems to be valid for git &gt;= 2.18 per &lt;denchmark-link:https://git-scm.com/docs/git-submodule/2.18.0&gt;https://git-scm.com/docs/git-submodule/2.18.0&lt;/denchmark-link&gt;

But I just removed it anyway for users with earlier versions.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Not sure if you're willing to upgrade, but I just ran your model with TRT 7 and it works fine, so whatever the issue was in TRT 6 is likely fixed in TRT 7:
&lt;denchmark-code&gt;$ nvidia-docker run -it -v ${PWD}:/mnt --workdir=/mnt nvcr.io/nvidia/tensorrt:20.01-py3
$ trtexec --explicitBatch --onnx=my_model.onnx
...
&amp;&amp;&amp;&amp; PASSED TensorRT.trtexec # trtexec --explicitBatch --onnx=my_model.onnx
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>