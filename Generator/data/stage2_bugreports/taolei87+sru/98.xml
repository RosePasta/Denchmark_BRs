<bug id='98' author='ywatanabe1989' open_date='2019-08-04T13:33:31Z' closed_time='2020-10-19T22:52:24Z'>
	<summary>float16 handling</summary>
	<description>
When I convert my model, which using this SRU unit, into float16 enabled one, it fails.
Is this SRU not implemented to use in float16 environment, or is it hard to fix it?
	</description>
	<comments>
		<comment id='1' author='ywatanabe1989' date='2019-08-29T18:28:32Z'>
		Hi, I'm working on this PR and it should support it.
		</comment>
		<comment id='2' author='ywatanabe1989' date='2020-10-11T22:02:51Z'>
		Closing this issue since SRU supports fp16 and amp now.
		</comment>
		<comment id='3' author='ywatanabe1989' date='2020-10-19T19:24:33Z'>
		When I use Nvidia APEX for AMP, I get the following error:
&lt;denchmark-code&gt;File "/home/freddy/.virtualenvs/nuhame/lib/python3.7/site-packages/sru/modules.py", line 261, in forward
    h, c = self.apply_recurrence(U, V, residual, c0, scale_val, mask_c, mask_pad)
  File "/home/freddy/.virtualenvs/nuhame/lib/python3.7/site-packages/sru/modules.py", line 284, in apply_recurrence
    self.amp_recurrence_fp16)
  File "/home/freddy/.virtualenvs/nuhame/lib/python3.7/site-packages/sru/ops.py", line 139, in elementwise_recurrence_gpu
    mask_pad
  File "/home/freddy/.virtualenvs/nuhame/lib/python3.7/site-packages/sru/cuda_functional.py", line 83, in forward
    is_custom
RuntimeError: expected scalar type Half but found Float
&lt;/denchmark-code&gt;

I'm on SRU version 2.5.1
Is there something specific I should do to use SRU with float16?
		</comment>
		<comment id='4' author='ywatanabe1989' date='2020-10-19T19:30:32Z'>
		^ P.S. I tried with amp_recurrence_fp16=True and False (no difference)
		</comment>
		<comment id='5' author='ywatanabe1989' date='2020-10-19T21:27:00Z'>
		cc &lt;denchmark-link:https://github.com/hpasapp&gt;@hpasapp&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='ywatanabe1989' date='2020-10-19T22:48:33Z'>
		Hi &lt;denchmark-link:https://github.com/visionscaper&gt;@visionscaper&lt;/denchmark-link&gt;
 we support native pytorch AMP, we don't support Apex AMP. If you need to use APEX AMP, and are unable to migrate to pytorch native AMP, you will need to tweak the SRU code slightly, and ideally submit a PR with the changes. I'll go over what we did to make SRU work with pytorch native AMP, and then you can probably use that to create a PR to enable APEX AMP?
There are two main parts to be considered:

a large matrix multiplication, using torch.mm.
a custom recurrence kernel.

The large matrix multiplication seemed to work 'out of the box' in native AMP.
For the custom recurrence kernel, in native AMP, we did two things to get this to work inside an AMP autocast region:
1. placed the call to the recurrence kernel inside a non-autocast region, 


sru/sru/ops.py


        Lines 99 to 101
      in
      1deeca9






 in_autocast = getattr(torch, 'is_autocast_enabled', lambda: False)() 



 if in_autocast: 



 with torch.cuda.amp.autocast(enabled=False): 





2. prior to calling the recurrence kernel, cast all tensors either to all float16, or to all float32, 


sru/sru/ops.py


        Lines 102 to 110
      in
      1deeca9






 cast = torch.Tensor.half if amp_recurrence_fp16 else torch.Tensor.float 



 



 U = cast(U) 



 x = cast(x) 



 weight_c = cast(weight_c) 



 bias = cast(bias) 



 c_init = cast(c_init) 



 scale_x = cast(scale_x) if scale_x is not None else scale_x 



 dropout_mask_c = cast(dropout_mask_c) if dropout_mask_c is not None else dropout_mask_c 





As you can see, the check for autocast, and the autocast disable, are pytorch native AMP specific. You will probably need to find an APEX AMP specific to way to handle these two things?
		</comment>
		<comment id='7' author='ywatanabe1989' date='2020-10-19T22:52:24Z'>
		I'm going to close this issue since:

SRU does now support fp16 (using native PyTorch AMP), which was the original topic of this issue, and
we don't currently support APEX AMP (though PRs welcome)

		</comment>
		<comment id='8' author='ywatanabe1989' date='2020-10-20T13:21:07Z'>
		Hi &lt;denchmark-link:https://github.com/hpasapp&gt;@hpasapp&lt;/denchmark-link&gt;
, thanks for the fast response. I will first try to move to native AMP, it's the future anyway.
		</comment>
		<comment id='9' author='ywatanabe1989' date='2020-10-20T15:18:49Z'>
		&lt;denchmark-link:https://github.com/visionscaper&gt;@visionscaper&lt;/denchmark-link&gt;
 Awesome! Sounds good :)
		</comment>
		<comment id='10' author='ywatanabe1989' date='2020-10-20T16:01:01Z'>
		Hi &lt;denchmark-link:https://github.com/hpasapp&gt;@hpasapp&lt;/denchmark-link&gt;
, I updated my code to use native Pytorch AMP, and it now works like a charm!
		</comment>
		<comment id='11' author='ywatanabe1989' date='2020-10-20T17:27:20Z'>
		Great! :)
		</comment>
	</comments>
</bug>