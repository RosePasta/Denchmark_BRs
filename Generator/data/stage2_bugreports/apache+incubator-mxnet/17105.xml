<bug id='17105' author='zburning' open_date='2019-12-18T08:35:56Z' closed_time='2019-12-20T16:34:37Z'>
	<summary>Bug with fusion</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

I'm running the script at &lt;denchmark-link:https://github.com/dmlc/gluon-nlp/tree/master/scripts/language_model/run_glue.py&gt;https://github.com/dmlc/gluon-nlp/tree/master/scripts/language_model/run_glue.py&lt;/denchmark-link&gt;
. The command line to reproduce:
python run_glue.py --task MRPC --gpu 8 --batch_size 32
I have the following observation:


With model.hybridize()
With mxnet-cu100 &gt;= 1.6.0b20191102, the time cost of the first batch is more than 283s with a single GPU, more than 1000s with 4 GPUs.
With mxnet-cu100==1.6.0b20191101, the time cost of the first batch is about 11s  with a single GPU.


With model.hybridize() and  os.environ['MXNET_USE_FUSION'] = '0'
for both mxnet-cu100 == 1.6.0b201911102 and 1.6.0b20191215, the time cost of the first batch is around 11s.


Without hybridize():
The performance are nearly the same.


&lt;denchmark-h:h3&gt;Error Message&lt;/denchmark-h&gt;

(Paste the complete error message. Please also include stack trace by setting environment variable DMLC_LOG_STACK_TRACE_DEPTH=10 before running your script.)
&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

(If you developed your own code, please provide a short script that reproduces the error. For existing examples, please provide link.)
&lt;denchmark-h:h3&gt;Steps to reproduce&lt;/denchmark-h&gt;

(Paste the commands you ran that produced the error.)




&lt;denchmark-h:h2&gt;What have you tried to solve it?&lt;/denchmark-h&gt;





&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;

We recommend using our script for collecting the diagnositc information. Run the following command and paste the outputs below:
&lt;denchmark-code&gt;curl --retry 10 -s https://raw.githubusercontent.com/dmlc/gluon-nlp/master/tools/diagnose.py | python

# paste outputs here
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='zburning' date='2019-12-18T17:05:13Z'>
		Some increase in the time of the first batch is expected with fusion enabled, but that is definitely excessive. Just to confirm - by  you mean that you tested both builds form 11/02 and 12/15? There was a change &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/16783&gt;#16783&lt;/denchmark-link&gt;
 that went to master on November 12 (and backported to 1.6 on November 15) that sped up the compilation by caching the results.
Anyway, will look into it.
		</comment>
		<comment id='2' author='zburning' date='2019-12-18T18:08:36Z'>
		Ok, I can reproduce it. The first observation is that it is not actually the compilation time that takes so much time (so probably the fusion graph pass?).
I will dig further into it. At least when comparing the second epoch between fusion-enabled and disabled runs the fusion is faster (83.8s vs 87s) ;-)
		</comment>
		<comment id='3' author='zburning' date='2019-12-19T00:38:35Z'>
		I created the PR with a fix to this. Locally the graph pass takes now ~4.5 s on a single GPU, down from over 200 s (since you are not using multiple processes in that script, the time for multiple GPU would still scale linearly unfortunately, but changing that would require much bigger changes to MXNet).
		</comment>
		<comment id='4' author='zburning' date='2019-12-19T02:41:38Z'>
		Thank you for your great work!
		</comment>
	</comments>
</bug>