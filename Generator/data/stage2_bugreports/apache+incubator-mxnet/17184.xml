<bug id='17184' author='liuzh91' open_date='2019-12-27T07:38:33Z' closed_time='2019-12-27T08:36:28Z'>
	<summary>Weight tied cannot be used with weight sharing</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

We experience some weight initialization error when we use weight sharing and weight tied simultaneously. We share weights between model and model_eval. The code is shown below:
model = nlp.model.train.AWDRNN(args.model, len(vocab), args.emsize, args.nhid, args.nlayers,
                               args.tied, args.dropout, args.weight_dropout,
                               args.dropout_h, args.dropout_i, args.dropout_e)
model_eval = nlp.model.AWDRNN(args.model, len(vocab), args.emsize, args.nhid, args.nlayers,
                              args.tied, args.dropout, args.weight_dropout,
                              args.dropout_h, args.dropout_i, args.dropout_e,
                              params=model.collect_params())

model.initialize(mx.init.Xavier(), ctx=context)

model.hybridize(static_alloc=True)

print(model)

def check_initialized(net):
    params = net.collect_params()
    for param in params:
        try:
            params[param].list_ctx()
        except RuntimeError:
            return False
    return True

print(check_initialized(model))
print(check_initialized(model_eval))
&lt;denchmark-h:h3&gt;Log Message&lt;/denchmark-h&gt;

If args.tied is set True, we get the following log message:
True
False
If we turn off args.tied, the initialization works correctly.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

(If you developed your own code, please provide a short script that reproduces the error. For existing examples, please provide link.)
The file can be found in (&lt;denchmark-link:https://github.com/dmlc/gluon-nlp/blob/v0.8.x/scripts/language_model/word_language_model.py&gt;https://github.com/dmlc/gluon-nlp/blob/v0.8.x/scripts/language_model/word_language_model.py&lt;/denchmark-link&gt;
). To reproduce the above message, you may need to replace line 153 onward  with the above code snippet. Run the following command:
&lt;denchmark-code&gt;python -m pdb word_language_model.py --tied --dropout_e=0
&lt;/denchmark-code&gt;

You will encounter the above error.
&lt;denchmark-h:h2&gt;What have you tried to solve it?&lt;/denchmark-h&gt;

The parameter that not initialized properly is the parameter awdrnn0_hybridsequential0_embedding0_bias. It is the weight used in the decoder of AWDRNN.  After some investigation, we found it is the tied weights introducing this error:
    if self._tie_weights:
         output.add(nn.Dense(self._vocab_size, flatten=False,
                             params=self.embedding[0].params))
I print some debug information which may be helpful here:
&lt;denchmark-code&gt;(Pdb) model_eval.decoder[0]._params['awdrnn0_hybridsequential0_embedding0_bias'].list_ctx()
*** RuntimeError: Parameter 'awdrnn0_hybridsequential0_embedding0_bias' has not been initialized
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;

We recommend using our script for collecting the diagnositc information. Run the following command and paste the outputs below:
&lt;denchmark-code&gt;curl --retry 10 -s https://raw.githubusercontent.com/dmlc/gluon-nlp/master/tools/diagnose.py | python

# paste outputs here
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='liuzh91' date='2019-12-27T08:12:17Z'>
		According to the class AWDRNN(train.AWDRNN), tie_weights boolean value means that Whether to tie the weight matrices of output dense layer and input embedding layer. And there is no bias param in a embeddings table, right? So output dense should be defined with use_bias=False.  Does it make sense?
		</comment>
		<comment id='2' author='liuzh91' date='2019-12-27T08:25:11Z'>
		
According to the class AWDRNN(train.AWDRNN), tie_weights boolean value means that Whether to tie the weight matrices of output dense layer and input embedding layer. And there is no bias param in a embeddings table, right? So output dense should be defined with no_bias=True. Does it make sense?

Thank you for the reply. It sounds good to me.
[UPDATE] Thanks for the suggestion, it fixes the initialization bug.
		</comment>
	</comments>
</bug>