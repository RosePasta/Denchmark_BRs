<bug id='15268' author='Ishitori' open_date='2019-06-18T18:31:26Z' closed_time='2019-06-21T01:41:01Z'>
	<summary>Backward doesn't work on LSTM with sequence_length</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

LSTM with out-of-the-box variable length was introduced in &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/14208/&gt;this PR&lt;/denchmark-link&gt;
. I tried to use it, and while the forward pass works well, the backward pass fails.
I provide minimum reproducible example. To my best knowledge, the backward pass is not covered with a unit test.
&lt;denchmark-h:h2&gt;Environment info (Required)&lt;/denchmark-h&gt;

The latest version with --pre
Package used (Python/R/Scala/Julia):
Python
&lt;denchmark-h:h2&gt;Error Message:&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;MXNetError: [17:18:04] src/operator/./rnn-inl.h:1006: Check failed: in_data.size() == num_inputs (4 vs. 5) : 
Stack trace:
  [bt] (0) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x4a157b) [0x7fdedb45957b]
  [bt] (1) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x507b9ad) [0x7fdee00339ad]
  [bt] (2) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x50b5cac) [0x7fdee006dcac]
  [bt] (3) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(mxnet::imperative::PushOperator(mxnet::OpStatePtr const&amp;, nnvm::Op const*, nnvm::NodeAttrs const&amp;, mxnet::Context const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::Resource, std::allocator&lt;mxnet::Resource&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, std::vector&lt;unsigned int, std::allocator&lt;unsigned int&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;, mxnet::DispatchMode)::{lambda(mxnet::RunContext, mxnet::engine::CallbackOnComplete)#3}::operator()(mxnet::RunContext, mxnet::engine::CallbackOnComplete) const+0x396) [0x7fdedd6b3d36]
  [bt] (4) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(std::_Function_handler&lt;void (mxnet::RunContext), mxnet::imperative::PushOperator(mxnet::OpStatePtr const&amp;, nnvm::Op const*, nnvm::NodeAttrs const&amp;, mxnet::Context const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::Resource, std::allocator&lt;mxnet::Resource&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, std::vector&lt;unsigned int, std::allocator&lt;unsigned int&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;, mxnet::DispatchMode)::{lambda(mxnet::RunContext)#4}&gt;::_M_invoke(std::_Any_data const&amp;, mxnet::RunContext)+0x5d) [0x7fdedd6b43cd]
  [bt] (5) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x264c4f9) [0x7fdedd6044f9]
  [bt] (6) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2658961) [0x7fdedd610961]
  [bt] (7) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x265be70) [0x7fdedd613e70]
  [bt] (8) /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x265c106) [0x7fdedd614106]

&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Minimum reproducible example&lt;/denchmark-h&gt;

You have to use GPU to run it, as this feature is GPU only.
The devil is in the fact that the backward fails silently and mx.nd.waitall() is necessary at the end
import mxnet as mx
import numpy as np
from mxnet.gluon import nn
from mxnet.gluon.rnn import LSTM

ctx = mx.gpu(0)

label = mx.nd.array([1, 2, 3, 4, 5, 6, 7], ctx=ctx)
# random numbers, but with ones at the end as a padding symbol
x = mx.nd.array([[5434, 3232, 776, 323, 1, 1, 1], [4353, 545, 37, 23, 23, 545, 1]], ctx=ctx)

embedding = nn.Embedding(input_dim=6000,
                          output_dim=100,
                          weight_initializer=mx.initializer.Uniform(0.001))

lstm = LSTM(hidden_size=100,
            num_layers=1, dropout=0.2, bidirectional=True,
            use_sequence_length=True)

dense = nn.Dense(1)
l1 = mx.gluon.loss.L1Loss()

embedding.initialize(ctx=ctx)
lstm.initialize(ctx=ctx)
dense.initialize(ctx=ctx)


with mx.autograd.record():
    x_mask = x != 1
    x_len = mx.nd.sum(x_mask, axis=1).astype(np.int32)    
    state = lstm.begin_state(batch_size=x.shape[0], ctx=x.context)
    x_emb = embedding(x)
    x_emb = x_emb.transpose((1, 0, 2))
    a, _ = lstm(x_emb, states=state, sequence_length=x_len)
    out = dense(a)
    loss = l1(out, label)
    # this prints the loss, showing that forward pass works fine
    print(loss)

# this one will fail
loss.backward()
mx.nd.waitall()
	</description>
	<comments>
		<comment id='1' author='Ishitori' date='2019-06-18T18:31:32Z'>
		Hey, this is the MXNet Label Bot.
Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it.
Here are my recommended labels: Bug
		</comment>
		<comment id='2' author='Ishitori' date='2019-06-18T18:34:12Z'>
		&lt;denchmark-link:https://github.com/stephenrawls&gt;@stephenrawls&lt;/denchmark-link&gt;
, any help with that?
		</comment>
		<comment id='3' author='Ishitori' date='2019-06-18T20:39:58Z'>
		&lt;denchmark-link:https://github.com/mxnet-label-bot&gt;@mxnet-label-bot&lt;/denchmark-link&gt;
 add [Bug, Gluon]
		</comment>
		<comment id='4' author='Ishitori' date='2019-06-18T22:12:50Z'>
		I could reproduce this issue. Here is a full callstack.
ubuntu@ip-172-31-31-181:~$ python lstm_test.py
[1.0000387 2.0000887 3.000114  4.000115  5.000068  6.0000405 7.       ]
&lt;NDArray 7 &lt;denchmark-link:https://github.com/gpu&gt;@gpu&lt;/denchmark-link&gt;
(0)&gt;
Traceback (most recent call last):
File "lstm_test.py", line 42, in 
mx.nd.waitall()
File "/home/ubuntu/incubator-mxnet/python/mxnet/ndarray/ndarray.py", line 166, in waitall
check_call(_LIB.MXNDArrayWaitAll())
File "/home/ubuntu/incubator-mxnet/python/mxnet/base.py", line 253, in check_call
raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [22:04:02] src/operator/./rnn-inl.h:1006: Check failed: in_data.size() == num_inputs (4 vs. 5) :
Stack trace:
[bt] (0) /home/ubuntu/incubator-mxnet/python/mxnet/../../lib/libmxnet.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x32) [0x7ffa222a1082]
[bt] (1) /home/ubuntu/incubator-mxnet/python/mxnet/../../lib/libmxnet.so(mxnet::op::RNNOp&lt;mshadow::gpu, float, float&gt;::Backward(mxnet::OpContext const&amp;, std::vector&lt;mxnet::TBlob, std::allocatormxnet::TBlob &gt; const&amp;, std::vector&lt;mxnet::TBlob, std::allocatormxnet::TBlob &gt; const&amp;, std::vector&lt;mxnet::TBlob, std::allocatormxnet::TBlob &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocatormxnet::OpReqType &gt; const&amp;, std::vector&lt;mxnet::TBlob, std::allocatormxnet::TBlob &gt; const&amp;)+0x1dc) [0x7ffa26c335bc]
[bt] (2) /home/ubuntu/incubator-mxnet/python/mxnet/../../lib/libmxnet.so(void mxnet::op::RNNStatefulGradComputemshadow::gpu(mxnet::OpStatePtr const&amp;, mxnet::OpContext const&amp;, std::vector&lt;mxnet::TBlob, std::allocatormxnet::TBlob &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocatormxnet::OpReqType &gt; const&amp;, std::vector&lt;mxnet::TBlob, std::allocatormxnet::TBlob &gt; const&amp;)+0x21b6) [0x7ffa26c68186]
[bt] (3) /home/ubuntu/incubator-mxnet/python/mxnet/../../lib/libmxnet.so(mxnet::imperative::PushOperator(mxnet::OpStatePtr const&amp;, nnvm::Op const*, nnvm::NodeAttrs const&amp;, mxnet::Context const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocatormxnet::engine::Var* &gt; const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocatormxnet::engine::Var* &gt; const&amp;, std::vector&lt;mxnet::Resource, std::allocatormxnet::Resource &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocatormxnet::NDArray* &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocatormxnet::NDArray* &gt; const&amp;, std::vector&lt;unsigned int, std::allocator &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocatormxnet::OpReqType &gt; const&amp;, mxnet::DispatchMode)::{lambda(mxnet::RunContext, mxnet::engine::CallbackOnComplete)&lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/3&gt;#3&lt;/denchmark-link&gt;
}::operator()(mxnet::RunContext, mxnet::engine::CallbackOnComplete) const+0x1333) [0x7ffa24680473]
[bt] (4) /home/ubuntu/incubator-mxnet/python/mxnet/../../lib/libmxnet.so(std::_Function_handler&lt;void (mxnet::RunContext), mxnet::imperative::PushOperator(mxnet::OpStatePtr const&amp;, nnvm::Op const*, nnvm::NodeAttrs const&amp;, mxnet::Context const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocatormxnet::engine::Var* &gt; const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocatormxnet::engine::Var* &gt; const&amp;, std::vector&lt;mxnet::Resource, std::allocatormxnet::Resource &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocatormxnet::NDArray* &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocatormxnet::NDArray* &gt; const&amp;, std::vector&lt;unsigned int, std::allocator &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocatormxnet::OpReqType &gt; const&amp;, mxnet::DispatchMode)::{lambda(mxnet::RunContext)&lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/4&gt;#4&lt;/denchmark-link&gt;
}&gt;::_M_invoke(std::_Any_data const&amp;, mxnet::RunContext&amp;&amp;)+0x1d) [0x7ffa2468173d]
[bt] (5) /home/ubuntu/incubator-mxnet/python/mxnet/../../lib/libmxnet.so(std::_Function_handler&lt;void (mxnet::RunContext, mxnet::engine::CallbackOnComplete), mxnet::engine::ThreadedEngine::BulkFlush()::{lambda(mxnet::RunContext, mxnet::engine::CallbackOnComplete)&lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/1&gt;#1&lt;/denchmark-link&gt;
}&gt;::_M_invoke(std::_Any_data const&amp;, mxnet::RunContext&amp;&amp;, mxnet::engine::CallbackOnComplete&amp;&amp;)+0x1ec) [0x7ffa24e3f3dc]
[bt] (6) /home/ubuntu/incubator-mxnet/python/mxnet/../../lib/libmxnet.so(mxnet::engine::ThreadedEngine::ExecuteOprBlock(mxnet::RunContext, mxnet::engine::OprBlock*)+0x945) [0x7ffa24e42c35]
[bt] (7) /home/ubuntu/incubator-mxnet/python/mxnet/../../lib/libmxnet.so(void mxnet::engine::ThreadedEnginePerDevice::GPUWorker&lt;(dmlc::ConcurrentQueueType)0&gt;(mxnet::Context, bool, mxnet::engine::ThreadedEnginePerDevice::ThreadWorkerBlock&lt;(dmlc::ConcurrentQueueType)0&gt;, bool)::{lambda()&lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/4&gt;#4&lt;/denchmark-link&gt;
}::operator()() const::{lambda(std::shared_ptrdmlc::ManualEvent)&lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/1&gt;#1&lt;/denchmark-link&gt;
}&gt;::_M_invoke(std::_Any_data const&amp;, std::shared_ptrdmlc::ManualEvent&amp;&amp;)+0x4e) [0x7ffa24e5a8fe]
		</comment>
		<comment id='5' author='Ishitori' date='2019-06-18T22:49:43Z'>
		Looking at this now.
		</comment>
		<comment id='6' author='Ishitori' date='2019-06-19T05:49:29Z'>
		I think I have a solution, at least I have tested locally and it appears to work.
Just need to update unit tests and then I will file a PR.
		</comment>
		<comment id='7' author='Ishitori' date='2019-06-19T06:00:06Z'>
		Thanks &lt;denchmark-link:https://github.com/Ishitori&gt;@Ishitori&lt;/denchmark-link&gt;
  for the catch!
Hi &lt;denchmark-link:https://github.com/stephenrawls&gt;@stephenrawls&lt;/denchmark-link&gt;
, could you tag &lt;denchmark-link:https://github.com/szha&gt;@szha&lt;/denchmark-link&gt;
 and me in your PR? We would like to include your fix in MXNet 1.5.0 release. This feature is already included in &lt;denchmark-link:https://github.com/apache/incubator-mxnet/releases/tag/1.5.0.rc1&gt;1.5.0.rc1&lt;/denchmark-link&gt;

Thanks!
		</comment>
		<comment id='8' author='Ishitori' date='2019-06-20T20:06:52Z'>
		I verified that the issue is fixed in the latest code.
&lt;denchmark-link:https://github.com/lanking520&gt;@lanking520&lt;/denchmark-link&gt;
  I would recommend closing this issue.
		</comment>
	</comments>
</bug>