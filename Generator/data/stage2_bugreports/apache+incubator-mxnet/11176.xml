<bug id='11176' author='ThomasDelteil' open_date='2018-06-06T17:46:46Z' closed_time='2018-07-24T23:57:46Z'>
	<summary>Failed to find any forward convolution algorithm.</summary>
	<description>
See this test failing: &lt;denchmark-link:http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/incubator-mxnet/detail/master/915/pipeline/&gt;http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/incubator-mxnet/detail/master/915/pipeline/&lt;/denchmark-link&gt;

with
&lt;denchmark-code&gt; src/operator/nn/./cudnn/cudnn_convolution-inl.h:744: Failed to find any forward convolution algorithm.
&lt;/denchmark-code&gt;

I have encountered this in the wild very rarely too.
	</description>
	<comments>
		<comment id='1' author='ThomasDelteil' date='2018-06-06T18:07:39Z'>
		&lt;denchmark-link:https://github.com/DickJC123&gt;@DickJC123&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='ThomasDelteil' date='2018-06-07T17:25:02Z'>
		Hi &lt;denchmark-link:https://github.com/nswamy&gt;@nswamy&lt;/denchmark-link&gt;
 can you addd this one with 'CI' label
		</comment>
		<comment id='3' author='ThomasDelteil' date='2018-06-08T01:04:25Z'>
		This is not CI related
		</comment>
		<comment id='4' author='ThomasDelteil' date='2018-06-08T23:11:35Z'>
		Is it possible that memory is exhausted on CI?
		</comment>
		<comment id='5' author='ThomasDelteil' date='2018-06-19T15:36:59Z'>
		Also encountered this error on Windows, CUDA 9.2, cudnn 7.1.4.
Ran the following command python train_imagenet.py --benchmark 1 --gpus 0 --network inception-v3 --batch-size 64 --image-shape 3,299,299 --num-epochs 1 --kv-store device
Reducing the batch size to 16 resolved the issue.
		</comment>
		<comment id='6' author='ThomasDelteil' date='2018-07-07T00:04:19Z'>
		Currently facing this issue. Reducing batch size does not seem to fix the issue.
Trying to train &lt;denchmark-link:https://github.com/SineYuan/mxnet-fast-neural-style/tree/605da92ba3ece0107eff64f3b7e1c13f28c6f0dd&gt;fast neural style transfer&lt;/denchmark-link&gt;

Issues seems to arise when trying to do mod.save_params() throws the following error:
&lt;denchmark-code&gt;mxnet.base.MXNetError: [23:59:34] src/operator/nn/./cudnn/cudnn_convolution-inl.h:744: Failed to find any forward convolution algorithm.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='ThomasDelteil' date='2018-07-09T18:17:58Z'>
		Update: I've managed to find a rather bizarre workaround to this issue.
I was facing this issue when I was trying to do a model.save_checkpoint(). However, if I caught the exception and saved it in the except block, it seemed to work flawlessly
    try:
        mod.save_checkpoint(model_save_path, epoch)
    except Exception as excep:
        print("Exception caught: ", excep)
        mod.save_checkpoint(model_save_path, epoch)
		</comment>
		<comment id='8' author='ThomasDelteil' date='2018-07-10T18:03:39Z'>
		Update: Sleeping for 0.5 seconds before saving the checkpoint also seems to help.
    time.sleep(0.5)
    mod.save_checkpoint(model_save_path, epoch)
		</comment>
		<comment id='9' author='ThomasDelteil' date='2018-07-20T22:03:32Z'>
		&lt;denchmark-link:https://github.com/ThomasDelteil&gt;@ThomasDelteil&lt;/denchmark-link&gt;
 Is this still occurring on CI now? If it's not appearing again would you mind closing this?
&lt;denchmark-link:https://github.com/aluo-x&gt;@aluo-x&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/codewithsk&gt;@codewithsk&lt;/denchmark-link&gt;
 Usually this is due to lack of GPU memories, reducing the consumption of GPU memories such as reducing batch sizes and using a smaller model would help. If you experience more issues on this, please create a separate issue with a title like "GPU memory overflow on xxx model with yyy batch size and zzz dataset". Meanwhile I'll look for ways to improve this error message to indicate the actual root cause of this error.
		</comment>
	</comments>
</bug>