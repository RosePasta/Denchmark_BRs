<bug id='1903' author='mli' open_date='2016-04-20T01:25:11Z' closed_time='2017-09-30T19:06:31Z'>
	<summary>core dump when terminated quickly</summary>
	<description>
when shutdown the system too quickly, we may get a core dump. we can reduce it with high probability by put the following simple codes into a file test.py and then python test.py multiple times
import mxnet as mx
data = mx.sym.Variable('data')
conv = mx.sym.Convolution(data=data, kernel=(3,3), num_filter=10)
print conv.infer_shape(data=(10,2,8,8))
terminal info
terminate called after throwing an instance of 'std::system_error'
  what():  Operation not permitted
Aborted (core dumped)
core dump info
&lt;denchmark-code&gt;Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".
Core was generated by `python test.py'.
Program terminated with signal SIGABRT, Aborted.
#0  0x00007fda69f55cc9 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
56  ../nptl/sysdeps/unix/sysv/linux/raise.c: No such file or directory.
(gdb) bt
#0  0x00007fda69f55cc9 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
#1  0x00007fda69f590d8 in __GI_abort () at abort.c:89
#2  0x00007fda5072e535 in __gnu_cxx::__verbose_terminate_handler() () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#3  0x00007fda5072c6d6 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#4  0x00007fda5072c703 in std::terminate() () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#5  0x00007fda5077faf5 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00007fda6a2ec182 in start_thread (arg=0x7fda3d1a7700) at pthread_create.c:312
#7  0x00007fda6a01947d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111
(gdb) info threads
  Id   Target Id         Frame
  19   Thread 0x7fda3e9aa700 (LWP 3734) clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:81
  18   Thread 0x7fda6a709740 (LWP 3719) 0x00007fda6a2ed66b in pthread_join (threadid=140575329920768, thread_return=0x0) at pthread_join.c:92
  17   Thread 0x7fda45cb9700 (LWP 3732) 0x00007fda69fea3f7 in sched_yield () at ../sysdeps/unix/syscall-template.S:81
  16   Thread 0x7fda3e1a9700 (LWP 3735) clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:81
  15   Thread 0x7fda504be700 (LWP 3727) 0x00007fda69fea3f7 in sched_yield () at ../sysdeps/unix/syscall-template.S:81
  14   Thread 0x7fda4fcbd700 (LWP 3728) 0x00007fda69fea3f7 in sched_yield () at ../sysdeps/unix/syscall-template.S:81
  13   Thread 0x7fda454b8700 (LWP 3733) 0x00007fda69fea3f7 in sched_yield () at ../sysdeps/unix/syscall-template.S:81
  12   Thread 0x7fda3d9a8700 (LWP 3736) clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:81
  11   Thread 0x7fda484ba700 (LWP 3731) 0x00007fda69fea3f7 in sched_yield () at ../sysdeps/unix/syscall-template.S:81
  10   Thread 0x7fda4ccbb700 (LWP 3730) 0x00007fda50dae027 in ?? () from /usr/lib/libopenblas.so.0
  9    Thread 0x7fda5988c700 (LWP 3726) pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  8    Thread 0x7fda4f4bc700 (LWP 3729) 0x00007fda69fea3f7 in sched_yield () at ../sysdeps/unix/syscall-template.S:81
  7    Thread 0x7fda6408d700 (LWP 3725) pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  6    Thread 0x7fda6508f700 (LWP 3723) pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  5    Thread 0x7fda65890700 (LWP 3722) pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  4    Thread 0x7fda66091700 (LWP 3721) pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  3    Thread 0x7fda6488e700 (LWP 3724) pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  2    Thread 0x7fda66892700 (LWP 3720) pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
* 1    Thread 0x7fda3d1a7700 (LWP 3737) 0x00007fda69f55cc9 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
&lt;/denchmark-code&gt;

this output is often because a thread accessed something that has bee destroied, e.g. a thread is blocked by a mutex, while that mutex has been destroied. we might solve this problem by adding thread.join() for each thread, and properly arrange the destruction order.
	</description>
	<comments>
		<comment id='1' author='mli' date='2016-04-20T03:17:36Z'>
		It tends to be easier for me to reproduce this kinds of crashes on a Mac OS X.
		</comment>
		<comment id='2' author='mli' date='2016-04-20T08:23:35Z'>
		I think we should make structures wait for all the operations it has pushed to the engine to complete before destructing.
		</comment>
		<comment id='3' author='mli' date='2016-04-20T09:02:31Z'>
		I'm not sure if it's the same problem, but I experienced something similar when i force stop a running script:
2 of 4 K80's are going into an ERR state: "GPU has fallen off the bus" that only can be solved by cold reboot of the machine.
Could this be related?
		</comment>
		<comment id='4' author='mli' date='2016-04-20T15:55:50Z'>
		Most of these errors are due to destruction order of singletons during systems shutdown, so it was not likely that it will affect a real training.

The problem in shutdown time is tricky, many of them related to destruction order of CUDA runtime singleton and mxnet library. We already tried to catch most of them. But there is no way we can control destruction order of singletons in other libraries, so things can happen due to this reason.
Most structures @piiswrong mentioned already uses lazy delete or copy by value in lambda for lazy destructing.

The current way is simply to ignore the error, as if there is ever a synchronization step that print out things or write data to file, it must have already been executed and the effect taken.
		</comment>
		<comment id='5' author='mli' date='2017-06-21T19:58:19Z'>
		PR fix for this:  &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/6773&gt;#6773&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>