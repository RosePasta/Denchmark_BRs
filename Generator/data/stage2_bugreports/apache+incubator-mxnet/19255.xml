<bug id='19255' author='leezu' open_date='2020-09-29T23:06:29Z' closed_time='2020-11-02T17:00:24Z'>
	<summary>@with_seed reports test failure for skipped tests</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

For skipped tests, we will see the output ../tests/python/unittest/test_gluon.py::test_hybrid_static_memory[True-False] [WARNING] Error seen with seeded test, use MXNET_TEST_SEED=458439587 to reproduce. implying an error was seen, even though the skipping of the test was intentional.
This is due to the very broad exception handling in



incubator-mxnet/tests/python/unittest/common.py


        Lines 225 to 235
      in
      7dcfedc






 try: 



 orig_test(*args, **kwargs) 



 except: 



 # With exceptions, repeat test_msg at WARNING level to be sure it's seen. 



 if log_level &lt; logging.WARNING: 



 logger.warning(on_err_test_msg) 



 raise 



 finally: 



 # Provide test-isolation for any test having this decorator 



 mx.nd.waitall() 



 np.random.set_state(post_test_state) 





as skipping a test in pytest will raise the Skipped BaseException, which the with_seed decorator interprets as an error
&lt;denchmark-link:https://github.com/pytest-dev/pytest/blob/303030c14130a5777bdaace678b9f4adb07416ab/src/_pytest/outcomes.py#L51-L63&gt;https://github.com/pytest-dev/pytest/blob/303030c14130a5777bdaace678b9f4adb07416ab/src/_pytest/outcomes.py#L51-L63&lt;/denchmark-link&gt;

Full output
&lt;denchmark-code&gt;% python3 -m pytest -s --verbose ../tests/python/unittest/test_gluon.py::test_hybrid_static_memory                               ~/src/mxnet-master/build testscleanup ip-172-31-95-96
================================================================================= test session starts =================================================================================
platform linux -- Python 3.8.5, pytest-6.0.2, py-1.9.0, pluggy-0.13.1 -- /home/ubuntu/.pyenv/versions/3.8.5/bin/python3
cachedir: .pytest_cache
rootdir: /home/ubuntu/src/mxnet-master, configfile: pytest.ini
plugins: timeout-1.4.2, env-0.6.2
timeout: 1200.0s
timeout method: signal
timeout func_only: False
collected 4 items

../tests/python/unittest/test_gluon.py::test_hybrid_static_memory[False-False] Setting module np/mx/python random seeds, use MXNET_MODULE_SEED=1639191096 to reproduce.
[23:00:40] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
PASSED
../tests/python/unittest/test_gluon.py::test_hybrid_static_memory[False-True] PASSED
../tests/python/unittest/test_gluon.py::test_hybrid_static_memory[True-False] [WARNING] Error seen with seeded test, use MXNET_TEST_SEED=458439587 to reproduce.
SKIPPED
../tests/python/unittest/test_gluon.py::test_hybrid_static_memory[True-True] PASSED

============================================================================ 3 passed, 1 skipped in 1.56s =============================================================================

&lt;/denchmark-code&gt;

One approach to solve the issue, is to remove the  decorator from  and just rely on the pytest seed marker. (This requires further investigation for cases where  isn't applied to test functions, but applied to inline functions inside test functions; &lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/97d4ba5a133f93ff6075dcde3ef842b23d498a12/tests/nightly/test_large_vector.py#L137-L157&gt;for example in the test large tensor files&lt;/denchmark-link&gt;
)



incubator-mxnet/conftest.py


        Lines 133 to 162
      in
      7dcfedc






 @pytest.fixture(scope='function', autouse=True) 



 def function_scope_seed(request): 



 """A function scope fixture that manages rng seeds. 



  



     This fixture automatically initializes the python, numpy and mxnet random 



     number generators randomly on every test run. 



  



     def test_ok_with_random_data(): 



         ... 



  



     To fix the seed used for a test case mark the test function with the 



     desired seed: 



  



     @pytest.mark.seed(1) 



     def test_not_ok_with_random_data(): 



         '''This testcase actually works.''' 



         assert 17 == random.randint(0, 100) 



  



     When a test fails, the fixture outputs the seed used. The user can then set 



     the environment variable MXNET_TEST_SEED to the value reported, then rerun 



     the test with: 



  



         pytest --verbose -s &lt;test_module_name.py&gt; -k &lt;failing_test&gt; 



  



     To run a test repeatedly, install pytest-repeat and add the --count argument: 



  



         pip install pytest-repeat 



         pytest --verbose -s &lt;test_module_name.py&gt; -k &lt;failing_test&gt; --count 1000 



  



     """ 





	</description>
	<comments>
		<comment id='1' author='leezu' date='2020-10-06T19:39:53Z'>
		It seems like for the issue reported, it would be simple to test for and ignore the 'Skipped' exception.  Do you have a problem with that as an easy incremental fix?
Also, with my brief exposure to pytest-repeat, I experienced a non-linear runtime expansion with the use of --count, making useful values like 1000 unworkable.  Have you seen that?
		</comment>
		<comment id='2' author='leezu' date='2020-10-06T22:42:37Z'>
		
It seems like for the issue reported, it would be simple to test for and ignore the 'Skipped' exception. Do you have a problem with that as an easy incremental fix?

Yes, it's fine to add special handling for the Skipped exception.
As we also have the "issue" of executing the seeding stage twice for every test function (once via the pytest seed fixture and another time via the @with_seed decorator), another solution (with more effort) would be removing @with_seed and using only the pytest fixture (which was originally derived from the @with_seed decorator for the GluonNLP project).

Also, with my brief exposure to pytest-repeat, I experienced a non-linear runtime expansion with the use of --count, making useful values like 1000 unworkable. Have you seen that?

I haven't seen that but must admit that I haven't played much with pytest-repeat. The implementation is relatively short though &lt;denchmark-link:https://github.com/pytest-dev/pytest-repeat/blob/9b71e7ffb335b15765bc315220b0950dafcc86ec/pytest_repeat.py#L51-L70&gt;https://github.com/pytest-dev/pytest-repeat/blob/9b71e7ffb335b15765bc315220b0950dafcc86ec/pytest_repeat.py#L51-L70&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='leezu' date='2020-10-12T18:10:20Z'>
		&lt;denchmark-link:https://github.com/DickJC123&gt;@DickJC123&lt;/denchmark-link&gt;
 does the following reproduce the non-linearruntime expansion with  you are concerned about?
If below looks good to you, please take a look at &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/19336&gt;#19336&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;% python3 -m pytest ../tests/python/unittest/test_deferred_compute.py::test_indexing_empty_shape --count 100
================================================================== test session starts ==================================================================
platform linux -- Python 3.8.5, pytest-6.0.2, py-1.9.0, pluggy-0.13.1
rootdir: /home/ubuntu/src/mxnet-master, configfile: pytest.ini
plugins: timeout-1.4.2, repeat-0.8.0, env-0.6.2
timeout: 1200.0s
timeout method: signal
timeout func_only: False
collected 100 items

../tests/python/unittest/test_deferred_compute.py ............................................................................................... [ 95%]
.....                                                                                                                                             [100%]

================================================================== 100 passed in 3.11s ==================================================================
% python3 -m pytest ../tests/python/unittest/test_deferred_compute.py::test_indexing_empty_shape --count 1000
================================================================== test session starts ==================================================================
platform linux -- Python 3.8.5, pytest-6.0.2, py-1.9.0, pluggy-0.13.1
rootdir: /home/ubuntu/src/mxnet-master, configfile: pytest.ini
plugins: timeout-1.4.2, repeat-0.8.0, env-0.6.2
timeout: 1200.0s
timeout method: signal
timeout func_only: False
collected 1000 items

../tests/python/unittest/test_deferred_compute.py ............................................................................................... [  9%]
................................................................................................................................................. [ 24%]
................................................................................................................................................. [ 38%]
................................................................................................................................................. [ 53%]
................................................................................................................................................. [ 67%]
................................................................................................................................................. [ 82%]
................................................................................................................................................. [ 96%]
...................................                                                                                                               [100%]

================================================================= 1000 passed in 36.78s =================================================================
% python3 -m pytest ../tests/python/unittest/test_deferred_compute.py::test_indexing_empty_shape                              38s ~/src/mxnet-master/build 2020-10/pytest-with-seed-fixture ip-172-31-95-96
=========================================================================================== test session starts ============================================================================================
platform linux -- Python 3.8.5, pytest-6.0.2, py-1.9.0, pluggy-0.13.1
rootdir: /home/ubuntu/src/mxnet-master, configfile: pytest.ini
plugins: timeout-1.4.2, repeat-0.8.0, env-0.6.2
timeout: 1200.0s
timeout method: signal
timeout func_only: False
collected 1 item

../tests/python/unittest/test_deferred_compute.py .                                                                                                                                                  [100%]

============================================================================================ 1 passed in 0.08s =============================================================================================
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='leezu' date='2020-10-12T18:39:31Z'>
		Back when we used nosetests, we could stress-test a unittest with a command like:
&lt;denchmark-code&gt;MXNET_TEST_COUNT=100 nosetests --verbose -s --logging-level=DEBUG &lt;my_test&gt;
&lt;/denchmark-code&gt;

The purpose of the DEBUG logging level was you could get some output with each test and hence get a sense that it wasn't hung, and also how long the entire run would take.  Also the seed was announced in advance of running the test so that if the failure one was troubleshooting was a segfault, then the seed is not lost.  Do these capabilities exist with the pytest-repeat approach?
A possibly useful pytest equivalent arg might be --log-cli-level=DEBUG.
		</comment>
		<comment id='5' author='leezu' date='2020-10-12T18:44:18Z'>
		The implementation is based on the with_seed function, thus things work alike. In particular, what you describe works. Please see:
&lt;denchmark-code&gt;% python3 -m pytest ../tests/python/unittest/test_deferred_compute.py::test_indexing_empty_shape --count 3 --log-cli-level=DEBUG
================================================================================= test session starts =================================================================================
platform linux -- Python 3.8.5, pytest-6.0.2, py-1.9.0, pluggy-0.13.1
rootdir: /home/ubuntu/src/mxnet-master, configfile: pytest.ini
plugins: timeout-1.4.2, repeat-0.8.0, env-0.6.2
timeout: 1200.0s
timeout method: signal
timeout func_only: False
collected 3 items

../tests/python/unittest/test_deferred_compute.py::test_indexing_empty_shape[1-3]
----------------------------------------------------------------------------------- live log setup ------------------------------------------------------------------------------------
DEBUG    root:conftest.py:213 Setting np/mx/python random seeds to 1513391544. Use MXNET_TEST_SEED=1513391544 to reproduce.
------------------------------------------------------------------------------------ live log call ------------------------------------------------------------------------------------
INFO     root:util.py:85 NumPy-shape semantics has been activated in your code. This is required for creating and manipulating scalar and zero-size tensors, which were not supported in MXNet before, as in the official NumPy library. Please DO NOT manually deactivate this semantics while using `mxnet.numpy` and `mxnet.numpy_extension` modules.
INFO     root:util.py:755 NumPy array semantics has been activated in your code. This allows you to use operators from MXNet NumPy and NumPy Extension modules as well as MXNet NumPy `ndarray`s.
PASSED                                                                                                                                                                          [ 33%]
../tests/python/unittest/test_deferred_compute.py::test_indexing_empty_shape[2-3]
----------------------------------------------------------------------------------- live log setup ------------------------------------------------------------------------------------
DEBUG    root:conftest.py:213 Setting np/mx/python random seeds to 662599169. Use MXNET_TEST_SEED=662599169 to reproduce.
PASSED                                                                                                                                                                          [ 66%]
../tests/python/unittest/test_deferred_compute.py::test_indexing_empty_shape[3-3]
----------------------------------------------------------------------------------- live log setup ------------------------------------------------------------------------------------
DEBUG    root:conftest.py:213 Setting np/mx/python random seeds to 1899500804. Use MXNET_TEST_SEED=1899500804 to reproduce.
PASSED                                                                                                                                                                          [100%]

================================================================================== 3 passed in 0.15s ==================================================================================
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='leezu' date='2020-11-02T17:00:24Z'>
		As of &lt;denchmark-link:https://github.com/apache/incubator-mxnet/commit/3f436fb1fe1a839f6f5d981e3ec626cd840045df&gt;3f436fb&lt;/denchmark-link&gt;
 @with_seed is only used in ndarray large tensor nighlty tests: &lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/3f436fb1fe1a839f6f5d981e3ec626cd840045df/tests/nightly/common.py&gt;https://github.com/apache/incubator-mxnet/blob/3f436fb1fe1a839f6f5d981e3ec626cd840045df/tests/nightly/common.py&lt;/denchmark-link&gt;
 and I think we can thus close this issues
		</comment>
	</comments>
</bug>