<bug id='9171' author='kalpitdixit' open_date='2017-12-21T23:32:39Z' closed_time='2018-08-14T19:44:42Z'>
	<summary>MXNet: Using FusedRNNCell with its "bidirectional" flag turned True, can lead to hanging of training run.</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

MXNet
Using FusedRNNCell with its "bidirectional" flag turned True, can lead to hanging (i.e. infinite pause without progress/error/crash) of training run.
&lt;denchmark-h:h2&gt;Details&lt;/denchmark-h&gt;

I am running a single training run of a Sequence-to-Sequence model using the BucketingModule. Iam using an Encoder-Decoder network. I am using a FusedRNNCell with its "bidirectional" flag turned on for the Encoder and an unfused RNNCell for the Decoder.
GPU utilization is 15000MB / 16000MB. CPU utilization is 95%.
For each batch during training, I do a forward() pass and a backward() pass. After a 5-15 epochs, the training run gets stuck in the forward() pass of one of the mini-batches. The forward pass does not complete. No errors are thrown nor does anything crash. GPU/CPU utilization remains identically the same.
I have tried an ablation of many-many things in my training run (architecture, data, code etc). The conclusion is that specifically using the FusedRNNCell with the "bidirectional" flag turned True causes this problem.
&lt;denchmark-h:h2&gt;Package used&lt;/denchmark-h&gt;

Python
&lt;denchmark-h:h2&gt;Environment info&lt;/denchmark-h&gt;

----------Python Info----------
Version      : 3.5.2
Compiler     : GCC 5.4.0 20160609
Build        : ('default', 'Nov 23 2017 16:37:01')
Arch         : ('64bit', 'ELF')
------------Pip Info-----------
Version      : 9.0.1
Directory    : /usr/local/lib/python3.5/dist-packages/pip
----------MXNet Info-----------
Version      : 1.0.0
Directory    : /usr/local/lib/python3.5/dist-packages/mxnet
Commit Hash   : &lt;denchmark-link:https://github.com/apache/incubator-mxnet/commit/25720d0e3c29232a37e2650f3ba3a2454f9367bb&gt;25720d0&lt;/denchmark-link&gt;

----------System Info----------
Platform     : Linux-4.4.0-1039-aws-x86_64-with-Ubuntu-16.04-xenial
system       : Linux
node         : ip-172-31-85-194
release      : 4.4.0-1039-aws
version      : &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/48&gt;#48&lt;/denchmark-link&gt;
-Ubuntu SMP Wed Oct 11 15:15:01 UTC 2017
----------Hardware Info----------
machine      : x86_64
processor    : x86_64
Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                64
On-line CPU(s) list:   0-63
Thread(s) per core:    2
Core(s) per socket:    16
Socket(s):             2
NUMA node(s):          2
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 79
Model name:            Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz
Stepping:              1
CPU MHz:               1200.582
CPU max MHz:           3000.0000
CPU min MHz:           1200.0000
BogoMIPS:              4600.09
Hypervisor vendor:     Xen
Virtualization type:   full
L1d cache:             32K
L1i cache:             32K
L2 cache:              256K
L3 cache:              46080K
NUMA node0 CPU(s):     0-15,32-47
NUMA node1 CPU(s):     16-31,48-63
Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq monitor est ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch fsgsbase bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx xsaveopt ida
----------Network Test----------
Setting timeout: 10
Timing for Conda: &lt;denchmark-link:https://repo.continuum.io/pkgs/free/&gt;https://repo.continuum.io/pkgs/free/&lt;/denchmark-link&gt;
, DNS: 0.0300 sec, LOAD: 0.0514 sec.
Timing for Gluon Tutorial(en): &lt;denchmark-link:http://gluon.mxnet.io&gt;http://gluon.mxnet.io&lt;/denchmark-link&gt;
, DNS: 0.1141 sec, LOAD: 0.1956 sec.
Timing for MXNet: &lt;denchmark-link:https://github.com/apache/incubator-mxnet&gt;https://github.com/apache/incubator-mxnet&lt;/denchmark-link&gt;
, DNS: 0.0016 sec, LOAD: 0.4062 sec.
Timing for Gluon Tutorial(cn): &lt;denchmark-link:https://zh.gluon.ai&gt;https://zh.gluon.ai&lt;/denchmark-link&gt;
, DNS: 0.1799 sec, LOAD: 0.3847 sec.
Timing for PYPI: &lt;denchmark-link:https://pypi.python.org/pypi/pip&gt;https://pypi.python.org/pypi/pip&lt;/denchmark-link&gt;
, DNS: 0.0046 sec, LOAD: 0.0126 sec.
Timing for FashionMNIST: &lt;denchmark-link:https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/fashion-mnist/train-labels-idx1-ubyte.gz&gt;https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/fashion-mnist/train-labels-idx1-ubyte.gz&lt;/denchmark-link&gt;
, DNS: 0.0154 sec, LOAD: 0.1567 sec.
&lt;denchmark-code&gt;&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='kalpitdixit' date='2017-12-21T23:35:37Z'>
		I am using:
MXNet==1.0.0
CUDA==9.0
cuDNN==7.0
As I understand, the FusedRNNCell is faster than the unused RNNCell because it makes direct function calls to a cuda kernel. It seems that the "bidirectional" flag in FusedRNNCell is directly passed to the cuda kernel call. This is just fyi. This might imply some cuda kernel issue but I am not a cuda expert.
&lt;denchmark-link:https://github.com/eric-haibin-lin&gt;@eric-haibin-lin&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='kalpitdixit' date='2017-12-21T23:43:37Z'>
		&lt;denchmark-h:h2&gt;Want (but leads to hanging)&lt;/denchmark-h&gt;

cell = FusedRNNCell(.... bidirectional=True....)
&lt;denchmark-h:h2&gt;best workaround (but still slow)&lt;/denchmark-h&gt;

l_cell = FusedRNNCell(.... bidirectional=False....)
r_cell = FusedRNNCell(.... bidirectional=False....)
cell = BidirectionalCell(l_cell, r_cell....)
All other workarounds are 3x-10x slower than what we ideally "Want" to use above. This workaround is "only" 2x slower.
		</comment>
		<comment id='3' author='kalpitdixit' date='2017-12-21T23:53:23Z'>
		What's the patch version for cudnn? Would you confirm if the hanging still happens with the latest cuda 9.0.176/9.1.x and cudnn 7.0.5? Also, what's the GPU?
		</comment>
		<comment id='4' author='kalpitdixit' date='2017-12-21T23:54:15Z'>
		Could you provide runnable code snippet that reproduces the hanging problem? You can use random input if data is not related to the hanging problem.
		</comment>
		<comment id='5' author='kalpitdixit' date='2017-12-27T20:18:51Z'>
		&lt;denchmark-link:https://github.com/szha&gt;@szha&lt;/denchmark-link&gt;

I tested CUDA 9.0.176 with cuDNN 7.0.3  and with cuDNN 7.0.5.
In both cases, the hanging problem happens.
		</comment>
		<comment id='6' author='kalpitdixit' date='2018-03-20T16:09:35Z'>
		Proposed labels: Bug, Python, RNN
		</comment>
		<comment id='7' author='kalpitdixit' date='2018-03-20T18:49:32Z'>
		&lt;denchmark-link:https://github.com/kalpitdixit&gt;@kalpitdixit&lt;/denchmark-link&gt;
 does the problem still happen?
		</comment>
		<comment id='8' author='kalpitdixit' date='2018-04-06T22:22:27Z'>
		Hi &lt;denchmark-link:https://github.com/DickJC123&gt;@DickJC123&lt;/denchmark-link&gt;
 this is the issue I mentioned with fused RNN with bidirectional=True.
&lt;denchmark-link:https://github.com/sxjscience&gt;@sxjscience&lt;/denchmark-link&gt;
 this is also related to the error you're seeing?
		</comment>
		<comment id='9' author='kalpitdixit' date='2018-04-06T22:41:05Z'>
		Looks similar. I'm trying to give a MWE.
		</comment>
		<comment id='10' author='kalpitdixit' date='2018-04-07T01:17:03Z'>
		The error message:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "sentiment_analysis.py", line 270, in &lt;module&gt;
    train(args)
  File "sentiment_analysis.py", line 261, in train
    test_avg_L, test_acc = evaluate(net, test_dataloader, context)
  File "sentiment_analysis.py", line 136, in evaluate
    total_L += L.sum().asscalar()
  File "/home/ubuntu/mxnet/python/mxnet/ndarray/ndarray.py", line 1844, in asscalar
    return self.asnumpy()[0]
  File "/home/ubuntu/mxnet/python/mxnet/ndarray/ndarray.py", line 1826, in asnumpy
    ctypes.c_size_t(data.size)))
  File "/home/ubuntu/mxnet/python/mxnet/base.py", line 149, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [22:08:57] src/operator/./cudnn_rnn-inl.h:457: Check failed: e == CUDNN_STATUS_SUCCESS (8 vs. 0) cuDNN: CUDNN_STATUS_EXECUTION_FAILED

Stack trace returned 10 entries:
[bt] (0) /home/ubuntu/mxnet/python/mxnet/../../lib/libmxnet.so(dmlc::StackTrace[abi:cxx11]()+0x5b) [0x7f4cee092c5b]
[bt] (1) /home/ubuntu/mxnet/python/mxnet/../../lib/libmxnet.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x28) [0x7f4cee093798]
[bt] (2) /home/ubuntu/mxnet/python/mxnet/../../lib/libmxnet.so(mxnet::op::CuDNNRNNOp&lt;float&gt;::Init(mshadow::Stream&lt;mshadow::gpu&gt;*, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;)+0x2142) [0x7f4cf27b4f22]
[bt] (3) /home/ubuntu/mxnet/python/mxnet/../../lib/libmxnet.so(mxnet::op::CuDNNRNNOp&lt;float&gt;::Forward(mxnet::OpContext const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;)+0xa5d) [0x7f4cf27c2f1d]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='11' author='kalpitdixit' date='2018-08-07T23:55:02Z'>
		&lt;denchmark-link:https://github.com/kalpitdixit&gt;@kalpitdixit&lt;/denchmark-link&gt;
 Could you provide a script with which this issue occurs?
		</comment>
		<comment id='12' author='kalpitdixit' date='2018-08-14T19:44:42Z'>
		&lt;denchmark-link:https://github.com/vandanavk&gt;@vandanavk&lt;/denchmark-link&gt;

Re-ran my code on the latest version of MXNet. This issues does not happen any longer.
		</comment>
	</comments>
</bug>