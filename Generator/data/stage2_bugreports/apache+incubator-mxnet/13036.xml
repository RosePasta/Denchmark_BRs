<bug id='13036' author='zheng-da' open_date='2018-10-30T05:54:12Z' closed_time='2019-06-06T22:51:04Z'>
	<summary>fix operators to support large arrays.</summary>
	<description>
We're working on a model that requires very large NDArrays. For example, we want to create an NDArray as follows:
arr = mx.nd.random.normal(shape=(50000000, 100))
The current implementation doesn't fail with an error, but it doesn't generate a matrix correctly (it only fills the rows at the beginning).
mx.nd.zeros also fails.
It's unclear what operators support and which operators don't.
	</description>
	<comments>
		<comment id='1' author='zheng-da' date='2018-10-30T15:34:59Z'>
		&lt;denchmark-link:https://github.com/mxnet-label-bot&gt;@mxnet-label-bot&lt;/denchmark-link&gt;
 [Bug, Operator]
		</comment>
		<comment id='2' author='zheng-da' date='2018-10-31T08:40:45Z'>
		The type of iterator is int in MXNet kernel.
So it will fail when the number of iteration is greater than 2^31 - 1, (namely 2147483647).
50000000 * 100 &gt; 2^31 - 1
Please see the code:
CPU:
&lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/src/operator/mxnet_op.h#L506&gt;https://github.com/apache/incubator-mxnet/blob/master/src/operator/mxnet_op.h#L506&lt;/denchmark-link&gt;

GPU:
&lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/src/operator/mxnet_op.h#L627&gt;https://github.com/apache/incubator-mxnet/blob/master/src/operator/mxnet_op.h#L627&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='zheng-da' date='2018-10-31T15:42:19Z'>
		Will take a look
		</comment>
		<comment id='4' author='zheng-da' date='2018-10-31T21:24:16Z'>
		&lt;denchmark-link:https://github.com/wkcn&gt;@wkcn&lt;/denchmark-link&gt;
 is right. We need to chang int to index_t. I am busy with other tasks now and can only come to this in one week. Let me know if it requires an immediate fix.
		</comment>
		<comment id='5' author='zheng-da' date='2018-10-31T21:24:33Z'>
		JIRA task created: &lt;denchmark-link:https://issues.apache.org/jira/browse/MXNET-1185&gt;https://issues.apache.org/jira/browse/MXNET-1185&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='zheng-da' date='2018-10-31T23:17:38Z'>
		&lt;denchmark-link:https://github.com/apeforest&gt;@apeforest&lt;/denchmark-link&gt;

will it drop the performance down If changing the type of iteration to int64_t?
In PyTorch, the type of iteration is a template type.
		</comment>
		<comment id='7' author='zheng-da' date='2018-10-31T23:28:57Z'>
		i'm fixing some of the operators. but we need a systematic fix. The problem is everywhere. i'll provide a temp fix for some of the operators.
		</comment>
		<comment id='8' author='zheng-da' date='2018-10-31T23:30:27Z'>
		&lt;denchmark-link:https://github.com/wkcn&gt;@wkcn&lt;/denchmark-link&gt;
 in cpu, it shouldn't be a problem. I heard concerns on GPUs. Potentially, we can use int64_t for CPU and int for GPU.
		</comment>
		<comment id='9' author='zheng-da' date='2018-11-01T03:08:31Z'>
		&lt;denchmark-link:https://github.com/zheng-da&gt;@zheng-da&lt;/denchmark-link&gt;

Maybe we can try to use  for small for-loop, and  for large for-loop.
		</comment>
		<comment id='10' author='zheng-da' date='2018-11-01T20:37:34Z'>
		&lt;denchmark-link:https://github.com/wkcn&gt;@wkcn&lt;/denchmark-link&gt;
 My concern is that this modification makes the code complex. As for using different int types for CPU and GPU, it's relatively easier. We can use the template argument to easily achieve it.
		</comment>
		<comment id='11' author='zheng-da' date='2018-11-01T20:38:21Z'>
		&lt;denchmark-link:https://github.com/pengzhao-intel&gt;@pengzhao-intel&lt;/denchmark-link&gt;
 what is the performance difference between int32 and int64 in intel CPUs?
		</comment>
		<comment id='12' author='zheng-da' date='2018-11-01T20:53:20Z'>
		&lt;denchmark-link:https://github.com/apeforest&gt;@apeforest&lt;/denchmark-link&gt;
 I have fixed some of the operators, including all random generators, zeros, ones, full, arange, gather_nd.
&lt;denchmark-link:https://github.com/zheng-da/incubator-mxnet/commit/2c3d9a3a491d33497c2b37897e73796a0c28e19d&gt;zheng-da@2c3d9a3&lt;/denchmark-link&gt;

But we need to do more to fix the rest of the operators.
		</comment>
		<comment id='13' author='zheng-da' date='2018-11-02T01:26:50Z'>
		&lt;denchmark-link:https://github.com/zheng-da&gt;@zheng-da&lt;/denchmark-link&gt;
 Maybe  is better.
		</comment>
		<comment id='14' author='zheng-da' date='2018-11-02T02:53:38Z'>
		&lt;denchmark-link:https://github.com/zheng-da&gt;@zheng-da&lt;/denchmark-link&gt;
 Do you plan to create a PR with your change? I will be glad to review. Also, I have created an epic (&lt;denchmark-link:https://issues.apache.org/jira/browse/MXNET-1184&gt;https://issues.apache.org/jira/browse/MXNET-1184&lt;/denchmark-link&gt;
) to address this support in a systematic way. Please feel free to add additional tasks to it as needed. Thanks.
		</comment>
		<comment id='15' author='zheng-da' date='2018-11-02T06:18:27Z'>
		&lt;denchmark-link:https://github.com/zheng-da&gt;@zheng-da&lt;/denchmark-link&gt;
 in general, int64 is only half of int32 performance.
		</comment>
		<comment id='16' author='zheng-da' date='2018-11-02T09:11:40Z'>
		Hi, I modified &lt;denchmark-link:https://github.com/wkcn/incubator-mxnet/blob/support_large_array/src/operator/mxnet_op.h#L41&gt;src/operator/mxnet_op.h&lt;/denchmark-link&gt;
 to support  and  as the type of iterator. It may be helpful.
And I wrote a &lt;denchmark-link:https://gist.github.com/wkcn/59f6867a217e1a4f68c83ab3d845bba5&gt;script&lt;/denchmark-link&gt;
 to replace the type of interator to .
Usage:



Install  the_silver_searcher




Input the command: ag "MSHADOW_XINLINE static void Map" &gt; map.txt




python replace_index.py



However, there was some bug in the script. :-(
		</comment>
		<comment id='17' author='zheng-da' date='2018-11-03T00:00:09Z'>
		&lt;denchmark-link:https://github.com/apeforest&gt;@apeforest&lt;/denchmark-link&gt;
 I just fixed the operators I use in my model. Could you help add test and fix other operators?
		</comment>
		<comment id='18' author='zheng-da' date='2018-11-03T14:17:30Z'>
		In my test, it seem that the performances of +/- between int32_t and int64_t are approximate.
CPU: Intel i7-7500U
OS: Arch Linux x64
Compiler: g++ 8.2.1
Compiler Flag: 
Test Code: &lt;denchmark-link:https://github.com/wkcn/c_performance&gt;https://github.com/wkcn/c_performance&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;Test int8_t
929 ms
798 ms
831 ms
2024 ms
Test int16_t
860 ms
803 ms
840 ms
1950 ms
Test int32_t
858 ms
822 ms
878 ms
1947 ms
Test int64_t
899 ms
837 ms
828 ms
7345 ms
Test float
1187 ms
1191 ms
1198 ms
1199 ms
Test double
1209 ms
1211 ms
1205 ms
1205 ms
&lt;/denchmark-code&gt;

		</comment>
		<comment id='19' author='zheng-da' date='2018-11-03T17:00:58Z'>
		integer operations are cheap. even if int64 is a little more expensive, it's hard to believe that it can affect the overall performance by much.
		</comment>
		<comment id='20' author='zheng-da' date='2018-11-04T01:41:38Z'>
		try the gemm with int 32 and int
64 and see how much peak GFLOPS it can achieve
		</comment>
		<comment id='21' author='zheng-da' date='2018-12-18T21:20:58Z'>
		I believe the following is also a repo of this issue:
import mxnet as mx
mx.nd.eye(10240 * 5) * 2
 
[[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
		</comment>
		<comment id='22' author='zheng-da' date='2019-06-06T22:51:04Z'>
		This issue has been fixed. In 1.5.0 release, user need to build MXNet from source with the compilation flag USE_INT64_TENSOR_SIZE=1. We are working to make this flag on by default and available in pip package in next minor release. Closing this issue for now.
		</comment>
	</comments>
</bug>