<bug id='10807' author='lionel92' open_date='2018-05-04T04:53:08Z' closed_time='2018-10-09T05:04:57Z'>
	<summary>Ndarray.asnumpy() error with gluon dense under both GPU and CPU  environment</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

I try to use gluon API to test mkldnn while an error occurs when doing asnumpy() operation after the network's backward propogation. So I performed the test under gpu and native cpu environment, and the error still exists. What' more, the small size of input data will not trigger the error.
You can run the following mininum example to reproduce the error.
&lt;denchmark-code&gt;import mxnet as mx
from mxnet import gluon
from mxnet.gluon import nn
from mxnet.test_utils import assert_almost_equal
from common import setup_module, with_seed
import numpy as np
import random
from nose.tools import raises

#mx.Context.default_ctx = mx.Context('gpu', 0)
mx.Context.default_ctx = mx.Context('cpu', 0)

layer = nn.Dense(1000)
x = mx.nd.random.uniform(shape=(16, 128, 300, 300))
#x = mx.nd.random.uniform(shape=(1, 2, 30, 30)) #This input is ok.
x.attach_grad()
layer.collect_params().initialize()
with mx.autograd.record():
    out = layer(x)
out.backward()
print x.grad.shape
print x.grad.asnumpy().shape
&lt;/denchmark-code&gt;

Error message:
print x.grad.asnumpy().shape
File "/home/linliu/mxnet_gpu/incubator-mxnet/python/mxnet/ndarray/ndarray.py", line 1876, in asnumpy
ctypes.c_size_t(data.size)))
File "/home/linliu/mxnet_gpu/incubator-mxnet/python/mxnet/base.py", line 149, in check_call
raise MXNetError(py_str(LIB.MXGetLastError()))
mxnet.base.MXNetError:  ([04:25:06] include/mxnet/././tensor_blob.h:257: Check failed: this-&gt;shape.Size() == shape.Size() (11520000000 vs. 2930065408) TBlob.get_with_shape: new and old shape do not match total elements
	</description>
	<comments>
		<comment id='1' author='lionel92' date='2018-05-04T09:28:06Z'>
		modified your script as this:
from mxnet.gluon import nn
import mxnet as mx

mx.Context.default_ctx = mx.Context('cpu', 0)

layer = nn.Dense(1000)
x = mx.nd.random.uniform(shape=(16, 128, 300, 300))
x.attach_grad()
layer.collect_params().initialize()
with mx.autograd.record():
    out = layer(x)
out.backward()
print(x.grad.shape)
print(x.grad)
print(x.grad.asnumpy().shape)
with mxnet-mkl 1.1.0 from pypi, I got this:
&lt;denchmark-code&gt;% python3 script.py                                                                        134 ↵
(16, 128, 300, 300)
terminate called after throwing an instance of 'std::bad_alloc'
  what():  std::bad_alloc
[1]    17413 abort      python3 script.py
&lt;/denchmark-code&gt;

with mxnet-mkl 1.2.0b20180503 from pypi, I got this:
&lt;denchmark-code&gt;% python3 script.py                                                                          1 ↵
(16, 128, 300, 300)
Traceback (most recent call last):
  File "script.py", line 14, in &lt;module&gt;
    print(x.grad)
  File "/home/david/.virtualenvs/mkl-dnn/local/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py", line 189, in __repr__
    return '\n%s\n&lt;%s %s @%s&gt;' % (str(self.asnumpy()),
  File "/home/david/.virtualenvs/mkl-dnn/local/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py", line 1876, in asnumpy
    ctypes.c_size_t(data.size)))
  File "/home/david/.virtualenvs/mkl-dnn/local/lib/python3.6/site-packages/mxnet/base.py", line 149, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [17:24:12] src/storage/./cpu_device_storage.h:73: Failed to allocate CPU Memory

Stack trace returned 10 entries:
[bt] (0) /home/david/.virtualenvs/mkl-dnn/local/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x17009d) [0x7f39de8c009d]
[bt] (1) /home/david/.virtualenvs/mkl-dnn/local/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x170468) [0x7f39de8c0468]
[bt] (2) /home/david/.virtualenvs/mkl-dnn/local/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2dc701d) [0x7f39e151701d]
[bt] (3) /home/david/.virtualenvs/mkl-dnn/local/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2dc704d) [0x7f39e151704d]
[bt] (4) /home/david/.virtualenvs/mkl-dnn/local/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2dcc77b) [0x7f39e151c77b]
[bt] (5) /home/david/.virtualenvs/mkl-dnn/local/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x29140f4) [0x7f39e10640f4]
[bt] (6) /home/david/.virtualenvs/mkl-dnn/local/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x291469f) [0x7f39e106469f]
[bt] (7) /home/david/.virtualenvs/mkl-dnn/local/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2914ab0) [0x7f39e1064ab0]
[bt] (8) /home/david/.virtualenvs/mkl-dnn/local/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2891843) [0x7f39e0fe1843]
[bt] (9) /home/david/.virtualenvs/mkl-dnn/local/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2899644) [0x7f39e0fe9644]
&lt;/denchmark-code&gt;

So, I am totally confused...
		</comment>
		<comment id='2' author='lionel92' date='2018-05-09T21:22:55Z'>
		&lt;denchmark-link:https://github.com/sandeep-krishnamurthy&gt;@sandeep-krishnamurthy&lt;/denchmark-link&gt;
  could you help to add label NDArray, MKL? Thanks
		</comment>
		<comment id='3' author='lionel92' date='2018-05-10T12:11:44Z'>
		&lt;denchmark-link:https://github.com/roywei&gt;@roywei&lt;/denchmark-link&gt;
 Thanks for follow-up, this issue happens on GPU platform as well, so MKL lable might limit the scope. May I have your double-check? Thanks. :)
		</comment>
		<comment id='4' author='lionel92' date='2018-05-25T02:06:47Z'>
		are you sure you can do it in CPU without MKLDNN?
what you are doing here is trying to allocate a weight array whose dimension is 11520000x1000, which is 85GB.
It seems to me that the code fails in initializing the weight matrix with random numbers. It's before calling MKLDNN operators.
		</comment>
		<comment id='5' author='lionel92' date='2018-05-25T02:51:25Z'>
		&lt;denchmark-link:https://github.com/zheng-da&gt;@zheng-da&lt;/denchmark-link&gt;
  As I mentioned before, all environments(GPU/CPU/MKLDNN) failed. According to your response, can I consider it as an out-of-memory error?
		</comment>
		<comment id='6' author='lionel92' date='2018-05-25T15:02:57Z'>
		i think so
		</comment>
		<comment id='7' author='lionel92' date='2018-05-29T21:16:09Z'>
		I think this is due to the use of index_t (which is uint32_t) vs int64_t in tesnorblob.
This is a legacy issue. We should use int64_t for all indexing
		</comment>
		<comment id='8' author='lionel92' date='2018-10-05T20:46:22Z'>
		Verified the PR &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/11742&gt;#11742&lt;/denchmark-link&gt;
 will fix this issue. &lt;denchmark-link:https://github.com/sandeep-krishnamurthy&gt;@sandeep-krishnamurthy&lt;/denchmark-link&gt;
 please close this. Thanks!
		</comment>
	</comments>
</bug>