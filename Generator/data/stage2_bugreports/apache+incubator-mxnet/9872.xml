<bug id='9872' author='dotelos' open_date='2018-02-23T15:04:44Z' closed_time='2018-02-26T22:43:30Z'>
	<summary>A bug in an example in the python API document</summary>
	<description>
This is an example found in the doc for &lt;denchmark-link:https://mxnet.incubator.apache.org/api/python/autograd.html#mxnet.autograd.Function&gt;mxnet.autograd.Function&lt;/denchmark-link&gt;
.
class sigmoid(Function):
    def forward(self, x):
        y = 1 / (1 + mx.nd.exp(-x))
        self.save_for_backward(y)
        return y

    def backward(self, dy):
        # backward takes as many inputs as forward's return value,
        # and returns as many NDArrays as forward's arguments.
        y, = self.saved_tensors
        return y * (1-y)
The backward method should return dy * y * (1-y) instead of y * (1-y) according to the chain rule.
	</description>
	<comments>
		<comment id='1' author='dotelos' date='2018-02-23T19:03:50Z'>
		The gradient should be ograd * layer_grad and should be correct. dy means the gradient passed by the next layer and should be multiplied to y * (1-y)
		</comment>
		<comment id='2' author='dotelos' date='2018-02-26T04:19:52Z'>
		&lt;denchmark-link:https://github.com/sxjscience&gt;@sxjscience&lt;/denchmark-link&gt;
 Please fix the example in the doc. &lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/autograd.py#L375&gt;https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/autograd.py#L375&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='dotelos' date='2018-02-26T04:57:26Z'>
		Sorry for not submitting the fix.
		</comment>
	</comments>
</bug>