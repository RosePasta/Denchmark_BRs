<bug id='16869' author='liuzh91' open_date='2019-11-20T10:28:51Z' closed_time='2019-12-13T06:28:12Z'>
	<summary>Cannot manipulate gradients during training in estimator</summary>
	<description>
Description
In current estimator framework, api fit_batch() forward the training batch through the network, compute the loss and gradients and update network parameters with gradients. The default
fit_batch() function is shown as below:
&lt;denchmark-code&gt;def fit_batch(self, train_batch, batch_axis=0):
        data, label = self._get_data_and_label(train_batch, self.context, batch_axis)
        batch_size = train_batch[0].shape[batch_axis]

        with autograd.record():
            pred = [self.net(x) for x in data]
            loss = [self.loss(y_hat, y) for y_hat, y in zip(pred, label)]

        for l in loss:
            l.backward()

        self.trainer.step(batch_size)

        return data, label, pred, loss`
&lt;/denchmark-code&gt;

The problem is in some use cases like gradient accumulation, users may not update gradient at each batch. They apply gradients on network parameters periodically. There is no way to implement the gradient accumulation feature in customized estimators.
One possible option is to create a handler to update/apply gradients at the end of each batch, but it violates the definition of handlers since handlers are not supposed to apply gradients on network parameters.  Gradient updates should be processed in the fit_batch() api.
So a viable way to resolve the problem is to decompose the fit_batch() function into two methods:

computations of loss/gradient
applying gradients on network parameters

between these two methods we could add some customized api to process gradients, e.g., gradient clipping or gradient accumulation.
	</description>
	<comments>
		<comment id='1' author='liuzh91' date='2019-11-20T12:38:21Z'>
		Thanks &lt;denchmark-link:https://github.com/liuzh91&gt;@liuzh91&lt;/denchmark-link&gt;
 for raising the issue. I agree the current  is buggy as it cannot support this standard use-case.
What do you mean with "handlers are not supposed to apply gradients on network parameters"?
If we allow handlers to apply gradient to network parameter, an alternative solution to your problem could be to remove the trainer.step from the current def fit_batch implementation and introduce a GradientUpdateHandler? Similar to how LoggingHandler and MetricHandler are constructed by default in estimator.fit, if not provided by the user, the GradientUpdateHandler would be default implement the estimator.step behavior.
Users would be free to provide their own subclass of GradientUpdateHandler that could extend the handler to do gradient clipping or gradient accumulation.
		</comment>
		<comment id='2' author='liuzh91' date='2019-11-21T02:29:14Z'>
		
Thanks @liuzh91 for raising the issue. I agree the current fit_batch is buggy as it cannot support this standard use-case.
What do you mean with "handlers are not supposed to apply gradients on network parameters"?
If we allow handlers to apply gradient to network parameter, an alternative solution to your problem could be to remove the trainer.step from the current def fit_batch implementation and introduce a GradientUpdateHandler? Similar to how LoggingHandler and MetricHandler are constructed by default in estimator.fit, if not provided by the user, the GradientUpdateHandler would be default implement the estimator.step behavior.
Users would be free to provide their own subclass of GradientUpdateHandler that could extend the handler to do gradient clipping or gradient accumulation.

Hi &lt;denchmark-link:https://github.com/leezu&gt;@leezu&lt;/denchmark-link&gt;
 , if we can introduce a gradient update handler for estimator class and fix the  routine, then the problem is solved. "handlers are not supposed to apply gradients on network parameters" for this part, I mean the common workflow in current estimator framework is to update/apply gradients in the  routine instead of using a standalone handler. Handlers are used to process other information, e.g., metrics, logging information, etc.
		</comment>
		<comment id='3' author='liuzh91' date='2019-11-25T09:43:42Z'>
		&lt;denchmark-link:https://github.com/roywei&gt;@roywei&lt;/denchmark-link&gt;
: &lt;denchmark-link:https://github.com/liuzh91&gt;@liuzh91&lt;/denchmark-link&gt;
 just noticed you self-assigned this issue 4 days ago. In the meantime he prepared &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/16900&gt;#16900&lt;/denchmark-link&gt;
 Do you think that change is in order? Or would you suggest any other change. Sorry for the inconvenience in case you also started working on it already..
		</comment>
	</comments>
</bug>