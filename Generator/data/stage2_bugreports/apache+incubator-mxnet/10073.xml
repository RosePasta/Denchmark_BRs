<bug id='10073' author='dmas-at-wiris' open_date='2018-03-12T16:17:50Z' closed_time='2018-04-07T03:49:17Z'>
	<summary>NaN in loss when using gluon ELU block</summary>
	<description>
I have a CNN that uses ELUs as activation function. When I use the gluon implementation ( from &lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/gluon/nn/activations.py#L161&gt;here&lt;/denchmark-link&gt;
) I obtain NaN values in the loss after few epochs.
However, if I create a block with this F.LeakyReLU(x, act_type='elu', alpha) the loss doesn't diverge.  Any idea?
I'll try to provide a minimal working example asap, if necessary.
	</description>
	<comments>
		<comment id='1' author='dmas-at-wiris' date='2018-03-12T18:19:33Z'>
		Thanks for reporting this! It would be great if you could get an MWE.
		</comment>
		<comment id='2' author='dmas-at-wiris' date='2018-04-02T20:31:13Z'>
		&lt;denchmark-link:https://github.com/dmas-at-wiris&gt;@dmas-at-wiris&lt;/denchmark-link&gt;

Is this still an issue for you? If so, could you provide an MWE? Thanks!
		</comment>
		<comment id='3' author='dmas-at-wiris' date='2018-04-07T03:31:37Z'>
		Since we didnt get a MWE can we close this ? &lt;denchmark-link:https://github.com/sxjscience&gt;@sxjscience&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>