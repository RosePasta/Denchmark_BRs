<bug id='6846' author='langwei' open_date='2017-06-28T02:54:17Z' closed_time='2019-01-29T02:27:06Z'>
	<summary>Check failed: err == cudaSuccess (8 vs. 0) Name: MapPlanKernel ErrStr:invalid device function</summary>
	<description>
&lt;denchmark-h:h2&gt;Environment info&lt;/denchmark-h&gt;

Operating System:  Ubuntu 16.04  ，TX1
MXNet version: 0.10.1
Python version and distribution:Python 2.7
&lt;denchmark-h:h2&gt;Error Message:&lt;/denchmark-h&gt;

tx1@tegra-ubuntu:~/git/mxnet/example/image-classification$ **python train_
mnist.py --gpus 0 --network lenet
INFO:root:start with arguments Namespace(batch_size=64, disp_batches=100, gpus='0', kv_store='device', load_epoch=None, lr=0.05, lr_factor=0.1, lr_step_epochs='10', model_prefix=None, mom=0.9, monitor=0, network='lenet', num_classes=10, num_epochs=20, num_examples=60000, num_layers=None, optimizer='sgd', test_io=0, top_k=0, wd=0.0001)
[02:42:41] /home/tx1/git/mxnet/dmlc-core/include/dmlc/./logging.h:304: [02:42:41] /home/tx1/git/mxnet/mshadow/mshadow/././././cuda/tensor_gpu-inl.cuh:110: Check failed: err == cudaSuccess (8 vs. 0) Name: MapPlanKernel ErrStr:invalid device function
Stack trace returned 10 entries:
[bt] (0) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(_ZN4dmlc15LogMessageFatalD1Ev+0x44) [0x7f7e805194]
[bt] (1) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(_ZN7mshadow4cuda7MapPlanINS_2sv6savetoENS_6TensorINS_3gpuELi2EfEENS_4expr9ScalarExpIfEEfEEvNS7_4PlanIT0_T2_EERKNSA_IT1_SC_EENS_5ShapeILi2EEEP11CUstream_st+0x1a4) [0x7f7fc4b864]
[bt] (2) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(_ZN5mxnet7ndarray4EvalIN7mshadow3gpuEEEvRKfPNS_5TBlobENS_10RunContextE+0x1b8) [0x7f7fc98f80]
[bt] (3) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(+0xf9457c) [0x7f7f0f657c]
[bt] (4) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(ZNSt17_Function_handlerIFvN5mxnet10RunContextENS0_6engine18CallbackOnCompleteEEZNS0_6Engine8PushSyncESt8functionIFvS1_EENS0_7ContextERKSt6vectorIPNS2_3VarESaISC_EESG_NS0_10FnPropertyEiPKcEUlS1_S3_E_E9_M_invokeERKSt9_Any_dataOS1_OS3+0x4c) [0x7f7effc38c]
[bt] (5) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(_ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS_10RunContextEPNS0_8OprBlockE+0x88) [0x7f7f37da70]
[bt] (6) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(ZNSt17_Function_handlerIFvSt10shared_ptrIN5mxnet6engine10ThreadPool11SimpleEventEEEZZNS2_23ThreadedEnginePerDevice13PushToExecuteEPNS2_8OprBlockEbENKUlvE1_clEvEUlS5_E_E9_M_invokeERKSt9_Any_dataOS5+0x110) [0x7f7f388a80]
[bt] (7) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(_ZNSt6thread5_ImplISt12_Bind_simpleIFSt8functionIFvSt10shared_ptrIN5mxnet6engine10ThreadPool11SimpleEventEEEES8_EEE6_M_runEv+0x48) [0x7f7f3800b8]
[bt] (8) /usr/lib/aarch64-linux-gnu/libstdc++.so.6(+0xb8280) [0x7f68b99280]
[bt] (9) /lib/aarch64-linux-gnu/libpthread.so.0(+0x6fc4) [0x7f8a8b3fc4]
[02:42:41] /home/tx1/git/mxnet/dmlc-core/include/dmlc/./logging.h:304: [02:42:41] /home/tx1/git/mxnet/mshadow/mshadow/././././cuda/tensor_gpu-inl.cuh:110: Check failed: err == cudaSuccess (8 vs. 0) Name: MapPlanKernel ErrStr:invalid device function
Stack trace returned 10 entries:
[bt] (0) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(_ZN4dmlc15LogMessageFatalD1Ev+0x44) [0x7f7e805194]
[bt] (1) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(_ZN7mshadow4cuda7MapPlanINS_2sv6savetoENS_6TensorINS_3gpuELi2EfEENS_4expr9ScalarExpIfEEfEEvNS7_4PlanIT0_T2_EERKNSA_IT1_SC_EENS_5ShapeILi2EEEP11CUstream_st+0x1a4) [0x7f7fc4b864]
[bt] (2) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(_ZN5mxnet7ndarray4EvalIN7mshadow3gpuEEEvRKfPNS_5TBlobENS_10RunContextE+0x1b8) [0x7f7fc98f80]
[bt] (3) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(+0xf9457c) [0x7f7f0f657c]
[bt] (4) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(ZNSt17_Function_handlerIFvN5mxnet10RunContextENS0_6engine18CallbackOnCompleteEEZNS0_6Engine8PushSyncESt8functionIFvS1_EENS0_7ContextERKSt6vectorIPNS2_3VarESaISC_EESG_NS0_10FnPropertyEiPKcEUlS1_S3_E_E9_M_invokeERKSt9_Any_dataOS1_OS3+0x4c) [0x7f7effc38c]
[bt] (5) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(_ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS_10RunContextEPNS0_8OprBlockE+0x88) [0x7f7f37da70]
[bt] (6) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(ZNSt17_Function_handlerIFvSt10shared_ptrIN5mxnet6engine10ThreadPool11SimpleEventEEEZZNS2_23ThreadedEnginePerDevice13PushToExecuteEPNS2_8OprBlockEbENKUlvE1_clEvEUlS5_E_E9_M_invokeERKSt9_Any_dataOS5+0x110) [0x7f7f388a80]
[bt] (7) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(_ZNSt6thread5_ImplISt12_Bind_simpleIFSt8functionIFvSt10shared_ptrIN5mxnet6engine10ThreadPool11SimpleEventEEEES8_EEE6_M_runEv+0x48) [0x7f7f3800b8]
[bt] (8) /usr/lib/aarch64-linux-gnu/libstdc++.so.6(+0xb8280) [0x7f68b99280]
[bt] (9) /lib/aarch64-linux-gnu/libpthread.so.0(+0x6fc4) [0x7f8a8b3fc4]
[02:42:41] /home/tx1/git/mxnet/dmlc-core/include/dmlc/./logging.h:304: [02:42:41] src/engine/./threaded_engine.h:329: [02:42:41] /home/tx1/git/mxnet/mshadow/mshadow/././././cuda/tensor_gpu-inl.cuh:110: Check failed: err == cudaSuccess (8 vs. 0) Name: MapPlanKernel ErrStr:invalid device function
Stack trace returned 10 entries:
[bt] (0) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(_ZN4dmlc15LogMessageFatalD1Ev+0x44) [0x7f7e805194]
[bt] (1) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(_ZN7mshadow4cuda7MapPlanINS_2sv6savetoENS_6TensorINS_3gpuELi2EfEENS_4expr9ScalarExpIfEEfEEvNS7_4PlanIT0_T2_EERKNSA_IT1_SC_EENS_5ShapeILi2EEEP11CUstream_st+0x1a4) [0x7f7fc4b864]
[bt] (2) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(_ZN5mxnet7ndarray4EvalIN7mshadow3gpuEEEvRKfPNS_5TBlobENS_10RunContextE+0x1b8) [0x7f7fc98f80]
[bt] (3) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(+0xf9457c) [0x7f7f0f657c]
[bt] (4) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(ZNSt17_Function_handlerIFvN5mxnet10RunContextENS0_6engine18CallbackOnCompleteEEZNS0_6Engine8PushSyncESt8functionIFvS1_EENS0_7ContextERKSt6vectorIPNS2_3VarESaISC_EESG_NS0_10FnPropertyEiPKcEUlS1_S3_E_E9_M_invokeERKSt9_Any_dataOS1_OS3+0x4c) [0x7f7effc38c]
[bt] (5) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(_ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS_10RunContextEPNS0_8OprBlockE+0x88) [0x7f7f37da70]
[bt] (6) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(ZNSt17_Function_handlerIFvSt10shared_ptrIN5mxnet6engine10ThreadPool11SimpleEventEEEZZNS2_23ThreadedEnginePerDevice13PushToExecuteEPNS2_8OprBlockEbENKUlvE1_clEvEUlS5_E_E9_M_invokeERKSt9_Any_dataOS5+0x110) [0x7f7f388a80]
[bt] (7) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(_ZNSt6thread5_ImplISt12_Bind_simpleIFSt8functionIFvSt10shared_ptrIN5mxnet6engine10ThreadPool11SimpleEventEEEES8_EEE6_M_runEv+0x48) [0x7f7f3800b8]
[bt] (8) /usr/lib/aarch64-linux-gnu/libstdc++.so.6(+0xb8280) [0x7f68b99280]
[bt] (9) /lib/aarch64-linux-gnu/libpthread.so.0(+0x6fc4) [0x7f8a8b3fc4]
An fatal error occurred in asynchronous engine operation. If you do not know what caused this error, you can try set environment variable MXNET_ENGINE_TYPE to NaiveEngine and run with debugger (i.e. gdb). This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error. Remember to set MXNET_ENGINE_TYPE back to empty after debugging.
Stack trace returned 6 entries:
[bt] (0) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(_ZN4dmlc15LogMessageFatalD1Ev+0x44) [0x7f7e805194]
[bt] (1) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(_ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS_10RunContextEPNS0_8OprBlockE+0x2c4) [0x7f7f37dcac]
[bt] (2) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(ZNSt17_Function_handlerIFvSt10shared_ptrIN5mxnet6engine10ThreadPool11SimpleEventEEEZZNS2_23ThreadedEnginePerDevice13PushToExecuteEPNS2_8OprBlockEbENKUlvE1_clEvEUlS5_E_E9_M_invokeERKSt9_Any_dataOS5+0x110) [0x7f7f388a80]
[bt] (3) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(_ZNSt6thread5_ImplISt12_Bind_simpleIFSt8functionIFvSt10shared_ptrIN5mxnet6engine10ThreadPool11SimpleEventEEEES8_EEE6_M_runEv+0x48) [0x7f7f3800b8]
[bt] (4) /usr/lib/aarch64-linux-gnu/libstdc++.so.6(+0xb8280) [0x7f68b99280]
[bt] (5) /lib/aarch64-linux-gnu/libpthread.so.0(+0x6fc4) [0x7f8a8b3fc4]
terminate called after throwing an instance of 'dmlc::Error'
what():  [02:42:41] src/engine/./threaded_engine.h:329: [02:42:41] /home/tx1/git/mxnet/mshadow/mshadow/././././cuda/tensor_gpu-inl.cuh:110: Check failed: err == cudaSuccess (8 vs. 0) Name: MapPlanKernel ErrStr:invalid device function
Stack trace returned 10 entries:
[bt] (0) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(_ZN4dmlc15LogMessageFatalD1Ev+0x44) [0x7f7e805194]
[bt] (1) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(_ZN7mshadow4cuda7MapPlanINS_2sv6savetoENS_6TensorINS_3gpuELi2EfEENS_4expr9ScalarExpIfEEfEEvNS7_4PlanIT0_T2_EERKNSA_IT1_SC_EENS_5ShapeILi2EEEP11CUstream_st+0x1a4) [0x7f7fc4b864]
[bt] (2) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(_ZN5mxnet7ndarray4EvalIN7mshadow3gpuEEEvRKfPNS_5TBlobENS_10RunContextE+0x1b8) [0x7f7fc98f80]
[bt] (3) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(+0xf9457c) [0x7f7f0f657c]
[bt] (4) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(ZNSt17_Function_handlerIFvN5mxnet10RunContextENS0_6engine18CallbackOnCompleteEEZNS0_6Engine8PushSyncESt8functionIFvS1_EENS0_7ContextERKSt6vectorIPNS2_3VarESaISC_EESG_NS0_10FnPropertyEiPKcEUlS1_S3_E_E9_M_invokeERKSt9_Any_dataOS1_OS3+0x4c) [0x7f7effc38c]
[bt] (5) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(_ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS_10RunContextEPNS0_8OprBlockE+0x88) [0x7f7f37da70]
[bt] (6) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(ZNSt17_Function_handlerIFvSt10shared_ptrIN5mxnet6engine10ThreadPool11SimpleEventEEEZZNS2_23ThreadedEnginePerDevice13PushToExecuteEPNS2_8OprBlockEbENKUlvE1_clEvEUlS5_E_E9_M_invokeERKSt9_Any_dataOS5+0x110) [0x7f7f388a80]
[bt] (7) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(_ZNSt6thread5_ImplISt12_Bind_simpleIFSt8functionIFvSt10shared_ptrIN5mxnet6engine10ThreadPool11SimpleEventEEEES8_EEE6_M_runEv+0x48) [0x7f7f3800b8]
[bt] (8) /usr/lib/aarch64-linux-gnu/libstdc++.so.6(+0xb8280) [0x7f68b99280]
[bt] (9) /lib/aarch64-linux-gnu/libpthread.so.0(+0x6fc4) [0x7f8a8b3fc4]
An fatal error occurred in asynchronous engine operation. If you do not know what caused this error, you can try set environment variable MXNET_ENGINE_TYPE to NaiveEngine and run with debugger (i.e. gdb). This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error. Remember to set MXNET_ENGINE_TYPE back to empty after debugging.
Stack trace returned 6 entries:
[bt] (0) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(_ZN4dmlc15LogMessageFatalD1Ev+0x44) [0x7f7e805194]
[bt] (1) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(_ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS_10RunContextEPNS0_8OprBlockE+0x2c4) [0x7f7f37dcac]
[bt] (2) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(ZNSt17_Function_handlerIFvSt10shared_ptrIN5mxnet6engine10ThreadPool11SimpleEventEEEZZNS2_23ThreadedEnginePerDevice13PushToExecuteEPNS2_8OprBlockEbENKUlvE1_clEvEUlS5_E_E9_M_invokeERKSt9_Any_dataOS5+0x110) [0x7f7f388a80]
[bt] (3) /usr/local/lib/python2.7/dist-packages/mxnet-0.10.1-py2.7.egg/mxnet/libmxnet.so(_ZNSt6thread5_ImplISt12_Bind_simpleIFSt8functionIFvSt10shared_ptrIN5mxnet6engine10ThreadPool11SimpleEventEEEES8_EEE6_M_runEv+0x48) [0x7f7f3800b8]
[bt] (4) /usr/lib/aarch64-linux-gnu/libstdc++.so.6(+0xb8280) [0x7f68b99280]
[bt] (5) /lib/aarch64-linux-gnu/libpthread.so.0(+0x6fc4) [0x7f8a8b3fc4]
Aborted
when I  run the example with GPU on TX1，how to solve the problem ?thank you
	</description>
	<comments>
		<comment id='1' author='langwei' date='2017-06-28T03:33:03Z'>
		Did you compile from source? Try adding 53 to KNOWN_CUDA_ARCHS in Makefile and compiling.
		</comment>
		<comment id='2' author='langwei' date='2017-06-28T03:50:39Z'>
		&lt;denchmark-link:https://github.com/ptrendx&gt;@ptrendx&lt;/denchmark-link&gt;
  Yes, I compile from source, and I have add 53  in the Makefile ,but it still appears the problem.
		</comment>
		<comment id='3' author='langwei' date='2017-06-29T06:26:48Z'>
		I also have same issue for TX1. I've added 53 too, but still same.
		</comment>
		<comment id='4' author='langwei' date='2017-06-30T02:19:04Z'>
		&lt;denchmark-link:https://github.com/wangmir&gt;@wangmir&lt;/denchmark-link&gt;
  hello, did you solved the problem?
		</comment>
		<comment id='5' author='langwei' date='2017-06-30T05:03:52Z'>
		No I didn't. I'm also in same stage with you.
		</comment>
		<comment id='6' author='langwei' date='2017-09-07T03:03:02Z'>
		&lt;denchmark-link:https://github.com/langwei&gt;@langwei&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/wangmir&gt;@wangmir&lt;/denchmark-link&gt;
 Did you solve this issue? I met this same error.
		</comment>
		<comment id='7' author='langwei' date='2017-10-19T00:51:07Z'>
		I was able to build mxnet on my TX1 with gpu today. I did add 53 in the Makefile for thoroughness.
&lt;denchmark-link:https://mxnet.incubator.apache.org/get_started/install.html&gt;https://mxnet.incubator.apache.org/get_started/install.html&lt;/denchmark-link&gt;

Devices -&gt; Nvidia Jetson TX2
Linux -&gt; Python -&gt; GPU -&gt; Build from Source
nvidia@tegra-ubuntu:~/mxnet/python$ uname -a
Linux tegra-ubuntu 4.4.38-jetsonbot-doc-v0.3 &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/1&gt;#1&lt;/denchmark-link&gt;
 SMP PREEMPT Thu Oct 5 15:58:24 EDT 2017 aarch64 aarch64 aarch64 GNU/Linux
nvidia@tegra-ubuntu:~/mxnet/python$ make USE_OPENCV=1 USE_BLAS=openblas USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1
nvidia@tegra-ubuntu:~/mxnet/python$ pip list -e
mxnet (0.11.1, /home/nvidia/mxnet/python)
nvidia@tegra-ubuntu:~/mxnet/python$ python
Python 2.7.12 (default, Nov 19 2016, 06:48:10)
[GCC 5.4.0 20160609] on linux2
Type "help", "copyright", "credits" or "license" for more information.



import mxnet as mx
a = mx.nd.ones((2, 3), mx.gpu())
b = a * 2 + 1
b.asnumpy()
array([[ 3.,  3.,  3.],
[ 3.,  3.,  3.]], dtype=float32)



		</comment>
		<comment id='8' author='langwei' date='2018-01-18T12:26:28Z'>
		&lt;denchmark-link:https://github.com/orgs/apache/teams/mxnet-committers&gt;@apache/mxnet-committers&lt;/denchmark-link&gt;
: This issue has been inactive for the past 90 days. It has no label and needs triage.
For general "how-to" questions, our &lt;denchmark-link:https://discuss.mxnet.io/&gt;user forum&lt;/denchmark-link&gt;
 (and &lt;denchmark-link:https://discuss.gluon.ai/&gt;Chinese version&lt;/denchmark-link&gt;
) is a good place to get help.
		</comment>
		<comment id='9' author='langwei' date='2018-11-15T17:59:44Z'>
		Hello &lt;denchmark-link:https://github.com/langwei&gt;@langwei&lt;/denchmark-link&gt;

Were you able to resolve the issue following the steps above?
		</comment>
		<comment id='10' author='langwei' date='2018-11-16T02:52:41Z'>
		
Hello @langwei
Were you able to resolve the issue following the steps above?
Try adding
" CUDA_ARCH = -m64   -gencode arch=computer_53 ,  code = sm_53  -gencode arch = computer_53, code=computer_53"
to KNOWN_CUDA_ARCHS in Makefile and compiling.

		</comment>
		<comment id='11' author='langwei' date='2018-11-20T17:58:56Z'>
		&lt;denchmark-link:https://github.com/langwei&gt;@langwei&lt;/denchmark-link&gt;
 could you please elaborate more? If the issue was resolved then could you please close it?
		</comment>
		<comment id='12' author='langwei' date='2018-11-27T00:58:17Z'>
		&lt;denchmark-link:https://github.com/langwei&gt;@langwei&lt;/denchmark-link&gt;
 requesting an update on your comment above, has the issue been resolved?
		</comment>
		<comment id='13' author='langwei' date='2019-01-28T23:08:20Z'>
		&lt;denchmark-link:https://github.com/langwei&gt;@langwei&lt;/denchmark-link&gt;
 bouncing again for an update as asked in Nov 2018.
If this is no longer an issue, please close. Thanks!
		</comment>
	</comments>
</bug>