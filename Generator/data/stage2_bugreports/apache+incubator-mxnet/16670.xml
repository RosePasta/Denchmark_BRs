<bug id='16670' author='DickJC123' open_date='2019-10-29T17:57:58Z' closed_time='2019-11-04T19:58:44Z'>
	<summary>cuDNN RNN dtype_with_fallback_ calc needs update</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

In src/operator/rnn-inl.h, the calculation of the mathPrec passed to cudnnSetRNNDescriptor_v6() needs review (the variable is dtype_with_fallback_).  The current logic is:
&lt;denchmark-code&gt;#if __CUDA_ARCH__ &lt; 530 &amp;&amp; CUDNN_VERSION &gt;= 7500
      if (dtype_ == CUDNN_DATA_HALF) {
        dtype_with_fallback_ = CUDNN_DATA_FLOAT;
      } else {
        dtype_with_fallback_ = dtype_;
      }
#else
        dtype_with_fallback_ = dtype_;
#endif
&lt;/denchmark-code&gt;

The use of __CUDA_ARCH__ in determining host-side code behavior is not appropriate (the variable would be undefined in this case and interpreted as 0 in all cases).  Thus the intent of the code (to ensure pseudo-fp16 handling of RNNs for Maxwell GPUs even after the API change of cuDNN 7.5) is being applied to all architectures.
My recommendation would be to adopt pseudo-fp16 for all architectures anyway.  A possible fix:
&lt;denchmark-code&gt;    cudnnDataType_t dtype_with_fallback_ =
        (CUDNN_VERSION &gt;= 7500 &amp;&amp; dtype_ == CUDNN_DATA_HALF) ? CUDNN_DATA_FLOAT
                                                             : dtype_; 
&lt;/denchmark-code&gt;

Technically, since cuDNN prior to v7.5 ignores the use mathPrec in cudnnSetRNNDescriptor_v6(), the CUDNN_VERSION &gt;= 7500 term could be dropped with no impact.
While we're fixing this, I think we should consider turning on Tensor Cores by default, or at least giving the user a mechanism to do so.
&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

To verify my claim about __CUDA_ARCH__, I inserted the following code in rnn-inl.h just before the definition of dtype_with_fallback_:
&lt;denchmark-code&gt;#ifdef __CUDA_ARCH__
      LOG(INFO) &lt;&lt; "************* __CUDA_ARCH__ = " &lt;&lt; __CUDA_ARCH__;
#else
      LOG(INFO) &lt;&lt; "************* __CUDA_ARCH__ is not defined";
#endif
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;$ nosetests --verbose -s tests/python/gpu/test_operator_gpu.py:test_rnn
...
[10:53:00] src/operator/./rnn-inl.h:1388: ************* __CUDA_ARCH__ is not defined
[10:53:00] src/operator/./rnn-inl.h:1388: ************* __CUDA_ARCH__ is not defined
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/stu1130&gt;@stu1130&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ptrendx&gt;@ptrendx&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='DickJC123' date='2019-10-29T21:01:59Z'>
		Thanks for the suggestion &lt;denchmark-link:https://github.com/DickJC123&gt;@DickJC123&lt;/denchmark-link&gt;

I created a quick fix &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/16671&gt;#16671&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='DickJC123' date='2019-11-04T19:50:26Z'>
		&lt;denchmark-link:https://github.com/DickJC123&gt;@DickJC123&lt;/denchmark-link&gt;
 do you think we can close the issue?
My though is that there are two things in the issue, first one was addressed but the the second one which is enabling the Tensor Cores by default is not tackled yet so I kept it open. But feel free to close it and maybe create enabling Tensor Cores issue if you want.
		</comment>
	</comments>
</bug>