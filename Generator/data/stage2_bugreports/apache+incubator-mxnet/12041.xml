<bug id='12041' author='DuCheng2018' open_date='2018-08-06T01:53:44Z' closed_time='2018-11-19T02:52:39Z'>
	<summary>Bug in v1.2.0 ?</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

Mxnet v1.2.0 gpu training failed on my computer.
&lt;denchmark-h:h2&gt;Environment info (Required)&lt;/denchmark-h&gt;

Here is my environment:
Windows 7
Gtx 1080 (x4)
Python 3.6.4 (anaconda3)
CUDA 8.0
mxnet-cu80
&lt;denchmark-h:h2&gt;Error Message:&lt;/denchmark-h&gt;

I run the following code under mxnet v1.2.0.


import mxnet as mx
a = mx.nd.ones(1,mx.gpu())
#python.exe crash "cudaErrorCudartUnloading : device kernel image is invalid".
b = mx.nd.ones(1) #works well only using cpu


&lt;denchmark-h:h2&gt;What have I tried to solve it?&lt;/denchmark-h&gt;

I have solved it just by using mxnet v1.1.0 instead.
	</description>
	<comments>
		<comment id='1' author='DuCheng2018' date='2018-08-06T18:29:55Z'>
		&lt;denchmark-link:https://github.com/DuCheng2018&gt;@DuCheng2018&lt;/denchmark-link&gt;
 Do you get any particular error? Can you paste stacktrace?
&lt;denchmark-link:https://github.com/sandeep-krishnamurthy&gt;@sandeep-krishnamurthy&lt;/denchmark-link&gt;
 Could you add label: PendingRequestorInfo
		</comment>
		<comment id='2' author='DuCheng2018' date='2018-08-07T05:13:45Z'>
		What platform are you running on(I am assuming Windows since I python.exe)?
What version of MXNet are installing ?  Do you have the corresponding version of CUDA installed in place? ie., pip install mxnet-cu92 requires CUDA 9.2
		</comment>
		<comment id='3' author='DuCheng2018' date='2018-08-07T08:27:39Z'>
		The error message is "cudaErrorCudartUnloading : device kernel image is invalid"
		</comment>
		<comment id='4' author='DuCheng2018' date='2018-08-07T08:49:08Z'>
		When I train mnist dataset by :
python train_mnist.py --network lenet --gpus 0
&lt;denchmark-h:h2&gt;Traceback Message:&lt;/denchmark-h&gt;

Traceback (most recent call last):
File "C:\Anaconda3\lib\site-packages\mxnet\symbol\symbol.py", line 1513, in simple_bind    ctypes.byref(exe_handle)))
File "C:\Anaconda3\lib\site-packages\mxnet\base.py", line 149, in check_call
raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [16:46:29] c:\jenkins\workspace\mxnet-tag\mxnet\src\storage./pooled_storage_manager.h:108: cudaMalloc failed: device kernel image is invalid
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
File "train_mnist.py", line 96, in 
fit.fit(args, sym, get_mnist_iter)
File "D:\MXNet\apache-mxnet-src-1.2.1-incubating\example\image-classification\common\fit.py", line 307, in fit monitor=monitor)
File "C:\Anaconda3\lib\site-packages\mxnet\module\base_module.py", line 484, in fit
for_training=True, force_rebind=force_rebind)
File "C:\Anaconda3\lib\site-packages\mxnet\module\module.py", line 430, in bind
state_names=self._state_names)
File "C:\Anaconda3\lib\site-packages\mxnet\module\executor_group.py", line 265, in init
self.bind_exec(data_shapes, label_shapes, shared_group)
File "C:\Anaconda3\lib\site-packages\mxnet\module\executor_group.py", line 361, in bind_exec
shared_group))
File "C:\Anaconda3\lib\site-packages\mxnet\module\executor_group.py", line 639, in _bind_ith_exec
shared_buffer=shared_data_arrays, **input_shapes)
File "C:\Anaconda3\lib\site-packages\mxnet\symbol\symbol.py", line 1519, in simple_bind
raise RuntimeError(error_msg)
RuntimeError: simple_bind error. Arguments:
data: (1, 1, 28, 28)
softmax_label: (1,)
[16:46:29] c:\jenkins\workspace\mxnet-tag\mxnet\src\storage./pooled_storage_manager.h:108: cudaMalloc failed: device kernel image is invalid
		</comment>
		<comment id='5' author='DuCheng2018' date='2018-08-16T02:57:24Z'>
		No one answer my question?
		</comment>
		<comment id='6' author='DuCheng2018' date='2018-09-02T12:45:49Z'>
		I have had the same problem as you and tried many times without success. I think it might be a bug
		</comment>
		<comment id='7' author='DuCheng2018' date='2018-10-19T22:48:17Z'>
		&lt;denchmark-link:https://github.com/DuCheng2018&gt;@DuCheng2018&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/hitdongfeng&gt;@hitdongfeng&lt;/denchmark-link&gt;
 have you tried with 1.3 builds. Is this issue reproducible there?
		</comment>
		<comment id='8' author='DuCheng2018' date='2018-10-20T01:02:22Z'>
		Yeah, I have tried v1.3 which also didn’t work.




On 10/20/2018 06:49, Lanking wrote:

&lt;denchmark-link:https://github.com/DuCheng2018&gt;@DuCheng2018&lt;/denchmark-link&gt;
@hitdongfeng have you tried with 1.3 builds. Is this issue reproducible there?

—
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub, or mute the thread.
		</comment>
		<comment id='9' author='DuCheng2018' date='2018-11-15T22:08:20Z'>
		&lt;denchmark-link:https://github.com/mxnet-label-bot&gt;@mxnet-label-bot&lt;/denchmark-link&gt;
 update [Bug, Build, Pending Requester Info]
Can you run our diagnostic script and comment with the output?
&lt;denchmark-code&gt;What to do:
1. Download the diagnosis script from https://raw.githubusercontent.com/apache/incubator-mxnet/master/tools/diagnose.py
2. Run the script using `python diagnose.py` and paste its output here.

&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='DuCheng2018' date='2018-11-19T02:50:57Z'>
		The bug is fixed after I update mxnet. Thx a lot.
		</comment>
	</comments>
</bug>