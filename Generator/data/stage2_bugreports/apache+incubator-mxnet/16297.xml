<bug id='16297' author='shesung' open_date='2019-09-27T10:17:10Z' closed_time='2020-06-08T17:41:36Z'>
	<summary>incorrect grad of gluon.nn.BatchNorm when scale=False</summary>
	<description>
When using gluon.nn.BatchNorm(scale=False) on gpu,  the computed grad for beta is not correct. The grad of beta seem to be accumulated between iterations.
When setting scale=True or running on cpu, it goes correctly.
This problem may make network hard to converge during trainning.
&lt;denchmark-h:h2&gt;Environment info (Required)&lt;/denchmark-h&gt;

CentOS Linux release 7.2.1511 (Core)
GTX 1080Ti
Driver Version: 384.69
CUDA Version 9.0.176
installed with pip:
numpy                              1.17.2
mxnet-cu90                         1.5.0
&lt;denchmark-h:h2&gt;Code&lt;/denchmark-h&gt;

In this example, the grad of beta shuold be [1, 1, 1] at each iteration.
import mxnet as mx
from mxnet import gluon, autograd

ctx = mx.gpu()
x = mx.nd.ones((1,3,1,1), ctx=ctx)

net = gluon.nn.BatchNorm(scale=False, epsilon=2e-5, momentum=0.0)
net.initialize(ctx=ctx)
trainer = gluon.Trainer(params=net.collect_params(),
                        optimizer='sgd',
                        optimizer_params={'learning_rate': 0.01, 'wd': 0.0005, 'momentum': 0.9})
net.hybridize()

for i in range(10):
    with autograd.record():
        out = net(x)
    out.backward()
    trainer.step(x.shape[0])
    for name, param in net.collect_params().items():
        if 'beta' in name:
            print(name, param.grad(ctx).asnumpy())
output:
&lt;denchmark-code&gt;batchnorm0_beta [1. 1. 1.]
batchnorm0_beta [2. 2. 2.]
batchnorm0_beta [3. 3. 3.]
batchnorm0_beta [4. 4. 4.]
batchnorm0_beta [5. 5. 5.]
batchnorm0_beta [6. 6. 6.]
batchnorm0_beta [7. 7. 7.]
batchnorm0_beta [8. 8. 8.]
batchnorm0_beta [9. 9. 9.]
batchnorm0_beta [10. 10. 10.]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='shesung' date='2019-09-27T10:17:14Z'>
		Hey, this is the MXNet Label Bot.
Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it.
Here are my recommended label(s): Gluon, Bug
		</comment>
		<comment id='2' author='shesung' date='2019-09-27T17:26:29Z'>
		Hi &lt;denchmark-link:https://github.com/shesung&gt;@shesung&lt;/denchmark-link&gt;
 have you tried: setting scale=False running on cpu, does the output came correctly?
		</comment>
		<comment id='3' author='shesung' date='2019-09-29T01:57:38Z'>
		&lt;denchmark-link:https://github.com/lanking520&gt;@lanking520&lt;/denchmark-link&gt;
 Yes. It's correct on cpu.
I also found that, defining the layer with symbol api, the result is correct.
		</comment>
		<comment id='4' author='shesung' date='2019-10-07T19:00:57Z'>
		&lt;denchmark-link:https://github.com/zachgk&gt;@zachgk&lt;/denchmark-link&gt;
 assign [&lt;denchmark-link:https://github.com/szha&gt;@szha&lt;/denchmark-link&gt;
  ]
		</comment>
		<comment id='5' author='shesung' date='2020-06-10T02:19:58Z'>
		Hi &lt;denchmark-link:https://github.com/shesung&gt;@shesung&lt;/denchmark-link&gt;
 , the bug has been fixed in &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/18500&gt;#18500&lt;/denchmark-link&gt;
 .
Thank you for the report!
		</comment>
	</comments>
</bug>