<bug id='16723' author='sxjscience' open_date='2019-11-05T02:36:17Z' closed_time='2019-11-23T01:18:56Z'>
	<summary>[Bug] fused_op does not support boolean type</summary>
	<description>
&lt;denchmark-link:https://github.com/ptrendx&gt;@ptrendx&lt;/denchmark-link&gt;
 I find that the FusedOp does not support the boolean type. The following script will trigger the error.
import mxnet as mx
import numpy as np
from mxnet.gluon import HybridBlock
mx.npx.set_np()

class Foo(HybridBlock):
    def __init__(self, prefix=None, params=None):
        super(Foo, self).__init__(prefix=prefix, params=params)

    def hybrid_forward(self, F, valid_length):
        mask = (F.np.ones((10,)) &lt; valid_length).astype(np.float32)
        mask2 = (F.np.ones((10,)) &lt; valid_length).astype(np.float32)
        mask = mask * F.np.expand_dims(mask2, axis=-1)
        return mask

foo = Foo()
foo.hybridize()
out = foo(mx.np.ones((10,), ctx=mx.gpu()))
print(out)
Stack Trace:
&lt;denchmark-code&gt;MXNetError: [02:32:00] src/operator/fusion/fused_op.cu:76: Unknown type enum 7
Stack trace:
  [bt] (0) /home/ubuntu/mxnet/python/mxnet/../../lib/libmxnet.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x32) [0x7f310563bed2]
  [bt] (1) /home/ubuntu/mxnet/python/mxnet/../../lib/libmxnet.so(mxnet::FusedOp::CheckShapesAndTypes(std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;, std::vector&lt;int, std::allocator&lt;int&gt; &gt;*, std::vector&lt;int, std::allocator&lt;int&gt; &gt;*, std::vector&lt;int, std::allocator&lt;int&gt; &gt;*, std::vector&lt;int, std::allocator&lt;int&gt; &gt;*, int*)+0x17b3) [0x7f310b4743c3]
  [bt] (2) /home/ubuntu/mxnet/python/mxnet/../../lib/libmxnet.so(void mxnet::FusedOp::Forward&lt;mshadow::gpu&gt;(nnvm::NodeAttrs const&amp;, mxnet::OpContext const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;)+0x1a0) [0x7f310b47df50]
  [bt] (3) /home/ubuntu/mxnet/python/mxnet/../../lib/libmxnet.so(mxnet::imperative::PushFCompute(std::function&lt;void (nnvm::NodeAttrs const&amp;, mxnet::OpContext const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;)&gt; const&amp;, nnvm::Op const*, nnvm::NodeAttrs const&amp;, mxnet::Context const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::Resource, std::allocator&lt;mxnet::Resource&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, std::vector&lt;unsigned int, std::allocator&lt;unsigned int&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;)::{lambda(mxnet::RunContext)#1}::operator()(mxnet::RunContext) const+0x1423) [0x7f3108a01733]
  [bt] (4) /home/ubuntu/mxnet/python/mxnet/../../lib/libmxnet.so(std::_Function_handler&lt;void (mxnet::RunContext), mxnet::imperative::PushFCompute(std::function&lt;void (nnvm::NodeAttrs const&amp;, mxnet::OpContext const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;)&gt; const&amp;, nnvm::Op const*, nnvm::NodeAttrs const&amp;, mxnet::Context const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::Resource, std::allocator&lt;mxnet::Resource&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, std::vector&lt;unsigned int, std::allocator&lt;unsigned int&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;)::{lambda(mxnet::RunContext)#1}&gt;::_M_invoke(std::_Any_data const&amp;, mxnet::RunContext&amp;&amp;)+0x17) [0x7f3108a01c17]
  [bt] (5) /home/ubuntu/mxnet/python/mxnet/../../lib/libmxnet.so(std::_Function_handler&lt;void (mxnet::RunContext, mxnet::engine::CallbackOnComplete), mxnet::engine::ThreadedEngine::BulkFlush()::{lambda(mxnet::RunContext, mxnet::engine::CallbackOnComplete)#1}&gt;::_M_invoke(std::_Any_data const&amp;, mxnet::RunContext&amp;&amp;, mxnet::engine::CallbackOnComplete&amp;&amp;)+0x1bf) [0x7f310916266f]
  [bt] (6) /home/ubuntu/mxnet/python/mxnet/../../lib/libmxnet.so(mxnet::engine::ThreadedEngine::ExecuteOprBlock(mxnet::RunContext, mxnet::engine::OprBlock*)+0x995) [0x7f3109166475]
  [bt] (7) /home/ubuntu/mxnet/python/mxnet/../../lib/libmxnet.so(void mxnet::engine::ThreadedEnginePerDevice::GPUWorker&lt;(dmlc::ConcurrentQueueType)0&gt;(mxnet::Context, bool, mxnet::engine::ThreadedEnginePerDevice::ThreadWorkerBlock&lt;(dmlc::ConcurrentQueueType)0&gt;*, std::shared_ptr&lt;dmlc::ManualEvent&gt; const&amp;)+0x11d) [0x7f310917ed7d]
  [bt] (8) /home/ubuntu/mxnet/python/mxnet/../../lib/libmxnet.so(std::_Function_handler&lt;void (std::shared_ptr&lt;dmlc::ManualEvent&gt;), mxnet::engine::ThreadedEnginePerDevice::PushToExecute(mxnet::engine::OprBlock*, bool)::{lambda()#4}::operator()() const::{lambda(std::shared_ptr&lt;dmlc::ManualEvent&gt;)#1}&gt;::_M_invoke(std::_Any_data const&amp;, std::shared_ptr&lt;dmlc::ManualEvent&gt;&amp;&amp;)+0x4e) [0x7f310917f02e]
&lt;/denchmark-code&gt;

We can also manually disable the Fuse OP, which will generate the correct answer.
import mxnet as mx
import numpy as np
import os
from mxnet.gluon import HybridBlock
mx.npx.set_np()

os.environ['MXNET_USE_FUSION'] = '0'

class Foo(HybridBlock):
    def __init__(self, prefix=None, params=None):
        super(Foo, self).__init__(prefix=prefix, params=params)

    def hybrid_forward(self, F, valid_length):
        mask = (F.np.ones((10,)) &lt; valid_length).astype(np.float32)
        mask2 = (F.np.ones((10,)) &lt; valid_length).astype(np.float32)
        mask = mask * F.np.expand_dims(mask2, axis=-1)
        return mask

foo = Foo()
foo.hybridize()
out = foo(mx.np.ones((10,), ctx=mx.gpu()))
print(out)
	</description>
	<comments>
		<comment id='1' author='sxjscience' date='2019-11-05T02:48:38Z'>
		I see, this is a newly added type. We will fix this.
		</comment>
	</comments>
</bug>