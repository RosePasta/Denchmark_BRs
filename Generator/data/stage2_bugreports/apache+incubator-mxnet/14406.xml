<bug id='14406' author='anirudh2290' open_date='2019-03-13T01:47:51Z' closed_time='2019-04-12T22:10:00Z'>
	<summary>resnet cpp-package test is broken</summary>
	<description>
&lt;denchmark-link:http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Funix-gpu/detail/PR-14397/5/pipeline&gt;http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Funix-gpu/detail/PR-14397/5/pipeline&lt;/denchmark-link&gt;

after adding waitall support the resnet example is failing with cudamalloc out of memory error.
	</description>
	<comments>
		<comment id='1' author='anirudh2290' date='2019-03-13T01:47:55Z'>
		Hey, this is the MXNet Label Bot.
Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it.
Here are my recommended labels: Example
		</comment>
		<comment id='2' author='anirudh2290' date='2019-03-13T02:12:22Z'>
		I wonder the memory of GPU in CI.
The input shape is (50,3,224,224), which may triggers OOM.
		</comment>
		<comment id='3' author='anirudh2290' date='2019-03-13T02:14:30Z'>
		In addition, the model in cpp-package seems to be not convergent.
		</comment>
		<comment id='4' author='anirudh2290' date='2019-03-13T02:38:55Z'>
		I think its running on a p3.8xlarge which should be sufficient to run this test. &lt;denchmark-link:https://github.com/marcoabreu&gt;@marcoabreu&lt;/denchmark-link&gt;
 can you confirm.
		</comment>
		<comment id='5' author='anirudh2290' date='2019-03-13T02:40:19Z'>
		
In addition, the model in cpp-package seems to be not convergent.

yes i observed that too.
		</comment>
		<comment id='6' author='anirudh2290' date='2019-03-13T03:03:50Z'>
		Since the input shape of ResNet is (3, 224, 224), so I resized the MNIST image (1, 28, 28) to (3, 224, 224).
		</comment>
		<comment id='7' author='anirudh2290' date='2019-03-13T12:16:56Z'>
		We run on a g3.8xlarge
		</comment>
		<comment id='8' author='anirudh2290' date='2019-03-13T12:38:28Z'>
		Changing batch size to a smaller value will address the OOM issue.
		</comment>
		<comment id='9' author='anirudh2290' date='2019-03-13T18:13:53Z'>
		&lt;denchmark-link:https://github.com/marcoabreu&gt;@marcoabreu&lt;/denchmark-link&gt;
  There are no changes to the alexnet.cpp, resnet.cpp or cpp-package recently.
Are there any changes to underlying cuda or mxnet implementation.
These tests were part of CI tests and have been passing before. We can change the examples so that pass on lower capacity instances, in my opinion that won't be the right solution.
		</comment>
		<comment id='10' author='anirudh2290' date='2019-03-13T18:14:19Z'>
		Did infra that these tests are run on have changed recently? It seems that the test would be running fine on p3.8xl but would fail on g3.8x (legacy hardware)... &lt;denchmark-link:https://github.com/marcoabreu&gt;@marcoabreu&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='anirudh2290' date='2019-03-13T18:27:08Z'>
		as i said this happened in waitall change. waitall earlier used to hide exceptions, but with the PR: &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/14397&gt;#14397&lt;/denchmark-link&gt;
 it is thrown. These problems would have been there from before but surfacing now.
		</comment>
		<comment id='12' author='anirudh2290' date='2019-03-13T22:45:28Z'>
		I tried these examples with the recent code change in "WaitAll()" on p2.16x instances and c5.18x instances. I did not see the crash.
However,  we still need to add missing exception handling in the example so that we can prevent the crashes due to unhandled exceptions.
		</comment>
		<comment id='13' author='anirudh2290' date='2019-03-13T23:44:20Z'>
		hi &lt;denchmark-link:https://github.com/leleamol&gt;@leleamol&lt;/denchmark-link&gt;
 . to reproduce you will have to use g3.8xlarge. I was able to reproduce on a g3.8xlarge.
		</comment>
		<comment id='14' author='anirudh2290' date='2019-03-13T23:53:44Z'>
		Could someone please look the GPU memory used by the model?
		</comment>
		<comment id='15' author='anirudh2290' date='2019-03-13T23:57:50Z'>
		the last i observed it was around 11GB. For now I am going to use smaller batch_size for tests and later &lt;denchmark-link:https://github.com/leleamol&gt;@leleamol&lt;/denchmark-link&gt;
 will revisit and improve the cpp tests.
		</comment>
		<comment id='16' author='anirudh2290' date='2019-03-14T23:26:17Z'>
		&lt;denchmark-link:https://github.com/anirudh2290&gt;@anirudh2290&lt;/denchmark-link&gt;

I could reproduce this issue on p2.8 as well when I change the batch size to 100.
The example uses only one GPU. With batch size = 50, the GPU memory reaches 11GB.
		</comment>
		<comment id='17' author='anirudh2290' date='2019-04-12T22:08:48Z'>
		This issue can be closed since the PR is merged. &lt;denchmark-link:https://github.com/lanking520&gt;@lanking520&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>