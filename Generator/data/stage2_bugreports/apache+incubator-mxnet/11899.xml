<bug id='11899' author='eric-haibin-lin' open_date='2018-07-26T16:29:40Z' closed_time='2018-08-13T18:30:57Z'>
	<summary>non-deterministic backward of scatter_nd</summary>
	<description>
For &lt;denchmark-link:https://mxnet.incubator.apache.org/versions/master/api/python/ndarray/ndarray.html?highlight=logis#mxnet.ndarray.scatter_nd&gt;scatter_nd&lt;/denchmark-link&gt;
, it uses atomic add on GPU, thus
&lt;denchmark-code&gt;If the indices have duplicates, the result will be non-deterministic and the gradient of scatter_nd will not be correct!!
&lt;/denchmark-code&gt;

This should be fixed.
	</description>
	<comments>
		<comment id='1' author='eric-haibin-lin' date='2018-07-26T18:05:51Z'>
		I can probably take a look at this as I've tackled the same problem with take operator.
		</comment>
		<comment id='2' author='eric-haibin-lin' date='2018-07-30T23:50:35Z'>
		Why do we need to this to be deterministic? Is there a use case?
Note that we don't promise determinism by default.
		</comment>
		<comment id='3' author='eric-haibin-lin' date='2018-08-06T17:17:16Z'>
		If MXNet dosen't promise nondeterminism, then any model using this operator doesn't produce consistent result even if random seed is set. We should try our best to avoid this
		</comment>
		<comment id='4' author='eric-haibin-lin' date='2018-08-06T17:18:21Z'>
		&lt;denchmark-link:https://github.com/sxjscience&gt;@sxjscience&lt;/denchmark-link&gt;
 has such use case and that's why the warning documentation is added.
		</comment>
	</comments>
</bug>