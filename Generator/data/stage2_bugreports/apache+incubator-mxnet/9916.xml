<bug id='9916' author='zheng-da' open_date='2018-02-28T10:26:40Z' closed_time='2018-07-21T06:49:32Z'>
	<summary>precision bug in batchnorm.</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

I noticed this error when MKLDNN is built into MXNet. I haven't tested the case without MKLDNN.
&lt;denchmark-code&gt;$ export MXNET_TEST_SEED=1307519208
$ nosetests -v tests/python/gpu/test_operator_gpu.py:test_batchnorm_with_type
[INFO] Setting module np/mx/python random seeds, use MXNET_MODULE_SEED=570740139 to reproduce.
[WARNING] *** test-level seed set: all "@with_seed()" tests run deterministically ***
test_operator_gpu.test_batchnorm_with_type ... [INFO] Setting test np/mx/python random seeds, use MXNET_TEST_SEED=1307519208 to reproduce.
Traceback (most recent call last):
  File "/home/ubuntu/incubator-mxnet/python/mxnet/test_utils.py", line 1341, in check_consistency
    equal_nan=equal_nan)
  File "/home/ubuntu/incubator-mxnet/python/mxnet/test_utils.py", line 493, in assert_almost_equal
    raise AssertionError(msg)
AssertionError: 
Items are not equal:
Error 1.608877 exceeds tolerance rtol=0.100000, atol=0.100000.  Location of maximum error:(1,), a=-0.890410, b=-0.628418
 a: array([ 1786.13305664,    -0.89041042], dtype=float32)
 b: array([ 1786.        ,    -0.62841797], dtype=float16)
FAIL

======================================================================
FAIL: test_operator_gpu.test_batchnorm_with_type
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/nose/case.py", line 197, in runTest
    self.test(*self.arg)
  File "/home/ubuntu/incubator-mxnet/tests/python/gpu/../unittest/common.py", line 155, in test_new
    orig_test(*args, **kwargs)
  File "/home/ubuntu/incubator-mxnet/tests/python/gpu/test_operator_gpu.py", line 324, in test_batchnorm_with_type
    check_consistency(sym, ctx_list_v2_2D)
  File "/home/ubuntu/incubator-mxnet/python/mxnet/test_utils.py", line 1346, in check_consistency
    raise e
AssertionError: 
Items are not equal:
Error 1.608877 exceeds tolerance rtol=0.100000, atol=0.100000.  Location of maximum error:(1,), a=-0.890410, b=-0.628418
 a: array([ 1786.13305664,    -0.89041042], dtype=float32)
 b: array([ 1786.        ,    -0.62841797], dtype=float16)
-------------------- &gt;&gt; begin captured stdout &lt;&lt; ---------------------
Train Err: ctx 1 vs ctx 2 at norm_beta

--------------------- &gt;&gt; end captured stdout &lt;&lt; ----------------------
-------------------- &gt;&gt; begin captured logging &lt;&lt; --------------------
common: INFO: Setting module np/mx/python random seeds, use MXNET_MODULE_SEED=570740139 to reproduce.
common: WARNING: *** test-level seed set: all "@with_seed()" tests run deterministically ***
common: INFO: Setting test np/mx/python random seeds, use MXNET_TEST_SEED=1307519208 to reproduce.
--------------------- &gt;&gt; end captured logging &lt;&lt; ---------------------

----------------------------------------------------------------------
Ran 1 test in 1.675s

FAILED (failures=1)

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='zheng-da' date='2018-03-02T06:58:01Z'>
		&lt;denchmark-link:https://github.com/zheng-da&gt;@zheng-da&lt;/denchmark-link&gt;
 could you try w/o MKL-DNN backend?
		</comment>
		<comment id='2' author='zheng-da' date='2018-03-06T04:08:37Z'>
		Tested w/o MKL-DNN, the same issue is observed.
The computation result from FP16 is not matched w/ FP32.
		</comment>
		<comment id='3' author='zheng-da' date='2018-03-06T04:41:48Z'>
		Also saw failure w/o MKL in one of CI jobs: Python3 GPU
&lt;denchmark-link:http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/incubator-mxnet/detail/master/429/pipeline/&gt;http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/incubator-mxnet/detail/master/429/pipeline/&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='zheng-da' date='2018-03-09T07:35:55Z'>
		After some investigation, I think it's due to the low precision that FP16 has.
When doing batch norm backward pass, gradients of gamma and beta are accumulated on a fixed channel with the forward output as input data. In this case, 1000 values are added together.
I measure the two kinds of data difference between batchnorm op of fp64 and fp16:

input data: ~0.575 total difference
output data: ~3.879 total difference

The data is obtained from the ndarrays dumped from the failing case. When doing backward pass, for each channel with 1000 addition operations, it could result in roughly 1.9*0.6=1.14 difference between fp16 and fp64 for the gradient of gamma.
We can do two things to make the test stable and better:

Reduce the size of input data in the consistency test so that not too much rounding errors are accumulated. There is no need to test data with shape as big as (10, 2, 10, 10) here.
In the function check_consistency, initialize the arg_params and aux_params with the data type of the lowest precision in the ctx_list so that less data type casting errors are introduced while initializing executors.

I have tried the first approach with shape equal to (2, 2, 3, 3) and repeat the test more than 1000 times without failure.
		</comment>
		<comment id='5' author='zheng-da' date='2018-03-13T23:01:35Z'>
		Relevant suggestion &lt;denchmark-link:https://github.com/apache/incubator-mxnet/issues/8509&gt;#8509&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='zheng-da' date='2018-07-19T00:44:45Z'>
		Seems like this issue has been addressed already, can we close this? &lt;denchmark-link:https://github.com/marcoabreu&gt;@marcoabreu&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/szha&gt;@szha&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>