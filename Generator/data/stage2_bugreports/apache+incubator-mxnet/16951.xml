<bug id='16951' author='larroy' open_date='2019-11-30T07:05:44Z' closed_time='2019-12-06T19:46:35Z'>
	<summary>CentOS GPU tests failing in master</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

Centos GPU tests are failing in master:
&lt;denchmark-link:http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Fcentos-gpu/detail/master/1341/&gt;http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Fcentos-gpu/detail/master/1341/&lt;/denchmark-link&gt;

I couldn't reproduce in p3 instance over ubuntu 18.04. Trying in the CI AMI now.
Seems to be a problem in the base AMI, reproduced by running the following commands:
&lt;denchmark-code&gt;time ci/build.py --docker-registry mxnetci --platform centos7_gpu --docker-build-retries 3 --shm-size 500m /work/runtime_functions.sh build_centos7_gpu
time ci/build.py --docker-registry mxnetci --nvidiadocker --platform centos7_gpu --docker-build-retries 3 --shm-size 500m /work/runtime_functions.sh unittest_centos7_gpu

&lt;/denchmark-code&gt;

Failure is:
&lt;denchmark-code&gt;[07:03:53] src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!
terminate called after throwing an instance of 'dmlc::Error'
  what():  [07:03:59] /work/mxnet/3rdparty/mshadow/mshadow/./stream_gpu-inl.h:107: Check failed: err == CUBLAS_STATUS_SUCCESS (7 vs. 0) : Destory cublas handle failed
Stack trace:
  [bt] (0) /work/mxnet/python/mxnet/../../lib/libmxnet.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x2b) [0x7f0376aa865b]
  [bt] (1) /work/mxnet/python/mxnet/../../lib/libmxnet.so(void mshadow::DeleteStream&lt;mshadow::gpu&gt;(mshadow::Stream&lt;mshadow::gpu&gt;*)+0x227) [0x7f037aa308e7]
  [bt] (2) /work/mxnet/python/mxnet/../../lib/libmxnet.so(mshadow::Stream&lt;mshadow::gpu&gt;* mshadow::NewStream&lt;mshadow::gpu&gt;(bool, bool, int)+0x244) [0x7f037aa30e14]
  [bt] (3) /work/mxnet/python/mxnet/../../lib/libmxnet.so(void mxnet::engine::ThreadedEnginePerDevice::GPUWorker&lt;(dmlc::ConcurrentQueueType)0&gt;(mxnet::Context, bool, mxnet::engine::ThreadedEnginePerDevice::ThreadWorkerBlock&lt;(dmlc::ConcurrentQueueType)0&gt;*, std::shared_ptr&lt;dmlc::ManualEvent&gt; const&amp;)+0x19f) [0x7f037aa513ef]
  [bt] (4) /work/mxnet/python/mxnet/../../lib/libmxnet.so(std::_Function_handler&lt;void (std::shared_ptr&lt;dmlc::ManualEvent&gt;), mxnet::engine::ThreadedEnginePerDevice::PushToExecute(mxnet::engine::OprBlock*, bool)::{lambda()#4}::operator()() const::{lambda(std::shared_ptr&lt;dmlc::ManualEvent&gt;)#1}&gt;::_M_invoke(std::_Any_data const&amp;, std::shared_ptr&lt;dmlc::ManualEvent&gt;)+0x46) [0x7f037aa51626]
  [bt] (5) /work/mxnet/python/mxnet/../../lib/libmxnet.so(std::thread::_Impl&lt;std::_Bind_simple&lt;std::function&lt;void (std::shared_ptr&lt;dmlc::ManualEvent&gt;)&gt; (std::shared_ptr&lt;dmlc::ManualEvent&gt;)&gt; &gt;::_M_run()+0x44) [0x7f037aa3d1c4]
  [bt] (6) /usr/lib64/libstdc++.so.6(+0xb5070) [0x7f03e2478070]
  [bt] (7) /usr/lib64/libpthread.so.0(+0x7e65) [0x7f03f4f92e65]
  [bt] (8) /usr/lib64/libc.so.6(clone+0x6d) [0x7f03f45b288d]


/work/runtime_functions.sh: line 1312:     6 Aborted                 (core dumped) python3.6 -m "nose" $NOSE_COVERAGE_ARGUMENTS $NOSE_TIMER_ARGUMENTS --with-xunit --xunit-file nosetests_gpu.xml --verbose tests/python/gpu
2019-11-30 07:03:59,955 - root - INFO - Waiting for status of container ea33d765417a for 600 s.
2019-11-30 07:04:00,117 - root - INFO - Container exit status: {'StatusCode': 134, 'Error': None}
2019-11-30 07:04:00,117 - root - ERROR - Container exited with an error ðŸ˜ž
2019-11-30 07:04:00,117 - root - INFO - Executed command for reproduction:

ci/build.py --docker-registry mxnetci --nvidiadocker --platform centos7_gpu --docker-build-retries 3 --shm-size 500m /work/runtime_functions.sh unittest_centos7_gpu

2019-11-30 07:04:00,117 - root - INFO - Stopping container: ea33d765417a
2019-11-30 07:04:00,119 - root - INFO - Removing container: ea33d765417a
2019-11-30 07:04:00,140 - root - CRITICAL - Execution of ['/work/runtime_functions.sh', 'unittest_centos7_gpu'] failed with status: 134

&lt;/denchmark-code&gt;

A solution would be to update the AMI
	</description>
	<comments>
		<comment id='1' author='larroy' date='2019-11-30T07:10:13Z'>
		&lt;denchmark-link:https://github.com/mxnet-label-bot&gt;@mxnet-label-bot&lt;/denchmark-link&gt;
 add [CI]
		</comment>
		<comment id='2' author='larroy' date='2019-12-01T03:16:49Z'>
		For more info, I've made a change to print the cublaserror's error message out:
&lt;denchmark-code&gt;terminate called after throwing an instance of 'dmlc::Error'

  what():  [05:07:32] /work/mxnet/include/mshadow/./stream_gpu-inl.h:125: Check failed: err == CUBLAS_STATUS_SUCCESS (7 vs. 0) : Destory cublas handle failed with error CUBLAS_STATUS_INVALID_VALUE

Stack trace:

  [bt] (0) build/tests/mxnet_unit_tests(dmlc::LogMessageFatal::~LogMessageFatal()+0x32) [0x194a5f2]

  [bt] (1) build/tests/mxnet_unit_tests(mshadow::Stream&lt;mshadow::gpu&gt;::DestroyBlasHandle()+0x14f) [0x1985b2f]

  [bt] (2) build/tests/mxnet_unit_tests(void mshadow::DeleteStream&lt;mshadow::gpu&gt;(mshadow::Stream&lt;mshadow::gpu&gt;*)+0xb7) [0x1986617]

  [bt] (3) build/tests/mxnet_unit_tests(mshadow::Stream&lt;mshadow::gpu&gt;* mshadow::NewStream&lt;mshadow::gpu&gt;(bool, bool, int)+0x30b) [0x1986c4b]

  [bt] (4) build/tests/mxnet_unit_tests(mxnet::test::op::GPUStreamScope::GPUStreamScope(mxnet::OpContext*)+0xfd) [0x198888d]

  [bt] (5) build/tests/mxnet_unit_tests(std::__shared_ptr&lt;mxnet::test::op::CoreOpExecutor&lt;float, float&gt;, (__gnu_cxx::_Lock_policy)2&gt;::__shared_ptr&lt;std::allocator&lt;mxnet::test::op::CoreOpExecutor&lt;float, float&gt; &gt;, bool, std::vector&lt;mxnet::TShape, std::allocator&lt;mxnet::TShape&gt; &gt; &gt;(std::_Sp_make_shared_tag, std::allocator&lt;mxnet::test::op::CoreOpExecutor&lt;float, float&gt; &gt; const&amp;, bool&amp;&amp;, std::vector&lt;mxnet::TShape, std::allocator&lt;mxnet::TShape&gt; &gt;&amp;&amp;)+0x3c7) [0x19a1e57]

  [bt] (6) build/tests/mxnet_unit_tests(mxnet::test::OperatorRunner&lt;mxnet::test::op::CoreOpProp, mxnet::test::op::CoreOpExecutor&lt;float, float&gt; &gt;::RunGenericOperatorForward(bool, std::vector&lt;mxnet::TShape, std::allocator&lt;mxnet::TShape&gt; &gt; const&amp;, std::vector&lt;std::pair&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt;, std::allocator&lt;std::pair&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt; &gt; &gt; const&amp;, unsigned long)+0xb6) [0x19a8546]

  [bt] (7) build/tests/mxnet_unit_tests(ACTIVATION_PERF_ExecuteBidirectional_Test::TestBody()+0x74e) [0x197ebbe]

  [bt] (8) build/tests/mxnet_unit_tests(void testing::internal::HandleExceptionsInMethodIfSupported&lt;testing::Test, void&gt;(testing::Test*, void (testing::Test::*)(), char const*)+0x43) [0x1ab60d3]
&lt;/denchmark-code&gt;

The error type is CUBLAS_STATUS_INVALID_VALUE.
		</comment>
		<comment id='3' author='larroy' date='2019-12-01T05:04:16Z'>
		I think the error doesn't say much. I think the issue is the driver inside the docker image causes problems, at least I saw nvidia engineers acknowledging such an issue. If you see one of my PRs the failure goes away but some jobs require cuda libs in the container.
		</comment>
		<comment id='4' author='larroy' date='2019-12-06T19:46:34Z'>
		Fixed by &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/16968&gt;#16968&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>