<bug id='19463' author='sxjscience' open_date='2020-11-02T07:18:05Z' closed_time='2020-11-19T23:58:33Z'>
	<summary>[Bug][AMP][2.0] AMP issue of the concatenate operator</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

Reproducible example:
import mxnet as mx
from mxnet.gluon import nn
from mxnet import amp
mx.npx.set_np()
amp.init()


class Foo(nn.HybridBlock):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.dense0 = nn.Dense(16, in_units=8)

    def forward(self, x):
        return mx.np.concatenate([self.dense0(x), x], axis=-1)

foo = Foo()
foo.initialize(ctx=mx.gpu())

data = mx.np.random.normal(0, 1, (32, 8), ctx=mx.gpu())
out = foo(data)
print(out.dtype)
Output:
&lt;denchmark-code&gt;---------------------------------------------------------------------------
MXNetError                                Traceback (most recent call last)
&lt;ipython-input-9-07e1c93ce642&gt; in &lt;module&gt;
     18 
     19 data = mx.np.random.normal(0, 1, (32, 8), ctx=mx.gpu())
---&gt; 20 out = foo(data)
     21 print(out.dtype)

~/.local/lib/python3.6/site-packages/mxnet/gluon/block.py in __call__(self, x, *args)
   1419             if not self._active:
   1420                 # Normal imperative computation of forward()
-&gt; 1421                 return super().__call__(x, *args)
   1422 
   1423             if dc.is_deferred_compute():

~/.local/lib/python3.6/site-packages/mxnet/gluon/block.py in __call__(self, *args)
    709             hook(self, args)
    710 
--&gt; 711         out = self.forward(*args)
    712 
    713         for hook in self._forward_hooks.values():

&lt;ipython-input-9-07e1c93ce642&gt; in forward(self, x)
     12 
     13     def forward(self, x):
---&gt; 14         return mx.np.concatenate([self.dense0(x), x], axis=-1)
     15 
     16 foo = Foo()

~/.local/lib/python3.6/site-packages/mxnet/numpy/multiarray.py in concatenate(seq, axis, out)
   6520     array([1., 2., 3., 4., 5., 6.])
   6521     """
-&gt; 6522     return _mx_nd_np.concatenate(seq, axis=axis, out=out)
   6523 
   6524 

~/.local/lib/python3.6/site-packages/mxnet/ndarray/numpy/_op.py in concatenate(seq, axis, out)
   4522            [3., 4., 6.]])
   4523     """
-&gt; 4524     return _api_internal.concatenate(*seq, axis, out)
   4525 
   4526 

~/.local/lib/python3.6/site-packages/mxnet/_ffi/_ctypes/function.py in __call__(self, *args)
    113                 self.handle, values, tcodes, ctypes.c_int(num_args),
    114                 ctypes.byref(ret_val), ctypes.byref(ret_tcode)) != 0:
--&gt; 115             raise get_last_ffi_error()
    116         _ = temp_args
    117         _ = args

MXNetError: MXNetError: Type inconsistent, Provided = float32, inferred type = float16
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/mk-61&gt;@mk-61&lt;/denchmark-link&gt;
 Would you take a look on this? I'm not super familiar with the amp code so it may take me more time to resolve this issue.
	</description>
	<comments>
		<comment id='1' author='sxjscience' date='2020-11-03T23:49:13Z'>
		Removed the  because &lt;denchmark-link:https://github.com/mk-61&gt;@mk-61&lt;/denchmark-link&gt;
 is able to reproduce the issue per offline discussion.
		</comment>
	</comments>
</bug>