<bug id='19043' author='sxjscience' open_date='2020-08-30T03:34:31Z' closed_time='2020-09-01T02:09:59Z'>
	<summary>[Numpy][Bug] The gradient of np.pad is wrong!</summary>
	<description>
The gradient of np.pad is wrong. See the following reproducible example:
MXNet Implementation:
import mxnet as mx
mx.npx.set_np()

ctx = mx.gpu()
a = mx.np.ones((3, 3, 3), ctx=ctx)
mult = np.random.normal(0, 1, (3, 3, 3))
a.attach_grad()
with mx.autograd.record():
    b = mx.np.pad(a[:, 1:], ((0, 0), (0, 1), (0, 0))) * mx.np.array(mult, ctx=ctx)
    b = b.sum()
b.backward()
print(a.grad)
Output:
&lt;denchmark-code&gt;[[[0. 0. 0.]
  [1. 1. 1.]
  [1. 1. 1.]]

 [[0. 0. 0.]
  [1. 1. 1.]
  [1. 1. 1.]]

 [[0. 0. 0.]
  [1. 1. 1.]
  [1. 1. 1.]]] @gpu(0)
&lt;/denchmark-code&gt;

Jax Implementation:
from jax import grad
import jax.numpy as jnp
import numpy as np
mult = np.random.normal(0, 1, (3, 3, 3))

a = jnp.ones((3, 3, 3))

def f(x):
    b = jnp.pad(x[:, 1:], ((0, 0), (0, 1), (0, 0))) * jnp.array(mult)
    return b.sum()
print(grad(f)(a))
Output:
&lt;denchmark-code&gt;[[[ 0.          0.          0.        ]
  [ 0.3545383  -0.84326786 -0.31482664]
  [ 1.0994871  -1.230104    2.8007567 ]]

 [[ 0.          0.          0.        ]
  [ 1.0447861  -0.16119051 -0.39860427]
  [-0.7756538   0.5314936   1.4601654 ]]

 [[ 0.          0.          0.        ]
  [ 0.37878916 -2.0777514   0.96676654]
  [ 0.45230922  0.3094176  -0.43687683]]]
&lt;/denchmark-code&gt;

Basically, the following line is not correct:



incubator-mxnet/src/operator/numpy/np_pad_op-inl.h


        Lines 544 to 551
      in
      b0c39f7






 template &lt;typename xpu, int req&gt; 



 struct pad_grad { 



 template&lt;typename DType&gt; 



   MSHADOW_XINLINE static void Map(index_t i, DType *out, const DType *a){ 



 using namespace mxnet_op; 



 KERNEL_ASSIGN(out[i], req, 1); 



   } 



 }; 





We should change that to
&lt;denchmark-code&gt; KERNEL_ASSIGN(out[i], req, a[i]);
&lt;/denchmark-code&gt;

In addition, I do not know why we need the . &lt;denchmark-link:https://github.com/CassiniXu&gt;@CassiniXu&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='sxjscience' date='2020-08-30T03:34:57Z'>
		The bug causes the training issue of MobileBERT in GluonNLP: &lt;denchmark-link:https://github.com/dmlc/gluon-nlp/issues/1322&gt;dmlc/gluon-nlp#1322&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>