<bug id='15674' author='YouhuiBai' open_date='2019-07-27T08:26:35Z' closed_time='2020-08-31T05:59:05Z'>
	<summary>Straggler in latest mxnet when training with distributed parameter server</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

Hi, I found that there is a strange straggler in current newest mxnet when training CNN using distributed parameter server architecture (BSP model) and GPU, it is a special worker whose rank == 0. I think it is a bug of mxnet , because I deployed mxnet in hemogeneous environment means that every participated machine has the same hardware and software environment as follows, and the straggler still existed even I changed the number of workers or physical machine like running at AWS.
&lt;denchmark-h:h2&gt;Environment info (Required)&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;system version：CentOS 7.5.1804
kernel version：3.10.0-862.9.1.el7.x86_64
cuda version：cuda_9.2.148
cudnn version：cudnn-9.2-linux-x64-v7.1
nvidia driver version：396.37
GPU: GeForce GTX 1080 Ti
NIC: 10GE

&lt;/denchmark-code&gt;

Software and parameters:
&lt;denchmark-code&gt;parameter server architecture: m servers n workers, n &gt;= m and each role locates on different physical machine
application: image classification
database: Imagenet 2012
CNN model: inception-v4, lenet, resnet, etc.
GPU usage: one physical GPU per worker
scaling model: strong scaling
consistency model: BSP
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;what is a straggler?&lt;/denchmark-h&gt;

When I start training with above environment and parameter set up, and did some break down in the critical path, found that a worker's behavior is strange. In BSP consistency model of parameter server, the server would not execute response of one key for push operations unless receiving updates from all workers to the same key, we found that there was a slower worker, always waited by other workers every iteration, it is the straggler. The straggler has other features:

rank == 0, the first worker
higher CPU usage
higher CPU memory throughput
higher GPU usage
cost more time when calling cudamemcopy
higher LLC miss rate
lower CPU memory occupancy

It's very very strange. Thanks a lot.
	</description>
	<comments>
		<comment id='1' author='YouhuiBai' date='2019-07-27T08:26:38Z'>
		Hey, this is the MXNet Label Bot.
Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it.
Here are my recommended labels: Bug
		</comment>
		<comment id='2' author='YouhuiBai' date='2019-07-27T08:28:08Z'>
		The reason for the straggler is the reutilization of pageable memory.
There is an unorder map int to NDArray comm_buf_ (class KVStoreDist, in kvstore_dist.h), if the compression isn't active, this buffer store the data in pull and push, namely, push pull will share the buffer. When start the distributed kvstore, the first worker whose rank number is zero will push all keys of the model to server(s), and initialize the comm_buf_ with pageable allocated memory (in push_ function of KVStoreDist), then all workers pull keys from server(s) ultil the first worker's push operations have been done, and initialize the comm_buf_ with new allocated pinned (page-locked) memory. As we can see, the comm_buf_ of first worker is different from others', and the memory copy between CPU and GPU is very different with pinned memory or pageable memory at host, the details are in &lt;denchmark-link:https://devblogs.nvidia.com/how-optimize-data-transfers-cuda-cc/&gt;https://devblogs.nvidia.com/how-optimize-data-transfers-cuda-cc/&lt;/denchmark-link&gt;
. This discovery will explain all the strange point above, and you can just allocate new pinned memory for comm_buf_ of first worker, the straggler will be eliminated.
		</comment>
		<comment id='3' author='YouhuiBai' date='2019-08-02T17:38:47Z'>
		FYI - &lt;denchmark-link:https://github.com/apeforest&gt;@apeforest&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/yuxihu&gt;@yuxihu&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='YouhuiBai' date='2019-08-27T18:43:48Z'>
		I think the reason is when using cpu pinned memory, the pinned device id is default to 0. The caller will need to pass in the device id: &lt;denchmark-link:http://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=cpu%20pinned&gt;http://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=cpu%20pinned&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='YouhuiBai' date='2019-08-28T01:43:27Z'>
		&lt;denchmark-link:https://github.com/apeforest&gt;@apeforest&lt;/denchmark-link&gt;
 I think it's probably not related to the Dataloader,  the pinned memory is used at worker to communicate with server through parameter server, all the workers use the pinned memory except the worker whose rank id is zero.
		</comment>
		<comment id='6' author='YouhuiBai' date='2019-09-05T17:47:53Z'>
		&lt;denchmark-link:https://github.com/YouhuiBai&gt;@YouhuiBai&lt;/denchmark-link&gt;
 Thanks for the explanation.When you say "current newest mxnet" has straggler, are you   implying there was no such behavior before? If so, do you remember the last working version? I can do a trace back to see which PR introduced this.
		</comment>
		<comment id='7' author='YouhuiBai' date='2019-09-17T00:38:02Z'>
		&lt;denchmark-link:https://github.com/apeforest&gt;@apeforest&lt;/denchmark-link&gt;
 Very sorry, I forgot to check the gmail because it is uncommonly used. I used branch 1.5.0 ("current newest mxnet" as I said), and checked from version 1.0.0 to  branch, there is straggler. What can we do is just allocate the pinned memory for  rather than reuse the , which is pageable memory. Thanks a lot!
		</comment>
	</comments>
</bug>