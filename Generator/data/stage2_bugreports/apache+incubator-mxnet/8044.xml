<bug id='8044' author='goswamig' open_date='2017-09-26T19:38:17Z' closed_time='2018-07-10T18:31:07Z'>
	<summary>Fix the Test  test_operator_gpu.test_batchnorm_training</summary>
	<description>
For bugs or installation issues, please provide the following information.
The more information you provide, the more likely people will be able to help you.
&lt;denchmark-h:h2&gt;Environment info&lt;/denchmark-h&gt;

Operating System: linux
Compiler: gcc
Package used (Python/R/Scala/Julia): python
MXNet version: master
Or if installed from source:
MXNet commit hash (git rev-parse HEAD):
If you are using python package, please provide 2 and 3
Python version and distribution:
If you are using R package, please provide
R sessionInfo():
&lt;denchmark-h:h2&gt;Error Message:&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;======================================================================
FAIL: test_operator_gpu.test_batchnorm_training&lt;/denchmark-h&gt;

Traceback (most recent call last):
File "C:\Anaconda3\envs\py2\lib\site-packages\nose\case.py", line 197, in runTest
self.test(*self.arg)
File "C:\Anaconda3\envs\py2\lib\site-packages\nose\util.py", line 620, in newfunc
return func(*arg, **kw)
File "c:\jenkins_slave\workspace\ut-python-gpu\tests\python\gpu../unittest\test_operator.py", line 969, in test_batchnorm_training
check_batchnorm_training(stype)
File "c:\jenkins_slave\workspace\ut-python-gpu\tests\python\gpu../unittest\test_operator.py", line 921, in check_batchnorm_training
check_numeric_gradient(test, in_location, mean_std, numeric_eps=1e-2, rtol=0.16, atol=1e-4)
File "c:\jenkins_slave\workspace\ut-python-gpu\pkg_vc14_gpu\python\mxnet\test_utils.py", line 880, in check_numeric_gradient
("NUMERICAL_%s"%name, "BACKWARD_%s"%name))
File "c:\jenkins_slave\workspace\ut-python-gpu\pkg_vc14_gpu\python\mxnet\test_utils.py", line 467, in assert_almost_equal
raise AssertionError(msg)
AssertionError:
Items are not equal:
Error 2.211290 exceeds tolerance rtol=0.160000, atol=0.000100.  Location of maximum error:(1, 1, 0, 0), a=-0.000764, b=-0.000401
NUMERICAL_data: array([[[[-3.25536728, -1.11701787],
[ 0.21415949,  3.06491852]],
...
BACKWARD_data: array([[[[-3.25574946, -1.11719143],
[ 0.2142154 ,  3.06527352]],
...
&lt;denchmark-h:h2&gt;Minimum reproducible example&lt;/denchmark-h&gt;

run  test_operator_gpu.test_batchnorm_training
&lt;denchmark-h:h2&gt;Steps to reproduce&lt;/denchmark-h&gt;

or if you are running standard examples, please provide the commands you have run that lead to the error.





&lt;denchmark-h:h2&gt;What have you tried to solve it?&lt;/denchmark-h&gt;






	</description>
	<comments>
		<comment id='1' author='goswamig' date='2018-06-27T18:02:48Z'>
		running test:
&lt;denchmark-code&gt; nosetests  --verbose -s test_operator_gpu.py:test_batchnorm_training 
&lt;/denchmark-code&gt;

gives the following error
&lt;denchmark-code&gt;/usr/local/lib/python3.5/dist-packages/nose/util.py:453: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead
  inspect.getargspec(func)
[INFO] Setting module np/mx/python random seeds, use MXNET_MODULE_SEED=1115529943 to reproduce.
test_operator_gpu.test_batchnorm_training ... [17:58:38] src/operator/nn/./../../common/utils.h:417: 
Storage type fallback detected:
operator = BatchNorm_v1
input storage types = [row_sparse, row_sparse, row_sparse, row_sparse, row_sparse, ]
output storage types = [default, default, default, ]
params = {"fix_gamma" : True, }
context.dev_mask = gpu
The operator with default storage type will be dispatched for execution. You're seeing this warning message because the operator above is unable to process the given ndarrays with specified storage types, context and parameter. Temporary dense ndarrays are generated in order to execute the operator. This does not affect the correctness of the programme. You can set environment variable MXNET_STORAGE_FALLBACK_LOG_VERBOSE to 0 to suppress this warning.
[17:58:38] src/operator/nn/./../../common/utils.h:417: 
Storage type fallback detected:
operator = _backward_BatchNorm_v1
input storage types = [default, default, default, row_sparse, row_sparse, row_sparse, row_sparse, ]
output storage types = [default, default, default, ]
params = {"fix_gamma" : True, }
context.dev_mask = gpu
The operator with default storage type will be dispatched for execution. You're seeing this warning message because the operator above is unable to process the given ndarrays with specified storage types, context and parameter. Temporary dense ndarrays are generated in order to execute the operator. This does not affect the correctness of the programme. You can set environment variable MXNET_STORAGE_FALLBACK_LOG_VERBOSE to 0 to suppress this warning.
[17:58:38] src/ndarray/./../operator/tensor/.././../common/utils.h:417: 
Storage fallback detected:
Copy from default storage type on gpu to row_sparse storage type on gpu.
A temporary ndarray with row_sparse storage type will be generated in order to perform the copy. This does not affect the correctness of the programme. You can set environment variable MXNET_STORAGE_FALLBACK_LOG_VERBOSE to 0 to suppress this warning.
/home/ubuntu/incubator-mxnet/python/mxnet/ndarray/sparse.py:696: RuntimeWarning: Assigning non-NDArray object to RowSparseNDArray is not efficient
  RuntimeWarning)
[INFO] 1 of 100000: Setting test np/mx/python random seeds, use MXNET_TEST_SEED=189902078 to reproduce.
ERROR

======================================================================
ERROR: test_operator_gpu.test_batchnorm_training
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/nose/case.py", line 198, in runTest
    self.test(*self.arg)
  File "/usr/local/lib/python3.5/dist-packages/nose/util.py", line 620, in newfunc
    return func(*arg, **kw)
  File "/home/ubuntu/incubator-mxnet/tests/python/gpu/../unittest/common.py", line 157, in test_new
    orig_test(*args, **kwargs)
  File "/home/ubuntu/incubator-mxnet/tests/python/gpu/../unittest/test_operator.py", line 1509, in test_batchnorm_training
    check_batchnorm_training(stype)
  File "/home/ubuntu/incubator-mxnet/tests/python/gpu/../unittest/test_operator.py", line 1449, in check_batchnorm_training
    check_numeric_gradient(test, in_location, mean_std, numeric_eps=1e-2, rtol=0.16, atol=1e-4)
  File "/home/ubuntu/incubator-mxnet/python/mxnet/test_utils.py", line 902, in check_numeric_gradient
    symbolic_grads = {k:executor.grad_dict[k].asnumpy() for k in grad_nodes}
  File "/home/ubuntu/incubator-mxnet/python/mxnet/test_utils.py", line 902, in &lt;dictcomp&gt;
    symbolic_grads = {k:executor.grad_dict[k].asnumpy() for k in grad_nodes}
  File "/home/ubuntu/incubator-mxnet/python/mxnet/ndarray/ndarray.py", line 1894, in asnumpy
    ctypes.c_size_t(data.size)))
  File "/home/ubuntu/incubator-mxnet/python/mxnet/base.py", line 210, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [17:58:38] src/operator/tensor/././cast_storage-inl.cuh:108: Check failed: ctx.requested.size() &gt; 0 (0 vs. 0) 

Stack trace returned 10 entries:
[bt] (0) /home/ubuntu/incubator-mxnet/python/mxnet/../../lib/libmxnet.so(dmlc::StackTrace[abi:cxx11]()+0x5b) [0x7f3cb19aefdb]
[bt] (1) /home/ubuntu/incubator-mxnet/python/mxnet/../../lib/libmxnet.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x28) [0x7f3cb19afb48]
[bt] (2) /home/ubuntu/incubator-mxnet/python/mxnet/../../lib/libmxnet.so(void mxnet::op::CastStorageDnsRspGPUImpl_&lt;float, long&gt;(mxnet::OpContext const&amp;, mxnet::TBlob const&amp;, mxnet::NDArray*)+0x450) [0x7f3cb509b050]
[bt] (3) /home/ubuntu/incubator-mxnet/python/mxnet/../../lib/libmxnet.so(mxnet::op::CastStorageDnsRspImpl(mxnet::OpContext const&amp;, mshadow::gpu const&amp;, mxnet::TBlob const&amp;, mxnet::NDArray*)+0x58e) [0x7f3cb50a36fe]
[bt] (4) /home/ubuntu/incubator-mxnet/python/mxnet/../../lib/libmxnet.so(void mxnet::op::CastStorageComputeImpl&lt;mshadow::gpu&gt;(mxnet::OpContext const&amp;, mxnet::NDArray const&amp;, mxnet::NDArray const&amp;)+0x233) [0x7f3cb50b9473]
[bt] (5) /home/ubuntu/incubator-mxnet/python/mxnet/../../lib/libmxnet.so(mxnet::common::CastNonDefaultStorage(std::vector&lt;mxnet::NDArray, std::allocator&lt;mxnet::NDArray&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray, std::allocator&lt;mxnet::NDArray&gt; &gt; const&amp;, mxnet::OpContext const&amp;, bool)+0x267) [0x7f3cb3f7d177]
[bt] (6) /home/ubuntu/incubator-mxnet/python/mxnet/../../lib/libmxnet.so(mxnet::exec::FComputeExecutor::Run(mxnet::RunContext, bool)+0x71) [0x7f3cb3f855d1]
[bt] (7) /home/ubuntu/incubator-mxnet/python/mxnet/../../lib/libmxnet.so(+0x31b5a10) [0x7f3cb3f31a10]
[bt] (8) /home/ubuntu/incubator-mxnet/python/mxnet/../../lib/libmxnet.so(mxnet::engine::ThreadedEngine::ExecuteOprBlock(mxnet::RunContext, mxnet::engine::OprBlock*)+0x8e5) [0x7f3cb45ca605]
[bt] (9) /home/ubuntu/incubator-mxnet/python/mxnet/../../lib/libmxnet.so(void mxnet::engine::ThreadedEnginePerDevice::GPUWorker&lt;(dmlc::ConcurrentQueueType)0&gt;(mxnet::Context, bool, mxnet::engine::ThreadedEnginePerDevice::ThreadWorkerBlock&lt;(dmlc::ConcurrentQueueType)0&gt;*, std::shared_ptr&lt;dmlc::ManualEvent&gt; const&amp;)+0xeb) [0x7f3cb45e179b]


-------------------- &gt;&gt; begin captured logging &lt;&lt; --------------------
common: INFO: Setting module np/mx/python random seeds, use MXNET_MODULE_SEED=1115529943 to reproduce.
common: INFO: 1 of 100000: Setting test np/mx/python random seeds, use MXNET_TEST_SEED=189902078 to reproduce.
--------------------- &gt;&gt; end captured logging &lt;&lt; ---------------------

----------------------------------------------------------------------
Ran 1 test in 2.627s

FAILED (errors=1)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='goswamig' date='2018-06-27T18:23:48Z'>
		unittest works after removing 'sparse' in stype. &lt;denchmark-link:https://github.com/haojin2&gt;@haojin2&lt;/denchmark-link&gt;
  please create another issue for this. Thanks!
		</comment>
		<comment id='3' author='goswamig' date='2018-06-27T19:32:09Z'>
		there'll will be an issue for tracking all ops affected by MKLDNN.
		</comment>
		<comment id='4' author='goswamig' date='2018-06-29T22:38:14Z'>
		issue causing the ctx error is tracked at &lt;denchmark-link:https://github.com/apache/incubator-mxnet/issues/11448&gt;#11448&lt;/denchmark-link&gt;

working on flakiness:
tried changing numeric_eps=1e-3
using following will reproduce the error
&lt;denchmark-code&gt;MXNET_TEST_SEED=335962305 nosetests -s --verbose test_operator_gpu.py:test_batchnorm_training
&lt;/denchmark-code&gt;

error
&lt;denchmark-code&gt;======================================================================
FAIL: test_operator_gpu.test_batchnorm_training
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/nose/case.py", line 198, in runTest
    self.test(*self.arg)
  File "/usr/local/lib/python3.5/dist-packages/nose/util.py", line 620, in newfunc
    return func(*arg, **kw)
  File "/home/ubuntu/incubator-mxnet/tests/python/gpu/../unittest/common.py", line 157, in test_new
    orig_test(*args, **kwargs)
  File "/home/ubuntu/incubator-mxnet/tests/python/gpu/../unittest/test_operator.py", line 1510, in test_batchnorm_training
    check_batchnorm_training(stype)
  File "/home/ubuntu/incubator-mxnet/tests/python/gpu/../unittest/test_operator.py", line 1462, in check_batchnorm_training
    check_numeric_gradient(test, in_location, mean_std, numeric_eps=1e-3, rtol=0.16, atol=1e-4)
  File "/usr/local/lib/python3.5/dist-packages/mxnet/test_utils.py", line 914, in check_numeric_gradient
    ("NUMERICAL_%s"%name, "BACKWARD_%s"%name))
  File "/usr/local/lib/python3.5/dist-packages/mxnet/test_utils.py", line 493, in assert_almost_equal
    raise AssertionError(msg)
AssertionError: 
Items are not equal:
Error 1.203853 exceeds tolerance rtol=0.160000, atol=0.000100.  Location of maximum error:(1, 2, 1, 1), a=-0.006936, b=-0.008740
 NUMERICAL_data: array([[[[-3.2685995 ,  4.8455296 ],
         [ 2.1361113 , -0.71915984]],
...
 BACKWARD_data: array([[[[-3.2687469 ,  4.845755  ],
         [ 2.135889  , -0.71962786]],
...
-------------------- &gt;&gt; begin captured logging &lt;&lt; --------------------
common: INFO: Setting module np/mx/python random seeds, use MXNET_MODULE_SEED=262813092 to reproduce.
common: WARNING: *** test-level seed set: all "@with_seed()" tests run deterministically ***
common: INFO: Setting test np/mx/python random seeds, use MXNET_TEST_SEED=335962305 to reproduce.
--------------------- &gt;&gt; end captured logging &lt;&lt; ---------------------

----------------------------------------------------------------------
Ran 1 test in 2.915s

FAILED (failures=1)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='goswamig' date='2018-07-02T17:55:44Z'>
		Trying higher precision fp64 and different numeric_eps
&lt;denchmark-code&gt;dtype=np.float64
numeric_eps=1e-3
atol=1e-3
rtol=0.16
&lt;/denchmark-code&gt;

with following seed:
&lt;denchmark-code&gt;MXNET_TEST_SEED=1679967765  nosetests -s --verbose test_operator_gpu.py:test_batchnorm_training
&lt;/denchmark-code&gt;

error message:
&lt;denchmark-code&gt;AssertionError: 
Items are not equal:
Error 1.458948 exceeds tolerance rtol=0.160000, atol=0.001000.  Location of maximum error:(0, 1, 1, 0), a=-0.003532, b=-0.001680
 NUMERICAL_data: array([[[[-3.7802458 ,  1.0998845 ],
         [-2.3617148 , -1.0118484 ]],
...
 BACKWARD_data: array([[[[-3.7802837 ,  1.0991578 ],
         [-2.3593516 , -1.0117426 ]],
...
&lt;/denchmark-code&gt;

changing numeric_eps to 1e-2 fix the problem
&lt;denchmark-code&gt;numeric_eps=1e-2
&lt;/denchmark-code&gt;

However
running config:
&lt;denchmark-code&gt;dtype=float64
numeric_eps=1e-2
atol=1e-3
rtol=0.16
&lt;/denchmark-code&gt;

using seed:
&lt;denchmark-code&gt;MXNET_TEST_SEED=184308247 nosetests -s --verbose gpu/test_operator_gpu.py:test_batchnorm_training
&lt;/denchmark-code&gt;

gives:
&lt;denchmark-code&gt;AssertionError: 
Items are not equal:
Error 1.007613 exceeds tolerance rtol=0.160000, atol=0.001000.  Location of maximum error:(0, 2, 1, 0), a=-0.006804, b=-0.004992
 NUMERICAL_data: array([[[[-0.0189662 , -2.474314  ],
         [-0.8934498 ,  1.12257   ]],
...
 BACKWARD_data: array([[[[-0.0196085 , -2.474647  ],
         [-0.8938365 ,  1.1222763 ]],
...
&lt;/denchmark-code&gt;

changing
&lt;denchmark-code&gt;numeric_eps=1e-3
&lt;/denchmark-code&gt;

fix the problem
		</comment>
		<comment id='6' author='goswamig' date='2018-07-03T18:59:17Z'>
		Comparing result from GPU with cuDNN, GPU without cuDNN (setting cudnn_off=True in mx.sym.BatchNorm), and CPU (unittest/test_operator.py)
The results of different implementations of batchnorm are consistent and should be correct.
Config:
&lt;denchmark-code&gt;dtype=np.float32
numeric_eps=1e-2
atol=1e-4
rtol=0.16 
&lt;/denchmark-code&gt;

Seed:
&lt;denchmark-code&gt;MXNET_TEST_SEED=1825397829 nosetests -s --verbose gpu/test_operator_gpu.py:test_batchnorm_training

&lt;/denchmark-code&gt;

With  cuDNN:
&lt;denchmark-code&gt;Error 1.725329 exceeds tolerance rtol=0.160000, atol=0.000100.  Location of maximum error:(1, 2, 1, 1), a=-0.001319, b=-0.002060
 NUMERICAL_data: array([[[[-2.1502256 ,  3.3022761 ],
         [-6.034297  ,  4.7290564 ]],
...
 BACKWARD_data: array([[[[-2.1509824 ,  3.3031416 ],
         [-6.035325  ,  4.7293053 ]],
&lt;/denchmark-code&gt;

Without cuDNN:
&lt;denchmark-code&gt;Error 1.742179 exceeds tolerance rtol=0.160000, atol=0.000100.  Location of maximum error:(1, 2, 1, 1), a=-0.001311, b=-0.002060
 NUMERICAL_data: array([[[[-2.150321  ,  3.3023834 ],
         [-6.0342016 ,  4.728958  ]],
...
 BACKWARD_data: array([[[[-2.1509795 ,  3.3031416 ],
         [-6.0353217 ,  4.7293077 ]],
&lt;/denchmark-code&gt;

CPU:
&lt;denchmark-code&gt;Error 1.798323 exceeds tolerance rtol=0.160000, atol=0.000100.  Location of maximum error:(1, 2, 1, 1), a=-0.001287, b=-0.002060
 NUMERICAL_data: array([[[[-2.150312  ,  3.3022494 ],
         [-6.034231  ,  4.7288475 ]],
...
 BACKWARD_data: array([[[[-2.15098   ,  3.3031418 ],
         [-6.035323  ,  4.7293077 ]],
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='goswamig' date='2018-07-03T20:18:55Z'>
		&lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/11544&gt;#11544&lt;/denchmark-link&gt;
 should fix this
		</comment>
		<comment id='8' author='goswamig' date='2018-07-10T18:31:07Z'>
		Fixed.
		</comment>
	</comments>
</bug>