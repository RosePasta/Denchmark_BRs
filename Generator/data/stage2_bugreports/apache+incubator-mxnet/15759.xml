<bug id='15759' author='sxjscience' open_date='2019-08-06T00:16:21Z' closed_time='2019-09-05T18:33:47Z'>
	<summary>[Optimizer][Bug] Gradient is mutated in the Adam optimizer</summary>
	<description>
In the implementation of Adam: grad, mean and var are all mutated (See &lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/src/operator/optimizer_op-inl.h#L1307-L1315&gt;https://github.com/apache/incubator-mxnet/blob/master/src/operator/optimizer_op-inl.h#L1307-L1315&lt;/denchmark-link&gt;
). However, the  flag is only set to {2, 3}, which should be {1, 2, 3}. (See &lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/src/operator/optimizer_op.cc#L699&gt;https://github.com/apache/incubator-mxnet/blob/master/src/operator/optimizer_op.cc#L699&lt;/denchmark-link&gt;
)
To reproduce the bug, you may check the value of the gradient before/after  &lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/optimizer/optimizer.py#L1226-L1227&gt;https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/optimizer/optimizer.py#L1226-L1227&lt;/denchmark-link&gt;
 .
We can add the following into optimizer.py
grad1 = grad.asnumpy()
adam_update(weight, grad, mean, var, out=weight, ...)
grad2 = grad.asnumpy()
import numpy as np
np.testing.assert_allclose(grad1, grad2)
Create an adam optimizer with wd=1E-3 and train any model. You will find that the gradient has been mutated.
	</description>
	<comments>
		<comment id='1' author='sxjscience' date='2019-08-06T00:16:23Z'>
		Hey, this is the MXNet Label Bot.
Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it.
Here are my recommended labels: Bug
		</comment>
		<comment id='2' author='sxjscience' date='2019-08-07T14:30:46Z'>
		On looking at other optimizer's updater,
These are the cases where original grad is being updated.
Will update the PR accordingly.
RMSPropAlexUpdate



incubator-mxnet/src/operator/optimizer_op-inl.h


        Lines 1600 to 1619
      in
      7186123






 inline void RMSPropAlexUpdate(const nnvm::NodeAttrs &amp;attrs, 



 const OpContext &amp;ctx, 



 const std::vector&lt;TBlob&gt; &amp;inputs, 



 const std::vector&lt;OpReqType&gt; &amp;req, 



 const std::vector&lt;TBlob&gt; &amp;outputs) { 



 using namespace mshadow; 



 using namespace mshadow::expr; 



 using namespace mshadow_op; 



 const RMSPropAlexParam &amp;param = nnvm::get&lt;RMSPropAlexParam&gt;(attrs.parsed); 



   Stream&lt;xpu&gt; *s = ctx.get_stream&lt;xpu&gt;(); 



 MSHADOW_REAL_TYPE_SWITCH(inputs[0].type_flag_, DType, { 



     Tensor&lt;xpu, 2, DType&gt; weight = inputs[0].FlatTo2D&lt;xpu, DType&gt;(s); 



     Tensor&lt;xpu, 2, DType&gt; grad = inputs[1].FlatTo2D&lt;xpu, DType&gt;(s); 



     Tensor&lt;xpu, 2, DType&gt; state_n = inputs[2].FlatTo2D&lt;xpu, DType&gt;(s); 



     Tensor&lt;xpu, 2, DType&gt; state_g = inputs[3].FlatTo2D&lt;xpu, DType&gt;(s); 



     Tensor&lt;xpu, 2, DType&gt; delta = inputs[4].FlatTo2D&lt;xpu, DType&gt;(s); 



     Tensor&lt;xpu, 2, DType&gt; out = outputs[0].FlatTo2D&lt;xpu, DType&gt;(s); 



 



     grad = scalar&lt;DType&gt;(param.rescale_grad) * grad + 



            scalar&lt;DType&gt;(param.wd) * weight; 





RMSPropUpdate



incubator-mxnet/src/operator/optimizer_op-inl.h


        Lines 1691 to 1708
      in
      7186123






 template &lt;typename xpu&gt; 



 inline void RMSPropUpdate(const nnvm::NodeAttrs &amp;attrs, const OpContext &amp;ctx, 



 const std::vector&lt;TBlob&gt; &amp;inputs, 



 const std::vector&lt;OpReqType&gt; &amp;req, 



 const std::vector&lt;TBlob&gt; &amp;outputs) { 



 using namespace mshadow; 



 using namespace mshadow::expr; 



 using namespace mshadow_op; 



 const RMSPropParam &amp;param = nnvm::get&lt;RMSPropParam&gt;(attrs.parsed); 



   Stream&lt;xpu&gt; *s = ctx.get_stream&lt;xpu&gt;(); 



 MSHADOW_REAL_TYPE_SWITCH(inputs[0].type_flag_, DType, { 



     Tensor&lt;xpu, 2, DType&gt; weight = inputs[0].FlatTo2D&lt;xpu, DType&gt;(s); 



     Tensor&lt;xpu, 2, DType&gt; grad = inputs[1].FlatTo2D&lt;xpu, DType&gt;(s); 



     Tensor&lt;xpu, 2, DType&gt; state_n = inputs[2].FlatTo2D&lt;xpu, DType&gt;(s); 



     Tensor&lt;xpu, 2, DType&gt; out = outputs[0].FlatTo2D&lt;xpu, DType&gt;(s); 



 



     grad = scalar&lt;DType&gt;(param.rescale_grad) * grad + 



            scalar&lt;DType&gt;(param.wd) * weight; 





FtrlUpdate



incubator-mxnet/src/operator/optimizer_op-inl.h


        Lines 1784 to 1803
      in
      7186123






 template&lt;typename xpu&gt; 



 inline void FtrlUpdate(const nnvm::NodeAttrs&amp; attrs, 



 const OpContext &amp;ctx, 



 const std::vector&lt;TBlob&gt; &amp;inputs, 



 const std::vector&lt;OpReqType&gt; &amp;req, 



 const std::vector&lt;TBlob&gt; &amp;outputs) { 



 using namespace mshadow; 



 using namespace mshadow::expr; 



 using namespace mshadow_op; 



 const FtrlParam&amp; param = nnvm::get&lt;FtrlParam&gt;(attrs.parsed); 



   Stream&lt;xpu&gt;* s = ctx.get_stream&lt;xpu&gt;(); 



 MSHADOW_REAL_TYPE_SWITCH(inputs[0].type_flag_, DType, { 



     Tensor&lt;xpu, 2, DType&gt; weight = inputs[0].FlatTo2D&lt;xpu, DType&gt;(s); 



     Tensor&lt;xpu, 2, DType&gt; grad = inputs[1].FlatTo2D&lt;xpu, DType&gt;(s); 



     Tensor&lt;xpu, 2, DType&gt; z = inputs[2].FlatTo2D&lt;xpu, DType&gt;(s); 



     Tensor&lt;xpu, 2, DType&gt; n = inputs[3].FlatTo2D&lt;xpu, DType&gt;(s); 



     Tensor&lt;xpu, 2, DType&gt; out = outputs[0].FlatTo2D&lt;xpu, DType&gt;(s); 



 



     grad = scalar&lt;DType&gt;(param.rescale_grad) * grad; 



 





		</comment>
		<comment id='3' author='sxjscience' date='2019-09-05T18:34:58Z'>
		Thanks &lt;denchmark-link:https://github.com/kshitij12345&gt;@kshitij12345&lt;/denchmark-link&gt;
 !
		</comment>
	</comments>
</bug>