<bug id='12823' author='wkcn' open_date='2018-10-14T15:03:52Z' closed_time='2018-10-22T15:59:18Z'>
	<summary>Is it necessary to wait CUDA stream when calling `WaitToRead` or `WaitToWrite`?</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

Hi! there.
I found a problem about the asynchronous execution.
In the two functions &lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/include/mxnet/ndarray.h#L335&gt;NDArray::WaitToRead&lt;/denchmark-link&gt;
 and &lt;denchmark-link:WaitToWrite&gt;NDArray::WaitToWrite&lt;/denchmark-link&gt;
, there is no any statement to wait the CUDA stream to finish.
It means that the task pushed before calling the two functions may start to execute . But the task before calling the two functions should have executed .
In the PR &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/12047&gt;[MXNET-779]Add DLPack Transformation API&lt;/denchmark-link&gt;
 I submitted,
&lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/ndarray/ndarray.py#L3980&gt;[Code]python/mxnet/ndarray/ndarray.py#L3980&lt;/denchmark-link&gt;

def to_dlpack_for_write(data):
    """Returns a reference view of NDArray that represents as DLManagedTensor until
       all previous read/write operations on the current array are finished.
    Parameters
    ----------
    data: NDArray
        input data.
    Returns
    -------
    PyCapsule (the pointer of DLManagedTensor)
        a reference view of NDArray that represents as DLManagedTensor.
    """
    check_call(_LIB.MXNDArrayWaitToWrite(data.handle))
    dlpack = DLPackHandle()
    check_call(_LIB.MXNDArrayToDLPack(data.handle, ctypes.byref(dlpack)))
    return ctypes.pythonapi.PyCapsule_New(dlpack, _c_str_dltensor, _c_dlpack_deleter)
After calling MXNDArrayWaitToWrite, there may be some task on the CUDA stream because WaitToWrite and WaitToRead don't wait the CUDA stream to finish. So the data in the DLPack may be wrong.
&lt;denchmark-h:h2&gt;Environment info (Required)&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;What to do:
1. Download the diagnosis script from https://raw.githubusercontent.com/apache/incubator-mxnet/master/tools/diagnose.py
2. Run the script using `python diagnose.py` and paste its output here.

&lt;/denchmark-code&gt;

Package used (Python/R/Scala/Julia):
Python
For Scala user, please provide:

Java version: (java -version)
Maven version: (mvn -version)
Scala runtime if applicable: (scala -version)

For R user, please provide R sessionInfo():
&lt;denchmark-h:h2&gt;Build info (Required if built from source)&lt;/denchmark-h&gt;

Compiler (gcc/clang/mingw/visual studio):
MXNet commit hash:
&lt;denchmark-link:https://github.com/apache/incubator-mxnet/commit/efa7d3ad96408fe4c5d290dcf19b296ab2ce0fd9&gt;efa7d3a&lt;/denchmark-link&gt;

Build config:
(Paste the content of config.mk, or the build command.)
&lt;denchmark-h:h2&gt;Error Message:&lt;/denchmark-h&gt;

(Paste the complete error message, including stack trace.)
&lt;denchmark-h:h2&gt;Minimum reproducible example&lt;/denchmark-h&gt;

(If you are using your own code, please provide a short script that reproduces the error. Otherwise, please provide link to the existing example.)
&lt;denchmark-h:h2&gt;Steps to reproduce&lt;/denchmark-h&gt;

(Paste the commands you ran that produced the error.)




&lt;denchmark-h:h2&gt;What have you tried to solve it?&lt;/denchmark-h&gt;





	</description>
	<comments>
		<comment id='1' author='wkcn' date='2018-10-14T16:11:32Z'>
		Thank you for posting your issue. We will look into this.
&lt;denchmark-link:https://github.com/mxnet-label-bot&gt;@mxnet-label-bot&lt;/denchmark-link&gt;
 [ Bug, Cuda]
		</comment>
		<comment id='2' author='wkcn' date='2018-10-14T22:34:20Z'>
		&lt;denchmark-link:https://github.com/piyushghai&gt;@piyushghai&lt;/denchmark-link&gt;
 Thank you!
		</comment>
		<comment id='3' author='wkcn' date='2018-10-15T22:53:39Z'>
		MXNet calls stream-&gt;wait at operator level: &lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/src/imperative/imperative_utils.h#L405&gt;https://github.com/apache/incubator-mxnet/blob/master/src/imperative/imperative_utils.h#L405&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='wkcn' date='2018-10-16T03:59:56Z'>
		&lt;denchmark-link:https://github.com/eric-haibin-lin&gt;@eric-haibin-lin&lt;/denchmark-link&gt;

Thank you!
I call MXNDArrayWaitToWrite firstly, then call MXNDArrayGetData to get the data pointer of a NDArray.
Passing the data pointer into a CUDA kernel.
In the CUDA kernel, there are some assignments through the data pointer.
The CUDA kernel will run with NULL stream (The CUDA code is out of MXNet, so I couldn't obtain the CUDA stream from RunContext in MXNet).
It works sometime, however it triggers the CUDA error illegal memory access randomly.
I couldn't find the position which triggers the error.
I will check it.
		</comment>
		<comment id='5' author='wkcn' date='2018-10-16T16:24:47Z'>
		Did you make sure the reference to the original NDArray is kept and the memory is not freed?
		</comment>
		<comment id='6' author='wkcn' date='2018-10-17T03:43:42Z'>
		&lt;denchmark-link:https://github.com/eric-haibin-lin&gt;@eric-haibin-lin&lt;/denchmark-link&gt;

Yes. I found that the CUDA kernel I wrote may runs failed sometime.
I'm looking for the reason.
Thank you!
		</comment>
		<comment id='7' author='wkcn' date='2018-10-22T15:59:18Z'>
		Sorry, there is a bug in my project.
I use Python Code to call cudaSetDevice and kernel function, like that:
class CFuncDef:
    [...]
    def __call__(self, arg_datas, arg_types, dev_id):
        if dev_id is None:
            ctx = 'cpu'
        else:
            set_device(dev_id)
            ctx = gpu_ctx_name
        # function loader
        func = self.loader(self, arg_types, ctx, **self.loader_kwargs)
        return func(*arg_datas)
func is a ctypes reference of a CUDA kernel function.
The function CFuncDef.__call__ will be called in MXNet Custom Operator.
However, the device ID may be changed between calling set_device and func(*arg_datas) because of asynchronous execution. It causes the problem access illegal memory.
Calling WaitToRead or WaitToWrite is enough.
Solved it.
Thank you!
		</comment>
		<comment id='8' author='wkcn' date='2018-10-23T05:04:29Z'>
		Good to know it's resolved.
		</comment>
	</comments>
</bug>