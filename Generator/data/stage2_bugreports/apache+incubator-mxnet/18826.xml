<bug id='18826' author='sxjscience' open_date='2020-07-30T04:02:56Z' closed_time='2020-08-07T05:41:49Z'>
	<summary>[Activation] GELU precision mismatch between MXNet and PyTorch in the CPU version</summary>
	<description>
The CPU version of mx.npx.leaky_relu(x, act_type='gelu') has different precision from PyTorch.
The minimal reproducible example:
import mxnet as mx
mx.npx.set_np()
a = mx.np.random.normal(0, 1, (10000,)) 
b = mx.npx.leaky_relu(a, act_type='gelu')
c = a * 0.5 * (1.0 + mx.npx.erf(a / math.sqrt(2.0)))

import torch
a_torch = torch.from_numpy(a.asnumpy()).cuda() 
b_torch = torch.nn.functional.gelu(a_torch)
assert_allclose(b_torch.cpu().numpy(), c.asnumpy(), 1E-4, 1E-4)  
assert_allclose(b_torch.cpu().numpy(), b.asnumpy(), 1E-4, 1E-4)  
The GPU version has no issue:
import mxnet as mx
mx.npx.set_np()
a = mx.np.random.normal(0, 1, (10000,), ctx=mx.gpu()) 
b = mx.npx.leaky_relu(a, act_type='gelu')
c = a * 0.5 * (1.0 + mx.npx.erf(a / math.sqrt(2.0)))

import torch
a_torch = torch.from_numpy(a.asnumpy()).cuda() 
b_torch = torch.nn.functional.gelu(a_torch)
assert_allclose(b_torch.cpu().numpy(), c.asnumpy(), 1E-4, 1E-4)  
assert_allclose(b_torch.cpu().numpy(), b.asnumpy(), 1E-4, 1E-4)  
&lt;denchmark-link:https://github.com/pengzhao-intel&gt;@pengzhao-intel&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ciyongch&gt;@ciyongch&lt;/denchmark-link&gt;

Error:
&lt;denchmark-code&gt;&lt;ipython-input-48-6f3377797f65&gt; in &lt;module&gt;
      9 b_torch = torch.nn.functional.gelu(a_torch)
     10 assert_allclose(b_torch.cpu().numpy(), c.asnumpy(), 1E-4, 1E-4)
---&gt; 11 assert_allclose(b_torch.cpu().numpy(), b.asnumpy(), 1E-4, 1E-4)

~/.local/lib/python3.6/site-packages/numpy/testing/_private/utils.py in assert_allclose(actual, desired, rtol, atol, equal_nan, err_msg, verbose)
   1526     header = 'Not equal to tolerance rtol=%g, atol=%g' % (rtol, atol)
   1527     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
-&gt; 1528                          verbose=verbose, header=header, equal_nan=equal_nan)
   1529 
   1530 

~/.local/lib/python3.6/site-packages/numpy/testing/_private/utils.py in assert_array_compare(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)
    838                                 verbose=verbose, header=header,
    839                                 names=('x', 'y'), precision=precision)
--&gt; 840             raise AssertionError(msg)
    841     except ValueError:
    842         import traceback

AssertionError: 
Not equal to tolerance rtol=0.0001, atol=0.0001

Mismatched elements: 2258 / 10000 (22.6%)
Max absolute difference: 0.0004735
Max relative difference: 0.8255573
 x: array([ 0.684651,  0.508604, -0.165598, ...,  1.706593,  0.288036,
        1.006167], dtype=float32)
 y: array([ 0.68455 ,  0.508554, -0.165716, ...,  1.706508,  0.288026,
        1.005966], dtype=float32)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='sxjscience' date='2020-07-30T04:31:35Z'>
		&lt;denchmark-link:https://github.com/sxjscience&gt;@sxjscience&lt;/denchmark-link&gt;
 Can you confirm the operator runs into its mkldnn version?
		</comment>
		<comment id='2' author='sxjscience' date='2020-07-30T04:37:52Z'>
		Sorry I do not have the bandwidth to confirm that. I think mkldnn should be turned on by default. Are you able to reproduce this?

Get Outlook for iOS&lt;&lt;denchmark-link:https://aka.ms/o0ukef&gt;https://aka.ms/o0ukef&lt;/denchmark-link&gt;
&gt;
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


________________________________
From: Tao Lv &lt;notifications@github.com&gt;
Sent: Wednesday, July 29, 2020 9:31:49 PM
To: apache/incubator-mxnet &lt;incubator-mxnet@noreply.github.com&gt;
Cc: Xingjian SHI &lt;xshiab@connect.ust.hk&gt;; Mention &lt;mention@noreply.github.com&gt;
Subject: Re: [apache/incubator-mxnet] [Activation] GELU precision mismatch between MXNet and PyTorch in the CPU version (#18826)


@sxjscience&lt;https://github.com/sxjscience&gt; Can you confirm the operator runs into its mkldnn version?

—
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub&lt;#18826 (comment)&gt;, or unsubscribe&lt;https://github.com/notifications/unsubscribe-auth/ABHQH3XYWAH6FQB5D4YO4DDR6DZTLANCNFSM4PMXSPLQ&gt;.

		</comment>
		<comment id='3' author='sxjscience' date='2020-07-30T05:15:05Z'>
		In fact, I cannot correctly run the reproducer. I try to fix the precision problem with &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/18827&gt;#18827&lt;/denchmark-link&gt;
. Please let me know if it works for you. Thanks.
		</comment>
		<comment id='4' author='sxjscience' date='2020-07-30T05:26:45Z'>
		&lt;denchmark-link:https://github.com/TaoLv&gt;@TaoLv&lt;/denchmark-link&gt;
 Sorry, missed some imports.
import mxnet as mx
import math
from numpy.testing import assert_allclose
mx.npx.set_np()
a = mx.np.random.normal(0, 1, (10000,)) 
b = mx.npx.leaky_relu(a, act_type='gelu')
c = a * 0.5 * (1.0 + mx.npx.erf(a / math.sqrt(2.0)))

import torch
a_torch = torch.from_numpy(a.asnumpy())
b_torch = torch.nn.functional.gelu(a_torch)
assert_allclose(b_torch.cpu().numpy(), c.asnumpy(), 1E-4, 1E-4)  
assert_allclose(b_torch.cpu().numpy(), b.asnumpy(), 1E-4, 1E-4)  
(Compiling MXNet takes some time for me so it will be helpful if you can check that...)
		</comment>
		<comment id='5' author='sxjscience' date='2020-08-07T05:35:06Z'>
		
@TaoLv Sorry, missed some imports.
import mxnet as mx
import math
from numpy.testing import assert_allclose
mx.npx.set_np()
a = mx.np.random.normal(0, 1, (10000,)) 
b = mx.npx.leaky_relu(a, act_type='gelu')
c = a * 0.5 * (1.0 + mx.npx.erf(a / math.sqrt(2.0)))

import torch
a_torch = torch.from_numpy(a.asnumpy())
b_torch = torch.nn.functional.gelu(a_torch)
assert_allclose(b_torch.cpu().numpy(), c.asnumpy(), 1E-4, 1E-4)  
assert_allclose(b_torch.cpu().numpy(), b.asnumpy(), 1E-4, 1E-4)  
(Compiling MXNet takes some time for me so it will be helpful if you can check that...)

Does the issue still exist after Tao's PR?
		</comment>
		<comment id='6' author='sxjscience' date='2020-08-07T05:41:49Z'>
		Yes, it's solved.
		</comment>
	</comments>
</bug>