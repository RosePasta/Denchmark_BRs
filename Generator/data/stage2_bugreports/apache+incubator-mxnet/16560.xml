<bug id='16560' author='classicsong' open_date='2019-10-21T07:32:50Z' closed_time='2020-08-09T00:06:38Z'>
	<summary>It is easy to crash MXNet when tensor goes larger</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

When I use large tensor, it is easy to crash the MXNet kernel.
Using following python code to reproduce:
&lt;denchmark-code&gt;&gt;&gt;&gt; import mxnet.ndarray as nd

&gt;&gt;&gt; a = nd.random.randn(4, 256, 1, 100, 100)
&gt;&gt;&gt; b = nd.broadcast_axis(a, axis=2, size=256)
&gt;&gt;&gt; b.size
2621440000
&gt;&gt;&gt; b.asnumpy()
CRASH HERE
&lt;/denchmark-code&gt;

The error looks like an int32 overflow on shape.size.
Any easy way to fix this out? The only way I found out is to compile MXNet with USE_INT64_TENSOR_SIZE = ON, which is slower than the default one.
&lt;denchmark-h:h2&gt;Environment info (Required)&lt;/denchmark-h&gt;

mxnet 1.5.1 (pip3 install)
Package used (Python/R/Scala/Julia):
Python
&lt;denchmark-h:h2&gt;Error Message:&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
  File "/usr/local/lib/python3.5/dist-packages/mxnet/ndarray/ndarray.py", line 1996, in asnumpy
    ctypes.c_size_t(data.size)))
  File "/usr/local/lib/python3.5/dist-packages/mxnet/base.py", line 253, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [07:26:09] include/mxnet/././tensor_blob.h:290: Check failed: this-&gt;shape_.Size() == static_cast&lt;size_t&gt;(shape.Size()) (2621440000 vs. 18446744072036024320) : TBlob.get_with_shape: new and old shape do not match total elements
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='classicsong' date='2019-10-21T18:38:52Z'>
		&lt;denchmark-link:https://github.com/mxnet-label-bot&gt;@mxnet-label-bot&lt;/denchmark-link&gt;
 Add [Bug, Large Tensor Support]
		</comment>
		<comment id='2' author='classicsong' date='2019-10-21T18:40:57Z'>
		cc &lt;denchmark-link:https://github.com/access2rohit&gt;@access2rohit&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='classicsong' date='2019-10-21T23:45:55Z'>
		We should raise an error message in the C++ side when we are going to create a large NDArray.
		</comment>
		<comment id='4' author='classicsong' date='2019-10-28T17:43:29Z'>
		Yes it is being tracked here &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/16570&gt;#16570&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='classicsong' date='2019-11-07T22:00:50Z'>
		Is this resolved now that &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/16570&gt;#16570&lt;/denchmark-link&gt;
 is merged?
		</comment>
		<comment id='6' author='classicsong' date='2019-11-18T19:56:09Z'>
		&lt;denchmark-link:https://github.com/lanking520&gt;@lanking520&lt;/denchmark-link&gt;
 assign &lt;denchmark-link:https://github.com/ChaiBapchya&gt;@ChaiBapchya&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='classicsong' date='2020-06-15T07:33:24Z'>
		We can close this ticket. As the solution is to build with large tensor as the issue author pointed it out. An error message is already raised as part of &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/16570&gt;#16570&lt;/denchmark-link&gt;
 if large array is created when large tensor support isn't enabled.
		</comment>
	</comments>
</bug>