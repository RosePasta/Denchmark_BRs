<bug id='16576' author='sxjscience' open_date='2019-10-21T23:39:09Z' closed_time='2019-11-07T18:25:51Z'>
	<summary>[Numpy][Bug] einsum bug</summary>
	<description>
Example:
import mxnet as mx
import numpy as np
import numpy.testing as npt
from mxnet.gluon import HybridBlock



class MultiNDimBatchDot1(HybridBlock):
    def hybrid_forward(self, F, lhs, rhs):
        """

        Parameters
        ----------
        F
        lhs :
            Shape (N0, N1, T0, C)
        rhs :
            Shape (N0, N1, T1, C)

        Returns
        -------
        ret :
            Shape (N0, N1, T0, T1)
        """
        return F.batch_dot(F.reshape(lhs, (-3, 0, 0)),
                           F.reshape(rhs, (-3, 0, 0)), transpose_b=True).reshape((-4, -1, 8, 0, 0))


class MultiNDimBatchDot2(HybridBlock):
    def hybrid_forward(self, F, lhs, rhs):
        """

        Parameters
        ----------
        F
        lhs :
            Shape (N0, N1, T0, C)
        rhs :
            Shape (N0, N1, T1, C)

        Returns
        -------
        ret :
            Shape (N0, N1, T0, T1)
        """
        return F.np.einsum('abiz,abjz-&gt;abij', lhs, rhs)


batch_dot1 = MultiNDimBatchDot1()


batch_dot1.hybridize()

lhs = mx.np.array(np.random.normal(0, 1, (64, 8, 128, 512)), dtype=np.float32, ctx=mx.cpu())
rhs = mx.np.array(np.random.normal(0, 1, (64, 8, 128, 512)), dtype=np.float32, ctx=mx.cpu())
mx.npx.waitall()

gt = np.einsum('abiz,abjz-&gt;abij', lhs.asnumpy(), rhs.asnumpy())
out = batch_dot1(lhs.as_nd_ndarray(), rhs.as_nd_ndarray())

npt.assert_allclose(gt, out.asnumpy(), rtol=1E-3, atol=1E-3)


mx.npx.set_np()
batch_dot2 = MultiNDimBatchDot2()
batch_dot2.hybridize()

out2 = batch_dot2(lhs, rhs)
npt.assert_allclose(gt, out2.asnumpy(), rtol=1E-3, atol=1E-3)
The CPU results are not correct.
Also, if we use the gpu, it will trigger a segfault:
import mxnet as mx
import numpy as np
import numpy.testing as npt
from mxnet.gluon import HybridBlock



class MultiNDimBatchDot1(HybridBlock):
    def hybrid_forward(self, F, lhs, rhs):
        """

        Parameters
        ----------
        F
        lhs :
            Shape (N0, N1, T0, C)
        rhs :
            Shape (N0, N1, T1, C)

        Returns
        -------
        ret :
            Shape (N0, N1, T0, T1)
        """
        return F.batch_dot(F.reshape(lhs, (-3, 0, 0)),
                           F.reshape(rhs, (-3, 0, 0)), transpose_b=True).reshape((-4, -1, 8, 0, 0))


class MultiNDimBatchDot2(HybridBlock):
    def hybrid_forward(self, F, lhs, rhs):
        """

        Parameters
        ----------
        F
        lhs :
            Shape (N0, N1, T0, C)
        rhs :
            Shape (N0, N1, T1, C)

        Returns
        -------
        ret :
            Shape (N0, N1, T0, T1)
        """
        return F.np.einsum('abiz,abjz-&gt;abij', lhs, rhs)


batch_dot1 = MultiNDimBatchDot1()


batch_dot1.hybridize()

lhs = mx.np.array(np.random.normal(0, 1, (64, 8, 128, 512)), dtype=np.float32, ctx=mx.gpu())
rhs = mx.np.array(np.random.normal(0, 1, (64, 8, 128, 512)), dtype=np.float32, ctx=mx.gpu())
mx.npx.waitall()

gt = np.einsum('abiz,abjz-&gt;abij', lhs.asnumpy(), rhs.asnumpy())
out = batch_dot1(lhs.as_nd_ndarray(), rhs.as_nd_ndarray())

npt.assert_allclose(gt, out.asnumpy(), rtol=1E-3, atol=1E-3)


mx.npx.set_np()
batch_dot2 = MultiNDimBatchDot2()
batch_dot2.hybridize()

out2 = batch_dot2(lhs, rhs)
npt.assert_allclose(gt, out2.asnumpy(), rtol=1E-3, atol=1E-3)
	</description>
	<comments>
		<comment id='1' author='sxjscience' date='2019-10-21T23:39:33Z'>
		&lt;denchmark-link:https://github.com/hzfan&gt;@hzfan&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='sxjscience' date='2019-10-21T23:44:36Z'>
		I compiled the latest version from source and I'm using g4 machine.
		</comment>
		<comment id='3' author='sxjscience' date='2019-10-22T10:25:03Z'>
		&lt;denchmark-link:https://github.com/sxjscience&gt;@sxjscience&lt;/denchmark-link&gt;
 Try build with  (for cmake) or  (for make).
		</comment>
		<comment id='4' author='sxjscience' date='2019-10-22T14:05:04Z'>
		I think the input size has not reached the int64 limit. I tested with a common shape in NLP. Also, I tried to use batch_dot, which could derive the correct result. (See Block1 in the example)

Get Outlook for iOS&lt;&lt;denchmark-link:https://aka.ms/o0ukef&gt;https://aka.ms/o0ukef&lt;/denchmark-link&gt;
&gt;
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


________________________________
From: Haozheng Fan &lt;notifications@github.com&gt;
Sent: Tuesday, October 22, 2019 3:25:29 AM
To: apache/incubator-mxnet &lt;incubator-mxnet@noreply.github.com&gt;
Cc: Xingjian SHI &lt;xshiab@connect.ust.hk&gt;; Author &lt;author@noreply.github.com&gt;
Subject: Re: [apache/incubator-mxnet] [Numpy][Bug] einsum bug (#16576)


Try build with -DUSE_INT64_TENSOR_SIZE=ON (for cmake) or USE_INT64_TENSOR_SIZE=1 (for make).

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub&lt;#16576?email_source=notifications&amp;email_token=ABHQH3U2A3KCJMPIIAIENMTQP3IJTA5CNFSM4JDIIEB2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEB5HUXI#issuecomment-544897629&gt;, or unsubscribe&lt;https://github.com/notifications/unsubscribe-auth/ABHQH3XI2J5HFL3B634FF2TQP3IJTANCNFSM4JDIIEBQ&gt;.

		</comment>
		<comment id='5' author='sxjscience' date='2019-10-23T06:22:18Z'>
		Thanks &lt;denchmark-link:https://github.com/sxjscience&gt;@sxjscience&lt;/denchmark-link&gt;
 . The cause may be overflow of intermediate index. I created a fix in &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/16589&gt;#16589&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='6' author='sxjscience' date='2019-11-07T18:25:50Z'>
		I've rerun the script and can confirm that &lt;denchmark-link:https://github.com/hzfan&gt;@hzfan&lt;/denchmark-link&gt;
's commit has solved the problem. I'll later try to use einsum in the implementation of neural attention.
		</comment>
	</comments>
</bug>