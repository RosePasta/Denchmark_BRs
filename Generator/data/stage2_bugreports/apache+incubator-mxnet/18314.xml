<bug id='18314' author='leezu' open_date='2020-05-14T05:32:55Z' closed_time='2020-05-15T01:20:37Z'>
	<summary>ThreadedEngine: Segfault</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

The following code shouldn't crash, but can reliably trigger segfault in ThreadedEngine. (No segfault with NaiveEngine).
&lt;denchmark-code&gt;import gc
gc.set_debug(gc.DEBUG_SAVEALL)
import mxnet as mx
net = mx.gluon.nn.Dense(10, in_units=10)
net.initialize()
del net
print(gc.collect())
for i, x in enumerate(gc.garbage):
    print(i, type(x), id(x), end=' ')
    try:
        print('\t', x)
    except Exception as e:
        print('failed', e)
&lt;/denchmark-code&gt;

cc &lt;denchmark-link:https://github.com/eric-haibin-lin&gt;@eric-haibin-lin&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='leezu' date='2020-05-14T05:36:40Z'>
		On 1.6 it also triggers segfault with NaiveEngine. But not on master.
		</comment>
		<comment id='2' author='leezu' date='2020-05-14T06:05:51Z'>
		Use &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/18315&gt;#18315&lt;/denchmark-link&gt;
 to get the coredump associated with the segfault. Loading it with gdb gives this backtrace
&lt;denchmark-code&gt;#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51
#1  0x00007f144d380801 in __GI_abort () at abort.c:79
#2  0x00007f13973c94c2 in mxnet::SegfaultLogger (sig=11) at ../src/initialize.cc:66
#3  &lt;signal handler called&gt;
#4  __GI___pthread_mutex_lock (mutex=0x7ffc00000020) at ../nptl/pthread_mutex_lock.c:65
#5  0x00007f1397096b3d in __gthread_mutex_lock (__mutex=0x7ffc00000020) at /usr/include/x86_64-linux-gnu/c++/7/bits/gthr-default.h:748
#6  0x00007f13970be6f2 in std::mutex::lock (this=0x7ffc00000020) at /usr/include/c++/7/bits/std_mutex.h:103
#7  0x00007f13970d7812 in std::lock_guard&lt;std::mutex&gt;::lock_guard (this=0x7ffc08a27cb0, __m=...) at /usr/include/c++/7/bits/std_mutex.h:162
#8  0x00007f13972c4759 in mxnet::engine::ThreadedVar::ready_to_read (this=0x7ffc00000000) at ../src/engine/threaded_engine.cc:198
#9  0x00007f13972c18fd in mxnet::engine::ThreadedEngine::WaitForVar (this=0x560344701760, var=0x7ffc00000000) at ../src/engine/threaded_engine.cc:382
#10 0x00007f13975c73f1 in mxnet::NDArray::WaitToRead (this=0x560343f7e750) at ../src/ndarray/ndarray.cc:2178
#11 0x00007f13975c66dd in mxnet::NDArray::SyncCopyToCPU (this=0x560343f7e750, data=0x7f1218001b00, size=10) at ../src/ndarray/ndarray.cc:2112
#12 0x00007f13970ac51b in MXNDArraySyncCopyToCPU (handle=0x560343f7e750, data=0x7f1218001b00, size=10) at ../src/c_api/c_api.cc:1575
#13 0x00007f144cc85dae in ffi_call_unix64 () from /usr/lib/x86_64-linux-gnu/libffi.so.6
#14 0x00007f144cc8571f in ffi_call () from /usr/lib/x86_64-linux-gnu/libffi.so.6
#15 0x00007f144cec7415 in _call_function_pointer (flags=4353, pProc=0x7f13970ac4e3 &lt;MXNDArraySyncCopyToCPU(NDArrayHandle, void*, size_t)&gt;, avalues=0x7ffc08a283c0, atypes=0x7ffc08a28390,
    restype=0x7f144d14bdb8, resmem=0x7ffc08a283f0, argcount=3) at /tmp/python-build.20200514035455.63369/Python-3.8.2/Modules/_ctypes/callproc.c:871
#16 0x00007f144cec7e19 in _ctypes_callproc (pProc=0x7f13970ac4e3 &lt;MXNDArraySyncCopyToCPU(NDArrayHandle, void*, size_t)&gt;, argtuple=0x7f12f4166ef0, flags=4353, argtypes=0x0, restype=0x560342e1b8e0,
    checker=0x0) at /tmp/python-build.20200514035455.63369/Python-3.8.2/Modules/_ctypes/callproc.c:1199
#17 0x00007f144cec2169 in PyCFuncPtr_call (self=0x7f12f4145050, inargs=0x7f12f4166ef0, kwds=0x0) at /tmp/python-build.20200514035455.63369/Python-3.8.2/Modules/_ctypes/_ctypes.c:4201
#18 0x0000560342627076 in ?? ()
#19 0x0000000000000000 in ?? ()

&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='leezu' date='2020-05-14T18:46:56Z'>
		not sure completely what it is, but it could be that the one of the objects in garbage is a ndarray and when you call print it calls its repr and asnumpy, the dependency engine tries to figure if the underlying threadedvar is present but looks like it may have been deleted. Maybe it is worth to figure that for all the objects appended to garbage which were ndarray, if delete was called on them already ?
		</comment>
		<comment id='4' author='leezu' date='2020-05-14T18:47:19Z'>
		The mutex in above core dump is actually non existant. The problem here is that the ffi call to MXNDArraySyncCopyToCPU passes a handle whose Chunk was cleaned up. So the segfault is just a symptom of memory and reference management issues.
the array
&lt;denchmark-code&gt;(gdb) p *this
$6 = {ptr_ = std::shared_ptr&lt;mxnet::NDArray::Chunk&gt; (use count 1645762192, weak count 22028) = {get() = 0x560d62a70cb0}, shape_ = {&lt;mxnet::Tuple&lt;long&gt;&gt; = {static kStackCache = 4, ndim_ = 2,
      num_heap_allocated_ = 0, data_stack_ = {10, 10, -3617008641903833651, -3617008641903833651}, data_heap_ = 0x0}, &lt;No data fields&gt;}, byte_offset_ = 0, dtype_ = 0, reuse_ = false,
  storage_type_ = mxnet::kDefaultStorage, autograd_entry_ = {node = std::shared_ptr&lt;nnvm::Node&gt; (use count 1647685840, weak count 22028) = {get() = 0x560d622fe220}, index = 0, version = 0},
  deferredcompute_entry_ = {node = std::shared_ptr&lt;nnvm::Node&gt; (empty) = {get() = 0x0}, index = 0, version = 0}, tblob_ = {dptr_ = 0x0, shape_ = {&lt;mxnet::Tuple&lt;long&gt;&gt; = {static kStackCache = 4,
        ndim_ = -1, num_heap_allocated_ = 0, data_stack_ = {-3617008641903833651, -2459565876494606899, -2459565876494606883, -2459565876494606883}, data_heap_ = 0x0}, &lt;No data fields&gt;}, type_flag_ = 0,
    dltensor_ = {data = 0x0, ctx = {device_type = kDLCPU, device_id = 0}, ndim = -1, dtype = {code = 2 '\002', bits = 32 ' ', lanes = 1}, shape = 0x560d62188f78, strides = 0x0, byte_offset = 0}}}
&lt;/denchmark-code&gt;

and the chunk
&lt;denchmark-code&gt;p *ptr_
$4 = {shandle = {dptr = 0xdddddddddddddddd, size = 481, ctx = {dev_type = 1655088448, dev_id = 22029, static kMaxDevType = 6, static kMaxDevID = 16}, shared_pid = 1645776592, shared_id = 22029,
    profiler_scope = Python Exception &lt;class 'OverflowError'&gt; int too big to convert:
, Python Exception &lt;class 'OverflowError'&gt; int too big to convert:
name = }, aux_handles = std::vector of length 0, capacity 0, mkl_mem_ = &lt;error reading variable: Cannot access memory at address 0xdddddddddddddde5&gt;, var = 0xdddddddddddddddd,
  static_data = 221, delay_alloc = 221, storage_type = -572662307, aux_types = std::vector of length 0, capacity 0, ctx = {dev_type = 3722304989, dev_id = -572662307, static kMaxDevType = 6,
    static kMaxDevID = 16}, storage_shape = {&lt;mxnet::Tuple&lt;long&gt;&gt; = {static kStackCache = 4, ndim_ = -572662307, num_heap_allocated_ = -572662307, data_stack_ = {-2459565876494606883,
        -2459565876494606883, -2459565876494606883, -2459565876494606883}, data_heap_ = 0xdddddddddddddddd}, &lt;No data fields&gt;}, aux_shapes = std::vector of length 0, capacity 0,
  storage_ref_ = &lt;error reading variable: Cannot access memory at address 0xdddddddddddddde5&gt;, engine_ref_ = &lt;error reading variable: Cannot access memory at address 0xdddddddddddddde5&gt;}

&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='leezu' date='2020-05-14T19:01:57Z'>
		I can confirm that del is called on the object, but it's still kept around in Python afterwards.
In practice that may only make problems when debugging memory leaks (as is the case for how I ran into these segfaults)
		</comment>
		<comment id='6' author='leezu' date='2020-05-14T19:05:49Z'>
		From &lt;denchmark-link:https://docs.python.org/3/library/gc.html#gc.garbage&gt;https://docs.python.org/3/library/gc.html#gc.garbage&lt;/denchmark-link&gt;
 , "Changed in version 3.4: Following PEP 442, objects with a () method donâ€™t end up in gc.garbage anymore."
but this statement doesnt seem to be true here.
		</comment>
		<comment id='7' author='leezu' date='2020-05-14T19:09:55Z'>
		That's because I set gc.set_debug(gc.DEBUG_SAVEALL). I want to preserve all objects found by gc, because I need to figure out where the reference cycle that prevented them from being freed by reference counting, came from.
Howeve, I find that despite gc.set_debug(gc.DEBUG_SAVEALL), the gc will still call the finalizer on the objects it finds. That's why we end up with zombie objects in gc.garbage:
&lt;denchmark-code&gt;
Calling finalizer for c_void_p(94420426532208)
Calling finalizer for c_void_p(94420426532208)
Calling collect
Calling finalizer for c_void_p(94420426437968)
Calling finalizer for c_void_p(94420427373968)
Calling finalizer for c_void_p(94420428079184)
Calling finalizer for c_void_p(94420426198000)

&lt;/denchmark-code&gt;

We see that after del net is called, two ndarrays are finalized thanks to reference counting. 4 arrays are only finalized after a full garbage collection.
		</comment>
		<comment id='8' author='leezu' date='2020-05-14T19:35:08Z'>
		
We see that after del net is called, two ndarrays are finalized thanks to reference counting. 4 arrays are only finalized after a full garbage collection.

aah okay, maybe make the del a no op for testing and try to see which objects are uncollectable ?
		</comment>
		<comment id='9' author='leezu' date='2020-05-15T01:20:37Z'>
		Let's close this issue as it's specific to the use of gc.DEBUG_SAVEALL. The leak is fixed in &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/18328&gt;#18328&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>