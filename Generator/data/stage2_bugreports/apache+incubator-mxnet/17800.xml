<bug id='17800' author='aGiant' open_date='2020-03-10T14:56:30Z' closed_time='2020-03-10T17:09:20Z'>
	<summary>_MinusScalar type &amp;lt;class 'mxnet.ndarray.ndarray.NDArray'&amp;gt; not supported` ?</summary>
	<description>
&lt;denchmark-code&gt;&lt;ipython-input-92-6fb5c4df8b27&gt; in hybrid_forward(self, F, x)
     36         #x = self.normalizer(data)
     37         flat_x = self.flatten(x)
---&gt; 38         above = flat_x - self.min
     39         under = self.max - self.min
     40         x_normalized = above/under

~/anaconda3/lib/python3.7/site-packages/mxnet/symbol/symbol.py in __sub__(self, other)
    127             return _internal._MinusScalar(self, scalar=other)
    128         else:
--&gt; 129             raise TypeError('type %s not supported' % str(type(other)))
    130 
    131     def __isub__(self, other):

TypeError: type &lt;class 'mxnet.ndarray.ndarray.NDArray'&gt; not supported
&lt;/denchmark-code&gt;

those were defined in hybrid_forward of changing official VAE code example.
	</description>
	<comments>
		<comment id='1' author='aGiant' date='2020-03-10T15:04:12Z'>
		Maybe I was too old to count the number ;)
&lt;denchmark-code&gt;model_ctx = mx.gpu()
&lt;ipython-input-102-09c5bc45d5b4&gt; in hybrid_forward(self, F, x)
     35     def hybrid_forward(self, F, x):
     36         #x = self.normalizer(data)
---&gt; 37         x.as_in_context(model_ctx)
     38         flat_x = self.flatten(x)
     39         above = flat_x - self.min

TypeError: **as_in_context() takes 1 positional argument but 2 were given**
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='aGiant' date='2020-03-10T15:42:32Z'>
		More errors
&lt;denchmark-code&gt;&lt;ipython-input-133-18747e174c30&gt; in __init__(self, n_hidden, n_latent, n_layers, n_output, min_vec, max_vec, act_type, **kwargs)
     11         # https://mxnet.apache.org/api/python/docs/api/gluon/constant.html
     12         # self.const = self.params.get_constant('const', [[1,2],[3,4]])
---&gt; 13         self.min_v = mx.gluon.Constant('const_min', min_vec)
     14         self.max_v = mx.gluon.Constant('const_max', max_vec)
     15         # note to self: requring batch_size in model definition is sad,

~/anaconda3/lib/python3.7/site-packages/mxnet/gluon/block.py in __setattr__(self, name, value)
    732     def __setattr__(self, name, value):
    733         """Registers parameters."""
--&gt; 734         super(HybridBlock, self).__setattr__(name, value)
    735         if isinstance(value, HybridBlock):
    736             self._clear_cached_op()

~/anaconda3/lib/python3.7/site-packages/mxnet/gluon/block.py in __setattr__(self, name, value)
    202             self.register_child(value, name)
    203         elif isinstance(value, Parameter):
--&gt; 204             assert name not in self._reg_params, \
    205                 "Overriding Parameter attribute %s is not allowed. " \
    206                 "If you want to share parameters between blocks, please set " \

AttributeError: 'VAE' object has no attribute '_reg_params'
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='aGiant' date='2020-03-10T16:30:54Z'>
		Just tried example from &lt;denchmark-link:https://mxnet.apache.org/api/python/docs/tutorials/packages/gluon/blocks/custom_layer_beginners.html&gt;https://mxnet.apache.org/api/python/docs/tutorials/packages/gluon/blocks/custom_layer_beginners.html&lt;/denchmark-link&gt;
 to build ** min-max normalization using saved min-max values**, but more and more errors:
&lt;denchmark-code&gt;--------------------------------------------------------------------
AssertionError                     Traceback (most recent call last)
&lt;ipython-input-213-cbd52c8ac2cb&gt; in &lt;module&gt;
      2 net.collect_params().initialize(mx.init.Xavier(), ctx=model_ctx)
      3 #net(mx.nd.random.uniform(shape=(128,feature_n), ctx=model_ctx))
----&gt; 4 print(net.summary(mx.nd.random.uniform(shape=(1,feature_n), ctx=model_ctx)))
      5 net.hybridize()
      6 trainer = gluon.Trainer(net.collect_params(), 'SGD', {'wd':0.01}) #'learning_rate': .001,

~/anaconda3/lib/python3.7/site-packages/mxnet/gluon/block.py in summary(self, *inputs)
    648         try:
    649             self.apply(_register_summary_hook)
--&gt; 650             self(*inputs)
    651 
    652             line_format = '{:&gt;20}  {:&gt;42} {:&gt;15}'

~/anaconda3/lib/python3.7/site-packages/mxnet/gluon/block.py in __call__(self, *args)
    546             hook(self, args)
    547 
--&gt; 548         out = self.forward(*args)
    549 
    550         for hook in self._forward_hooks.values():

&lt;ipython-input-211-58f911007fa2&gt; in forward(self, x)
     43     def forward(self,x):
     44         self.batch_size = x.shape[0]
---&gt; 45         return gluon.HybridBlock.forward(self, x)
     46 
     47     # https://mxnet.apache.org/api/python/docs/tutorials/extend/custom_layer.html

~/anaconda3/lib/python3.7/site-packages/mxnet/gluon/block.py in forward(self, x, *args)
    923                     params = {i: j.data(ctx) for i, j in self._reg_params.items()}
    924 
--&gt; 925                 return self.hybrid_forward(ndarray, x, *args, **params)
    926 
    927         assert isinstance(x, Symbol), \

&lt;ipython-input-211-58f911007fa2&gt; in hybrid_forward(self, F, x)
     49     def hybrid_forward(self, F, x):
     50         #x = self.normalizer(data)
---&gt; 51         x_normalized = F.broadcast_div(F.broadcast_sub(self.flatten(x), self.min_v), (F.broadcast_sub(self.max_v, self.min_v)))
     52         h = self.encoder(x_normalized)
     53         #print(h.asnumpy()[0])

~/anaconda3/lib/python3.7/site-packages/mxnet/ndarray/register.py in broadcast_sub(lhs, rhs, out, name, **kwargs)

AssertionError: Argument rhs must have NDArray type, but got Parameter vae54_scales_min (shape=(70,), dtype=&lt;class 'numpy.float32'&gt;)


&lt;/denchmark-code&gt;

Saved constant min-max values:
&lt;denchmark-code&gt;        self.min_v = self.params.get('scales_min',
                                      shape=min_vec.shape,
                                      init=mx.init.Constant(min_vec),
                                      differentiable=False)
        self.max_v = self.params.get('scales_max',
                                      shape=max_vec.shape,
                                      init=mx.init.Constant(max_vec),
                                      differentiable=False)
&lt;/denchmark-code&gt;

Also tried
&lt;denchmark-code&gt;        self.min_v = mx.gluon.Constant('min_v', min_vec)
        self.max_v = mx.gluon.Constant('max_v', max_vec) 
&lt;/denchmark-code&gt;

but got similar error:
&lt;denchmark-code&gt;&lt;ipython-input-214-026df9c27e7e&gt; in hybrid_forward(self, F, x)
     49     def hybrid_forward(self, F, x):
     50         #x = self.normalizer(data)
---&gt; 51         x_normalized = F.broadcast_div(F.broadcast_sub(self.flatten(x), self.min_v), (F.broadcast_sub(self.max_v, self.min_v)))
     52         h = self.encoder(x_normalized)
     53         #print(h.asnumpy()[0])

~/anaconda3/lib/python3.7/site-packages/mxnet/ndarray/register.py in broadcast_sub(lhs, rhs, out, name, **kwargs)

AssertionError: Argument rhs must have NDArray type, but got Constant min_v (shape=(70,), dtype=&lt;class 'numpy.float32'&gt;)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='aGiant' date='2020-03-10T17:09:20Z'>
		self.min_v is a constant parameter. In the current hybrid_forward API, you need to def hybrid_forward(self, F, x, min_v, max_v): and you'll get the ndarray associated with the parameter as an argument.
The API will be simplified with &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/17530&gt;#17530&lt;/denchmark-link&gt;

You can read &lt;denchmark-link:https://mxnet.apache.org/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html&gt;https://mxnet.apache.org/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='aGiant' date='2020-03-10T17:12:53Z'>
		Then min-max values must be prepared outsides the NN, our plan is to save the min-max as constant within at the initial stage of NN, and all trained model with be saved and transferred to other platform without any preparation of min-max values in production environment such as from Spark.
The problem based on your link was not solved.
		</comment>
		<comment id='6' author='aGiant' date='2020-03-10T17:16:05Z'>
		
self.min_v is a constant parameter. In the current hybrid_forward API, you need to def hybrid_forward(self, F, x, min_v, max_v): and you'll get the ndarray associated with the parameter as an argument.
The API will be simplified with #17530
You can read https://mxnet.apache.org/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html

Your solution is based on the assumption that all the min-max values were already prepared or at least accessible. Our problem from production environment is that the final user or platform does not have the access to gather the min-max values.
One possible solution for our problem in production is to save the min-max values default within the NN as constant vectors.
		</comment>
		<comment id='7' author='aGiant' date='2020-03-10T17:19:37Z'>
		hybrid_forward(self, F, x, min_v, max_v): still mean users call the net(x), with min_v, max_v being automatically passed and taking the value of the constants.
My point is that you're not using the hybridize API correctly, thus the link to the documentation.
		</comment>
		<comment id='8' author='aGiant' date='2020-03-10T17:22:22Z'>
		
self.min_v is a constant parameter. In the current hybrid_forward API, you need to def hybrid_forward(self, F, x, min_v, max_v): and you'll get the ndarray associated with the parameter as an argument.
The API will be simplified with #17530
You can read https://mxnet.apache.org/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html

Or in short, our final user just have the x, they do not have any min_v and max_v basedon

link

Could you pleas point out the errors？ I have already visited that links many times from last month.
Many thanks!
		</comment>
		<comment id='9' author='aGiant' date='2020-03-10T17:27:38Z'>
		
hybrid_forward(self, F, x, min_v, max_v): still mean users call the net(x), with min_v, max_v being automatically passed and taking the value of the constants.
My point is that you're not using the hybridize API correctly, thus the link to the documentation.

Because from official link in hybird_forward:
&lt;denchmark-code&gt;return (x - nd.min(x)) / (nd.max(x) - nd.min(x))
&lt;/denchmark-code&gt;

this works very well, but
&lt;denchmark-code&gt;return (x - self.min_v) 
&lt;/denchmark-code&gt;

did not work.
		</comment>
		<comment id='10' author='aGiant' date='2020-03-10T17:48:56Z'>
		The parameters (constant or not), are handled special in the current hybrid_forward API.
You can see the implementation:



incubator-mxnet/python/mxnet/gluon/block.py


        Lines 1204 to 1206
      in
      afb8742






 params = {i: j.var() for i, j in self._reg_params.items()} 



 with self.name_scope(): 



 return self.hybrid_forward(symbol, x, *args, **params) 





So you should use hybrid_forward(self, F, x, min_v, max_v): and not access self.min_v inside the hybrid_forward.
		</comment>
		<comment id='11' author='aGiant' date='2020-03-10T17:54:57Z'>
		
The parameters (constant or not), are handled special in the current hybrid_forward API.
You can see the implementation:



incubator-mxnet/python/mxnet/gluon/block.py


        Lines 1204 to 1206
      in
      afb8742






 params = {i: j.var() for i, j in self._reg_params.items()} 



 with self.name_scope(): 



 return self.hybrid_forward(symbol, x, *args, **params) 





So you should use hybrid_forward(self, F, x, min_v, max_v): and not access self.min_v inside the hybrid_forward.

The by calling for prediction, our client must use net(x, min_v, max_v), right? But they do not have any access to prepare the min_v and max_v in production.
		</comment>
		<comment id='12' author='aGiant' date='2020-03-10T19:13:51Z'>
		No. net(x)
		</comment>
		<comment id='13' author='aGiant' date='2020-03-10T19:34:22Z'>
		By calling net(x):
&lt;denchmark-code&gt;
~/anaconda3/lib/python3.7/site-packages/mxnet/gluon/block.py in forward(self, x, *args)
    923                     params = {i: j.data(ctx) for i, j in self._reg_params.items()}
    924 
--&gt; 925                 return self.hybrid_forward(ndarray, x, *args, **params)
    926 
    927         assert isinstance(x, Symbol), \

TypeError: hybrid_forward() missing 2 required positional arguments: 'min_v' and 'max_v'
&lt;/denchmark-code&gt;

		</comment>
		<comment id='14' author='aGiant' date='2020-03-10T19:45:59Z'>
		
No. net(x)

&lt;denchmark-code&gt; def hybrid_forward(self, F, x, min_v, max_v):
        #x = self.normalizer(data)
        #x_normalized = F.broadcast_div(F.broadcast_sub(self.flatten(x), self.min_v), (F.broadcast_sub(self.max_v, self.min_v)))
        x_normalized = self.trans(x) - min_v
        h = self.encoder(x_normalized)
        #print(h.asnumpy()[0])
&lt;/denchmark-code&gt;

		</comment>
		<comment id='15' author='aGiant' date='2020-03-31T22:50:32Z'>
		&lt;denchmark-link:https://github.com/aGiant&gt;@aGiant&lt;/denchmark-link&gt;
 You are still using  and  in your . Are you sure you have them set as parameters? Here is a simple working example of using parameters:
class Test(gluon.HybridBlock):
    def __init__(self, **kwargs):
        super(Test, self).__init__(**kwargs)
        with self.name_scope():
            self.x_min = self.params.get('scales_min',
                                         shape=(1,),
                                         init=mx.init.Constant(5),
                                         differentiable=False)
            self.x_max = self.params.get('scales_max',
                                         shape=(1,),
                                         init=mx.init.Constant(10),
                                         differentiable=False)
            
    def hybrid_forward(self, F, x, x_min, x_max):
        return (x * x_min) + x_max

    
t = Test()
t.initialize()
t(mx.nd.ones((1,)))
[15.]
&lt;NDArray 1 @cpu(0)&gt;
		</comment>
	</comments>
</bug>