<bug id='9715' author='nazikus' open_date='2018-02-06T17:11:56Z' closed_time='2018-08-10T17:49:39Z'>
	<summary>IndexError in labels when size of training dataset is not multiple of batch size</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

IndexError is thrown during training if training dataset size is not multiple of batch size.
&lt;denchmark-h:h2&gt;Long description&lt;/denchmark-h&gt;

I have a simple transfer learning algorithm based on &lt;denchmark-link:https://mxnet.incubator.apache.org/how_to/finetune.html&gt;mxnet tutorial&lt;/denchmark-link&gt;
.
However, I had a nasty exception during training that occurs every time on the last batch of the first epoch. Empirically I have figure out if you manually ensure that dataset size (number of records in .lst file) is multiple of batch size,  than this exception is gone.
Is it expected behavior, or is proper validation missing?
&lt;denchmark-h:h2&gt;Environment info (Required)&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;python 2.7.12
mxnet-cu91mkl 1.0.0.post4
CUDA 9.1.85
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Error Message:&lt;/denchmark-h&gt;

IndexError example during training:
&lt;denchmark-code&gt;2018-02-06 16:24:39,605 - Epoch[1] Batch [200]	Speed: 37.22 samples/sec	cross-entropy=3.293303	accuracy=0.192188
2018-02-06 16:25:13,382 - Epoch[1] Batch [220]	Speed: 37.90 samples/sec	cross-entropy=3.286735	accuracy=0.190625
Traceback (most recent call last):
  File "train_multilabel.py", line 204, in &lt;module&gt;
    train_model(args)
  File "train_multilabel.py", line 135, in train_model
    num_epoch          = params.num_epoch,
  File "/home/otkach/.virtualenvs/mxnet-cu91mkl/local/lib/python2.7/site-packages/mxnet/module/base_module.py", line 496, in fit
    self.update_metric(eval_metric, data_batch.label)
  File "/home/otkach/.virtualenvs/mxnet-cu91mkl/local/lib/python2.7/site-packages/mxnet/module/module.py", line 749, in update_metric
    self._exec_group.update_metric(eval_metric, labels)
  File "/home/otkach/.virtualenvs/mxnet-cu91mkl/local/lib/python2.7/site-packages/mxnet/module/executor_group.py", line 616, in update_metric
    eval_metric.update_dict(labels_, preds)
  File "/home/otkach/.virtualenvs/mxnet-cu91mkl/local/lib/python2.7/site-packages/mxnet/metric.py", line 281, in update_dict
    metric.update_dict(labels, preds)
  File "/home/otkach/.virtualenvs/mxnet-cu91mkl/local/lib/python2.7/site-packages/mxnet/metric.py", line 109, in update_dict
    self.update(label, pred)
  File "/home/otkach/.virtualenvs/mxnet-cu91mkl/local/lib/python2.7/site-packages/mxnet/metric.py", line 924, in update
    prob = pred[numpy.arange(label.shape[0]), numpy.int64(label)]
IndexError: index -9223372036854775808 is out of bounds for axis 1 with size 52
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Steps to reproduce&lt;/denchmark-h&gt;

simply create ImageIter object with dataset size (number of lines in .lst file) not divisible by batch size, and iterate over it, print label values:
&lt;denchmark-code&gt;kv = mx.kvstore.create('device')

train_data = mx.image.ImageIter(
    batch_size   = 64,
    data_shape   = (3, 224, 224),
    path_imglist = 'dataset/train_data.lst',
    path_root    = 'dataset/images/,
    part_index   = kv.rank,
    num_parts    = kv.num_workers,
    shuffle      = True,
    data_name    = 'data',
    label_name   = 'softmax_label',
)

for i, batch in enumerate(train_data):
    print("batch idx {:3d}\n{}\n".format(i, batch.label[0].asnumpy().tolist()))
&lt;/denchmark-code&gt;

For example, dataset size 15226, batch size 64, number of batches 238 (rounded).
In my case, some remaining label values in the last batch are non-integers, which is the source of exception during training:
&lt;denchmark-code&gt;...
batch index   236
[24.0, 13.0, 29.0, 21.0, 48.0, 44.0, 22.0, 47.0, 36.0, 9.0, 47.0, 43.0, 33.0, 3.0, 25.0, 34.0, 47.0, 1.0, 34.0, 40.0, 11.0, 10.0, 43.0, 3.0, 43.0, 27.0, 39.0, 39.0, 13.0, 48.0, 28.0, 42.0, 24.0, 39.0, 31.0, 45.0, 51.0, 6.0, 1.0, 48.0, 17.0, 42.0, 23.0, 9.0, 27.0, 39.0, 19.0, 36.0, 14.0, 10.0, 26.0, 37.0, 42.0, 7.0, 47.0, 29.0, 37.0, 6.0, 9.0, 9.0, 39.0, 5.0, 11.0, 22.0]
batch index   237
[5.0, 50.0, 11.0, 12.0, 50.0, 10.0, 35.0, 31.0, 11.0, 13.0, 2.0, 26.0, 51.0, 6.0, 48.0, 37.0, 25.0, 24.0, 14.0, 20.0, 44.0, 40.0, 21.0, 45.0, 23.0, 18.0, 10.0, 15.0, 21.0, 7.0, 33.0, 32.0, 50.0, 44.0, 10.0, 22.0, 7.0, 9.0, 3.0, 7.0, 49.0, 47.0, 49.0, 26.0, 0.0, 23.0, 2.0, 1.0, 27.0, 0.0, 13.0, 18.0, 38.0, 27.0, 50.0, 18.0, 46.0, 5.0, 7.315163621112927e-37, 0.0, 7.315152859140721e-37, 0.0, 7.315142097168515e-37, 0.0]
&lt;/denchmark-code&gt;

I suppose its a quite common mistake using round() whereas floor() is needed, somewhere in the mxnet code.
&lt;denchmark-h:h2&gt;What have you tried to solve it?&lt;/denchmark-h&gt;

manually remove several lines from trainining_data.lst file, so total number of training dataset size (eg 15168) is divisible by batch size (64).
	</description>
	<comments>
		<comment id='1' author='nazikus' date='2018-02-27T20:56:46Z'>
		&lt;denchmark-link:https://github.com/sandeep-krishnamurthy&gt;@sandeep-krishnamurthy&lt;/denchmark-link&gt;
 Please kindly add Label: Bug, Python
Thanks!
		</comment>
		<comment id='2' author='nazikus' date='2018-03-08T22:18:00Z'>
		&lt;denchmark-link:https://github.com/nazikus&gt;@nazikus&lt;/denchmark-link&gt;
 Hello, I tried reproducing your issue with:

Generating a similar dataset with the same number of images (15226 noisy images with 3 channels and of size (224, 224)) and running the code snippet you provided in "Steps to reproduce" on the dataset.
Creating an MLP model and tried training with the generated dataset on the model.
Following the same steps in the fine-tune tutorial (https://mxnet.incubator.apache.org/faq/finetune.html) that you used and tried training with the generated dataset on the pre-trained model and none of these was able to reproduce the bug you reported.
I wonder if you could give more details on the bug or give access to the dataset you were working on so that we may be able to reproduce and fix it? Thanks!

		</comment>
		<comment id='3' author='nazikus' date='2018-07-20T23:01:17Z'>
		&lt;denchmark-link:https://github.com/eric-haibin-lin&gt;@eric-haibin-lin&lt;/denchmark-link&gt;
  could you help to add pending-requester-info label.
		</comment>
		<comment id='4' author='nazikus' date='2018-08-08T21:03:00Z'>
		&lt;denchmark-link:https://github.com/nazikus&gt;@nazikus&lt;/denchmark-link&gt;
 Thanks for reporting this issue. Can you please provide access to the dataset to reproduce this bug?
		</comment>
		<comment id='5' author='nazikus' date='2018-08-10T17:35:29Z'>
		&lt;denchmark-link:https://github.com/nazikus&gt;@nazikus&lt;/denchmark-link&gt;
 Wasn't able to reproduce this issue on a similar dataset by running your code snippet.
I will be closing this issue for now. Please feel free to reopen if closed in error and if the issue still persists.
&lt;denchmark-link:https://github.com/sandeep-krishnamurthy&gt;@sandeep-krishnamurthy&lt;/denchmark-link&gt;
 Can you please close this issue?
		</comment>
	</comments>
</bug>