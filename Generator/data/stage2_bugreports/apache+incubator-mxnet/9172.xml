<bug id='9172' author='sxjscience' open_date='2017-12-21T23:38:05Z' closed_time='2018-01-08T21:44:33Z'>
	<summary>Wrong gradient of gather_nd when the indices have duplicates</summary>
	<description>
This issue is borrowed from &lt;denchmark-link:https://discuss.gluon.ai/t/topic/3389&gt;https://discuss.gluon.ai/t/topic/3389&lt;/denchmark-link&gt;
.
It's caused by a bug in the gradient computation of . Currently, the gradient of  is , which has not considered the case that the indices may be the same
&lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/src/operator/tensor/indexing_op.h#L1156&gt;https://github.com/apache/incubator-mxnet/blob/master/src/operator/tensor/indexing_op.h#L1156&lt;/denchmark-link&gt;
 . The correct way to implement it is to use the same backward logic as .
import mxnet as mx
import mxnet.ndarray as nd
import numpy as np

data = mx.nd.array([[0, 1], [2, 3]])
indices = mx.nd.array([[1, 1, 0], [1, 1, 0]])
data.attach_grad()
with mx.autograd.record():
    ret = mx.nd.gather_nd(data, indices)
    loss = mx.nd.sum(ret)
loss.backward()
print(data.grad)
&lt;denchmark-code&gt;[[ 1.  0.]
 [ 0.  1.]]
&lt;NDArray 2x2 @cpu(0)&gt;
&lt;/denchmark-code&gt;

The correct result should be
&lt;denchmark-code&gt;[[ 1.  0.]
 [ 0.  2.]]
&lt;NDArray 2x2 @cpu(0)&gt;
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/piiswrong&gt;@piiswrong&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/szha&gt;@szha&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='sxjscience' date='2017-12-21T23:52:00Z'>
		We could either use automicAdd or call the backward of take. atomicAdd seems to be simpler.
		</comment>
		<comment id='2' author='sxjscience' date='2017-12-21T23:57:24Z'>
		What is the cpu version of atomicAdd?
		</comment>
		<comment id='3' author='sxjscience' date='2017-12-21T23:59:04Z'>
		Directly call += if openmp is not used. If openmp is used, we can use the atomic support of omp #pragma omp atomic
		</comment>
	</comments>
</bug>