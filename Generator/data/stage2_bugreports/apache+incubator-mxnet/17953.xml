<bug id='17953' author='konioyxgq' open_date='2020-04-01T10:54:29Z' closed_time='2020-04-06T01:27:47Z'>
	<summary>Models saved at different training stages with different forward speeds</summary>
	<description>
C++ interface under Version 1.3.1 . During training, the first-time saved model, the forward time is 0.2s. But the model saved when the training is complete, the forward time is 0.8s.
What causes this problem? How to fix it? anyone can give some advises? Thanks!
envs: cpu/c++/mxnet1.3.1
	</description>
	<comments>
		<comment id='1' author='konioyxgq' date='2020-04-04T01:20:04Z'>
		Hi. I've previously had a similar problem, which turned out to be due to &lt;denchmark-link:https://en.wikipedia.org/wiki/Denormal_number&gt;denormal numbers&lt;/denchmark-link&gt;
 in the model weights. This issue only affects inference times on the CPU, since on some modern CPUs floating-point computations are significantly slower for denormal numbers.
In my experience, denormal numbers often appear in model weights, when you have vanishing gradients and/or recurrent neural connections. The fix for me was to simply manually set all the denormalized weights to 0. This shouldn't significantly change the outputs of your model since the denormalized numbers are so small in the first place.
import numpy as np
import mxnet as mx

model = com.load_hybrid(model_path)
for p, v in model.collect_params().items():
    vd = v.data().asnumpy()
    vd[np.abs(vd) &lt; 1e-37] = 0.0
    v.set_data(mx.nd.array(vd))
		</comment>
		<comment id='2' author='konioyxgq' date='2020-04-05T08:55:55Z'>
		Thank you very much for your reply.
This situation occurs. Is it because of what went wrong with my training? Or is there a problem with my model? Do you have any suggestions?
Thank you very muchÔºÅ
		</comment>
		<comment id='3' author='konioyxgq' date='2020-04-05T12:55:08Z'>
		I don't think, this situation necessarily means, that there is something wrong with your training/model. It just means that some weights are slowly converging to 0s during training.
If you are worried about your models correctness, you can manually check, which parameters are close to 0 after training. But unless it's some obvious problem, I don't think, that there's something wrong with parameters converging to 0s.
In fact, some regularization techniques explicitly aim for such convergence because if you have a whole row or column of 0s in your weights, you can remove that feature from your network without changing the output, while also saving on computation complexity.
		</comment>
		<comment id='4' author='konioyxgq' date='2020-04-06T01:27:22Z'>
		Thank you again for your patient response. Thank you very much!!
		</comment>
	</comments>
</bug>