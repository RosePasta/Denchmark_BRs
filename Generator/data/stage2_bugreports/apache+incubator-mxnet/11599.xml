<bug id='11599' author='junrushao1994' open_date='2018-07-07T06:52:45Z' closed_time='2018-08-16T00:07:19Z'>
	<summary>Autograd fails when using `take` operator repeatedly</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

Here provides a strange example that a backward pass on the accumulated sum of a 2-d NDArray sc, may potentially cause autograd to fail, depending on the shape of sc.
&lt;denchmark-h:h2&gt;Minimum reproducible example&lt;/denchmark-h&gt;

TL;DR. This code calculates x = sum(sc[i, j] for each i, j), and then invokes a backward pass x.backward(). The error message says that it could not calculate the gradient w.r.t. some variables, but I didn't require gradient for non-differentiable variables like i or j.
import mxnet as mx

def _array(shape):
    """Create an NDArray with random entries and the given shape
    """
    return mx.nd.random.uniform(-1.0, 1.0, shape=shape, dtype="float32")

def index(sc, i, j):
    """Equivalent to sc[i, j] in numpy
    """
    return sc.take(i).squeeze(axis=0)  \
             .take(j).squeeze(axis=0)

# tweaking `sc` in different shapes, the code sometimes fails, sometimes not
row_len, col_len = 2, 8

# the scanned variable
sc = _array((row_len, col_len))
sc.attach_grad()  # we only require the gradient w.r.t. `sc`

# `i`, `j` are loop variables which don't require grad
i = mx.nd.array([0], dtype="int64")
j = mx.nd.array([0], dtype="int64")

with mx.autograd.record(train_mode=True):
    xs = []
    for _ in range(row_len):
        x_i = []
        for _ in range(col_len):
            x_ij = index(sc, i, j)
            x_i.append(x_ij)
            j = j + 1
        i = i + 1
        j = j - col_len  # reset j
        xs.append(mx.nd.stack(*x_i))
    x = mx.nd.stack(*xs)
    x = x.sum()

x.backward()
print(sc.grad.asnumpy())   # the expected result should be all `1`s
&lt;denchmark-h:h2&gt;Error Message&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;&gt;&gt; python2 test.py
/home/ubuntu/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Traceback (most recent call last):
  File "test.py", line 33, in &lt;module&gt;
    print(sc.grad.asnumpy())
  File "/home/ubuntu/Projects/mxnet/python/mxnet/ndarray/ndarray.py", line 1910, in asnumpy
    ctypes.c_size_t(data.size)))
  File "/home/ubuntu/Projects/mxnet/python/mxnet/base.py", line 210, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [06:43:28] src/operator/tensor/./indexing_op.h:829: Check failed: req[take_::kIdx] == kNullOp (1 vs. 0) take layer doesn't support gradient into index

Stack trace returned 10 entries:
[bt] (0) /home/ubuntu/Projects/mxnet/python/mxnet/../../lib/libmxnet.so(dmlc::StackTrace[abi:cxx11]()+0x5b) [0x7f740cb6231b]
[bt] (1) /home/ubuntu/Projects/mxnet/python/mxnet/../../lib/libmxnet.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x28) [0x7f740cb62b58]
[bt] (2) /home/ubuntu/Projects/mxnet/python/mxnet/../../lib/libmxnet.so(void mxnet::op::TakeOpBackward&lt;mshadow::cpu&gt;(nnvm::NodeAttrs const&amp;, mxnet::OpContext const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;)+0x220) [0x7f740e46c710]
[bt] (3) /home/ubuntu/Projects/mxnet/python/mxnet/../../lib/libmxnet.so(mxnet::imperative::PushFCompute(std::function&lt;void (nnvm::NodeAttrs const&amp;, mxnet::OpContext const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;)&gt; const&amp;, nnvm::Op const*, nnvm::NodeAttrs const&amp;, mxnet::Context const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::Resource, std::allocator&lt;mxnet::Resource&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, std::vector&lt;unsigned int, std::allocator&lt;unsigned int&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;)::{lambda(mxnet::RunContext)#1}::operator()(mxnet::RunContext) const+0x291) [0x7f740f26a7b1]
[bt] (4) /home/ubuntu/Projects/mxnet/python/mxnet/../../lib/libmxnet.so(std::_Function_handler&lt;void (mxnet::RunContext), mxnet::engine::ThreadedEngine::BulkAppend(std::function&lt;void (mxnet::RunContext)&gt;, mxnet::Context, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;)::{lambda(mxnet::RunContext)#1}&gt;::_M_invoke(std::_Any_data const&amp;, mxnet::RunContext&amp;&amp;)+0x68) [0x7f740f7a7538]
[bt] (5) /home/ubuntu/Projects/mxnet/python/mxnet/../../lib/libmxnet.so(std::_Function_handler&lt;void (mxnet::RunContext), mxnet::engine::ThreadedEngine::BulkAppend(std::function&lt;void (mxnet::RunContext)&gt;, mxnet::Context, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;)::{lambda(mxnet::RunContext)#1}&gt;::_M_invoke(std::_Any_data const&amp;, mxnet::RunContext&amp;&amp;)+0x47) [0x7f740f7a7517]
[bt] (6) /home/ubuntu/Projects/mxnet/python/mxnet/../../lib/libmxnet.so(std::_Function_handler&lt;void (mxnet::RunContext), mxnet::engine::ThreadedEngine::BulkAppend(std::function&lt;void (mxnet::RunContext)&gt;, mxnet::Context, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;)::{lambda(mxnet::RunContext)#1}&gt;::_M_invoke(std::_Any_data const&amp;, mxnet::RunContext&amp;&amp;)+0x47) [0x7f740f7a7517]
[bt] (7) /home/ubuntu/Projects/mxnet/python/mxnet/../../lib/libmxnet.so(std::_Function_handler&lt;void (mxnet::RunContext), mxnet::engine::ThreadedEngine::BulkAppend(std::function&lt;void (mxnet::RunContext)&gt;, mxnet::Context, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;)::{lambda(mxnet::RunContext)#1}&gt;::_M_invoke(std::_Any_data const&amp;, mxnet::RunContext&amp;&amp;)+0x47) [0x7f740f7a7517]
[bt] (8) /home/ubuntu/Projects/mxnet/python/mxnet/../../lib/libmxnet.so(std::_Function_handler&lt;void (mxnet::RunContext), mxnet::engine::ThreadedEngine::BulkAppend(std::function&lt;void (mxnet::RunContext)&gt;, mxnet::Context, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;)::{lambda(mxnet::RunContext)#1}&gt;::_M_invoke(std::_Any_data const&amp;, mxnet::RunContext&amp;&amp;)+0x47) [0x7f740f7a7517]
[bt] (9) /home/ubuntu/Projects/mxnet/python/mxnet/../../lib/libmxnet.so(std::_Function_handler&lt;void (mxnet::RunContext), mxnet::engine::ThreadedEngine::BulkAppend(std::function&lt;void (mxnet::RunContext)&gt;, mxnet::Context, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;)::{lambda(mxnet::RunContext)#1}&gt;::_M_invoke(std::_Any_data const&amp;, mxnet::RunContext&amp;&amp;)+0x47) [0x7f740f7a7517]
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Steps to reproduce&lt;/denchmark-h&gt;


Save the code as test.py, and run python2 test.py

&lt;denchmark-h:h2&gt;What have you tried to solve it?&lt;/denchmark-h&gt;

I have no idea where it comes from. That by tweaking row_len and col_len, it behaves differently seems weird to me. I only observed that

When row_len = 1, it never fails
When row_len = 2, and col_len &lt; 8, it doesn't fails; when col_len &gt;= 8, it fails
When row_len = 3, and col_len &lt; 7, it doesn't fails; when col_len &gt;= 7, it fails
When row_len = 4, and col_len &lt; 7, it doesn't fails; when col_len &gt;= 7, it fails

&lt;denchmark-h:h2&gt;Environment info&lt;/denchmark-h&gt;

Not relevant though.
&lt;denchmark-code&gt;----------Python Info----------
('Version      :', '2.7.15')
('Compiler     :', 'GCC 7.2.0')
('Build        :', ('default', 'May  1 2018 23:32:55'))
('Arch         :', ('64bit', ''))
------------Pip Info-----------
('Version      :', '10.0.1')
('Directory    :', '/home/ubuntu/anaconda2/lib/python2.7/site-packages/pip')
----------MXNet Info-----------
/home/ubuntu/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
('Version      :', '1.3.0')
('Directory    :', '/home/ubuntu/Projects/junru-mxnet/python/mxnet')
Hashtag not found. Not installed from pre-built package.
----------System Info----------
('Platform     :', 'Linux-4.4.0-1062-aws-x86_64-with-debian-stretch-sid')
('system       :', 'Linux')
('node         :', 'ip-172-31-42-30')
('release      :', '4.4.0-1062-aws')
('version      :', '#71-Ubuntu SMP Fri Jun 15 10:07:39 UTC 2018')
----------Hardware Info----------
('machine      :', 'x86_64')
('processor    :', 'x86_64')
Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                72
On-line CPU(s) list:   0-71
Thread(s) per core:    2
Core(s) per socket:    18
Socket(s):             2
NUMA node(s):          2
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 85
Model name:            Intel(R) Xeon(R) Platinum 8124M CPU @ 3.00GHz
Stepping:              3
CPU MHz:               3000.000
BogoMIPS:              6000.00
Hypervisor vendor:     KVM
Virtualization type:   full
L1d cache:             32K
L1i cache:             32K
L2 cache:              1024K
L3 cache:              25344K
NUMA node0 CPU(s):     0-17,36-53
NUMA node1 CPU(s):     18-35,54-71
Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single kaiser fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f rdseed adx smap clflushopt clwb avx512cd xsaveopt xsavec xgetbv1 ida arat
----------Network Test----------
Setting timeout: 10
Timing for MXNet: https://github.com/apache/incubator-mxnet, DNS: 0.0019 sec, LOAD: 0.5934 sec.
Timing for PYPI: https://pypi.python.org/pypi/pip, DNS: 0.0070 sec, LOAD: 0.1024 sec.
Timing for FashionMNIST: https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/fashion-mnist/train-labels-idx1-ubyte.gz, DNS: 0.0099 sec, LOAD: 0.5567 sec.
Timing for Conda: https://repo.continuum.io/pkgs/free/, DNS: 0.0035 sec, LOAD: 0.0305 sec.
Timing for Gluon Tutorial(en): http://gluon.mxnet.io, DNS: 0.0994 sec, LOAD: 0.5750 sec.
Timing for Gluon Tutorial(cn): https://zh.gluon.ai, DNS: 0.1169 sec, LOAD: 0.5358 sec.
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Build info&lt;/denchmark-h&gt;

Compiler: gcc
MXNet commit hash: &lt;denchmark-link:https://github.com/apache/incubator-mxnet/commit/dd954b45a35d5eb9e8cdb6c6e19dd872a0a3d84d&gt;dd954b4&lt;/denchmark-link&gt;
 (current HEAD)
Build config: default
	</description>
	<comments>
		<comment id='1' author='junrushao1994' date='2018-07-07T06:54:01Z'>
		&lt;denchmark-link:https://github.com/haojin2&gt;@haojin2&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/zheng-da&gt;@zheng-da&lt;/denchmark-link&gt;
 Could you help take a look at this? Did I misunderstand anything about the  operator? Thanks!
		</comment>
		<comment id='2' author='junrushao1994' date='2018-07-09T23:09:05Z'>
		&lt;denchmark-link:https://github.com/sandeep-krishnamurthy&gt;@sandeep-krishnamurthy&lt;/denchmark-link&gt;
 Could you please label this issue: ,
		</comment>
		<comment id='3' author='junrushao1994' date='2018-07-09T23:11:12Z'>
		&lt;denchmark-link:https://github.com/junrushao1994&gt;@junrushao1994&lt;/denchmark-link&gt;
 will take a look later today
		</comment>
		<comment id='4' author='junrushao1994' date='2018-07-11T00:43:44Z'>
		So the problem is that the req type of the indices is not null, maybe this is due to the usage, I'll take a deeper dive into your issue later.
		</comment>
		<comment id='5' author='junrushao1994' date='2018-07-31T18:55:52Z'>
		Hey any updates?
		</comment>
		<comment id='6' author='junrushao1994' date='2018-08-02T00:26:05Z'>
		Fix in &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/11983&gt;#11983&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='junrushao1994' date='2018-08-02T00:37:43Z'>
		Thank you &lt;denchmark-link:https://github.com/haojin2&gt;@haojin2&lt;/denchmark-link&gt;
! You rock!
		</comment>
		<comment id='8' author='junrushao1994' date='2018-08-02T00:40:17Z'>
		Will close the issue once &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/11983&gt;#11983&lt;/denchmark-link&gt;
 is merged
		</comment>
		<comment id='9' author='junrushao1994' date='2018-08-16T00:07:19Z'>
		Closed since &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/11983&gt;#11983&lt;/denchmark-link&gt;
 is merged. Thank you &lt;denchmark-link:https://github.com/haojin2&gt;@haojin2&lt;/denchmark-link&gt;
!
		</comment>
	</comments>
</bug>