<bug id='17818' author='keerthanvasist' open_date='2020-03-11T20:13:21Z' closed_time='2020-04-03T18:08:52Z'>
	<summary>RNN operator produces inconsistent gradients for h2h_bias for stacked RNNs</summary>
	<description>
The RNN operator produces different inconsistent values for gradient for h2h_bias in the topmost stack in different mxnet variants.
This only happens when num_layers &gt; 1. I compared the values after one forward pass and one backward pass. I compared between osx-cpu-mkl and linux-gpu-mkl (both built from source. Snapshot versions are not up-to-date, and the versions available are really flaky in this case). In the cpu variant, the gradient for is all zeros.
These differences are leading to exception in DJL in CPU context. I also confirmed these differences in the values exist in Python.
&lt;denchmark-h:h3&gt;Error Message&lt;/denchmark-h&gt;

I don't see any error in Python but in DJL we have observed Nan values during the training process.
&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

I used the following script to look at gradients.
&lt;denchmark-code&gt;import mxnet as mx
from mxnet import gpu, cpu, gluon
from mxnet import np, npx
from mxnet import autograd as ag
from mxnet.gluon import nn

npx.set_np()

def check_param(net):
    param_dict = net.collect_params()
    array = ('lstm0_{}{}_{}_{}'.format(d, l, g, t)
             for t in ['weight', 'bias']
             for l in range(2)
             for d in ['l', 'r'][:1]
             for g in ['i2h', 'h2h'])
    for key in array:
        param = param_dict[key]
        print("checking param: " + str(param))
        print("weight sum: " + str(param.data().sum()))
        print("weight mean: " + str(param.data().mean()))
        print("weight max: " + str(param.data().max()))
        print("weight min: " + str(param.data().min()))
        if param.grad_req != "null":
            print("checking the gradient of para: " + str(param))
            print("grad sum: " + str(param.grad().sum()))
            print("grad mean: " + str(param.grad().mean()))
            print("grad max: " + str(param.grad().max()))
            print("grad min: " + str(param.grad().min()))

def print_ndarray_stats(ndarray, name) :
    print("#####", name, "#####")
    print("checking " + name)
    print("sum: " + str(ndarray.sum()))
    print("mean: " + str(ndarray.mean()))
    print("max: " + str(ndarray.max()))
    print("min: " + str(ndarray.min()))
    print("Shape: " + str(ndarray.shape))

batch = 32
time = 28
channel = 28
state = 64
num_layers = 2

mx.random.seed(1234)
data = np.random.uniform(0, 10, size=(batch, time, channel))
mx.random.seed(1234)
labels = np.random.uniform(0, 1, size=(batch, time, state))

net = gluon.rnn.LSTM(state, num_layers=2, h2h_weight_initializer=mx.initializer.Xavier(), i2h_weight_initializer=mx.initializer.Xavier(), layout='NTC')
loss = gluon.loss.SoftmaxCrossEntropyLoss(sparse_label=False, from_logits=True)
net.collect_params().initialize()

with ag.record():
    z = net(data)
    L = loss(z, labels).mean()
    print_ndarray_stats(z, "OUTPUT")
    print("Loss = ", L)
L.backward()
check_param(net)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Steps to reproduce&lt;/denchmark-h&gt;

(Paste the commands you ran that produced the error.)

Install the appropriate mxnet version
Run the above script (python rnntest.py)

	</description>
	<comments>
		<comment id='1' author='keerthanvasist' date='2020-03-11T22:32:11Z'>
		&lt;denchmark-link:https://github.com/mxnet-label-bot&gt;@mxnet-label-bot&lt;/denchmark-link&gt;
 add [Backend]
		</comment>
		<comment id='2' author='keerthanvasist' date='2020-03-11T22:42:52Z'>
		cc &lt;denchmark-link:https://github.com/pengzhao-intel&gt;@pengzhao-intel&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='keerthanvasist' date='2020-03-12T00:57:34Z'>
		&lt;denchmark-link:https://github.com/zixuanweeei&gt;@zixuanweeei&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ciyongch&gt;@ciyongch&lt;/denchmark-link&gt;
 please help take a look of this issue, thanks.
		</comment>
		<comment id='4' author='keerthanvasist' date='2020-03-12T02:24:27Z'>
		Thanks for reporting this issue. I can reproduce the result by your script. I will take a look. &lt;denchmark-link:https://github.com/keerthanvasist&gt;@keerthanvasist&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='keerthanvasist' date='2020-03-12T02:54:36Z'>
		&lt;denchmark-link:https://github.com/keerthanvasist&gt;@keerthanvasist&lt;/denchmark-link&gt;
 thanks to reporting the issue.
Could you give us some background on your application, DJL, and how it works?
		</comment>
		<comment id='6' author='keerthanvasist' date='2020-03-12T03:35:54Z'>
		DJL is an open-source, framework-agnostic deep learning java library. You can check it out at &lt;denchmark-link:https://djl.ai&gt;https://djl.ai&lt;/denchmark-link&gt;
. We are using the MxNet engine through a JNA layer.
		</comment>
		<comment id='7' author='keerthanvasist' date='2020-03-12T03:57:34Z'>
		Hi, &lt;denchmark-link:https://github.com/keerthanvasist&gt;@keerthanvasist&lt;/denchmark-link&gt;
 . Thanks for the information about the DJL.
We provided a patch for this issue &lt;denchmark-link:https://github.com/zixuanweeei/mxnet/tree/rnn/gradients&gt;https://github.com/zixuanweeei/mxnet/tree/rnn/gradients&lt;/denchmark-link&gt;
. It would be highly appreciated if you could give it a try. BTW, we need some extra fix for the unit test before it is ready to be merged into MASTER branch.
		</comment>
		<comment id='8' author='keerthanvasist' date='2020-03-12T04:59:40Z'>
		&lt;denchmark-link:https://github.com/zixuanweeei&gt;@zixuanweeei&lt;/denchmark-link&gt;
 Thank for your quick response. We will try it out and let you know the results.
By the way, we need this fix in 1.7.x branch as well.
		</comment>
		<comment id='9' author='keerthanvasist' date='2020-03-13T02:02:43Z'>
		&lt;denchmark-link:https://github.com/zixuanweeei&gt;@zixuanweeei&lt;/denchmark-link&gt;
 Thank you for your fix. I verified that it does fix the issue in CPU. We verified it with the 1.7.x branch as well.
		</comment>
		<comment id='10' author='keerthanvasist' date='2020-03-13T02:40:10Z'>
		
@zixuanweeei Thank you for your fix. I verified that it does fix the issue in CPU. We verified it with the 1.7.x branch as well.

Thanks for trying out this. PR is on the way.
		</comment>
	</comments>
</bug>