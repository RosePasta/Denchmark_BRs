<bug id='18475' author='nttstar' open_date='2020-06-03T02:30:54Z' closed_time='2020-07-21T22:55:01Z'>
	<summary>BatchNorm can not converge with scale=False</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

BatchNorm operator with scale=False can not converge.
&lt;denchmark-h:h3&gt;Error Message&lt;/denchmark-h&gt;

No error message, but loss value and training accuracy is abnormal comparing with scale=True BatchNorm.
&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

We can try https://github.com/nttstar/arcface.np to train arcface. Add one BatchNorm op with scale=False after final embedding layer
&lt;denchmark-h:h2&gt;What have you tried to solve it?&lt;/denchmark-h&gt;


Set Scale=True, it can work but with slightly worse test accuracy.

&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;

----------Python Info----------
Version      : 3.6.9
Compiler     : GCC 7.3.0
Build        : ('default', 'Jul 30 2019 19:07:31')
Arch         : ('64bit', '')
------------Pip Info-----------
Version      : 19.3.1
Directory    : /root/anaconda2/envs/py36/lib/python3.6/site-packages/pip
----------MXNet Info-----------
Version      : 2.0.0
Directory    : /root/anaconda2/envs/py36/lib/python3.6/site-packages/mxnet
Num GPUs     : 8
Hashtag not found. Not installed from pre-built package.
----------System Info----------
Platform     : Linux-3.10.0-327.el7.x86_64-x86_64-with-centos-7.5.1804-Core
system       : Linux
node         : gpu06
release      : 3.10.0-327.el7.x86_64
version      : &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/1&gt;#1&lt;/denchmark-link&gt;
 SMP Thu Nov 19 22:10:57 UTC 2015
	</description>
	<comments>
		<comment id='1' author='nttstar' date='2020-06-04T23:55:32Z'>
		Hi &lt;denchmark-link:https://github.com/nttstar&gt;@nttstar&lt;/denchmark-link&gt;
 , there was a bug in BatchNorm in the previous version of MXNet. The bug was reported in &lt;denchmark-link:https://github.com/apache/incubator-mxnet/issues/18373&gt;#18373&lt;/denchmark-link&gt;
 , and it was fixed in PR &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/18377&gt;#18377&lt;/denchmark-link&gt;
 . Could you please try the latest version of MXNet?
		</comment>
		<comment id='2' author='nttstar' date='2020-06-05T05:02:57Z'>
		&lt;denchmark-link:https://github.com/wkcn&gt;@wkcn&lt;/denchmark-link&gt;
 I communicated with &lt;denchmark-link:https://github.com/nttstar&gt;@nttstar&lt;/denchmark-link&gt;
 offline and &lt;denchmark-link:https://github.com/apache/incubator-mxnet/issues/18373&gt;#18373&lt;/denchmark-link&gt;
 should not be the casue. Would you help try with  and see if there is anything wrong?
		</comment>
		<comment id='3' author='nttstar' date='2020-06-05T09:14:03Z'>
		&lt;denchmark-link:https://github.com/sxjscience&gt;@sxjscience&lt;/denchmark-link&gt;
 I'm sorry that I don't have any machine with GPU to check it recently.
I read the code of batch norm and its unittest.
There is a gradient check when fix_gamma=True:
&lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/tests/python/unittest/test_operator.py#L1777&gt;https://github.com/apache/incubator-mxnet/blob/master/tests/python/unittest/test_operator.py#L1777&lt;/denchmark-link&gt;
,
but no output check when fix_gamma=True:
&lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/tests/python/unittest/test_operator.py#L1882&gt;https://github.com/apache/incubator-mxnet/blob/master/tests/python/unittest/test_operator.py#L1882&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='nttstar' date='2020-06-05T09:51:41Z'>
		I try to test the batch norm with fix_gamma=True.
The result on CPU is right, but that on GPU is wrong when fix_gamma=True, axis = 1 and cudnn_off=False. The bn_beta.grad is the only wrong value.
Here are the failure cases when fix_gamma=True.



operator
shape
axis
cudnn_off
output_mean_var




BatchNorm
(24, 2)
1
False
False


BatchNorm
(24, 2)
1
False
True


BatchNorm
(24, 3, 4)
1
False
False


BatchNorm
(24, 3, 4)
1
False
True


BatchNorm
(24, 4, 4, 4)
1
False
False


BatchNorm
(24, 4, 4, 4)
1
False
True


BatchNorm
(24, 8, 4, 4)
1
False
False


BatchNorm
(24, 8, 4, 4)
1
False
True


BatchNorm
(24, 5, 6, 4, 4)
1
False
False


BatchNorm
(24, 5, 6, 4, 4)
1
False
True



		</comment>
		<comment id='5' author='nttstar' date='2020-06-05T11:06:24Z'>
		I'm fixing the bug and I will submit a PR later.
		</comment>
		<comment id='6' author='nttstar' date='2020-06-10T02:15:55Z'>
		Hi &lt;denchmark-link:https://github.com/nttstar&gt;@nttstar&lt;/denchmark-link&gt;
 , the bug of BatchNorm has been fixed in &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/18500&gt;#18500&lt;/denchmark-link&gt;
 .
Thank you for the report!
		</comment>
		<comment id='7' author='nttstar' date='2020-06-10T02:39:14Z'>
		&lt;denchmark-link:https://github.com/wkcn&gt;@wkcn&lt;/denchmark-link&gt;
 Thanks! I will check it in the next pip package release.
		</comment>
	</comments>
</bug>