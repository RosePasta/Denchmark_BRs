<bug id='13135' author='kohr-h' open_date='2018-11-06T13:25:39Z' closed_time='2019-06-26T23:41:08Z'>
	<summary>[Python] CUDNN error from 3D deconvolution</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

When I build a simple net (Gluon interface) that involves a 3D deconvolution, and run it on some sample data, I get an error indicating that CUDNN couldn't find an algorithm.
&lt;denchmark-h:h2&gt;Environment info (Required)&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;----------Python Info----------
Version      : 3.6.7
Compiler     : MSC v.1915 64 bit (AMD64)
Build        : ('default', 'Oct 28 2018 19:44:12')
Arch         : ('64bit', 'WindowsPE')
------------Pip Info-----------
Version      : 18.1
Directory    : C:\Users\Holger\AppData\Local\conda\conda\envs\mxnet\lib\site-packages\pip
----------MXNet Info-----------
Version      : 1.3.1
Directory    : c:\users\holger\git\mxnet\python\mxnet
Hashtag not found. Not installed from pre-built package.
----------System Info----------
Platform     : Windows-10-10.0.17134-SP0
system       : Windows
node         : DESKTOP-3DBNGT7
release      : 10
version      : 10.0.17134
----------Hardware Info----------
machine      : AMD64
processor    : Intel64 Family 6 Model 85 Stepping 4, GenuineIntel
Name
Intel(R) Xeon(R) W-2175 CPU @ 2.50GHz
&lt;/denchmark-code&gt;

Package used (Python/R/Scala/Julia): Python
&lt;denchmark-h:h2&gt;Build info (Required if built from source)&lt;/denchmark-h&gt;

Compiler (gcc/clang/mingw/visual studio): VC++ 14.11
MXNet commit hash: &lt;denchmark-link:https://github.com/apache/incubator-mxnet/commit/a91b364c490b163da424a1c2935652ea7b799050&gt;a91b364&lt;/denchmark-link&gt;

Build config:

CUDA 9.2
CUDNN 7.3
no MKL-DNN

&lt;denchmark-h:h2&gt;Error Message:&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;[14:23:43] c:\users\holger\git\mxnet\src\operator\nn\cudnn\./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)
[14:23:43] c:\users\holger\git\mxnet\src\operator\nn\cudnn\./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)
Traceback (most recent call last):
  File "tmp.py", line 15, in &lt;module&gt;
    output_npy = output.asnumpy()
  File "c:\users\holger\git\mxnet\python\mxnet\ndarray\ndarray.py", line 1980, in asnumpy
    ctypes.c_size_t(data.size)))
  File "c:\users\holger\git\mxnet\python\mxnet\base.py", line 252, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [14:23:43] c:\users\holger\git\mxnet\src\operator\nn\./cudnn/cudnn_deconvolution-inl.h:849: Failed to find any forward deconvolution algorithm with workspace size of 536870912 bytes, please consider reducing batch/model size or increasing the workspace size
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Minimum reproducible example&lt;/denchmark-h&gt;

import mxnet as mx
from mxnet import nd
from mxnet.gluon import nn

net = nn.Sequential()
net.add(nn.Conv3D(1, (3, 3, 3), padding=(1, 1, 1)))
# Without the following line, everything is okay
net.add(nn.Conv3DTranspose(1, (3, 3, 3), padding=(1, 1, 1)))

net.initialize(ctx=mx.gpu(0))

input = nd.zeros((1, 1, 64, 64, 64), ctx=mx.gpu(0))
output = net(input)

output_npy = output.asnumpy()
	</description>
	<comments>
		<comment id='1' author='kohr-h' date='2018-11-06T13:32:26Z'>
		Note: If I replace the last line with output = output.as_in_context(mx.cpu()), there's no problem. Likely because copyto does not check _LIB.MXGetLastError() like asnumpy does. The curious thing is that the code blows up at a completely unrelated place to the original error.
		</comment>
		<comment id='2' author='kohr-h' date='2018-11-06T13:39:42Z'>
		Furthermore: If I leave out the last line and do python -i tmp.py I observe this:
&gt;&gt;&gt; [14:34:59] c:\users\holger\git\mxnet\src\operator\nn\cudnn\./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)
[14:34:59] c:\users\holger\git\mxnet\src\operator\nn\cudnn\./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)

&gt;&gt;&gt; output.shape
(1, 1, 64, 64, 64)
&gt;&gt;&gt; from mxnet.base import _LIB, py_str
&gt;&gt;&gt; py_str(_LIB.MXGetLastError())
''
&gt;&gt;&gt; input = input.asnumpy()
&gt;&gt;&gt; py_str(_LIB.MXGetLastError())
''
&gt;&gt;&gt; output = output.asnumpy()
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
  File "c:\users\holger\git\mxnet\python\mxnet\ndarray\ndarray.py", line 1980, in asnumpy
    ctypes.c_size_t(data.size)))
  File "c:\users\holger\git\mxnet\python\mxnet\base.py", line 252, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [14:34:59] c:\users\holger\git\mxnet\src\operator\nn\./cudnn/cudnn_deconvolution-inl.h:849: Failed to find any forward deconvolution algorithm with workspace size of 536870912 bytes, please consider reducing batch/model size or increasing the workspace size
So the error has in fact not even occurred before the call to asnumpy.
		</comment>
		<comment id='3' author='kohr-h' date='2018-11-06T13:53:37Z'>
		More digging, again from python -i tmp.py:
&gt;&gt;&gt; import ctypes, numpy as np
&gt;&gt;&gt; from mxnet.base import _LIB, py_str
&gt;&gt;&gt; self = output
&gt;&gt;&gt; data = np.ones(self.shape, dtype=self.dtype)  # use ones to see when contents change
&gt;&gt;&gt; # First call fails
&gt;&gt;&gt; _LIB.MXNDArraySyncCopyToCPU(self.handle, data.ctypes.data_as(ctypes.c_void_p), ctypes.c_size_t(data.size))
-1
&gt;&gt;&gt; py_str(_LIB.MXGetLastError())
'[14:49:18] c:\\users\\holger\\git\\mxnet\\src\\operator\\nn\\./cudnn/cudnn_deconvolution-inl.h:849: Failed to find any forward deconvolution algorithm with workspace size of 536870912 bytes, please consider reducing batch/model size or increasing the workspace size'
&gt;&gt;&gt; # Nothing changed in `data`
&gt;&gt;&gt; data.ravel()[:10]
array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)
&gt;&gt;&gt; # Second call succeeds
&gt;&gt;&gt; _LIB.MXNDArraySyncCopyToCPU(self.handle, data.ctypes.data_as(ctypes.c_void_p), ctypes.c_size_t(data.size))
0
&gt;&gt;&gt; # Something changed, not clear if it makes sense
&gt;&gt;&gt; data.ravel()[:10]
array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)
		</comment>
		<comment id='4' author='kohr-h' date='2018-11-06T19:53:40Z'>
		&lt;denchmark-link:https://github.com/kohr-h&gt;@kohr-h&lt;/denchmark-link&gt;

Thank you for submitting the issue! I'm labeling it so MXNet community members can help resolve it.
If you have built from source, can you please share the make file flags that you have used?
Thanks
&lt;denchmark-link:https://github.com/mxnet-label-bot&gt;@mxnet-label-bot&lt;/denchmark-link&gt;
 [Gluon, CUDA, Windows, Bug]
		</comment>
		<comment id='5' author='kohr-h' date='2018-11-08T09:50:32Z'>
		&lt;denchmark-link:https://gist.github.com/kohr-h/7d35e349d44dcb66ef0c1929a659522d&gt;Here&lt;/denchmark-link&gt;
's a link to my .
		</comment>
		<comment id='6' author='kohr-h' date='2018-11-08T09:52:16Z'>
		I saw that some of the recent commits changed code in the affected header, so I tried again with &lt;denchmark-link:https://github.com/apache/incubator-mxnet/commit/a32fa840262cdad5e7556a53d7ce3d7218ae7120&gt;a32fa84&lt;/denchmark-link&gt;
. Same result as before, unfortunately.
		</comment>
		<comment id='7' author='kohr-h' date='2018-11-27T17:41:10Z'>
		Update: I just ran the minimal example on the same machine under Linux (although with CUDA 10), and the error is the same:
&lt;denchmark-code&gt;&gt;&gt;&gt; import mxnet as mx
&gt;&gt;&gt; from mxnet import nd
&gt;&gt;&gt; from mxnet.gluon import nn
&gt;&gt;&gt; 
&gt;&gt;&gt; net = nn.Sequential()
&gt;&gt;&gt; net.add(nn.Conv3D(1, (3, 3, 3), padding=(1, 1, 1)))
&gt;&gt;&gt; # Without the following line, everything is okay
&gt;&gt;&gt; net.add(nn.Conv3DTranspose(1, (3, 3, 3), padding=(1, 1, 1)))
&gt;&gt;&gt; 
&gt;&gt;&gt; net.initialize(ctx=mx.gpu(0))
&gt;&gt;&gt; 
&gt;&gt;&gt; input = nd.zeros((1, 1, 64, 64, 64), ctx=mx.gpu(0))
&gt;&gt;&gt; output = net(input)
&gt;&gt;&gt; 
&gt;&gt;&gt; output_npy = output.asnumpy()
[18:33:58] src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)
[18:33:58] src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)
---------------------------------------------------------------------------
MXNetError                                Traceback (most recent call last)
&lt;ipython-input-1-d0c61cb28340&gt; in &lt;module&gt;
     13 output = net(input)
     14
---&gt; 15 output_npy = output.asnumpy()

~/git/mxnet_cu100/python/mxnet/ndarray/ndarray.py in asnumpy(self)
   1978             self.handle,
   1979             data.ctypes.data_as(ctypes.c_void_p),
-&gt; 1980             ctypes.c_size_t(data.size)))
   1981         return data
   1982

~/git/mxnet_cu100/python/mxnet/base.py in check_call(ret)
    250     """
    251     if ret != 0:
--&gt; 252         raise MXNetError(py_str(_LIB.MXGetLastError()))
    253
    254

MXNetError: [18:33:58] src/operator/nn/./cudnn/cudnn_deconvolution-inl.h:849: Failed to find any forward deconvolution algorithm with workspace size of 536870912 bytes, please consider reducing batch/model size or increasing the workspace size

Stack trace returned 10 entries:
[bt] (0) /home/hkohr/git/mxnet_cu100/python/mxnet/../../lib/libmxnet.so(dmlc::StackTrace[abi:cxx11]()+0x53) [0x7f53485996a3]
[bt] (1) /home/hkohr/git/mxnet_cu100/python/mxnet/../../lib/libmxnet.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x39) [0x7f5348599f39]
[bt] (2) /home/hkohr/git/mxnet_cu100/python/mxnet/../../lib/libmxnet.so(mxnet::op::CuDNNDeconvolutionOp&lt;float&gt;::CuDNNAlgoSetter(mxnet::RunContext const&amp;, std::vector&lt;nnvm::TShape, std::allocator&lt;nnvm::TShape&gt; &gt; const&amp;, std::vector&lt;nnvm::TShape, std::allocator&lt;nnvm::TShape&gt; &gt; const&amp;, cudnnDataType_t, cudnnDataType_t, mxnet::op::CuDNNAlgo&lt;cudnnConvolutionFwdAlgo_t&gt;*, mxnet::op::CuDNNAlgo&lt;cudnnConvolutionBwdDataAlgo_t&gt;*, mxnet::op::CuDNNAlgo&lt;cudnnConvolutionBwdFilterAlgo_t&gt;*)+0x7af) [0x7f534d6d15df]
[bt] (3) /home/hkohr/git/mxnet_cu100/python/mxnet/../../lib/libmxnet.so(std::_Function_handler&lt;void (mxnet::op::CuDNNAlgo&lt;cudnnConvolutionFwdAlgo_t&gt;*, mxnet::op::CuDNNAlgo&lt;cudnnConvolutionBwdDataAlgo_t&gt;*,mxnet::op::CuDNNAlgo&lt;cudnnConvolutionBwdFilterAlgo_t&gt;*), mxnet::op::CuDNNDeconvolutionOp&lt;float&gt;::SelectAlgo(mxnet::RunContext const&amp;, std::vector&lt;nnvm::TShape, std::allocator&lt;nnvm::TShape&gt; &gt; const&amp;, std::vector&lt;nnvm::TShape, std::allocator&lt;nnvm::TShape&gt; &gt; const&amp;, cudnnDataType_t, cudnnDataType_t)::{lambda(mxnet::op::CuDNNAlgo&lt;cudnnConvolutionFwdAlgo_t&gt;*, mxnet::op::CuDNNAlgo&lt;cudnnConvolutionBwdDataAlgo_t&gt;*, mxnet::op::CuDNNAlgo&lt;cudnnConvolutionBwdFilterAlgo_t&gt;*)#1}&gt;::_M_invoke(std::_Any_data const&amp;, mxnet::op::CuDNNAlgo&lt;cudnnConvolutionFwdAlgo_t&gt;*&amp;&amp;, mxnet::op::CuDNNAlgo&lt;cudnnConvolutionBwdDataAlgo_t&gt;*&amp;&amp;, mxnet::op::CuDNNAlgo&lt;cudnnConvolutionBwdFilterAlgo_t&gt;*&amp;&amp;)+0xae2) [0x7f534d6eeba2]
[bt] (4) /home/hkohr/git/mxnet_cu100/python/mxnet/../../lib/libmxnet.so(mxnet::op::CuDNNAlgoReg&lt;mxnet::op::DeconvolutionParam&gt;::FindOrElseRegister(mxnet::op::DeconvolutionParam const&amp;, std::vector&lt;nnvm::TShape, std::allocator&lt;nnvm::TShape&gt; &gt; const&amp;, std::vector&lt;nnvm::TShape, std::allocator&lt;nnvm::TShape&gt; &gt; const&amp;, cudnnDataType_t, cudnnDataType_t, cudnnDataType_t, int, bool, mxnet::op::CuDNNAlgo&lt;cudnnConvolutionFwdAlgo_t&gt;*, mxnet::op::CuDNNAlgo&lt;cudnnConvolutionBwdDataAlgo_t&gt;*, mxnet::op::CuDNNAlgo&lt;cudnnConvolutionBwdFilterAlgo_t&gt;*, std::function&lt;void (mxnet::op::CuDNNAlgo&lt;cudnnConvolutionFwdAlgo_t&gt;*, mxnet::op::CuDNNAlgo&lt;cudnnConvolutionBwdDataAlgo_t&gt;*, mxnet::op::CuDNNAlgo&lt;cudnnConvolutionBwdFilterAlgo_t&gt;*)&gt; const&amp;)+0x18ae) [0x7f534d6f219e]
[bt] (5) /home/hkohr/git/mxnet_cu100/python/mxnet/../../lib/libmxnet.so(mxnet::op::CuDNNDeconvolutionOp&lt;float&gt;::SelectAlgo(mxnet::RunContext const&amp;, std::vector&lt;nnvm::TShape, std::allocator&lt;nnvm::TShape&gt; &gt; const&amp;, std::vector&lt;nnvm::TShape, std::allocator&lt;nnvm::TShape&gt; &gt; const&amp;, cudnnDataType_t, cudnnDataType_t)+0x2e1) [0x7f534d6f2aa1]
[bt] (6) /home/hkohr/git/mxnet_cu100/python/mxnet/../../lib/libmxnet.so(+0x5bf6785) [0x7f534d6a2785]
[bt] (7) /home/hkohr/git/mxnet_cu100/python/mxnet/../../lib/libmxnet.so(void mxnet::op::DeconvolutionCompute&lt;mshadow::gpu&gt;(nnvm::NodeAttrs const&amp;, mxnet::OpContext const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;)+0x625) [0x7f534d6a69b5]
[bt] (8) /home/hkohr/git/mxnet_cu100/python/mxnet/../../lib/libmxnet.so(mxnet::imperative::PushFCompute(std::function&lt;void (nnvm::NodeAttrs const&amp;, mxnet::OpContext const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;)&gt; const&amp;, nnvm::Op const*, nnvm::NodeAttrs const&amp;, mxnet::Context const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::Resource, std::allocator&lt;mxnet::Resource&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;,std::vector&lt;unsigned int, std::allocator&lt;unsigned int&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;)::{lambda(mxnet::RunContext)#1}::operator()(mxnet::RunContext) const+0x244) [0x7f534b0cb5f4]
[bt] (9) /home/hkohr/git/mxnet_cu100/python/mxnet/../../lib/libmxnet.so(+0x3be8c1b) [0x7f534b694c1b]

&lt;/denchmark-code&gt;

 I guess I can't.
&lt;denchmark-link:https://github.com/mxnet-label-bot&gt;@mxnet-label-bot&lt;/denchmark-link&gt;
 remove [Windows]
		</comment>
		<comment id='8' author='kohr-h' date='2019-03-27T02:58:12Z'>
		I am faced with the same problem. Did u fix it?
		</comment>
		<comment id='9' author='kohr-h' date='2019-03-27T03:05:03Z'>
		The problem is:  when the 'padding=0', everything is ok. But padding=1 may cause this problem.
So, right now, I set 'kernel size=2,padding=0' in order to shape matching....Hope fix it ASAP~~~~
		</comment>
		<comment id='10' author='kohr-h' date='2019-04-09T22:04:31Z'>
		Removing the Windows label :
&lt;denchmark-link:https://github.com/mxnet-label-bot&gt;@mxnet-label-bot&lt;/denchmark-link&gt;
 Update[ Bug,  CUDA,  Gluon]
		</comment>
		<comment id='11' author='kohr-h' date='2019-06-05T20:21:34Z'>
		Reproducible in Ubuntu 16.04, mxnet 1.4.1, cuda 9.2, cudnn 7.3.1
		</comment>
		<comment id='12' author='kohr-h' date='2019-06-05T22:39:50Z'>
		Using the following condensed example for debugging:
&lt;denchmark-code&gt;import mxnet as mx
from mxnet import nd
from mxnet.gluon import nn

in_data = nd.ones(shape=(1, 1, 2, 2, 2),ctx=mx.gpu())
layer = nn.Conv3DTranspose(channels=1, kernel_size=3, padding=1)

layer.initialize(ctx=mx.gpu())
output = layer(in_data)
#print(output)
&lt;/denchmark-code&gt;

I try to print output after running the example and the error is raised but if I print again the result is of the correct shape but all zeros:
&lt;denchmark-code&gt;&gt;&gt;&gt; output
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
  File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/ndarray/ndarray.py", line 189, in __repr__
    return '\n%s\n&lt;%s %s @%s&gt;' % (str(self.asnumpy()),
  File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/ndarray/ndarray.py", line 1980, in asnumpy
    ctypes.c_size_t(data.size)))
  File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/base.py", line 252, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [22:32:23] src/operator/nn/./cudnn/cudnn_deconvolution-inl.h:849: Failed to find any forward deconvolution algorithm with workspace size of 536870912 bytes, please consider reducing batch/model size or increasing the workspace size

Stack trace returned 10 entries:
[bt] (0) /home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x40b29a) [0x7f0a155e129a]
[bt] (1) /home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x40b8b1) [0x7f0a155e18b1]
[bt] (2) /home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x37c9c44) [0x7f0a1899fc44]
[bt] (3) /home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x37cdf33) [0x7f0a189a3f33]
[bt] (4) /home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x37b586c) [0x7f0a1898b86c]
[bt] (5) /home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x37b5cee) [0x7f0a1898bcee]
[bt] (6) /home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x37b6d38) [0x7f0a1898cd38]
[bt] (7) /home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x37b7ca7) [0x7f0a1898dca7]
[bt] (8) /home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x37bd981) [0x7f0a18993981]
[bt] (9) /home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so(mxnet::imperative::PushFCompute(std::function&lt;void (nnvm::NodeAttrs const&amp;, mxnet::OpContext const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;)&gt; const&amp;, nnvm::Op const*, nnvm::NodeAttrs const&amp;, mxnet::Context const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::Resource, std::allocator&lt;mxnet::Resource&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, std::vector&lt;unsigned int, std::allocator&lt;unsigned int&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;)::{lambda(mxnet::RunContext)#1}::operator()(mxnet::RunContext) const+0x2e8) [0x7f0a17f44fc8]


&gt;&gt;&gt; output

[[[[[0. 0.]
    [0. 0.]]

   [[0. 0.]
    [0. 0.]]]]]
&lt;NDArray 1x1x2x2x2 @gpu(0)&gt;
&lt;/denchmark-code&gt;

		</comment>
		<comment id='13' author='kohr-h' date='2019-06-07T17:53:02Z'>
		Some more digging, tried doing output.as_in_context(mx.cpu()) before printing. The first time the error pops up and on the second time its able to print, but the values look incorrect.
&lt;denchmark-code&gt;&gt;&gt;&gt; output = output.as_in_context(mx.cpu())
&gt;&gt;&gt; output
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
  File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/ndarray/ndarray.py", line 189, in __repr__
    return '\n%s\n&lt;%s %s @%s&gt;' % (str(self.asnumpy()),
  File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/ndarray/ndarray.py", line 1980, in asnumpy
    ctypes.c_size_t(data.size)))
  File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/base.py", line 252, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [17:40:57] src/operator/nn/./cudnn/cudnn_deconvolution-inl.h:849: Failed to find any forward deconvolution algorithm with workspace size of 536870912 bytes, please consider reducing batch/model size or increasing the workspace size

Stack trace returned 10 entries:
[bt] (0) /home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x40b29a) [0x7f2f5ee5b29a]
[bt] (1) /home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x40b8b1) [0x7f2f5ee5b8b1]
[bt] (2) /home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x37c9c44) [0x7f2f62219c44]
[bt] (3) /home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x37cdf33) [0x7f2f6221df33]
[bt] (4) /home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x37b586c) [0x7f2f6220586c]
[bt] (5) /home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x37b5cee) [0x7f2f62205cee]
[bt] (6) /home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x37b6d38) [0x7f2f62206d38]
[bt] (7) /home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x37b7ca7) [0x7f2f62207ca7]
[bt] (8) /home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x37bd981) [0x7f2f6220d981]
[bt] (9) /home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so(mxnet::imperative::PushFCompute(std::function&lt;void (nnvm::NodeAttrs const&amp;, mxnet::OpContext const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;)&gt; const&amp;, nnvm::Op const*, nnvm::NodeAttrs const&amp;, mxnet::Context const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::Resource, std::allocator&lt;mxnet::Resource&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, std::vector&lt;unsigned int, std::allocator&lt;unsigned int&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;)::{lambda(mxnet::RunContext)#1}::operator()(mxnet::RunContext) const+0x2e8) [0x7f2f617befc8]


&gt;&gt;&gt; output

[[[[[-4.8527788e-24  4.5624877e-41]
    [ 5.0263566e-08  3.0661812e-41]]

   [[ 2.8025969e-45  0.0000000e+00]
    [ 2.8025969e-45  0.0000000e+00]]]]]
&lt;NDArray 1x1x2x2x2 @cpu(0)&gt;
&lt;/denchmark-code&gt;

Also as pointed out by &lt;denchmark-link:https://github.com/Aktcob&gt;@Aktcob&lt;/denchmark-link&gt;
 padding=0 works without any issue.
		</comment>
		<comment id='14' author='kohr-h' date='2019-06-07T20:24:26Z'>
		Looks like the issue has already been fixed. I ran the example on the latest nightly package using pip install mxnet-cu92 --pre and also on the latest source build and am able to get correct output:
&lt;denchmark-code&gt;&gt;&gt;&gt; output

[[[[[0.08305737 0.13836569]
    [0.08325263 0.1620139 ]]

   [[0.028945   0.06969839]
    [0.00803468 0.10540323]]]]]
&lt;NDArray 1x1x2x2x2 @gpu(0)&gt;
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/kohr-h&gt;@kohr-h&lt;/denchmark-link&gt;
 Can you check again?
		</comment>
		<comment id='15' author='kohr-h' date='2019-06-26T23:41:08Z'>
		Resolving as the issue is fixed in latest master. Please reopen if you still find any issues.
		</comment>
	</comments>
</bug>