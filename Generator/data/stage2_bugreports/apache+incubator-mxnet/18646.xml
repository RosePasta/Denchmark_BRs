<bug id='18646' author='stu1130' open_date='2020-06-30T21:07:10Z' closed_time='2020-07-09T17:06:10Z'>
	<summary>BatchNorm with axis=-1 is much slower than axis=1</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import mxnet as mx
from mxnet import autograd, np, npx, gluon, init
from mxnet.gluon import nn
import time

npx.set_np()

data = mx.np.random.uniform(size=(32, 100, 100), ctx=mx.gpu())
label = mx.np.ones((32, 100, 100), ctx=mx.gpu())
net = nn.Sequential()
net.add(nn.BatchNorm(axis=-1))
net.initialize(init.Xavier(), ctx=mx.gpu())
loss = gluon.loss.L2Loss()
t = time.time()
for _ in range(5000):
    with autograd.record():
        l = loss(net(data), label)
    l.backward()
mx.nd.waitall()
print('spent: {}s'.format(time.time() - t))
&lt;/denchmark-code&gt;

MXNet version: static build with branch v1.7x commit &lt;denchmark-link:https://github.com/apache/incubator-mxnet/commit/75ab15569bd0f20a90806ce2fc38df08be208ed7&gt;75ab155&lt;/denchmark-link&gt;

I  got around 5 sec with axis=1 and 30 sec with axis=-1 on P3.8xlarge (V100).
Both of case are computing the 32 * 100 data for each axis
similar to &lt;denchmark-link:https://github.com/apache/incubator-mxnet/issues/10095&gt;#10095&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;Solution&lt;/denchmark-h&gt;

Thanks &lt;denchmark-link:https://github.com/ptrendx&gt;@ptrendx&lt;/denchmark-link&gt;
 to point out that cudnn 7.4 (&lt;denchmark-link:https://docs.nvidia.com/deeplearning/sdk/cudnn-release-notes/rel_7xx.html#rel_741&gt;https://docs.nvidia.com/deeplearning/sdk/cudnn-release-notes/rel_7xx.html#rel_741&lt;/denchmark-link&gt;
) added a new cudnnBatchNormalization*Ex API that gives much better speed for axis = -1
	</description>
	<comments>
		<comment id='1' author='stu1130' date='2020-07-01T00:01:20Z'>
		The reason is that MKLDNN and CuDNN are only applied when axis = 1.
The open PR &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/18504&gt;#18504&lt;/denchmark-link&gt;
 fixes it.
However, we will replace mkldnn_off and cudnn_off attributes with environment variables, so the PR is blocked.
		</comment>
		<comment id='2' author='stu1130' date='2020-07-01T00:11:07Z'>
		&lt;denchmark-link:https://github.com/wkcn&gt;@wkcn&lt;/denchmark-link&gt;
 Thanks for you detailed explanation.
So I think there are two phrases.

enable cuDNN when axis is not 1
use cudnnBatchNormalizationForwardTrainingEx for NHWC case (I checked the source code, we are all using cudnnBatchNormalizationForwardTraining)

		</comment>
		<comment id='3' author='stu1130' date='2020-07-06T15:09:57Z'>
		I think NHWC layout is very important in point cloud algorithms.
		</comment>
		<comment id='4' author='stu1130' date='2020-07-09T17:06:09Z'>
		I have verified the performance is almost the same after the fix &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/18504&gt;#18504&lt;/denchmark-link&gt;
. Close the issue
		</comment>
	</comments>
</bug>