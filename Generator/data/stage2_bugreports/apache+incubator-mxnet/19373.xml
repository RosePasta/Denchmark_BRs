<bug id='19373' author='Neutron3529' open_date='2020-10-18T08:12:11Z' closed_time='2020-10-21T11:56:18Z'>
	<summary>what's wrong with PR test?</summary>
	<description>
currently, many PR failed due to bad network connection.
not only me myself, but also many PR is affected.
&lt;denchmark-code&gt;[2020-10-17T00:02:07.148Z] 2020-10-17 00:02:05,525 - root - ERROR - ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))

[2020-10-17T00:02:07.148Z] Traceback (most recent call last):

[2020-10-17T00:02:07.148Z]   File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 601, in urlopen

[2020-10-17T00:02:07.148Z]     chunked=chunked)

[2020-10-17T00:02:07.148Z]   File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 387, in _make_request

[2020-10-17T00:02:07.148Z]     six.raise_from(e, None)

[2020-10-17T00:02:07.148Z]   File "&lt;string&gt;", line 3, in raise_from

[2020-10-17T00:02:07.148Z]   File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 383, in _make_request

[2020-10-17T00:02:07.148Z]     httplib_response = conn.getresponse()

[2020-10-17T00:02:07.148Z]   File "/usr/lib/python3.6/http/client.py", line 1346, in getresponse

[2020-10-17T00:02:07.148Z]     response.begin()

[2020-10-17T00:02:07.148Z]   File "/usr/lib/python3.6/http/client.py", line 307, in begin

[2020-10-17T00:02:07.148Z]     version, status, reason = self._read_status()

[2020-10-17T00:02:07.148Z]   File "/usr/lib/python3.6/http/client.py", line 268, in _read_status

[2020-10-17T00:02:07.148Z]     line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")

[2020-10-17T00:02:07.148Z]   File "/usr/lib/python3.6/socket.py", line 586, in readinto

[2020-10-17T00:02:07.148Z]     return self._sock.recv_into(b)

[2020-10-17T00:02:07.148Z] ConnectionResetError: [Errno 104] Connection reset by peer

&lt;/denchmark-code&gt;

such exception could not resolved by any PR.
I re-run test for at least 5 times, and most of the test failed.
what's wrong?
	</description>
	<comments>
		<comment id='1' author='Neutron3529' date='2020-10-19T16:03:42Z'>
		It's a CI issue and need to be fixed. It's unrelated to your PR.
cc &lt;denchmark-link:https://github.com/sandeep-krishnamurthy&gt;@sandeep-krishnamurthy&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/josephevans&gt;@josephevans&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='Neutron3529' date='2020-10-19T20:33:32Z'>
		I think there's two issues today. First, I found an expired GPG key that was preventing R packages from being installed on Ubuntu 16.04. I created PR &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/19377&gt;#19377&lt;/denchmark-link&gt;
 for this. Waiting for my PR to pass to backport.
Second, I see errors trying to uncompress the Packages file from &lt;denchmark-link:https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/&gt;https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/&lt;/denchmark-link&gt;

It looks like a new file was recently pushed, so maybe this has been fixed by nvidia.
		</comment>
		<comment id='3' author='Neutron3529' date='2020-10-19T21:03:50Z'>
		&lt;denchmark-link:https://github.com/josephevans&gt;@josephevans&lt;/denchmark-link&gt;
 thank you for looking into the issue. But please note that the issues you mention are unrelated as they only affect the v1.x branch, whereas the issue described here affects the master branch.
		</comment>
		<comment id='4' author='Neutron3529' date='2020-10-19T22:32:29Z'>
		The nvidia issue may affect more than the 16.04 mentioned above. &lt;denchmark-link:https://github.com/NVIDIA/nvidia-docker/issues/1402&gt;NVIDIA/nvidia-docker#1402&lt;/denchmark-link&gt;
 contains some more info (though the issue was closed as it isn't directed to the correct owner)
		</comment>
		<comment id='5' author='Neutron3529' date='2020-10-20T06:23:52Z'>
		Ok, I believe I finally found the culprit. Our AMIs that are used for Jenkins slaves have auto-update turned on, and based on the logfiles of the slave instances, it looks like docker was being auto-updated and restarted, which was killing the log-output of the containers (and therefore jenkins jobs.)
I've created a new AMI for mxnetlinux_cpu hosts with updated software versions, which also adds an option to the docker config to hopefully prevent this in the future.  See &lt;denchmark-link:https://docs.docker.com/config/containers/live-restore/&gt;https://docs.docker.com/config/containers/live-restore/&lt;/denchmark-link&gt;
  - Thanks &lt;denchmark-link:https://github.com/leezu&gt;@leezu&lt;/denchmark-link&gt;
 for the recommendation.
		</comment>
		<comment id='6' author='Neutron3529' date='2020-10-21T00:24:41Z'>
		CI seems much more stable today with the new AMI. Released an updated AMI to address ARMv8 test failures due to qemu installation. We should no longer be seeing the docker connection issues (or unexpected EOF errors) and 2 of 3 PRs to fix the other CI issue (expired GPG key) has been merged.
This issue can be closed.
		</comment>
		<comment id='7' author='Neutron3529' date='2020-10-21T00:40:56Z'>
		Thank you so much Joe.
		</comment>
		<comment id='8' author='Neutron3529' date='2020-10-21T11:56:18Z'>
		Seems the CI works now, close this issue.
		</comment>
	</comments>
</bug>