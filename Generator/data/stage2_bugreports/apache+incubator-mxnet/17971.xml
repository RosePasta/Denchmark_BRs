<bug id='17971' author='djaym7' open_date='2020-04-04T06:25:25Z' closed_time='2020-06-03T22:18:17Z'>
	<summary>DOT product too slow on CPU and GPU compared to np and pytorch</summary>
	<description>
Given below are times for CPU-np, mxnet-CPU, mxnet-GPU, pytorch-CPU, pytorch-GPU.
CPU times on numpy (np) and and pytorch CPU are comparable but mxnet is crazy slow.
GPU time on pytorch is way faster than mxnet.
hardware/software : sagemaker p3.2x latest (1.6.0 cu102mkl)
&lt;denchmark-link:https://user-images.githubusercontent.com/12378820/78420214-fa068e80-7601-11ea-9629-1f82cf7f8665.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/12378820/78420204-e6f3be80-7601-11ea-8aea-702cf7b0de52.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/12378820/78420290-89ac3d00-7602-11ea-8bbc-fbf05095c882.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/12378820/78420210-f1ae5380-7601-11ea-8a8f-c1616fffdb83.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='djaym7' date='2020-04-07T13:46:27Z'>
		&lt;denchmark-link:https://github.com/anko-intel&gt;@anko-intel&lt;/denchmark-link&gt;
 please help take a look for this issue, thanks.
		</comment>
		<comment id='2' author='djaym7' date='2020-04-29T19:12:23Z'>
		Hi &lt;denchmark-link:https://github.com/djaym7&gt;@djaym7&lt;/denchmark-link&gt;

I reproduced the issue locally on Skylake-X i9-7920X. But I am not sure if I observed the same issue. Could you run following python scripts on your environment and upload the results?
&lt;denchmark-link:https://github.com/apache/incubator-mxnet/files/4553919/test_dot_issue.py.txt&gt;test_dot_issue.py.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/apache/incubator-mxnet/files/4553920/test_dot_issue_logs.py.txt&gt;test_dot_issue_logs.py.txt&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='djaym7' date='2020-05-11T19:09:48Z'>
		&lt;denchmark-link:https://github.com/anko-intel&gt;@anko-intel&lt;/denchmark-link&gt;
 here are the results
&lt;denchmark-link:https://user-images.githubusercontent.com/12378820/81601763-44f18f80-9380-11ea-92a3-49506033ebd5.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/12378820/81601783-4de26100-9380-11ea-95b7-517490c7dad1.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='djaym7' date='2020-05-11T19:23:38Z'>
		Any relation to &lt;denchmark-link:https://github.com/apache/incubator-mxnet/issues/17980&gt;#17980&lt;/denchmark-link&gt;
 for cpu side? Are you comparing to mkl enabled np / pytorch?
		</comment>
		<comment id='5' author='djaym7' date='2020-05-12T00:28:27Z'>
		np is not using mkl
&lt;denchmark-link:https://user-images.githubusercontent.com/12378820/81624835-0d98d800-93ac-11ea-954f-878f5ef597da.png&gt;&lt;/denchmark-link&gt;

Pytorch is using mkl
&lt;denchmark-link:https://user-images.githubusercontent.com/12378820/81625114-cc54f800-93ac-11ea-8dc8-f16c21048e0d.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='djaym7' date='2020-05-25T19:43:25Z'>
		Hi &lt;denchmark-link:https://github.com/djaym7&gt;@djaym7&lt;/denchmark-link&gt;

Thank you for your results.  I observe some similarity in the results measured locally on  Skylake-X i9-7920X   on MxNet 1.6.0 cu102mkl binary.  The only exception is the time for 512x512 tensor on MxNet(?).
MxNet compiled from master branch (on &lt;denchmark-link:https://github.com/apache/incubator-mxnet/commit/b2144777b0b49e3e899f3f8af8afe760eaa9da3e&gt;b214477&lt;/denchmark-link&gt;
 - fix (&lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/18313&gt;#18313&lt;/denchmark-link&gt;
))  uses MKL if available, and the results are much better. But Mxnet is still worse than NumPy for smaller tensors.
&lt;denchmark-link:https://user-images.githubusercontent.com/58251767/82837689-13bda700-9eca-11ea-90af-0fbd15f9e2c5.png&gt;&lt;/denchmark-link&gt;

Additional measurements on the master with MxNet Profiler enabled  show that &gt; 80us is spent between python and time noted by Profiler for dot operation.
It seems to be an already know issue  &lt;denchmark-link:https://github.com/apache/incubator-mxnet/issues/14883&gt;#14883&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/apache/incubator-mxnet/issues/17097&gt;#17097&lt;/denchmark-link&gt;
 regarding passing python/C++ barrier. For me it sounds like fixing python-MXNet binding overhead issue should also fix this issue.
&lt;denchmark-link:https://user-images.githubusercontent.com/58251767/82837709-20da9600-9eca-11ea-8df1-75f4e8f0be56.png&gt;&lt;/denchmark-link&gt;

Results in table below, neglecting measurement noise,  shows that differences between time measured in python and MKL are almost the same as between python and MXNet Profiler, so it confirms python &lt;-&gt; C++ API issue.
&lt;denchmark-link:https://user-images.githubusercontent.com/58251767/82837739-364fc000-9eca-11ea-827c-5202bfcf87b1.png&gt;&lt;/denchmark-link&gt;

In the last table there are results for MxNet when both profiler and MKL verbose are enabled (adding additional time for both measurements). We can see here that the difference between python time and profile time is similar to the results in the previous tables and it is the most significant one.
&lt;denchmark-link:https://user-images.githubusercontent.com/58251767/82837761-4cf61700-9eca-11ea-8eba-8091f0c48deb.png&gt;&lt;/denchmark-link&gt;

Exact results of my measurements could be find in logs: &lt;denchmark-link:https://github.com/apache/incubator-mxnet/files/4678496/dot_issue_logs.zip&gt;dot_issue_logs.zip&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='djaym7' date='2020-06-03T01:56:30Z'>
		&lt;denchmark-link:https://github.com/djaym7&gt;@djaym7&lt;/denchmark-link&gt;
 please take a review whether &lt;denchmark-link:https://github.com/anko-intel&gt;@anko-intel&lt;/denchmark-link&gt;
 answered your question :)
		</comment>
	</comments>
</bug>