<bug id='14858' author='sandeep-krishnamurthy' open_date='2019-05-01T23:56:52Z' closed_time='2019-05-07T05:33:05Z'>
	<summary>add_n operator with MXNet-MKL producing wrong results when input count &amp;gt;4</summary>
	<description>
Problem:
With mxnet-mkl (1.4.0)
If number of input symbols &gt; 4 and I perform add_n after a FC layer produces wrong results.
i.e.,
&lt;denchmark-code&gt;data_0 -&gt; fc_0  \
data_1 -&gt; fc_1   \ 
data_2 -&gt; fc_2      =&gt; add_n
data_3 -&gt; fc_3  /
data_4 -&gt; fc_4 /
&lt;/denchmark-code&gt;

Minimum reproducible code below:
Run below code which is full network:
import mxnet as mx

num_inp_symbols = 5
data_shape = (5,5)
hidden_layer_size = 8

input_symbols = [mx.sym.var('data_'+str(i)) for i in range(num_inp_symbols)]
fully_connected_symbols = [mx.sym.FullyConnected(data=input_symbols[i], 
                                                                                          num_hidden=hidden_layer_size, 
                                                                                          name='fc_'+str(i))
                                                for i in range(num_datasets)]

#Create final symbol
net = mx.sym.add_n(*fully_connected_symbols)
#Validate topology
#mx.viz.plot_network(net)

mod = mx.mod.Module(symbol=net, data_names=['data_0', 'data_1', 'data_2', 'data_3', 'data_4'], label_names=None)
mod.bind(for_training=False, data_shapes=[('data_0', data_shape), ('data_1', data_shape), ('data_2', data_shape), ('data_3', data_shape), ('data_4', data_shape)])
mod.set_params(full_module.get_params()[0], full_module.get_params()[1])

mod.forward(mx.io.DataBatch([mx.nd.ones(data_shape), mx.nd.ones(data_shape), mx.nd.ones(data_shape), mx.nd.ones(data_shape), mx.nd.ones(data_shape)]))
print(mod.get_outputs()[0])
Output
&lt;denchmark-code&gt;[[ 2.2989948  -3.3271918   0.64880913  2.2778904   0.9859241   2.0046096
  -1.6065626   1.5986269 ]
 [ 2.2989948  -3.3271918   0.64880913  2.2778904   0.9859241   2.0046096
  -1.6065626   1.5986269 ]
 [ 2.2989948  -3.3271918   0.64880913  2.2778904   0.9859241   2.0046096
  -1.6065626   1.5986269 ]
 [ 2.2989948  -3.3271918   0.64880913  2.2778904   0.9859241   2.0046096
  -1.6065626   1.5986269 ]
 [ 2.2989948  -3.3271918   0.64880913  2.2778904   0.9859241   2.0046096
  -1.6065626   1.5986269 ]]
&lt;NDArray 5x8 @cpu(0)&gt;
&lt;/denchmark-code&gt;

However, Let us now compute output of each FC in above network (fc0_output, fc1_output,... fc4_output). What I observe is the if I do individual fc output calculation and sum it up it is not same result as running everything together.
&lt;denchmark-code&gt;constituent_fc0 = fully_connected_symbols[0]
print(constituent_fc0.get_internals().list_outputs())

mod_cons_fc0 = mx.mod.Module(symbol=constituent_fc0, data_names=['data_0'], label_names=None)
mod_cons_fc0.bind(for_training=False, data_shapes=[('data_0', data_shape)])
mod_cons_fc0.set_params(mod.get_params()[0], mod.get_params()[1])
mod_cons_fc0.forward(mx.io.DataBatch([mx.nd.ones(data_shape)]))
o1 = mod_cons_fc0.get_outputs()[0]

#and so on for fc1, fc2, fc3, fc4
#and then do
print(nd.add_n(o1, o2, o3, o4, o5))
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/ZhennanQin&gt;@ZhennanQin&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/pengzhao-intel&gt;@pengzhao-intel&lt;/denchmark-link&gt;
 - Can you please help debug this issue?
Please Note:

storage type is all dense
Number of inputs &gt; 4
Happens only from Module APIs and from mxnet-mkl 1.3.0 version onwards.

	</description>
	<comments>
		<comment id='1' author='sandeep-krishnamurthy' date='2019-05-01T23:56:55Z'>
		Hey, this is the MXNet Label Bot.
Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it.
Here are my recommended labels: Bug
		</comment>
		<comment id='2' author='sandeep-krishnamurthy' date='2019-05-02T14:48:13Z'>
		we are on the public holiday this week :)
&lt;denchmark-link:https://github.com/rongzha1&gt;@rongzha1&lt;/denchmark-link&gt;
 please help take a look for this issue after the holiday.
		</comment>
		<comment id='3' author='sandeep-krishnamurthy' date='2019-05-05T15:09:31Z'>
		add_n output mem overlap with input mem (due to FInplaceOption), but in Forward function : ElementwiseSumContainsDnsImpl( ),  output mem was set_zero which makes input zero:     Kernel&lt;set_zero, cpu&gt;::Launch(s, out_data.Size(), out_data.dptr());
In fact, this is not a MKLDNN Bug.
&lt;denchmark-link:https://github.com/pengzhao-intel&gt;@pengzhao-intel&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/zheng-da&gt;@zheng-da&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/TaoLv&gt;@TaoLv&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='sandeep-krishnamurthy' date='2019-05-06T04:43:05Z'>
		&lt;denchmark-link:https://github.com/rongzha1&gt;@rongzha1&lt;/denchmark-link&gt;
 thanks for the analysis. Would you mind to file a PR and fix this issue?
		</comment>
		<comment id='5' author='sandeep-krishnamurthy' date='2019-05-06T04:45:11Z'>
		btw, please add the example as a test case.
		</comment>
		<comment id='6' author='sandeep-krishnamurthy' date='2019-05-07T05:33:05Z'>
		Good catch and fixed now :)
		</comment>
	</comments>
</bug>