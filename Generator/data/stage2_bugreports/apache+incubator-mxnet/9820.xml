<bug id='9820' author='marcoabreu' open_date='2018-02-18T11:13:26Z' closed_time='2018-03-01T10:55:31Z'>
	<summary>Flaky test_gluon_model_zoo_gpu.test_training @ Python3: MKLDNN-GPU</summary>
	<description>
&lt;denchmark-link:http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/incubator-mxnet/detail/PR-9809/4/pipeline&gt;http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/incubator-mxnet/detail/PR-9809/4/pipeline&lt;/denchmark-link&gt;

&lt;denchmark-link:http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/incubator-mxnet/detail/master/389/pipeline&gt;http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/incubator-mxnet/detail/master/389/pipeline&lt;/denchmark-link&gt;

&lt;denchmark-link:http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/incubator-mxnet/detail/master/391/pipeline&gt;http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/incubator-mxnet/detail/master/391/pipeline&lt;/denchmark-link&gt;

&lt;denchmark-link:http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/incubator-mxnet/detail/master/387/pipeline&gt;http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/incubator-mxnet/detail/master/387/pipeline&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;======================================================================

FAIL: test_gluon_model_zoo_gpu.test_training

----------------------------------------------------------------------

Traceback (most recent call last):

  File "/usr/local/lib/python3.5/dist-packages/nose/case.py", line 198, in runTest

    self.test(*self.arg)

  File "/workspace/tests/python/gpu/../unittest/common.py", line 155, in test_new

    orig_test(*args, **kwargs)

  File "/workspace/tests/python/gpu/test_gluon_model_zoo_gpu.py", line 159, in test_training

    assert_almost_equal(cpu_out.asnumpy(), gpu_out.asnumpy(), rtol=1e-2, atol=1e-2)

  File "/workspace/python/mxnet/test_utils.py", line 493, in assert_almost_equal

    raise AssertionError(msg)

AssertionError: 

Items are not equal:

Error 84.491165 exceeds tolerance rtol=0.010000, atol=0.010000.  Location of maximum error:(9, 877), a=0.816808, b=-0.181214

 a: array([[ 0.46908194,  0.25131682,  0.27432024, ..., -0.3243659 ,

        -0.8637756 ,  0.57461524],

       [ 0.18945426,  0.32339886,  0.18884647, ...,  0.00044107,...

 b: array([[ 0.34904164,  0.3221189 ,  0.05189171, ..., -0.14858764,

        -0.9381798 ,  0.39666444],

       [ 0.3438487 ,  0.4455883 ,  0.32494336, ...,  0.15454161,...

-------------------- &gt;&gt; begin captured logging &lt;&lt; --------------------

urllib3.connectionpool: DEBUG: Starting new HTTP connection (1): data.mxnet.io

urllib3.connectionpool: DEBUG: http://data.mxnet.io:80 "GET /data/val-5k-256.rec HTTP/1.1" 200 150874780

root: INFO: downloaded http://data.mxnet.io/data/val-5k-256.rec into data/val-5k-256.rec successfully

common: INFO: Setting test np/mx/python random seeds, use MXNET_TEST_SEED=1760574333 to reproduce.

--------------------- &gt;&gt; end captured logging &lt;&lt; ---------------------

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='marcoabreu' date='2018-02-18T17:03:00Z'>
		I think we should disable it for now.
I run the tests many times. It seems both CPU and GPU occasionally generate
incorrect results. I need more time to test the tests.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Sun, Feb 18, 2018 at 3:13 AM Marco de Abreu ***@***.***&gt; wrote:
 ´´´ FAIL: test_gluon_model_zoo_gpu.test_training

 Traceback (most recent call last):
 File "/usr/local/lib/python2.7/dist-packages/nose/case.py", line 197, in
 runTest
 self.test(*self.arg)
 File "/workspace/tests/python/gpu/test_gluon_model_zoo_gpu.py", line 150,
 in test_training
 assert_almost_equal(cpu_out.asnumpy(), gpu_out.asnumpy(), rtol=1e-2,
 atol=1e-2)
 File "/workspace/python/mxnet/test_utils.py", line 495, in
 assert_almost_equal
 raise AssertionError(msg)
 AssertionError:
 Items are not equal:
 Error 76.641708 exceeds tolerance rtol=0.010000, atol=0.010000. Location
 of maximum error:(9, 304), a=0.754152, b=-0.052508
 a: array([[ 0.45648926, -0.19809955, 0.35798055, ..., 1.0708873 ,
 -0.3380539 , -0.22070615],
 [ 0.42138988, 0.02357741, 0.38420224, ..., 1.0584493 ,...
 b: array([[ 0.28696495, -0.17991166, 0.37088192, ..., 1.0100358 ,
 -0.3728746 , -0.14392784],
 [ 0.27283525, 0.03558442, 0.45698166, ..., 0.9900025 ,...
 -------------------- &gt;&gt; begin captured logging &lt;&lt; --------------------
 urllib3.connectionpool: DEBUG: Starting new HTTP connection (1):
 data.mxnet.io
 urllib3.connectionpool: DEBUG: http://data.mxnet.io:80 "GET
 /data/val-5k-256.rec HTTP/1.1" 200 150874780
 root: INFO: downloaded http://data.mxnet.io/data/val-5k-256.rec into
 data/val-5k-256.rec successfully
 --------------------- &gt;&gt; end captured logging &lt;&lt; ---------------------
 ´´´

 —
 You are receiving this because you are subscribed to this thread.
 Reply to this email directly, view it on GitHub
 &lt;#9820&gt;, or mute the
 thread
 &lt;https://github.com/notifications/unsubscribe-auth/AAETUbgbE8AKGK5QtCNCn119Qr8M2JNlks5tWAXYgaJpZM4SJpzP&gt;
 .



		</comment>
		<comment id='2' author='marcoabreu' date='2018-02-18T17:05:37Z'>
		Sounds good. Since we've now got the CI randomness PR merged into the master, it should be no problem to generate faulty tests.
		</comment>
		<comment id='3' author='marcoabreu' date='2018-02-21T08:21:32Z'>
		&lt;denchmark-link:https://github.com/piiswrong&gt;@piiswrong&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/szha&gt;@szha&lt;/denchmark-link&gt;
 are you fine with disabling this test?
		</comment>
		<comment id='4' author='marcoabreu' date='2018-02-21T08:24:49Z'>
		Ah wait. I just checked a few runs of test failures and it ONLY happens with MKLDNN. It looks to me like this is not a flaky test but could rather have a problem in the underlying implementation.
&lt;denchmark-link:https://github.com/zheng-da&gt;@zheng-da&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/mli&gt;@mli&lt;/denchmark-link&gt;
 can you please make sure this gets resolved? This test is one of the biggest failure reasons and there might be a valid test failure.
		</comment>
		<comment id='5' author='marcoabreu' date='2018-02-21T15:49:50Z'>
		&lt;denchmark-link:https://github.com/marcoabreu&gt;@marcoabreu&lt;/denchmark-link&gt;
 Thanks for the testing.
Seems it's a non-deterministic failure both in GPU and CPU backend.
We will take look this case to figure out the root cause w/ .
BTW, the root cause of  &lt;denchmark-link:https://github.com/apache/incubator-mxnet/issues/9843&gt;#9843&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/apache/incubator-mxnet/issues/9844&gt;#9844&lt;/denchmark-link&gt;
 &amp; &lt;denchmark-link:https://github.com/apache/incubator-mxnet/issues/9845&gt;#9845&lt;/denchmark-link&gt;
 may be the same as this one so I think we can duplicate these issues and track the progress in one thread.
		</comment>
		<comment id='6' author='marcoabreu' date='2018-02-21T17:07:17Z'>
		At least &lt;denchmark-link:https://github.com/apache/incubator-mxnet/issues/9845&gt;#9845&lt;/denchmark-link&gt;
 is not MKLDNN specific - I've seen (deterministic) failures in that test before, see &lt;denchmark-link:https://github.com/apache/incubator-mxnet/issues/8780&gt;#8780&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='7' author='marcoabreu' date='2018-02-21T17:58:12Z'>
		&lt;denchmark-link:https://github.com/marcoabreu&gt;@marcoabreu&lt;/denchmark-link&gt;
 I was testing it yesterday. It seems the problem exists when MKLDNN is compiled into MXNet, so it should be a bug caused by the code inside USE_MKLDNN. I'll look into the problem this week and try to fix it as soon as possible.
I agree with &lt;denchmark-link:https://github.com/pengzhao-intel&gt;@pengzhao-intel&lt;/denchmark-link&gt;
 . This problem might be related to the other flaky failures.
		</comment>
		<comment id='8' author='marcoabreu' date='2018-02-21T21:51:10Z'>
		I run more tests. The bug occurs when MKLDNN is compiled into MXNet and we use the threaded engine, which is the default executing engine. If we use the naive executing engine, the problem doesn't show up. It means that the bug isn't in the operator implementation. It should be in the executor engine or in the NDArray.
		</comment>
		<comment id='9' author='marcoabreu' date='2018-02-21T21:55:05Z'>
		Thank you for the elaboration!
&lt;denchmark-link:https://github.com/szha&gt;@szha&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/piiswrong&gt;@piiswrong&lt;/denchmark-link&gt;
 what do you think?
		</comment>
		<comment id='10' author='marcoabreu' date='2018-02-21T21:57:06Z'>
		Most likely, what happens is that an operation needs to wait for some variables before proceeding to the next operator, but somehow the implementation doesn't wait for the right variables or misses some variables.
		</comment>
		<comment id='11' author='marcoabreu' date='2018-02-22T19:22:43Z'>
		After one day of testing, I think I know where the problem is now. Basically, the current implementation sometimes needs to convert data format (from the MKLDNN format to the default format) inside an NDArray. In the threaded execution engine, while an NDArray is being converted in a thread, the array can also be read by another thread. In this case, the other thread can read wrong data.
I changed the code to avoid data layout conversion inside an NDArray. It seems to work fine now. I'll have more tests.
		</comment>
		<comment id='12' author='marcoabreu' date='2018-02-22T19:26:44Z'>
		Great news, thanks for diving deep on this problem!
		</comment>
		<comment id='13' author='marcoabreu' date='2018-03-01T10:55:31Z'>
		Fixed as of &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/9862&gt;#9862&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>