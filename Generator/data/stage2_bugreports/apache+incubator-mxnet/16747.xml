<bug id='16747' author='leezu' open_date='2019-11-07T01:33:45Z' closed_time='2019-11-12T18:07:01Z'>
	<summary>Fused Op causes MXNetError</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

After &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/15167&gt;#15167&lt;/denchmark-link&gt;
 is merged, GluonNLP CI broke.
&lt;denchmark-h:h3&gt;Error Message&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;[2019-11-06T06:44:48.223Z] mxnet.base.MXNetError: Error in operator Embedding_Dropout_Embedding_Dropout__FusedOp__contrib_arange_like__FusedOp_broadcast_lesser__FusedOp_broadcast_mul__FusedOp_broadcast_mul_expand_dims_broadcast_axis_Embedding__FusedOp_broadcast_add_Dropout_amp_cast_amp_cast_amp_cast_LayerNorm_amp_cast_amp_cast_amp_cast_FullyConnected_Reshape_transpose_Reshape__contrib_div_sqrt_dim_amp_cast_amp_cast_FullyConnected_Reshape_transpose__FusedOp_batch_dot__FusedOp_where_amp_cast_softmax__FusedOp_Dropout_amp_cast_amp_cast_FullyConnected_Reshape_transpose__FusedOp_batch_dot_Reshape_transpose__FusedOp_amp_cast_amp_cast_FullyConnected_Dropout__FusedOp_amp_cast_amp_cast_LayerNorm_amp_cast_amp_cast_amp_cast_FullyConnected__FusedOp_amp_cast_amp_cast_FullyConnected_Dropout__FusedOp_amp_cast_amp_cast_LayerNorm_amp_cast_amp_cast_amp_cast_FullyConnected_Reshape_transpose_Reshape__contrib_div_sqrt_dim_amp_cast_amp_cast_FullyConnected_Reshape_transpose__FusedOp_batch_dot__FusedOp_where_amp_cast_softmax__FusedOp_Dropout_amp_cast_amp_cast_FullyConnected_Reshape_transpose__FusedOp_batch_dot_Reshape_transpose__FusedOp_amp_cast_amp_cast_FullyConnected_Dropout__FusedOp_amp_cast_amp_cast_LayerNorm_amp_cast_amp_cast_amp_cast_FullyConnected__FusedOp_amp_cast_amp_cast_FullyConnected_Dropout__FusedOp_amp_cast_amp_cast_LayerNorm_amp_cast_amp_cast_amp_cast_FullyConnected_Reshape_transpose_Reshape__contrib_div_sqrt_dim_amp_cast_amp_cast_FullyConnected_Reshape_transpose__FusedOp_batch_dot__FusedOp_where_amp_cast_softmax__FusedOp_Dropout_amp_cast_amp_cast_FullyConnected_Reshape_transpose__FusedOp_batch_dot_Reshape_transpose__FusedOp_amp_cast_amp_cast_FullyConnected_Dropout__FusedOp_amp_cast_amp_cast_LayerNorm_amp_cast_amp_cast_amp_cast_FullyConnected__FusedOp_amp_cast_amp_cast_FullyConnected_Dropout__FusedOp_amp_cast_amp_cast_LayerNorm_amp_cast_amp_cast_amp_cast_FullyConnected_Reshape_transpose_Reshape__contrib_div_sqrt_dim_amp_cast_amp_cast_FullyConnected_Reshape_transpose__FusedOp_batch_dot__FusedOp_where_amp_cast_softmax__FusedOp_Dropout_amp_cast_amp_cast_FullyConnected_Reshape_transpose__FusedOp_batch_dot_Reshape_transpose__FusedOp_amp_cast_amp_cast_FullyConnected_Dropout__FusedOp_amp_cast_amp_cast_LayerNorm_amp_cast_amp_cast_amp_cast_FullyConnected__FusedOp_amp_cast_amp_cast_FullyConnected_Dropout__FusedOp_amp_cast_amp_cast_LayerNorm_amp_cast_amp_cast_amp_cast_FullyConnected_Reshape_transpose_Reshape__contrib_div_sqrt_dim_amp_cast_amp_cast_FullyConnected_Reshape_transpose__FusedOp_batch_dot__FusedOp_where_amp_cast_softmax__FusedOp_Dropout_amp_cast_amp_cast_FullyConnected_Reshape_transpose__FusedOp_batch_dot_Reshape_transpose__FusedOp_amp_cast_amp_cast_FullyConnected_Dropout__FusedOp_amp_cast_amp_cast_LayerNorm_amp_cast_amp_cast_amp_cast_FullyConnected__FusedOp_amp_cast_amp_cast_FullyConnected_Dropout__FusedOp_amp_cast_amp_cast_LayerNorm_amp_cast_amp_cast_amp_cast_FullyConnected_Reshape_transpose_Reshape__contrib_div_sqrt_dim_amp_cast_amp_cast_FullyConnected_Reshape_transpose__FusedOp_batch_dot__FusedOp_where_amp_cast_softmax__FusedOp_Dropout_amp_cast_amp_cast_FullyConnected_Reshape_transpose__FusedOp_batch_dot_Reshape_transpose__FusedOp_amp_cast_amp_cast_FullyConnected_Dropout__FusedOp_amp_cast_amp_cast_LayerNorm_amp_cast_amp_cast_amp_cast_FullyConnected__FusedOp_amp_cast_amp_cast_FullyConnected_Dropout__FusedOp_amp_cast_amp_cast_LayerNorm_amp_cast_amp_cast_amp_cast_FullyConnected_Reshape_transpose_Reshape__contrib_div_sqrt_dim_amp_cast_amp_cast_FullyConnected_Reshape_transpose__FusedOp_batch_dot__FusedOp_where_amp_cast_softmax__FusedOp_Dropout_amp_cast_amp_cast_FullyConnected_Reshape_transpose__FusedOp_batch_dot_Reshape_transpose__FusedOp_amp_cast_amp_cast_FullyConnected_Dropout__FusedOp_amp_cast_amp_cast_LayerNorm_amp_cast_amp_cast_amp_cast_FullyConnected__FusedOp_amp_cast_amp_cast_FullyConnected_Dropout__FusedOp_amp_cast_amp_cast_LayerNorm_amp_cast_amp_cast_amp_cast_FullyConnected_Reshape_transpose_Reshape__contrib_div_sqrt_dim_amp_cast_amp_cast_FullyConnected_Reshape_transpose__FusedOp_batch_dot__FusedOp_where_amp_cast_softmax__FusedOp_Dropout_amp_cast_amp_cast_FullyConnected_Reshape_transpose__FusedOp_batch_dot_Reshape_transpose__FusedOp_amp_cast_amp_cast_FullyConnected_Dropout__FusedOp_amp_cast_amp_cast_LayerNorm_amp_cast_amp_cast_amp_cast_FullyConnected__FusedOp_amp_cast_amp_cast_FullyConnected_Dropout__FusedOp_amp_cast_amp_cast_LayerNorm_amp_cast_amp_cast_amp_cast_FullyConnected_Reshape_transpose_Reshape__contrib_div_sqrt_dim_amp_cast_amp_cast_FullyConnected_Reshape_transpose__FusedOp_batch_dot__FusedOp_where_amp_cast_softmax__FusedOp_Dropout_amp_cast_amp_cast_FullyConnected_Reshape_transpose__FusedOp_batch_dot_Reshape_transpose__FusedOp_amp_cast_amp_cast_FullyConnected_Dropout__FusedOp_amp_cast_amp_cast_LayerNorm_amp_cast_amp_cast_amp_cast_FullyConnected__FusedOp_amp_cast_amp_cast_FullyConnected_Dropout__FusedOp_amp_cast_amp_cast_LayerNorm_amp_cast_amp_cast_amp_cast_FullyConnected_Reshape_transpose_Reshape__contrib_div_sqrt_dim_amp_cast_amp_cast_FullyConnected_Reshape_transpose__FusedOp_batch_dot__FusedOp_where_amp_cast_softmax__FusedOp_Dropout_amp_cast_amp_cast_FullyConnected_Reshape_transpose__FusedOp_batch_dot_Reshape_transpose__FusedOp_amp_cast_amp_cast_FullyConnected_Dropout__FusedOp_amp_cast_amp_cast_LayerNorm_amp_cast_amp_cast_amp_cast_FullyConnected__FusedOp_amp_cast_amp_cast_FullyConnected_Dropout__FusedOp_amp_cast_amp_cast_LayerNorm_amp_cast_amp_cast_amp_cast_FullyConnected_Reshape_transpose_Reshape__contrib_div_sqrt_dim_amp_cast_amp_cast_FullyConnected_Reshape_transpose__FusedOp_batch_dot__FusedOp_where_amp_cast_softmax__FusedOp_Dropout_amp_cast_amp_cast_FullyConnected_Reshape_transpose__FusedOp_batch_dot_Reshape_transpose__FusedOp_amp_cast_amp_cast_FullyConnected_Dropout__FusedOp_amp_cast_amp_cast_LayerNorm_amp_cast_amp_cast_amp_cast_FullyConnected__FusedOp_amp_cast_amp_cast_FullyConnected_Dropout__FusedOp_amp_cast_amp_cast_LayerNorm_amp_cast_amp_cast_amp_cast_FullyConnected_Reshape_transpose_Reshape__contrib_div_sqrt_dim_amp_cast_amp_cast_FullyConnected_Reshape_transpose__FusedOp_batch_dot__FusedOp_where_zeros_like__FusedOpHelper_amp_cast_softmax__FusedOp_Dropout_amp_cast_amp_cast_FullyConnected_Reshape_transpose__FusedOp_batch_dot_Reshape_transpose__FusedOp_amp_cast_amp_cast_FullyConnected_Dropout__FusedOp_amp_cast_amp_cast_LayerNorm_amp_cast_amp_cast_amp_cast_FullyConnected__FusedOp_amp_cast_amp_cast_FullyConnected_Dropout__FusedOp_amp_cast_amp_cast_LayerNorm_SequenceMask__FusedOp_amp_cast_amp_cast_FullyConnected_Activation_Dropout_amp_cast_amp_cast_amp_cast_FullyConnected__backward_FullyConnected__backward_amp_cast__backward_Dropout__backward_Activation__backward_FullyConnected__FusedOp__FusedOpHelper__backward_slice__backward_SequenceMask__backward_LayerNorm__FusedOp__backward_Dropout__backward_FullyConnected__FusedOp__backward_FullyConnected__FusedOp__backward_LayerNorm__FusedOp__backward_Dropout__backward_FullyConnected__FusedOpHelper__backward_amp_cast__FusedOpHelper__backward_reshape_transpose__backward_reshape_batch_dot_batch_dot__FusedOp__backward_Dropout__FusedOpHelper__backward_mul__FusedOpHelper__backward_amp_multicast__backward_amp_multicast: _Map_base::at
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

git clone https://github.com/dmlc/gluon-nlp; cd gluon-nlp; pytest --color=yes -s scripts -k 'test_finetune_train[float16-WNLI-bert_12_768_12-2]'
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;

&lt;denchmark-link:https://pypi.org/project/mxnet-cu100/1.6.0b20191102/&gt;https://pypi.org/project/mxnet-cu100/1.6.0b20191102/&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='leezu' date='2019-11-07T01:34:09Z'>
		&lt;denchmark-link:https://github.com/ptrendx&gt;@ptrendx&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='leezu' date='2019-11-07T01:53:14Z'>
		I suggest turn the fused_op off by default in the 1.6.0 release and announce it as experimental feature, or revert the PR. &lt;denchmark-link:https://github.com/szha&gt;@szha&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/eric-haibin-lin&gt;@eric-haibin-lin&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/junrushao1994&gt;@junrushao1994&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/DickJC123&gt;@DickJC123&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/wkcn&gt;@wkcn&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/reminisce&gt;@reminisce&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/haojin2&gt;@haojin2&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/TaoLv&gt;@TaoLv&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/marcoabreu&gt;@marcoabreu&lt;/denchmark-link&gt;
  What do you think?
		</comment>
		<comment id='3' author='leezu' date='2019-11-07T01:56:01Z'>
		&lt;denchmark-link:https://github.com/zhreshold&gt;@zhreshold&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='leezu' date='2019-11-07T08:13:52Z'>
		I agree to turn the fused_op off by default until fused_op is stable.
The reason is that users couldn't use the 1.6.0 release if it is not compatible with their code.
		</comment>
		<comment id='5' author='leezu' date='2019-11-07T08:36:48Z'>
		+1
		</comment>
		<comment id='6' author='leezu' date='2019-11-07T19:39:30Z'>
		Isn't right now the period of finding those integration bugs and fixing them for 1.6 release? I will definitely look into this issue and fix it, not sure why you propose to turn the feature off by default?
		</comment>
		<comment id='7' author='leezu' date='2019-11-07T19:49:39Z'>
		&lt;denchmark-link:https://github.com/ptrendx&gt;@ptrendx&lt;/denchmark-link&gt;
 I think we are already in a code-freeze status and the simplest fix is to turn it off by default. We could easily turn it on in 1.6.1 once we have confirmed that it has no impact in all the training scripts (there are plenty of them) and some may take time to run.
		</comment>
		<comment id='8' author='leezu' date='2019-11-07T21:57:16Z'>
		Ok, I sent a clarification email to dev@ as you are not actually the first person to reach out to me with this misunderstanding of code freeze. Code freeze is a period where bugs are found and fixed in order to polish the release and provide the best experience for the end users.
I treat the bugs about fusion with highest priority and will do my best to fix them. If I fail to address all issues before the time to make RC, then I agree it should be turned off by default and marked experimental.
		</comment>
		<comment id='9' author='leezu' date='2019-11-07T23:23:54Z'>
		I agree with &lt;denchmark-link:https://github.com/ptrendx&gt;@ptrendx&lt;/denchmark-link&gt;
, we should try to fix the bugs and ship the features if time allows.
		</comment>
		<comment id='10' author='leezu' date='2019-11-07T23:37:39Z'>
		I received the clarification email about the meaning of code freeze and I agree with &lt;denchmark-link:https://github.com/ptrendx&gt;@ptrendx&lt;/denchmark-link&gt;
 that we should try to fix it these days and consider to turn it off by default if we fail to do so. BTW, what's the expected date for 1.6 RC?
		</comment>
		<comment id='11' author='leezu' date='2019-11-12T00:54:49Z'>
		I created a PR with a fix. &lt;denchmark-link:https://github.com/leezu&gt;@leezu&lt;/denchmark-link&gt;
, could you validate it?
		</comment>
		<comment id='12' author='leezu' date='2019-11-13T03:01:07Z'>
		&lt;denchmark-link:https://github.com/ptrendx&gt;@ptrendx&lt;/denchmark-link&gt;
 thanks for the fix. Just confirmed it works.
		</comment>
	</comments>
</bug>