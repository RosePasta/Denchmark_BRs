<bug id='11849' author='wgchang' open_date='2018-07-21T16:50:16Z' closed_time='2018-09-18T22:47:15Z'>
	<summary>gluon.SymbolBlock cannot imports resnet trained with dtype="float16"</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

Cannot load fine-tuned resnet101 (incubator-mxnet/example/image-classification/symbols/resnet.py) with dtype="float16"  with "gluon.SymbolBlock.imports" method.
&lt;denchmark-h:h2&gt;Error Message:&lt;/denchmark-h&gt;

AssertionError: Failed loading Parameter 'stage3_unit2_conv2_weight' from saved params: dtype incompatible expected &lt;type 'numpy.float32'&gt; vs saved &lt;type 'numpy.float16'&gt;
&lt;denchmark-h:h2&gt;Minimum reproducible example&lt;/denchmark-h&gt;

net = gluon.SymbolBlock.imports('resnet-101-symbol.json',['data','softmax_label'],'resnet-101-0007.params')  # This line gives the error message.
net = gluon.SymbolBlock.imports('resnet-101-symbol.json',['data','softmax_label']) 
print(net.collect_params())
&lt;denchmark-h:h2&gt;My Questions&lt;/denchmark-h&gt;

In incubator-mxnet/example/image-classification/symbols/resnet.py,
there is mx.sym.Cast for type conversion.
I fine-tuned resnet101 with dtype="float16", and I need to load this model as HybridBlock, However, the method gluon.SymbolBlock.imports makes every params' type in the network as float32. Therefore, the trained model cannot be updated.
Here, resnet-101-0007.params are trained with argument dtype='float16'
In resnet-101-symbol.json file, there is the Cast op.
{
"op": "Cast",
"name": "cast0",
"attrs": {"dtype": "float16"},
"inputs": [[7, 0, 0]]
},
It seems that gluon.SymbolBlock.imports does not consider the type conversion operator.
For now, I think I need to load all parameter manualy, and change types then save.
Is there any other solution to solve this problem?
	</description>
	<comments>
		<comment id='1' author='wgchang' date='2018-07-23T22:24:28Z'>
		&lt;denchmark-link:https://github.com/sandeep-krishnamurthy&gt;@sandeep-krishnamurthy&lt;/denchmark-link&gt;
 Please help to label this issue 
		</comment>
		<comment id='2' author='wgchang' date='2018-08-14T08:36:57Z'>
		Got the same problem here. Any updates here?
		</comment>
		<comment id='3' author='wgchang' date='2018-08-22T16:59:52Z'>
		&lt;denchmark-link:https://github.com/rahul003&gt;@rahul003&lt;/denchmark-link&gt;
 same problem here, can't load back saved float16 models
		</comment>
		<comment id='4' author='wgchang' date='2018-08-29T22:16:44Z'>
		Using below snippet:
&lt;denchmark-code&gt;import mxnet as mx

ctx = mx.cpu(0)
data = mx.nd.zeros((1,3,224,224), ctx=ctx, dtype='float64')
net_fp32 = mx.gluon.model_zoo.vision.resnet34_v2(pretrained=True, ctx=ctx)
net_fp32.cast('float64')
net_fp32.hybridize()
pred = net_fp32.forward(data)
net_fp32.export('resnet34_fp16', 0)
print('export fp16 model')

sm = mx.sym.load('resnet34_fp16-symbol.json')
inputs = mx.sym.var('data', dtype='float64')
net_fp16 = mx.gluon.SymbolBlock(sm, inputs)
net_fp16.collect_params().load('resnet34_fp16-0000.params', ctx)
pred = net_fp16.forward(data)
&lt;/denchmark-code&gt;

Below are my findings:

Casting worked fine.
Saved parameters are in the correct format (fp64 in my sample code)
sym.load worked fine. If I infer symbol's type (sym.infer_type(data='float64') I get correct inferred type (float64) for all params.

Below is the issue:

When you create mx.gluon.SymbolBlock(sm, input). It creates the parameters in the Block. Type is not passed for creating the parameter.
See here - https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/gluon/block.py#L1058 and the behavior is "If there is not parameter to get, it creates one and uses default type (fp32) https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/gluon/parameter.py#L688

I am working on the fix.
&lt;denchmark-link:https://github.com/apeforest&gt;@apeforest&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ThomasDelteil&gt;@ThomasDelteil&lt;/denchmark-link&gt;
  - FYI
		</comment>
		<comment id='5' author='wgchang' date='2018-09-18T22:47:15Z'>
		Resolving as changes are merged.
		</comment>
	</comments>
</bug>