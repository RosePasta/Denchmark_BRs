<bug id='13825' author='thomelane' open_date='2019-01-10T01:22:48Z' closed_time='2019-05-02T22:04:53Z'>
	<summary>Dropout is Slow</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

Adding dropout to network seems to reduce speed of forward pass substantially.
I've seen this a few times now, and the latest was occasion was on the discussion forum:
&lt;denchmark-link:https://discuss.mxnet.io/t/training-speed-in-mxnet-is-nearly-2-5x-times-slower-than-pytorch&gt;https://discuss.mxnet.io/t/training-speed-in-mxnet-is-nearly-2-5x-times-slower-than-pytorch&lt;/denchmark-link&gt;
. User's training was x2.5 slower than PyTorch originally, but reported a , ultimately making training faster than PyTorch. User also reported a significant reduction in memory usage.
Could be related to &lt;denchmark-link:https://github.com/apache/incubator-mxnet/issues/12976&gt;#12976&lt;/denchmark-link&gt;
.
&lt;denchmark-h:h2&gt;Environment info (Required)&lt;/denchmark-h&gt;

User above is using MXNet version 1.3.1 on Ubuntu 16.04.5 with GTX1080.
And I have been able to replicate on AWS EC2 p3.2xlarge.
&lt;denchmark-code&gt;(mxnet_p36) ubuntu@ip-172-31-14-75:~$ python diagnose.py
----------Python Info----------
Version      : 3.6.5
Compiler     : GCC 7.2.0
Build        : ('default', 'Apr 29 2018 16:14:56')
Arch         : ('64bit', '')
------------Pip Info-----------
Version      : 10.0.1
Directory    : /home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/pip
----------MXNet Info-----------
Version      : 1.3.1
Directory    : /home/ubuntu/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet
Commit Hash   : 19c501680183237d52a862e6ae1dc4ddc296305b
----------System Info----------
Platform     : Linux-4.4.0-1074-aws-x86_64-with-debian-stretch-sid
system       : Linux
node         : ip-172-31-14-75
release      : 4.4.0-1074-aws
version      : #84-Ubuntu SMP Thu Dec 6 08:57:58 UTC 2018
----------Hardware Info----------
machine      : x86_64
processor    : x86_64
Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                8
On-line CPU(s) list:   0-7
Thread(s) per core:    2
Core(s) per socket:    4
Socket(s):             1
NUMA node(s):          1
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 79
Model name:            Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz
Stepping:              1
CPU MHz:               2699.984
CPU max MHz:           3000.0000
CPU min MHz:           1200.0000
BogoMIPS:              4600.16
Hypervisor vendor:     Xen
Virtualization type:   full
L1d cache:             32K
L1i cache:             32K
L2 cache:              256K
L3 cache:              46080K
NUMA node0 CPU(s):     0-7
Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single kaiser fsgsbase bmi1 hle avx2smep bmi2 erms invpcid rtm rdseed adx xsaveopt
----------Network Test----------
Setting timeout: 10
Timing for MXNet: https://github.com/apache/incubator-mxnet, DNS: 0.0019 sec, LOAD: 0.3825 sec.
Timing for Gluon Tutorial(en): http://gluon.mxnet.io, DNS: 0.0998 sec, LOAD: 0.0959 sec.
Timing for Gluon Tutorial(cn): https://zh.gluon.ai, DNS: 0.5308 sec, LOAD: 0.2854 sec.
Timing for FashionMNIST: https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/fashion-mnist/train-labels-idx1-ubyte.gz, DNS: 0.0082 sec, LOAD: 0.1051 sec.
Timing for PYPI: https://pypi.python.org/pypi/pip, DNS: 0.0099 sec, LOAD: 0.3274 sec.
Timing for Conda: https://repo.continuum.io/pkgs/free/, DNS: 0.0109 sec, LOAD: 0.0471 sec.
&lt;/denchmark-code&gt;

Package used (Python/R/Scala/Julia): Python
&lt;denchmark-h:h2&gt;Minimum reproducible example&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Using mx.nd.Dropout&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;%%timeit
data = mx.nd.random.uniform(shape=(100, 3, 224, 224), ctx=mx.gpu())
for i in range(100):
    # using mode='always' to force dropout behaviour
    data = mx.nd.Dropout(data, 0.5, mode='always')
mx.nd.waitall()
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;# 1.44 s ± 512 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
&lt;/denchmark-code&gt;

Approx ~4.5 times slower than custom dropout.
&lt;denchmark-h:h3&gt;Using Custom Dropout&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;%%timeit
data = mx.nd.random.uniform(shape=(100, 3, 224, 224), ctx=mx.gpu())
for i in range(100):
    dropout_mask = mx.nd.random.uniform(shape=(100, 3, 224, 224), ctx=mx.gpu()) &gt; 0.5
    data = data * dropout_mask
mx.nd.waitall()
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;# 325 ms ± 338 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
&lt;/denchmark-code&gt;

Approx ~4.5 times faster than mx.nd.Dropout.
&lt;denchmark-h:h2&gt;Other examples, including backward pass.&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Network using mx.gluon.nn.Dropout&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;net = mx.gluon.nn.HybridSequential()
net.add(mx.gluon.nn.Conv2D(channels=32, kernel_size=3, strides=2))
net.add(mx.gluon.nn.BatchNorm())
net.add(mx.gluon.nn.Activation('relu'))
net.add(mx.gluon.nn.Dropout(0.5))
net.add(mx.gluon.nn.GlobalMaxPool2D())
net.add(mx.gluon.nn.Dense(units=1000))
net.initialize(ctx=mx.gpu())
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;%%timeit
for i in range(10):
    data = mx.nd.random.uniform(shape=(100, 3, 224, 224), ctx=mx.gpu())
    with mx.autograd.record():
        output = net(data)
    output.backward()
    mx.nd.waitall()
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;# 524 ms ± 882 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
&lt;/denchmark-code&gt;

Approx 4 times slower than without.
&lt;denchmark-h:h3&gt;Network without mx.gluon.nn.Dropout&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;net = mx.gluon.nn.HybridSequential()
net.add(mx.gluon.nn.Conv2D(channels=32, kernel_size=3, strides=2))
net.add(mx.gluon.nn.BatchNorm())
net.add(mx.gluon.nn.Activation('relu'))
net.add(mx.gluon.nn.GlobalMaxPool2D())
net.add(mx.gluon.nn.Dense(units=1000))
net.initialize(ctx=mx.gpu())
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;%%timeit
for i in range(10):
    data = mx.nd.random.uniform(shape=(100, 3, 224, 224), ctx=mx.gpu())
    with mx.autograd.record():
        output = net(data)
    output.backward()
    mx.nd.waitall()
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;# 123 ms ± 187 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
&lt;/denchmark-code&gt;

Approx 4 times faster than with.
	</description>
	<comments>
		<comment id='1' author='thomelane' date='2019-01-10T01:24:20Z'>
		&lt;denchmark-link:https://github.com/mxnet-label-bot&gt;@mxnet-label-bot&lt;/denchmark-link&gt;
 add [Bug, Operator, Python, Performance, CUDA]
		</comment>
		<comment id='2' author='thomelane' date='2019-02-26T21:02:30Z'>
		Probably resolved by &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/13896&gt;#13896&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='thomelane' date='2019-02-28T05:10:23Z'>
		after &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/13896&gt;#13896&lt;/denchmark-link&gt;
, seems only  has performance improvements, but not  mx.nd.Dropout(data, 0.5, mode='always')
following the above code sample:
&lt;denchmark-h:h3&gt;Using mx.nd.Dropout&lt;/denchmark-h&gt;

1.43 s ± 293 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
&lt;denchmark-h:h3&gt;Using Custom Dropout&lt;/denchmark-h&gt;

331 ms ± 411 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
&lt;denchmark-h:h3&gt;Network using mx.gluon.nn.Dropout&lt;/denchmark-h&gt;

128 ms ± 39.4 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
&lt;denchmark-h:h3&gt;Network without mx.gluon.nn.Dropout&lt;/denchmark-h&gt;

128 ms ± 30.2 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
tested with both pip install mxnet-cu92 --pre and build from source on p3.2x
		</comment>
		<comment id='4' author='thomelane' date='2019-02-28T05:36:52Z'>
		&lt;denchmark-link:https://github.com/roywei&gt;@roywei&lt;/denchmark-link&gt;
 by default cudnn_off is turned on. You need to turn it off to benefit from cudnn dropout.
		</comment>
		<comment id='5' author='thomelane' date='2019-04-01T21:51:59Z'>
		&lt;denchmark-link:https://github.com/thomelane&gt;@thomelane&lt;/denchmark-link&gt;
 - Can you please confirm if the issue is resolved with CUDNN support?
Or, this may be related to this - &lt;denchmark-link:https://github.com/apache/incubator-mxnet/issues/14198&gt;#14198&lt;/denchmark-link&gt;
 ?
		</comment>
		<comment id='6' author='thomelane' date='2019-05-02T19:14:04Z'>
		Tested with mxnet-cu90mkl==1.5.0b20190501 on g3.8xlarge instance.
Code example (same as above):
&lt;denchmark-code&gt;import mxnet as mx
import time

tic = time.time()

data = mx.nd.random.uniform(shape=(100, 3, 224, 224), ctx=mx.gpu())
for i in range(100):
    # using mode='always' to force dropout behaviour
    data = mx.nd.Dropout(data, 0.5, mode='always')
mx.nd.waitall()

print("mx.nd.Dropout: ", time.time() - tic)

tic = time.time()
data = mx.nd.random.uniform(shape=(100, 3, 224, 224), ctx=mx.gpu())
for i in range(100):
    dropout_mask = mx.nd.random.uniform(shape=(100, 3, 224, 224), ctx=mx.gpu()) &gt; 0.5
    data = data * dropout_mask
mx.nd.waitall()

print("Custom Dropout: ", time.time() - tic)

net = mx.gluon.nn.HybridSequential()
net.add(mx.gluon.nn.Conv2D(channels=32, kernel_size=3, strides=2))
net.add(mx.gluon.nn.BatchNorm())
net.add(mx.gluon.nn.Activation('relu'))
net.add(mx.gluon.nn.Dropout(0.5))
net.add(mx.gluon.nn.GlobalMaxPool2D())
net.add(mx.gluon.nn.Dense(units=1000))
net.initialize(ctx=mx.gpu())

tic = time.time()
for i in range(10):
    data = mx.nd.random.uniform(shape=(100, 3, 224, 224), ctx=mx.gpu())
    with mx.autograd.record():
        output = net(data)
    output.backward()
    mx.nd.waitall()

print("mx.gluon.nn.Dropout: ", time.time() - tic)

net = mx.gluon.nn.HybridSequential()
net.add(mx.gluon.nn.Conv2D(channels=32, kernel_size=3, strides=2))
net.add(mx.gluon.nn.BatchNorm())
net.add(mx.gluon.nn.Activation('relu'))
net.add(mx.gluon.nn.GlobalMaxPool2D())
net.add(mx.gluon.nn.Dense(units=1000))
net.initialize(ctx=mx.gpu())

tic = time.time()
for i in range(10):
    data = mx.nd.random.uniform(shape=(100, 3, 224, 224), ctx=mx.gpu())
    with mx.autograd.record():
        output = net(data)
    output.backward()
    mx.nd.waitall()

print("Without mx.gluon.nn.Dropout: ", time.time() - tic)
&lt;/denchmark-code&gt;

Performance results (in seconds):
&lt;denchmark-code&gt;mx.nd.Dropout:  2.713014841079712
Custom Dropout:  3.227015733718872
mx.gluon.nn.Dropout:  0.9665834903717041
Without mx.gluon.nn.Dropout:  0.7113254070281982
&lt;/denchmark-code&gt;

Looks like the dropout performance has been significantly improved with CUDNN support.
&lt;denchmark-link:https://github.com/sandeep-krishnamurthy&gt;@sandeep-krishnamurthy&lt;/denchmark-link&gt;
 Please help close this issue.
&lt;denchmark-link:https://github.com/thomelane&gt;@thomelane&lt;/denchmark-link&gt;
 Please reopen the issue if you have any further questions. Thanks.
		</comment>
	</comments>
</bug>