<bug id='17020' author='perdasilva' open_date='2019-12-09T07:42:29Z' closed_time='2019-12-10T04:21:10Z'>
	<summary>[CUDA 9.0] NVRTC Compilation failed</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

Since &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/15167&gt;#15167&lt;/denchmark-link&gt;
, the CD pipeline for CUDA 9.0 have been failing.
&lt;denchmark-h:h3&gt;Error Message&lt;/denchmark-h&gt;

Many examples can be taken from the CD pipeline, &lt;denchmark-link:http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/restricted-mxnet-cd%2Fmxnet-cd-release-job/detail/mxnet-cd-release-job/276/pipeline&gt;e.g.&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;======================================================================
ERROR: test_operator_gpu.test_batchnorm_training
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/nose/case.py", line 197, in runTest
    self.test(*self.arg)
  File "/usr/local/lib/python2.7/dist-packages/nose/util.py", line 620, in newfunc
    return func(*arg, **kw)
  File "/work/mxnet/tests/python/gpu/../unittest/common.py", line 177, in test_new
    orig_test(*args, **kwargs)
  File "/work/mxnet/tests/python/gpu/../unittest/test_operator.py", line 1830, in test_batchnorm_training
    check_batchnorm_training('default')
  File "/work/mxnet/tests/python/gpu/../unittest/test_operator.py", line 1769, in check_batchnorm_training
    check_numeric_gradient(test, in_location, mean_std, numeric_eps=1e-2, rtol=0.16, atol=1e-2)
  File "/work/mxnet/python/mxnet/test_utils.py", line 1101, in check_numeric_gradient
    symbolic_grads = {k:executor.grad_dict[k].asnumpy() for k in grad_nodes}
  File "/work/mxnet/python/mxnet/test_utils.py", line 1101, in &lt;dictcomp&gt;
    symbolic_grads = {k:executor.grad_dict[k].asnumpy() for k in grad_nodes}
  File "/work/mxnet/python/mxnet/ndarray/ndarray.py", line 2532, in asnumpy
    ctypes.c_size_t(data.size)))
  File "/work/mxnet/python/mxnet/base.py", line 255, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
MXNetError: [21:10:06] src/operator/fusion/fused_op.cu:558: Check failed: compileResult == NVRTC_SUCCESS (6 vs. 0) : NVRTC Compilation failed. Please set environment variable MXNET_USE_FUSION to 0.
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

Run the CD pipeline for cu90 and/or cu90mkl
&lt;denchmark-h:h2&gt;What have you tried to solve it?&lt;/denchmark-h&gt;

I noticed that USE_NVTX=1 wasn't set in the &lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/make/pip/pip_linux_cu90.mk&gt;make configuration&lt;/denchmark-link&gt;
 for CUDA 9.0 - but this had no effect.
	</description>
	<comments>
		<comment id='1' author='perdasilva' date='2019-12-09T07:42:55Z'>
		&lt;denchmark-link:https://github.com/DickJC123&gt;@DickJC123&lt;/denchmark-link&gt;
 I've created a proper issue so we can track this problem - thanks for looking into it!
		</comment>
		<comment id='2' author='perdasilva' date='2019-12-09T08:03:10Z'>
		As  &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/15167&gt;#15167&lt;/denchmark-link&gt;
 is part of 1.6 release, is this a release blocker?
		</comment>
		<comment id='3' author='perdasilva' date='2019-12-09T19:39:39Z'>
		&lt;denchmark-link:https://github.com/apeforest&gt;@apeforest&lt;/denchmark-link&gt;
 assign &lt;denchmark-link:https://github.com/ptrendx&gt;@ptrendx&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='perdasilva' date='2019-12-09T21:31:42Z'>
		I was looking into it and it seems like a bug in nvrtc in cuda 9 toolkit. I believe the workaround should be easy - I will have the PR ready today.
		</comment>
	</comments>
</bug>