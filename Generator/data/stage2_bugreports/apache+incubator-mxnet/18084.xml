<bug id='18084' author='sxjscience' open_date='2020-04-16T16:50:05Z' closed_time='2020-05-09T19:21:38Z'>
	<summary>[Numpy] Backward error in mixed int64 + float32</summary>
	<description>
This is related to &lt;denchmark-link:https://github.com/apache/incubator-mxnet/issues/18022&gt;#18022&lt;/denchmark-link&gt;
.
Reproducible example:
import mxnet as mx
from mxnet.gluon import HybridBlock
mx.npx.set_np()

class Foo(HybridBlock):
    def hybrid_forward(self, F, query):
        query_shape = F.npx.shape_array(query)
        return query / F.np.sqrt(query_shape[-1])

foo = Foo()
foo.hybridize()
a = mx.np.ones((5, 5, 5))
out = foo(a)
print(out)

a.attach_grad()
with mx.autograd.record():
    out = foo(a)
    out.backward()
print(a.grad)
Error message:
&lt;denchmark-code&gt;MXNetError: Traceback (most recent call last):
  File "include/mxnet/./tensor_blob.h", line 256
MXNetError: Check failed: mshadow: :DataType&lt;DType&gt;::kFlag == type_flag_: TBlob.get_with_shape: data type do not match specified type.Expected: long long v.s. given float
&lt;/denchmark-code&gt;

Currently, I have to use query / F.np.sqrt(query_shape[-1].astype(np.float32)).
	</description>
	<comments>
		<comment id='1' author='sxjscience' date='2020-04-16T17:11:35Z'>
		Another failure case:
import mxnet as mx
from mxnet.gluon import HybridBlock
mx.npx.set_np()

class Foo(HybridBlock):
    def hybrid_forward(self, F, query):
        query_shape = F.npx.shape_array(query)
        return query / F.np.sqrt(query_shape[-1].astype(mx.np.float32))

foo = Foo()
foo.hybridize()
a = mx.np.ones((5, 5, 5), dtype=mx.np.float16)
out = foo(a)
print(out)

a.attach_grad()
with mx.autograd.record():
    out = foo(a)
    out.backward()
print(a.grad)
Error:
&lt;denchmark-code&gt;[10:10:29] src/operator/numpy/./np_true_divide-inl.h:244: not implemented yet...
[[[ 0.0000000e+00 -0.0000000e+00 -2.0039270e-21 -3.6902478e+19
    1.0000038e+00]
  [ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
    1.0000000e+00]
  [ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
    1.0000000e+00]
  [ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
    1.0000000e+00]
  [ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
    1.0000000e+00]]

 [[ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
    1.0000000e+00]
  [ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
    1.0000000e+00]
  [ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
    1.0000000e+00]
  [ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
    1.0000000e+00]
  [ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
    1.0000000e+00]]

 [[ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
    1.0000000e+00]
  [ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
    1.0000000e+00]
  [ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
    1.0000000e+00]
  [ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
    1.0000000e+00]
  [ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
    1.0000000e+00]]

 [[ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
    1.0000000e+00]
  [ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
    1.0000000e+00]
  [ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
    1.0000000e+00]
  [ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
    1.0000000e+00]
  [ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
    1.0000000e+00]]

 [[ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
    1.0000000e+00]
  [ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
    1.0000000e+00]
  [ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
    1.0000000e+00]
  [ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
    1.0000000e+00]
  [ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
    1.0000000e+00]]]
[10:10:29] src/operator/numpy/./np_true_divide-inl.h:244: not implemented yet...
---------------------------------------------------------------------------
MXNetError                                Traceback (most recent call last)
&lt;ipython-input-5-cc974fcd01c7&gt; in &lt;module&gt;
     18     out = foo(a)
     19     out.backward()
---&gt; 20 print(a.grad)

~/miniconda3/lib/python3.7/site-packages/mxnet/numpy/multiarray.py in __str__(self)
   1178     def __str__(self):
   1179         """Returns a string representation of the array."""
-&gt; 1180         array_str = self.asnumpy().__str__()
   1181         context = self.ctx
   1182         if context.device_type == 'cpu' or self.ndim == 0:

~/miniconda3/lib/python3.7/site-packages/mxnet/ndarray/ndarray.py in asnumpy(self)
   2564             self.handle,
   2565             data.ctypes.data_as(ctypes.c_void_p),
-&gt; 2566             ctypes.c_size_t(data.size)))
   2567         return data
   2568 

~/miniconda3/lib/python3.7/site-packages/mxnet/base.py in check_call(ret)
    244     """
    245     if ret != 0:
--&gt; 246         raise get_last_ffi_error()
    247 
    248 

MXNetError: Traceback (most recent call last):
  File "include/mxnet/./tensor_blob.h", line 256
MXNetError: Check failed: mshadow: :DataType&lt;DType&gt;::kFlag == type_flag_: TBlob.get_with_shape: data type do not match specified type.Expected: float v.s. given half

&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='sxjscience' date='2020-04-29T23:01:38Z'>
		Assignee: &lt;denchmark-link:https://github.com/BenjaminCHEN2016&gt;@BenjaminCHEN2016&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='sxjscience' date='2020-05-09T19:21:38Z'>
		fixed by &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/18250&gt;#18250&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>