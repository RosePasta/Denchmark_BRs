<bug id='17705' author='eric-haibin-lin' open_date='2020-02-27T05:30:21Z' closed_time='2020-03-10T11:54:18Z'>
	<summary>mkldnn quantized FC is slow</summary>
	<description>
quantized BERT model with int8 is 2x slower than float32.
Download trained SST params from: &lt;denchmark-link:https://dist-bert.s3.amazonaws.com/demo/finetune/sst.params&gt;https://dist-bert.s3.amazonaws.com/demo/finetune/sst.params&lt;/denchmark-link&gt;

Clone is install gluon-nlp v0.9
&lt;denchmark-code&gt;# calibration
KMP_AFFINITY=granularity=fine,noduplicates,compact,1,0 OMP_NUM_THREADS=1 numactl --physcpubind=0 --membind=0 python3 finetune_classifier.py --task_name SST --only_calibration --model_parameters  sst.params
# fp32 inference
python3 finetune_classifier.py --task_name SST --epoch 1 --only_inference --model_parameters sst.params --round_to 128 --dev_batch_size 1
# int8 inference
python3 finetune_classifier.py --task_name SST --epoch 1 --only_inference --model_prefix ./output_dir/model_bert_SST_quantized_customize --deploy --round_to 128 --dev_batch_size 1
&lt;/denchmark-code&gt;

I'm using c5.12xlarge. I tried to set OMP_NUM_THREAD=8, but int8 is still slower than float32.
	</description>
	<comments>
		<comment id='1' author='eric-haibin-lin' date='2020-02-27T05:31:37Z'>
		&lt;denchmark-link:https://github.com/wuxun-zhang&gt;@wuxun-zhang&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ciyongch&gt;@ciyongch&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='eric-haibin-lin' date='2020-02-27T05:36:56Z'>
		Thanks for reporting this, I will take a look.
		</comment>
		<comment id='3' author='eric-haibin-lin' date='2020-02-27T09:24:28Z'>
		&lt;denchmark-link:https://github.com/eric-haibin-lin&gt;@eric-haibin-lin&lt;/denchmark-link&gt;
 I just created a PR &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/17707&gt;#17707&lt;/denchmark-link&gt;
 to address this issue, please take a review.
		</comment>
		<comment id='4' author='eric-haibin-lin' date='2020-03-10T11:54:43Z'>
		Feel free to reopen if the issue is not resolved.
		</comment>
	</comments>
</bug>