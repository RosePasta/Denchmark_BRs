<bug id='12713' author='eric-haibin-lin' open_date='2018-10-01T23:16:49Z' closed_time='2019-03-17T06:48:31Z'>
	<summary>distributed kvstore bug in MXNet</summary>
	<description>
I'm using distributed kvstore with Gluon trainer. I found the two following bugs:


Initializing trainer = gluon.Trainer(update_on_kvstore=True) doesn't work. Inspecting trainer._update_on_kvstore shows that the value is still set to False.


When distributed kvstore is used, by default gluon.Trainer doesn't work with mx.optimizer.LRScheduler if a worker has more than 1 GPU. To be more specific, the trainer updates once per GPU, the LRScheduler object is shared across GPUs and get a wrong update count.


This means one cannot train imagenet classification using resnet with gluon trainer.
	</description>
	<comments>
		<comment id='1' author='eric-haibin-lin' date='2018-10-01T23:23:20Z'>
		&lt;denchmark-link:https://github.com/eric-haibin-lin&gt;@eric-haibin-lin&lt;/denchmark-link&gt;
 Thank you for reporting the issue,
&lt;denchmark-link:https://github.com/mxnet-label-bot&gt;@mxnet-label-bot&lt;/denchmark-link&gt;
 [Bug, Gluon]
		</comment>
		<comment id='2' author='eric-haibin-lin' date='2018-10-02T16:16:12Z'>
		+1
		</comment>
		<comment id='3' author='eric-haibin-lin' date='2018-10-08T22:48:45Z'>
		Working on this. Will update my findings.
		</comment>
		<comment id='4' author='eric-haibin-lin' date='2018-11-05T07:02:34Z'>
		&lt;denchmark-link:https://github.com/sandeep-krishnamurthy&gt;@sandeep-krishnamurthy&lt;/denchmark-link&gt;
 is this issue fixed with &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/12786&gt;#12786&lt;/denchmark-link&gt;
 ?
If so - please close the issue or comment.
		</comment>
		<comment id='5' author='eric-haibin-lin' date='2018-11-05T07:04:03Z'>
		

Initializing trainer = gluon.Trainer(update_on_kvstore=True) doesn't work. Inspecting trainer._update_on_kvstore shows that the value is still set to False.


This is fixed.


When distributed kvstore is used, by default gluon.Trainer doesn't work with mx.optimizer.LRScheduler if a worker has more than 1 GPU. To be more specific, the trainer updates once per GPU, the LRScheduler object is shared across GPUs and get a wrong update count.


This needs to be fixed.
		</comment>
		<comment id='6' author='eric-haibin-lin' date='2018-11-12T21:29:12Z'>
		As mentioned in &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/12786&gt;#12786&lt;/denchmark-link&gt;
 the fix for the 1st problem has issues on the v1.3.x release branch.
		</comment>
	</comments>
</bug>