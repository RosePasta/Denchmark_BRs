<bug id='10453' author='sxjscience' open_date='2018-04-07T02:33:09Z' closed_time='2018-06-14T01:08:17Z'>
	<summary>Bug of CuDNN RNN with variable sequence length</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

Segfault will be triggered by the following code:
from mxnet.gluon.rnn import LSTM
import mxnet as mx
import numpy as np

ctx = mx.gpu()
lstm = LSTM(num_layers=1, hidden_size=200, dropout=0.5)
lstm.initialize(ctx=ctx)
batch_size = 32
for seq_len in range(500, 10, -1):
    for repeat in range(10):
        print(seq_len, repeat)
        inputs_nd = mx.nd.random.normal(0, 1, shape=(seq_len, batch_size, 200), ctx=ctx)
        out = lstm(inputs_nd)
        print(out[0].sum().asscalar())
        mx.nd.waitall()
I'm using V100 + cuda 9.0 + cudnn 7.0.4 (P3 instance).  The GPU memory keeps increasing and finally raises seg fault.
Also, the same script + configuration has not triggered an error in M60 (g3 instance).
&lt;denchmark-link:https://github.com/eric-haibin-lin&gt;@eric-haibin-lin&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/DickJC123&gt;@DickJC123&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/szha&gt;@szha&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/szhengac&gt;@szhengac&lt;/denchmark-link&gt;

backtrace:
&lt;denchmark-code&gt;#0  __GI___libc_free (mem=0x7f0000000000) at malloc.c:2951
#1  0x00007fff7252e6f9 in cudnnDestroyFilterDescriptor () from /usr/local/cuda/lib64/libcudnn.so.7
#2  0x00007fff99c3683c in mxnet::op::CuDNNRNNOp&lt;float&gt;::~CuDNNRNNOp() () from /home/ubuntu/mxnet/python/mxnet/../../lib/libmxnet.so
#3  0x00007fff99c36bd9 in mxnet::op::CuDNNRNNOp&lt;float&gt;::~CuDNNRNNOp() () from /home/ubuntu/mxnet/python/mxnet/../../lib/libmxnet.so
#4  0x00007fff97d9bde5 in dmlc::any::TypeOnHeap&lt;mxnet::op::OperatorState&gt;::destroy(dmlc::any::Data*) () from /home/ubuntu/mxnet/python/mxnet/../../lib/libmxnet.so
#5  0x00007fff9579d8e1 in std::_Sp_counted_ptr_inplace&lt;mxnet::OpStatePtr::OpState, std::allocator&lt;mxnet::OpStatePtr::OpState&gt;, (__gnu_cxx::_Lock_policy)2&gt;::_M_dispose() () from /home/ubuntu/mxnet/python/mxnet/../../lib/libmxnet.so
#6  0x00007fff9551bed7 in std::_Sp_counted_base&lt;(__gnu_cxx::_Lock_policy)2&gt;::_M_release() () from /home/ubuntu/mxnet/python/mxnet/../../lib/libmxnet.so
#7  0x00007fff97def885 in mxnet::imperative::PushOperator(mxnet::OpStatePtr const&amp;, nnvm::Op const*, nnvm::NodeAttrs const&amp;, mxnet::Context const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::Resource, std::allocator&lt;mxnet::Resource&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, std::vector&lt;unsigned int, std::allocator&lt;unsigned int&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;, mxnet::DispatchMode)::{lambda(mxnet::RunContext, mxnet::engine::CallbackOnComplete)#2}::~CallbackOnComplete() () from /home/ubuntu/mxnet/python/mxnet/../../lib/libmxnet.so
#8  0x00007fff97deff08 in std::_Function_base::_Base_manager&lt;mxnet::imperative::PushOperator(mxnet::OpStatePtr const&amp;, nnvm::Op const*, nnvm::NodeAttrs const&amp;, mxnet::Context const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::Resource, std::allocator&lt;mxnet::Resource&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, std::vector&lt;unsigned int, std::allocator&lt;unsigned int&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;, mxnet::DispatchMode)::{lambda(mxnet::RunContext)#3}&gt;::_M_manager(std::_Any_data&amp;, std::_Function_base::_Base_manager&lt;mxnet::imperative::PushOperator(mxnet::OpStatePtr const&amp;, nnvm::Op const*, nnvm::NodeAttrs const&amp;, mxnet::Context const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::Resource, std::allocator&lt;mxnet::Resource&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, std::vector&lt;unsigned int, std::allocator&lt;unsigned int&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;, mxnet::DispatchMode)::{lambda(mxnet::RunContext)#3}&gt; const&amp;, std::_Manager_operation) () from /home/ubuntu/mxnet/python/mxnet/../../lib/libmxnet.so
#9  0x00007fff982233a6 in std::_Function_base::_Base_manager&lt;mxnet::engine::ThreadedEngine::PushSync(std::function&lt;void (mxnet::RunContext)&gt;, mxnet::Context, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, mxnet::FnProperty, int, char const*)::{lambda(mxnet::RunContext, mxnet::engine::CallbackOnComplete)#1}&gt;::_M_manager(std::_Any_data&amp;, std::_Function_base::_Base_manager&lt;mxnet::engine::ThreadedEngine::PushSync(std::function&lt;void (mxnet::RunContext)&gt;, mxnet::Context, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, mxnet::FnProperty, int, char const*)::{lambda(mxnet::RunContext, mxnet::engine::CallbackOnComplete)#1}&gt; const&amp;, std::_Manager_operation) () from /home/ubuntu/mxnet/python/mxnet/../../lib/libmxnet.so
#10 0x00007fff98229cc4 in mxnet::engine::ThreadedEngine::OnComplete(mxnet::engine::ThreadedOpr*) () from /home/ubuntu/mxnet/python/mxnet/../../lib/libmxnet.so
#11 0x00007fff982245c6 in mxnet::engine::ThreadedEngine::OnCompleteStatic(mxnet::Engine*, void*) () from /home/ubuntu/mxnet/python/mxnet/../../lib/libmxnet.so
#12 0x00007fff9821e868 in mxnet::engine::ThreadedEngine::ExecuteOprBlock(mxnet::RunContext, mxnet::engine::OprBlock*) ()
   from /home/ubuntu/mxnet/python/mxnet/../../lib/libmxnet.so
#13 0x00007fff9823419b in void mxnet::engine::ThreadedEnginePerDevice::GPUWorker&lt;(dmlc::ConcurrentQueueType)0&gt;(mxnet::Context, bool, mxnet::engine::ThreadedEnginePerDevice::ThreadWorkerBlock&lt;(dmlc::ConcurrentQueueType)0&gt;*, std::shared_ptr&lt;dmlc::ManualEvent&gt; const&amp;) () from /home/ubuntu/mxnet/python/mxnet/../../lib/libmxnet.so
#14 0x00007fff982343fe in std::_Function_handler&lt;void (std::shared_ptr&lt;dmlc::ManualEvent&gt;), mxnet::engine::ThreadedEnginePerDevice::PushToExecute(mxnet::engine::OprBlock*, bool)::{lambda()#3}::operator()() const::{lambda(std::shared_ptr&lt;dmlc::ManualEvent&gt;)#1}&gt;::_M_invoke(std::_Any_data const&amp;, std::shared_ptr&lt;dmlc::ManualEvent&gt;&amp;&amp;) ()
   from /home/ubuntu/mxnet/python/mxnet/../../lib/libmxnet.so
#15 0x00007fff9822e2aa in std::thread::_Impl&lt;std::_Bind_simple&lt;std::function&lt;void (std::shared_ptr&lt;dmlc::ManualEvent&gt;)&gt; (std::shared_ptr&lt;dmlc::ManualEvent&gt;)&gt; &gt;::_M_run() () from /home/ubuntu/mxnet/python/mxnet/../../lib/libmxnet.so
#16 0x00007fffee4a2c80 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#17 0x00007ffff7bc16ba in start_thread (arg=0x7fff3ed07700) at pthread_create.c:333
#18 0x00007ffff78f741d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='sxjscience' date='2018-04-07T02:46:35Z'>
		The following code, which always use seq_len=500 will not trigger the seg fault. This is a very critical bug.
from mxnet.gluon.rnn import LSTM
import mxnet as mx
import numpy as np

ctx = mx.gpu()
lstm = LSTM(num_layers=1, hidden_size=200, dropout=0.0)
lstm.initialize(ctx=ctx)
batch_size = 32
for seq_len in range(500, 10, -1):
    for repeat in range(10):
        real_seq_len = 500
        print(real_seq_len, repeat)
        inputs_nd = mx.nd.random.normal(0, 1, shape=(real_seq_len, batch_size, 200), ctx=ctx)
        out = lstm(inputs_nd)
        print(out[0].sum().asscalar())
        mx.nd.waitall()
		</comment>
		<comment id='2' author='sxjscience' date='2018-04-07T03:38:12Z'>
		The bug occurs when we have variable sequence length. I think it may be related to how the mxnet reuses the memory.
		</comment>
		<comment id='3' author='sxjscience' date='2018-04-07T06:11:43Z'>
		I was able to finish running the script by setting export MXNET_GPU_MEM_POOL_RESERVE=7
		</comment>
		<comment id='4' author='sxjscience' date='2018-04-07T06:15:20Z'>
		&lt;denchmark-link:https://github.com/szha&gt;@szha&lt;/denchmark-link&gt;
 how much memory consumption did you observe?
One thing to try is to can run the code step by step and figure out which batch causes the err
		</comment>
		<comment id='5' author='sxjscience' date='2018-04-09T16:21:20Z'>
		What I observed is that it doesn't fail consistently on certain specific batch. Another team observed the same issue before, and it is likely caused by our backend memory pool holding too much memory, in which case the curand doesn't have enough memory to keep the random number generator states for each stream multiprocessor.
		</comment>
		<comment id='6' author='sxjscience' date='2018-04-09T22:15:19Z'>
		I have similar issue when training speech model. even after
export MXNET_GPU_MEM_POOL_RESERVE=7
will try larger RESERVE
		</comment>
		<comment id='7' author='sxjscience' date='2018-04-16T23:17:43Z'>
		I find that the  is created but never destroyed. See &lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/src/operator/cudnn_rnn-inl.h#L516&gt;https://github.com/apache/incubator-mxnet/blob/master/src/operator/cudnn_rnn-inl.h#L516&lt;/denchmark-link&gt;
 and  &lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/src/operator/cudnn_rnn-inl.h#L98&gt;https://github.com/apache/incubator-mxnet/blob/master/src/operator/cudnn_rnn-inl.h#L98&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='sxjscience' date='2018-04-17T03:04:56Z'>
		It's related to &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/953&gt;pytorch/pytorch#953&lt;/denchmark-link&gt;
.  uses a large amount of GPU memory. One choice to solve this problem is to create a DropoutDescriptor when we create a stream and always use cudnnGetDropoutDescriptor. This will also accelerate the speed of RNN layer in Gluon because we can avoid calling Alloc and Free.
		</comment>
		<comment id='9' author='sxjscience' date='2018-05-20T05:24:30Z'>
		&lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/11004&gt;#11004&lt;/denchmark-link&gt;
 "fixes" this issue. The filter descriptors that are freed in the destructor were not created if cudaMalloc would fail during  or .
Now the following error will be returned in an OOM situation:
&lt;denchmark-code&gt;
mxnet.base.MXNetError: [05:13:15] src/storage/./pooled_storage_manager.h:108: cudaMalloc failed: out of memory

Stack trace returned 10 entries:
[bt] (0) /home/leonard/software/mxnet-master/python/mxnet/../../lib/libmxnet.so(dmlc::StackTrace[abi:cxx11]()+0x5b) [0x7f358103783b]
[bt] (1) /home/leonard/software/mxnet-master/python/mxnet/../../lib/libmxnet.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x28) [0x7f35810383a8]
[bt] (2) /home/leonard/software/mxnet-master/python/mxnet/../../lib/libmxnet.so(mxnet::storage::GPUPooledStorageManager::Alloc(mxnet::Storage::Handle*)+0x154) [0x7f358398b384]
[bt] (3) /home/leonard/software/mxnet-master/python/mxnet/../../lib/libmxnet.so(mxnet::StorageImpl::Alloc(mxnet::Storage::Handle*)+0x5d) [0x7f358398d80d]
[bt] (4) /home/leonard/software/mxnet-master/python/mxnet/../../lib/libmxnet.so(mxnet::op::CuDNNRNNOp&lt;float&gt;::Init(mshadow::Stream&lt;mshadow::gpu&gt;*, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;)+0x1de5) [0x7f3585606a55]
[bt] (5) /home/leonard/software/mxnet-master/python/mxnet/../../lib/libmxnet.so(mxnet::op::CuDNNRNNOp&lt;float&gt;::Forward(mxnet::OpContext const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;, std::vector&lt;mxnet::TBlob, std::allocator&lt;mxnet::TBlob&gt; &gt; const&amp;)+0xa5d) [0x7f358560e07d]
...
&lt;/denchmark-code&gt;

In particular, &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/11004&gt;#11004&lt;/denchmark-link&gt;
 makes sure that the descriptors are always created during class initialization and not just somewhere down the line in / .
		</comment>
	</comments>
</bug>