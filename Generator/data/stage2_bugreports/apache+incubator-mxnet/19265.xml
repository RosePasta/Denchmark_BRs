<bug id='19265' author='Zha0q1' open_date='2020-10-01T18:35:35Z' closed_time='2020-11-02T19:32:13Z'>
	<summary>MKLDNN RNN seg fault</summary>
	<description>
A customer is experiencing seg fault when feeding in a large input to MKL LSTM. I have reduced the code to this:
&lt;denchmark-code&gt;import mxnet as mx
from mxnet import gluon, nd, autograd
from mxnet.gluon import nn, rnn, Trainer

hidden_size = 30
num_embed = 100
vocab_size = 13028#len(vocab.token_to_idx.keys())

inp = nd.random.uniform(0, vocab_size, (16758,500))
print(inp)

context = mx.cpu()

model = nn.Sequential()
model.add(nn.Embedding(vocab_size, num_embed), # Embedding layer
          rnn.LSTM(hidden_size, num_layers=1,bidirectional=True),  # Recurrent layer ,bidirectional=True
          nn.Dense(3))  # Output layer

model.collect_params().initialize(mx.init.Xavier(), ctx=context)

val_predictions = model(inp)
nd.waitall()
print(val_predictions)
&lt;/denchmark-code&gt;

I think this is some sort of out of memory issue because if we shrink the input (first dim of inp) then there will not be a seg fault, but still, shall we add some error message here so that users will be notified to reduce the input size?
I also noticed the same input will run fine with export MXNET_USE_MKLDNN_RNN=0 but that is 3x slower than the mkldnn implementation. Another suggestion I made to the customer was to try out a magic number for the seg fault threshold and do multiple batches that are smaller than that (customer was trying to forward pass the entire validation set), but this is also a pretty hacky solution. So maybe better yet, we can optimize the mkldnn implementation to process data that's currently too large?
&lt;denchmark-link:https://github.com/PatricZhao&gt;@PatricZhao&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='Zha0q1' date='2020-10-01T18:40:10Z'>
		seg fault:
&lt;denchmark-code&gt;Segmentation fault: 11

terminate called without an active exception
Aborted (core dumped)
&lt;/denchmark-code&gt;

GDB:
&lt;denchmark-code&gt;
Thread 9 "python" received signal SIGSEGV, Segmentation fault.
[Switching to Thread 0x7fffbac26700 (LWP 18164)]
bt
0x00007fff9c0743f0 in ?? ()
(gdb) bt
#0  0x00007fff9c0743f0 in ?? ()
#1  0x00007fffe5e905ec in float** dnnl::impl::memory_tracking::grantor_t::get&lt;float*&gt;(unsigned int const&amp;) const
    () from /home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so
#2  0x00007fffe5e93697 in dnnl::impl::cpu::_ref_rnn_common_t&lt;(dnnl_prop_kind_t)64, (dnnl_data_type_t)3, (dnnl_data_type_t)3, (dnnl_data_type_t)3&gt;::execute_(dnnl::impl::exec_ctx_t const&amp;) const ()
   from /home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so
#3  0x00007fffe5d05de9 in dnnl::impl::cpu::_ref_rnn_common_t&lt;(dnnl_prop_kind_t)64, (dnnl_data_type_t)3, (dnnl_data_type_t)3, (dnnl_data_type_t)3&gt;::execute(dnnl::impl::exec_ctx_t const&amp;) const ()
   from /home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so
#4  0x00007fffe5890788 in dnnl_primitive_execute ()
   from /home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so
#5  0x00007fffe0a5eb1a in mxnet::MKLDNNStream::Submit(bool) ()
   from /home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so
#6  0x00007fffe0b13343 in mxnet::op::MKLDNNRnnOp::Forward(mxnet::OpContext const&amp;, std::vector&lt;mxnet::NDArray, std::allocator&lt;mxnet::NDArray&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray, std::allocator&lt;mxnet::NDArray&gt; &gt; const&amp;) ()
   from /home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so
#7  0x00007fffe5306633 in mxnet::op::RNNStatefulComputeExCPU(mxnet::OpStatePtr const&amp;, mxnet::OpContext const&amp;, std::vector&lt;mxnet::NDArray, std::allocator&lt;mxnet::NDArray&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray, std::allocator&lt;mxnet::NDArray&gt; &gt; const&amp;) ()
   from /home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so
#8  0x00007fffe4f503fd in mxnet::imperative::PushOperator(mxnet::OpStatePtr const&amp;, nnvm::Op const*, nnvm::NodeAttrs const&amp;, mxnet::Context const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::Resource, std::allocator&lt;mxnet::Resource&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, std::vector&lt;unsigned int, std::allocator&lt;unsigned int&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;, mxnet::DispatchMode)::{lambda(mxnet::RunContext, mxnet::engine::CallbackOnComplete)#1}::operator()(mxnet::RunContext, mxnet::engine::CallbackOnComplete) const () from /home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so
#9  0x00007fffe4f506cd in std::_Function_handler&lt;void (mxnet::RunContext), mxnet::imperative::PushOperator(mxnet::O---Type &lt;return&gt; to continue, or q &lt;return&gt; to quit---
pStatePtr const&amp;, nnvm::Op const*, nnvm::NodeAttrs const&amp;, mxnet::Context const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::Resource, std::allocator&lt;mxnet::Resource&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray*, std::allocator&lt;mxnet::NDArray*&gt; &gt; const&amp;, std::vector&lt;unsigned int, std::allocator&lt;unsigned int&gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;, mxnet::DispatchMode)::{lambda(mxnet::RunContext)#2}&gt;::_M_invoke(std::_Any_data const&amp;, mxnet::RunContext) () from /home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so
#10 0x00007fffe501d754 in std::_Function_handler&lt;void (mxnet::RunContext, mxnet::engine::CallbackOnComplete), mxnet::engine::ThreadedEngine::PushSync(std::function&lt;void (mxnet::RunContext)&gt;, mxnet::Context, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, std::vector&lt;mxnet::engine::Var*, std::allocator&lt;mxnet::engine::Var*&gt; &gt; const&amp;, mxnet::FnProperty, int, char const*)::{lambda(mxnet::RunContext, mxnet::engine::CallbackOnComplete)#1}&gt;::_M_invoke(std::_Any_data const&amp;, mxnet::RunContext, mxnet::engine::CallbackOnComplete) ()
   from /home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so
#11 0x00007fffe50180a5 in mxnet::engine::ThreadedEngine::ExecuteOprBlock(mxnet::RunContext, mxnet::engine::OprBlock*) () from /home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so
#12 0x00007fffe502a294 in std::_Function_handler&lt;void (std::shared_ptr&lt;dmlc::ManualEvent&gt;), mxnet::engine::ThreadedEnginePerDevice::PushToExecute(mxnet::engine::OprBlock*, bool)::{lambda()#1}::operator()() const::{lambda(std::shared_ptr&lt;dmlc::ManualEvent&gt;)#1}&gt;::_M_invoke(std::_Any_data const&amp;, std::shared_ptr&lt;dmlc::ManualEvent&gt;) ()
   from /home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so
#13 0x00007fffe5016934 in std::thread::_Impl&lt;std::_Bind_simple&lt;std::function&lt;void (std::shared_ptr&lt;dmlc::ManualEvent&gt;)&gt; (std::shared_ptr&lt;dmlc::ManualEvent&gt;)&gt; &gt;::_M_run() ()
   from /home/ubuntu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so
#14 0x00007fffded79421 in std::execute_native_thread_routine_compat (__p=&lt;optimized out&gt;)
    at /home/nwani/m3/conda-bld/compilers_linux-64_1560109574129/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/src/c++11/thread.cc:94
#15 0x00007ffff7bbd6db in start_thread (arg=0x7fffbac26700) at pthread_create.c:463
#16 0x00007ffff78e6a3f in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:95
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='Zha0q1' date='2020-10-01T20:15:41Z'>
		&lt;denchmark-link:https://github.com/TaoLv&gt;@TaoLv&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ciyongch&gt;@ciyongch&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/PatricZhao&gt;@PatricZhao&lt;/denchmark-link&gt;
 - Hello guys. Can you please help in this issue. We saw atleast 2 production users impacted by this and USE_MKLDNN=0 was temp fix, but performance is really bad as expected. This is a blocker.
		</comment>
		<comment id='3' author='Zha0q1' date='2020-10-01T21:10:43Z'>
		&lt;denchmark-link:https://github.com/anko-intel&gt;@anko-intel&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='Zha0q1' date='2020-10-02T16:37:55Z'>
		Thanks, &lt;denchmark-link:https://github.com/Zha0q1&gt;@Zha0q1&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/sandeep-krishnamurthy&gt;@sandeep-krishnamurthy&lt;/denchmark-link&gt;
! I have a look at this issue.
		</comment>
		<comment id='5' author='Zha0q1' date='2020-10-05T11:21:56Z'>
		&lt;denchmark-link:https://github.com/Zha0q1&gt;@Zha0q1&lt;/denchmark-link&gt;
 Could you please tell me a little bit more details about this issue, such as the branch name and its commit sha and what version of MKLDNN you have (commit-sha)? Thanks!
		</comment>
		<comment id='6' author='Zha0q1' date='2020-10-05T17:31:59Z'>
		I am using mxnet 1.7 (&lt;denchmark-link:https://github.com/apache/incubator-mxnet/releases/tag/1.7.0&gt;https://github.com/apache/incubator-mxnet/releases/tag/1.7.0&lt;/denchmark-link&gt;
) from . The machine was a C5.9xlarge DLAMI Ubuntu 18 EC2 instance.
		</comment>
		<comment id='7' author='Zha0q1' date='2020-10-13T13:38:25Z'>
		Hi,
Well,
When running our pre-model (this is a simple imitation of the LSTM model). While a test, I want to create a large LSTM tensor, for example: (20758,500). It could be seen that ~ 170GB of memory is allocated for scratchpad computations. We can see that global memory is always true. Well, as a result, for a different oneDNN version, I got the error messages: as following:

For a given v.1.3 version of mkldnn: Segmentation fault: 11
For a given v.1.6 version of mkldnn:  mxnet.base.MXNetError: MXNetError: could not create a primitive

This error is only visible for a large LSTM tensor. Step-by-step reproduction casts light on this issue. If we have a look at the code, a lot of things might be visible there. First off, the standard Vanilla-LSTM algorithm of MKLDNN leads to allocate sufficient/insufficient block of memory. The block is allocated based on this equation: sizeof(float) * work_space, where work_space is an offset (in bytes). For a given test (input: 20758,500) we can see that ~170 GB od memory is allocated for scratchpad computation, where workspace = 47952392192 * sizeof(float) =  191809568768 bytes ~ 170 GB.  If you don't have enough space, you will get both errors: see 1 &amp; 2. In Intel, MKLDNN primitives can use either individual memory or global buffer memory for an intermediate computation. The first one might lead to getting better performance result since memory most likely will be attached to any thread. The second one, might save a lot of memory.
For brevity:
The input tensor is T x N x C, well, for a given example (10758, 500), T is 10758, C is 500, That means that we need at least 4 * 10758 * 500 * 500 * 4 bytes ~ 40 GB, or maybe more. Basically the work-space would be comparable with the grid size n_layers * mb * n_times_stamps * 4 (gates) * max(sic, slc, dhsc) ^ 2.  For a given oneDNN version (1.3 and 1.6) the size of work-space (i.e LSTM space) is equal book&lt;float&gt;(num_elems, ....) ~ 40 GB * sizeof(T) = 40 GB * 4 ~160GB.  An upper_bound (the size of input tensor) has not been clearly defined and its upper_bound has been limited by physical side of memory.  Well, the size of buffer which is need to allocate LSTM tensor is determined, as following: 4 * 10758 * 500 * 500 * 4 bytes ~ 40 GB.  Yet, this value is multiply by the constant value of its type (in this case:  = float). 
Approximately: it should be defined, as following: 

The size of work-space * , where  is &lt;uint8_t&gt; ~ * 1byte [potentially]
The workspace is only limited by the total number of elements of a given tensor. 

An upper_bound of a given tensor is equal (the upper-bound of LSTM)
n^2 * m = memory_space / (16 bytes)
		</comment>
		<comment id='8' author='Zha0q1' date='2020-10-20T09:56:40Z'>
		Hi &lt;denchmark-link:https://github.com/Zha0q1&gt;@Zha0q1&lt;/denchmark-link&gt;

There’s a bug in oneDNN LSTM forward inference that results in using ~4x more memory for LSTM workspace in inference cases.
Could you please tell me whether this addressing (look at the table), is acceptable and it allows you to resolve any issues?



  (dim: 20756, 500)
Before
After




The total size of memory needed to allocate LSTM tensor
230 GB (~4x more memory)
56 GB (~4x less memory)



		</comment>
		<comment id='9' author='Zha0q1' date='2020-10-20T21:03:40Z'>
		&lt;denchmark-link:https://github.com/mozga-intel&gt;@mozga-intel&lt;/denchmark-link&gt;
 Thanks for you investigation! Yes, this improvement is huge and will help our users who run inference tasks on pre-trained models. It would be great to include this fix in the next oneDNN release
		</comment>
		<comment id='10' author='Zha0q1' date='2020-10-29T03:00:14Z'>
		
@TaoLv @ciyongch @PatricZhao - Hello guys. Can you please help in this issue. We saw atleast 2 production users impacted by this and USE_MKLDNN=0 was temp fix, but performance is really bad as expected. This is a blocker.

Sorry for that and the team is working on fixing any possible issues. Feel free to ping us for any issue :)
		</comment>
	</comments>
</bug>