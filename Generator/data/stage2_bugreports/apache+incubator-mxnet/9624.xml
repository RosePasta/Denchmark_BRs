<bug id='9624' author='solin319' open_date='2018-01-30T01:36:43Z' closed_time='2020-09-26T18:48:45Z'>
	<summary>problem when set fix_gamma=True in batchnorm</summary>
	<description>
If fix_gamma is true, then set gamma to 1 and its gradient to 0.
But the value of gamma will be changed during parameters update. So the gamma saved in param file was not 1. These will bring a problem in convert MXNet parameters to other deep-learning platforms.
This problem was caused by we set a default weight-decay in SGD optimizer.
We must define variable gamma with wd_mult=0 to fix gamma=1 during training.
Can MXNet set wd of gamma to 0 automatically when fix_gamma=1?
	</description>
	<comments>
		<comment id='1' author='solin319' date='2018-02-01T20:40:21Z'>
		This is a legacy design defect. When fix_gamma is true there shouldn't be a gamma parameter.
In the future this could be solved by creating new operator batch_norm and deprecating BatchNorm
		</comment>
		<comment id='2' author='solin319' date='2018-02-27T20:19:33Z'>
		&lt;denchmark-link:https://github.com/sandeep-krishnamurthy&gt;@sandeep-krishnamurthy&lt;/denchmark-link&gt;
 : Tag: Bug
		</comment>
		<comment id='3' author='solin319' date='2018-06-14T08:04:33Z'>
		&lt;denchmark-link:https://github.com/solin319&gt;@solin319&lt;/denchmark-link&gt;
 how did you check the value of gamma? I suspect I am facing the same issue and hence, want to verify. But unfortunately, I couldn't find gamma in either arg_params or aux_params.
		</comment>
		<comment id='4' author='solin319' date='2019-03-14T00:52:34Z'>
		&lt;denchmark-link:https://github.com/mxnet-label-bot&gt;@mxnet-label-bot&lt;/denchmark-link&gt;
 add [Operator]
		</comment>
		<comment id='5' author='solin319' date='2019-04-15T20:24:10Z'>
		&lt;denchmark-link:https://github.com/solin319&gt;@solin319&lt;/denchmark-link&gt;
 I guess, the fix is already merged and closed.
&lt;denchmark-link:https://github.com/anirudh2290&gt;@anirudh2290&lt;/denchmark-link&gt;
 Can we close this one.
		</comment>
		<comment id='6' author='solin319' date='2019-04-15T21:08:16Z'>
		&lt;denchmark-link:https://github.com/Vikas89&gt;@Vikas89&lt;/denchmark-link&gt;
 the fix is only for coreml converter. The operator hasnt been fixed yet.
		</comment>
		<comment id='7' author='solin319' date='2020-09-26T08:55:21Z'>
		&lt;denchmark-link:https://github.com/solin319&gt;@solin319&lt;/denchmark-link&gt;
 Does it means that the parameters would be updated even its grad_req is set to "null" if the wd_mult is not set to zero?
I think this behavior is unexpected.
		</comment>
		<comment id='8' author='solin319' date='2020-09-26T18:48:45Z'>
		I think &lt;denchmark-link:https://github.com/wkcn&gt;@wkcn&lt;/denchmark-link&gt;
 fixed this issue in &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/18500&gt;#18500&lt;/denchmark-link&gt;
. Now the batchnorm op would respond to grad_req=null correctly.
		</comment>
	</comments>
</bug>