<bug id='13438' author='samskalicky' open_date='2018-11-28T16:26:33Z' closed_time='2019-08-09T21:12:56Z'>
	<summary>libc getenv is not threadsafe</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

getenv() calls in libc are not threadsafe according to:
&lt;denchmark-link:https://rachelbythebay.com/w/2017/01/30/env/&gt;https://rachelbythebay.com/w/2017/01/30/env/&lt;/denchmark-link&gt;

and
&lt;denchmark-link:https://github.com/xianyi/OpenBLAS/issues/716&gt;xianyi/OpenBLAS#716&lt;/denchmark-link&gt;

There are indirect calls to dmlc::GetEnv() all across the mxnet codebase, here are a few:



incubator-mxnet/src/engine/threaded_engine_perdevice.cc


         Line 79
      in
      266de6b






 cpu_worker_nthreads_ = dmlc::GetEnv("MXNET_CPU_WORKER_NTHREADS", 1); 





or



incubator-mxnet/src/executor/graph_executor.cc


         Line 1194
      in
      5a83b6b






 bool prefer_bulk_exec_inference = dmlc::GetEnv("MXNET_EXEC_BULK_EXEC_INFERENCE", true); 





&lt;denchmark-h:h2&gt;Error Message:&lt;/denchmark-h&gt;

/lib64/libc.so.6(+0x35250) [0x7fdc5b99b250]
/lib64/libc.so.6(getenv+0xad) [0x7fdc5b99e0cd]
/opt/amazon/lib/libmxnet.so(ZN4dmlc6GetEnvIbEET_PKcS1+0x1b) [0x7fdc4a1bedab]
/opt/amazon/lib/libmxnet.so(_ZN5mxnet4exec13GraphExecutor10InitOpSegsEv+0x1cb) [0x7fdc4a1b6e0b]
/opt/amazon/lib/libmxnet.so(_ZN5mxnet4exec13GraphExecutor15FinishInitGraphEN4nnvm6SymbolENS2_5GraphEPNS_8ExecutorERKSt13unordered_mapINS2_9NodeEntryENS_7NDArrayENS2_13NodeEntryHashENS2_14NodeEntryEqualESaISt4pairIKS8_S9_EEE+0x71b) [0x7fdc4a1b760b]
/opt/amazon/lib/libmxnet.so(_ZN5mxnet4exec13GraphExecutor4InitEN4nnvm6SymbolERKNS_7ContextERKSt3mapISsS4_St4lessISsESaISt4pairIKSsS4_EEERKSt6vectorIS4_SaIS4_EESL_SL_RKSt13unordered_mapISsNS2_6TShapeESt4hashISsESt8equal_toISsESaISA_ISB_SN_EEERKSM_ISsiSP_SR_SaISA_ISB_iEEES11_RKSH_INS_9OpReqType
ESaIS12_EERKSt13unordered_setISsSP_SR_SaISsEEPSH_INS_7NDArrayESaIS1C_EES1F_S1F_PSM_ISsS1C_SP_SR_SaISA_ISB_S1C_EEEPNS_8ExecutorERKSM_INS2_9NodeEntryES1C_NS2_13NodeEntryHashENS2_14NodeEntryEqualESaISA_IKS1M_S1C_EEE+0x75d) [0x7fdc4a1b958d]
/opt/amazon/lib/libmxnet.so(ZN5mxnet8Executor10SimpleBindEN4nnvm6SymbolERKNS_7ContextERKSt3mapISsS3_St4lessISsESaISt4pairIKSsS3_EEERKSt6vectorIS3_SaIS3_EESK_SK_RKSt13unordered_mapISsNS1_6TShapeESt4hashISsESt8equal_toISsESaIS9_ISA_SM_EEERKSL_ISsiSO_SQ_SaIS9_ISA_iEEES10_RKSG_INS_9OpReqTypeESaI
S11_EERKSt13unordered_setISsSO_SQ_SaISsEEPSG_INS_7NDArrayESaIS1B_EES1E_S1E_PSL_ISsS1B_SO_SQ_SaIS9_ISA_S1B_EEEPS0+0x1a6) [0x7fdc4a1b9f46]
/opt/amazon/lib/libmxnet.so(MXExecutorSimpleBind+0x1e38) [0x7fdc4a1031a8]
/opt/amazon/python2.7/lib/python2.7/lib-dynload/_ctypes.so(ffi_call_unix64+0x4c) [0x7fdc5af9b858]
&lt;denchmark-h:h2&gt;Minimum reproducible example&lt;/denchmark-h&gt;

TODO
&lt;denchmark-h:h2&gt;Steps to reproduce&lt;/denchmark-h&gt;

TODO
	</description>
	<comments>
		<comment id='1' author='samskalicky' date='2018-11-28T20:40:19Z'>
		&lt;denchmark-link:https://github.com/samskalicky&gt;@samskalicky&lt;/denchmark-link&gt;
 Thanks for identifying the thread safety issue
		</comment>
		<comment id='2' author='samskalicky' date='2018-11-28T20:52:19Z'>
		&lt;denchmark-link:https://github.com/mxnet-label-bot&gt;@mxnet-label-bot&lt;/denchmark-link&gt;
 add [Backend, Bug]
		</comment>
		<comment id='3' author='samskalicky' date='2018-11-28T21:34:00Z'>
		I found a set/get in MXNet for MXNET_CPU_WORKER_NTHREADS:
&lt;denchmark-link:https://user-images.githubusercontent.com/1541993/49183630-34776180-f312-11e8-9df8-2a2295d77b43.png&gt;&lt;/denchmark-link&gt;

So if we're running multiple programs using MXNet on the same machine, its possible that one will be setting MXNET_CPU_WORKER_NTHREADS when the other is getting MXNET_CPU_WORKER_NTHREADS and since they both use the same libc, we're going to hit this issue.
		</comment>
		<comment id='4' author='samskalicky' date='2018-11-28T21:35:49Z'>
		FYI &lt;denchmark-link:https://github.com/KellenSunderland&gt;@KellenSunderland&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/larroy&gt;@larroy&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='samskalicky' date='2018-11-28T22:10:17Z'>
		&lt;denchmark-link:https://github.com/samskalicky&gt;@samskalicky&lt;/denchmark-link&gt;
 I don't think the problem is running different processes, one process shouldn't affect the other in terms of environment and per separation of the process space. The problem is using getenv / setenv from the same process in different threads, as is the case from the code you linked.
		</comment>
		<comment id='6' author='samskalicky' date='2018-11-28T22:10:46Z'>
		Could you decode the stack trace via c++filt?
		</comment>
		<comment id='7' author='samskalicky' date='2018-11-28T22:33:03Z'>
		Looks like the evidence points to where we suspected, initialize.cc We should rework that code.
		</comment>
		<comment id='8' author='samskalicky' date='2018-11-28T22:35:00Z'>
		&lt;denchmark-code&gt;/lib64/libc.so.6(+0x35250) [0x7fdc5b99b250]
/lib64/libc.so.6(getenv+0xad) [0x7fdc5b99e0cd]
/opt/amazon/lib/libmxnet.so(ZN4dmlc6GetEnvIbEET_PKcS1+0x1b) [0x7fdc4a1bedab]
/opt/amazon/lib/libmxnet.so(mxnet::exec::GraphExecutor::InitOpSegs()+0x1cb) [0x7fdc4a1b6e0b]
/opt/amazon/lib/libmxnet.so(mxnet::exec::GraphExecutor::FinishInitGraph(nnvm::Symbol, nnvm::Graph, mxnet::Executor*, std::unordered_map&lt;nnvm::NodeEntry, mxnet::NDArray, nnvm::NodeEntryHash, nnvm::NodeEntryEqual, std::allocator&lt;std::pair&lt;nnvm::NodeEntry const, mxnet::NDArray&gt; &gt; &gt; const&amp;)+0x71b) [0x7fdc4a1b760b]
/opt/amazon/lib/libmxnet.so(mxnet::exec::GraphExecutor::Init(nnvm::Symbol, mxnet::Context const&amp;, std::map&lt;std::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, mxnet::Context, std::less&lt;std::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt;, std::allocator&lt;std::pair&lt;std::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const, mxnet::Context&gt; &gt; &gt; const&amp;, std::vector&lt;mxnet::Context, std::allocator&lt;mxnet::Context&gt; &gt; const&amp;, std::vector&lt;mxnet::Context, std::allocator&lt;mxnet::Context&gt; &gt; const&amp;, std::vector&lt;mxnet::Context, std::allocator&lt;mxnet::Context&gt; &gt; const&amp;, std::unordered_map&lt;std::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, nnvm::TShape, std::hash&lt;std::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt;, std::equal_to&lt;std::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt;, std::allocator&lt;std::pair&lt;std::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const, nnvm::TShape&gt; &gt; &gt; const&amp;, std::unordered_map&lt;std::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, int, std::hash&lt;std::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt;, std::equal_to&lt;std::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt;, std::allocator&lt;std::pair&lt;std::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const, int&gt; &gt; &gt; const&amp;, std::unordered_map&lt;std::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, int, std::hash&lt;std::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt;, std::equal_to&lt;std::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt;, std::allocator&lt;std::pair&lt;std::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const, int&gt; &gt; &gt; const&amp;, std::vector&lt;mxnet::OpReqType, std::allocator&lt;mxnet::OpReqType&gt; &gt; const&amp;, std::unordered_set&lt;std::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, std::hash&lt;std::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt;, std::equal_to&lt;std::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt;, std::allocator&lt;std::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;, std::vector&lt;mxnet::NDArray, std::allocator&lt;mxnet::NDArray&gt; &gt;*, std::vector&lt;mxnet::NDArray, std::allocator&lt;mxnet::NDArray&gt; &gt;*, std::vector&lt;mxnet::NDArray, std::allocator&lt;mxnet::NDArray&gt; &gt;*, std::unordered_map&lt;std::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, mxnet::NDArray, std::hash&lt;std::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt;, std::equal_to&lt;std::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt;, std::allocator&lt;std::pair&lt;std::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const, mxnet::NDArray&gt; &gt; &gt;*, mxnet::Executor*, std::unordered_map&lt;nnvm::NodeEntry, mxnet::NDArray, nnvm::NodeEntryHash, nnvm::NodeEntryEqual, std::allocator&lt;std::pair&lt;nnvm::NodeEntry const, mxnet::NDArray&gt; &gt; &gt; const&amp;)+0x75d)
[0x7fdc4a1b958d] /opt/amazon/lib/libmxnet.so(ZN5mxnet8Executor10SimpleBindEN4nnvm6SymbolERKNS_7ContextERKSt3mapISsS3_St4lessISsESaISt4pairIKSsS3_EEERKSt6vectorIS3_SaIS3_EESK_SK_RKSt13unordered_mapISsNS1_6TShapeESt4hashISsESt8equal_toISsESaIS9_ISA_SM_EEERKSL_ISsiSO_SQ_SaIS9_ISA_iEEES10_RKSG_INS_9OpReqTypeESaIS11_EERKSt13unordered_setISsSO_SQ_SaISsEEPSG_INS_7NDArrayESaIS1B_EES1E_S1E_PSL_ISsS1B_SO_SQ_SaIS9_ISA_S1B_EEEPS0+0x1a6) [0x7fdc4a1b9f46]
/opt/amazon/lib/libmxnet.so(MXExecutorSimpleBind+0x1e38) [0x7fdc4a1031a8]
/opt/amazon/python2.7/lib/python2.7/lib-dynload/_ctypes.so(ffi_call_unix64+0x4c) [0x7fdc5af9b858]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='samskalicky' date='2018-11-29T16:26:37Z'>
		What if we set a process wide mutex here: &lt;denchmark-link:https://github.com/dmlc/dmlc-core/blob/600bc28bc476a1b7866cfe9f444c91b6d49760b7/include/dmlc/parameter.h#L1062&gt;https://github.com/dmlc/dmlc-core/blob/600bc28bc476a1b7866cfe9f444c91b6d49760b7/include/dmlc/parameter.h#L1062&lt;/denchmark-link&gt;

and here: &lt;denchmark-link:https://github.com/dmlc/dmlc-core/blob/600bc28bc476a1b7866cfe9f444c91b6d49760b7/include/dmlc/parameter.h#L1076&gt;https://github.com/dmlc/dmlc-core/blob/600bc28bc476a1b7866cfe9f444c91b6d49760b7/include/dmlc/parameter.h#L1076&lt;/denchmark-link&gt;

Should fix the issue right?  No deadlocks? We'd have to make sure the mutex wasn't threadpool scoped as it's getting set in a new threadpool
		</comment>
		<comment id='10' author='samskalicky' date='2018-11-29T16:36:15Z'>
		as you said, what about libraries like openmp which might access the environment variable without a lock?.  We could try monkey patching getenv with our own wrapper at link time or runtime. Best is not to use setenv after we have multiple threads running.
		</comment>
		<comment id='11' author='samskalicky' date='2018-11-30T11:56:00Z'>
		I don't think we need any mutex. We can just pass any config to the thread as data if we need. Definitely we shouldn't be using the environment to config workers.
		</comment>
		<comment id='12' author='samskalicky' date='2018-12-03T18:11:15Z'>
		In an attempt to reproduce the issue (assuming its coming from simultaneous processes initializing at the same time) I created a cpp engine unit test by modifying tests/cpp/engine/threaded_engine_test.cc and adding the following two functions:
&lt;denchmark-code&gt;void handler(int sig) {
  void *array[10];
  size_t size;

  // get void*'s for all entries on the stack                                                                                                                        
  size = backtrace(array, 10);

  // print out all the frames to stderr                                                                                                                              
  fprintf(stderr, "Error: signal %d:\n", sig);
  backtrace_symbols_fd(array, size, STDERR_FILENO);
  exit(1);
}

TEST(Engine, envsegfault) {
  signal(SIGSEGV, handler);
 mxnet::Engine* engine = mxnet::engine::CreateThreadedEnginePerDevice();

 for (int i = 0; i &lt; 10000; ++i) {
   engine-&gt;Stop();
   engine-&gt;Start();
 }
}
&lt;/denchmark-code&gt;

The handler attempts to capture the segfault and print the stack trace, and the envsegfault test creates a threaded engine object and repeatedly stops and starts it to trigger the SetEnv call in initialize.cc and GetEnv call in threaded_engine_perdevice.cc.
Then I wrote this python script to run multiple independent processes doing this:
&lt;denchmark-code&gt;import multiprocessing
import time
import os

def mxnet_worker():
    for i in range(100):
        ret = os.system('./mxnet_unit_tests  --gtest_filter=Engine.envsegfault')
        if ret != 0:
            print('Failed@@@@@@@@@@@@@@@@')
            exit(2)


read_process = [multiprocessing.Process(target=mxnet_worker) for i in range(10)]
for p in read_process:
    p.daemon = True
    p.start()

for p in read_process:
    p.join()
&lt;/denchmark-code&gt;

I was able to trigger a segfault, but I hadnt added the handler yet to print the stack trace. I need to run again and try to reproduce again.
		</comment>
		<comment id='13' author='samskalicky' date='2018-12-03T18:24:37Z'>
		Do you think the multiple process have any impact? Env is not shared across processes.
		</comment>
		<comment id='14' author='samskalicky' date='2018-12-05T18:25:14Z'>
		process != thread.  In this case I think you mean thread.
Other than that I agree with your analysis.
I think we need to understand if not using this variable as in &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/13472&gt;#13472&lt;/denchmark-link&gt;
  makes unwanted threads to be spawn inside the engine workers, because it might actually not be the case.
Depending on that, we need to see if we need to use a different mechanism in the engine to control OMP threads, then run performance regression tests.
As you see from my PR, not changing the environment is functionally correct, but I don't know if it has performance impact because of the reason explained above.
		</comment>
		<comment id='15' author='samskalicky' date='2018-12-05T18:26:43Z'>
		I would suggest using an atomic counter in pthread_atfork that is incremented whenever we create a thread.  In this way see how many threads we are creating and printing it out and comparing two runs with, and without the variables that I removed to see if there's any difference, that would be a quick check.
		</comment>
		<comment id='16' author='samskalicky' date='2018-12-05T18:31:18Z'>
		Heres the relevant code from the version of libc (2.17) that was being used in the failure:
getenv.c: &lt;denchmark-link:https://pastebin.com/gj4XZgdE&gt;https://pastebin.com/gj4XZgdE&lt;/denchmark-link&gt;

setenv.c: &lt;denchmark-link:https://pastebin.com/NuTfDdDp&gt;https://pastebin.com/NuTfDdDp&lt;/denchmark-link&gt;

It looks like the issue is the __environ variable being iterated over in lines 15, 32, 55. This variable is defined in setenv.c:
&lt;denchmark-code&gt;#if !_LIBC
# define __environ      environ
# ifndef HAVE_ENVIRON_DECL
extern char **environ;
# endif
#endif
&lt;/denchmark-code&gt;

So there is this nasty global pointer to the environment. While setenv has a lock around changing the environment, theres no lock for getenv so (as mentioned in the block post linked in the description) :

"this is a char**, so it's supposed to have a bunch of pointers to char* buffers, one for each var=val pairing. What if that region has been reused and now contains a bogus pointer into the weeds? Yep, segfault."

So I think &lt;denchmark-link:https://github.com/KellenSunderland&gt;@KellenSunderland&lt;/denchmark-link&gt;
's suggestion of adding a lock/mutex in dmls parameter.h's GetEnv and SetEnv should prevent any failure due to MXNet's various threads accessing the environment simultaneously. Although without a reproducible test case we're unable to validate this.
Anybody have any ideas for reproducing something similar to this code from the blog post with mxnet's engine tests so that we have a reproducible test case?
&lt;denchmark-code&gt;#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;pthread.h&gt;
 
static void* worker(void* arg) {
  for (;;) {
    int i;
    char var[256], *p = var;
 
    for (i = 0; i &lt; 8; ++i) {
      *p++ = 65 + (random() % 26);
    }
 
    *p++ = '\0';
 
    setenv(var, "test", 1);
  }
 
  return NULL;
}
 
int main() {
  pthread_t t;
 
  printf("start\n");
  setenv("foo", "bar", 0);
  pthread_create(&amp;t, NULL, worker, 0);
 
  for (;;) {
    getenv("foo");
  }
 
  return 0;
}
&lt;/denchmark-code&gt;

I tried simplifying the start_stop test &lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/tests/cpp/engine/threaded_engine_test.cc#L124&gt;https://github.com/apache/incubator-mxnet/blob/master/tests/cpp/engine/threaded_engine_test.cc#L124&lt;/denchmark-link&gt;
 for just the threaded_engine_perdevice and stop/started it a few hundred times and it didnt cause a segfault.
		</comment>
		<comment id='17' author='samskalicky' date='2018-12-05T18:45:45Z'>
		We should not use locking in the handlers in pthread_atfork, it will likely have negative impacts in terms of lock contention and performance and architecturally is not a good solution. We should find a solution using good architecture to pass the number of threads in the engine. threads get a data pointer where you can put configuration or pass any other data at creation time.
		</comment>
		<comment id='18' author='samskalicky' date='2018-12-05T19:08:54Z'>
		&lt;denchmark-link:https://github.com/larroy&gt;@larroy&lt;/denchmark-link&gt;
 We were already using locks inside the handlers. setenv uses a lock in libc around modifying the environment. This doesnt change anything.
Are you suggesting that we dont use an EnvVar to pass the OMP_NUM_THREADS=1 to subsequent processes? What do you think the impact will be the other processes that are using MXNet using more threads?
Was there a conscious decision to use an EnvVar to limit OMP_NUM_THREADS in subsequent processes?
		</comment>
		<comment id='19' author='samskalicky' date='2018-12-06T04:08:55Z'>
		The problem as provided in the article linked in this issue and related article here: &lt;denchmark-link:https://rachelbythebay.com/w/2017/01/30/env/&gt;https://rachelbythebay.com/w/2017/01/30/env/&lt;/denchmark-link&gt;
 is that if the main thread spawns another thread, which calls setenv and while we call setenv the process is forked, the mutex is currently in locked state in the child process and it will never be unlocked since there is no thread to release the lock which causes it to hang.
This can be replicated in MXNet in the following way. Pull the code from &lt;denchmark-link:https://github.com/anirudh2290/mxnet/tree/setenv_issue&gt;https://github.com/anirudh2290/mxnet/tree/setenv_issue&lt;/denchmark-link&gt;
 and build it similar to the following:
&lt;denchmark-code&gt;cd build &amp;&amp; cmake VERBOSE=1 -DUSE_CUDA=ON -DUSE_CUDNN=ON -DUSE_MKLDNN=ON -DUSE_OPENMP=ON -DUSE_OPENCV=OFF -DCMAKE_BUILD_TYPE=Debug -GNinja ..
&lt;/denchmark-code&gt;

Run the following script:
&lt;denchmark-code&gt;  import multiprocessing
  import os
  import sys
  import mxnet as mx

  def mxnet_worker():
       print 'inside mxnet_worker'

  mx.base._LIB.MXStartBackgroundThread(mx.base.c_str("dummy"))
  read_process = [multiprocessing.Process(target=mxnet_worker) for i in range(8)]
  for p in read_process:
      p.daemon = True
      p.start()
      p.join()
&lt;/denchmark-code&gt;

Now run the script, you will be able to see the process hangs.
When I attach gdb to the process I see the following:
&lt;denchmark-code&gt;#0  __lll_lock_wait_private () at ../sysdeps/unix/sysv/linux/x86_64/lowlevellock.S:95
#1  0x00007fc0fabab99c in __add_to_environ (name=0x7fc093a935fc "MXNET_CPU_WORKER_NTHREADS", value=0x7fffec2eff10 "1", combined=0x0,
    replace=1) at setenv.c:133
&lt;/denchmark-code&gt;

which means it is stuck trying to acquire the lock: &lt;denchmark-link:https://github.com/lattera/glibc/blob/master/stdlib/setenv.c#L133&gt;https://github.com/lattera/glibc/blob/master/stdlib/setenv.c#L133&lt;/denchmark-link&gt;

I checked the mxnet codebase to see if we are calling SetEnv anywhere else and we dont seem to be calling it anywhere except here. Also, pthread_at_fork statement calls Engine::Get()-&gt;Stop() which would mean that all engine threads are suspended. It is still possible that it could be called from other multithreaded code in MXNet iterators for example, but I couldnt find it and it is unlikely that we are not using dmlc::SetEnv but something else to set env vars for mxnet or dmlc-core code. I think it is more likely that the customer application spawned a thread, which called SetEnv at the same time pthread_at_fork was called which let to this behavior.
		</comment>
		<comment id='20' author='samskalicky' date='2018-12-06T12:27:59Z'>
		Good analysis &lt;denchmark-link:https://github.com/anirudh2290&gt;@anirudh2290&lt;/denchmark-link&gt;
 agree with it and makes sense. I would suggest we don't use environment for this and have a propper configuration object that we can pass when needed, or in the worst case, can be a singleton, what do you think?
		</comment>
		<comment id='21' author='samskalicky' date='2018-12-06T22:19:23Z'>
		&lt;denchmark-link:https://github.com/larroy&gt;@larroy&lt;/denchmark-link&gt;
 Can you elaborate more on the singleton solution . For example even if its a singleton its still possible that we may fork during setenv call inside a thread and run across the same issue. Removing env variables and changing it to config would be a breaking change. I dont think we can do it before 2.0.
Maybe we should understand more on the customer application and customer issue and come up with a set of guidelines when using multiprocessing with mxnet and document it for now.
		</comment>
		<comment id='22' author='samskalicky' date='2018-12-11T23:03:01Z'>
		&lt;denchmark-link:https://github.com/anirudh2290&gt;@anirudh2290&lt;/denchmark-link&gt;
 good one!
I think this is problem in general. For this particular case we can try to use fork handlers:
pthread_at_fork(prepare, parent, child)
in prepare method, we should try to set bogus env variable, which will unlock the lock and then child will get that lock in unlocked state.
Something like dmlc::setEnv("Bogus", "Bogus") here: &lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/src/initialize.cc#L54&gt;https://github.com/apache/incubator-mxnet/blob/master/src/initialize.cc#L54&lt;/denchmark-link&gt;

		</comment>
		<comment id='23' author='samskalicky' date='2018-12-12T08:38:30Z'>
		Thanks for the suggestion &lt;denchmark-link:https://github.com/Vikas89&gt;@Vikas89&lt;/denchmark-link&gt;
 . I think with your suggested change, it may be less likely to be encountered, but it is possible that mutex is in locked state in child process, if the thread calls setenv after the prepare step and before fork.
		</comment>
		<comment id='24' author='samskalicky' date='2018-12-12T18:58:00Z'>
		If there is only 2 places that too after fork where we setEnv(initialize.cc), why are we setting env ? Can we just keep these values in memory or in some file tied to this mxnet processID ?
		</comment>
		<comment id='25' author='samskalicky' date='2018-12-13T19:34:33Z'>
		I was able to run the user's test script and reproduce in GDB. Looks like its failing when calling getenv("MXNET_EXEC_BULK_EXEC_INFERENCE"). This only seems to happen here:



incubator-mxnet/src/executor/graph_executor.cc


        Lines 1186 to 1209
      in
      5a83b6b






 void GraphExecutor::InitOpSegs() { 



 size_t total_num_nodes = graph_.indexed_graph().num_nodes(); 



   cached_seg_opr_.clear(); 



   CachedSegOpr p; 



   cached_seg_opr_.resize(total_num_nodes, p); 



 if (monitor_callback_) return; 



 



 // Generate segments based on the graph structure 



 bool prefer_bulk_exec_inference = dmlc::GetEnv("MXNET_EXEC_BULK_EXEC_INFERENCE", true); 



 // Whether to perform bulk exec for training 



 const profiler::Profiler *prof = profiler::Profiler::Get(); 



 bool prefer_bulk_exec = dmlc::GetEnv("MXNET_EXEC_BULK_EXEC_TRAIN", 1) 



                           &amp;&amp; (!prof || !prof-&gt;AggregateEnabled()); 



 



 bool is_training = num_forward_nodes_ != total_num_nodes; 



 



 if (prefer_bulk_exec  &amp;&amp; is_training) { 



 this-&gt;BulkTrainingOpSegs(total_num_nodes); 



   } 



 



 if (prefer_bulk_exec_inference &amp;&amp; !is_training) { 



 this-&gt;BulkInferenceOpSegs(); 



   } 



 } 





This matches the original stack trace where we found it failing in GraphExecutor::InitOpSegs
		</comment>
		<comment id='26' author='samskalicky' date='2018-12-17T21:33:25Z'>
		So far, we've been able to verify that compiling mxnet without openmp does not trigger the segfault. This validates that the problem is between one thread in mxnet running openmp and another thread in mxnet calling getenv.
Thus the current workaround (to avoid the segfault) is to recompile mxnet without openmp. This will cause performance degredation for any operators that used to run using openmp (ie. those that are not using BLAS/LAPACK routines or MKLDNN).
		</comment>
		<comment id='27' author='samskalicky' date='2018-12-17T21:40:08Z'>
		&lt;denchmark-link:https://github.com/Vikas89&gt;@Vikas89&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/anirudh2290&gt;@anirudh2290&lt;/denchmark-link&gt;
 have discussed a couple of options for implementing a permanent solution:


Moving all get/setenv calls to the beginning of mxnet initialization, before any openmp or any other dependent library is initialized. This would clearly prevent the problem by moving the getenv to before openmp runs. But then this may cause a breaking change with how developers are using mxnet to change its behavior using environment variables at runtime. If we only sample env vars at init time this will no longer work.


We could write our own thread-safe getenv/setenv functions and implement them across mxnet and openmp (since we're compiling it from source when we build mxnet). This would eliminate the problem, but would be harder to maintain as we would have to constantly patch openmp


We could create APIs to enable the behavior that users currently have when setting env vars, and with the change described in #1 have the same functionality albeit with a different use-model.


None of these seem like the best options, any other comments or thoughts?
		</comment>
		<comment id='28' author='samskalicky' date='2018-12-18T08:54:34Z'>
		I think to arrive at the right solution need to understand more on the openmp behavior and why it calls the library initialize within an operator execution when an omp for loop is called. Need to also confirm if this happens only during forking but also outside forking and what specific openmp versions the customer has and if this can be reproduced on other openmp versions.
		</comment>
		<comment id='29' author='samskalicky' date='2018-12-31T23:13:44Z'>
		Good analysis.  Has anyone checked to see this problem is reproducible with all versions of OMP?  I'm wondering if another workaround that may not have a perf impact would be to use a different implementation than what is used by default.
		</comment>
		<comment id='30' author='samskalicky' date='2018-12-31T23:17:53Z'>
		Regarding option 1.  I actually think this is a good design, but several tests are currently relying on the fact that we can change behaviour via env var changes during runtime.  However, we could probably fix this be reinit'ing the library whenever we set an env var in a test.  I think we should avoid reading env vars at runtime in any case for performance reasons, so it makes sense to read these once at startup and not again after.
		</comment>
		<comment id='31' author='samskalicky' date='2019-01-03T13:46:53Z'>
		I would go for option 1.
		</comment>
		<comment id='32' author='samskalicky' date='2019-01-03T15:44:44Z'>
		Thanks &lt;denchmark-link:https://github.com/larroy&gt;@larroy&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/KellenSunderland&gt;@KellenSunderland&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/anirudh2290&gt;@anirudh2290&lt;/denchmark-link&gt;
, just found out that the problem was due to using Intel OpenMP rather than the LLVM OpenMP submodule. Upon switching back to using LLVM OpenMP the problem did not occur. I think the problem is effectively avoided since we're not advocating for using Intel OpenMP.
&lt;denchmark-link:https://github.com/TaoLv&gt;@TaoLv&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/pengzhao-intel&gt;@pengzhao-intel&lt;/denchmark-link&gt;
 any interest in seeing MXNet switch to Intel OpenMP and investigating this issue? Are there any perceived benefits?
		</comment>
		<comment id='33' author='samskalicky' date='2019-01-04T04:28:28Z'>
		thanks &lt;denchmark-link:https://github.com/samskalicky&gt;@samskalicky&lt;/denchmark-link&gt;
 It's a long thread, we need to warm up and understand the background :)
&lt;denchmark-link:https://github.com/TaoLv&gt;@TaoLv&lt;/denchmark-link&gt;
 could you help take a look for the questions?
		</comment>
		<comment id='34' author='samskalicky' date='2019-01-04T08:04:47Z'>
		&lt;denchmark-link:https://github.com/samskalicky&gt;@samskalicky&lt;/denchmark-link&gt;
 Can you explain more about how to use Intel OMP in MXNet and how to switch back to LLVM OMP? I think LLVM OMP is using the same runtime as Intel OMP.
		</comment>
		<comment id='35' author='samskalicky' date='2019-01-04T17:39:09Z'>
		&lt;denchmark-link:https://github.com/TaoLv&gt;@TaoLv&lt;/denchmark-link&gt;
 according to this page: &lt;denchmark-link:https://software.intel.com/en-us/forums/intel-c-compiler/topic/793552&gt;https://software.intel.com/en-us/forums/intel-c-compiler/topic/793552&lt;/denchmark-link&gt;
 they should be the same but may differ slightly as they have different release schedules. So this is a possibility that the functionality differs between a particular versions of LLVM/Intel OMP.
As for how to use it, I didnt do the building myself but I imagine that those who did simply modified the makefile to link against the Intel OMP binary rather than building the LLVM OMP submodule. Unfortunately I dont have step-by-step instructions for you.
I was just hoping to see what your thoughts were on the current way MXNet is architected in regards to OMP and if what you thought could be improved (possibly by using Intel OMP). And if you had any interest in seeing MXNet switch to Intel OMP and what any benefits might be. Of course we'd still need to maintain a build without Intel OMP for other architectures.
		</comment>
		<comment id='36' author='samskalicky' date='2019-01-06T02:22:15Z'>
		&lt;denchmark-link:https://github.com/samskalicky&gt;@samskalicky&lt;/denchmark-link&gt;
 Yes. I would expect ICC + Intel OMP should have better performance and I know there is an effort ongoing for enabling them for MXNet. Currently MXNet cannot be successfully built with ICC.
		</comment>
	</comments>
</bug>