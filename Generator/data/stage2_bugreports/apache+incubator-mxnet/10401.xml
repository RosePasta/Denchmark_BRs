<bug id='10401' author='Will0622' open_date='2018-04-04T17:03:52Z' closed_time='2018-07-13T20:52:22Z'>
	<summary>Gluon nn BatchNorm layer cannot ignore the beta</summary>
	<description>
In mxnet symbol API, we cannot fix the beta to be 0. So, I tried to use the Gluon nn BatchNorm layer which allows setting center=False to ignore the beta term. However, When I use
bn = mx.nn.BatchNorm(scale=False, center=False)
I checked the mean of the output from this BatchNorm during training. I found that the mean is changing and also is significantly larger than 0 (e.g., it could be 0.7). Then, I looked at the implementation of the nn BatchNorm layer. It sets grad_req = 'null' and differentiable = False for beta when center = False. I have no clue why this is not working, or I got it wrong.
	</description>
	<comments>
		<comment id='1' author='Will0622' date='2018-05-23T04:26:43Z'>
		Hi &lt;denchmark-link:https://github.com/Will0622&gt;@Will0622&lt;/denchmark-link&gt;
. I wasn't able to reproduce this issue with simple code samples. Could you provide a snippet for reproducing the issue?
		</comment>
		<comment id='2' author='Will0622' date='2018-07-13T20:52:21Z'>
		&lt;denchmark-link:https://github.com/Will0622&gt;@Will0622&lt;/denchmark-link&gt;
 closing for lack of activity. If this is still relevant, whenever you're ready, you can provide the information here and ping me to reopen the issue. Thanks.
		</comment>
	</comments>
</bug>