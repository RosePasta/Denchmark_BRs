<bug id='17195' author='cyrusbehr' open_date='2019-12-30T18:07:31Z' closed_time='2020-01-13T18:24:46Z'>
	<summary>Unable to convert Insightface resnet 100 model to onnx</summary>
	<description>
I originally posted this question to the onnxruntime github page, but was hold to repost it here. The original post can be found &lt;denchmark-link:https://github.com/microsoft/onnxruntime/issues/2743#issue-542643478&gt;here&lt;/denchmark-link&gt;
:
I am trying to convert the Insightface resnet 100 model () which can be found &lt;denchmark-link:https://github.com/deepinsight/insightface/wiki/Model-Zoo&gt;here&lt;/denchmark-link&gt;
 to work with onnxruntime.
I use the following conversion python script to convert the model. The model converts successfully.
&lt;denchmark-code&gt;import mxnet as mx
import numpy as np
from mxnet.contrib import onnx as onnx_mxnet
import logging
logging.basicConfig(level=logging.INFO)

sym = 'model-symbol.json'
params = 'model-0000.params'

input_shape = (1,3,112,112)

# Path of the output file
onnx_file = './insightface_resnet100.onnx'
converted_model_path = onnx_mxnet.export_model(sym, params, [input_shape], np.float32, onnx_file)

from onnx import checker
import onnx

model_proto = onnx.load_model(converted_model_path)

# Check if converted ONNX protobuf is valid
checker.check_graph(model_proto.graph)
&lt;/denchmark-code&gt;

Next, I try to load the model in C++ using the following code. Note, I am following the example found &lt;denchmark-link:https://github.com/microsoft/onnxruntime/blob/master/csharp/test/Microsoft.ML.OnnxRuntime.EndToEndTests.Capi/CXX_Api_Sample.cpp&gt;here&lt;/denchmark-link&gt;
 :
&lt;denchmark-code&gt;int main(int argc, char* argv[]) {
    Ort::Env env(ORT_LOGGING_LEVEL_WARNING, "test");
    Ort::SessionOptions session_options;
    session_options.SetIntraOpNumThreads(1);

    session_options.SetGraphOptimizationLevel(GraphOptimizationLevel::ORT_ENABLE_EXTENDED);
    const char* model_path = "../models/insightface_resnet100.onnx";

    Ort::Session session(env, model_path, session_options);

    return 0;
}
&lt;/denchmark-code&gt;

When I run this code, I get the following error:
&lt;denchmark-code&gt;terminate called after throwing an instance of 'Ort::Exception'
  what():  Type Error: Type parameter (T) bound to different types (tensor(float) and tensor(double) in node (_minusscalar0).
Aborted

&lt;/denchmark-code&gt;

I am using libonnxruntime.so.1.1.0
	</description>
	<comments>
		<comment id='1' author='cyrusbehr' date='2020-01-02T08:35:23Z'>
		Faced similar issue 2 months and decided against it. Glad it is being taken up.
		</comment>
		<comment id='2' author='cyrusbehr' date='2020-01-11T05:09:52Z'>
		Tried your scripts and cpp file. The cpp application can load the exported onnx without any error.
I am using

onnx: pip3 install --user onnx==1.2.1
onnxruntime: wget https://github.com/microsoft/onnxruntime/releases/download/v1.1.0/onnxruntime-linux-x64-1.1.0.tgz
mxnet: master branch, commit id: 6f95702
Docker container: built from ci/docker/Dockerfile.build.ubuntu_gpu_tensorrt.

You only mentioned the onnxruntime version, but which version mxnet and onnx are you using? Maybe just upgrade to latest  mxnet and try again?
		</comment>
		<comment id='3' author='cyrusbehr' date='2020-01-13T18:24:46Z'>
		Thank you for your answer &lt;denchmark-link:https://github.com/litaotju&gt;@litaotju&lt;/denchmark-link&gt;

After looking into this again, I realized that when I convert the model using , then the error is generated when loading the model. However, if I use  to convert the model, the model can be loaded without error.
Solution: Use python 3 to convert the model
		</comment>
	</comments>
</bug>