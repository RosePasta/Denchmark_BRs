<bug id='9881' author='eric-haibin-lin' open_date='2018-02-25T09:22:17Z' closed_time='2020-08-01T23:18:39Z'>
	<summary>Inconsistent weight decay logics in multiple optimizers</summary>
	<description>
&lt;denchmark-h:h2&gt;Issue&lt;/denchmark-h&gt;

The default behaviors of many optimizers are not optimal/consistent for optimization. The desired implementation proposed below will help convergence.
&lt;denchmark-h:h2&gt;Gradient Clipping&lt;/denchmark-h&gt;

In Tensorflow/Pytorch, weight decay is usually applied before gradient clipping. Not clipping weight decay would lead to much larger update caused by wd regularization compared to the derivative of the loss. For the following optimizers, the weight decay term is applied after gradient clipping, which should be corrected:

 SGD
 Signum
 LBSGD
 DCASGD
 NAG
 SGLD
 Adam
 AdaDelta
 AdaGrad

&lt;denchmark-h:h2&gt;Weight Decay Not Used to Update Optimizer State&lt;/denchmark-h&gt;

The following optimizers apply wd on weight directly, and the state is not updated. This can make the training slow if a small learning rate is applied, while divergence if a large learning rate is used.

 AdaDelta
 AdaGrad

&lt;denchmark-h:h2&gt;Other Optimizers&lt;/denchmark-h&gt;

FTRL is a proximal optimizer which doesn't use weight decay for gradient clipping nor updating state, which is fine.
The following optimizers apply wd before clipping gradient, which is also fine:

 RMSProp
 Adamax
 Nadam
 FTML

	</description>
	<comments>
		<comment id='1' author='eric-haibin-lin' date='2018-02-26T20:07:52Z'>
		Could you clarify which ones multiply wd with lr and which ones don't?
		</comment>
		<comment id='2' author='eric-haibin-lin' date='2018-02-26T20:29:08Z'>
		Just curious, is clip_gradient used anywhere?
		</comment>
		<comment id='3' author='eric-haibin-lin' date='2018-02-28T02:59:07Z'>
		&lt;denchmark-link:https://github.com/sxjscience&gt;@sxjscience&lt;/denchmark-link&gt;
 supposedly it's used in all optimizers
AdaDelta doesn't multiply wd and lr.
		</comment>
		<comment id='4' author='eric-haibin-lin' date='2018-03-12T19:46:20Z'>
		Unless explicitly specified, for most of the optimizers as implemented in packages such as TF and torch, wd is merged into the gradient before the gradient clipping. When the proximal operator is used, the wd term is not merged.
		</comment>
		<comment id='5' author='eric-haibin-lin' date='2018-04-03T01:14:09Z'>
		&lt;denchmark-link:https://github.com/szhengac&gt;@szhengac&lt;/denchmark-link&gt;
 thanks for your inputs.
My concern is that if we change the wd behavior now, the change is incompatible with previous versions of MXNet and users have to change their code for new hyperparameters. I don't think this is provides a good user experience.
&lt;denchmark-link:https://github.com/szha&gt;@szha&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/piiswrong&gt;@piiswrong&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/szhengac&gt;@szhengac&lt;/denchmark-link&gt;
 What about specifically documenting the update rules for the existing optimizers like &lt;denchmark-link:https://mxnet.incubator.apache.org/versions/master/api/python/optimization/optimization.html#mxnet.optimizer.SGD&gt;https://mxnet.incubator.apache.org/versions/master/api/python/optimization/optimization.html#mxnet.optimizer.SGD&lt;/denchmark-link&gt;
 ? For new optimizers, during code review committers should check if the implementation is similar to pytorch/tf ones?
		</comment>
		<comment id='6' author='eric-haibin-lin' date='2018-04-03T01:30:41Z'>
		Looks good. We can write math formulas instead.
		</comment>
		<comment id='7' author='eric-haibin-lin' date='2018-04-03T22:01:42Z'>
		&lt;denchmark-link:https://github.com/sxjscience&gt;@sxjscience&lt;/denchmark-link&gt;
 I could do that but math formulas is not very convenient when clip is involved..
		</comment>
		<comment id='8' author='eric-haibin-lin' date='2018-05-18T22:08:49Z'>
		Thank Haibin for raising such issues.
Besides, weight decay should only apply to weights (not bias). [1][2] Thus, users usually do
&lt;denchmark-code&gt;trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': learning_rate,
                                       'wd': weight_decay})
&lt;/denchmark-code&gt;

by assuming that weight decay only applies to weights. However, our current implementation applies weight decay to all model parameters including bias.
References:
[1] Franklin, J. (2005). The elements of statistical learning: data mining, inference and prediction. The Mathematical Intelligencer, 27(2), 83-85.
[2] Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). Deep learning (Vol. 1). Cambridge: MIT press.
		</comment>
		<comment id='9' author='eric-haibin-lin' date='2018-06-15T23:55:32Z'>
		Also there is this paper that discusses how weight_decay must be applied independently from optimizer: &lt;denchmark-link:https://arxiv.org/abs/1711.05101&gt;https://arxiv.org/abs/1711.05101&lt;/denchmark-link&gt;

This was recently implemented in TF: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/17438&gt;tensorflow/tensorflow#17438&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>