<bug id='13752' author='eric-haibin-lin' open_date='2019-01-01T22:14:19Z' closed_time='2019-03-17T06:48:31Z'>
	<summary>Adam, AdaMax and FTML cannot be used with Trainer(update_on_kv=False)</summary>
	<description>
These optimizers adjust the learning rate by the number of optimization steps, which is problematic if  is set and multiple GPUs share the same optimizer object (leading to incorrect count of optimization steps). For example: &lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/optimizer/optimizer.py#L1093&gt;https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/optimizer/optimizer.py#L1093&lt;/denchmark-link&gt;

cc &lt;denchmark-link:https://github.com/szha&gt;@szha&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/sandeep-krishnamurthy&gt;@sandeep-krishnamurthy&lt;/denchmark-link&gt;

	</description>
	<comments>
	</comments>
</bug>