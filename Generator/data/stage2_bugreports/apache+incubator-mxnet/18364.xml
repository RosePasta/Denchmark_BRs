<bug id='18364' author='TriLoo' open_date='2020-05-19T10:56:50Z' closed_time='2020-05-21T02:23:06Z'>
	<summary>mx.gluon.data.Dataset class not support multiprocessing.Pool()</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

I installed mxnet using , followed the tutorial: &lt;denchmark-link:https://mxnet.incubator.apache.org/get_started/ubuntu_setup.html&gt;get started-mx&lt;/denchmark-link&gt;
. However, the program cannot continue execution when using followed code:
if self._num_workers &gt; 0:
            self._worker_pool = multiprocessing.Pool(
                self._num_workers, initializer=_worker_initializer, initargs=[self._dataset])
the code is intercepted from &lt;denchmark-link:https://github.com/dmlc/gluon-cv/blob/3c4150a964c776e4f7da0eb30b55ab05b7554c8d/gluoncv/data/dataloader.py#L273&gt;here - yolov3&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Error Message&lt;/denchmark-h&gt;

No error message, the process is still running but cannot step out the above Pool() function.
&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Steps to reproduce&lt;/denchmark-h&gt;


build the latest mxnet using cmake, with CUDA, MKLDNN, OpenMP enabled
run the train_yolov3.sh from gluon-cv

&lt;denchmark-h:h2&gt;What have you tried to solve it?&lt;/denchmark-h&gt;


the mxnet installed by pip install mxnet-cu100mkl can step out the multiprocessing.Pool() and continue execution
mxnet installed by cmake and pip install --user -e ., then  cannot step out.
when installed using cmake, the Dataloader can work through with num_workers&gt;0, but its derived class cannot.

&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;----------Python Info----------
Version      : 3.6.8
Compiler     : GCC 7.3.0
Build        : ('default', 'Dec 30 2018 01:22:34')
Arch         : ('64bit', '')
------------Pip Info-----------
Version      : 19.1.1
Directory    : /search/odin/songminghui/anaconda3/lib/python3.6/site-packages/pip
----------MXNet Info-----------
Version      : 2.0.0
Directory    : /search/odin/songminghui/githubs/incubator-mxnet/python/mxnet
Num GPUs     : 8
Hashtag not found. Not installed from pre-built package.
----------System Info----------
Platform     : Linux-3.10.0-327.el7.x86_64-x86_64-with-centos-7.2.1511-Core
system       : Linux
node         : nmyjs_176_61
release      : 3.10.0-327.el7.x86_64
version      : #1 SMP Thu Nov 19 22:10:57 UTC 2015
----------Hardware Info----------
machine      : x86_64
processor    : x86_64
Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                48
On-line CPU(s) list:   0-47
Thread(s) per core:    2
Core(s) per socket:    12
Socket(s):             2
NUMA node(s):          2
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 79
Model name:            Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GHz
Stepping:              1
CPU MHz:               2195.104
BogoMIPS:              4398.47
Virtualization:        VT-x
L1d cache:             32K
L1i cache:             32K
L2 cache:              256K
L3 cache:              30720K
NUMA node0 CPU(s):     0-11,24-35
NUMA node1 CPU(s):     12-23,36-47
----------Network Test----------
Setting timeout: 10
Error open MXNet: https://github.com/apache/incubator-mxnet, &lt;urlopen error timed out&gt;, DNS finished in 1.2967002391815186 sec.
Error open GluonNLP GitHub: https://github.com/dmlc/gluon-nlp, &lt;urlopen error timed out&gt;, DNS finished in 4.1484832763671875e-05 sec.
Timing for GluonNLP: http://gluon-nlp.mxnet.io, DNS: 1.1775 sec, LOAD: 3.5363 sec.
Timing for D2L: http://d2l.ai, DNS: 0.4321 sec, LOAD: 0.8394 sec.
Timing for D2L (zh-cn): http://zh.d2l.ai, DNS: 0.3948 sec, LOAD: 1.0763 sec.
Timing for FashionMNIST: https://repo.mxnet.io/gluon/dataset/fashion-mnist/train-labels-idx1-ubyte.gz, DNS: 0.4420 sec, LOAD: 2.9380 sec.
Timing for PYPI: https://pypi.python.org/pypi/pip, DNS: 0.1293 sec, LOAD: 13.5263 sec.
Error open Conda: https://repo.continuum.io/pkgs/free/, HTTP Error 403: Forbidden, DNS finished in 0.47435927391052246 sec.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='TriLoo' date='2020-05-19T20:50:03Z'>
		Should this bug be in the gluon-cv issue tracker?
cc &lt;denchmark-link:https://github.com/zhreshold&gt;@zhreshold&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='TriLoo' date='2020-05-20T02:26:00Z'>
		&lt;denchmark-link:https://github.com/leezu&gt;@leezu&lt;/denchmark-link&gt;

I tried  and found no problem running the mentioned ,. However, a build error happened when the  is enabled in , the error info is:
&lt;denchmark-code&gt;In file included from cpp-package/include/mxnet-cpp/optimizer.hpp:37:0,
                 from cpp-package/include/mxnet-cpp/MxNetCpp.h:35,
                 from cpp-package/example/mlp.cpp:26:
cpp-package/include/mxnet-cpp/op.h:3511:22: error: 'begin' has not been declared
                      begin,
                      ^~~~~
cpp-package/include/mxnet-cpp/op.h:3512:22: error: 'end' has not been declared
                      end,
                      ^~~
cpp-package/include/mxnet-cpp/op.h:3513:22: error: 'step' has not been declared
                      step = Shape()) {
                      ^~~~
cpp-package/include/mxnet-cpp/op.h:3513:29: error: could not convert 'mxnet::cpp::Shape()' from 'mxnet::cpp::Shape' to 'int'
                      step = Shape()) {
                             ^~~~~~~
cpp-package/include/mxnet-cpp/op.h: In function 'mxnet::cpp::Symbol mxnet::cpp::slice(const string&amp;, mxnet::cpp::Symbol, int, int, int)':
cpp-package/include/mxnet-cpp/op.h:3515:31: error: 'begin' was not declared in this scope
            .SetParam("begin", begin)
...
&lt;/denchmark-code&gt;

I'm going to check the flag difference between config.mk &amp; Makefile and CMakeLists.txt, or instead debug the begin, end not declared error.
		</comment>
		<comment id='3' author='TriLoo' date='2020-05-20T03:20:06Z'>
		You can also try delete the 3rdparty/openmp and use the cmake build.
		</comment>
		<comment id='4' author='TriLoo' date='2020-05-20T03:23:44Z'>
		I will try this later ~
		</comment>
		<comment id='5' author='TriLoo' date='2020-05-20T05:00:09Z'>
		I used system openmp instead of , the problem still exists. when running into , the CPU is 100% used but always cannot continue execution &lt;denchmark-link:https://github.com/leezu&gt;@leezu&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='TriLoo' date='2020-05-20T05:06:49Z'>
		Thanks for confirming this. You mentioned before that the bug does not exist when compiling with Makefile. Are you certain about that? If so, can you provide a minimal reproducible example?
		</comment>
		<comment id='7' author='TriLoo' date='2020-05-20T05:15:49Z'>
		Sure, when I compile it using Makefile, this bug doesn't exist, but the USE_INT64_TENSOR_SIZE =1 would raise the begin,end not declared error, i found the types of these variables in corresponding function declaration is indeed missed.
According to the CMakeLists.txt, the system openmp is used when the  is used as the BLAS library by default, here: &lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/b904d4838f6bd6a29171389c6e213ca03ec772b9/CMakeLists.txt#L404&gt;openmp-mkl enabled&lt;/denchmark-link&gt;

Steps to reproduce: I downloaded the latest source code from master branch, compiled it with cmake 3.16.6 and g++ 7.3.1, and latest gluon-cv package, then training the YOLOv3 with random input shape enabled would cause this bug. I think I can share my CMakeLists.txt to you if you needed it. Any other info I can provide, let me know ~
		</comment>
		<comment id='8' author='TriLoo' date='2020-05-20T05:37:11Z'>
		I found that the shared lib  generate by  did not depend on , but depend on  only.  meanwhile, the shared lib generate by  depend on both  and , but not depend on  althrough I set  in . I am not sure if this caused the  difference. &lt;denchmark-link:https://github.com/leezu&gt;@leezu&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='TriLoo' date='2020-05-20T05:48:12Z'>
		I suspect you're looking at transitive dependencies instead of the actual dependencies of libmxnet. Please use readelf -d libmxnet.so | grep NEEDED to check the direct dependencies.
For cmake, if you like to try without mkl, be sure to call cmake with cmake -DUSE_MKL_IF_AVAILABLE=0
		</comment>
		<comment id='10' author='TriLoo' date='2020-05-20T05:55:49Z'>
		The readelf -d libmxnet.so | grep NEEDED outputs:
CMake:
&lt;denchmark-code&gt; 0x0000000000000001 (NEEDED)             Shared library: [libdl.so.2]
 0x0000000000000001 (NEEDED)             Shared library: [libpthread.so.0]
 0x0000000000000001 (NEEDED)             Shared library: [libmkl_rt.so]
 0x0000000000000001 (NEEDED)             Shared library: [librt.so.1]
 0x0000000000000001 (NEEDED)             Shared library: [libopencv_highgui.so.3.4]
 0x0000000000000001 (NEEDED)             Shared library: [libopencv_videoio.so.3.4]
 0x0000000000000001 (NEEDED)             Shared library: [libopencv_imgcodecs.so.3.4]
 0x0000000000000001 (NEEDED)             Shared library: [libopencv_imgproc.so.3.4]
 0x0000000000000001 (NEEDED)             Shared library: [libopencv_core.so.3.4]
 0x0000000000000001 (NEEDED)             Shared library: [liblapack.so.3]
 0x0000000000000001 (NEEDED)             Shared library: [libcudnn.so.7]
 0x0000000000000001 (NEEDED)             Shared library: [libcudart.so.10.0]
 0x0000000000000001 (NEEDED)             Shared library: [libcufft.so.10.0]
 0x0000000000000001 (NEEDED)             Shared library: [libcublas.so.10.0]
 0x0000000000000001 (NEEDED)             Shared library: [libcusolver.so.10.0]
 0x0000000000000001 (NEEDED)             Shared library: [libcusparse.so.10.0]
 0x0000000000000001 (NEEDED)             Shared library: [libcurand.so.10.0]
 0x0000000000000001 (NEEDED)             Shared library: [libnvrtc.so.10.0]
 0x0000000000000001 (NEEDED)             Shared library: [libcuda.so.1]
 0x0000000000000001 (NEEDED)             Shared library: [libnvidia-ml.so.1]
 0x0000000000000001 (NEEDED)             Shared library: [libnvToolsExt.so.1]
 0x0000000000000001 (NEEDED)             Shared library: [libgomp.so.1]
 0x0000000000000001 (NEEDED)             Shared library: [libstdc++.so.6]
 0x0000000000000001 (NEEDED)             Shared library: [libm.so.6]
 0x0000000000000001 (NEEDED)             Shared library: [libgcc_s.so.1]
 0x0000000000000001 (NEEDED)             Shared library: [libc.so.6]
 0x0000000000000001 (NEEDED)             Shared library: [ld-linux-x86-64.so.2]
&lt;/denchmark-code&gt;

Makefile
&lt;denchmark-code&gt;0x0000000000000001 (NEEDED)              Shared library: [libdl.so.2]
 0x0000000000000001 (NEEDED)             Shared library: [libcudart.so.10.0]
 0x0000000000000001 (NEEDED)             Shared library: [libcublas.so.10.0]
 0x0000000000000001 (NEEDED)             Shared library: [libcurand.so.10.0]
 0x0000000000000001 (NEEDED)             Shared library: [libcusolver.so.10.0]
 0x0000000000000001 (NEEDED)             ==Shared library: [libiomp5.so]==
 0x0000000000000001 (NEEDED)             Shared library: [libpthread.so.0]
 0x0000000000000001 (NEEDED)             Shared library: [libm.so.6]
 0x0000000000000001 (NEEDED)             Shared library: [librt.so.1]
 0x0000000000000001 (NEEDED)             Shared library: [libopencv_imgcodecs.so.3.4]
 0x0000000000000001 (NEEDED)             Shared library: [libopencv_highgui.so.3.4]
 0x0000000000000001 (NEEDED)             Shared library: [libopencv_imgproc.so.3.4]
 0x0000000000000001 (NEEDED)             Shared library: [libopencv_core.so.3.4]
 0x0000000000000001 (NEEDED)             Shared library: [liblapack.so.3]
 0x0000000000000001 (NEEDED)             Shared library: [libcudnn.so.7]
 0x0000000000000001 (NEEDED)             Shared library: [libcufft.so.10.0]
 0x0000000000000001 (NEEDED)             Shared library: [libcuda.so.1]
 0x0000000000000001 (NEEDED)             Shared library: [libnvrtc.so.10.0]
 0x0000000000000001 (NEEDED)             Shared library: [libstdc++.so.6]
 0x0000000000000001 (NEEDED)             Shared library: [libgomp.so.1]
 0x0000000000000001 (NEEDED)             Shared library: [libgcc_s.so.1]
 0x0000000000000001 (NEEDED)             Shared library: [libc.so.6]
 0x0000000000000001 (NEEDED)             Shared library: [ld-linux-x86-64.so.2]
&lt;/denchmark-code&gt;

Actually, I want to use MKL  but looks like the Makefile did not work. I am going to enable libiomp5.so to check out if it is the source of problem.
		</comment>
		<comment id='11' author='TriLoo' date='2020-05-20T20:06:30Z'>
		&lt;denchmark-link:https://github.com/TriLoo&gt;@TriLoo&lt;/denchmark-link&gt;
 I found myself that the issue is not reproducible as I am able to pass throught the pool creation code. However, due to latest weakref change(&lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/18328&gt;#18328&lt;/denchmark-link&gt;
).
I build from source using cmake, and from the attached log you will notice that the training already starts without stucking
&lt;denchmark-code&gt;[23:45:38] ../src/base.cc:84: Upgrade advisory: this mxnet has been built against cuDNN lib version 7501, which is older than the oldest version tested by CI (7600).  Set MXNET_CUDNN_LIB_CHECKING=0 to quiet this warning.
learning rate from ``lr_scheduler`` has been overwritten by ``learning_rate`` in optimizer.
INFO:root:Namespace(amp=False, batch_size=64, data_shape=416, dataset='voc', epochs=200, gpus='0,1,2,3,4,5,6,7', horovod=False, label_smooth=False, log_interval=100, lr=0.001, lr_decay=0.1, lr_decay_epoch='160,180', lr_decay_period=0, lr_mode='step', mixup=False, momentum=0.9, network='darknet53', no_mixup_epochs=20, no_random_shape=False, no_wd=False, num_samples=16551, num_workers=16, resume='', save_interval=10, save_prefix='yolo3_darknet53_voc', seed=233, start_epoch=0, syncbn=True, val_interval=1, warmup_epochs=4, warmup_lr=0.0, wd=0.0005)
INFO:root:Start training from [Epoch 0]
Traceback (most recent call last):
  File "train_yolo3.py", line 374, in &lt;module&gt;
    train(net, train_data, val_data, eval_metric, ctx, args)
  File "train_yolo3.py", line 270, in train
    for i, batch in enumerate(train_data):
  File "/home/ubuntu/mxnet/python/mxnet/gluon/data/dataloader.py", line 485, in __next__
    batch = pickle.loads(ret.get(self._timeout))
  File "/usr/lib/python3.6/multiprocessing/pool.py", line 644, in get
    raise self._value
  File "/usr/lib/python3.6/multiprocessing/pool.py", line 424, in _handle_tasks
    put(task)
  File "/usr/lib/python3.6/multiprocessing/connection.py", line 206, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "/usr/lib/python3.6/multiprocessing/reduction.py", line 51, in dumps
    cls(buf, protocol).dump(obj)
TypeError: can't pickle weakref objects
&lt;/denchmark-code&gt;

		</comment>
		<comment id='12' author='TriLoo' date='2020-05-21T02:22:54Z'>
		thanks for your reply &lt;denchmark-link:https://github.com/zhreshold&gt;@zhreshold&lt;/denchmark-link&gt;
 .
I tried again and the problem on my machine still exists, through the  is added. The training log does not start  in my case actually.
I will close this issue temporarily and add the details if I find out the cause of the problem on my machine ~
		</comment>
		<comment id='13' author='TriLoo' date='2020-05-22T12:54:49Z'>
		Looks like some modification errors to FindMKL.cmake and the missed libiomp5 caused this error, now it works.
		</comment>
	</comments>
</bug>