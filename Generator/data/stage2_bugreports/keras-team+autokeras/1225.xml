<bug id='1225' author='CharlesDove' open_date='2020-07-07T23:42:37Z' closed_time='2020-09-17T04:05:30Z'>
	<summary>Horovod multinode: 'TrackableWeightHandler' object has no attribute 'assign'</summary>
	<description>
Good afternoon! I'm currently working on implementing multi-node GPU training for this project using Horovod. I'm getting through to the end of the first training batch, where horovod throws the following error:
AttributeError: 'TrackableWeightHandler' object has no attribute 'assign'
Does autokeras define its own version of the TrackableWeightHandler, which horovod might not be expecting?
UPDATE: This appears to be specific to some of autokeras's custom blocks used in the AutoModel interface. My guess would be the input/output blocks. My implementation works correctly on the MNIST example. Any thoughts on how/where to fix this?
Here's the full error dump:
&lt;denchmark-code&gt;
Tue Jul  7 19:21:06 2020[0]&lt;stderr&gt;:2020-07-07 19:21:06.121899: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
Tue Jul  7 19:21:06 2020[0]&lt;stderr&gt;:2020-07-07 19:21:06.145203: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2400000000 Hz
Tue Jul  7 19:21:06 2020[0]&lt;stderr&gt;:2020-07-07 19:21:06.146991: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5628bc9b14f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
Tue Jul  7 19:21:06 2020[0]&lt;stderr&gt;:2020-07-07 19:21:06.147007: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
Tue Jul  7 19:21:06 2020[0]&lt;stderr&gt;:2020-07-07 19:21:06.148341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1564] Found device 0 with properties: 
Tue Jul  7 19:21:06 2020[0]&lt;stderr&gt;:pciBusID: 0000:3b:00.0 name: Tesla V100-PCIE-16GB computeCapability: 7.0
Tue Jul  7 19:21:06 2020[0]&lt;stderr&gt;:coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s
Tue Jul  7 19:21:06 2020[0]&lt;stderr&gt;:2020-07-07 19:21:06.148381: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2
Tue Jul  7 19:21:06 2020[0]&lt;stderr&gt;:2020-07-07 19:21:06.148445: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
Tue Jul  7 19:21:06 2020[0]&lt;stderr&gt;:2020-07-07 19:21:06.148472: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
Tue Jul  7 19:21:06 2020[0]&lt;stderr&gt;:2020-07-07 19:21:06.148480: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
Tue Jul  7 19:21:06 2020[0]&lt;stderr&gt;:2020-07-07 19:21:06.148492: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
Tue Jul  7 19:21:06 2020[0]&lt;stderr&gt;:2020-07-07 19:21:06.148504: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
Tue Jul  7 19:21:06 2020[0]&lt;stderr&gt;:2020-07-07 19:21:06.148515: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Tue Jul  7 19:21:06 2020[0]&lt;stderr&gt;:2020-07-07 19:21:06.149696: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1706] Adding visible gpu devices: 0
Tue Jul  7 19:21:06 2020[0]&lt;stderr&gt;:2020-07-07 19:21:06.149721: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2
Tue Jul  7 19:21:06 2020[0]&lt;stderr&gt;:2020-07-07 19:21:06.638877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Device interconnect StreamExecutor with strength 1 edge matrix:
Tue Jul  7 19:21:06 2020[0]&lt;stderr&gt;:2020-07-07 19:21:06.638904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1111]      0 
Tue Jul  7 19:21:06 2020[0]&lt;stderr&gt;:2020-07-07 19:21:06.638910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1124] 0:   N 
Tue Jul  7 19:21:06 2020[0]&lt;stderr&gt;:2020-07-07 19:21:06.641156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1250] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14765 MB memory) -&gt; physical GPU (device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:3b:00.0, compute capability: 7.0)
Tue Jul  7 19:21:06 2020[0]&lt;stderr&gt;:2020-07-07 19:21:06.643258: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5628ef32f010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
Tue Jul  7 19:21:06 2020[0]&lt;stderr&gt;:2020-07-07 19:21:06.643272: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-16GB, Compute Capability 7.0
Tue Jul  7 19:21:11 2020[0]&lt;stdout&gt;:[Starting new trial]
Tue Jul  7 19:21:21 2020[0]&lt;stdout&gt;:Epoch 1/5
Tue Jul  7 19:21:21 2020[0]&lt;stderr&gt;:2020-07-07 19:21:21.562218: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
Tue Jul  7 19:21:21 2020[0]&lt;stderr&gt;:2020-07-07 19:21:21.851099: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Tue Jul  7 19:21:23 2020[0]&lt;stderr&gt;:Traceback (most recent call last):
Tue Jul  7 19:21:23 2020[0]&lt;stderr&gt;:  File "learn_distance_parallel.py", line 112, in &lt;module&gt;
Tue Jul  7 19:21:23 2020[0]&lt;stderr&gt;:    reg.fit([image_array,data],[output_to_learn],batch_size=1,epochs=5,validation_split=0.2,steps_per_epoch=500//hvd.size(),callbacks=callbacks,verbose=verbose)
Tue Jul  7 19:21:23 2020[0]&lt;stderr&gt;:  File "/home/cadove/.conda/envs/wmlce_env/lib/python3.6/site-packages/autokeras/auto_model.py", line 264, in fit
Tue Jul  7 19:21:23 2020[0]&lt;stderr&gt;:    **kwargs)
Tue Jul  7 19:21:23 2020[0]&lt;stderr&gt;:  File "/home/cadove/.conda/envs/wmlce_env/lib/python3.6/site-packages/autokeras/engine/tuner.py", line 165, in search
Tue Jul  7 19:21:23 2020[0]&lt;stderr&gt;:    super().search(epochs=epochs, callbacks=new_callbacks, **fit_kwargs)
Tue Jul  7 19:21:23 2020[0]&lt;stderr&gt;:  File "/home/cadove/.conda/envs/wmlce_env/lib/python3.6/site-packages/kerastuner/engine/base_tuner.py", line 132, in search
Tue Jul  7 19:21:23 2020[0]&lt;stderr&gt;:    self.run_trial(trial, *fit_args, **fit_kwargs)
Tue Jul  7 19:21:23 2020[0]&lt;stderr&gt;:  File "/home/cadove/.conda/envs/wmlce_env/lib/python3.6/site-packages/kerastuner/engine/tuner.py", line 154, in run_trial
Tue Jul  7 19:21:23 2020[0]&lt;stderr&gt;:    model.fit(*fit_args, **copied_fit_kwargs)
Tue Jul  7 19:21:23 2020[0]&lt;stderr&gt;:  File "/home/cadove/.conda/envs/wmlce_env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py", line 66, in _method_wrapper
Tue Jul  7 19:21:23 2020[0]&lt;stderr&gt;:    return method(self, *args, **kwargs)
Tue Jul  7 19:21:23 2020[0]&lt;stderr&gt;:  File "/home/cadove/.conda/envs/wmlce_env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py", line 855, in fit
Tue Jul  7 19:21:23 2020[0]&lt;stderr&gt;:    callbacks.on_train_batch_end(step, logs)
Tue Jul  7 19:21:23 2020[0]&lt;stderr&gt;:  File "/home/cadove/.conda/envs/wmlce_env/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py", line 390, in on_train_batch_end
Tue Jul  7 19:21:23 2020[0]&lt;stderr&gt;:    self._call_batch_hook(ModeKeys.TRAIN, 'end', batch, logs=logs)
Tue Jul  7 19:21:23 2020[0]&lt;stderr&gt;:  File "/home/cadove/.conda/envs/wmlce_env/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py", line 298, in _call_batch_hook
Tue Jul  7 19:21:23 2020[0]&lt;stderr&gt;:    batch_hook(batch, logs)
Tue Jul  7 19:21:23 2020[0]&lt;stderr&gt;:  File "/home/cadove/.conda/envs/wmlce_env/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py", line 615, in on_train_batch_end
Tue Jul  7 19:21:23 2020[0]&lt;stderr&gt;:    self.on_batch_end(batch, logs=logs)
Tue Jul  7 19:21:23 2020[0]&lt;stderr&gt;:  File "/home/cadove/.conda/envs/wmlce_env/lib/python3.6/site-packages/horovod/_keras/callbacks.py", line 38, in on_batch_end
Tue Jul  7 19:21:23 2020[0]&lt;stderr&gt;:    root_rank=self.root_rank)
Tue Jul  7 19:21:23 2020[0]&lt;stderr&gt;:  File "/home/cadove/.conda/envs/wmlce_env/lib/python3.6/site-packages/horovod/tensorflow/__init__.py", line 159, in broadcast_variables
Tue Jul  7 19:21:23 2020[0]&lt;stderr&gt;:    return broadcast_group(variables, root_rank)
Tue Jul  7 19:21:23 2020[0]&lt;stderr&gt;:  File "/home/cadove/.conda/envs/wmlce_env/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py", line 564, in __call__
Tue Jul  7 19:21:23 2020[0]&lt;stderr&gt;:    return self._python_function(*args, **kwds)
Tue Jul  7 19:21:23 2020[0]&lt;stderr&gt;:  File "/home/cadove/.conda/envs/wmlce_env/lib/python3.6/site-packages/horovod/tensorflow/__init__.py", line 138, in broadcast_group
Tue Jul  7 19:21:23 2020[0]&lt;stderr&gt;:    var.assign(broadcast(var, root_rank))
Tue Jul  7 19:21:23 2020[0]&lt;stderr&gt;:AttributeError: 'TrackableWeightHandler' object has no attribute 'assign'
Process 0 exit with status code 1.
Traceback (most recent call last):
  File "/home/cadove/.conda/envs/wmlce_env/bin/horovodrun", line 21, in &lt;module&gt;
    run_commandline()
  File "/home/cadove/.conda/envs/wmlce_env/lib/python3.6/site-packages/horovod/run/runner.py", line 723, in run_commandline
    _run(args)
  File "/home/cadove/.conda/envs/wmlce_env/lib/python3.6/site-packages/horovod/run/runner.py", line 656, in _run
    _launch_job(args, remote_host_names, settings, nics, command)
  File "/home/cadove/.conda/envs/wmlce_env/lib/python3.6/site-packages/horovod/run/runner.py", line 717, in _launch_job
    args.verbose)
  File "/home/cadove/.conda/envs/wmlce_env/lib/python3.6/site-packages/horovod/run/runner.py", line 673, in run_controller
    gloo_run()
  File "/home/cadove/.conda/envs/wmlce_env/lib/python3.6/site-packages/horovod/run/runner.py", line 706, in gloo_run_fn
    gloo_run(settings, remote_host_names, nics, env, driver_ip, command)
  File "/home/cadove/.conda/envs/wmlce_env/lib/python3.6/site-packages/horovod/run/gloo_run.py", line 312, in gloo_run
    launch_gloo(command, exec_command, settings, nics, env, server_ip)
  File "/home/cadove/.conda/envs/wmlce_env/lib/python3.6/site-packages/horovod/run/gloo_run.py", line 304, in launch_gloo
    .format(name=name, code=exit_code))
RuntimeError: Gloo job detected that one or more processes exited with non-zero status, thus causing the job to be terminated. The first process to do so was:
Process name: 0
Exit code: 1

&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Setup Details&lt;/denchmark-h&gt;

Include the details about the versions of:

OS type and version: linux
Python: 3.6
autokeras: 1.0.3
tensorflow: 2.2

	</description>
	<comments>
		<comment id='1' author='CharlesDove' date='2020-09-10T02:21:20Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>