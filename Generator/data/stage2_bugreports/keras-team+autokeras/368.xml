<bug id='368' author='abagmut' open_date='2018-12-08T13:18:05Z' closed_time='2019-02-18T08:58:08Z'>
	<summary>Got 'NotImplementedError' on macOS</summary>
	<description>
&lt;denchmark-h:h3&gt;Bug Description&lt;/denchmark-h&gt;

Traceback (most recent call last): File "test.py", line 29, in &lt;module&gt; clf.fit(x_train, y_train, time_limit=60 * 60) File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/autokeras/image/image_supervised.py", line 114, in fit super().fit(x, y, time_limit) File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/autokeras/supervised.py", line 129, in fit self.cnn.fit(self.get_n_output_node(), x_train.shape, train_data, test_data, time_limit) File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/autokeras/net_module.py", line 65, in fit self.searcher.search(train_data, test_data, int(time_remain)) File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/autokeras/search.py", line 200, in search generated_other_info, generated_graph = self.generate(remaining_time, q) File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/autokeras/search.py", line 251, in generate remaining_time, multiprocessing_queue) File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/autokeras/bayesian.py", line 350, in generate if multiprocessing_queue.qsize() != 0: File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/queues.py", line 117, in qsize return self._maxsize - self._sem._semlock._get_value() NotImplementedError
&lt;denchmark-h:h3&gt;Reproducing Steps&lt;/denchmark-h&gt;

Just run 'Data with numpy array (.npy) format.' example
&lt;denchmark-h:h3&gt;Setup Details&lt;/denchmark-h&gt;

Include the details about the versions of:

OS type and version: macOS 10.14.2
Python: 3.6

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

seems this is causing the issue: &lt;denchmark-link:https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Queue.qsize&gt;https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Queue.qsize&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='abagmut' date='2019-01-09T11:50:03Z'>
		I have similar errors.
+----------------------------------------------+ |               Training model 1               | +----------------------------------------------+ /Users/liuuuche/.virtualenv3/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.1) compiler. This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore. Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler. You can install the OpenMP library by the following command: ``brew install libomp``. "You can install the OpenMP library by the following command: ``brew install libomp``.", UserWarning) Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "/Users/liuuuche/.virtualenv3/lib/python3.6/site-packages/autokeras/image/image_supervised.py", line 114, in fit super().fit(x, y, time_limit) File "/Users/liuuuche/.virtualenv3/lib/python3.6/site-packages/autokeras/supervised.py", line 129, in fit self.cnn.fit(self.get_n_output_node(), x_train.shape, train_data, test_data, time_limit) File "/Users/liuuuche/.virtualenv3/lib/python3.6/site-packages/autokeras/net_module.py", line 65, in fit self.searcher.search(train_data, test_data, int(time_remain)) File "/Users/liuuuche/.virtualenv3/lib/python3.6/site-packages/autokeras/search.py", line 200, in search generated_other_info, generated_graph = self.generate(remaining_time, q) File "/Users/liuuuche/.virtualenv3/lib/python3.6/site-packages/autokeras/search.py", line 251, in generate remaining_time, multiprocessing_queue) File "/Users/liuuuche/.virtualenv3/lib/python3.6/site-packages/autokeras/bayesian.py", line 350, in generate if multiprocessing_queue.qsize() != 0: File "/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/queues.py", line 117, in qsize return self._maxsize - self._sem._semlock._get_value() NotImplementedError
		</comment>
		<comment id='2' author='abagmut' date='2019-01-21T18:13:44Z'>
		Try brew install libomp.
Install the latest version of Auto-Keras.
If it still doesn't work, please reopen the issue.
		</comment>
		<comment id='3' author='abagmut' date='2019-02-07T16:59:56Z'>
		Same error.
It seems to be an issue with multiprocessing and macOS.





&lt;denchmark-link:https://user-images.githubusercontent.com/2808425/52428381-1d1ed680-2b02-11e9-9076-5fb0bd4e03f2.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='abagmut' date='2019-02-07T20:35:13Z'>
		&lt;denchmark-link:https://github.com/boyuangong&gt;@boyuangong&lt;/denchmark-link&gt;
 can you take look into this issue? Thanks
Sent with GitHawk
		</comment>
		<comment id='5' author='abagmut' date='2019-02-07T22:52:14Z'>
		Since this is an old problem, I've found a posible solution that other project (lemon) has taken: to subclass the Queue class in order to make it portable. This is the code (comes from &lt;denchmark-link:https://github.com/vterron/lemon/commit/9ca6b4b1212228dbd4f69b88aaf88b12952d7d6f&gt;vterron/lemon@9ca6b4b&lt;/denchmark-link&gt;
):
&lt;denchmark-code&gt;class SharedCounter(object):
    """ A synchronized shared counter.

    The locking done by multiprocessing.Value ensures that only a single
    process or thread may read or write the in-memory ctypes object. However,
    in order to do n += 1, Python performs a read followed by a write, so a
    second process may read the old value before the new one is written by the
    first process. The solution is to use a multiprocessing.Lock to guarantee
    the atomicity of the modifications to Value.

    This class comes almost entirely from Eli Bendersky's blog:
    http://eli.thegreenplace.net/2012/01/04/shared-counter-with-pythons-multiprocessing/

    """

    def __init__(self, n = 0):
        self.count = multiprocessing.Value('i', n)

    def increment(self, n = 1):
        """ Increment the counter by n (default = 1) """
        with self.count.get_lock():
            self.count.value += n

    @property
    def value(self):
        """ Return the value of the counter """
        return self.count.value


class Queue(multiprocessing.queues.Queue):
    """ A portable implementation of multiprocessing.Queue.

    Because of multithreading / multiprocessing semantics, Queue.qsize() may
    raise the NotImplementedError exception on Unix platforms like Mac OS X
    where sem_getvalue() is not implemented. This subclass addresses this
    problem by using a synchronized shared counter (initialized to zero) and
    increasing / decreasing its value every time the put() and get() methods
    are called, respectively. This not only prevents NotImplementedError from
    being raised, but also allows us to implement a reliable version of both
    qsize() and empty().

    """

    def __init__(self, *args, **kwargs):
        super(Queue, self).__init__(*args, **kwargs)
        self.size = SharedCounter(0)

    def put(self, *args, **kwargs):
        self.size.increment(1)
        super(Queue, self).put(*args, **kwargs)

    def get(self, *args, **kwargs):
        self.size.increment(-1)
        return super(Queue, self).get(*args, **kwargs)

    def qsize(self):
        """ Reliable implementation of multiprocessing.Queue.qsize() """
        return self.size.value

    def empty(self):
        """ Reliable implementation of multiprocessing.Queue.empty() """
        return not self.qsize()
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='abagmut' date='2019-02-12T05:51:45Z'>
		
Same error.
It seems to be an issue with multiprocessing and macOS.
multiprocessing/queues.py in qsize(self)
def qsize(self):
 # Raises NotImplementedError on Mac OSX because of broken sem_getvalue()
--&gt; self._maxsize - self._sem._semlock._get_value()
def empty(self):


I'm having same problem
		</comment>
		<comment id='7' author='abagmut' date='2019-02-12T10:35:17Z'>
		
Since this is an old problem, I've found a posible solution that other project (lemon) has taken: to subclass the Queue class in order to make it portable. This is the code (comes from vterron/lemon@9ca6b4b):
class SharedCounter(object):
    """ A synchronized shared counter.

    The locking done by multiprocessing.Value ensures that only a single
    process or thread may read or write the in-memory ctypes object. However,
    in order to do n += 1, Python performs a read followed by a write, so a
    second process may read the old value before the new one is written by the
    first process. The solution is to use a multiprocessing.Lock to guarantee
    the atomicity of the modifications to Value.

    This class comes almost entirely from Eli Bendersky's blog:
    http://eli.thegreenplace.net/2012/01/04/shared-counter-with-pythons-multiprocessing/

    """

    def __init__(self, n = 0):
        self.count = multiprocessing.Value('i', n)

    def increment(self, n = 1):
        """ Increment the counter by n (default = 1) """
        with self.count.get_lock():
            self.count.value += n

    @property
    def value(self):
        """ Return the value of the counter """
        return self.count.value


class Queue(multiprocessing.queues.Queue):
    """ A portable implementation of multiprocessing.Queue.

    Because of multithreading / multiprocessing semantics, Queue.qsize() may
    raise the NotImplementedError exception on Unix platforms like Mac OS X
    where sem_getvalue() is not implemented. This subclass addresses this
    problem by using a synchronized shared counter (initialized to zero) and
    increasing / decreasing its value every time the put() and get() methods
    are called, respectively. This not only prevents NotImplementedError from
    being raised, but also allows us to implement a reliable version of both
    qsize() and empty().

    """

    def __init__(self, *args, **kwargs):
        super(Queue, self).__init__(*args, **kwargs)
        self.size = SharedCounter(0)

    def put(self, *args, **kwargs):
        self.size.increment(1)
        super(Queue, self).put(*args, **kwargs)

    def get(self, *args, **kwargs):
        self.size.increment(-1)
        return super(Queue, self).get(*args, **kwargs)

    def qsize(self):
        """ Reliable implementation of multiprocessing.Queue.qsize() """
        return self.size.value

    def empty(self):
        """ Reliable implementation of multiprocessing.Queue.empty() """
        return not self.qsize()


This worked for me...
Thanks!
		</comment>
		<comment id='8' author='abagmut' date='2019-02-12T12:11:12Z'>
		Glad it helped!
Did you modified autokeras or subclassed the Queue class in any other way? Maybe the best option would be to PR autokeras.
		</comment>
		<comment id='9' author='abagmut' date='2019-02-12T12:31:25Z'>
		I did modify autokeras and just used the Queue class mentioned above instead of the multiprocessing.queues.Queue class.
		</comment>
		<comment id='10' author='abagmut' date='2019-02-12T12:38:06Z'>
		Would you kindly make a Pull Request?
		</comment>
		<comment id='11' author='abagmut' date='2019-02-12T22:19:03Z'>
		Can anyone help me with implementation of this fix? not really sure what to do
		</comment>
		<comment id='12' author='abagmut' date='2019-02-14T13:00:13Z'>
		Hi Sharif,
Trying to run setup.py of your package and its not installing properly the .egg
python doesn't recognize the autokeras or the custom_queue class
I am working on python 3.6 virt_env
Thank you
Keren
		</comment>
		<comment id='13' author='abagmut' date='2019-02-14T15:23:20Z'>
		Hey,
just clone my branch and reference autokeras locally from it. That worked for me....
		</comment>
		<comment id='14' author='abagmut' date='2019-02-14T22:37:38Z'>
		Hi Sharif, Im getting an error on the line
from . import connection
in the custom_queue.py file
		</comment>
		<comment id='15' author='abagmut' date='2019-02-15T10:31:01Z'>
		Hey ai-high,
yeah I fixed that. Just go and clone my forked repo (&lt;denchmark-link:https://github.com/SharifElfouly/autokeras&gt;https://github.com/SharifElfouly/autokeras&lt;/denchmark-link&gt;
). That should work now...
		</comment>
		<comment id='16' author='abagmut' date='2019-10-18T04:10:28Z'>
		class Queue(multiprocessing.Queues.Queue):
AttributeError: module 'multiprocessing' has no attribute 'Queues'
I am on py3
		</comment>
		<comment id='17' author='abagmut' date='2019-10-30T06:53:26Z'>
		same error,  py3
		</comment>
		<comment id='18' author='abagmut' date='2019-12-03T22:51:25Z'>
		&lt;denchmark-link:https://github.com/nanshihui&gt;@nanshihui&lt;/denchmark-link&gt;
 It is , not . Just tried on Python3.7 and Python2.7.
EDIT: snap NO!!! It works if you try to import it in the Python shell, but referencing it directly like the code given doesn't work.
EDIT: do this (Python2)
&lt;denchmark-code&gt;import multiprocessing
from multiprocessing.queues import Queue as mp_queue

class Queue(mp_queue):
  # ...
&lt;/denchmark-code&gt;

EDIT: in Python3, this code has a problem:
&lt;denchmark-code&gt;  File "...[redacted].../shared_queue.py", line 53, in __init__
    super(Queue, self).__init__(*args, **kwargs)
TypeError: __init__() missing 1 required keyword-only argument: 'ctx'
&lt;/denchmark-code&gt;

Solution: &lt;denchmark-link:https://stackoverflow.com/questions/24941359/ctx-parameter-in-multiprocessing-queue&gt;https://stackoverflow.com/questions/24941359/ctx-parameter-in-multiprocessing-queue&lt;/denchmark-link&gt;

		</comment>
		<comment id='19' author='abagmut' date='2020-01-20T07:36:23Z'>
		&lt;denchmark-link:https://github.com/Leedehai&gt;@Leedehai&lt;/denchmark-link&gt;
 thank you so much!!!
		</comment>
	</comments>
</bug>