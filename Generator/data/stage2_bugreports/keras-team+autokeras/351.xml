<bug id='351' author='Npccc' open_date='2018-11-29T07:23:31Z' closed_time='2019-08-19T03:32:05Z'>
	<summary>RuntimeError: Given groups=1, weight of size [256, 384, 1, 1], expected input[20, 512, 32, 32] to have 384 channels, but got 512 channels instead</summary>
	<description>
&lt;denchmark-h:h3&gt;Bug Description&lt;/denchmark-h&gt;

It will report an error halfway through.
+----------------------------------------------+
|              Training model 54               |
+----------------------------------------------+
Using TensorFlow backend.
Epoch-1, Current Metric - 0:   0%|                                       | 0/61 [00:00&lt;?, ? batch/s]Process SpawnProcess-55:
Traceback (most recent call last):
File "/home/guost/anaconda3/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
self.run()
File "/home/guost/anaconda3/lib/python3.6/multiprocessing/process.py", line 93, in run
self._target(*self._args, **self._kwargs)
File "/home/guost/autokeras/Newversion/034/autokeras/autokeras/search.py", line 297, in train
verbose=verbose).train_model(**trainer_args)
File "/home/guost/autokeras/Newversion/034/autokeras/autokeras/nn/model_trainer.py", line 122, in train_model
self._train()
File "/home/guost/autokeras/Newversion/034/autokeras/autokeras/nn/model_trainer.py", line 156, in _train
outputs = self.model(inputs)
File "/home/guost/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in call
result = self.forward(*input, **kwargs)
File "/home/guost/autokeras/Newversion/034/autokeras/autokeras/nn/graph.py", line 677, in forward
temp_tensor = torch_layer(edge_input_tensor)
File "/home/guost/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in call
result = self.forward(*input, **kwargs)
File "/home/guost/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 301, in forward
self.padding, self.dilation, self.groups)
RuntimeError: Given groups=1, weight of size [256, 384, 1, 1], expected input[20, 512, 32, 32] to have 384 channels, but got 512 channels instead
&lt;denchmark-h:h3&gt;My code：&lt;/denchmark-h&gt;

if name == 'main':
train_data, train_labels = load_image_dataset(csv_file_path=TRAIN_CSV_DIR, images_path=TRAIN_IMG_DIR)
test_data, test_labels = load_image_dataset(csv_file_path=TEST_CSV_DIR, images_path=TEST_IMG_DIR)
train_data = train_data.astype('float32') / 255.
test_data = test_data.astype('float32') / 255.
clf = ImageClassifier(verbose=True,path='/home/guost/autokeras/Newversion/034/modelPath',resume=False)
clf.fit(train_data, train_labels, time_limit = 60 * 60)
clf.final_fit(train_data, train_labels, test_data, test_labels, retrain=True)
y = clf.evaluate(test_data, test_labels)
print("evaluate:", y)
&lt;denchmark-h:h3&gt;My enviroment：&lt;/denchmark-h&gt;

autokeras：'0.3.4'
python：3.6.6
torch:0.4.1
keras:2.2.4
tensorflow:1.12.0
system:Ubuntu16.04
numpy:1.15.2
	</description>
	<comments>
		<comment id='1' author='Npccc' date='2018-11-29T17:14:08Z'>
		&lt;denchmark-link:https://github.com/Npccc&gt;@Npccc&lt;/denchmark-link&gt;
 Thank you so much for the bug report.
It is very hard for us to reproduce this bug without the full log.
Would you please provide the full log if it is available?
Thank you!
		</comment>
		<comment id='2' author='Npccc' date='2018-12-01T03:35:48Z'>
		i get same exception (ver 0.3.4 , ImageClassifier)
+----------------------------------------------+
|              Training model 22               |
+----------------------------------------------+
Epoch-1, Current Metric - 0:   0%|                                        | 0/4 [00:00&lt;?, ? batch/s]Process ForkProcess-25:
Traceback (most recent call last):
File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
self.run()
File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
self._target(*self._args, **self._kwargs)
File "/usr/local/lib/python3.6/dist-packages/autokeras/search.py", line 277, in train
raise e
File "/usr/local/lib/python3.6/dist-packages/autokeras/search.py", line 270, in train
verbose=verbose).train_model(**trainer_args)
File "/usr/local/lib/python3.6/dist-packages/autokeras/nn/model_trainer.py", line 122, in train_model
self._train()
File "/usr/local/lib/python3.6/dist-packages/autokeras/nn/model_trainer.py", line 156, in _train
outputs = self.model(inputs)
File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 477, in call
result = self.forward(*input, **kwargs)
File "/usr/local/lib/python3.6/dist-packages/autokeras/nn/graph.py", line 681, in forward
temp_tensor = torch_layer(edge_input_tensor)
File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 477, in call
result = self.forward(*input, **kwargs)
File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py", line 301, in forward
self.padding, self.dilation, self.groups)
RuntimeError: Given groups=1, weight of size [64, 192, 1, 1], expected input[128, 256, 32, 32] to have 192 channels, but got 256 channels instead
		</comment>
		<comment id='3' author='Npccc' date='2018-12-01T03:39:50Z'>
		&lt;denchmark-link:https://github.com/codecrack3&gt;@codecrack3&lt;/denchmark-link&gt;
 Thank you for the bug report. I think we need the entire log to reproduce the error.
		</comment>
		<comment id='4' author='Npccc' date='2019-01-07T14:06:31Z'>
		I'm experiencing the same issue with version 0.3.5. I've set MAX_IMAGE_SIZE to 224 * 224.
It already occurred two times with identical settings:
Using TensorFlow backend.
{'dataPath': '/Data/', 'imageBaseDir': '/Datasets/224x224/', 'trainDataFile': 'train.csv', 'valDataFile': 'val.csv', 'testDataFile': 'test.csv', 'augmentData': False, 'timeLimit': 24.0, 'max_iter_num': 100, 'max_iter_num_eval': 50, 'max_no_improvement_num': 20, 'modelPath': '224x224', 'modelName': 'model'}
Loading data...
Training data shape:  (44830, 224, 224, 3)
Validation data shape:  (4760, 224, 224, 3)
Test data shape:  (5510, 224, 224, 3)
Start architecture search...
Saving Directory: /tmp/autokeras_4G7EAB
Preprocessing the images.
Preprocessing finished.

Initializing search.
Initialization finished.

+----------------------------------------------+
|               Training model 0               |
+----------------------------------------------+

.
.
.
.
.
.

Epoch-43, Current Metric - 0.212:   0%|                                   | 0/4 [00:00&lt;?, ? batch/s]
Epoch-43, Current Metric - 0.212: 10 batch [00:00, 91.13 batch/s]                                   
                                                                 
No loss decrease after 20 epochs.


Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           26           |   11.985831153392791   |  0.20940000000000003   |
+--------------------------------------------------------------------------+


+----------------------------------------------+
|              Training model 27               |
+----------------------------------------------+

Epoch-1, Current Metric - 0:   0%|                                      | 0/347 [00:00&lt;?, ? batch/s]
                                                                                                    
Process ForkProcess-28:
Traceback (most recent call last):
  File "/home/lucieri/anaconda3/envs/project/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/home/lucieri/anaconda3/envs/project/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "/home/lucieri/anaconda3/envs/project/lib/python3.6/site-packages/autokeras/search.py", line 473, in train
    raise e
  File "/home/lucieri/anaconda3/envs/project/lib/python3.6/site-packages/autokeras/search.py", line 466, in train
    verbose=verbose).train_model(**trainer_args)
  File "/home/lucieri/anaconda3/envs/project/lib/python3.6/site-packages/autokeras/nn/model_trainer.py", line 137, in train_model
    self._train()
  File "/home/lucieri/anaconda3/envs/project/lib/python3.6/site-packages/autokeras/nn/model_trainer.py", line 173, in _train
    outputs = self.model(inputs)
  File "/home/lucieri/anaconda3/envs/project/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/lucieri/anaconda3/envs/project/lib/python3.6/site-packages/autokeras/nn/graph.py", line 685, in forward
    temp_tensor = torch_layer(edge_input_tensor)
  File "/home/lucieri/anaconda3/envs/project/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/lucieri/anaconda3/envs/project/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 301, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: Given groups=1, weight of size [128, 384, 1, 1], expected input[128, 512, 64, 64] to have 384 channels, but got 512 channels instead
Process ForkProcess-17:
Traceback (most recent call last):
  File "/home/lucieri/anaconda3/envs/project/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/home/lucieri/anaconda3/envs/project/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "/home/lucieri/anaconda3/envs/project/lib/python3.6/site-packages/autokeras/search.py", line 473, in train
    raise e
  File "/home/lucieri/anaconda3/envs/project/lib/python3.6/site-packages/autokeras/search.py", line 466, in train
    verbose=verbose).train_model(**trainer_args)
  File "/home/lucieri/anaconda3/envs/project/lib/python3.6/site-packages/autokeras/nn/model_trainer.py", line 137, in train_model
    self._train()
  File "/home/lucieri/anaconda3/envs/project/lib/python3.6/site-packages/autokeras/nn/model_trainer.py", line 173, in _train
    outputs = self.model(inputs)
  File "/home/lucieri/anaconda3/envs/project/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/lucieri/anaconda3/envs/project/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py", line 123, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/lucieri/anaconda3/envs/project/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py", line 133, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/lucieri/anaconda3/envs/project/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py", line 77, in parallel_apply
    raise output
  File "/home/lucieri/anaconda3/envs/project/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py", line 53, in _worker
    output = module(*input, **kwargs)
  File "/home/lucieri/anaconda3/envs/project/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/lucieri/anaconda3/envs/project/lib/python3.6/site-packages/autokeras/nn/graph.py", line 685, in forward
    temp_tensor = torch_layer(edge_input_tensor)
  File "/home/lucieri/anaconda3/envs/project/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/lucieri/anaconda3/envs/project/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 301, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: Given groups=1, weight of size [1024, 384, 1, 1], expected input[64, 512, 32, 32] to have 384 channels, but got 512 channels instead
		</comment>
		<comment id='5' author='Npccc' date='2019-01-15T00:38:09Z'>
		I am also experiencing this issue.
&lt;denchmark-code&gt;+----------------------------------------------+
|              Training model 163              |
+----------------------------------------------+
Epoch-1, Current Metric - 0:   0%|                                        | 0/6 [00:00&lt;?, ? batch/s]Process SpawnProcess-1:
Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "/home/zane/workspace/prediction/lib/python3.6/site-packages/autokeras/search.py", line 277, in train
    raise e
  File "/home/zane/workspace/prediction/lib/python3.6/site-packages/autokeras/search.py", line 270, in train
    verbose=verbose).train_model(**trainer_args)
  File "/home/zane/workspace/prediction/lib/python3.6/site-packages/autokeras/nn/model_trainer.py", line 126, in train_model
    self._train()
  File "/home/zane/workspace/prediction/lib/python3.6/site-packages/autokeras/nn/model_trainer.py", line 160, in _train
    outputs = self.model(inputs)
  File "/home/zane/workspace/prediction/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/zane/workspace/prediction/lib/python3.6/site-packages/autokeras/nn/graph.py", line 681, in forward
    temp_tensor = torch_layer(edge_input_tensor)
  File "/home/zane/workspace/prediction/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/zane/workspace/prediction/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 176, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: Given groups=1, weight of size [64, 192, 1], expected input[256, 256, 8] to have 192 channels, but got 256 channels instead
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='Npccc' date='2019-01-22T19:13:27Z'>
		I also have the same issue.
&lt;denchmark-h:h2&gt;+----------------------------------------------+
|              Training model 29               |
+----------------------------------------------+
Epoch-1, Current Metric - 0:   0%|                                       | 0/22 [00:00&lt;?, ? batch/s]&lt;/denchmark-h&gt;

RuntimeError                              Traceback (most recent call last)
 in ()
1 clf = ImageClassifier(verbose=True, augment=False)
----&gt; 2 clf.fit(X_train, y_train, time_limit = 12 * 60 * 60)
3 clf.final_fit(X_train, y_train, X_test, y_test, retrain=True)
4 y = clf.evaluate(X_test, y_test)
5
/usr/local/lib/python3.6/dist-packages/autokeras/image/image_supervised.py in fit(self, x, y, time_limit)
109             print("Preprocessing finished.")
110
--&gt; 111         super().fit(x, y, time_limit)
112
113     def init_transformer(self, x):
/usr/local/lib/python3.6/dist-packages/autokeras/supervised.py in fit(self, x, y, time_limit)
133             time_limit = 24 * 60 * 60
134
--&gt; 135         self.cnn.fit(self.get_n_output_node(), x_train.shape, train_data, test_data, time_limit)
136
137     def final_fit(self, x_train, y_train, x_test, y_test, trainer_args=None, retrain=False):
/usr/local/lib/python3.6/dist-packages/autokeras/net_module.py in fit(self, n_output_node, input_shape, train_data, test_data, time_limit)
70         try:
71             while time_remain &gt; 0:
---&gt; 72                 self.searcher.search(train_data, test_data, int(time_remain))
73                 pickle_to_file(self, os.path.join(self.path, 'module'))
74                 if len(self.searcher.history) &gt;= Constant.MAX_MODEL_NUM:
/usr/local/lib/python3.6/dist-packages/autokeras/search.py in search(self, train_data, test_data, timeout)
156         if get_system() == Constant.SYS_GOOGLE_COLAB:
157             # When using Google Colab, use single process for searching and training.
--&gt; 158             self.sp_search(graph, other_info, model_id, train_data, test_data)
159         else:
160             # Use two processes
/usr/local/lib/python3.6/dist-packages/autokeras/search.py in sp_search(self, graph, other_info, model_id, train_data, test_data)
190         try:
191             metric_value, loss, graph = train(None, graph, train_data, test_data, self.trainer_args,
--&gt; 192                                               self.metric, self.loss, self.verbose, self.path)
193             # Do the search in current thread.
194             search_results = self._search_common()
/usr/local/lib/python3.6/dist-packages/autokeras/search.py in train(q, graph, train_data, test_data, trainer_args, metric, loss, verbose, path)
348     except RuntimeError as e:
349         if not re.search('out of memory', str(e)):
--&gt; 350             raise e
351         if verbose:
352             print('\nCurrent model size is too big. Discontinuing training this model to search for other models.')
/usr/local/lib/python3.6/dist-packages/autokeras/search.py in train(q, graph, train_data, test_data, trainer_args, metric, loss, verbose, path)
341                                           metric=metric,
342                                           loss_function=loss,
--&gt; 343                                           verbose=verbose).train_model(**trainer_args)
344         model.set_weight_to_graph()
345         if q:
/usr/local/lib/python3.6/dist-packages/autokeras/nn/model_trainer.py in train_model(self, max_iter_num, max_no_improvement_num, timeout)
135         for epoch in range(max_iter_num):
136             self.scheduler.step()
--&gt; 137             self._train()
138             test_loss, metric_value = self._test()
139             self.current_metric_value = metric_value
/usr/local/lib/python3.6/dist-packages/autokeras/nn/model_trainer.py in _train(self)
171             inputs, targets = inputs.to(self.device), targets.to(self.device)
172             self.optimizer.zero_grad()
--&gt; 173             outputs = self.model(inputs)
174             loss = self.loss_function(outputs, targets)
175             loss.backward()
/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in call(self, *input, **kwargs)
475             result = self._slow_forward(*input, **kwargs)
476         else:
--&gt; 477             result = self.forward(*input, **kwargs)
478         for hook in self._forward_hooks.values():
479             hook_result = hook(self, input, result)
/usr/local/lib/python3.6/dist-packages/autokeras/nn/graph.py in forward(self, input_tensor)
684                 else:
685                     edge_input_tensor = node_list[u]
--&gt; 686                 temp_tensor = torch_layer(edge_input_tensor)
687                 node_list[v] = temp_tensor
688         return node_list[output_id]
/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in call(self, *input, **kwargs)
475             result = self._slow_forward(*input, **kwargs)
476         else:
--&gt; 477             result = self.forward(*input, **kwargs)
478         for hook in self._forward_hooks.values():
479             hook_result = hook(self, input, result)
/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py in forward(self, input)
299     def forward(self, input):
300         return F.conv2d(input, self.weight, self.bias, self.stride,
--&gt; 301                         self.padding, self.dilation, self.groups)
302
303
RuntimeError: Given groups=1, weight of size [64, 76, 1, 1], expected input[128, 80, 26, 26] to have 76 channels, but got 80 channels instead
		</comment>
		<comment id='7' author='Npccc' date='2019-01-30T02:31:13Z'>
		@jhfjhfj1 Requirement already satisfied: autokeras in /usr/local/lib/python3.6/dist-packages (0.3.6)
Requirement already satisfied: nltk==3.3 in /usr/local/lib/python3.6/dist-packages (from autokeras) (3.3)
Requirement already satisfied: requests==2.20.1 in /usr/local/lib/python3.6/dist-packages (from autokeras) (2.20.1)
Requirement already satisfied: scikit-image==0.14.1 in /usr/local/lib/python3.6/dist-packages (from autokeras) (0.14.1)
Requirement already satisfied: tensorflow==1.10.0 in /usr/local/lib/python3.6/dist-packages (from autokeras) (1.10.0)
Requirement already satisfied: librosa==0.6.2 in /usr/local/lib/python3.6/dist-packages (from autokeras) (0.6.2)
Requirement already satisfied: torch==0.4.1 in /usr/local/lib/python3.6/dist-packages (from autokeras) (0.4.1)
Requirement already satisfied: opencv-python==4.0.0.21 in /usr/local/lib/python3.6/dist-packages (from autokeras) (4.0.0.21)
Requirement already satisfied: pandas==0.23.4 in /usr/local/lib/python3.6/dist-packages (from autokeras) (0.23.4)
Requirement already satisfied: imageio==2.4.1 in /usr/local/lib/python3.6/dist-packages (from autokeras) (2.4.1)
Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (from autokeras) (1.0.23)
Requirement already satisfied: scipy==1.1.0 in /usr/local/lib/python3.6/dist-packages (from autokeras) (1.1.0)
Requirement already satisfied: numpy==1.15.4 in /usr/local/lib/python3.6/dist-packages (from autokeras) (1.15.4)
Requirement already satisfied: torchvision==0.2.1 in /usr/local/lib/python3.6/dist-packages (from autokeras) (0.2.1)
Requirement already satisfied: lws==1.2 in /usr/local/lib/python3.6/dist-packages (from autokeras) (1.2)
Requirement already satisfied: tqdm==4.29.0 in /usr/local/lib/python3.6/dist-packages (from autokeras) (4.29.0)
Requirement already satisfied: inflect in /usr/local/lib/python3.6/dist-packages (from autokeras) (2.1.0)
Requirement already satisfied: lightgbm==2.2.2 in /usr/local/lib/python3.6/dist-packages (from autokeras) (2.2.2)
Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from autokeras) (0.40.1)
Requirement already satisfied: scikit-learn==0.20.1 in /usr/local/lib/python3.6/dist-packages (from autokeras) (0.20.1)
Requirement already satisfied: keras==2.2.4 in /usr/local/lib/python3.6/dist-packages (from autokeras) (2.2.4)
Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.3-&gt;autokeras) (1.11.0)
Requirement already satisfied: idna&lt;2.8,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.20.1-&gt;autokeras) (2.6)
Requirement already satisfied: urllib3&lt;1.25,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.20.1-&gt;autokeras) (1.22)
Requirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.20.1-&gt;autokeras) (3.0.4)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.20.1-&gt;autokeras) (2018.11.29)
Requirement already satisfied: networkx&gt;=1.8 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.14.1-&gt;autokeras) (2.2)
Requirement already satisfied: matplotlib&gt;=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.14.1-&gt;autokeras) (3.0.2)
Requirement already satisfied: dask[array]&gt;=0.9.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.14.1-&gt;autokeras) (0.20.2)
Requirement already satisfied: PyWavelets&gt;=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.14.1-&gt;autokeras) (1.0.1)
Requirement already satisfied: pillow&gt;=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.14.1-&gt;autokeras) (5.4.1)
Requirement already satisfied: cloudpickle&gt;=0.2.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.14.1-&gt;autokeras) (0.6.1)
Requirement already satisfied: termcolor&gt;=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0-&gt;autokeras) (1.1.0)
Requirement already satisfied: setuptools&lt;=39.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0-&gt;autokeras) (39.1.0)
Requirement already satisfied: astor&gt;=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0-&gt;autokeras) (0.7.1)
Requirement already satisfied: tensorboard&lt;1.11.0,&gt;=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0-&gt;autokeras) (1.10.0)
Requirement already satisfied: grpcio&gt;=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0-&gt;autokeras) (1.15.0)
Requirement already satisfied: gast&gt;=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0-&gt;autokeras) (0.2.2)
Requirement already satisfied: wheel&gt;=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0-&gt;autokeras) (0.32.3)
Requirement already satisfied: absl-py&gt;=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0-&gt;autokeras) (0.7.0)
Requirement already satisfied: protobuf&gt;=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.0-&gt;autokeras) (3.6.1)
Requirement already satisfied: audioread&gt;=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.6.2-&gt;autokeras) (2.1.6)
Requirement already satisfied: joblib&gt;=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa==0.6.2-&gt;autokeras) (0.13.1)
Requirement already satisfied: decorator&gt;=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.6.2-&gt;autokeras) (4.3.2)
Requirement already satisfied: resampy&gt;=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.6.2-&gt;autokeras) (0.2.1)
Requirement already satisfied: python-dateutil&gt;=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas==0.23.4-&gt;autokeras) (2.5.3)
Requirement already satisfied: pytz&gt;=2011k in /usr/local/lib/python3.6/dist-packages (from pandas==0.23.4-&gt;autokeras) (2018.9)
Requirement already satisfied: llvmlite&gt;=0.25.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba-&gt;autokeras) (0.27.0)
Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4-&gt;autokeras) (3.13)
Requirement already satisfied: keras-applications&gt;=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4-&gt;autokeras) (1.0.6)
Requirement already satisfied: keras-preprocessing&gt;=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4-&gt;autokeras) (1.0.5)
Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4-&gt;autokeras) (2.8.0)
Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.0.0-&gt;scikit-image==0.14.1-&gt;autokeras) (0.10.0)
Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.0.0-&gt;scikit-image==0.14.1-&gt;autokeras) (2.3.1)
Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib&gt;=2.0.0-&gt;scikit-image==0.14.1-&gt;autokeras) (1.0.1)
Requirement already satisfied: toolz&gt;=0.7.3; extra == "array" in /usr/local/lib/python3.6/dist-packages (from dask[array]&gt;=0.9.0-&gt;scikit-image==0.14.1-&gt;autokeras) (0.9.0)
Requirement already satisfied: werkzeug&gt;=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard&lt;1.11.0,&gt;=1.10.0-&gt;tensorflow==1.10.0-&gt;autokeras) (0.14.1)
Requirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard&lt;1.11.0,&gt;=1.10.0-&gt;tensorflow==1.10.0-&gt;autokeras) (3.0.1)
Saving Directory: /tmp/autokeras_FPRYV5
tokenlizing texts...
data readed and convert to 400 length sequences
generating preprocessing model...
loading pretrain weights...
try downloading pre train weights from link &lt;denchmark-link:http://nlp.stanford.edu/data/glove.6B.zip&gt;http://nlp.stanford.edu/data/glove.6B.zip&lt;/denchmark-link&gt;

file already extracted in the path /tmp/autokeras_store/glove/
Total 400000 word vectors embedded.
generating preprocessing model...
converting text to vector...
Initializing search.
Initialization finished.
+----------------------------------------------+
|               Training model 0               |
+----------------------------------------------+
No loss decrease after 5 epochs.
Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           0            |   0.6255926847457886   |          0.7           |
+--------------------------------------------------------------------------+
+----------------------------------------------+
|               Training model 1               |
+----------------------------------------------+
No loss decrease after 5 epochs.
Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           1            |   0.6155815839767456   |          0.7           |
+--------------------------------------------------------------------------+
+----------------------------------------------+
|               Training model 2               |
+----------------------------------------------+
No loss decrease after 5 epochs.
Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           2            |   0.6602308273315429   |          0.7           |
+--------------------------------------------------------------------------+
+----------------------------------------------+
|               Training model 3               |
+----------------------------------------------+
No loss decrease after 5 epochs.
Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           3            |   0.6266433477401734   |          0.7           |
+--------------------------------------------------------------------------+
+----------------------------------------------+
|               Training model 4               |
+----------------------------------------------+
No loss decrease after 5 epochs.
Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           4            |   0.6265841841697692   |          0.7           |
+--------------------------------------------------------------------------+
+----------------------------------------------+
|               Training model 5               |
+----------------------------------------------+
No loss decrease after 5 epochs.
Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           5            |    0.62655268907547    |          0.7           |
+--------------------------------------------------------------------------+
+----------------------------------------------+
|               Training model 6               |
+----------------------------------------------+
No loss decrease after 5 epochs.
Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           6            |   0.6266003966331481   |          0.7           |
+--------------------------------------------------------------------------+
+----------------------------------------------+
|               Training model 7               |
+----------------------------------------------+
No loss decrease after 5 epochs.
Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           7            |    0.62654128074646    |          0.7           |
+--------------------------------------------------------------------------+
+----------------------------------------------+
|               Training model 8               |
+----------------------------------------------+
No loss decrease after 5 epochs.
Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           8            |   0.6323509812355042   |          0.7           |
+--------------------------------------------------------------------------+
+----------------------------------------------+
|               Training model 9               |
+----------------------------------------------+
No loss decrease after 5 epochs.
Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           9            |   0.6324479937553406   |          0.7           |
+--------------------------------------------------------------------------+
+----------------------------------------------+
|              Training model 10               |
+----------------------------------------------+
No loss decrease after 5 epochs.
Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           10           |   0.6266794085502625   |          0.7           |
+--------------------------------------------------------------------------+
+----------------------------------------------+
|              Training model 11               |
+----------------------------------------------+
No loss decrease after 5 epochs.
Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           11           |   0.6265178203582764   |          0.7           |
+--------------------------------------------------------------------------+
+----------------------------------------------+
|              Training model 12               |
+----------------------------------------------+
No loss decrease after 5 epochs.
Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           12           |   0.6275265574455261   |          0.7           |
+--------------------------------------------------------------------------+
+----------------------------------------------+
|              Training model 13               |
+----------------------------------------------+
Epoch-1, Current Metric - 0:   0%|                                        | 0/1 [00:00&lt;?, ? batch/s]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

RuntimeError                              Traceback (most recent call last)
 in ()
36
37     clf = TextClassifier(verbose=True)
---&gt; 38 clf.fit(x=x_train, y=y_train, time_limit=6 * 60 * 60)
/usr/local/lib/python3.6/dist-packages/autokeras/text/text_supervised.py in fit(self, x, y, time_limit)
54         y = np.array(y).flatten()
55
---&gt; 56         super().fit(x, y, time_limit)
57
58     def init_transformer(self, x):
/usr/local/lib/python3.6/dist-packages/autokeras/supervised.py in fit(self, x, y, time_limit)
133             time_limit = 24 * 60 * 60
134
--&gt; 135         self.cnn.fit(self.get_n_output_node(), x_train.shape, train_data, test_data, time_limit)
136
137     def final_fit(self, x_train, y_train, x_test, y_test, trainer_args=None, retrain=False):
/usr/local/lib/python3.6/dist-packages/autokeras/net_module.py in fit(self, n_output_node, input_shape, train_data, test_data, time_limit)
70         try:
71             while time_remain &gt; 0:
---&gt; 72                 self.searcher.search(train_data, test_data, int(time_remain))
73                 pickle_to_file(self, os.path.join(self.path, 'module'))
74                 if len(self.searcher.history) &gt;= Constant.MAX_MODEL_NUM:
/usr/local/lib/python3.6/dist-packages/autokeras/search.py in search(self, train_data, test_data, timeout)
156         if get_system() == Constant.SYS_GOOGLE_COLAB:
157             # When using Google Colab, use single process for searching and training.
--&gt; 158             self.sp_search(graph, other_info, model_id, train_data, test_data)
159         else:
160             # Use two processes
/usr/local/lib/python3.6/dist-packages/autokeras/search.py in sp_search(self, graph, other_info, model_id, train_data, test_data)
190         try:
191             metric_value, loss, graph = train(None, graph, train_data, test_data, self.trainer_args,
--&gt; 192                                               self.metric, self.loss, self.verbose, self.path)
193             # Do the search in current thread.
194             search_results = self._search_common()
/usr/local/lib/python3.6/dist-packages/autokeras/search.py in train(q, graph, train_data, test_data, trainer_args, metric, loss, verbose, path)
348     except RuntimeError as e:
349         if not re.search('out of memory', str(e)):
--&gt; 350             raise e
351         if verbose:
352             print('\nCurrent model size is too big. Discontinuing training this model to search for other models.')
/usr/local/lib/python3.6/dist-packages/autokeras/search.py in train(q, graph, train_data, test_data, trainer_args, metric, loss, verbose, path)
341                                           metric=metric,
342                                           loss_function=loss,
--&gt; 343                                           verbose=verbose).train_model(**trainer_args)
344         model.set_weight_to_graph()
345         if q:
/usr/local/lib/python3.6/dist-packages/autokeras/nn/model_trainer.py in train_model(self, max_iter_num, max_no_improvement_num, timeout)
135         for epoch in range(max_iter_num):
136             self.scheduler.step()
--&gt; 137             self._train()
138             test_loss, metric_value = self._test()
139             self.current_metric_value = metric_value
/usr/local/lib/python3.6/dist-packages/autokeras/nn/model_trainer.py in _train(self)
171             inputs, targets = inputs.to(self.device), targets.to(self.device)
172             self.optimizer.zero_grad()
--&gt; 173             outputs = self.model(inputs)
174             loss = self.loss_function(outputs, targets)
175             loss.backward()
/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in call(self, *input, **kwargs)
475             result = self._slow_forward(*input, **kwargs)
476         else:
--&gt; 477             result = self.forward(*input, **kwargs)
478         for hook in self._forward_hooks.values():
479             hook_result = hook(self, input, result)
/usr/local/lib/python3.6/dist-packages/autokeras/nn/graph.py in forward(self, input_tensor)
684                 else:
685                     edge_input_tensor = node_list[u]
--&gt; 686                 temp_tensor = torch_layer(edge_input_tensor)
687                 node_list[v] = temp_tensor
688         return node_list[output_id]
/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in call(self, *input, **kwargs)
475             result = self._slow_forward(*input, **kwargs)
476         else:
--&gt; 477             result = self.forward(*input, **kwargs)
478         for hook in self._forward_hooks.values():
479             hook_result = hook(self, input, result)
/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py in forward(self, input)
174     def forward(self, input):
175         return F.conv1d(input, self.weight, self.bias, self.stride,
--&gt; 176                         self.padding, self.dilation, self.groups)
177
178
RuntimeError: Given groups=1, weight of size [512, 768, 1], expected input[118, 1024, 100] to have 768 channels, but got 1024 channels instead
		</comment>
		<comment id='8' author='Npccc' date='2019-01-31T15:26:55Z'>
		I am also have this issue, anyone know how to fix it ?
+----------------------------------------------+
|              Training model 23               |
+----------------------------------------------+
No loss decrease after 5 epochs.
Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           23           |   0.8022831022739411   |   0.6621621621621621   |
+--------------------------------------------------------------------------+
+----------------------------------------------+
|              Training model 24               |
+----------------------------------------------+
Epoch-1, Current Metric - 0:   0%|                                        | 0/7 [00:00&lt;?, ? batch/s]Process ForkProcess-25:
Traceback (most recent call last):
File "/home/ec2-user/anaconda3/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
self.run()
File "/home/ec2-user/anaconda3/lib/python3.6/multiprocessing/process.py", line 93, in run
self._target(*self._args, **self._kwargs)
File "/home/ec2-user/anaconda3/lib/python3.6/site-packages/autokeras/search.py", line 350, in train
raise e
File "/home/ec2-user/anaconda3/lib/python3.6/site-packages/autokeras/search.py", line 343, in train
verbose=verbose).train_model(**trainer_args)
File "/home/ec2-user/anaconda3/lib/python3.6/site-packages/autokeras/nn/model_trainer.py", line 137, in train_model
self._train()
File "/home/ec2-user/anaconda3/lib/python3.6/site-packages/autokeras/nn/model_trainer.py", line 173, in _train
outputs = self.model(inputs)
File "/home/ec2-user/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in call
result = self.forward(*input, **kwargs)
File "/home/ec2-user/anaconda3/lib/python3.6/site-packages/autokeras/nn/graph.py", line 686, in forward
temp_tensor = torch_layer(edge_input_tensor)
File "/home/ec2-user/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in call
result = self.forward(*input, **kwargs)
File "/home/ec2-user/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 301, in forward
self.padding, self.dilation, self.groups)
RuntimeError: Given groups=1, weight of size [1024, 192, 1, 1], expected input[128, 256, 44, 23] to have 192 channels, but got 256 channels instead
		</comment>
		<comment id='9' author='Npccc' date='2019-02-08T13:58:18Z'>
		I get the same error. The text show the error on the console, after that I provide the full log by the file run_08_02_2019 : _00_49.log like requested by @jhfjhfj1.
Using TensorFlow backend.
(70000, 100, 100, 1)
(70000,)
(25596, 100, 100, 1)
(25596,)
Saving Directory: /tmp/autokeras_UZMGTI
Preprocessing the images.
Preprocessing finished.
Initializing search.
Initialization finished.
+----------------------------------------------+
|               Training model 0               |
+----------------------------------------------+
No loss decrease after 5 epochs.
Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           0            |   2.633747911453247    |   0.6235999999999999   |
+--------------------------------------------------------------------------+
+----------------------------------------------+
|               Training model 1               |
+----------------------------------------------+
Epoch-1, Current Metric - 0:   0%|                                      | 0/543 [00:00&lt;?, ? batch/s]
Current model size is too big. Discontinuing training this model to search for other models.
+----------------------------------------------+
|               Training model 2               |
+----------------------------------------------+
Epoch-1, Current Metric - 0:   0%|                                      | 0/543 [00:00&lt;?, ? batch/s]
Current model size is too big. Discontinuing training this model to search for other models.
+----------------------------------------------+
|               Training model 3               |
+----------------------------------------------+
No loss decrease after 5 epochs.
Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           3            |   2.481710124015808    |   0.6628000000000001   |
+--------------------------------------------------------------------------+
+----------------------------------------------+
|               Training model 4               |
+----------------------------------------------+
No loss decrease after 5 epochs.
Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           4            |   2.513569700717926    |          0.66          |
+--------------------------------------------------------------------------+
+----------------------------------------------+
|               Training model 5               |
+----------------------------------------------+
No loss decrease after 5 epochs.
Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           5            |   2.5458521485328673   |         0.6388         |
+--------------------------------------------------------------------------+
+----------------------------------------------+
|               Training model 6               |
+----------------------------------------------+
No loss decrease after 5 epochs.
Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           6            |   2.5482874870300294   |   0.6416000000000001   |
+--------------------------------------------------------------------------+
+----------------------------------------------+
|               Training model 7               |
+----------------------------------------------+
No loss decrease after 5 epochs.
Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           7            |   2.4640462756156922   |         0.6608         |
+--------------------------------------------------------------------------+
+----------------------------------------------+
|               Training model 8               |
+----------------------------------------------+
No loss decrease after 5 epochs.
Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           8            |   2.539222741127014    |         0.6452         |
+--------------------------------------------------------------------------+
+----------------------------------------------+
|               Training model 9               |
+----------------------------------------------+
No loss decrease after 5 epochs.
Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           9            |   2.5334874629974364   |         0.6532         |
+--------------------------------------------------------------------------+
+----------------------------------------------+
|              Training model 10               |
+----------------------------------------------+
No loss decrease after 5 epochs.
Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           10           |   2.5555039286613463   |         0.6468         |
+--------------------------------------------------------------------------+
+----------------------------------------------+
|              Training model 11               |
+----------------------------------------------+
No loss decrease after 5 epochs.
Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           11           |   2.5215076923370363   |         0.648          |
+--------------------------------------------------------------------------+
+----------------------------------------------+
|              Training model 12               |
+----------------------------------------------+
No loss decrease after 5 epochs.
Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           12           |   2.527649772167206    |   0.6464000000000001   |
+--------------------------------------------------------------------------+
+----------------------------------------------+
|              Training model 13               |
+----------------------------------------------+
No loss decrease after 5 epochs.
Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           13           |   2.548809218406677    |   0.6456000000000001   |
+--------------------------------------------------------------------------+
+----------------------------------------------+
|              Training model 14               |
+----------------------------------------------+
No loss decrease after 5 epochs.
Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           14           |   2.6139551758766175   |         0.6408         |
+--------------------------------------------------------------------------+
+----------------------------------------------+
|              Training model 15               |
+----------------------------------------------+
No loss decrease after 5 epochs.
Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           15           |   2.493605923652649    |         0.6588         |
+--------------------------------------------------------------------------+
+----------------------------------------------+
|              Training model 16               |
+----------------------------------------------+
Epoch-1, Current Metric - 0:   0%|                                      | 0/543 [00:00&lt;?, ? batch/s]Process ForkProcess-25:
Traceback (most recent call last):
File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
self.run()
File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
self._target(*self._args, **self._kwargs)
File "/usr/local/lib/python3.6/dist-packages/autokeras/search.py", line 351, in train
raise e
File "/usr/local/lib/python3.6/dist-packages/autokeras/search.py", line 344, in train
verbose=verbose).train_model(**trainer_args)
File "/usr/local/lib/python3.6/dist-packages/autokeras/nn/model_trainer.py", line 137, in train_model
self._train()
File "/usr/local/lib/python3.6/dist-packages/autokeras/nn/model_trainer.py", line 173, in _train
outputs = self.model(inputs)
File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 489, in call
result = self.forward(*input, **kwargs)
File "/usr/local/lib/python3.6/dist-packages/autokeras/nn/graph.py", line 689, in forward
temp_tensor = torch_layer(edge_input_tensor)
File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 489, in call
result = self.forward(*input, **kwargs)
File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py", line 320, in forward
self.padding, self.dilation, self.groups)
RuntimeError: Given groups=1, weight of size [512, 192, 1, 1], expected input[128, 256, 25, 25] to have 192 channels, but got 256 channels instead
2019-02-08 00:55:12,034 - utils.py - New Model Id - 3
2019-02-08 00:55:12,034 - utils.py -
+--------------------------------------------------------------------------+
2019-02-08 00:55:12,034 - utils.py - |    Father Model ID     |                 Added Operation                 |
2019-02-08 00:55:12,034 - utils.py - +--------------------------------------------------------------------------+
2019-02-08 00:55:12,034 - utils.py - |           0            |               to_wider_model 2 64               |
2019-02-08 00:55:12,034 - utils.py - +--------------------------------------------------------------------------+
2019-02-08 01:09:16,365 - utils.py - New Model Id - 4
2019-02-08 01:09:16,365 - utils.py -
+--------------------------------------------------------------------------+
2019-02-08 01:09:16,365 - utils.py - |    Father Model ID     |                 Added Operation                 |
2019-02-08 01:09:16,365 - utils.py - +--------------------------------------------------------------------------+
2019-02-08 01:09:16,365 - utils.py - |           0            |       to_deeper_model 0 Conv2d(1, 1, 3, 1)      |
2019-02-08 01:09:16,365 - utils.py - +--------------------------------------------------------------------------+
2019-02-08 01:20:59,017 - utils.py - New Model Id - 5
2019-02-08 01:20:59,017 - utils.py -
+--------------------------------------------------------------------------+
2019-02-08 01:20:59,017 - utils.py - |    Father Model ID     |                 Added Operation                 |
2019-02-08 01:20:59,017 - utils.py - +--------------------------------------------------------------------------+
2019-02-08 01:20:59,017 - utils.py - |           3            |              to_add_skip_model 3 9              |
2019-02-08 01:20:59,017 - utils.py - +--------------------------------------------------------------------------+
2019-02-08 01:26:58,800 - utils.py - New Model Id - 6
2019-02-08 01:26:58,800 - utils.py -
+--------------------------------------------------------------------------+
2019-02-08 01:26:58,800 - utils.py - |    Father Model ID     |                 Added Operation                 |
2019-02-08 01:26:58,800 - utils.py - +--------------------------------------------------------------------------+
2019-02-08 01:26:58,801 - utils.py - |           3            |              to_add_skip_model 3 8              |
2019-02-08 01:26:58,801 - utils.py - +--------------------------------------------------------------------------+
2019-02-08 01:38:15,857 - utils.py - New Model Id - 7
2019-02-08 01:38:15,857 - utils.py -
+--------------------------------------------------------------------------+
2019-02-08 01:38:15,857 - utils.py - |    Father Model ID     |                 Added Operation                 |
2019-02-08 01:38:15,857 - utils.py - +--------------------------------------------------------------------------+
2019-02-08 01:38:15,857 - utils.py - |                        |              to_add_skip_model 4 8              |
2019-02-08 01:38:15,858 - utils.py - |           3            |               to_wider_model 6 64               |
2019-02-08 01:38:15,858 - utils.py - |                        |      to_deeper_model 8 BatchNormalization2d     |
2019-02-08 01:38:15,858 - utils.py - +--------------------------------------------------------------------------+
2019-02-08 01:47:12,485 - utils.py - New Model Id - 8
2019-02-08 01:47:12,485 - utils.py -
+--------------------------------------------------------------------------+
2019-02-08 01:47:12,485 - utils.py - |    Father Model ID     |                 Added Operation                 |
2019-02-08 01:47:12,485 - utils.py - +--------------------------------------------------------------------------+
2019-02-08 01:47:12,485 - utils.py - |           3            |              to_add_skip_model 0 3              |
2019-02-08 01:47:12,485 - utils.py - +--------------------------------------------------------------------------+
2019-02-08 01:54:36,586 - utils.py - New Model Id - 9
2019-02-08 01:54:36,586 - utils.py -
+--------------------------------------------------------------------------+
2019-02-08 01:54:36,586 - utils.py - |    Father Model ID     |                 Added Operation                 |
2019-02-08 01:54:36,586 - utils.py - +--------------------------------------------------------------------------+
2019-02-08 01:54:36,586 - utils.py - |           3            |              to_add_skip_model 2 6              |
2019-02-08 01:54:36,586 - utils.py - +--------------------------------------------------------------------------+
2019-02-08 02:08:38,497 - utils.py - New Model Id - 10
2019-02-08 02:08:38,497 - utils.py -
+--------------------------------------------------------------------------+
2019-02-08 02:08:38,497 - utils.py - |    Father Model ID     |                 Added Operation                 |
2019-02-08 02:08:38,497 - utils.py - +--------------------------------------------------------------------------+
2019-02-08 02:08:38,497 - utils.py - |           3            |            to_concat_skip_model 0 10            |
2019-02-08 02:08:38,497 - utils.py - +--------------------------------------------------------------------------+
2019-02-08 02:17:25,902 - utils.py - New Model Id - 11
2019-02-08 02:17:25,903 - utils.py -
+--------------------------------------------------------------------------+
2019-02-08 02:17:25,903 - utils.py - |    Father Model ID     |                 Added Operation                 |
2019-02-08 02:17:25,903 - utils.py - +--------------------------------------------------------------------------+
2019-02-08 02:17:25,903 - utils.py - |           3            |              to_add_skip_model 4 6              |
2019-02-08 02:17:25,903 - utils.py - +--------------------------------------------------------------------------+
2019-02-08 02:37:57,623 - utils.py - New Model Id - 12
2019-02-08 02:37:57,623 - utils.py -
+--------------------------------------------------------------------------+
2019-02-08 02:37:57,623 - utils.py - |    Father Model ID     |                 Added Operation                 |
2019-02-08 02:37:57,623 - utils.py - +--------------------------------------------------------------------------+
2019-02-08 02:37:57,623 - utils.py - |                        |              to_add_skip_model 5 7              |
2019-02-08 02:37:57,623 - utils.py - |           3            |            to_concat_skip_model 4 10            |
2019-02-08 02:37:57,623 - utils.py - +--------------------------------------------------------------------------+
2019-02-08 02:47:27,498 - utils.py - New Model Id - 13
2019-02-08 02:47:27,498 - utils.py -
+--------------------------------------------------------------------------+
2019-02-08 02:47:27,498 - utils.py - |    Father Model ID     |                 Added Operation                 |
2019-02-08 02:47:27,498 - utils.py - +--------------------------------------------------------------------------+
2019-02-08 02:47:27,498 - utils.py - |           3            |              to_add_skip_model 2 10             |
2019-02-08 02:47:27,498 - utils.py - +--------------------------------------------------------------------------+
2019-02-08 02:57:34,812 - utils.py - New Model Id - 14
2019-02-08 02:57:34,812 - utils.py -
+--------------------------------------------------------------------------+
2019-02-08 02:57:34,812 - utils.py - |    Father Model ID     |                 Added Operation                 |
2019-02-08 02:57:34,812 - utils.py - +--------------------------------------------------------------------------+
2019-02-08 02:57:34,812 - utils.py - |           3            |              to_add_skip_model 2 3              |
2019-02-08 02:57:34,812 - utils.py - +--------------------------------------------------------------------------+
2019-02-08 03:06:12,639 - utils.py - New Model Id - 15
2019-02-08 03:06:12,640 - utils.py -
+--------------------------------------------------------------------------+
2019-02-08 03:06:12,640 - utils.py - |    Father Model ID     |                 Added Operation                 |
2019-02-08 03:06:12,640 - utils.py - +--------------------------------------------------------------------------+
2019-02-08 03:06:12,640 - utils.py - |           3            |              to_add_skip_model 8 9              |
2019-02-08 03:06:12,640 - utils.py - +--------------------------------------------------------------------------+
2019-02-08 03:14:23,431 - utils.py - New Model Id - 16
2019-02-08 03:14:23,432 - utils.py -
+--------------------------------------------------------------------------+
2019-02-08 03:14:23,432 - utils.py - |    Father Model ID     |                 Added Operation                 |
2019-02-08 03:14:23,432 - utils.py - +--------------------------------------------------------------------------+
2019-02-08 03:14:23,432 - utils.py - |                        |             to_concat_skip_model 6 9            |
2019-02-08 03:14:23,432 - utils.py - |                        |               to_wider_model 6 64               |
2019-02-08 03:14:23,432 - utils.py - |                        |               to_wider_model 20 64              |
2019-02-08 03:14:23,432 - utils.py - |           3            |              to_wider_model 20 128              |
2019-02-08 03:14:23,432 - utils.py - |                        |              to_wider_model 20 256              |
2019-02-08 03:14:23,432 - utils.py - |                        |               to_wider_model 2 128              |
2019-02-08 03:14:23,432 - utils.py - |                        |     to_deeper_model 9 Conv2d(128, 128, 3, 1)    |
2019-02-08 03:14:23,432 - utils.py - +--------------------------------------------------------------------------+
		</comment>
		<comment id='10' author='Npccc' date='2019-03-06T16:52:13Z'>
		I am also experiencing this error:
autokeras:0.3.7
python:3.6.7
torch:1.0.1.post2
keras:2.2.4
tensorflow:1.12.0
system:Ubuntu 18.04
numpy:1.15.4
The images are 56x56 RGB, two possible labels. Using RTX 2070 for training.
&lt;denchmark-code&gt;+----------------------------------------------+
|              Training model 67               |
+----------------------------------------------+
                                                                                                    
No loss decrease after 5 epochs.


Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           67           |   0.3128864407539368   |   0.9393939393939394   |
+--------------------------------------------------------------------------+


+----------------------------------------------+
|              Training model 68               |
+----------------------------------------------+
Epoch-1, Current Metric - 0:   0%|                                        | 0/9 [00:00&lt;?, ? batch/s]Process ForkProcess-69:
Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "/home/omaralv/.virtualenvs/autokeras-PX4bqcu8/lib/python3.6/site-packages/autokeras/search.py", line 351, in train
    raise e
  File "/home/omaralv/.virtualenvs/autokeras-PX4bqcu8/lib/python3.6/site-packages/autokeras/search.py", line 344, in train
    verbose=verbose).train_model(**trainer_args)
  File "/home/omaralv/.virtualenvs/autokeras-PX4bqcu8/lib/python3.6/site-packages/autokeras/nn/model_trainer.py", line 137, in train_model
    self._train()
  File "/home/omaralv/.virtualenvs/autokeras-PX4bqcu8/lib/python3.6/site-packages/autokeras/nn/model_trainer.py", line 173, in _train
    outputs = self.model(inputs)
  File "/home/omaralv/.virtualenvs/autokeras-PX4bqcu8/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/omaralv/.virtualenvs/autokeras-PX4bqcu8/lib/python3.6/site-packages/autokeras/nn/graph.py", line 689, in forward
    temp_tensor = torch_layer(edge_input_tensor)
  File "/home/omaralv/.virtualenvs/autokeras-PX4bqcu8/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/omaralv/.virtualenvs/autokeras-PX4bqcu8/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 320, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: Given groups=1, weight of size [64, 192, 1, 1], expected input[128, 256, 28, 28] to have 192 channels, but got 256 channels instead
&lt;/denchmark-code&gt;

		</comment>
		<comment id='11' author='Npccc' date='2019-03-07T01:00:57Z'>
		I have the problem also,Its happend after few trainings   so confused:
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           28           |   0.1570680882781744   |         0.994          |
+--------------------------------------------------------------------------+
+----------------------------------------------+
|              Training model 29               |
+----------------------------------------------+
Epoch-1, Current Metric - 0:   0%|                                      | 0/508 [00:00&lt;?, ? batch/s]Process SpawnProcess-30:
Traceback (most recent call last):
File "C:\Anaconda3\envs\autokeras\lib\multiprocessing\process.py", line 249, in _bootstrap
self.run()
File "C:\Anaconda3\envs\autokeras\lib\multiprocessing\process.py", line 93, in run
self._target(*self._args, **self._kwargs)
File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\autokeras\search.py", line 351, in train
raise e
File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\autokeras\search.py", line 344, in train
verbose=verbose).train_model(**trainer_args)
File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\autokeras\nn\model_trainer.py", line 137, in train_model
self._train()
File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\autokeras\nn\model_trainer.py", line 173, in _train
outputs = self.model(inputs)
File "C:\Anaconda3\envs\autokeras\lib\site-packages\torch\nn\modules\module.py", line 489, in call
result = self.forward(*input, **kwargs)
File "C:\Users\Administrator\AppData\Roaming\Python\Python36\site-packages\autokeras\nn\graph.py", line 689, in forward
temp_tensor = torch_layer(edge_input_tensor)
File "C:\Anaconda3\envs\autokeras\lib\site-packages\torch\nn\modules\module.py", line 489, in call
result = self.forward(*input, **kwargs)
File "C:\Anaconda3\envs\autokeras\lib\site-packages\torch\nn\modules\conv.py", line 320, in forward
self.padding, self.dilation, self.groups)
RuntimeError: Given groups=1, weight of size [64, 384, 1, 1], expected input[128, 512, 11, 3] to have 384 channels, but got 512 channels instead
Waiting for reply
		</comment>
		<comment id='12' author='Npccc' date='2019-03-24T09:32:06Z'>
		&lt;denchmark-link:https://github.com/Santamax&gt;@Santamax&lt;/denchmark-link&gt;
 hello, did you solve this problem? i also meet the same problem,thx
		</comment>
		<comment id='13' author='Npccc' date='2019-03-26T13:47:51Z'>
		Same issue:
&lt;denchmark-code&gt;+----------------------------------------------+
|              Training model 52               |
+----------------------------------------------+
Using TensorFlow backend.
Epoch-1, Current Metric - 0:   0%|                                       | 0/25 [00:00&lt;?, ? batch/s]
Epoch-1, Current Metric - 0:  40%|████████████                  | 10/25 [00:00&lt;00:01, 11.64 batch/s]
Epoch-1, Current Metric - 0:  80%|████████████████████████      | 20/25 [00:00&lt;00:00, 15.53 batch/s]
Epoch-1, Current Metric - 0: 30 batch [00:01, 20.29 batch/s]                                        
                                                            
Epoch-1, Current Metric - 0:   0%|                                        | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
Epoch-2, Current Metric - 0.12488710692736871:   0%|                     | 0/25 [00:00&lt;?, ? batch/s]
Epoch-2, Current Metric - 0.12488710692736871:  80%|████████▊  | 20/25 [00:00&lt;00:00, 116.36 batch/s]
Epoch-2, Current Metric - 0.12488710692736871: 30 batch [00:00, 93.43 batch/s]                      
                                                                              
Epoch-2, Current Metric - 0.12488710692736871:   0%|                      | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
Epoch-3, Current Metric - 0.12643735560694064:   0%|                     | 0/25 [00:00&lt;?, ? batch/s]
Epoch-3, Current Metric - 0.12643735560694064:  80%|████████▊  | 20/25 [00:00&lt;00:00, 128.00 batch/s]
Epoch-3, Current Metric - 0.12643735560694064: 30 batch [00:00, 98.46 batch/s]                      
                                                                              
Epoch-3, Current Metric - 0.12643735560694064:   0%|                      | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
Epoch-4, Current Metric - 0.13139597000014777:   0%|                     | 0/25 [00:00&lt;?, ? batch/s]
Epoch-4, Current Metric - 0.13139597000014777:  80%|████████▊  | 20/25 [00:00&lt;00:00, 116.36 batch/s]
Epoch-4, Current Metric - 0.13139597000014777: 30 batch [00:00, 93.43 batch/s]                      
                                                                              
Epoch-4, Current Metric - 0.13139597000014777:   0%|                      | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
Epoch-5, Current Metric - 0.12447471108940003:   0%|                     | 0/25 [00:00&lt;?, ? batch/s]
Epoch-5, Current Metric - 0.12447471108940003:  80%|████████▊  | 20/25 [00:00&lt;00:00, 116.36 batch/s]
Epoch-5, Current Metric - 0.12447471108940003: 30 batch [00:00, 89.51 batch/s]                      
                                                                              
Epoch-5, Current Metric - 0.12447471108940003:   0%|                      | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
Epoch-6, Current Metric - 0.13351544241454447:   0%|                     | 0/25 [00:00&lt;?, ? batch/s]
Epoch-6, Current Metric - 0.13351544241454447:  80%|████████▊  | 20/25 [00:00&lt;00:00, 128.00 batch/s]
Epoch-6, Current Metric - 0.13351544241454447: 30 batch [00:00, 98.46 batch/s]                      
                                                                              
Epoch-6, Current Metric - 0.13351544241454447:   0%|                      | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
Epoch-7, Current Metric - 0.13811510653115905:   0%|                     | 0/25 [00:00&lt;?, ? batch/s]
Epoch-7, Current Metric - 0.13811510653115905:  80%|████████▊  | 20/25 [00:00&lt;00:00, 128.00 batch/s]
Epoch-7, Current Metric - 0.13811510653115905: 30 batch [00:00, 98.46 batch/s]                      
                                                                              
Epoch-7, Current Metric - 0.13811510653115905:   0%|                      | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
Epoch-8, Current Metric - 0.13811514988391316:   0%|                     | 0/25 [00:00&lt;?, ? batch/s]
Epoch-8, Current Metric - 0.13811514988391316:  80%|████████▊  | 20/25 [00:00&lt;00:00, 128.00 batch/s]
Epoch-8, Current Metric - 0.13811514988391316: 30 batch [00:00, 98.46 batch/s]                      
                                                                              
Epoch-8, Current Metric - 0.13811514988391316:   0%|                      | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
Epoch-9, Current Metric - 0.14264668616483409:   0%|                     | 0/25 [00:00&lt;?, ? batch/s]
Epoch-9, Current Metric - 0.14264668616483409:  80%|████████▊  | 20/25 [00:00&lt;00:00, 128.00 batch/s]
Epoch-9, Current Metric - 0.14264668616483409: 30 batch [00:00, 98.46 batch/s]                      
                                                                              
Epoch-9, Current Metric - 0.14264668616483409:   0%|                      | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
No loss decrease after 5 epochs.
Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           52           |   0.4131729885935783   |  0.13916135467817575   |
+--------------------------------------------------------------------------+
+----------------------------------------------+
|              Training model 53               |
+----------------------------------------------+
Using TensorFlow backend.
Epoch-1, Current Metric - 0:   0%|                                       | 0/25 [00:00&lt;?, ? batch/s]
Epoch-1, Current Metric - 0:  40%|████████████                  | 10/25 [00:00&lt;00:01, 11.85 batch/s]
Epoch-1, Current Metric - 0:  80%|████████████████████████      | 20/25 [00:00&lt;00:00, 15.80 batch/s]
Epoch-1, Current Metric - 0: 30 batch [00:01, 20.41 batch/s]                                        
                                                            
Epoch-1, Current Metric - 0:   0%|                                        | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
Epoch-2, Current Metric - 0.12774907524608334:   0%|                     | 0/25 [00:00&lt;?, ? batch/s]
Epoch-2, Current Metric - 0.12774907524608334:  80%|████████▊  | 20/25 [00:00&lt;00:00, 128.00 batch/s]
Epoch-2, Current Metric - 0.12774907524608334: 30 batch [00:00, 98.46 batch/s]                      
                                                                              
Epoch-2, Current Metric - 0.12774907524608334:   0%|                      | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
Epoch-3, Current Metric - 0.12868593904531053:   0%|                     | 0/25 [00:00&lt;?, ? batch/s]
Epoch-3, Current Metric - 0.12868593904531053:  80%|████████▊  | 20/25 [00:00&lt;00:00, 128.00 batch/s]
Epoch-3, Current Metric - 0.12868593904531053: 30 batch [00:00, 98.46 batch/s]                      
                                                                              
Epoch-3, Current Metric - 0.12868593904531053:   0%|                      | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
Epoch-4, Current Metric - 0.14223495769984323:   0%|                     | 0/25 [00:00&lt;?, ? batch/s]
Epoch-4, Current Metric - 0.14223495769984323:  80%|████████▊  | 20/25 [00:00&lt;00:00, 142.22 batch/s]
Epoch-4, Current Metric - 0.14223495769984323: 30 batch [00:00, 104.07 batch/s]                     
                                                                               
Epoch-4, Current Metric - 0.14223495769984323:   0%|                      | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
Epoch-5, Current Metric - 0.13478653627181486:   0%|                     | 0/25 [00:00&lt;?, ? batch/s]
Epoch-5, Current Metric - 0.13478653627181486:  80%|████████▊  | 20/25 [00:00&lt;00:00, 116.36 batch/s]
Epoch-5, Current Metric - 0.13478653627181486: 30 batch [00:00, 93.43 batch/s]                      
                                                                              
Epoch-5, Current Metric - 0.13478653627181486:   0%|                      | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
Epoch-6, Current Metric - 0.1501177909360736:   0%|                      | 0/25 [00:00&lt;?, ? batch/s]
Epoch-6, Current Metric - 0.1501177909360736:  80%|█████████▌  | 20/25 [00:00&lt;00:00, 128.00 batch/s]
Epoch-6, Current Metric - 0.1501177909360736: 30 batch [00:00, 103.23 batch/s]                      
                                                                              
Epoch-6, Current Metric - 0.1501177909360736:   0%|                       | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
Epoch-7, Current Metric - 0.14959561513031183:   0%|                     | 0/25 [00:00&lt;?, ? batch/s]
Epoch-7, Current Metric - 0.14959561513031183:  80%|████████▊  | 20/25 [00:00&lt;00:00, 128.00 batch/s]
Epoch-7, Current Metric - 0.14959561513031183: 30 batch [00:00, 98.46 batch/s]                      
                                                                              
Epoch-7, Current Metric - 0.14959561513031183:   0%|                      | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
Epoch-8, Current Metric - 0.14129765416510295:   0%|                     | 0/25 [00:00&lt;?, ? batch/s]
Epoch-8, Current Metric - 0.14129765416510295:  80%|████████▊  | 20/25 [00:00&lt;00:00, 128.00 batch/s]
Epoch-8, Current Metric - 0.14129765416510295: 30 batch [00:00, 98.46 batch/s]                      
                                                                              
Epoch-8, Current Metric - 0.14129765416510295:   0%|                      | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
Epoch-9, Current Metric - 0.15909613511022466:   0%|                     | 0/25 [00:00&lt;?, ? batch/s]
Epoch-9, Current Metric - 0.15909613511022466:  80%|████████▊  | 20/25 [00:00&lt;00:00, 128.00 batch/s]
Epoch-9, Current Metric - 0.15909613511022466: 30 batch [00:00, 98.46 batch/s]                      
                                                                              
Epoch-9, Current Metric - 0.15909613511022466:   0%|                      | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
No loss decrease after 5 epochs.
Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           53           |   0.4588329613208771   |   0.1518198517130523   |
+--------------------------------------------------------------------------+
+----------------------------------------------+
|              Training model 54               |
+----------------------------------------------+
Using TensorFlow backend.
Epoch-1, Current Metric - 0:   0%|                                       | 0/25 [00:00&lt;?, ? batch/s]Process SpawnProcess-55:
Traceback (most recent call last):
                                                                                                      File "C:\Anaconda3\envs\neuroling_project\lib\multiprocessing\process.py", line 258, in _bootstrap
    self.run()
  File "C:\Anaconda3\envs\neuroling_project\lib\multiprocessing\process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "C:\Anaconda3\envs\neuroling_project\lib\site-packages\autokeras\search.py", line 351, in train
    raise e
  File "C:\Anaconda3\envs\neuroling_project\lib\site-packages\autokeras\search.py", line 344, in train
    verbose=verbose).train_model(**trainer_args)
  File "C:\Anaconda3\envs\neuroling_project\lib\site-packages\autokeras\nn\model_trainer.py", line 137, in train_model
    self._train()
  File "C:\Anaconda3\envs\neuroling_project\lib\site-packages\autokeras\nn\model_trainer.py", line 173, in _train
    outputs = self.model(inputs)
  File "C:\Anaconda3\envs\neuroling_project\lib\site-packages\torch\nn\modules\module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "C:\Anaconda3\envs\neuroling_project\lib\site-packages\autokeras\nn\graph.py", line 689, in forward
    temp_tensor = torch_layer(edge_input_tensor)
  File "C:\Anaconda3\envs\neuroling_project\lib\site-packages\torch\nn\modules\module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "C:\Anaconda3\envs\neuroling_project\lib\site-packages\torch\nn\modules\conv.py", line 187, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: Given groups=1, weight of size [64, 192, 1], expected input[128, 256, 3] to have 192 channels, but got 256 channels instead
&lt;/denchmark-code&gt;

@jhfjhfj1 I'll give you access to the private repo where the code is in and upload an example of the used data (so that execution of the experiment works). I'll also put a yaml of the conda env in there
It seems that the searcher alters the layer size inadequately.
I got another error that is definitely related:
&lt;denchmark-code&gt;Epoch-1, Current Metric - 0:  40%|████████████                  | 10/25 [00:00&lt;00:01, 11.98 batch/s]
Epoch-1, Current Metric - 0:  80%|████████████████████████      | 20/25 [00:01&lt;00:00, 15.18 batch/s]
Epoch-1, Current Metric - 0: 30 batch [00:01, 18.43 batch/s]                                        
                                                            
Epoch-1, Current Metric - 0:   0%|                                        | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
Epoch-2, Current Metric - 0.12290543672313972:   0%|                     | 0/25 [00:00&lt;?, ? batch/s]
Epoch-2, Current Metric - 0.12290543672313972:  80%|█████████▌  | 20/25 [00:00&lt;00:00, 69.23 batch/s]
Epoch-2, Current Metric - 0.12290543672313972: 30 batch [00:00, 53.85 batch/s]                      
                                                                              
Epoch-2, Current Metric - 0.12290543672313972:   0%|                      | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
Epoch-3, Current Metric - 0.12383724298179866:   0%|                     | 0/25 [00:00&lt;?, ? batch/s]
Epoch-3, Current Metric - 0.12383724298179866:  80%|█████████▌  | 20/25 [00:00&lt;00:00, 68.75 batch/s]
Epoch-3, Current Metric - 0.12383724298179866: 30 batch [00:00, 55.90 batch/s]                      
                                                                              
Epoch-3, Current Metric - 0.12383724298179866:   0%|                      | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
Epoch-4, Current Metric - 0.1227938792591689:   0%|                      | 0/25 [00:00&lt;?, ? batch/s]
Epoch-4, Current Metric - 0.1227938792591689:  80%|██████████▍  | 20/25 [00:00&lt;00:00, 63.31 batch/s]
Epoch-4, Current Metric - 0.1227938792591689: 30 batch [00:00, 54.34 batch/s]                       
                                                                             
Epoch-4, Current Metric - 0.1227938792591689:   0%|                       | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
Epoch-5, Current Metric - 0.1266208110584486:   0%|                      | 0/25 [00:00&lt;?, ? batch/s]
Epoch-5, Current Metric - 0.1266208110584486:  80%|██████████▍  | 20/25 [00:00&lt;00:00, 71.45 batch/s]
Epoch-5, Current Metric - 0.1266208110584486: 30 batch [00:00, 57.42 batch/s]                       
                                                                             
Epoch-5, Current Metric - 0.1266208110584486:   0%|                       | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
Epoch-6, Current Metric - 0.12670472838612737:   0%|                     | 0/25 [00:00&lt;?, ? batch/s]
Epoch-6, Current Metric - 0.12670472838612737:  80%|█████████▌  | 20/25 [00:00&lt;00:00, 70.20 batch/s]
Epoch-6, Current Metric - 0.12670472838612737: 30 batch [00:00, 57.15 batch/s]                      
                                                                              
Epoch-6, Current Metric - 0.12670472838612737:   0%|                      | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
Epoch-7, Current Metric - 0.13579259996247325:   0%|                     | 0/25 [00:00&lt;?, ? batch/s]
Epoch-7, Current Metric - 0.13579259996247325:  80%|█████████▌  | 20/25 [00:00&lt;00:00, 68.05 batch/s]
Epoch-7, Current Metric - 0.13579259996247325: 30 batch [00:00, 55.76 batch/s]                      
                                                                              
Epoch-7, Current Metric - 0.13579259996247325:   0%|                      | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
Epoch-8, Current Metric - 0.13407541190650835:   0%|                     | 0/25 [00:00&lt;?, ? batch/s]
Epoch-8, Current Metric - 0.13407541190650835:  80%|█████████▌  | 20/25 [00:00&lt;00:00, 72.49 batch/s]
Epoch-8, Current Metric - 0.13407541190650835: 30 batch [00:00, 58.29 batch/s]                      
                                                                              
Epoch-8, Current Metric - 0.13407541190650835:   0%|                      | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
Epoch-9, Current Metric - 0.1379582250010873:   0%|                      | 0/25 [00:00&lt;?, ? batch/s]
Epoch-9, Current Metric - 0.1379582250010873:  80%|██████████▍  | 20/25 [00:00&lt;00:00, 70.69 batch/s]
Epoch-9, Current Metric - 0.1379582250010873: 30 batch [00:00, 55.65 batch/s]                       
                                                                             
Epoch-9, Current Metric - 0.1379582250010873:   0%|                       | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
Epoch-10, Current Metric - 0.13624785117471777:   0%|                    | 0/25 [00:00&lt;?, ? batch/s]
Epoch-10, Current Metric - 0.13624785117471777:  80%|████████▊  | 20/25 [00:00&lt;00:00, 71.97 batch/s]
Epoch-10, Current Metric - 0.13624785117471777: 30 batch [00:00, 57.16 batch/s]                     
                                                                               
Epoch-10, Current Metric - 0.13624785117471777:   0%|                     | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
No loss decrease after 5 epochs.
Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|          358           |   0.4047888070344925   |  0.13656011946032445   |
+--------------------------------------------------------------------------+
+----------------------------------------------+
|              Training model 359              |
+----------------------------------------------+
Using TensorFlow backend.
Epoch-1, Current Metric - 0:   0%|                                       | 0/25 [00:00&lt;?, ? batch/s]
Epoch-1, Current Metric - 0:  40%|████████████                  | 10/25 [00:00&lt;00:01, 12.11 batch/s]
Epoch-1, Current Metric - 0:  80%|████████████████████████      | 20/25 [00:01&lt;00:00, 15.33 batch/s]
Epoch-1, Current Metric - 0: 30 batch [00:01, 18.81 batch/s]                                        
                                                            
Epoch-1, Current Metric - 0:   0%|                                        | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
Epoch-2, Current Metric - 0.12355520981874844:   0%|                     | 0/25 [00:00&lt;?, ? batch/s]
Epoch-2, Current Metric - 0.12355520981874844:  80%|█████████▌  | 20/25 [00:00&lt;00:00, 71.96 batch/s]
Epoch-2, Current Metric - 0.12355520981874844: 30 batch [00:00, 58.26 batch/s]                      
                                                                              
Epoch-2, Current Metric - 0.12355520981874844:   0%|                      | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
Epoch-3, Current Metric - 0.1237318097595806:   0%|                      | 0/25 [00:00&lt;?, ? batch/s]
Epoch-3, Current Metric - 0.1237318097595806:  80%|██████████▍  | 20/25 [00:00&lt;00:00, 68.98 batch/s]
Epoch-3, Current Metric - 0.1237318097595806: 30 batch [00:00, 53.75 batch/s]                       
                                                                             
Epoch-3, Current Metric - 0.1237318097595806:   0%|                       | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
Epoch-4, Current Metric - 0.1281043480177284:   0%|                      | 0/25 [00:00&lt;?, ? batch/s]
Epoch-4, Current Metric - 0.1281043480177284:  80%|██████████▍  | 20/25 [00:00&lt;00:00, 68.99 batch/s]
Epoch-4, Current Metric - 0.1281043480177284: 30 batch [00:00, 57.16 batch/s]                       
                                                                             
Epoch-4, Current Metric - 0.1281043480177284:   0%|                       | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
Epoch-5, Current Metric - 0.12668128207560211:   0%|                     | 0/25 [00:00&lt;?, ? batch/s]
Epoch-5, Current Metric - 0.12668128207560211:  80%|█████████▌  | 20/25 [00:00&lt;00:00, 73.28 batch/s]
Epoch-5, Current Metric - 0.12668128207560211: 30 batch [00:00, 57.34 batch/s]                      
                                                                              
Epoch-5, Current Metric - 0.12668128207560211:   0%|                      | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
Epoch-6, Current Metric - 0.13132767045923463:   0%|                     | 0/25 [00:00&lt;?, ? batch/s]
Epoch-6, Current Metric - 0.13132767045923463:  80%|█████████▌  | 20/25 [00:00&lt;00:00, 71.20 batch/s]
Epoch-6, Current Metric - 0.13132767045923463: 30 batch [00:00, 57.41 batch/s]                      
                                                                              
Epoch-6, Current Metric - 0.13132767045923463:   0%|                      | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
Epoch-7, Current Metric - 0.13078966470082815:   0%|                     | 0/25 [00:00&lt;?, ? batch/s]
Epoch-7, Current Metric - 0.13078966470082815:  80%|█████████▌  | 20/25 [00:00&lt;00:00, 68.75 batch/s]
Epoch-7, Current Metric - 0.13078966470082815: 30 batch [00:00, 56.47 batch/s]                      
                                                                              
Epoch-7, Current Metric - 0.13078966470082815:   0%|                      | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
Epoch-8, Current Metric - 0.13533035701312265:   0%|                     | 0/25 [00:00&lt;?, ? batch/s]
Epoch-8, Current Metric - 0.13533035701312265:  80%|█████████▌  | 20/25 [00:00&lt;00:00, 70.94 batch/s]
Epoch-8, Current Metric - 0.13533035701312265: 30 batch [00:00, 57.59 batch/s]                      
                                                                              
Epoch-8, Current Metric - 0.13533035701312265:   0%|                      | 0/3 [00:00&lt;?, ? batch/s]
                                                                                                    
No loss decrease after 5 epochs.
Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|          359           |   0.4063565507531166   |  0.13456349463940695   |
+--------------------------------------------------------------------------+
+----------------------------------------------+
|              Training model 360              |
+----------------------------------------------+
Using TensorFlow backend.
Epoch-1, Current Metric - 0:   0%|                                       | 0/25 [00:00&lt;?, ? batch/s]Process SpawnProcess-361:
                                                                                                    Traceback (most recent call last):
  File "C:\Anaconda3\envs\neuroling_project\lib\multiprocessing\process.py", line 258, in _bootstrap
    self.run()
  File "C:\Anaconda3\envs\neuroling_project\lib\multiprocessing\process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "C:\Anaconda3\envs\neuroling_project\lib\site-packages\autokeras\search.py", line 351, in train
    raise e
  File "C:\Anaconda3\envs\neuroling_project\lib\site-packages\autokeras\search.py", line 344, in train
    verbose=verbose).train_model(**trainer_args)
  File "C:\Anaconda3\envs\neuroling_project\lib\site-packages\autokeras\nn\model_trainer.py", line 137, in train_model
    self._train()
  File "C:\Anaconda3\envs\neuroling_project\lib\site-packages\autokeras\nn\model_trainer.py", line 173, in _train
    outputs = self.model(inputs)
  File "C:\Anaconda3\envs\neuroling_project\lib\site-packages\torch\nn\modules\module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "C:\Anaconda3\envs\neuroling_project\lib\site-packages\autokeras\nn\graph.py", line 689, in forward
    temp_tensor = torch_layer(edge_input_tensor)
  File "C:\Anaconda3\envs\neuroling_project\lib\site-packages\torch\nn\modules\module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "C:\Anaconda3\envs\neuroling_project\lib\site-packages\torch\nn\modules\batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "C:\Anaconda3\envs\neuroling_project\lib\site-packages\torch\nn\functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 5 elements not 10```

Tried this for both regression and classification so the bug should defenitely be in the search, in the generation of a new models to be explicit on that
&lt;/denchmark-code&gt;

		</comment>
		<comment id='14' author='Npccc' date='2019-04-17T03:40:04Z'>
		from PIL import Image
import torch.nn as nn
import torch as t
from torch.autograd import Variable as V
from torchvision.transforms import ToTensor,ToPILImage
to_tensor=ToTensor() # image  -&gt; tensor
to_pil=ToPILImage()
lena=Image.open('C:/Users/Administrator/Desktop/lena.jpg')
lena
input=to_tensor(lena).unsqueeze(0) # 输入是一个batch, batch_size=1
#锐化卷积核
kernel=t.ones(3,3)/-9
kernel[1][1]=1
conv=nn.Conv2d(1,1,(3,3),1,bias=False)
conv.weight.data=kernel.view(1,1,3,3)
out=conv(V(input))
to_pil(out.data.squeeze(0))
#它会出现错误提示
RuntimeError                              Traceback (most recent call last)
 in 
7 conv.weight.data=kernel.view(1,1,3,3)
8
----&gt; 9 out=conv(V(input))
10 to_pil(out.data.squeeze(0))
D:\ProgramData\Anaconda3\lib\site-packages\torch\nn\modules\module.py in call(self, *input, **kwargs)
487             result = self._slow_forward(*input, **kwargs)
488         else:
--&gt; 489             result = self.forward(*input, **kwargs)
490         for hook in self._forward_hooks.values():
491             hook_result = hook(self, input, result)
D:\ProgramData\Anaconda3\lib\site-packages\torch\nn\modules\conv.py in forward(self, input)
318     def forward(self, input):
319         return F.conv2d(input, self.weight, self.bias, self.stride,
--&gt; 320                         self.padding, self.dilation, self.groups)
321
322
RuntimeError: Given groups=1, weight of size [1, 1, 3, 3], expected input[1, 3, 75, 121] to have 1 channels, but got 3 channels instead
#修改代码
input=to_tensor(lena).unsqueeze(0) # 输入是一个batch, batch_size=1
input=input.resize(1,75*3,121).unsqueeze(0)
#锐化卷积核
kernel=t.ones(3,3)/-9
kernel[1][1]=1
conv=nn.Conv2d(1,1,(3,3),1,bias=False)
conv.weight.data=kernel.view(1,1,3,3)
out=conv(V(input))
to_pil(out.data.squeeze(0))
#它会出来结果，它的结果应该是一张图片，但是修改后它出现三张图片，虽然它不一定是解决方案，但是希望有所帮助，当然，也希望哪位大神能解决这个问题
		</comment>
		<comment id='15' author='Npccc' date='2019-08-12T02:55:28Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
		<comment id='16' author='Npccc' date='2019-12-26T07:01:13Z'>
		
@Npccc Thank you so much for the bug report.
It is very hard for us to reproduce this bug without the full log.
Would you please provide the full log if it is available?
Thank you!

Still experiencing the same problem. Looking for a fix. &lt;denchmark-link:https://github.com/haifeng-jin&gt;@haifeng-jin&lt;/denchmark-link&gt;
  Do you know where the problem is at? I guess autokeras/torch seems to pass an unexpected dimension to the next conv layer while it is searching an architecture
		</comment>
	</comments>
</bug>