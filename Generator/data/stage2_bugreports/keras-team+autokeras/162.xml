<bug id='162' author='swenkel' open_date='2018-08-31T07:17:33Z' closed_time='2018-08-31T21:16:40Z'>
	<summary>Model evaluation on test set yields different results on the same model</summary>
	<description>
&lt;denchmark-h:h3&gt;Bug Description&lt;/denchmark-h&gt;

Test set evaluation yields different results if re-run.
&lt;denchmark-h:h3&gt;Reproducing Steps&lt;/denchmark-h&gt;

Steps to reproduce the behavior:
re-run multiple times:
y_pred = clf.evaluate(X_test, y_test)
print(y_pred)
will yield different results:
&lt;denchmark-code&gt;0.0992
0.1032
0.101
0.0989
&lt;/denchmark-code&gt;

Further, using manual evaluation:
y_prediction = clf.predict(x_test=X_test)
from sklearn.metrics import accuracy_score
accuracy_score(y_pred=y_prediction, y_true=y_test)
leads to different results as well.  It looks like the model either uses some random function (AFAIK all: random_states=42 in the source code) or there is some major error in the pipeline that causes different predictions of the test set all the time.
&lt;denchmark-h:h3&gt;Expected Behavior&lt;/denchmark-h&gt;

Final evaluation on a test set should not lead to different results using the same model on the same data.
&lt;denchmark-h:h3&gt;Setup Details&lt;/denchmark-h&gt;

Include the details about the versions of:

OS type and version: Linux
Python: 3.6.5
autokeras: 0.2.11
scikit-learn:0.19.1
numpy:1.14.5
keras: 2.2.2
scipy:1.1.0
tensorflow: 1.10.0
pytorch:0.4.1

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

This error is verified on MNIST and Fashion-MNIST.
	</description>
	<comments>
		<comment id='1' author='swenkel' date='2018-08-31T21:16:40Z'>
		Thank you so much! &lt;denchmark-link:https://github.com/swenkel&gt;@swenkel&lt;/denchmark-link&gt;

This is a very important bug.
I have just fixed it in the develop branch, which will be used for the next release.
		</comment>
		<comment id='2' author='swenkel' date='2019-06-13T14:17:06Z'>
		thank you for fixing this
		</comment>
	</comments>
</bug>