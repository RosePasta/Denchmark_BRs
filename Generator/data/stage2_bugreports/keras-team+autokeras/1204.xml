<bug id='1204' author='castrovictor' open_date='2020-06-24T15:51:05Z' closed_time='2020-09-04T21:14:16Z'>
	<summary>Training does not start if we feed ImageClassifier.fit with tf.Dataset</summary>
	<description>
&lt;denchmark-h:h3&gt;Bug Description&lt;/denchmark-h&gt;

The first batch of images seems to load due to memory usage, but training does not start. I am using ImageClassifier and I feed the fit() method with a tf.Dataset.
&lt;denchmark-h:h3&gt;Bug Reproduction&lt;/denchmark-h&gt;

Code for reproducing the bug:
&lt;denchmark-code&gt;import autokeras as ak
import os
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import tensorflow.keras.models
import tensorflow as tf

# The path for the partition
base_path  = "/mnt/sdd/vicastro/SubBDFinal5cv1/partition0/"
train_path = os.path.join(base_path, "train")

# The size each image will be transformed onto (should be compatible with the network input size)
target_size = (256, 256)
# The size for each training batch
batch_size = 32

# The ImageDataGenerator class allows reading images batch by batch
# and preprocessing them on the fly.
preprocessing_images = ImageDataGenerator(
    rotation_range=0., # tested rotation options = 0, 3
    horizontal_flip=True
)

# This instance is created without preprocessing options, for
# validation and test.
validation_images = ImageDataGenerator()

# Calling `flow_from_directory` produces a generator object that can
# be fed into a keras training call.

def get_train_generator():
    train_generator = preprocessing_images.flow_from_directory(
        train_path,
        target_size=target_size,
        batch_size=batch_size,
        class_mode="categorical", # classes are provided in categorical format for a 2-unit output layer
        shuffle=True,
        color_mode="rgb",
        seed=1234567890
    )

    return train_generator

train_generator = tf.data.Dataset.from_generator(get_train_generator,output_types=('float32', 'uint8'),
    output_shapes=(tf.TensorShape((32, 256, 256, 3)), tf.TensorShape((32,2))))

clf = ak.ImageClassifier(max_trials=1)
clf.fit(train_generator, epochs=1)
model = clf.export_model()
model.save("/mnt/sdd/vicastro/model.h5")

&lt;/denchmark-code&gt;

Data used by the code:
Can't provide the actual dataset since it's not public, but under directories N/P there are .png images. Here is the tree structure:
.
├── train
│   ├── N
│   └── P
&lt;denchmark-h:h3&gt;Expected Behavior&lt;/denchmark-h&gt;

Start the epoch to fit a model.
&lt;denchmark-h:h3&gt;Setup Details&lt;/denchmark-h&gt;

Include the details about the versions of:

OS type and version: Linux 3.13.0-88-generic x86_64
Python: 3.8.3
autokeras: 1.0.3 (1.0.2 doesn't work either)
keras-tuner: 1.0.2rc0
scikit-learn: 0.23.1
numpy: 1.18.15
pandas: 1.0.5
tensorflow: tensorflow-gpu 2.2.0

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

The epoch status is not printed, it stops at the beginning of the fitting. Neither the process finish nor error shows up.
It has been tested on a GPU server, it does not work either in a local machine.
	</description>
	<comments>
		<comment id='1' author='castrovictor' date='2020-06-29T15:30:11Z'>
		I found the bug. When  is called, it get stuck when calling, which get stucks in. Then,  get sutck when calling  This happens because Image data preprocessing from Keras creates an infinite generator, but according to &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/data/Dataset#reduce&gt;tensorflow docs, reduce() &lt;/denchmark-link&gt;
runs until the dataset is exhausted. The point is that as it is infinite, the dataset never gets exhausted.
		</comment>
		<comment id='2' author='castrovictor' date='2020-08-28T17:40:23Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
		<comment id='3' author='castrovictor' date='2020-12-27T16:22:05Z'>
		
I found the bug. When .fit() is called, it get stuck when calling _prepare_data(), which get stucks insplit_data(). Then, split_data() get sutck when calling dataset.reduce(). This happens because Image data preprocessing from Keras creates an infinite generator, but according to tensorflow docs, reduce() runs until the dataset is exhausted. The point is that as it is infinite, the dataset never gets exhausted.

Hi man how did you by pass it ? I also have the same problem ?
did you mangae to make it finite in some way limit the generator ?
		</comment>
	</comments>
</bug>