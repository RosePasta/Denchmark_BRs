<bug id='160' author='MattVil' open_date='2018-08-30T08:40:53Z' closed_time='2018-09-10T17:00:28Z'>
	<summary>Minimum hardware required</summary>
	<description>
What is the minimal hardware required to run a search ?  On CIFAR, a Nvidia GTX 1080 8Go doesn't  seems to be enough.  The GPU memory used varies a lot according to the model built. Is there a way to limit the model build space or something like that ?
&lt;denchmark-code&gt;  File "cifar.py", line 17, in &lt;module&gt; clf.fit(x_train, y_train, time_limit=12 * 60 * 60)  
  File "/myPath/python3.6/site-packages/autokeras/image_supervised.py", line 238, in fit run_searcher_once(train_data, test_data, self.path, int(time_remain))  
  File "/myPath/python3.6/site-packages/autokeras/image_supervised.py", line 41, in run_searcher_once searcher.search(train_data, test_data, timeout)  
  File "/myPath/python3.6/site-packages/autokeras/search.py", line 177, in search self.metric, self.loss, self.verbose)]))  
  File "/myPath/python3.6/site-packages/autokeras/search.py", line 280, in train verbose).train_model(**trainer_args)  
  File "/myPath/python3.6/site-packages/autokeras/utils.py", line 124, in train_model self._train()  
  File "/myPath/python3.6/site-packages/autokeras/utils.py", line 169, in _train loss.backward()  
  File "/myPath/python3.6/site-packages/torch/tensor.py", line 93, in backward torch.autograd.backward(self, gradient, retain_graph, create_graph)  
  File "/myPath/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward allow_unreachable=True)  # allow_unreachable flag  
  RuntimeError: CUDA error: out of memory```
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='MattVil' date='2018-08-30T14:58:31Z'>
		i think this is due to the accuracy of model below some "initial threshold", resulting this program piled a large number of CNN layers in a single model and run out of graphics card memory. i used just 500 images (15MB in total)to train a model and met this problem too.
		</comment>
		<comment id='2' author='MattVil' date='2018-08-30T17:21:43Z'>
		&lt;denchmark-link:https://github.com/MattVil&gt;@MattVil&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/fliessen&gt;@fliessen&lt;/denchmark-link&gt;

You can import autokeras.constant.Constant. Then change the value of Constant.MAX_MODEL_WIDTH and Constant.MAX_MODEL_DEPTH to limit the memory usage.
We are trying to find more complicated ways to limit the memory.
Will close this issue afterward.
thanks.
		</comment>
		<comment id='3' author='MattVil' date='2018-08-31T04:09:33Z'>
		I tried to set both width and depth to 4,(default are 1024 and 10),this problem still exists.
		</comment>
		<comment id='4' author='MattVil' date='2018-08-31T06:52:08Z'>
		This looks more like a memory allocation/deallocation problem.
I am running out of memory on a regular basis. Using max_model_width and depth does not help. Neither does limiting 'max_iter_num'. It simply delays the problem.
The good thing is that you build autokeras with snapshots that allow to continue training after an error. In such a case all memory is deallocated and if I restart/continue training, then I am able to train the model for quite some time again before it happens again.
I am not so sure if this problem can be solved by PyTorchs cuda management (&lt;denchmark-link:https://pytorch.org/docs/stable/notes/cuda.html&gt;https://pytorch.org/docs/stable/notes/cuda.html&lt;/denchmark-link&gt;
). Compared to tensorflows GPU management it is still quite poor.
I would suggest to implement full GPU memory deallocation after each model or after a defineable threshold of GPU memory.
		</comment>
	</comments>
</bug>