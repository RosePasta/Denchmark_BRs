<bug id='193' author='aojue1109' open_date='2018-09-13T09:47:35Z' closed_time='2018-11-14T05:58:21Z'>
	<summary>Compress image before feed to NN if image is too large</summary>
	<description>
&lt;denchmark-h:h3&gt;Bug Description&lt;/denchmark-h&gt;

I did a cat and dog image classification task, only used 100 pictures to train, and a RuntimeError appeared on a single 8GB system: $ Torch: not enough memory: you tried to allocate 0GB. The image input is 100 * 224 * 224 *3. This problem occurs. The input standard for reducing the image is 100 * 32 * 32 * 3. This problem is not the case, but the accuracy is very low for the training result. The final evaluation value of the second classification. Less than 0.6.
The specific issues are as follows:
&lt;denchmark-code&gt;╒==============================================╕
|               Training model 1               |
╘==============================================╛
Using TensorFlow backend.
Current Epoch:   0%|                             | 0/1 [00:00&lt;?, ? batch/s]Exception ignored in: &lt;bound method tqdm.__del__ of Current Epoch:   0%|                             | 0/1 [07:59&lt;?, ? batch/s]&gt;                                                   
Traceback (most recent call last):
  File "/root/anaconda3/envs/automl-formal/lib/python3.6/site-packages/tqdm/_tqdm.py", line 885, in __del__
    self.close()
  File "/root/anaconda3/envs/automl-formal/lib/python3.6/site-packages/tqdm/_tqdm.py", line 1090, in close
    self._decr_instances(self)
  File "/root/anaconda3/envs/automl-formal/lib/python3.6/site-packages/tqdm/_tqdm.py", line 454, in _decr_instances
    cls.monitor.exit()
  File "/root/anaconda3/envs/automl-formal/lib/python3.6/site-packages/tqdm/_monitor.py", line 52, in exit
    self.join()
  File "/root/anaconda3/envs/automl-formal/lib/python3.6/threading.py", line 1053, in join
    raise RuntimeError("cannot join current thread")
RuntimeError: cannot join current thread
multiprocessing.pool.RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/root/anaconda3/envs/automl-formal/lib/python3.6/multiprocessing/pool.py", line 119, in worker
    result = (True, func(*args, **kwds))
  File "/root/anaconda3/envs/automl-formal/lib/python3.6/multiprocessing/pool.py", line 44, in mapstar
    return list(map(*args))
  File "/root/anaconda3/envs/automl-formal/lib/python3.6/site-packages/autokeras/search.py", line 296, in train
    verbose=verbose).train_model(**trainer_args)
  File "/root/anaconda3/envs/automl-formal/lib/python3.6/site-packages/autokeras/model_trainer.py", line 103, in train_model
    self._train()
  File "/root/anaconda3/envs/automl-formal/lib/python3.6/site-packages/autokeras/model_trainer.py", line 147, in _train
    outputs = self.model(inputs)
  File "/root/anaconda3/envs/automl-formal/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/root/anaconda3/envs/automl-formal/lib/python3.6/site-packages/autokeras/graph.py", line 610, in forward
    temp_tensor = torch_layer(edge_input_tensor)
  File "/root/anaconda3/envs/automl-formal/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/root/anaconda3/envs/automl-formal/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 66, in forward
    exponential_average_factor, self.eps)
  File "/root/anaconda3/envs/automl-formal/lib/python3.6/site-packages/torch/nn/functional.py", line 1254, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: $ Torch: not enough memory: you tried to allocate 0GB. Buy new RAM! at [/pytorch/aten/src/TH/THGeneral.cpp:204]
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Reproducing Steps&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Step 1: Choose 1 CPU to use (among 8 CPUs):
Step 2: Run the Cat and Dog Wars classification task, select 50 cats and 50 dogs for the classification task, enter the image parameters: 100 * 224 * 224 *3

&lt;denchmark-h:h3&gt;Expected Behavior&lt;/denchmark-h&gt;

Succeed in running the classification task without this error
Image classification accuracy increased to an acceptable greater than 80%
&lt;denchmark-h:h3&gt;Setup Details&lt;/denchmark-h&gt;

Include the details about the versions of:

OS type and version:
Python: 3.6
autokeras: 0.2.14
scikit-learn:0.19.1
numpy:1.14.5
keras:2.2.2
scipy:1.1.0
tensorflow:1.10.1
pytorch:

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='aojue1109' date='2018-10-02T14:13:18Z'>
		&lt;denchmark-link:https://github.com/satyakesav&gt;@satyakesav&lt;/denchmark-link&gt;

First, you may examine whether the data is loaded to GPU memory batch by batch, or all together.
If is loaded all together, try to change it to load batch by batch.
&lt;denchmark-link:https://github.com/jhfjhfj1/autokeras/blob/master/autokeras/model_trainer.py#L219&gt;https://github.com/jhfjhfj1/autokeras/blob/master/autokeras/model_trainer.py#L219&lt;/denchmark-link&gt;

Second, if the image is oversize (the total size is larger than 64643), resize the image before feed into the NN, according to their original length/width ratio. Keep the size just right.
Thanks.
		</comment>
		<comment id='2' author='aojue1109' date='2018-10-29T23:25:58Z'>
		&lt;denchmark-link:https://github.com/aojue1109&gt;@aojue1109&lt;/denchmark-link&gt;

I was trying to reproduce the error that you reported using the following scenario but unable to do that.
My training set has 2000 images (1000 each of cat and dog images from Cat vs Dog image classification task). The images have arbitrary sizes in the training set and hence I resized all of them to 324 x 324 x 3 since you have mentioned that the images in your scenario are the same. I have verified that I am training my network on a CPU not GPU.
The following is the code that I used
&lt;denchmark-code&gt;x_train, y_train = load_image_dataset(csv_file_path="labels.csv", images_path="data")

x_temp = []
for x in x_train:
    x_temp.append(scipy.misc.imresize(x, [324, 324, 3]))
x_train = numpy.array(x_temp)

if __name__ == '__main__':
    clf = ImageClassifier(verbose=True, augment=False)
    clf.fit(x_train, y_train, time_limit=12 * 60 * 60)

&lt;/denchmark-code&gt;

Despite the scenario, my model is training without any failure but it is insanely slow because of the obvious reasons.
Can you please let me know the exact steps to reproduce the issue. Before doing that, it will be good If you can verify whether the issue is reproducible in your environment with the latest pull request. Please let me know.
Thanks,
Satya
		</comment>
	</comments>
</bug>