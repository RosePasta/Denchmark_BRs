<bug id='191' author='swenkel' open_date='2018-09-13T07:39:52Z' closed_time='2018-09-13T13:53:57Z'>
	<summary>Dynamic memory limitation is disfunctional</summary>
	<description>
&lt;denchmark-h:h3&gt;Bug Description&lt;/denchmark-h&gt;

&lt;denchmark-link:https://github.com/jhfjhfj1/autokeras/pull/180&gt;#180&lt;/denchmark-link&gt;
 should have solved the  problem. However, during training () it will simply output the current model it wants to train and that it is out of memory. This message is repeated until the time limit is reached:
&lt;denchmark-code&gt;╒==============================================╕
|              Training model 6                |
╘==============================================╛
out of memory
&lt;/denchmark-code&gt;

While displaying this , autokeras will try to load the model into memory again and will break down againg caused this message.
&lt;denchmark-h:h3&gt;Expected Behavior&lt;/denchmark-h&gt;


building smaller models
limit the memory more agrressively
or: sequential CUDA memory allocation and deallocation

&lt;denchmark-h:h3&gt;Setup Details&lt;/denchmark-h&gt;

Include the details about the versions of:

OS type and version:
Python:
autokeras: develop (todays version from 06:00 UTC)
scikit-learn:
numpy:
keras:
scipy:
tensorflow:
pytorch:

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

dataset used: subset of the &lt;denchmark-link:http://benchmark.ini.rub.de/?section=gtsrb&amp;subsection=news&gt;German Traffic Sign Recognition Benchmark&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='swenkel' date='2018-09-13T13:53:49Z'>
		&lt;denchmark-link:https://github.com/swenkel&gt;@swenkel&lt;/denchmark-link&gt;
 Thanks. This is the expected behavior. Whenever it is out of memory, it would never search such large models, but smaller ones.
Sent with GitHawk
		</comment>
	</comments>
</bug>