<bug id='834' author='pezdorado' open_date='2019-11-13T10:29:59Z' closed_time='2020-01-19T20:38:45Z'>
	<summary>AutoKeras 1.0 much slower than 0.4 on Google Colab</summary>
	<description>
When I try to run a simple MNIST example on Google Colab with GPU with Autokeras 0.4 it runs very fast (1 epoch of the first model takes &lt; 2 s) but with 1.0 it runs much slower (1 epoch of the first model takes &gt; 80 s). When I disable the GPU 0.4 runs as slow as 1.0 which suggests that 1.0 isnâ€™t using the GPU. How can I make Autokeras 1.0 run as fast as 0.4 with GPU?
To reproduce go to &lt;denchmark-link:url&gt;colab.research.google.com&lt;/denchmark-link&gt;
, choose a Python 3 runtime with GPU accelerator, and execute the following
0.4 code
&lt;denchmark-code&gt;%tensorflow_version 1.x

!pip install autokeras

import autokeras
import tensorflow

( ( x, y ), validation_data ) = tensorflow.keras.datasets.mnist.load_data( )
model = autokeras.ImageClassifier( verbose = True )
model.fit( x, y )
&lt;/denchmark-code&gt;

1.0 code
&lt;denchmark-code&gt;%tensorflow_version 2.x

!pip install git+git://github.com/keras-team/keras-tuner@master#egg=keras-tuner
!pip install git+git://github.com/keras-team/autokeras@master#egg=autokeras

import tensorflow
import autokeras

( ( x, y ), validation_data ) = tensorflow.keras.datasets.mnist.load_data( )
model = autokeras.ImageClassifier( )
model.fit( x, y, validation_data = validation_data )
&lt;/denchmark-code&gt;

The issue is breakdown to the following issues.
After solving them, the speed should be improved.
&lt;denchmark-link:https://github.com/keras-team/autokeras/issues/906&gt;#906&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/keras-team/autokeras/issues/907&gt;#907&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/keras-team/autokeras/issues/908&gt;#908&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/keras-team/autokeras/issues/909&gt;#909&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/keras-team/autokeras/issues/910&gt;#910&lt;/denchmark-link&gt;
.
	</description>
	<comments>
		<comment id='1' author='pezdorado' date='2019-11-14T09:38:03Z'>
		In your 1.0 code, you have validation_data, so it does make sense.
You still need to validate it, it takes more time.
		</comment>
		<comment id='2' author='pezdorado' date='2019-11-14T12:53:56Z'>
		
In your 1.0 code, you have validation_data, so it does make sense.
You still need to validate it, it takes more time.

The validation_data is only a small fraction of the training data (10000 versus 60000 samples I believe) and does not explain the speed factor of &gt; 40. When I use validation_split = 0.2 instead of validation_data the speed difference remains (I understand 0.4 also implicitly uses a validation split but I cannot find what fraction).
		</comment>
		<comment id='3' author='pezdorado' date='2019-11-15T04:13:42Z'>
		Thank you for the issue. We are trying to solve it.
		</comment>
		<comment id='4' author='pezdorado' date='2019-12-05T07:05:40Z'>
		Thank you for reopening this and trying to solve this.
In &lt;denchmark-link:https://github.com/keras-team/autokeras/issues/847&gt;#847&lt;/denchmark-link&gt;
 it was suggested that this problem has to do with data augmentation.
I'm having a hard time understanding how this could explain the fact that without GPU 0.4 and 1.0 are equally fast but with GPU 0.4 is much faster and 1.0 is still as slow as without GPU.
Also I didn't know AutoKeras did image augmentation. I tried to find more information about this, and I found that the 0.4 ImageClassifier( ) has an augment parameter that defaults to False but the 1.0 ImageClassifier( ) doesn't seem to have an augment parameter. Does 1.0 do augmentation and if so is there a way to switch it off?
		</comment>
		<comment id='5' author='pezdorado' date='2019-12-05T07:54:53Z'>
		&lt;denchmark-link:https://github.com/pezdorado&gt;@pezdorado&lt;/denchmark-link&gt;
 You can turn off the augmentation by using the functional API of 1.0. The image block has that arg. Please refer to the tutorial on the official website. Thanks
		</comment>
		<comment id='6' author='pezdorado' date='2019-12-09T13:54:47Z'>
		I've tried the functional API with an ImageBlock( augment = False ) but it's as slow as the task API. I hope I applied the functional API correctly since the tutorial is quite sparse
&lt;denchmark-code&gt;!pip install git+git://github.com/keras-team/autokeras@master#egg=autokeras

import tensorflow
import autokeras

( ( x, y ), validation_data ) = tensorflow.keras.datasets.mnist.load_data( )

inputs = autokeras.ImageInput( )
blocks = autokeras.hypermodel.hyperblock.ImageBlock( augment = False )( inputs )
outputs = autokeras.ClassificationHead( )( blocks )

model = autokeras.GraphAutoModel(
    inputs = inputs,
    outputs = outputs
)

model.fit( x, y, validation_split = 0.1, batch_size = 128 )
&lt;/denchmark-code&gt;

Did I apply the functional API correctly and did I correctly turn off augmentation? If so, then the problem does not seem to be caused by augmentation.
		</comment>
		<comment id='7' author='pezdorado' date='2019-12-09T16:36:28Z'>
		&lt;denchmark-link:https://github.com/pezdorado&gt;@pezdorado&lt;/denchmark-link&gt;
 You did it correctly. Thank you!
You can further break down the ImageBlock into [Normalization] + [ImageAugmentation] + [ConvBlock, ResNetBlock, XceptionBlock]. In this way, we can see which one is the slow one.
I remember I tried just ConvBlock + ClassificationHead. It was fast.
		</comment>
		<comment id='8' author='pezdorado' date='2019-12-13T12:02:54Z'>
		@jhfjhfj1 Thank you for your answers. I measured the execution times of the different blocks for 1 epoch both with and without GPU
&lt;denchmark-code&gt;inputs = autokeras.ImageInput( )
# t1 = autokeras.hypermodel.preprocessor.Normalization( )( inputs )
# t1 = autokeras.hypermodel.preprocessor.ImageAugmentation( )( inputs )
# t1 = autokeras.hypermodel.block.ConvBlock( )( inputs )
# t1 = autokeras.hypermodel.block.XceptionBlock( )( inputs )
t1 = autokeras.hypermodel.block.ResNetBlock( )( inputs )
outputs = autokeras.ClassificationHead( )( t1 )

model = autokeras.GraphAutoModel(
    inputs = inputs,
    outputs = outputs
)
&lt;/denchmark-code&gt;

The results are as follows
With GPU
Normalization( ): 85 s
ImageAugmentation( ): 278 s
ConvBlock( ): 11 s
ResNetBlock( ): 45 s
XceptionBlock( ): 42 s
Without GPU:
Normalization( ): 33 s
ImageAugmentation( ): 80 s
ConvBlock( ): 111 s
ResNetBlock( ): 3882 s
XceptionBlock( ): 2659 s
So on 1.0 the GPU makes ConvBlock( ), ResNetBlock( ) and XceptionBlock( ) much faster but Normalization( ) and ImageAugmentation( ) slower. However the 1.0 ConvBlock( ) with GPU is still is much slower than the 0.4 full ImageClassifier( ) that only takes 2 s.
		</comment>
		<comment id='9' author='pezdorado' date='2019-12-17T06:57:22Z'>
		There is no option to export/save a model in autokeras 1.0. How can we save the model for deployment ?
		</comment>
		<comment id='10' author='pezdorado' date='2020-01-19T20:37:34Z'>
		&lt;denchmark-link:https://github.com/KalidindiMounika&gt;@KalidindiMounika&lt;/denchmark-link&gt;
 We have the function  in the new release now.
It can export everything except for the preprocessors.
		</comment>
		<comment id='11' author='pezdorado' date='2020-01-19T20:38:43Z'>
		This issue has been breakdown to multiple parts in the AutoKeras Project.
After adopting the preprocessing layers the issue should be resolved.
		</comment>
	</comments>
</bug>