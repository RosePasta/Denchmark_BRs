<bug id='221' author='monikatomar92' open_date='2018-09-25T23:36:36Z' closed_time='2018-10-22T13:37:22Z'>
	<summary>Dataloader load from disk by batches</summary>
	<description>
&lt;denchmark-h:h3&gt;Bug Description&lt;/denchmark-h&gt;

After the first model training, in the second model, I am getting the Memory error. I have also added the latest release as in another issue it was mentioned that memory error bug has been fixed in the latest issue. The error looks as follows:
Traceback (most recent call last):
File "/home/monika/.local/lib/python3.6/site-packages/tqdm/_tqdm.py", line 885, in del
self.close()
File "/home/monika/.local/lib/python3.6/site-packages/tqdm/_tqdm.py", line 1090, in close
self._decr_instances(self)
File "/home/monika/.local/lib/python3.6/site-packages/tqdm/_tqdm.py", line 454, in _decr_instances
cls.monitor.exit()
File "/home/monika/.local/lib/python3.6/site-packages/tqdm/_monitor.py", line 52, in exit
self.join()
File "/usr/lib/python3.6/threading.py", line 1053, in join
raise RuntimeError("cannot join current thread")
RuntimeError: cannot join current thread
/usr/lib/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
len(cache))
Traceback (most recent call last):
File "mnist.py", line 21, in 
clf.fit(x_train, y_train, time_limit=12 * 60 * 60)
File "/home/monika/.local/lib/python3.6/site-packages/autokeras/image_supervised.py", line 239, in fit
run_searcher_once(train_data, test_data, self.path, int(time_remain))
File "/home/monika/.local/lib/python3.6/site-packages/autokeras/image_supervised.py", line 40, in run_searcher_once
searcher.search(train_data, test_data, timeout)
File "/home/monika/.local/lib/python3.6/site-packages/autokeras/search.py", line 190, in search
timeout)
File "/home/monika/.local/lib/python3.6/site-packages/autokeras/bayesian.py", line 257, in optimize_acq
for temp_graph in transform(elem.graph):
File "/home/monika/.local/lib/python3.6/site-packages/autokeras/net_transformer.py", line 83, in transform
temp_graph = to_deeper_graph(deepcopy(graph))
File "/home/monika/.local/lib/python3.6/site-packages/autokeras/net_transformer.py", line 63, in to_deeper_graph
graph.to_conv_deeper_model(layer_id, 3)
File "/home/monika/.local/lib/python3.6/site-packages/autokeras/graph.py", line 313, in to_conv_deeper_model
new_layers = deeper_conv_block(target, kernel_size, self.weighted)
File "/home/monika/.local/lib/python3.6/site-packages/autokeras/layer_transformer.py", line 11, in deeper_conv_block
weight = np.zeros((n_filters, n_filters) + filter_shape)
MemoryError
If I run the code with the mnist classification example, I get the following error after the 8th model üëç
Process SpawnPoolWorker-9:
Traceback (most recent call last):
File "/usr/lib/python3.6/multiprocessing/pool.py", line 125, in worker
put((job, i, result))
File "/usr/lib/python3.6/multiprocessing/queues.py", line 347, in put
self._writer.send_bytes(obj)
File "/usr/lib/python3.6/multiprocessing/connection.py", line 200, in send_bytes
self._send_bytes(m[offset:offset + size])
File "/usr/lib/python3.6/multiprocessing/connection.py", line 397, in _send_bytes
self._send(header)
File "/usr/lib/python3.6/multiprocessing/connection.py", line 368, in _send
n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
self.run()
File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
self._target(*self._args, **self._kwargs)
File "/usr/lib/python3.6/multiprocessing/pool.py", line 130, in worker
put((job, i, (False, wrapped)))
File "/usr/lib/python3.6/multiprocessing/queues.py", line 347, in put
self._writer.send_bytes(obj)
File "/usr/lib/python3.6/multiprocessing/connection.py", line 200, in send_bytes
self._send_bytes(m[offset:offset + size])
File "/usr/lib/python3.6/multiprocessing/connection.py", line 404, in _send_bytes
self._send(header + buf)
File "/usr/lib/python3.6/multiprocessing/connection.py", line 368, in _send
n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
/usr/lib/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
len(cache))
/usr/lib/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 4 leaked semaphores to clean up at shutdown
len(cache))
&lt;denchmark-h:h3&gt;Reproducing Steps&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Step 1: ... use the mnist.py from examples.
Step 2: ... run the script with either the custom dataset for classification or the default example

&lt;denchmark-h:h3&gt;Expected Behavior&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Setup Details&lt;/denchmark-h&gt;

Include the details about the versions of:

OS type and version: Ubuntu 18.04
Python: 3.6.5
autokeras: 0.2.14
scikit-learn:0.19.1
numpy:1.15.2
keras:2.2.2
scipy:1.1.0
tensorflow:1.10.1
pytorch:0.4.1

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

My data is of the following dimension :
Training Data : (42532, 128,128), training label :  ( 42532,24)
Testing Data : ( 1325,128,128) , testing label : (1325,24)
Running the code on GTX 1080TI 11GB and 32GB RAM
	</description>
	<comments>
		<comment id='1' author='monikatomar92' date='2018-09-27T12:27:55Z'>
		I met the same error
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "ak_aoi_img.py", line 23, in &lt;module&gt;
    clf.fit(x_train, y_train, time_limit=2 * 60 * 60)
  File "/home/adt/wen/ak_aoi/autokeras/image_supervised.py", line 239, in fit
    run_searcher_once(train_data, test_data, self.path, int(time_remain))
  File "/home/adt/wen/ak_aoi/autokeras/image_supervised.py", line 40, in run_searcher_once
    searcher.search(train_data, test_data, timeout)
  File "/home/adt/wen/ak_aoi/autokeras/search.py", line 193, in search
    remaining_time)
  File "/home/adt/wen/ak_aoi/autokeras/bayesian.py", line 256, in optimize_acq
    for temp_graph in transform(elem.graph):
  File "/home/adt/wen/ak_aoi/autokeras/net_transformer.py", line 83, in transform
    temp_graph = to_deeper_graph(deepcopy(graph))
  File "/home/adt/wen/ak_aoi/autokeras/net_transformer.py", line 63, in to_deeper_graph
    graph.to_conv_deeper_model(layer_id, 3)
  File "/home/adt/wen/ak_aoi/autokeras/graph.py", line 313, in to_conv_deeper_model
    new_layers = deeper_conv_block(target, kernel_size, self.weighted)
  File "/home/adt/wen/ak_aoi/autokeras/layer_transformer.py", line 11, in deeper_conv_block
    weight = np.zeros((n_filters, n_filters) + filter_shape)
MemoryError
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='monikatomar92' date='2018-09-29T20:51:31Z'>
		The solution would be to load the data from disk batch by batch, which can be done by pytorch dataloader.
		</comment>
	</comments>
</bug>