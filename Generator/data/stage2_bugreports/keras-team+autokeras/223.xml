<bug id='223' author='rafaelmarconiramos' open_date='2018-09-26T15:05:55Z' closed_time='2018-09-29T20:55:54Z'>
	<summary>Out of memory in my small dataset</summary>
	<description>
&lt;denchmark-h:h3&gt;Bug Description&lt;/denchmark-h&gt;

I executed an original example and works.
I change the example to my small dataset to try, but the search starts and stops on Training Model 1. It works to load the dataset, execute the first model, then stop in the second model and it still unsuccessful trying repeatedly.
I put in the searcher.train() a code to release memory suggest on &lt;denchmark-link:https://github.com/keras-team/autokeras/issues/76&gt;#76&lt;/denchmark-link&gt;
, but didn´t work.
&lt;denchmark-h:h3&gt;Reproducing Steps&lt;/denchmark-h&gt;

The log of my execution:
Using TensorFlow backend.
(43, 512, 512, 4)
(43,)
(46, 512, 512, 4)
(46,)
Initializing search.
Initialization finished.
+----------------------------------------------+
|               Training model 0               |
+----------------------------------------------+
totalMemory: 15.90GiB freeMemory: 15.27GiB
.........
Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           0            |   3.3943511486053466   |          0.5           |
+--------------------------------------------------------------------------+
+----------------------------------------------+
|               Training model 1               |
+----------------------------------------------+
totalMemory: 15.90GiB freeMemory: 15.27GiB
out of memory: 4198245
........
&lt;denchmark-h:h3&gt;Expected Behavior&lt;/denchmark-h&gt;

I expected the process didn´t stop and stilled searching the architecture.
&lt;denchmark-h:h3&gt;Setup Details&lt;/denchmark-h&gt;

Include the details about the versions of:

OS type and version:
Python: 3.6.2
autokeras: 0.2.14
scikit-learn: 0.19.1
numpy: 1.14.5
keras: 2.2.2
scipy: 1.1.0
tensorflow: 1.10.1
tensorflow-gpu: 1.3.0
torch: 0.4.1

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

This problem is a bug of implementation or the autokeras generate a heavy models and don´t get trainng?
	</description>
	<comments>
		<comment id='1' author='rafaelmarconiramos' date='2018-09-27T09:57:39Z'>
		i get the same error too, it seems the bug is not fixed.
		</comment>
		<comment id='2' author='rafaelmarconiramos' date='2018-09-29T20:55:54Z'>
		&lt;denchmark-link:https://github.com/rafaelmarconiramos&gt;@rafaelmarconiramos&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/zuoxiang95&gt;@zuoxiang95&lt;/denchmark-link&gt;

This is an expected behavior.
It would affect the search too much, but continue to search smaller models.
		</comment>
		<comment id='3' author='rafaelmarconiramos' date='2018-10-03T19:05:23Z'>
		@jhfjhfj1 I got the same error in my custom dataset, saying out of memory in the second model and stop searching. Any suggestion for this?

Saving model.
+--------------------------------------------------------------------------+
|        Model ID        |          Loss          |      Metric Value      |
+--------------------------------------------------------------------------+
|           0            |   4.703066980093718    |   0.9453525641025642   |
+--------------------------------------------------------------------------+


out of memory
╒==============================================╕
|               Training model 1               |
╘==============================================╛
Using TensorFlow backend.
                                                                       out of memory

╒==============================================╕
|               Training model 1               |
╘==============================================╛
Using TensorFlow backend.

		</comment>
	</comments>
</bug>