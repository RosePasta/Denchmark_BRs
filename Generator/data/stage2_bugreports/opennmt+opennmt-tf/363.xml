<bug id='363' author='BigFishMaster' open_date='2019-03-04T13:30:47Z' closed_time='2019-03-05T11:56:58Z'>
	<summary>Sharing Embedding meets ERROR!</summary>
	<description>
when I set "share_embeddings=EmbeddingsSharingLevel.ALL" in Transformer model, The trained model can not be correctly infered, e.g. the model will output serveral same tokens for each input query. There may be a bug under this parameter. I also tried share_embeddings=EmbeddingsSharingLevel.NONE, it's ok.
my env:
gpu: k40
os:redhat
cuda:9.0
opennmt-tf version: v1.20.1 and the current master.
	</description>
	<comments>
		<comment id='1' author='BigFishMaster' date='2019-03-04T13:50:36Z'>
		Did you make sure to train with a joint vocabulary? If yes, how does the training loss look like with share_embeddings=EmbeddingsSharingLevel.ALL?
		</comment>
		<comment id='2' author='BigFishMaster' date='2019-03-04T16:06:11Z'>
		Yes.My task is Dialogue so I use same vocabulary for input and target. The loss curve seems well but the problem only happens during inference.
I guess the embedding sharing has something bug. Please help me the fix that.
The embedding sharing Transformer I used is just from config/models/transformer_share_embeddings.py
		</comment>
		<comment id='3' author='BigFishMaster' date='2019-03-04T16:07:07Z'>
		&lt;denchmark-link:https://github.com/guillaumekln&gt;@guillaumekln&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='BigFishMaster' date='2019-03-04T17:26:49Z'>
		Yeah looks like there is a bug. Thanks for testing and reporting, that's helpful.
		</comment>
	</comments>
</bug>