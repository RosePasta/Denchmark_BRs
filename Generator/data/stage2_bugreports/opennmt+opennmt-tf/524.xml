<bug id='524' author='yura415' open_date='2019-10-18T08:57:18Z' closed_time='2019-10-18T10:08:37Z'>
	<summary>TypeError: 'NoneType' object is not iterable</summary>
	<description>
OpenNMT-tf v2.1.0
Python v3.5.2
tensorflow-gpu v2.0.0
&lt;denchmark-code&gt;model_dir: toy_enru_transformer_withalign

data:
  train_features_file: data/train.en
  train_labels_file: data/train.ru
  train_alignments: data/train.align
  eval_features_file: data/valid.en
  eval_labels_file: data/valid.ru
  source_vocabulary: data/mdl-en.vocab
  target_vocabulary: data/mdl-ru.vocab

params:
  guided_alignment_type: "ce"
  guided_alignment_weight: 1.0

train:
  save_checkpoints_steps: 1000

eval:
  external_evaluators: BLEU
  export_on_best: bleu
  early_stopping:
    metric: bleu
    min_improvement: 0.01
    steps: 4

infer:
  batch_size: 32
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/usr/local/bin/onmt-main", line 8, in &lt;module&gt;
    sys.exit(main())
  File "/usr/local/lib/python3.5/dist-packages/opennmt/bin/main.py", line 189, in main
    checkpoint_path=args.checkpoint_path)
  File "/usr/local/lib/python3.5/dist-packages/opennmt/runner.py", line 205, in train
    export_on_best=eval_config.get("export_on_best"))
  File "/usr/local/lib/python3.5/dist-packages/opennmt/training.py", line 70, in __call__
    self._model.create_variables(optimizer=self._optimizer)
  File "/usr/local/lib/python3.5/dist-packages/opennmt/models/model.py", line 271, in create_variables
    features = self.examples_inputter.make_features(features=features, training=True)
  File "/usr/local/lib/python3.5/dist-packages/opennmt/models/sequence_to_sequence.py", line 426, in make_features
    element, alignment = element
TypeError: 'NoneType' object is not iterable
&lt;/denchmark-code&gt;




OpenNMT-tf/opennmt/models/model.py


         Line 271
      in
      bec38f9






 features = self.examples_inputter.make_features(features=features, training=True) 








OpenNMT-tf/opennmt/models/sequence_to_sequence.py


        Lines 433 to 435
      in
      bec38f9






 def make_features(self, element=None, features=None, training=None): 



 if training and self.alignment_file is not None: 



 element, alignment = element 





Trying to unpack a NoneType tuple. There's no check for None in sequence_to_sequence.py and in model.py we don't send a value for element.
This looks like a bug. Can you actually train a model on v2.1.0?
	</description>
	<comments>
		<comment id='1' author='yura415' date='2019-10-18T09:07:28Z'>
		Thanks for reporting! The error happens when using guided alignment. I will fix this ASAP.
		</comment>
		<comment id='2' author='yura415' date='2019-10-18T09:43:50Z'>
		Seems to be working now. Big thanks! Now I've got another issue regarding loss reduction. I'm trying to fix this by looking into configuration.
That's the error:
&lt;denchmark-code&gt;ValueError: Please use `tf.keras.losses.Reduction.SUM` or `tf.keras.losses.Reduction.NONE` for loss reduction when losses are used with `tf.distribute.Strategy` outside of the built-in training loops. You can implement `tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE` using global batch size like:
&lt;/denchmark-code&gt;

BTW with_eval won't be recognized unless --auto_config is specified.
&lt;denchmark-code&gt;root@c7f21e7c5c10:/opt/training# onmt-main --model_type Transformer --config config/toy.yml train --with_eval
usage: onmt-main [-h] [-v] --config CONFIG [CONFIG ...] [--auto_config]
                 [--model_type {GPT2Small,ListenAttendSpell,LstmCnnCrfTagger,LuongAttention,Transformer,TransformerBig}]
                 [--model MODEL] [--run_dir RUN_DIR] [--data_dir DATA_DIR]
                 [--checkpoint_path CHECKPOINT_PATH]
                 [--log_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET}]
                 [--seed SEED] [--gpu_allow_growth]
                 [--intra_op_parallelism_threads INTRA_OP_PARALLELISM_THREADS]
                 [--inter_op_parallelism_threads INTER_OP_PARALLELISM_THREADS]
                 [--mixed_precision]
                 {train,eval,infer,export,score,average_checkpoints,update_vocab}
                 ...
onmt-main: error: unrecognized arguments: --with_eval
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='yura415' date='2019-10-18T09:51:43Z'>
		Keeping this open for the PR. I've got just another question. Is there a way to observe current training progress? (loss, bleu score)
		</comment>
		<comment id='4' author='yura415' date='2019-10-18T10:08:31Z'>
		
Now I've got another issue regarding loss reduction. I'm trying to fix this by looking into configuration.

Thanks. That's indeed another issue. I opened &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-tf/issues/526&gt;#526&lt;/denchmark-link&gt;
.

BTW with_eval won't be recognized unless --auto_config is specified.

This comes from a limitation in Python argument parsing.  should not be the last option before . See this &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-tf/issues/504#issuecomment-537390282&gt;comment&lt;/denchmark-link&gt;
 to know why.

I've got just another question. Is there a way to observe current training progress? (loss, bleu score)

Some information are printed in the console at regular interval. You can also try TensorBoard: &lt;denchmark-link:http://opennmt.net/OpenNMT-tf/training.html#monitoring&gt;http://opennmt.net/OpenNMT-tf/training.html#monitoring&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='yura415' date='2019-10-18T10:10:07Z'>
		Thanks! I've reported more details regarding &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-tf/issues/526&gt;#526&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>