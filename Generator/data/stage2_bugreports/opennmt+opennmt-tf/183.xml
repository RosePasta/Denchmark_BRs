<bug id='183' author='lockder' open_date='2018-07-26T07:54:39Z' closed_time='2018-08-06T12:13:24Z'>
	<summary>average_last_checkpoints crashes in distributed training</summary>
	<description>
1.5.0 (2018-06-08)
New features
Training option average_last_checkpoints to automatically average checkpoints at the end of the training
Reading the new features I found on 1.5.0 an amazing new parameter to automatically do the avg of the checkpoints  but when I try to use I get this message.
Traceback (most recent call last):
File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py", line 1322, in _do_call
return fn(*args)
File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py", line 1307, in _run_fn
options, feed_dict, fetch_list, target_list, run_metadata)
File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py", line 1409, in _call_tf_sessionrun
run_metadata)
tensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /home/ubuntu/NaturalLanguageRecognition/NaturalLanguageManager/src/Intent_Classification/model/model.ckpt-369
[[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_INT64, DT_FLOAT, DT_INT64], _device="/job:ps/replica:0/task:0/device:CPU:0"](_recv_save/Const_0_S1, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
[[Node: save/restore_all/NoOp_1_S8 = _Recv&lt;denchmark-link:&gt;client_terminated=false, recv_device="/job:chief/replica:0/task:0/device:GPU:0", send_device="/job:ps/replica:0/task:0/device:CPU:0", send_device_incarnation=8143571150034301869, tensor_name="edge_247_save/restore_all/NoOp_1", tensor_type=DT_FLOAT, _device="/job:chief/replica:0/task:0/device:GPU:0"&lt;/denchmark-link&gt;
]]
I went to the console and did the ls inside the folder
ubuntu@mltraining:~/NaturalLanguageRecognition/NaturalLanguageManager/src/Intent_Classification/model$ ls
checkpoint  events.out.tfevents.1532590583.mltraining  graph.pbtxt        model.ckpt-369.meta    projector_config.pbtxt  src_word_vocab.txt
eval        events.out.tfevents.1532591278.mltraining  model.ckpt-0.meta  model_description.pkl  src_char_vocab.txt
Then I went to the checkpoint file and took a look:
model_checkpoint_path: "model.ckpt-369"
all_model_checkpoint_paths: "model.ckpt-0"
all_model_checkpoint_paths: "model.ckpt-369"
looks like its trying to find the name inside the checkpoint but its saved with the extension .meta
The weird thing its the restore training works very well its just when doing the average of the checkpoints
	</description>
	<comments>
		<comment id='1' author='lockder' date='2018-07-26T08:20:24Z'>
		Are you using distributed training? I'm not sure checkpoint averaging is correctly supported in this scenario. Thanks for reporting.
		</comment>
		<comment id='2' author='lockder' date='2018-07-26T08:37:16Z'>
		yes I'm using distributed training , looking to the code, its checking if its a chief machine before doing the avg. Also I tried to run the script manually and its happening the same thing
Right now I have 3 machines. 1 chief(gpu) 1 worker(gpu) and 1 ps server (cpu)
I'm not running the script from the same folder as the model. I'm just sending the full path from another path because I have a central python script running all the scripts from there
		</comment>
		<comment id='3' author='lockder' date='2018-07-26T09:16:02Z'>
		Thanks. Not sure how to handle correctly this case yet so I need to investigate. Feel free to share any insights.
		</comment>
		<comment id='4' author='lockder' date='2018-07-26T09:20:21Z'>
		I will try to train it without distributed training and see what happens. if I find the fix I will send it to you
		</comment>
		<comment id='5' author='lockder' date='2018-07-26T11:25:51Z'>
		on the first look. Looks like the files are splitted between chief and ps server ( not sure if I have more than one ps server)
When doing non distributed training the model folder contains files .index and 0000-of-0002
and the meta files with the checkpoint file
but with distributed training
the chief contains .meta files and checkpoint
the ps server contains .index and 0000-of-0002
I will keep investigating
		</comment>
		<comment id='6' author='lockder' date='2018-07-26T12:33:51Z'>
		non distributed training:
checkpoints_path = tf.train.get_checkpoint_state(model_dir).all_model_checkpoint_paths
is returning a list of strings with the path of the file
but also there is only 1 checkpoint instead all of them. I had 4 saved checkpoints inside the folder
Instead on distributed:
is returning an object. RepeatedScalarContainer with the path of the meta files
		</comment>
		<comment id='7' author='lockder' date='2018-07-27T07:29:56Z'>
		Well Its not a fix but if you setup the chief and ps machine the same. then it will work since it will have all the files on the same machine and the other pc can be setup as workers.
But I find it weird because its saying Averaging 1 checkpoints... and I had 5 checkpoints to be saved on the config file
		</comment>
		<comment id='8' author='lockder' date='2018-07-27T07:45:30Z'>
		
Well Its not a fix but if you setup the chief and ps machine the same. then it will work since it will have all the files on the same machine and the other pc can be setup as workers.

I think that is the expected behavior. I also read that one should consider using a shared filesystem to properly load a checkpoint produced by a distributed training.
With all this information, I don't think there is anything to change in OpenNMT-tf.
		</comment>
		<comment id='9' author='lockder' date='2018-07-27T08:05:10Z'>
		Well it will only work if you have on the same machine all files :) but it will not work if you setup a different ps machine to free up gpu memory so you can have a bigger model or bigger batch size. I tried to setup the ps on a cpu machine but it was more slow.
and I'm not sure if you have more than 1 ps server what will happen jejeje
The other issue is the code its finding only the last checkpoint not sure why even if there are more than one checkpoint , so its getting the last one, and not doing and avg
		</comment>
		<comment id='10' author='lockder' date='2018-07-27T08:11:20Z'>
		HDFS could be used to share a filesystem across separate machines. Not sure if it requires code change or not.

The other issue is the code its finding only the last checkpoint not sure why even if there are more than one checkpoint , so its getting the last one, and not doing and avg

What value did you set to average_last_checkpoints?
		</comment>
		<comment id='11' author='lockder' date='2018-07-27T08:50:45Z'>
		I see, for the readme I was adding true instead a number :P my mistake! thanks
		</comment>
		<comment id='12' author='lockder' date='2018-08-06T12:13:24Z'>
		According to me, the only action required by this issue is to better support remote/shared filesystems like HDFS or S3. The PR above improves the support of such configurations for the model directory and data files.
Closing for now.
		</comment>
	</comments>
</bug>