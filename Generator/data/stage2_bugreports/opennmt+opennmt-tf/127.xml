<bug id='127' author='luozhouyang' open_date='2018-05-22T03:11:24Z' closed_time='2018-05-30T09:06:38Z'>
	<summary>Program stuck when restoring parameters after saving checkpoints</summary>
	<description>
I start train_and_eval:
onmt-main train_and_eval --config/sample.yml --model_type Transformer
and set save_checkpoints_steps: 5000 in config/sample.yml.
But the training process seems to stop every time after saving checkpoints and trying to restoring parameters from checkpoints.
These are logouts:
...
INFO:tensorflow:loss = 30730.3, step = 45595 (13.718 sec)
INFO:tensorflow:global_step/sec: 7.14987
INFO:tensorflow:words_per_sec/features: 1893.38
INFO:tensorflow:words_per_sec/labels: 2041.83
INFO:tensorflow:loss = 37210.4, step = 45695 (13.992 sec)
INFO:tensorflow:Saving checkpoints for 45764 into models/20180521_2/model.ckpt.
INFO:tensorflow:Loss for final step: 47047.0.
INFO:tensorflow:Starting evaluation at 2018-05-22-02:47:37
2018-05-22 02:47:37.947617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1154] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
INFO:tensorflow:Restoring parameters from models/20180521_2/model.ckpt-45764
The last line of logout show that program trying to restore parameters. But program stuck here. If you don't cancel it, it will block here forever.
If I cancel the training by CTRL+C, and then restart training:
onmt-main train_and_eval --config/sample.yml --model_type Transformer
The program seems to continue training process. The logout is like this:
^CTraceback (most recent call last):
  File "/usr/local/bin/onmt-main", line 11, in &lt;module&gt;
    sys.exit(main())
  File "/usr/local/lib/python3.5/dist-packages/opennmt/bin/main.py", line 120, in main
    runner.train_and_evaluate()
  File "/usr/local/lib/python3.5/dist-packages/opennmt/runner.py", line 141, in train_and_evaluate
    tf.estimator.train_and_evaluate(self._estimator, train_spec, eval_spec)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py", line 430, in train_and_evaluate
    executor.run_local()
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py", line 616, in run_local
    metrics = evaluator.evaluate_and_export()
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py", line 751, in evaluate_and_export
    hooks=self._eval_spec.hooks)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py", line 366, in evaluate
    name=name)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py", line 857, in _evaluate_model
    config=self._session_config)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/evaluation.py", line 212, in _evaluate_once
    session.run(eval_ops, feed_dict)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py", line 538, in run
    run_metadata=run_metadata)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py", line 1012, in run
    run_metadata=run_metadata)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py", line 1088, in run
    return self._sess.run(*args, **kwargs)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py", line 1160, in run
    run_metadata=run_metadata)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py", line 940, in run
    return self._sess.run(*args, **kwargs)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 889, in run
    run_metadata_ptr)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 1317, in _do_run
    options, run_metadata)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 1323, in _do_call
    return fn(*args)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 1302, in _run_fn
    status, run_metadata)
KeyboardInterrupt
root@user-Z270-Phoenix-Gaming:~/opennmt# onmt-main train_and_eval --config config/sample.yml --model_type Transformer
WARNING:tensorflow:You provided a model configuration but a checkpoint already exists. The model configuration must define the same model as the one used for the initial training. However, you can change non structural values like dropout.
2018-05-22 02:48:33.559063: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-05-22 02:48:33.695403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-05-22 02:48:33.695652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1064] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:01:00.0
totalMemory: 10.91GiB freeMemory: 10.57GiB
2018-05-22 02:48:33.695880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1154] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-05-22 02:48:34.126956: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1154] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
INFO:tensorflow:Using config: {'_log_step_count_steps': 100, '_task_id': 0, '_tf_random_seed': None, '_cluster_spec': &lt;tensorflow.python.training.server_lib.ClusterSpec object at 0x7f7a27ce4780&gt;, '_num_ps_replicas': 0, '_task_type': 'worker', '_service': None, '_is_chief': True, '_save_checkpoints_steps': 5000, '_keep_checkpoint_every_n_hours': 10000, '_num_worker_replicas': 1, '_session_config': gpu_options {
}
allow_soft_placement: true
, '_model_dir': 'models/20180521_2', '_master': '', '_save_checkpoints_secs': None, '_save_summary_steps': 100, '_keep_checkpoint_max': 3}
INFO:tensorflow:Running training and evaluation locally (non-distributed).
INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.
WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_impl.py:666: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 132895232 elements. This may consume a large amount of memory.
  "This may consume a large amount of memory." % num_elements)
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Number of trainable parameters: 285471551
2018-05-22 02:48:43.593904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1154] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
INFO:tensorflow:Restoring parameters from models/20180521_2/model.ckpt-45764
INFO:tensorflow:Saving checkpoints for 45765 into models/20180521_2/model.ckpt.
INFO:tensorflow:loss = 34044.8, step = 45765
INFO:tensorflow:global_step/sec: 7.79407
INFO:tensorflow:loss = 76865.0, step = 45865 (12.830 sec)
INFO:tensorflow:global_step/sec: 9.10472
I think this is a bug.
I use OpenNMT-tf in a docker container, the image of the container is Ubuntu:16.04
	</description>
	<comments>
		<comment id='1' author='luozhouyang' date='2018-05-22T07:38:21Z'>
		This sounds like a TensorFlow issue. Which TF version are you using?
If it is old, try using a newer version. Otherwise, please share:

the OpenNMT-tf version
the run configuration
the model configuration

		</comment>
		<comment id='2' author='luozhouyang' date='2018-05-23T02:19:44Z'>
		Tensorflow version is 1.5.0-dev20171120, with GPU support.
OpenNMT-tf version is 1.2.0
I used --model_type Transformer argument, so the model configuration is the default Transformer configuration.
The config/sample.yml confiuration is:
# WARNING:
# Do not use this file as is! It only lists and documents available options
# without caring about their consistency.

# The directory where models and summaries will be saved. It is created if it does not exist.
model_dir: models/20180521_2

data:
  # (required for train_and_eval and train run types).
  train_features_file: data/20180516/src-train.txt
  train_labels_file: data/20180516/tgt-train.txt

  # (required for train_end_eval and eval run types).
  eval_features_file: data/20180516/src-val.txt
  eval_labels_file: data/20180516/tgt-val.txt

  # (optional) Models may require additional resource files (e.g. vocabularies).
  source_words_vocabulary: data/20180516/src-vocab.txt
  target_words_vocabulary: data/20180516/tgt-vocab.txt

# Model and optimization parameters.
params:
  # The optimizer class name in tf.train or tf.contrib.opt.
  optimizer: AdamOptimizer
  # (optional) Additional optimizer parameters as defined in their documentation.
  optimizer_params:
    beta1: 0.8
    beta2: 0.998
  learning_rate: 1.0

  # (optional) Global parameter initialization [-param_init, param_init].
  param_init: 0.1
  # (optional) Maximum gradients norm (default: None).
  clip_gradients: 5.0
  # (optional) Weights regularization penalty (default: null).
  regularization:
    type: l2  # can be "l1", "l2", "l1_l2" (case-insensitive).
    scale: 1e-4  # if using "l1_l2" regularization, this should be a YAML list.
  # (optional) Average loss in the time dimension in addition to the batch dimension (default: False).
  average_loss_in_time: false
  # (optional) The type of learning rate decay (default: None). See:
  #  * https://www.tensorflow.org/versions/master/api_guides/python/train#Decaying_the_learning_rate
  #  * opennmt/utils/decay.py
  # This value may change the semantics of other decay options. See the documentation or the code.
  decay_type: exponential_decay
  # (optional unless decay_type is set) The learning rate decay rate.
  decay_rate: 0.7
  # (optional unless decay_type is set) Decay every this many steps.
  decay_steps: 5000 
  # (optional) The number of training steps that make 1 decay step (default: 1).
  decay_step_duration: 1
  # (optional) If true, the learning rate is decayed in a staircase fashion (default: True).
  staircase: true
  # (optional) After how many steps to start the decay (default: 0).
  start_decay_steps: 50000
  # (optional) Stop decay when this learning rate value is reached (default: 0).
  minimum_learning_rate: 0.0001
  # (optional) Type of scheduled sampling (can be "constant", "linear", "exponential",
  # or "inverse_sigmoid", default: "constant").
  # scheduled_sampling_type: 'constant'
  # (optional) Probability to read directly from the inputs instead of sampling categorically
  # from the output ids (default: 1).
  # scheduled_sampling_read_probability: 1
  # (optional unless scheduled_sampling_type is set) The constant k of the schedule.
  # scheduled_sampling_k: 0
  # (optional) The label smoothing value.
  label_smoothing: 0.1
  # (optional) Width of the beam search (default: 1).
  beam_width: 5
  # (optional) Length penaly weight to apply on hypotheses (default: 0).
  length_penalty: 0.2
  # (optional) Maximum decoding iterations before stopping (default: 250).
  maximum_iterations: 200
  # (optional) Replace unknown target tokens by the original source token with the
  # highest attention (default: false).
  replace_unknown_target: false

# Training options.
train:
  batch_size: 32 

  # (optional) Batch size is the number of "examples" or "tokens" (default: "examples").
  batch_type: examples
  # (optional) Save a checkpoint every this many steps.
  save_checkpoints_steps: 5000
  # (optional) How many checkpoints to keep on disk.
  keep_checkpoint_max: 3
  # (optional) Save summaries every this many steps.
  save_summary_steps: 100
  # (optional) Train for this many steps. If not set, train forever.
  train_steps: 100000
  # (optional) If true, makes a single pass over the training data (default: false).
  single_pass: false
  # (optional) The maximum length of feature sequences during training (default: None).
  maximum_features_length: 20
  # (optional) The maximum length of label sequences during training (default: None).
  maximum_labels_length: 20
  # (optional) The width of the length buckets to select batch candidates from (default: 5).
  bucket_width: 5
  # (optional) The number of threads to use for processing data in parallel (default: 4).
  num_threads: 4
  # (optional) The number of elements from which to sample during shuffling (default: 500000).
  # Set 0 or null to disable shuffling, -1 to match the number of training examples.
  sample_buffer_size: -1 
  # (optional) The number of batches to prefetch asynchronously. If not set, use an
  # automatically tuned value on TensorFlow 1.8+ and 1 on older versions. (default: null).
  prefetch_buffer_size: 1

# (optional) Evaluation options.
eval:
  # (optional) The batch size to use (default: 32).
  batch_size: 32
  # (optional) The number of threads to use for processing data in parallel (default: 1).
  num_threads: 1
  # (optional) The number of batches to prefetch asynchronously (default: 1).
  prefetch_buffer_size: 1
  # (optional) Evaluate every this many seconds (default: 18000).
  eval_delay: 600 
  # (optional) Save evaluation predictions in model_dir/eval/.
  save_eval_predictions: true 
  # (optional) Evalutator or list of evaluators that are called on the saved evaluation predictions.
  # Available evaluators: BLEU, BLEU-detok
  external_evaluators: BLEU
  # (optional) Export model after evaluation (default: true).
  export: true

# (optional) Inference options.
infer:
  # (optional) The batch size to use (default: 1).
  batch_size: 10
  # (optional) The number of threads to use for processing data in parallel (default: 1).
  num_threads: 1
  # (optional) The number of batches to prefetch asynchronously (default: 1).
  prefetch_buffer_size: 1
  # (optional) For compatible models, the number of hypotheses to output (default: 1).
  n_best: 1
		</comment>
		<comment id='3' author='luozhouyang' date='2018-05-23T07:33:22Z'>
		Could you try with a newer version of TensorFlow? We had similar reports and they were all using TensorFlow 1.5.
Also, please note that it is not recommended to use the sample.yml configuration file as indicated in its header. There is a mix of all options that most likely don't work well together.
		</comment>
		<comment id='4' author='luozhouyang' date='2018-05-30T09:06:38Z'>
		Thanks!
		</comment>
		<comment id='5' author='luozhouyang' date='2018-08-03T23:20:42Z'>
		Figured that if models are created in one version and evaluated in another, similar issue happens. I had multiple versions of tensorflow on my machine and the server hence the hanging.
		</comment>
	</comments>
</bug>