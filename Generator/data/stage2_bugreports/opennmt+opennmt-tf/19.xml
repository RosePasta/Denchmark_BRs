<bug id='19' author='homink' open_date='2017-11-09T23:13:58Z' closed_time='2018-10-10T11:59:24Z'>
	<summary>Data loss: corrupted record at xx...</summary>
	<description>
I have randomly encountered " Data loss: corrupted record at xx..." with the LAS model as follows. I tried recreating record file, single/multiple GPU and  single/multiple num_parallel_process_calls but anything doesn't work. Here are data I have used for training. Could you let me know If my configuration is incorrect?
&lt;denchmark-code&gt;[kwon@ssi-dnn-slave-002 data]$ wc -l *
      38571 dev.records
         33 dev.txt
   41002559 train.records
      37416 train.txt
         34 vocab.txt
   41078613 total
[kwon@ssi-dnn-slave-002 data]$ ls -hal
total 14G
drwxr-xr-x 2 kwon domain users   95 Nov  8 15:00 .
drwxr-xr-x 5 kwon domain users  205 Nov  9 14:48 ..
-rw-r--r-- 1 kwon domain users  14M Nov  8 15:00 dev.records
-rw-r--r-- 1 kwon domain users 5.9K Nov  8 15:00 dev.txt
-rw-r--r-- 1 kwon domain users  14G Nov  8 15:00 train.records
-rw-r--r-- 1 kwon domain users 7.3M Nov  8 15:00 train.txt
-rw-r--r-- 1 kwon domain users   81 Nov  8 15:00 vocab.txt
&lt;/denchmark-code&gt;

Here is command I used for training
&lt;denchmark-code&gt;CUDA_VISIBLE_DEVICES=0 python -m bin.main train \
  --model config/models/las_wsj.py \
  --config config/las_wsj.yml
&lt;/denchmark-code&gt;

Here is yml configuration
&lt;denchmark-code&gt;model_dir: /home/kwon/Project/wsj_kaldi_tf/exp

data:
  train_features_file: /home/kwon/Project/wsj_kaldi_tf/data/train.records
  train_labels_file: /home/kwon/Project/wsj_kaldi_tf/data/train.txt
  eval_features_file: /home/kwon/Project/wsj_kaldi_tf/data/dev.records
  eval_labels_file: /home/kwon/Project/wsj_kaldi_tf/data/dev.txt
  input_depth: 120
  target_words_vocabulary: /home/kwon/Project/wsj_kaldi_tf/data/vocab.txt

params:
  optimizer: GradientDescentOptimizer
  learning_rate: 0.2
  clip_gradients: 5.0
  decay_type: exponential_decay
  decay_rate: 1.0
  decay_steps: 10000
  staircase: true
  start_decay_steps: 50000
  scheduled_sampling_type: inverse_sigmoid
  scheduled_sampling_read_probability: 1
  scheduled_sampling_k: 7.5
  beam_width: 5
  maximum_iterations: 200

train:
  batch_size: 20
  save_checkpoints_steps: 40000
  keep_checkpoint_max: 300
  save_summary_steps: 40000
  train_steps: 1122480
  eval_delay: 7200
  save_eval_predictions: false
  maximum_features_length: 2435
  maximum_labels_length: 245
  num_buckets: 5
  num_parallel_process_calls: 1
  buffer_size: 32726
  input_depth: 120

infer:
  batch_size: 3
  num_parallel_process_calls: 1
  buffer_size: 100
  n_best: 1
&lt;/denchmark-code&gt;

Here is the model I used for LAS
&lt;denchmark-code&gt;import tensorflow as tf
import opennmt as onmt

def model():
  return onmt.models.SequenceToSequence(
    source_inputter=onmt.inputters.SequenceRecordInputter(
      input_depth_key="input_depth"),
    target_inputter=onmt.inputters.WordEmbedder(
      vocabulary_file_key="target_words_vocabulary",
      embedding_size=20),
    encoder=onmt.encoders.PyramidalRNNEncoder(
      num_layers=3,
      num_units=256,
      reduction_factor=2,
      cell_class=tf.contrib.rnn.LSTMCell,
      dropout=0),
    decoder=onmt.decoders.MultiAttentionalRNNDecoder(
      num_layers=3,
      num_units=256,
      attention_layers=[0],
      attention_mechanism_class=tf.contrib.seq2seq.LuongAttention,
      cell_class=tf.contrib.rnn.LSTMCell,
      dropout=0,
      residual_connections=False))

&lt;/denchmark-code&gt;

Logs
&lt;denchmark-code&gt;2017-11-09 13:56:04.155749: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 15594 of 32726
2017-11-09 13:56:14.154468: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 20877 of 32726
2017-11-09 13:56:23.832362: W tensorflow/core/framework/op_kernel.cc:1192] Data loss: corrupted record at 9945442474
Traceback (most recent call last):
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;2017-11-08 11:00:07.539005: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, nam
e: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
2017-11-08 11:00:14.879136: W tensorflow/core/framework/op_kernel.cc:1192] Data loss: corrupted record at 4135444529
Traceback (most recent call last):
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;INFO:tensorflow:loss = 2.90062, step = 3939 (249.989 sec)
INFO:tensorflow:loss = 2.91381, step = 4039 (252.683 sec)
INFO:tensorflow:loss = 2.90139, step = 4139 (248.092 sec)
INFO:tensorflow:loss = 2.90898, step = 4239 (250.705 sec)
2017-11-09 01:20:41.776980: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 29790 of 40000
2017-11-09 01:20:44.049085: W tensorflow/core/framework/op_kernel.cc:1192] Data loss: corrupted record at 13966444711
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;INFO:tensorflow:loss = 2.97916, step = 1002 (204.074 sec)
INFO:tensorflow:loss = 2.94589, step = 1102 (208.752 sec)
INFO:tensorflow:loss = 2.93662, step = 1202 (214.285 sec)
2017-11-07 23:27:42.923039: W tensorflow/core/framework/op_kernel.cc:1192] Data loss: corrupted record at 16288775880
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='homink' date='2017-11-10T12:33:02Z'>
		Thanks for the detailed report. The configuration looks good.
I wrote a standalone script to simulate a pass over the TFRecord dataset but was not able to reproduce the issue. If the error only seems to appear randomly, it will be hard to debug. I will check on the TensorFlow side...
Related issue: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/13463&gt;tensorflow/tensorflow#13463&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='homink' date='2018-10-10T11:59:24Z'>
		Let's close this issue. The root cause is corrupted system memory. This does not even look like a TensorFlow issue but I'm asking if they could add a flag to ignore corrupted records during training.
		</comment>
	</comments>
</bug>