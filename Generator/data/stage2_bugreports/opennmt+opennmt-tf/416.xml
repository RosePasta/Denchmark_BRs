<bug id='416' author='atebbifakhr' open_date='2019-04-25T18:00:24Z' closed_time='2019-04-29T10:50:08Z'>
	<summary>Problem in sharing embeddings</summary>
	<description>
I noticed that when I use DualTransformer with sharing parameters between Encoders and also setting sharing level to 'ALL', two sets of embeddings are created with these prefixes:

transformer//w_embs
transformer/shared_embeddings/w_embs

However, it should be one for everything.
	</description>
	<comments>
		<comment id='1' author='atebbifakhr' date='2019-04-25T21:45:50Z'>
		I tried to change
def _get_shared_name(self): return ""
to:
def _get_shared_name(self): return "shared_embeddings"
in ParallelInputter but I'm getting Duplicate Node error.
Do you know a quick fix for it?
		</comment>
		<comment id='2' author='atebbifakhr' date='2019-04-26T22:53:47Z'>
		The best way I found is reimplementing build() function in ExampleInputter.
		</comment>
		<comment id='3' author='atebbifakhr' date='2019-04-28T21:00:43Z'>
		Thanks for reporting. Do you have a working fix that you can share?
		</comment>
		<comment id='4' author='atebbifakhr' date='2019-04-29T09:46:01Z'>
		I sent a pull request.
It works for DualTransformer, but I'm not sure for other use cases.
		</comment>
		<comment id='5' author='atebbifakhr' date='2019-04-29T10:30:07Z'>
		Thanks. This indeed needs a more general fix: the issue is that the built flag is not properly set for MultiInputter classes and weights are recreated when the layer is first called. I will push an appropriate fix to that.
		</comment>
	</comments>
</bug>