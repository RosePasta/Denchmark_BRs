<bug id='83' author='rragrawal' open_date='2018-03-12T13:37:28Z' closed_time='2018-03-15T22:39:30Z'>
	<summary>Transformer model produces "." as output</summary>
	<description>
Hello,
I've been trying to train a transformer model using the latest code version.
I tried using tensorflow 1.6 (had to convert the code to python3 for that) as well as tensorflow 1.5.
However, I get only "." as the output on testing the model using the toy-ende dataset provided in the distribution. I use the transformer.py and the transformer1gpu.yml as the config files.
On the other hand, training a vanilla RNN model (with nmt_small.py and opennmtdefaults.yml) works fine and produces legit outputs. Any lead on this?
	</description>
	<comments>
		<comment id='1' author='rragrawal' date='2018-03-12T13:43:37Z'>
		Hello,
Sorry for that, I introduced a regression in &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-tf/commit/f59f011a8cc84b025b8d260503968cf33f650bdc&gt;f59f011&lt;/denchmark-link&gt;
 that I just fixed a few minutes ago in &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-tf/commit/40ce318368a275f98f95662274f1f5f0948a44e5&gt;40ce318&lt;/denchmark-link&gt;
.
Could you update the code and restart the training?
Thanks.
		</comment>
		<comment id='2' author='rragrawal' date='2018-03-12T13:51:36Z'>
		Thanks for the response. I shall restart the training. Before that, could you let me know if there's a config file for Transformer with multi-gpu training? I see only one for single gpu.
		</comment>
		<comment id='3' author='rragrawal' date='2018-03-12T13:58:00Z'>
		You can use the same file but apply the recommendation of line 10:



OpenNMT-tf/config/transformer_1gpu.yml


        Lines 10 to 11
      in
      047ac49






 # Divide this value by the total number of GPUs used. 



 decay_step_duration: 8 # 1 decay step is 8 training steps. 





then set the number of GPUs with the option --num_gpus.
Also see the WMT English-German training scripts: &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-tf/tree/master/scripts/wmt&gt;https://github.com/OpenNMT/OpenNMT-tf/tree/master/scripts/wmt&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='rragrawal' date='2018-03-12T15:01:55Z'>
		This is not relevant to the current issue (you can use the &lt;denchmark-link:https://gitter.im/OpenNMT/OpenNMT-tf&gt;Gitter channel&lt;/denchmark-link&gt;
 for such questions). To answer it anyway, the code on  should be compatible with all TensorFlow versions superior or equal to 1.4. If not, open an issue!
		</comment>
		<comment id='5' author='rragrawal' date='2018-03-12T15:16:27Z'>
		Thanks! Closing this issue for now.
		</comment>
		<comment id='6' author='rragrawal' date='2018-03-12T21:00:20Z'>
		Update : It still doesn't work with the updated code. I get the following error on testing :
INFO:tensorflow:Restoring parameters from transformerEnIt/model.ckpt-20001
terminate called after throwing an instance of 'std::bad_alloc'
what():  std::bad_alloc
Aborted
		</comment>
		<comment id='7' author='rragrawal' date='2018-03-12T21:28:51Z'>
		Looks like there is not enough memory. Can you check?
		</comment>
		<comment id='8' author='rragrawal' date='2018-03-12T21:55:47Z'>
		Well I ran on 12 GB GPU memory on a not so large dataset. It hangs up for quite some time on testing, after which it throws the above mentioned error.
I'm testing again on toy-ende with higher GPU memory just for confirming.
		</comment>
		<comment id='9' author='rragrawal' date='2018-03-12T22:46:22Z'>
		Reproduced on toy-ende. Freezes on testing (even on 36 GB GPU memory) - both for single gpu as well as multi gpu. Using standard config files transformer_1gpu and transformer.py without any modification.
		</comment>
		<comment id='10' author='rragrawal' date='2018-03-13T09:25:21Z'>
		What is the TensorFlow you are using? You mentioned both 1.5 and 1.6 in the first comment. It seems others encounter such issues with 1.5.
		</comment>
		<comment id='11' author='rragrawal' date='2018-03-13T10:02:26Z'>
		I am using 1.5.
		</comment>
		<comment id='12' author='rragrawal' date='2018-03-13T10:12:59Z'>
		Can you try with 1.6 instead?
		</comment>
		<comment id='13' author='rragrawal' date='2018-03-15T22:39:30Z'>
		Works with 1.6, thanks. Closing the issue.
		</comment>
		<comment id='14' author='rragrawal' date='2018-06-28T09:35:02Z'>
		Can you please explain why 1.5 is not working?
I am training it on ML-engine with the same configuration with 4 gpu and after submitting the job, it's throwing this error.
The replica master 0 ran out-of-memory and exited with a non-zero status of 9(SIGKILL). To find out more about why your job exited please check the logs: &lt;denchmark-link:https://console.cloud.google.com/logs/viewer?project=1002083926433&amp;resource=ml_job%2Fjob_id%2Fnmt_v1_27&amp;advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%22nmt_v1_27%22&gt;https://console.cloud.google.com/logs/viewer?project=1002083926433&amp;resource=ml_job%2Fjob_id%2Fnmt_v1_27&amp;advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%22nmt_v1_27%22&lt;/denchmark-link&gt;

		</comment>
		<comment id='15' author='rragrawal' date='2018-06-28T09:40:49Z'>
		Out of memory is a common issue but most likely not related to TensorFlow 1.5 in particular. Please open a new issue if you feel like there is bug. Otherwise, you should probably reduce the training batch size, or train a smaller model to fit in the GPU memory available to you.
		</comment>
	</comments>
</bug>