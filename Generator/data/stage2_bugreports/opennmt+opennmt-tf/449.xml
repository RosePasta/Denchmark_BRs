<bug id='449' author='lockder' open_date='2019-05-31T15:09:02Z' closed_time='2019-06-11T08:50:22Z'>
	<summary>Crash using distributed async training on the same machine</summary>
	<description>
until now I still was using opennmt-tf 1.19.2 with tensorflow 1.13.1
now I tried to upgrade to the last version 1.23.0 but when I tried to run it i got a crash on the training
Then I tried to do it on 1.20.0 I got crash on imports so I tried in 1.21.0 I got the same crash while using distribution mode that worked on 1.19.2
I forgot to add I'm using 8 gpu machine on amazon, each gpu has a training async
Here there is the configuration and the crash:
&lt;denchmark-code&gt;INFO:tensorflow:TF_CONFIG environment variable: {'cluster': {'chief': ['localhost:2220'], 'worker': ['localhost:2222', 'localhost:2223', 'localhost:2224', 'localhost:2225', 'localhost:2226', 'localhost:2227'], 'ps': ['localhost:2221']}, 'task': {'type': 'chief', 'index': 0}}
2019-05-31 17:02:07.913459: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
&lt;/denchmark-code&gt;

I get pass this moment:
&lt;denchmark-code&gt;INFO:tensorflow:Number of trainable parameters: 10975738
&lt;/denchmark-code&gt;

then this happens:
&lt;denchmark-code&gt;INFO:tensorflow:An error was raised while a session was being created. This may be due to a preemption of a connected worker or parameter server. A new session will be created. This error may also occur due to a gRPC failure caused by high memory or network bandwidth usage in the parameter servers. If this error occurs repeatedly, try increasing the number of parameter servers assigned to the job. Error: The same RecvTensor (GrpcWorker) request was received twice. step_id: 114096320461246469 rendezvous_key: "/job:ps/replica:0/task:0/device:GPU:0;bbaad744e3083143;/job:worker/replica:0/task:0/device:CPU:0;edge_233_report_uninitialized_variables_1/IsVariableInitialized_5;0:0" request_id: 8264937404371139865
	 [[node report_uninitialized_variables_1/IsVariableInitialized_5 (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:1403) ]]

Caused by op 'report_uninitialized_variables_1/IsVariableInitialized_5', defined at:
File "/usr/local/lib/python3.6/dist-packages/opennmt/bin/main.py", line 172, in main
    runner.train_and_evaluate(checkpoint_path=args.checkpoint_path)
  File "/usr/local/lib/python3.6/dist-packages/opennmt/runner.py", line 301, in train_and_evaluate
    result = tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py", line 471, in train_and_evaluate
    return executor.run()
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py", line 638, in run
    getattr(self, task_to_run)()
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py", line 648, in run_worker
    return self._start_distributed_training()
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py", line 789, in _start_distributed_training
    saving_listeners=saving_listeners)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py", line 358, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py", line 1124, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py", line 1158, in _train_model_default
    saving_listeners)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py", line 1403, in _train_with_estimator_spec
    log_step_count_steps=log_step_count_steps) as mon_sess:
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py", line 466, in MonitoredTrainingSession
    stop_grace_period_secs=stop_grace_period_secs)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py", line 934, in __init__
    stop_grace_period_secs=stop_grace_period_secs)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py", line 648, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py", line 1122, in __init__
    _WrappedSession.__init__(self, self._create_session())
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py", line 1127, in _create_session
    return self._sess_creator.create_session()
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py", line 805, in create_session
    self.tf_sess = self._session_creator.create_session()
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py", line 610, in create_session
    self._scaffold.finalize()
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py", line 206, in finalize
    default_ready_for_local_init_op)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py", line 269, in get_or_default
    op = default_constructor()
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py", line 200, in default_ready_for_local_init_op
    variables.global_variables()),
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py", line 193, in wrapped
    return _add_should_use_warning(fn(*args, **kwargs))
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py", line 2999, in report_uninitialized_variables
    init_vars = [state_ops.is_variable_initialized(v) for v in var_list]
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py", line 2999, in &lt;listcomp&gt;
    init_vars = [state_ops.is_variable_initialized(v) for v in var_list]
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/state_ops.py", line 131, in is_variable_initialized
    return gen_state_ops.is_variable_initialized(ref=ref, name=name)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_state_ops.py", line 257, in is_variable_initialized
    "IsVariableInitialized", ref=ref, name=name)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py", line 788, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py", line 3300, in create_op
    op_def=op_def)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py", line 1801, in __init__
    self._traceback = tf_stack.extract_stack()

AbortedError (see above for traceback): The same RecvTensor (GrpcWorker) request was received twice. step_id: 114096320461246469 rendezvous_key: "/job:ps/replica:0/task:0/device:GPU:0;bbaad744e3083143;/job:worker/replica:0/task:0/device:CPU:0;edge_233_report_uninitialized_variables_1/IsVariableInitialized_5;0:0" request_id: 8264937404371139865
	 [[node report_uninitialized_variables_1/IsVariableInitialized_5 (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:1403) ]]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='lockder' date='2019-06-03T12:22:37Z'>
		where you able to reproduce the issue?
		</comment>
		<comment id='2' author='lockder' date='2019-06-03T12:38:40Z'>
		No I did not. What is the model?
		</comment>
		<comment id='3' author='lockder' date='2019-06-03T13:22:08Z'>
		the features are just the input words (glove fixed + trainable) + pos tag + custom sequence record
I also tried with another classification model and its happening the same thing
&lt;denchmark-code&gt;def model():
    return SequenceTagger(inputter=ParallelInputter([MixedInputter([WordEmbedder(vocabulary_file_key="source_words_vocabulary",
                                                                            embedding_size=None,
                                                                            embedding_file_key='src_embedding_words_file',
                                                                            embedding_file_with_header=False,
                                                                            tokenizer=onmt.tokenizers.SpaceTokenizer(),
                                                                            trainable=False,
                                                                            dropout=0.0),
                                                               WordEmbedder(vocabulary_file_key="source_words_vocabulary",
                                                                            embedding_size=100,
                                                                            tokenizer=onmt.tokenizers.SpaceTokenizer(),
                                                                            trainable=True,
                                                                            dropout=0.0)
                                                               ],
                                                 reducer=ConcatReducer(),
                                                 dropout=0.3),
                                   WordEmbedder(vocabulary_file_key="feature_1_vocabulary",  # POS_TAG
                                                embedding_size=100,
                                                tokenizer=onmt.tokenizers.SpaceTokenizer(),
                                                trainable=True,
                                                dropout=0.2),
                                                SequenceRecordInputter() # DB_WORD_EMB
                                   ],
                                  reducer=ConcatReducer()),
                     encoder=BidirectionalRNNEncoder(num_units=512,num_layers=3, reducer=ConcatReducer(),cell_class=tf.nn.rnn_cell.LSTMCell, dropout=0.3),
                     labels_vocabulary_file_key='target_words_vocabulary',
                     tagging_scheme='bioes',
                     crf_decoding=True,
                     daisy_chain_variables=False)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='lockder' date='2019-06-03T13:28:20Z'>
		I run on local for each gpu one process of the cluster
		</comment>
		<comment id='5' author='lockder' date='2019-06-03T13:55:26Z'>
		I reduced everything to:
&lt;denchmark-code&gt;def model():
    return SequenceTagger(inputter=WordEmbedder(vocabulary_file_key="source_words_vocabulary",
                                                                            embedding_size=100,
                                                                            tokenizer=onmt.tokenizers.SpaceTokenizer(),
                                                                            trainable=True,
                                                                            dropout=0.1),
                     encoder=BidirectionalRNNEncoder(num_units=512,num_layers=1, reducer=ConcatReducer(),cell_class=tf.nn.rnn_cell.LSTMCell, dropout=0.3),
                     labels_vocabulary_file_key='target_words_vocabulary',
                     tagging_scheme='bio',
                     crf_decoding=True,
                     daisy_chain_variables=False)

&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;INFO:tensorflow:Using parameters:
data:
  eval_features_file: /home/shurtado/NaturalLanguageRecognition/NaturalLanguageManager/src/../data/shared/src_test.txt
  eval_labels_file: /home/shurtado/NaturalLanguageRecognition/NaturalLanguageManager/src/../data/shared/ner_test.txt
  source_words_vocabulary: /home/shurtado/NaturalLanguageRecognition/NaturalLanguageManager/src/../data/shared/src_word_vocab.txt
  target_words_vocabulary: /home/shurtado/NaturalLanguageRecognition/NaturalLanguageManager/src/../data/shared/ner_word_vocab.txt
  train_features_file: /home/shurtado/NaturalLanguageRecognition/NaturalLanguageManager/src/../data/shared/src_train.txt
  train_labels_file: /home/shurtado/NaturalLanguageRecognition/NaturalLanguageManager/src/../data/shared/ner_train.txt
eval:
  batch_size: 32
  eval_delay: 500
  exporters: null
  num_threads: 4
  prefetch_buffer_size: 4
  save_eval_predictions: true
infer:
  batch_size: 1
  bucket_width: null
  n_best: 1
  num_threads: 1
  prefetch_buffer_size: 1
  with_scores: true
model_dir: /home/shurtado/NaturalLanguageRecognition/NaturalLanguageManager/src/tagger_entityBio/model/model
params:
  clip_gradients: 5.0
  decay_rate: 512
  decay_steps: 1000
  decay_type: noam_decay
  learning_rate: 1.5
  optimizer: AdamOptimizer
score:
  batch_size: 64
train:
  average_last_checkpoints: 8
  batch_size: 32
  batch_type: examples
  bucket_width: 5
  keep_checkpoint_max: 8
  num_threads: 4
  prefetch_buffer_size: 2
  sample_buffer_size: 500000
  save_checkpoints_steps: 1000
  save_summary_steps: 1000
  single_pass: false
  train_steps: 30000

INFO:tensorflow:TF_CONFIG environment variable: {'cluster': {'chief': ['localhost:2220'], 'worker': ['localhost:2222', 'localhost:2223', 'localhost:2224', 'localhost:2225', 'localhost:2226', 'localhost:2227'], 'ps': ['localhost:2221']}, 'task': {'type': 'worker', 'index': 0}}
&lt;/denchmark-code&gt;

its always saying the same thing
		</comment>
		<comment id='6' author='lockder' date='2019-06-03T15:50:54Z'>
		if I remove  CUDA_VISIBLE_DEVICES and train as sync mode , it will work. its only an issue with async code
		</comment>
		<comment id='7' author='lockder' date='2019-06-04T13:58:21Z'>
		I don't reproduce this.

Does the error occur in the chief or worker process?
Do you start the instances in a specific order?

		</comment>
		<comment id='8' author='lockder' date='2019-06-04T14:22:35Z'>
		what i do is launch one process for each gpu setting the enviroment variables to allow that process see only that gpu.
I start first the chief, then the evaluator and ps and finally the workers
I forgot to add I do train_eval
		</comment>
		<comment id='9' author='lockder' date='2019-06-04T14:32:24Z'>
		I'm using for start the command I use is using all this parameters --model --config --run_dir --data_dir --export_dir_base --gpu_allow_growth --task_type --task_index --seed 2
		</comment>
		<comment id='10' author='lockder' date='2019-06-04T16:44:01Z'>
		
I start first the chief, then the evaluator and ps and finally the workers

In which one does the error happen? And is it a fresh training or a retraining?
I still can't reproduce using the configuration in &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-tf/issues/449#issuecomment-498266925&gt;#449 (comment)&lt;/denchmark-link&gt;
. It would help if you can further narrow it down.
		</comment>
		<comment id='11' author='lockder' date='2019-06-05T08:32:58Z'>
		I reduced everything just a chief + 1 worker + 1 ps
I send you all the log, I believe its the worker because if I let the process work, I see some kind of traning with the crash from ps talking with the worker
&lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-tf/files/3256238/crash.opennmt.async.txt&gt;crash opennmt async.txt&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='lockder' date='2019-06-05T08:52:33Z'>
		And it is working fine if you rollback to 1.19.2? Maybe you can try a git bisect in the repository to find the offending commit.
		</comment>
		<comment id='13' author='lockder' date='2019-06-05T09:14:36Z'>
		yep if I roll back to 1.19.2 works, I'm not sure how to do this bit bisect.
Also I installed the opennmt with pip I guess I should upload manually the code?
		</comment>
		<comment id='14' author='lockder' date='2019-06-05T09:16:34Z'>
		I tried to use 1.20.0 but It was complaining about importing some classes, let me send you the error, so if I fix it the imports I can try if in 1.20.0 was still working
I forgot to add , its always a clean fresh start, I delete the model folder each time I start the training
		</comment>
		<comment id='15' author='lockder' date='2019-06-05T09:22:18Z'>
		daisy_chain_variables=False)
File "/usr/local/lib/python3.6/dist-packages/opennmt/models/sequence_classifier.py", line 42, in init
labels_inputter=ClassInputter(labels_vocabulary_file_key),
File "/usr/local/lib/python3.6/dist-packages/opennmt/models/sequence_classifier.py", line 103, in init
super(TagsInputter, self).init(
NameError: name 'TagsInputter' is not defined
this is the error on 1.20.0 but If I try train with SequenceTagger it works fine, so the async issue did not start in 1.20.0 I will try to upload each version using SequenceTagger to try find with version did stop working
		</comment>
		<comment id='16' author='lockder' date='2019-06-05T09:26:54Z'>
		okey something did change between 1.20.1 ( this version the SequenceTagger is still working in async mode) but when I update to 1.21.0 then the crash did appear
		</comment>
		<comment id='17' author='lockder' date='2019-06-05T09:28:57Z'>
		I have installed lib cuda 10
		</comment>
		<comment id='18' author='lockder' date='2019-06-05T10:09:31Z'>
		I have been uploading commit to commit to my machine so I was able to run my app with opennmt commit by commit and I found this:
the commit 27 feb 11:02 will show the crash
the commit 27 feb 10:40 working is working fine
this is my model.py file
&lt;denchmark-code&gt;import opennmt as onmt
import tensorflow as tf

from opennmt.inputters import WordEmbedder
from opennmt.layers.reducer import ConcatReducer
from opennmt.encoders import BidirectionalRNNEncoder

def model():
    return onmt.models.sequence_tagger.SequenceTagger(inputter=WordEmbedder(vocabulary_file_key="source_words_vocabulary",
                                                                            embedding_size=100,
                                                                            tokenizer=onmt.tokenizers.SpaceTokenizer(),
                                                                            trainable=True,
                                                                            dropout=0.0),
                     encoder=BidirectionalRNNEncoder(num_units=256,num_layers=1, reducer=ConcatReducer(),cell_class=tf.nn.rnn_cell.LSTMCell, dropout=0.3),
                     labels_vocabulary_file_key='target_words_vocabulary',
                     tagging_scheme='bio',
                     crf_decoding=True,
                     daisy_chain_variables=False)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='19' author='lockder' date='2019-06-05T10:36:08Z'>
		The commit &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-tf/commit/034c0e52af29b20ea0117c40201d717a6fd5bdaa&gt;034c0e5&lt;/denchmark-link&gt;
 changes a layer you are not using. So it's this one &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-tf/commit/199ef519d661a0164747043a67f0cc6c35b7c121&gt;199ef51&lt;/denchmark-link&gt;
, right?
		</comment>
		<comment id='20' author='lockder' date='2019-06-05T10:51:12Z'>
		its quite weird because I'm not using the position encoder it on that model but I may load it from imports in another model because everything is inside project as I use opennmt as lib.
&lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-tf/commit/199ef519d661a0164747043a67f0cc6c35b7c121&gt;199ef51&lt;/denchmark-link&gt;
 &lt;--- at this commit everything was fine
&lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-tf/commit/034c0e52af29b20ea0117c40201d717a6fd5bdaa&gt;034c0e5&lt;/denchmark-link&gt;
 &lt;--- when I move the lib to this commit then I get the error
		</comment>
		<comment id='21' author='lockder' date='2019-06-06T08:35:30Z'>
		any luck on the issue? How more can I help you?
		</comment>
		<comment id='22' author='lockder' date='2019-06-06T08:58:08Z'>
		I'm still not reproducing the error. Are you facing memory issues during the training?
		</comment>
		<comment id='23' author='lockder' date='2019-06-06T09:33:54Z'>
		nop I have 10gb gpu ram its tesla cards its a linux OS, I will list you the pip list may help
		</comment>
		<comment id='24' author='lockder' date='2019-06-06T09:35:06Z'>
		its an Ubuntu 16.04.6 LTS (GNU/Linux 4.4.0-1075-aws x86_64)
maybe protobuf version changes? or something used by google for the distribution
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

absl-py                        0.7.1
astor                          0.7.1
awscli                         1.16.169
beautifulsoup4                 4.4.1
blinker                        1.3
boto                           2.49.0
boto3                          1.9.130
botocore                       1.12.159
bz2file                        0.98
chardet                        2.3.0
cloud-init                     18.5
colorama                       0.3.9
command-not-found              0.3
configobj                      5.0.6
cryptography                   1.2.3
cycler                         0.10.0
defer                          1.0.6
docutils                       0.14
elasticsearch                  6.3.1
gast                           0.2.2
gensim                         3.7.1
grpcio                         1.19.0
grpcio-tools                   1.19.0
h5py                           2.9.0
hibagent                       1.0.1
html5lib                       0.999
idna                           2.0
Jinja2                         2.8
jmespath                       0.9.4
jsonpatch                      1.23
jsonpointer                    1.9
Keras-Applications             1.0.7
Keras-Preprocessing            1.0.9
kiwisolver                     1.0.1
language-selector              0.1
lxml                           3.5.0
Markdown                       3.1
MarkupSafe                     0.23
matplotlib                     3.0.3
mock                           2.0.0
nltk                           3.4
numpy                          1.16.2
oauthlib                       1.0.3
OpenNMT-tf                     1.23.0
pandas                         0.24.2
pbr                            5.1.3
pip                            19.0.3
prettytable                    0.7.2
protobuf                       3.7.1
psutil                         5.6.1
pyasn1                         0.1.9
pycups                         1.9.73
pycurl                         7.43.0
pygobject                      3.20.0
PyJWT                          1.3.0
pyonmttok                      1.11.0
pyparsing                      2.4.0
pyserial                       3.0.1
python-apt                     1.1.0b1+ubuntu0.16.4.2
python-dateutil                2.8.0
python-debian                  0.1.27
python-systemd                 231
pytz                           2018.9
pyxdg                          0.25
PyYAML                         5.1
requests                       2.9.1
requests-aws4auth              0.9
rouge                          0.3.1
rsa                            3.4.2
s3transfer                     0.2.0
sacrebleu                      1.3.1
scikit-learn                   0.20.3
scipy                          1.2.1
screen-resolution-extra        0.0.0
setuptools                     20.7.0
singledispatch                 3.4.0.3
six                            1.10.0
sklearn                        0.0
smart-open                     1.8.1
ssh-import-id                  5.5
system-service                 0.3
tensorboard                    1.13.1
tensorflow-estimator           1.13.0
tensorflow-gpu                 1.13.1
tensorflow-serving-api-python3 1.8.0
termcolor                      1.1.0
typing                         3.6.6
ufw                            0.35
unattended-upgrades            0.1
urllib3                        1.24.1
virtualenv                     16.4.3
Werkzeug                       0.15.2
wheel                          0.29.0
		</comment>
		<comment id='25' author='lockder' date='2019-06-06T14:35:56Z'>
		y removed all installed pips and reinstalled and is still happening the same thing :_(((
so sad
Package                        Version
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

absl-py                        0.7.1
astor                          0.8.0
awscli                         1.16.172
blinker                        1.3
boto                           2.49.0
boto3                          1.9.162
botocore                       1.12.162
bz2file                        0.98
certifi                        2019.3.9
chardet                        3.0.4
colorama                       0.3.9
command-not-found              0.3
cycler                         0.10.0
dill                           0.2.9
docutils                       0.14
elasticsearch                  7.0.2
gast                           0.2.2
gensim                         3.7.3
grpcio                         1.21.1
grpcio-tools                   1.21.1
h5py                           2.9.0
idna                           2.8
jmespath                       0.9.4
joblib                         0.13.2
Keras-Applications             1.0.8
Keras-Preprocessing            1.1.0
kiwisolver                     1.1.0
langdetect                     1.0.7
Markdown                       3.1.1
matplotlib                     3.1.0
mock                           3.0.5
nltk                           3.4.2
numpy                          1.16.4
OpenNMT-tf                     1.23.0
pandas                         0.24.2
pip                            19.0.3
protobuf                       3.8.0
psutil                         5.6.2
pyasn1                         0.4.5
pycups                         1.9.73
pycurl                         7.43.0
pygobject                      3.20.0
pyonmttok                      1.12.1
pyparsing                      2.4.0
pyserial                       3.0.1
python-apt                     1.1.0b1+ubuntu0.16.4.2
python-dateutil                2.8.0
python-debian                  0.1.27
python-systemd                 231
pytz                           2019.1
pyxdg                          0.25
PyYAML                         5.1
rake-nltk                      1.0.4
requests                       2.22.0
requests-aws4auth              0.9
rouge                          0.3.1
rsa                            3.4.2
s3transfer                     0.2.1
sacrebleu                      1.3.4
scikit-learn                   0.21.2
scipy                          1.3.0
screen-resolution-extra        0.0.0
setuptools                     41.0.1
six                            1.10.0
sklearn                        0.0
smart-open                     1.8.1
ssh-import-id                  5.5
system-service                 0.3
tensorboard                    1.13.1
tensorflow                     1.13.1
tensorflow-estimator           1.13.0
tensorflow-gpu                 1.13.1
tensorflow-serving-api-python3 1.8.0
termcolor                      1.1.0
typing                         3.6.6
ufw                            0.35
unattended-upgrades            0.1
urllib3                        1.24.1
virtualenv                     16.4.3
Werkzeug                       0.15.4
wheel                          0.33.4
xkit                           0.0.0
		</comment>
		<comment id='26' author='lockder' date='2019-06-06T14:45:55Z'>
		more info here.. I just tried to use --horovod command
I installed openmpi then horovod with pip and I still receive the same crash ps with the worker :(
		</comment>
		<comment id='27' author='lockder' date='2019-06-07T12:11:24Z'>
		could do you tell me how are you trying the issue? I'm trying to find the difference why its happening to me and not to you
		</comment>
		<comment id='28' author='lockder' date='2019-06-07T15:05:51Z'>
		ok more info here. I found if I open N terminal sessions by ssh and manually launching the cluster it will work.
So is some kind of incompatibility with the current way to launch opennmt.
I'm using it as lib and what I do is import -&gt; from opennmt.bin.main import main inside my python file
then with multiprocessing I open one process for each element in the cluster.
then I add os.environ['CUDA_VISIBLE_DEVICES'] and add to sys.argv the command inside each process. this way it was working but something has changed and doesn't work anymore
		</comment>
		<comment id='29' author='lockder' date='2019-06-07T15:12:21Z'>
		
I found if I open N terminal sessions by ssh and manually launching the cluster it will work.

Yes, I tested like that.

from opennmt.bin.main import main inside my python file then with multiprocessing I open one process for each element in the cluster.

Do you import OpenNMT in the top level scope or inside each process? Can you try importing inside each process if that's not already done? (TensorFlow is not fork-safe).
		</comment>
		<comment id='30' author='lockder' date='2019-06-07T15:14:46Z'>
		I was doing it in the top but I changed to do it inside each process, but since everything is inside the same python file maybe there are some issues, so I'm trying to move some code to a new file
		</comment>
		<comment id='31' author='lockder' date='2019-06-07T16:08:02Z'>
		finally is working but not the way I would like to do it.
I had to find the path installed of opennmt with
&lt;denchmark-code&gt; from importlib.machinery import PathFinder

        module = PathFinder().find_module("opennmt")
&lt;/denchmark-code&gt;

then run each command to subprocess.Popen(command, shell=True, stdout=subprocess.PIPE).communicate()
		</comment>
	</comments>
</bug>