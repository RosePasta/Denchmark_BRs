<bug id='542' author='guillaumekln' open_date='2019-11-07T10:39:42Z' closed_time='2019-11-07T12:43:45Z'>
	<summary>experimental_distribute_dataset does not always split the batches</summary>
	<description>
In &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-tf/commit/4f8189b7a84c23f1fdd33db67a1e25579065d042&gt;4f8189b&lt;/denchmark-link&gt;
, we scaled the batch size by the number of devices based on  documentation:

We will assume that the input dataset is batched by the global batch size

However, the method does not always split the input batches. This was at least found when using batch_type: tokens even though the first dimension is divisible by the number of replicas.
	</description>
	<comments>
	</comments>
</bug>