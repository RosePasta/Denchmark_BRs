<bug id='118' author='atebbifakhr' open_date='2018-05-07T12:56:38Z' closed_time='2018-06-09T13:20:47Z'>
	<summary>Using dynamic_decode_and_search in training.</summary>
	<description>
Hi,
I want to use output (log_probs) of dynamic_decode_and_search function of decoder during training, but I'm getting "No gradients provided for any variable". It seems there is no connection between output of beam_search and variables. Is there any possibility to resolve this issue?
	</description>
	<comments>
		<comment id='1' author='atebbifakhr' date='2018-05-12T15:22:29Z'>
		Hello,
That's an interesting use case I haven't tried yet.
How did you define the loss? This error usually happens when there is no paths between a part of the computation graph and the loss function.
Also, which decoder class did you use? I would like to also test it when I have some time. Thanks.
		</comment>
		<comment id='2' author='atebbifakhr' date='2018-05-16T16:34:53Z'>
		Let's say I want to maximize the probability of the best hypothesis through the beam search. I'm using the SelfAttentionDecoder, but I don't think it's related to the decoder class. I think in beam search a non-differentiable function is being used. Thanks.
		</comment>
		<comment id='3' author='atebbifakhr' date='2018-05-17T16:40:23Z'>
		In line 557 of &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-tf/blob/master/opennmt/utils/beam_search.py&gt;beam_search.py&lt;/denchmark-link&gt;
, I need to change  to .
		</comment>
		<comment id='4' author='atebbifakhr' date='2018-05-22T13:11:00Z'>
		Nice catch! Let me know if you had success using the beam search during training. If yes, I will update the code to properly set back_prop=True in training mode.
		</comment>
	</comments>
</bug>