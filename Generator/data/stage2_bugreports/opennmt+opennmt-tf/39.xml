<bug id='39' author='gsoul' open_date='2017-12-09T22:15:27Z' closed_time='2017-12-20T09:11:00Z'>
	<summary>Vocabulary builder is not compatible with Lua tokenizer</summary>
	<description>
When running Lua tokenizer in BPE mode (from OpenNMT), the default delimiter is some non-printable character.
And later when I try to build vocabulary from this tokenized text, I get:
&lt;denchmark-code&gt;File "opennmt/utils/vocab.py", line 43, in add_from_text
    line = line.decode("utf-8").strip()
  File "/home/soul/anaconda2/lib/python2.7/encodings/utf_8.py", line 16, in decode
    return codecs.utf_8_decode(input, errors, True)
UnicodeDecodeError: 'utf8' codec can't decode bytes in position 1-2: invalid continuation byte
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;line = line.decode("utf-8", "replace").strip()
&lt;/denchmark-code&gt;

solves the issue
	</description>
	<comments>
		<comment id='1' author='gsoul' date='2017-12-10T12:10:49Z'>
		What are the tokenization commands you ran on the Lua side?
		</comment>
		<comment id='2' author='gsoul' date='2017-12-10T13:37:24Z'>
		&lt;denchmark-code&gt;cat [pathto]/enfr/giga-fren.release2.fixed.en [pathto]/enfr/giga-fren.release2.shuf.fr | th tools/learn_bpe.lua -size 32000 -save_bpe giga_codes.txt

nohup th tools/tokenize.lua -bpe_model giga_codes.txt -nparallel 6 -joiner_annotate -mode aggressive -segment_numbers &lt; [pathto]/enfr/giga-fren.release2.shuf.en &gt; [pathto]/enfr/giga-fren.release2.bpe.shuf.en &amp; 

nohup th tools/tokenize.lua -bpe_model giga_codes.txt -nparallel 6 -joiner_annotate -mode aggressive -segment_numbers &lt; [pathto]/enfr/giga-fren.release2.shuf.fr &gt; [pathto]/enfr/giga-fren.release2.bpe.shuf.fr &amp;

cat ./enfr/giga-fren.release2.bpe.shuf.en ./enfr/giga-fren.release2.bpe.shuf.fr &gt; data/enfr/tmp.txt

python -m bin.build_vocab --save_vocab data/enfr/src-bpe.txt data/enfr/tmp.txt --size 32000
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='gsoul' date='2017-12-20T09:11:00Z'>
		The issue most likely happened on the Lua side. There might be issues with multi-threaded tokenization.
		</comment>
	</comments>
</bug>