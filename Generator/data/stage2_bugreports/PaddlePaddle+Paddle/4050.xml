<bug id='4050' author='reyoung' open_date='2017-09-12T17:31:55Z' closed_time='2018-05-16T03:26:18Z'>
	<summary>Paddle do not fit PEP 513 `manylinux1` standard</summary>
	<description>
Currently, we rename Paddle's wheel package into manylinux1. It is not good because our binary does not fit manylinux1 standard.
In &lt;denchmark-link:https://www.python.org/dev/peps/pep-0513/&gt;manylinux1&lt;/denchmark-link&gt;
 standard, the dependencies are:
&lt;denchmark-code&gt;GLIBC &lt;= 2.5
CXXABI &lt;= 3.4.8
GLIBCXX &lt;= 3.4.9
GCC &lt;= 4.2.0
&lt;/denchmark-code&gt;

And we should build our wheel package in CentOS 5.
But we are using C++ 11, which depends on higher GLIBCXX. The current depends is GLIBCXX==3.4.21.
The pypa give a docker image to build  standard wheel package in &lt;denchmark-link:https://github.com/pypa/manylinux/blob/master/docker/Dockerfile-x86_64&gt;here&lt;/denchmark-link&gt;
. We should use that docker image to build our wheel package.
	</description>
	<comments>
		<comment id='1' author='reyoung' date='2017-11-02T05:06:50Z'>
		Indeed, supporting manylinux1 is reaaaally needed, but I doubt that we should make a lot of changes to our build system. I'll give it a try.
		</comment>
		<comment id='2' author='reyoung' date='2017-11-02T12:41:42Z'>
		Found some &lt;denchmark-link:https://github.com/numenta/manylinux&gt;work&lt;/denchmark-link&gt;
 which port  to centos6.
Using centos 5 may introduce too many problems, including CUDA installation or third-party libraries. Will try this on CentOS6.
By the way, &lt;denchmark-link:https://pypi.python.org/pypi/auditwheel&gt;auditwheel&lt;/denchmark-link&gt;
 is a good tool to test whether the  package is sufficient for the dependencies.
Trying this Dockerfile:
FROM quay.io/numenta/manylinux1_x86_64_centos6:0.1.2

RUN NVIDIA_GPGKEY_SUM=d1be581509378368edeec8c1eb2958702feedf3bc3d17011adbf24efacce4ab5 &amp;&amp; \
    curl -fsSL https://developer.download.nvidia.com/compute/cuda/repos/rhel6/x86_64/7fa2af80.pub | sed '/^Version/d' &gt; /etc/pki/rpm-gpg/RPM-GPG-KEY-NVIDIA &amp;&amp; \
    echo "$NVIDIA_GPGKEY_SUM  /etc/pki/rpm-gpg/RPM-GPG-KEY-NVIDIA" | sha256sum -c -

COPY cuda.repo /etc/yum.repos.d/cuda.repo

ENV CUDA_VERSION 7.5.18

ENV CUDA_PKG_VERSION 7-5-7.5-18
RUN yum install -y \
        cuda-nvrtc-$CUDA_PKG_VERSION \
        cuda-cusolver-$CUDA_PKG_VERSION \
        cuda-cublas-$CUDA_PKG_VERSION \
        cuda-cufft-$CUDA_PKG_VERSION \
        cuda-curand-$CUDA_PKG_VERSION \
        cuda-cusparse-$CUDA_PKG_VERSION \
        cuda-npp-$CUDA_PKG_VERSION \
        cuda-cudart-$CUDA_PKG_VERSION &amp;&amp; \
    ln -s cuda-7.5 /usr/local/cuda &amp;&amp; \
    rm -rf /var/cache/yum/*

RUN echo "/usr/local/cuda/lib64" &gt;&gt; /etc/ld.so.conf.d/cuda.conf &amp;&amp; \
    ldconfig

# nvidia-docker 1.0
LABEL com.nvidia.volumes.needed="nvidia_driver"
LABEL com.nvidia.cuda.version="${CUDA_VERSION}"

RUN echo "/usr/local/nvidia/lib" &gt;&gt; /etc/ld.so.conf.d/nvidia.conf &amp;&amp; \
    echo "/usr/local/nvidia/lib64" &gt;&gt; /etc/ld.so.conf.d/nvidia.conf

ENV PATH /usr/local/nvidia/bin:/usr/local/cuda/bin:${PATH}
ENV LD_LIBRARY_PATH /usr/local/nvidia/lib:/usr/local/nvidia/lib64

# nvidia-container-runtime
ENV NVIDIA_VISIBLE_DEVICES all
ENV NVIDIA_DRIVER_CAPABILITIES compute,utility
ENV NVIDIA_REQUIRE_CUDA "cuda&gt;=7.5"

# for devel
RUN yum install -y \
        cuda-core-$CUDA_PKG_VERSION \
        cuda-misc-headers-$CUDA_PKG_VERSION \
        cuda-command-line-tools-$CUDA_PKG_VERSION \
        cuda-license-$CUDA_PKG_VERSION \
        cuda-nvrtc-dev-$CUDA_PKG_VERSION \
        cuda-cusolver-dev-$CUDA_PKG_VERSION \
        cuda-cublas-dev-$CUDA_PKG_VERSION \
        cuda-cufft-dev-$CUDA_PKG_VERSION \
        cuda-curand-dev-$CUDA_PKG_VERSION \
        cuda-cusparse-dev-$CUDA_PKG_VERSION \
        cuda-npp-dev-$CUDA_PKG_VERSION \
        cuda-cudart-dev-$CUDA_PKG_VERSION \
        cuda-driver-dev-$CUDA_PKG_VERSION \
        gcc-c++ \
        yum-utils &amp;&amp; \
    rm -rf /var/cache/yum/*

RUN mkdir /tmp/gpu-deployment-kit &amp;&amp; cd /tmp/gpu-deployment-kit &amp;&amp; \
    rpm2cpio $(repoquery --location  gpu-deployment-kit) | cpio -id &amp;&amp; \
    mv usr/include/nvidia/gdk/* /usr/local/cuda/include &amp;&amp; \
    mv usr/src/gdk/nvml/lib/* /usr/local/cuda/lib64/stubs &amp;&amp; \
    rm -rf /tmp/gpu-deployment-kit* &amp;&amp; \
    rm -rf /var/cache/yum/*

ENV LIBRARY_PATH /usr/local/cuda/lib64/stubs:${LIBRARY_PATH}

# for paddle
# no using /opt/devtools2
ENV PATH=/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin

RUN wget -q https://cmake.org/files/v3.5/cmake-3.5.2.tar.gz &amp;&amp; tar xzf cmake-3.5.2.tar.gz &amp;&amp; \
    cd cmake-3.5.2 &amp;&amp; ./bootstrap &amp;&amp; \
    make -j4 &amp;&amp; make install &amp;&amp; cd .. &amp;&amp; rm cmake-3.5.2.tar.gz

RUN wget --no-check-certificate -qO- https://storage.googleapis.com/golang/go1.8.1.linux-amd64.tar.gz | \
    tar -xz -C /usr/local &amp;&amp; \
    mkdir /root/gopath &amp;&amp; \
    mkdir /root/gopath/bin &amp;&amp; \
    mkdir /root/gopath/src


ENV GOROOT=/usr/local/go GOPATH=/root/gopath
# no using /opt/devtools2
ENV PATH=${GOROOT}/bin:${GOPATH}/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
ENV LD_LIBRARY_PATH=/opt/_internal/cpython-2.7.11-ucs4/lib:${LD_LIBRARY_PATH}

# protobuf 3.1.0
RUN cd /opt &amp;&amp; wget -q --no-check-certificate https://github.com/google/protobuf/releases/download/v3.1.0/protobuf-cpp-3.1.0.tar.gz &amp;&amp; \
    tar xzf protobuf-cpp-3.1.0.tar.gz &amp;&amp; \
    cd protobuf-3.1.0 &amp;&amp; ./configure &amp;&amp; make -j4 &amp;&amp; make install &amp;&amp; cd .. &amp;&amp; rm -f protobuf-cpp-3.1.0.tar.gz

RUN /opt/python/cp27-cp27mu/bin/pip install protobuf==3.1.0

RUN yum install -y sqlite-devel zlib-devel openssl-devel boost boost-devel pcre-devel vim

RUN /opt/python/cp27-cp27mu/bin/pip install numpy &amp;&amp; go get github.com/Masterminds/glide

RUN wget -O /opt/swig-2.0.12.tar.gz https://sourceforge.net/projects/swig/files/swig/swig-2.0.12/swig-2.0.12.tar.gz/download &amp;&amp; \
    cd /opt &amp;&amp; tar xzf swig-2.0.12.tar.gz &amp;&amp; cd /opt/swig-2.0.12 &amp;&amp; ./configure &amp;&amp; make &amp;&amp; make install &amp;&amp; cd /opt &amp;&amp; rm swig-2.0.12.tar.gz
		</comment>
		<comment id='3' author='reyoung' date='2017-11-04T12:57:25Z'>
		Updated using quay.io/numenta/manylinux1_x86_64_centos6:0.1.2, this images seems have only cp27-cp27mu python included. Adding cp27-cp27m support is also needed, build image from https://github.com/numenta/manylinux seems not passing.
		</comment>
		<comment id='4' author='reyoung' date='2017-11-06T05:26:02Z'>
		Update using above docker generated whl, auditwheel says:
&lt;denchmark-code&gt;LD_LIBRARY_PATH=/opt/_internal/cpython-3.5.1/lib:$LD_LIBRARY_PATH auditwheel show python/dist/paddlepaddle-0.10.0-cp27-cp27mu-linux_x86_64.whl
Traceback (most recent call last):
  File "/usr/local/bin/auditwheel", line 11, in &lt;module&gt;
    sys.exit(main())
  File "/opt/_internal/cpython-3.5.1/lib/python3.5/site-packages/auditwheel/main.py", line 49, in main
    rval = args.func(args, p)
  File "/opt/_internal/cpython-3.5.1/lib/python3.5/site-packages/auditwheel/main_show.py", line 28, in execute
    winfo = analyze_wheel_abi(args.WHEEL_FILE)
  File "/opt/_internal/cpython-3.5.1/lib/python3.5/site-packages/auditwheel/wheel_abi.py", line 73, in analyze_wheel_abi
    get_wheel_elfdata(wheel_fn)
  File "/opt/_internal/cpython-3.5.1/lib/python3.5/site-packages/auditwheel/wheel_abi.py", line 42, in get_wheel_elfdata
    so_path_split[-1])
RuntimeError: Invalid binary wheel, found shared library "core.so" in purelib folder.
The wheel has to be platlib compliant in order to be repaired by auditwheel.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='reyoung' date='2017-11-06T09:30:19Z'>
		After &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/5396&gt;#5396&lt;/denchmark-link&gt;
 merged, build under above image will generate a  package results:
&lt;denchmark-code&gt;auditwheel show paddlepaddle-0.10.0-cp27-cp27mu-linux_x86_64.whl

paddlepaddle-0.10.0-cp27-cp27mu-linux_x86_64.whl is consistent with
the following platform tag: "linux_x86_64".

The wheel references external versioned symbols in these system-
provided shared libraries: libm.so.6 with versions {'GLIBC_2.2.5'},
libstdc++.so.6 with versions {'GLIBCXX_3.4.11', 'GLIBCXX_3.4.10',
'GLIBCXX_3.4.9', 'CXXABI_1.3.3', 'CXXABI_1.3', 'GLIBCXX_3.4.13',
'GLIBCXX_3.4'}, libdl.so.2 with versions {'GLIBC_2.2.5'},
libgcc_s.so.1 with versions {'GCC_3.3', 'GCC_3.0'}, libc.so.6 with
versions {'GLIBC_2.3.4', 'GLIBC_2.2.5', 'GLIBC_2.3.2', 'GLIBC_2.7',
'GLIBC_2.6'}, libpthread.so.0 with versions {'GLIBC_2.2.5',
'GLIBC_2.3.2'}

This constrains the platform tag to "linux_x86_64". In order to
achieve a more compatible tag, you would to recompile a new wheel from
source on a system with earlier versions of these libraries, such as
CentOS 5.
&lt;/denchmark-code&gt;

Install this whl on a pure centos 6 works fine. This tool chain can support most of our cases. Will refine the build images/scripts and put it on CI.
		</comment>
		<comment id='6' author='reyoung' date='2017-11-07T02:08:05Z'>
		Update: I added a new repo &lt;denchmark-link:https://github.com/PaddlePaddle/buildtools&gt;https://github.com/PaddlePaddle/buildtools&lt;/denchmark-link&gt;
 containing scripts to build development docker images of different version.
		</comment>
		<comment id='7' author='reyoung' date='2017-11-07T04:26:07Z'>
		&lt;denchmark-link:https://github.com/typhoonzero&gt;@typhoonzero&lt;/denchmark-link&gt;
 How are you going to keep the relationship between versions of Paddle and buildtools?  We are facing such a challenge to keep versions of models/book up to date with Paddle.  The current solution is that we need a git submodule link in the models/book repo pointing to a certain version (recently released) of PaddlePaddle.  We are not yet sure if this solution is perfect.
		</comment>
		<comment id='8' author='reyoung' date='2017-11-07T04:51:47Z'>
		&lt;denchmark-link:https://github.com/wangkuiyi&gt;@wangkuiyi&lt;/denchmark-link&gt;
 No.  contains only  that can build  sufficient building environment, and it should be static since  defined dependencies are static.  repo is not going to update until a new  definition came out.
		</comment>
		<comment id='9' author='reyoung' date='2017-11-07T05:13:43Z'>
		Sounds reasonable. According to my experience adding the default Dockerfile, it is static. And it seems that Dockerfile.android is also static. Let's go for your proposal for a while. If it runs all good, we can switch completely to it.
		</comment>
		<comment id='10' author='reyoung' date='2017-11-15T05:26:28Z'>
		I reopen this issue, because we also need to add some project on teamCity:

 Build different version whl package with manlinux standard




feature gate
option switch




GPU ON + CUDA/CUDNN version
cuda7.5_cudnn5, cuda8.0_cudnn5, cuda8.0_cudnn7


GPU OFF + AVX
AVX ON/OFF (Only use default ON)


GPU OFF + cblas
MKL/OpenBlas (Only use one as default)


android
build for androind




 All PR should pass unit test with manlinux develop Docker image.
 Production Docker image should be built from manlinux Python package.
 C-API should be built on manlinux develop Docker image.

		</comment>
		<comment id='11' author='reyoung' date='2017-11-15T05:48:38Z'>
		How much time of TeamCity if we add all above projects? &lt;denchmark-link:https://github.com/Yancey1989&gt;@Yancey1989&lt;/denchmark-link&gt;

And should we enhance the TeamCity agent at first? Currently, we only have 3 agents. &lt;denchmark-link:https://github.com/helinwang&gt;@helinwang&lt;/denchmark-link&gt;

&lt;denchmark-link:https://www.jetbrains.com/teamcity/buy/#license-type=new-license&gt;https://www.jetbrains.com/teamcity/buy/#license-type=new-license&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='reyoung' date='2017-11-15T05:57:17Z'>
		Hi &lt;denchmark-link:https://github.com/luotao1&gt;@luotao1&lt;/denchmark-link&gt;

If we don't enought agents on TeamCity, maybe we can make all project except PR Test building on midnight. But I think enhance the TeamCity agent is very important, there are often more than 10 tasks waitting in the Queue.
		</comment>
		<comment id='13' author='reyoung' date='2017-11-15T06:02:37Z'>
		However, our midnight is the daytime of American colleagues. If we make all project building on midnight, do they wait a lot of time?
		</comment>
		<comment id='14' author='reyoung' date='2017-11-15T06:10:18Z'>
		Yep, I miss this question, how about disperse the builting time, make sure all project to run once a day?
		</comment>
		<comment id='15' author='reyoung' date='2017-11-15T06:13:41Z'>
		
how about disperse the builting time, make sure all project to run once a day?

I agree with you!
		</comment>
		<comment id='16' author='reyoung' date='2017-11-15T11:25:16Z'>
		Update:

Use schedule trigger instead of CSV trigger in all project except PR CI.
Build different version whl Python package on TeamCity https://paddleci.ngrok.io/project.html?projectId=Manylinux1&amp;tab=projectOverview.

		</comment>
		<comment id='17' author='reyoung' date='2017-11-15T22:06:25Z'>
		How many extra projects are we planning to build at midnight Beijing time? If there are within 6 projects, and each one takes 30min, it only takes an hour on three machines, it's probably fine.
		</comment>
		<comment id='18' author='reyoung' date='2017-11-17T14:58:23Z'>
		&lt;denchmark-link:https://github.com/helinwang&gt;@helinwang&lt;/denchmark-link&gt;
 There are 15 configurations for totally except the PR CI, I have already configured them as schedule trigger at different time.
		</comment>
		<comment id='19' author='reyoung' date='2017-11-21T08:55:12Z'>
		Update:
limiting the number of different version of whl package, because the community edition of TeamCity limits the number of congurations.



TeamCity Configuration
WITH_AVX
WITH_GPU
WITH_MKL
Docker Image
cp27-cp27mu
cp27-cp27m
C-API




cpu_avx_mkl
ON
OFF
ON
paddle:latest
paddlepaddle-0.10.0-cp27-cp27mu-linux_x86_64.whl
paddlepaddle-0.10.0-cp27-cp27m-linux_x86_64.whl
paddle.tgz


cpu_avx_openblas
ON
OFF
ON
paddle:latest-openblas
paddlepaddle-0.10.0-cp27-cp27mu-linux_x86_64.whl
paddlepaddle-0.10.0-cp27-cp27m-linux_x86_64.whl
None


cuda7.5_cudnn5_avx_mkl
ON
ON
ON
None
paddlepaddle-0.10.0-cp27-cp27mu-linux_x86_64.whl
paddlepaddle-0.10.0-cp27-cp27m-linux_x86_64.whl
paddle.tgz


cuda8.0_cudnn5_avx_mkl
ON
ON
ON
None
paddlepaddle-0.10.0-cp27-cp27mu-linux_x86_64.whl
paddlepaddle-0.10.0-cp27-cp27m-linux_x86_64.whl
paddle.tgz


cuda8.0_cudnn7_avx_mkl
ON
ON
ON
paddle:latest-gpu
paddlepaddle-0.10.0-cp27-cp27mu-linux_x86_64.whl
paddlepaddle-0.10.0-cp27-cp27m-linux_x86_64.whl
paddle.tgz



		</comment>
		<comment id='20' author='reyoung' date='2017-11-21T09:08:58Z'>
		Also add corresponding capi links?
		</comment>
		<comment id='21' author='reyoung' date='2017-11-21T09:30:08Z'>
		&lt;denchmark-link:https://github.com/typhoonzero&gt;@typhoonzero&lt;/denchmark-link&gt;
 will do that :)
		</comment>
		<comment id='22' author='reyoung' date='2017-11-22T08:15:48Z'>
		&lt;denchmark-link:https://github.com/typhoonzero&gt;@typhoonzero&lt;/denchmark-link&gt;
 add C-API links.
		</comment>
	</comments>
</bug>