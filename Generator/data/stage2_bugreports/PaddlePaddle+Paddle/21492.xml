<bug id='21492' author='Sand3r-' open_date='2019-12-02T14:42:17Z' closed_time='2019-12-20T02:29:35Z'>
	<summary>Malformed graph of ernie when ran with benchmark application</summary>
	<description>
&lt;denchmark-h:h2&gt;Current behaviour&lt;/denchmark-h&gt;

The error has been discovered thanks to level 3 logging enabled by GLOG_v environmental variable.
GLOG has reported, that:
Some operators use the same variables for reading/writing output. For example, when the fp32_model has been ran, one could observe that scale_op as well transpose2_op accept transpose_4.tmp_0 as their input to the operator (while according to the original graph they do not)
Subpart of a GLOG error documenting this:
&lt;denchmark-code&gt;operator.cc:172 CPUPlace Op(scale), inputs:{X[transpose_4.tmp_0:float[1, 12, 128, 64]({})]}, outputs:{Out[scale_12.tmp_0:float[1, 12, 128, 64]({})]}.
(...several ops later...)
operator.cc:172 CPUPlace Op(transpose2), inputs:{X[transpose_4.tmp_0:float[1, 12, 128, 64]({})]}, outputs:{Out[fc_66.tmp_0:float[1, 128, 12, 64]({})], XShape[transpose_47.tmp_1:[0, 1, 12, 128, 64]({})]}.
&lt;/denchmark-code&gt;

As far as I understand that, this is a bug, since variable names should be unique (as long as they are enclosed in the same scope).
To illustrate the problem, please see the following figure depicting a  model (ernie_quant) which suffers from the same problem:
&lt;denchmark-link:https://user-images.githubusercontent.com/4967505/70145384-38b22980-16a0-11ea-9264-0412724b010c.png&gt;&lt;/denchmark-link&gt;

This is a blocking issue for INT8 Ernie quantization task, since our quantization system associates scales with variable names. And if the variable repeats in several places, we have end up with the same scales where we didn't mean to.
&lt;denchmark-h:h2&gt;Reproduction&lt;/denchmark-h&gt;


based on 8da0cd5
-CPU: including MKLDNN version v.20
-OS Platform Ubuntu 16.04
-Cmake orders -DCMAKE_BUILD_TYPE=RelWithDebInfo -DWITH_GPU=OFF -DON_INFER=ON -DWITH_MKLDNN=ON -DWITH_TESTING=ON -DWITH_PROFILER=ON -DWITH_STYLE_CHECK=OFF -DWITH_INFERENCE_API_TEST=ON
-API information
To Reproduce


Build paddle
Build benchmark Inference application for ernie https://github.com/PaddlePaddle/benchmark/tree/master/Inference/c%2B%2B/ernie
Run any 4-input ernie model.

&lt;denchmark-link:https://github.com/luotao1&gt;@luotao1&lt;/denchmark-link&gt;
 Could you please assign someone to help solving this issue?
	</description>
	<comments>
		<comment id='1' author='Sand3r-' date='2019-12-04T11:12:19Z'>
		图中variable混乱问题：
用benchmark repo跑ernie的时候无论是fp32模型还是int模型，都会出现variable连接到错误的op上的情况，这个可能会影响ernie 精度的输出。如下图所示，左图为正确的输出，右图为apply pass前保存下来的图：
&lt;denchmark-link:https://user-images.githubusercontent.com/33643817/70137600-3fa16200-16c9-11ea-824d-70a377332690.png&gt;&lt;/denchmark-link&gt;

混乱的variable如下图所示：
&lt;denchmark-link:https://user-images.githubusercontent.com/33643817/70137881-fac9fb00-16c9-11ea-96be-d3199a1b3817.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='Sand3r-' date='2019-12-11T22:51:18Z'>
		The graph is malformed when the memory_optimize_pass is enabled (e.g. via EnableMemoryOptim() method). With the pass being disabled, the graph of the model looks fine.
		</comment>
		<comment id='3' author='Sand3r-' date='2019-12-13T06:57:17Z'>
		related &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/issues/21598&gt;#21598&lt;/denchmark-link&gt;
, how about disabling this pass when using MKLDNN?
		</comment>
		<comment id='4' author='Sand3r-' date='2019-12-18T14:36:33Z'>
		
related #21598, how about disabling this pass when using MKLDNN?

We can surely do that. I've opened up a PR implementing this: &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/21826&gt;#21826&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='Sand3r-' date='2019-12-20T02:29:35Z'>
		&lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/21826&gt;#21826&lt;/denchmark-link&gt;
 disable memory optimization pass when mkldnn is on
		</comment>
	</comments>
</bug>