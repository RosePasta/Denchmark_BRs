<bug id='14377' author='TheodoreG' open_date='2018-11-13T03:02:01Z' closed_time='2018-12-07T04:48:47Z'>
	<summary>Two questions about image classification distributed training.</summary>
	<description>
I have used the distributed training code for image classification in &lt;denchmark-link:url&gt;https://github.com/PaddlePaddle/models/blob/develop/fluid/PaddleCV/image_classification/dist_train/dist_train.py&lt;/denchmark-link&gt;
 and got some questions.

It seems that variable nccl_id_var keeps None in the whole program, and doesn't change even after the append_nccl2_prepare() function is called. Buy the way, function append_nccl2_prepare() seems returned nothing.
In the training loop I can't tell how the program allocate different training data batch into different device. Ideally, different devices should train different training batches, or may be ParallelExecutor do such things for me ?

	</description>
	<comments>
		<comment id='1' author='TheodoreG' date='2018-11-13T03:47:12Z'>
		Thanks for reporting. These two are very good questions.

for nccl2 mode, I've updated append_nccl2_prepare to use latest transpiler API to simplify the code. Actually, you should not care about nccl_id_var, will remove this in next commit.
data partition is done by the reader: https://github.com/PaddlePaddle/models/blob/develop/fluid/PaddleCV/image_classification/reader.py#L139. Yet there's still issue for the current shuffling method, each node should use one part of the shuffled whole data set for each epoch, I'll update this in next commit too.

		</comment>
		<comment id='2' author='TheodoreG' date='2018-11-13T06:20:43Z'>
		Ok, thanks a lot for your nice explanation.
		</comment>
	</comments>
</bug>