<bug id='13054' author='ws441572091' open_date='2018-08-29T12:27:42Z' closed_time='2018-10-26T06:16:24Z'>
	<summary>以字符为单元做CNN</summary>
	<description>
你好，看了教程CNN的文档，以词为单元对句子进行CNN（输入为一个词的emb，长度默认为lod_tensor长度） 处理lod_level =1
words = [1,2,3]
data = fluid.layers.data( ame="words", shape=[1], dtype="int64", lod_level=1)
emb = fluid.layers.embedding(input=data, size=[input_dim, emb_dim], is_sparse=True)
seq_conv = fluid.nets.sequence_conv_pool(input=emb,
num_filters=hid_dim,
filter_size=3,
act="tanh",
pool_type="sqrt")
chars = [[1,1],[2,2],[3,3]]
想请问如何先以字符为单元进行CNN得到每个词的词表示，之后再对词表示做其他操作
	</description>
	<comments>
		<comment id='1' author='ws441572091' date='2018-08-30T02:12:45Z'>
		根据值班同学回答修改lod_level报错：
例如一个chars = [[37, 57], [6, 0, 61, 0, 11, 2, 7]]
但是怎么设置cnn只在词的维度呢
如果简单设置lod_level = 2
&lt;denchmark-code&gt;chars = fluid.layers.data(
        name = 'chars',shape=[1],dtype= 'int64',lod_level=2)

emb_chars = fluid.layers.embedding(
        input = chars,size = [len(char_dict),50],is_sparse=True)

seq_conv = fluid.nets.sequence_conv_pool(input = emb_chars,
        num_filters = 200,
        filter_size = 3,
        act = 'tanh',
        pool_type = 'sqrt')
&lt;/denchmark-code&gt;

结果会报错
paddle.fluid.core.EnforceNotMet: enforce in-&gt;lod().size() == 1UL failed, 2 != 1
Only support one level sequence now. at [/paddle/paddle/fluid/operators/sequence_conv_op.h:43]
		</comment>
		<comment id='2' author='ws441572091' date='2018-08-30T05:53:52Z'>
		这种在char和word粒度进行cnn的做法，建议是分两个粒度上独立的cnn去做。如果word 的embedding的想通过char粒度的embedding 通过pooling得到，则需要自己构建lod信息，用新的lodtensor再cnn
		</comment>
		<comment id='3' author='ws441572091' date='2018-08-30T12:55:12Z'>
		&lt;denchmark-link:http://www.paddlepaddle.org/documentation/api/zh/0.14.0/layers.html#dynamicrnn&gt;http://www.paddlepaddle.org/documentation/api/zh/0.14.0/layers.html#dynamicrnn&lt;/denchmark-link&gt;
  可以利用dynamicrnn将lod层次拆解在做cnn
		</comment>
		<comment id='4' author='ws441572091' date='2018-08-31T09:17:25Z'>
		根据值班同学回答
拆2维为1维的一个办法是用DynamicRNN
&lt;denchmark-code&gt;chars = fluid.layers.data(
        name = 'chars',shape=[1],dtype= 'int64',lod_level=2)
drnn = fluid.layers.DynamicRNN()
with drnn.block():
    word = drnn.step_input(chars)
    emb_chars = fluid.layers.embedding(
            input = word,size = [len(char_dict),50],is_sparse=True)
    seq_conv = fluid.nets.sequence_conv_pool(input = emb_chars,
            num_filters = 200,
            filter_size = 3,
            act = 'tanh',
            pool_type = 'sqrt')
    drnn.output(seq_conv)
rnn_out = drnn()

emb = fluid.layers.fc(input = rnn_out,size=200)

emb = fluid.layers.dropout(emb,dropout_prob = 0.5)

lstm_0,_ = fluid.layers.dynamic_lstm(input = emb,size = lstm_size,is_reverse = 0 )
lstm_1,_ = fluid.layers.dynamic_lstm(input = emb,size = lstm_size,is_reverse = 1 )
&lt;/denchmark-code&gt;

结果报错
paddle.fluid.core.EnforceNotMet: enforce rank_level &lt; x.lod().size() failed, 0 &gt;= 0
Input should be a LOD tensor, and size is at least 1 at [/paddle/paddle/fluid/operators/lod_tensor_to_array_op.cc:52]
		</comment>
		<comment id='5' author='ws441572091' date='2018-08-31T09:19:48Z'>
		经过检查输出rnn_out结果貌似正常
rnn_out        Tensor[array_to_lod_tensor_0.tmp_0]
shape: [12,200,]
dtype: f
LoD: [[ 0,3,12, ]]
值班同学怀疑是一些op在反向上的问题
		</comment>
		<comment id='6' author='ws441572091' date='2018-08-31T09:25:58Z'>
		配置环境变量env GLOG_logtostderr=1 GLOG_vmodule=operator=10
截取两段输出
&lt;denchmark-code&gt;I0831 17:21:20.378895 45247 operator.cc:111] + Op(lstm_grad), inputs:{BatchCellPreAct[lstm_0.tmp_3[103, 50]({})], BatchCellPreAct@GRAD[lstm_0.tmp_3@GRAD[-1]({{}})], BatchGate[lstm_0.tmp_2[103, 200]({{0, 10, 19, 25, 31, 36, 41, 46, 50, 54, 57, 60, 63, 66, 69, 72, 75, 78, 81, 84, 87, 90, 93, 96, 99, 102, 103}{57, 7, 32, 83, 0, 93, 97, 99, 101, 92, 58, 8, 33, 84, 1, 94, 98, 100, 102, 59, 9, 34, 85, 2, 95, 60, 10, 35, 86, 3, 96, 61, 11, 36, 87, 4, 62, 12, 37, 88, 5, 63, 13, 38, 89, 6, 64, 14, 39, 90, 65, 15, 40, 91, 66, 16, 41, 67, 17, 42, 68, 18, 43, 69, 19, 44, 70, 20, 45, 71, 21, 46, 72, 22, 47, 73, 23, 48, 74, 24, 49, 75, 25, 50, 76, 26, 51, 77, 27, 52, 78, 28, 53, 79, 29, 54, 80, 30, 55, 81, 31, 56, 82}{3, 1, 2, 4, 0, 6, 7, 8, 9, 5}})], BatchGate@GRAD[lstm_0.tmp_2@GRAD[-1]({{}})], Bias[lstm_0.b_0[1, 350]({})], C0[], Cell[lstm_0.tmp_1[103, 50]({{0, 7, 32, 57, 83, 92, 93, 97, 99, 101, 103}})], Cell@GRAD[lstm_0.tmp_1@GRAD[-1]({{}})], H0[], Hidden[lstm_0.tmp_0[103, 50]({{0, 7, 32, 57, 83, 92, 93, 97, 99, 101, 103}})], Hidden@GRAD[lstm_0.tmp_0@GRAD[103, 50]({})], Input[dropout_0.tmp_0[103, 200]({{0, 7, 32, 57, 83, 92, 93, 97, 99, 101, 103}})], Weight[lstm_0.w_0[50, 200]({})]}, outputs:{Bias@GRAD[lstm_0.b_0@GRAD[1, 350]({})], C0@GRAD[], H0@GRAD[], Input@GRAD[dropout_0.tmp_0@GRAD@RENAME@1[103, 200]({})], Weight@GRAD[lstm_0.w_0@GRAD[50, 200]({})]}.

I0831 17:21:20.379901 45247 operator.cc:101] - Op(lod_tensor_to_array), inputs:{RankTable[lod_rank_table_0[-1]({{}})], X[array_to_lod_tensor_0.tmp_0@GRAD[103, 200]({})]}, outputs:{Out[dynamic_rnn_0_output_array_sequence_pool_0.tmp_0_0@GRAD[-1]({{}})]}.
&lt;/denchmark-code&gt;

可以发现Op(lstm_grad)时后面的output中的{}丢失了lod的信息，为空
根据值班同学讲许多Op都会存在这个问题，丢失lod信息，导致反向时不能将lod信息传回去，这个问题正在修改中
目前判断在字符粒度做了cnn再以词粒度做lstm不能反向传播lod信息给cnn导致错误
		</comment>
		<comment id='7' author='ws441572091' date='2018-10-26T06:16:24Z'>
		您好，此issue在近一个月内暂无更新，我们将于今天内关闭。若在关闭后您仍需跟进提问，可重新开启此问题，我们将在24小时内回复您。因关闭带来的不便我们深表歉意，请您谅解~感谢您对PaddlePaddle的支持!
		</comment>
	</comments>
</bug>