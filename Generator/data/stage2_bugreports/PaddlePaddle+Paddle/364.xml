<bug id='364' author='linrongyi' open_date='2016-11-05T12:49:50Z' closed_time='2016-11-05T13:44:38Z'>
	<summary>使用预测工具预测sparse_binary_vector的instance时候出错</summary>
	<description>
使用预测工具预测sparse_binary_vector的instance, 执行network.forwardTest(in_arg) 出错
代码
# -*- coding: utf-8 -*-

import sys

import numpy as np

from py_paddle import swig_paddle
from py_paddle import DataProviderConverter

from paddle.trainer.PyDataProvider2 import *

#from paddle.trainer.PyDataProviderWrapper import DenseSlot, IndexSlot
from paddle.trainer.config_parser import parse_config

swig_paddle.initPaddle("--use_gpu=0")

def get_slots():
    slots = [sparse_binary_vector(800000)]
    for i in xrange(8):
        slots.append(sparse_binary_vector(800000))
        slots.append(sparse_binary_vector(800000))
    return slots

slots = get_slots()

conf = parse_config("./output/pass-00000/predictor_config.conf", "")
network = swig_paddle.GradientMachine.createFromConfigProto(conf.model_config)
converter = DataProviderConverter(slots)

instance_ = [[1]] * 17
in_arg = converter([instance_])
network.forwardTest(in_arg)
错误提示
&lt;denchmark-code&gt;I1105 20:46:14.398023 11953 Util.cpp:155] commandline:  --use_gpu=0 
I1105 20:46:14.398080 11953 Util.cpp:130] Calling runInitFunctions
I1105 20:46:14.398355 11953 Util.cpp:143] Call runInitFunctions done.
F1105 20:46:14.660048 11953 SparseRowMatrix.h:63] Check failed: globalIndices_[row] != kUnusedId_ (4294967295 vs. 4294967295) 
*** Check failure stack trace: ***
    @     0x7f45a5946a3d  google::LogMessage::Fail()
    @     0x7f45a594aed7  google::LogMessage::SendToLog()
    @     0x7f45a5948d39  google::LogMessage::Flush()
    @     0x7f45a594903d  google::LogMessageFatal::~LogMessageFatal()
    @     0x7f45a725eea2  paddle::CpuMatrix::mul&lt;&gt;()
    @     0x7f45a7256237  paddle::CpuMatrix::mul()
    @     0x7f45a70e716e  paddle::FullyConnectedLayer::forward()
    @     0x7f45a70aad64  paddle::NeuralNetwork::forward()
    @     0x7f45a701d955  _wrap_GradientMachine_forward
    @     0x7f45b2e273a3  PyEval_EvalFrameEx
    @     0x7f45b2e29130  PyEval_EvalCodeEx
    @     0x7f45b2e274a1  PyEval_EvalFrameEx
    @     0x7f45b2e29130  PyEval_EvalCodeEx
    @     0x7f45b2e274a1  PyEval_EvalFrameEx
    @     0x7f45b2e29130  PyEval_EvalCodeEx
    @     0x7f45b2e29242  PyEval_EvalCode
    @     0x7f45b2e4362c  run_mod
    @     0x7f45b2e43700  PyRun_FileExFlags
    @     0x7f45b2e44c0c  PyRun_SimpleFileExFlags
    @     0x7f45b2e564cc  Py_Main
    @       0x318ae1ecdd  (unknown)
Thread [139937329645312] Forwarding query_vec, site7, title7, site6, title6, site5, title5, site4, title4, site3, title3, site2, title2, site1, title1, site0, title0, query, 
*** Aborted at 1478349974 (unix time) try "date -d @1478349974" if you are using GNU date ***
PC: @       0x318ae328a5 (unknown)
*** SIGABRT (@0x1f500002eb1) received by PID 11953 (TID 0x7f45b2d30700) from PID 11953; stack trace: ***
    @       0x318b20f500 (unknown)
    @       0x318ae328a5 (unknown)
    @       0x318ae34085 (unknown)
    @     0x7f45a5953246 google::FindSymbol()

&lt;/denchmark-code&gt;

版本号, &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/commit/93e4d0cce6d3048f77aa46dc9d4ee5c3fb777d0c&gt;93e4d0c&lt;/denchmark-link&gt;
.
麻烦 &lt;denchmark-link:https://github.com/reyoung&gt;@reyoung&lt;/denchmark-link&gt;
 看一下?
	</description>
	<comments>
		<comment id='1' author='linrongyi' date='2016-11-05T13:11:47Z'>
		Please do not at any of us, because we will see the issue and assign it to someone.
But in this situation, are you sure about the size of data_layer in predictor_config.conf is 17?
		</comment>
		<comment id='2' author='linrongyi' date='2016-11-05T13:14:04Z'>
		Because the log shows some tips for me
&lt;denchmark-code&gt;query_vec
site7
title7
site6
title6
site5
title5
site4
title4
site3
title3
site2
title2
site1
title1
site0
title0
query
&lt;/denchmark-code&gt;

are 18 inputs.
		</comment>
		<comment id='3' author='linrongyi' date='2016-11-05T13:15:08Z'>
		This my data layer conf
training = False
TITLE_NUM = 8

inputs = ["query"]
input_dims = [TERM_TOK_NUM]

for i in xrange(TITLE_NUM):

  inputs.append("title%d" % i)
  input_dims.append(TERM_TOK_NUM)

  inputs.append("site%d" % i)
  input_dims.append(SITE_TOK_NUM)

if training:

  inputs.append("w0")
  input_dims.append(1)

  inputs.append("w1")
  input_dims.append(1)

  inputs.append("w2")
  input_dims.append(1)

  inputs.append("label0")
  input_dims.append(30)

  inputs.append("label1")
  input_dims.append(30)

  inputs.append("label2")
  input_dims.append(30)

  inputs.append("fake_label")
  input_dims.append(2)

assert len(inputs) == len(input_dims)

for name, size_ in zip(inputs, input_dims):
  Layer(
      name = name,
      type = "data",
      size = size_,
  )



		</comment>
		<comment id='4' author='linrongyi' date='2016-11-05T13:16:26Z'>
		&lt;denchmark-link:https://github.com/linrongyi&gt;@linrongyi&lt;/denchmark-link&gt;
 Please make sure, why is  in your error log?
		</comment>
		<comment id='5' author='linrongyi' date='2016-11-05T13:17:39Z'>
		query_vec is sum of token embedding not input layer
Layer(#
    name = "query_vec",
    type = "fc",
    active_type='',
    size = TERM_EMB_DIM,
    bias = False,
    drop_rate=0.2,
    inputs = Input("query",
    initial_std = 1.0 / math.sqrt(TERM_TOK_NUM) / 3.0,
    parameter_name = "_term_emb",
    sparse_update = True,
    sparse_remote_update = True),
)
		</comment>
		<comment id='6' author='linrongyi' date='2016-11-05T13:33:24Z'>
		I guess it happened because the enable of sparse_update and sparse_remote_update. Please unset them firstly. But I am not sure this could help you to resolve this problem.
		</comment>
		<comment id='7' author='linrongyi' date='2016-11-05T13:42:03Z'>
		&lt;denchmark-link:https://github.com/reyoung&gt;@reyoung&lt;/denchmark-link&gt;
 it works after setting sparse_update = False and sparse_remote_update = False
		</comment>
		<comment id='8' author='linrongyi' date='2016-11-05T13:43:36Z'>
		Cool, I think I know what happened. Please close this issue, and I will add another issue to make this thing correct.
The predict code should not care about sparse_update flag.
		</comment>
		<comment id='9' author='linrongyi' date='2017-10-16T09:42:32Z'>
		请问如果是稀疏矩阵，想要提高训练速度，把sparse_update设置为True后报上述错误，要怎么解决
		</comment>
	</comments>
</bug>