<bug id='1220' author='yangyi02' open_date='2017-01-23T22:37:57Z' closed_time='2017-09-23T09:27:27Z'>
	<summary>Backward error for FeatureMapExpandLayer in PaddlePaddle</summary>
	<description>
Hi all,
I defined a constant layer in PaddlePaddle by myself and use FeatureMapExpandLayer on top of that for some learning algorithms. The constant layer I defined does not need backpropagation.
When I run the training, PaddlePaddle gives segmentation fault.
The reason I find is because getInputGrad(0) in FeatureMapExpandLayer is empty (because the constant layer does not need backpropagation).
And  "int imgSize = inGrad-&gt;getWidth();" gives segmentation fault.
Below is the backward code in FeatureMapExpandLayer.cpp
96 void FeatureMapExpandLayer::backward(const UpdateCallback&amp; callback) {
97   LOG(INFO) &lt;&lt; "I am here";
98   MatrixPtr inGrad = getInputGrad(0);
99   MatrixPtr outGrad = getOutputGrad();
100   size_t batchSize = getInput(0).getBatchSize();
101   int imgSize = inGrad-&gt;getWidth();
102   {
103     AsyncGpuBlock asyncGpuBlock;
104     for (size_t i = 0; i &lt; batchSize; i++) {
105       MatrixPtr outGradTmp =
106           Matrix::create(outGrad-&gt;getData() + i * imgSize * numFilters_,
107                          numFilters_,
108                          imgSize,
109                          false,
110                          useGpu_);
111       MatrixPtr inGradTmp = Matrix::create(
112           inGrad-&gt;getData() + i * imgSize, 1, imgSize, false, useGpu_);
113       inGradTmp-&gt;collectBias(outGradTmp, 1);
114     }
115   }
116   / Do derivation */ {
117     REGISTER_TIMER_INFO("BpAvtTimer", getName().c_str());
118     backwardActivation();
119   }
120 }
	</description>
	<comments>
		<comment id='1' author='yangyi02' date='2017-01-24T02:21:22Z'>
		Maybe this question is similar to the &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/1173&gt;PR&lt;/denchmark-link&gt;
, see the comments. The FeatureMapExpandLayer needs to judge whether the  is empty.  But if the constant layer is the first input layer and does not need backpropagation, the FeatureMapExpandLayer should also skip the backpropagation according to the code logic.
		</comment>
		<comment id='2' author='yangyi02' date='2017-01-26T21:27:04Z'>
		Yes, I think they are the similar reason. Can you add this check for FeatureMapExpandLayer ?
Thanks.
		</comment>
		<comment id='3' author='yangyi02' date='2017-09-23T09:27:27Z'>
		I close this issue due to inactivities. Please feel free to reopen it if more information is available.
		</comment>
	</comments>
</bug>