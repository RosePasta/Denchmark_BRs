<bug id='3073' author='xinghai-sun' open_date='2017-07-26T10:36:35Z' closed_time='2017-07-27T06:47:21Z'>
	<summary>Incorrect Inference.infer results when running with multiple GPUs.</summary>
	<description>
Inference.infer will have incorrect results when running with multiple GPUs.
Below is the output data of the first convolution layer for 4 example instances, when trainer_count=1 (upper figure) and trainer_count=2 (lower figure).
&lt;denchmark-link:https://user-images.githubusercontent.com/7038341/28616654-1a02941e-7230-11e7-9bc8-ad36cbc3170a.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/7038341/28616665-2861c6ba-7230-11e7-8399-9c40c7a324e4.png&gt;&lt;/denchmark-link&gt;

The output results are wrong if trainer_count &gt; 1 (use_gpu=True).
If we just print the input data layer, no difference is found between the two cases, indicating that the problem might exist in models instead of data allocation across GPUs.
	</description>
	<comments>
		<comment id='1' author='xinghai-sun' date='2017-07-26T16:13:50Z'>
		I print the weight of first convolution layer in each thread when trainer_count=2, the weights of first thread are same with the saved model. But the weights of second thread are zeros. We can make certain that there is a bug in the weight dispatch for multi-GPU in infer mode.  And I'll keep on tracking.
		</comment>
	</comments>
</bug>