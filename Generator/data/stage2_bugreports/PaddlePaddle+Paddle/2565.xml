<bug id='2565' author='lcy-seso' open_date='2017-06-22T11:30:12Z' closed_time='2017-06-24T00:01:59Z'>
	<summary>inferring by using multi-thread will be hung and the results are not right</summary>
	<description>
I am running text generation by using the encoder-decoder model, here is my codes: &lt;denchmark-link:https://github.com/lcy-seso/models/blob/refine_seq2seq/nmt_without_attention/generate.py&gt;https://github.com/lcy-seso/models/blob/refine_seq2seq/nmt_without_attention/generate.py&lt;/denchmark-link&gt;
.
I found that:

if trainer_count is set larger than 1, the generation process will be hung when infer is called the second time.
the prediction results are different between trainer_count=1 and trainer_count &gt; 1.
This bug occurs both in CPU and GPU mode.

outputs when setting trainer_count=1 and use_gpu=True goes like this:
&lt;denchmark-code&gt;Les &lt;unk&gt; se &lt;unk&gt; au sujet de la &lt;unk&gt; des &lt;unk&gt; alors que de &lt;unk&gt; &lt;unk&gt; sont en jeu
-119.7212       The &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; &lt;unk&gt; . &lt;e&gt;
-170.2804       The &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; &lt;unk&gt; , &lt;unk&gt; &lt;unk&gt; &lt;unk&gt;
-170.3101       The &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the
-170.5066       The &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; &lt;unk&gt; &lt;unk&gt;
-170.5434       The &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of the &lt;unk&gt; of &lt;unk&gt;
&lt;/denchmark-code&gt;

but when setting trainer_count=4 and use_gpu=True, the outputs are different:
&lt;denchmark-code&gt;Les &lt;unk&gt; se &lt;unk&gt; au sujet de la &lt;unk&gt; des &lt;unk&gt; alors que de &lt;unk&gt; &lt;unk&gt; sont en jeu
-8.0064 &lt;e&gt;
-16.0127        &lt;s&gt; &lt;e&gt;
-16.0127        the &lt;e&gt;
-16.0127        , &lt;e&gt;
-16.0127        &lt;unk&gt; &lt;e&gt;
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='lcy-seso' date='2017-06-22T12:00:03Z'>
		I also encountered this bug when running &lt;denchmark-link:https://github.com/PaddlePaddle/models/tree/develop/language_model&gt;language_model&lt;/denchmark-link&gt;
.
I set trainer_count=4, it will be hung when infer for the second time.
gdb log:
(gdb) bt
#0  0x000000318b20d720 in sem_wait () from /lib64/libpthread.so.0
#1  0x00007fffefaad23f in paddle::MultiGradientMachine::getOutArgs(std::vector&lt;paddle::Argument, std::allocator&lt;paddle::Argument&gt; &gt;*,
 paddle::enumeration_wrapper::PassType) () at /home/lizhao/Paddle/paddle/gserver/gradientmachines/MultiGradientMachine.h:354
#2  0x00007fffef932ad3 in _wrap_GradientMachine_forward () at /home/lizhao/Paddle/build/paddle/api/PaddlePYTHON_wrap.cxx:22906
#3  0x00007ffff7d1e3a3 in ext_do_call (f=&lt;value optimized out&gt;, throwflag=&lt;value optimized out&gt;) at Python/ceval.c:4331
#4  PyEval_EvalFrameEx (f=&lt;value optimized out&gt;, throwflag=&lt;value optimized out&gt;) at Python/ceval.c:2705
#5  0x00007ffff7d20130 in PyEval_EvalCodeEx (co=0x7ffff1c2cab0, globals=&lt;value optimized out&gt;, locals=&lt;value optimized out&gt;,
    args=&lt;value optimized out&gt;, argcount=4, kws=&lt;value optimized out&gt;, kwcount=0, defs=0x0, defcount=0, closure=0x0)
    at Python/ceval.c:3253
#6  0x00007ffff7d1e4a1 in fast_function (f=&lt;value optimized out&gt;, throwflag=&lt;value optimized out&gt;) at Python/ceval.c:4117
#7  call_function (f=&lt;value optimized out&gt;, throwflag=&lt;value optimized out&gt;) at Python/ceval.c:4042
#8  PyEval_EvalFrameEx (f=&lt;value optimized out&gt;, throwflag=&lt;value optimized out&gt;) at Python/ceval.c:2666
#9  0x00007ffff7d20130 in PyEval_EvalCodeEx (co=0x7ffff7b38d30, globals=&lt;value optimized out&gt;, locals=&lt;value optimized out&gt;,
    args=&lt;value optimized out&gt;, argcount=2, kws=&lt;value optimized out&gt;, kwcount=0, defs=0x0, defcount=0, closure=0x0)
    at Python/ceval.c:3253
#10 0x00007ffff7d1e4a1 in fast_function (f=&lt;value optimized out&gt;, throwflag=&lt;value optimized out&gt;) at Python/ceval.c:4117
#11 call_function (f=&lt;value optimized out&gt;, throwflag=&lt;value optimized out&gt;) at Python/ceval.c:4042
#12 PyEval_EvalFrameEx (f=&lt;value optimized out&gt;, throwflag=&lt;value optimized out&gt;) at Python/ceval.c:2666
#13 0x00007ffff7ca16b7 in gen_send_ex (gen=0x37966e0, arg=0x0, exc=&lt;value optimized out&gt;) at Objects/genobject.c:84
#14 0x00007ffff7d199ed in PyEval_EvalFrameEx (f=&lt;value optimized out&gt;, throwflag=&lt;value optimized out&gt;) at Python/ceval.c:2497
#15 0x00007ffff7ca16b7 in gen_send_ex (gen=0x379e1e0, arg=0x0, exc=&lt;value optimized out&gt;) at Objects/genobject.c:84
#16 0x00007ffff7d199ed in PyEval_EvalFrameEx (f=&lt;value optimized out&gt;, throwflag=&lt;value optimized out&gt;) at Python/ceval.c:2497
#17 0x00007ffff7d20130 in PyEval_EvalCodeEx (co=0x37a9bb0, globals=&lt;value optimized out&gt;, locals=&lt;value optimized out&gt;,
    args=&lt;value optimized out&gt;, argcount=1, kws=&lt;value optimized out&gt;, kwcount=1, defs=0x37a6468, defcount=1, closure=0x0)
    at Python/ceval.c:3253
#18 0x00007ffff7d1e4a1 in fast_function (f=&lt;value optimized out&gt;, throwflag=&lt;value optimized out&gt;) at Python/ceval.c:4117
#19 call_function (f=&lt;value optimized out&gt;, throwflag=&lt;value optimized out&gt;) at Python/ceval.c:4042
#20 PyEval_EvalFrameEx (f=&lt;value optimized out&gt;, throwflag=&lt;value optimized out&gt;) at Python/ceval.c:2666
#21 0x00007ffff7d20130 in PyEval_EvalCodeEx (co=0x7ffff7b26830, globals=&lt;value optimized out&gt;, locals=&lt;value optimized out&gt;,
    args=&lt;value optimized out&gt;, argcount=0, kws=&lt;value optimized out&gt;, kwcount=3, defs=0x0, defcount=0, closure=0x0)
    at Python/ceval.c:3253
#22 0x00007ffff7d1e4a1 in fast_function (f=&lt;value optimized out&gt;, throwflag=&lt;value optimized out&gt;) at Python/ceval.c:4117
#23 call_function (f=&lt;value optimized out&gt;, throwflag=&lt;value optimized out&gt;) at Python/ceval.c:4042
#24 PyEval_EvalFrameEx (f=&lt;value optimized out&gt;, throwflag=&lt;value optimized out&gt;) at Python/ceval.c:2666
#25 0x00007ffff7d1ec56 in fast_function (f=&lt;value optimized out&gt;, throwflag=&lt;value optimized out&gt;) at Python/ceval.c:4107
#26 call_function (f=&lt;value optimized out&gt;, throwflag=&lt;value optimized out&gt;) at Python/ceval.c:4042
#27 PyEval_EvalFrameEx (f=&lt;value optimized out&gt;, throwflag=&lt;value optimized out&gt;) at Python/ceval.c:2666
#28 0x00007ffff7d20130 in PyEval_EvalCodeEx (co=0x7ffff7b28ab0, globals=&lt;value optimized out&gt;, locals=&lt;value optimized out&gt;,
    args=&lt;value optimized out&gt;, argcount=0, kws=&lt;value optimized out&gt;, kwcount=0, defs=0x0, defcount=0, closure=0x0)
    at Python/ceval.c:3253
#29 0x00007ffff7d20242 in PyEval_EvalCode (co=&lt;value optimized out&gt;, globals=&lt;value optimized out&gt;, locals=&lt;value optimized out&gt;)
    at Python/ceval.c:667
#30 0x00007ffff7d3a62c in run_mod (mod=&lt;value optimized out&gt;, filename=&lt;value optimized out&gt;, globals=0x640160, locals=0x640160,
    flags=&lt;value optimized out&gt;, arena=&lt;value optimized out&gt;) at Python/pythonrun.c:1353
#31 0x00007ffff7d3a700 in PyRun_FileExFlags (fp=0x6c69c0, filename=0x7fffffffe137 "infer.py", start=&lt;value optimized out&gt;, globals=
    0x640160, locals=0x640160, closeit=1, flags=0x7fffffffdd10) at Python/pythonrun.c:1339
#32 0x00007ffff7d3bc0c in PyRun_SimpleFileExFlags (fp=0x6c69c0, filename=0x7fffffffe137 "infer.py", closeit=1, flags=0x7fffffffdd10)
    at Python/pythonrun.c:943
---Type &lt;return&gt; to continue, or q &lt;return&gt; to quit---start infer.pyq
#33 0x00007ffff7d4d4cc in Py_Main (argc=&lt;value optimized out&gt;, argv=&lt;value optimized out&gt;) at Modules/main.c:639
#34 0x000000318ae1ecdd in __libc_start_main () from /lib64/libc.so.6
#35 0x0000000000400659 in _start ()
(gdb) f 1
#1  0x00007fffefaad23f in paddle::MultiGradientMachine::getOutArgs(std::vector&lt;paddle::Argument, std::allocator&lt;paddle::Argument&gt; &gt;*,
 paddle::enumeration_wrapper::PassType) () at /home/lizhao/Paddle/paddle/gserver/gradientmachines/MultiGradientMachine.h:354
354       void waitOutArgsReady() { outArgsReadySem_.wait(); }
(gdb) l
349
350       void start();
351
352       void onPassEnd() { gradientMachine_-&gt;onPassEnd(); }
353
354       void waitOutArgsReady() { outArgsReadySem_.wait(); }
355
356       void notifyTaskReady() { taskReadySem_.post(); }
357
358       int getDeviceId() const { return deviceId_; }
(gdb) i threads
  29 Thread 0x7fffa29cc700 (LWP 15404)  0x000000318aeddfc3 in poll () from /lib64/libc.so.6
  28 Thread 0x7fffa33cd700 (LWP 15403)  0x000000318aee99af in accept4 () from /lib64/libc.so.6
* 1 Thread 0x7ffff7c3b700 (LWP 14790)  0x000000318b20d720 in sem_wait () from /lib64/libpthread.so.0
		</comment>
		<comment id='2' author='lcy-seso' date='2017-06-22T12:02:23Z'>
		And the same as &lt;denchmark-link:https://github.com/lcy-seso&gt;@lcy-seso&lt;/denchmark-link&gt;
 , when I set , the first  result is , which is obviously wrong.
		</comment>
		<comment id='3' author='lcy-seso' date='2017-06-23T23:59:52Z'>
		I found this issue is duplicate to &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/issues/2534&gt;#2534&lt;/denchmark-link&gt;
, so I close it. We can trace the problem-solving process in  &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/issues/2534&gt;#2534&lt;/denchmark-link&gt;
.
		</comment>
	</comments>
</bug>