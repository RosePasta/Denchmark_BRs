<bug id='16248' author='xiaolil1' open_date='2019-03-18T07:40:45Z' closed_time='2019-04-01T02:01:37Z'>
	<summary>Performance Regression Caused by Tensor Modification.</summary>
	<description>
We found performance regression measured by single batch size and 28 cores in our regular test. We root-caused the regression was caused by the code change of "set_format" introduced by Tensor modification, which commit ID is &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/commit/dec9cf53c89e0acc605a053b436ba24be68f62c7&gt;dec9cf5&lt;/denchmark-link&gt;
.
There are 5% regression for FP32, and over 10% regression for INT8 on CLX (below table):



INT8 ( BS = 1, Cores = 28)
ResNet-50
MobileNet
SSD-MobileNet




Current (dec9c)
236
494
169


Last (08c96)
265
536
185


Curr / Last
0.89
0.92
0.92






FP32 ( BS = 1, Cores = 28)
ResNet-50
MobileNet
SSD-MobileNet




Current (dec9c)
97
298
116


Last (08c96)
100
309
122


Curr / Last
0.97
0.97
0.95



The code implementation always creates the primitive descriptor in "set_format", which seems unnecessary and brings potential performance overhead. Moreover, “set_format” accepts the default data type FP32, however, the API should consider both FP32 and INT8. (e.g., conv_mkldnn_op.cc:632)
	</description>
	<comments>
		<comment id='1' author='xiaolil1' date='2019-03-18T07:43:51Z'>
		&lt;denchmark-link:https://github.com/jacek&gt;@jacek&lt;/denchmark-link&gt;
 @kbinas &lt;denchmark-link:https://github.com/tensor-tang&gt;@tensor-tang&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/luotao&gt;@luotao&lt;/denchmark-link&gt;

Please help to solve this issue, thanks!
		</comment>
		<comment id='2' author='xiaolil1' date='2019-03-18T07:44:52Z'>
		&lt;denchmark-link:https://github.com/kbinias&gt;@kbinias&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='xiaolil1' date='2019-03-18T08:13:43Z'>
		This should be inappropriate adding this hard code in paddle &lt;denchmark-link:https://github.com/jczaja&gt;@jczaja&lt;/denchmark-link&gt;
 :


If MKL-DNN memory structure changed, such as update version, then this should crash at somewhere unknown inside  MKLDNN without any error information in paddle side.
And this should have some bugs, for example, if the dims is larger than 5 which is very normal for users, it should fail and throw std::exception at creating MKLDNN pd mkldnn::memory::primitive_desc(mem_fmt, cpu_engine) since is not support by MKLDNN.
		</comment>
		<comment id='4' author='xiaolil1' date='2019-03-18T13:49:12Z'>
		&lt;denchmark-link:https://github.com/tensor-tang&gt;@tensor-tang&lt;/denchmark-link&gt;
 This general way of creating non-blocked format was suggested by MKL-DNN team:
&lt;denchmark-link:https://github.com/oneapi-src/oneDNN/issues/156#issuecomment-345772775&gt;oneapi-src/oneDNN#156 (comment)&lt;/denchmark-link&gt;

You are right, that there is a risk they may change structure, but currently memory format names: nchw, ncw, x etc. are not enough to represent more dimensional data. Starting from MKL-DNN 1.0 this will be the way to define memory format eg. Logical dims + strides it just MKL-DNN will provide function for that , so we do not have to write ourselves.
I do not quite understand why is should fail for more than 5 dims tensors. This function is used in Transpose MKLDNN op and there is unit tests and one of them (TestCase4) is creating 6D data. And it does work. So I do not quite understand why exception should happen? In fact MKL-DNN does support memory primitives up to 10 dims, but its computational primitives do support no more that 4-5 dims.  Reorder does support all dimensionality available (upto 10).
		</comment>
		<comment id='5' author='xiaolil1' date='2019-03-18T13:51:53Z'>
		Regarding performance degradation.  Tensor modifications is work in progress We managed to transform most of FP32 ops (We miss pooling &amp; elementwise_mul) , but no of INT8 ops were transformed. This is work to be done and I will be working on that. At the end set_format should not be used and set_mkldnn_prim_desc should be used. When All functionality is implemented then performance degradation should go away. So we are working on that, thanks for letting me know on fp32 &amp; int8 performance degradation
		</comment>
		<comment id='6' author='xiaolil1' date='2019-03-18T15:50:40Z'>
		Thanks for your explanations &lt;denchmark-link:https://github.com/jczaja&gt;@jczaja&lt;/denchmark-link&gt;


This general way of creating non-blocked format was suggested by MKL-DNN team:
oneapi-src/oneDNN#156 (comment)

Firstly, by saying "This should be inappropriate adding this hard code in paddle", I am not saying this way is wrong, instead, I am say why this code should be in paddle. I think this is something highly related with MKL-DNN itself, which we should just call mkldnn's API.

I do not quite understand why is should fail for more than 5 dims tensors, ..., In fact MKL-DNN does support memory primitives up to 10 dims,

Secondly, here 5 is just an example, maybe it's 6 or other number,  we do not know how much users would use. And the max dim size of the data memory, not the weight memory, in mkldnn is not larger than 5 I remember, maybe I am wrong with this number. Anyway, the point is it may cause some crash if something does not suit MKL-DNN's rules. Maybe it's the dims size, or maybe some combination of dims and dtype, or something I did not figure out right now.

This is work to be done and I will be working on that. At the end set_format should not be used and set_mkldnn_prim_desc should be used. So we are working on that

It seems no doc saying that we should avoid calling  and I found so much &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/search?q=set_format&amp;unscoped_q=set_format&gt;references&lt;/denchmark-link&gt;
 of this function including mkldnn_conv .

set_format should not be used and set_mkldnn_prim_desc should be used

So what if I just want to set_format, then I have to force using set_mkldnn_prim_desc instead? and needs more infos like dims, data_type and engine, does not make sense, right?
And replacing with set_mkldnn_prim_desc in tensor.h does not solve the second problem as well.
		</comment>
		<comment id='7' author='xiaolil1' date='2019-03-18T16:24:28Z'>
		Anyway, if you've already plan or schedule to fix this, please go ahead and make it done ASAP.
And since it is added in tensor.h, please make sure the fix would cover both the fp32 and int8, and of course the performance issue here.
Thanks very much.
		</comment>
		<comment id='8' author='xiaolil1' date='2019-03-20T07:05:51Z'>
		
Thanks for your explanations @jczaja

This general way of creating non-blocked format was suggested by MKL-DNN team:
intel/mkl-dnn#156 (comment)

Firstly, by saying "This should be inappropriate adding this hard code in paddle", I am not saying this way is wrong, instead, I am say why this code should be in paddle. I think this is something highly related with MKL-DNN itself, which we should just call mkldnn's API.

I do not quite understand why is should fail for more than 5 dims tensors, ..., In fact MKL-DNN does support memory primitives up to 10 dims,

Secondly, here 5 is just an example, maybe it's 6 or other number, we do not know how much users would use. And the max dim size of the data memory, not the weight memory, in mkldnn is not larger than 5 I remember, maybe I am wrong with this number. Anyway, the point is it may cause some crash if something does not suit MKL-DNN's rules. Maybe it's the dims size, or maybe some combination of dims and dtype, or something I did not figure out right now.

This is work to be done and I will be working on that. At the end set_format should not be used and set_mkldnn_prim_desc should be used. So we are working on that

It seems no doc saying that we should avoid calling set_format and I found so much references of this function including mkldnn_conv .

set_format should not be used and set_mkldnn_prim_desc should be used

So what if I just want to set_format, then I have to force using set_mkldnn_prim_desc instead? and needs more infos like dims, data_type and engine, does not make sense, right?
And replacing with set_mkldnn_prim_desc in tensor.h does not solve the second problem as well.

Yes, activation allows the max dimension 5 (e.g., ncdhw), while weight allows more dimensions. &lt;denchmark-link:https://github.com/tensor-tang&gt;@tensor-tang&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jczaja&gt;@jczaja&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='xiaolil1' date='2019-03-20T10:59:01Z'>
		First (out of two) PR: &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/16325&gt;#16325&lt;/denchmark-link&gt;
 was set up to fix that issue. It does adapt pooling &amp; conv2d int8 to tensor modifications. So some performance gain should be visible, but for restoring full performance transpose  * quantize* ops should adapted as well (next PR will have it done).
&lt;denchmark-link:https://github.com/xiaolil1&gt;@xiaolil1&lt;/denchmark-link&gt;
 If you could test &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/16325&gt;#16325&lt;/denchmark-link&gt;
 for Int8 performance then  it would be very helpful to hear if you notice any int8 performance improvement.
		</comment>
		<comment id='10' author='xiaolil1' date='2019-03-21T09:36:50Z'>
		&lt;denchmark-link:https://github.com/jczaja&gt;@jczaja&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tensor-tang&gt;@tensor-tang&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/luotao1&gt;@luotao1&lt;/denchmark-link&gt;

We tested the patch of &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/16325&gt;#16325&lt;/denchmark-link&gt;
 both on public paddle and local paddle on CLX (below table):
It seems the performance issue was not been fixed completely.
Public code:



INT8 ( BS = 1, Cores = 28)
ResNet-50
MobileNet




Current (da39a)
258
616


Last (08c96)
262
538


Curr / Last
0.98
1.15






FP32 ( BS = 1, Cores = 28)
ResNet-50
MobileNet




Current (da39a)
91
279


Last (08c96)
100
307


Curr / Last
0.91
0.91



Local code:



INT8 ( BS = 1, Cores = 28)
ResNet-50
MobileNet




Current (02304)
260
629


Last (b4ce6)
281
707


Curr / Last
0.92
0.89






FP32 ( BS = 1, Cores = 28)
ResNet-50
MobileNet




Current (02304)
91
279


Last (b4ce6)
96
298


Curr / Last
0.96
0.93



		</comment>
		<comment id='11' author='xiaolil1' date='2019-03-21T10:28:33Z'>
		&lt;denchmark-link:https://github.com/xiaolil1&gt;@xiaolil1&lt;/denchmark-link&gt;
 Thanks very much for testing &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/16325&gt;#16325&lt;/denchmark-link&gt;
. However I'm not sure how to interpret them. It seems
you are comparing some old codebase(Last) vs Patch+develop(Current). I think it would be more clear If I could see comparison of develop vs develop+patch, as meantime lots of other functionality was introduced.
Meantime , I will carry on working on adapting other ops: transpose int8, *quantize*
		</comment>
		<comment id='12' author='xiaolil1' date='2019-03-21T14:54:19Z'>
		&lt;denchmark-link:https://github.com/xiaolil1&gt;@xiaolil1&lt;/denchmark-link&gt;
 From your results, We understood that INT8 performance was improved, but FP32 performance
went down. This situation worried us, so we tested FP32 performance.
According to our results (tested on SKX and CLX) the above changes on the PR &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/16325&gt;#16325&lt;/denchmark-link&gt;
 for FP32  but improve performance for INT8 (based on your results). We compared 2 commits: &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/commit/d3acf680445a7cadd05949a7cc7bbb95a0c82641&gt;d3acf68&lt;/denchmark-link&gt;
 (before changes), &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/commit/ab5071d92b5b165ae7fca4771a2f707354ab1fde&gt;ab5071d&lt;/denchmark-link&gt;
 (Partial fix to performance regression &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/16325&gt;#16325&lt;/denchmark-link&gt;
), tested latency (batch=1, thread=1) for FP32 on CLX 8280.
&lt;denchmark-h:h4&gt;Results for latency, ResNet-50 FP32 (BS=1, Cores=1)&lt;/denchmark-h&gt;


d3acf68 (before changes) 102ms
ab5071d (partial fix) 101ms

&lt;denchmark-h:h4&gt;Command for reproducing our results:&lt;/denchmark-h&gt;

test_analyzer_resnet50 --infer_model=/inference_demo/resnet50/model --repeat=100 --gtest_filter=Analyzer_resnet50.profile_mkldnn --batch_size=1 --num_threads=1 --repeat=1000 --profile
&lt;denchmark-link:https://github.com/xiaolil1&gt;@xiaolil1&lt;/denchmark-link&gt;
 Could you share your command line for reproducing your FP32 results ?
		</comment>
		<comment id='13' author='xiaolil1' date='2019-03-22T06:59:35Z'>
		&lt;denchmark-link:https://github.com/kbinias&gt;@kbinias&lt;/denchmark-link&gt;

We tested &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/16325&gt;#16325&lt;/denchmark-link&gt;
 on "latest develop vs latest develop + patch"
and "reference (develop without regression, commit 08c96) vs latest develop + patch".
The result (below table) shows INT8 performance was improved but there are still performance regression in FP32.



INT8 ( BS = 1, Cores = 28)
ResNet-50 ( FPS )
MobileNet ( FPS )




patched
259
629


develop
247
601


reference
262
538


patched / develop
1.05
1.05


patched / reference
0.99
1.17






FP32 ( BS = 1, Cores = 28)
ResNet-50 ( FPS )
MobileNet ( FPS )




patched
91
279


develop
91
282


reference
100
307


patched / develop
1.01
0.99


patched / reference
0.91
0.91



Please note that our test is on CLX with single batch and one socket ( 28 cores ).
You can use this command line to reproduce FP32 result:
taskset -c 0-27 numactl -l ./capi/build/infer_image_classification --use_fake_data=True --skip_batch_num=10 --profile --use_mkldnn --with_labels=false --infer_model=weights/resnet50-fp32 --batch_size=1 --iterations=1000 --paddle_num_threads 28
		</comment>
		<comment id='14' author='xiaolil1' date='2019-03-22T08:45:09Z'>
		&lt;denchmark-link:https://github.com/xiaolil1&gt;@xiaolil1&lt;/denchmark-link&gt;

Thanks alot for testing &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/16325&gt;#16325&lt;/denchmark-link&gt;
 . So it seems after patching code, INT8 performance went up, but FP32 is same it on develop . Is my understanding  correct?
Regarding FP32 measures, First time (begining of this thread)  results from develop were:



Current (dec9c)
97
298



But now  develop results are (Third performance measures you sent):



develop
91
282



So, it seems meantime there was performance regression in FP32 on develop, or something in configuration of platform used for taking performance measures changed . Please explain
		</comment>
		<comment id='15' author='xiaolil1' date='2019-03-22T09:34:53Z'>
		Hi &lt;denchmark-link:https://github.com/jczaja&gt;@jczaja&lt;/denchmark-link&gt;
 , we used the standard configuration in our cluster to run the regular test. So, I believe there is no issues in HW configurations. You are probably right there are some potential FP32 regression caused by other PRs. Would you please identify the regression PR using the automatic command like git-bisect?
		</comment>
		<comment id='16' author='xiaolil1' date='2019-03-24T11:22:57Z'>
		&lt;denchmark-link:https://github.com/xiaolil1&gt;@xiaolil1&lt;/denchmark-link&gt;
 I updated &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/16325&gt;#16325&lt;/denchmark-link&gt;
 , We see Fp32 performance improvement with it (ResNet50, SKX and CLX), but We haven't tested int8. Moreover due to potential other performance regression , We cannot tell if &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/16325&gt;#16325&lt;/denchmark-link&gt;
 is good enough to cover performance gap that tensor modifications introduced. So If possible please test &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/16325&gt;#16325&lt;/denchmark-link&gt;
 (both Int8, fp32 , patched and develop) and tell us if this is good enough  improvement. If you still think that performance problem is there then We start work on reverting tensor modifications.
As for potential other performance regression. Some work started led by &lt;denchmark-link:https://github.com/kbinias&gt;@kbinias&lt;/denchmark-link&gt;
. Perhaps he can comment on that one.
		</comment>
		<comment id='17' author='xiaolil1' date='2019-03-24T14:50:29Z'>
		&lt;denchmark-link:https://github.com/xiaolil1&gt;@xiaolil1&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/jczaja&gt;@jczaja&lt;/denchmark-link&gt;

There are 2 performance regressions for fp32:

Between commits 08c96d1 and dec9cf5 (reason: tensor modification, ~3 fps)
Between commits 9e2c7e6 and 3c60446 (reason: MKL-DNN update from 863ff6e to 830a100, ~2 fps, use git diff 9e2c7e6..3c60446 to see all changes)

		</comment>
		<comment id='18' author='xiaolil1' date='2019-03-25T06:01:29Z'>
		hi &lt;denchmark-link:https://github.com/jczaja&gt;@jczaja&lt;/denchmark-link&gt;
, we verified INT8 and FP32 latency performance ( below table ), the result shows the FP32 regression was not fixed. Please compare to the reference version: &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/commit/08c96d1b484ffd3614a797dd1e8ee6de83de9a82&gt;08c96d1&lt;/denchmark-link&gt;
, use this command line:
taskset -c 0-27 numactl -l ./capi/build/infer_image_classification --use_fake_data=True --skip_batch_num=10 --profile --use_mkldnn --with_labels=false --infer_model=weights/resnet50-fp32 --batch_size=1 --iterations=1000 --paddle_num_threads 28



INT8 ( BS = 1, Cores = 28)
ResNet-50 ( FPS )
MobileNet ( FPS )




patched
259
616


develop
247
602


reference
262
538


patched / develop
1.05
1.02


patched / reference
0.99
1.15






FP32 ( BS = 1, Cores = 28)
ResNet-50 ( FPS )
MobileNet ( FPS )




patched
92
277


develop
91
281


reference
100
307


patched / develop
1.01
0.99


patched / reference
0.92
0.91



		</comment>
		<comment id='19' author='xiaolil1' date='2019-03-25T08:50:55Z'>
		&lt;denchmark-link:https://github.com/jczaja&gt;@jczaja&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/kbinias&gt;@kbinias&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/luotao1&gt;@luotao1&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jianhang-liu&gt;@jianhang-liu&lt;/denchmark-link&gt;

On the two points identified from &lt;denchmark-link:https://github.com/kbinias&gt;@kbinias&lt;/denchmark-link&gt;
, I also communicated with &lt;denchmark-link:https://github.com/luotao1&gt;@luotao1&lt;/denchmark-link&gt;
. Here are my understandings and suggestions:


Between commits 08c96d1 and dec9cf5 (reason: tensor modification, ~3 fps). @kbinias Did you have a chance to investigate further and figure out the exact commit that leads to the regression? 3 fps seems does not help improve much.


Between commits 9e2c7e6 and 3c60446 (reason: MKL-DNN update from 863ff6e to 830a100, ~2 fps, use git diff 9e2c7e6..3c60446 to see all changes). 2 fps is not so much, so I would suggest to keep the latest MKL-DNN commit.


		</comment>
		<comment id='20' author='xiaolil1' date='2019-03-25T09:10:03Z'>
		&lt;denchmark-link:https://github.com/luotao1&gt;@luotao1&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jianhang-liu&gt;@jianhang-liu&lt;/denchmark-link&gt;
 I also discussed with &lt;denchmark-link:https://github.com/kbinias&gt;@kbinias&lt;/denchmark-link&gt;
, my suggestion is to investigate the exact commit that leads to the FP32 performance regression. If it is related with tensor modification, we need to follow up the instructions: either fixing it today or reverting the change; otherwise, we can merge this &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/16325&gt;#16325&lt;/denchmark-link&gt;
 first and &lt;denchmark-link:https://github.com/kbinias&gt;@kbinias&lt;/denchmark-link&gt;
, please file another issue to track it.
Moreover, &lt;denchmark-link:https://github.com/xiaolil1&gt;@xiaolil1&lt;/denchmark-link&gt;
 thought to keep the set_format API: &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/16325#issuecomment-476047424&gt;#16325 (comment)&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='21' author='xiaolil1' date='2019-03-25T09:14:44Z'>
		&lt;denchmark-link:https://github.com/hshen14&gt;@hshen14&lt;/denchmark-link&gt;
 Does &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/16325&gt;#16325&lt;/denchmark-link&gt;
 fix the INT8 performance regression?
		</comment>
		<comment id='22' author='xiaolil1' date='2019-03-25T09:15:46Z'>
		
@hshen14 Does #16325 fix the INT8 performance regression?

Yes, INT8 is good with &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/16325&gt;#16325&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='23' author='xiaolil1' date='2019-03-25T09:22:10Z'>
		&lt;denchmark-link:https://github.com/hshen14&gt;@hshen14&lt;/denchmark-link&gt;
 Is this &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/16325#issuecomment-476047424&gt;#16325 (comment)&lt;/denchmark-link&gt;
 fixed by &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/16325&gt;#16325&lt;/denchmark-link&gt;
 ?
		</comment>
		<comment id='24' author='xiaolil1' date='2019-03-25T09:25:55Z'>
		
@hshen14 Is this #16325 (comment) fixed by #16325 ?

No, that is not fixed yet. I understand INT8 performance regression is fixed.
		</comment>
		<comment id='25' author='xiaolil1' date='2019-03-25T12:44:40Z'>
		It seems that Int8 works fine while FP32 still is behind expectations. Reason I can think of is convolution mkldnn op integration. INt8 and FP32 got separate integration functions. Perhaps something is wrong fp32 integration.
&lt;denchmark-link:https://github.com/xiaolil1&gt;@xiaolil1&lt;/denchmark-link&gt;
  You used  If you have still your logs, then could you take a look and confirm that it is performance of convolution op that went down ?
I will look for possible problems in conv fp32 .
		</comment>
		<comment id='26' author='xiaolil1' date='2019-03-26T10:11:31Z'>
		&lt;denchmark-link:https://github.com/tensor-tang&gt;@tensor-tang&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/xiaolil1&gt;@xiaolil1&lt;/denchmark-link&gt;
 I will postpone work on PR &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/16325&gt;#16325&lt;/denchmark-link&gt;
 , and prepare #PR with reverted tensor modifications , so we can test performance on reverted changes.
		</comment>
		<comment id='27' author='xiaolil1' date='2019-03-27T13:01:43Z'>
		&lt;denchmark-link:https://github.com/tensor-tang&gt;@tensor-tang&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/xiaolil1&gt;@xiaolil1&lt;/denchmark-link&gt;
 PR with revert of tensor modifications is &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/16462&gt;#16462&lt;/denchmark-link&gt;
 .  Please test performance, review and merge if ok.
		</comment>
	</comments>
</bug>