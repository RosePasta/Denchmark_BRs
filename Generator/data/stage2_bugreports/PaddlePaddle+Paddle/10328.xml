<bug id='10328' author='reyoung' open_date='2018-05-02T04:23:50Z' closed_time='2018-08-15T10:27:30Z'>
	<summary>Protential memcpy and malloc error on CUDA</summary>
	<description>
According to the manual of CUDA, cudaMallocHost is the suggested API for allocating CPU memory to data exchange between GPU and CPU.

Allocates size bytes of host memory that is page-locked and accessible to the device. The driver tracks the virtual memory ranges allocated with this function and automatically accelerates calls to functions such as cudaMemcpy*(). Since the memory can be accessed directly by the device, it can be read or written with much higher bandwidth than pageable memory obtained with functions such as malloc(). Allocating excessive amounts of memory with cudaMallocHost() may degrade system performance, since it reduces the amount of memory available to the system for paging. As a result, this function is best used sparingly to allocate staging areas for data exchange between host and device.

We should use cudaMallcHost for CPUAllocator, because

We use BuddyAllocator to wrap device allocator in memory. Even cudaMallocHost is much slower than malloc, we just invoke cudaMallocHost once to alloc memory buffer and use buddy allocator to manage the rest allocating.
All CPU memory of tensor mlock to physical memory. This is our current memory strategy. Introducing cudaMallocHost will not change this strategy.



Paddle/paddle/fluid/memory/detail/system_allocator.cc


        Lines 59 to 60
      in
      9a8be9d






 *index = 1; 



 mlock(p, size);  // lock memory 






However, when we change the  to , we found several bugs in Fluid. The PR is &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/10136&gt;#10136&lt;/denchmark-link&gt;
.


there are many places in our code, which invoke cudaMemcpyAsync, are not actually wait or synchronized.

When using malloc and cudaMemcpyAsync, it will copy the CPU memory into a staging area of CUDA driver and then invoke cudaMemcpyAsync to that staging area. It is a synchronized method in CPU and it is slow.  If we want to MemcpyAsync, we should not use malloc for data exchanging.



In our experiment, cudaMemcpyAsync on a tiny data buffer and invoke cudaSynchronizeStream() immediately, the memcpy may not be synchronized in multi-threads, multi-streams configuration. It is strange that it should be able to synchronize.  However, since it is not the best practice to use malloc memory for data exchange and the result is correct when using cudaMallocHost, we will use cudaMallocHost for CPUAllocator to avoid this bug.


	</description>
	<comments>
		<comment id='1' author='reyoung' date='2018-08-15T10:27:28Z'>
		您好，此issue在近一个月内暂无更新，我们将于今天内关闭。若在关闭后您仍需跟进提问，可重新开启此问题，我们将在24小时内回复您。因关闭带来的不便我们深表歉意，请您谅解~感谢您对PaddlePaddle的支持!
Hello, this issue has not been updated in the past month. We will close it today for the sake of other user‘s experience. If you still need to follow up on this question after closing, please feel free to reopen it. In that case, we will get back to you within 24 hours. We apologize for the inconvenience caused by the closure and thank you so much for your support of PaddlePaddle Group!
		</comment>
	</comments>
</bug>