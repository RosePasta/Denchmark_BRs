<bug id='25506' author='cryoco' open_date='2020-07-13T13:36:46Z' closed_time='2020-07-29T03:22:14Z'>
	<summary>Memory leaks when doing CPU inference with MKLDNN</summary>
	<description>
System information
Paddle version: 1.8.2
Paddle With CUDA: False
OS: Ubuntu 16.04
CPU: 16  Intel(R) Xeon(R) CPU E5-2620 v3 @ 2.40GHz
Python version: 3.5.2
CUDA version: 9.0.176
cuDNN version: None.None.None
Nvidia driver version: None
API information: inference configuration
config.disable_gpu()
config.enable_mkldnn()
config.set_cpu_math_library_num_threads(4)
To Reproduce

download models and data


ocr detection model
ocr recognition model
test data
The test data pictures are binary files saved with np.save, and can be loaded with np.load.


perform inference with mkldnn
code to reproduce:
test_ocr_mkldnn_mem.txt(rename to test_ocr_mkldnn_mem.py)


detection

python3 test_ocr_mkldnn_mem.py --model_file=./ch_det_mv3_db/model --params_file=./ch_det_mv3_db/params --mode=det --mkldnn

recognition

python3 test_ocr_mkldnn_mem.py --model_file=./ch_rec_mv3_crnn/model --params_file=./ch_rec_mv3_crnn/params --mode=rec --mkldnn
Memory usage increasing can be witnessed with top command.

perform inference without mkldnn


detection

python3 test_ocr_mkldnn_mem.py --model_file=./ch_det_mv3_db/model --params_file=./ch_det_mv3_db/params --mode=det 

recognition

python3 test_ocr_mkldnn_mem.py --model_file=./ch_rec_mv3_crnn/model --params_file=./ch_rec_mv3_crnn/params --mode=rec
Memory usage will maintain stable.
The input shapes we use in OCR are dynamic, which might be relevant to this issue.
	</description>
	<comments>
		<comment id='1' author='cryoco' date='2020-07-13T14:16:53Z'>
		Please use SetMkldnnCacheCapacity interface.



Paddle/paddle/fluid/inference/api/paddle_analysis_config.h


        Lines 348 to 355
      in
      126d3d6






 /// \brief Set the cache capacity of different input shapes for MKLDNN. 



 /// Default value 0 means not caching any shape. 



 /// Please see MKL-DNN Data Caching Design Document: 



 /// https://github.com/PaddlePaddle/FluidDoc/blob/develop/doc/fluid/design/mkldnn/caching/caching.md 



 /// 



 /// \param capacity The cache capacity. 



 /// 



 void SetMkldnnCacheCapacity(int capacity); 






It is used in paddle/fluid/inference/tests/api/analyzer_detect_tester.cc, which is dynamic shape as well.
This interface only has C++/C API, you may wrap a python API for it.

		</comment>
		<comment id='2' author='cryoco' date='2020-07-14T07:01:38Z'>
		
Please use SetMkldnnCacheCapacity interface.



Paddle/paddle/fluid/inference/api/paddle_analysis_config.h


        Lines 348 to 355
      in
      126d3d6






 /// \brief Set the cache capacity of different input shapes for MKLDNN. 



 /// Default value 0 means not caching any shape. 



 /// Please see MKL-DNN Data Caching Design Document: 



 /// https://github.com/PaddlePaddle/FluidDoc/blob/develop/doc/fluid/design/mkldnn/caching/caching.md 



 /// 



 /// \param capacity The cache capacity. 



 /// 



 void SetMkldnnCacheCapacity(int capacity); 






It is used in paddle/fluid/inference/tests/api/analyzer_detect_tester.cc, which is dynamic shape as well.
This interface only has C++/C API, you may wrap a python API for it.


Tried setting MKLDNN cache capacity to 10/20/100/1024, but still got the memory leak :(
		</comment>
		<comment id='3' author='cryoco' date='2020-07-14T08:15:29Z'>
		Added python api set_mkldnn_cache_capacity in &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/25524&gt;PR#25524&lt;/denchmark-link&gt;
, which might be useful in debugging and solving this issue.
		</comment>
		<comment id='4' author='cryoco' date='2020-07-15T12:33:47Z'>
		&lt;denchmark-link:https://github.com/cryoco&gt;@cryoco&lt;/denchmark-link&gt;

I got the same error as in &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/issues/25507#issuecomment-658228096&gt;#25507 (comment)&lt;/denchmark-link&gt;
. Is another model required here as well?
		</comment>
		<comment id='5' author='cryoco' date='2020-07-15T13:45:19Z'>
		This issue use the same model as &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/issues/25507&gt;#25507&lt;/denchmark-link&gt;
. And &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/issues/25507#issuecomment-658636558&gt;#25507 (comment)&lt;/denchmark-link&gt;
 updates the model link.
		</comment>
		<comment id='6' author='cryoco' date='2020-07-15T14:07:14Z'>
		&lt;denchmark-link:https://github.com/luotao1&gt;@luotao1&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/cryoco&gt;@cryoco&lt;/denchmark-link&gt;

I took the model from &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/issues/25507#issuecomment-658636558&gt;#25507 (comment)&lt;/denchmark-link&gt;
 and got the following error with the script  (with or without the  option; this error does not occur when running the script  from the issue &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/issues/25507&gt;#25507&lt;/denchmark-link&gt;
 on the same model) :
&lt;denchmark-code&gt;$ python test_ocr_mkldnn_mem.py --model_file=ch_det_mv3_db/model --params_file=ch_det_mv3_db/params
...
I0715 09:04:47.246189 30616 naive_executor.cc:95] ---  skip [feed], feed -&gt; image
I0715 09:04:47.247287 30616 naive_executor.cc:95] ---  skip [concat_1.tmp_0], fetch -&gt; fetch
Traceback (most recent call last):
  File "test_ocr_mkldnn_mem.py", line 88, in &lt;module&gt;
    result = run(pred)
  File "test_ocr_mkldnn_mem.py", line 42, in run
    predictor.zero_copy_run()
paddle.fluid.core_avx.EnforceNotMet:

--------------------------------------------
C++ Call Stacks (More useful to developers):
--------------------------------------------
0   std::string paddle::platform::GetTraceBackString&lt;std::string const&amp;&gt;(std::string const&amp;, char const*, int)
1   paddle::platform::EnforceNotMet::EnforceNotMet(std::string const&amp;, char const*, int)
2   paddle::operators::GetBroadcastDimsArrays(paddle::framework::DDim const&amp;, paddle::framework::DDim const&amp;, int*, int*, int*, int, int)
3   paddle::operators::ElementwiseOp::InferShape(paddle::framework::InferShapeContext*) const
4   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&amp;, paddle::platform::Place const&amp;, paddle::framework::RuntimeContext*) const
5   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&amp;, paddle::platform::Place const&amp;) const
6   paddle::framework::OperatorBase::Run(paddle::framework::Scope const&amp;, paddle::platform::Place const&amp;)
7   paddle::framework::NaiveExecutor::Run()
8   paddle::AnalysisPredictor::ZeroCopyRun()

------------------------------------------
Python Call Stacks (More useful to users):
------------------------------------------
  File "/root/miniconda3/envs/ocrpy36/lib/python3.6/site-packages/paddle/fluid/framework.py", line 2525, in append_op
    attrs=kwargs.get("attrs", None))
  File "/root/miniconda3/envs/ocrpy36/lib/python3.6/site-packages/paddle/fluid/layer_helper.py", line 43, in append_op
    return self.main_program.current_block().append_op(*args, **kwargs)
  File "/root/miniconda3/envs/ocrpy36/lib/python3.6/site-packages/paddle/fluid/layers/nn.py", line 10348, in _elementwise_op
    'use_mkldnn': use_mkldnn})
  File "/root/miniconda3/envs/ocrpy36/lib/python3.6/site-packages/paddle/fluid/layers/nn.py", line 10521, in elementwise_add
    return _elementwise_op(LayerHelper('elementwise_add', **locals()))
  File "/paddle/PaddleOCR/github/PaddleOCR/ppocr/modeling/heads/det_db_head.py", line 159, in __call__
    input=out4, scale=2), y=in3)  # 1/8
  File "/paddle/PaddleOCR/github/PaddleOCR/ppocr/modeling/architectures/det_model.py", line 112, in __call__
    predicts = self.head(conv_feas)
  File "/paddle/PaddleOCR/github/PaddleOCR/tools/program.py", line 193, in build_export
    image, outputs = model(mode='export')
  File "tools/export_model.py", line 67, in main
    config, eval_program, startup_prog)
  File "tools/export_model.py", line 93, in &lt;module&gt;
    main()

----------------------
Error Message Summary:
----------------------
InvalidArgumentError: Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 96, 4, 56] and the shape of Y = [1, 96, 4, 55]. Received [56] in X is not equal to [55] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || x_dims_array[i] &lt;= 1 || y_dims_array[i] &lt;= 1 == true, but received x_dims_array[i] == y_dims_array[i] || x_dims_array[i] &lt;= 1 || y_dims_array[i] &lt;= 1:0 != true:1.] at (/paddle/paddle/fluid/operators/elementwise/elementwise_op_function.h:157)
  [operator &lt; elementwise_add &gt; error]
&lt;/denchmark-code&gt;

Also, there is no --mode option available for the test_ocr_mkldnn_mem.py script.
		</comment>
		<comment id='7' author='cryoco' date='2020-07-15T14:46:31Z'>
		&lt;denchmark-link:https://github.com/wojtuss&gt;@wojtuss&lt;/denchmark-link&gt;

My apologize.
&lt;denchmark-link:https://paddleocr.bj.bcebos.com/ch_models/ch_det_mv3_db_infer.tar&gt;detection model&lt;/denchmark-link&gt;

&lt;denchmark-link:https://paddleocr.bj.bcebos.com/ch_models/ch_rec_mv3_crnn_infer.tar&gt;recognition model&lt;/denchmark-link&gt;

code:
&lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/files/4925953/test_ocr_mkldnn_mem.txt&gt;test_ocr_mkldnn_mem.txt&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='cryoco' date='2020-07-16T12:08:26Z'>
		&lt;denchmark-link:https://github.com/cryoco&gt;@cryoco&lt;/denchmark-link&gt;

Thank you! I reproduced the result and can confirm that MKLDNN cache is growing during the test execution. That is because in the models inputs have variable size and MKLDNN operators with different input size are being added to the cache over and over.
The cache grows less and less over time, however, setting limits to the cache size could also be helpful. Can you confirm that the PR &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/25524&gt;#25524&lt;/denchmark-link&gt;
 helps here?
		</comment>
		<comment id='9' author='cryoco' date='2020-07-16T14:03:07Z'>
		I know the reason:

#25524 only works on predictor.run(inputs, outputs, batch_size) before, since it needs to know the input's size. AnalysisPredictor::MkldnnPreSet() is used only in AnalysisPredictor::Run(inputs, outputs, batch_size).
Thus, it doesn't work on predictor.zero_copy_run()

&lt;denchmark-link:https://github.com/cryoco&gt;@cryoco&lt;/denchmark-link&gt;


Could you change to predictor.run(inputs, outputs, batch_size)?
Or you need predictor.zero_copy_run() support limits to the cache size as well? In this scenario, maybe we need to wrapper some new interface.

		</comment>
	</comments>
</bug>