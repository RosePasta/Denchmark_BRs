<bug id='9454' author='typhoonzero' open_date='2018-03-28T12:05:32Z' closed_time='2018-03-29T03:40:33Z'>
	<summary>Run distributed text_classification error proc param error:name:[fc_0.w_0@GRAD.block0.trainer_3] ep:[192.168.16.28:30256] grpc error:Connect Failed</summary>
	<description>
Background, run distributed vgg16 goes well but model:
&lt;denchmark-link:https://github.com/typhoonzero/fluid_gpu_benchmark/blob/master/text_fluid.py&gt;https://github.com/typhoonzero/fluid_gpu_benchmark/blob/master/text_fluid.py&lt;/denchmark-link&gt;
 results in following error first-time trainer want to send variables.
&lt;denchmark-code&gt;E0328 11:59:35.739938   211 grpc_client.cc:189] proc param error:name:[fc_0.w_0@GRAD.block3.trainer_3] ep:[192.168.16.27:30256] grpc error:Connect Failed
E0328 11:59:35.739984   208 grpc_client.cc:189] proc param error:name:[fc_0.b_0@GRAD.trainer_3] ep:[192.168.16.27:30256] grpc error:Connect Failed
E0328 11:59:35.740000   203 grpc_client.cc:189] proc param error:name:[sequence_conv_0.w_0@GRAD.block0.trainer_3] ep:[192.168.16.27:30256] grpc error:Connect Failed
E0328 11:59:35.740012   204 grpc_client.cc:189] proc param error:name:[embedding_0.w_0@GRAD.block0.trainer_3] ep:[192.168.16.27:30256] grpc error:Connect Failed
E0328 11:59:35.740399   202 grpc_client.cc:189] proc param error:name:[fc_1.b_0@GRAD.trainer_3] ep:[192.168.16.28:30256] grpc error:Connect Failed
E0328 11:59:35.740417   198 grpc_client.cc:189] proc param error:name:[sequence_conv_0.w_0@GRAD.block1.trainer_3] ep:[192.168.16.28:30256] grpc error:Connect Failed
E0328 11:59:35.740432   209 grpc_client.cc:189] proc param error:name:[embedding_0.w_0@GRAD.block1.trainer_3] ep:[192.168.16.28:30256] grpc error:Connect Failed
E0328 11:59:35.740423   207 grpc_client.cc:189] proc param error:name:[fc_0.w_0@GRAD.block0.trainer_3] ep:[192.168.16.28:30256] grpc error:Connect Failed
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='typhoonzero' date='2018-03-28T14:16:03Z'>
		Can not reproduce this on a single node to start 4 pservers and 4 trainers using different ports, but this issue can reproduce on two of our test kubernetes clusters, using hostNetwork mode.
		</comment>
		<comment id='2' author='typhoonzero' date='2018-03-29T03:40:33Z'>
		Connection error due to pserver starts very slow, trainer has to wait utils server is ready. Closing.
		</comment>
	</comments>
</bug>