<bug id='5005' author='typhoonzero' open_date='2017-10-23T02:06:13Z' closed_time='2018-05-13T11:46:22Z'>
	<summary>Fault tolerent job default global learning rate is not set</summary>
	<description>
Running fault tolerent job with pserver written in go currently go to this &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/trainer/NewRemoteParameterUpdater.cpp#L109&gt;line&lt;/denchmark-link&gt;

This is because the default learning rate for v2 API is always "poly": &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/trainer_config_helpers/optimizers.py#L362&gt;https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/trainer_config_helpers/optimizers.py#L362&lt;/denchmark-link&gt;
 , and the pserver does not support it yet. should implement "poly" method.
	</description>
	<comments>
		<comment id='1' author='typhoonzero' date='2017-10-23T18:23:30Z'>
		Even though we are using constant learning rate policy instead of "ploy", the model still does not converge, is it because the constant learning rate is not using the value from trainerConfig_.learning_rate() as the learning rate? And I have not see anywhere learning rate is set in that line, so I am curious what is the default value?
		</comment>
		<comment id='2' author='typhoonzero' date='2017-10-24T12:49:37Z'>
		&lt;denchmark-link:https://github.com/helinwang&gt;@helinwang&lt;/denchmark-link&gt;
 V2 API  will call &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/trainer_config_helpers/optimizers.py#L362&gt;https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/trainer_config_helpers/optimizers.py#L362&lt;/denchmark-link&gt;
 , the learning rate policy is "poly", and this configuration will be convert to protobuf format .
		</comment>
		<comment id='3' author='typhoonzero' date='2017-10-24T12:56:57Z'>
		I did some detailed test today, the result is:

Start a fault tolerant mnist training job using train_mnist_ft.py with 5 trainers and 2 pservers. The job is running fine and can converge.
Scale down the job by decreasing 2 trainer pod(kubectl create -f autoscale_load), the job continues fine.
Scale up the job by adding 2 trainer pod(kubectl delete -f autoscale_load). The newly created trainer does not converge but the old trainers does:

Newly scaled trainer pod log:
&lt;denchmark-code&gt;label selector: paddle-job-master=example, desired: 1
label selector: paddle-job=example, desired: 2
Starting training job:  /tmp, num_gradient_servers: 2, trainer_id:  1, version:
I1024 12:07:55.353288    32 Util.cpp:166] commandline:  --num_gradient_servers=2 --ports_num_for_sparse=1 --use_gpu=0 --trainer_id=1 --trainer_count=1 --num_passes=1 --ports_num=1 --port=7164
[INFO 2017-10-24 12:07:55,364 layers.py:2556] output for __conv_pool_0___conv: c = 20, h = 24, w = 24, size = 11520
[INFO 2017-10-24 12:07:55,405 layers.py:2684] output for __conv_pool_0___pool: c = 20, h = 12, w = 12, size = 2880
[INFO 2017-10-24 12:07:55,411 layers.py:2556] output for __conv_pool_1___conv: c = 50, h = 8, w = 8, size = 3200
[INFO 2017-10-24 12:07:55,417 layers.py:2684] output for __conv_pool_1___pool: c = 50, h = 4, w = 4, size = 800
I1024 12:07:55.534462    32 GradientMachine.cpp:94] Initing parameters..
I1024 12:07:55.543933    32 GradientMachine.cpp:101] Init parameters done.
run test here
run test here
run test here
run test here
run test here
run test here
run test here
run test here
run test here
Pass 9, Batch 0, Cost 2.980498, {'classification_error_evaluator': 0.9140625}
run test here
Pass 10, Batch 0, Cost 2.312326, {'classification_error_evaluator': 0.8984375}
run test here
Pass 11, Batch 0, Cost 2.306100, {'classification_error_evaluator': 0.8984375}
&lt;/denchmark-code&gt;

Old trainer pod log:
&lt;denchmark-code&gt;label selector: paddle-job-master=example, desired: 1
label selector: paddle-job=example, desired: 2
Starting training job:  /tmp, num_gradient_servers: 2, trainer_id:  0, version:
I1024 11:38:16.641094    36 Util.cpp:166] commandline:  --num_gradient_servers=2 --ports_num_for_sparse=1 --use_gpu=0 --trainer_id=0 --trainer_count=1 --num_passes=1 --ports_num=1 --port=7164
[INFO 2017-10-24 11:38:16,683 layers.py:2556] output for __conv_pool_0___conv: c = 20, h = 24, w = 24, size = 11520
[INFO 2017-10-24 11:38:16,686 layers.py:2684] output for __conv_pool_0___pool: c = 20, h = 12, w = 12, size = 2880
[INFO 2017-10-24 11:38:16,692 layers.py:2556] output for __conv_pool_1___conv: c = 50, h = 8, w = 8, size = 3200
[INFO 2017-10-24 11:38:16,734 layers.py:2684] output for __conv_pool_1___pool: c = 50, h = 4, w = 4, size = 800
I1024 11:38:16.764544    36 GradientMachine.cpp:94] Initing parameters..
I1024 11:38:16.767114    36 GradientMachine.cpp:101] Init parameters done.
Pass 0, Batch 0, Cost 3.109155, {'classification_error_evaluator': 0.8984375}
run test here
Pass 1, Batch 0, Cost 0.870059, {'classification_error_evaluator': 0.25}
run test here
Pass 2, Batch 0, Cost 0.689995, {'classification_error_evaluator': 0.2109375}
run test here
Pass 3, Batch 0, Cost 0.526903, {'classification_error_evaluator': 0.171875}
run test here
Pass 4, Batch 0, Cost 0.287425, {'classification_error_evaluator': 0.0859375}
run test here
Pass 5, Batch 0, Cost 0.402872, {'classification_error_evaluator': 0.140625}
run test here
Pass 6, Batch 0, Cost 0.426507, {'classification_error_evaluator': 0.0703125}
Pass 6, Batch 100, Cost 0.272435, {'classification_error_evaluator': 0.0625}
run test here
Pass 7, Batch 0, Cost 0.048170, {'classification_error_evaluator': 0.015625}
Pass 7, Batch 100, Cost 0.084337, {'classification_error_evaluator': 0.0390625}
run test here
Pass 8, Batch 0, Cost 0.050831, {'classification_error_evaluator': 0.03125}
Pass 8, Batch 100, Cost 0.058482, {'classification_error_evaluator': 0.015625}
run test here
Pass 9, Batch 0, Cost 0.045268, {'classification_error_evaluator': 0.015625}
run test here
Pass 10, Batch 0, Cost 0.013293, {'classification_error_evaluator': 0.0}
run test here
Pass 11, Batch 0, Cost 0.127966, {'classification_error_evaluator': 0.046875}
&lt;/denchmark-code&gt;

The scale-up operation is at "pass 9", and it seems that old trainers is still affected by the scale operation.
		</comment>
		<comment id='4' author='typhoonzero' date='2017-10-25T00:26:13Z'>
		&lt;denchmark-link:https://github.com/typhoonzero&gt;@typhoonzero&lt;/denchmark-link&gt;
 thanks, I am looking into this, one question, from the log:
&lt;denchmark-code&gt;commandline:  --num_gradient_servers=2 --ports_num_for_sparse=1 --use_gpu=0 --trainer_id=1 --trainer_count=1 --num_passes=1 --ports_num=1 --port=7164
&lt;/denchmark-code&gt;

Is the new pserver being started (or the old pserver?)
I have added more logs to the pserver, and its seems that the newly added trainer gets the parameter from the new pserver upon start. It's just not converging (are you using the same train_mnist_ft.py as the one in the develop branch? If you have fixed something, do you mind send a PR :) )
I am manually starting two trainers in my machine, here are the logs:
The first trainer, who initialized the parameters:
&lt;denchmark-code&gt;root@0eae91a3fa55:/workspace# PADDLE_INIT_TRAINER_ID=0 ETCD_IP=127.0.0.1 python train_mnist_ft.py train
I1025 00:59:42.603524  1957 Util.cpp:166] commandline:  --trainer_id=0 
[INFO 2017-10-25 00:59:42,610 layers.py:2556] output for __conv_pool_0___conv: c = 20, h = 24, w = 24, size = 11520
[INFO 2017-10-25 00:59:42,612 layers.py:2684] output for __conv_pool_0___pool: c = 20, h = 12, w = 12, size = 2880
[INFO 2017-10-25 00:59:42,614 layers.py:2556] output for __conv_pool_1___conv: c = 50, h = 8, w = 8, size = 3200
[INFO 2017-10-25 00:59:42,616 layers.py:2684] output for __conv_pool_1___pool: c = 50, h = 4, w = 4, size = 800
I1025 00:59:42.619835  1957 GradientMachine.cpp:94] Initing parameters..
I1025 00:59:42.621282  1957 GradientMachine.cpp:101] Init parameters done.
INFO[0000] Connected to etcd: http://127.0.0.1:2379
    
INFO[0000] Trying to acquire lock at /init_ps/lock.     
INFO[0000] Successfully acquired lock at /init_ps/lock. 
INFO[0000] Trainer selected.                            
I1025 00:59:42.793243  1957 NewRemoteParameterUpdater.cpp:68] paddle_begin_init_params start
E1025 00:59:42.793328  1957 NewRemoteParameterUpdater.cpp:109] got unsupported v1 learning_rate_schedule config: poly, set to const
I1025 00:59:42.800812  1957 NewRemoteParameterUpdater.cpp:161] paddle_begin_init_params done
I1025 00:59:42.800851  1957 NewRemoteParameterUpdater.cpp:166] NewRemoteParameterUpdater initialized
Pass 0, Batch 0, Cost 2.302585, {'classification_error_evaluator': 0.890625}
Pass 0, Batch 100, Cost 54.417175, {'classification_error_evaluator': 0.875}
&lt;/denchmark-code&gt;

The second trainer, who downloaded the parameters:
&lt;denchmark-code&gt;root@0eae91a3fa55:/workspace# PADDLE_INIT_TRAINER_ID=1 ETCD_IP=127.0.0.1 python train_mnist_ft.py train
I1025 00:59:44.838419  1996 Util.cpp:166] commandline:  --trainer_id=1 
[INFO 2017-10-25 00:59:44,851 layers.py:2556] output for __conv_pool_0___conv: c = 20, h = 24, w = 24, size = 11520
[INFO 2017-10-25 00:59:44,866 layers.py:2684] output for __conv_pool_0___pool: c = 20, h = 12, w = 12, size = 2880
[INFO 2017-10-25 00:59:44,868 layers.py:2556] output for __conv_pool_1___conv: c = 50, h = 8, w = 8, size = 3200
[INFO 2017-10-25 00:59:44,870 layers.py:2684] output for __conv_pool_1___pool: c = 50, h = 4, w = 4, size = 800
I1025 00:59:44.889663  1996 GradientMachine.cpp:94] Initing parameters..
I1025 00:59:44.895870  1996 GradientMachine.cpp:101] Init parameters done.
INFO[0000] Connected to etcd: http://127.0.0.1:2379
    
INFO[0000] Trying to acquire lock at /init_ps/lock.     
INFO[0000] Successfully acquired lock at /init_ps/lock. 
INFO[0000] Initialization is already done.              
I1025 00:59:44.943898  1996 NewRemoteParameterUpdater.cpp:166] NewRemoteParameterUpdater initialized
Pass 0, Batch 0, Cost 56.500000, {'classification_error_evaluator': 0.8828125}
Pass 0, Batch 100, Cost 56.000000, {'classification_error_evaluator': 0.875}
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='typhoonzero' date='2017-10-25T01:27:02Z'>
		Sorry，in the above test case I've changed is_local=True. When training with remote pservers, the result is just like what you said.
		</comment>
		<comment id='6' author='typhoonzero' date='2017-10-25T01:38:22Z'>
		No worries!
		</comment>
	</comments>
</bug>