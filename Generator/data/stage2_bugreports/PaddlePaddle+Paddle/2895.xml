<bug id='2895' author='fty8788' open_date='2017-07-16T01:07:57Z' closed_time='2017-08-11T08:15:43Z'>
	<summary>写模型到本地目录的时候崩溃</summary>
	<description>
一轮训练完成之后，写入文件，崩溃，代码是：
with gzip.open("dssm_%s_pass_%05d.tar.gz" %
                           (model_save_name_prefix, event.pass_id), "w") as f:
                parameters.to_tar(f)
错误日志是：
[INFO 2017-07-13 15:26:48,320 train.py:199] Pass 0, Batch 4000, Cost 0.530298, {'__auc_evaluator_0__': 0.7309514880180359, 'classification_error_evaluator': 0.2705000042915344}

(paddle_box) (paddle_box) tail log.bigram
    num_passes=num_passes)
  File "/home/yanchunwei/third_party/paddle_env/python27/lib/python2.7/site-packages/paddle/v2/trainer.py", line 175, in train
    event_handler(v2_event.EndPass(pass_id, evaluator=pass_evaluator))
  File "train.py", line 211, in _event_handler
    parameters.to_tar(f)
  File "/home/yanchunwei/third_party/paddle_env/python27/lib/python2.7/site-packages/paddle/v2/parameters.py", line 274, in to_tar
    self.serialize(nm, buf)
  File "/home/yanchunwei/third_party/paddle_env/python27/lib/python2.7/site-packages/paddle/v2/parameters.py", line 256, in serialize
    f.write(param.tostring())
OverflowError: length too large
用的DSSM模型，字典维度200w
	</description>
	<comments>
		<comment id='1' author='fty8788' date='2017-08-08T12:47:52Z'>
		对paddle无法保存参数很多的模型吐槽下。我觉得这应该是一个很强烈的需求，请考虑实现。你们都支持读入了caffe导出的模型，却没做好自己的模型的保存和再次load。
对paddle内部实现机制不熟悉，请问如果要自己修改to_tar函数，要具体怎么修改，路径在哪里？请尽量描述的详细一些。
		</comment>
		<comment id='2' author='fty8788' date='2017-08-08T13:04:45Z'>
		&lt;denchmark-link:https://github.com/cszhou&gt;@cszhou&lt;/denchmark-link&gt;
  应该是调用tostring 和 frombuffer 这两个函数来实现序列化和反序列话，紧急的话，试试把这两个函数替换掉，换成pickle之类任何其他序列化方式试试看。
&lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/v2/parameters.py#L253&gt;https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/v2/parameters.py#L253&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/v2/parameters.py#L264&gt;https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/v2/parameters.py#L264&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='fty8788' date='2017-08-08T13:12:35Z'>
		&lt;denchmark-link:https://github.com/lcy-seso&gt;@lcy-seso&lt;/denchmark-link&gt;

thanks!
		</comment>
		<comment id='4' author='fty8788' date='2017-08-08T13:14:33Z'>
		我们会在尽快在主干上修复一下这个问题，如果确实非常紧急，我觉得试试把上面两个地方用numpy做的序列替换掉。非常抱歉。。
		</comment>
		<comment id='5' author='fty8788' date='2017-08-09T02:11:19Z'>
		&lt;denchmark-link:https://github.com/lcy-seso&gt;@lcy-seso&lt;/denchmark-link&gt;

明白了。谢谢~
		</comment>
		<comment id='6' author='fty8788' date='2017-08-09T08:16:33Z'>
		&lt;denchmark-link:https://github.com/lcy-seso&gt;@lcy-seso&lt;/denchmark-link&gt;

报错是出现在

我将这一行拆分成两行


报错在第二行。
f是cStringIO.StringIO()对象。所以问题不在序列化，而是在这个buffer文件无法保存这么大的文件。所以我觉得要么tarfile不用cStringIO.StringIO，用其他？？
//考虑到后期上线需要用到c-api，是要读取这个tar文件的结果。所以有什么建议么。
		</comment>
		<comment id='7' author='fty8788' date='2017-08-09T09:01:34Z'>
		&lt;denchmark-link:https://github.com/cszhou&gt;@cszhou&lt;/denchmark-link&gt;
 感觉可以尝试先保存成pickle或者RecordIO格式的数据再打包，我测试一下。
		</comment>
		<comment id='8' author='fty8788' date='2017-08-09T09:09:03Z'>
		&lt;denchmark-link:https://github.com/Yancey1989&gt;@Yancey1989&lt;/denchmark-link&gt;

嗯。如果你们能测试后告诉我下怎么修改就更好了。
		</comment>
		<comment id='9' author='fty8788' date='2017-08-09T09:10:04Z'>
		好的，我们会尽量在主分支上修复这个问题。
		</comment>
		<comment id='10' author='fty8788' date='2017-08-09T09:14:03Z'>
		&lt;denchmark-link:https://github.com/Yancey1989&gt;@Yancey1989&lt;/denchmark-link&gt;
 有个大概的时间节点么。如果隔得不远的话，我就自己动手改了，毕竟还有其他开发任务。
		</comment>
		<comment id='11' author='fty8788' date='2017-08-09T09:17:35Z'>
		我先看一下，尽量在本周内吧，如果紧急的话可以先改下用着。
		</comment>
		<comment id='12' author='fty8788' date='2017-08-09T09:19:47Z'>
		&lt;denchmark-link:https://github.com/Yancey1989&gt;@Yancey1989&lt;/denchmark-link&gt;

没那么紧急。bug fixed后，要是方便的话告诉我一声。辛苦啦。
		</comment>
	</comments>
</bug>