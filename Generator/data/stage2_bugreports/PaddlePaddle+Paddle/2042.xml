<bug id='2042' author='qingqing01' open_date='2017-05-07T15:44:51Z' closed_time='2017-08-18T08:05:27Z'>
	<summary>The hyper parameters of paddle.optimizer does not work in v2 API.</summary>
	<description>
The hyper parameters in the paddle.optimizer does not work in v2 API.  For example, using momentum optimizer in the sentiment demo as follows,
&lt;denchmark-code&gt;    optimizer = paddle.optimizer.Momentum(
        learning_rate=2e-3,
        momentum=0.9,
        gradient_clipping_threshold=25.0,
        regularization=paddle.optimizer.L2Regularization(rate=8e-4),
        model_average=paddle.optimizer.ModelAverage(average_window=0.5))
&lt;/denchmark-code&gt;

Then print the  of config before this &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/v2/trainer.py#L77&gt;line&lt;/denchmark-link&gt;
 in , it can be found that the  of parameters does not contain the hyper parameters, such as L2 regularization and momentum.  The momentum is 0 if you print it before this &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/parameter/FirstOrderOptimizer.h#L40&gt;line&lt;/denchmark-link&gt;
 in . The  of parameters are as follows,
&lt;denchmark-code&gt;parameters {
  name: "___embedding_layer_0__.w0"
  size: 658816
  initial_mean: 0.0
  initial_std: 0.0139387206988
  dims: 5147
  dims: 128
  initial_strategy: 0
  initial_smart: true
}
parameters {
  name: "___sequence_conv_pool_0___conv_fc.w0"
  size: 49152
  initial_mean: 0.0
  initial_std: 0.051031036308
  dims: 384
  dims: 128
  initial_strategy: 0
  initial_smart: true
}
....
&lt;/denchmark-code&gt;

But the correct proto-string of parameters should contain decay_rate and momentum, as follows,
&lt;denchmark-code&gt;parameters {
  name: "___embedding_0__.w0"
  size: 3840000
  momentum: 0.9
  initial_mean: 0.0
  initial_std: 0.0057735026919
  decay_rate: 0.0008
  dims: 30000
  dims: 128
  initial_strategy: 0
  initial_smart: true
  gradient_clipping_threshold: 25.0
}
parameters {
  name: "___fc_layer_0__.w0"
  size: 65536
  momentum: 0.9
  initial_mean: 0.0
  initial_std: 0.0883883476483
  decay_rate: 0.0008
  dims: 128
  dims: 512
  initial_strategy: 0
  initial_smart: true
  gradient_clipping_threshold: 25.0
}
...
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='qingqing01' date='2017-05-10T07:26:06Z'>
		This bug is hard to fix. Because we split model configuration into two parts, the topology configuration, and the optimizer settings. When we config and parse topology, there is no optimizer information we set. However, the weight_decay belongs to topology now.
Here is a step by step solution to this issue.


Disable weight_decay in optimizer settings. If a user wants a global weight_decay, he can maintain a ParamAttr by himself.


Make weight_decay out of Parameter Configuration or set the global weight_decay in topology configuration(e.g. ModelConfig) itself.


The second step is a little bit hard to implement, may change the C++ Core of Paddle.
		</comment>
		<comment id='2' author='qingqing01' date='2017-05-10T09:15:59Z'>
		不仅仅是weight_decay的问题，还包括momentum, gradient_clipping_threshold。
		</comment>
		<comment id='3' author='qingqing01' date='2017-05-10T09:30:42Z'>
		The solution might be as following.
The global weight_decay,  momentum, gradient_clipping_threshold should be saved into proto::OptimizationConfig which saved inside TrainerConfig.proto, just like learning_rate in OptimizationConfig.
And then we can get the global weight_decay in all optimizers.
		</comment>
		<comment id='4' author='qingqing01' date='2017-06-07T07:01:20Z'>
		This problem has been fixed by this PR &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/2288&gt;#2288&lt;/denchmark-link&gt;
.
Thank you for the issue.
		</comment>
		<comment id='5' author='qingqing01' date='2017-06-22T08:00:20Z'>
		This problem is not solved yet, so I reopen it.
		</comment>
		<comment id='6' author='qingqing01' date='2017-08-18T08:05:27Z'>
		close since the v2 API has fixed this issue.
		</comment>
	</comments>
</bug>