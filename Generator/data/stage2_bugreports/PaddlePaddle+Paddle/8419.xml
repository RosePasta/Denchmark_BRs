<bug id='8419' author='emailweixu' open_date='2018-02-13T00:18:49Z' closed_time='2018-08-15T11:38:26Z'>
	<summary>Incorrect handling of bias_attr for fc and other layers</summary>
	<description>
According to the comment of nn.fc, if bias_attr is None, which is the default, there will be no bias for the layer.



Paddle/python/paddle/v2/fluid/layers/nn.py


        Lines 135 to 137
      in
      f82fa64






        bias_attr(ParamAttr|list): The parameter attribute for the bias parameter 



                                   for this layer. If set None, no bias will be 



                                   added to the output units. 





However, bias is still added to the computation even it is not set. This is caused by a bug of LayerHelper.append_bias_op(). It checks for whether bias_attr is defined at the following line:



Paddle/python/paddle/v2/fluid/layer_helper.py


        Lines 357 to 358
      in
      f82fa64






 if not bias_attr: 



 return input_var 





However, bias_attr is always defined because the property access function:



Paddle/python/paddle/v2/fluid/layer_helper.py


        Lines 72 to 74
      in
      f82fa64






 @property 



 def bias_attr(self): 



 return ParamAttr.to_attr(self.kwargs.get('bias_attr', None)) 





So there is no way not to use bias for fc layer (and other layers using same mechanism). It's easy to fix it. However, there are tuned models implicitly depending on the unexpected bug. If we fix this, we need to re-check the training of existing trained models.
	</description>
	<comments>
		<comment id='1' author='emailweixu' date='2018-08-15T11:38:24Z'>
		您好，此issue在近一个月内暂无更新，我们将于今天内关闭。若在关闭后您仍需跟进提问，可重新开启此问题，我们将在24小时内回复您。因关闭带来的不便我们深表歉意，请您谅解~感谢您对PaddlePaddle的支持!
Hello, this issue has not been updated in the past month. We will close it today for the sake of other user‘s experience. If you still need to follow up on this question after closing, please feel free to reopen it. In that case, we will get back to you within 24 hours. We apologize for the inconvenience caused by the closure and thank you so much for your support of PaddlePaddle Group!
		</comment>
	</comments>
</bug>