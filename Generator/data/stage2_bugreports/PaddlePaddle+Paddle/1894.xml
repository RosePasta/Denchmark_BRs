<bug id='1894' author='qingqing01' open_date='2017-04-25T14:43:57Z' closed_time='2017-05-23T11:00:23Z'>
	<summary>Enable gradient clipping.</summary>
	<description>
This issue is related to &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/issues/775&gt;#775&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/issues/1891&gt;#1891&lt;/denchmark-link&gt;
. I'll try to fix it. On the other hand, is it necessary to enable gradient and error clipping in gru-memory and lstm-memory which are implemented in one cpp file (not in the recurrent layer group)?  &lt;denchmark-link:https://github.com/hedaoyuan&gt;@hedaoyuan&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/lcy-seso&gt;@lcy-seso&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='qingqing01' date='2017-05-03T04:49:40Z'>
		Is it necessary to implement norm gradient clipping in PaddlePaddle, for example, &lt;denchmark-link:https://www.reddit.com/r/MachineLearning/comments/31b6x8/gradient_clipping_rnns/&gt;constraint a certain norm&lt;/denchmark-link&gt;
 of gradient matrix by a threshold?
		</comment>
		<comment id='2' author='qingqing01' date='2017-05-03T04:54:43Z'>
		Gradient clipping is critical to a stable training process of RNN models, this issue &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/issues/1961&gt;#1961&lt;/denchmark-link&gt;
 is also related to gradient clipping.
		</comment>
	</comments>
</bug>