<bug id='18658' author='jczaja' open_date='2019-07-16T13:49:09Z' closed_time='2019-08-30T09:13:20Z'>
	<summary>[MKL-DNN] Failure to run face model (demark)</summary>
	<description>
Hi,
We have just tested that internal face model of yours (demark) stopped to work with mkl-dnn (it is fine when paddle naive ops are used).
commit tested:
&lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/commit/fd6631ef2fb8c896f89ddaa94a5112c08665f35b&gt;fd6631e&lt;/denchmark-link&gt;

commandline used:
./paddle/fluid/inference/tests/api/test_analyzer_image_classification  --infer_model=/home/jczaja/models/demark/demark/ --gtest_filter=*profile_mkldnn --batch_size=1
Output:
Exception: /home/jczaja/paddle/paddle/fluid/memory/detail/meta_cache.cc:33 Assertiondesc-&gt;check_guards()failed. terminate called after throwing an instance of 'std::runtime_error' what():  Exception encounter. Aborted
We do not know yet when exactly problem appeared
&lt;denchmark-link:https://github.com/luotao1&gt;@luotao1&lt;/denchmark-link&gt;
 Could you please confirm that demark model does fail  for you when mkl-dnn is used? If all works for you then tell us revision used and commandline
	</description>
	<comments>
		<comment id='1' author='jczaja' date='2019-07-17T02:12:51Z'>
		I will confirm it ASAP.
		</comment>
		<comment id='2' author='jczaja' date='2019-07-19T04:58:46Z'>
		I could run successfully with the latest develop branch on demark model.
please check the md5sum for it.
&lt;denchmark-code&gt;e5fa00447f90969a95e2b77fd5b77c26  demark/model
9c8ab6ea9db3b5d59b70d68bf0086524  demark/params
&lt;/denchmark-code&gt;

command:
&lt;denchmark-code&gt;./test_analyzer_image_classification --gtest_filter=Analyzer_resnet50.profile_mkldnn --batch_size=1 --warmup --repeat=100 --paddle_num_threads=4 --infer_model=face_model/demark
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='jczaja' date='2019-07-19T08:28:38Z'>
		&lt;denchmark-link:https://github.com/luotao1&gt;@luotao1&lt;/denchmark-link&gt;
 it keeps failing for me, md5sums are fine. Could you provide me build command you used?
		</comment>
		<comment id='4' author='jczaja' date='2019-07-19T08:54:00Z'>
		Which mkldnn version do you use?
The build command is in &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/issues/18658#issuecomment-513090602&gt;#18658 (comment)&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='jczaja' date='2019-07-19T09:14:23Z'>
		I meant cmake command you used to build PaddlePaddle. I just cloned current develop branch so mkldnn version is 0.18
		</comment>
		<comment id='6' author='jczaja' date='2019-07-19T09:36:39Z'>
		How about give your CMake command at first?
		</comment>
		<comment id='7' author='jczaja' date='2019-07-19T10:00:12Z'>
		My cmake command is
&lt;denchmark-code&gt;cmake .. -DON_INFER=ON -DWITH_GPU=OFF -DWITH_MKLDNN=ON -DCMAKE_BUILD_TYPE=Release -DWITH_TESTING=ON -DWITH_PYTHON=ON -DWITH_INFERENCE_API_TEST=ON
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/grygielski&gt;@grygielski&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='jczaja' date='2019-07-19T11:16:47Z'>
		I've been using same command for building. We'll have to take look at it.
		</comment>
		<comment id='9' author='jczaja' date='2019-08-02T17:36:32Z'>
		Status:
Problem root caused. Mkl-dnn activation op for getting y tensor allocated is using:
T *y_data = y-&gt;mutable_data&lt;T&gt;(ctx.GetPlace());
This is wrong, for out of place computation as when blocked formats (NCHW16C) are used
allocation may be a bit bigger that N*C*H*W*sizeof(data_type), as implicit padding may be present.
Proper allocation should be:
 T *y_data = y-&gt;mutable_data&lt;T&gt;(ctx.GetPlace(), activation_pd-&gt;dst_primitive_desc().get_size());
as destination primitive descriptor holds total number of allocation size.
&lt;denchmark-h:h4&gt;Notes:&lt;/denchmark-h&gt;


If allocation is not big enough then it is likely that meta-data of next chunk of memory
in buddy allocator will be overwritten resulting in a crash as presented in first entry of this issue.
Problem does not manifest on Face Model of BDW platform because activation (tanh) is using
NCHW format then no implict padding is used by mkl-dnn so size of allocation is proper

		</comment>
		<comment id='10' author='jczaja' date='2019-08-03T14:40:11Z'>
		
Problem does not manifest on Face Model of BDW platform

What's the meaning of BDW? I still don't know why I could run OK on this model but you got fails. What's the reason of it, different machine?
		</comment>
		<comment id='11' author='jczaja' date='2019-08-06T15:16:16Z'>
		I guess Jacek meant that You are running models on Broadwell (BDW) architecture: &lt;denchmark-link:https://en.wikipedia.org/wiki/Broadwell_(microarchitecture)&gt;https://en.wikipedia.org/wiki/Broadwell_(microarchitecture)&lt;/denchmark-link&gt;
. It doesn't support AVX-512 processor operations (&lt;denchmark-link:https://en.wikipedia.org/wiki/Advanced_Vector_Extensions&gt;https://en.wikipedia.org/wiki/Advanced_Vector_Extensions&lt;/denchmark-link&gt;
) so it uses older ones which have different data formats (e.g. NCHW instead of NCHW16C). &lt;denchmark-link:https://github.com/luotao1&gt;@luotao1&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='jczaja' date='2019-08-07T02:02:41Z'>
		Got it. We use E5-2650 v4 CPU, which only support AVX2.
		</comment>
		<comment id='13' author='jczaja' date='2019-08-22T17:11:07Z'>
		Status of investigation: Failure (accuracy problems) are a result that fetch operator does not perform conversion of mkl-dnn data into paddle data. OperatorsWithKernel are invoking PrepareData which may trigger conversion from MKL-DNN data format into Paddle data format. For face model just before fetch op there is "tanh" operator which is executed by mkl-dnn . For AVX512 tanh will work on mkl-dnn format nchw16c  and will send tensor of such data format into fetch op. Fetch assumes this tensor is NCHW and will copy it to output. NCHW16C is diffrent/not compatible with NCHW so this is a problem. To fix this  conversion(reorder) has to be added before fetch op.
Fast workaround:

Disable tanh op in face model eg. use paddlepaddle tanh activation. MKL-DNN tanh is not faster.
So no performance penalty is by disabling tanh.

&lt;denchmark-link:https://github.com/luotao1&gt;@luotao1&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/Superjomn&gt;@Superjomn&lt;/denchmark-link&gt;
  Please advice how to proceed . Eg. Where to call TransDataLayoutFromMKLDNN  (data_layout_transform.cc:~119) to have output from MKL-DNN ops converted for the purpose of consuming it by fetch op ?
		</comment>
		<comment id='14' author='jczaja' date='2019-08-23T03:26:18Z'>
		&lt;denchmark-link:https://github.com/jczaja&gt;@jczaja&lt;/denchmark-link&gt;
  Could the accuracy diff be solved If there is only first  in , not each time?
		</comment>
		<comment id='15' author='jczaja' date='2019-08-23T03:28:21Z'>
		
Disable tanh op in face model eg. use paddlepaddle tanh activation. MKL-DNN tanh is not faster.
So no performance penalty is by disabling tanh.

I think it's Ok for just this model on avx512.

Where to call TransDataLayoutFromMKLDNN (data_layout_transform.cc:~119) to have output from MKL-DNN ops converted for the purpose of consuming it by fetch op ?

&lt;denchmark-link:https://github.com/LeoZhao-Intel&gt;@LeoZhao-Intel&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jianhang-liu&gt;@jianhang-liu&lt;/denchmark-link&gt;
 How do you think about it?
		</comment>
		<comment id='16' author='jczaja' date='2019-08-23T05:48:28Z'>
		

Disable tanh op in face model eg. use paddlepaddle tanh activation. MKL-DNN tanh is not faster.
So no performance penalty is by disabling tanh.

I think it's Ok for just this model on avx512.

Where to call TransDataLayoutFromMKLDNN (data_layout_transform.cc:~119) to have output from MKL-DNN ops converted for the purpose of consuming it by fetch op ?

@LeoZhao-Intel @jianhang-liu How do you think about it?

Just my 2 cents:
Currently for ops based on OperatorsWithKernel, PrepareData is just doing data layout transform on input tensors instead of outputs, in one graph it follows this rule and works well.
But this case is special since last op based on OperatorsWithKernel is connected with fetch op which is not based on OperatorsWithKernel, then there is issue.
2 options I think, one is do layout conversion for output in OperatorsWithKernel, but I don't think it is good solution, since it don't know which format is needed by next op. The other is doing conversion in fetch op, which I think is good.
		</comment>
		<comment id='17' author='jczaja' date='2019-08-23T07:41:50Z'>
		Ok, I will work on adding conversion into fetch op eg.call TransDataLayoutFromMKLDNN inside fetch::RunImpl
		</comment>
		<comment id='18' author='jczaja' date='2019-08-23T17:40:31Z'>
		&lt;denchmark-link:https://github.com/LeoZhao-Intel&gt;@LeoZhao-Intel&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/luotao1&gt;@luotao1&lt;/denchmark-link&gt;
 PR &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/19282&gt;#19282&lt;/denchmark-link&gt;
 with my proposal of modifications to fetch op so it solves most recent issue discussed here (lack of conversion from MKL-DNN to Paddle in fetch op). It works for Face model and some other testing I have done. Please take a look and let me know what you think.
		</comment>
		<comment id='19' author='jczaja' date='2019-08-30T09:13:20Z'>
		Fixed in &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/19282&gt;#19282&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>