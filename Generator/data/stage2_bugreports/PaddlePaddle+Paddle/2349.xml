<bug id='2349' author='alvations' open_date='2017-06-02T01:21:30Z' closed_time='2017-06-12T03:35:26Z'>
	<summary>StaticInputV2 and GeneratedInputV2 missing in paddle.v2.layer</summary>
	<description>
From the &lt;denchmark-link:https://github.com/PaddlePaddle/book/blob/develop/08.machine_translation/train.py&gt;MT chapter in the DL101 book&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/blob/develop/demo/seqToseq/api_train_v2.py&gt;the machine translation demo&lt;/denchmark-link&gt;
 with the new python API,  and  throws an AttributeError, e.g.:
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-5-5ce86945bbbe&gt; in &lt;module&gt;()
----&gt; 1 cost = seqToseq_net(source_dict_dim, target_dict_dim)

/Users/liling.tan/seqtoseq.py in seqToseq_net(source_dict_dim, target_dict_dim, is_generating)
    160 
    161     decoder_group_name = "decoder_group"
--&gt; 162     group_input1 = paddle.layer.StaticInputV2(input=encoded_vector, is_seq=True)
    163     group_input2 = StaticInputV2(input=encoded_proj, is_seq=True)
    164     group_inputs = [group_input1, group_input2]

AttributeError: 'module' object has no attribute 'StaticInputV2'
A closer look, they're missing from the top level paddle.layer.__init__.py:
&gt;&gt;&gt; import paddle.v2 as paddle
&gt;&gt;&gt; dir(paddle.layer)
['AggregateLevel', 'BaseGeneratedInput', 'ExpandLevel', 'GeneratedInput', 'LayerOutput', 'ModelConfig', 'SubModelConfig', 'SubsequenceInput', '__all__', '__builtins__', '__convert_name__', '__convert_to_v2__', '__data_layer__', '__doc__', '__file__', '__get_used_evaluators__', '__get_used_layers__', '__get_used_parameters__', '__get_used_submodels__', '__map_data_docstr__', '__name__', '__need_to_keep__', '__need_to_wrap__', '__package__', '__trim_submodel__', 'addto', 'batch_norm', 'beam_search', 'bilinear_interp', 'block_expand', 'classification_cost', 'collections', 'concat', 'config_base', 'context_projection', 'conv_operator', 'conv_projection', 'conv_shift', 'convex_comb', 'copy', 'cos_sim', 'cp', 'crf', 'crf_decoding', 'cross_channel_norm', 'cross_entropy_cost', 'cross_entropy_with_selfnorm_cost', 'ctc', 'data', 'dotmul_operator', 'dotmul_projection', 'embedding', 'eos', 'expand', 'fc', 'first_seq', 'full_matrix_projection', 'get_layer', 'get_output', 'gru_step', 'gru_step_naive', 'grumemory', 'hsigmoid', 'huber_cost', 'identity_projection', 'img_cmrnorm', 'img_conv', 'img_pool', 'interpolation', 'lambda_cost', 'last_seq', 'linear_comb', 'lstm_step', 'lstmemory', 'max_id', 'maxout', 'memory', 'mixed', 'mse_cost', 'multi_binary_label_cross_entropy_cost', 'multiplex', 'name', 'nce', 'new_name', 'obj', 'out_prod', 'pad', 'parse_network', 'pooling', 'power', 'print', 'priorbox', 'rank_cost', 're', 'recurrent', 'recurrent_group', 'regression_cost', 'repeat', 'rotate', 'sampling_id', 'scaling', 'scaling_projection', 'selective_fc', 'seq_concat', 'seq_reshape', 'slope_intercept', 'smooth_l1_cost', 'spp', 'sum_cost', 'sum_to_one_norm', 'table_projection', 'tensor', 'trans', 'trans_full_matrix_projection', 'v1_layers', 'warp_ctc']

&gt;&gt;&gt; 'StaticInputV2' in dir(paddle.layer)
False
&gt;&gt;&gt; 'GeneratedInputV2' in dir(paddle.layer)
False
Currently the ad-hoc solution is to use the old paddle.trainer_config_helpers, e.g.
&lt;denchmark-code&gt;&gt;&gt;&gt; from paddle.trainer_config_helpers import GeneratedInput 
&gt;&gt;&gt; from paddle.trainer_config_helpers import StaticInput 
&lt;/denchmark-code&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

This is from version 0.10.0 of the Python API.
&lt;denchmark-code&gt;$ pip show paddle
Name: paddle
Version: 0.10.0
Summary: Parallel Distributed Deep Learning
Home-page: UNKNOWN
Author: UNKNOWN
Author-email: UNKNOWN
License: UNKNOWN
Location: /usr/local/lib/python2.7/site-packages
Requires: rarfile, matplotlib, protobuf, opencv-python, numpy, requests


$ paddle version
PaddlePaddle 0.10.0, compiled with
    with_avx: ON
    with_gpu: OFF
    with_double: OFF
    with_python: ON
    with_rdma: OFF
    with_timer: OFF
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='alvations' date='2017-06-02T05:16:23Z'>
		We decide not to maintain the demo since it uses old api. Do you run the &lt;denchmark-link:https://github.com/PaddlePaddle/book/blob/develop/08.machine_translation/train.py&gt;train.py&lt;/denchmark-link&gt;
 in  book and find the above bug?
		</comment>
		<comment id='2' author='alvations' date='2017-06-02T06:57:55Z'>
		Yes, the same bug for  appears in the book from &lt;denchmark-link:https://github.com/PaddlePaddle/book/blob/develop/08.machine_translation/train.py&gt;train.py&lt;/denchmark-link&gt;
.
I was using v0.10.0 and paddle.v2.layer.StaticInputV2 doesn't exist.
I'm reinstalling with the develop branch and see whether the same AttributeError occurs
		</comment>
		<comment id='3' author='alvations' date='2017-06-05T02:43:41Z'>
		&lt;denchmark-link:https://github.com/alvations&gt;@alvations&lt;/denchmark-link&gt;


I'm reinstalling with the develop branch and see whether the same AttributeError occurs

How are about it?
		</comment>
		<comment id='4' author='alvations' date='2017-06-05T05:18:30Z'>
		&lt;denchmark-link:https://github.com/luotao1&gt;@luotao1&lt;/denchmark-link&gt;
 Thanks for checking in the issue.
I'm still getting the AttributeError with the latest installation from the develop branch. Neither the StaticInputV2 nor the StaticInput is within the namespace of paddle.v2.layer.
&lt;denchmark-code&gt;&gt;&gt;&gt; import paddle.v2 as paddle

&gt;&gt;&gt; paddle.layer.StaticInputV2
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
AttributeError: 'module' object has no attribute 'StaticInputV2'

&gt;&gt;&gt; 'StaticInputV2' in dir(paddle.layer)
False
&gt;&gt;&gt; 'StaticInput' in dir(paddle.layer)
False

&gt;&gt;&gt; dir(paddle.layer)
['AggregateLevel', 'BaseGeneratedInput', 'ExpandLevel', 'GeneratedInput', 'LayerOutput', 'ModelConfig', 'SubModelConfig', 'SubsequenceInput', '__all__', '__builtins__', '__convert_name__', '__convert_to_v2__', '__data_layer__', '__doc__', '__file__', '__get_used_evaluators__', '__get_used_layers__', '__get_used_parameters__', '__get_used_submodels__', '__map_data_docstr__', '__name__', '__need_to_keep__', '__need_to_wrap__', '__package__', '__trim_submodel__', 'addto', 'batch_norm', 'beam_search', 'bilinear_interp', 'block_expand', 'classification_cost', 'collections', 'concat', 'config_base', 'context_projection', 'conv_operator', 'conv_projection', 'conv_shift', 'convex_comb', 'copy', 'cos_sim', 'cp', 'crf', 'crf_decoding', 'cross_channel_norm', 'cross_entropy_cost', 'cross_entropy_with_selfnorm_cost', 'ctc', 'data', 'dotmul_operator', 'dotmul_projection', 'embedding', 'eos', 'expand', 'fc', 'first_seq', 'full_matrix_projection', 'get_layer', 'get_output', 'gru_step', 'gru_step_naive', 'grumemory', 'hsigmoid', 'huber_cost', 'identity_projection', 'img_cmrnorm', 'img_conv', 'img_pool', 'interpolation', 'lambda_cost', 'last_seq', 'linear_comb', 'lstm_step', 'lstmemory', 'max_id', 'maxout', 'memory', 'mixed', 'mse_cost', 'multi_binary_label_cross_entropy_cost', 'multiplex', 'name', 'nce', 'new_name', 'obj', 'out_prod', 'pad', 'parse_network', 'pooling', 'power', 'print', 'priorbox', 'rank_cost', 're', 'recurrent', 'recurrent_group', 'regression_cost', 'repeat', 'rotate', 'sampling_id', 'scaling', 'scaling_projection', 'selective_fc', 'seq_concat', 'seq_reshape', 'slope_intercept', 'smooth_l1_cost', 'spp', 'sum_cost', 'sum_to_one_norm', 'table_projection', 'tensor', 'trans', 'trans_full_matrix_projection', 'v1_layers', 'warp_ctc']
&lt;/denchmark-code&gt;

The strange part is that at &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/v2/layer.py#L75&gt;https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/v2/layer.py#L75&lt;/denchmark-link&gt;
 is inheriting the  to put into the  but the  is still thrown when using 
		</comment>
		<comment id='5' author='alvations' date='2017-06-05T06:03:31Z'>
		Sorry, this is a bug after PaddlePaddle changes its way to parse the network configuration. I am checking it now.
		</comment>
	</comments>
</bug>