<bug id='26673' author='sfraczek' open_date='2020-08-25T14:39:22Z' closed_time='2020-11-02T10:18:40Z'>
	<summary>NaN in CPU training of resnet</summary>
	<description>
NaNs in training Resnet on CPU from models repo (without MKLDNN)
&lt;denchmark-h:h3&gt;Paddle models version&lt;/denchmark-h&gt;

commit 64cde5d16de78181e348abc85fa45cee29512ec6
Author: LiuHao &lt;denchmark-link:mailto:liuhao19900412@gmail.com&gt;liuhao19900412@gmail.com&lt;/denchmark-link&gt;

Date:   Thu Aug 6 18:23:44 2020 +0800
&lt;denchmark-code&gt;Update run_ernie_classifier.py (#4790)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Paddle version&lt;/denchmark-h&gt;

I chcked below two commits and some in between and all don't work
&lt;denchmark-h:h4&gt;This I chcked first&lt;/denchmark-h&gt;

commit &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/commit/36868e840c582047480f4c999ddf1f5f004de678&gt;36868e8&lt;/denchmark-link&gt;

Author: yukavio &lt;denchmark-link:mailto:67678385+yukavio@users.noreply.github.com&gt;67678385+yukavio@users.noreply.github.com&lt;/denchmark-link&gt;

Date:   Mon Aug 24 18:35:03 2020 +0800
&lt;denchmark-code&gt;fix one_hot example doc test=document_fix (#26585)

* fix one_hot example doc test=document_fix

* fix some bug
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;This I checked later too&lt;/denchmark-h&gt;

commit &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/commit/9d2bd0ac387080c5a047565ba9f28d380d1c42f1&gt;9d2bd0a&lt;/denchmark-link&gt;

Author: 123malin &lt;denchmark-link:mailto:malin10@baidu.com&gt;malin10@baidu.com&lt;/denchmark-link&gt;

Date:   Wed Jun 3 14:05:17 2020 +0800
&lt;denchmark-code&gt;downpour_worker增加try_catch机制，打印program所有参数 (#24700)

* test=develop, add try_catch for debug
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Other info&lt;/denchmark-h&gt;

Build type: Release
CPU: Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz
OS: Ubuntu 16.04.5 LTS
Python: 2.7.12
&lt;denchmark-h:h3&gt;To reproduce&lt;/denchmark-h&gt;

in models/dygraph/resnet
Command: python train.py
change place from CUDA to CPU
-    place = fluid.CUDAPlace(fluid.dygraph.parallel.Env().dev_id) \
-        if args.use_data_parallel else fluid.CUDAPlace(0)
+    place = fluid.CPUPlace()
&lt;denchmark-h:h3&gt;Log&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;[Epoch 0, batch 0] loss 4.60813, acc1 0.03125, acc5 0.06250, batch_cost: 14.17363 s, reader_cost: 0.14763 s
[Epoch 0, batch 10] loss nan, acc1 0.01136, acc5 0.06250, batch_cost: 11.84618 s, reader_cost: 0.00004 s
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='sfraczek' date='2020-09-14T14:00:53Z'>
		I try to reproduce result, but I don't get NAN.
Log.
&lt;denchmark-code&gt;[Epoch 0, batch 0] loss 4.83761, acc1 0.03125, acc5 0.06250, batch_cost: 28.49166 s, reader_cost: 0.65109 s
[Epoch 0, batch 10] loss 13.95101, acc1 0.02841, acc5 0.11648, batch_cost: 25.39008 s, reader_cost: 0.00007 s
&lt;/denchmark-code&gt;

From my experience, the loss in batch 10 is around 11-14, becaure of data shuffle and weight initialization.
Cound you please me some more information.
Like cmake command (cmake ../ -DWITH_GPU=OFF ...) and docker information if used.
		</comment>
		<comment id='2' author='sfraczek' date='2020-09-15T18:18:23Z'>
		The training has been run on bare metal Xeon.
cmake .. -DWITH_GPU=OFF -DWITH_DISTRIBUTE=OFF -DWITH_MKLDNN=ON -DCMAKE_EXPORT_COMPILE_COMMANDS=OFF -DWITH_PROFILER=OFF -DON_INFER=ON -DCMAKE_BUILD_TYPE=Release -DWITH_TESTING=ON -DWITH_INFERENCE_API_TEST=ON -DWITH_NCCL=OFF -DWITH_COVERAGE=OFF

newest commit id: &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/commit/6947a58a1f52bed347341d373ce86452b17c3366&gt;6947a58&lt;/denchmark-link&gt;
 Mon Sep 14 20:14:26 2020
Model repo (branch develop):
newest commit id: bde994e12c93b4f1c6f0b195653192a76a6015ab  Tue Sep 15 15:13:27
cmd line:
models/dygraph/resnet$ python train.py --use_gpu=0 --ce
result:

ce mode
[Epoch 0, batch 0] loss 4.82573, acc1 0.00000, acc5 0.00000, batch_cost: 11.09144 s, reader_cost: 0.18639 s
[Epoch 0, batch 10] loss nan, acc1 0.00852, acc5 0.03693, batch_cost: 9.12530 s, reader_cost: 0.00004 s

cmd line:
models/dygraph/resnet$ python train.py --use_gpu=0
result:

[Epoch 0, batch 0] loss 4.76966, acc1 0.00000, acc5 0.00000, batch_cost: 11.87695 s, reader_cost: 0.18596 s
[Epoch 0, batch 10] loss nan, acc1 0.00852, acc5 0.04830, batch_cost: 8.95958 s, reader_cost: 0.00004 s

		</comment>
		<comment id='3' author='sfraczek' date='2020-09-16T07:19:46Z'>
		Hi, &lt;denchmark-link:https://github.com/arlesniak&gt;@arlesniak&lt;/denchmark-link&gt;
 What is this machine ? Is it 6248? use  and update to Baidu
And also if possible try to reproduyce on 6148, because baidu can not reproduce this error...
		</comment>
		<comment id='4' author='sfraczek' date='2020-09-16T07:46:07Z'>
		The above result are from :
Model name:            Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz
		</comment>
		<comment id='5' author='sfraczek' date='2020-09-16T09:44:49Z'>
		&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Paddle version: &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/commit/6947a58a1f52bed347341d373ce86452b17c3366&gt;6947a58&lt;/denchmark-link&gt;
 Date:   Mon Sep 14 20:14:26 2020 +0800
Paddle branch: develop
Paddle With CUDA: False
Paddle models repo: &lt;denchmark-link:https://github.com/PaddlePaddle/models.git&gt;https://github.com/PaddlePaddle/models.git&lt;/denchmark-link&gt;

Paddle models version: bde994e12c93b4f1c6f0b195653192a76a6015ab Tue Sep 15 15:13:27 2020 +0800
Paddle models branch: develop
CPU: Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz
OS: Ubuntu 16.04.6 LTS
Python version: 2.7.12
CUDA version: None
cuDNN version: None.None.None
Nvidia driver version: None
Cmake orders: -DWITH_GPU=OFF -DWITH_DISTRIBUTE=OFF -DWITH_MKLDNN=ON -DCMAKE_EXPORT_COMPILE_COMMANDS=OFF -DWITH_PROFILER=OFF -DON_INFER=ON -DCMAKE_BUILD_TYPE=Release -DWITH_TESTING=ON -DWITH_INFERENCE_API_TEST=ON -DWITH_NCCL=OFF -DWITH_COVERAGE=OFF
-C++ version : gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
-cmake version: cmake version 3.17.3
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Steps to reproduce:
having Paddle built and models repo cloned run the following cmd (please use your paths):

PYTHONPATH=Paddle/build/python/ python models/dygraph/resnet/train.py --use_gpu=0 --ce

		</comment>
		<comment id='6' author='sfraczek' date='2020-09-16T11:13:58Z'>
		&lt;denchmark-link:https://github.com/phlrain&gt;@phlrain&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/luotao1&gt;@luotao1&lt;/denchmark-link&gt;
  Could you reproduce the issue with the updated reproducing details. please ?
		</comment>
		<comment id='7' author='sfraczek' date='2020-09-17T04:07:47Z'>
		
@phlrain @luotao1 Could you reproduce the issue with the updated reproducing details. please ?

I can get an NAN by the updated reproducing details. I will check the reason why cause this.
		</comment>
		<comment id='8' author='sfraczek' date='2020-09-24T02:52:10Z'>
		&lt;denchmark-link:https://github.com/phlrain&gt;@phlrain&lt;/denchmark-link&gt;
 Could you give the environment and cmake of &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/issues/26673#issuecomment-692072288&gt;#26673 (comment)&lt;/denchmark-link&gt;
 ?
		</comment>
		<comment id='9' author='sfraczek' date='2020-09-24T06:50:17Z'>
		&lt;denchmark-link:https://github.com/phlrain&gt;@phlrain&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/luotao1&gt;@luotao1&lt;/denchmark-link&gt;
 Do you have any update of status on the issue ?
		</comment>
		<comment id='10' author='sfraczek' date='2020-10-20T11:11:16Z'>
		&lt;denchmark-link:https://github.com/phlrain&gt;@phlrain&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/luotao1&gt;@luotao1&lt;/denchmark-link&gt;
  Hi, Please help fixing this issue. Since we already enbaled dyGraph oneDNN resnet50 training, and we also know for mobilenetv1, and v2, oneDNN training has 5-6X speed-up compared to Native config version, but for resnet50, since NativeConfig verision has NAN error, we can not calculate the speed-up.
		</comment>
		<comment id='11' author='sfraczek' date='2020-10-29T12:29:57Z'>
		&lt;denchmark-link:https://github.com/arlesniak&gt;@arlesniak&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/lidanqing-intel&gt;@lidanqing-intel&lt;/denchmark-link&gt;
 We find the reason, the cmake option is for inference, you should set  which is for training. In this cmake option, CPU training of resnet is normal.
You can have a double-check.
Maybe the reason is that softmax has different version on Inference and Training mode (see &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/14337&gt;#14337&lt;/denchmark-link&gt;
), and some other differences.
		</comment>
		<comment id='12' author='sfraczek' date='2020-10-30T09:55:06Z'>
		Thank you for the info we will check internally.
		</comment>
		<comment id='13' author='sfraczek' date='2020-11-02T10:18:40Z'>
		We have verified and it solved the problem.
		</comment>
	</comments>
</bug>