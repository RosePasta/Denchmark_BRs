<bug id='5836' author='lcy-seso' open_date='2017-11-22T09:43:51Z' closed_time='2018-08-15T11:40:46Z'>
	<summary>Fix the bug that regularization does not take effect in Adam</summary>
	<description>
Adam works well in practice and compares favorably to other adaptive learning-method algorithms. It becomes a popular (almost a default optimizer for many tasks) optimizer for deep neural networks.
But the current implementation of Adam in V2 API ignore the weight_decay_rate  parameter. This means the l2 regularization does not work for Adam and Adamax at all even if the user sets it.
A recent paper &lt;denchmark-link:https://arxiv.org/abs/1711.05101&gt;Fixing Weight Decay Regularization in Adam&lt;/denchmark-link&gt;
 points out that decoupling weight decay and the optimization steps achieve a better learning performance.
Correctly implementing regularization is important for a learning task.
We have this PR &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/2097&gt;#2097&lt;/denchmark-link&gt;
 , but it does not correctly implement the L2 regularization in Adam and Adamax.
A related issue reported by one of our users: &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/issues/4162&gt;#4162&lt;/denchmark-link&gt;

I will fix this.
	</description>
	<comments>
	</comments>
</bug>