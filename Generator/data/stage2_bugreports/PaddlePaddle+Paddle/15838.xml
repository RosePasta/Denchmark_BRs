<bug id='15838' author='luotao1' open_date='2019-02-21T04:04:18Z' closed_time='2020-05-22T10:17:35Z'>
	<summary>Conv_op fails on MM_DNN GPU inference</summary>
	<description>
MM_DNN在4w+样本上预测，
编译时需打开：-DWITH_INFERENCE_API_TEST=ON
运行命令：
&lt;denchmark-code&gt;cd build
./paddle/fluid/inference/tests/api/test_analyzer_mm_dnn --infer_model=third_party/inference_demo/mm_dnn/model/ --infer_data=third_party/inference_demo/mm_dnn/data.txt --gtest_filter=Analyzer_MM_DNN.profile --test_all_data
&lt;/denchmark-code&gt;

环境：V100，镜像：
CPU可以跑通，GPU报错：
&lt;denchmark-link:https://user-images.githubusercontent.com/6836917/53142835-54948500-35d0-11e9-9f84-75425bcbd26a.png&gt;&lt;/denchmark-link&gt;

GPU 需要修改下 
&lt;denchmark-code&gt;diff --git a/paddle/fluid/inference/tests/api/analyzer_mm_dnn_tester.cc b/paddle/fluid/inference/tests/api/analyzer_mm_dnn_tester.cc
index 089f655..e55f45d 100644
--- a/paddle/fluid/inference/tests/api/analyzer_mm_dnn_tester.cc
+++ b/paddle/fluid/inference/tests/api/analyzer_mm_dnn_tester.cc
@@ -76,7 +76,8 @@ void PrepareInputs(std::vector&lt;PaddleTensor&gt; *input_slots, DataRecord *data,

 void SetConfig(AnalysisConfig *cfg) {
   cfg-&gt;SetModel(FLAGS_infer_model);
-  cfg-&gt;DisableGpu();
+  cfg-&gt;EnableUseGpu(600, 0);
   cfg-&gt;SwitchSpecifyInputNames();
   cfg-&gt;SwitchIrOptim();
 }
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='luotao1' date='2019-02-21T05:12:38Z'>
		&lt;denchmark-link:https://github.com/luotao1&gt;@luotao1&lt;/denchmark-link&gt;
 From the error,  the input data shape is not correct. If the batch size is 1, the shape should be [1, C, H, W], not [C, H, W]
		</comment>
		<comment id='2' author='luotao1' date='2020-05-22T10:17:35Z'>
		Since you haven't replied for more than a year, we have closed this issue/pr.
If the problem is not solved or there is a follow-up one, please reopen it at any time and we will continue to follow up.
由于您超过一年未回复，我们将关闭这个issue/pr。
若问题未解决或有后续问题，请随时重新打开，我们会继续跟进。
		</comment>
	</comments>
</bug>