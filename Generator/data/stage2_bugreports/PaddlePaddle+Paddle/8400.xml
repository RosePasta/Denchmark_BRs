<bug id='8400' author='xymyeah' open_date='2018-02-12T07:18:49Z' closed_time='2018-05-09T11:27:44Z'>
	<summary>image_classification在centos下，gcc是4.4.7的时候出现SIGSEGV的错误，请帮忙看看，thanks</summary>
	<description>
I0212 12:28:50.517459  1322 Util.cpp:166] commandline:  --num_passes=1 --ports_num_for_sparse=1 --use_gpu=0 --trainer_id=0 --pservers=10.10.10.10 --trainer_count=1 --nu
m_gradient_servers=1 --ports_num=1 --port=30002
[INFO 2018-02-12 12:28:50,523 layers.py:2714] output for : c = 64, h = 32, w = 32, size = 65536
[INFO 2018-02-12 12:28:50,524 layers.py:3282] output for : c = 64, h = 32, w = 32, size = 65536
[INFO 2018-02-12 12:28:50,525 layers.py:2714] output for : c = 64, h = 32, w = 32, size = 65536
[INFO 2018-02-12 12:28:50,526 layers.py:3282] output for : c = 64, h = 32, w = 32, size = 65536
[INFO 2018-02-12 12:28:50,527 layers.py:2856] output for : c = 64, h = 16, w = 16, size = 16384
[INFO 2018-02-12 12:28:50,528 layers.py:2714] output for : c = 128, h = 16, w = 16, size = 32768
[INFO 2018-02-12 12:28:50,529 layers.py:3282] output for : c = 128, h = 16, w = 16, size = 32768
[INFO 2018-02-12 12:28:50,530 layers.py:2714] output for : c = 128, h = 16, w = 16, size = 32768
[INFO 2018-02-12 12:28:50,531 layers.py:3282] output for : c = 128, h = 16, w = 16, size = 32768
[INFO 2018-02-12 12:28:50,532 layers.py:2856] output for : c = 128, h = 8, w = 8, size = 8192
[INFO 2018-02-12 12:28:50,533 layers.py:2714] output for : c = 256, h = 8, w = 8, size = 16384
[INFO 2018-02-12 12:28:50,534 layers.py:3282] output for : c = 256, h = 8, w = 8, size = 16384
[INFO 2018-02-12 12:28:50,535 layers.py:2714] output for : c = 256, h = 8, w = 8, size = 16384
[INFO 2018-02-12 12:28:50,536 layers.py:3282] output for : c = 256, h = 8, w = 8, size = 16384
[INFO 2018-02-12 12:28:50,538 layers.py:2714] output for : c = 256, h = 8, w = 8, size = 16384
[INFO 2018-02-12 12:28:50,539 layers.py:3282] output for : c = 256, h = 8, w = 8, size = 16384
[INFO 2018-02-12 12:28:50,539 layers.py:2856] output for : c = 256, h = 4, w = 4, size = 4096
[INFO 2018-02-12 12:28:50,540 layers.py:2714] output for : c = 512, h = 4, w = 4, size = 8192
[INFO 2018-02-12 12:28:50,541 layers.py:3282] output for : c = 512, h = 4, w = 4, size = 8192
[INFO 2018-02-12 12:28:50,542 layers.py:2714] output for : c = 512, h = 4, w = 4, size = 8192
[INFO 2018-02-12 12:28:50,543 layers.py:3282] output for : c = 512, h = 4, w = 4, size = 8192
[INFO 2018-02-12 12:28:50,544 layers.py:2714] output for : c = 512, h = 4, w = 4, size = 8192
[INFO 2018-02-12 12:28:50,545 layers.py:3282] output for : c = 512, h = 4, w = 4, size = 8192
[INFO 2018-02-12 12:28:50,546 layers.py:2856] output for : c = 512, h = 2, w = 2, size = 2048
[INFO 2018-02-12 12:28:50,547 layers.py:2714] output for : c = 512, h = 2, w = 2, size = 2048
[INFO 2018-02-12 12:28:50,548 layers.py:3282] output for : c = 512, h = 2, w = 2, size = 2048
[INFO 2018-02-12 12:28:50,549 layers.py:2714] output for : c = 512, h = 2, w = 2, size = 2048
[INFO 2018-02-12 12:28:50,550 layers.py:3282] output for : c = 512, h = 2, w = 2, size = 2048
[INFO 2018-02-12 12:28:50,551 layers.py:2714] output for : c = 512, h = 2, w = 2, size = 2048
[INFO 2018-02-12 12:28:50,552 layers.py:3282] output for : c = 512, h = 2, w = 2, size = 2048
[INFO 2018-02-12 12:28:50,553 layers.py:2856] output for : c = 512, h = 1, w = 1, size = 512
I0212 12:28:50.699533  1322 GradientMachine.cpp:94] Initing parameters..
I0212 12:28:51.627924  1322 GradientMachine.cpp:101] Init parameters done.
I0212 12:28:51.642782  1322 ParameterClient2.cpp:113] pserver 0 10.90.245.28:30002
I0212 12:28:53.938444  1395 ParameterClient2.cpp:113] pserver 0 10.90.245.28:30002
Thread [140664660924160] Forwarding ,
*** Aborted at 1518409737 (unix time) try "date -d @1518409737" if you are using GNU date ***
PC: @                0x0 (unknown)
*** SIGSEGV (&lt;denchmark-link:https://github.com/0x0&gt;@0x0&lt;/denchmark-link&gt;
) received by PID 1322 (TID 0x7fef0b259700) from PID 0; stack trace: ***
@     0x7fef0a859500 (unknown)
@     0x7feeb5cf4d6a __kmp_register_root
@     0x7feeb5cff11e __kmp_middle_initialize
@     0x7feeb5ce8dae __kmp_api_omp_get_num_procs
@     0x7fee825ea2ae mkl_serv_get_num_stripes
@     0x7fee8249f234 mkl_blas_sgemm_host
@     0x7fee8246fa3b mkl_blas_sgemm
@     0x7fee823f13c3 SGEMM
@     0x7fee823b4881 cblas_sgemm
@     0x7feeb67f7279 paddle::gemm&lt;&gt;()
@     0x7feeb66baec3 paddle::GemmConvFunction&lt;&gt;::calc()
@     0x7feeb65adcdf paddle::ExpandConvLayer::forward()
@     0x7feeb65256cd paddle::NeuralNetwork::forward()
@     0x7feeb6526323 paddle::GradientMachine::forwardBackward()
@     0x7feeb6870864 GradientMachine::forwardBackward()
@     0x7feeb64be9c9 _wrap_GradientMachine_forwardBackward
@     0x7fef0ab5ebac PyEval_EvalFrameEx
@     0x7fef0ab605ee PyEval_EvalCodeEx
@     0x7fef0ab5ec27 PyEval_EvalFrameEx
@     0x7fef0ab605ee PyEval_EvalCodeEx
@     0x7fef0ab5ec27 PyEval_EvalFrameEx
@     0x7fef0ab605ee PyEval_EvalCodeEx
@     0x7fef0ab5ec27 PyEval_EvalFrameEx
@     0x7fef0ab605ee PyEval_EvalCodeEx
@     0x7fef0ab5ec27 PyEval_EvalFrameEx
@     0x7fef0ab605ee PyEval_EvalCodeEx
@     0x7fef0ab60702 PyEval_EvalCode
@     0x7fef0ab80450 PyRun_FileExFlags
@     0x7fef0ab8062f PyRun_SimpleFileExFlags
@     0x7fef0ab95f24 Py_Main
@     0x7fef09e4acdd __libc_start_main
Aborted                 (core dumped) stdbuf -oL sh -c "${ENTRY_CMD}"
	</description>
	<comments>
		<comment id='1' author='xymyeah' date='2018-02-12T07:26:32Z'>
		在一个trainer、一个pserver、trainers_count=1，docker memory=10Gi的时候出现问题
但2个trainer、2个pserver、trainers_count=5，docker memory=10Gi的时候可以正常跑
		</comment>
		<comment id='2' author='xymyeah' date='2018-02-12T07:42:10Z'>
		训练还没开始就SIGSEGV了，请先检查数据格式是否正常
		</comment>
		<comment id='3' author='xymyeah' date='2018-02-12T08:00:09Z'>
		数据格式是正常的
1&gt;在centos下
在一个trainer、一个pserver、trainers_count=1，docker memory=10Gi的时候出现问题
但2个trainer、2个pserver、trainers_count=5，docker memory=10Gi的时候可以正常跑
2&gt;在ubuntu下
都可以正常跑
		</comment>
		<comment id='4' author='xymyeah' date='2018-02-12T08:06:32Z'>
		
在一个trainer、一个pserver、trainers_count=1，docker memory=10Gi的时候出现问题

试下trainer_count &gt; 1的情况呢
		</comment>
		<comment id='5' author='xymyeah' date='2018-02-12T08:31:11Z'>
		在一个trainer、一个pserver、trainers_count=5，docker memory=20Gi可以跑通
在一个trainer、一个pserver、trainers_count=5，docker memory=10Gi可以跑通
		</comment>
		<comment id='6' author='xymyeah' date='2018-02-12T08:33:24Z'>
		&lt;denchmark-link:https://github.com/xymyeah&gt;@xymyeah&lt;/denchmark-link&gt;
 推荐用trainer_count &gt; 1去跑，这个应该是V2存在的一个bug，trainer_count==1在底层是不同的实现。
		</comment>
		<comment id='7' author='xymyeah' date='2018-02-12T08:37:30Z'>
		好的，了解
		</comment>
		<comment id='8' author='xymyeah' date='2018-05-09T11:27:44Z'>
		由于长时间没有更新的信息，先关闭这个issue了，如有进一步反馈请随时重新打开。
		</comment>
	</comments>
</bug>