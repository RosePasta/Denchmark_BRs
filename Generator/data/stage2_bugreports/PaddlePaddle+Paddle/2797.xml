<bug id='2797' author='XinyuZhou-1014' open_date='2017-07-10T22:26:35Z' closed_time='2017-07-11T07:16:44Z'>
	<summary>Very slow (exponential) parsing speed when having a large depth of addto_layer</summary>
	<description>
When there's a lot of addto_layers in a network, it will takes exponential parsing time.
E.G. 1:
&lt;denchmark-code&gt;def whole_network(src_embedding):
    enc = src_embedding
    for i in range(depth):
        enc = addto_layer([enc, enc])

    pred = fc_layer(input=fc_layer(
                        input=enc,
                        size=dim_embedding,
                        act=ReluActivation()
                        ),
                    size=label_dict_len,
                    act=SoftmaxActivation())
    return pred
&lt;/denchmark-code&gt;

E.G. 2:
&lt;denchmark-code&gt;def whole_network(src_embedding):
    enc = src_embedding
    for i in range(depth):
        enc_res = fc_layer(input=enc, size=dim_embedding)
        enc_res = fc_layer(input=enc_res, size=dim_embedding)
        enc = addto_layer([enc, enc_res])

    pred = fc_layer(input=fc_layer(
                        input=enc,
                        size=dim_embedding,
                        act=ReluActivation()
                        ),
                    size=label_dict_len,
                    act=SoftmaxActivation())
    return pred
&lt;/denchmark-code&gt;

Both will costs a huge amount of time to parse (test by the nest_diagram tool).
My parsing time:
&lt;denchmark-code&gt;depth: 4,   parsing time: 0.16.
depth: 8,   parsing time: 0.16.
depth: 12, parsing time: 0.33.
depth: 16, parsing time: 2.02.
depth: 20, parsing time: 32.08.
depth: 21, parsing time: 67.05.
depth: 22, parsing time: 131.48.
depth: 23, parsing time: 268.82.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='XinyuZhou-1014' date='2017-07-11T00:11:49Z'>
		Just mark that " parsing time means the time used for parsing and generating the protobuf".
		</comment>
		<comment id='2' author='XinyuZhou-1014' date='2017-07-11T07:19:38Z'>
		This issue is fixed by &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/2802&gt;#2802&lt;/denchmark-link&gt;
. It because when we parsing network topology by , it just runs a depth-first search without any optimization.
In &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/2802&gt;#2802&lt;/denchmark-link&gt;
, if a node is visited before, that node is just skipped. It will fix this issue.
You can just change  in paddle python package by this &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/2802/files#diff-a06e8d6c21eec2ca486aa4ce1099273a&gt;patch&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='3' author='XinyuZhou-1014' date='2017-07-11T18:29:20Z'>
		Thanks a lot!
		</comment>
	</comments>
</bug>