<bug id='318' author='jamestang0219' open_date='2016-11-02T10:01:25Z' closed_time='2016-11-02T17:47:13Z'>
	<summary>How to test some samples while training in newest version?</summary>
	<description>
I updated my paddle version to 0.8.0b3, and then trained a RNN model.
I found that there is no Tester while training.
But I used previous version to train the model, Tester can test the test data every 100 batches.
Here is my train config:
&lt;denchmark-code&gt;paddle train \
--config=$mod \
--save_dir=./model_desc_full \
--trainer_count=4 \
--log_period=100 \
--num_passes=10 \
--use_gpu=true \
--show_parameter_stats_period=1000 \
--test_all_data_in_one_period=1 \
--config_args=is_predict=0 \
&lt;/denchmark-code&gt;

This config worked in previous version, but the Tester disappear in newest version.
I wanna know hot to set the config to TEST while TRAINING
	</description>
	<comments>
		<comment id='1' author='jamestang0219' date='2016-11-02T10:17:41Z'>
		&lt;denchmark-link:https://github.com/jamestang0219&gt;@jamestang0219&lt;/denchmark-link&gt;
 set --test_period=1000 will do test job after 1000 mini-batch.
We changed the 's default value to zero, means test after one pass.
It is very kind if you have time to submit a PR change demo's run.sh, if you don't have time, please let us know and leave them to us.
&lt;denchmark-link:https://github.com/backyes&gt;@backyes&lt;/denchmark-link&gt;
 Please change the demo's shell script if &lt;denchmark-link:https://github.com/jamestang0219&gt;@jamestang0219&lt;/denchmark-link&gt;
 don't want submit a PR, because the default value of  has been changed.
		</comment>
		<comment id='2' author='jamestang0219' date='2016-11-02T10:29:37Z'>
		&lt;denchmark-link:https://github.com/reyoung&gt;@reyoung&lt;/denchmark-link&gt;

Here is the log of previous version:
`I1101 07:53:16.276069 119355 TrainerInternal.cpp:162]  Batch=8800 samples=281600 AvgCost=0.0174819 CurrentCost=0.0205525 Eval: classification_error_evaluator=0.00603693  CurrentEval: classification_error_evaluator=0.008125
...................................................................................................
I1101 07:53:42.518815 119355 TrainerInternal.cpp:162]  Batch=8900 samples=284800 AvgCost=0.0175253 CurrentCost=0.0213492 Eval: classification_error_evaluator=0.00604986  CurrentEval: classification_error_evaluator=0.0071875
...................................................................................................I1101 07:54:08.425493 119355 TrainerInternal.cpp:204] .w0  avg_abs_val=0.106658    max_val=3.63432     avg_abs_grad=1.56957e-07 max_grad=0.00512181
I1101 07:54:08.426228 119355 TrainerInternal.cpp:204] .w0   avg_abs_val=0.16739     max_val=0.997636    avg_abs_grad=0.00018073  max_grad=0.00459319
I1101 07:54:08.426569 119355 TrainerInternal.cpp:204] .wbias avg_abs_val=0.195303    max_val=0.520971    avg_abs_grad=0.000887678 max_grad=0.0156855
I1101 07:54:08.427664 119355 TrainerInternal.cpp:204] .w0  avg_abs_val=0.125715    max_val=1.15954     avg_abs_grad=0.000132394 max_grad=0.0128729
I1101 07:54:08.428009 119355 TrainerInternal.cpp:204] .wbias avg_abs_val=0.160678    max_val=0.589867    avg_abs_grad=0.000734516 max_grad=0.0196687
I1101 07:54:08.428349 119355 TrainerInternal.cpp:204] .w0   avg_abs_val=0.461006    max_val=1.20577     avg_abs_grad=0.00776989  max_grad=0.0120312
I1101 07:54:08.428617 119355 TrainerInternal.cpp:204] .wbias avg_abs_val=0.310792    max_val=0.310793    avg_abs_grad=0.0090016   max_grad=0.00900169
I1101 07:54:08.428644 119355 TrainerInternal.cpp:162]  Batch=9000 samples=288000 AvgCost=0.0175542 CurrentCost=0.0201257 Eval: classification_error_evaluator=0.00605903  CurrentEval: classification_error_evaluator=0.006875
I1101 07:54:10.472939 119355 Tester.cpp:111]  Test samples=1000 cost=0.466531 Eval: classification_error_evaluator=0.159
...................................................................................................
I1101 07:54:35.642098 119355 TrainerInternal.cpp:162]  Batch=9100 samples=291200 AvgCost=0.0175926 CurrentCost=0.0210439 Eval: classification_error_evaluator=0.00606799  CurrentEval: classification_error_evaluator=0.006875
...................................................................................................
I1101 07:55:01.101780 119355 TrainerInternal.cpp:162]  Batch=9200 samples=294400 AvgCost=0.01767 CurrentCost=0.0247131 Eval: classification_error_evaluator=0.00611073  CurrentEval: classification_error_evaluator=0.01`
It worked well for testing every 1000 batches
		</comment>
		<comment id='3' author='jamestang0219' date='2016-11-02T10:31:49Z'>
		&lt;denchmark-link:https://github.com/reyoung&gt;@reyoung&lt;/denchmark-link&gt;

And here is the newest version logs:
`I1102 18:22:04.354857 34647 TrainerInternal.cpp:165]  Batch=3800 samples=486400 AvgCost=0.107311 CurrentCost=0.0766458 Eval: classification_error_evaluator=0.041801  CurrentEval: classification_error_evaluator=0.0296875
...................................................................................................
I1102 18:23:05.413488 34647 TrainerInternal.cpp:165]  Batch=3900 samples=499200 AvgCost=0.10815 CurrentCost=0.140034 Eval: classification_error_evaluator=0.0420613  CurrentEval: classification_error_evaluator=0.0519531
...................................................................................................I1102 18:24:05.555019 34647 TrainerInternal.cpp:207] .w0  avg_abs_val=0.0516422   max_val=0.87552     avg_abs_grad=1.31203e-05 max_grad=0.202811
I1102 18:24:05.556380 34647 TrainerInternal.cpp:207] .w0   avg_abs_val=0.133105    max_val=1.14789     avg_abs_grad=0.00251638  max_grad=0.252082
I1102 18:24:05.556483 34647 TrainerInternal.cpp:207] .wbias avg_abs_val=0.13841     max_val=0.452819    avg_abs_grad=0.0268143   max_grad=2.08488
I1102 18:24:05.557827 34647 TrainerInternal.cpp:207] .w0  avg_abs_val=0.0976409   max_val=0.819044    avg_abs_grad=0.00756949  max_grad=2.52075
I1102 18:24:05.557920 34647 TrainerInternal.cpp:207] .wbias avg_abs_val=0.117548    max_val=0.493404    avg_abs_grad=0.0481846   max_grad=5.2654
I1102 18:24:05.558040 34647 TrainerInternal.cpp:207] .w0   avg_abs_val=0.109183    max_val=0.359466    avg_abs_grad=1.06527     max_grad=5.85286
I1102 18:24:05.558117 34647 TrainerInternal.cpp:207] .wbias avg_abs_val=0.0851114   max_val=0.0851116   avg_abs_grad=3.39263     max_grad=3.39263
I1102 18:24:05.558130 34647 TrainerInternal.cpp:165]  Batch=4000 samples=512000 AvgCost=0.110242 CurrentCost=0.191823 Eval: classification_error_evaluator=0.0426113  CurrentEval: classification_error_evaluator=0.0640625
...................................................................................................
I1102 18:25:05.374027 34647 TrainerInternal.cpp:165]  Batch=4100 samples=524800 AvgCost=0.115381 CurrentCost=0.320934 Eval: classification_error_evaluator=0.0453944  CurrentEval: classification_error_evaluator=0.156719`
and I also add the --test_period=1000 config, but it doesn't test the test data while training.
		</comment>
		<comment id='4' author='jamestang0219' date='2016-11-02T10:49:54Z'>
		&lt;denchmark-link:https://github.com/reyoung&gt;@reyoung&lt;/denchmark-link&gt;

One more question.
While training, the AvgCost sometimes became nan, and it caused training failed.
So what's problem?
`I1102 18:24:05.558130 34647 TrainerInternal.cpp:165]  Batch=4000 samples=512000 AvgCost=0.110242 CurrentCost=0.191823 Eval: classification_error_evaluator=0.0426113  CurrentEval: classification_error_evaluator=0.0640625
...................................................................................................
I1102 18:25:05.374027 34647 TrainerInternal.cpp:165]  Batch=4100 samples=524800 AvgCost=0.115381 CurrentCost=0.320934 Eval: classification_error_evaluator=0.0453944  CurrentEval: classification_error_evaluator=0.156719
...................................................................................................
I1102 18:26:06.421185 34647 TrainerInternal.cpp:165]  Batch=4200 samples=537600 AvgCost=nan CurrentCost=nan Eval: classification_error_evaluator=0.0540532  CurrentEval: classification_error_evaluator=0.469609
...................................................................................................
I1102 18:27:06.688400 34647 TrainerInternal.cpp:165]  Batch=4300 samples=550400 AvgCost=nan CurrentCost=nan Eval: classification_error_evaluator=0.0715807  CurrentEval: classification_error_evaluator=0.952656
*** Aborted at 1478082836 (unix time) try "date -d @1478082836" if you are using GNU date ***
PC: @           0x700e89 paddle::GpuVectorT&lt;&gt;::getAbsMax()
*** SIGFPE (@0x700e89) received by PID 34647 (TID 0x7f7d72fb9700) from PID 7343753; stack trace: ***
&lt;denchmark-code&gt;@     0x7f7da965a330 (unknown)

@           0x700e89 paddle::GpuVectorT&lt;&gt;::getAbsMax()

@           0x6a5a69 _ZNSt17_Function_handlerIFvPN6paddle9ParameterEEZNS0_15TrainerInternal13trainOneBatchElRKNS0_9DataBatchEPSt6vectorINS0_8ArgumentESaIS9_EEEUlS2_E_E9_M_invokeERKSt9_Any_dataS2_

@           0x6579bc paddle::TrainerThread::doCallback()

@           0x657e91 paddle::TrainerThread::gradCollectThread()

@     0x7f7da8836a60 (unknown)

@     0x7f7da9652184 start_thread

@     0x7f7da7f9e37d (unknown)

@                0x0 (unknown)
&lt;/denchmark-code&gt;

/usr/local/paddle/bin//paddle: line 81: 34647 Floating point exception(core dumped) ${DEBUGGER} $MYDIR/../opt/paddle/bin/paddle_trainer ${@:2}`
		</comment>
		<comment id='5' author='jamestang0219' date='2016-11-02T13:47:56Z'>
		&lt;denchmark-link:https://github.com/reyoung&gt;@reyoung&lt;/denchmark-link&gt;
 had found some clue for this BUG, he will fix it later
		</comment>
	</comments>
</bug>