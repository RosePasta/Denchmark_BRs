<bug id='14988' author='kbinias' open_date='2018-12-20T16:41:33Z' closed_time='2019-07-03T08:25:59Z'>
	<summary>Transformer Fails when MKLDNN is used</summary>
	<description>
This commit made Transformer (used with MKL-DNN) crashed:
&lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/commit/a872eb90c2384596ee46d29ad9f39bfe3f6ebc1b&gt;a872eb9&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/panyx0718&gt;@panyx0718&lt;/denchmark-link&gt;
 Could you please take a look and advice on this issue?
if there is a problem with reproduction of issue or more information is needed, feel free to ask.
Example log &amp; reference command line for reproduction:
&lt;denchmark-code&gt; FLAGS_use_mkldnn=True OMP_NUM_THREADS=1    python -u infer.py --src_vocab_fpath /nfs/fm/home/jczaja/data/wmt16_ende_data_bpe_clean/vocab_all.bpe.32000  --trg_vocab_fpath /nfs/fm/home/jczaja/data/wmt16_ende_data_bpe_clean/vocab_all.bpe.32000   --special_token '&lt;s&gt;' '&lt;e&gt;' '&lt;unk&gt;'   --test_file_pattern /nfs/fm/home/jczaja/data/wmt16_ende_data_bpe_clean/newstest2016.tok.bpe.32000.en-de   --token_delimiter ' '    model_path /nfs/fm/home/jczaja/models/iter_100000.infer.model   beam_size 4 max_out_len 255
-----------  Configuration Arguments -----------
batch_size: 1
device: CPU
display_output: False
num_profiling_passes: 100
opts: ['model_path', '/nfs/fm/home/jczaja/models/iter_100000.infer.model', 'beam_size', '4', 'max_out_len', '255']
pool_size: 10000
profile: True
skip_pass_num: 0
special_token: ['&lt;s&gt;', '&lt;e&gt;', '&lt;unk&gt;']
src_vocab_fpath: /nfs/fm/home/jczaja/data/wmt16_ende_data_bpe_clean/vocab_all.bpe.32000
test_file_pattern: /nfs/fm/home/jczaja/data/wmt16_ende_data_bpe_clean/newstest2016.tok.bpe.32000.en-de
token_delimiter:
trg_vocab_fpath: /nfs/fm/home/jczaja/data/wmt16_ende_data_bpe_clean/vocab_all.bpe.32000
------------------------------------------------
Traceback (most recent call last):
  File "infer-profile.py", line 308, in &lt;module&gt;
    infer(args)
  File "infer-profile.py", line 296, in infer
    inferencer(test_data, trg_idx2word)
  File "infer-profile.py", line 208, in fast_infer
    return_numpy=False)
  File "/home/jczaja/paddle/virtualpython/local/lib/python2.7/site-packages/paddle/fluid/executor.py", line 472, in run
    self.executor.run(program.desc, scope, 0, True, True)
paddle.fluid.core.EnforceNotMet: Enforce failed. Expected x_rank == axis_size, but received x_rank:3 != axis_size:4.
The input tensor's rank(3) should be equal to the axis's size(4) at [/home/jczaja/paddle/paddle/fluid/operators/transpose_op.cc:43]
PaddlePaddle Call Stacks:
0       0x7fd2a422ec7dp void paddle::platform::EnforceNotMet::Init&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt;(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, char const*, int) + 509
1       0x7fd2a4230042p paddle::platform::EnforceNotMet::EnforceNotMet&lt;char const*, char const*, char const*, char const*, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, char const*, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt;(char const*, int, char const*, char const*, char const*, char const*, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, char const*, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;) + 162
2       0x7fd2a49b0b95p paddle::operators::TransposeOp::InferShape(paddle::framework::InferShapeContext*) const + 1397
3       0x7fd2a49b12b2p paddle::operators::Transpose2Op::InferShape(paddle::framework::InferShapeContext*) const + 50
4       0x7fd2a500a3c7p paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&amp;, boost::variant&lt;paddle::platform::CUDAPlace, paddle::platform::CPUPlace, paddle::platform::CUDAPinnedPlace, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_&gt; const&amp;) const + 599
5       0x7fd2a5005127p paddle::framework::OperatorBase::Run(paddle::framework::Scope const&amp;, boost::variant&lt;paddle::platform::CUDAPlace, paddle::platform::CPUPlace, paddle::platform::CUDAPinnedPlace, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_&gt; const&amp;) + 535
6       0x7fd2a431af77p paddle::framework::Executor::RunPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, bool, bool, bool) + 295
7       0x7fd2a4de84b5p paddle::operators::WhileOp::RunImpl(paddle::framework::Scope const&amp;, boost::variant&lt;paddle::platform::CUDAPlace, paddle::platform::CPUPlace, paddle::platform::CUDAPinnedPlace, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_&gt; const&amp;) const + 981
8       0x7fd2a5005127p paddle::framework::OperatorBase::Run(paddle::framework::Scope const&amp;, boost::variant&lt;paddle::platform::CUDAPlace, paddle::platform::CPUPlace, paddle::platform::CUDAPinnedPlace, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_, boost::detail::variant::void_&gt; const&amp;) + 535
9       0x7fd2a431af77p paddle::framework::Executor::RunPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, bool, bool, bool) + 295
10      0x7fd2a431d9fap paddle::framework::Executor::Run(paddle::framework::ProgramDesc const&amp;, paddle::framework::Scope*, int, bool, bool) + 218
11      0x7fd2a42060fap
12      0x7fd2a425225dp
13            0x4cada2p PyEval_EvalFrameEx + 28066
14            0x4c2765p PyEval_EvalCodeEx + 597
15            0x4ca099p PyEval_EvalFrameEx + 24729
16            0x4c9d8fp PyEval_EvalFrameEx + 23951
17            0x4c2765p PyEval_EvalCodeEx + 597
18            0x4ca099p PyEval_EvalFrameEx + 24729
19            0x4c2765p PyEval_EvalCodeEx + 597
20            0x4c2509p PyEval_EvalCode + 25
21            0x4f1defp
22            0x4ec652p PyRun_FileExFlags + 130
23            0x4eae31p PyRun_SimpleFileExFlags + 401
24            0x49e14ap Py_Main + 1674
25      0x7fd2c0644830p __libc_start_main + 240
26            0x49d9d9p _start + 41
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='kbinias' date='2018-12-21T02:02:31Z'>
		Is there any test codes or model codes in Paddle repo that I can use to reproduce? It seems your example is using your local saved model
		</comment>
		<comment id='2' author='kbinias' date='2018-12-21T02:35:52Z'>
		Does it work without MKLDNN?
		</comment>
		<comment id='3' author='kbinias' date='2018-12-21T02:51:30Z'>
		&lt;denchmark-link:https://user-images.githubusercontent.com/2887803/50321976-3b0e8880-050e-11e9-9465-22a18d757ea3.png&gt;&lt;/denchmark-link&gt;

There is a change in while_op.cc default behavior (which I probably shouldn't do)
Compile time infer shape now always SetShape (It used to SetShape only when input is LodTensor and LodTensorArray
		</comment>
		<comment id='4' author='kbinias' date='2018-12-21T06:35:19Z'>
		It works good without MKL-DNN (FLAGS_use_mkldnn=False)
		</comment>
		<comment id='5' author='kbinias' date='2018-12-21T13:39:24Z'>
		It is public python infer.py, to reproduce check next comment.
Dataset: &lt;denchmark-link:http://transformer-model-data.bj.bcebos.com/wmt16_ende_data_bpe_clean.tar.gz&gt;http://transformer-model-data.bj.bcebos.com/wmt16_ende_data_bpe_clean.tar.gz&lt;/denchmark-link&gt;

Model: &lt;denchmark-link:http://transformer-model-data.bj.bcebos.com/iter_100000.infer.model.tar.gz&gt;http://transformer-model-data.bj.bcebos.com/iter_100000.infer.model.tar.gz&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='kbinias' date='2019-01-03T11:24:02Z'>
		
paddlepaddle fails when testing transformer with use_mkldnn=1
Reproduce: location:
[https://github.com/PaddlePaddle/models/tree/develop/fluid/PaddleNLP/neural_machine_translation/transformer](public transformer)
script:
export FLAGS_use_mkldnn=1
export OMP_NUM_THREADS=1
python infer.py
--src_vocab_fpath ~/data/wmt16_ende_data_bpe_clean/vocab_all.bpe.32000
--trg_vocab_fpath ~/data/wmt16_ende_data_bpe_clean/vocab_all.bpe.32000
--test_file_pattern ~/data/wmt16_ende_data_bpe_clean/newstest2016.tok.bpe.32000.en-de
--batch_size 8
model_path ~/models/iter_100000.infer.model
beam_size 4
max_out_len 255
use_gpu False

&lt;denchmark-link:https://github.com/panyx0718&gt;@panyx0718&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/kbinias&gt;@kbinias&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='kbinias' date='2019-01-10T15:08:19Z'>
		&lt;denchmark-link:https://github.com/panyx0718&gt;@panyx0718&lt;/denchmark-link&gt;
 any update on that?
A quick check showed that simple limiting  to LodTensor or LodTensorArray 's inputs doesn't solve the issue.
		</comment>
	</comments>
</bug>