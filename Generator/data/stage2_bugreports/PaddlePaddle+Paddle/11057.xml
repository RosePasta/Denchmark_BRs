<bug id='11057' author='daming-lu' open_date='2018-05-30T22:17:20Z' closed_time='2020-05-22T08:14:34Z'>
	<summary>GPU results are non-deterministic</summary>
	<description>
When we stabilize all the randomness and run the same training twice on GPU (same GPU core), the results are different by a tiny precision. This is NOT happening on CPU.
See the attachments. The demo code is in &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/11058&gt;this&lt;/denchmark-link&gt;
 PR
vimdiff w2v_nocuda_t1.txt w2v_nocuda_t2.txt  // no difference

vimdiff w2v_t1_cuda.txt w2v_t2_cuda.txt  // has some tiny difference
&lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/files/2055839/w2v_nocuda_t1.txt&gt;w2v_nocuda_t1.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/files/2055840/w2v_nocuda_t2.txt&gt;w2v_nocuda_t2.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/files/2055841/w2v_t1_cuda.txt&gt;w2v_t1_cuda.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/files/2055842/w2v_t2_cuda.txt&gt;w2v_t2_cuda.txt&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='daming-lu' date='2018-05-30T22:21:20Z'>
		I think there is probably some issue with the "embedding" layer (and/or the "lookup_table_op"). Even this sentiment analysis code (&lt;denchmark-link:https://github.com/sidgoyal78/Paddle/blob/a801e7bcb2f4f1e131f6a640ecd84a03d21588ff/test_sa_conv.py&gt;https://github.com/sidgoyal78/Paddle/blob/a801e7bcb2f4f1e131f6a640ecd84a03d21588ff/test_sa_conv.py&lt;/denchmark-link&gt;
) produces inconsistent results when run with the same seed on GPU.
But if the same code is run on CPU, then it yields consistent results.
		</comment>
		<comment id='2' author='daming-lu' date='2018-05-30T23:05:08Z'>
		&lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/issues/10405&gt;#10405&lt;/denchmark-link&gt;
 (related issue)
		</comment>
		<comment id='3' author='daming-lu' date='2018-05-31T02:31:09Z'>
		&lt;denchmark-link:https://github.com/daming-lu&gt;@daming-lu&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/sidgoyal78&gt;@sidgoyal78&lt;/denchmark-link&gt;

We have noticed this phenomenon, and we have found that some operation's result on GPU is non-determinism, such as cross_entropy and some operations of cudnn.
Other frameworks also have this similar issue, such as TF(&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/2732&gt;tensorflow/tensorflow#2732&lt;/denchmark-link&gt;
), Pytorch(&lt;denchmark-link:https://github.com/soumith/cudnn.torch/issues/270&gt;soumith/cudnn.torch#270&lt;/denchmark-link&gt;
). I saw the same question in Nvidia forums too.
		</comment>
		<comment id='4' author='daming-lu' date='2018-06-04T02:31:05Z'>
		This bug has been located. If the kernel using CudaAtomicAdd, caused by the non-associated of floating number algebra,  we will get the non-deterministic result.
		</comment>
		<comment id='5' author='daming-lu' date='2018-06-04T02:31:58Z'>
		That's also convinced by our experiment on the benchmark ops work.&lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/10646&gt;#10646&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='daming-lu' date='2018-06-04T02:37:22Z'>
		Here is siddharth's reproduce PR. &lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/pull/11133&gt;#11133&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='daming-lu' date='2018-06-25T18:51:32Z'>
		&lt;denchmark-link:https://github.com/dzhwinter&gt;@dzhwinter&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/chengduoZH&gt;@chengduoZH&lt;/denchmark-link&gt;
 : Thanks for the updates! One question: do we have a plan to fix it? As you know, Baidu is a major contributor to &lt;denchmark-link:https://mlperf.org/&gt;MLPerf&lt;/denchmark-link&gt;
 and we want to get performance metrics for our own PaddlePaddle framework 
		</comment>
		<comment id='8' author='daming-lu' date='2018-08-15T10:24:33Z'>
		您好，此issue在近一个月内暂无更新，我们将于今天内关闭。若在关闭后您仍需跟进提问，可重新开启此问题，我们将在24小时内回复您。因关闭带来的不便我们深表歉意，请您谅解~感谢您对PaddlePaddle的支持!
Hello, this issue has not been updated in the past month. We will close it today for the sake of other user‘s experience. If you still need to follow up on this question after closing, please feel free to reopen it. In that case, we will get back to you within 24 hours. We apologize for the inconvenience caused by the closure and thank you so much for your support of PaddlePaddle Group!
		</comment>
		<comment id='9' author='daming-lu' date='2020-05-22T08:14:34Z'>
		Since you haven't replied for more than a year, we have closed this issue/pr.
If the problem is not solved or there is a follow-up one, please reopen it at any time and we will continue to follow up.
由于您超过一年未回复，我们将关闭这个issue/pr。
若问题未解决或有后续问题，请随时重新打开，我们会继续跟进。
		</comment>
	</comments>
</bug>