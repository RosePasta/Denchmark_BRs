<bug id='10963' author='Yancey1989' open_date='2018-05-28T04:31:10Z' closed_time='2018-08-15T11:35:37Z'>
	<summary>distributed training hung with ParallelExecutor</summary>
	<description>
Distributed training hung with ParallleExecutor:
2 trainers + 2 pservers on vgg16 + followers dataset
I0528 04:26:15.910966 13997 operator.cc:546] expected_kernel_key:data_type[float]:data_layout[ANY_LAYOUT]:place[CUDAPlace(0)]:library_type[PLAIN]
I0528 04:26:15.911159 14019 operator.cc:546] expected_kernel_key:data_type[float]:data_layout[ANY_LAYOUT]:place[CUDAPlace(0)]:library_type[PLAIN]
I0528 04:26:15.911401 14010 operator.cc:546] expected_kernel_key:data_type[float]:data_layout[ANY_LAYOUT]:place[CUDAPlace(0)]:library_type[PLAIN]
I0528 04:26:15.911592 14032 operator.cc:546] expected_kernel_key:data_type[float]:data_layout[ANY_LAYOUT]:place[CUDAPlace(0)]:library_type[CUDNN]
I0528 04:26:15.916076 14004 send_vars_op.cc:57] sending batch_norm_0.w_0@GRAD.trainer_1 to 10.1.14.3:18210
I0528 04:26:15.916328 14000 send_vars_op.cc:57] sending conv2d_0.w_0@GRAD.trainer_1 to 10.1.14.3:18210
I0528 04:26:15.916420 14016 send_vars_op.cc:57] sending conv2d_0.b_0@GRAD.trainer_1 to 10.1.14.3:18210
I0528 04:26:15.916592 14014 send_vars_op.cc:57] sending batch_norm_0.b_0@GRAD.trainer_1 to 10.1.14.3:18211
	</description>
	<comments>
		<comment id='1' author='Yancey1989' date='2018-06-18T10:08:23Z'>
		ParallelExecutor has a thread pool to make ops parallelism and use the inputs/outputs of each op to identify the dependency of ops, if we use send_op as async mode, it would exist immediately and running backend(run cudaMemcpy etc..), this would lead to other ops hung on cuda stream.
		</comment>
		<comment id='2' author='Yancey1989' date='2018-08-15T11:35:34Z'>
		您好，此issue在近一个月内暂无更新，我们将于今天内关闭。若在关闭后您仍需跟进提问，可重新开启此问题，我们将在24小时内回复您。因关闭带来的不便我们深表歉意，请您谅解~感谢您对PaddlePaddle的支持!
Hello, this issue has not been updated in the past month. We will close it today for the sake of other user‘s experience. If you still need to follow up on this question after closing, please feel free to reopen it. In that case, we will get back to you within 24 hours. We apologize for the inconvenience caused by the closure and thank you so much for your support of PaddlePaddle Group!
		</comment>
	</comments>
</bug>