<bug id='327' author='buptzzl' open_date='2016-11-03T02:21:09Z' closed_time='2016-12-06T09:47:24Z'>
	<summary>怎么相同训练配置的GPU 作业比CPU 模式AUC差呢？</summary>
	<description>
发现新版的GPU训练效果比CPU效果略差是为什么呢？能优化到一致吗？
数据集：数据(feature_size=1.6k)：train=31w, test=13.3w
网络：Input（dim=1000）直接接Output(dim=2)的二分类LR
算法：AdamOptimizer
	</description>
	<comments>
		<comment id='1' author='buptzzl' date='2016-11-03T02:34:13Z'>
		能贴出详细一点的信息么，比如网络配置，两者结果差了几个点，GPU和CPU都是使用几个训练？
		</comment>
		<comment id='2' author='buptzzl' date='2016-11-03T05:21:38Z'>
		&lt;denchmark-link:https://github.com/luotao1&gt;@luotao1&lt;/denchmark-link&gt;
   补充一下信息：
本地训练模式，CPU10 比GPU2 模式训练出来的模型的AUC高约0.005的。
评估结果数据：
实验分组      AUC-test
CPU10    0.69118|p4
GPU2    0.6845|p48
GPU4    0.6854|p85
其中，CPU10即关闭GPU&amp;trainer_count=10， GPU2即打开GPU&amp;trainer_count=2，依此类推。
网络配置：
fea_size = [1604]
nbatch_size = 200 if not is_predict else 1
settings(
    batch_size=nbatch_size,
    learning_rate=2e-3,
    learning_method=AdamOptimizer(),
    regularization=L2Regularization(8e-4),
    gradient_clipping_threshold=25
)
fea_list = []
for i, sz in enumerate(fea_size):
    fea_list.append(data_layer(name="slot%d" % i, size=sz))
label_size = 2 
output = fc_layer(input=fea_list, size=label_size, act=SoftmaxActivation())
		</comment>
		<comment id='3' author='buptzzl' date='2016-11-03T06:43:22Z'>
		May be the problem of gradient_clipping_threshold, we will have a discuss.
		</comment>
		<comment id='4' author='buptzzl' date='2016-11-03T08:14:20Z'>
		CPU10 0.69118|p4，后面的p4是什么意思？4个pass么
		</comment>
		<comment id='5' author='buptzzl' date='2016-11-03T09:20:59Z'>
		&lt;denchmark-link:https://github.com/luotao1&gt;@luotao1&lt;/denchmark-link&gt;
  是的。每组训练都输出了100个pass的模型，取的最优的那组模型。 CPU10 0.69118|p4 即在第4轮输出的模型在测试集上的AUC为0.6912
		</comment>
		<comment id='6' author='buptzzl' date='2016-11-07T12:42:40Z'>
		&lt;denchmark-link:https://github.com/luotao1&gt;@luotao1&lt;/denchmark-link&gt;
  对应的数据和代码都搬到icode 环境 了，具体如下：
ssh://zhouliaoming01@icode.baidu.com:8235/baidu/personal-code/zhouliaoming_debug
复现方式：
将代码库拷贝下来后， cd zhouliaoming_debug  &amp;&amp; sh train_fsg_1k.sh 即执行CPU模型训练与测试。
sh  train_gpu.sh 对应GPU模式。
然后在两组模型的测试日志中的最优效果所在Pass，对比即可。
		</comment>
		<comment id='7' author='buptzzl' date='2016-12-02T09:02:14Z'>
		问题已复现，我跑了cpu (trainer_count=2和20) 以及gpu (trainer_count=2) 这三种情况，发现：

100个pass后，classification_error_evaluator都为0.00987179。
100个pass后，cost都在0.054左右，三种情况区别在于小数点后4位不同。
auc_evaluator_0：cpu比gpu的结果好


gpu (trainer_count=2) ：最好的是pass-79, AUC=0.611518
cpu (trainer_count=2) ：最好的是pass-76, AUC=0.62353
cpu (trainer_count=20) ：最好的是pass-67, AUC=0.628484

&lt;denchmark-link:https://github.com/PaddlePaddle/Paddle/issues/329&gt;#329&lt;/denchmark-link&gt;
 和这个问题类似，都是AUC有差。
		</comment>
		<comment id='8' author='buptzzl' date='2016-12-06T08:06:29Z'>
		你好，请问上述实验重复过几组？稳定永远是GPU 差于 CPU 吗？
我不认为这是bug。如果确保以下两个因素是固定的再对比 gpu / cpu 训练出的模型评估 auc 。（1） fix 模型初始化时的随机种子，设置paddle_trainer 的启动参数 ：thread_local_rand_use_global_seed=1， seed=1；（2）在dataprovider 中关闭 data shuffle。对比同 pass cpu 和  gpu 训练出的模型，auc 是否有差异，而不是对比最佳pass。
否则，以上结果极大可能只是由于数值优化差异引起的。
		</comment>
	</comments>
</bug>