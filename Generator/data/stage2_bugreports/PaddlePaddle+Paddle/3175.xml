<bug id='3175' author='FrankRouter' open_date='2017-08-02T09:23:12Z' closed_time='2018-03-20T03:26:24Z'>
	<summary>v2的MPI任务，设置sparse update后速度没提高</summary>
	<description>
网络结构
输入为80万稀疏特征
一个隐层128维
softmax
损失函数
multiclass cross entropy
对比实验
实验零
v1版本，不设sparse update，一个pass约3小时
实验一
v1版本，设置sparse update，一个pass约半小时
实验二
v2版本，不设sparse update，一个pass约4.5小时
实验三
v2版本，设置sparse update，一个pass约4.5小时
问题
v2版本，训练速度没有提高
	</description>
	<comments>
		<comment id='1' author='FrankRouter' date='2017-08-02T09:30:31Z'>
		请问您是怎么设置的？
		</comment>
		<comment id='2' author='FrankRouter' date='2017-08-02T12:05:53Z'>
		试验零和试验一是用paddle_1103版本提交的
1、试验零、trainer_config.conf
Layer(inputs = [Input("input1", parameter_name = "_layer1_1.w", decay_rate_l1 = 0.01)], 
name = "layer1_1", 
bias = Bias(parameter_name = "_layer1_1.bias"), 
active_type = "tanh", 
type = "fc", 
size = 128)
2、试验一、trainer_config.conf
Layer(inputs = [Input("input1", parameter_name = "_layer1_1.w", sparse_remote_update = True, decay_rate_l1 = 0.01)], 
name = "layer1_1", 
bias = Bias(parameter_name = "_layer1_1.bias"), 
active_type = "tanh", 
type = "fc", 
size = 128)
3、试验二、trainer_config.conf
 h = paddle.layer.fc(                                                         
         input=data,                                                              
         size=h_size,                                                             
         act=paddle.activation.Tanh(),                                            
         param_attr=paddle.attr.Param(initial_std=1.0 / math.sqrt(h_size)))
4、试验三、trainer_config.conf
     h = paddle.layer.fc(                                                         
         input=data,                                                              
         size=h_size,                                                             
         act=paddle.activation.Tanh(),                                            
         param_attr=paddle.attr.Param(initial_std=1.0 / math.sqrt(h_size), sparse_update=True))
		</comment>
		<comment id='3' author='FrankRouter' date='2017-08-03T06:29:41Z'>
		看这里，实验1是在_layer1_1.w这个层加了sparse update，而实验2是在fc加的？fc层本身不够稀疏，这样两个实验的网络是有区别的，可否保持实验2的网络配置一致再测试下呢？
		</comment>
		<comment id='4' author='FrankRouter' date='2017-08-03T06:44:52Z'>
		那试验三的网络配置应该怎么配置才能对layer1_1.w加sparse_update，而不是对fc呢
		</comment>
		<comment id='5' author='FrankRouter' date='2017-08-03T06:50:04Z'>
		试下把input=data的data这层改成sparse呢？
		</comment>
		<comment id='6' author='FrankRouter' date='2017-08-03T06:54:48Z'>
		我知道你的意思，但是不知道怎么配置，这样？
data = paddle.layer.data("word",
paddle.data_type.sparse_binary_vector(dict_dim), param_attr=paddle.attr.Param(initial_std=1.0 / math.sqrt(h_size), sparse_update=True))
h = paddle.layer.fc(
input=data,
size=h_size,
act=paddle.activation.Tanh(),
param_attr=paddle.attr.Param(initial_std=1.0 / math.sqrt(h_size)))
		</comment>
		<comment id='7' author='FrankRouter' date='2017-08-03T06:55:32Z'>
		data层应该没有初始化param_attr=paddle.attr.Param这个参数吧
		</comment>
		<comment id='8' author='FrankRouter' date='2017-08-03T07:05:39Z'>
		刚咨询了下，sparse_update配置再data_layer是没有效果的data_layer是不会上传梯度的。v1的Input层实际是包含了参数的，上面的v1的配置转成v2可能不是直接data后接fc，或许中间应该加一个embedding_layer，并且设置embedding_layer为sparse_update。
麻烦 &lt;denchmark-link:https://github.com/jacquesqiao&gt;@jacquesqiao&lt;/denchmark-link&gt;
 帮看下？
		</comment>
		<comment id='9' author='FrankRouter' date='2017-08-08T07:17:41Z'>
		我觉得实验三的配置方法是没问题的。如果确实没有加速效果，也有可能是v2 本身实现时候，sparse update 有问题。这个需要一点时间排查。
		</comment>
		<comment id='10' author='FrankRouter' date='2017-08-16T05:55:22Z'>
		&lt;denchmark-link:https://github.com/FrankRouter&gt;@FrankRouter&lt;/denchmark-link&gt;
 这几个实验是本地执行的还是集群方式执行的呢？
		</comment>
		<comment id='11' author='FrankRouter' date='2017-08-16T09:08:26Z'>
		集群。任务链接已经失效。需要的话，可以复现。
		</comment>
		<comment id='12' author='FrankRouter' date='2017-08-20T02:37:12Z'>
		Hi &lt;denchmark-link:https://github.com/FrankRouter&gt;@FrankRouter&lt;/denchmark-link&gt;
 ，方便的话把可以把Job链接发到我的Hi上么（yanxu05)
		</comment>
		<comment id='13' author='FrankRouter' date='2017-10-26T08:18:16Z'>
		求问这个问题解决了吗？应该怎么设置sparse update
		</comment>
		<comment id='14' author='FrankRouter' date='2018-03-20T03:26:24Z'>
		根据Hi群里提供的复现链接来看，sparse update是生效的，需要在训练日志中区分实际training和test的时间。
我先关闭这个issue了，如果有进一步的问题可以再打开它。
		</comment>
	</comments>
</bug>