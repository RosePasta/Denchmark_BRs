<bug id='9293' author='raincoastchris' open_date='2019-05-07T03:05:51Z' closed_time='2021-01-05T08:03:59Z'>
	<summary>Error loading ONNX models without named nodes</summary>
	<description>
It appears that Vespa assumes that ONNX compute nodes are explicitly named the same as their output tensors.  This is not necessarily true. A fork of the basic-search-tensor sample app that exhibits the issue is here:
&lt;denchmark-link:https://github.com/raincoastchris/sample-apps/tree/onnx_issues&gt;https://github.com/raincoastchris/sample-apps/tree/onnx_issues&lt;/denchmark-link&gt;

Attempting to deploy leads to:
&lt;denchmark-code&gt;{"error-code":"INVALID_APPLICATION_PACKAGE","message":"Invalid application package: testapp.default: Error loading model: Node 'output' not found in ONNX graph"}
&lt;/denchmark-code&gt;

Uncommenting the commented lines in generate_onnx.py and regenerating the ONNX file avoids the issue.
Note that the ONNX model in the above branch has other issues that prevent it from working for its intended application, which was to define a tensor multiplication scorer via ONNX model rather than explicitly in the search definition. It seems this isn't possible using MatMul, because Vespa does not support the Transpose operator.
	</description>
	<comments>
		<comment id='1' author='raincoastchris' date='2021-01-05T08:03:59Z'>
		So, some time ago we switched to using ONNX Runtime to evaluate ONNX models during ranking. See &lt;denchmark-link:https://blog.vespa.ai/stateful-model-serving-how-we-accelerate-inference-using-onnx-runtime/&gt;https://blog.vespa.ai/stateful-model-serving-how-we-accelerate-inference-using-onnx-runtime/&lt;/denchmark-link&gt;

With this, we expect ONNX deploy issues to be resolved. Closing this ticket (as it probably has timed out quite some time ago), please reopen or file a new ticket if problems arise.
		</comment>
	</comments>
</bug>