<bug id='5313' author='vekterli' open_date='2018-03-13T13:09:44Z' closed_time='2018-03-13T16:41:57Z'>
	<summary>Edge case preventing replica convergence with redundancy level &amp;gt; 16</summary>
	<description>
There exists a latent edge case in the distributor replica merge logic that may manifest itself as follows:

System has buckets with too few replicas present
System is constantly executing merges, but the merges appear to have no effect

From debug logging on the merge operation component (distributor.operation.idealstate.merge), an excerpt such as the following may be observed:
&lt;denchmark-code&gt;DEBUG   : distributor      vds.distributor.operation.idealstate.merge	storage/distributor/operations/idealstate/mergeoperation.cpp:153 Sending MergeBucketCommand(BucketId(0x4000000000001f59), to time 1520935976000024, cluster state version: 85, nodes: [25, 22, 21, 6, 13, 27, 24, 16, 8, 7, 26, 17, 14, 29, 11, 20], chain: [], reasons to start: ) to storage node 6
DEBUG   : distributor      vds.distributor.operation.idealstate.merge	storage/distributor/operations/idealstate/mergeoperation.cpp:271 Merge operation for bucket BucketId(0x4000000000001f59) finished
&lt;/denchmark-code&gt;

Inspecting that bucket we get:
&lt;denchmark-code&gt;vespa-stat --bucket 0x4000000000001f59
Bucket maps to the following actual files:
	BucketInfo(BucketId(0x4000000000001f59): [distributor:9] merge: [Adding missing node 9][Adding missing node 12][Adding missing node 18]
[Adding missing node 23][Adding missing node 15][Adding missing node 10][Adding missing node 28][Adding missing node 19], garbagecollection: [Needs garbage collection: Last check at 1520894467, current time 1520936062, configured interval 3600]
 [node(idx=20,crc=0x0,docs=0/3,bytes=0/165,trusted=true,active=true,ready=true), node(idx=11,crc=0x0,docs=0/3,bytes=0/165,trusted=true,active=true,ready=true), 
node(idx=29,crc=0x0,docs=0/3,bytes=0/165,trusted=true,active=true,ready=true), node(idx=14,crc=0x0,docs=0/3,bytes=0/165,trusted=true,active=true,ready=true), 
node(idx=17,crc=0x0,docs=0/3,bytes=0/165,trusted=true,active=true,ready=true), node(idx=26,crc=0x0,docs=0/3,bytes=0/165,trusted=true,active=true,ready=true), 
node(idx=7,crc=0x0,docs=0/3,bytes=0/165,trusted=true,active=true,ready=true), node(idx=8,crc=0x0,docs=0/3,bytes=0/165,trusted=true,active=true,ready=true), 
node(idx=16,crc=0x0,docs=0/3,bytes=0/165,trusted=true,active=true,ready=true), node(idx=24,crc=0x0,docs=0/3,bytes=0/165,trusted=true,active=true,ready=true), 
node(idx=27,crc=0x0,docs=0/3,bytes=0/165,trusted=true,active=true,ready=true), node(idx=13,crc=0x0,docs=0/3,bytes=0/165,trusted=true,active=true,ready=true), 
node(idx=6,crc=0x0,docs=0/3,bytes=0/165,trusted=true,active=true,ready=true), node(idx=21,crc=0x0,docs=0/3,bytes=0/165,trusted=true,active=true,ready=true), 
node(idx=22,crc=0x0,docs=0/3,bytes=0/165,trusted=true,active=true,ready=true), node(idx=25,crc=0x0,docs=0/3,bytes=0/165,trusted=true,active=true,ready=true)])
&lt;/denchmark-code&gt;

What we can observe from this is that the set of nodes that are listed as missing and the set of nodes that the merge is sent for have an empty intersection set, i.e. the merge will not actually do anything to restore redundancy.
Merges are limited by the current wire format to only support 16 nodes as part of a single merge operation, so there is a notion of "merge limiting" in the distributor. Normally, this logic will attempt to include nodes from a maximally diverse set of checksums to exchange as many different documents/document versions as possible. This process will be repeated until all replicas have converged. From the above bucket metadata dump, we see that all checksums are zero due to the bucket containing no visible documents, only remove entries. Unfortunately, missing replicas also receive an implicit checksum of zero in this context. The merge limiter logic does not account for this edge case, so it cannot distinguish between the two.
This edge can happen if the following conditions are met:

The cluster has a replication level (redundancy) &gt; 16
A significant amount of data has been removed, causing buckets to contain only metadata entries

Some possible changes to the merge limiting logic:

Ensure that missing replicas and replicas with no visible documents are treated as different checksum groups, ensuring that we pick from both groups during the limiting process
If only a single checksum group is present, replace as many of its members with missing replica nodes as possible (down to a minimum of 1 remaining node in the checksum group).
If a merge is of maximum size (16), ensure that at least 1 missing replica node is present as part of the node set.

	</description>
	<comments>
	</comments>
</bug>