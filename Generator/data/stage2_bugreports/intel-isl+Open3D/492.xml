<bug id='492' author='rnunziata' open_date='2018-08-04T21:42:19Z' closed_time='2018-09-28T06:13:06Z'>
	<summary>register_fragments memory error</summary>
	<description>
python3 register_fragments.py /home/rjn/Downloads/TestData/
looks like global_registration.json was written but not global_registration_optimization.json
&lt;denchmark-code&gt;ICP Iteration #10: Fitness 0.4935, RMSE 0.0058
Residual : 1.23e-04 (# of elements : 46034)
Update PoseGraph
PoseGraph with 26 nodes and 171 edges.
[GlobalOptimizationLM] Optimizing PoseGraph having 26 nodes and 171 edges. 
Line process weight : 2571.869648
*** Error in `python3': malloc(): memory corruption: 0x000000000198af40 ***
======= Backtrace: =========
/lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7f04cb6b57e5]
/lib/x86_64-linux-gnu/libc.so.6(+0x8213e)[0x7f04cb6c013e]
/lib/x86_64-linux-gnu/libc.so.6(__libc_malloc+0x54)[0x7f04cb6c2184]
/home/rjn/.local/lib/python3.5/site-packages/open3d.cpython-35m-x86_64-linux-gnu.so(+0xab309)[0x7f04b332b309]
/home/rjn/.local/lib/python3.5/site-packages/open3d.cpython-35m-x86_64-linux-gnu.so(+0x1d3454)[0x7f04b3453454]
/home/rjn/.local/lib/python3.5/site-packages/open3d.cpython-35m-x86_64-linux-gnu.so(_ZNK5three36GlobalOptimizationLevenbergMarquardt17OptimizePoseGraphERNS_9PoseGraphERKNS_37GlobalOptimizationConvergenceCriteriaERKNS_24GlobalOptimizationOptionE+0x310)[0x7f04b34568d0]
/home/rjn/.local/lib/python3.5/site-packages/open3d.cpython-35m-x86_64-linux-gnu.so(_ZN5three18GlobalOptimizationERNS_9PoseGraphERKNS_24GlobalOptimizationMethodERKNS_37GlobalOptimizationConvergenceCriteriaERKNS_24GlobalOptimizationOptionE+0x8e)[0x7f04b345415e]
/home/rjn/.local/lib/python3.5/site-packages/open3d.cpython-35m-x86_64-linux-gnu.so(+0x99f7c)[0x7f04b3319f7c]
/home/rjn/.local/lib/python3.5/site-packages/open3d.cpython-35m-x86_64-linux-gnu.so(+0xc50b8)[0x7f04b33450b8]
python3(PyCFunction_Call+0x77)[0x4e9ba7]
python3(PyEval_EvalFrameEx+0x614)[0x5372f4]
python3[0x540199]
python3(PyEval_EvalFrameEx+0x54f0)[0x53c1d0]
python3(PyEval_EvalFrameEx+0x4b04)[0x53b7e4]
python3[0x540199]
python3(PyEval_EvalCode+0x1f)[0x540e4f]
python3[0x60c272]
python3(PyRun_FileExFlags+0x9a)[0x60e71a]
python3(PyRun_SimpleFileExFlags+0x1bc)[0x60ef0c]
python3(Py_Main+0x456)[0x63fb26]
python3(main+0xe1)[0x4cfeb1]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0)[0x7f04cb65e830]
python3(_start+0x29)[0x5d6049]

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='rnunziata' date='2018-08-04T21:50:43Z'>
		Could you share more information to reproduce this error? For example, environment or json files.
		</comment>
		<comment id='2' author='rnunziata' date='2018-08-05T15:29:59Z'>
		I ran it clean in docker ...and get the same error.
My system :
cat /etc/*-release
&lt;denchmark-code&gt;DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=16.04
DISTRIB_CODENAME=xenial
DISTRIB_DESCRIPTION="Ubuntu 16.04.5 LTS"
NAME="Ubuntu"
VERSION="16.04.5 LTS (Xenial Xerus)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 16.04.5 LTS"
VERSION_ID="16.04"
HOME_URL="http://www.ubuntu.com/"
SUPPORT_URL="http://help.ubuntu.com/"
BUG_REPORT_URL="http://bugs.launchpad.net/ubuntu/"
VERSION_CODENAME=xenial
UBUNTU_CODENAME=xenial

&lt;/denchmark-code&gt;

using i7 with nivdia gtx1060.
The docker image I used was the docker with nvidia runtime
Let me know if I can supply other data.
The data I used is as follows:
From the Redwood dataset livingroom1...images and depth and camera parameters for the data.
I placed them in my own TestData folder I placed in root of Open3D
&lt;denchmark-code&gt;TestData:
  depth
  images
  camera.json

ran:
python3 register_fragments.py /Open3D/TestData
   created fragments
python3 make_fragments.py /Open3D/TestData -path_intrinsic /Open3D/TestData/camera.json
   created global_registration.json
   then get memory error.

&lt;/denchmark-code&gt;

Please note that the fragments folder also contains....all optimized json files up and including the last expected file with matching ply fragments.
fragment_optimized_028.json
I can run other tutorial pgms successfully.
		</comment>
		<comment id='3' author='rnunziata' date='2018-08-07T05:30:58Z'>
		Thanks. Could you also share json file? Let me check on that. Please also check the available memory space in docker environment.
		</comment>
		<comment id='4' author='rnunziata' date='2018-08-07T16:08:22Z'>
		I only ran on docker to verify that it was not my installation....this error happens on my machine without docker. I also ran this successfully by using  only the first 63 image and depth files and the whole pipe line worked it only fails if run for the whole sequence. Here is a link to my TestData folder will all the relevant files.
&lt;denchmark-link:https://www.dropbox.com/s/s5ta62bsocpfaw1/TestData.tar.gz?dl=0%5B%5D(url)&gt;https://www.dropbox.com/s/s5ta62bsocpfaw1/TestData.tar.gz?dl=0[](url)&lt;/denchmark-link&gt;

I have been able to run this if I remove all fragments after 14.
However the resulting ply mesh is corrupt as displayed in meshlab.
This link will be left up tile the end of Aug.
		</comment>
		<comment id='5' author='rnunziata' date='2018-08-08T13:14:05Z'>
		I also tried to run colored_pointcloud_registration.py from the Advanced tutorial on fragments 5 and 6....the results are not good. The fragments look fine independently. Not sure if this is related or should be a different issue.
		</comment>
		<comment id='6' author='rnunziata' date='2018-08-17T05:44:29Z'>
		Hi, I can confirm, on my own dataset I see the same error on Ubuntu 16.04. Probably because of bad fragments alignment. Any ideas how to fix this? Maybe you can hint where to look in the code so I can debug and fix it? I am new to Open3D.
Error occurs in
def global_optimization(pose_graph, method, criteria, option)
		</comment>
		<comment id='7' author='rnunziata' date='2018-08-17T06:06:23Z'>
		Oh, I had focused on other things, so I missed several feedbacks. Let me check.
		</comment>
		<comment id='8' author='rnunziata' date='2018-08-18T14:24:06Z'>
		&lt;denchmark-link:https://github.com/syncle&gt;@syncle&lt;/denchmark-link&gt;
 thanks for answer, you can verify this memory error on the following data set:
&lt;denchmark-link:http://redwood-data.org/indoor/dataset.html&gt;http://redwood-data.org/indoor/dataset.html&lt;/denchmark-link&gt;

Living Room 1
Noisy Depth Sequence
The error occurs during global optimization in register_fragments.py
		</comment>
		<comment id='9' author='rnunziata' date='2018-08-18T15:54:57Z'>
		Thanks &lt;denchmark-link:https://github.com/rnunziata&gt;@rnunziata&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/urock&gt;@urock&lt;/denchmark-link&gt;
. As &lt;denchmark-link:https://github.com/rnunziata&gt;@rnunziata&lt;/denchmark-link&gt;
 stated, this is due to the failure of matching adjacent fragments. This will make zero entry in the Jacobian matrix, and results in null point error. The fix is to modify register_fragment.py as
&lt;denchmark-code&gt;    for s in range(n_files):
        for t in range(s + 1, n_files):
            print("reading %s ..." % ply_file_names[s])
            source = read_point_cloud(ply_file_names[s])
            print("reading %s ..." % ply_file_names[t])
            target = read_point_cloud(ply_file_names[t])
            (source_down, source_fpfh) = preprocess_point_cloud(source)
            (target_down, target_fpfh) = preprocess_point_cloud(target)
            if t == s + 1:
                (_, transformation_icp, information_icp) = \
                        local_refinement(s, t, source, target,
                        np.identity(4), draw_result)
            else:
                (success_global, transformation_init) = \
                        compute_initial_registration(
                        s, t, source_down, target_down,
                        source_fpfh, target_fpfh, path_dataset, draw_result)
                if not success_global:
                    continue
                (success_local, transformation_icp, information_icp) = \
                        local_refinement(s, t, source, target,
                        transformation_init, draw_result)
                if not success_local:
                    continue
            (odometry, pose_graph) = update_odometry_posegrph(s, t,
                    transformation_icp, information_icp,
                    odometry, pose_graph)
            print(pose_graph)
&lt;/denchmark-code&gt;

I fixed the issue but the result is not very beautiful yet. I am looking for another reason.
		</comment>
		<comment id='10' author='rnunziata' date='2018-08-19T14:45:33Z'>
		&lt;denchmark-link:https://github.com/syncle&gt;@syncle&lt;/denchmark-link&gt;
 although with your fix memory error is gone, but now it breaks 3D reconstruction even on clean noiseless Redwood Living data set..
		</comment>
		<comment id='11' author='rnunziata' date='2018-08-19T16:26:41Z'>
		can you also comment on  colored_pointcloud_registration.py using ply fragment files....I can validate &lt;denchmark-link:https://github.com/urock&gt;@urock&lt;/denchmark-link&gt;
 findings....memory error gone...but the intergrated.ply produced is still broken.
		</comment>
		<comment id='12' author='rnunziata' date='2018-08-20T17:11:48Z'>
		As a follow-up, I have fixed several issues and could get a fairly reasonable result for the dataset requested by &lt;denchmark-link:https://github.com/rnunziata&gt;@rnunziata&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/18297113/44355247-f6d88200-a460-11e8-8470-b7f7ea0a1be7.png&gt;&lt;/denchmark-link&gt;

I am working with &lt;denchmark-link:https://github.com/syncle/open3d/tree/fix_register_fragments&gt;this branch&lt;/denchmark-link&gt;
 and polishing my fixes. I will submit PR quite soon.
		</comment>
		<comment id='13' author='rnunziata' date='2018-08-20T18:06:55Z'>
		Thanks I guess we can close this...I will let you do that just in case you want to leave it open until the release. If  you want me test out the fixes when you are done ... let me know I will do so. I did try to run with your current changes following this fix link...but it did not work against TestData folder.
		</comment>
		<comment id='14' author='rnunziata' date='2018-08-22T19:50:21Z'>
		Hi &lt;denchmark-link:https://github.com/syncle&gt;@syncle&lt;/denchmark-link&gt;
! I have tried your &lt;denchmark-link:https://github.com/syncle/open3d/tree/fix_register_fragments&gt;fix branch&lt;/denchmark-link&gt;
. Clean sequence of Redwood Living Room data set is reconstructed ok, but I am still having troubles with noisy data set - room geometry is broken. I have even tried using ground truth camera trajectory instead of computing opencv pose estimation in make_fragments.py, but without visible success.
Can you confirm that it is theoretically possible? What camera intrinsics should be used for noisy sequence?
I am also trying to achieve good reconstruction result with Intel Realsense D435 camera and am facing the same issue as with noisy living room data set...
		</comment>
		<comment id='15' author='rnunziata' date='2018-08-23T17:10:55Z'>
		I wanted to test the fix code on my system...I checkouted the fix and built it but when I try to run I get the following. The build did not fail. Any ideas?
&lt;denchmark-code&gt;python make_fragments.py ~/Downloads/TestData/ ~/Downloads/TestData/camera.json 
Traceback (most recent call last):
  File "make_fragments.py", line 10, in &lt;module&gt;
    from open3d import *
ImportError: No module named open3d
&lt;/denchmark-code&gt;

		</comment>
		<comment id='16' author='rnunziata' date='2018-08-23T17:47:35Z'>
		Did you do make install?
It was commented in build.sh...
		</comment>
		<comment id='17' author='rnunziata' date='2018-08-23T18:41:33Z'>
		I reinstalled the Open3D master and found I have to run it with python3 which I must have done at one time. That works...but if I try it with the new fragment fix I now get:
OpenCV is not detected. Using Identity as an initial
which I no not get under the master version.
OK....I got it working.....not sure I understand why I can not call make_fragment directly now....but I got it working with run_system...thanks
		</comment>
		<comment id='18' author='rnunziata' date='2018-08-23T22:58:27Z'>
		I ran run_system.py.
The  system no longer seems to create: fragment_optimized_024.json  files. It runs if I leave the old json files in the fragments folder but not good results. If I leave the files out it fails with file error unable to open file. If I put empty files in it complains there are 0 vertices.
What is your set up and parameters...what data are you passing.
Also colored_pointcloud_registration.py is giving me odd results  if I jsut run it against ply 15,16
&lt;denchmark-link:https://user-images.githubusercontent.com/5377074/44556176-82267180-a706-11e8-8e4b-7aeb421c6349.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='19' author='rnunziata' date='2018-08-24T00:56:44Z'>
		
Please use the PrimeSenseDefault parameters

&lt;denchmark-code&gt;{
	"width" : 640,
	"height" : 480,
	"intrinsic_matrix" :
	[
		525.0,
		0,
		0,
		0,
		525.0,
		0,
		319.5,
		239.5,
		1
	]
}
&lt;/denchmark-code&gt;


valid range of Redwood dataset is 4m. You may need to increase depth clipping values from 3m to 4m.

		</comment>
		<comment id='20' author='rnunziata' date='2018-08-24T02:46:38Z'>
		that was it....thanks so much....looks good.
		</comment>
		<comment id='21' author='rnunziata' date='2018-08-24T06:59:50Z'>
		&lt;denchmark-link:https://github.com/rnunziata&gt;@rnunziata&lt;/denchmark-link&gt;
 on what data set are you testing?
Also, guys, pls help to achive good results on Readwood Living Room 1 dataset!
Currently, as I've  said, after testing &lt;denchmark-link:https://github.com/syncle&gt;@syncle&lt;/denchmark-link&gt;
 branch that fixed memory issue, I have good result on clean depth sequnece
&lt;denchmark-link:https://user-images.githubusercontent.com/1125366/44569681-dcb1e300-a783-11e8-9f5a-10b16b4e143f.png&gt;&lt;/denchmark-link&gt;

But on noisy depth sequence it is not that good
&lt;denchmark-link:https://user-images.githubusercontent.com/1125366/44569739-0539dd00-a784-11e8-89f3-aa62da9c11be.png&gt;&lt;/denchmark-link&gt;

I use PrimeSenseDefault camera parameters with max_depth = 3m.
Can you pls check reconstruction of noisy sequence and tell what params are you using?
Thanx!
		</comment>
		<comment id='22' author='rnunziata' date='2018-08-24T15:20:28Z'>
		I am using living room 1. Yes I get a corrupted final image ...also I noticed that several of the fragments plys are too I think this is were things start to go wrong ..check out number 27.. I am wondering apply a different noise algorithm.....I have not looked at the depth images directly to see if there is something off about the noise but it would be nice to rule it  out before looking elsewhere .
Here is a little pgm I wrote to display the depth images noise and no noise side by side...there is some oddity...not sure..have a look.
&lt;denchmark-code&gt;import numpy as np
import cv2
import math
import sys
import time
 
if __name__ == "__main__":
  for fragment_id in range(2600,2800):
    filepathn = "/home/rjn/Downloads/TestData/depth/"+ str(fragment_id).rjust(5, '0')+".png"  
    cv_imagen = cv2.imread(filepathn)
    cv_image_arrayn = np.array(cv_imagen, dtype = np.dtype('f8'))
    cv_image_normn = cv2.normalize(cv_image_arrayn, cv_image_arrayn, 0, 1, cv2.NORM_MINMAX)
    cv2.imshow("xxxxxx", cv_image_normn)
    
  
    filepathn = "/home/rjn/Downloads/TestData/xxxdepth/"+ str(fragment_id).rjust(5, '0')+".png"  
    cv_imagen = cv2.imread(filepathn)
    cv_image_arrayn = np.array(cv_imagen, dtype = np.dtype('f8'))
    cv_image_normn = cv2.normalize(cv_image_arrayn, cv_image_arrayn, 0, 1, cv2.NORM_MINMAX)
    cv2.imshow("yyyyyy", cv_image_normn)
    
    
    cv2.waitKey(1)
    time.sleep(.2)
  exit(0)
&lt;/denchmark-code&gt;

It looks like some of the nosie depth images are cropped , some additonal erros around the frame edge, intensities off globally and locally. I am not sure if this nose is a good model for the realsence camera....do they have there own noise model?
		</comment>
		<comment id='23' author='rnunziata' date='2018-08-27T00:45:29Z'>
		Umm that is interesting. &lt;denchmark-link:https://github.com/qianyizh&gt;@qianyizh&lt;/denchmark-link&gt;
 Could you comment on &lt;denchmark-link:https://github.com/rnunziata&gt;@rnunziata&lt;/denchmark-link&gt;
's questions?
		</comment>
		<comment id='24' author='rnunziata' date='2018-08-28T05:29:35Z'>
		&lt;denchmark-link:https://github.com/rnunziata&gt;@rnunziata&lt;/denchmark-link&gt;
 can you post some figures?
The noise pattern should be pretty standard. It is a low frequency pattern (corrupt depth by ~3%) plus a white noise, then discretized to mimic the scan line pattern. There should be no dramatic changes.
		</comment>
		<comment id='25' author='rnunziata' date='2018-09-02T17:10:12Z'>
		Here is a vid of the depth comparisons.....note the main issue is the cropping at the boarder.
&lt;denchmark-link:https://www.dropbox.com/s/r203rifon2h2rmr/out.ogv?dl=0&gt;https://www.dropbox.com/s/r203rifon2h2rmr/out.ogv?dl=0&lt;/denchmark-link&gt;

Here is a png from the vid that shows some sever cropping and odd intensity issues
&lt;denchmark-link:https://user-images.githubusercontent.com/5377074/44960747-e8637f00-aed2-11e8-9587-465edadc9d62.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='26' author='rnunziata' date='2018-09-07T07:46:19Z'>
		Yeah, this looks wrong. I think some parameter is set wrong. Hence the strong artifacts. Can you post a minimal code snippet and an example file so I can look into it?
		</comment>
		<comment id='27' author='rnunziata' date='2018-09-07T19:38:50Z'>
		The code I used is given in my prior post.  I downloaded both the normal depth and noisy depth and displayed  them side by side which is what the code does.....A link to a vid showing the comparison is given in my most recent post.   I used LivingRoom1 for the test....
http://redwood-data.org/indoor/dataset.html
Side note: I think this is a very poor depth model...it does not include co-variance noise between the image and depth, nor does it consider temporal noise just spatial and then just for I guess a defective depth sensor.
It also seems to me this thread has moved beyond the initial question.
		</comment>
	</comments>
</bug>