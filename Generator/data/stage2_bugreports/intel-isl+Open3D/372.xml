<bug id='372' author='channingxiao' open_date='2018-05-24T02:22:05Z' closed_time='2019-09-16T15:08:10Z'>
	<summary>GLFW Error: GLX: No GLXFBConfigs returned when using Open3D together with tensorflow</summary>
	<description>
Hello, I was using Open3D together with tensorflow with GPU, and I to "GLFW Error: GLX: No GLXFBConfigs" when using draw_geometry([pc]) to show point cloud.
The point cloud can be showed normally before CNN model loaded, and the error was shown in the case where using draw_geometry after a CNN model was loaded.
	</description>
	<comments>
		<comment id='1' author='channingxiao' date='2018-05-24T02:56:04Z'>
		I found that If I used draw_geometries() before loading CNN models with tensorflow. Then everything is going to be normal, I can use draw_geometries() even after loading the model.
		</comment>
		<comment id='2' author='channingxiao' date='2018-05-24T05:28:24Z'>
		This is interesting. My guess about the failure case is that tensorflow holds all graphical resources, and Open3D's GLFW is failed to retrieve any GPU resource. If draw_geometry called before tensorflow, open3d declares some of GPU resource globally, and tensorflow does not take it. This guess is from the fact that tensorflow usually takes all GPU memories by default.
		</comment>
		<comment id='3' author='channingxiao' date='2018-12-17T19:44:52Z'>
		&lt;denchmark-link:https://github.com/yxlao&gt;@yxlao&lt;/denchmark-link&gt;
: I guess you have experienced similar issue while you worked with open3d-pointnet-xx. Could you take a look this issue?
		</comment>
		<comment id='4' author='channingxiao' date='2018-12-19T22:38:50Z'>
		I am unable to recreate the issue by the following steps:

Setting config.gpu_options.allow_growth = False
Load and run a TF model (saver.restore and then sess.run)

At this point, it's true that almost all GPU memory is used since TF reserves most GPU memory


Read a large point cloud (3e8 points with rgb color) and call draw_geometries afterwards

This still works in my case
Theorigicaly, a point with x, y, z, r, g, b each in GL_FLOAT takes 6 * 4 = 24 bytes, thus in total, it shall consume 3 * (10 ^ 8) * 24 = 7.2GBytes if all points and colors are copied to GPU memory. However, draw_geometries still works, indicating that not all points and colors are copied to GPU memory at once
If I run the draw_geometries independently, the GPU memory consuption is 8.945G (loaded) - 1.404G (idle) = 7.541 G, which matches the theoretical size. My guess is that when GPU memoy is sufficient, the points' info will all be cached on GPU memory



&lt;denchmark-link:https://github.com/channingxiao&gt;@channingxiao&lt;/denchmark-link&gt;
 could you provied a minimal working example to reproduce the issue?
		</comment>
		<comment id='5' author='channingxiao' date='2019-09-16T15:08:10Z'>
		Stall.
		</comment>
		<comment id='6' author='channingxiao' date='2020-09-05T18:03:46Z'>
		I have the same issue:
&lt;denchmark-code&gt;INFO: Reference thread starting
QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Open3D ERROR] GLFW Error: GLX: No GLXFBConfigs returned
Subprocess failed (exit code: 134)

&lt;/denchmark-code&gt;

Is there a way to load the model before successfully calling draw_geometries? Not using tensorflow but pytorch.
		</comment>
	</comments>
</bug>