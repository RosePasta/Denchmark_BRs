<bug id='210' author='kocmitom' open_date='2016-12-20T11:37:30Z' closed_time='2017-01-09T11:58:24Z'>
	<summary>Memory bug in validation</summary>
	<description>
I have conducted several experiments to find out why the training is always killed after some time. I have found out that at the beginning of each validation something is loaded into the memory (I am using lazy mode on val_dataset) and then it is not released or reused.
When I disable validation, this problem do not occur (or maybe i didn't run the experiment for enough time to ocur).
I have tried to find the source of the problem but I do not see anything in the code which should stay in memory indefinitely
	</description>
	<comments>
		<comment id='1' author='kocmitom' date='2016-12-21T20:45:08Z'>
		Does it happen even if you don't use LazyDataset for the validation. My guess is it is because the validation file remains open after printing the examples in the log (because the file was not read until the end).
As a simple solution, I would just ban using lazy dataset for validation data. It makes sense to have them in memory because they are used often (unlike the training data).
		</comment>
		<comment id='2' author='kocmitom' date='2016-12-21T20:49:59Z'>
		The proper solution would be: readers will use file handles instead of file names and a lazy dataset instance will keep a dictionary of the handles and every time you want to start reading the series, the dataset will check whether the series' file handle isn't open. We were also taking about asynchronous validation. In that case, we would need to keep the file-handle table for each thread separately.
		</comment>
		<comment id='3' author='kocmitom' date='2016-12-22T14:28:33Z'>
		Yes, it does happen even when using Lazy mode. And I would guess it happens even faster. And also I am talking about 500MB of allocated memory per validation (when not using Lazy ... note that the validation set has onlu 200kb) so it will not be anything simple as forgotten pointer.
		</comment>
		<comment id='4' author='kocmitom' date='2016-12-22T14:31:43Z'>
		It sounds to me like a problem with CPU memory allocation in tensorflow.. Can we profile the code using something like this: &lt;denchmark-link:https://pypi.python.org/pypi/memory_profiler&gt;https://pypi.python.org/pypi/memory_profiler&lt;/denchmark-link&gt;
 ?
		</comment>
		<comment id='5' author='kocmitom' date='2017-01-09T11:58:24Z'>
		Coudn't reproduce.
		</comment>
		<comment id='6' author='kocmitom' date='2017-01-16T11:38:23Z'>
		I checked the problem with monster decoder and it didn't appear.
		</comment>
		<comment id='7' author='kocmitom' date='2017-01-16T11:43:13Z'>
		There's no monster decoder in master for a week now - did you mean this master or you did not update the newest changes?
		</comment>
		<comment id='8' author='kocmitom' date='2017-01-16T15:11:14Z'>
		I know there is no monster decoder in master :) ... I only wanted to specify, that I run it with the new version of decoder (master branch from last tuesday)
		</comment>
	</comments>
</bug>