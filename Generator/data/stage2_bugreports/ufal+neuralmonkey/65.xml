<bug id='65' author='tomasmcz' open_date='2016-07-12T10:31:39Z' closed_time='2016-08-30T14:59:43Z'>
	<summary>bleu_ref.py returns 0 when wrapper fails</summary>
	<description>
Why does the call to BLEUReferenceImplWrapper return 0.0 when receiving incorrect output from the wrapper? Shouldn't it throw an error?
Also, why does it print and not log?
	</description>
	<comments>
		<comment id='1' author='tomasmcz' date='2016-07-12T11:48:49Z'>
		I made this decision because the evaluation used to crash on suspicious data. But we don't want to crash when the mteval script crashes, because it happens rarely and the rest of the evaluations still yield reasonable numbers. The evaluation itself should never interrupt the training process - the numbers that come out of it do not affect the training so it should not matter if they are sometimes broken.
You're welcome to fix the prints.
		</comment>
		<comment id='2' author='tomasmcz' date='2016-07-12T12:04:45Z'>
		I think I would like to crash when the evaluation crashes. Or at least read in the log that something strange happened.
		</comment>
		<comment id='3' author='tomasmcz' date='2016-07-12T12:09:23Z'>
		Btw. if there was code review or at least a pull request, this would have been revealed and discussed before we found that in certain conditions (e.g. on Travis), bleu_ref doesn't work at all (but without failing), so this (another layer of) error-hiding behavior really isn't ideal.
		</comment>
		<comment id='4' author='tomasmcz' date='2016-07-12T12:13:02Z'>
		But that is the reason why there are the tests, in the first place.. They check for this error and expose it.
		</comment>
		<comment id='5' author='tomasmcz' date='2016-07-12T12:15:01Z'>
		It is not good when you run the experiment for a weekend, and when you get back, you learn that it crashed after two hours on an error that had no chance to influence the training itself.
		</comment>
		<comment id='6' author='tomasmcz' date='2016-07-12T12:16:00Z'>
		During training, every error that can be handled, should be handled. Crashes during training should be avoided at all costs.
		</comment>
		<comment id='7' author='tomasmcz' date='2016-07-12T12:18:38Z'>
		Lastly, there  a pull request where you can adress this problem ( &lt;denchmark-link:https://github.com/ufal/neuralmonkey/pull/63&gt;#63&lt;/denchmark-link&gt;
 ).
		</comment>
		<comment id='8' author='tomasmcz' date='2016-07-12T12:30:37Z'>
		Test cannot catch certain errors, for example when the error is that you are masking errors, like here. Tests also don't catch unreadable code and other things that would be easier solved before adding that code to the codebase, not half a year later when they cause real problems.
I disagree. When you sustain the training at all costs, you might end up with weekend of useless, incorrect data and don't even know about it. Whenever there is an error, the program should fail. If you fear that your over-the-weekend experiment will fail, test your configuration on smaller data beforehand.
There was no pull request for , it was commited and pushed directly to master (&lt;denchmark-link:https://github.com/ufal/neuralmonkey/commit/af9e96f29f06b1bf352075bb0ea499a6fbfee5ba&gt;af9e96f&lt;/denchmark-link&gt;
). The pull request you are referencing has nothing to do with this issue, except for my tangential note about  not working on Travis, which as far as I can tell, is something in the Perl code, in no direct connection to the problems I have with .
		</comment>
		<comment id='9' author='tomasmcz' date='2016-07-12T12:41:50Z'>
		As if models running with bugs without failing already wasn't one of the biggest challenges in machine learning...
		</comment>
		<comment id='10' author='tomasmcz' date='2016-07-12T12:54:46Z'>
		Yeah, except the bleu computation is not a part of the model.
		</comment>
		<comment id='11' author='tomasmcz' date='2016-07-12T13:01:56Z'>
		Well, it might be one day (we talked about training to BLEU) and I will not remember it forever that this module might behave strange without failing. And even when it isn't, I don't want to be wondering whether an evaluation module encountered an error without failing or my model is really that bad when all the scores are zero.
		</comment>
		<comment id='12' author='tomasmcz' date='2016-07-12T13:07:51Z'>
		As I wrote earlier, the wrapper used to crash on suspicious circumstances. Now, it works. The suspicious circumstances are subject to the unit test. The exception handling serves but as a remnant of those cruel times. Do what you want, but do it already.
		</comment>
		<comment id='13' author='tomasmcz' date='2016-07-12T13:19:40Z'>
		Now it "works" (&lt;denchmark-link:https://github.com/ufal/neuralmonkey/issues/62&gt;#62&lt;/denchmark-link&gt;
) so there is no need to mask its errors so we should just re-raise them instead of returning 0. I don't have time for that now, so either fix it (it's your code after all) or leave this issue open until I get to it.
		</comment>
		<comment id='14' author='tomasmcz' date='2016-08-30T14:59:43Z'>
		wont fix, bleu_ref is deprecated by 8d5112
		</comment>
	</comments>
</bug>