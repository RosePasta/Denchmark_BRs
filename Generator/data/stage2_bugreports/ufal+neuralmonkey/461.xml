<bug id='461' author='jindrahelcl' open_date='2017-06-05T14:27:43Z' closed_time='2017-09-21T11:30:28Z'>
	<summary>Using BaseAttention crashes somewhere else than it should</summary>
	<description>
When using BaseAttention by mistake (e.g. using imagenet encoder without specified attention type and the recurrent decoder with attention), the program crashes on unexpected keyword argument 'input_weights'.
It should either crash on the NotImplementedError or in the decoder that would check if there is attention defined on that object.
This issue is probably a wontfix, because of the upcoming refactor, but I'd keep it until the refactor is done or this issue is resolved.
	</description>
	<comments>
		<comment id='1' author='jindrahelcl' date='2017-06-05T14:29:42Z'>
		Will get solved by implementing &lt;denchmark-link:https://github.com/ufal/neuralmonkey/issues/454&gt;#454&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='2' author='jindrahelcl' date='2017-09-21T11:30:27Z'>
		Will get resolved after merging &lt;denchmark-link:https://github.com/ufal/neuralmonkey/pull/502&gt;#502&lt;/denchmark-link&gt;
.
		</comment>
	</comments>
</bug>