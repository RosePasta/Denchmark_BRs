<bug id='582' author='wonkeelee' open_date='2017-11-08T15:29:10Z' closed_time='2017-11-09T12:39:04Z'>
	<summary>vocabulary problem with max_size in .ini file</summary>
	<description>
I tried to create a post-editing model by referring to the tutorial.
As you said before, the tutorial was an old version, so I made 'post-edit.ini' referring to 'vocab.ini' and 'small.ini' as you commented before. (&lt;denchmark-link:https://github.com/ufal/neuralmonkey/issues/579&gt;#579&lt;/denchmark-link&gt;
)
but there were some problems.
If I define the max_size of the [vocabulary] section as 50000, there was a problem that the vocabulary could not be created normally (created with ['&lt;/s&gt;', '&lt;pad&gt;', '&lt;unk&gt;', '&lt;s&gt;', '&lt;pad&gt;']only),
but there was no problem if max_size was reduced to 3000.
Furthermore, I wonder if my 'post-edit.ini' file is correct. so can you check it out?
Below are the results when the vocabulary size is 50000 and 3000 respectively.
and I also attached my post-edit.ini together.
Thank you very much for your help.

loading result for vocabulary size=50000(max_size=50000)

&lt;denchmark-code&gt;2017-11-09 00:19:28: Loading INI file: 'exp-nm-ape/post-edit_2.ini'
2017-11-09 00:19:28: INI file is parsed.
2017-11-09 00:19:28: Directory with experiment.ini 'exp-nm-ape/training' exists, overwriting enabled, proceeding.
2017-11-09 00:19:28: Building model based on the config.
2017-11-09 00:19:28: Initializing dataset with: translated, edits, source
2017-11-09 00:19:28: Dataset length: 12000
2017-11-09 00:19:28: Vocabulary for series ['edits', 'translated'] initialized, containing 4 words
2017-11-09 00:19:28: Sample of the vocabulary: ['&lt;/s&gt;', '&lt;pad&gt;', '&lt;unk&gt;', '&lt;s&gt;', '&lt;pad&gt;']
2017-11-09 00:19:28: Vocabulary for series ['source'] initialized, containing 4 words
2017-11-09 00:19:28: Sample of the vocabulary: ['&lt;/s&gt;', '&lt;unk&gt;', '&lt;unk&gt;', '&lt;unk&gt;', '&lt;s&gt;']
2017-11-09 00:19:29: Hidden features: Tensor("attention_src_encoder/Conv2D:0", shape=(?, ?, 1, 600), dtype=float32)
2017-11-09 00:19:29: Attention mask: Tensor("src_encoder_input/sequence_mask:0", shape=(?, ?), dtype=float32)
2017-11-09 00:19:29: Hidden features: Tensor("attention_trans_encoder/Conv2D:0", shape=(?, ?, 1, 600), dtype=float32)
2017-11-09 00:19:29: Attention mask: Tensor("trans_encoder_input/sequence_mask:0", shape=(?, ?), dtype=float32)
2017-11-09 00:19:29: Initializing decoder, name: 'decoder'
2017-11-09 00:19:29: Using linear projection of encoders as the initial state
2017-11-09 00:19:29: No output projection specified - using tanh projection
2017-11-09 00:19:29: Decoder initalized. Cost var: Tensor("decoder/Mean:0", shape=(), dtype=float32)
2017-11-09 00:19:29: Runtime logits tensor: Tensor("decoder/TensorArrayStack_7/TensorArrayGatherV3:0", shape=(?, ?, 4), dtype=float32)
2017-11-09 00:19:32: Initializing dataset with: translated, edits, source
2017-11-09 00:19:32: Dataset length: 1000
&lt;/denchmark-code&gt;


loading result for vocabulary size=3000(max_size=3000)

&lt;denchmark-code&gt;2017-11-09 00:03:55: Loading INI file: 'exp-nm-ape/post-edit_2.ini'
2017-11-09 00:03:55: INI file is parsed.
2017-11-09 00:03:55: Building model based on the config.
2017-11-09 00:03:55: Initializing dataset with: translated, source, edits
2017-11-09 00:03:56: Dataset length: 12000
2017-11-09 00:03:56: Vocabulary for series ['edits', 'translated'] initialized, containing 3000 words
2017-11-09 00:03:56: Sample of the vocabulary: ['rechteckigen', 'Folgenden', 'Importieren', 'enden', 'aufgef√ºhrt']
2017-11-09 00:03:56: Vocabulary for series ['source'] initialized, containing 3000 words
2017-11-09 00:03:56: Sample of the vocabulary: ['redefine', 'criteria', 'reader', 'downloading', 'Documents']
2017-11-09 00:03:56: Hidden features: Tensor("attention_src_encoder/Conv2D:0", shape=(?, ?, 1, 600), dtype=float32)
2017-11-09 00:03:56: Attention mask: Tensor("src_encoder_input/sequence_mask:0", shape=(?, ?), dtype=float32)
2017-11-09 00:03:57: Hidden features: Tensor("attention_trans_encoder/Conv2D:0", shape=(?, ?, 1, 600), dtype=float32)
2017-11-09 00:03:57: Attention mask: Tensor("trans_encoder_input/sequence_mask:0", shape=(?, ?), dtype=float32)
2017-11-09 00:03:57: Initializing decoder, name: 'decoder'
2017-11-09 00:03:57: Using linear projection of encoders as the initial state
2017-11-09 00:03:57: No output projection specified - using tanh projection
2017-11-09 00:03:57: Decoder initalized. Cost var: Tensor("decoder/Mean:0", shape=(), dtype=float32)
2017-11-09 00:03:57: Runtime logits tensor: Tensor("decoder/TensorArrayStack_7/TensorArrayGatherV3:0", shape=(?, ?, 3000), dtype=float32)
2017-11-09 00:04:00: Initializing dataset with: translated, source, edits
2017-11-09 00:04:00: Dataset length: 1000
&lt;/denchmark-code&gt;


my 'post-edit.ini' file:

&lt;denchmark-code&gt;[main]
name="post editing"
output="exp-nm-ape/training"
runners=[&lt;runner&gt;]
tf_manager=&lt;tf_manager&gt;
trainer=&lt;trainer&gt;
train_dataset=&lt;train_dataset&gt;
val_dataset=&lt;val_dataset&gt;
evaluation=[("greedy_edits", "edits", &lt;bleu&gt;), ("greedy_edits", "edits", evaluators.ter.TER)]
batch_size=128
runners_batch_size=256
epochs=100
validation_period=1000
logging_period=20
overwrite_output_dir=True

[tf_manager]
class=tf_manager.TensorFlowManager
num_threads=4
num_sessions=1
minimize_metric=True
save_n_best=3

[bleu]
class=evaluators.bleu.BLEUEvaluator
name="BLEU-4"

[train_dataset]
class=dataset.load_dataset_from_files
s_source="exp-nm-ape/data/train/train.src"
s_translated="exp-nm-ape/data/train/train.mt"
s_edits="exp-nm-ape/data/train/train.edits"

[val_dataset]
class=dataset.load_dataset_from_files
s_source="exp-nm-ape/data/dev/dev.src"
s_translated="exp-nm-ape/data/dev/dev.mt"
s_edits="exp-nm-ape/data/dev/dev.edits"

[source_vocabulary]
class=vocabulary.from_dataset
datasets=[&lt;train_dataset&gt;]
series_ids=["source"]
max_size=50000
save_file="exp-nm-ape/training/vocab/source_vocabulary.tsv"
overwrite=True

[target_vocabulary]
class=vocabulary.from_dataset
datasets=[&lt;train_dataset&gt;]
series_ids=["edits", "translated"]
max_size=50000
save_file="exp-nm-ape/training/vocab/target_vocabulary.tsv"
overwrite=True

[src_encoder]
class=encoders.recurrent.SentenceEncoder
name="src_encoder"
rnn_size=300
max_input_len=50
embedding_size=300
dropout_keep_prob=0.8
data_id="source"
vocabulary=&lt;source_vocabulary&gt;

[trans_encoder]
class=encoders.recurrent.SentenceEncoder
name="trans_encoder"
rnn_size=300
max_input_len=50
embedding_size=300
dropout_keep_prob=0.8
data_id="translated"
vocabulary=&lt;target_vocabulary&gt;

[src_attention]
class=attention.Attention
name="attention_src_encoder"
encoder=&lt;src_encoder&gt;

[trans_attention]
class=attention.Attention
name="attention_trans_encoder"
encoder=&lt;trans_encoder&gt;

[decoder]
class=decoders.Decoder
conditional_gru=True
name="decoder"
encoders=[&lt;trans_encoder&gt;, &lt;src_encoder&gt;]
attentions=[&lt;src_attention&gt;, &lt;trans_attention&gt;]
rnn_size=300
max_output_len=50
embedding_size=300
dropout_keep_prob=0.5
data_id="edits"
vocabulary=&lt;target_vocabulary&gt;

[trainer]
class=trainers.cross_entropy_trainer.CrossEntropyTrainer
decoders=[&lt;decoder&gt;]
l2_weight=1.0e-8

[runner]
class=runners.runner.GreedyRunner
decoder=&lt;decoder&gt;
output_series="greedy_edits"
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='wonkeelee' date='2017-11-08T16:13:53Z'>
		Hi,
Thanks for pointing this out. There was a bug in &lt;denchmark-link:https://github.com/ufal/neuralmonkey/blob/master/neuralmonkey/vocabulary.py#L382&gt;https://github.com/ufal/neuralmonkey/blob/master/neuralmonkey/vocabulary.py#L382&lt;/denchmark-link&gt;

Whenever a max_size argument was larger than the actual size of the vocabulary, all words were deleted from that vocabulary (except for special tokens).
I have already created a fix and PR for this issue.
		</comment>
		<comment id='2' author='wonkeelee' date='2017-11-09T12:39:04Z'>
		Fixed in &lt;denchmark-link:https://github.com/ufal/neuralmonkey/commit/9808a30da9220e04db3ae5d04c57a86d770901e7&gt;9808a30&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>