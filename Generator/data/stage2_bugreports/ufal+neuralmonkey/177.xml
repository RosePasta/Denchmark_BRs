<bug id='177' author='jindrahelcl' open_date='2016-12-05T19:05:46Z' closed_time='2016-12-06T21:21:08Z'>
	<summary>RNN Runner computes `train_logprobs` when there is no target data series</summary>
	<description>
Shouldn't it use runtime logprobs?
	</description>
	<comments>
		<comment id='1' author='jindrahelcl' date='2016-12-06T09:46:46Z'>
		To je protože tam celou dobu feeduju do těch train vstupů (aby se to netlouklo s greedy runem, když se to pustí najednou, ale nejspíš by to nevadilo), takže tam taky vylézají ty train_logprobs. Přišlo mi, že to nevadí, protože RNN runner se stejně nikdy nepustí zároveň s trainerem.
		</comment>
		<comment id='2' author='jindrahelcl' date='2016-12-06T11:12:12Z'>
		Já teď dělám refaktor dekodéru, kde train_inputs byl seznam tenzorů a stačilo nafeedovat první položku v tom seznamu  a už to fungovalo. Train_inputs jsem předělal na jeden tensor time x batch, a pokud neexistuje target series, tak by ho nikdo neměl ani feedovat a ani by se nemělo fetchovat train_cokoliv.
		</comment>
		<comment id='3' author='jindrahelcl' date='2016-12-06T11:38:32Z'>
		Tak to zkus v tom run runneru přepsat všechno "train_" na "runtime_", myslím, že by to mohlo fungovat samo od sebe.
		</comment>
		<comment id='4' author='jindrahelcl' date='2016-12-06T11:43:19Z'>
		ok, vyzkoušim
		</comment>
		<comment id='5' author='jindrahelcl' date='2016-12-06T11:53:26Z'>
		omg fecthes místo fetches.. :-) tak jak jsi to napsal to asi nepujde, protoze on ten rnn runner pro ten beam search feeduje ty train inputs (řádka 250). Misto toho by mel asi feedovat nejaky runtime inputs, ale ty to nema, protoze si je to supluje tou loop function..
		</comment>
		<comment id='6' author='jindrahelcl' date='2016-12-06T11:56:00Z'>
		Co mi přijde nejčistší varianta je dát dekodéru ještě nějakej placeholder na slovo a předchozí stav a pomocí toho vyprodukovat další slovo (nebo logprobs) a používat to pro beam search
		</comment>
		<comment id='7' author='jindrahelcl' date='2016-12-06T12:34:18Z'>
		To by šlo, ale musel by se teda úplně změnit TF Manager, aby bylo možný feedovat do každý TF session něco úplně jinýho (protože v každý session je ten předchozí stav jinej).
		</comment>
		<comment id='8' author='jindrahelcl' date='2016-12-06T12:35:39Z'>
		Ale v tý loopfunction zase muse musí vznikat ty tenzory, co by se daly feedovat. Jestli se nepletu, tak tu loop function si stejně pouštíme sami decoding function, taby bysme je tam prostě mohli posbírat.
		</comment>
		<comment id='9' author='jindrahelcl' date='2016-12-06T18:46:16Z'>
		Loop function bys na tohle právě nepoužil, loop function je od toho, aby ti dala current input, ale ty ten budeš feedovat - takže jako kdybys měl gold data a cpal to tam v train módu, ale budeš je tam cpát jedno po druhým a ten loop function bude dělat ten pythoní kód okolo.
		</comment>
		<comment id='10' author='jindrahelcl' date='2016-12-06T19:21:16Z'>
		Znáte &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/seq2seq.py&gt;dynamic_rnn_decoder&lt;/denchmark-link&gt;
? Možná to vůbec nesouvisí s tím co tu řešíte - pak se omlouvám.
		</comment>
		<comment id='11' author='jindrahelcl' date='2016-12-06T19:24:48Z'>
		Neznám, vypadá to použitelně, dík za tip.. :) Nicméně to, co tady řešíme, bude potřeba beztak vyřešit..
		</comment>
		<comment id='12' author='jindrahelcl' date='2016-12-06T20:48:11Z'>
		Změnil jsem rnn runner, aby používal runtime logprobs a testy prochází. Takže tohle zavírám a můžeš se na to kouknout v rámci review. Commit: 823c635a09661f79608d51c6d30d6132e98527d9
		</comment>
	</comments>
</bug>