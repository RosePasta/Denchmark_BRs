<bug id='381' author='obo' open_date='2017-04-07T08:20:53Z' closed_time='2018-05-29T13:28:41Z'>
	<summary>Avoid wasting vocabulary size on unseen BPE parts</summary>
	<description>
NM with BPE wastes vocabulary entries on "intermediate" BPE merges that never occur alone and only serve to compose larger bpe tokens (or complete words).
Workaround option 1: preprocess all corpora manually, disable bpe pre/postprocessing in NM altogether
Workaround option 1b: keep bpe post-processing on, but remember: the training target side should be provided after bpe splitting while validation target side should be normal tokens (so option 1b is quite messy).
Workaround option 2: keep using bpe processors but beforehand process corpora manually and manually pickle the vocabulary or all seen bpe tokens, load this dictionary instead of dictionary from merges
	</description>
	<comments>
		<comment id='1' author='obo' date='2017-07-17T21:25:44Z'>
		I believe that creating the vocabulary from the BPE codes directly might also cause some training tokens to become unknown.
		</comment>
	</comments>
</bug>