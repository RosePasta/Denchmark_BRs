<bug id='545' author='mfruhner' open_date='2020-10-20T11:00:59Z' closed_time='2020-11-23T15:06:45Z'>
	<summary>Exporting fast.ai ResNet to ONNX and TRT</summary>
	<description>
Hi,
I trained a ResNet 18 with 6 classes using fast.ai v2 and it works really great so far.
Now I would like to export it, so I can use it on a Nvidia Jetson.
I donâ€™t want to install fastai on the Jetson, as I is quite complex as I found out.
So I exported the model to ONNX with
torch.onnx.export()
Then I try to convert it to a TRT engine right on the Jetson with onnx2trt, but I get the following error:
&lt;denchmark-code&gt;Input filename:   opset11.onnx
ONNX IR version:  0.0.6
Opset version:    11
Producer name:    pytorch
Producer version: 1.7
Domain:
Model version:    0
Doc string:
----------------------------------------------------------------
Parsing model
[2020-10-20 09:49:21 WARNING] [TRT]/home/nvidia/git/onnx-tensorrt/onnx2trt_utils.cpp:220: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.
[2020-10-20 09:49:21   ERROR] (Unnamed Layer* 76) [Shuffle]: at most one dimension may be inferred
While parsing node number 75 [BatchNormalization -&gt; "210"]:
ERROR: /home/nvidia/git/onnx-tensorrt/onnx2trt_utils.cpp:1498 In function scaleHelper:
[8] Assertion failed: dims.nbDims == 4 || dims.nbDims == 5
&lt;/denchmark-code&gt;

Does this mean, that some operations are not supported?
It does not give much info to me.
Do you have any suggestions?
Thanks!
	</description>
	<comments>
		<comment id='1' author='mfruhner' date='2020-10-21T08:31:54Z'>
		I have the same problem.

Input filename:   veid_resnet34.onnx
ONNX IR version:  0.0.6
Opset version:    12
Producer name:    pytorch
Producer version: 1.6
Domain:
Model version:    0
Doc string:
[10/21/2020-16:27:06] [W] [TRT] onnx2trt_utils.cpp:220: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.
[10/21/2020-16:27:06] [E] [TRT] (Unnamed Layer* 155) [Shuffle]: at most one dimension may be inferred
ERROR: onnx2trt_utils.cpp:1498 In function scaleHelper:
[8] Assertion failed: dims.nbDims == 4 || dims.nbDims == 5
[10/21/2020-16:27:06] [E] Failed to parse onnx file
[10/21/2020-16:27:06] [E] Parsing model failed
[10/21/2020-16:27:06] [E] Engine creation failed
[10/21/2020-16:27:06] [E] Engine set up failed
&amp;&amp;&amp;&amp; FAILED TensorRT.trtexec # F:\HeWu_WorkSpace\Linux\NVIDIA\TensorRT-7.1.3.4\lib\trtexec.exe --onnx=veid_resnet34.onnx --explicitBatch

		</comment>
		<comment id='2' author='mfruhner' date='2020-10-21T09:58:23Z'>
		it work. I solved it by t.view(b, -1) or t.reshape(b, -1)  change to torch.flatten(t, 1).
		</comment>
		<comment id='3' author='mfruhner' date='2020-10-22T11:50:36Z'>
		Hey, thanks for your answer. Where exactly did you change that? In the fast.ai model code? Or in the converter?
		</comment>
		<comment id='4' author='mfruhner' date='2020-10-23T18:43:30Z'>
		&lt;denchmark-link:https://github.com/mfruhner&gt;@mfruhner&lt;/denchmark-link&gt;
 is it possible for you to provide your ONNX model to us to investigate? It looks like a pytorch2onnx export pattern that's generating an ONNX graph that's incompatible with TensorRT.
		</comment>
		<comment id='5' author='mfruhner' date='2020-10-24T08:58:47Z'>
		Hey,
here is a &lt;denchmark-link:https://hsosnabrueck-my.sharepoint.com/:u:/g/personal/m_fruhner_hs-osnabrueck_de/EVWB3MWiDBFHjCejujSglaYB0cX2trmsOWerN9KUyzhEBw?e=ribf0I&gt;OneDrive Link&lt;/denchmark-link&gt;
 to my model. It is basically a fast.ai ResNet 18 exported to onnx from the fast.ai learner.
Thanks for your help!
		</comment>
		<comment id='6' author='mfruhner' date='2020-11-12T13:06:50Z'>
		I think I am having a similar issue: &lt;denchmark-link:https://github.com/onnx/onnx-tensorrt/issues/566&gt;#566&lt;/denchmark-link&gt;

I am also converting from a Pytorch model to ONNX then to TensorRT. This model converted fine in the previous 7.1 release.
		</comment>
		<comment id='7' author='mfruhner' date='2020-11-12T18:26:09Z'>
		
it work. I solved it by t.view(b, -1) or t.reshape(b, -1) change to torch.flatten(t, 1).

It does not seem to work in my case! Did you change something esle ?? Thanks
		</comment>
		<comment id='8' author='mfruhner' date='2020-11-23T15:06:43Z'>
		I tried again with the latest commit and it seems to work now, thanks!
		</comment>
	</comments>
</bug>