<bug id='302' author='tairen99' open_date='2019-10-25T21:11:07Z' closed_time='2020-11-10T22:06:12Z'>
	<summary>Assertion failed: tensors.count(input_name) error when converting onnx to tensorrt</summary>
	<description>
Hi, all,
I am trying to convert a trained tiny yolov2 model into tensorrt format.
I was able to transform the tiny_yolov2.weights into tiny_yolov2.onnx using onnxmltools. But when I  tried to use onnx2trt, I got this error:
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;Input filename:   tiny_yolov2.onnx
ONNX IR version:  0.0.6
Opset version:    10
Producer name:    OnnxMLTools
Producer version: 1.6.0
Domain:           onnxconverter-common
Model version:    0
Doc string:&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;Parsing model
While parsing node number 30 [Mul -&gt; "scalerPreprocessor_scaled"]:
ERROR: /home/cisco/onnx-tensorrt-5.1/ModelImporter.cpp:552 In function importModel:
[5] Assertion failed: tensors.count(input_name)&lt;/denchmark-h&gt;

I am using the ubuntu 18.04 LTS; Tensorrt 5.1.6; and onnx-tensort 5.1 version.
Thank you!
	</description>
	<comments>
		<comment id='1' author='tairen99' date='2019-10-26T03:10:09Z'>
		I had the same problem, using PyTorch 1.3.   Using PyTorch 1.2 with torchvision 0.4 works better for me.
		</comment>
		<comment id='2' author='tairen99' date='2019-11-06T19:07:17Z'>
		Hi,
I think this error comes from the ONNX parser having an issue parsing your graph, specifically that it found a node with no input I believe.
There are a few things you can try for this:


It seems that PyTorch 1.3 is a little more up to date than the current parsers (opset 11), so that may be the root cause for now. You can try downgrading to 1.2 just for exporting your Pytorch to ONNX, as mentioned above.


You can try inspecting your graph with the ONNX python library to see if it gives you any hints to the issue:


import onnx
onnx.checker.check_model(onnx_model)


You can visually inspect your graph using something like https://lutzroeder.github.io/netron/ to see if anything looks wrong with the graph.


One user even tried changing the parser code, so there may be some helpful things here, but I don't think that is generally necessary: #199


		</comment>
		<comment id='3' author='tairen99' date='2019-12-18T05:36:02Z'>
		Get the same error with a Tensorflow MobileSSD net. The check_model() said everything is ok : /
I don't see any of my nodes not having any input when I debug my graph.
		</comment>
		<comment id='4' author='tairen99' date='2019-12-20T15:13:20Z'>
		Updating to TensorRT 7.0.0 somehow fixed it
		</comment>
		<comment id='5' author='tairen99' date='2020-01-20T10:40:21Z'>
		I want to convert ssd_mobilenet_v3_small_coco_2019_08_14.onnx model to tensorrt, and I get the same error with jetson nano, How I do to upgrade TensorRT6 to 7?
		</comment>
		<comment id='6' author='tairen99' date='2020-01-22T06:44:51Z'>
		&lt;denchmark-link:https://github.com/PythonImageDeveloper&gt;@PythonImageDeveloper&lt;/denchmark-link&gt;
 TensorRT 7 hasn't been released yet for Jetpack (Jetson). Sorry, I can't say anything about upcoming roadmaps.
		</comment>
		<comment id='7' author='tairen99' date='2020-01-31T07:32:24Z'>
		&lt;denchmark-code&gt;pytorch==1.2.0
onnx==1.6.0
TensorRT==7.0.0.11
&lt;/denchmark-code&gt;

should be a workaround.
It seems that there is a dummy constant of resize layer if you use pytorch 1.3 or newer.
		</comment>
		<comment id='8' author='tairen99' date='2020-02-24T09:18:25Z'>
		TensorRT-7 with the latest Pytorch+torchvision version gives the same error
		</comment>
		<comment id='9' author='tairen99' date='2020-03-06T19:54:53Z'>
		Can confirm, still gives same error.
		</comment>
		<comment id='10' author='tairen99' date='2020-03-24T07:41:16Z'>
		Set opset_version to 10 somehow solved the problem : |
		</comment>
		<comment id='11' author='tairen99' date='2020-03-24T08:38:58Z'>
		&lt;denchmark-link:https://github.com/Bisgates&gt;@Bisgates&lt;/denchmark-link&gt;
 can you post your system specs/library versions/CUDA version?
		</comment>
		<comment id='12' author='tairen99' date='2020-03-30T09:40:14Z'>
		I am also facing this problem on TRT 6.0, however, have had success  with TRT 7.0 and making this change &lt;denchmark-link:https://github.com/onnx/onnx-tensorrt/pull/369/files&gt;https://github.com/onnx/onnx-tensorrt/pull/369/files&lt;/denchmark-link&gt;
.
The main issue seems to be a interpolation layer at the start of my network (F.interpolate) but have noticed differencess ONNX opsets:
Opset 11 changes nn.Upsample and F.interpolate layers to "Resize layers" with a constant input as well as previous layers:
&lt;denchmark-link:https://user-images.githubusercontent.com/57392179/77897518-adbdf600-7271-11ea-953b-2679d50508f8.png&gt;&lt;/denchmark-link&gt;

Opset 10:
&lt;denchmark-link:https://user-images.githubusercontent.com/57392179/77897565-b7dff480-7271-11ea-869e-8159725ce6a8.png&gt;&lt;/denchmark-link&gt;

and Opset 9:
&lt;denchmark-link:https://user-images.githubusercontent.com/57392179/77897589-c0d0c600-7271-11ea-9d16-362c1c7caf04.png&gt;&lt;/denchmark-link&gt;

Despite warnings when converting ONNX, I get success with opset 9 and opset 10 providing "size=" is used rather than a scale factor for nn.Upsample or F.interpolate layers. However, I still get count(input_name) error with opset 11. All done with pytorch 1.3.
As mentioned, my main aim is to run on Jetson devices which only support TRT 6.0 so my engine file created with TRT 7.0 still doesn't work.
		</comment>
		<comment id='13' author='tairen99' date='2020-03-30T12:52:16Z'>
		Anyone who has run out of ideas try with torch==1.0.1 torchvision==0.22 instead. This worked for me when previously got count(input_name) errors whatever opsets I used when using pytorch 1.3 to create engine files for TRT 6.0. This worked even with F.interpolate and Upsample layers (yes, I know, I should update them) in the same model
		</comment>
		<comment id='14' author='tairen99' date='2020-04-08T20:58:48Z'>
		I'm having trouble with this conversion as well. My goal is to run models on the jetson.
Error log:
&lt;denchmark-code&gt;Loading ONNX file from path resnet50.onnx...
Beginning ONNX file parsing
[TensorRT] VERBOSE: input_1_1:01_permuted:Transpose -&gt; (3, 224, 224)
ERROR: Failed to parse the ONNX file.
In node 1 (importModel): INVALID_GRAPH: Assertion failed: tensors.count(input_name)
&lt;/denchmark-code&gt;


.onnx model created with keras2onnx with opset 10 (for TRT 6.0 compatibility)
checker.check_model() doesn't give an error nor does it output anything (I'm assuming that's a good sign)
Tried removing every custom code from my model, I'm simply trying to port the default tf.keras.applications.ResNet50 model using onnx as an intermediate representation (tf-trt has been a pain to work with)

Desktop Environment (model training and keras2onnx conversion):

Ubuntu 18.04
Tensorflow 1.15.2
keras2onnx 1.6.5
onnx 1.6.0

Jetson environment (onnx to tensorrt conversion)

JetPack 4.3
TRT 6.0.1.10

		</comment>
		<comment id='15' author='tairen99' date='2020-04-10T08:44:23Z'>
		Hi &lt;denchmark-link:https://github.com/guischmitd&gt;@guischmitd&lt;/denchmark-link&gt;
 I would first definitely try with opset 9 if possible. Also, I would use netron to visualise my model and make sure everything looks good. Also, what's the permute/ transposing referencing? Do you have permute layers? I would double check if they are compatible in TRT.
Tom.
		</comment>
		<comment id='16' author='tairen99' date='2020-04-11T14:39:34Z'>
		Hey &lt;denchmark-link:https://github.com/tom-garford&gt;@tom-garford&lt;/denchmark-link&gt;
 thanks for the reply! I've tried using opset 9 as well and everything seems ok on netron (regarding inputs and outputs, disconnected layers and such). I've noticed that there's a transpose layer at the start of my network, which I assume derives from the channels last standard of Keras networks.
&lt;denchmark-link:https://user-images.githubusercontent.com/22595406/79046729-fddd7680-7be8-11ea-8f51-ef8d61f19fd8.png&gt;&lt;/denchmark-link&gt;

I've also crossed out installation problems since I can run the darknet -&gt; onnx -&gt; trt example from &lt;denchmark-link:https://jkjung-avt.github.io/tensorrt-yolov3/&gt;this post&lt;/denchmark-link&gt;
.
I'll try changing the input to NCHW on the keras model to see if it makes a difference. Will report back with () results!
		</comment>
		<comment id='17' author='tairen99' date='2020-04-11T19:09:33Z'>
		Hi &lt;denchmark-link:https://github.com/guischmitd&gt;@guischmitd&lt;/denchmark-link&gt;

Good to hear! Yes, I would try and scrub out that transpose layer and see what happens. I know from bitter experience the input must be NCHW not NHWC (RRRRGGGBBB, not RGBRGBRGB, etc.) for tensorRT anyway. I would wager you at least get a more informative error message (if not solve the problem) by removing the transpose layer.
		</comment>
		<comment id='18' author='tairen99' date='2020-04-13T20:45:49Z'>
		Ok, here's the latest update:
I was able to change the keras standard to channels_first using  to generate a new graph:
&lt;denchmark-link:https://user-images.githubusercontent.com/22595406/79155103-0380bb00-7da7-11ea-9359-4e7d86cffbe5.png&gt;&lt;/denchmark-link&gt;

You can see the transpose layer is gone here. Unfortunately for some reason the flatten layer after my convolution blocks creates another transpose layer:
&lt;denchmark-link:https://user-images.githubusercontent.com/22595406/79155181-227f4d00-7da7-11ea-97fb-53bf8084c002.png&gt;&lt;/denchmark-link&gt;

I've tried time and again with different models from tensorflow.keras.applications (Mobilenet, InceptioV3, ResNet50) but none of these can be parsed by the tensorrt.ONNXParser.
I could finally run my engine builder on the jetson after switching to opset 7 on keras2onnx.convert_keras(path_to_h5_model, target_opset=7). I've yet to test inference using the generated .trt engine. Once again, I'll post my results here for future reference.
Thanks for the help so far!
		</comment>
		<comment id='19' author='tairen99' date='2020-04-14T08:40:58Z'>
		Hi &lt;denchmark-link:https://github.com/guischmitd&gt;@guischmitd&lt;/denchmark-link&gt;
  great news!
Glad you finally got it going. As you saw from my attempts, it takes a bit of experimentation! Thanks for posting your results.
Tom.
		</comment>
		<comment id='20' author='tairen99' date='2020-05-29T04:48:59Z'>
		
TensorRT-7 with the latest Pytorch+torchvision version gives the same error

I meet the same errors. Have you solved it ? Thanks.
		</comment>
		<comment id='21' author='tairen99' date='2020-10-02T14:18:16Z'>
		Hi,
I am Converting my Custom Pytorch Model to TRT Engine file. I am able to convert the model to Onnx, but I am facing an Issue while converting from Onnx to the Engine file.
I am getting the error as:
AttributeError: ‘NoneType’ object has no attribute ‘serialize’.
I am giving the correct shape as an input to the build engine function.
And for the Resnet50 model, I am able to convert successfully but for my custom model, I am facing the above issue.
Please suggest if there is any specific reason behind that.
&lt;denchmark-link:https://user-images.githubusercontent.com/48723743/94933547-271bf700-04e8-11eb-90b1-0c3f88d170e8.png&gt;&lt;/denchmark-link&gt;

I tried with trtexec command, I am getting the error as attached below:
While parsing node number 195 [Resize]:
ERROR: ModelImporter.cpp:124 In function parseGraph:
[5] Assertion failed: ctx-&gt;tensors().count(inputName)
[09/25/2020-11:02:52] [E] Failed to parse onnx file
[09/25/2020-11:02:52] [E] Parsing model failed
[09/25/2020-11:02:52] [E] Engine creation failed
[09/25/2020-11:02:52] [E] Engine set up failed
		</comment>
		<comment id='22' author='tairen99' date='2020-11-10T22:06:12Z'>
		Thanks for those in the thread that provided help. This issue should be fixed in later TensorRT releases, and I would recommend anyone who is facing this issue to try the latest TensorRT version to parse their model.
If you are still hitting this issue, please open a new one specific to your model. Thanks!
		</comment>
		<comment id='23' author='tairen99' date='2020-11-12T10:50:23Z'>
		I have the same problem to convert interpolate from torch to onnx to tensorrt.
I sovle it with only change the param "opset_version" in the function torch.onnx.export.
pytorch==1.4.0
onnx==0.7.0
opset_version=10
The differnt with two param "opset_version=10" and "opset_version=11" in the onnx used netron app.
opset_version=10
&lt;denchmark-link:https://user-images.githubusercontent.com/14231410/98930533-bf5dc080-2517-11eb-9f2e-64f860084282.png&gt;&lt;/denchmark-link&gt;

opset_version=11
&lt;denchmark-link:https://user-images.githubusercontent.com/14231410/98930535-c08eed80-2517-11eb-87c1-29c3d7b49d39.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='24' author='tairen99' date='2021-01-08T04:07:48Z'>
		If it's issue relating to F.Interpolate, this is what i raised and the work around i found to convert from Torch-&gt;Onnx-&gt;TensorRT.
&lt;denchmark-link:https://github.com/onnx/onnx-tensorrt/issues/615&gt;#615&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>