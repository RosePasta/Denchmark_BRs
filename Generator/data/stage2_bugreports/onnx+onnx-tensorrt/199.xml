<bug id='199' author='electronicYH' open_date='2019-06-26T12:59:51Z' closed_time='2020-10-13T05:39:22Z'>
	<summary>WARNING: ONNX model has a newer ir_version (0.0.5) than this parser was built against (0.0.3).</summary>
	<description>
I meet the problem:
&lt;denchmark-h:h2&gt;a@a:/data/yh/onnx-tensorrt/build$ onnx2trt yh_fcn_14c_include_weights.onnx -o yh.trt -g&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;Input filename:   yh_fcn_14c_include_weights.onnx
ONNX IR version:  0.0.5
Opset version:    10
Producer name:    keras2onnx
Producer version: 1.5.0
Domain:           onnx
Model version:    0
Doc string:&lt;/denchmark-h&gt;

WARNING: ONNX model has a newer ir_version (0.0.5) than this parser was built against (0.0.3).
Parsing model
While parsing node number 1 [Conv -&gt; "convolution_output"]:
ERROR: /data/yh/onnx-tensorrt/ModelImporter.cpp:537 In function importModel:
[5] Assertion failed: tensors.count(input_name)
please help me to fix it, thank you so much!!!!!
	</description>
	<comments>
		<comment id='1' author='electronicYH' date='2019-07-19T16:51:56Z'>
		Hi &lt;denchmark-link:https://github.com/electronicYH&gt;@electronicYH&lt;/denchmark-link&gt;
  did you fix the issue? I had the same problem here.
I tried to resolve the ir version warning by setting different target opset in keras2onnx but that didn't help. (I tried all the way form 1 to 10).
For the assertion failure, I checked the ModelImport.cpp. I assume it's because the code finds that one of the nodes have 0 input? Though I checked the graph of my model, don't think that's the case.
&lt;denchmark-code&gt;for( size_t node_idx : topological_order ) {
    _current_node = node_idx;
    ::ONNX_NAMESPACE::NodeProto const&amp; node = graph.node(node_idx);
    std::vector&lt;TensorOrWeights&gt; inputs;
    for( auto const&amp; input_name : node.input() ) {
      ASSERT(tensors.count(input_name), ErrorCode::kINVALID_GRAPH);
      inputs.push_back(tensors.at(input_name));
    }
    // ...
}
&lt;/denchmark-code&gt;

On a separate note, have you run the onnx checker after exporting?
&lt;denchmark-code&gt;import onnx
onnx.checker.check_model(onnx_model)
&lt;/denchmark-code&gt;

The checker threw a warning for me:
&lt;denchmark-code&gt;Nodes in a graph must be topologically sorted, however input 'conv2d_31_Relu_0' of node: 
input: "conv2d_31_Relu_0" output: "transpose_output7" name: "Transpose21" op_type: "Transpose" attribute { name: "perm" ints: 0 ints: 2 ints: 3 ints: 1 type: INTS } doc_string: "" domain: ""
 is not output of any previous nodes.
&lt;/denchmark-code&gt;

However, my onnx model can run inference fine in Python API using the code below. So I assumed this shouldn't be a big issue?
&lt;denchmark-code&gt;import onnxruntime as rt
import numpy as np
X_test = np.ones((1,h,w,1))
sess    = rt.InferenceSession("my_model.onnx")
input_name = sess.get_inputs()[0].name
label_name = sess.get_outputs()[0].name
pred_onx    = sess.run([label_name], {input_name: X_test.astype(np.float32)})[0]
&lt;/denchmark-code&gt;

Any insights would be greatly appreciated!
		</comment>
		<comment id='2' author='electronicYH' date='2019-08-21T08:50:59Z'>
		I ran into the same problem when I ran the DeepLabV3 onnx model converted from keras on TensorRT SDK 5.1.5, how to fix the problem?
		</comment>
		<comment id='3' author='electronicYH' date='2019-08-23T01:26:43Z'>
		&lt;denchmark-link:https://github.com/pango99&gt;@pango99&lt;/denchmark-link&gt;
  I modify importModel code in ModelImporter.cpp, and rcompile onnx-tensorrt(&lt;denchmark-link:https://github.com/onnx/onnx-tensorrt&gt;https://github.com/onnx/onnx-tensorrt&lt;/denchmark-link&gt;
  v5.0)
&lt;denchmark-link:https://user-images.githubusercontent.com/7134030/63560207-c18cd580-c587-11e9-91a3-cb7d96b62522.png&gt;&lt;/denchmark-link&gt;

it works for me, you can have a try.
&lt;denchmark-link:https://github.com/onnx/onnx-tensorrt/files/3532464/ModelImporter.cpp.zip&gt;ModelImporter.cpp.zip&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='electronicYH' date='2019-08-23T02:13:01Z'>
		
[ ]


@pango99 I modify importModel code in ModelImporter.cpp, and rcompile onnx-tensorrt(https://github.com/onnx/onnx-tensorrt v5.0)

it works for me, you can have a try.
ModelImporter.cpp.zip

Thanks zjd1988,so can I compile onnx-tensorrt on windows platform? and can I use the compiled module with nvidia tensorrt sdk directly?
		</comment>
		<comment id='5' author='electronicYH' date='2019-08-23T02:23:24Z'>
		I compile onnx-tensorrt on ubuntu16.04, but it might be works well on windows, refferd from &lt;denchmark-link:https://github.com/onnx/onnx-tensorrt/issues/60&gt;#60&lt;/denchmark-link&gt;
 .
you need replace the old libs (in tensorrt sdk) with newly compiled, or  modify the LD_LIBRARY_PATH(add the path of onnx-tensorrt libs)
		</comment>
		<comment id='6' author='electronicYH' date='2019-08-23T02:29:59Z'>
		
I compile onnx-tensorrt on ubuntu16.04, but it might be works well on windows, refferd from #60 .
you need replace the old libs (in tensorrt sdk) with newly compiled, or modify the LD_LIBRARY_PATH(add the path of onnx-tensorrt libs)

OK,I'll try to compile few days later
		</comment>
		<comment id='7' author='electronicYH' date='2019-09-08T12:12:42Z'>
		
@pango99 I modify importModel code in ModelImporter.cpp, and rcompile onnx-tensorrt(https://github.com/onnx/onnx-tensorrt v5.0)

it works for me, you can have a try.
ModelImporter.cpp.zip

I used your source code and now I get different assert error:

While parsing node number 0 [Slice]:
3
ERROR: /home/ubuntu/projects/onnx-tensorrt/builtin_op_importers.cpp:1687 In function importSlice:
[6] Assertion failed: inputs.size() == 1

Any help would be appreciated.
		</comment>
		<comment id='8' author='electronicYH' date='2019-09-09T07:17:25Z'>
		Hi ＠drormeir .You can debug the onnx-tensorrt code step by step, to find out which layer assert.
After that,you can check the onnx-tensorrt suported ops from this link &lt;denchmark-link:https://github.com/onnx/onnx-tensorrt/blob/master/operators.md&gt;https://github.com/onnx/onnx-tensorrt/blob/master/operators.md&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='electronicYH' date='2019-11-07T20:45:40Z'>
		&lt;denchmark-link:https://github.com/pango99&gt;@pango99&lt;/denchmark-link&gt;
 Same issue with the parser. Ran onnx.checker.check_model(onnx_model) with no error. Were you able to build these libs on windows and resolve this error?
		</comment>
		<comment id='10' author='electronicYH' date='2019-12-17T17:32:58Z'>
		As &lt;denchmark-link:https://github.com/ttdd11&gt;@ttdd11&lt;/denchmark-link&gt;
 said, get same error while the checker did not complain.
		</comment>
		<comment id='11' author='electronicYH' date='2019-12-18T05:33:39Z'>
		Tried using the changes from &lt;denchmark-link:https://github.com/drormeir&gt;@drormeir&lt;/denchmark-link&gt;
 but somehow non of my changes actually have an effect. Also the line number of the assert does not match the one of the ModelImporter.cpp I have.
That's what my cmake looks like:
add_library(libnvonnxparser SHARED IMPORTED)
set_property(TARGET libnvonnxparser PROPERTY IMPORTED_LOCATION ${INSTALL_DIR}/onnx-tensorrt/dist/lib/libnvonnxparser.so)
add_library(libnvinfer SHARED IMPORTED)
set_property(TARGET libnvinfer PROPERTY IMPORTED_LOCATION $ENV{TRT_RELEASE}/lib/libnvinfer.so)
...
target_link_libraries(${PROJECT_NAME}
  ...
  libnvonnxparser
  libnvinfer
}
It seems that another ModelImporter.cpp is used than the one I am building but the one from the TENSORRT_ROOT folder :/ Anyone else had the same issue?
		</comment>
		<comment id='12' author='electronicYH' date='2019-12-20T15:14:28Z'>
		Updating to the recent 7.0.0 release of TensorRT (including 7.0.0 onnx-tensorrt) fixed the whole thing. Both the assertion and tensorrt using and old onnx-tensorrt version. At least in my case.
		</comment>
		<comment id='13' author='electronicYH' date='2020-05-09T11:13:09Z'>
		

[ ]


@pango99 I modify importModel code in ModelImporter.cpp, and rcompile onnx-tensorrt(https://github.com/onnx/onnx-tensorrt v5.0)

it works for me, you can have a try.
ModelImporter.cpp.zip

Thanks zjd1988,so can I compile onnx-tensorrt on windows platform? and can I use the compiled module with nvidia tensorrt sdk directly?

i copy your code  and get error : ‘initializer_map’ was not declared in this scope
		</comment>
		<comment id='14' author='electronicYH' date='2020-05-10T01:15:44Z'>
		Hi &lt;denchmark-link:https://github.com/tcxia&gt;@tcxia&lt;/denchmark-link&gt;
, which tensorrt version  you use?
		</comment>
		<comment id='15' author='electronicYH' date='2020-05-11T02:23:34Z'>
		
Hi @tcxia, which tensorrt version you use?

tensorRT 5.1 cuda10.1
		</comment>
		<comment id='16' author='electronicYH' date='2020-05-11T03:42:51Z'>
		Hi &lt;denchmark-link:https://github.com/tcxia&gt;@tcxia&lt;/denchmark-link&gt;
, I tested ModelImporter.cpp.zip with tensorRT5.0, I not sure ablout tensorRT5.1. could you try this with tensorRT5.0.
		</comment>
		<comment id='17' author='electronicYH' date='2020-06-02T08:39:05Z'>
		I was facing same issue in tensor docker container, solution was sh /opt/tensorrt/install_opensource.sh
		</comment>
		<comment id='18' author='electronicYH' date='2020-10-13T05:39:22Z'>
		This issue was fixed in later TensorRT versions. If you are still having trouble with the current release version (7.1) please open another issue.
		</comment>
	</comments>
</bug>