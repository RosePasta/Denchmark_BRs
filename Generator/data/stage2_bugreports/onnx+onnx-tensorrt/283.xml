<bug id='283' author='jinfagang' open_date='2019-10-12T03:23:05Z' closed_time='2020-12-01T16:37:46Z'>
	<summary>Gather not properly supported</summary>
	<description>
When convert maskrcnn.onnx model, this model actually can successfully inference using onnxruntime CPU, but onnx2trt not properly parse Gather node:
&lt;denchmark-code&gt;onnx-tensorrt/builtin_op_importers.cpp:772 In function importGather:
[8] Assertion failed: !(data-&gt;getType() == nvinfer1::DataType::kINT32 &amp;&amp; nbDims == 1 &amp;&amp; inputs.at(0).is_tensor()) &amp;&amp; "Cannot perform gather on a shape tensor!"

&lt;/denchmark-code&gt;

this assertions seems need to properly handle .
	</description>
	<comments>
		<comment id='1' author='jinfagang' date='2019-10-22T06:23:20Z'>
		It can be reproduced by using shape in pytorch. Here is an example.
class Net(nn.Module):
    def forward(self, x):
        return x.shape
As a result, a lot of models don't work.
		</comment>
		<comment id='2' author='jinfagang' date='2019-10-22T06:28:47Z'>
		&lt;denchmark-link:https://github.com/akirasosa&gt;@akirasosa&lt;/denchmark-link&gt;
  Exactly, IDK what these assertions means and indeed some of them does not really necessary or should handle some corner situations.
		</comment>
		<comment id='3' author='jinfagang' date='2019-10-24T01:34:46Z'>
		I used &lt;denchmark-link:https://github.com/daquexian/onnx-simplifier&gt;https://github.com/daquexian/onnx-simplifier&lt;/denchmark-link&gt;
 and my problem was solved.
		</comment>
		<comment id='4' author='jinfagang' date='2019-10-24T05:10:34Z'>
		&lt;denchmark-link:https://github.com/akirasosa&gt;@akirasosa&lt;/denchmark-link&gt;
 This lib just simplify some redundant nodes, can not applying on large models. I have tried this tool before.
So, what does the differences between converts and after? Can u take a look of inside model and what changed inside Gather?
		</comment>
		<comment id='5' author='jinfagang' date='2019-12-16T13:14:52Z'>
		Hey any update on this issue? facing this exact same error
		</comment>
		<comment id='6' author='jinfagang' date='2020-01-20T08:50:14Z'>
		
Hey any update on this issue? facing this exact same error

The suggestion above from &lt;denchmark-link:https://github.com/akirasosa&gt;@akirasosa&lt;/denchmark-link&gt;
 to use &lt;denchmark-link:https://github.com/daquexian/onnx-simplifier&gt;https://github.com/daquexian/onnx-simplifier&lt;/denchmark-link&gt;
 works perfectly here.
		</comment>
		<comment id='7' author='jinfagang' date='2020-02-25T07:34:15Z'>
		Hi there,
Any update on this issue? I have a similar problem with a Faster R-CNN pytorch model converted to ONNX. I do not have any problem running the model with onnxruntime-gpu (compiled with CUDA support). However, I get the following error when trying to run it using onnxruntime compiled with TensorRT support:
ShapeTensor.cpp:38: onnx2trt::ShapeTensor::ShapeTensor(int, int32_t): Assertion size_ &gt;= 0' failed.`
Really appreciate any help. I indeed tried the onnx-simplifier as pointed out above. But it failed without success.
		</comment>
		<comment id='8' author='jinfagang' date='2020-04-13T14:20:29Z'>
		The assertion is pretty confusing because in split:
DEFINE_BUILTIN_OP_IMPORTER(Split)

{

    const int numOutputs = node.output().size();



    nvinfer1::ITensor* tensorPtr = &amp;convertToTensor(inputs.at(0), ctx);

    const int rank = tensorPtr-&gt;getDimensions().nbDims;

    nvinfer1::ITensor* shape = ctx-&gt;network()-&gt;addShape(*tensorPtr)-&gt;getOutput(0);



    OnnxAttrs attrs(node);

    int axis = attrs.get&lt;int&gt;("axis", 0);

    TRT_CHECK(convert_axis(axis, rank));



    std::vector&lt;int&gt; outputLengths;

    if (attrs.count("split"))

    {

        outputLengths = attrs.get&lt;std::vector&lt;int&gt;&gt;("split");

        ASSERT(static_cast&lt;int&gt;(outputLengths.size()) == numOutputs, ErrorCode::kINVALID_NODE);

    }



    nvinfer1::ITensor* startSliceAxis{addConstantScalar&lt;int32_t&gt;(ctx, 0, ::ONNX_NAMESPACE::TensorProto::INT32, nvinfer1::Dims{1, 1})-&gt;getOutput(0)};

    // sizeSliceAxis = axisLength / numOutputs

    nvinfer1::ITensor* sizeSliceAxis{ctx-&gt;network()-&gt;addElementWise(

        *gatherDimension(ctx, shape, axis, nvinfer1::Dims{1, 1}),

        *addConstantScalar(ctx, numOutputs, ::ONNX_NAMESPACE::TensorProto::INT32, nvinfer1::Dims{1, 1})-&gt;getOutput(0),

        nvinfer1::ElementWiseOperation::kDIV

    )-&gt;getOutput(0)};
   ...
}
The input tensor to addGather(enclosed in gather Dimension) is exactly a shape tensor.
		</comment>
		<comment id='9' author='jinfagang' date='2020-11-10T20:14:12Z'>
		Is anyone still having trouble with this with the latest TensorRT release?
		</comment>
		<comment id='10' author='jinfagang' date='2020-12-01T16:37:46Z'>
		Closing due to inactivity - if you are still having issues with the latest version of onnx-tensorrt feel free to open a new issue.
		</comment>
	</comments>
</bug>