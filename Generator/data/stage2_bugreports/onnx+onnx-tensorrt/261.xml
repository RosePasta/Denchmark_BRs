<bug id='261' author='Legends0' open_date='2019-09-23T22:26:52Z' closed_time='2020-10-14T16:00:48Z'>
	<summary>GCC-8 Support</summary>
	<description>
Currently builds fail on GCC-8.3.0 and newer with the following error:
&lt;denchmark-code&gt;[ 32%] Building CXX object third_party/onnx-tensorrt/CMakeFiles/nvonnxparser_static.dir/builtin_op_importers.cpp.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
/root/pytorch-1.2.0/third_party/onnx-tensorrt/builtin_op_importers.cpp: In function ‘onnx2trt::NodeImportResult onnx2trt::{anonymous}::importGemm(onnx2trt::IImporterContext*, const onnx_torch::NodeProto&amp;, std::vector&lt;onnx2trt::TensorOrWeights&gt;&amp;)’:
/root/pytorch-1.2.0/third_party/onnx-tensorrt/builtin_op_importers.cpp:1078:20: error: the type ‘const onnx2trt::{anonymous}::importGemm(onnx2trt::IImporterContext*, const onnx_torch::NodeProto&amp;, std::vector&lt;onnx2trt::TensorOrWeights&gt;&amp;)::&lt;lambda(const nvinfer1::ITensor&amp;, bool)&gt;’ of ‘constexpr’ variable ‘getMatrixOp’ is not literal
 1078 |     constexpr auto getMatrixOp = [] (const nvinfer1::ITensor&amp; input, bool transpose)
      |                    ^~~~~~~~~~~
/root/pytorch-1.2.0/third_party/onnx-tensorrt/builtin_op_importers.cpp:1078:35: note: ‘onnx2trt::{anonymous}::importGemm(onnx2trt::IImporterContext*, const onnx_torch::NodeProto&amp;, std::vector&lt;onnx2trt::TensorOrWeights&gt;&amp;)::&lt;lambda(const nvinfer1::ITensor&amp;, bool)&gt;’ is not literal because:
 1078 |     constexpr auto getMatrixOp = [] (const nvinfer1::ITensor&amp; input, bool transpose)
      |                                   ^
cc1plus: note:   ‘onnx2trt::{anonymous}::importGemm(onnx2trt::IImporterContext*, const onnx_torch::NodeProto&amp;, std::vector&lt;onnx2trt::TensorOrWeights&gt;&amp;)::&lt;lambda(const nvinfer1::ITensor&amp;, bool)&gt;’ is a closure type, which is only literal in C++17 and later
/root/pytorch-1.2.0/third_party/onnx-tensorrt/builtin_op_importers.cpp: In function ‘onnx2trt::NodeImportResult onnx2trt::{anonymous}::importMatMul(onnx2trt::IImporterContext*, const onnx_torch::NodeProto&amp;, std::vector&lt;onnx2trt::TensorOrWeights&gt;&amp;)’:
/root/pytorch-1.2.0/third_party/onnx-tensorrt/builtin_op_importers.cpp:1237:20: error: the type ‘const onnx2trt::{anonymous}::importMatMul(onnx2trt::IImporterContext*, const onnx_torch::NodeProto&amp;, std::vector&lt;onnx2trt::TensorOrWeights&gt;&amp;)::&lt;lambda(const nvinfer1::ITensor&amp;)&gt;’ of ‘constexpr’ variable ‘getMatrixOp’ is not literal
 1237 |     constexpr auto getMatrixOp = [] (const nvinfer1::ITensor&amp; input)
      |                    ^~~~~~~~~~~
/root/pytorch-1.2.0/third_party/onnx-tensorrt/builtin_op_importers.cpp:1237:35: note: ‘onnx2trt::{anonymous}::importMatMul(onnx2trt::IImporterContext*, const onnx_torch::NodeProto&amp;, std::vector&lt;onnx2trt::TensorOrWeights&gt;&amp;)::&lt;lambda(const nvinfer1::ITensor&amp;)&gt;’ is not literal because:
 1237 |     constexpr auto getMatrixOp = [] (const nvinfer1::ITensor&amp; input)
      |                                   ^
cc1plus: note:   ‘onnx2trt::{anonymous}::importMatMul(onnx2trt::IImporterContext*, const onnx_torch::NodeProto&amp;, std::vector&lt;onnx2trt::TensorOrWeights&gt;&amp;)::&lt;lambda(const nvinfer1::ITensor&amp;)&gt;’ is a closure type, which is only literal in C++17 and later
make[2]: *** [third_party/onnx-tensorrt/CMakeFiles/nvonnxparser_static.dir/builtin_op_importers.cpp.o] Error 1
make[1]: *** [third_party/onnx-tensorrt/CMakeFiles/nvonnxparser_static.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....
&lt;/denchmark-code&gt;

Cuda 10.1 added support for GCC 8.
	</description>
	<comments>
		<comment id='1' author='Legends0' date='2019-09-23T22:49:26Z'>
		Some more details (outside of PyTorch):
&lt;denchmark-code&gt;make -j3
Scanning dependencies of target gen_onnx_proto
[  3%] Running gen_proto.py on onnx/onnx.in.proto
Processing /root/onnx-tensorrt/third_party/onnx/onnx/onnx.in.proto
Writing /root/onnx-tensorrt/build/third_party/onnx/onnx/onnx_onnx2trt_onnx-ml.proto
Writing /root/onnx-tensorrt/build/third_party/onnx/onnx/onnx_onnx2trt_onnx-ml.proto3
Writing /root/onnx-tensorrt/build/third_party/onnx/onnx/onnx-ml.pb.h
generating /root/onnx-tensorrt/build/third_party/onnx/onnx/onnx_pb.py
[  6%] Running C++ protocol buffer compiler on /root/onnx-tensorrt/build/third_party/onnx/onnx/onnx_onnx2trt_onnx-ml.proto
[  6%] Built target gen_onnx_proto
[ 10%] Running gen_proto.py on onnx/onnx-operators.in.proto
Processing /root/onnx-tensorrt/third_party/onnx/onnx/onnx-operators.in.proto
Writing /root/onnx-tensorrt/build/third_party/onnx/onnx/onnx-operators_onnx2trt_onnx-ml.proto
Writing /root/onnx-tensorrt/build/third_party/onnx/onnx/onnx-operators_onnx2trt_onnx-ml.proto3
Writing /root/onnx-tensorrt/build/third_party/onnx/onnx/onnx-operators-ml.pb.h
generating /root/onnx-tensorrt/build/third_party/onnx/onnx/onnx_operators_pb.py
[ 13%] Running C++ protocol buffer compiler on /root/onnx-tensorrt/build/third_party/onnx/onnx/onnx-operators_onnx2trt_onnx-ml.proto
Scanning dependencies of target onnx_proto
[ 16%] Building CXX object third_party/onnx/CMakeFiles/onnx_proto.dir/onnx/onnx_onnx2trt_onnx-ml.pb.cc.o
[ 20%] Building CXX object third_party/onnx/CMakeFiles/onnx_proto.dir/onnx/onnx-operators_onnx2trt_onnx-ml.pb.cc.o
[ 23%] Linking CXX static library libonnx_proto.a
[ 30%] Built target onnx_proto
Scanning dependencies of target nvonnxparser_static
Scanning dependencies of target nvonnxparser
[ 40%] Building CXX object CMakeFiles/nvonnxparser.dir/NvOnnxParser.cpp.o
[ 40%] Building CXX object CMakeFiles/nvonnxparser.dir/ModelImporter.cpp.o
[ 40%] Building CXX object CMakeFiles/nvonnxparser_static.dir/NvOnnxParser.cpp.o
[ 43%] Building CXX object CMakeFiles/nvonnxparser.dir/builtin_op_importers.cpp.o
[ 46%] Building CXX object CMakeFiles/nvonnxparser_static.dir/ModelImporter.cpp.o
/root/onnx-tensorrt/builtin_op_importers.cpp: In function ‘onnx2trt::NodeImportResult onnx2trt::{anonymous}::importGemm(onnx2trt::IImporterContext*, const onnx2trt_onnx::NodeProto&amp;, std::vector&lt;onnx2trt::TensorOrWeights&gt;&amp;)’:
/root/onnx-tensorrt/builtin_op_importers.cpp:863:20: error: the type ‘const onnx2trt::{anonymous}::importGemm(onnx2trt::IImporterContext*, const onnx2trt_onnx::NodeProto&amp;, std::vector&lt;onnx2trt::TensorOrWeights&gt;&amp;)::&lt;lambda(const nvinfer1::ITensor&amp;, bool)&gt;’ of ‘constexpr’ variable ‘getMatrixOp’ is not literal
  863 |     constexpr auto getMatrixOp = [](const nvinfer1::ITensor&amp; input, bool transpose) {
      |                    ^~~~~~~~~~~
/root/onnx-tensorrt/builtin_op_importers.cpp:863:35: note: ‘onnx2trt::{anonymous}::importGemm(onnx2trt::IImporterContext*, const onnx2trt_onnx::NodeProto&amp;, std::vector&lt;onnx2trt::TensorOrWeights&gt;&amp;)::&lt;lambda(const nvinfer1::ITensor&amp;, bool)&gt;’ is not literal because:
  863 |     constexpr auto getMatrixOp = [](const nvinfer1::ITensor&amp; input, bool transpose) {
      |                                   ^
cc1plus: note:   ‘onnx2trt::{anonymous}::importGemm(onnx2trt::IImporterContext*, const onnx2trt_onnx::NodeProto&amp;, std::vector&lt;onnx2trt::TensorOrWeights&gt;&amp;)::&lt;lambda(const nvinfer1::ITensor&amp;, bool)&gt;’ is a closure type, which is only literal in C++17 and later
/root/onnx-tensorrt/builtin_op_importers.cpp: In function ‘onnx2trt::NodeImportResult onnx2trt::{anonymous}::importMatMul(onnx2trt::IImporterContext*, const onnx2trt_onnx::NodeProto&amp;, std::vector&lt;onnx2trt::TensorOrWeights&gt;&amp;)’:
/root/onnx-tensorrt/builtin_op_importers.cpp:1302:20: error: the type ‘const onnx2trt::{anonymous}::importMatMul(onnx2trt::IImporterContext*, const onnx2trt_onnx::NodeProto&amp;, std::vector&lt;onnx2trt::TensorOrWeights&gt;&amp;)::&lt;lambda(const nvinfer1::ITensor&amp;)&gt;’ of ‘constexpr’ variable ‘getMatrixOp’ is not literal
 1302 |     constexpr auto getMatrixOp = [](const nvinfer1::ITensor&amp; input) {
      |                    ^~~~~~~~~~~
/root/onnx-tensorrt/builtin_op_importers.cpp:1302:35: note: ‘onnx2trt::{anonymous}::importMatMul(onnx2trt::IImporterContext*, const onnx2trt_onnx::NodeProto&amp;, std::vector&lt;onnx2trt::TensorOrWeights&gt;&amp;)::&lt;lambda(const nvinfer1::ITensor&amp;)&gt;’ is not literal because:
 1302 |     constexpr auto getMatrixOp = [](const nvinfer1::ITensor&amp; input) {
      |                                   ^
cc1plus: note:   ‘onnx2trt::{anonymous}::importMatMul(onnx2trt::IImporterContext*, const onnx2trt_onnx::NodeProto&amp;, std::vector&lt;onnx2trt::TensorOrWeights&gt;&amp;)::&lt;lambda(const nvinfer1::ITensor&amp;)&gt;’ is a closure type, which is only literal in C++17 and later
make[2]: *** [CMakeFiles/nvonnxparser.dir/builtin_op_importers.cpp.o] Error 1
make[2]: *** Waiting for unfinished jobs....
[ 50%] Building CXX object CMakeFiles/nvonnxparser_static.dir/builtin_op_importers.cpp.o
/root/onnx-tensorrt/builtin_op_importers.cpp: In function ‘onnx2trt::NodeImportResult onnx2trt::{anonymous}::importGemm(onnx2trt::IImporterContext*, const onnx2trt_onnx::NodeProto&amp;, std::vector&lt;onnx2trt::TensorOrWeights&gt;&amp;)’:
/root/onnx-tensorrt/builtin_op_importers.cpp:863:20: error: the type ‘const onnx2trt::{anonymous}::importGemm(onnx2trt::IImporterContext*, const onnx2trt_onnx::NodeProto&amp;, std::vector&lt;onnx2trt::TensorOrWeights&gt;&amp;)::&lt;lambda(const nvinfer1::ITensor&amp;, bool)&gt;’ of ‘constexpr’ variable ‘getMatrixOp’ is not literal
  863 |     constexpr auto getMatrixOp = [](const nvinfer1::ITensor&amp; input, bool transpose) {
      |                    ^~~~~~~~~~~
/root/onnx-tensorrt/builtin_op_importers.cpp:863:35: note: ‘onnx2trt::{anonymous}::importGemm(onnx2trt::IImporterContext*, const onnx2trt_onnx::NodeProto&amp;, std::vector&lt;onnx2trt::TensorOrWeights&gt;&amp;)::&lt;lambda(const nvinfer1::ITensor&amp;, bool)&gt;’ is not literal because:
  863 |     constexpr auto getMatrixOp = [](const nvinfer1::ITensor&amp; input, bool transpose) {
      |                                   ^
cc1plus: note:   ‘onnx2trt::{anonymous}::importGemm(onnx2trt::IImporterContext*, const onnx2trt_onnx::NodeProto&amp;, std::vector&lt;onnx2trt::TensorOrWeights&gt;&amp;)::&lt;lambda(const nvinfer1::ITensor&amp;, bool)&gt;’ is a closure type, which is only literal in C++17 and later
/root/onnx-tensorrt/builtin_op_importers.cpp: In function ‘onnx2trt::NodeImportResult onnx2trt::{anonymous}::importMatMul(onnx2trt::IImporterContext*, const onnx2trt_onnx::NodeProto&amp;, std::vector&lt;onnx2trt::TensorOrWeights&gt;&amp;)’:
/root/onnx-tensorrt/builtin_op_importers.cpp:1302:20: error: the type ‘const onnx2trt::{anonymous}::importMatMul(onnx2trt::IImporterContext*, const onnx2trt_onnx::NodeProto&amp;, std::vector&lt;onnx2trt::TensorOrWeights&gt;&amp;)::&lt;lambda(const nvinfer1::ITensor&amp;)&gt;’ of ‘constexpr’ variable ‘getMatrixOp’ is not literal
 1302 |     constexpr auto getMatrixOp = [](const nvinfer1::ITensor&amp; input) {
      |                    ^~~~~~~~~~~
/root/onnx-tensorrt/builtin_op_importers.cpp:1302:35: note: ‘onnx2trt::{anonymous}::importMatMul(onnx2trt::IImporterContext*, const onnx2trt_onnx::NodeProto&amp;, std::vector&lt;onnx2trt::TensorOrWeights&gt;&amp;)::&lt;lambda(const nvinfer1::ITensor&amp;)&gt;’ is not literal because:
 1302 |     constexpr auto getMatrixOp = [](const nvinfer1::ITensor&amp; input) {
      |                                   ^
cc1plus: note:   ‘onnx2trt::{anonymous}::importMatMul(onnx2trt::IImporterContext*, const onnx2trt_onnx::NodeProto&amp;, std::vector&lt;onnx2trt::TensorOrWeights&gt;&amp;)::&lt;lambda(const nvinfer1::ITensor&amp;)&gt;’ is a closure type, which is only literal in C++17 and later
make[1]: *** [CMakeFiles/nvonnxparser.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....
[ 53%] Building CXX object CMakeFiles/nvonnxparser_static.dir/onnx2trt_utils.cpp.o
make[2]: *** [CMakeFiles/nvonnxparser_static.dir/builtin_op_importers.cpp.o] Error 1
make[2]: *** Waiting for unfinished jobs....
make[1]: *** [CMakeFiles/nvonnxparser_static.dir/all] Error 2
make: *** [all] Error 2

&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='Legends0' date='2020-05-06T13:25:52Z'>
		One can downgrade to gcc-7, and that will work. Just as an option until this is fixed.
		</comment>
		<comment id='3' author='Legends0' date='2020-05-19T11:00:31Z'>
		Removing the s (not valid in C++11/14) does the trick (fix for branch 7.0: &lt;denchmark-link:https://github.com/onnx/onnx-tensorrt/commit/c5b696e0a830790d763981a597846d9952068b88&gt;c5b696e&lt;/denchmark-link&gt;
).
Just discovered that  already contains a fix in commit &lt;denchmark-link:https://github.com/onnx/onnx-tensorrt/commit/5e3f5d02ba40888b6e748341d6811628f649dea3&gt;5e3f5d0&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='4' author='Legends0' date='2020-10-14T16:00:48Z'>
		Closing since it's fixed.
		</comment>
	</comments>
</bug>