<bug id='2' author='ssfootball04' open_date='2019-05-02T08:03:47Z' closed_time='2019-05-03T07:46:45Z'>
	<summary>Reproducing Plain Model Baseline Accuracies</summary>
	<description>
Hi,
Thank you for releasing the code for your paper. Can you please clarify how to reproduce the accuracies for the plain model baseline on ImageNet-LT in Table 3 of your paper ? I'm running the following commands :
-&gt; python main.py --config ./config/ImageNet_LT/stage_1.py
-&gt; python main.py --config ./config/ImageNet_LT/stage_1.py --test
which gives me the following output :
Evaluation_accuracy_micro_top1: 0.119
Averaged F-measure: 0.108
Many_shot_accuracy_top1: 0.148 Median_shot_accuracy_top1: 0.112 Low_shot_accuracy_top1: 0.062
From the paper, the numbers should be :
Evaluation_accuracy_micro_top1: 0.209
Many_shot_accuracy_top1: 0.409 Median_shot_accuracy_top1: 0.107 Low_shot_accuracy_top1: 0.004
Stage 1 is simply the baseline resnet-10 training on the entire dataset, right, or am I missing something ?
	</description>
	<comments>
		<comment id='1' author='ssfootball04' date='2019-05-02T20:58:21Z'>
		Thank you very much for reporting this. We will take a look at this and get back to you as soon as possible!
		</comment>
		<comment id='2' author='ssfootball04' date='2019-05-03T03:09:49Z'>
		&lt;denchmark-link:https://github.com/ssfootball04&gt;@ssfootball04&lt;/denchmark-link&gt;

Sorry, I think you might need to reopen this issue. We tested both our release code and our testing code, it seems there are some bugs in our released version, which caused this issue. We thought this might was because of the learning rate, but it was not. Sorry for the bother, we will fix it as soon as possible.
Thank you very much.
		</comment>
		<comment id='3' author='ssfootball04' date='2019-05-03T05:52:26Z'>
		Thank you for looking into it, awaiting the fix :).
p.s - I don't have permission to reopen the issue
		</comment>
		<comment id='4' author='ssfootball04' date='2019-05-03T05:55:14Z'>
		Reopen the issue for the upcoming updates.
		</comment>
		<comment id='5' author='ssfootball04' date='2019-05-03T07:46:45Z'>
		&lt;denchmark-link:https://github.com/ssfootball04&gt;@ssfootball04&lt;/denchmark-link&gt;
 Thank you very much for your patience. The bug is fixed now. The first stage should work just like the paper reports. I forgot to put model.train() to the beginning of each epoch when cleaning the code. We are very sorry about this. I am also running stage 2 experiment. If there is any more changes, I will let you know. I think I will close this issue now. If you could not reproduce stage 2 results, please open a new issue. Thank you again for reporting.
		</comment>
		<comment id='6' author='ssfootball04' date='2019-05-04T00:13:15Z'>
		&lt;denchmark-link:https://github.com/ssfootball04&gt;@ssfootball04&lt;/denchmark-link&gt;
 And we changed the stage2 configuration file to 60 epochs as well so the stage 2 results are reproducible. Thank you very much!
		</comment>
		<comment id='7' author='ssfootball04' date='2019-07-02T21:32:27Z'>
		Hi! I am still getting slightly lower accuracies for both open and closed-set testing. Some possible places where I might be going wrong?
		</comment>
		<comment id='8' author='ssfootball04' date='2019-12-19T13:48:58Z'>
		&lt;denchmark-link:https://github.com/drcege&gt;@drcege&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ssfootball04&gt;@ssfootball04&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/sanjanamallya&gt;@sanjanamallya&lt;/denchmark-link&gt;
 Hello guys, as described in &lt;denchmark-link:https://github.com/zhmiao/OpenLongTailRecognition-OLTR/blob/master/run_networks.py#L297&gt;https://github.com/zhmiao/OpenLongTailRecognition-OLTR/blob/master/run_networks.py#L297&lt;/denchmark-link&gt;
 , We finally debugged the published code and current open set imagenet performance is:
============
Phase: test
Evaluation_accuracy_micro_top1: 0.361
Averaged F-measure: 0.501
Many_shot_accuracy_top1: 0.442 Median_shot_accuracy_top1: 0.352 Low_shot_accuracy_top1: 0.175
==========
This is higher than we reported in the paper. We updated some of the modules with clone() method, and set use_fc in the first stage to False. These changes will lead us to the proper results. Please have a try.
For Places, the current config won't work either. The reason why we could not get the reported results is that we forget that on the first stage, we actually did not freeze the weights. We only freeze the weights on the second stage. We will update the corresponding code as soon as possible.
		</comment>
		<comment id='9' author='ssfootball04' date='2020-02-11T21:56:15Z'>
		&lt;denchmark-link:https://github.com/drcege&gt;@drcege&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ssfootball04&gt;@ssfootball04&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/sanjanamallya&gt;@sanjanamallya&lt;/denchmark-link&gt;
 Hello, we have updated configuration files for Places as well. The current reproduced results are a little better than reported, please check out the updates. Thank you very much!.
		</comment>
	</comments>
</bug>