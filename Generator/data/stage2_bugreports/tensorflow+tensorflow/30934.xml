<bug id='30934' author='Kaynato' open_date='2019-07-22T19:13:34Z' closed_time='2019-08-06T00:27:26Z'>
	<summary>[BUG] [TF 2.0 Keras] Pointwise Conv2D numerically inconsistent in keras model vs manual run</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
Docker container: 2.0.0b1-gpu-py3-jupyter
CUDA/cuDNN version: CUDA 10.2
GPU model and memory: GeForce GTX 1080
Exact command to reproduce: See script below.

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

Wrapping an identical tf.nn.conv2d operation that has ksize of (1, 1) in a tf.keras.Model and calling the model, with or without predict, on identical data, produces different results.
The differences are small but accumulate through a deep network if many pointwise convs are used.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np

np.random.seed(123)
pool = np.ones([32, 64, 64, 64], 'float32')
w1 = np.random.randn(1, 1, 64, 64).astype('float32')

# Manual convolution
conv1 = tf.nn.conv2d(pool, w1, [1, 1], padding='SAME').numpy()

# The same convolution op via keras model
tmp_input = tf.keras.Input(shape=[64, 64, 64], dtype='float32')
tmp_out = tf.nn.conv2d(tmp_input, w1, [1, 1], padding='SAME)

# .predict can also be removed
conv2 = tf.keras.Model(inputs=tmp_input, outputs=tmp_out).predict(pool)

# The individual error is small, but it compounds through a deep network.
print('Disagreement between manual and keras-model wrapped conv:', np.abs(conv1 - conv2).sum())
&lt;/denchmark-code&gt;

This script can be run in a fresh pull of the docker container.
	</description>
	<comments>
		<comment id='1' author='Kaynato' date='2019-07-23T08:34:22Z'>
		I tried executing the code on Colab and I could able to reproduce the issue with TF 2.0.0.beta1. Please see the gist of &lt;denchmark-link:https://colab.research.google.com/drive/1fJaDnYtOjFjNklmmecfIXUIp_vKOO8aj&gt;Colab&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='2' author='Kaynato' date='2019-07-23T18:02:43Z'>
		&lt;denchmark-link:https://github.com/Kaynato&gt;@Kaynato&lt;/denchmark-link&gt;
 I think this was resolved in . Please check the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/39386ed6c56b2c16b6466cb41bb29228/tf_30934.ipynb&gt;here&lt;/denchmark-link&gt;
. I don't see any issues with tf-nightly-2.0. Thanks!
		</comment>
		<comment id='3' author='Kaynato' date='2019-07-24T20:41:48Z'>
		The issue is in fact not present on the CPU-only version, but still present in the latest GPU build.
I updated to tf-nightly-gpu-2.0-preview 2.0.0.dev20190724 and the issue is still present.
I care mostly about this issue in the gpu version of tensorflow - in fact I have a feeling that the discrepancy has to do with how data is passed to and from the GPU.
		</comment>
		<comment id='4' author='Kaynato' date='2019-07-25T04:01:38Z'>
		&lt;denchmark-link:https://github.com/Kaynato&gt;@Kaynato&lt;/denchmark-link&gt;
 Can you check my &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/af4efafb809c4558189b31a9463aa5a6/tf_30934.ipynb&gt;gist&lt;/denchmark-link&gt;
 and let me know what you think? When I ran the gist it results in  which means it working as expected right?
Could you create a gist and share it with the results. Thanks!
		</comment>
		<comment id='5' author='Kaynato' date='2019-07-25T15:06:20Z'>
		This gist works, but I can't replicate the fix on my local machine even with the same version of tensorflow. Maybe it has to do with the drivers, but it's unclear.
Here is the script, with a print at the beginning to verify that the version is in fact tf.nightly-gpu-2.0-preview==2.0.0.dev20190718
&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np

print('Tensorflow Version:', tf.__version__)

np.random.seed(123)
pool = np.ones([32, 64, 64, 64], 'float32')
w1 = np.random.randn(1, 1, 64, 64).astype('float32')

# Manual convolution
conv1 = tf.nn.conv2d(pool, w1, [1, 1], padding='SAME').numpy()

# The same convolution op via keras model
tmp_input = tf.keras.Input(shape=[64, 64, 64], dtype='float32')
tmp_out = tf.nn.conv2d(tmp_input, w1, [1, 1], padding='SAME)

# .predict can also be removed
conv2 = tf.keras.Model(inputs=tmp_input, outputs=tmp_out).predict(pool)

# The individual error is small, but it compounds through a deep network.
print('Disagreement between manual and keras-model wrapped conv:', np.abs(conv1 - conv2).sum())
&lt;/denchmark-code&gt;

Here is the output log I receive:
&lt;denchmark-code&gt;2019-07-25 14:59:44.966783: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2019-07-25 14:59:45.341260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:87:00.0
2019-07-25 14:59:45.343798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:88:00.0
2019-07-25 14:59:45.344169: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-07-25 14:59:45.346193: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-07-25 14:59:45.347898: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-07-25 14:59:45.348303: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-07-25 14:59:45.350655: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-07-25 14:59:45.352361: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-07-25 14:59:45.357594: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-07-25 14:59:45.364630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1
2019-07-25 14:59:45.365093: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-25 14:59:45.826040: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5ccd7c0 executing computations on platform CUDA. Devices:
2019-07-25 14:59:45.826109: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1080, Compute Capability 6.1
2019-07-25 14:59:45.826125: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): GeForce GTX 1080, Compute Capability 6.1
2019-07-25 14:59:45.851060: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200005000 Hz
2019-07-25 14:59:45.855590: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5d44940 executing computations on platform Host. Devices:
2019-07-25 14:59:45.855631: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;
2019-07-25 14:59:46.043201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:87:00.0
2019-07-25 14:59:46.043856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:88:00.0
2019-07-25 14:59:46.043903: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-07-25 14:59:46.043919: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-07-25 14:59:46.043932: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-07-25 14:59:46.043945: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-07-25 14:59:46.043957: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-07-25 14:59:46.043970: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-07-25 14:59:46.043984: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-07-25 14:59:46.046411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1
2019-07-25 14:59:46.046452: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-07-25 14:59:46.048207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-25 14:59:46.048223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 1 
2019-07-25 14:59:46.048231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N Y 
2019-07-25 14:59:46.048237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 1:   Y N 
2019-07-25 14:59:46.050196: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7606 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:87:00.0, compute capability: 6.1)
2019-07-25 14:59:46.051497: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7606 MB memory) -&gt; physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:88:00.0, compute capability: 6.1)
2019-07-25 14:59:49.843891: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-07-25 14:59:50.415257: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Tensorflow Version: 2.0.0-dev20190724
Disagreement between manual and keras-model wrapped conv: 5.8203125
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='Kaynato' date='2019-07-26T00:26:42Z'>
		&lt;denchmark-link:https://github.com/Kaynato&gt;@Kaynato&lt;/denchmark-link&gt;
 Thanks for your patience. Can you try with  which is what I used in my recent gist. Thanks!
		</comment>
		<comment id='7' author='Kaynato' date='2019-07-26T19:04:59Z'>
		Hello, thank you for your help. Though the gist works correctly in a colab instance, the gist is not reproducible on the local machine despite both being clean installations and fresh container pulls.
Either the issue is likely a driver / hardware bug and so at least should be made known (that perhaps some version of CUDA or CUDNN causes issues here, or that there is something else going on.
In either case, it is unexpected behavior.
To demonstrate that I am using the correct version of tensorflow, I recorded the procedure on the local machine and demonstrate that the inconsistency is still present.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3437068/screencapture.zip&gt;screencapture.zip&lt;/denchmark-link&gt;

The device information is the same as before and described in the more verbose log in my previous comment here.
		</comment>
		<comment id='8' author='Kaynato' date='2019-07-26T22:28:06Z'>
		&lt;denchmark-link:https://github.com/Kaynato&gt;@Kaynato&lt;/denchmark-link&gt;
 I don't know what else we can do to find root-cause of this issue as we cannot access your local machine. But surely we can say this is not related to Tensorflow. May be downgrading to CUDA10.0 and test again might help i think. Thanks!
		</comment>
		<comment id='9' author='Kaynato' date='2019-08-06T00:27:26Z'>
		Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
		<comment id='10' author='Kaynato' date='2019-08-06T00:27:28Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=30934&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=30934&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>