<bug id='30143' author='durandg12' open_date='2019-06-25T18:04:41Z' closed_time='2019-07-01T20:52:03Z'>
	<summary>.fit method fails with unnamed Input layer whereas training works with a custom function</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6
TensorFlow installed from (source or binary): from pip install
TensorFlow version (use command below): v1.12.1-3259-gf59745a381 2.0.0-beta0
Python version: v3.6.7:6ec5cf24b7, Oct 20 2018, 03:02:14

Describe the current behavior
I use a model written with the Functional API which uses a DenseFeatures layer. The call method of the model and a custom training function work well even if the Input layer is unnamed, but the fit method fails. This forces me to name the Input layer the same name as the key of the dictionary given as input. I am note sure if this is intended or not, but as this works with a custom training function I tend to think it may be a bug in the fit method.
Code to reproduce the issue
Here is a minimal working example reproducing the issue.
&lt;denchmark-code&gt;import tensorflow as tf
print('Using Tensorflow version {} (git version {})'.format(tf.version.VERSION, tf.version.GIT_VERSION))
import numpy as np
from tensorflow.data import Dataset
from tensorflow.feature_column import numeric_column
from tensorflow.keras import Input, Model
from tensorflow.keras.layers import DenseFeatures, Dense
from tensorflow.keras.losses import MeanSquaredError
from tensorflow.keras.optimizers import Adam

def make_model():
    fc1 = numeric_column('fc_name')
    dict_input = {'fc_name': Input(1)}
    
    out = DenseFeatures(fc1)(dict_input)
    out = Dense(5, name='dense_feature1')(out)
    out = Dense(1, name='dense_ouput')(out)
    
    return Model(inputs=dict_input, outputs=out)

array = np.ones((1000,1), dtype=np.float)
array_target = np.ones((1000,1), dtype=np.float)

batch_size = 4
dict_array = {'teddy_bear': array}
input_dataset = Dataset.from_tensor_slices(dict_array).batch(batch_size)
target_dataset = Dataset.from_tensor_slices(array_target).batch(batch_size)
complete_dataset = Dataset.zip((input_dataset, target_dataset)).shuffle(10000)

model = make_model()
#model.summary()
for x, y in complete_dataset.take(1):
    print(model(x))
    
loss_fn = MeanSquaredError()
optimizer = Adam(learning_rate=1e-3)

@tf.function
def train_step(inputs, target):
    with tf.GradientTape() as tape:
        outputs = model(inputs)
        loss = loss_fn(target, outputs)
    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))
    return loss

EPOCHS = 5
loss = 0
for epoch in range(EPOCHS):
    for x, y in complete_dataset:
        loss = train_step(x, y)
    print('Epoch n°{:d}, loss = {:5.4f}'.format(epoch + 1, loss))
for x, y in complete_dataset.take(1):
    print(model(x))
    
model = make_model()
model.summary()
model.compile(optimizer, loss_fn)
model.fit(complete_dataset, epochs=EPOCHS)
for x, y in complete_dataset.take(1):
    print(model(x))
&lt;/denchmark-code&gt;

The output of which is:
&lt;denchmark-code&gt;Using Tensorflow version 2.0.0-beta0 (git version v1.12.1-3259-gf59745a381)
tf.Tensor(
[[-0.18571138]
 [-0.18571138]
 [-0.18571138]
 [-0.18571138]], shape=(4, 1), dtype=float32)
Epoch n°1, loss = 0.0035
Epoch n°2, loss = 0.0000
Epoch n°3, loss = 0.0000
Epoch n°4, loss = 0.0000
Epoch n°5, loss = 0.0000
tf.Tensor(
[[0.99999917]
 [0.99999917]
 [0.99999917]
 [0.99999917]], shape=(4, 1), dtype=float32)
Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
dense_features_1 (DenseFeatu (None, 1)                 0         
_________________________________________________________________
dense_feature1 (Dense)       (None, 5)                 10        
_________________________________________________________________
dense_ouput (Dense)          (None, 1)                 6         
=================================================================
Total params: 16
Trainable params: 16
Non-trainable params: 0
_________________________________________________________________
Epoch 1/5

---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
~/Documents/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)
    446           if data[x].__class__.__name__ == 'DataFrame' else data[x]
--&gt; 447           for x in names
    448       ]

~/Documents/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py in &lt;listcomp&gt;(.0)
    446           if data[x].__class__.__name__ == 'DataFrame' else data[x]
--&gt; 447           for x in names
    448       ]

KeyError: 'input_2'

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
&lt;ipython-input-1-d41b98ef4a37&gt; in &lt;module&gt;
     59 model.summary()
     60 model.compile(optimizer, loss_fn)
---&gt; 61 model.fit(complete_dataset, epochs=EPOCHS)
     62 for x, y in complete_dataset.take(1):
     63     print(model(x))

~/Documents/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    641         max_queue_size=max_queue_size,
    642         workers=workers,
--&gt; 643         use_multiprocessing=use_multiprocessing)
    644 
    645   def evaluate(self,

~/Documents/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
    692         shuffle=shuffle,
    693         initial_epoch=initial_epoch,
--&gt; 694         steps_name='steps_per_epoch')
    695 
    696   def evaluate(self,

~/Documents/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)
    262 
    263       is_deferred = not model._is_compiled
--&gt; 264       batch_outs = batch_function(*batch_data)
    265       if not isinstance(batch_outs, list):
    266         batch_outs = [batch_outs]

~/Documents/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)
    894     x, y, sample_weights = self._standardize_user_data(
    895         x, y, sample_weight=sample_weight, class_weight=class_weight,
--&gt; 896         extract_tensors_from_dataset=True)
    897 
    898     # If `self._distribution_strategy` is True, then we are in a replica context

~/Documents/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)
   2426           feed_input_shapes,
   2427           check_batch_axis=False,  # Don't enforce the batch size.
-&gt; 2428           exception_prefix='input')
   2429 
   2430     if y is not None:

~/Documents/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)
    449     except KeyError as e:
    450       raise ValueError('No data provided for "' + e.args[0] + '". Need data '
--&gt; 451                        'for each key in: ' + str(names))
    452   elif isinstance(data, (list, tuple)):
    453     if isinstance(data[0], (list, tuple)):

ValueError: No data provided for "input_2". Need data for each key in: ['input_2']

&lt;/denchmark-code&gt;

We see on the output that the call method of the model works and that the training with  the custom function also works. We also see in the error log that the error is due to the Input layer, named input_2, not finding the key input_2 in the dictionaries produced by complete_dataset.
Now, in the code above, if we replace the line dict_input = {'fc_name': Input(1)} by dict_input = {'fc_name': Input(1, name='teddy_bear')}, everything works and the output looks like this:
&lt;denchmark-code&gt;Using Tensorflow version 2.0.0-beta0 (git version v1.12.1-3259-gf59745a381)
tf.Tensor(
[[0.7686093]
 [0.7686093]
 [0.7686093]
 [0.7686093]], shape=(4, 1), dtype=float32)
Epoch n°1, loss = 0.0000
Epoch n°2, loss = 0.0000
Epoch n°3, loss = 0.0000
Epoch n°4, loss = 0.0000
Epoch n°5, loss = 0.0000
tf.Tensor(
[[0.99999994]
 [0.99999994]
 [0.99999994]
 [0.99999994]], shape=(4, 1), dtype=float32)
Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
teddy_bear (InputLayer)      [(None, 1)]               0         
_________________________________________________________________
dense_features_1 (DenseFeatu (None, 1)                 0         
_________________________________________________________________
dense_feature1 (Dense)       (None, 5)                 10        
_________________________________________________________________
dense_ouput (Dense)          (None, 1)                 6         
=================================================================
Total params: 16
Trainable params: 16
Non-trainable params: 0
_________________________________________________________________
Epoch 1/5
250/250 [==============================] - 1s 2ms/step - loss: 0.0325
Epoch 2/5
250/250 [==============================] - 0s 1ms/step - loss: 3.5115e-14
Epoch 3/5
250/250 [==============================] - 0s 2ms/step - loss: 1.4211e-14
Epoch 4/5
250/250 [==============================] - 0s 1ms/step - loss: 1.4211e-14
Epoch 5/5
250/250 [==============================] - 0s 1ms/step - loss: 1.4211e-14
tf.Tensor(
[[1.0000002]
 [1.0000002]
 [1.0000002]
 [1.0000002]], shape=(4, 1), dtype=float32)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='durandg12' date='2019-06-26T10:01:40Z'>
		I have tried on colab with TF version 2.0 beta0 and was able to reproduce the issue.Thanks!
		</comment>
		<comment id='2' author='durandg12' date='2019-07-01T20:52:01Z'>
		In order to tie the incoming feature columns with the proper Inputs, you should pass in the same name as given to the feature column to the Input. We should document this better in the feature column guide (CC jbgordon@ who I believe is working on that), but this is working as intended.
		</comment>
		<comment id='3' author='durandg12' date='2019-07-01T20:52:04Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=30143&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=30143&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='durandg12' date='2019-07-01T20:52:40Z'>
		Sorry, meant &lt;denchmark-link:https://github.com/random-forests&gt;@random-forests&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='durandg12' date='2019-07-02T08:32:29Z'>
		
In order to tie the incoming feature columns with the proper Inputs, you should pass in the same name as given to the feature column to the Input.

As you see in my example, the feature column and the Input layer have different names and this works. On the one hand, the feature column and the dictionary key associated to the Input layer share one name, which is 'fc_name', on the other hand, the Input layer and the dictionary key associated to the input data share another name, which is 'teddy_bear'.

We should document this better in the feature column guide (CC jbgordon@ who I believe is working on that), but this is working as intended.

If this works as intended then can you explain me the difference of behaviour between using the fit method and the custom training method? I feel that I can understand why naming the Input layer the same as the input data is important but then I don't understand why it works with the custom function then.
Overall, I don't feel that you properly addressed the issue.
		</comment>
		<comment id='6' author='durandg12' date='2019-07-26T13:02:15Z'>
		Hi &lt;denchmark-link:https://github.com/karmel&gt;@karmel&lt;/denchmark-link&gt;
 do you have any comment on my last post here ?
Thanks in advance
		</comment>
		<comment id='7' author='durandg12' date='2019-08-12T22:55:50Z'>
		I am not sure exactly why it didn't fail, but I'm guessing it's more that it accidentally works, rather than an intentional implementation. In some cases, .fit is more restrictive, as we can't make assumptions about incoming data. If you want a precise answer as to why there is a difference, I would invite you to help us debug and determine what exactly the difference is in this case.
		</comment>
	</comments>
</bug>