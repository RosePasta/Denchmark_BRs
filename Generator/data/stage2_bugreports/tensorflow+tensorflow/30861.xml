<bug id='30861' author='liuyanqi' open_date='2019-07-18T20:20:05Z' closed_time='2019-11-15T21:59:40Z'>
	<summary>representative_dataset error for TFlite converter quantization</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.14
Python version: 3.6.8
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 10.0
GPU model and memory: Titan Xp, 12GB

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
My representative_data_gen() iterate through a dataset that i created with some custom images and I set converter.representative_dataset with the function and convert the frozen model to tflite with int8 quantization. It produces the following error:

Traceback (most recent call last):
File "./src/freeze_graph.py", line 100, in 
tflite_quant_model = converter.convert()
File "/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py", line 908, in convert
inference_output_type)
File "/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py", line 200, in _calibrate_quantize_model
inference_output_type, allow_float)
File "/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/calibrator.py", line 76, in calibrate_and_quantize
self._calibrator.FeedTensor(calibration_sample)
File "/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py", line 112, in FeedTensor
return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_FeedTensor(self, input_value)
ValueError: Cannot set tensor: Got tensor of type STRING but expected type FLOAT32 for input 98, name: image_input

I tried to only quantize the weights with tf.lite.Optimized set to OPTIMIZE_FOR_SIZE and without setting representative_dataset option. It can produce tflite model successfully.
Describe the expected behavior
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
&lt;denchmark-code&gt;train = []
path = '/home/liuyanqi/caffe/pyramid_cnn/data/adversarial'
for i in range(1, 11):
    filename = os.path.join(path, 'exp{:03d}_B'.format(i), 'scenergb.jpg' )
    im = cv2.imread(filename)
    im = im.astype(np.float32, copy=False)
    input_image = im - mc.BGR_MEANS
    train.append(input_image)

train = tf.convert_to_tensor(np.array(train, dtype='float32'))
my_ds = tf.data.Dataset.from_tensor_slices((train)).batch(1)

#POST TRAINING QUANTIZATION
def representative_dataset_gen():
    for input_value in my_ds.take(10):
        yield [input_value]



graph_def_file = '/tmp/logs/+zynqDet+tmp/train/freeze_graph.pbtxt'
input_arrays = ["image_input"]
# # output_arrays = ["probability/final_class_prob_concat","IOU_1/det_boxes_concat"]
output_arrays = ["predictor/bias_add","predictor_1/bias_add", "predictor_2/bias_add"]
converter = tf.lite.TFLiteConverter.from_frozen_graph(
    graph_def_file, input_arrays, output_arrays, input_shapes={input_arrays[0]:[1, 480, 640, 3]})
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset_gen
tflite_quant_model = converter.convert()
&lt;/denchmark-code&gt;

Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='liuyanqi' date='2019-07-24T06:31:44Z'>
		I met the same problem before, you can try add this tf. Enable_eager_execution()  at the beginning.
		</comment>
		<comment id='2' author='liuyanqi' date='2019-07-24T17:09:21Z'>
		&lt;denchmark-link:https://github.com/eric4337&gt;@eric4337&lt;/denchmark-link&gt;
 Wow. I'm not sure why but this solves the problem.
I visualize the generated tflite model using netron. Can anyone explain why it only insert Quantize layer after a certain convolution layer?
&lt;denchmark-link:https://user-images.githubusercontent.com/7076268/61813651-3dbebc80-ae14-11e9-8108-3521e49e71b0.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='liuyanqi' date='2019-07-25T02:06:20Z'>
		U need 'yield [sess.run(input value)]', or it's still a tensor
		</comment>
		<comment id='4' author='liuyanqi' date='2019-08-09T09:40:00Z'>
		&lt;denchmark-link:https://github.com/eric4337&gt;@eric4337&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/liuyanqi&gt;@liuyanqi&lt;/denchmark-link&gt;
 can you share you solution ? when I reproduce the demo ,i always get the error:Process finished with exit code 139 (interrupted by signal 11: SIGSEGV),any help ?
		</comment>
		<comment id='5' author='liuyanqi' date='2019-08-20T12:35:43Z'>
		
@eric4337 @liuyanqi can you share you solution ? when I reproduce the demo ,i always get the error:Process finished with exit code 139 (interrupted by signal 11: SIGSEGV),any help ?

Got the same problem with tf1.14!
		</comment>
		<comment id='6' author='liuyanqi' date='2019-10-08T23:34:39Z'>
		
@eric4337 Wow. I'm not sure why but this solves the problem.
I visualize the generated tflite model using netron. Can anyone explain why it only insert Quantize layer after a certain convolution layer?


Any update on why there is a Quantize node?
		</comment>
		<comment id='7' author='liuyanqi' date='2019-11-15T21:59:40Z'>
		When we concat two tensors, we need to make sure they use the same quantization range so that we can append the values together as is. The Quantize() op there is to adapt the range. The datatype remains as int8 after that op.
		</comment>
		<comment id='8' author='liuyanqi' date='2019-11-15T21:59:42Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30861&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30861&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='liuyanqi' date='2020-06-04T23:24:07Z'>
		
@eric4337 Wow. I'm not sure why but this solves the problem.
I visualize the generated tflite model using netron. Can anyone explain why it only insert Quantize layer after a certain convolution layer?


Hi, could you please share that how did you get the quantized graph?
Looking forward to it!
		</comment>
		<comment id='10' author='liuyanqi' date='2020-08-20T17:10:57Z'>
		&lt;denchmark-link:https://github.com/Ekta246&gt;@Ekta246&lt;/denchmark-link&gt;
 With &lt;denchmark-link:https://lutzroeder.github.io/netron/&gt;netron&lt;/denchmark-link&gt;
 download &amp; drag the  to the web/desktop app , hope the answer is not too late :)
		</comment>
		<comment id='11' author='liuyanqi' date='2020-10-09T02:54:49Z'>
		
When we concat two tensors, we need to make sure they use the same quantization range so that we can append the values together as is. The Quantize() op there is to adapt the range. The datatype remains as int8 after that op.

&lt;denchmark-link:https://github.com/liyunlu0618&gt;@liyunlu0618&lt;/denchmark-link&gt;

I have met the same problem about int8-quantization before concat。The trouble is that the int8-&gt;int8 Quantize op can not be supported by nnapi-dsp delegate。It means that the graph will be splited to several sub-graphs to run on nnapi-dsp and will take much more time to do the inference. So does the int8-&gt;int8 Quantize op is necessory, even though the two concat tensors are all from conv+relu and have the same data range? Or May I just modify the tflite model to remove the Quantize op using some tools, like coremltools to modify the apple's mlmodel?
		</comment>
	</comments>
</bug>