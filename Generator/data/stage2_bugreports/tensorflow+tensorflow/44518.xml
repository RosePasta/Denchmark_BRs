<bug id='44518' author='yoshihikoueno' open_date='2020-11-02T12:07:28Z' closed_time='2020-11-24T00:21:15Z'>
	<summary>Some variables are not restored only when MirroredStragy is used.</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): official docker image &lt;tensorflow/tensorflow:2.3.1-gpu&gt;
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): I've tried every version (&gt;=2.3.0) and I could reproduce this bug in each of them.
Python version: I could reproduce in 3.6, 3.7, and 3.8
GPU model and memory: Not relevant. Could reproduce this issue with and without GPU.

Description
tf.keras.models.Model.load_weights method doesn't restore the optimizer slot variables only when MirroredStrategy is used.
Here's the minimum code to demonstrate this bug.
import tensorflow as tf


def prepare_model():
    model = tf.keras.models.Sequential([
        tf.keras.layers.Dense(1),
    ])
    model.compile(optimizer='adam', loss='mse')
    model.build([None, 1])
    return model


def print_opt_weights(tag):
    print(tag, list(map(lambda x: x.name, model.optimizer.weights)))
    return


labels = inputs = tf.random.uniform([5, 1])
model = prepare_model()
print_opt_weights('After model creation:')
model.fit(x=inputs, y=labels, batch_size=1)
print_opt_weights('After training:')
model.save_weights('save')

print('------------Without Distribute------------')
model = prepare_model()
print_opt_weights('After model creation:')
status = model.load_weights('save')
print_opt_weights('After load weight:')
# model.fit(x=inputs, y=labels, batch_size=1)
# print_opt_weights('After retraining:')
# status.assert_consumed()

print('------------With Distribute------------')
with tf.distribute.MirroredStrategy().scope():
    model = prepare_model()
    print_opt_weights('After model creation:')
    status = model.load_weights('save')
    print_opt_weights('After load weight:')
    # model.fit(x=inputs, y=labels, batch_size=1)
    # print_opt_weights('After retraining:')
    # status.assert_consumed()


'''
The commented out lines will make optimizers in
&lt;without distribute&gt; and &lt;with distribute&gt;
have the same number of weights.

Upon `load_weights` call, all the slot variables in the optimizer
will be created when it's not using distribution strategy,
while they are not created until the model is actually retrained
with a distribution strategy in use.
'''
The above snippet will produce the output like below:
&lt;denchmark-code&gt;After model creation: []
After training: ['Adam/iter:0', 'Adam/dense/kernel/m:0', 'Adam/dense/bias/m:0', 'Adam/dense/kernel/v:0', 'Adam/dense/bias/v:0']
------------Without Distribute------------
After model creation: []
After load weight: ['dense_1/kernel/m:0', 'dense_1/kernel/v:0', 'dense_1/bias/m:0', 'dense_1/bias/v:0']
------------With Distribute------------
After model creation: []
After load weight: []
&lt;/denchmark-code&gt;

As we can see from the output, the optimizer slot variables are correctly restored without MirroredStrategy, while they are not restored with MirroredStrategy.
In my code, I was calling assert_consumed method just after calling load_weights to make sure everything is loaded correctly before proceeding to any other operations. And due to this unexpected difference in the behavior, it was crushing when it uses MirroredStrategy.
In either case (with/without distribution strategy), all the variables are (probably) restored after retraining (calling fit again).
So the easiest solution will be to just ignore this difference, but I want to know what makes this difference and how we can fix this.
Thank you so much for taking the time to take a look at this issue.
Any comments or suggestions will be very helpful.
	</description>
	<comments>
		<comment id='1' author='yoshihikoueno' date='2020-11-05T06:58:53Z'>
		&lt;denchmark-link:https://github.com/nikitamaia&gt;@nikitamaia&lt;/denchmark-link&gt;
 Hi! I would like to kindly ask you if you could have some time to take a look at this issue.
		</comment>
		<comment id='2' author='yoshihikoueno' date='2020-11-05T23:29:38Z'>
		This is (unfortunately) due to some historical constraints that optimizer slot variables are not immediately created in  when a strategy is effective. This &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/bc01b08d2480fe25dec20dd8f307c4b8c1f1478b/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L1308&gt;comment&lt;/denchmark-link&gt;
 sheds some details. This won't affect correctness as the slot variables will be restored after retraining. If you'd like to do some verification on loading, consider using .
I want to see if the constraint can be removed so the behavior is consistent w/ and w/o strategy, will post once there is a conclusion.
		</comment>
		<comment id='3' author='yoshihikoueno' date='2020-11-18T02:21:02Z'>
		The follow up, it turns out the behavior can't be changed. It is to allow use cases where  is put outside distribution strategy scope, so if slot variables are immediately created, they will not be created within the distribution strategy scope. If using distribution strategy, all variables must be created under the scope. See &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#scope&gt;https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#scope&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='4' author='yoshihikoueno' date='2020-11-18T02:21:06Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44518&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44518&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='yoshihikoueno' date='2020-11-18T03:00:15Z'>
		Thank you for taking the time to investigate this.
But there is one thing that I don't see. What would be a reason to delay variable creation when load_weights is called inside a distribution strategy as I show in the example code? I think it is possible to create them immediately in this case and it should, as It would make the behavior consistent.
What do you think?
		</comment>
		<comment id='6' author='yoshihikoueno' date='2020-11-18T18:11:55Z'>
		That's a good call. In fact giving it a second thought, we should be able to allow calling load_weights outside the strategy scope and meanwhile creating the slot variable immediately, because the optimizer is created under a scope already and the scope is captured inside the optimizer.
I'll send out a fix for this soon.
		</comment>
		<comment id='7' author='yoshihikoueno' date='2020-11-18T23:31:30Z'>
		&lt;denchmark-link:https://github.com/ckkuang&gt;@ckkuang&lt;/denchmark-link&gt;
 Glad to hear that! I would really appreciate the fix.
		</comment>
		<comment id='8' author='yoshihikoueno' date='2020-11-24T00:21:14Z'>
		This should have been fixed at tf-nightly. Thanks!
		</comment>
		<comment id='9' author='yoshihikoueno' date='2020-11-24T00:21:16Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44518&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44518&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='yoshihikoueno' date='2020-11-24T01:56:09Z'>
		&lt;denchmark-link:https://github.com/ckkuang&gt;@ckkuang&lt;/denchmark-link&gt;
 Thank you so much for the quick fix!!
		</comment>
	</comments>
</bug>