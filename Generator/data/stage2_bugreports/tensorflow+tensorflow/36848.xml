<bug id='36848' author='ankitesh97' open_date='2020-02-18T05:28:31Z' closed_time='2020-06-22T19:30:19Z'>
	<summary>Not able to use 'training' argument in the call method for a custom layer.</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information :
OS - CentOs
tensorflow-gpu = 2.1.0
tensorflow = 2.0.0
installed in the conda version - conda version : 4.8.1
using cuda - CUDA Version: 10.1
Gpu - Tesla K80
python - 3.6
Describe the current behavior
Not able to use the 'training' flag when designing a custom layer. Getting error saying that
&lt;denchmark-code&gt;operatorNotAllowedInGraphError            Traceback (most recent call last)
~/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    841                   with auto_control_deps.AutomaticControlDependencies() as acd:
--&gt; 842                     outputs = call_fn(cast_inputs, *args, **kwargs)
    843                     # Wrap Tensors in `outputs` in `tf.identity` to avoid

~/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    236         if hasattr(e, 'ag_error_metadata'):
--&gt; 237           raise e.ag_error_metadata.to_exception(e)
    238         else:

OperatorNotAllowedInGraphError: in converted code:

    &lt;ipython-input-26-3ff47d389914&gt;:43 call
        if not training:
    /home/ankitesh/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:765 __bool__
        self._disallow_bool_casting()
    /home/ankitesh/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:534 _disallow_bool_casting
        self._disallow_in_graph_mode("using a `tf.Tensor` as a Python `bool`")
    /home/ankitesh/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:523 _disallow_in_graph_mode
        " this function with @tf.function.".format(task))

    OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.
&lt;/denchmark-code&gt;

Describe the expected behavior
It should not give an error
Code to reproduce the issue Provide a reproducible test case that is the
bare minimum necessary to generate the problem.
This is the class that I made
from tensorflow.keras import initializers
from tensorflow.keras import layers


class CustomBatchNormalization(layers.Layer):
    def __init__(self, momentum=0.99, epsilon=1e-3,beta_initializer='zeros',
                 gamma_initializer='ones', moving_mean_initializer='zeros',
                 moving_range_initializer='ones',**kwargs):
        self.momentum = momentum
        self.epsilon = epsilon
        self.beta_initializer = initializers.get(beta_initializer)
        self.gamma_initializer = initializers.get(gamma_initializer)
        self.moving_mean_initializer = initializers.get(moving_mean_initializer)
        self.moving_range_initializer = (
            initializers.get(moving_range_initializer))
        
        super().__init__(**kwargs)
    
    def build(self,input_shape):
        dim = input_shape[-1]
        shape = (dim,)
        self.gamma = self.add_weight(shape=shape,
                             name='gamma',
                             initializer=self.gamma_initializer,trainable=True)
        self.beta = self.add_weight(shape=shape,
                            name='beta',
                            initializer=self.beta_initializer,
                                   trainable=True)
        
        self.moving_mean = self.add_weight(
            shape=shape,
            name='moving_mean',
            initializer=self.moving_mean_initializer,
            trainable=False)
        
        self.moving_range = self.add_weight(
            shape=shape,
            name='moving_range',
            initializer=self.moving_range_initializer,
            trainable=False)



    def call(self, inputs, training=None):
        input_shape = inputs.shape
        
        if not training:
            scaled = (inputs-self.moving_mean)/(self.moving_range+self.epsilon)
            return self.gamma*scaled + self.beta
        
        mean = tf.math.reduce_mean(inputs,axis=0)
        maxr = tf.math.reduce_max(inputs,axis=0)
        minr = tf.math.reduce_min(inputs,axis=0)
        
        range_diff = tf.math.subtract(maxr,minr)
        self.moving_mean = tf.math.add(self.momentum*self.moving_mean, (1-self.momentum)*mean)
        self.moving_range = tf.math.add(self.momentum*self.moving_range,(1-self.momentum)*range_diff)
        scaled = tf.math.divide(tf.math.subtract(inputs,mean),(range_diff+self.epsilon))
        return tf.math.add(tf.math.multiply(self.gamma,scaled),self.beta)
    
    def get_config(self):
        config = {
            'momentum': self.momentum,
            'epsilon': self.epsilon,
            'beta_initializer': initializers.serialize(self.beta_initializer),
            'gamma_initializer': initializers.serialize(self.gamma_initializer),
            'moving_mean_initializer':
                initializers.serialize(self.moving_mean_initializer),
            'moving_range_initializer':
                initializers.serialize(self.moving_range_initializer)
        }
        base_config = super(CustomBatchNormalization, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))
    
    def compute_output_shape(self, input_shape):
        return input_shape




##Below is the network 

inp = Input(shape=(64,))
batch_norm_1 = CustomBatchNormalization()(inp)
densout = Dense(128, activation='linear')(batch_norm_1)
densout = LeakyReLU(alpha=0.3)(densout)
for i in range (6):
    batch_norm_i = CustomBatchNormalization()(densout)
    densout = Dense(128, activation='linear')(batch_norm_i)
    densout = LeakyReLU(alpha=0.3)(densout)
batch_norm_out = CustomBatchNormalization()(densout)
out = Dense(64, activation='linear')(batch_norm_out)
Inp_RH_CBN = tf.keras.models.Model(inp, out)
Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
Here is the full log error
&lt;denchmark-code&gt;OperatorNotAllowedInGraphError            Traceback (most recent call last)
~/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    841                   with auto_control_deps.AutomaticControlDependencies() as acd:
--&gt; 842                     outputs = call_fn(cast_inputs, *args, **kwargs)
    843                     # Wrap Tensors in `outputs` in `tf.identity` to avoid

~/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    236         if hasattr(e, 'ag_error_metadata'):
--&gt; 237           raise e.ag_error_metadata.to_exception(e)
    238         else:

OperatorNotAllowedInGraphError: in converted code:

    &lt;ipython-input-26-3ff47d389914&gt;:43 call
        if not training:
    /home/ankitesh/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:765 __bool__
        self._disallow_bool_casting()
    /home/ankitesh/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:534 _disallow_bool_casting
        self._disallow_in_graph_mode("using a `tf.Tensor` as a Python `bool`")
    /home/ankitesh/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:523 _disallow_in_graph_mode
        " this function with @tf.function.".format(task))

    OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.


During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
&lt;ipython-input-27-3d2d9055924b&gt; in &lt;module&gt;
      1 inp = Input(shape=(64,))
----&gt; 2 batch_norm_1 = CustomBatchNormalization()(inp)
      3 densout = Dense(128, activation='linear')(batch_norm_1)
      4 densout = LeakyReLU(alpha=0.3)(densout)
      5 for i in range (6):

~/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    852                               'dynamic. Pass `dynamic=True` to the class '
    853                               'constructor.\nEncountered error:\n"""\n' +
--&gt; 854                               str(e) + '\n"""')
    855           else:
    856             # We will use static shape inference to return symbolic tensors

TypeError: You are attempting to use Python control flow in a layer that was not declared to be dynamic. Pass `dynamic=True` to the class constructor.
Encountered error:
"""
in converted code:

    &lt;ipython-input-26-3ff47d389914&gt;:43 call
        if not training:
    /home/ankitesh/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:765 __bool__
        self._disallow_bool_casting()
    /home/ankitesh/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:534 _disallow_bool_casting
        self._disallow_in_graph_mode("using a `tf.Tensor` as a Python `bool`")
    /home/ankitesh/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:523 _disallow_in_graph_mode
        " this function with @tf.function.".format(task))

    OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='ankitesh97' date='2020-02-18T09:11:41Z'>
		&lt;denchmark-link:https://github.com/ankitesh97&gt;@ankitesh97&lt;/denchmark-link&gt;

I tried to reproduce the issue in colab with Tensorflow-gpu = 2.1.0.However i am seeing the error message NameError: name 'Input' is not defined. Can you please help me with colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster.Thanks!
		</comment>
		<comment id='2' author='ankitesh97' date='2020-02-18T12:06:27Z'>
		Checkout the code below may helpful.
&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.python.ops import math_ops
from tensorflow.python.keras.utils import tf_utils
from tensorflow.python.keras import backend as K
Layer = layers.Layer

class SyncsBatchNormalization(Layer):
    def __init__(self,
                 axis: int = -1,
                 momentum=0.99,
                 epsilon: float = 1e-3,
                 center: bool = True,
                 scale: bool = True,
                 beta_initializer='zeros',
                 gamma_initializer='ones',
                 moving_mean_initializer='zeros',
                 moving_variance_initializer='ones',
                 beta_regularizer=None,
                 gamma_regularizer=None,
                 beta_constraint = None,
                 gamma_constraint = None,
                 trainable = True,
                 name: str = 'BatchNorm',
                 **kwargs):

        super(SyncsBatchNormalization, self).__init__(name=name, **kwargs)

        self.axis = axis
        self.center = center
        self.scale = scale
        self.epsilon = epsilon
        self.momentum = momentum
        self.trainable = trainable

        self.beta_initializer = beta_initializer
        self.gamma_initializer = gamma_initializer

        self.moving_mean_initializer = moving_mean_initializer
        self.moving_variance_initializer = moving_variance_initializer

        self.beta_regularizer = beta_regularizer
        self.gamma_regularizer = gamma_regularizer

        self.beta_constraint = beta_constraint
        self.gamma_constraint = gamma_constraint


    def build(self, input_shape: list):

        self.feature_dim = input_shape[self.axis]
        axes = list(range(len(input_shape)))
        axes.pop(self.axis)

        self.axes = axes

        if self.scale:
            self.gamma = self.add_weight(
                shape=(self.feature_dim,),
                name='gamma',
                initializer=self.gamma_initializer,
                regularizer = self.gamma_regularizer,
                constraint = self.gamma_constraint
            )

        if self.center:
            self.beta = self.add_weight(
                shape=(self.feature_dim,),
                name='beta',
                initializer=self.beta_initializer,
                regularizer = self.beta_regularizer,
                constraint = self.beta_constraint
            )

        self.moving_mean = self.add_weight(
            name='moving_mean',
            shape=(self.feature_dim,),
            initializer=self.moving_mean_initializer,
            synchronization=tf.VariableSynchronization.ON_READ,
            trainable=False,
            aggregation=tf.VariableAggregation.MEAN,
            experimental_autocast=False)

        self.moving_variance = self.add_weight(
            name='moving_variance',
            shape=(self.feature_dim,),
            initializer=self.moving_variance_initializer,
            synchronization=tf.VariableSynchronization.ON_READ,
            trainable=False,
            aggregation=tf.VariableAggregation.MEAN,
            experimental_autocast=False)

        super(SyncsBatchNormalization, self).build(input_shape)

    def _assign_moving_average(self, variable: tf.Tensor, value: tf.Tensor):
        return variable.assign(variable * (1.0 - self.momentum)
                               + value * self.momentum)

    def _get_training_value(self, training=None):

        if training is None:
            training = K.learning_phase()

        if isinstance(training, int):
            training = bool(training)

        return training

    def call(self, inputs, training = None, **kwargs):

        x = inputs

        training = self._get_training_value(training)
        training_value = tf_utils.constant_value(training)

        training_value = training_value and self.trainable

        if training_value:
            ctx = tf.distribute.get_replica_context()
            n = ctx.num_replicas_in_sync * 1.0

            mean = tf.reduce_mean(x, axis=self.axes)
            mean_square = tf.reduce_mean(tf.square(x), axis=self.axes)
            
            mean, mean_square = ctx.all_reduce(tf.distribute.ReduceOp.SUM, [mean, mean_square])
            
            mean = mean / n
            mean_square = mean_square / n

            variance = mean_square - tf.square(mean)

            mean_update = self._assign_moving_average(self.moving_mean, mean)
            variance_update = self._assign_moving_average(self.moving_variance, variance)

            self.add_update(mean_update)
            self.add_update(variance_update)
        else:
            mean = self.moving_mean
            variance = self.moving_variance

        return tf.nn.batch_normalization(x, mean=mean, variance=variance, offset=self.beta,
                                         scale=self.gamma,
                                         variance_epsilon=self.epsilon)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='ankitesh97' date='2020-02-19T05:14:55Z'>
		Hi,
you just need to import the library, below is the code to regenerate the issue
from __future__ import absolute_import, division, print_function, unicode_literals
from tensorflow.keras import initializers
from tensorflow.keras import layers
from tensorflow.keras.layers import Input, Dense, LeakyReLU


class CustomBatchNormalization(layers.Layer):
    def __init__(self, momentum=0.99, epsilon=1e-3,beta_initializer='zeros',
                 gamma_initializer='ones', moving_mean_initializer='zeros',
                 moving_range_initializer='ones',**kwargs):
        super(CustomBatchNormalization, self).__init__(**kwargs)
        self.supports_masking = True
        self.momentum = momentum
        self.epsilon = epsilon
        self.beta_initializer = initializers.get(beta_initializer)
        self.gamma_initializer = initializers.get(gamma_initializer)
        self.moving_mean_initializer = initializers.get(moving_mean_initializer)
        self.moving_range_initializer = (
            initializers.get(moving_range_initializer))

    
    def build(self,input_shape):
        dim = input_shape[-1]
        shape = (dim,)
        self.gamma = self.add_weight(shape=shape,
                             name='gamma',
                             initializer=self.gamma_initializer,trainable=True)
        self.beta = self.add_weight(shape=shape,
                            name='beta',
                            initializer=self.beta_initializer,
                                   trainable=True)
        
        self.moving_mean = self.add_weight(
            shape=shape,
            name='moving_mean',
            initializer=self.moving_mean_initializer,
            trainable=False)
        
        self.moving_range = self.add_weight(
            shape=shape,
            name='moving_range',
            initializer=self.moving_range_initializer,
            trainable=False)



    def call(self, inputs,training=None):
        input_shape = inputs.shape
        
        if training == False:
            scaled = (inputs-self.moving_mean)/(self.moving_range+self.epsilon)
            return self.gamma*scaled + self.beta
        
        mean = tf.math.reduce_mean(inputs,axis=0)
        maxr = tf.math.reduce_max(inputs,axis=0)
        minr = tf.math.reduce_min(inputs,axis=0)
        
        range_diff = tf.math.subtract(maxr,minr)
        self.moving_mean = tf.math.add(self.momentum*self.moving_mean, (1-self.momentum)*mean)
        self.moving_range = tf.math.add(self.momentum*self.moving_range,(1-self.momentum)*range_diff)
        scaled = tf.math.divide(tf.math.subtract(inputs,mean),(range_diff+self.epsilon))
        return tf.math.add(tf.math.multiply(self.gamma,scaled),self.beta)
    
    def get_config(self):
        config = {
            'momentum': self.momentum,
            'epsilon': self.epsilon,
            'beta_initializer': initializers.serialize(self.beta_initializer),
            'gamma_initializer': initializers.serialize(self.gamma_initializer),
            'moving_mean_initializer':
                initializers.serialize(self.moving_mean_initializer),
            'moving_range_initializer':
                initializers.serialize(self.moving_range_initializer)
        }
        base_config = super(CustomBatchNormalization, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))
    
    def compute_output_shape(self, input_shape):
        return input_shape


inp = Input(shape=(64,))
batch_norm_1 = CustomBatchNormalization()(inp)
densout = Dense(128, activation='linear')(batch_norm_1)
densout = LeakyReLU(alpha=0.3)(densout)
for i in range (6):
    batch_norm_i = CustomBatchNormalization()(densout)
    densout = Dense(128, activation='linear')(batch_norm_i)
    densout = LeakyReLU(alpha=0.3)(densout)
batch_norm_out = CustomBatchNormalization()(densout)
out = Dense(64, activation='linear')(batch_norm_out)
Inp_RH_CBN = tf.keras.models.Model(inp, out)
		</comment>
		<comment id='4' author='ankitesh97' date='2020-02-19T05:51:48Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36848&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36848&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='ankitesh97' date='2020-02-19T08:38:00Z'>
		&lt;denchmark-link:https://github.com/ankitesh97&gt;@ankitesh97&lt;/denchmark-link&gt;

I have tried on colab with TF version 2.1.0 and i am not seeing any issue.Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/ab298a28ea90e251d46e56dc838bb7dd/untitled651.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='6' author='ankitesh97' date='2020-02-19T08:50:30Z'>
		Hi, thanks for the reply.
If you check this part of the code
    def build(self,input_shape):
        dim = input_shape[-1]
        shape = (dim,)
        self.gamma = self.add_weight(shape=shape,
                             name='gamma',
                             initializer=self.gamma_initializer,trainable=True)
        self.beta = self.add_weight(shape=shape,
                            name='beta',
                            initializer=self.beta_initializer,
                                   trainable=True)
        
        self.moving_mean = self.add_weight(
            shape=shape,
            name='moving_mean',
            initializer=self.moving_mean_initializer,
            trainable=False)
        
        self.moving_range = self.add_weight(
            shape=shape,
            name='moving_range',
            initializer=self.moving_range_initializer,
            trainable=False)
There are 4 params 2 trainable and 2 non trainable. so basically there are 4 params for each feature
in the batch norm layer. so if I have x features in some layer it will have 4*x params for the corresponding batch norm layer. However, if you see the summary of the Model in you gist (I have attached a screen shot)
&lt;denchmark-link:https://user-images.githubusercontent.com/16163706/74817373-a2aa9a00-52b1-11ea-9dae-0078791d89eb.png&gt;&lt;/denchmark-link&gt;

You can only see 128 params in the batchnorm layer but there should be 64x4 (128 trainable and 128 non traniable ).
Thanks,
		</comment>
		<comment id='7' author='ankitesh97' date='2020-02-21T19:47:01Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 any updates?
		</comment>
		<comment id='8' author='ankitesh97' date='2020-05-14T01:23:14Z'>
		&lt;denchmark-link:https://colab.research.google.com/gist/jvishnuvardhan/6d88fcaf01ca394e9823c8de835a1f6f/36848.ipynb&gt;Here&lt;/denchmark-link&gt;
 is gist with  for our reference. Thanks!
		</comment>
		<comment id='9' author='ankitesh97' date='2020-06-22T19:30:12Z'>
		What's happening here is that the call method is re-assigning the python attributes self.moving_mean and self.moving_range, rather than assigning to the weights stored in those attributes. This makes the custom layer no longer have access to original weights added to moving_mean and moving_range, and they are dropped from the custom_layer's weights.
You can fix this by changing the code to assign to the weight instead of replacing the weight object itself with a tensor:
&lt;denchmark-code&gt;self.moving_mean.assign(tf.math.add(self.momentum*self.moving_mean, (1-self.momentum)*mean))
self.moving_range.assign(tf.math.add(self.momentum*self.moving_range,(1-self.momentum)*range_diff))
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='ankitesh97' date='2020-06-22T19:30:21Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36848&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36848&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>