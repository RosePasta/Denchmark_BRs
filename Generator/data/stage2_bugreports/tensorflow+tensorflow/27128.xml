<bug id='27128' author='fsx950223' open_date='2019-03-26T03:29:29Z' closed_time='2019-04-08T20:28:09Z'>
	<summary>dataset memory cache runs out of memory</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.13.1
Python version:
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

You can collect some of this information using our environment capture &lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with
python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
Describe the current behavior
I use tf.dataset.cache to cache data and it will run out of my computer's memory
Describe the expected behavior
cache won't run out of memory
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
dataset.cache()
If we can use some cache storages just like leveldb which will provide high performance and will avoid memory leak.
	</description>
	<comments>
		<comment id='1' author='fsx950223' date='2019-03-26T12:24:28Z'>
		Thank you for reaching out to us. Can you please provide minimum reproducible code snippet and the error log to help us proceed further and verify what is going wrong
		</comment>
		<comment id='2' author='fsx950223' date='2019-03-26T14:10:28Z'>
		
Thank you for reaching out to us. Can you please provide minimum reproducible code snippet and the error log to help us proceed further and verify what is going wrong

dataset.cache()
Just use cache when load data and memory will increase until ran out of memory.Because it will store data in memory without provide filename.
		</comment>
		<comment id='3' author='fsx950223' date='2019-04-05T21:52:52Z'>
		&lt;denchmark-link:https://github.com/fsx950223&gt;@fsx950223&lt;/denchmark-link&gt;
 Could you describe more details about the issue and its context? Thanks!
		</comment>
		<comment id='4' author='fsx950223' date='2019-04-06T03:12:05Z'>
		
@fsx950223 Could you describe more details about the issue and its context? Thanks!

If I use memory cache, it will run out of my memory.I won't use file cache because I read data from tfrecords and file cache just read data and write it to another file,it's useless for me.
Anyway to avoid memory leak and provide high performance
		</comment>
		<comment id='5' author='fsx950223' date='2019-04-08T20:28:09Z'>
		If your experiment runs out of memory when using tf.data.Dataset.cache() it means that your data does not fit into computers memory. This is not an indication of a memory leak but the intended behavior of your workload. The API is not intended to be using for caching only a subset of your dataset in memory.
		</comment>
		<comment id='6' author='fsx950223' date='2019-04-08T20:28:10Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=27128&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=27128&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='fsx950223' date='2020-04-03T21:25:22Z'>
		I see similar behavior, and my dataset is about 1Gb, but it fills out 22Gb of free memory
		</comment>
		<comment id='8' author='fsx950223' date='2020-09-17T15:00:01Z'>
		
I see similar behavior, and my dataset is about 1Gb, but it fills out 22Gb of free memory

That's probably because you have decoded/decompressed your data and stored raw bytes in the memory
		</comment>
	</comments>
</bug>