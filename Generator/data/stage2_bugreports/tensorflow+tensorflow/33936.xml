<bug id='33936' author='tornadoyi' open_date='2019-11-02T15:47:03Z' closed_time='2020-04-09T19:05:38Z'>
	<summary>sigmoid is ignored when calculating loss by calling method model.fit</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Windoiws

Version: ('v2.0.0-rc2-26-g64c3d382ca', '2.0.0')
Describe the current behavior
I got a incorrect loss from history returned by calling model.fit.  You can see the correct and  incorrect result by change parameter "error " from my code.
Code to reproduce the issue
import math
import numpy as np
import tensorflow as tf

error = True
n_features = 100
batch = 2

"""
model
"""
x = tf.keras.Input(shape=(n_features,), dtype=tf.float32)
w = tf.Variable([1.0] * n_features)
b = tf.Variable(1.0)
z = tf.reduce_sum(w * x, axis=1, keepdims=True) + b

"""
loss is incorrect if error is true
"""
if error:
    y_ = tf.sigmoid(z)
else:
    y_ = 1.0 / (1.0 + math.e ** (-z))

m = tf.keras.Model(inputs=x, outputs=y_)

"""
loss
"""
optimizer=tf.keras.optimizers.SGD(learning_rate=0.001)
loss = tf.keras.losses.BinaryCrossentropy()
m.compile(optimizer = optimizer, loss = loss)

"""
train dataset
"""
x = np.array([[1.0 for i in range(n_features)]] * batch, dtype=np.float32)
y = np.array([0.0] * batch, dtype=np.float32)

"""
get correct loss
"""
logits = m(x)
l = loss(y, logits)

"""
get incorrect loss
"""
history = m.fit(x, y)

"""
history.history['loss'] != l.numpy()
"""
print(history.history)
print(l.numpy())
	</description>
	<comments>
		<comment id='1' author='tornadoyi' date='2019-11-04T10:58:41Z'>
		I have tried on colab with TF version 2.0 ,2.1.0-dev20191103 and was able to reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/31f5ab164a9db29145e388857c50e05d/untitled327.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='tornadoyi' date='2019-11-04T13:37:46Z'>
		I have got the same result on macos. Please make sure whether this is a bug.
		</comment>
		<comment id='3' author='tornadoyi' date='2020-03-18T10:26:57Z'>
		Could replicate this issue with Tf 2.1 and Tf-nighly
Please find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/5e918b04d0826e7de1eeeb4a53fa1a6b/untitled467.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks
		</comment>
		<comment id='4' author='tornadoyi' date='2020-04-08T05:28:54Z'>
		I did some digging. This is an issue since this &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/03c045e146d441a0db4bd5cc1ecef1517e5c6708&gt;commit&lt;/denchmark-link&gt;

So both loss are not wrong. It's just different on how we make numeric approximation on INF. More specifically the binary_crossentropy loss is:
, and y=0, p=sigmoid(logits)=1, logits=101
This is theoretically -INF, however in graph mode our computation relies on , which under their &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits&gt;docstring&lt;/denchmark-link&gt;
 is:

so loss is same as logits, which is 101, and also what history.loss reports.
On the other hand, in eager mode our computation relies on:
output=clip(epsilon, 1-epsilon)
y * log(p + epsilon) + (1-y) * log(1-output_epsilon) == log(2*epsilon)
which ends up 15.33, different than above.
&lt;denchmark-link:https://github.com/tornadoyi&gt;@tornadoyi&lt;/denchmark-link&gt;
 I wouldn't think this is a huge issue, since this is on very saturated case and approximating how we deal with INF, and it's not how we usually initialize kernels. Let us know if you still have concerns.
		</comment>
		<comment id='5' author='tornadoyi' date='2020-04-09T19:05:38Z'>
		Closing it for now. Let us know if the previous comments didn't help you :-)
		</comment>
		<comment id='6' author='tornadoyi' date='2020-04-09T19:05:40Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33936&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33936&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>