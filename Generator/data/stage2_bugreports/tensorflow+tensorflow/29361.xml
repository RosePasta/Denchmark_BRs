<bug id='29361' author='nikitos9000' open_date='2019-06-03T14:11:30Z' closed_time='2019-06-14T08:45:57Z'>
	<summary>CuDNN LSTM fails with XLA on 2080 Ti and CUDA 10.1</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
TensorFlow installed from (source or binary): From Official Tensorflow GPU-powered Docker
TensorFlow version (use command below): 1.13.1
Python version: 3.5.2, 2.7
CUDA/cuDNN version: 10.1
GPU model and memory: NVIDIA GeForce 2080 Ti and NVIDIA Tesla T4

Describe the current behavior
When running with XLA on, the code given below fails with the following exception:
&lt;denchmark-code&gt;2019-06-03 14:05:14.646045: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz
2019-06-03 14:05:14.658647: I tensorflow/compiler/xla/service/service.cc:161] XLA service 0x4df19b0 executing computations on platform Host. Devices:
2019-06-03 14:05:14.658738: I tensorflow/compiler/xla/service/service.cc:168]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;
2019-06-03 14:05:15.083263: I tensorflow/compiler/xla/service/service.cc:161] XLA service 0x4e6c080 executing computations on platform CUDA. Devices:
2019-06-03 14:05:15.083333: I tensorflow/compiler/xla/service/service.cc:168]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5
2019-06-03 14:05:15.084392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.65
pciBusID: 0000:06:00.0
totalMemory: 10.73GiB freeMemory: 10.57GiB
2019-06-03 14:05:15.084478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-06-03 14:05:15.640030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-06-03 14:05:15.640114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0
2019-06-03 14:05:15.640129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N
2019-06-03 14:05:15.641178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10198 MB memory) -&gt;
 physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:06:00.0, compute capability: 7.5)

WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated
and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2019-06-03 14:05:21.949030: E tensorflow/compiler/xla/status_macros.cc:49] Internal: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc:3171) ShapeUtil::E
qual(first_reduce-&gt;shape(), inst-&gt;shape())
*** Begin stack trace ***




        tensorflow::Status xla::HloInstruction::Visit&lt;xla::HloInstruction*&gt;(xla::DfsHloVisitorBase&lt;xla::HloInstruction*&gt;*)

        tensorflow::Status xla::HloInstruction::Accept&lt;xla::HloInstruction*&gt;(xla::DfsHloVisitorBase&lt;xla::HloInstruction*&gt;*, bool, bool)
        tensorflow::Status xla::HloComputation::Accept&lt;xla::HloInstruction*&gt;(xla::DfsHloVisitorBase&lt;xla::HloInstruction*&gt;*) const
        xla::gpu::NVPTXCompiler::RunBackend(std::unique_ptr&lt;xla::HloModule, std::default_delete&lt;xla::HloModule&gt; &gt;, stream_executor::StreamExecutor*, xla::DeviceMemoryAllocator*)
        xla::Service::BuildExecutable(xla::HloModuleProto const&amp;, std::unique_ptr&lt;xla::HloModuleConfig, std::default_delete&lt;xla::HloModuleConfig&gt; &gt;, xla::Backend*, stream_executor::
StreamExecutor*, xla::DeviceMemoryAllocator*)


        tensorflow::XlaCompilationCache::BuildExecutable(tensorflow::XlaCompiler::Options const&amp;, tensorflow::XlaCompiler::CompilationResult const&amp;, std::unique_ptr&lt;xla::LocalExecut
able, std::default_delete&lt;xla::LocalExecutable&gt; &gt;*)
        tensorflow::XlaCompilationCache::CompileImpl(tensorflow::XlaCompiler::Options const&amp;, tensorflow::NameAttrList const&amp;, absl::Span&lt;tensorflow::XlaCompiler::Argument const&gt;, s
td::function&lt;tensorflow::Status (tensorflow::XlaCompiler*, tensorflow::XlaCompiler::CompilationResult*)&gt; const&amp;, absl::optional&lt;long long&gt;, tensorflow::XlaCompiler::CompilationResul
t const**, xla::LocalExecutable**)
        tensorflow::XlaCompilationCache::Compile(tensorflow::XlaCompiler::Options const&amp;, tensorflow::NameAttrList const&amp;, absl::Span&lt;tensorflow::XlaCompiler::Argument const&gt;, tenso
rflow::XlaCompiler::CompileOptions const&amp;, tensorflow::XlaCompilationCache::CompileMode, tensorflow::XlaCompiler::CompilationResult const**, xla::LocalExecutable**)

        tensorflow::XlaCompileOp::Compute(tensorflow::OpKernelContext*)
        tensorflow::BaseGPUDevice::ComputeHelper(tensorflow::OpKernel*, tensorflow::OpKernelContext*)
        tensorflow::BaseGPUDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*)


        Eigen::ThreadPoolTempl&lt;tensorflow::thread::EigenEnvironment&gt;::WorkerLoop(int)

        std::_Function_handler&lt;void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function&lt;void ()&gt;)::{lambda()#1}&gt;::_M_invoke(std::_Any_data const&amp;)


        clone
*** End stack trace ***

2019-06-03 14:05:21.949546: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at xla_ops.cc:429 : Internal: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/ir
_emitter_unnested.cc:3171) ShapeUtil::Equal(first_reduce-&gt;shape(), inst-&gt;shape())
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 1334, in _do_call
    return fn(*args)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InternalError: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc:3171) ShapeUtil::Equal(first_reduce-&gt;shape(), in
st-&gt;shape())
         [[{{node cluster_1_1/xla_compile}}]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "test_tf.py", line 20, in &lt;module&gt;
    session.run(output, feed_dict={input: np.zeros((1, steps, 1), dtype=np.float32)})
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 929, in run
    run_metadata_ptr)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 1328, in _do_run
    run_metadata)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc:3171) ShapeUtil::Equal(first_reduce-&gt;shape(), in
st-&gt;shape())
         [[{{node cluster_1_1/xla_compile}}]]
&lt;/denchmark-code&gt;

Reproduces in the following environments:
— Official Tensorflow Docker: tensorflow/tensorflow:latest-gpu
— NVIDIA Tensorflow Docker: nvcr.io/nvidia/tensorflow:19.03-py3
Fails with the same error on Keras code (CuDNN LSTM keras implementation) too.
Describe the expected behavior
Given code completes successfully.
Code to reproduce the issue
&lt;denchmark-code&gt;import numpy as np
import tensorflow as tf

config = tf.ConfigProto()
# no fail when XLA is disabled
config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1
session = tf.Session(config=config)

steps = 2  # no fail when = 1
input = tf.placeholder(dtype=tf.float32, shape=[None, steps, 1])

lstm = tf.contrib.cudnn_rnn.CudnnLSTM(num_layers=1, num_units=1, dtype=tf.float32)
lstm.build(input.get_shape())
lstm_output, _ = lstm(input)
output = tf.concat([lstm_output, input], axis=2)
output = tf.reduce_sum(output, axis=1, keepdims=False)
output = tf.nn.l2_normalize(output, axis=1)

session.run(tf.global_variables_initializer())
session.run(output, feed_dict={input: np.zeros((1, steps, 1), dtype=np.float32)})
&lt;/denchmark-code&gt;

If you remove any of the tf ops above, error doesn't reproduce.
	</description>
	<comments>
		<comment id='1' author='nikitos9000' date='2019-06-04T01:27:26Z'>
		I have similar problem, when I doing normal training, it works, but when I use FP 16, it gives me exactly the same error............
		</comment>
		<comment id='2' author='nikitos9000' date='2019-06-04T12:11:35Z'>
		I have tried on Colab with TensorFlow version 1.13.1 and was able to reproduce the issue.
		</comment>
		<comment id='3' author='nikitos9000' date='2019-06-05T23:39:07Z'>
		&lt;denchmark-link:https://github.com/thomasjoerg&gt;@thomasjoerg&lt;/denchmark-link&gt;
 do you have cycles to look at this?
		</comment>
		<comment id='4' author='nikitos9000' date='2019-06-06T15:40:00Z'>
		At first glance, this looks like an invalid reduce multi-output fusion that fails to codegen. The CHECK failure indicates that the shapes do not line up. I skimmed the relevant code, but didn't spot an obvious bugs.
I may have some cycles tomorrow, otherwise early next week, to debug this.
		</comment>
		<comment id='5' author='nikitos9000' date='2019-06-06T20:10:39Z'>
		Sorry I don't know whether my question is relevant to this, but I do have similar error msg.
All the functionality and model which works at tf 1.10, has issue in tf 1.13, in NCHW format. But NHWC works just fine.
For details, pls refer &lt;denchmark-link:https://stackoverflow.com/questions/56437502/mixed-precision-training-report-ret-check-failure-shapeutilequalfirst-reduce&gt;https://stackoverflow.com/questions/56437502/mixed-precision-training-report-ret-check-failure-shapeutilequalfirst-reduce&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='nikitos9000' date='2019-06-11T15:44:59Z'>
		I can reproduce the issue. Thank you for the high-quality bug report!
The issue is fixed at head . You are running TF 1.13 (stable). Would it be feasible for you to upgrade to TF 1.14?
		</comment>
		<comment id='7' author='nikitos9000' date='2019-06-12T12:55:40Z'>
		Yes, sure, that'd be great, thanks!
		</comment>
		<comment id='8' author='nikitos9000' date='2019-06-14T08:45:58Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=29361&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=29361&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='nikitos9000' date='2020-02-27T01:37:15Z'>
		
I can reproduce the issue. Thank you for the high-quality bug report!
The issue is fixed at head . You are running TF 1.13 (stable). Would it be feasible for you to upgrade to TF 1.14?

&lt;denchmark-link:https://github.com/thomasjoerg&gt;@thomasjoerg&lt;/denchmark-link&gt;
 Could you provide a file name or commit hash that points to the issue / the fix? I'm experiencing the same issue and as porting tf version may be hard in my use case I'm wondering if it's easier to apply the same patch. Thank you!
		</comment>
	</comments>
</bug>