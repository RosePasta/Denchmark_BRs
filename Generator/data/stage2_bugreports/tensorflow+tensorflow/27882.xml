<bug id='27882' author='ngelotte' open_date='2019-04-16T06:14:02Z' closed_time='2019-07-09T18:26:52Z'>
	<summary>Converting InceptionV3 to TfLite missing IdentityN</summary>
	<description>
System information
Google Colab

TensorFlow installed from (source or binary):
Installed from https://storage.googleapis.com/download.tensorflow.org/data/tensorflow_hub-0.4.0.dev0-py2.py3-none-any.whl
TensorFlow version (or github SHA if from source):

Provide the text output from tflite_convert
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: AVERAGE_POOL_2D, CONCATENATION, CONV_2D, FULLY_CONNECTED, MAX_POOL_2D, MEAN, RESHAPE, SOFTMAX. Here is a list of operators for which you will need custom implementations: IdentityN.
Also, please include a link to a GraphDef or the model if possible.
Google Colab Notebook: &lt;denchmark-link:https://colab.research.google.com/drive/1idF4ZZysBnfcGwfoj0DovQb8C4NVA42w&gt;https://colab.research.google.com/drive/1idF4ZZysBnfcGwfoj0DovQb8C4NVA42w&lt;/denchmark-link&gt;

I updated the Google Colab Tutorial and updated it to use InceptionV3(based on the link included in the notebook and modified the file size to be [299,299].
	</description>
	<comments>
		<comment id='1' author='ngelotte' date='2019-05-15T09:58:46Z'>
		&lt;denchmark-link:https://github.com/ngelotte&gt;@ngelotte&lt;/denchmark-link&gt;
 : I tried executing your colab link, but it throws error at "" stage, would you please check and resolve it, so that i can debug the issue. Otherwise you can share the frozen pb file directly if you have. Thanks!
		</comment>
		<comment id='2' author='ngelotte' date='2019-05-16T05:55:17Z'>
		&lt;denchmark-link:https://github.com/ANSHUMAN87&gt;@ANSHUMAN87&lt;/denchmark-link&gt;
 : Sorry I modified that same notebook to try and figure out how to freeze the graph. I copied over the network and took out what I had added - you can view it here. &lt;denchmark-link:https://colab.research.google.com/drive/17e9E56uQpkdaD7Lz-csAOS2PL8Tzoz_1&gt;https://colab.research.google.com/drive/17e9E56uQpkdaD7Lz-csAOS2PL8Tzoz_1&lt;/denchmark-link&gt;

However, It now gets a different error and I don't know what has changed. The new error is:
Invalid argument: Input 1 of node keras_layer_1/StatefulPartitionedCall was passed float from Variable_1:0 incompatible with expected resource.
		</comment>
		<comment id='3' author='ngelotte' date='2019-05-21T11:45:11Z'>
		&lt;denchmark-link:https://github.com/ngelotte&gt;@ngelotte&lt;/denchmark-link&gt;
 : Now i am able to run the notebook, for the error you faced("Invalid argument: Input 1 of node keras_layer_1/StatefulPartitionedCall was passed float from Variable_1:0 incompatible with expected resource."), is because of custom objects from Hub_Layer.
As tf.lite.TFLiteConverter.from_saved_model() does not support custom objects, i would suggest rather you save model in h5 file and use tf.lite.TFLiteConverter.from_keras_model_file("sample.h5", custom_objects={'KerasLayer':hub.KerasLayer}).
As for your original issue: below i have provided snapshot for working code.
converter = tf.lite.TFLiteConverter.from_keras_model_file("sample.h5", custom_objects={'KerasLayer':hub.KerasLayer})
converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,
tf.lite.OpsSet.SELECT_TF_OPS]
tflite_model = converter.convert()
NOTE: In COLAB Notebook some error is reported while saving to h5 file, so you can run offline. Or you can download the saved_model files and convert offline.
		</comment>
		<comment id='4' author='ngelotte' date='2019-06-27T23:23:45Z'>
		&lt;denchmark-link:https://github.com/ngelotte&gt;@ngelotte&lt;/denchmark-link&gt;
 Is this issue resolved by following &lt;denchmark-link:https://github.com/ANSHUMAN87&gt;@ANSHUMAN87&lt;/denchmark-link&gt;
 suggestion? Thanks!
		</comment>
		<comment id='5' author='ngelotte' date='2019-06-28T05:26:14Z'>
		I was not able to try it as I did not have a good local environment to run it in. As the goal of this conversion was to be able to use it on a EdgeTPU device I am able to follow the steps here &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/27880&gt;#27880&lt;/denchmark-link&gt;
 to do the conversion. This area is still in active development it seems but I think it is the better route. Thanks for the help.
		</comment>
		<comment id='6' author='ngelotte' date='2019-07-09T18:26:52Z'>
		Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!
		</comment>
		<comment id='7' author='ngelotte' date='2019-08-23T01:18:50Z'>
		The original reported issue should be resolved by commit &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/aca24307eba6d5581fec692ddd948e3eee67ac9a&gt;aca2430&lt;/denchmark-link&gt;
.
Could you try it again with the next TF nightly build? Thanks!
		</comment>
	</comments>
</bug>