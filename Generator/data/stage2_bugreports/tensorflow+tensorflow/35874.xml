<bug id='35874' author='ifed-ucsd' open_date='2020-01-14T17:39:36Z' closed_time='2020-02-18T19:57:49Z'>
	<summary>Executing XLA compiled function inside tf.GradientTape context leads to extraneous GPU kernels and D2D copies</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): Binary
TensorFlow version (use command below): v1.12.1-21967-gd80fda0 2.1.0-dev20200109
Python version: 3.6.9
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: CUDA Version 10.1.243 / cuDNN 7.6.4.38-1
GPU model and memory: TITAN V, 12GB

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
I am trying to speed my code using tf.function decoration, with experimental_compile=True. I observe that my code indeed runs faster using this decoration, but executing the function within a tf.GradientTape() context seems to generate two GPU kernels instead of one, even though I am not computing any gradients. Moreover, executing the function inside of the tf.GradientTape() context produces two D2D copies, which are not present when executing outside of tf.GradientTape(). The following illustrates my point (the first kernel is running outside tf.GradientTape()and the second is running inside tf.GradientTape():
&lt;denchmark-link:https://user-images.githubusercontent.com/10225518/72367919-0cd88980-36cb-11ea-9d0c-2fc0937a5db6.png&gt;&lt;/denchmark-link&gt;

Describe the expected behavior
I am not sure if this behavior is expected or not. If it is not expected, I would certainly like it if there were less GPU kernel calls, leading to much faster code.
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow.python import eager

def main():

    assert tf.executing_eagerly()

    gpus_p = tf.config.experimental.list_physical_devices('GPU')
    if gpus_p:
        try:
            # Currently, memory growth needs to be the same across GPUs
            for gpu in gpus_p:
                tf.config.experimental.set_memory_growth(gpu, True)
        except RuntimeError as e:
            # Memory growth must be set before GPUs have been initialized
            print(e)


    def tf_forward(x,xmax):
        return tf.quantization.fake_quant_with_min_max_vars(inputs=x,
                                                            min=-xmax,
                                                            max=xmax,
                                                            num_bits=8)


    tf_forward_tf_function = tf.function(func=tf_forward,
                                        experimental_compile=True)

    

    x = tf.random.uniform(shape=[3, 3, 96, 1])
    xmax = tf.Variable(0.5)

    tf_forward_tf_function(x,xmax)

    eager.profiler.start()
    for n in range(1000):
        x = tf_forward_tf_function(x,xmax)
        with tf.GradientTape() as tape:
            tape.watch(x)
            x = tf_forward_tf_function(x,xmax)
    
    profiler_result = eager.profiler.stop()
    eager.profiler.save('/test/log', profiler_result)

if __name__ == '__main__':
    main()
&lt;/denchmark-code&gt;

Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='ifed-ucsd' date='2020-02-12T17:21:37Z'>
		Thanks for such a detailed bug report! I have a fix ready which removes the two extra D2D copies.
		</comment>
		<comment id='2' author='ifed-ucsd' date='2020-02-12T18:05:45Z'>
		I'll mark the bug as fixed based on removing those extra copies. From what I understand, the fact that the kernel for gradient computation is launched regardless of whether derivative is used is just a property of eager runtime, and can not really be avoided (if you don't need the gradients =&gt; don't launch under the gradient tape).
		</comment>
		<comment id='3' author='ifed-ucsd' date='2020-02-12T18:18:13Z'>
		Thank you! I apologize for the silly question, but how do I get the fix for the D2D copies?
		</comment>
		<comment id='4' author='ifed-ucsd' date='2020-02-12T19:14:50Z'>
		You would have to wait for it to land after it passes internal testing.
		</comment>
		<comment id='5' author='ifed-ucsd' date='2020-02-13T02:54:53Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35874&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35874&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='ifed-ucsd' date='2020-02-13T17:07:20Z'>
		Actually apologies, this was the incorrect commit, the proper fix has still not landed yet.
		</comment>
		<comment id='7' author='ifed-ucsd' date='2020-02-18T19:57:50Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35874&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35874&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>