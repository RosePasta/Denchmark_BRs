<bug id='31736' author='vlasenkov' open_date='2019-08-18T07:26:09Z' closed_time='2019-08-21T06:20:34Z'>
	<summary>GPU placing issue with slurm</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): openSUSE Leap 42.3
TensorFlow installed from (source or binary): binary/pip
TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0
Python version: Python 3.6.9 :: Anaconda, Inc.
CUDA/cuDNN version: CUDA 10.1
GPU model and memory: 4x GeForce RTX 2080 11Gb

Describe the current behavior

If I do not change anything then the training crashes with the traceback below.
If I set allow_soft_placement=True the framework begins to place everything on CPU.

Describe the expected behavior
The training starts on 4 GPUs.
Code to reproduce the issue
Slurm command srun -N 1 --gres=gpu:4 -c 32 --mem=96G python train.py
&lt;denchmark-link:https://github.com/tkarras/progressive_growing_of_gans&gt;PGAN repo&lt;/denchmark-link&gt;

Other info / logs
Traceback
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "train.py", line 285, in &lt;module&gt;
    tfutil.call_func_by_name(**config.train)
  File ".../pgan-tf/tfutil.py", line 236, in call_func_by_name
    return import_obj(func)(*args, **kwargs)
  File ".../pgan-tf/train.py", line 161, in train_progressive_gan
    G = tfutil.Network('G', num_channels=training_set.shape[0], resolution=training_set.shape[1], label_size=training_set.label_size, **config.G)
  File ".../pgan-tf/tfutil.py", line 433, in __init__
    self.reset_vars()
  File ".../pgan-tf/tfutil.py", line 495, in reset_vars
    run([var.initializer for var in self.vars.values()])
  File ".../pgan-tf/tfutil.py", line 21, in run
    return tf.get_default_session().run(*args, **kwargs)
  File ".../miniconda3/envs/pgantf/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 950, in run
    run_metadata_ptr)
  File ".../miniconda3/envs/pgantf/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1173, in _run
    feed_dict_tensor, options, run_metadata)
  File ".../miniconda3/envs/pgantf/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1350, in _do_run
    run_metadata)
  File ".../miniconda3/envs/pgantf/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1370, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation G/lod: node G/lod (defined at .../pgan-tf/networks.py:176) was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:1, /job:localhost/replica:0/task:0/device:XLA_GPU:2, /job:localhost/replica:0/task:0/device:XLA_GPU:3 ]. Make sure the device specification refers to a valid device.
         [[G/lod]]
srun: error: nvidia99: task 0: Exited with exit code 120
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='vlasenkov' date='2019-08-19T06:22:26Z'>
		&lt;denchmark-link:https://github.com/vlasenkov&gt;@vlasenkov&lt;/denchmark-link&gt;

In order to expedite the trouble-shooting process, please provide a minimal code snippet to reproduce the issue reported here. Thanks!
		</comment>
		<comment id='2' author='vlasenkov' date='2019-08-19T14:24:57Z'>
		&lt;denchmark-code&gt;import tensorflow as tf

with tf.device('/gpu:0'):
    matrix1 = tf.constant([[3., 3.]])
    matrix2 = tf.constant([[2.], [2.]])

product = tf.matmul(matrix1, matrix2)

with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:
    result = sess.run(product)
    print(result)
&lt;/denchmark-code&gt;

This behaves in the same way. With allow_soft_placement=True it moves everything on CPU and says that it failed to load cudnn.so.7 With allow_soft_placement=False it crashes with some device naming inconsistency.
		</comment>
		<comment id='3' author='vlasenkov' date='2019-08-19T20:55:36Z'>
		I suspect that TF fails to recognize gpu. Can you please print the output from terminal;
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
		</comment>
		<comment id='4' author='vlasenkov' date='2019-08-20T06:38:38Z'>
		With allow_soft_placement=True:
&lt;denchmark-code&gt;MatMul: (MatMul): /job:localhost/replica:0/task:0/device:CPU:0
Const: (Const): /job:localhost/replica:0/task:0/device:CPU:0
Const_1: (Const): /job:localhost/replica:0/task:0/device:CPU:0
&lt;/denchmark-code&gt;

With allow_soft_placement=False the script crashes
		</comment>
		<comment id='5' author='vlasenkov' date='2019-08-20T17:57:17Z'>
		Your TF session fails to recognize gpu. The reason is TF 1.13 and above pre built binaries support cuda 10.0 Please switch to cuda 10.0 and update environment variables for cuda paths.
		</comment>
		<comment id='6' author='vlasenkov' date='2019-08-21T05:03:29Z'>
		CUDA 10 and CUDA 10.1 are already both installed on the system. After installation of CUDNN everything works now. This was the problem.
The TF 1.14 error message was misleading. In contrast TF 1.13 crashed exactly because it didn't find CUDNN.
		</comment>
		<comment id='7' author='vlasenkov' date='2019-08-21T06:20:34Z'>
		Thanks for sharing your fix. Closing this issue since it's resolved.
		</comment>
		<comment id='8' author='vlasenkov' date='2019-08-21T06:20:35Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=31736&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=31736&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>