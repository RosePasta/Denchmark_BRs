<bug id='38103' author='quetil' open_date='2020-04-01T06:54:30Z' closed_time='2020-07-13T20:34:16Z'>
	<summary>custom training logic in subclassing model not saved</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g.,Linux Ubuntu 16.04): Google Colab (Windows/Linux as well)
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): TensorFlow 2.2.0RC2
Python version: 3.7

Describe the current behavior
Hi all,
I'm using the 2.2 new feature: custom training logic with Model.fit by overriding Model.train_step. It's working well so far. I have the following issue (maybe it's not a feature, you'll tell me):
When I save my model (Model.save) with the custom training logic and then I want to load it, the custom training loop is not saved and when I apply a Model.fit for my loaded model, it comes back to the default one.
Describe the expected behavior
I'd expect to save as well the model.train_step (and also test_step and predict_step).

&lt;denchmark-link:https://colab.research.google.com/gist/quetil/5e92fb9af7cc959f8fe62f1090ef31e7/custom-train_step-in-subclassing-model.ipynb&gt;Gist&lt;/denchmark-link&gt;

Others
Can we expect in the future that the whole class will be saved? (Even variables declared in the __init__ for instance).
Thank you in advance :-)
	</description>
	<comments>
		<comment id='1' author='quetil' date='2020-04-01T09:58:16Z'>
		&lt;denchmark-link:https://github.com/quetil&gt;@quetil&lt;/denchmark-link&gt;

I have executed the code shared by you and get different error please refer to &lt;denchmark-link:https://colab.sandbox.google.com/gist/Saduf2019/936ac77698c63786e9c563e07da7797e/untitled121.ipynb&gt;this gist&lt;/denchmark-link&gt;
.
with respect to the error faced there is a similar resolved issue please refer to this &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/37141#issuecomment-592737155&gt;comment&lt;/denchmark-link&gt;
  and let us know if it helps.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/27949&gt;#27949&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://stackoverflow.com/questions/41689451/valueerror-no-gradients-provided-for-any-variable&gt;link1&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://en.dev4app.com/archives/51966406-no-gradients-provided-for-any-variable-check-your-graph-for-ops-that-do-not-support-gradients.html&gt;link2&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='quetil' date='2020-04-01T12:39:36Z'>
		Hi &lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;
, thanks for the quick reply.
Please refer to &lt;denchmark-link:https://colab.research.google.com/gist/quetil/5e92fb9af7cc959f8fe62f1090ef31e7/custom-train_step-in-subclassing-model.ipynb&gt;this new gist&lt;/denchmark-link&gt;
 (updated in the first post).
I used the last tf-nigthly. The train_step is not saved and once loaded, it's using the default train_step.
Thanks
		</comment>
		<comment id='3' author='quetil' date='2020-04-02T07:38:28Z'>
		i have replicated the issue reported as per gist shared above, please find the &lt;denchmark-link:https://colab.sandbox.google.com/drive/1K28D5mcVCiFrTwXXDopYU38ZJrEt6JNx#scrollTo=fliohxc90O-p&gt;gist here&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='quetil' date='2020-04-02T17:54:23Z'>
		please find the &lt;denchmark-link:https://colab.sandbox.google.com/gist/Saduf2019/37b047e54f8caa63fb8064e8f11b9f33/untitled122.ipynb&gt;gist here&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='quetil' date='2020-07-08T14:37:05Z'>
		Any news about such feature?
Planned in TF 2.4 ? :-)
		</comment>
		<comment id='6' author='quetil' date='2020-07-13T20:34:16Z'>
		Currently we do not plan to add this, if you want to retain the train_step function, decorate it with @tf.function(input_signature=...)
		</comment>
		<comment id='7' author='quetil' date='2020-07-13T20:34:17Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38103&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38103&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='quetil' date='2020-12-03T23:36:21Z'>
		&lt;denchmark-link:https://github.com/k-w-w&gt;@k-w-w&lt;/denchmark-link&gt;
 Decorating with  presents its own problems:
&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.python.eager import backprop

class TestModel(tf.keras.models.Model):

    def __init__(self):
        super().__init__()
        self.x = layers.Input((10,))
        self.output_ = layers.Dense(1)

    def call(self, inputs):
        return self.output_(inputs)

    @tf.function(
        input_signature=[(tf.TensorSpec(shape=(None, 10), dtype=tf.float32),
                          tf.TensorSpec(shape=(None, 1), dtype=tf.int64))])
    def train_step(self, data):
        tf.print('Flag')
        x, y = data
        with backprop.GradientTape() as tape:
            y_pred = self(x, training=True)
            loss = self.compiled_loss(
                y, y_pred, regularization_losses=self.losses)
        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
        self.compiled_metrics.update_state(y, y_pred)
        return {m.name: m.result() for m in self.metrics}

    def get_config(self):
        config = super().get_config()
        config.update({'train_step': self.train_step})
        return config

original_model = TestModel()
original_model.compile(loss='mse', optimizer='sgd')

for i in range(3):
    original_model.train_on_batch(np.random.rand(6, 10), np.arange(6))
    print('Completed train loop (original model)')

save_path = 'tmp'
original_model.save(save_path)

loaded_model = tf.keras.models.load_model(save_path)

for i in range(3):
    loaded_model.train_on_batch(np.random.rand(6, 10), np.arange(6))
    print('Completed train loop (loaded model)')```

Gives:

```import numpy as np
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.python.eager import backprop


class TestModel(tf.keras.models.Model):

    def __init__(self):
        super().__init__()
        self.x = layers.Input((10,))
        self.output_ = layers.Dense(1)

    def call(self, inputs):
        return self.output_(inputs)

    @tf.function(
        input_signature=[(tf.TensorSpec(shape=(None, 10), dtype=tf.float32),
                          tf.TensorSpec(shape=(None, 1), dtype=tf.int64))])
    def train_step(self, data):
        tf.print('Flag')
        x, y = data
        with backprop.GradientTape() as tape:
            y_pred = self(x, training=True)
            loss = self.compiled_loss(
                y, y_pred, regularization_losses=self.losses)
        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
        self.compiled_metrics.update_state(y, y_pred)
        return {m.name: m.result() for m in self.metrics}

    def get_config(self):
        config = super().get_config()
        config.update({'train_step': self.train_step})
        return config


original_model = TestModel()
original_model.compile(loss='mse', optimizer='sgd')

for i in range(3):
    original_model.train_on_batch(np.random.rand(6, 10), np.arange(6))
    print('Completed train loop (original model)')

save_path = 'tmp'
original_model.save(save_path)

loaded_model = tf.keras.models.load_model(save_path)

for i in range(3):
    loaded_model.train_on_batch(np.random.rand(6, 10), np.arange(6))
    print('Completed train loop (loaded model)')
&lt;/denchmark-code&gt;

If you modify the signature from
&lt;denchmark-code&gt;@tf.function(
        input_signature=[(tf.TensorSpec(shape=(None, 10), dtype=tf.float32),
                          tf.TensorSpec(shape=(None, 1), dtype=tf.int64))])
&lt;/denchmark-code&gt;

to
&lt;denchmark-code&gt;@tf.function(
        input_signature=[tf.TensorSpec(shape=(None, 10), dtype=tf.float32),
                         tf.TensorSpec(shape=(None, 1), dtype=tf.int64)])
&lt;/denchmark-code&gt;

(get rid of a pair of parenthesis), we get:
&lt;denchmark-code&gt;TypeError: tf__train_step() takes 2 positional arguments but 3 were given
&lt;/denchmark-code&gt;

If we only pass the shape of the inputs (x) values:
&lt;denchmark-code&gt;@tf.function(
        input_signature=[
            tf.TensorSpec(shape=(None, 10), dtype=tf.float32)])
&lt;/denchmark-code&gt;

we get
&lt;denchmark-code&gt;TypeError: tf__train_step() missing 1 required positional argument: 'data'
&lt;/denchmark-code&gt;

And if we do not decorate, train_step is not serialised and so loaded models revert to the train_step of the base class.
There is not a lot of documentation on how to modify train_step such that it can be serialised. Am I getting the wrong end of the stick here?
		</comment>
		<comment id='9' author='quetil' date='2020-12-04T00:39:21Z'>
		A model can be successfully run and saved with a @tf.function(input_signature=...) decoration, but the model either doesn't serialise the custom train_step or doesn't load it properly:
&lt;denchmark-code&gt;import numpy as np
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.python.eager import backprop


class TestModel(tf.keras.models.Model):

    def __init__(self):
        super().__init__()
        self.x = layers.Input((10,))
        self.output_ = layers.Dense(1)

    def call(self, inputs):
        return self.output_(inputs)

    @tf.function(input_signature=[tuple(((
            [tf.TensorSpec((None, 10), tf.float32),
             tf.TensorSpec((None,), tf.int64)])))])
    def train_step(self, data):
        tf.print('Flag')
        x, y = data
        assert isinstance(x, tf.Tensor)
        assert isinstance(y, tf.Tensor)
        with backprop.GradientTape() as tape:
            y_pred = self(x, training=True)
            loss = self.compiled_loss(
                y, y_pred, regularization_losses=self.losses)
        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
        self.compiled_metrics.update_state(y, y_pred)
        return {m.name: m.result() for m in self.metrics}

    def get_config(self):
        config = super().get_config()
        config.update({'train_step': tf.function(self.train_step)})
        return config

original_model = TestModel()
original_model.compile(loss='mse', optimizer='sgd')

for i in range(3):
    original_model.train_on_batch(
        np.random.rand(6, 10), np.arange(6))
    print('Completed train loop (original model)')

save_path = 'tmp'
original_model.save(save_path)

loaded_model = tf.keras.models.load_model(save_path, compile=False)
loaded_model.compile(loss='mse', optimizer='sgd')

for i in range(3):
    loaded_model.train_on_batch(np.random.rand(6, 10), np.arange(6))
    print('Completed train loop (loaded model)')
&lt;/denchmark-code&gt;

This gives:
&lt;denchmark-code&gt;Completed train loop (original model)
Flag
Completed train loop (original model)
Flag
Completed train loop (original model)
&lt;(LOADING INFORMATION)&gt;
Completed train loop (loaded model)
Completed train loop (loaded model)
Completed train loop (loaded model)
&lt;/denchmark-code&gt;

As you can see, train_step has reverted to that of the parent class (tf.keras.models.Model). How can we properly serialise vital methods like this?
The decoration argument for @tf.function(input_signature=[tuple((([tf.TensorSpec((None, 10), tf.float32), tf.TensorSpec((None,), tf.int64)])))]) is absurdly convoluted, but it is the only way I could get the code to accept the arguments to train_on_batch (and therefore fit). Surely this is not expected behaviour.
		</comment>
		<comment id='10' author='quetil' date='2020-12-15T05:10:10Z'>
		Hi &lt;denchmark-link:https://github.com/jscant&gt;@jscant&lt;/denchmark-link&gt;

I'm having the same problem but with a model with two inputs and one output only on the test_step. How would you rephrase the above input_signature? Should something like this work?
input_signature=[tuple((([(tf.TensorSpec((None, 224, 224, 3), tf.float32), tf.TensorSpec((None, 224, 224, 3), tf.float32)), tf.TensorSpec((None,), tf.int64)])))])
Thanks
		</comment>
	</comments>
</bug>