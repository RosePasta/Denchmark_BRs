<bug id='45839' author='UrmsOne' open_date='2020-12-18T08:34:19Z' closed_time='2021-01-01T15:55:43Z'>
	<summary>tf.contrib.distribute.CollectiveAllReduceStrategy can't load model from checkpoint</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): v1.15.4
Python version: 3.6.9
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A

Describe the current behavior
I am working on distributed learning in tensorflow through estimators API using below simple code template:
&lt;denchmark-code&gt;def main(argv):

    # Init, set model dir| export dir | log dir.
    model_dir, export_dir = init()
    # Loading dataset
    download_dataset()

    # Select distribute strategy, such as sync, async, allReduce, etc.
    # CollectiveAllReduceStrategy for allReduce
    dist_strategy = tf.contrib.distribute.CollectiveAllReduceStrategy(num_gpus_per_worker=FLAGS.num_gpus_per_worker)
    # Set run config, including checkpoint saving strategy, maximum number of checkpoints saved, etc.
    run_config = tf.estimator.RunConfig(train_distribute=dist_strategy,
                                        eval_distribute=dist_strategy,
                                        # save_checkpoints_secs=10,
                                        save_checkpoints_steps=FLAGS.save_checkpoints_steps,
                                        keep_checkpoint_max=FLAGS.keep_checkpoint_max)

    # Feature columns describe how to use the input.
    my_feature_columns = get_feature_columns()

    # Model
    # Build 2 hidden layer DNN with 10, 10 units respectively.
    classifier = Net(model_dir, my_feature_columns, run_config).net

    # TrainSpec for training
    train_spec = tf.estimator.TrainSpec(
        input_fn=lambda: csv_input_fn(TRAIN_PATH, FLAGS.batch_size, True),
        max_steps=FLAGS.train_steps,
        hooks=[])
    # EvalSpec for test
    eval_spec = tf.estimator.EvalSpec(
        input_fn=lambda: csv_input_fn(TEST_PATH, FLAGS.batch_size, True))
    print("---training and testing---")
    tf.estimator.train_and_evaluate(classifier, train_spec, eval_spec)
    print("---training finished---")

    # All role are workers, pick the task_index with 0 to save model
    if FLAGS.task_index == 0:
        classifier.export_saved_model(export_dir, serving_input_receiver_fn)
        # classifier.export_savedmodel(export_dir, serving_input_receiver_fn, strip_default_attrs=True)
    print("finish...")
&lt;/denchmark-code&gt;

Firstly, I train for 1000 rounds(train_stpes=1000) and save the checkpoint, it works normally.
Then I set the train_steps to 2000, only the is_chief role can restore the model from the checkpoint without any error.
Chief output as follow:
&lt;denchmark-code&gt;INFO:tensorflow:Graph was finalized.
I1218 15:24:28.721170 139824828032832 monitored_session.py:240] Graph was finalized.
INFO:tensorflow:Restoring parameters from /tmp/iris/iris-chief-0/checkpoint/model.ckpt-1000
I1218 15:24:28.722573 139824828032832 saver.py:1284] Restoring parameters from /tmp/iris/iris-chief-0/checkpoint/model.ckpt-1000
WARNING:tensorflow:From /root/PycharmProjects/multi_gpu_demo/venv/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file utilities to get mtimes.
W1218 15:24:28.753480 139824828032832 deprecation.py:323] From /root/PycharmProjects/multi_gpu_demo/venv/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file utilities to get mtimes.
INFO:tensorflow:Running local_init_op.
I1218 15:24:28.782264 139824828032832 session_manager.py:500] Running local_init_op.
INFO:tensorflow:Done running local_init_op.
I1218 15:24:28.938534 139824828032832 session_manager.py:502] Done running local_init_op.
INFO:tensorflow:Saving checkpoints for 1000 into /tmp/iris/iris-chief-0/checkpoint/model.ckpt.
I1218 15:24:29.215120 139824828032832 basic_session_run_hooks.py:606] Saving checkpoints for 1000 into /tmp/iris/iris-chief-0/checkpoint/model.ckpt.
&lt;/denchmark-code&gt;

Worker output as follow:
&lt;denchmark-code&gt;INFO:tensorflow:Graph was finalized.
I1218 15:24:59.388847 140629246773056 monitored_session.py:240] Graph was finalized.
&lt;/denchmark-code&gt;

Describe the expected behavior
I expect that when I increase train_steps（from 1000 to 2000）, no matter which roles should be able to restore the model from
checkpoint and continue training. Because I think each role only saves a part of the parameters of the model on allReduce mode,
so each role should participate in restoring the model from checkpoint.
But, I analyzed and debug the source code and found only roles with chief or worker index=0 can restore model parameters
from checkpoint
&lt;denchmark-code&gt;/venv/lib/python3.6/site-packages/tensorflow_core/python/distribute/multi_worker_util.py
line 93: is_chief
def is_chief(cluster_spec=None, task_type=None, task_id=None):
  ...
  if task_type == "chief" or task_type == "evaluator":
    return True

  if ("chief" not in cluster_spec and task_type == "worker" and task_id == 0):
    return True
  return False
  ...
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='UrmsOne' date='2020-12-18T14:37:54Z'>
		&lt;denchmark-link:https://github.com/UrmsOne&gt;@UrmsOne&lt;/denchmark-link&gt;
,
TensorFlow 1.x is not actively supported. Could you please update TensorFlow to v2.4 and check if you are facing the same issue. Thanks!
		</comment>
		<comment id='2' author='UrmsOne' date='2020-12-25T15:33:54Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='3' author='UrmsOne' date='2021-01-01T15:55:40Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='4' author='UrmsOne' date='2021-01-01T15:55:46Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45839&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45839&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>