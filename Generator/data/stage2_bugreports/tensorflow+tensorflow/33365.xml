<bug id='33365' author='tb438' open_date='2019-10-15T03:37:06Z' closed_time='2020-03-10T20:49:02Z'>
	<summary>No float64 support with batch normalization in Tensorflow 2.0?</summary>
	<description>
Stock Ubuntu 19.04 with Cuda 10.0, Tensorflow 2.0.0 installed via pip3, Python 3.7.3, GTX1060.
I have a float64 valued dataset with a simple conv2d network that includes tf.keras.layers.BatchNormalization() which is where the error is being thrown I think.
The first set of issues:
&lt;denchmark-code&gt;WARNING:tensorflow:Layer conv2d is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. 
&lt;/denchmark-code&gt;

After setting tf.keras.backend.set_floatx('float64'), next set of errors:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/home/aj/ga.py", line 183, in &lt;module&gt;
    encoder = make_encoder_model(z_dim)
  File "/home/aj/ga.py", line 138, in make_encoder_model
    x = tf.keras.layers.BatchNormalization()(x)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/base_layer.py", line 842, in __call__
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/layers/normalization.py", line 659, in call
    outputs = self._fused_batch_norm(inputs, training=training)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/layers/normalization.py", line 517, in _fused_batch_norm
    training, _fused_batch_norm_training, _fused_batch_norm_inference)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/utils/tf_utils.py", line 59, in smart_cond
    pred, true_fn=true_fn, false_fn=false_fn, name=name)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/smart_cond.py", line 59, in smart_cond
    name=name)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/control_flow_ops.py", line 1174, in cond
    return cond_v2.cond_v2(pred, true_fn, false_fn, name)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/cond_v2.py", line 84, in cond_v2
    op_return_value=pred)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/func_graph.py", line 915, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/layers/normalization.py", line 503, in _fused_batch_norm_training
    data_format=self._data_format)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/nn_impl.py", line 1509, in fused_batch_norm
    name=name)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/gen_nn_ops.py", line 4620, in fused_batch_norm_v3
    name=name)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/op_def_library.py", line 631, in _apply_op_helper
    param_name=input_name)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/op_def_library.py", line 60, in _SatisfiesTypeConstraint
    ", ".join(dtypes.as_dtype(x).name for x in allowed_list)))
TypeError: Value passed to parameter 'x' has DataType float64 not in list of allowed values: float16, bfloat16, float32
&lt;/denchmark-code&gt;

So is there perhaps another (hopefully drop in) method of batch normalization that supports float64? I don't want to go hacking at allowed_list and all that.
	</description>
	<comments>
		<comment id='1' author='tb438' date='2019-10-16T06:56:58Z'>
		&lt;denchmark-link:https://github.com/tb438&gt;@tb438&lt;/denchmark-link&gt;

In order to expedite the trouble-shooting process, please provide a minimal standalone code to reproduce the issue reported here. Thanks!
		</comment>
		<comment id='2' author='tb438' date='2019-10-25T10:43:17Z'>
		&lt;denchmark-link:https://github.com/tb438&gt;@tb438&lt;/denchmark-link&gt;

Please, let us know any update on this issue. Thanks!
		</comment>
		<comment id='3' author='tb438' date='2019-10-25T21:43:45Z'>
		Yes give me another day and I'll upload sanitized code.
		</comment>
		<comment id='4' author='tb438' date='2019-10-29T15:23:57Z'>
		Hi. I have a similar problem.
I used this keras code &lt;denchmark-link:https://github.com/keras-team/keras/blob/master/examples/cifar10_resnet.py&gt;https://github.com/keras-team/keras/blob/master/examples/cifar10_resnet.py&lt;/denchmark-link&gt;
.
And it works fine with TF2.
But if I set  K.set_floatx('float64')
It writed "Value passed to parameter 'x' has DataType float64 not in list of allowed values: float16, bfloat16, float32"
Are there working examples, where batch normalization works with float64 in TF 2?
		</comment>
		<comment id='5' author='tb438' date='2019-10-30T02:20:58Z'>
		Hi Ravikyram,
Here is the code and exception:
Python 3.7.5rc1 (default, Oct  8 2019, 16:47:45)
[GCC 9.2.1 20191008] on linux
Type "help", "copyright", "credits" or "license" for more information.



import numpy as np
import tensorflow as tf
print(tf.version)
2.0.0
tf.keras.backend.set_floatx('float64')
inputs = tf.keras.layers.Input(shape=(28, 28, 1))
x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, padding='same')(inputs)
[...]
x = tf.keras.layers.LeakyReLU(0.2)(x)
x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, padding='same')(x)
x = tf.keras.layers.BatchNormalization()(x)
Traceback (most recent call last):
File "", line 1, in 
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/base_layer.py", line 842, in call
outputs = call_fn(cast_inputs, *args, **kwargs)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/layers/normalization.py", line 659, in call
outputs = self._fused_batch_norm(inputs, training=training)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/layers/normalization.py", line 517, in _fused_batch_norm
training, _fused_batch_norm_training, _fused_batch_norm_inference)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/utils/tf_utils.py", line 59, in smart_cond
pred, true_fn=true_fn, false_fn=false_fn, name=name)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/smart_cond.py", line 59, in smart_cond
name=name)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/util/deprecation.py", line 507, in new_func
return func(*args, **kwargs)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/control_flow_ops.py", line 1174, in cond
return cond_v2.cond_v2(pred, true_fn, false_fn, name)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/cond_v2.py", line 84, in cond_v2
op_return_value=pred)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/func_graph.py", line 915, in func_graph_from_py_func
func_outputs = python_func(*func_args, **func_kwargs)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/layers/normalization.py", line 503, in _fused_batch_norm_training
data_format=self._data_format)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/nn_impl.py", line 1509, in fused_batch_norm
name=name)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/gen_nn_ops.py", line 4620, in fused_batch_norm_v3
name=name)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/op_def_library.py", line 631, in _apply_op_helper
param_name=input_name)
File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/op_def_library.py", line 60, in _SatisfiesTypeConstraint
", ".join(dtypes.as_dtype(x).name for x in allowed_list)))
TypeError: Value passed to parameter 'x' has DataType float64 not in list of allowed values: float16, bfloat16, float32



		</comment>
		<comment id='6' author='tb438' date='2019-10-30T10:03:43Z'>
		I have tried on colab with TF version 2.0 , 2.1.0-dev20191029 and was able to reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/e6adf13651041e582a832373063de997/untitled319.ipynb&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='7' author='tb438' date='2019-10-30T15:09:38Z'>
		Hi Ravikyram,
Thank you for the prompt response. Using your gist above on Colab, I am still getting the same error?
2.0.0:
&lt;denchmark-code&gt;---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-12-fffaff38b358&gt; in &lt;module&gt;()
      5 x = tf.keras.layers.LeakyReLU(0.2)(x)
      6 x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, padding='same')(x)
----&gt; 7 x = tf.keras.layers.BatchNormalization()(x)

13 frames
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py in _SatisfiesTypeConstraint(dtype, attr_def, param_name)
     58           "allowed values: %s" %
     59           (param_name, dtypes.as_dtype(dtype).name,
---&gt; 60            ", ".join(dtypes.as_dtype(x).name for x in allowed_list)))
     61 
     62 

TypeError: Value passed to parameter 'x' has DataType float64 not in list of allowed values: float16, bfloat16, float32
&lt;/denchmark-code&gt;

And with 2.1.0:
&lt;denchmark-code&gt;---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-3-fffaff38b358&gt; in &lt;module&gt;()
      5 x = tf.keras.layers.LeakyReLU(0.2)(x)
      6 x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, padding='same')(x)
----&gt; 7 x = tf.keras.layers.BatchNormalization()(x)

13 frames
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py in _SatisfiesTypeConstraint(dtype, attr_def, param_name)
     59           "allowed values: %s" %
     60           (param_name, dtypes.as_dtype(dtype).name,
---&gt; 61            ", ".join(dtypes.as_dtype(x).name for x in allowed_list)))
     62 
     63 

TypeError: Value passed to parameter 'x' has DataType float64 not in list of allowed values: float16, bfloat16, float32
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='tb438' date='2019-10-30T15:28:08Z'>
		I've updated my development workstation to 2.1.0 as well, same issue:
&lt;denchmark-code&gt;&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; tf.version
'2.1.0-dev20191029'
&gt;&gt;&gt; tf.keras.backend.set_floatx('float64')
&gt;&gt;&gt; inputs = tf.keras.layers.Input(shape=(28, 28, 1))
&gt;&gt;&gt; x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, padding='same')(inputs)
&gt;&gt;&gt; x = tf.keras.layers.LeakyReLU(0.2)(x)
&gt;&gt;&gt; x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, padding='same')(x)
&gt;&gt;&gt; x = tf.keras.layers.BatchNormalization()(x)
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
  File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/base_layer.py", line 773, in __call__
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/layers/normalization.py", line 695, in call
    outputs = self._fused_batch_norm(inputs, training=training)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/layers/normalization.py", line 553, in _fused_batch_norm
    training, _fused_batch_norm_training, _fused_batch_norm_inference)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/utils/tf_utils.py", line 59, in smart_cond
    pred, true_fn=true_fn, false_fn=false_fn, name=name)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/smart_cond.py", line 59, in smart_cond
    name=name)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/control_flow_ops.py", line 1174, in cond
    return cond_v2.cond_v2(pred, true_fn, false_fn, name)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/cond_v2.py", line 83, in cond_v2
    op_return_value=pred)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/func_graph.py", line 958, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/layers/normalization.py", line 539, in _fused_batch_norm_training
    data_format=self._data_format)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/nn_impl.py", line 1502, in fused_batch_norm
    name=name)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/gen_nn_ops.py", line 4248, in fused_batch_norm_v3
    name=name)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/op_def_library.py", line 576, in _apply_op_helper
    param_name=input_name)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/op_def_library.py", line 61, in _SatisfiesTypeConstraint
    ", ".join(dtypes.as_dtype(x).name for x in allowed_list)))
TypeError: Value passed to parameter 'x' has DataType float64 not in list of allowed values: float16, bfloat16, float32
&lt;/denchmark-code&gt;

Can you try running the same code with GPU-enabled Colab? Perhaps this would only fixed for the CPU branch and not tensorflow-gpu 2.1.0?
Thanks again for your help.
		</comment>
		<comment id='10' author='tb438' date='2019-11-25T05:16:42Z'>
		LOOK, AGAIN, YOUR REDUCED PRECISION INT8 AND INT16 AINT MEAN NOTHING TO US, DO YOU UNDERSTAND GOOGLE? YOUR GOOGLE TPU CAIN'T TRAIN, SO IMA GO WITH CUDA128 JUST AS SOON AS YOU BRING MY FLOAT64 BATCH NORMALIZATION BABY BACK
		</comment>
		<comment id='11' author='tb438' date='2020-01-08T01:03:18Z'>
		Hello,
Is there a solution for this?
i am getting it when i call the model.predict on boolean data.
it also causes a segmentation fault.
i am using tf2`s keras
		</comment>
		<comment id='12' author='tb438' date='2020-01-10T13:54:55Z'>
		Hi all, supporting float64 would be really cool indeed :)
&lt;denchmark-link:https://github.com/TrailBlazerAI&gt;@TrailBlazerAI&lt;/denchmark-link&gt;
 maybe consider a change of tone when asking for other people to help you?
		</comment>
		<comment id='13' author='tb438' date='2020-01-11T02:05:07Z'>
		
Hello,
Is there a solution for this?
i am getting it when i call the model.predict on boolean data.
it also causes a segmentation fault.
i am using tf2`s keras

I changed the data to np.float32 and it worked well with tf2, maybe it may help someone still having this issue.
Thanks
		</comment>
		<comment id='14' author='tb438' date='2020-02-11T14:15:56Z'>
		Hello,
Can anyone provide info on the matter?
Is it gonna be fixed? If no, why?Â Is there a timeline? Are there any blockers?
Many thanks,

Julien

		</comment>
		<comment id='15' author='tb438' date='2020-03-02T11:09:15Z'>
		Are there any updates to this? I'm facing the same problem and I'm having to set BatchNorm's dtype to float32 for now but it'd be nice to be able to use float64
		</comment>
		<comment id='16' author='tb438' date='2020-03-10T16:11:38Z'>
		This appears to be an issue with fused batch norm in particular, and if you disable the fused kernel, the code above does not raise an error:
&lt;denchmark-code&gt;import numpy as np
import tensorflow as tf
print(tf.version)
tf.keras.backend.set_floatx('float64')
inputs = tf.keras.layers.Input(shape=(28, 28, 1))
x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, padding='same')(inputs)
x = tf.keras.layers.LeakyReLU(0.2)(x)
x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, padding='same')(x)
x = tf.keras.layers.BatchNormalization(fused=False)(x)
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/reedwm&gt;@reedwm&lt;/denchmark-link&gt;
 / &lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;
 -- this is an unfortunate hard edge for BatchNorm. Can we either add fp64 support to the fused kernel or not default to fusing for unsupported datatypes?
		</comment>
		<comment id='17' author='tb438' date='2020-03-10T17:40:14Z'>
		I'll start by not defaulting to fused for unsupported datatypes. We can add fp64 support to the fused kernel later.
		</comment>
		<comment id='18' author='tb438' date='2020-03-10T20:49:04Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33365&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33365&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>