<bug id='45919' author='innat' open_date='2020-12-22T11:30:37Z' closed_time='2021-01-01T19:08:25Z'>
	<summary>Error using custom activation function while mixed precision enable</summary>
	<description>
Config
&lt;denchmark-code&gt;TensorFlow 2.3
&lt;/denchmark-code&gt;

I already posted in &lt;denchmark-link:https://stackoverflow.com/questions/65403976/typeerror-using-custom-activation-function-while-mixed-precision-enabled&gt;SO&lt;/denchmark-link&gt;
, a few hours ago without any response until now. But I need a quick pointer.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Problem&lt;/denchmark-h&gt;

I was trying to use a custom activation in mixed-precision enabled training pipelines but faced the following error:
&lt;denchmark-code&gt;TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float16 of argument 'x'.
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Reproduce&lt;/denchmark-h&gt;

Enabling Mixed precision...
&lt;denchmark-code&gt;import tensorflow as tf 

policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')
tf.keras.mixed_precision.experimental.set_policy(policy)
print('Mixed precision enabled')
&lt;/denchmark-code&gt;

Custom activation...
&lt;denchmark-code&gt;def ARelu(x, alpha=0.90, beta=2.0):
    alpha = tf.clip_by_value(alpha, clip_value_min=0.01, clip_value_max=0.99)
    beta  = 1 + tf.math.sigmoid(beta)
    return tf.nn.relu(x) * beta - tf.nn.relu(-x) * alpha
&lt;/denchmark-code&gt;

Training...
&lt;denchmark-code&gt;import tensorflow as tf

(xtrain, ytrain), (xtest, ytest) = tf.keras.datasets.mnist.load_data()

def pre_process(inputs, targets):
    inputs  = tf.expand_dims(inputs, -1)
    targets = tf.one_hot(targets, depth=10)
    return tf.divide(inputs, 255), targets

train_data = tf.data.Dataset.from_tensor_slices((xtrain, ytrain)).\
    take(10_000).shuffle(10_000).batch(8).map(pre_process)
test_data = tf.data.Dataset.from_tensor_slices((xtest, ytest)).\
    take(1_000).shuffle(1_000).batch(8).map(pre_process)

model = tf.keras.Sequential([
                             
            tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), strides=(1, 1),
                                   input_shape=(28, 28, 1), activation=ARelu),
            tf.keras.layers.MaxPool2D(pool_size=(2, 2)),

            tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), 
                                   activation=ARelu),
            tf.keras.layers.MaxPool2D(pool_size=(2, 2)),

            tf.keras.layers.Flatten(),
            tf.keras.layers.Dense(64, activation=ARelu), 
            tf.keras.layers.Dense(10, activation='softmax', dtype=tf.float32)]) 

opt = tf.keras.optimizers.Adam()

model.compile(loss='categorical_crossentropy', optimizer=opt)
history = model.fit(train_data, validation_data=test_data, epochs=10)

# ------------------

TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float16 of argument 'x'.
&lt;/denchmark-code&gt;

However, without mixed-precision, it works. I understand the problem simply types miss match but where I should look into it?
Additionally, while trying to solve it, I've found that using tf.keras.mixed_precision.LossScaleOptimizer is safe to avoid numeric underflow. Is it something that we should use for mixed-precision training?
	</description>
	<comments>
		<comment id='1' author='innat' date='2020-12-23T10:43:34Z'>
		Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/641251e2fb3ab5eeb4d30fc646d43f21/45919-2-4.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='innat' date='2020-12-23T12:47:22Z'>
		Are you telling me to get the gist or to the person whom you assigned? lolz 
		</comment>
		<comment id='3' author='innat' date='2020-12-24T10:29:51Z'>
		&lt;denchmark-link:https://github.com/innat&gt;@innat&lt;/denchmark-link&gt;
,

Are you telling me to get the gist or to the person whom you assigned? lolz 

It is an instruction to next level Engineers, so is the status, Awaiting Tensorflower.
		</comment>
		<comment id='4' author='innat' date='2020-12-25T18:03:49Z'>
		&lt;denchmark-h:h2&gt;Solved&lt;/denchmark-h&gt;

I have to typecast the input
&lt;denchmark-code&gt;def ARelu(x, alpha=0.90, beta=2.0):
    alpha = tf.clip_by_value(alpha, clip_value_min=0.01, clip_value_max=0.99)
    beta  = 1 + tf.math.sigmoid(beta)
	x = tf.cast(x, 'float32')
    return tf.nn.relu(x) * beta - tf.nn.relu(-x) * alpha
&lt;/denchmark-code&gt;

But I'm not sure by doing this, do we lose the benefit of mixed-precision as well.
		</comment>
		<comment id='5' author='innat' date='2021-01-01T19:08:26Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45919&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45919&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>