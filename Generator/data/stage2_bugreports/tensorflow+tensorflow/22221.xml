<bug id='22221' author='girving' open_date='2018-09-11T23:50:15Z' closed_time='2018-09-18T16:02:01Z'>
	<summary>Threading data out of one while loop into another interferes with gradients</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.13.6
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): b'v1.9.0-rc2-3217-g8e5c118ce8' 1.10.0
Python version: 3.6.2
Bazel version (if compiling from source): 0.15.2-homebrew
GCC/Compiler version (if compiling from source): Apple LLVM version 9.1.0 (clang-902.0.39.2)
CUDA/cuDNN version: n/a
GPU model and memory: n/a
Exact command to reproduce: Run included script
Mobile device: n/a

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

I have some code which

Generates some data using an inference-only tf.while_loop.
Uses a second while loop to run several minibatches of Adam, using the data from the first loop.

Both while loops have back_prop=False.  The second loop computes gradients, but these are used only inside the second loop (which includes running the train_op).  I use tf.stop_gradients to prevent gradients from flowing backwards from the second loop to the first...but to no avail:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "./while-bug", line 26, in &lt;module&gt;
    back_prop=False)
  File "/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 3281, in while_loop
    return_same_structure)
  File "/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 3001, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File "/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2936, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File "/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 3245, in &lt;lambda&gt;
    body = lambda i, lv: (i + 1, orig_body(lv))
  File "./while-bug", line 16, in body
    train = tf.train.AdamOptimizer(learning_rate=1e-2).minimize(loss)
  File "/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py", line 401, in minimize
    grad_loss=grad_loss)
  File "/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py", line 517, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File "/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 610, in gradients
    gate_gradients, aggregation_method, stop_gradients)
  File "/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 674, in _GradientsHelper
    to_ops, from_ops, colocate_gradients_with_ops, func_graphs, xs)
  File "/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 204, in _PendingCount
    between_op_list, between_ops, colocate_gradients_with_ops)
  File "/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 1442, in MaybeCreateControlFlowState
    loop_state.AddWhileContext(op, between_op_list, between_ops)
  File "/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 1247, in AddWhileContext
    grad_state = GradLoopState(forward_ctxt, outer_grad_state)
  File "/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 824, in __init__
    cnt, forward_index = forward_ctxt.AddForwardLoopCounter(outer_grad_state)
  File "/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2580, in AddForwardLoopCounter
    name="f_count")
  File "/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 249, in _Enter
    data, frame_name, is_constant, parallel_iterations, name=name)
  File "/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/ops/gen_control_flow_ops.py", line 179, in enter
    parallel_iterations=parallel_iterations, name=name)
  File "/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py", line 488, in new_func
    return func(*args, **kwargs)
  File "/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 3254, in create_op
    op_def=op_def)
  File "/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1787, in __init__
    self._control_flow_post_processing()
  File "/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1796, in _control_flow_post_processing
    control_flow_util.CheckInputFromValidContext(self, input_tensor.op)
  File "/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_util.py", line 322, in CheckInputFromValidContext
    raise ValueError(error_msg + " See info log for more details.")
ValueError: Cannot use 'while_1/gradients/f_count_1' as input to 'while_1/gradients/f_count' because they are in different while loops. See info log for more details.
&lt;/denchmark-code&gt;

(Caveat: line numbers for the above stacktrace may be slightly wrong, since my TF has some debug print statements.)
I've traced the problem to ops/gradients_impl.py:_PendingCount or the surrounding code.  _PendingCount seems to trace right through tf.stop_gradients.  I'm not sure where the right fix is, though, so could use some control flow expert help.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

Here's a minimized test case:
&lt;denchmark-code&gt;import tensorflow as tf

# First while loop
rollouts = tf.while_loop(
    cond=lambda _: True,
    body=lambda _: tf.get_variable('b', []),
    loop_vars=[tf.zeros([])],
    maximum_iterations=1,
    back_prop=False)
rollouts = tf.stop_gradient(rollouts)

def body(i):
    loss = tf.stop_gradient(rollouts)
    train = tf.train.AdamOptimizer(learning_rate=1e-2).minimize(loss)
    with tf.control_dependencies([train]):
        return i + 1

# Second while loop.  Crashes since it thinks the gradients depend on
# something from the previous while loop.
tf.while_loop(
    cond=lambda _: True,
    body=body, maximum_iterations=1, parallel_iterations=1,
    loop_vars=(tf.zeros((), dtype=tf.int32),),
    back_prop=False)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='girving' date='2018-09-12T06:44:41Z'>
		Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.
Mobile device
		</comment>
		<comment id='2' author='girving' date='2018-09-12T15:23:35Z'>
		^ Fixed.
		</comment>
		<comment id='3' author='girving' date='2018-09-12T17:41:50Z'>
		I also tried using the stop_gradients argument to tf.gradients, but the problem persists: it still thinks gradients are flowing out of the while loop.
		</comment>
		<comment id='4' author='girving' date='2018-09-12T18:55:31Z'>
		&lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
 recommended  as a workaround, and that seems to work great (as long as variables are created with ).  So this is still a bug, but it's no longer blocking me.
		</comment>
		<comment id='5' author='girving' date='2018-09-18T00:57:56Z'>
		&lt;denchmark-link:https://github.com/girving&gt;@girving&lt;/denchmark-link&gt;
 Hi, Can this be closed or any fix is being implemented ?
		</comment>
		<comment id='6' author='girving' date='2018-09-18T05:07:15Z'>
		&lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='7' author='girving' date='2018-09-18T16:02:01Z'>
		This is still a bug and expected to be fixed only when we transition to while_v2 as described in &lt;denchmark-link:https://github.com/tensorflow/community/pull/13&gt;tensorflow/community#13&lt;/denchmark-link&gt;
 . I'll close this for now.
		</comment>
		<comment id='8' author='girving' date='2020-03-11T00:06:14Z'>
		Please file a separate issue for your problem with instructions to
reproduce.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Tue, Mar 10, 2020 at 3:40 PM Stanley Gan ***@***.***&gt; wrote:
 I am also experiencing this error on TF1.15 when trying to compute
 Laplacian, with 1st gradient of some operations overwritten using decorator
 @tf.custom_gradient. I computed the Laplacian using tf.while_loop by
 creating each row of Hessian and summing relevant parts. With this code
 implemented without overwriting the 1st gradient, my code runs fine.
 However, when I introduce my code in overwriting 1st gradient, which
 involves another tf.while_loop solely for tensor manipulation and
 multiplication, it spits out this error. I changed to using
 tf.GradientTape() but apparently the gradient computed is None. I am
 wondering how exactly did you solve this or any suggestions on how I should
 I approach this issue?

 —
 You are receiving this because you modified the open/close state.
 Reply to this email directly, view it on GitHub
 &lt;#22221?email_source=notifications&amp;email_token=AAABHROW4OHDKAKAQBJBN3DRG26URA5CNFSM4FURUWJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEONOVIQ#issuecomment-597355170&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/AAABHRIDNITTXVAN7NDOTO3RG26URANCNFSM4FURUWJA&gt;
 .


-- 
 - Alex

		</comment>
		<comment id='9' author='girving' date='2020-03-11T00:25:21Z'>
		&lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
. Sorry I just deleted my comment as it is pretty vague and not a good way to explain my error. I am thinking if this should be a separate issue, but I ran the same example as described by &lt;denchmark-link:https://github.com/girving&gt;@girving&lt;/denchmark-link&gt;
 in this issue on TF1.15, the error still persists.
		</comment>
		<comment id='10' author='girving' date='2020-03-11T00:31:10Z'>
		And what about nightly?
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Tue, Mar 10, 2020 at 5:25 PM Stanley Gan ***@***.***&gt; wrote:
 @alextp &lt;https://github.com/alextp&gt;. Sorry I just deleted my comment as
 it is pretty vague and not a good way to explain my error. I am thinking if
 this should be a separate issue, but I ran the same example as described by
 @girving &lt;https://github.com/girving&gt; on TF1.15, the error still persists.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#22221?email_source=notifications&amp;email_token=AAABHRP6COBEJV73LZHXMDTRG3LAFA5CNFSM4FURUWJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEONVVWA#issuecomment-597383896&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/AAABHRKX2PD3GTX4JGWYNCLRG3LAFANCNFSM4FURUWJA&gt;
 .


-- 
 - Alex

		</comment>
		<comment id='11' author='girving' date='2020-03-11T00:55:06Z'>
		There are no more nightly builds for version 1.15 on pypi.
		</comment>
		<comment id='12' author='girving' date='2020-03-11T14:50:07Z'>
		I mean the 2.x branch; the 1.15 branch will get no further fixes.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Tue, Mar 10, 2020 at 5:55 PM Stanley Gan ***@***.***&gt; wrote:
 There are no more nightly builds for version 1.15 on pypi.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#22221?email_source=notifications&amp;email_token=AAABHRNOKRQ34D2PSYLOFOLRG3OPXA5CNFSM4FURUWJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEONXIOI#issuecomment-597390393&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/AAABHRMVVKAD5A7RXGL5TUTRG3OPXANCNFSM4FURUWJA&gt;
 .


-- 
 - Alex

		</comment>
		<comment id='13' author='girving' date='2020-03-11T15:43:44Z'>
		&lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
 Thanks for your reply! For specific reason (package dependencies), the code which I wrote is in 1.15, hence even if running on TF2 works, I still have to figure out a way as my code is written in 1.15. Unless you are suggesting that some other fixes which I am not aware of? Though based on the community link which you posted: &lt;denchmark-link:https://github.com/tensorflow/community/pull/13&gt;tensorflow/community#13&lt;/denchmark-link&gt;
, I checked the commits to fix this were merged into 1.15 branch (unless I am missing something!).
		</comment>
		<comment id='14' author='girving' date='2020-03-11T15:53:09Z'>
		So I need an example to reproduce this against nightly, which is 2.x, so I
can debug it.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Wed, Mar 11, 2020 at 8:44 AM Stanley Gan ***@***.***&gt; wrote:
 @alextp &lt;https://github.com/alextp&gt; Thanks for your reply! For specific
 reason (package dependencies), the code which I wrote is in 1.15, hence
 even if running on TF2 works, I still have to figure out a way as my code
 is written in 1.15. Unless you are suggesting that some other fixes which I
 am not aware of? Though based on the community link which you posted:
 tensorflow/community#13 &lt;tensorflow/community#13&gt;,
 I checked the commits to fix this were merged into 1.15 branch (unless I am
 missing something!).

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#22221 (comment)&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/AAABHRIA5M4VM7WVFLI7F6LRG6WULANCNFSM4FURUWJA&gt;
 .


-- 
 - Alex

		</comment>
		<comment id='15' author='girving' date='2020-03-11T17:21:22Z'>
		Using girving's example, ran using tf-nightly-2.2.0-dev20200311
&lt;denchmark-code&gt;import tensorflow as tf
  
# First while loop
rollouts = tf.while_loop(
    cond=lambda _: True,
    body=lambda _: tf.compat.v1.get_variable('b', []),
    loop_vars=[tf.zeros([])],
    maximum_iterations=1,
    back_prop=False)
rollouts = tf.stop_gradient(rollouts)

def body(i):
    loss = tf.stop_gradient(rollouts)
    train = tf.train.AdamOptimizer(learning_rate=1e-2).minimize(loss)
    with tf.control_dependencies([train]):
        return i + 1

# Second while loop.  Crashes since it thinks the gradients depend on
# something from the previous while loop.
tf.while_loop(
    cond=lambda _: True,
    body=body, maximum_iterations=1, parallel_iterations=1,
    loop_vars=(tf.zeros((), dtype=tf.int32),),
    back_prop=False)
&lt;/denchmark-code&gt;

I got a different error as shown below
&lt;denchmark-code&gt;2020-03-11 09:05:04.518937: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-03-11 09:05:04.531475: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ff78acc51e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-03-11 09:05:04.531492: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
WARNING:tensorflow:From dummy_example.py:9: calling while_loop_v2 (from tensorflow.python.ops.control_flow_ops) with back_prop=False is deprecated and will be removed in a future version.
Instructions for updating:
back_prop=False is deprecated. Consider using tf.stop_gradient instead.
Instead of:
results = tf.while_loop(c, b, vars, back_prop=False)
Use:
results = tf.nest.map_structure(tf.stop_gradient, tf.while_loop(c, b, vars))
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/tensorflow/python/util/nest.py", line 378, in assert_same_structure
    expand_composites)
ValueError: The two structures don't have the same nested structure.

First structure: type=list str=[TensorSpec(shape=(), dtype=tf.int32, name=None), [TensorSpec(shape=(), dtype=tf.float32, name=None)]]

Second structure: type=list str=[1, &lt;tf.Variable 'b:0' shape=() dtype=float32, numpy=-0.98595977&gt;]

More specifically: Substructure "type=list str=[TensorSpec(shape=(), dtype=tf.float32, name=None)]" is a sequence, while substructure "type=ResourceVariable str=&lt;tf.Variable 'b:0' shape=() dtype=float32, numpy=-0.98595977&gt;" is not

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "dummy_example.py", line 9, in &lt;module&gt;
    back_prop=False)
  File "/usr/local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py", line 574, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2491, in while_loop_v2
    return_same_structure=True)
  File "/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2731, in while_loop
    nest.assert_same_structure(loop_var_structure, list(loop_vars))
  File "/usr/local/lib/python3.6/site-packages/tensorflow/python/util/nest.py", line 385, in assert_same_structure
    % (str(e), str1, str2))
ValueError: The two structures don't have the same nested structure.

First structure: type=list str=[TensorSpec(shape=(), dtype=tf.int32, name=None), [TensorSpec(shape=(), dtype=tf.float32, name=None)]]

Second structure: type=list str=[1, &lt;tf.Variable 'b:0' shape=() dtype=float32, numpy=-0.98595977&gt;]

More specifically: Substructure "type=list str=[TensorSpec(shape=(), dtype=tf.float32, name=None)]" is a sequence, while substructure "type=ResourceVariable str=&lt;tf.Variable 'b:0' shape=() dtype=float32, numpy=-0.98595977&gt;" is not
Entire first structure:
[., [.]]
Entire second structure:
[., .]


&lt;/denchmark-code&gt;

		</comment>
		<comment id='16' author='girving' date='2020-03-11T17:24:18Z'>
		This is an unrelated bug in the example code where it's using tf.while_loop
incorrectly and returning variables where tf expects tensors, I think.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Wed, Mar 11, 2020 at 10:21 AM Stanley Gan ***@***.***&gt; wrote:
 Using girving's example, ran using tf-nightly-2.2.0-dev20200311

 import tensorflow as tf

 # First while loop
 rollouts = tf.while_loop(
     cond=lambda _: True,
     body=lambda _: tf.compat.v1.get_variable('b', []),
     loop_vars=[tf.zeros([])],
     maximum_iterations=1,
     back_prop=False)
 rollouts = tf.stop_gradient(rollouts)

 def body(i):
     loss = tf.stop_gradient(rollouts)
     train = tf.train.AdamOptimizer(learning_rate=1e-2).minimize(loss)
     with tf.control_dependencies([train]):
         return i + 1

 # Second while loop.  Crashes since it thinks the gradients depend on
 # something from the previous while loop.
 tf.while_loop(
     cond=lambda _: True,
     body=body, maximum_iterations=1, parallel_iterations=1,
     loop_vars=(tf.zeros((), dtype=tf.int32),),
     back_prop=False)

 I got a different error as shown below

 2020-03-11 09:05:04.518937: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
 2020-03-11 09:05:04.531475: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ff78acc51e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
 2020-03-11 09:05:04.531492: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
 WARNING:tensorflow:From dummy_example.py:9: calling while_loop_v2 (from tensorflow.python.ops.control_flow_ops) with back_prop=False is deprecated and will be removed in a future version.
 Instructions for updating:
 back_prop=False is deprecated. Consider using tf.stop_gradient instead.
 Instead of:
 results = tf.while_loop(c, b, vars, back_prop=False)
 Use:
 results = tf.nest.map_structure(tf.stop_gradient, tf.while_loop(c, b, vars))
 Traceback (most recent call last):
   File "/usr/local/lib/python3.6/site-packages/tensorflow/python/util/nest.py", line 378, in assert_same_structure
     expand_composites)
 ValueError: The two structures don't have the same nested structure.

 First structure: type=list str=[TensorSpec(shape=(), dtype=tf.int32, name=None), [TensorSpec(shape=(), dtype=tf.float32, name=None)]]

 Second structure: type=list str=[1, &lt;tf.Variable 'b:0' shape=() dtype=float32, numpy=-0.98595977&gt;]

 More specifically: Substructure "type=list str=[TensorSpec(shape=(), dtype=tf.float32, name=None)]" is a sequence, while substructure "type=ResourceVariable str=&lt;tf.Variable 'b:0' shape=() dtype=float32, numpy=-0.98595977&gt;" is not

 During handling of the above exception, another exception occurred:

 Traceback (most recent call last):
   File "dummy_example.py", line 9, in &lt;module&gt;
     back_prop=False)
   File "/usr/local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py", line 574, in new_func
     return func(*args, **kwargs)
   File "/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2491, in while_loop_v2
     return_same_structure=True)
   File "/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2731, in while_loop
     nest.assert_same_structure(loop_var_structure, list(loop_vars))
   File "/usr/local/lib/python3.6/site-packages/tensorflow/python/util/nest.py", line 385, in assert_same_structure
     % (str(e), str1, str2))
 ValueError: The two structures don't have the same nested structure.

 First structure: type=list str=[TensorSpec(shape=(), dtype=tf.int32, name=None), [TensorSpec(shape=(), dtype=tf.float32, name=None)]]

 Second structure: type=list str=[1, &lt;tf.Variable 'b:0' shape=() dtype=float32, numpy=-0.98595977&gt;]

 More specifically: Substructure "type=list str=[TensorSpec(shape=(), dtype=tf.float32, name=None)]" is a sequence, while substructure "type=ResourceVariable str=&lt;tf.Variable 'b:0' shape=() dtype=float32, numpy=-0.98595977&gt;" is not
 Entire first structure:
 [., [.]]
 Entire second structure:
 [., .]



 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#22221 (comment)&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/AAABHRICXL4P664CMAMHHTLRG7CCRANCNFSM4FURUWJA&gt;
 .


-- 
 - Alex

		</comment>
		<comment id='17' author='girving' date='2020-03-11T17:24:43Z'>
		Let me try to fix it.
		</comment>
		<comment id='18' author='girving' date='2020-03-11T18:22:31Z'>
		Should I run it with tf.compat.v1.disable_v2_behavior()? Because v2 is eagerly executed.
		</comment>
	</comments>
</bug>