<bug id='30702' author='VinothInspirit' open_date='2019-07-15T09:11:27Z' closed_time='2019-08-08T01:50:06Z'>
	<summary>Variation in Tensorflow execution latency</summary>
	<description>
System information:
Neural Network: 	LeNet 300-100
Target Platform: 	XEON PLATINUM 8000 SERIES - AMAZON AWS EC2 C5 INSTANCE
Supports:  A		VX-512, 72 Cores, 2-4 Threads; Up to 2 Instance Cores and 2 vCPUs
Baseline Framework: 	TensorFlow (AWS Image Version)
AMI ID: 		Deep Learning AMI (Ubuntu) Version 23.1 (ami-07262a45de118922e)
Python version:		p27
Describe the problem:
&lt;denchmark-code&gt;		To collect baseline results for TensorFlow model of lenet_300_100 ( Find the Accuracy and inference latency). By using MNIST dataset with a batch size of 16 and full(10000) number of batches.
&lt;/denchmark-code&gt;

Analysis Tensorflow inference runtime results, There is inconsistent on subsequent sessions. Please review the attached “ref_times” for  inconsistent runtime for various batches.
&lt;denchmark-link:https://user-images.githubusercontent.com/51189885/61205763-943e4380-a70e-11e9-8ead-568ddd829416.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3391578/ref_times.txt&gt;ref_times.txt&lt;/denchmark-link&gt;

Please review the following logs and advice on this.
Source code / logs:
Time session source code:
with tf.Session(graph=graph, config=config) as sess:
curr_batch = 0
warm_up = 5
for i in range(warm_up):
sess.run(output, feed_dict={input_: zeros})
while True:
batch = input_reader.receive_batch()
if batch is None:
break
start = monotonic.monotonic()
pred = sess.run(output, feed_dict={input_: batch})
sys.stderr.write('Batch Inference Time(s): {}\n'.format(
monotonic.monotonic() - start))
output_writer.output_send(pred)
Please let know need another details? Please review all and advice me on this.
	</description>
	<comments>
		<comment id='1' author='VinothInspirit' date='2019-07-16T10:51:05Z'>
		&lt;denchmark-link:https://github.com/VinothInspirit&gt;@VinothInspirit&lt;/denchmark-link&gt;
 ,
Please provide us TF version and also the complete code being used so that we can replicate the issue from our end.Thanks!
		</comment>
		<comment id='2' author='VinothInspirit' date='2019-07-17T10:27:39Z'>
		Thanks for your response.
I have pasted the complete source code for your evaluation, Results of evaluation time text file to understand the inconsistency inference time
And TF version : 1.13.1.
Please review and advice me on this
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3401386/Results_ref_times.txt&gt;Results_ref_times.txt&lt;/denchmark-link&gt;

`
#!/usr/bin/env python
&lt;denchmark-h:h1&gt;coding: utf-8&lt;/denchmark-h&gt;

&lt;denchmark-h:h1&gt;In[2]:&lt;/denchmark-h&gt;

import tensorflow as tf
import numpy as np
import sys
from scipy.io import savemat
import matplotlib.pyplot as plt
from tensorflow.examples.tutorials.mnist import input_data
&lt;denchmark-h:h1&gt;In[3]:&lt;/denchmark-h&gt;

mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)
&lt;denchmark-h:h1&gt;In[107]:&lt;/denchmark-h&gt;

output_graph =  "./lenet-300.pb"
&lt;denchmark-h:h1&gt;In[41]:&lt;/denchmark-h&gt;

&lt;denchmark-h:h1&gt;Parameters&lt;/denchmark-h&gt;

learning_rate_ini = 0.001
training_epochs = 2
batch_size = 16
display_step = 1
num_batches = 100
&lt;denchmark-h:h1&gt;In[33]:&lt;/denchmark-h&gt;

&lt;denchmark-h:h1&gt;Network Parameters&lt;/denchmark-h&gt;

n_hidden_1 = 300  # 1st layer num features
n_hidden_2 = 100  # 2nd layer num features
n_classes = 10  # MNIST total classes (0-9 digits)
&lt;denchmark-h:h1&gt;In[34]:&lt;/denchmark-h&gt;

&lt;denchmark-h:h1&gt;tf Graph input&lt;/denchmark-h&gt;

tf.reset_default_graph()
with tf.name_scope("input_tensor"):
x = tf.placeholder("float", [None, 784])
with tf.name_scope("label_tensor"):
y = tf.placeholder("float", [None, n_classes])
&lt;denchmark-h:h1&gt;In[35]:&lt;/denchmark-h&gt;

&lt;denchmark-h:h1&gt;Create model&lt;/denchmark-h&gt;

def model(_X, _W, _biases):
layer_1 = tf.nn.relu(tf.add(tf.matmul(_X, _W['fc1']), _biases['fc1']))  # Hidden layer with RELU activation
layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, _W['fc2']), _biases['fc2']))      # Hidden layer with RELU activation
with tf.name_scope("predictions"):
pred = tf.matmul(layer_2, _W['out']) + _biases['out']
return pred
&lt;denchmark-h:h1&gt;In[36]:&lt;/denchmark-h&gt;

&lt;denchmark-h:h1&gt;Store layers weight &amp; bias&lt;/denchmark-h&gt;

W = {
'fc1': tf.Variable(tf.random_normal([784, n_hidden_1], stddev=0.01)),
'fc2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2], stddev=0.01)),
'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes], stddev=0.01))
}
biases = {
'fc1': tf.Variable(tf.random_normal([n_hidden_1], stddev=0.01)),
'fc2': tf.Variable(tf.random_normal([n_hidden_2], stddev=0.01)),
'out': tf.Variable(tf.random_normal([n_classes], stddev=0.01))
}
&lt;denchmark-h:h1&gt;In[37]:&lt;/denchmark-h&gt;

import time
&lt;denchmark-h:h1&gt;In[42]:&lt;/denchmark-h&gt;

if(1):
# Construct model
pred = model(x, W, biases)
loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))  # Softmax loss
cost = loss
optimizer = tf.train.AdamOptimizer(learning_rate_ini, beta1=0.9, beta2=0.999,
epsilon=1e-08, use_locking=False).minimize(cost)
# Test model
correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))
# Calculate accuracy
accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))
# Initializing the variables
init = tf.initialize_all_variables()
# Run model
accuracy_total = None
config = tf.ConfigProto(device_count={'GPU': 0})
config.log_device_placement = True
with tf.Session(config=config) as sess:
sess.run(init)
# Training cycle
for epoch in range(training_epochs):
avg_loss = 0.
total_batch = int(mnist.train.num_examples/batch_size)
# Loop over all batches
for i in range(total_batch):
batch_x, batch_y = mnist.train.next_batch(batch_size)
# Run optimization op (backprop) and cost op (to get loss value)
accuracy_total, predictions,_, l = sess.run([accuracy,pred, optimizer, loss],
feed_dict={x: batch_x,
y: batch_y})
# Compute average loss
avg_loss += l / total_batch
# Display logs per epoch step
if epoch % display_step == 0:
print("Epoch:", '%04d' % (epoch+1), "loss=",                     "{:.9f}".format(avg_loss))
print("Optimization Finished!")
print("Accuracy : ", accuracy_total)
for i in range(num_batches):
batch_x, batch_y = mnist.test.next_batch(batch_size)
start_time = time.time()
val_acc = accuracy.eval({x: batch_x , y: batch_y})
print("Evaluation time batch {} : {}".format(i+1, time.time()-start_time))
print("Validation Acc : {}".format(val_acc))
&lt;denchmark-h:h1&gt;In[ ]:&lt;/denchmark-h&gt;

`
		</comment>
		<comment id='3' author='VinothInspirit' date='2019-07-18T11:21:39Z'>
		&lt;denchmark-link:https://github.com/VinothInspirit&gt;@VinothInspirit&lt;/denchmark-link&gt;
,
When i tried executing the last section of the code i faced the following .Kindly help us to reproduce the issue.Thanks!
		</comment>
		<comment id='4' author='VinothInspirit' date='2019-07-18T12:00:20Z'>
		&lt;denchmark-code&gt;#!/usr/bin/env python
# coding: utf-8


import tensorflow as tf
import numpy as np
import sys
from scipy.io import savemat
import matplotlib.pyplot as plt
from tensorflow.examples.tutorials.mnist import input_data

mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)


output_graph =  "./lenet-300.pb"




# Parameters
learning_rate_ini = 0.001
training_epochs = 2
batch_size = 16
display_step = 1
num_batches = 100



# Network Parameters
n_hidden_1 = 300  # 1st layer num features
n_hidden_2 = 100  # 2nd layer num features
n_classes = 10  # MNIST total classes (0-9 digits)



# tf Graph input
tf.reset_default_graph()
with tf.name_scope("input_tensor"):
    x = tf.placeholder("float", [None, 784])
with tf.name_scope("label_tensor"):
    y = tf.placeholder("float", [None, n_classes])




# Create model
def model(_X, _W, _biases):
    layer_1 = tf.nn.relu(tf.add(tf.matmul(_X, _W['fc1']), _biases['fc1']))  # Hidden layer with RELU activation
    layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, _W['fc2']), _biases['fc2']))      # Hidden layer with RELU activation
    with tf.name_scope("predictions"):
        pred = tf.matmul(layer_2, _W['out']) + _biases['out']
    return pred



# Store layers weight &amp; bias
W = {
    'fc1': tf.Variable(tf.random_normal([784, n_hidden_1], stddev=0.01)),
    'fc2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2], stddev=0.01)),
    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes], stddev=0.01))
}


biases = {
    'fc1': tf.Variable(tf.random_normal([n_hidden_1], stddev=0.01)),
    'fc2': tf.Variable(tf.random_normal([n_hidden_2], stddev=0.01)),
    'out': tf.Variable(tf.random_normal([n_classes], stddev=0.01))
}


import time




if(1):
    # Construct model
    pred = model(x, W, biases)
    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))  # Softmax loss
    cost = loss
    optimizer = tf.train.AdamOptimizer(learning_rate_ini, beta1=0.9, beta2=0.999,
            epsilon=1e-08, use_locking=False).minimize(cost)
    # Test model
    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))
    # Calculate accuracy
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))
    # Initializing the variables
    init = tf.initialize_all_variables()
    # Run model
    accuracy_total = None
    config = tf.ConfigProto(device_count={'GPU': 0})
    config.log_device_placement = True
    with tf.Session(config=config) as sess:
        sess.run(init)
        # Training cycle
        for epoch in range(training_epochs):
            avg_loss = 0.
            total_batch = int(mnist.train.num_examples/batch_size)
            # Loop over all batches
            for i in range(total_batch):
                batch_x, batch_y = mnist.train.next_batch(batch_size)
                # Run optimization op (backprop) and cost op (to get loss value)
                accuracy_total, predictions,_, l = sess.run([accuracy,pred, optimizer, loss], 
                                                            feed_dict={x: batch_x,
                                                              y: batch_y})
                # Compute average loss
                avg_loss += l / total_batch
            # Display logs per epoch step
            if epoch % display_step == 0:
                print("Epoch:", '%04d' % (epoch+1), "loss=",                     "{:.9f}".format(avg_loss))
        print("Optimization Finished!")
        print("Accuracy : ", accuracy_total)
        for i in range(num_batches):
            batch_x, batch_y = mnist.test.next_batch(batch_size)
            start_time = time.time()
            val_acc = accuracy.eval({x: batch_x , y: batch_y})
            print("Evaluation time batch {} : {}".format(i+1, time.time()-start_time))
            print("Validation Acc : {}".format(val_acc))

&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='VinothInspirit' date='2019-07-18T12:00:55Z'>
		I've pasted the indented code. This should work.
		</comment>
		<comment id='6' author='VinothInspirit' date='2019-07-19T19:36:40Z'>
		&lt;denchmark-link:https://github.com/azaks2&gt;@azaks2&lt;/denchmark-link&gt;
 do you know off the top of your head what could cause this?
		</comment>
		<comment id='7' author='VinothInspirit' date='2019-07-19T20:16:32Z'>
		&lt;denchmark-link:https://github.com/azaks2&gt;@azaks2&lt;/denchmark-link&gt;
 replied offline:

A profile would be nice
We do not generally encourage inference via python, so python gc comes to mind
3 We overlap with the computation of next_batch

		</comment>
		<comment id='8' author='VinothInspirit' date='2019-07-22T11:46:57Z'>
		Reg:2. We do not generally encourage inference via python, so python gc comes to mind
VinothInspirit: What then would be the tf prescribed way of running(and measuring) inferences?
Reg:3. We overlap with the computation of next_batch
VinothInspirit: We give the inputs here using feed_dict, not by a pipeline, so I'm not sure if sess.run(...) would still overlap with next_batch.
		</comment>
		<comment id='9' author='VinothInspirit' date='2019-07-25T02:00:42Z'>
		
For inference take a look at https://www.tensorflow.org/tfx/guide/serving
One important part is that the process will turn variables into constants.
According to the timeline (comparing slow and fast sess.run) it seems most of the diff comes from the MatMul op. I am not 100% sure but using taskset to run the script (taskset 0x1 python ...) improved the variance for my runs.

slow inference
&lt;denchmark-link:https://user-images.githubusercontent.com/53194144/61839822-e33d5480-ae43-11e9-90c0-bf267c7e3edd.png&gt;&lt;/denchmark-link&gt;

fast inference
&lt;denchmark-link:https://user-images.githubusercontent.com/53194144/61839867-0cf67b80-ae44-11e9-9a12-29b6e60bf929.png&gt;&lt;/denchmark-link&gt;

to get timeline
val_acc = sess.run([accuracy],
feed_dict={x: batch_x , y: batch_y},
options=tf.RunOptions(trace_level=tf.RunOptions.SOFTWARE_TRACE),
run_metadata=run_metadata)
...
tl = timeline.Timeline(run_metadata.step_stats)
ctf = tl.generate_chrome_trace_format()
with open(str(i)+'timeline.json', 'w') as f:
f.write(ctf)
		</comment>
		<comment id='10' author='VinothInspirit' date='2019-08-08T01:50:07Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=30702&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=30702&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>