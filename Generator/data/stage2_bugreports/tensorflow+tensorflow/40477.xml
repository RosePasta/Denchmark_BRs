<bug id='40477' author='anferico' open_date='2020-06-15T16:13:28Z' closed_time='2020-06-18T16:10:41Z'>
	<summary>Post-training full integer quantization produces model with float inputs/outputs</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Running from Colab notebook
TensorFlow installed from (source or binary): Running from Colab notebook
TensorFlow version (or github SHA if from source): v2.2.0-0-g2b96f3662b


Colab notebook:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb&lt;/denchmark-link&gt;

Failure details
The conversion is successful, however the quantized model with float inputs/outputs (/tmp/mnist_tflite_models/mnist_model_quant.tflite) and the one with supposed int8 inputs/outputs (/tmp/mnist_tflite_models/mnist_model_quant_io.tflite) are identical. I've verified this by running
&lt;denchmark-code&gt;diff mnist_model_quant.tflite mnist_model_quant_io.tflite
&lt;/denchmark-code&gt;

which resulted in an empty output (that is, the files are identical).

Here's what  looks like when I open it with netron:
&lt;denchmark-link:https://user-images.githubusercontent.com/30210403/84680403-f1500400-af32-11ea-980f-4a7b4b013dc9.png&gt;&lt;/denchmark-link&gt;

Rather than having a "Quantize" node right after the float input and a "Dequantize" node right before the float output, I would like to have int8 inputs/outputs directly. How to do that?
	</description>
	<comments>
		<comment id='1' author='anferico' date='2020-06-18T16:10:41Z'>
		Hi,
by default the post training tool keeps the input and output types float so that users can conveniently use the model interchangeably with their float model code. Users can change the input type and output type via the inferece_input_type and inference_output_type flags.
The colab here &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb&lt;/denchmark-link&gt;
 has an example of that usage and these flags.
Hope that helps!
		</comment>
		<comment id='2' author='anferico' date='2020-06-18T16:10:43Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40477&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40477&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='anferico' date='2020-06-19T09:16:48Z'>
		
Hi,
by default the post training tool keeps the input and output types float so that users can conveniently use the model interchangeably with their float model code. Users can change the input type and output type via the inferece_input_type and inference_output_type flags.
The colab here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb has an example of that usage and these flags.
Hope that helps!

Actually, the notebook you've referenced is the one I've referenced too. Setting inference_input_type and inference_output_type to tf.uint8 (or tf.int8) still results in a model with float inputs/outputs, as witnessed by the fact that mnist_model_quant.tflite andmnist_model_quant_io.tflite are identical.
I managed to solve the problem by updating to TensorFlow nightly 2.3.0, so it appears to be a problem with TensorFlow 2.2.0. With TF 2.3.0, I can obtain a model with int8 inputs/outputs.
I guess this issue can be closed now, although I believe you should at least update that notebook, because it doesn't work properly with TF 2.2.0
		</comment>
	</comments>
</bug>