<bug id='42834' author='ha3an' open_date='2020-08-31T18:34:57Z' closed_time='2020-09-01T16:01:49Z'>
	<summary>Nan Loss with Adam in MultiWorkerMirroredStrategy</summary>
	<description>
Hello
I have a custom training loop that uses MultiWorkerMirroredStrategy. I use tf.keras.losses.SparseCategoricalCrossentropy as the loss function and tf.keras.optimizers.Adam as the optimizer. I noticed that when I train the network with 4 nodes (each has 1 P100 16GB GPU) using gRPC communication layer, loss becomes NaN after some steps. With the same code, this time I use only 1 node with 4 V100 32GB GPUs and training works smoothly without any issue. For example:
&lt;denchmark-code&gt;### with 4 nodes the lost sharply drops with few steps
Step      10 ( 3.21%): loss = 3.98384 EER=0.38987 
Step      20 ( 6.41%): loss = 3.71272 EER=0.30024 
Step      30 ( 9.62%): loss = 3.50568 EER=0.25225 
Step      40 (12.82%): loss = 3.32774 EER=0.21305 
Step      50 (16.03%): loss = 3.19242 EER=0.19232 
Step      60 (19.23%): loss = 3.09014 EER=0.17402 
Step      80 (23.64%): loss = 0000nan EER=0.00000
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;with 1 node and 4 GPUs is working fine
Step       0 ( 0.00%): loss = 4.00920 EER=0.40084 
Step      10 ( 0.83%): loss = 3.92024 EER=0.38026 
Step      20 ( 1.67%): loss = 3.78321 EER=0.34209 
Step      30 ( 2.50%): loss = 3.77414 EER=0.33503 
Step      40 ( 3.33%): loss = 3.70165 EER=0.31749 
Step      50 ( 4.17%): loss = 3.65104 EER=0.30819 
Step      60 ( 5.00%): loss = 3.64083 EER=0.30599 
Step      70 ( 5.83%): loss = 3.58572 EER=0.29448 
Step      80 ( 6.67%): loss = 3.57196 EER=0.29532 
&lt;/denchmark-code&gt;

Could the problem be the GPUs? or the communication layer?
I also tested with low learning rate and did not help with Adam optimizer.
Thanks
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution: 'Red Hat Enterprise Linux Server', '7.7'
TensorFlow installed from: binary
TensorFlow version: v2.3.0-rc2-23-gb36436b087 2.3.0
Python version:  Python 3.7.7
CUDA/cuDNN version: 10.1
GPU model and memory: P100 16 GB and V100 32GB

	</description>
	<comments>
		<comment id='1' author='ha3an' date='2020-09-01T04:49:47Z'>
		&lt;denchmark-link:https://github.com/ha3an&gt;@ha3an&lt;/denchmark-link&gt;

Please provide with simple stand alone code for us to replicate the issue faced or if possible share a colab gist.
		</comment>
		<comment id='2' author='ha3an' date='2020-09-01T16:01:49Z'>
		It would take a while until I can create a stand alone code. I will close this for now. Thanks
		</comment>
		<comment id='3' author='ha3an' date='2020-09-01T16:01:51Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42834&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42834&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>