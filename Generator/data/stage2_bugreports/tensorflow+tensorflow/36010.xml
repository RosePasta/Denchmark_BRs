<bug id='36010' author='Jbiloki' open_date='2020-01-18T00:55:53Z' closed_time='2020-02-07T23:16:05Z'>
	<summary>Custom layer go_backwards does not work</summary>
	<description>
I am implementing a custom recurrent class that inherits tf.layers.Layer, when using the Bidirectional wrapper I get the error:
&lt;denchmark-code&gt;---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input-3-7bd5b5269810&gt; in &lt;module&gt;
----&gt; 1 a = TimeDistributed(Bidirectional(char_recurrent_cell))

~/opt/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/wrappers.py in __init__(self, layer, merge_mode, weights, backward_layer, **kwargs)
    434     if backward_layer is None:
    435       self.backward_layer = self._recreate_layer_from_config(
--&gt; 436           layer, go_backwards=True)
    437     else:
    438       self.backward_layer = backward_layer

~/opt/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/wrappers.py in _recreate_layer_from_config(self, layer, go_backwards)
    493     config = layer.get_config()
    494     if go_backwards:
--&gt; 495       config['go_backwards'] = not config['go_backwards']
    496     if 'custom_objects' in tf_inspect.getfullargspec(
    497         layer.__class__.from_config).args:

KeyError: 'go_backwards'
&lt;/denchmark-code&gt;

This is the code for the layer itself:
&lt;denchmark-code&gt;class RecurrentConfig(BaseLayer):
    '''Basic configurable recurrent layer'''
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.layers: List[layers.Layer] = stack_layers(self.params,
                                                       self.num_layers,
                                                       self.layer_name)

    def call(self, inputs: np.ndarray) -&gt; layers.Layer:
        '''This function is a sequential/functional call to this layers logic
        Args:
            inputs: Array to be processed within this layer
        Returns:
            inputs processed through this layer'''
        processed = inputs
        for layer in self.layers:
            processed = layer(processed)
        return processed

    @staticmethod
    def default_params() -&gt; Dict[Any, Any]:
        return{
            'units': 32,
            'recurrent_initializer': 'glorot_uniform',
            'dropout': 0,
            'recurrent_dropout': 0,
            'activation': None,
            'return_sequences': True
        }
&lt;/denchmark-code&gt;

I have attempted to add the go_backwards to the config that is retrieved when get_config() is called but this results in another error:
&lt;denchmark-code&gt;---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-3-7bd5b5269810&gt; in &lt;module&gt;
----&gt; 1 a = TimeDistributed(Bidirectional(char_recurrent_cell))

~/opt/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/wrappers.py in __init__(self, layer, merge_mode, weights, backward_layer, **kwargs)
    430     # Recreate the forward layer from the original layer config, so that it will
    431     # not carry over any state from the layer.
--&gt; 432     self.forward_layer = self._recreate_layer_from_config(layer)
    433 
    434     if backward_layer is None:

~/opt/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/wrappers.py in _recreate_layer_from_config(self, layer, go_backwards)
    506       return layer.__class__.from_config(config, custom_objects=custom_objects)
    507     else:
--&gt; 508       return layer.__class__.from_config(config)
    509 
    510   @tf_utils.shape_type_conversion

~/opt/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in from_config(cls, config)
    517         A layer instance.
    518     """
--&gt; 519     return cls(**config)
    520 
    521   def compute_output_shape(self, input_shape):

~/nlpv3-general/nlp-lib/src/main/python/mosaix_py/mosaix_learn/layers/recurrent_layers.py in __init__(self, *args, **kwargs)
     12     '''Basic configurable recurrent layer'''
     13     def __init__(self, *args, **kwargs):
---&gt; 14         super().__init__(*args, **kwargs)
     15         self.layers: List[layers.Layer] = stack_layers(self.params,
     16                                                        self.num_layers,

~/nlpv3-general/nlp-lib/src/main/python/mosaix_py/mosaix_learn/layers/base_layer.py in __init__(self, params, mode, layer_name, num_layers, cust_name, **kwargs)
     17                  cust_name: str = '',
     18                  **kwargs):
---&gt; 19         super().__init__(params, mode, **kwargs)
     20         self.layer_name = layer_name
     21         self.cust_name = cust_name

~/nlpv3-general/nlp-lib/src/main/python/mosaix_py/mosaix_learn/configurable.py in __init__(self, params, mode, **kwargs)
     61 
     62     def __init__(self, params: Dict[AnyStr, Any], mode: ModeKeys, **kwargs):
---&gt; 63         super().__init__(**kwargs) #type: ignore
     64         self._params = _parse_params(params, self.default_params())
     65         self._mode = mode

~/opt/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    455     self._self_setattr_tracking = False  # pylint: disable=protected-access
    456     try:
--&gt; 457       result = method(self, *args, **kwargs)
    458     finally:
    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

~/opt/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __init__(self, trainable, name, dtype, dynamic, **kwargs)
    184     }
    185     # Validate optional keyword arguments.
--&gt; 186     generic_utils.validate_kwargs(kwargs, allowed_kwargs)
    187 
    188     # Mutable properties

~/opt/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/generic_utils.py in validate_kwargs(kwargs, allowed_kwargs, error_message)
    716   for kwarg in kwargs:
    717     if kwarg not in allowed_kwargs:
--&gt; 718       raise TypeError(error_message, kwarg)

TypeError: ('Keyword argument not understood:', 'go_backwards')
&lt;/denchmark-code&gt;

Version info is:
tf_version: '2.1.0-dev20191125'
git_version: 'v1.12.1-19144-gf39f4ea3fa'
	</description>
	<comments>
		<comment id='1' author='Jbiloki' date='2020-01-20T10:12:23Z'>
		&lt;denchmark-link:https://github.com/Jbiloki&gt;@Jbiloki&lt;/denchmark-link&gt;
, can you provide the complete code to replicate the issue. Thanks
		</comment>
		<comment id='2' author='Jbiloki' date='2020-01-20T20:02:16Z'>
		Sure, here's a small example
&lt;denchmark-code&gt;import tensorflow as tf
class DummyLayer(tf.keras.layers.Layer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.a = tf.keras.layers.LSTM(2)
    
    def call(inputs):
        return self.a(inputs)
    
tf.keras.layers.Bidirectional(DummyLayer())
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='Jbiloki' date='2020-01-21T10:59:54Z'>
		Was able to reproduce the issue with 2.1 and 2.2.0.dev20200121.
Please find the &lt;denchmark-link:https://colab.research.google.com/gist/gadagashwini/08f950c8e5c24bca3e5db5531df29f4a/untitled352.ipynb&gt;gist&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='4' author='Jbiloki' date='2020-01-30T01:09:55Z'>
		I could reproduce the issue with . &lt;denchmark-link:https://colab.research.google.com/gist/jvishnuvardhan/8bae6dfdf3bd7cc94f89f16d6101ee7f/untitled801.ipynb&gt;Here&lt;/denchmark-link&gt;
 is the gist for our reference. Thanks!
		</comment>
		<comment id='5' author='Jbiloki' date='2020-02-07T18:48:29Z'>
		Hi &lt;denchmark-link:https://github.com/Jbiloki&gt;@Jbiloki&lt;/denchmark-link&gt;
.
Thanks for reporting the issue. For any layer that need to work with Bidirectional wrapper, it need to support processing the inputs in a reversed order. For all the default Keras RNN layers (LSTM/GRU), they all have a init param (go_backwards), and when it is configured to True, it will process the inputs in the reverse order.
You will need to update your custom RNN layer to have that init param as well if you want to use it with Bidirectional wrapper. So far the docstring for Bidirectional didn't state this point clearly, and we will update that.
As a workaround, you can also choose to implement a custom RNN cell, which define the math calculation for one single time step. Passing the cell to base RNN layer and wrap the RNN layer with Bidirectional wrapper, and the default RNN layer will handle the go_backwards correctly.
		</comment>
		<comment id='6' author='Jbiloki' date='2020-02-07T20:26:03Z'>
		&lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;
 Thank you for the response, I believe for our environment the optimal solution is to have layers implement the go_backwards in the  in the future we may implement custom cells which we can try the alternative method.
For now I am trying your initial suggestion but I am not sure I understand the correct implementation. I am getting keyword argument error not understood for go_backwards
&lt;denchmark-code&gt;
import tensorflow as tf
class DummyLayer(tf.keras.layers.Layer):
    def __init__(self, *args, **kwargs):
        super().__init__(go_backwards=True, *args, **kwargs)
        #self.a = tf.keras.layers.LSTM(2, go_backwards=True)
    
    def call(inputs):
        return None#self.a(inputs)
    
tf.keras.layers.Bidirectional(DummyLayer())
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='Jbiloki' date='2020-02-07T21:34:23Z'>
		Writing the custom layer with go_backwards will take quite some work, some sample code will be like:
&lt;denchmark-code&gt;import tensorflow as tf

class DummyLayer(tf.keras.layers.Layer):
  def __init__(self, go_backwards=False, *args, **kwargs):
    super(DummyLayer, self).__init__(*args, **kwargs)
    self.go_backwards = go_backwards
    self.a = tf.keras.layers.LSTM(2, go_backwards=self.go_backwards)

  def call(self, inputs):
    return self.a(inputs)

  @property
  def return_sequences(self):
    return self.a.return_sequences

  @property
  def return_state(self):
    return self.a.return_state

  def get_config(self):
    config = {'go_backwards': self.go_backwards}
    base_config = super(DummyLayer, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))

tf.keras.layers.Bidirectional(DummyLayer())
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='Jbiloki' date='2020-02-07T22:46:49Z'>
		I see, thank you for the starter I will see if I can get it working!
		</comment>
		<comment id='9' author='Jbiloki' date='2020-02-07T23:16:06Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36010&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36010&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='Jbiloki' date='2020-02-07T23:16:59Z'>
		We have update the docstring with more details about the requirement for the custom layer.
		</comment>
	</comments>
</bug>