<bug id='24744' author='grobman' open_date='2019-01-07T19:18:10Z' closed_time='2019-01-09T01:09:51Z'>
	<summary>why is there different latency for post-training quantization and quantization-aware trained models?</summary>
	<description>
System information

TensorFlow version: TFlite
Doc Link:https://www.tensorflow.org/lite/performance/model_optimization

in the model optimization documentation page:
&lt;denchmark-link:https://www.tensorflow.org/lite/performance/model_optimization&gt;https://www.tensorflow.org/lite/performance/model_optimization&lt;/denchmark-link&gt;

Table 1 gives a latency of 145ms for post-training quantization and 80.2 for quantization-aware training.
it is unclear to me why there should be a difference in latency as both methods results in models that use 8-bit operation. The documentation implies that the benefit of using quantization aware training is to increase model accuracy when using int8, but from the table it seems that latency is also improved. why is that?
	</description>
	<comments>
		<comment id='1' author='grobman' date='2019-01-08T13:30:52Z'>
		The reason is well explained in another doc, the &lt;denchmark-link:https://www.tensorflow.org/lite/performance/post_training_quantization&gt;post_training_quantization&lt;/denchmark-link&gt;
 page. In short, models quantized by post-training quantization compute using floating point kernels; models quantized by quantization aware training use fixed-point kernels.
		</comment>
		<comment id='2' author='grobman' date='2019-01-08T14:39:10Z'>
		Thanks for the reply!
However, I still find it confusing as the move from floating kerenl to fixed point kernels is done via graph_freezing as explained in the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/quantize/README.md&gt;quantization-aware training&lt;/denchmark-link&gt;
 doc.
Is there no way to freeze the eval graph created by post training quantization to get the full speed up offered by using fixed point kernels? After all these graphs are conceptually the same. It would seem to be a shame to force a user to go through the train process if she only wishes to gain the latency speed-up.
A slightly unrelated issue that I cannot find any documentation on - does the TFlite quantization use channel-wise quantization as explained in [1] or layer-wise quantization?
[1] &lt;denchmark-link:https://arxiv.org/abs/1806.08342&gt;Quantizing deep convolutional networks for efficient inference: A whitepaper&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='grobman' date='2019-01-09T01:09:51Z'>
		Hello &lt;denchmark-link:https://github.com/grobman&gt;@grobman&lt;/denchmark-link&gt;
 , we would like to thank &lt;denchmark-link:https://github.com/freedomtan&gt;@freedomtan&lt;/denchmark-link&gt;
 for addressing your post-training quantization vs. quantization-aware training query in TFLite.
Please kindly submit a follow up issue for your query:
A slightly unrelated issue that I cannot find any documentation on - does the TFlite quantization use channel-wise quantization as explained in [1] or layer-wise quantization? &lt;denchmark-link:https://arxiv.org/abs/1806.08342&gt;https://arxiv.org/abs/1806.08342&lt;/denchmark-link&gt;

Thanks. This issue is closed.
		</comment>
	</comments>
</bug>