<bug id='36136' author='Ryandry1st' open_date='2020-01-22T17:03:26Z' closed_time='2020-05-29T19:00:49Z'>
	<summary>TPU Socket Closed</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
Platform: Colab TPU
TensorFlow version (use command below): 2.1.0rc1


&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/4098637/TPU_Socket_Traceback.pdf&gt;TPU_Socket_Traceback.pdf&lt;/denchmark-link&gt;

Using model.fit raises an error which does not occur with GPU backend.
UnavailableError: Socket closed Additional GRPC error information: {"created":"@1579711911.023658059","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Socket closed","grpc_status":14}
Describe the expected behavior
Code should run the same in both instances.

&lt;denchmark-link:https://colab.research.google.com/drive/156Q2BsM9gVS3cvDytovQzEGKmjJpP4s8&gt;https://colab.research.google.com/drive/156Q2BsM9gVS3cvDytovQzEGKmjJpP4s8&lt;/denchmark-link&gt;

Other info / logs
Attached.
	</description>
	<comments>
		<comment id='1' author='Ryandry1st' date='2020-02-07T15:14:50Z'>
		same error here when I use tpu on colab, and  any update?
		</comment>
		<comment id='2' author='Ryandry1st' date='2020-02-07T22:13:33Z'>
		Same issue here
		</comment>
		<comment id='3' author='Ryandry1st' date='2020-02-14T00:36:50Z'>
		I'm encountering the same issue
		</comment>
		<comment id='4' author='Ryandry1st' date='2020-02-23T11:30:31Z'>
		Same issue here
		</comment>
		<comment id='5' author='Ryandry1st' date='2020-02-23T13:56:34Z'>
		Same here on non-Colab TPU
		</comment>
		<comment id='6' author='Ryandry1st' date='2020-02-23T16:57:57Z'>
		
Same here on non-Colab TPU

Answering my own question, turns out there was a bug with my host_call function. So this error message strongly suggests that there's something wrong in the model implementation, especially the parts that invokes host machine functions.
		</comment>
		<comment id='7' author='Ryandry1st' date='2020-02-28T03:36:32Z'>
		Any chance we can get an update on this? It seems like a pretty fundamental problem for using TPUs, and support was introduced a while ago suggesting it should work in simple cases at the least. Just to know that some progress is being made, or someone is working on this, would be great.
		</comment>
		<comment id='8' author='Ryandry1st' date='2020-02-28T04:38:48Z'>
		
Any chance we can get an update on this? It seems like a pretty fundamental problem for using TPUs, and support was introduced a while ago suggesting it should work in simple cases at the least. Just to know that some progress is being made, or someone is working on this, would be great.

Hi &lt;denchmark-link:https://github.com/Ryandry1st&gt;@Ryandry1st&lt;/denchmark-link&gt;
 ,
Took a quick look at your code and this seems to be the same problem as Issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/36996&gt;#36996&lt;/denchmark-link&gt;
 .
I changed your code to


and everything works as expected:

Train on 10000 samples
Epoch 1/10
10000/10000 [==============================] - 5s 547us/sample - loss: 0.1424 - mse: 0.1419
Epoch 2/10
10000/10000 [==============================] - 2s 164us/sample - loss: 0.0872 - mse: 0.0872
...

Also there's a small typo, it should be metrics=['mse'].
Hope that helps!
		</comment>
		<comment id='9' author='Ryandry1st' date='2020-02-28T05:01:13Z'>
		Yes, it appears that issue is in reference to the same problem. It is interesting that the issue seems to be solvable by simply casting the data to float32.
Also trying to cast the data into other types causes it to break in a different way as well:

float64 leads to the same socket closing error
float16 produces a warning about missing an Identity for the OpKernel for TPU for DT_HALF, because it appears to be missing from the list.
int16 does the same as float16
int32 does the same as int16 and float16
unsigned integers have the same output as signed integers
complex64 has the same missing OpKernel
float128 does not have an associated TF data type
booleans also have the missing OpKernel

So it appears that we are limited to float32 representations for everything.
Also the typo was intentional, I was desiring to see the mean absolute error as well.
Here is the full error for other types besides float32:
No registered 'Identity' OpKernel for 'TPU' devices compatible with node {{node Identity}}
(OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_HALF
.  Registered:  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_HALF, DT_UINT32, DT_UINT64, DT_RESOURCE, DT_VARIANT]
device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_HALF, DT_UINT32, DT_UINT64, DT_RESOURCE, DT_VARIANT]
device='XLA_TPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_BFLOAT16, DT_UINT32, DT_UINT64, DT_RESOURCE, DT_VARIANT]
device='XLA_CPU'; T in [DT_UINT8, DT_QUINT8, DT_UINT16, DT_INT8, DT_QINT8, ..., DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_BFLOAT16]
device='TPU'; T in [DT_INT32, DT_UINT32, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE, DT_BOOL, DT_COMPLEX64, DT_INT64, DT_UINT64]
device='TPU_SYSTEM'
device='GPU'; T in [DT_HALF]
device='GPU'; T in [DT_BFLOAT16]
device='GPU'; T in [DT_FLOAT]
device='GPU'; T in [DT_DOUBLE]
device='GPU'; T in [DT_INT64]
device='GPU'; T in [DT_UINT16]
device='GPU'; T in [DT_INT16]
device='GPU'; T in [DT_UINT8]
device='GPU'; T in [DT_INT8]
device='GPU'; T in [DT_COMPLEX64]
device='GPU'; T in [DT_COMPLEX128]
device='GPU'; T in [DT_VARIANT]
device='DEFAULT'; T in [DT_STRING]
device='DEFAULT'; T in [DT_VARIANT]
device='DEFAULT'; T in [DT_RESOURCE]
device='CPU'
&lt;denchmark-code&gt; [[Identity]] [Op:PrefetchDataset]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='Ryandry1st' date='2020-03-18T14:44:27Z'>
		Is there any new progress here? I attempted to run the gist after updating tensorflow to 2.2rc0, however, this fails in tpu initialization, so I cannot test newer versions for a fix.
It has been 2 months though and this encompasses all tpu use on colab that does not use purely 32-bit floating point data, so it is a pretty widespread issue.
		</comment>
		<comment id='11' author='Ryandry1st' date='2020-03-28T18:16:58Z'>
		I am facing a similar issue, for a different script, I am beginning to think that when I use TPU's through google colab, the underlying engine that manages TPU instances just cuts out one of the TPU workers, in an attempt to manage loads, I am not a premium colab user, and colab preimum isnt available in my location yet... and as wild guess this issue would not appear for colab preium users, since they are promised dedicated instances for longer.
The same script works on Kaggle kernels though
		</comment>
		<comment id='12' author='Ryandry1st' date='2020-04-30T18:38:28Z'>
		I suspect this issue is a memory leak.
In my experience I'm seeing a constant slow rise in TPU memory usage while training, until the TPU crashes:
&lt;denchmark-link:https://user-images.githubusercontent.com/6796648/80746558-fc350d80-8b19-11ea-84a6-daabbd67c667.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='Ryandry1st' date='2020-05-13T04:04:11Z'>
		This appears to be an exception raised in a variety of different situations, so it can be quite difficult to debug. Things to try that've solved issues for me thus far:

Cast everything to tf.float32 (i.e. if working with images, using tf.uint8 is a non-starter)
Be careful using tf.dataset.Dataset.cache with large datasets ( @mxbi thanks for the inspiration - this solved my most recent issue).
Use from_logits=True when using a categorical crossentropy loss.
Upgrade to Colab Pro for less disconnects. They do disconnect runtimes from time to time.

I'm sure I'll be back to add more. Would be incredible to have insightful error messages on these.
		</comment>
		<comment id='14' author='Ryandry1st' date='2020-05-29T19:00:49Z'>
		I reran the gist using tf2.2 and found that it runs. I also tried the various data types and it seems to be running now under most cases. The cases that I found it did not work were for:

uint8, uint16, uint64
int8, int16

All other cases worked including booleans and half/double precision floats. When an error occurs it is for a missing OpKernel  type as mentioned earlier.
So it seems like the issue is resolved. An additional issue could (should) be raised to handle the missing datatypes especially since uint8 is pretty fundamental to image processing.
		</comment>
		<comment id='15' author='Ryandry1st' date='2020-05-29T19:00:51Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36136&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36136&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='16' author='Ryandry1st' date='2020-07-21T20:33:37Z'>
		I have the same error when using TPU on TF2.2. It is not an error caused by model implementation because, trying on CPU, it works fine. I'm currently trying to run the code with the following data preparation steps:
&lt;denchmark-code&gt;bs = 128

train_data = tf.data.Dataset.from_tensor_slices((X_data_training.astype('float32'), y_data_training.astype('float32')))
train_data = train_data.shuffle(len(X_data_training)).repeat()
train_data = train_data.batch(bs)

val_data = tf.data.Dataset.from_tensor_slices((X_data_validation.astype('float32'), y_data_validation.astype('float32')))
val_data = val_data.shuffle(len(X_data_validation))
val_data = val_data.batch(bs)
&lt;/denchmark-code&gt;

I've also tried to change data types but without any success. I get the same error even feeding to the fit function the data without using tf.data.Dataset.from_tensor_slices. Any idea about how to quick solve this issue?
		</comment>
		<comment id='17' author='Ryandry1st' date='2020-07-22T05:18:22Z'>
		I am not sure, I have not run into that problem since updating to TF 2.2, although I do use the rc4 which may have a difference. Looking at the snippet you provided, I cannot see any reason to get the error. It also looks like it is something solved, or believed to be solved, in TF 2.3 coming to release candidate soon?
		</comment>
		<comment id='18' author='Ryandry1st' date='2020-07-22T10:24:32Z'>
		Hi, unfortunately, I've just tried the same piece of code after upgrading both my GCP VM and the TPU to TF2.3-rc2 and I am getting the same kind of error, even if with some more details (which are certainly not insightful from my perspective):

UnavailableError: Socket closed
Additional GRPC error information from remote target /job:worker/replica:0/task:0:
:{"created":"@X.X","description":"Error received from peer ipv4:X.X.X.X:X","file":"external/com_github_grpc_grpc/src/core/lib/surface/call.cc","file_line":1056,"grpc_message":"Socket closed","grpc_status":14}

EDIT:
We discovered that the issue disappear if we replace LayerNormalization with BatchNormalization. Therefore, it might be a bug related to LayerNormalization implementation when using TPU.
		</comment>
		<comment id='19' author='Ryandry1st' date='2020-08-05T07:37:10Z'>
		I hit the same issue with 2.2, 2.3 and 2.4 nightly, however it was due to me trying to cache a large dataset into memory, so please do check if you are trying to cache a large dataset!
		</comment>
		<comment id='20' author='Ryandry1st' date='2020-08-27T06:57:57Z'>
		
I hit the same issue with 2.2, 2.3 and 2.4 night ly, however it was due to me trying to cache a large dataset into memory, so please do check if you are trying to cache a large dataset!

How can I check and mitigate this? Apologies if I am new to this.
		</comment>
	</comments>
</bug>