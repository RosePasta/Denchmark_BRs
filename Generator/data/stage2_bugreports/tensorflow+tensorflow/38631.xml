<bug id='38631' author='stefan-falk' open_date='2020-04-17T09:14:48Z' closed_time='2020-04-23T17:38:44Z'>
	<summary>ValueError: No gradients provided for any variable</summary>
	<description>
I just don't see the problem here, so I created two simple examples. One simple Feed Forwad network to solve XOR and one simple LSTM for a word-completion task. The XOR works as it should but the word-completion keeps throwing the ValueError and it's not clear why.
System information

Have I written custom code: Yes
OS Platform and Distribution: Ubuntu 18.04
TensorFlow installed from: pip
TensorFlow version: tf-nightly==2.2.0.dev20200410
Python version: 3.7

Describe the current behavior
Trying to fit() a model raises ValueError: No gradients provided for any variable for no apparent reason.
Describe the expected behavior
It should work as shown in the XOR example.
Standalone code to reproduce the issue
&lt;denchmark-h:h2&gt;Not working: Word Completion&lt;/denchmark-h&gt;

I created an executable &lt;denchmark-link:https://gist.github.com/stefan-falk/42ef89c6636fd9f91fc471584659512f&gt;gist here&lt;/denchmark-link&gt;
.
The exception I am getting:
&lt;denchmark-code&gt;ValueError: No gradients provided for any variable: ['embedding/embeddings:0', 'rnn/gru_cell/kernel:0', 'rnn/gru_cell/recurrent_kernel:0', 'rnn/gru_cell/bias:0', 'rnn_1/gru_cell_1/kernel:0', 'rnn_1/gru_cell_1/recurrent_kernel:0', 'rnn_1/gru_cell_1/bias:0', 'time_distributed/kernel:0', 'time_distributed/bias:0', 'outputs/kernel:0', 'outputs/bias:0'].
&lt;/denchmark-code&gt;


Click to show code
import random
from functools import partial

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow_datasets.core.features.text import SubwordTextEncoder

EOS = '&lt;eos&gt;'
PAD = '&lt;pad&gt;'

RESERVED_TOKENS = [EOS, PAD]
EOS_ID = RESERVED_TOKENS.index(EOS)
PAD_ID = RESERVED_TOKENS.index(PAD)

dictionary = [
    'verstehen',
    'verstanden',
    'vergessen',
    'verlegen',
    'verlernen',
    'vertun',
    'vertan',
    'verloren',
    'verlieren',
    'verlassen',
    'verhandeln',
]

dictionary = [word.lower() for word in dictionary]


def get_model(params) -&gt; keras.models.Model:

    inputs = layers.Input((None,), dtype=tf.int64, name='inputs')

    x = inputs

    vocab_size = params['vocab_size']
    hidden_size = params['hidden_size']
    max_input_length = params['max_input_length']
    max_target_length = params['max_target_length']

    x = layers.Embedding(vocab_size, hidden_size, input_length=max_input_length)(x)

    # Encoder
    x = layers.RNN(layers.GRUCell(hidden_size))(x)
    x = layers.RepeatVector(max_target_length)(x)

    # Deoder
    x = layers.RNN(layers.GRUCell(hidden_size), return_sequences=True)(x)
    x = layers.TimeDistributed(layers.Dense(hidden_size, activation='relu'))(x)

    # Outputs
    output_dense_layer = layers.Dense(vocab_size, activation='softmax')
    outputs = layers.TimeDistributed(output_dense_layer, name='outputs')(x)

    return keras.models.Model(inputs=[inputs], outputs=[outputs])


def sample_generator(text_encoder: SubwordTextEncoder, max_sample: int = None):
    count = 0

    while True:
        random.shuffle(dictionary)

        for word in dictionary:

            for i in range(1, len(word)):

                inputs = word[:i]
                targets = word

                example = dict(
                    inputs=text_encoder.encode(inputs) + [EOS_ID],
                    targets=text_encoder.encode(targets) + [EOS_ID],
                )
                count += 1

                yield example

                if max_sample is not None and count &gt;= max_sample:
                    print('Reached max_samples (%d)' % max_sample)
                    return


def make_dataset(generator_fn, params, training):

    dataset = tf.data.Dataset.from_generator(
        generator_fn,
        output_types={
            'inputs': tf.int64,
            'targets': tf.int64,
        }
    )

    if training:
        dataset = dataset.shuffle(100)

    dataset = dataset.padded_batch(
        params['batch_size'],
        padded_shapes={
            'inputs': (None,),
            'targets': (None,)
        },
    )

    if training:
        dataset = dataset.map(lambda example: to_train_example(example, params=params)).repeat()

    return dataset


def to_train_example(example: dict, params: dict):
    # Make sure targets are one-hot encoded
    example['targets'] = tf.one_hot(example['targets'], depth=params['vocab_size'])
    return example


def main():

    text_encoder = SubwordTextEncoder.build_from_corpus(
        iter(dictionary),
        target_vocab_size=1000,
        max_subword_length=6,
        reserved_tokens=RESERVED_TOKENS
    )

    generator_fn = partial(sample_generator, text_encoder=text_encoder, max_sample=10)

    params = dict(
        batch_size=20,
        vocab_size=text_encoder.vocab_size,
        hidden_size=32,
        max_input_length=30,
        max_target_length=30,
        enable_metrics_in_training=True
    )

    model = get_model(params)

    model.compile(
        optimizer=keras.optimizers.Adam(0.001),
        loss='categorical_crossentropy',
    )

    assert len(model.trainable_variables), 'There are no trainable_variables'
    model.summary()

    train_dataset = make_dataset(generator_fn, params, training=True)

    model.fit(
        train_dataset,
        epochs=5,
        steps_per_epoch=100,
    )


if __name__ == '__main__':
    main()

&lt;denchmark-h:h2&gt;Working: XOR&lt;/denchmark-h&gt;

This example works as it should. Here I am subclassing keras.models.Model. The reason why I am using the functional API above is s.t. I can debug more easily.

Click to show code
import tensorflow as tf
from tensorflow import keras


def xor_data():
    inputs = [[0, 0], [0, 1], [1, 0], [1, 1]]
    targets = [[0], [1], [1], [0]]
    while True:
        for x, y in zip(inputs, targets):
            yield x, y


class FeedForwardNetwork(keras.models.Model):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._layers = [
            keras.layers.Dense(4, activation='sigmoid'),
            keras.layers.Dense(4, activation='sigmoid'),
            keras.layers.Dense(1, activation='sigmoid')
        ]

    def call(self, x, **kwargs):
        for layer in self._layers:
            x = layer(x)
        return x


class XorCallback(keras.callbacks.Callback):

    def on_epoch_end(self, epoch, logs=None):
        all_data = [[0, 0], [0, 1], [1, 0], [1, 1]]
        y = self.model.predict(all_data)

        print('\nPredictions: ')
        print((y &gt; 0.5) * 1)


def main():

    dataset = tf.data.Dataset.from_generator(xor_data, output_types=(tf.int64, tf.int64)).batch(10).shuffle(100)

    train_dataset = dataset.repeat()
    dev_dataset = dataset

    for batch in dataset:
        print(batch)
        break

    model_internal = FeedForwardNetwork()

    inputs = keras.layers.Input(shape=(2,))
    logits = model_internal(inputs)

    model = keras.models.Model(inputs=[inputs], outputs=[logits])

    model.compile(
        optimizer=keras.optimizers.Adam(0.0001),
        loss='mse',
        metrics=['mse']
    )

    model.summary()

    model_fp = '/tmp/xor/model'

    callbacks = [
        keras.callbacks.ModelCheckpoint(
          model_fp,
          save_best_only=True,
          save_weights_only=False
        ),
        XorCallback()
    ]

    model.fit(
        train_dataset,
        epochs=5,
        steps_per_epoch=5000,
        validation_data=dev_dataset,
        validation_steps=100,
        callbacks=callbacks
    )


if __name__ == '__main__':
    main()

&lt;denchmark-link:https://stackoverflow.com/questions/61249708/valueerror-no-gradients-provided-for-any-variable-tensorflow-2-0-keras&gt;stackoverflow&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Potentially related&lt;/denchmark-h&gt;


#1511
#27949

	</description>
	<comments>
		<comment id='1' author='stefan-falk' date='2020-04-20T15:01:13Z'>
		i am able to replicate this issue, please find the &lt;denchmark-link:https://colab.sandbox.google.com/gist/Saduf2019/62f46d7c5e446ca443a763d9c84bec53/38631.ipynb&gt;gist here&lt;/denchmark-link&gt;
 same issue is replicatable on &lt;denchmark-link:https://colab.sandbox.google.com/gist/Saduf2019/cf26b50082f74a3eb15806b7cc08f086/38631.ipynb&gt;2.1 as well&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='stefan-falk' date='2020-04-20T15:07:09Z'>
		&lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;
 I have to say that I am not 100% sure if I am doing everything right w.r.t. dimensions in the word completion example. So maybe there's an issue in the cross-entropy loss function between the targets and the predictions. But it's hard to tell because the exception does not give any clues unfortunately.
		</comment>
		<comment id='3' author='stefan-falk' date='2020-04-23T17:38:44Z'>
		Thanks, &lt;denchmark-link:https://github.com/stefan-falk&gt;@stefan-falk&lt;/denchmark-link&gt;
 . The code above will take some work to debug, and StackOverflow is a better place to do that, as there is a community of users that can help. If there is a specific bug you can point to, please refile with a minimal example and a description of the bug.
		</comment>
		<comment id='4' author='stefan-falk' date='2020-04-23T17:38:46Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38631&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38631&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>