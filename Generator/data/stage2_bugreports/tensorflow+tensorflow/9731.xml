<bug id='9731' author='kemaswill' open_date='2017-05-06T22:26:48Z' closed_time='2017-09-08T12:45:47Z'>
	<summary>More description for optimizers Adagrad, Adadelta, FTRL...</summary>
	<description>
Is it necessary to add more description for optimizers such as Adagrad, Adadelta, FTRL and so on just as what &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer&gt;Adam&lt;/denchmark-link&gt;
 does? Since these are several quite new optimizers and I think it's better to show users more details about these optimizers so that they can understand why do these optimizers work better than SGD in some situations.
If more descriptions are welcomed, I'm glad to make new PRs to do this.
Please go to Stack Overflow for help and support:
&lt;denchmark-link:http://stackoverflow.com/questions/tagged/tensorflow&gt;http://stackoverflow.com/questions/tagged/tensorflow&lt;/denchmark-link&gt;

If you open a GitHub issue, here is our policy:

It must be a bug or a feature request.
The form below must be filled out.

Here's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
TensorFlow installed from (source or binary):
TensorFlow version (use command below):
Bazel version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:
Exact command to reproduce:

You can collect some of this information using our environment capture script:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&lt;/denchmark-link&gt;

You can obtain the TensorFlow version with
python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
	</description>
	<comments>
		<comment id='1' author='kemaswill' date='2017-05-09T01:53:21Z'>
		&lt;denchmark-link:https://github.com/wolffg&gt;@wolffg&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/aselle&gt;@aselle&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='kemaswill' date='2017-05-09T16:09:49Z'>
		What kind of extra language would you want?
		</comment>
		<comment id='3' author='kemaswill' date='2017-05-09T22:16:47Z'>
		Just English, but to show more details of these optimization algorithms, such as how are the parameters updated, how do these methods adapt their learning rates.
		</comment>
		<comment id='4' author='kemaswill' date='2017-05-09T22:39:04Z'>
		&lt;denchmark-link:https://github.com/wolffg&gt;@wolffg&lt;/denchmark-link&gt;
 I'll let you decide how much we want to do here.
		</comment>
		<comment id='5' author='kemaswill' date='2017-06-09T15:03:30Z'>
		Hello,
I have a question about "AdagradOptimizer", I created a node for the learning rates to be updated but I don't see any change from the initial learning rate.
This is the code for the optimizer within the Trainer object that I'm using
self.learning_rate_node = tf.Variable(learning_rate)
optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate).minimize(self.net.cost, global_step=global_step)
Do I have to specify something else in "AdagradOptimizer" for the learning rate to be updated? Do I have to define the decay that I want to use when I create "self.learning_rate_node"? Thanks in advance.
Best regards, Amelia.
		</comment>
		<comment id='6' author='kemaswill' date='2017-09-08T17:14:00Z'>
		This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!
		</comment>
	</comments>
</bug>