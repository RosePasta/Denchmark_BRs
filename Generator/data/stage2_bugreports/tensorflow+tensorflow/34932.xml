<bug id='34932' author='MikeOfZen' open_date='2019-12-07T22:13:28Z' closed_time='2020-01-21T08:49:20Z'>
	<summary>Severe TPU/CPU behaviour discrepency</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):No
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No
TensorFlow installed from (source or binary): pip
TensorFlow version (use command below): 2.1.0-dev20191203
Python version: 3.5
GPU model and memory: TPU (nightly-2.x)

Describe the current behavior
When training using a TPU backend, if a @tf.function function code is defined before connecting to a TPU cluster. Calling the function (as Dataset.map(function)) results in a python kernel crash, without any errors or other info.
This is especially severe (IMHO), since, as the training code base grows, more autograph functions are defined in modules instead of on the main program. As it's natural to import modules at the start of the program, if the TPU connection is initiated after the imports, using the @tf.fucntion code defined in the modules results in a kernel crash.
If the same code is, however, being run just on the CPU, it works as expected.
This leads to a somewhat frustrating experience of everything working on a CPU dev env, and then crashing inexplicably when connecting to a TPU.
The unintuitive solution is to run the TPU connection boilerplate before any imports.
Describe the expected behavior

connecting to a TPU shouldn't create an implicit scope, if a scope is required it should be with a with idiom
It should be better documented if all autograph function definitions should be defined after connecting to a TPU
If a code executed within a TPU scope depends on a code defined outside, it should fail gracefully and informatively (not crash the kernel)

Code to reproduce the issue
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Fail case:
&lt;denchmark-code&gt;import tensorflow as tf

@tf.function
def test_func(a):
    return a**3

train, test = tf.keras.datasets.fashion_mnist.load_data()
images, labels = train
images = images/255
ds = tf.data.Dataset.from_tensor_slices((images))

TPU_IP = '10.0.3.2' #this requires a working nightly-2.x TPU cluster (2v-8)
tpu_address = 'grpc://' + TPU_IP + ':8470'
resolver=tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu_address)
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.experimental.TPUStrategy(resolver)

dsf=ds.map(test_func)
&lt;/denchmark-code&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Working case:
&lt;denchmark-code&gt;import tensorflow as tf

TPU_IP = '10.0.3.2' #this requires a working nightly-2.x TPU cluster (2v-8)
tpu_address = 'grpc://' + TPU_IP + ':8470'
resolver=tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu_address)
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.experimental.TPUStrategy(resolver)

train, test = tf.keras.datasets.fashion_mnist.load_data()
images, labels = train
images = images/255
ds = tf.data.Dataset.from_tensor_slices((images))

@tf.function
def test_func(a):
    return a**3

dsf=ds.map(test_func)
&lt;/denchmark-code&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

The reproduction code doesn't use imports, but
&lt;denchmark-code&gt;@tf.function
def test_func(a):
    return a**3
&lt;/denchmark-code&gt;

Would typically run as part of the import code, and not the main program.
Other info / logs
No error logs produced
	</description>
	<comments>
		<comment id='1' author='MikeOfZen' date='2019-12-08T00:23:14Z'>
		Thank you for the update!
As for myself, I can get by with using the aforementioned solution of initiating connection before imports.
But its good to know that a formal fix is coming soon.
Best regards.
		</comment>
		<comment id='2' author='MikeOfZen' date='2019-12-08T03:21:08Z'>
		Sorry I did not take a closer look; your problem seems a different one. Btw Iâ€™m not related to Google so take my advice at your discretion.
		</comment>
		<comment id='3' author='MikeOfZen' date='2019-12-08T04:53:21Z'>
		To my knowledge, strategy = tf.distribute.experimental.TPUStrategy(resolver) will help you get into the correct device scope because TPU is always the remote client.
&lt;denchmark-link:https://github.com/rxsang&gt;@rxsang&lt;/denchmark-link&gt;
 as expert on this.
Probably need creating a TPUStrategy at the beginning.
		</comment>
		<comment id='4' author='MikeOfZen' date='2019-12-10T18:25:05Z'>
		The reason is because in TPU case is a 2VM environment (client + Cloud TPU), there are lots of things happening in the initialization time. E.g. in experimental_connect_to_cluster we will enter into the tpu worker device scope, because we expect everything to happen on the tpu worker rather the coordinator/client, otherwise the performance may be bad. Thus we actually expect user always put the TPUStrategy creation and initialization code in the beginning of the program. I will try to make the documentation better.
		</comment>
		<comment id='5' author='MikeOfZen' date='2019-12-10T18:35:10Z'>
		&lt;denchmark-link:https://github.com/rxsang&gt;@rxsang&lt;/denchmark-link&gt;
 , Thank you for the information.
I don't doubt that the interaction between the TPU VM and the client VM is very complicated. but that was my meaning of 'implicit' scope.
In this scenario, I believe it would be better to force all commands to run within an explicit scope, with the pythonic  idiom, this greatly reduces the chance for strange errors that fall between the cracks of the 2 VMs.
Right now, the documentation only mentions running the model definition and compilation inside the strategy scope. but if I understood correctly, this is another inner scope.
		</comment>
		<comment id='6' author='MikeOfZen' date='2019-12-10T18:50:03Z'>
		Your observation is correct. There are two scopes here: the strategy scope and the default device scope. For strategy scope, we expose it to the user and would like user to handle that. For default device scope, currently it is set in tf.config.experimental_connect_to_cluster(resolver), and you can always set tf.config.experimental_connect_to_cluster(resolver, make_master_device_default=False) to disable entering into scope automatically. But that means you may have to put your code in a "with tf.device("/job:worker:1"):" scope, and that has to be after initialization code as well. Because before the experimental_connect_to_cluster, you don't see any remote devices. That's why we handle it for users, because we think it is troublesome for users to handle the default device scope themselves.
I'm still not clear why this is a scope issue, my understanding is that you are trying to build something like dataset/tf.function before TPU initialization, the expectation is users should always do TPUStrategy creation and initialization first.
		</comment>
		<comment id='7' author='MikeOfZen' date='2019-12-10T19:17:12Z'>
		&lt;denchmark-link:https://github.com/rxsang&gt;@rxsang&lt;/denchmark-link&gt;

I think some of it comes down to miscommunication.
I'm a relatively new TF user, joined after 2.0, so I may be unaware of some of the intricacies of TF, especially in distributed settings. I followed the documentation guides, and though I might have missed it, I haven't seen such an expectation ("the expectation is users should always do TPUStrategy creation and initialization first."). I only realized it after encountering these crashes.
While I personally would prefer an explicit scope definition if its required, I can certainly understand the design decision. but it should be better communicated that it should be done first, and before even the imports.
But the other part of the issue is that it crashes (the whole python kernel) ungracefully and without any indication of the reason.
If I had some exception information, indicating the unavailability of certain definitions on the device, it would have been much faster to deal with.
Thanks!
		</comment>
		<comment id='8' author='MikeOfZen' date='2020-01-21T08:49:17Z'>
		Thanks Mike for filing the issue. We should improve our documentation &lt;denchmark-link:https://www.tensorflow.org/guide/tpu&gt;https://www.tensorflow.org/guide/tpu&lt;/denchmark-link&gt;
 and improve the error message, so users can be less confused by those issues. Close this as the root cause is found.
		</comment>
		<comment id='9' author='MikeOfZen' date='2020-01-21T08:49:22Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34932&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34932&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>