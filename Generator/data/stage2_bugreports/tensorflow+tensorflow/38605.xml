<bug id='38605' author='guatavita' open_date='2020-04-16T15:08:36Z' closed_time='2020-05-07T22:42:18Z'>
	<summary>Adam optimizer behavior between W10/Dell and Ubuntu/DGX</summary>
	<description>
Hello,
I am working on semantic segmentation of 2D/3D images for medical applications and I have a Dell workstation with a Quadro GV100 32Go for development and DGX servers (workstation and server) with 4 and 8 Tesla V100 DGXS (32Go) for model deployment, training, parameters optimization, ...
The Dell and the DGX's use Tensorflow 1.15.x with the only differences being that the Dell run Windows 10 and the DGX use GNU/Linux 4.15 and the models are trained within Docker images of Ubuntu 18.0x provided by &lt;denchmark-link:https://docs.nvidia.com/deeplearning/frameworks/tensorflow-release-notes/index.html&gt;NVIDIA&lt;/denchmark-link&gt;
. I think the CUDA library are different between the workstation and NVIDIA servers but both are using CUDA 10.
I am currently working on the Tensorflow implementation of the DeepLabV3+ for the segmentation of background + 12 classes. And my data consist of CT scan slices of around 55,000 images+labels for the training and 13,000 images+labels for the validation set.
The issue that I have is that for the exact same codes, data and model architecture, the Adam optimizer behavior is completely different between the Dell and DGX. The difference is that on the DGX the validation loss increases after 3-6 epochs (depending on the learning rate) and reach a plateau with give us a prediction of an empty label for each class. The training accuracy/loss is however exactly the same between the Dell and DGX. On the Dell, the validation accuracy/loss increase/decrease smoothly and is stable, never get close to zero/reach a high plateau. Again, same exact code, data, data_generator, no data augmentation... no weird value in the images/label on the Dell workstation nor on the RAID hard drive on any of the DGX. Behavior is the same for the DGX workstation and DGX server 1.
&lt;denchmark-link:https://drive.google.com/file/d/1hIv4r70QHH7Ukg5SJknfkzrsS2dq6ZOq/view?usp=sharing&gt;Tensorboard screenshot of training/validation&lt;/denchmark-link&gt;

I have tried to simplify the model by working on other data with only 1 label and small FOV around this label. The behavior is exactly the same and as soon as we use a LR &gt; 0.0001 the validation accuracy drops to zero on the DGX. If we specify the LR &lt; 0.00005 the validation accuracy increase but is still VERY unstable.
In the end, I switched to SGD optimizer and there is no difference in term of training and validation accuracy/loss between the Dell and the DGX workstation anymore, regardless of fixed LR value or LR scheduler pattern. However, I would like to use the Adam as well and I cannot afford to no use the DGX because of that. I did not try another model such a 2D UNet.
We are very confused in our team about why the Adam optimizer does not behave the same way with the exact same codes/model/data. I also asked a colleague of mine to use the same model with his codes on my data on the DGX, validation accuracy dropped to zero for him as well. What could explain such difference while the SGD provide the same behavior?
Hope I provided enough information, I can share more information if needed.
Best,
	</description>
	<comments>
		<comment id='1' author='guatavita' date='2020-04-17T15:27:03Z'>
		&lt;denchmark-link:https://github.com/guatavita&gt;@guatavita&lt;/denchmark-link&gt;
 Can you please provide a standalone code with a public dataset? Issue resolution can be faster with a standalone code. Thanks!
		</comment>
		<comment id='2' author='guatavita' date='2020-04-20T20:17:04Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 Can I share a private repo with you so you could replicate the issue on one of your system?
If yes, I am working on repo with a data_generator / deeplabv3+ / LiTS public database of liver segmentation.
		</comment>
		<comment id='3' author='guatavita' date='2020-04-21T19:07:38Z'>
		Hello &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;
,
Here is a google drive link of the training/validation/test set of 2D slices for the CT/masks &lt;denchmark-link:https://drive.google.com/file/d/1nklG4gb9hn9JrL20d4vcqKsOB7Ps6_Ct/view?usp=sharing&gt;Google drive folder LiTs liver&lt;/denchmark-link&gt;

I invited you to the private repository.
Thank you
		</comment>
		<comment id='4' author='guatavita' date='2020-04-22T18:15:13Z'>
		I was not able to replicate the issue with a basic binary classification model (dog/cat tutorial from Keras) and I was also not able to replicate the issue with a very simple/basic Unet model on the same data.
Following that, it seems that the issue comes from the DeepLabV3+ model. Could it be due to the Batch Normalization layers that behave differently on Windows 10/tf1.15.0 and Ubuntu/tf1.15.0+nv?
I will push the Unet model on the private repo that I have shared.
Will try the Unet with batch norm see if it changes something.
EDIT: Unet with batchnorm behaves the same on the DELL/DGX so it must come from the DeepLabv+
		</comment>
		<comment id='5' author='guatavita' date='2020-04-29T22:00:04Z'>
		Hello,
I wanted to update this issue. We end up contacting the NVIDIA support to ask them to reproduce the behavior on a similar system (DGX server with Docker images provided by Nvidia).
The issue has been isolated, we were able to reproduce the stable results of our models using directly the tensorflow/tensorflow:1.15.2-gpu-py docker image.
Thanks to &lt;denchmark-link:https://github.com/rmccorm4&gt;@rmccorm4&lt;/denchmark-link&gt;
 for working on the issue.
Best,
PS: I will let the assignees close the issue.
		</comment>
		<comment id='6' author='guatavita' date='2020-05-07T22:42:20Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38605&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38605&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>