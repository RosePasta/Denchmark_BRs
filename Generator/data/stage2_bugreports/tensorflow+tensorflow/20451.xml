<bug id='20451' author='MaeThird' open_date='2018-07-01T09:39:44Z' closed_time='2018-08-07T06:21:49Z'>
	<summary>with quantized-training model,PC ok but tf-lite failed.</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux  4.17.2-1-ARCH SMP
TensorFlow installed from (source or binary):source
TensorFlow version (use command below):b'v1.8.0-3238-g52bf2fe0f6' 1.9.0-rc0
Python version: 3.6.5
Bazel version (if compiling from source):0.14.1- (@non-git)
GCC/Compiler version (if compiling from source):gcc-7.1
CUDA/cuDNN version:cuda-9.2 cuDNN-7.1
GPU model and memory:16G
Exact command to reproduce:

I trained label_image on mobilenetv2 backbone with quantization and everything works well on PC.
Then I tried to convert it to tf-lite,even the converting processing is well-down(no error,no unsupported ops),but when I finally ran it I got tensorflow/contrib/lite/kernels/conv.cc:260 real_multiplier &lt; 1.0 was not true
This the log:
tensors size: 174 nodes size: 66 inputs: 1 input(0) name: input 0: MobilenetV2/Conv/Conv2D_Fold_bias, 128, 2, 0.0408043, 0 1: MobilenetV2/Conv/Relu6, 100352, 3, 0.0235285, 0 2: MobilenetV2/Conv/weights_quant/FakeQuantWithMinMaxVars, 864, 3, 0.0408043, 121 3: MobilenetV2/Conv_1/Conv2D_Fold_bias, 5120, 2, 0.00108845, 0 4: MobilenetV2/Conv_1/Relu6, 20480, 3, 0.0235285, 0 5: MobilenetV2/Conv_1/weights_quant/FakeQuantWithMinMaxVars, 409600, 3, 0.00680513, 119 6: MobilenetV2/Logits/AvgPool, 1280, 3, 0.0235285, 0 7: MobilenetV2/Logits/Conv2d_1c_1x1/BiasAdd, 3, 3, 0.174401, 120 8: MobilenetV2/Logits/Conv2d_1c_1x1/Conv2D_bias, 12, 2, 2.91482e-05, 0 9: MobilenetV2/Logits/Conv2d_1c_1x1/weights_quant/FakeQuantWithMinMaxVars, 3840, 3, 0.00123885, 125 10: MobilenetV2/Logits/Squeeze, 3, 3, 0.174401, 120 11: MobilenetV2/Logits/Squeeze_shape, 8, 2, 0, 0 12: MobilenetV2/Predictions/Reshape_1, 3, 3, 0.00390625, 0 13: MobilenetV2/expanded_conv/depthwise/Relu6, 100352, 3, 0.0235285, 0 14: MobilenetV2/expanded_conv/depthwise/depthwise_Fold_bias, 128, 2, 0.00805492, 0 15: MobilenetV2/expanded_conv/depthwise/weights_quant/FakeQuantWithMinMaxVars, 288, 3, 0.342348, 165 16: MobilenetV2/expanded_conv/project/Conv2D_Fold_bias, 64, 2, 0.00091254, 0 17: MobilenetV2/expanded_conv/project/add_fold, 50176, 3, 0.354141, 130 18: MobilenetV2/expanded_conv/project/weights_quant/FakeQuantWithMinMaxVars, 512, 3, 0.0387845, 150 19: MobilenetV2/expanded_conv_1/depthwise/Relu6, 75264, 3, 0.0235285, 0 20: MobilenetV2/expanded_conv_1/depthwise/depthwise_Fold_bias, 384, 2, 0.00060018, 0 21: MobilenetV2/expanded_conv_1/depthwise/weights_quant/FakeQuantWithMinMaxVars, 864, 3, 0.0255087, 109 22: MobilenetV2/expanded_conv_1/expand/Conv2D_Fold_bias, 384, 2, 0.00352435, 0 23: MobilenetV2/expanded_conv_1/expand/Relu6, 301056, 3, 0.0235285, 0 24: MobilenetV2/expanded_conv_1/expand/weights_quant/FakeQuantWithMinMaxVars, 1536, 3, 0.00995183, 126 25: MobilenetV2/expanded_conv_1/project/Conv2D_Fold_bias, 96, 2, 0.000607076, 0 26: MobilenetV2/expanded_conv_1/project/add_fold, 18816, 3, 0.294347, 131 27: MobilenetV2/expanded_conv_1/project/weights_quant/FakeQuantWithMinMaxVars, 2304, 3, 0.0258018, 151 28: MobilenetV2/expanded_conv_10/depthwise/Relu6, 18816, 3, 0.0235285, 0 29: MobilenetV2/expanded_conv_10/depthwise/depthwise_Fold_bias, 1536, 2, 0.000711485, 0 30: MobilenetV2/expanded_conv_10/depthwise/weights_quant/FakeQuantWithMinMaxVars, 3456, 3, 0.0302393, 140 31: MobilenetV2/expanded_conv_10/expand/Conv2D_Fold_bias, 1536, 2, 0.000350018, 0 32: MobilenetV2/expanded_conv_10/expand/Relu6, 18816, 3, 0.0235285, 0 33: MobilenetV2/expanded_conv_10/expand/weights_quant/FakeQuantWithMinMaxVars, 24576, 3, 0.00174141, 149 34: MobilenetV2/expanded_conv_10/project/Conv2D_Fold_bias, 384, 2, 0.000179579, 0 35: MobilenetV2/expanded_conv_10/project/add_fold, 4704, 3, 0.150007, 128 36: MobilenetV2/expanded_conv_10/project/weights_quant/FakeQuantWithMinMaxVars, 36864, 3, 0.00763239, 134 37: MobilenetV2/expanded_conv_11/add, 4704, 3, 0.149921, 125 38: MobilenetV2/expanded_conv_11/depthwise/Relu6, 28224, 3, 0.0235285, 0 39: MobilenetV2/expanded_conv_11/depthwise/depthwise_Fold_bias, 2304, 2, 0.0013777, 0 40: MobilenetV2/expanded_conv_11/depthwise/weights_quant/FakeQuantWithMinMaxVars, 5184, 3, 0.0585545, 92 41: MobilenetV2/expanded_conv_11/expand/Conv2D_Fold_bias, 2304, 2, 0.000252474, 0 42: MobilenetV2/expanded_conv_11/expand/Relu6, 28224, 3, 0.0235285, 0 43: MobilenetV2/expanded_conv_11/expand/weights_quant/FakeQuantWithMinMaxVars, 55296, 3, 0.00168308, 133 44: MobilenetV2/expanded_conv_11/project/Conv2D_Fold_bias, 384, 2, 0.000203408, 0 45: MobilenetV2/expanded_conv_11/project/add_fold, 4704, 3, 0.102331, 126 46: MobilenetV2/expanded_conv_11/project/weights_quant/FakeQuantWithMinMaxVars, 55296, 3, 0.00864519, 141 47: MobilenetV2/expanded_conv_12/add, 4704, 3, 0.213277, 145 48: MobilenetV2/expanded_conv_12/depthwise/Relu6, 28224, 3, 0.0235285, 0 49: MobilenetV2/expanded_conv_12/depthwise/depthwise_Fold_bias, 2304, 2, 0.00214226, 0 50: MobilenetV2/expanded_conv_12/depthwise/weights_quant/FakeQuantWithMinMaxVars, 5184, 3, 0.0910496, 173 51: MobilenetV2/expanded_conv_12/expand/Conv2D_Fold_bias, 2304, 2, 0.000217147, 0 52: MobilenetV2/expanded_conv_12/expand/Relu6, 28224, 3, 0.0235285, 0 53: MobilenetV2/expanded_conv_12/expand/weights_quant/FakeQuantWithMinMaxVars, 55296, 3, 0.00144841, 139 54: MobilenetV2/expanded_conv_12/project/Conv2D_Fold_bias, 384, 2, 0.000596996, 0 55: MobilenetV2/expanded_conv_12/project/add_fold, 4704, 3, 0.170068, 144 56: MobilenetV2/expanded_conv_12/project/weights_quant/FakeQuantWithMinMaxVars, 55296, 3, 0.0253734, 149 57: MobilenetV2/expanded_conv_13/depthwise/Relu6, 9216, 3, 0.0235285, 0 58: MobilenetV2/expanded_conv_13/depthwise/depthwise_Fold_bias, 2304, 2, 0.00034101, 0 59: MobilenetV2/expanded_conv_13/depthwise/weights_quant/FakeQuantWithMinMaxVars, 5184, 3, 0.0144935, 90 60: MobilenetV2/expanded_conv_13/expand/Conv2D_Fold_bias, 2304, 2, 0.000297714, 0 61: MobilenetV2/expanded_conv_13/expand/Relu6, 28224, 3, 0.0235285, 0 62: MobilenetV2/expanded_conv_13/expand/weights_quant/FakeQuantWithMinMaxVars, 55296, 3, 0.0013959, 122 63: MobilenetV2/expanded_conv_13/project/Conv2D_Fold_bias, 640, 2, 0.000194787, 0 64: MobilenetV2/expanded_conv_13/project/add_fold, 2560, 3, 0.144367, 122 65: MobilenetV2/expanded_conv_13/project/weights_quant/FakeQuantWithMinMaxVars, 92160, 3, 0.00827876, 139 66: MobilenetV2/expanded_conv_14/add, 2560, 3, 0.150496, 130 67: MobilenetV2/expanded_conv_14/depthwise/Relu6, 15360, 3, 0.0235285, 0 68: MobilenetV2/expanded_conv_14/depthwise/depthwise_Fold_bias, 3840, 2, 0.0010313, 0 69: MobilenetV2/expanded_conv_14/depthwise/weights_quant/FakeQuantWithMinMaxVars, 8640, 3, 0.0438318, 148 70: MobilenetV2/expanded_conv_14/expand/Conv2D_Fold_bias, 3840, 2, 0.000355036, 0 71: MobilenetV2/expanded_conv_14/expand/Relu6, 15360, 3, 0.0235285, 0 72: MobilenetV2/expanded_conv_14/expand/weights_quant/FakeQuantWithMinMaxVars, 153600, 3, 0.00245926, 111 73: MobilenetV2/expanded_conv_14/project/Conv2D_Fold_bias, 640, 2, 0.000174049, 0 74: MobilenetV2/expanded_conv_14/project/add_fold, 2560, 3, 0.0903757, 130 75: MobilenetV2/expanded_conv_14/project/weights_quant/FakeQuantWithMinMaxVars, 153600, 3, 0.00739738, 137 76: MobilenetV2/expanded_conv_15/add, 2560, 3, 0.300834, 122 77: MobilenetV2/expanded_conv_15/depthwise/Relu6, 15360, 3, 0.0235285, 0 78: MobilenetV2/expanded_conv_15/depthwise/depthwise_Fold_bias, 3840, 2, 0.00129889, 0 79: MobilenetV2/expanded_conv_15/depthwise/weights_quant/FakeQuantWithMinMaxVars, 8640, 3, 0.0552048, 110 80: MobilenetV2/expanded_conv_15/expand/Conv2D_Fold_bias, 3840, 2, 0.000226284, 0 81: MobilenetV2/expanded_conv_15/expand/Relu6, 15360, 3, 0.0235285, 0 82: MobilenetV2/expanded_conv_15/expand/weights_quant/FakeQuantWithMinMaxVars, 153600, 3, 0.00150359, 99 83: MobilenetV2/expanded_conv_15/project/Conv2D_Fold_bias, 640, 2, 0.000805016, 0 84: MobilenetV2/expanded_conv_15/project/add_fold, 2560, 3, 0.226103, 131 85: MobilenetV2/expanded_conv_15/project/weights_quant/FakeQuantWithMinMaxVars, 153600, 3, 0.0342145, 139 86: MobilenetV2/expanded_conv_16/depthwise/Relu6, 15360, 3, 0.0235285, 0 87: MobilenetV2/expanded_conv_16/depthwise/depthwise_Fold_bias, 3840, 2, 0.0040495, 0 88: MobilenetV2/expanded_conv_16/depthwise/weights_quant/FakeQuantWithMinMaxVars, 8640, 3, 0.17211, 201 89: MobilenetV2/expanded_conv_16/expand/Conv2D_Fold_bias, 3840, 2, 0.000576843, 0 90: MobilenetV2/expanded_conv_16/expand/Relu6, 15360, 3, 0.0235285, 0 91: MobilenetV2/expanded_conv_16/expand/weights_quant/FakeQuantWithMinMaxVars, 153600, 3, 0.00191748, 125 92: MobilenetV2/expanded_conv_16/project/Conv2D_Fold_bias, 1280, 2, 0.000119162, 0 93: MobilenetV2/expanded_conv_16/project/add_fold, 5120, 3, 0.159945, 146 94: MobilenetV2/expanded_conv_16/project/weights_quant/FakeQuantWithMinMaxVars, 307200, 3, 0.0050646, 130 95: MobilenetV2/expanded_conv_2/add, 18816, 3, 0.376629, 129 96: MobilenetV2/expanded_conv_2/depthwise/Relu6, 112896, 3, 0.0235285, 0 97: MobilenetV2/expanded_conv_2/depthwise/depthwise_Fold_bias, 576, 2, 0.00397992, 0 98: MobilenetV2/expanded_conv_2/depthwise/weights_quant/FakeQuantWithMinMaxVars, 1296, 3, 0.169153, 51 99: MobilenetV2/expanded_conv_2/expand/Conv2D_Fold_bias, 576, 2, 0.00106746, 0 100: MobilenetV2/expanded_conv_2/expand/Relu6, 112896, 3, 0.0235285, 0 101: MobilenetV2/expanded_conv_2/expand/weights_quant/FakeQuantWithMinMaxVars, 3456, 3, 0.00362654, 142 102: MobilenetV2/expanded_conv_2/project/Conv2D_Fold_bias, 96, 2, 0.000610138, 0 103: MobilenetV2/expanded_conv_2/project/add_fold, 18816, 3, 0.342911, 133 104: MobilenetV2/expanded_conv_2/project/weights_quant/FakeQuantWithMinMaxVars, 3456, 3, 0.0259319, 129 105: MobilenetV2/expanded_conv_3/depthwise/Relu6, 28224, 3, 0.0235285, 0 106: MobilenetV2/expanded_conv_3/depthwise/depthwise_Fold_bias, 576, 2, 0.000397524, 0 107: MobilenetV2/expanded_conv_3/depthwise/weights_quant/FakeQuantWithMinMaxVars, 1296, 3, 0.0168954, 126 108: MobilenetV2/expanded_conv_3/expand/Conv2D_Fold_bias, 576, 2, 0.00108431, 0 109: MobilenetV2/expanded_conv_3/expand/Relu6, 112896, 3, 0.0235285, 0 110: MobilenetV2/expanded_conv_3/expand/weights_quant/FakeQuantWithMinMaxVars, 3456, 3, 0.00287898, 107 111: MobilenetV2/expanded_conv_3/project/Conv2D_Fold_bias, 128, 2, 0.000396253, 0 112: MobilenetV2/expanded_conv_3/project/add_fold, 6272, 3, 0.20811, 126 113: MobilenetV2/expanded_conv_3/project/weights_quant/FakeQuantWithMinMaxVars, 4608, 3, 0.0168414, 109 114: MobilenetV2/expanded_conv_4/add, 6272, 3, 0.250818, 134 115: MobilenetV2/expanded_conv_4/depthwise/Relu6, 37632, 3, 0.0235285, 0 116: MobilenetV2/expanded_conv_4/depthwise/depthwise_Fold_bias, 768, 2, 0.00236945, 0 117: MobilenetV2/expanded_conv_4/depthwise/weights_quant/FakeQuantWithMinMaxVars, 1728, 3, 0.100705, 79 118: MobilenetV2/expanded_conv_4/expand/Conv2D_Fold_bias, 768, 2, 0.000398864, 0 119: MobilenetV2/expanded_conv_4/expand/Relu6, 37632, 3, 0.0235285, 0 120: MobilenetV2/expanded_conv_4/expand/weights_quant/FakeQuantWithMinMaxVars, 6144, 3, 0.0019166, 151 121: MobilenetV2/expanded_conv_4/project/Conv2D_Fold_bias, 128, 2, 0.000489459, 0 122: MobilenetV2/expanded_conv_4/project/add_fold, 6272, 3, 0.200968, 132 123: MobilenetV2/expanded_conv_4/project/weights_quant/FakeQuantWithMinMaxVars, 6144, 3, 0.0208029, 147 124: MobilenetV2/expanded_conv_5/add, 6272, 3, 0.276968, 127 125: MobilenetV2/expanded_conv_5/depthwise/Relu6, 37632, 3, 0.0235285, 0 126: MobilenetV2/expanded_conv_5/depthwise/depthwise_Fold_bias, 768, 2, 0.00202895, 0 127: MobilenetV2/expanded_conv_5/depthwise/weights_quant/FakeQuantWithMinMaxVars, 1728, 3, 0.086234, 63 128: MobilenetV2/expanded_conv_5/expand/Conv2D_Fold_bias, 768, 2, 0.000362468, 0 129: MobilenetV2/expanded_conv_5/expand/Relu6, 37632, 3, 0.0235285, 0 130: MobilenetV2/expanded_conv_5/expand/weights_quant/FakeQuantWithMinMaxVars, 6144, 3, 0.00144514, 120 131: MobilenetV2/expanded_conv_5/project/Conv2D_Fold_bias, 128, 2, 0.000435147, 0 132: MobilenetV2/expanded_conv_5/project/add_fold, 6272, 3, 0.205061, 128 133: MobilenetV2/expanded_conv_5/project/weights_quant/FakeQuantWithMinMaxVars, 6144, 3, 0.0184945, 128 134: MobilenetV2/expanded_conv_6/depthwise/Relu6, 9408, 3, 0.0235285, 0 135: MobilenetV2/expanded_conv_6/depthwise/depthwise_Fold_bias, 768, 2, 0.000267618, 0 136: MobilenetV2/expanded_conv_6/depthwise/weights_quant/FakeQuantWithMinMaxVars, 1728, 3, 0.0113742, 126 137: MobilenetV2/expanded_conv_6/expand/Conv2D_Fold_bias, 768, 2, 0.000528109, 0 138: MobilenetV2/expanded_conv_6/expand/Relu6, 37632, 3, 0.0235285, 0 139: MobilenetV2/expanded_conv_6/expand/weights_quant/FakeQuantWithMinMaxVars, 6144, 3, 0.00190675, 128 140: MobilenetV2/expanded_conv_6/project/Conv2D_Fold_bias, 256, 2, 0.00033262, 0 141: MobilenetV2/expanded_conv_6/project/add_fold, 3136, 3, 0.182997, 123 142: MobilenetV2/expanded_conv_6/project/weights_quant/FakeQuantWithMinMaxVars, 12288, 3, 0.0141369, 135 143: MobilenetV2/expanded_conv_7/add, 3136, 3, 0.182534, 120 144: MobilenetV2/expanded_conv_7/depthwise/Relu6, 18816, 3, 0.0235285, 0 145: MobilenetV2/expanded_conv_7/depthwise/depthwise_Fold_bias, 1536, 2, 0.00131166, 0 146: MobilenetV2/expanded_conv_7/depthwise/weights_quant/FakeQuantWithMinMaxVars, 3456, 3, 0.0557479, 130 147: MobilenetV2/expanded_conv_7/expand/Conv2D_Fold_bias, 1536, 2, 0.000259114, 0 148: MobilenetV2/expanded_conv_7/expand/Relu6, 18816, 3, 0.0235285, 0 149: MobilenetV2/expanded_conv_7/expand/weights_quant/FakeQuantWithMinMaxVars, 24576, 3, 0.00141595, 134 150: MobilenetV2/expanded_conv_7/project/Conv2D_Fold_bias, 256, 2, 0.000434401, 0 151: MobilenetV2/expanded_conv_7/project/add_fold, 3136, 3, 0.150338, 112 152: MobilenetV2/expanded_conv_7/project/weights_quant/FakeQuantWithMinMaxVars, 24576, 3, 0.0184628, 129 153: MobilenetV2/expanded_conv_8/add, 3136, 3, 0.182103, 121 154: MobilenetV2/expanded_conv_8/depthwise/Relu6, 18816, 3, 0.0235285, 0 155: MobilenetV2/expanded_conv_8/depthwise/depthwise_Fold_bias, 1536, 2, 0.000993556, 0 156: MobilenetV2/expanded_conv_8/depthwise/weights_quant/FakeQuantWithMinMaxVars, 3456, 3, 0.0422278, 135 157: MobilenetV2/expanded_conv_8/expand/Conv2D_Fold_bias, 1536, 2, 0.000266144, 0 158: MobilenetV2/expanded_conv_8/expand/Relu6, 18816, 3, 0.0235285, 0 159: MobilenetV2/expanded_conv_8/expand/weights_quant/FakeQuantWithMinMaxVars, 24576, 3, 0.00145805, 128 160: MobilenetV2/expanded_conv_8/project/Conv2D_Fold_bias, 256, 2, 0.000281633, 0 161: MobilenetV2/expanded_conv_8/project/add_fold, 3136, 3, 0.122043, 130 162: MobilenetV2/expanded_conv_8/project/weights_quant/FakeQuantWithMinMaxVars, 24576, 3, 0.0119699, 127 163: MobilenetV2/expanded_conv_9/add, 3136, 3, 0.200997, 130 164: MobilenetV2/expanded_conv_9/depthwise/Relu6, 18816, 3, 0.0235285, 0 165: MobilenetV2/expanded_conv_9/depthwise/depthwise_Fold_bias, 1536, 2, 0.000984803, 0 166: MobilenetV2/expanded_conv_9/depthwise/weights_quant/FakeQuantWithMinMaxVars, 3456, 3, 0.0418558, 151 167: MobilenetV2/expanded_conv_9/expand/Conv2D_Fold_bias, 1536, 2, 0.000224455, 0 168: MobilenetV2/expanded_conv_9/expand/Relu6, 18816, 3, 0.0235285, 0 169: MobilenetV2/expanded_conv_9/expand/weights_quant/FakeQuantWithMinMaxVars, 24576, 3, 0.00123257, 123 170: MobilenetV2/expanded_conv_9/project/Conv2D_Fold_bias, 256, 2, 0.000418117, 0 171: MobilenetV2/expanded_conv_9/project/add_fold, 3136, 3, 0.157535, 127 172: MobilenetV2/expanded_conv_9/project/weights_quant/FakeQuantWithMinMaxVars, 24576, 3, 0.0177707, 145 173: input, 37632, 3, 1, 128 number of inputs: 1 number of outputs: 1 tensorflow/contrib/lite/kernels/conv.cc:260 real_multiplier &lt; 1.0 was not true.
I just add tf.contrib.quantize.create_training_graph( input_graph=tf.get_default_graph(),         quant_delay=FLAGS.quan_delay) in training and  tf.contrib.quantize.create_eval_graph()
in evaluation compared to the official version.
Is there any extra processes I need to care? Anyone help ?
	</description>
	<comments>
		<comment id='1' author='MaeThird' date='2018-07-06T08:27:58Z'>
		Same situation with MobilenetV1, that is used as a backbone for separate model. Model was trained and then freezed with tf.contrib.quantize.create_training_graph and tf.contrib.quantize.create_eval_graph. Backbone then was separated and converted with flag --allow_nudging_weights_to_use_fast_gemm_kernel. My tensorflow version is 1.9.0-rc2.
My thoughts so far: in the source code for TFLite there is 2 quantization routins QuantizeMultiplierGreaterThanOne and QuantizeMultiplierSmallerThanOneExp defined here. May be one can just add logic in tensorflow/contrib/lite/kernels/conv.cc to handle various real_multiplier? If i won't be able to find out anything better, then i will probably give it a try and report here.
		</comment>
		<comment id='2' author='MaeThird' date='2018-07-06T17:29:48Z'>
		Thanks for reporting this. This seems to be an issue with convs and fully connected layers that was fixed for DepthwiseConvs in this commit &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/3a17101171d3e51fcba2189d09416c5106bfe4ac#diff-ca0f46c80fd3cf1f2040be6147a2d8cf&gt;3a17101#diff-ca0f46c80fd3cf1f2040be6147a2d8cf&lt;/denchmark-link&gt;

We likely need a similar fix for Conv and FC as well.
		</comment>
		<comment id='3' author='MaeThird' date='2018-08-02T19:09:04Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/liyunlu0618&gt;@liyunlu0618&lt;/denchmark-link&gt;
: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='4' author='MaeThird' date='2018-08-02T23:00:14Z'>
		&lt;denchmark-link:https://github.com/liyunlu0618&gt;@liyunlu0618&lt;/denchmark-link&gt;
 change should resolve, it will be available in the next nightly.
		</comment>
		<comment id='5' author='MaeThird' date='2018-08-07T06:21:49Z'>
		&lt;denchmark-link:https://github.com/suharshs&gt;@suharshs&lt;/denchmark-link&gt;
 Thanks.It works now.
		</comment>
	</comments>
</bug>