<bug id='41724' author='g-soto' open_date='2020-07-25T10:48:28Z' closed_time='2020-07-30T00:39:07Z'>
	<summary>Tensorflow stuck in training with more than two GPUs</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No, I am using the example presented in https://www.tensorflow.org/guide/gpu#using_multiple_gpus


OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 18.04.3 LTS


TensorFlow installed from (source or binary):
I am using the TensorFlow Docker image tensorflow/tensorflow:latest-gpu-jupyter


TensorFlow version (use command below):
2.2.0


Python version:
3.6.9


CUDA/cuDNN version:
CUDA version: 10.1


GPU model and memory:
Nvidia Tesla P100-SXM2


Describe the current behavior
When I try to run the code below in a jupyter notebook with more than two GPUs it get stuck in the training step. It works fine with two GPUs.
Standalone code to reproduce the issue
&lt;denchmark-code&gt;import tensorflow as tf
strategy = tf.distribute.MirroredStrategy()
input_values = tf.random.uniform((256,), minval=0, maxval=1, dtype=tf.dtypes.float32)
with strategy.scope():
    inputs = tf.keras.layers.Input(shape=(1,))
    predictions = tf.keras.layers.Dense(1)(inputs)
    model = tf.keras.models.Model(inputs=inputs, outputs=predictions)
    model.compile(loss='mse', optimizer=tf.keras.optimizers.SGD(learning_rate=0.2))
    
    model.fit(input_values, input_values, batch_size=32)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='g-soto' date='2020-07-27T15:54:19Z'>
		Hi &lt;denchmark-link:https://github.com/g-soto&gt;@g-soto&lt;/denchmark-link&gt;
, I ran this code with 4 GPUs and training completed successfully.
Since I am unable to reproduce, I'm wondering if maybe there's an issue with one of your GPUs. Can you first run a test to make sure each GPU is functioning by running training with a single GPU at a time. If you have more than one GPU in your system, the GPU with the lowest ID will be selected by default, so you'll need to specify which GPU you want utilized. You can do that with tf.config.set_visible_devices to specify which devices are visible to the runtime.
		</comment>
		<comment id='2' author='g-soto' date='2020-07-28T18:44:53Z'>
		Hi &lt;denchmark-link:https://github.com/nikitamaia&gt;@nikitamaia&lt;/denchmark-link&gt;
, I ran the training on each GPU individually without a problem.
		</comment>
		<comment id='3' author='g-soto' date='2020-07-29T23:16:10Z'>
		Can you try the following:
strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.ReductionToOneDevice())
this is to check whether NCCL is causing problems.
		</comment>
		<comment id='4' author='g-soto' date='2020-07-30T00:15:51Z'>
		With the cross_device_ops=tf.distribute.ReductionToOneDevice() argument it works perfectly in eight GPUs. Thanks
		</comment>
		<comment id='5' author='g-soto' date='2020-07-30T00:39:08Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41724&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41724&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>