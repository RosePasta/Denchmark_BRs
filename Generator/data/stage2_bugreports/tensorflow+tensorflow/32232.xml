<bug id='32232' author='akanyaani' open_date='2019-09-05T11:11:00Z' closed_time='2019-12-27T18:53:01Z'>
	<summary>Distributed Training is not working with input signature</summary>
	<description>
Distributed training is not working when I am passing input signature in tf.function
&lt;denchmark-code&gt;@tf.function(input_signature=input_signature)
def _distributed_train_step(self, inputs, targets):
        def step_fn(inp, tar):
            with tf.GradientTape() as tape:
                logits = self(inp, training=True)
                cross_entropy = self.get_loss(tar, logits)
                loss = tf.reduce_sum(cross_entropy) * (1.0 / self.batch_size)

            with tf.name_scope("gradients"):
                gradients = tape.gradient(loss, self.trainable_variables)
                if self.grad_clip:
                    gradients = [(tf.clip_by_value(grad, -self.clip_value, self.clip_value))
                                 for grad in gradients]
                self.optimizer.apply_gradients(list(zip(gradients, self.trainable_variables)))
            return cross_entropy

        per_example_losses = self.mirrored_strategy.experimental_run_v2(
            step_fn, args=(inputs, targets))

        mean_loss = self.mirrored_strategy.reduce(
            tf.distribute.ReduceOp.SUM, per_example_losses, axis=0)

        mean_loss = mean_loss / self.batch_size
        perplexity = self.get_perplexity(mean_loss)
        step = self.optimizer.iterations
        self._log_model_summary_data(self.train_writer,
                                     step,
                                     mean_loss,
                                     perplexity)

        return step, mean_loss, perplexity
&lt;/denchmark-code&gt;

Please have a look at traceback.
Traceback (most recent call last): File "train_gpt2.py", line 86, in &lt;module&gt; train() File "/home/abhay/anaconda3/lib/python3.7/site-packages/click/core.py", line 764, in __call__ return self.main(*args, **kwargs) File "/home/abhay/anaconda3/lib/python3.7/site-packages/click/core.py", line 717, in main rv = self.invoke(ctx) File "/home/abhay/anaconda3/lib/python3.7/site-packages/click/core.py", line 956, in invoke return ctx.invoke(self.callback, **ctx.params) File "/home/abhay/anaconda3/lib/python3.7/site-packages/click/core.py", line 555, in invoke return callback(*args, **kwargs) File "train_gpt2.py", line 59, in train model.fit([dist_train_dataset, dist_test_dataset]) File "/home/abhay/edge_gpt2/model_gpt.py", line 359, in fit step, train_loss, perplexity = self.distributed_train_step(inputs, targets) File "/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 439, in __call__ return self._stateless_fn(*args, **kwds) File "/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1821, in __call__ graph_function, args, kwargs = self._maybe_define_function(args, kwargs) File "/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 2104, in _maybe_define_function *args, **kwargs) File "/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1650, in canonicalize_function_inputs self._flat_input_signature) File "/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1710, in _convert_inputs_to_signature format_error_message(inputs, input_signature)) ValueError: When input_signature is provided, all inputs to the Python function must be convertible to tensors: inputs: ( PerReplica:{ 0 /job:localhost/replica:0/task:0/device:GPU:0: tf.Tensor
ValueError: When input_signature is provided, all inputs to the Python function must be convertible to tensors:
	</description>
	<comments>
		<comment id='1' author='akanyaani' date='2019-09-06T06:49:12Z'>
		&lt;denchmark-link:https://github.com/akanyaani&gt;@akanyaani&lt;/denchmark-link&gt;
 ,
Thank you for reporting the issue.
Can you share a simple and standalone code to reproduce the issue?
Also mention the TF version being used.
		</comment>
		<comment id='2' author='akanyaani' date='2019-09-06T22:19:57Z'>
		This is because the model is passed as the first argument in the decorated function (which isn't convertible to a tensor), I don't think there's currently a way to specify a tensorspec for a decorated function which take a tf model. This is inconvenient, as currently experimental_run_v2 calls must be inside a decorated wrapper function.
&lt;denchmark-link:https://github.com/akanyaani&gt;@akanyaani&lt;/denchmark-link&gt;
 Does, something like this work?
&lt;denchmark-code&gt;def distributed_wrapper(self, inputs, targets):

    @tf.function(input_signature=input_signature)
    def _distributed_train_step(inputs, targets):
            def step_fn(inp, tar):
                with tf.GradientTape() as tape:
                    logits = self(inp, training=True)
                    cross_entropy = self.get_loss(tar, logits)
                    loss = tf.reduce_sum(cross_entropy) * (1.0 / self.batch_size)

                with tf.name_scope("gradients"):
                    gradients = tape.gradient(loss, self.trainable_variables)
                    if self.grad_clip:
                        gradients = [(tf.clip_by_value(grad, -self.clip_value, self.clip_value))
                                    for grad in gradients]
                    self.optimizer.apply_gradients(list(zip(gradients, self.trainable_variables)))
                return cross_entropy

            per_example_losses = self.mirrored_strategy.experimental_run_v2(
                step_fn, args=(inputs, targets))

            mean_loss = self.mirrored_strategy.reduce(
                tf.distribute.ReduceOp.SUM, per_example_losses, axis=0)

            mean_loss = mean_loss / self.batch_size
            perplexity = self.get_perplexity(mean_loss)
            step = self.optimizer.iterations
            self._log_model_summary_data(self.train_writer,
                                        step,
                                        mean_loss,
                                        perplexity)

            return step, mean_loss, perplexity
    step, mean_loss, perplexity =  _distributed_train_step(inputs, targets)
return step, mean_loss, perplexity  
&lt;/denchmark-code&gt;

If any of the arguments called from the wrapper's scope other than the model are python primitives, it seems like this induces retracing and thus poor performance.
		</comment>
		<comment id='3' author='akanyaani' date='2019-09-09T10:51:28Z'>
		Hi,
&lt;denchmark-link:https://github.com/mjlbach&gt;@mjlbach&lt;/denchmark-link&gt;
 As you suggested i will try but using the same method on single GPU it is working fine.
You can replicate the issues using this repo code.
&lt;denchmark-link:https://github.com/akanyaani/gpt-2-tensorflow2.0&gt;Repo Link gpt-2-tensorflow2.0&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='akanyaani' date='2019-09-10T10:01:00Z'>
		Hi, &lt;denchmark-link:https://github.com/mjlbach&gt;@mjlbach&lt;/denchmark-link&gt;
 I tried what you suggested but I got the same error.
&lt;denchmark-code&gt;could not be loaded or symbol could not be found.
WARNING: Logging before flag parsing goes to stderr.
W0910 15:28:20.957778 139744540489472 cross_device_ops.py:779] Efficient allreduce is not supported for 1 IndexedSlices
Traceback (most recent call last):
 File "/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1704, in _convert_inputs_to_signature
   value, dtype_hint=spec.dtype)
 File "/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py", line 1184, in convert_to_tensor
   return convert_to_tensor_v2(value, dtype, preferred_dtype, name)
 File "/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py", line 1242, in convert_to_tensor_v2
   as_ref=False)
 File "/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py", line 1296, in internal_convert_to_tensor
   ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
 File "/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py", line 286, in _constant_tensor_conversion_function
   return constant(v, dtype=dtype, name=name)
 File "/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py", line 227, in constant
   allow_broadcast=True)
 File "/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py", line 235, in _constant_impl
   t = convert_to_eager_tensor(value, ctx, dtype)
 File "/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py", line 96, in convert_to_eager_tensor
   return ops.EagerTensor(value, ctx.device_name, dtype)
ValueError: Attempt to convert a value (PerReplica:{
 0 /job:localhost/replica:0/task:0/device:GPU:0: &lt;tf.Tensor: id=282, shape=(16, 512), dtype=int32, numpy=
array([[    3,   663,   289, ...,     0,     0,     0],
      [    3,   524,   276, ...,     0,     0,     0],
      [    3, 15756,   987, ...,     0,     0,     0],
      ...,
      [    3,  3164,  4571, ...,     0,     0,     0],
      [    3,  1089,   289, ...,     0,     0,     0],
      [    3,   798,  4138, ...,     0,     0,     0]], dtype=int32)&gt;,
 1 /job:localhost/replica:0/task:0/device:GPU:1: &lt;tf.Tensor: id=285, shape=(16, 512), dtype=int32, numpy=
array([[    3,   663,   987, ...,     0,     0,     0],
      [    3,   175, 16233, ...,     0,     0,     0],
      [    3,   698,    53, ...,     0,     0,     0],
      ...,
      [    3,    83,   632, ...,     0,     0,     0],
      [    3, 13453,   168, ...,     0,     0,     0],
      [    3,   421,    53, ...,     0,     0,     0]], dtype=int32)&gt;
}) with an unsupported type (&lt;class 'tensorflow.python.distribute.values.PerReplica'&gt;) to a Tensor.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
 File "train_gpt2.py", line 98, in &lt;module&gt;
   train()
 File "/home/abhay/anaconda3/lib/python3.7/site-packages/click/core.py", line 764, in __call__
   return self.main(*args, **kwargs)
 File "/home/abhay/anaconda3/lib/python3.7/site-packages/click/core.py", line 717, in main
   rv = self.invoke(ctx)
 File "/home/abhay/anaconda3/lib/python3.7/site-packages/click/core.py", line 956, in invoke
   return ctx.invoke(self.callback, **ctx.params)
 File "/home/abhay/anaconda3/lib/python3.7/site-packages/click/core.py", line 555, in invoke
   return callback(*args, **kwargs)
 File "train_gpt2.py", line 69, in train
   model.fit([dist_train_dataset, dist_test_dataset])
 File "/home/abhay/edge_gpt2/model_gpt.py", line 362, in fit
   step, train_loss, perplexity = self.distributed_train_step(inputs, targets)
 File "/home/abhay/edge_gpt2/model_gpt.py", line 283, in distributed_train_step
   return _distributed_train_step(inputs, targets)
 File "/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 439, in __call__
   return self._stateless_fn(*args, **kwds)
 File "/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1821, in __call__
   graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
 File "/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 2104, in _maybe_define_function
   *args, **kwargs)
 File "/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1650, in canonicalize_function_inputs
   self._flat_input_signature)
 File "/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1710, in _convert_inputs_to_signature
   format_error_message(inputs, input_signature))
ValueError: When input_signature is provided, all inputs to the Python function must be convertible to tensors:
 inputs: (
   PerReplica:{
 0 /job:localhost/replica:0/task:0/device:GPU:0: tf.Tensor(
[[    3   663   289 ...     0     0     0]
[    3   524   276 ...     0     0     0]
[    3 15756   987 ...     0     0     0]
...
[    3  3164  4571 ...     0     0     0]
[    3  1089   289 ...     0     0     0]
[    3   798  4138 ...     0     0     0]], shape=(16, 512), dtype=int32),
 1 /job:localhost/replica:0/task:0/device:GPU:1: tf.Tensor(
[[    3   663   987 ...     0     0     0]
[    3   175 16233 ...     0     0     0]
[    3   698    53 ...     0     0     0]
...
[    3    83   632 ...     0     0     0]
[    3 13453   168 ...     0     0     0]
[    3   421    53 ...     0     0     0]], shape=(16, 512), dtype=int32)
},
   PerReplica:{
 0 /job:localhost/replica:0/task:0/device:GPU:0: tf.Tensor(
[[  663   289   168 ...     0     0     0]
[  524   276    30 ...     0     0     0]
[15756   987    24 ...     0     0     0]
...
[ 3164  4571    24 ...     0     0     0]
[ 1089   289   396 ...     0     0     0]
[  798  4138   295 ...     0     0     0]], shape=(16, 512), dtype=int32),
 1 /job:localhost/replica:0/task:0/device:GPU:1: tf.Tensor(
[[  663   987   142 ...     0     0     0]
[  175 16233  2043 ...     0     0     0]
[  698    53    45 ...     0     0     0]
...
[   83   632  1739 ...     0     0     0]
[13453   168  5897 ...     0     0     0]
[  421    53    45 ...     0     0     0]], shape=(16, 512), dtype=int32)
})
 input_signature: (
   TensorSpec(shape=(None, None), dtype=tf.int32, name=None),
   TensorSpec(shape=(None, None), dtype=tf.int32, name=None))
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='akanyaani' date='2019-09-10T17:10:03Z'>
		Hi - thank you for reporting, this is a known issue that we plan to fix. (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/29911&gt;#29911&lt;/denchmark-link&gt;
)
Meanwhile, I listed some workarounds in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/29911#issuecomment-505688141&gt;#29911 (comment)&lt;/denchmark-link&gt;
 that can be useful to you ?
		</comment>
		<comment id='6' author='akanyaani' date='2019-09-10T18:25:37Z'>
		Hi, &lt;denchmark-link:https://github.com/guptapriya&gt;@guptapriya&lt;/denchmark-link&gt;
 I am able to train without input signature.
It works when you remove the signature part from tf.function
&lt;denchmark-code&gt;@tf.function(input_signature=input_signature)

To

@tf.function
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='akanyaani' date='2019-12-27T18:53:01Z'>
		This issue has now been fixed and an example has been provided in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/29911&gt;#29911&lt;/denchmark-link&gt;
 for how to use the  property of distributed datasets/iterators to specify the . Please reopen this issue as needed. Thanks!
		</comment>
		<comment id='8' author='akanyaani' date='2019-12-27T18:53:02Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32232&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32232&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>