<bug id='23664' author='sfujiwara' open_date='2018-11-11T20:14:43Z' closed_time='2019-01-29T00:15:57Z'>
	<summary>Documents for MirroredStrategy</summary>
	<description>
System information

TensorFlow version: 1.12
Doc Link:

https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute
https://www.tensorflow.org/api_docs/python/tf/contrib/distribute/MirroredStrategy



Describe the documentation issue
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute&gt;Document i&lt;/denchmark-link&gt;
 says that  is for only the case of "single worker and multi GPUs".
But &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/contrib/distribute/MirroredStrategy&gt;document ii&lt;/denchmark-link&gt;
 says that  can work in multi-worker (multi-node) cluster.
Which is right?
If &lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute&gt;document i&lt;/denchmark-link&gt;
 is out of date, we have to update it.
	</description>
	<comments>
		<comment id='1' author='sfujiwara' date='2018-11-14T18:04:16Z'>
		It looks like that README is out of date, but I'm no expert on the subject.
@yuefengzhou can you confirm?
		</comment>
		<comment id='2' author='sfujiwara' date='2018-11-14T18:20:39Z'>
		MirroredStrategy works for multi-worker cluster but is not recommended. For multi-worker case, please use CollectiveAllReduceStrategy.
		</comment>
		<comment id='3' author='sfujiwara' date='2018-11-20T07:08:34Z'>
		I guess  will be the main stream of TensorFlow high level API because Estimator seems to be moved to &lt;denchmark-link:https://github.com/tensorflow/estimator&gt;another repository&lt;/denchmark-link&gt;
.
However,  can not use  for multi-worker case (that is spacified &lt;denchmark-link:https://www.tensorflow.org/guide/keras#multiple_gpus&gt;here&lt;/denchmark-link&gt;
).
If we want to implement multi-worker case, the best way is use tf.keras, convert it to Estimator with  tf.keras.estimator.model_to_estimator, and using CollectiveAllReduceStrategy?
		</comment>
		<comment id='4' author='sfujiwara' date='2018-11-20T07:20:45Z'>
		&lt;denchmark-link:https://github.com/sfujiwara&gt;@sfujiwara&lt;/denchmark-link&gt;
 This is right. And we are working on multi-worker support for Keras.
		</comment>
		<comment id='5' author='sfujiwara' date='2018-12-18T13:07:45Z'>
		&lt;denchmark-link:https://github.com/yuefengz&gt;@yuefengz&lt;/denchmark-link&gt;
 I recently tested both  and  for various models on a 8 GPU server. I found  is constantly slower than  by about 30%. Is this expected? Or I am missing some configuration? Thanks!
		</comment>
		<comment id='6' author='sfujiwara' date='2019-01-28T23:45:48Z'>
		&lt;denchmark-link:https://github.com/ustcyue&gt;@ustcyue&lt;/denchmark-link&gt;
 The 30% sounds bigger than what we expected. cc &lt;denchmark-link:https://github.com/poxvoculi&gt;@poxvoculi&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='sfujiwara' date='2019-01-29T00:15:56Z'>
		Closing this thread since the discussion for 's performance has been migrated to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/24611&gt;#24611&lt;/denchmark-link&gt;
.
		</comment>
	</comments>
</bug>