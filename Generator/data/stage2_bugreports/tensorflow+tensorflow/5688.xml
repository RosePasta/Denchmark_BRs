<bug id='5688' author='AlgotecOhadSilbert' open_date='2016-11-18T07:17:07Z' closed_time='2017-10-04T18:43:51Z'>
	<summary>tensorflow crashes when using large image with 3d convolutional network</summary>
	<description>
I'm trying to implement a 3d fully convolutional network on my GPU. But for some reason I get a crash.
&lt;denchmark-h:h3&gt;Environment info&lt;/denchmark-h&gt;

Operating System: Ubuntu 14.04 LTS
GPU: GeForce Titan X .
Installed version of CUDA and cuDNN: 8.0 and 5
(attach the output of `ls -l /path/to/cuda/lib/libcud*
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/599236/cud.filelist.txt&gt;cud.filelist.txt&lt;/denchmark-link&gt;
 )
I installed tensorflow version 0.11.0rc2, and it also reproduce in docker installation (gcr.io/tensorflow/tensorflow:latest-gpu)
&lt;denchmark-h:h3&gt;Example code&lt;/denchmark-h&gt;

The following code reproduce the problem:
&lt;denchmark-code&gt;import numpy as np
import tensorflow as tf

graph = tf.Graph()

with graph.as_default():
    tf_dataset = tf.placeholder(tf.float32, shape=(1, 512, 512, 512, 1))
    tf_label = tf.placeholder(tf.float32, shape=(1, 512, 512, 512, 1))

    layer1_weights = tf.Variable(tf.truncated_normal((2, 2, 2, 1, 1), stddev=0.1))
    layer1_bias = tf.Variable(tf.zeros(1))

    conv = tf.nn.conv3d(tf_dataset, layer1_weights, (1, 1, 1, 1, 1), padding='SAME')
    logits = tf.nn.relu(conv+layer1_bias)

    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_label))
    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)

with tf.Session(graph=graph) as session:
    tf.initialize_all_variables().run()
    batchData = np.random.rand(1, 512, 512, 512, 1).astype(np.float32)
    batchLabels = (np.random.rand(1, 512, 512, 512, 1)&gt;0.5).astype(np.float32)
    feed_dict = {tf_dataset : batchData, tf_label : batchLabels}
    _ = session.run((optimizer, ), feed_dict=feed_dict)
&lt;/denchmark-code&gt;

with the following output:

I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties:
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:01:00.0
Total memory: 11.92GiB
Free memory: 11.68GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:01:00.0)
F tensorflow/stream_executor/cuda/cuda_dnn.cc:2440] failed to enqueue convolution on stream: CUDNN_STATUS_NOT_SUPPORTED

	</description>
	<comments>
		<comment id='1' author='AlgotecOhadSilbert' date='2016-11-18T16:00:53Z'>
		Does it work with smaller images?  It vaguely sounds like running out of memory, but it doesn't seem like it should given the numbers and batch size 1.
		</comment>
		<comment id='2' author='AlgotecOhadSilbert' date='2016-11-18T16:21:20Z'>
		Can it be a memory issue? My GPU has 12GB. The size of the input is 0.25GB. Anyhow the following size works:  (32, 32, 512, 512, 1) which is larger in size, but smaller in one convolutional direction.
		</comment>
		<comment id='3' author='AlgotecOhadSilbert' date='2016-11-18T16:50:31Z'>
		@HggsHntr I'm just a simple computer scientist.  Your questions are reasonable, but it's hard for me to think beyond testing whether a smaller image works or not.
		</comment>
		<comment id='4' author='AlgotecOhadSilbert' date='2016-11-20T08:13:12Z'>
		I guess it might be memory issue after all.
I set config.gpu_options.allow_growth = True and traced the memory consumption of the GPU. It raised to the maximum before it crashed.
Also, by changing the stride size to 4 in each direction, or reducing the image size the crash disappear.
So I wonder if this large memory consumption in done intentionally or is it a bug.
		</comment>
		<comment id='5' author='AlgotecOhadSilbert' date='2016-11-21T20:46:25Z'>
		&lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
 Is there any possible issue with GPU convolution scratch space?  (e.g. can the autotuner selects an algorithm based on available memory and then fail later on)
		</comment>
		<comment id='6' author='AlgotecOhadSilbert' date='2016-12-29T16:21:15Z'>
		I've encountered the same issue on tensorflow==0.11.0rc2 with latest keras:
&lt;denchmark-code&gt;F tensorflow/stream_executor/cuda/cuda_dnn.cc:2674] failed to enqueue convolution on stream: CUDNN_STATUS_NOT_SUPPORTED
...... 104 Aborted                 (core dumped) .......
&lt;/denchmark-code&gt;

I was feeding the CNN 10k 200x200x3 images per fit. When I gradually lower the number to 8k, the same issue no longer existed. On the other hand, with the 10k images per fit, the same CNN written in tflearn was just fine.
		</comment>
		<comment id='7' author='AlgotecOhadSilbert' date='2017-01-07T22:54:00Z'>
		can confirm a similar issue with large 3D convolutions:
inshape = (2,258,258,34,1)  # tf img-order
filter_shape = (3,3,3,1,32)
x = tf.placeholder('float32', shape=inshape, name='X')
f = tf.placeholder('float32', shape=filter_shape, name='filter')
c = tf.nn.conv3d(x, f, padding='VALID', strides=[1,1,1,1,1])
grads = tf.gradients(c,f)[0]

xx = np.random.rand(*inshape)
ff = np.random.rand(*filter_shape)

with tf.Session().as_default():
    Q = c.eval(feed_dict={x:xx, f:ff})
    gradQ = grads.eval(feed_dict={x:xx, f:ff})
will yield tensorflow/stream_executor/cuda/cuda_dnn.cc:2674] failed to enqueue convolution on stream: CUDNN_STATUS_EXECUTION_FAILED.
It runs fine if I set inshape = (2,257,257,34,1) instead, so there's some issue with the sizes. However I dont think its the usual GPU-out-of-memory issue:

the same convolution runs perfectly fine in theano (there I can even increase to (2,300,300,34,1) without trouble)
it doesnt respond to all dimension the same: decreasing the batchsize from 2-&gt;1 will still crash even though we just halfed the input

Turns out that it's somehow related to the gradient computations, i.e. commenting the gradQ =... line, just evaluating the result of the convolutions works!
Code was run on tensorflow 0.11.0rc2 (same happens for 0.12.1), Titan X, CUDA-8.0, cudnn 5
		</comment>
		<comment id='8' author='AlgotecOhadSilbert' date='2017-01-26T09:46:00Z'>
		&lt;denchmark-link:https://github.com/redst4r&gt;@redst4r&lt;/denchmark-link&gt;
 I have a similar issue. I too think there could be some issue when calculating the gradients. Any update on how you resolved it?
		</comment>
		<comment id='9' author='AlgotecOhadSilbert' date='2017-01-26T10:10:44Z'>
		&lt;denchmark-link:https://github.com/deepak09027&gt;@deepak09027&lt;/denchmark-link&gt;
 Unfortunately I dont have any solution yet. I was hoping that some googler picks it up here :)
		</comment>
		<comment id='10' author='AlgotecOhadSilbert' date='2017-01-26T10:15:09Z'>
		Can confirm this issue as well. I'm having exactly the same problem! Would be great if this could be resolved :)
		</comment>
		<comment id='11' author='AlgotecOhadSilbert' date='2017-01-30T17:38:44Z'>
		Similar problem here with deep 2d convolution network and huge batch size (because my input is [seq_length * batch_size, height, width] ).
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:02:00.0
Total memory: 11.90GiB
Free memory: 11.75GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x3a9e500
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: 
name: GeForce GTX 980 Ti
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:01:00.0
Total memory: 5.93GiB
Free memory: 4.99GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   N Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: GeForce GTX 980 Ti, pci bus id: 0000:01:00.0)
F tensorflow/stream_executor/cuda/cuda_dnn.cc:2684] failed to enqueue convolution on stream: CUDNN_STATUS_NOT_SUPPORTED
Aborted (core dumped)

Change batch size to a smaller one solves the issue. It'd great if it can throw a python exception : )
		</comment>
		<comment id='12' author='AlgotecOhadSilbert' date='2017-02-06T00:52:13Z'>
		&lt;denchmark-link:https://github.com/prb12&gt;@prb12&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
 any updates on this issue?
		</comment>
		<comment id='13' author='AlgotecOhadSilbert' date='2017-02-17T18:49:32Z'>
		I am having the same issue as well
		</comment>
		<comment id='14' author='AlgotecOhadSilbert' date='2017-02-22T20:30:32Z'>
		Same here. I can reproduce this issue with the example code above. Can confirm CUDA 8, CUDNN 5.1 with tf 1.0.
I ran into an interesting aspect when determining the maximum size of the tensor. It run when tensors with shape less than [1, 256, 256, 256,1] are used. The example below will run and sz=256 will break.
Is anyone looking into this?
&lt;denchmark-code&gt;import numpy as np
import tensorflow as tf

graph = tf.Graph()

sz = 255

with graph.as_default():
    tf_dataset = tf.placeholder(tf.float32, shape=(1, sz, sz, sz, 1))
    tf_label = tf.placeholder(tf.float32, shape=(1, sz, sz, sz, 1))

    layer1_weights = tf.Variable(tf.truncated_normal((2, 2, 2, 1, 1), stddev=0.1))
    layer1_bias = tf.Variable(tf.zeros(1))

    conv = tf.nn.conv3d(tf_dataset, layer1_weights, (1, 1, 1, 1, 1), padding='SAME')
    logits = tf.nn.relu(conv+layer1_bias)

    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_label))
    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)

with tf.Session(graph=graph) as session:
    tf.initialize_all_variables().run()
    batchData = np.random.rand(1, sz, sz, sz, 1).astype(np.float32)
    batchLabels = (np.random.rand(1, sz, sz, sz, 1)&gt;0.5).astype(np.float32)
    feed_dict = {tf_dataset : batchData, tf_label : batchLabels}
    _ = session.run((optimizer, ), feed_dict=feed_dict)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='15' author='AlgotecOhadSilbert' date='2017-03-23T23:29:54Z'>
		I've also run into the same issue, where a P100 with 16GB RAM would get the same error, but I know that my data is not even big enough to take up the entire GPU RAM.
I'm using TF 0.12, CUDA 8.0, CuDNN 5.1.
Has this been fixed in the newest version of TF?
		</comment>
		<comment id='16' author='AlgotecOhadSilbert' date='2017-03-25T22:13:04Z'>
		Mostly its not related to VRAM (I am using latest TF with keras)
In first layer for a VGG like architecture
Convoulution3D(NumChannels, 3,3,3,)(inputs)
if I set , it crashes with the above message.
This somehow contradicting with &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/2190&gt;waTeim &lt;/denchmark-link&gt;

		</comment>
		<comment id='17' author='AlgotecOhadSilbert' date='2017-04-04T04:16:21Z'>
		Hmm, I am trying to run on tensors of shape (1,125,125,125,128) = 1 gig and getting the standard OOM error. I am wondering how much working memory the 3d conv needs. It seems like doing a conv of this size should work ok.
		</comment>
		<comment id='18' author='AlgotecOhadSilbert' date='2017-04-07T23:11:00Z'>
		This issue also goes away for me when I reduce my batch size. Seems like a memory issue that Tensorflow doesn't catch the way it does other OOM issues.
		</comment>
		<comment id='19' author='AlgotecOhadSilbert' date='2017-05-13T05:58:21Z'>
		Did anyone find any solution for this?
I am also hitting this issue when trying to train a CNN. When I reduce the batch size to 2 its runs and at this point the GPU VRAM consumption is 217 MB/3072 MB and on increasing the batch size to 3, it gives the "failed to enqueue convolution on stream: CUDNN_STATUS_EXECUTION_FAILED" error. I doubt if this is due to low VRAM.
TensorFlow's response/support has been very weak on the bugs being reported here. Really disappointing.
		</comment>
		<comment id='20' author='AlgotecOhadSilbert' date='2017-06-16T22:13:20Z'>
		&lt;denchmark-link:https://github.com/tfboyd&gt;@tfboyd&lt;/denchmark-link&gt;
 Any ideas?
		</comment>
		<comment id='21' author='AlgotecOhadSilbert' date='2017-08-05T13:30:10Z'>
		same problem .
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate (GHz) 1.3285 pciBusID 0000:05:00.0 Total memory: 15.89GiB Free memory: 15.61GiB I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:05:00.0) F tensorflow/stream_executor/cuda/cuda_dnn.cc:1989] failed to enqueue convolution on stream: CUDNN_STATUS_EXECUTION_FAILED ('Sample data and label: ', 50000, (3072,), array([ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.])) ('Training data: ', (35000, 3072), (35000, 10), '\nValidation data: ', (15000, 3072), (15000, 10)) Training started !!!!! /cm/local/apps/slurm/var/spool/job109000/slurm_script: line 14: 38773 Aborted                 /home/ksrivastava/tensorflow/bin/python /home/ksrivastava/DL/CIFAR10/scripts/trainGPU.py
		</comment>
		<comment id='22' author='AlgotecOhadSilbert' date='2017-08-05T15:09:55Z'>
		I will try to escalate this.  The issue has been open for a long time and reproduced many times based on my skimming of the thread.
		</comment>
		<comment id='23' author='AlgotecOhadSilbert' date='2017-08-05T18:49:10Z'>
		thanks &lt;denchmark-link:https://github.com/tfboyd&gt;@tfboyd&lt;/denchmark-link&gt;
 !!
		</comment>
		<comment id='24' author='AlgotecOhadSilbert' date='2017-08-15T01:56:09Z'>
		Internal email thread started, looking for an owner to "dig in".  Sorry for the delays hopefully we get this moving forward.
		</comment>
		<comment id='25' author='AlgotecOhadSilbert' date='2017-08-15T19:17:59Z'>
		b/64718915  &lt;denchmark-link:https://github.com/yzhwang&gt;@yzhwang&lt;/denchmark-link&gt;
 is working on it and thank you to &lt;denchmark-link:https://github.com/SrivastavaKshitij&gt;@SrivastavaKshitij&lt;/denchmark-link&gt;
 for following up via email and helping get this prioritized.
		</comment>
		<comment id='26' author='AlgotecOhadSilbert' date='2017-08-16T17:18:20Z'>
		Hi &lt;denchmark-link:https://github.com/SrivastavaKshitij&gt;@SrivastavaKshitij&lt;/denchmark-link&gt;
 could you let me know which version of tensorflow you are using? I tested with @HggsHntr 's example code on tf 1.2.1 with CUDA 8 and cudnn 5.1. This seems to be a duplicated issue with &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/11327&gt;#11327&lt;/denchmark-link&gt;
, which I have already fixed in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/db596594b5653b43fcb558a4753b39904bb62cbd&gt;db59659&lt;/denchmark-link&gt;
. Unfortunately the fix is still not included in 1.3rc2. If you believe this is a separate issue, could you provide a reproducer? Thank you!
		</comment>
		<comment id='27' author='AlgotecOhadSilbert' date='2017-08-23T17:51:43Z'>
		Hi &lt;denchmark-link:https://github.com/yzhwang&gt;@yzhwang&lt;/denchmark-link&gt;
 : I was able to solve the problem. I am working on a cluster and I was using an incompatible version of PGI+OpenMPI with cuda and cudnn version which was causing the problem. But as soon as I corrected it, everything worked fine. If you want I can give u the details.
Thanks
		</comment>
		<comment id='28' author='AlgotecOhadSilbert' date='2017-08-23T18:48:43Z'>
		If it is not too complicated, we would like you to provide a more detailed description, and how you solved it. I think it would help other users with the same configurations.
As for this issue, I will close it for now as I have a fix for this (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/db596594b5653b43fcb558a4753b39904bb62cbd&gt;db59659&lt;/denchmark-link&gt;
) and it has been verified both internally and on the head of OSS version. @HggsHntr if you believe the fix didn't solve the issue, please let us know so that we can reopen and investigate more.
&lt;denchmark-link:https://github.com/SrivastavaKshitij&gt;@SrivastavaKshitij&lt;/denchmark-link&gt;
 Please do put more details if you like even when the issue is closed. Much appreciated!
		</comment>
		<comment id='29' author='AlgotecOhadSilbert' date='2017-09-28T13:16:08Z'>
		&lt;denchmark-link:https://github.com/yzhwang&gt;@yzhwang&lt;/denchmark-link&gt;

I have upgraded to  nightly build (windows-gpu) and when I run the following:
import numpy as np
from keras.engine import Input, Model
from keras.layers import Conv3D, Activation
dim = 256
input_shape = [dim,dim,dim,1]
inputs = Input(input_shape)
conv1 = Conv3D(filters=1, kernel_size=(2, 2, 2), strides=(1,1,1), activation='relu', padding='same')(inputs)
logits = Activation('softmax')(conv1)
model = Model(inputs=inputs, outputs=logits)
model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])
batchData = np.random.rand(1, dim, dim, dim, 1).astype(np.float32)
batchLabels = (np.random.rand(1, dim, dim, dim, 1)&gt;0.5).astype(np.float32)
model.fit(x=batchData, y=batchLabels)
it crashes with an error about Conv3DBackpropFilterV2
		</comment>
		<comment id='30' author='AlgotecOhadSilbert' date='2017-10-01T05:14:44Z'>
		&lt;denchmark-link:https://github.com/yzhwang&gt;@yzhwang&lt;/denchmark-link&gt;

I get the same thing as &lt;denchmark-link:https://github.com/hadarpo&gt;@hadarpo&lt;/denchmark-link&gt;
 .
Is it the same bug or something else?
		</comment>
		<comment id='31' author='AlgotecOhadSilbert' date='2017-10-02T03:45:11Z'>
		Hi &lt;denchmark-link:https://github.com/hadarpo&gt;@hadarpo&lt;/denchmark-link&gt;
 and @HggsHntr , I cannot reproduce this on my linux box. I do not have a Windows machine to test this right now. I will ask my colleagues to help reproduce this. Meanwhile, if you could include a full error log here, I will start looking at it and see where we go from there. Thank you!
		</comment>
		<comment id='32' author='AlgotecOhadSilbert' date='2017-10-02T08:32:59Z'>
		Hi &lt;denchmark-link:https://github.com/yzhwang&gt;@yzhwang&lt;/denchmark-link&gt;
, thanks for your response. I have tried the latest nightly build (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/55&gt;#55&lt;/denchmark-link&gt;
) with the original tester of @HggsHntr and got the following error output:
&lt;denchmark-code&gt;2017-10-02 11:30:17.785801: E C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\stream_executor\cuda\cuda_driver.cc:1080] failed to synchronize the stop event: CUDA_ERROR_LAUNCH_FAILED
2017-10-02 11:30:17.785922: E C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\stream_executor\cuda\cuda_timer.cc:54] Internal: error destroying CUDA event in context 0000028FBE051ED0: CUDA_ERROR_LAUNCH_FAILED
2017-10-02 11:30:17.787104: E C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\stream_executor\cuda\cuda_timer.cc:59] Internal: error destroying CUDA event in context 0000028FBE051ED0: CUDA_ERROR_LAUNCH_FAILED
2017-10-02 11:30:17.787561: E C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\stream_executor\cuda\cuda_timer.cc:54] Invalid argument: input event cannot be null
2017-10-02 11:30:17.787960: E C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\stream_executor\cuda\cuda_timer.cc:59] Invalid argument: input event cannot be null
2017-10-02 11:30:17.788460: E C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\stream_executor\cuda\cuda_timer.cc:54] Invalid argument: input event cannot be null
2017-10-02 11:30:17.788998: E C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\stream_executor\cuda\cuda_timer.cc:59] Invalid argument: input event cannot be null
2017-10-02 11:30:17.789578: E C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\stream_executor\cuda\cuda_timer.cc:54] Invalid argument: input event cannot be null
2017-10-02 11:30:17.790140: E C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\stream_executor\cuda\cuda_timer.cc:59] Invalid argument: input event cannot be null
2017-10-02 11:30:17.790606: E C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\stream_executor\cuda\cuda_timer.cc:54] Invalid argument: input event cannot be null
2017-10-02 11:30:17.790944: E C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\stream_executor\cuda\cuda_timer.cc:59] Invalid argument: input event cannot be null
2017-10-02 11:30:17.790987: E C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\stream_executor\cuda\cuda_timer.cc:54] Invalid argument: input event cannot be null
2017-10-02 11:30:17.791331: E C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\stream_executor\cuda\cuda_timer.cc:59] Invalid argument: input event cannot be null
---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
~\AppData\Local\conda\conda\envs\hadar-tensorflow\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)
   1322     try:
-&gt; 1323       return fn(*args)
   1324     except errors.OpError as e:

~\AppData\Local\conda\conda\envs\hadar-tensorflow\lib\site-packages\tensorflow\python\client\session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
   1301                                    feed_dict, fetch_list, target_list,
-&gt; 1302                                    status, run_metadata)
   1303

~\AppData\Local\conda\conda\envs\hadar-tensorflow\lib\contextlib.py in __exit__(self, type, value, traceback)
     65             try:
---&gt; 66                 next(self.gen)
     67             except StopIteration:

~\AppData\Local\conda\conda\envs\hadar-tensorflow\lib\site-packages\tensorflow\python\framework\errors_impl.py in raise_exception_on_not_ok_status()
    466           compat.as_text(c_api.TF_Message(status.status)),
--&gt; 467           c_api.TF_GetCode(status.status))
    468   # Delete the underlying status object from memory otherwise it stays alive

NotFoundError: No algorithm worked!
         [[Node: gradients/Conv3D_grad/Conv3DBackpropFilterV2 = Conv3DBackpropFilterV2[T=DT_FLOAT, data_format="NDHWC", padding="SAME", strides=[1, 1, 1, 1, 1], _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_0_0/_1, gradients/Conv3D_grad/Shape_1, gradients/add_grad/tuple/control_dependency)]]

During handling of the above exception, another exception occurred:

NotFoundError                             Traceback (most recent call last)
&lt;ipython-input-1-0ef7131b03e5&gt; in &lt;module&gt;()
     22     batchLabels = (np.random.rand(1, 512, 512, 512, 1)&gt;0.5).astype(np.float32)
     23     feed_dict = {tf_dataset : batchData, tf_label : batchLabels}
---&gt; 24     _ = session.run((optimizer, ), feed_dict=feed_dict)
     25

~\AppData\Local\conda\conda\envs\hadar-tensorflow\lib\site-packages\tensorflow\python\client\session.py in run(self, fetches, feed_dict, options, run_metadata)
    887     try:
    888       result = self._run(None, fetches, feed_dict, options_ptr,
--&gt; 889                          run_metadata_ptr)
    890       if run_metadata:
    891         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

~\AppData\Local\conda\conda\envs\hadar-tensorflow\lib\site-packages\tensorflow\python\client\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1118     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1119       results = self._do_run(handle, final_targets, final_fetches,
-&gt; 1120                              feed_dict_tensor, options, run_metadata)
   1121     else:
   1122       results = []

~\AppData\Local\conda\conda\envs\hadar-tensorflow\lib\site-packages\tensorflow\python\client\session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1315     if handle is None:
   1316       return self._do_call(_run_fn, self._session, feeds, fetches, targets,
-&gt; 1317                            options, run_metadata)
   1318     else:
   1319       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)

~\AppData\Local\conda\conda\envs\hadar-tensorflow\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)
   1334         except KeyError:
   1335           pass
-&gt; 1336       raise type(e)(node_def, op, message)
   1337
   1338   def _extend_graph(self):

NotFoundError: No algorithm worked!
         [[Node: gradients/Conv3D_grad/Conv3DBackpropFilterV2 = Conv3DBackpropFilterV2[T=DT_FLOAT, data_format="NDHWC", padding="SAME", strides=[1, 1, 1, 1, 1], _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_0_0/_1, gradients/Conv3D_grad/Shape_1, gradients/add_grad/tuple/control_dependency)]]

Caused by op 'gradients/Conv3D_grad/Conv3DBackpropFilterV2', defined at:
  File "C:\Users\hadar\AppData\Local\conda\conda\envs\hadar-tensorflow\Scripts\ipython-script.py", line 5, in &lt;module&gt;
    sys.exit(IPython.start_ipython())
  File "C:\Users\hadar\AppData\Local\conda\conda\envs\hadar-tensorflow\lib\site-packages\IPython\__init__.py", line 125, in start_ipython
    return launch_new_instance(argv=argv, **kwargs)
  File "C:\Users\hadar\AppData\Local\conda\conda\envs\hadar-tensorflow\lib\site-packages\traitlets\config\application.py", line 658, in launch_instance
    app.start()
  File "C:\Users\hadar\AppData\Local\conda\conda\envs\hadar-tensorflow\lib\site-packages\IPython\terminal\ipapp.py", line 356, in start
    self.shell.mainloop()
  File "C:\Users\hadar\AppData\Local\conda\conda\envs\hadar-tensorflow\lib\site-packages\IPython\terminal\interactiveshell.py", line 480, in mainloop
    self.interact()
  File "C:\Users\hadar\AppData\Local\conda\conda\envs\hadar-tensorflow\lib\site-packages\IPython\terminal\interactiveshell.py", line 471, in interact
    self.run_cell(code, store_history=True)
  File "C:\Users\hadar\AppData\Local\conda\conda\envs\hadar-tensorflow\lib\site-packages\IPython\core\interactiveshell.py", line 2698, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File "C:\Users\hadar\AppData\Local\conda\conda\envs\hadar-tensorflow\lib\site-packages\IPython\core\interactiveshell.py", line 2802, in run_ast_nodes
    if self.run_code(code, result):
  File "C:\Users\hadar\AppData\Local\conda\conda\envs\hadar-tensorflow\lib\site-packages\IPython\core\interactiveshell.py", line 2862, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "&lt;ipython-input-1-0ef7131b03e5&gt;", line 17, in &lt;module&gt;
    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)
  File "C:\Users\hadar\AppData\Local\conda\conda\envs\hadar-tensorflow\lib\site-packages\tensorflow\python\training\optimizer.py", line 343, in minimize
    grad_loss=grad_loss)
  File "C:\Users\hadar\AppData\Local\conda\conda\envs\hadar-tensorflow\lib\site-packages\tensorflow\python\training\optimizer.py", line 414, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File "C:\Users\hadar\AppData\Local\conda\conda\envs\hadar-tensorflow\lib\site-packages\tensorflow\python\ops\gradients_impl.py", line 581, in gradients
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File "C:\Users\hadar\AppData\Local\conda\conda\envs\hadar-tensorflow\lib\site-packages\tensorflow\python\ops\gradients_impl.py", line 353, in _MaybeCompile
    return grad_fn()  # Exit early
  File "C:\Users\hadar\AppData\Local\conda\conda\envs\hadar-tensorflow\lib\site-packages\tensorflow\python\ops\gradients_impl.py", line 581, in &lt;lambda&gt;
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File "C:\Users\hadar\AppData\Local\conda\conda\envs\hadar-tensorflow\lib\site-packages\tensorflow\python\ops\nn_grad.py", line 88, in _Conv3DGrad
    data_format=data_format)]
  File "C:\Users\hadar\AppData\Local\conda\conda\envs\hadar-tensorflow\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py", line 964, in conv3d_backprop_filter_v2
    data_format=data_format, name=name)
  File "C:\Users\hadar\AppData\Local\conda\conda\envs\hadar-tensorflow\lib\site-packages\tensorflow\python\framework\op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "C:\Users\hadar\AppData\Local\conda\conda\envs\hadar-tensorflow\lib\site-packages\tensorflow\python\framework\ops.py", line 3090, in create_op
    op_def=op_def)
  File "C:\Users\hadar\AppData\Local\conda\conda\envs\hadar-tensorflow\lib\site-packages\tensorflow\python\framework\ops.py", line 1638, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

...which was originally created as op 'Conv3D', defined at:
  File "C:\Users\hadar\AppData\Local\conda\conda\envs\hadar-tensorflow\Scripts\ipython-script.py", line 5, in &lt;module&gt;
    sys.exit(IPython.start_ipython())
[elided 7 identical lines from previous traceback]
  File "C:\Users\hadar\AppData\Local\conda\conda\envs\hadar-tensorflow\lib\site-packages\IPython\core\interactiveshell.py", line 2862, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "&lt;ipython-input-1-0ef7131b03e5&gt;", line 13, in &lt;module&gt;
    conv = tf.nn.conv3d(tf_dataset, layer1_weights, (1, 1, 1, 1, 1), padding='SAME')
  File "C:\Users\hadar\AppData\Local\conda\conda\envs\hadar-tensorflow\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py", line 846, in conv3d
    padding=padding, data_format=data_format, name=name)
  File "C:\Users\hadar\AppData\Local\conda\conda\envs\hadar-tensorflow\lib\site-packages\tensorflow\python\framework\op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "C:\Users\hadar\AppData\Local\conda\conda\envs\hadar-tensorflow\lib\site-packages\tensorflow\python\framework\ops.py", line 3090, in create_op
    op_def=op_def)
  File "C:\Users\hadar\AppData\Local\conda\conda\envs\hadar-tensorflow\lib\site-packages\tensorflow\python\framework\ops.py", line 1638, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

NotFoundError (see above for traceback): No algorithm worked!
         [[Node: gradients/Conv3D_grad/Conv3DBackpropFilterV2 = Conv3DBackpropFilterV2[T=DT_FLOAT, data_format="NDHWC", padding="SAME", strides=[1, 1, 1, 1, 1], _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_0_0/_1, gradients/Conv3D_grad/Shape_1, gradients/add_grad/tuple/control_dependency)]]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='33' author='AlgotecOhadSilbert' date='2017-10-02T20:28:31Z'>
		Hi &lt;denchmark-link:https://github.com/hadarpo&gt;@hadarpo&lt;/denchmark-link&gt;
 , I'm trying to reproduce this on a Windows machine. Just a couple of things I want to make sure:

Are you using the head of TensorFlow on our github repo?
Are you using this reproducer (#5688 (comment)) or the original reproducer (#5688 (comment))?
It would be nice if you could also provide us the CUDA version, cudnn version, and display driver version you use.

Thank you!
		</comment>
		<comment id='34' author='AlgotecOhadSilbert' date='2017-10-03T06:04:01Z'>
		Hi &lt;denchmark-link:https://github.com/yzhwang&gt;@yzhwang&lt;/denchmark-link&gt;


I've installed tensorflow from jenkins nightly build using the following wheel:
https://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-windows/M=windows-gpu,PY=35/55/artifact/cmake_build/tf_python/dist/tf_nightly_gpu-1.4.0.dev20170929-cp35-cp35m-win_amd64.whl
I am using the original tensorflow version of the reproducer (even though the keras one gives the same results)
I'm using CUDA 8.0 and cudnn6, and the NVIDIA drivers are as follow:

Thank you!

		</comment>
		<comment id='35' author='AlgotecOhadSilbert' date='2017-10-03T21:33:24Z'>
		Hi &lt;denchmark-link:https://github.com/hadarpo&gt;@hadarpo&lt;/denchmark-link&gt;
 I updated the original reproducer and ran it on a windows machine with the nightly build TF that you had and CUDA 8/cudnnv6, and I wasn't able to reproduce your error. Here's the result:
&lt;denchmark-code&gt;C:\Users\yangzihao\Desktop&gt;"C:\Program Files\Anaconda3\python.exe" tfversion.py
1.4.0-dev20170929

C:\Users\yangzihao\Desktop&gt;"C:\Program Files\Anaconda3\python.exe" run.py
2017-10-03 20:41:13.569273: I C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\core\platform\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX
2017-10-03 20:41:14.419846: I C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:965] Found device 0 with properties:
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:04.0
totalMemory: 11.18GiB freeMemory: 11.12GiB
2017-10-03 20:41:14.560192: I C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:965] Found device 1 with properties:
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:05.0
totalMemory: 11.18GiB freeMemory: 11.12GiB
2017-10-03 20:41:14.696383: I C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:965] Found device 2 with properties:
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:06.0
totalMemory: 11.18GiB freeMemory: 11.12GiB
2017-10-03 20:41:14.830435: I C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:965] Found device 3 with properties:
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:07.0
totalMemory: 11.18GiB freeMemory: 11.12GiB
2017-10-03 20:41:14.962369: I C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:965] Found device 4 with properties:
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:08.0
totalMemory: 11.18GiB freeMemory: 11.12GiB
2017-10-03 20:41:15.093104: I C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:965] Found device 5 with properties:
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:09.0
totalMemory: 11.18GiB freeMemory: 11.12GiB
2017-10-03 20:41:15.230357: I C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:965] Found device 6 with properties:
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:0a.0
totalMemory: 11.18GiB freeMemory: 11.12GiB
2017-10-03 20:41:15.368208: I C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:965] Found device 7 with properties:
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:0b.0
totalMemory: 11.18GiB freeMemory: 11.12GiB
2017-10-03 20:41:15.369462: I C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:980] Device peer to peer matrix
2017-10-03 20:41:15.371303: I C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:986] DMA: 0 1 2 3 4 5 6 7
2017-10-03 20:41:15.371382: I C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:996] 0:   Y N N N N N N N
2017-10-03 20:41:15.371969: I C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:996] 1:   N Y N N N N N N
2017-10-03 20:41:15.372523: I C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:996] 2:   N N Y N N N N N
2017-10-03 20:41:15.373115: I C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:996] 3:   N N N Y N N N N
2017-10-03 20:41:15.373629: I C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:996] 4:   N N N N Y N N N
2017-10-03 20:41:15.374197: I C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:996] 5:   N N N N N Y N N
2017-10-03 20:41:15.374683: I C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:996] 6:   N N N N N N Y N
2017-10-03 20:41:15.375206: I C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:996] 7:   N N N N N N N Y
2017-10-03 20:41:15.375761: I C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)
2017-10-03 20:41:15.376257: I C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:1) -&gt; (device: 1, name: Tesla K80, pci bus id: 0000:00:05.0, compute capability: 3.7)
2017-10-03 20:41:15.376797: I C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:2) -&gt; (device: 2, name: Tesla K80, pci bus id: 0000:00:06.0, compute capability: 3.7)
2017-10-03 20:41:15.377331: I C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:3) -&gt; (device: 3, name: Tesla K80, pci bus id: 0000:00:07.0, compute capability: 3.7)
2017-10-03 20:41:15.377845: I C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:4) -&gt; (device: 4, name: Tesla K80, pci bus id: 0000:00:08.0, compute capability: 3.7)
2017-10-03 20:41:15.378384: I C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:5) -&gt; (device: 5, name: Tesla K80, pci bus id: 0000:00:09.0, compute capability: 3.7)
2017-10-03 20:41:15.378910: I C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:6) -&gt; (device: 6, name: Tesla K80, pci bus id: 0000:00:0a.0, compute capability: 3.7)
2017-10-03 20:41:15.379449: I C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:7) -&gt; (device: 7, name: Tesla K80, pci bus id: 0000:00:0b.0, compute capability: 3.7)
&lt;/denchmark-code&gt;

Here's the modified original reproducer (only interface changes to make it run on v1.4):
&lt;denchmark-code&gt;import numpy as np
import tensorflow as tf

graph = tf.Graph()

with graph.as_default():
    tf_dataset = tf.placeholder(tf.float32, shape=(1, 512, 512, 512, 1))
    tf_label = tf.placeholder(tf.float32, shape=(1, 512, 512, 512, 1))

    layer1_weights = tf.Variable(tf.truncated_normal((2, 2, 2, 1, 1), stddev=0.1))
    layer1_bias = tf.Variable(tf.zeros(1))

    conv = tf.nn.conv3d(tf_dataset, layer1_weights, (1, 1, 1, 1, 1), padding='SAME')
    logits = tf.nn.relu(conv+layer1_bias)

    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_label))
    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)

with tf.Session(graph=graph) as session:
    tf.global_variables_initializer().run()
    batchData = np.random.rand(1, 512, 512, 512, 1).astype(np.float32)
    batchLabels = (np.random.rand(1, 512, 512, 512, 1)&gt;0.5).astype(np.float32)
    feed_dict = {tf_dataset : batchData, tf_label : batchLabels}
    _ = session.run((optimizer, ), feed_dict=feed_dict)
&lt;/denchmark-code&gt;

Could you try to run other conv-related tests (for example this one):
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/conv_ops_3d_test.py&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/conv_ops_3d_test.py&lt;/denchmark-link&gt;

And also other CUDA samples to make sure your CUDA/cudnn works correctly first?
Since although it looks like the error was from conv3d_backprop, but the stream has already been polluted by earlier CUDA_ERROR_LAUNCH_FAILEDs in cuda_driver.cc and cuda_timer.cc.
		</comment>
		<comment id='36' author='AlgotecOhadSilbert' date='2017-10-04T18:43:51Z'>
		&lt;denchmark-link:https://github.com/hadarpo&gt;@hadarpo&lt;/denchmark-link&gt;
 have you tried my suggestions? Since I cannot reproduce it and from the error log you posted I don't think it's related to conv3d_backprop, I will close this issue for now. Feel free to comment or open another issue. Thank you!
		</comment>
		<comment id='37' author='AlgotecOhadSilbert' date='2017-10-05T19:20:11Z'>
		Hi &lt;denchmark-link:https://github.com/yzhwang&gt;@yzhwang&lt;/denchmark-link&gt;
 ,
Thank you for your work.
I tested the tesnsor-flow 1.4 windows-gpu night build on two different machines:
windows server 2016 + Titan X + cuda 8 + cudnn 6
windows 10 + GTX 1050 + cuda 8 + cudnn 6
On both the script conv_ops_3d_test.py ran without any problem.
On both the modified reproducer code failed: titan X with  CUDA_ERROR_LAUNCH_FAILED and GTX 1050 with CUDA_ERROR_LAUNCH_TIMEOUT .
On the Titan X (12GB of memory) if I change in all places 512 to 202 it works perfectly well, and any number above it produces the same failure.
On the GTX 1050, (3GB of available memory) I can get as high as 255. Any number above it produces the following error:
&lt;denchmark-code&gt;2017-10-05 22:09:24.921703: E C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\stream_executor\cuda\cuda_driver.cc:1110] could not synchronize on CUDA context: CUDA_ERROR_LAUNCH_TIMEOUT :: No stack trace available
&lt;/denchmark-code&gt;

I have no idea if these errors relates to the same problem or not.
		</comment>
		<comment id='38' author='AlgotecOhadSilbert' date='2017-10-09T16:04:50Z'>
		Hi @HggsHntr , as I can't find any Windows server with Pascal/Maxwell architecture, could you try adding this line to the python reproducer:
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
or type:
set CUDA_LAUNCH_BLOCKING=1
in command line?
This is to force CUDA kernels to synchronize after each call, and hopefully will show us more useful error logs. So that we can tell whether it is related to conv3d kernel or not. Thank you!
		</comment>
		<comment id='39' author='AlgotecOhadSilbert' date='2017-10-09T17:53:40Z'>
		Hi &lt;denchmark-link:https://github.com/yzhwang&gt;@yzhwang&lt;/denchmark-link&gt;
 , It seems like I get the same error. No new information. please see below.
&lt;denchmark-code&gt;2017-10-09 20:46:28.203661: I C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\core\platform\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2017-10-09 20:46:28.494815: I C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:965] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076
pciBusID: 0000:01:00.0
totalMemory: 12.00GiB freeMemory: 10.06GiB
2017-10-09 20:46:28.494997: I C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
2017-10-09 20:47:40.380734: E C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\stream_executor\cuda\cuda_timer.cc:54] Internal: error destroying CUDA event in context 0000014DD584E910: CUDA_ERROR_LAUNCH_FAILED
2017-10-09 20:47:40.380902: E C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\stream_executor\cuda\cuda_timer.cc:59] Internal: error destroying CUDA event in context 0000014DD584E910: CUDA_ERROR_LAUNCH_FAILED
2017-10-09 20:47:40.382666: E C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\stream_executor\cuda\cuda_timer.cc:54] Invalid argument: input event cannot be null
2017-10-09 20:47:40.383087: E C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\stream_executor\cuda\cuda_timer.cc:59] Invalid argument: input event cannot be null
2017-10-09 20:47:40.383572: E C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\stream_executor\cuda\cuda_timer.cc:54] Invalid argument: input event cannot be null
2017-10-09 20:47:40.384041: E C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\stream_executor\cuda\cuda_timer.cc:59] Invalid argument: input event cannot be null
2017-10-09 20:47:40.384500: E C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\stream_executor\cuda\cuda_timer.cc:54] Invalid argument: input event cannot be null
2017-10-09 20:47:40.384520: E C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\stream_executor\cuda\cuda_timer.cc:59] Invalid argument: input event cannot be null
2017-10-09 20:47:40.384919: E C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\stream_executor\cuda\cuda_timer.cc:54] Invalid argument: input event cannot be null
2017-10-09 20:47:40.385438: E C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\stream_executor\cuda\cuda_timer.cc:59] Invalid argument: input event cannot be null
2017-10-09 20:47:40.385884: E C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\stream_executor\cuda\cuda_timer.cc:54] Invalid argument: input event cannot be null
2017-10-09 20:47:40.386252: E C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\stream_executor\cuda\cuda_timer.cc:59] Invalid argument: input event cannot be null
---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
~\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)
   1322     try:
-&gt; 1323       return fn(*args)
   1324     except errors.OpError as e:

~\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\client\session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
   1301                                    feed_dict, fetch_list, target_list,
-&gt; 1302                                    status, run_metadata)
   1303

~\AppData\Local\conda\conda\envs\tftest\lib\contextlib.py in __exit__(self, type, value, traceback)
     65             try:
---&gt; 66                 next(self.gen)
     67             except StopIteration:

~\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\framework\errors_impl.py in raise_exception_on_not_ok_status()
    466           compat.as_text(c_api.TF_Message(status.status)),
--&gt; 467           c_api.TF_GetCode(status.status))
    468   # Delete the underlying status object from memory otherwise it stays alive

NotFoundError: No algorithm worked!
         [[Node: gradients/Conv3D_grad/Conv3DBackpropFilterV2 = Conv3DBackpropFilterV2[T=DT_FLOAT, data_format="NDHWC", padding="SAME", strides=[1, 1, 1, 1, 1], _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_0_0/_1, gradients/Conv3D_grad/Shape_1, gradients/add_grad/tuple/control_dependency)]]

During handling of the above exception, another exception occurred:

NotFoundError                             Traceback (most recent call last)
&lt;ipython-input-1-7824d7b75ec7&gt; in &lt;module&gt;()
     26     batchLabels = (np.random.rand(1, n, n, n, 1)&gt;0.5).astype(np.float32)
     27     feed_dict = {tf_dataset : batchData, tf_label : batchLabels}
---&gt; 28     _ = session.run((optimizer, ), feed_dict=feed_dict)
     29

~\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\client\session.py in run(self, fetches, feed_dict, options, run_metadata)
    887     try:
    888       result = self._run(None, fetches, feed_dict, options_ptr,
--&gt; 889                          run_metadata_ptr)
    890       if run_metadata:
    891         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

~\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\client\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1118     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1119       results = self._do_run(handle, final_targets, final_fetches,
-&gt; 1120                              feed_dict_tensor, options, run_metadata)
   1121     else:
   1122       results = []

~\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\client\session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1315     if handle is None:
   1316       return self._do_call(_run_fn, self._session, feeds, fetches, targets,
-&gt; 1317                            options, run_metadata)
   1318     else:
   1319       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)

~\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)
   1334         except KeyError:
   1335           pass
-&gt; 1336       raise type(e)(node_def, op, message)
   1337
   1338   def _extend_graph(self):

NotFoundError: No algorithm worked!
         [[Node: gradients/Conv3D_grad/Conv3DBackpropFilterV2 = Conv3DBackpropFilterV2[T=DT_FLOAT, data_format="NDHWC", padding="SAME", strides=[1, 1, 1, 1, 1], _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_0_0/_1, gradients/Conv3D_grad/Shape_1, gradients/add_grad/tuple/control_dependency)]]

Caused by op 'gradients/Conv3D_grad/Conv3DBackpropFilterV2', defined at:
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\Scripts\ipython-script.py", line 5, in &lt;module&gt;
    sys.exit(IPython.start_ipython())
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\IPython\__init__.py", line 125, in start_ipython
    return launch_new_instance(argv=argv, **kwargs)
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\traitlets\config\application.py", line 658, in launch_instance
    app.start()
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\IPython\terminal\ipapp.py", line 356, in start
    self.shell.mainloop()
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\IPython\terminal\interactiveshell.py", line 480, in mainloop
    self.interact()
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\IPython\terminal\interactiveshell.py", line 471, in interact
    self.run_cell(code, store_history=True)
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\IPython\core\interactiveshell.py", line 2698, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\IPython\core\interactiveshell.py", line 2802, in run_ast_nodes
    if self.run_code(code, result):
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\IPython\core\interactiveshell.py", line 2862, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "&lt;ipython-input-1-7824d7b75ec7&gt;", line 21, in &lt;module&gt;
    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\training\optimizer.py", line 343, in minimize
    grad_loss=grad_loss)
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\training\optimizer.py", line 414, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\ops\gradients_impl.py", line 581, in gradients
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\ops\gradients_impl.py", line 353, in _MaybeCompile
    return grad_fn()  # Exit early
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\ops\gradients_impl.py", line 581, in &lt;lambda&gt;
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\ops\nn_grad.py", line 88, in _Conv3DGrad
    data_format=data_format)]
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py", line 964, in conv3d_backprop_filter_v2
    data_format=data_format, name=name)
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\framework\op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\framework\ops.py", line 2936, in create_op
    op_def=op_def)
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\framework\ops.py", line 1464, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

...which was originally created as op 'Conv3D', defined at:
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\Scripts\ipython-script.py", line 5, in &lt;module&gt;
    sys.exit(IPython.start_ipython())
[elided 7 identical lines from previous traceback]
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\IPython\core\interactiveshell.py", line 2862, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "&lt;ipython-input-1-7824d7b75ec7&gt;", line 17, in &lt;module&gt;
    conv = tf.nn.conv3d(tf_dataset, layer1_weights, (1, 1, 1, 1, 1), padding='SAME')
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py", line 846, in conv3d
    padding=padding, data_format=data_format, name=name)
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\framework\op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\framework\ops.py", line 2936, in create_op
    op_def=op_def)
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\framework\ops.py", line 1464, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

NotFoundError (see above for traceback): No algorithm worked!
         [[Node: gradients/Conv3D_grad/Conv3DBackpropFilterV2 = Conv3DBackpropFilterV2[T=DT_FLOAT, data_format="NDHWC", padding="SAME", strides=[1, 1, 1, 1, 1], _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_0_0/_1, gradients/Conv3D_grad/Shape_1, gradients/add_grad/tuple/control_dependency)]]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='40' author='AlgotecOhadSilbert' date='2017-10-10T18:29:37Z'>
		Hi @HggsHntr , thanks for the info! Since we do not have any Windows machine with Pascal/Maxwell card, I will provide some suggestions on how to further debugging the issue:

os.environ['TF_CUDNN_USE_AUTOTUNE'] = '0'
or type:
set TF_CUDNN_USE_AUTOTUNE=0
This will disable the autotune on convolutions, and force all cudnn convolution calls to use the default algorithm.
If this doesn't solve the problem, that means it is not related to conv3d backprop op and must be other CUDA issues. I can tell you where to go from there. If this solves the problem, then it means that during the profiling of one of the internal cudnn kernels, there was some kernel launch error. I'm working on an extensive test suite for cudnn's convolution algorithms to help us spot which internal algorithm caused the problem. Before I release that tool, there is a hacky way to do it yourself though, you could try the following two things:
Add cuda synchronization code and error checking code after each cudnnConvolutionBackwardFilter call:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L3398
The code can be something like this:

&lt;denchmark-code&gt;cudaThreadSynchronize();                                                                                                              
cudaError_t err = cudaPeekAtLastError();
if (err != cudaSuccess) {
  LOG(FATAL) &lt;&lt; "kernel launch error: " &lt;&lt; err;
}
&lt;/denchmark-code&gt;


Modify this function to get only one internal kernel for cudnnConvolutionBackwardFilter and test which one caused the failure:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L2540
Either way, if you manage to reproduce the bug in TensorFlow, I will write a standalone cudnn C++ reproducer according to your shape and internal algorithm used and file it to NVIDIA. Let me know if that sounds OK to you. Another way is to wait for the extensive cudnn test tool I'm working on. I should be able to finish it before November.

		</comment>
		<comment id='41' author='AlgotecOhadSilbert' date='2017-10-11T07:04:52Z'>
		Hi &lt;denchmark-link:https://github.com/yzhwang&gt;@yzhwang&lt;/denchmark-link&gt;
 ,
I set both env vars as you suggested. still no luck. It is still crashing on network sizes larger than roughly 200.  Let me know if you want to see the errors.
About adding some debugging code messages, I don't mind to give it a try but I don't have a building environment for tensorflow. Is there an instruction page on how to do it?
thanks for all you effort.
		</comment>
		<comment id='42' author='AlgotecOhadSilbert' date='2017-10-11T17:38:38Z'>
		@HggsHntr Please do let me see the error log as if no autotune is involved, there shouldn't be any profiling so I assume the error must came from a different place instead of cuda_timer.cc.
		</comment>
		<comment id='43' author='AlgotecOhadSilbert' date='2017-10-11T18:52:14Z'>
		&lt;denchmark-link:https://github.com/yzhwang&gt;@yzhwang&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;2017-10-11 21:48:30.115149: I C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\core\platform\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2017-10-11 21:48:30.402802: I C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:965] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076
pciBusID: 0000:01:00.0
totalMemory: 12.00GiB freeMemory: 10.06GiB
2017-10-11 21:48:30.402991: I C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
2017-10-11 21:48:47.601437: E C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows-gpu\PY\35\tensorflow\stream_executor\cuda\cuda_dnn.cc:3387] failed to enqueue convolution on stream: CUDNN_STATUS_EXECUTION_FAILED
---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
~\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)
   1322     try:
-&gt; 1323       return fn(*args)
   1324     except errors.OpError as e:

~\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\client\session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
   1301                                    feed_dict, fetch_list, target_list,
-&gt; 1302                                    status, run_metadata)
   1303

~\AppData\Local\conda\conda\envs\tftest\lib\contextlib.py in __exit__(self, type, value, traceback)
     65             try:
---&gt; 66                 next(self.gen)
     67             except StopIteration:

~\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\framework\errors_impl.py in raise_exception_on_not_ok_status()
    466           compat.as_text(c_api.TF_Message(status.status)),
--&gt; 467           c_api.TF_GetCode(status.status))
    468   # Delete the underlying status object from memory otherwise it stays alive

InternalError: cuDNN Backward Filter function launch failure : input shape([1,512,512,512,1]) filter shape([2,2,2,1,1])
         [[Node: gradients/Conv3D_grad/Conv3DBackpropFilterV2 = Conv3DBackpropFilterV2[T=DT_FLOAT, data_format="NDHWC", padding="SAME", strides=[1, 1, 1, 1, 1], _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_0_0/_1, gradients/Conv3D_grad/Shape_1, gradients/add_grad/tuple/control_dependency)]]

During handling of the above exception, another exception occurred:

InternalError                             Traceback (most recent call last)
&lt;ipython-input-1-436eb9c8573a&gt; in &lt;module&gt;()
     28     batchLabels = (np.random.rand(1, n, n, n, 1)&gt;0.5).astype(np.float32)
     29     feed_dict = {tf_dataset : batchData, tf_label : batchLabels}
---&gt; 30     _ = session.run((optimizer, ), feed_dict=feed_dict)
     31

~\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\client\session.py in run(self, fetches, feed_dict, options, run_metadata)
    887     try:
    888       result = self._run(None, fetches, feed_dict, options_ptr,
--&gt; 889                          run_metadata_ptr)
    890       if run_metadata:
    891         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

~\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\client\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1118     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1119       results = self._do_run(handle, final_targets, final_fetches,
-&gt; 1120                              feed_dict_tensor, options, run_metadata)
   1121     else:
   1122       results = []

~\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\client\session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1315     if handle is None:
   1316       return self._do_call(_run_fn, self._session, feeds, fetches, targets,
-&gt; 1317                            options, run_metadata)
   1318     else:
   1319       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)

~\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)
   1334         except KeyError:
   1335           pass
-&gt; 1336       raise type(e)(node_def, op, message)
   1337
   1338   def _extend_graph(self):

InternalError: cuDNN Backward Filter function launch failure : input shape([1,512,512,512,1]) filter shape([2,2,2,1,1])
         [[Node: gradients/Conv3D_grad/Conv3DBackpropFilterV2 = Conv3DBackpropFilterV2[T=DT_FLOAT, data_format="NDHWC", padding="SAME", strides=[1, 1, 1, 1, 1], _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_0_0/_1, gradients/Conv3D_grad/Shape_1, gradients/add_grad/tuple/control_dependency)]]

Caused by op 'gradients/Conv3D_grad/Conv3DBackpropFilterV2', defined at:
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\Scripts\ipython-script.py", line 5, in &lt;module&gt;
    sys.exit(IPython.start_ipython())
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\IPython\__init__.py", line 125, in start_ipython
    return launch_new_instance(argv=argv, **kwargs)
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\traitlets\config\application.py", line 658, in launch_instance
    app.start()
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\IPython\terminal\ipapp.py", line 356, in start
    self.shell.mainloop()
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\IPython\terminal\interactiveshell.py", line 480, in mainloop
    self.interact()
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\IPython\terminal\interactiveshell.py", line 471, in interact
    self.run_cell(code, store_history=True)
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\IPython\core\interactiveshell.py", line 2698, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\IPython\core\interactiveshell.py", line 2802, in run_ast_nodes
    if self.run_code(code, result):
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\IPython\core\interactiveshell.py", line 2862, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "&lt;ipython-input-1-436eb9c8573a&gt;", line 23, in &lt;module&gt;
    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\training\optimizer.py", line 343, in minimize
    grad_loss=grad_loss)
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\training\optimizer.py", line 414, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\ops\gradients_impl.py", line 581, in gradients
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\ops\gradients_impl.py", line 353, in _MaybeCompile
    return grad_fn()  # Exit early
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\ops\gradients_impl.py", line 581, in &lt;lambda&gt;
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\ops\nn_grad.py", line 88, in _Conv3DGrad
    data_format=data_format)]
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py", line 964, in conv3d_backprop_filter_v2
    data_format=data_format, name=name)
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\framework\op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\framework\ops.py", line 2936, in create_op
    op_def=op_def)
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\framework\ops.py", line 1464, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

...which was originally created as op 'Conv3D', defined at:
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\Scripts\ipython-script.py", line 5, in &lt;module&gt;
    sys.exit(IPython.start_ipython())
[elided 7 identical lines from previous traceback]
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\IPython\core\interactiveshell.py", line 2862, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "&lt;ipython-input-1-436eb9c8573a&gt;", line 19, in &lt;module&gt;
    conv = tf.nn.conv3d(tf_dataset, layer1_weights, (1, 1, 1, 1, 1), padding='SAME')
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py", line 846, in conv3d
    padding=padding, data_format=data_format, name=name)
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\framework\op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\framework\ops.py", line 2936, in create_op
    op_def=op_def)
  File "C:\Users\HggsHntr\AppData\Local\conda\conda\envs\tftest\lib\site-packages\tensorflow\python\framework\ops.py", line 1464, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InternalError (see above for traceback): cuDNN Backward Filter function launch failure : input shape([1,512,512,512,1]) filter shape([2,2,2,1,1])
         [[Node: gradients/Conv3D_grad/Conv3DBackpropFilterV2 = Conv3DBackpropFilterV2[T=DT_FLOAT, data_format="NDHWC", padding="SAME", strides=[1, 1, 1, 1, 1], _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_0_0/_1, gradients/Conv3D_grad/Shape_1, gradients/add_grad/tuple/control_dependency)]]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='44' author='AlgotecOhadSilbert' date='2017-11-26T08:36:18Z'>
		Hi,
Might be helpful for some of the people that are having the same problem.
A small workaround for this problem that I am using is changing the driver model from WDDM to TCC.
Doing it by
nvidia-smi -dm 1
		</comment>
		<comment id='45' author='AlgotecOhadSilbert' date='2017-11-27T13:55:39Z'>
		@HggsHntr
This solves your problem completely on windows 10?
		</comment>
		<comment id='46' author='AlgotecOhadSilbert' date='2017-11-27T14:03:52Z'>
		I am having problem with evaluation network with slim.conv3d_transpose(which is wrapper of convolution3d_transpose, which is computed by the backprop of conv3d, therefore is related to this?)
What's interesting is that the error message does not occur all the time, almost like a coin flipping...
It happens on both:
win7 + py3.6 + QuadroM4000 8g + pip tensorflow-gpu 1.4
win10 + py3.5 + GTX980m 8g + pip tensorflow-gpu 1.4
apparently, it occurs more frequently on the win7 machine
&lt;denchmark-code&gt;File "C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\contrib\framework\python\ops\arg_scope.py", line 181, in func_with_args
    return func(*args, **current_args)
  File "C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\contrib\layers\python\layers\layers.py", line 1359, in convolution3d_transpose
    outputs = layer.apply(inputs)
  File "C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\layers\base.py", line 671, in apply
    return self.__call__(inputs, *args, **kwargs)
  File "C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\layers\base.py", line 575, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File "C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\layers\convolutional.py", line 1588, in call
    padding=self.padding.upper())
  File "C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\nn_ops.py", line 1428, in conv3d_transpose
    name=name)
  File "C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py", line 1083, in conv3d_backprop_input_v2
    data_format=data_format, name=name)
  File "C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py", line 2956, in create_op
    op_def=op_def)
  File "C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InternalError (see above for traceback): cuDNN Backward Data function launch failure : input shape([1,80,160,160,32]) filter shape([5,5,5,32,32])
	 [[Node: head_block/conv0/conv3d_transpose = Conv3DBackpropInputV2[T=DT_FLOAT, data_format="NDHWC", padding="SAME", strides=[1, 1, 2, 2, 1], _device="/job:localhost/replica:0/task:0/device:GPU:0"](head_block/conv0/stack, head_block/conv0/weights/read/_4949, decode_block_0/Residual_Block/add)]]
	 [[Node: componentwise_overlap_loss/sub/_4957 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_3379_componentwise_overlap_loss/sub", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='47' author='AlgotecOhadSilbert' date='2017-11-28T08:24:11Z'>
		&lt;denchmark-link:https://github.com/qianyizhang&gt;@qianyizhang&lt;/denchmark-link&gt;

I am not sure about windows 10. I only tested it on windows server 2016.
		</comment>
		<comment id='48' author='AlgotecOhadSilbert' date='2018-01-29T20:02:40Z'>
		I encountered the same issue, with 3D convolution, cuda 8.0, cudnn 6.0, tensorflow 1.4.0. I encountered this actually while debugging a more strange phenomenon with 1.5.0: the GPU memory is fine but consumption of memory in CPU keeps growing almost linearly with time until it uses up all the memory (e.g., I set the limit to 40 GB, and it keeps running until 40GB memory is used by the CPU). So I downgraded to 1.4.0, but encountered this new issue as reported by the users above.
		</comment>
	</comments>
</bug>