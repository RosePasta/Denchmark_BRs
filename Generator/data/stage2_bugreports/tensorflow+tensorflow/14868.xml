<bug id='14868' author='qixiang109' open_date='2017-11-24T19:27:25Z' closed_time='2018-01-05T18:02:00Z'>
	<summary>consuming Dataset becomes slower and slower, if make_one_shot_iterator each epoch</summary>
	<description>
&lt;denchmark-h:h3&gt;Problem&lt;/denchmark-h&gt;

I run make_one_shot_iterator() each epoch because  I want re-shuffle the dataset each epoch. I Know that the dataset.shuffle().repeat().batch() pipeline can do almost the same thing, but when data_num can not be divided exactly by batch_size, the pipeline merges two epochs at their boundary to construct a complete batch,  which I HATE.
So, I choose to run dataset.shuffle().batch() and make_one_shot_iterator() before each epoch. But I find that the speed of consuming dataset becomes slower and slower, significantly. I tried different settings to find out that it is make_one_shot_iterator() which makes consuming slow.
By the way, I build tensorflow 1.4 from source on OS X with support of GPU, some hacky workaround.
&lt;denchmark-h:h3&gt;Code&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;num_data = 1000
num_epoch = 50
batch_size = 32
dataset = tf.data.Dataset.range(num_data)

mode=3 
# model = 1,2,or 3
# 1: re-shuffle, re-batch and re-make-iterator each epoch
# 2: re-batch and re-make-iterator
# 3: only re-make-iterator

with tf.Session() as sess:
    for epoch in xrange(num_epoch):
        t1 = time.time()
        if mode==1: 
            _dataset = dataset.shuffle(num_data).batch(batch_size)
            iterator = _dataset.make_one_shot_iterator()
        elif mode==2:
            _dataset = dataset.batch(batch_size)
            iterator = _dataset.make_one_shot_iterator()
        elif mode==3: 
            iterator = dataset.make_one_shot_iterator()
        t2 = time.time()
        for i in xrange(num_data/batch_size):
            a = sess.run(iterator.get_next())
        t3 =time.time()
        print 'epoch %d make_iterator_time %.4f comsuming_time %.4f'%(epoch,t2-t1,t3-t2)
&lt;/denchmark-code&gt;

and the outputs:
&lt;denchmark-code&gt;epoch 0 make_iterator_time 0.0181 comsuming_time 0.1366
epoch 1 make_iterator_time 0.0036 comsuming_time 0.1444
epoch 2 make_iterator_time 0.0040 comsuming_time 0.1559
epoch 3 make_iterator_time 0.0036 comsuming_time 0.1695
epoch 4 make_iterator_time 0.0036 comsuming_time 0.1899
epoch 5 make_iterator_time 0.0036 comsuming_time 0.1955
epoch 6 make_iterator_time 0.0036 comsuming_time 0.2082
epoch 7 make_iterator_time 0.0037 comsuming_time 0.2191
epoch 8 make_iterator_time 0.0036 comsuming_time 0.2334
epoch 9 make_iterator_time 0.0040 comsuming_time 0.2461
epoch 10 make_iterator_time 0.0036 comsuming_time 0.2621
epoch 11 make_iterator_time 0.0036 comsuming_time 0.2720
epoch 12 make_iterator_time 0.0036 comsuming_time 0.2886
epoch 13 make_iterator_time 0.0036 comsuming_time 0.3006
epoch 14 make_iterator_time 0.0037 comsuming_time 0.3134
epoch 15 make_iterator_time 0.0039 comsuming_time 0.3260
epoch 16 make_iterator_time 0.0445 comsuming_time 0.3438
epoch 17 make_iterator_time 0.0037 comsuming_time 0.3576
epoch 18 make_iterator_time 0.0037 comsuming_time 0.3678
epoch 19 make_iterator_time 0.0040 comsuming_time 0.3827
epoch 20 make_iterator_time 0.0037 comsuming_time 0.3937
epoch 21 make_iterator_time 0.0038 comsuming_time 0.4172
epoch 22 make_iterator_time 0.0036 comsuming_time 0.4222
...
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OS X 10.12.5
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.4
Python version:  2.7
Bazel version (if compiling from source): 0.7.0
GCC/Compiler version (if compiling from source): clang-802.0.42
CUDA/cuDNN version: 9.0/7
GPU model and memory: GTX1080 8G
Exact command to reproduce:

You can collect some of this information using our environment capture script:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&lt;/denchmark-link&gt;

You can obtain the TensorFlow version with
python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
	</description>
	<comments>
		<comment id='1' author='qixiang109' date='2017-11-25T08:22:52Z'>
		Predifining get_next_op = iterator.get_next() outside the loop and run sess.run(get_next_op) will reduce the consuming time, but it is still getting slower and slower, as long as I make_one_shot_iterator() each epoch.
		</comment>
		<comment id='2' author='qixiang109' date='2017-11-27T22:17:26Z'>
		&lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
 I believe you were looking into a similar bug internally, could you please look into this.
		</comment>
		<comment id='3' author='qixiang109' date='2017-12-20T01:30:12Z'>
		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
		</comment>
		<comment id='4' author='qixiang109' date='2017-12-20T17:29:10Z'>
		Creating an iterator in a loop (every epoch) will keep adding nodes to the graph, so the slowdown you are seeing is expected. I am not aware of any use case where creating an iterator in a loop would be the right thing to do.
What is the reason you do not want to create a batch using data from different epochs? In any case, you can always do dataset.shuffle().batch().repeat() instead of dataset.shuffle().repeat().batch() in case you would prefer to have the last batch of each epoch to be possibly smaller (in the case the batch size does not divide the size of the epoch, which seems to be your case).
		</comment>
		<comment id='5' author='qixiang109' date='2018-01-04T19:11:55Z'>
		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
		</comment>
		<comment id='6' author='qixiang109' date='2018-11-02T16:44:25Z'>
		I see a similar slow down though I dont add new nodes to the graph,
    dataset = tf.data.TFRecordDataset(data_files, num_parallel_reads=79)

if is_training:
    dataset = dataset.apply(tf.contrib.data.shuffle_and_repeat(1743, -1, 42))

dataset = dataset.apply(tf.contrib.data.map_and_batch(decode, 128,
                                                        num_parallel_batches=58,
                                                        drop_remainder=False)
                        )
dataset = dataset.prefetch(1064)
and fit a tf.keras model like so,
model.fit(
    dataset.make_one_shot_iterator(),
    steps_per_epoch=model_meta.train_records // 128,
    epochs=2,
    validation_data=test_dataset.make_one_shot_iterator(),
    validation_steps=test_records // 128,
    verbose=0
)
I'm using for a 1 GPU 7GB RAM training. What am I missing?
		</comment>
		<comment id='7' author='qixiang109' date='2018-11-02T18:13:01Z'>
		&lt;denchmark-link:https://github.com/Nithanaroy&gt;@Nithanaroy&lt;/denchmark-link&gt;
 can you please create a separate issue with minimal reproducible example and tag me? Thanks.
		</comment>
	</comments>
</bug>