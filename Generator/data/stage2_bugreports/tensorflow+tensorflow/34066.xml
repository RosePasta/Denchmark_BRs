<bug id='34066' author='psitronic' open_date='2019-11-07T08:38:07Z' closed_time='2020-06-23T03:16:56Z'>
	<summary>TF2.0 distributed training fails after adding an embedding layer</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): somewhat
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0
Python version: 3.7.4
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

Describe the current behavior
Try to train with MultiWorkerMirroredStrategy a simple model with and without an Embedding layer. In the former case the distribute training fails (see below). The following code is executed on two workers with TF_CONFIG={"cluster": {"worker": ["localhost:65535","localhost:65532"]}, "task": {"index": 0, "type": "worker"}} and TF_CONFIG={"cluster": {"worker": ["localhost:65535","localhost:65532"]}, "task": {"index": 1, "type": "worker"}}, respectively.
Code to reproduce the issue
&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import losses
import numpy as np

batch_size = 32

strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()

NUM_WORKERS = strategy.num_replicas_in_sync
GLOBAL_BATCH_SIZE = batch_size * NUM_WORKERS
N_CAT = 47

def some_func(*args: tf.Tensor):
    tensor_dict_x, tensor_dict_y = {}, {}

    for index in range(1):
        tensor_dict_x[
            f"input_{index+1}"
        ] = tf.expand_dims(args[index], axis=-1)
        tensor_dict_y[
            f"dense"
        ] = tf.expand_dims(args[index], axis=-1)

    return tensor_dict_x, tensor_dict_y

def read_data():
    train_data = np.random.randint(1,N_CAT, size=9000)
    val_data = np.random.randint(1,N_CAT, size=999)

    dataset_train = (tf.data.Dataset.from_tensor_slices(train_data)
                     .prefetch(-1)
                     .map(map_func=some_func)
                     .batch(batch_size=GLOBAL_BATCH_SIZE)
                     .shuffle(1000)
                     .repeat())
    dataset_val = (tf.data.Dataset.from_tensor_slices(val_data)
                   .prefetch(-1)
                   .map(map_func=some_func)
                   .batch(batch_size=GLOBAL_BATCH_SIZE)
                   .shuffle(1000)
                   .repeat())

    return dataset_train, dataset_val

with strategy.scope():
    optimizer = Adam(lr=0.1)
    loss = losses.sparse_categorical_crossentropy
    model = build_and_compile_model(optimizer, loss)

dataset_train, dataset_val = read_data()
model.fit(x=dataset_train,
          epochs=5,
          steps_per_epoch=9000//batch_size,
          validation_data=dataset_val,
          validation_steps=999//batch_size,
)
&lt;/denchmark-code&gt;

where build_and_compile_model:
&lt;denchmark-code&gt;def build_and_compile_model(optimizer, loss):
    my_input = tf.keras.layers.Input(shape=(1,))
    my_dense = tf.keras.layers.Dense(N_CAT)(my_input)

    model = tf.keras.Model(my_input, my_dense)

    model.compile(optimizer=optimizer,loss=loss)

    return model
&lt;/denchmark-code&gt;

The above case works fine with the output:
&lt;denchmark-code&gt;2019-11-07 09:23:57.387629: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-11-07 09:23:57.410325: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
2019-11-07 09:23:57.411228: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562e2c78c6a0 executing computations on platform Host. Devices:
2019-11-07 09:23:57.411241: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2019-11-07 09:23:57.413306: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:258] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:65535, 1 -&gt; localhost:65532}
2019-11-07 09:23:57.414734: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:65535
WARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an "evaluator" task exists in the cluster.
WARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.
WARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.
2019-11-07 09:24:09.204390: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:400] Cannot find shardable dataset, adding a shard node at the end of the dataset instead. This may have performance implications.
Train for 281 steps, validate for 31 steps
Epoch 1/5
2019-11-07 09:24:09.208710: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:400] Cannot find shardable dataset, adding a shard node at the end of the dataset instead. This may have performance implications.
281/281 [==============================] - 4s 13ms/step - loss: 15.9838 - val_loss: 16.1050
Epoch 2/5
281/281 [==============================] - 1s 3ms/step - loss: 16.0496 - val_loss: 16.1015
Epoch 3/5
281/281 [==============================] - 1s 3ms/step - loss: 16.0210 - val_loss: 16.1007
Epoch 4/5
281/281 [==============================] - 1s 3ms/step - loss: 16.0517 - val_loss: 16.0995
Epoch 5/5
281/281 [==============================] - 1s 3ms/step - loss: 16.0147 - val_loss: 16.0992
2019-11-07 09:24:16.090800: W tensorflow/core/common_runtime/eager/context.cc:290] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.

Process finished with exit code 0
&lt;/denchmark-code&gt;

However, adding just one Embedding layer so that build_and_compile_model now is:
&lt;denchmark-code&gt;def build_and_compile_model(optimizer, loss):

    my_input = tf.keras.layers.Input(shape=(1,))
    emb_layer = tf.keras.layers.Embedding(N_CAT,5)
    emb_inp = emb_layer(my_input)
    my_dense = tf.keras.layers.Dense(N_CAT)(emb_inp)

    model = tf.keras.Model(my_input, my_dense)

    model.compile(optimizer=optimizer,loss=loss)

    return model
&lt;/denchmark-code&gt;

leads to the error:
&lt;denchmark-code&gt;2019-11-07 09:26:45.812362: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-11-07 09:26:45.834388: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
2019-11-07 09:26:45.835018: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ea66430da0 executing computations on platform Host. Devices:
2019-11-07 09:26:45.835032: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2019-11-07 09:26:45.836899: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:258] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:65535, 1 -&gt; localhost:65532}
2019-11-07 09:26:45.838638: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:65535
WARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an "evaluator" task exists in the cluster.
WARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.
WARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.
2019-11-07 09:26:50.351284: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:400] Cannot find shardable dataset, adding a shard node at the end of the dataset instead. This may have performance implications.
Train for 281 steps, validate for 31 steps
Epoch 1/5
2019-11-07 09:26:50.355581: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:400] Cannot find shardable dataset, adding a shard node at the end of the dataset instead. This may have performance implications.
  1/281 [..............................] - ETA: 9:05 - loss: 9.83752019-11-07 09:26:52.321935: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingGather with Internal: [_Derived_]Inconsistent output shapes, got [16], but expected is [64].
	 [[{{node Adam/allreduce_1/CollectiveGather_1}}]]
Additional GRPC error information:
{"created":"@1573115212.321897522","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"[_Derived_]Inconsistent output shapes, got [16], but expected is [64].\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]","grpc_status":13}
2019-11-07 09:26:52.321956: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: [_Derived_]Inconsistent output shapes, got [16], but expected is [64].
	 [[{{node Adam/allreduce_1/CollectiveGather_1}}]]
Additional GRPC error information:
{"created":"@1573115212.321897522","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"[_Derived_]Inconsistent output shapes, got [16], but expected is [64].\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]","grpc_status":13}
2019-11-07 09:26:52.321966: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Internal: [_Derived_]Inconsistent output shapes, got [16], but expected is [64].
	 [[{{node Adam/allreduce_1/CollectiveGather_1}}]]
Additional GRPC error information:
{"created":"@1573115212.321897522","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"[_Derived_]Inconsistent output shapes, got [16], but expected is [64].\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]","grpc_status":13}
2019-11-07 09:26:52.321971: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: [_Derived_]Inconsistent output shapes, got [16], but expected is [64].
	 [[{{node Adam/allreduce_1/CollectiveGather_1}}]]
Additional GRPC error information:
{"created":"@1573115212.321897522","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"[_Derived_]Inconsistent output shapes, got [16], but expected is [64].\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]","grpc_status":13}
2019-11-07 09:26:52.321976: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingGather with Internal: [_Derived_]Inconsistent output shapes, got [16], but expected is [64].
	 [[{{node Adam/allreduce_1/CollectiveGather_1}}]]
Additional GRPC error information:
{"created":"@1573115212.321897522","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"[_Derived_]Inconsistent output shapes, got [16], but expected is [64].\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]","grpc_status":13}
2019-11-07 09:26:52.321996: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: [_Derived_]Inconsistent output shapes, got [16], but expected is [64].
	 [[{{node Adam/allreduce_1/CollectiveGather_1}}]]
Additional GRPC error information:
{"created":"@1573115212.321897522","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"[_Derived_]Inconsistent output shapes, got [16], but expected is [64].\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]","grpc_status":13}
2019-11-07 09:26:52.322010: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Internal: [_Derived_]Inconsistent output shapes, got [16], but expected is [64].
	 [[{{node Adam/allreduce_1/CollectiveGather_1}}]]
Additional GRPC error information:
{"created":"@1573115212.321897522","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"[_Derived_]Inconsistent output shapes, got [16], but expected is [64].\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]","grpc_status":13}
2019-11-07 09:26:52.322017: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: [_Derived_]Inconsistent output shapes, got [16], but expected is [64].
	 [[{{node Adam/allreduce_1/CollectiveGather_1}}]]
Additional GRPC error information:
{"created":"@1573115212.321897522","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"[_Derived_]Inconsistent output shapes, got [16], but expected is [64].\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]","grpc_status":13}
2019-11-07 09:26:52.322045: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Cancelled: [_Derived_]Cancelled
Additional GRPC error information:
{"created":"@1573115212.321980682","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Cancelled","grpc_status":1}
2019-11-07 09:26:52.322051: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Cancelled: [_Derived_]Cancelled
Additional GRPC error information:
{"created":"@1573115212.321980682","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Cancelled","grpc_status":1}
2019-11-07 09:26:52.322053: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:125 : Internal: [_Derived_]Inconsistent output shapes, got [16], but expected is [64].
	 [[{{node Adam/allreduce_1/CollectiveGather_1}}]]
Additional GRPC error information:
{"created":"@1573115212.321897522","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"[_Derived_]Inconsistent output shapes, got [16], but expected is [64].\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]","grpc_status":13}
2019-11-07 09:26:52.322054: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:234 : Internal: [_Derived_]Inconsistent output shapes, got [16], but expected is [64].
	 [[{{node Adam/allreduce_1/CollectiveGather_1}}]]
Additional GRPC error information:
{"created":"@1573115212.321897522","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"[_Derived_]Inconsistent output shapes, got [16], but expected is [64].\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]","grpc_status":13}
2019-11-07 09:26:52.322101: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: [_Derived_]Inconsistent output shapes, got [16], but expected is [64].
	 [[{{node Adam/allreduce_1/CollectiveGather_1}}]]
Additional GRPC error information:
{"created":"@1573115212.321897522","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"[_Derived_]Inconsistent output shapes, got [16], but expected is [64].\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]","grpc_status":13}
	 [[Adam/allreduce_1/CollectiveGather]]
2019-11-07 09:26:52.322097: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:125 : Internal: [_Derived_]Inconsistent output shapes, got [16], but expected is [64].
	 [[{{node Adam/allreduce_1/CollectiveGather_1}}]]
Additional GRPC error information:
{"created":"@1573115212.321897522","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"[_Derived_]Inconsistent output shapes, got [16], but expected is [64].\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]","grpc_status":13}
2019-11-07 09:26:52.322123: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:234 : Cancelled: [_Derived_]Cancelled
Additional GRPC error information:
{"created":"@1573115212.321980682","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Cancelled","grpc_status":1}
2019-11-07 09:26:52.322108: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:234 : Internal: [_Derived_]Inconsistent output shapes, got [16], but expected is [64].
	 [[{{node Adam/allreduce_1/CollectiveGather_1}}]]
Additional GRPC error information:
{"created":"@1573115212.321897522","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"[_Derived_]Inconsistent output shapes, got [16], but expected is [64].\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]","grpc_status":13}
Traceback (most recent call last):
  File ".../.PyCharmCE2019.2/config/scratches/scratch_4.py", line 71, in &lt;module&gt;
    validation_steps=999//batch_size,
  File ".../lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py", line 728, in fit
    use_multiprocessing=use_multiprocessing)
  File .../lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py", line 789, in fit
    *args, **kwargs)
  File ".../lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py", line 776, in wrapper
    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)
  File ".../lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py", line 853, in run_distribute_coordinator
    task_id, session_config, rpc_layer)
  File ".../mostly-engine-tf20-p374/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py", line 360, in _run_single_worker
    return worker_fn(strategy)
  File ".../mostly-engine-tf20-p374/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py", line 771, in _worker_fn
    return method(model, **kwargs)
  File ".../mostly-engine-tf20-p374/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 324, in fit
    total_epochs=epochs)
  File ".../mostly-engine-tf20-p374/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 123, in run_one_epoch
    batch_outs = execution_function(iterator)
  File ".../mostly-engine-tf20-p374/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py", line 86, in execution_function
    distributed_function(input_fn))
  File ".../mostly-engine-tf20-p374/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 457, in __call__
    result = self._call(*args, **kwds)
  File ".../mostly-engine-tf20-p374/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 487, in _call
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
  File ".../mostly-engine-tf20-p374/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1823, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File "/.../mostly-engine-tf20-p374/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1141, in _filtered_call
    self.captured_inputs)
  File ".../lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1224, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager)
  File ".../lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 511, in call
    ctx=ctx)
  File ".../lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File "&lt;string&gt;", line 3, in raise_from
tensorflow.python.framework.errors_impl.InternalError:  [_Derived_]Inconsistent output shapes, got [16], but expected is [64].
	 [[node Adam/allreduce_1/CollectiveGather_1 (defined at /miniconda3/envs/mostly-engine-tf20-p374/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]
Additional GRPC error information:
{"created":"@1573115212.321897522","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"[_Derived_]Inconsistent output shapes, got [16], but expected is [64].\n\t [[node Adam/allreduce_1/CollectiveGather_1 (defined at /miniconda3/envs/mostly-engine-tf20-p374/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]","grpc_status":13}
	 [[Adam/allreduce_1/CollectiveGather]] [Op:__inference_distributed_function_888]

Function call stack:
distributed_function

2019-11-07 09:26:52.540347: W tensorflow/core/common_runtime/eager/context.cc:290] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.

Process finished with exit code 1
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='psitronic' date='2020-06-23T03:16:55Z'>
		Sounds like a similar issue to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/33339&gt;#33339&lt;/denchmark-link&gt;
, which has been fixed. Closing this issue now, but feel free to reopen if this is still a problem.
		</comment>
		<comment id='2' author='psitronic' date='2020-06-23T03:16:57Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34066&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34066&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>