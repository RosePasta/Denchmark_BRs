<bug id='37988' author='jwlawson' open_date='2020-03-27T22:57:17Z' closed_time='2020-05-01T09:02:58Z'>
	<summary>AttributeError with TF2.2.0-rc1 using `keras.model.train_on_batch` inside `tf.function`</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab (both CPU and GPU runtime
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): From default Colab TF2.x version
TensorFlow version (use command below): v2.2.0-rc1-0-gacf4951a2f (2.2.0-rc1)
Python version: 3
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
When trying to train a keras model using train_on_batch inside a tf.function:
&lt;denchmark-code&gt;@tf.function(input_signature=[tf.TensorSpec(shape=(None, 10), dtype=tf.float32),
                              tf.TensorSpec(shape=(None, 10), dtype=tf.int32)
                              ])
def train(inp, extra):
  expected = calc_expected(inp, extra)
  return model_1.train_on_batch((inp, extra), expected)
&lt;/denchmark-code&gt;

TensorFlow raises an AttributeError:
&lt;denchmark-code&gt;AttributeError: in user code:

    &lt;ipython-input-6-49c8760b225c&gt;:6 train  *
        return model_1.train_on_batch((inp, extra), expected)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1287 train_on_batch  **
        logs = tf_utils.to_numpy_or_python_type(logs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py:523 to_numpy_or_python_type
        return nest.map_structure(_to_single_numpy_or_python_type, tensors)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:617 map_structure
        structure[0], [func(*x) for x in entries],
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:617 &lt;listcomp&gt;
        structure[0], [func(*x) for x in entries],
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py:519 _to_single_numpy_or_python_type
        x = t.numpy()

    AttributeError: 'Tensor' object has no attribute 'numpy'
&lt;/denchmark-code&gt;

Describe the expected behavior
The code works fine in eager execution, and should not raise an error when run in a tf.function.
This also works fine with TensorFlow 2.1, so it looks to be a recent regression.

&lt;denchmark-link:https://colab.research.google.com/drive/1uy1awEQE4_t_0uZ4MzGXvfeeRnG_EJVQ&gt;https://colab.research.google.com/drive/1uy1awEQE4_t_0uZ4MzGXvfeeRnG_EJVQ&lt;/denchmark-link&gt;

Other info / logs

Full backtrace copied from Colab

---------------------------------------------------------------------------

AttributeError                            Traceback (most recent call last)

&lt;ipython-input-28-c04cb8faf387&gt; in &lt;module&gt;()
      2 b = tf.random.uniform(shape=(2, 10), maxval=10, dtype=tf.int32)
      3 
----&gt; 4 train(a, b)

8 frames

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    578         xla_context.Exit()
    579     else:
--&gt; 580       result = self._call(*args, **kwds)
    581 
    582     if tracing_count == self._get_tracing_count():

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    625       # This is the first call of __call__, so we have to initialize.
    626       initializers = []
--&gt; 627       self._initialize(args, kwds, add_initializers_to=initializers)
    628     finally:
    629       # At this point we know that the initialization is complete (or less

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    504     self._concrete_stateful_fn = (
    505         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--&gt; 506             *args, **kwds))
    507 
    508     def invalid_creator_scope(*unused_args, **unused_kwds):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2444       args, kwargs = None, None
   2445     with self._lock:
-&gt; 2446       graph_function, _, _ = self._maybe_define_function(args, kwargs)
   2447     return graph_function
   2448 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   2775 
   2776       self._function_cache.missed.add(call_context_key)
-&gt; 2777       graph_function = self._create_graph_function(args, kwargs)
   2778       self._function_cache.primary[cache_key] = graph_function
   2779       return graph_function, args, kwargs

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   2665             arg_names=arg_names,
   2666             override_flat_arg_shapes=override_flat_arg_shapes,
-&gt; 2667             capture_by_value=self._capture_by_value),
   2668         self._function_attributes,
   2669         # Tell the ConcreteFunction to clean up its graph once it goes out of

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    979         _, original_func = tf_decorator.unwrap(python_func)
    980 
--&gt; 981       func_outputs = python_func(*func_args, **func_kwargs)
    982 
    983       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    439         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    440         # the function a weak reference to itself to avoid a reference cycle.
--&gt; 441         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    442     weak_wrapped_fn = weakref.ref(wrapped_fn)
    443 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    966           except Exception as e:  # pylint:disable=broad-except
    967             if hasattr(e, "ag_error_metadata"):
--&gt; 968               raise e.ag_error_metadata.to_exception(e)
    969             else:
    970               raise

AttributeError: in user code:

    &lt;ipython-input-6-49c8760b225c&gt;:6 train  *
        return model_1.train_on_batch((inp, extra), expected)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1287 train_on_batch  **
        logs = tf_utils.to_numpy_or_python_type(logs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py:523 to_numpy_or_python_type
        return nest.map_structure(_to_single_numpy_or_python_type, tensors)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:617 map_structure
        structure[0], [func(*x) for x in entries],
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:617 &lt;listcomp&gt;
        structure[0], [func(*x) for x in entries],
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py:519 _to_single_numpy_or_python_type
        x = t.numpy()

    AttributeError: 'Tensor' object has no attribute 'numpy'



	</description>
	<comments>
		<comment id='1' author='jwlawson' date='2020-03-28T05:44:00Z'>
		&lt;denchmark-link:https://github.com/jwlawson&gt;@jwlawson&lt;/denchmark-link&gt;
 ,  I think  is not executed eagerly for performance purposes. So you need to remove the  decorator  for  to work. For your reference link of gist is &lt;denchmark-link:https://gist.github.com/khimraj/465004a32f5a64dbf4a78a2e441c4ac2&gt;here&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='2' author='jwlawson' date='2020-03-30T08:21:13Z'>
		I guess my ticket wasn't clear. I know that this works when you remove the tf.function annotation, however I would expect it to work fine inside a tf.function as well.
In TF 2.1 you could use Model.train_on_batch inside a tf.function. Are you saying that this crash is now expected behaviour?
If the crash is now expected behaviour then it is a breaking change and should be documented (and should probably give a better error message than referencing a function deep inside keras).
		</comment>
		<comment id='3' author='jwlawson' date='2020-03-30T21:14:10Z'>
		&lt;denchmark-link:https://github.com/jwlawson&gt;@jwlawson&lt;/denchmark-link&gt;
 Thanks for the issue!
Model.train_on_batch runs a single step of the Model.fit logic, it's a high-level endpoint that's not safe to run in a tf.function since it creates an Iterator, converts results to NumPy, etc. Probably the best thing to use is Model.train_step, which is just the logic of the training step and is safe to tf.function
		</comment>
		<comment id='4' author='jwlawson' date='2020-04-17T04:12:12Z'>
		&lt;denchmark-link:https://github.com/jwlawson&gt;@jwlawson&lt;/denchmark-link&gt;
 Can you please check &lt;denchmark-link:https://github.com/omalleyt12&gt;@omalleyt12&lt;/denchmark-link&gt;
 comments and let us know whether it was resolved for you or not. Please close the issue if this was resolved for you. Thanks!
		</comment>
		<comment id='6' author='jwlawson' date='2020-05-01T09:02:58Z'>
		I'll close this, but I do think that there is a missing level of abstraction here that train_on_batch looked like it filled until these changes.

model.fit is very high level and handles everything for the user - including splitting up a dataset and sharding across distributed strategies etc
model.train_step is very low level and doesn't handle anything to do with strategies or datasets.

What I wanted from model.train_on_batch was a middle ground that still handled the strategy sharding, but didn't require a dataset. It looked like this was the case prior to TF 2.2. If the only answer if to use the very low level function then as a user I need to care much more about the low level aspects that I would like keras to handle for me.
		</comment>
		<comment id='7' author='jwlawson' date='2020-05-01T09:03:02Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37988&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37988&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>