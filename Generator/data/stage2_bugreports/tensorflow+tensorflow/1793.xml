<bug id='1793' author='Thrasi' open_date='2016-04-06T17:36:59Z' closed_time='2017-04-10T23:10:17Z'>
	<summary>Missing gradient for tf.nn.max_pool_with_argmax</summary>
	<description>
I swapped out the first max_pool operation in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/models/image/mnist/convolutional.py&gt;convolutional.py&lt;/denchmark-link&gt;
 tutorial for a max_pool_with_argmax operation and there doesn't seem to exist a gradient for that op:
(virtenv)$ python convolutional.py
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
Traceback (most recent call last):
File "convolutional.py", line 316, in &lt;module&gt;
tf.app.run()
File "/home/name/virtenv/local/lib/python2.7/site-packages/tensorflow/python/platform/default/_app.py", line 30, in run
sys.exit(main(sys.argv))
File "convolutional.py", line 244, in main
global_step=batch)
File "/home/name/virtenv/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py", line 190, in minimize
colocate_gradients_with_ops=colocate_gradients_with_ops)
File "/home/name/virtenv/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py", line 241, in compute_gradients
colocate_gradients_with_ops=colocate_gradients_with_ops)
File "/home/name/virtenv/local/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py", line 453, in gradients
(op.name, op.type))
LookupError: No gradient defined for operation 'MaxPoolWithArgmax' (op type: MaxPoolWithArgmax)
&lt;denchmark-h:h3&gt;Environment info&lt;/denchmark-h&gt;

Operating System: Ubuntu 14.04.1 LTS
If installed from binary pip package, provide:
(virtenv)$ pip install --upgrade &lt;denchmark-link:https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl&gt;https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl&lt;/denchmark-link&gt;

(virtenv)$ python -c "import tensorflow; print(tensorflow.)"
0.7.1
&lt;denchmark-h:h3&gt;Steps to reproduce&lt;/denchmark-h&gt;



Change line 192-5 of convolutional.py from:
pool = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
to:
pool, pos = tf.nn.max_pool_with_argmax(relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')


Run:
(virtenv)$ python convolutional.py


&lt;denchmark-h:h3&gt;What have you tried?&lt;/denchmark-h&gt;




&lt;denchmark-h:h3&gt;Logs or other output that would be helpful&lt;/denchmark-h&gt;

(If logs are large, please upload as attachment).
	</description>
	<comments>
		<comment id='1' author='Thrasi' date='2016-04-20T02:23:18Z'>
		I had the same issue when I was trying to get the max indices.
		</comment>
		<comment id='2' author='Thrasi' date='2016-05-25T01:14:30Z'>
		this is a bit of a hack, but copying the gradient for maxpool and registering it for max_pool_with_argmax works. The function signature has an additional arg (presumably this has to do with the argmax). Also the op doesn't have a data_format, so that is left default (NHWC).
from tensorflow.python.framework import ops
from tensorflow.python.ops import gen_nn_ops
@ops.RegisterGradient("MaxPoolWithArgmax")
def _MaxPoolWithArgmaxGrad(op, grad, some_other_arg):
  return gen_nn_ops._max_pool_grad(op.inputs[0],
                                   op.outputs[0],
                                   grad,
                                   op.get_attr("ksize"),
                                   op.get_attr("strides"),
                                   padding=op.get_attr("padding"),
                                   data_format='NHWC')
		</comment>
		<comment id='3' author='Thrasi' date='2016-06-07T00:44:45Z'>
		&lt;denchmark-link:https://github.com/mwalton&gt;@mwalton&lt;/denchmark-link&gt;
:  should be called , but otherwise that's basically it.  Would you be interesting it submitting your change as a PR?  We'd have to add tests if so (mirroring those for the gradient of the normal max_pool.
		</comment>
		<comment id='4' author='Thrasi' date='2016-06-07T15:40:52Z'>
		&lt;denchmark-link:https://github.com/girving&gt;@girving&lt;/denchmark-link&gt;
  Sure! Should I go ahead and put in the PR; or implement the tests first?
		</comment>
		<comment id='5' author='Thrasi' date='2016-06-07T15:43:54Z'>
		&lt;denchmark-link:https://github.com/mwalton&gt;@mwalton&lt;/denchmark-link&gt;
: The tests should go in the same PR.
		</comment>
		<comment id='6' author='Thrasi' date='2016-07-20T20:24:03Z'>
		&lt;denchmark-link:https://github.com/mwalton&gt;@mwalton&lt;/denchmark-link&gt;
 Not sure if you've made any progress on this, but I believe the correct change would be the following:
@ops.RegisterGradient("MaxPoolWithArgmax")
def _MaxPoolGradWithArgmax(op, grad, unused_argmax_grad):
  return gen_nn_ops._max_pool_grad_with_argmax(op.inputs[0],
                                               grad,
                                               op.outputs[1],
                                               op.get_attr("ksize"),
                                               op.get_attr("strides"),
                                               padding=op.get_attr("padding"))
Seems to be working on a model I'm training. Haven't had time to write tests for it yet, nor do I know how/what tests are needed for registering an existing op (that appears to have  test coverage, see &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/pooling_ops_test.py#L521&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/pooling_ops_test.py#L521&lt;/denchmark-link&gt;
).
&lt;denchmark-link:https://github.com/girving&gt;@girving&lt;/denchmark-link&gt;
 If you can provide some direction on what tests I'd need to add for this change, I can try to pull together a PR.
		</comment>
		<comment id='7' author='Thrasi' date='2016-07-20T20:28:27Z'>
		&lt;denchmark-link:https://github.com/bcaine&gt;@bcaine&lt;/denchmark-link&gt;
 Awesome!  Tests for gradients are pretty easy: just use  and verify that the error is small (1e-4 or 1e-3, typically, due to numerical differences).  You can look at other  examples and mimic.
		</comment>
		<comment id='8' author='Thrasi' date='2017-04-03T17:02:00Z'>
		Are there any updates on this issue? It seems that adding this gradient was attempted in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/4014&gt;#4014&lt;/denchmark-link&gt;
, but not taken place due to commit conflicts. Should I cherry-pick commit &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/dc3d4782cbdf2d98bff64f83a147c9ccc0f4e280&gt;dc3d478&lt;/denchmark-link&gt;
 into a PR?
		</comment>
		<comment id='9' author='Thrasi' date='2017-04-03T17:06:56Z'>
		&lt;denchmark-link:https://github.com/Enet4&gt;@Enet4&lt;/denchmark-link&gt;
 Definitely looks stagnated, so feel free to take over.
		</comment>
		<comment id='10' author='Thrasi' date='2017-05-19T12:14:00Z'>
		Hi. I trained a network with &lt;denchmark-link:https://github.com/bcaine&gt;@bcaine&lt;/denchmark-link&gt;
 register ops solution in python. Now I exported my graph using freeze_graph.py to use my network in C++. But there, this gradient is not registered.
Can anyone help how this can be for C++/ source?
		</comment>
		<comment id='11' author='Thrasi' date='2017-05-25T02:58:14Z'>
		Hey, is anyone doing anything about the lack of gradient?
		</comment>
	</comments>
</bug>