<bug id='25073' author='mshiffer' open_date='2019-01-21T15:18:10Z' closed_time='2020-09-19T13:17:23Z'>
	<summary>fit gets slower on consecutive calls when model is recompiled</summary>
	<description>
System information


OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10 Home


TensorFlow installed from (source or binary):
using pip3


TensorFlow version (use command below):
reproduced on both
b'v1.12.0-rc2-3-ga6d8ffae09' 1.12.0
b'v1.11.0-rc2-4-gc19e29306c' 1.11.0


Python version:
Python 3.6.7 :: Anaconda, Inc.



Over multiple iterations with a re-compile of model, fit() time continuously increases.  If the re-compile is commented out, fit() time remains constant.  I found a similar bug in the js project but not sure if it's related: &lt;denchmark-link:https://github.com/tensorflow/tfjs/issues/448&gt;tensorflow/tfjs#448&lt;/denchmark-link&gt;

Describe the expected behavior
Expect fit() time to remain constant even with multiple re-compiles
Code to reproduce the issue
`
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Flatten
import numpy as np
input1 = np.array([[0, 1, 1, 1, 0, 0, 0, 1, 1, 1]])
dOutput1 = np.array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
model = Sequential()
initializer = tf.keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=1)
model.add(Dense(25, input_dim=10, kernel_initializer=initializer, bias_initializer='zeros'))
model.add(Activation('relu'))
initializer = tf.keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=2)
model.add(Dense(25, kernel_initializer=initializer, bias_initializer='zeros'))
model.add(Activation('relu'))
initializer = tf.keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=3)
model.add(Dense(10, kernel_initializer=initializer, bias_initializer='zeros'))
model.add(Activation('softmax'))
#loss='categorical_crossentropy'
sgd = tf.keras.optimizers.SGD(lr=0.1, momentum=0.0, decay=0.0, nesterov=False)
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
for i in range(100):
sgd = tf.keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
model.fit(input1, dOutput1, epochs=1, batch_size=1)
output1 = model.predict(input1)
output1
`
Output
1/1 [==============================] - 0s 155ms/step - loss: 2.2998 - acc: 0.0000e+00
Epoch 1/1
1/1 [==============================] - 0s 131ms/step - loss: 2.2906 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 183ms/step - loss: 2.2815 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 211ms/step - loss: 2.2724 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 235ms/step - loss: 2.2633 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 221ms/step - loss: 2.2543 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 367ms/step - loss: 2.2452 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 324ms/step - loss: 2.2362 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 269ms/step - loss: 2.2271 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 307ms/step - loss: 2.2181 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 316ms/step - loss: 2.2091 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 315ms/step - loss: 2.2002 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 346ms/step - loss: 2.1912 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 404ms/step - loss: 2.1822 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 380ms/step - loss: 2.1732 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 417ms/step - loss: 2.1643 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 421ms/step - loss: 2.1553 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 442ms/step - loss: 2.1464 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 465ms/step - loss: 2.1375 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 486ms/step - loss: 2.1286 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 0s 468ms/step - loss: 2.1197 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 509ms/step - loss: 2.1108 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 544ms/step - loss: 2.1020 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 676ms/step - loss: 2.0932 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 543ms/step - loss: 2.0844 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 584ms/step - loss: 2.0756 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 608ms/step - loss: 2.0668 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 643ms/step - loss: 2.0580 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 651ms/step - loss: 2.0493 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 660ms/step - loss: 2.0406 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 664ms/step - loss: 2.0319 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 702ms/step - loss: 2.0232 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 735ms/step - loss: 2.0145 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 709ms/step - loss: 2.0059 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 773ms/step - loss: 1.9973 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 901ms/step - loss: 1.9887 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 902ms/step - loss: 1.9801 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 948ms/step - loss: 1.9715 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 865ms/step - loss: 1.9629 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 839ms/step - loss: 1.9544 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 987ms/step - loss: 1.9459 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 1s/step - loss: 1.9374 - acc: 1.0000
Epoch 1/1
1/1 [==============================] - 1s 1s/step - loss: 1.9289 - acc: 1.0000
Epoch 1/1
	</description>
	<comments>
		<comment id='1' author='mshiffer' date='2019-01-23T17:02:22Z'>
		I've been doing some playing around trying (unsuccessfully) to rig a temporary solution.  I thought it might have to do something with garbage collection and experimented with the following:

force collection with gc.collect() - doesn't work
save network to disk, delete variable referencing the NN using %xdel or %reset_selective, call gc.collect(), reload saved network from disk - doesn't work
save network to disk, full reset of kernel with %reset, reload saved network from disk - doesn't work

The only thing that I've found so far to bring fit() time back to initial run-time levels is restarting the kernel.
		</comment>
		<comment id='2' author='mshiffer' date='2019-01-23T19:03:08Z'>
		So after more searching I learned about tf.Session().  I'm able to save the model to disk, call tf.keras.backend.clear_session(), reload the model and continue without the creeping calculation time.  I have not been able to figure out how to clear the session without completely destroying the model (and hence destroying the training to that point) without saving to disk.  Any suggestions?
Pointers to a good tutorial/write up/explanation of Sessions and what exactly is going on in the interaction between the C++ and python code would also be greatly appreciated (and might help me figure out an answer to my above question).  I've found the rfcs but no good design docs that clue me in on what's going on.
Based on what little more I now know, this is a leak in the underlying c++ code?  I'll let someone who knows more than me decide if this bug should be closed or addressed in code.
Also curious if this would be addressed by the proposal to move from sessions to functions in the rfcs.
		</comment>
		<comment id='3' author='mshiffer' date='2019-01-26T13:46:07Z'>
		Using tensorflow 1.12.0
Thank you for the hint with tf.keras.backend.clear_session()
That saved me!
Before my model was getting slower and slower and the RAM utilization grew by about 1.5 MB/s.
Doing a objgraph.show_growth(limit=3)
I get the following;
&lt;denchmark-code&gt;tuple 10304508 + 104274
list 591883 +8521
dict 44840 + 5958
&lt;/denchmark-code&gt;

Destroying the model every time is fine for me since I do a hyperparameter search where I need to rebuild the model anyway but I guess it may be a bit of a pain in other situations.
		</comment>
		<comment id='4' author='mshiffer' date='2019-01-27T14:20:19Z'>
		&lt;denchmark-link:https://github.com/markste-in&gt;@markste-in&lt;/denchmark-link&gt;
 glad that helped!  On the note of hyper-parameter search, I'm actually updating my models learning rate fairly often and have to re-compile every time.  I'm wondering if you (or anyone) knows why that's part of the model compilation as opposed to just a constant that can be passed in for the call to fit()...
		</comment>
		<comment id='5' author='mshiffer' date='2019-01-27T21:08:29Z'>
		I don't know why it's part of the compile process (probably historical reasons in Keras?). I just know that you need to recompile if you change either the loss function, the optimizer/lr or the metrics according to &lt;denchmark-link:https://stackoverflow.com/questions/47995324/does-model-compile-initialize-all-the-weights-and-biases-in-keras-tensorflow/47996024&gt;StackOverflow&lt;/denchmark-link&gt;

I guess you can circumvent this if you use this little &lt;denchmark-link:https://stackoverflow.com/questions/41533489/how-to-initialise-only-optimizer-variables-in-tensorflow&gt;trick&lt;/denchmark-link&gt;
 with plain tensorflow:
&lt;denchmark-code&gt;optimizer = tf.train.SomeOptimizer(learning_rate)
session.run(tf.variables_initializer(optimizer.variables()))
&lt;/denchmark-code&gt;

Then you could just do
&lt;denchmark-code&gt;train = optimizer.minimize(SomeCost)
sess.run(train, feed_dict....)
&lt;/denchmark-code&gt;

But I never tried that since I create a new model every time anyway.
		</comment>
		<comment id='6' author='mshiffer' date='2019-02-04T13:30:31Z'>
		&lt;denchmark-link:https://github.com/mshiffer&gt;@mshiffer&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/markste-in&gt;@markste-in&lt;/denchmark-link&gt;

Memory leak is also observed when model is re-compiled:
Codes to demostrate memory leak:
&lt;denchmark-code&gt;import os
import psutil
from tensorflow import keras
model = keras.Sequential()
model.add(keras.layers.Dense(1, input_shape=(1,)))
process = psutil.Process(os.getpid())
for count in range(1000):
    model.compile(optimizer='sgd', loss='mse')
    if count % 100 == 0:
        print('#{} : mem = {} Byte'.format(count, process.memory_info().rss))
&lt;/denchmark-code&gt;

Output
&lt;denchmark-code&gt;#0 : mem = 284536832 Byte
#100 : mem = 419696640 Byte
#200 : mem = 554618880 Byte
#300 : mem = 690429952 Byte
#400 : mem = 826294272 Byte
#500 : mem = 962461696 Byte
#600 : mem = 1097363456 Byte
#700 : mem = 1234108416 Byte
#800 : mem = 1369239552 Byte
#900 : mem = 1504563200 Byte
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='mshiffer' date='2019-02-04T18:57:12Z'>
		
@mshiffer @fchollet @markste-in
Memory leak is also observed when model is re-compiled:

Can confirm:
Used a slightly modified version:
&lt;denchmark-code&gt;import os
import psutil
import objgraph
from tensorflow import keras
import tensorflow as tf
print('Tensorflow: ',tf.__version__)
model = keras.Sequential()
model.add(keras.layers.Dense(1, input_shape=(1,)))
process = psutil.Process(os.getpid())
for count in range(1000):
    model.compile(optimizer='sgd', loss='mse')
    if count % 100 == 0:
        print('#{} : mem = {} Byte'.format(count, process.memory_info().rss))
        objgraph.show_growth(limit=3)
&lt;/denchmark-code&gt;

and get the following output:
&lt;denchmark-code&gt;Tensorflow:  1.12.0
#0 : mem = 180330496 Byte
function    38783    +38783
tuple       22278    +22278
dict        18244    +18244
#100 : mem = 316846080 Byte
tuple   545785   +523507
list     68151    +57902
dict     57345    +39101
#200 : mem = 454078464 Byte
tuple  1069285   +523500
list    126051    +57900
dict     96445    +39100
#300 : mem = 594722816 Byte
tuple  1592782   +523497
list    183950    +57899
dict    135545    +39100
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='mshiffer' date='2019-04-20T13:49:02Z'>
		&lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;
 Is there any timeline for this, or if not any way I can help to get it fixed?  Even my workaround makes the computation obnoxious because I have to serialize, write to disk, read from disk and de-serialize for each iteration...  I'd be happy to help out if doing so would help get a solution out.
		</comment>
		<comment id='9' author='mshiffer' date='2019-05-16T17:14:47Z'>
		&lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;
 Same question.  :)  I don't know the process for fixes/updates, but if I can help out in anyway I'd love to.
		</comment>
		<comment id='10' author='mshiffer' date='2019-06-30T11:03:10Z'>
		Similar issue with tensorflow.keras.model.fit memory leak (tf_nightly 1.15.0-dev20190628):
&lt;denchmark-link:https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/overfit_and_underfit.ipynb&gt;https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/overfit_and_underfit.ipynb&lt;/denchmark-link&gt;

Memory leak crash with following code:
baseline_history = baseline_model.fit(train_data,
train_labels,
epochs=20,
batch_size=512,
validation_data=(test_data, test_labels),
verbose=2)
tcmalloc: large alloc 2000003072 bytes == 0x1f80fc000 @ 0x7f3ea0f47887 0x7f3e9f83dbf9 0x7f3e9f83eacb 0x7f3e9f83eb84 0x7f3e9f83ef6c 0x7f3e71909d79 0x7f3e719991be 0x7f3e719316f7 0x7f3e7193228c 0x7f3e718f0ffa 0x7f3e718f117a 0x7f3e74711dce 0x7f3e744e46e2 0x4f858d 0x4f98c7 0x4f7a28 0x4f876d 0x4f98c7 0x4f6128 0x4f426e 0x5a1481 0x512a60 0x53ee21 0x57ec0c 0x4f88ba 0x4fa6c0 0x4f6128 0x4f7d60 0x4f876d 0x4f98c7 0x4f6128
replace
from tensorflow import keras
with below code to use keras directly rather than tensorflow.keras, and downgrade numpy to 1.16.2:
import keras
import keras.backend
!pip install numpy==1.16.2
import numpy as np
Then the memory leak and crash is not happening.
Have also tested the notebook locally on windows 10, the crash could be prevented with pagefile increased to 200G, the pagefile could grow up to 160G with this notebook.
		</comment>
		<comment id='11' author='mshiffer' date='2020-09-13T15:21:48Z'>
		&lt;denchmark-link:https://github.com/mshiffer&gt;@mshiffer&lt;/denchmark-link&gt;
 Recently there were some updates related to performance.
I think this was resolved in recent . &lt;denchmark-link:https://colab.research.google.com/gist/jvishnuvardhan/4ea1bda75a261a759263b896aab3948b/untitled37.ipynb&gt;Here&lt;/denchmark-link&gt;
 is the gist for reference.
Can you please verify once and close the issue if this was resolved for you. Thanks!
		</comment>
		<comment id='12' author='mshiffer' date='2020-09-19T13:17:23Z'>
		I am closing this issue as this was resolved in recent tf-nightly. Please feel free to reopen if this was not resolved for you. Thanks!
		</comment>
		<comment id='13' author='mshiffer' date='2020-09-19T13:17:25Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25073&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25073&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>