<bug id='28613' author='jasonzhang2022' open_date='2019-05-10T21:33:20Z' closed_time='2019-05-29T17:14:31Z'>
	<summary>Issue with transformer guide</summary>
	<description>
&lt;denchmark-h:h2&gt;URL(s) with the issue:&lt;/denchmark-h&gt;

&lt;denchmark-link:https://www.tensorflow.org/alpha/tutorials/text/transformer&gt;https://www.tensorflow.org/alpha/tutorials/text/transformer&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;Description of issue (what needs changing):&lt;/denchmark-h&gt;

format and normalization layer.
&lt;denchmark-h:h3&gt;Clear description&lt;/denchmark-h&gt;


when describe the multi-head attention, there is this text in one line:  "Multi-head attention consists of four parts: * Linear layers and split into heads. * Scaled dot-product attention. * Concatenation of heads. * Final linear layer. "
I guess the * mean bullet items. It is not formatted correctly.
The EncoderLayer and DecoderLayer use LayerNormalization. There is no LayerNormalization in keras.layers. There is only BatchNormalization.
Evaluate step creates mask. No mask is needed since a) there is no padding for a single sentence. b) there is no look_ahead since we are trying to predict next words.

	</description>
	<comments>
		<comment id='1' author='jasonzhang2022' date='2019-05-14T21:37:32Z'>
		&lt;denchmark-link:https://github.com/jasonzhang2022&gt;@jasonzhang2022&lt;/denchmark-link&gt;
 Thanks for finding this. First one  was already updated in  branch &lt;denchmark-link:https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/text/transformer.ipynb&gt;here&lt;/denchmark-link&gt;
 and it will be correctly rendered when the update are pushed to website. We will take a look at the rest of the two issues. Thanks!
		</comment>
		<comment id='2' author='jasonzhang2022' date='2019-05-29T13:21:21Z'>
		It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue?
		</comment>
		<comment id='3' author='jasonzhang2022' date='2019-05-29T17:14:30Z'>
		&lt;denchmark-link:https://github.com/jasonzhang2022&gt;@jasonzhang2022&lt;/denchmark-link&gt;
 Regarding item(2), There is  &lt;denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/LayerNormalization&gt;here&lt;/denchmark-link&gt;
. There is a difference between  and .
Regarding item(3), there is a look_ahead and the functionality was described in the tutorial
&lt;denchmark-code&gt;The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.

This means that to predict the third word, only the first and second word will be used. Similarly to predict the fourth word, only the first, second and the third word will be used and so on.

&lt;/denchmark-code&gt;

I think this is not an issue. I am closing this, but please let me know if I'm mistaken. Thanks!
		</comment>
	</comments>
</bug>