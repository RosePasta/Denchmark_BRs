<bug id='11724' author='dburkhardt' open_date='2017-07-24T19:38:44Z' closed_time='2018-05-14T22:10:08Z'>
	<summary>Importing TensorFlow breaks numpy.linalg.matrix_power()</summary>
	<description>
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/10771&gt;#10771&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
TensorFlow installed from (source or binary): Source
TensorFlow version (use command below): 1.2.1
Python version: 3.6.1
Bazel version (if compiling from source): bazel release 0.5.1
CUDA/cuDNN version: CUDA V8.0.61 /
GPU model and memory: NVIDIA Titan X Fall 2016
Exact command to reproduce:

import numpy as np
import tensorflow as tf
np.random.seed(seed=0)
X = np.random.randn(2000, 2000)
Y = np.linalg.matrix_power(X, 30)
&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

This seems to be a bug where importing tensorflow breaks numpy.linalg.matrix_power()
The two following code snippets are producing different results on my system when entered directly into the python interpreter. Note, I am restarting the python kernel between running each block.
This works:
import numpy as np
np.random.seed(seed=0)
X = np.random.randn(2000, 2000)
Y = np.linalg.matrix_power(X, 30)
&gt;&gt;&gt; X 
array([[ 1.76405235,  0.40015721,  0.97873798, ...,  0.15843385,
        -1.14190142, -1.31097037],
       [-1.53292105, -1.71197016,  0.04613506, ..., -0.03057244,
         1.57708821, -0.8128021 ],
       [ 0.61334917,  1.84369998,  0.27109098, ..., -0.53788475,
         0.39344443,  0.28651827],
       ..., 
       [-0.17117027,  0.57332063, -0.89516715, ..., -0.01409412,
         1.28756456, -0.6953778 ],
       [-1.53627571,  0.57441228, -0.20564476, ...,  0.90499929,
         0.51428298,  0.72148202],
       [ 0.51262101, -0.90758583,  1.78121159, ..., -1.12554283,
         0.95170926, -1.15237806]])
&gt;&gt;&gt; Y
array([[ -1.04752205e+48,   2.10841282e+47,  -4.54826843e+47, ...,
         -7.84526353e+46,  -4.45185369e+47,  -1.96340973e+47],
       [ -5.40802471e+46,   1.12546832e+48,  -1.88764494e+47, ...,
         -3.72182046e+47,   7.97461852e+47,  -5.04546546e+46],
       [ -3.59835691e+47,  -6.90559050e+46,  -8.78538707e+47, ...,
          7.67940928e+47,   2.10052546e+47,   1.75193723e+47],
       ...,
       [  2.20970288e+46,   3.60679821e+47,   5.76631889e+47, ...,
          1.21938369e+48,  -8.61048462e+47,  -3.93610572e+47],
       [ -1.19116636e+48,   2.47318954e+48,   2.65693291e+46, ...,
          9.18513286e+47,   3.91490216e+47,  -7.08113716e+47],
       [ -2.25527724e+47,  -4.94088618e+46,  -2.69359430e+47, ...,
         -4.07174632e+47,   7.38250907e+47,   5.86758288e+46]])
This produces the wrong result:
import numpy as np
import tensorflow as tf
np.random.seed(seed=0)
X = np.random.randn(2000, 2000)
Y = np.linalg.matrix_power(X, 30)
&gt;&gt;&gt; X
array([[ 1.76405235,  0.40015721,  0.97873798, ...,  0.15843385,
        -1.14190142, -1.31097037],
       [-1.53292105, -1.71197016,  0.04613506, ..., -0.03057244,
         1.57708821, -0.8128021 ],
       [ 0.61334917,  1.84369998,  0.27109098, ..., -0.53788475,
         0.39344443,  0.28651827],
       ..., 
       [-0.17117027,  0.57332063, -0.89516715, ..., -0.01409412,
         1.28756456, -0.6953778 ],
       [-1.53627571,  0.57441228, -0.20564476, ...,  0.90499929,
         0.51428298,  0.72148202],
       [ 0.51262101, -0.90758583,  1.78121159, ..., -1.12554283,
         0.95170926, -1.15237806]])
&gt;&gt;&gt; Y
array([[ -3.40382764e+91,   2.85027458e+91,   1.14039870e+91, ...,
          3.32682992e+91,   6.00166234e+91,   2.33233825e+91],
       [  3.86088264e+91,   1.15500453e+92,   2.83821815e+91, ...,
         -6.16959058e+91,  -1.91501705e+91,  -4.67672849e+91],
       [  5.05026067e+91,   7.72796711e+91,  -4.70112473e+91, ...,
          8.41553063e+90,  -2.66176140e+91,   8.50899233e+90],
       ...,
       [ -2.08592878e+91,   2.28435173e+91,   9.15188619e+90, ...,
         -1.25550051e+91,   1.85247259e+91,  -8.73231986e+90],
       [ -4.73923534e+91,   1.61385540e+92,   1.26364668e+92, ...,
          2.83667716e+91,   5.06236372e+90,  -5.18395025e+91],
       [ -1.52984791e+91,  -5.57421948e+90,   3.27657918e+91, ...,
         -7.08972359e+91,  -1.58912068e+91,  -1.22216698e+91]])
The values for X are exactly the same. Interestingly, if you run the first code block, then import tensorflow without restarting the kernel and recompute the matrix power, the correct result is returned.
I haven't been able to find a reference to this anywhere online, and it's a big problem not being able to use tensorflow and numpy in the same script.
	</description>
	<comments>
		<comment id='1' author='dburkhardt' date='2017-07-25T01:25:20Z'>
		Hmm, I am not able to reproduce it. These are  large numbers. Does this occur when you use say matrix power 2? Could you print out the X's in both cases (at least as many as you do for Y) just so we can verify they are exactly the same.
My guess would be TensorFlow configures the floating point flag settings different from default.
		</comment>
		<comment id='2' author='dburkhardt' date='2017-07-25T13:25:59Z'>
		I do still get the wrong result if I use matrix power 2:
In [1]:
import numpy as np
np.random.seed(seed=0)
X = np.random.randn(1000, 1000)
Y = np.linalg.matrix_power(X, 2)
In [2]:
X
Out[2]:
array([[ 1.76405235,  0.40015721,  0.97873798, ...,  0.0941923 ,
        -1.14761094, -0.35811408],
       [ 0.55596268,  0.89247389, -0.42231482, ...,  0.15843385,
        -1.14190142, -1.31097037],
       [-1.53292105, -1.71197016,  0.04613506, ..., -0.19240421,
        -1.21251574, -0.08059852],
       ..., 
       [ 0.43624889, -1.2972301 ,  1.99717648, ..., -0.42071168,
        -0.10894278, -0.45209518],
       [-0.10034023, -0.13437346, -0.15060322, ...,  0.35111034,
         1.36105416, -1.41088647],
       [-1.24468854, -1.20417642,  0.99255852, ..., -0.79728264,
         0.87475609,  1.37183066]])
In [3]:

Y
Out[3]:
array([[-20.90456457,  63.66601763,   4.60530742, ...,  22.82832076,
        -79.92214929,  37.8851418 ],
       [-16.69474231,  13.14948903,  -5.72608797, ...,  65.28426251,
         19.15823444, -15.81612538],
       [ -2.13591144, -62.87672722,   7.74009454, ..., -21.37731197,
         22.43966432, -23.05161539],
       ..., 
       [ 53.28579179,  56.25607449, -16.17015098, ..., -21.17820313,
        -15.59329817, -10.84431817],
       [  9.36027737, -38.47373171, -13.87875906, ...,  80.51944872,
        -34.13373385, -33.20098989],
       [-32.40022125,   8.00801434, -20.68091457, ..., -44.3388419 ,
         69.68672123,  10.77702875]])
In [1]:
import numpy as np
import tensorflow as tf
np.random.seed(seed=0)
X = np.random.randn(2000, 2000)
Y = np.linalg.matrix_power(X, 2)
In [2]:
X
Out[2]:
array([[ 1.76405235,  0.40015721,  0.97873798, ...,  0.15843385,
        -1.14190142, -1.31097037],
       [-1.53292105, -1.71197016,  0.04613506, ..., -0.03057244,
         1.57708821, -0.8128021 ],
       [ 0.61334917,  1.84369998,  0.27109098, ..., -0.53788475,
         0.39344443,  0.28651827],
       ..., 
       [-0.17117027,  0.57332063, -0.89516715, ..., -0.01409412,
         1.28756456, -0.6953778 ],
       [-1.53627571,  0.57441228, -0.20564476, ...,  0.90499929,
         0.51428298,  0.72148202],
       [ 0.51262101, -0.90758583,  1.78121159, ..., -1.12554283,
         0.95170926, -1.15237806]])
In [3]:
Y
Out[3]:
array([[  896.31054468,  2315.19199227,   106.53963366, ...,
         2919.22268215,   194.34572697,   698.64701546],
       [  688.43757414,  1574.75771259,  1761.10054579, ...,
         1757.49918001,  1817.00223281,   777.72532453],
       [ 1412.42877031, -1482.4939304 ,  1034.41817963, ...,
          494.08789763,   537.5932247 ,   371.50003778],
       ..., 
       [ -427.99247673,   942.29074458, -2296.56391085, ...,
         -833.13985797,  1251.9797819 , -1913.47322301],
       [ -307.34497424,  -981.15390609,  -204.04086649, ...,
         -207.92168833,   218.47627601,    67.95123058],
       [ -530.56347763,  -281.01241822,  -398.9338794 , ...,
         -912.01258927,   692.58480244,  -921.82876347]])
Interestingly, if the matrix is smaller, I can get the correct result. Unfortunately I'm not working with small matrices, I'm working with 100,000 x 20,000 matrices:
In [1]:
import numpy as np
np.random.seed(seed=0)
X = np.random.randn(5, 5)
Y = np.linalg.matrix_power(X, 2)

In [2]:
X
Out[2]:
array([[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,  1.86755799],
       [-0.97727788,  0.95008842, -0.15135721, -0.10321885,  0.4105985 ],
       [ 0.14404357,  1.45427351,  0.76103773,  0.12167502,  0.44386323],
       [ 0.33367433,  1.49407907, -0.20515826,  0.3130677 , -0.85409574],
       [-2.55298982,  0.6536186 ,  0.8644362 , -0.74216502,  2.26975462]])
In [3]:

Y
Out[3]:
array([[ -1.15833119,   7.07817798,   3.56548186,   3.34635217,
          6.21816086],
       [ -3.75696704,   0.40564612,  -0.83937785,  -2.64350512,  -0.4820835 ],
       [ -2.15008482,   3.01799485,   0.85877337,  -0.02605015,
          2.10746742],
       [  1.3838972 ,   1.16416823,  -0.85823278,   1.30044014,
         -1.06041697],
       [-11.06016005,   1.23122545,   0.17454991,  -7.60014337,
          1.66987577]])
Also correct:
In [1]:
import numpy as np
import tensorflow as tf
np.random.seed(seed=0)
X = np.random.randn(5, 5)
Y = np.linalg.matrix_power(X, 2)
In [2]:
X
Out[2]:
array([[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,  1.86755799],
       [-0.97727788,  0.95008842, -0.15135721, -0.10321885,  0.4105985 ],
       [ 0.14404357,  1.45427351,  0.76103773,  0.12167502,  0.44386323],
       [ 0.33367433,  1.49407907, -0.20515826,  0.3130677 , -0.85409574],
       [-2.55298982,  0.6536186 ,  0.8644362 , -0.74216502,  2.26975462]])
In [3]:
Y
Out[3]:
array([[ -1.15833119,   7.07817798,   3.56548186,   3.34635217,
          6.21816086],
       [ -3.75696704,   0.40564612,  -0.83937785,  -2.64350512,  -0.4820835 ],
       [ -2.15008482,   3.01799485,   0.85877337,  -0.02605015,
          2.10746742],
       [  1.3838972 ,   1.16416823,  -0.85823278,   1.30044014,
         -1.06041697],
       [-11.06016005,   1.23122545,   0.17454991,  -7.60014337,
          1.66987577]])
		</comment>
		<comment id='3' author='dburkhardt' date='2017-07-25T17:15:34Z'>
		Can you reproduce this outside of Jupyter or Ipython?
		</comment>
		<comment id='4' author='dburkhardt' date='2017-07-25T18:25:48Z'>
		Yes.
[dan@orkney ~]$ python
Python 3.6.1 (default, Mar 27 2017, 00:27:06)
[GCC 6.3.1 20170306] on linux
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; np.random.seed(seed=0)
&gt;&gt;&gt; X = np.random.randn(2000, 2000)
&gt;&gt;&gt; Y = np.linalg.matrix_power(X, 2)
&gt;&gt;&gt; X
array([[ 1.76405235,  0.40015721,  0.97873798, ...,  0.15843385,
        -1.14190142, -1.31097037],
       [-1.53292105, -1.71197016,  0.04613506, ..., -0.03057244,
         1.57708821, -0.8128021 ],
       [ 0.61334917,  1.84369998,  0.27109098, ..., -0.53788475,
         0.39344443,  0.28651827],
       ...,
       [-0.17117027,  0.57332063, -0.89516715, ..., -0.01409412,
         1.28756456, -0.6953778 ],
       [-1.53627571,  0.57441228, -0.20564476, ...,  0.90499929,
         0.51428298,  0.72148202],
       [ 0.51262101, -0.90758583,  1.78121159, ..., -1.12554283,
         0.95170926, -1.15237806]])
&gt;&gt;&gt; Y
array([[  910.34426793,  2319.0172763 ,    63.12608588, ...,
         2958.19091355,   -27.49231812,   567.53132384],
       [  769.19726704,  1604.25576828,  1787.40414157, ...,
         1687.0179502 ,  1923.88228017,   832.52806049],
       [ 1491.82177556, -1398.59394668,  1029.47973498, ...,
          603.01417689,   446.60541119,   484.82890574],
       ...,
       [ -452.5080765 ,   840.10603188, -2303.87826274, ...,
         -763.55186962,  1181.29736415, -1825.21634485],
       [ -199.92457917,  -952.89987624,  -279.06374383, ...,
         -146.58255678,   174.25003392,   183.98291186],
       [ -554.67074632,  -223.03466867,  -479.93292745, ...,
         -950.28529065,   800.70076349,  -898.34404592]])
Or with a larger power:
[dan@orkney ~]$ python
Python 3.6.1 (default, Mar 27 2017, 00:27:06)
[GCC 6.3.1 20170306] on linux
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; np.random.seed(seed=0)
&gt;&gt;&gt; X = np.random.randn(2000, 2000)
&gt;&gt;&gt; Y = np.linalg.matrix_power(X, 30)
&gt;&gt;&gt; X
array([[ 1.76405235,  0.40015721,  0.97873798, ...,  0.15843385,
        -1.14190142, -1.31097037],
       [-1.53292105, -1.71197016,  0.04613506, ..., -0.03057244,
         1.57708821, -0.8128021 ],
       [ 0.61334917,  1.84369998,  0.27109098, ..., -0.53788475,
         0.39344443,  0.28651827],
       ...,
       [-0.17117027,  0.57332063, -0.89516715, ..., -0.01409412,
         1.28756456, -0.6953778 ],
       [-1.53627571,  0.57441228, -0.20564476, ...,  0.90499929,
         0.51428298,  0.72148202],
       [ 0.51262101, -0.90758583,  1.78121159, ..., -1.12554283,
         0.95170926, -1.15237806]])
&gt;&gt;&gt; Y
array([[ -1.69238825e+91,   1.72147982e+91,   1.09701486e+91, ...,
          8.96734224e+90,   2.21217699e+91,   2.31904059e+91],
       [ -7.07657012e+89,   4.85685830e+91,   1.74187144e+91, ...,
         -1.61727709e+91,   7.45551485e+90,  -2.72503249e+91],
       [  2.97126997e+91,   2.33421956e+91,  -1.95668005e+91, ...,
          8.42491285e+90,  -2.30603740e+91,  -9.68175387e+90],
       ...,
       [ -3.91887830e+91,   6.66524075e+90,   1.44916935e+91, ...,
         -1.31096552e+91,  -6.69442115e+90,  -2.46114967e+88],
       [ -1.88415868e+91,   8.90475448e+91,   6.85311519e+91, ...,
         -3.61608344e+90,   8.49098335e+90,  -3.72585092e+91],
       [ -2.17044444e+91,   1.01337006e+91,   3.26124960e+90, ...,
         -2.58026794e+91,  -5.82000633e+89,  -5.54466196e+90]])
If I don't import tensorflow, then I get the same result as posted above.
		</comment>
		<comment id='5' author='dburkhardt' date='2017-07-25T18:28:52Z'>
		Could you try with the latest stock build and see if it still occurs?
		</comment>
		<comment id='6' author='dburkhardt' date='2017-07-25T18:29:44Z'>
		Sorry, could you please clarify what you mean by stock build? Do you just mean the precompiled binary?
		</comment>
		<comment id='7' author='dburkhardt' date='2017-07-25T21:15:01Z'>
		Yeah that's what I mean. Unfortunately I can't really reproduce this on my ubuntu machine and I don't have an Arch machine. This is definitely bizarre.
		</comment>
		<comment id='8' author='dburkhardt' date='2017-07-26T16:00:41Z'>
		&lt;denchmark-link:https://github.com/dburkhardt&gt;@dburkhardt&lt;/denchmark-link&gt;
 are you using MKL? It comes by default if you use Anaconda distro. There is some weird interaction between tensorflow and scipy when MKL is used (I reproduced one here &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/10005&gt;#10005&lt;/denchmark-link&gt;
)
		</comment>
		<comment id='9' author='dburkhardt' date='2017-07-26T18:57:57Z'>
		I am indeed using Intel MKL, &lt;denchmark-link:https://github.com/yaroslavvb&gt;@yaroslavvb&lt;/denchmark-link&gt;
. Actually the timing of discovering this bug coincided with compiling numpy with MKL.
I would be a shame if it turns out that Tensorflow isn't compatible with MKL. I've been thinking about switching to OpenBLAS, perhaps this is the time.
		</comment>
		<comment id='10' author='dburkhardt' date='2017-07-27T16:49:46Z'>
		If you can make a reliable repro it could make sense to report it with Intel. I recently found another MKL-caused bug in linalg and they fixed it promptly after &lt;denchmark-link:https://software.intel.com/en-us/forums/intel-distribution-for-python/topic/628049#comment-1904231&gt;reporting&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='dburkhardt' date='2017-07-28T16:41:46Z'>
		Thanks for the insight &lt;denchmark-link:https://github.com/yaroslavvb&gt;@yaroslavvb&lt;/denchmark-link&gt;
!
		</comment>
		<comment id='12' author='dburkhardt' date='2017-07-28T16:51:30Z'>
		&lt;denchmark-link:https://github.com/dburkhardt&gt;@dburkhardt&lt;/denchmark-link&gt;
 as a possible work-around, can you try lowering the number of MKL threads?
Set env var MKL_NUM_THREADS
		</comment>
		<comment id='13' author='dburkhardt' date='2017-08-02T18:56:00Z'>
		If I run export MKL_NUM_THREADS="1" before opening python, then I get the correct result, but if I set MKL_NUM_THREADS="2" or greater, then I get the wrong answer again.
		</comment>
		<comment id='14' author='dburkhardt' date='2017-08-02T19:17:58Z'>
		Sounds like an MKL bug. I also saw a bug where MKL's SVD gave incorrect results for certain numbers of MKL_NUM_THREADS (however it was order of magnitude faster than openblas version)
		</comment>
		<comment id='15' author='dburkhardt' date='2017-08-04T20:14:10Z'>
		But why would this bug only appear after importing Tensorflow? I don't have any problems with MKL outside of trying to use it with Tensorflow.
		</comment>
		<comment id='16' author='dburkhardt' date='2017-08-04T20:33:43Z'>
		Dunno....my first suspicion would be that it's related to dynamic linking. Both MKL and TensorFlow are .so files loaded with dlopen, at which point dynamic linker tries to resolve all the symbols, remap all internal references to global variables and do other dark magic. If .so libraries were compiled with incompatible flags, bad things happen, like &lt;denchmark-link:https://github.com/tensorflow/models/issues/523&gt;tensorflow/models#523&lt;/denchmark-link&gt;

		</comment>
		<comment id='17' author='dburkhardt' date='2017-12-20T19:22:23Z'>
		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
		</comment>
		<comment id='18' author='dburkhardt' date='2018-01-04T19:09:20Z'>
		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
		</comment>
		<comment id='19' author='dburkhardt' date='2018-01-24T13:12:08Z'>
		Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='20' author='dburkhardt' date='2018-02-08T19:30:53Z'>
		Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='21' author='dburkhardt' date='2018-02-23T14:08:51Z'>
		Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='22' author='dburkhardt' date='2018-03-10T13:20:45Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/tfboyd&gt;@tfboyd&lt;/denchmark-link&gt;
: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='23' author='dburkhardt' date='2018-03-25T12:42:00Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/tfboyd&gt;@tfboyd&lt;/denchmark-link&gt;
: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='24' author='dburkhardt' date='2018-04-09T12:37:40Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/tfboyd&gt;@tfboyd&lt;/denchmark-link&gt;
: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='25' author='dburkhardt' date='2018-04-24T18:45:39Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/tfboyd&gt;@tfboyd&lt;/denchmark-link&gt;
: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='26' author='dburkhardt' date='2018-04-24T21:15:22Z'>
		I had an issue very similar to this. Running Numpy 1.11.1 with MKL 2017 and TF 1.2.0.
Test script:
import numpy as np

bad = True
if bad:
    import tensorflow as tf

if __name__ == '__main__':
    rows = 100
    cols = 100
    X = np.ones((rows, cols))
    w = np.ones(cols)

    print np.dot(X, w).sum(), np.dot(X, w).sum(), np.dot(X, w).sum()
    print 'correct:', X.sum()
When bad is True, it outputs incorrect numbers. The exact numbers are non-deterministic, but they're almost always wrong. Example output:
&lt;denchmark-code&gt;15300.0 30000.0 26800.0
correct: 10000.0
&lt;/denchmark-code&gt;

If I either don't import TF or I set MKL_NUM_THREADS=1, then all works correctly
&lt;denchmark-code&gt;10000.0 10000.0 10000.0
correct: 10000.0
&lt;/denchmark-code&gt;

It would be nice for this to work so that I could use multi-threaded Numpy and TF in the same Python process without postponing TF import as much as possible.
		</comment>
		<comment id='27' author='dburkhardt' date='2018-04-28T04:05:51Z'>
		&lt;denchmark-link:https://github.com/eamartin&gt;@eamartin&lt;/denchmark-link&gt;
 1.2 is really old, this kind of issue should have been fixed by changing how TF symbols get linked in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/5c7f9e316d8c7735308a217310350d416d7498cc&gt;5c7f9e3&lt;/denchmark-link&gt;

		</comment>
		<comment id='28' author='dburkhardt' date='2018-05-12T18:31:53Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/tfboyd&gt;@tfboyd&lt;/denchmark-link&gt;
: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='29' author='dburkhardt' date='2018-05-14T22:10:08Z'>
		I am going to mark this as closed unless someone has a report from TF 1.8.  No disrespect.  It just pops up in my queue and without a repro for TF 1.8 and there are PRs that claim to have possibly fixed the problem it is really stale.  If the issue still exists with MKL then we can reopen and then assign to &lt;denchmark-link:https://github.com/tatianashp&gt;@tatianashp&lt;/denchmark-link&gt;
 and we can get Intel to address it.
Edit: that indicated we can reopen I meant to include that as an option.
		</comment>
	</comments>
</bug>