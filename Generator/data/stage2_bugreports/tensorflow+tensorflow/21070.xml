<bug id='21070' author='sibyjackgrove' open_date='2018-07-23T21:06:16Z' closed_time='2019-11-12T04:31:27Z'>
	<summary>Documentation request: tf.contrib.data.AUTOTUNE</summary>
	<description>
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): Binary
TensorFlow version (use command below): 1.9
Python version: 3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: tf.contrib.data.AUTOTUNE

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
According to release notes for 1.8, there is a new parameter called tf.contrib.data.AUTOTUNE. It's functionality is described in release notes as follows:
'Add tf.contrib.data.AUTOTUNE, which allows the tf.data runtime to automatically tune the prefetch buffer sizes based on your system and environment.'
But I couldn't find any other documentation associated with it in &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/data&gt;tf.dataset &lt;/denchmark-link&gt;

I used it in my pipeline as shown below and it didn't give any errors. But I have no idea how it works and which circumstances I should use it. Please provide some documentation on how to use this feature.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;#Make dataset for training
dataset_train = tf.data.Dataset.from_tensor_slices((file_ids_training,file_names_training))
dataset_train = dataset_train.flat_map(lambda file_id,file_name: tf.data.Dataset.from_tensor_slices(
    tuple (tf.py_func(_get_data_for_dataset, [file_id,file_name], [tf.float32,tf.float32]))))

dataset_train= dataset_train.shuffle(buffer_size=train_buffer_size)
dataset_train= dataset_train.repeat()
dataset_train= dataset_train.batch(train_batch_size) #Make dataset, shuffle, and create batches
dataset_train = dataset_train.prefetch(tf.contrib.data.AUTOTUNE)

dataset_train_iterator = dataset_train.make_one_shot_iterator()
get_train_batch = dataset_train_iterator.get_next()
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='sibyjackgrove' date='2019-01-16T21:13:52Z'>
		that's my concern too. There is not any documentation on this AUTOTUNE. how does this work?
		</comment>
		<comment id='2' author='sibyjackgrove' date='2019-03-04T18:57:36Z'>
		
that's my concern too. There is not any documentation on this AUTOTUNE. how does this work?

checkout the interleave doc part &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset&gt;https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='sibyjackgrove' date='2019-11-12T04:31:27Z'>
		This symbol has been moved to . Details about its implementation can be found in the &lt;denchmark-link:https://www.tensorflow.org/guide/data_performance#pipelining&gt;tf.data performance guide&lt;/denchmark-link&gt;
.
Thanks for the suggestion! ðŸ˜„
		</comment>
		<comment id='4' author='sibyjackgrove' date='2019-11-27T01:59:41Z'>
		&lt;denchmark-link:https://github.com/dynamicwebpaige&gt;@dynamicwebpaige&lt;/denchmark-link&gt;
 : I didn't find any documentation about 's implementation in the guide you linked. It seems as though it's easy to set up a 3 or 4 dimensional parameter space if I use  for several  and  arguments.
Documenting how AUTOTUNE functions seems important since I'm not sure how long it takes TF to optimize these values or how to profile my pipeline while it's performing the optimization.
		</comment>
		<comment id='5' author='sibyjackgrove' date='2020-07-17T12:31:00Z'>
		agree on the need of having a better understanding of how the AUTOTUNE actually works. i have just tried to get a quick good grasp of what this actually does and couldn't do so. out of curiosity, at the end of the day this is open source
		</comment>
	</comments>
</bug>