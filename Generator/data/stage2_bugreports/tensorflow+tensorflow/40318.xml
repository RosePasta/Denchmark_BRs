<bug id='40318' author='andreydung' open_date='2020-06-09T13:13:24Z' closed_time='2020-06-13T09:59:56Z'>
	<summary>[TF Lite C API]  Zero outputs</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: SnapDragon 645, Android 10
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): v2.2.0
Python version: 2.7
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: CUDA 10.2
GPU model and memory: NA

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with:

TF 1.0: python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
TF 2.0: python -c "import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"

Describe the current behavior
I'm inferencing the same model in C++ API (using libtensorflowlite.so) and C API (using libtensorflowlite_c.so) on Android NDK r18b. While the C++ code works well and passed test cases, the C version output all zeros.
Describe the expected behavior
Output from C and C++ version should be exactly the same.
Standalone code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
&lt;denchmark-code&gt;void forward(const float* real, const float* imag, float* pout) {
    TfLiteInterpreterAllocateTensors(interpreter);
    TfLiteTensor* tflite_real = TfLiteInterpreterGetInputTensor(interpreter, 0);
    TfLiteTensor* tflite_imag = TfLiteInterpreterGetInputTensor(interpreter, 1);
    TfLiteTensorCopyFromBuffer(tflite_real, real, N1 * sizeof(float));
    TfLiteTensorCopyFromBuffer(tflite_imag, imag, N2 * sizeof(float));

    TfLiteInterpreterInvoke(interpreter);
    const TfLiteTensor* out = TfLiteInterpreterGetOutputTensor(interpreter, 0);
    TfLiteTensorCopyToBuffer(out, pout, N_OUT * sizeof(float));
}
&lt;/denchmark-code&gt;

Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='andreydung' date='2020-06-11T00:21:15Z'>
		Just to confirm, you built the libtensorflowlite.so and libtensorflowlite_c.so from source using NDK r18b? Is the client code that uses these libs also built using NDK r18b?
You might also try inspecting the TfLiteTensor* values directly (see also the TfLiteTensor definition in lite/c/common.h) to see if there's an issues with the memcpy behavior.
		</comment>
		<comment id='2' author='andreydung' date='2020-06-11T03:00:05Z'>
		Hi &lt;denchmark-link:https://github.com/jdduke&gt;@jdduke&lt;/denchmark-link&gt;
: Yes, I built both  and  from  using NDK r18b. Client code is using NDK r18b as well.
We were able to inference all other models and got exactly the same outputs as C++ API. Only with this specific model that it keeps returning zero, even though input is not zeros.
If it helps for debugging, here is the link to the model:
&lt;denchmark-link:https://drive.google.com/file/d/1ZoRjTupSMHydsbGy4cWDLIuZD7LnybsA/view?usp=sharing&gt;https://drive.google.com/file/d/1ZoRjTupSMHydsbGy4cWDLIuZD7LnybsA/view?usp=sharing&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='andreydung' date='2020-06-11T23:37:59Z'>
		Can you confirm what sizes you're using for N1, N2 and N_OUT? And whether you did an explicit resize? It would be helpful to have some reference input values as well, if possible. Also, can you confirm that the entire output was zero? Thanks again.
		</comment>
		<comment id='4' author='andreydung' date='2020-06-12T00:21:24Z'>
		I created a local test feeding random/fixed float values to the inputs, and I get identical (non-zeroed) outputs using the C and C++ APIs for that model. I wouldn't think that this is 2.2-specific, but you might try also building from head to see if that resolves the issue?
		</comment>
		<comment id='5' author='andreydung' date='2020-06-12T07:46:12Z'>
		Thanks &lt;denchmark-link:https://github.com/jdduke&gt;@jdduke&lt;/denchmark-link&gt;
 let me retry the code.
		</comment>
		<comment id='6' author='andreydung' date='2020-06-13T09:59:56Z'>
		&lt;denchmark-link:https://github.com/jdduke&gt;@jdduke&lt;/denchmark-link&gt;
 Problem resolved, it's exactly due to memcpy and I set the wrong number of elements.
		</comment>
		<comment id='7' author='andreydung' date='2020-06-13T09:59:58Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40318&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40318&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>