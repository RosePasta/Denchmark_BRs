<bug id='42643' author='CoreyCole' open_date='2020-08-25T02:38:29Z' closed_time='2020-09-02T01:32:03Z'>
	<summary>TFLite Int8 Quantization Conversion - There are unresolved custom ops: []</summary>
	<description>
Error
&lt;denchmark-code&gt;RuntimeError: Failed to initialize op resolver for calibration:
There are unresolved custom ops: []Encountered unresolved custom op: RandomStandardNormal.Node number 0 (RandomStandardNormal) failed to prepare.
&lt;/denchmark-code&gt;

System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
TensorFlow installed from (source or binary): binary
TensorFlow version (or github SHA if from source): 2.3.0

Command used to run the converter or code if youâ€™re using the Python API
If possible, please share a link to Colab/Jupyter/any notebook.
&lt;denchmark-code&gt;import os, glob
os.environ["CUDA_VISIBLE_DEVICES"]="-1"
import tensorflow.compat.v1 as tf

from tensorflow.python.client import device_lib
import numpy as np

import faulthandler
faulthandler.enable()

import pdb

path = os.path.dirname(os.path.abspath(__file__))
pb_model_name = "model.pb"
pb_model_path = os.path.join(path, pb_model_name)

model_name = 'model_int8.tflite'

if __name__ == '__main__':
  with tf.device("/cpu:0"):
    configuration = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)
    with tf.Session(config=configuration) as sess:
      converter = tf.lite.TFLiteConverter.from_frozen_graph(
        graph_def_file=pb_model_path,
        # input_arrays=["device_0/wav_and_noisy:1"],
        input_arrays=["device_0/wav_and_noisy:1"],
        output_arrays=["device_0/g_ae_1/Tanh"],
        input_shapes={"device_0/wav_and_noisy:1": [100, 16384]}
      )
      converter.allow_custom_ops = True
      # converter.experimental_new_converter = True
      converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8,
                                             tf.lite.OpsSet.SELECT_TF_OPS]
      # converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8,
      #                                        tf.lite.OpsSet.TFLITE_BUILTINS,
      #                                        tf.lite.OpsSet.SELECT_TF_OPS]
      converter.optimizations = [tf.lite.Optimize.DEFAULT]
      # converter.target_spec.supported_types = [tf.int8]
      converter.inference_input_type = tf.int8  # or tf.uint8
      converter.inference_output_type = tf.int8  # or tf.uint8

      def test():
        pdb.set_trace()
        zeros = np.zeros(shape=(1, 100, 16384), dtype='int8')
        dataset = tf.data.Dataset.from_tensor_slices(zeros).batch(1)
        yield [zeros]
      converter.representative_dataset = test

      # pdb.set_trace()
      tflite_model = converter.convert()

      tflite_model_size = open(model_name, 'wb').write(tflite_model)
      print('TFLite Model is %d bytes' % tflite_model_size)
&lt;/denchmark-code&gt;

I've tried also including the TFLITE_BUILTINS in addition to TFLITE_BUILTINS_INT8. In other issues on github, I've seen that adding SELECT_TF_OPS solves errors similar to mine, but it did not fix it for me.
The output from the converter invocation
&lt;denchmark-code&gt;2020-08-25 02:22:18.873117: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory
2020-08-25 02:22:18.873149: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
WARNING:tensorflow:From convert_model_to_tflite_int8.py:24: experimental_run_functions_eagerly (from tensorflow.python.eager.def_function) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.config.run_functions_eagerly` instead of the experimental version.
2020-08-25 02:22:19.959195: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2020-08-25 02:22:19.959229: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)
2020-08-25 02:22:19.959253: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-31-5-134): /proc/driver/nvidia/version does not exist
2020-08-25 02:22:19.959493: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-08-25 02:22:19.984669: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2999995000 Hz
2020-08-25 02:22:19.984899: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3d64580 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-08-25 02:22:19.984922: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-08-25 02:22:19.987088: I tensorflow/core/common_runtime/direct_session.cc:360] Device mapping:
/job:localhost/replica:0/task:0/device:XLA_CPU:0 -&gt; device: XLA_CPU device

2020-08-25 02:22:22.412284: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count &gt;= 8, compute capability &gt;= 0.0): 0
2020-08-25 02:22:22.412445: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-08-25 02:22:23.196542: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize
2020-08-25 02:22:23.196595: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.005ms.
2020-08-25 02:22:23.196614: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-08-25 02:22:24.510075: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.
2020-08-25 02:22:24.510118: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.
Traceback (most recent call last):
  File "convert_model_to_tflite_int8.py", line 102, in &lt;module&gt;
    tflite_model = converter.convert()
  File "../python3.6/site-packages/tensorflow/lite/python/lite.py", line 1970, in convert
    return super(TFLiteConverter, self).convert()
  File "../python3.6/site-packages/tensorflow/lite/python/lite.py", line 1339, in convert
    result = self._calibrate_quantize_model(result, **flags)
  File "../python3.6/site-packages/tensorflow/lite/python/lite.py", line 452, in _calibrate_quantize_model
    inference_output_type, allow_float, activations_type)
  File "../python3.6/site-packages/tensorflow/lite/python/optimize/calibrator.py", line 91, in calibrate_and_quantize
    self._calibrator.Prepare([list(s.shape) for s in sample])
RuntimeError: Failed to initialize op resolver for calibration:
There are unresolved custom ops: []Encountered unresolved custom op: RandomStandardNormal.Node number 0 (RandomStandardNormal) failed to prepare.
&lt;/denchmark-code&gt;

Also, please include a link to the saved model or GraphDef
Unfortunately cannot provide publicly at this time. I can share privately if needed.
Failure details
The conversion hits a RuntimeError and stops. From PDB (below) I know it is hitting my representative_dataset function twice before crashing.
Any other info / logs
With a break point inside my representative_dataset function:
&lt;denchmark-code&gt;2020-08-25 02:29:39.675277: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory
2020-08-25 02:29:39.675311: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
WARNING:tensorflow:From /home/ubuntu/denoise-gst/denoise_deploy/inference_module/convert_model_to_tflite_int8.py:18: experimental_run_functions_eagerly (from tensorflow.python.eager.def_function) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.config.run_functions_eagerly` instead of the experimental version.
2020-08-25 02:29:40.563260: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2020-08-25 02:29:40.563287: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)
2020-08-25 02:29:40.563307: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-31-5-134): /proc/driver/nvidia/version does not exist
2020-08-25 02:29:40.563535: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-08-25 02:29:40.588667: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2999995000 Hz
2020-08-25 02:29:40.588892: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3c1a8b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-08-25 02:29:40.588920: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-08-25 02:29:40.591168: I tensorflow/core/common_runtime/direct_session.cc:360] Device mapping:
/job:localhost/replica:0/task:0/device:XLA_CPU:0 -&gt; device: XLA_CPU device

2020-08-25 02:29:42.998639: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count &gt;= 8, compute capability &gt;= 0.0): 0
2020-08-25 02:29:42.998794: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-08-25 02:29:43.773981: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize
2020-08-25 02:29:43.774020: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.006ms.
2020-08-25 02:29:43.774031: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-08-25 02:29:45.082195: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.
2020-08-25 02:29:45.082241: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.
&gt; /home/ubuntu/denoise-gst/denoise_deploy/inference_module/convert_model_to_tflite_int8.py(93)test()
-&gt; zeros = np.zeros(shape=(1, 100, 16384), dtype='int8')
(Pdb) n
&gt; /home/ubuntu/denoise-gst/denoise_deploy/inference_module/convert_model_to_tflite_int8.py(94)test()
-&gt; dataset = tf.data.Dataset.from_tensor_slices(zeros).batch(1)
(Pdb)
&gt; /home/ubuntu/denoise-gst/denoise_deploy/inference_module/convert_model_to_tflite_int8.py(97)test()
-&gt; yield [zeros]
(Pdb)
GeneratorExit
&gt; /home/ubuntu/denoise-gst/denoise_deploy/inference_module/convert_model_to_tflite_int8.py(97)test()
-&gt; yield [zeros]
(Pdb)
Traceback (most recent call last):
  File "/usr/lib/python3.6/pdb.py", line 1667, in main
    pdb._runscript(mainpyfile)
  File "/usr/lib/python3.6/pdb.py", line 1548, in _runscript
    self.run(statement)
  File "/usr/lib/python3.6/bdb.py", line 434, in run
    exec(cmd, globals, locals)
  File "&lt;string&gt;", line 1, in &lt;module&gt;
  File "/home/ubuntu/denoise-gst/denoise_deploy/inference_module/convert_model_to_tflite_int8.py", line 18, in &lt;module&gt;
    '''
  File ".../python3.6/site-packages/tensorflow/lite/python/lite.py", line 1970, in convert
    return super(TFLiteConverter, self).convert()
  File ".../python3.6/site-packages/tensorflow/lite/python/lite.py", line 1339, in convert
    result = self._calibrate_quantize_model(result, **flags)
  File ".../python3.6/site-packages/tensorflow/lite/python/lite.py", line 452, in _calibrate_quantize_model
    inference_output_type, allow_float, activations_type)
  File ".../python3.6/site-packages/tensorflow/lite/python/optimize/calibrator.py", line 91, in calibrate_and_quantize
    self._calibrator.Prepare([list(s.shape) for s in sample])
RuntimeError: Failed to initialize op resolver for calibration:
There are unresolved custom ops: []Encountered unresolved custom op: RandomStandardNormal.Node number 0 (RandomStandardNormal) failed to prepare.

Uncaught exception. Entering post mortem debugging
Running 'cont' or 'step' will restart the program
&gt; .../python3.6/site-packages/tensorflow/lite/python/optimize/calibrator.py(91)calibrate_and_quantize()
-&gt; self._calibrator.Prepare([list(s.shape) for s in sample])
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='CoreyCole' date='2020-08-27T06:20:47Z'>
		&lt;denchmark-link:https://github.com/CoreyCole&gt;@CoreyCole&lt;/denchmark-link&gt;
 Can you please share a standalone code and *.pb file. &lt;denchmark-link:https://colab.research.google.com/gist/jvishnuvardhan/a35756d09a46c7c82b8dd6c449bea591/untitled12.ipynb&gt;Here&lt;/denchmark-link&gt;
 is a gist that is throwing different error. thanks!
		</comment>
		<comment id='2' author='CoreyCole' date='2020-09-01T02:23:38Z'>
		Could you try the conversion with tf-nightly version? We've landed a fix for supporting integer eight kernels with Flex operators recently.
		</comment>
		<comment id='3' author='CoreyCole' date='2020-09-02T01:32:03Z'>
		Thank you for the support &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/abattery&gt;@abattery&lt;/denchmark-link&gt;

I believe turned out to be a problem with how I was exporting the saved model before further converting to tflite. By using the correct input node/shape and upgrading to tensorflow==2.3.0 the problem has gone away.
		</comment>
		<comment id='4' author='CoreyCole' date='2020-09-02T01:32:04Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42643&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42643&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>