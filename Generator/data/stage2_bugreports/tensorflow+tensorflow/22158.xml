<bug id='22158' author='EvansDaniel' open_date='2018-09-08T01:34:29Z' closed_time='2019-03-14T17:47:34Z'>
	<summary>Unexpected behavior of per_process_gpu_memory_fraction</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): See below
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): binary (used pip)
TensorFlow version (use command below):  See below
Python version: 3.6
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: See below
GPU model and memory: See below
Exact command to reproduce: Just run the script with python

&lt;denchmark-h:h3&gt;System information from your tf env script:&lt;/denchmark-h&gt;

== cat /etc/issue ===============================================
Linux ip-172-31-54-194 4.4.0-1062-aws &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/71&gt;#71&lt;/denchmark-link&gt;
-Ubuntu SMP Fri Jun 15 10:07:39 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION="16.04.4 LTS (Xenial Xerus)"
VERSION_ID="16.04"
VERSION_CODENAME=xenial
== are we in docker =============================================
No
== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
== uname -a =====================================================
Linux ip-172-31-54-194 4.4.0-1062-aws &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/71&gt;#71&lt;/denchmark-link&gt;
-Ubuntu SMP Fri Jun 15 10:07:39 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
== check pips ===================================================
numpy               1.14.5
protobuf            3.6.0
tensorflow-gpu      1.10.0
== check for virtualenv =========================================
False
== tensorflow import ============================================
tf.VERSION = 1.10.0
tf.GIT_VERSION = v1.10.0-0-g656e7a2b34
tf.COMPILER_VERSION = v1.10.0-0-g656e7a2b34
Sanity check: array([1], dtype=int32)
/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/init.py:36: FutureWarning: Conversion of the second argument of issubdtype from float to np.floating is deprecated. In future, it will be treated as np.float64 == np.dtype(float).type.
from ._conv import register_converters as _register_converters
== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda-9.0/lib64:/usr/local/cuda-9.0/extras/CUPTI/lib64:/lib/nccl/cuda-9.0/lib:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:
DYLD_LIBRARY_PATH is unset
== nvidia-smi ===================================================
Sat Sep  8 01:19:08 2018
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 390.46                 Driver Version: 390.46                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:00:1B.0 Off |                    0 |
| N/A   45C    P0    63W / 300W |      0MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-SXM2...  On   | 00000000:00:1C.0 Off |                    0 |
| N/A   42C    P0    38W / 300W |      0MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-SXM2...  On   | 00000000:00:1D.0 Off |                    0 |
| N/A   42C    P0    43W / 300W |      0MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |
| N/A   44C    P0    42W / 300W |      0MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
== cuda libs  ===================================================
/usr/local/cuda-9.1/doc/man/man7/libcudart.7
/usr/local/cuda-9.1/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.1/lib64/libcudart.so.9.1.85
/usr/local/cuda-9.1/lib64/libcudart_static.a
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/lib64/libcudart_static.a
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61
/usr/local/cuda-9.0/doc/man/man7/libcudart.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.0/lib64/libcudart.so.9.0.176
/usr/local/cuda-9.0/lib64/libcudart_static.a
&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I have written a small example that uses per_process_gpu_memory_fraction. I set it to .1 and allow_growth=false (by default). My expectation when I run nvidia-smi during program execution is that I will see a maximum of 10% of the gpu memory being used. During some trial runs of the program though, I have seen it using around 13%. So I am wondering if this is a bug or if it is expected behavior. If it's expected behavior, why is this happening?
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
&lt;denchmark-h:h3&gt;Script to reproduce with:&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import tensorflow as tf

from time import sleep

i = tf.constant(0)
x = tf.constant(10)
r = tf.add(i,x)

# Use at most 10% of gpu memory 
gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=.1)

# sleep is used to see what nvidia-smi says for gpu memory usage, 
# I expect that it will be at most 10% of gpu memory (which is 1616.0 mib for my gpu)
# but instead I see up to 2120 mib used by the process
with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
        sess.run(r);
        sleep(10) 

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='EvansDaniel' date='2018-09-24T22:41:21Z'>
		/CC &lt;denchmark-link:https://github.com/ekelsen&gt;@ekelsen&lt;/denchmark-link&gt;
, do you know why this occurs? We allocate 10% of the GPU memory, but the CUDA runtime takes some memory itself, I think.
		</comment>
		<comment id='2' author='EvansDaniel' date='2019-02-22T22:05:21Z'>
		unassign
		</comment>
		<comment id='3' author='EvansDaniel' date='2019-02-26T20:06:15Z'>
		&lt;denchmark-link:https://github.com/reedwm&gt;@reedwm&lt;/denchmark-link&gt;
 Yeah from my experimentation it seems like allocating cuda contexts takes some memory. Also, some memory is used when initializing cudnn and cublas. I think this mostly accounts for it. Does anyone have any additional insight?
		</comment>
		<comment id='4' author='EvansDaniel' date='2019-03-14T17:47:34Z'>
		I think it was resolved. I am closing the issue. If you think I made a mistake, please open another issue. Please open a new ticket if you see similar issue again with new version of TF. Thanks!
		</comment>
	</comments>
</bug>