<bug id='42500' author='drebain' open_date='2020-08-19T19:21:57Z' closed_time='2020-08-28T19:25:23Z'>
	<summary>tf.dynamic_partition causes crash when using multiple GPUs via tf.distribute.MirroredStrategy</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.3.0
Python version: 3.6.9
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: CUDA 10.1 / cuDNN 7.6.1
GPU model and memory: RTX 2080 8GB

Describe the current behavior
The tf.dynamic_partition operation crashes when running on multiple GPUs using tf.distribute.MirroredStrategy.
Describe the expected behavior
The same code, also using tf.distribute.MirroredStrategy runs succesfully when limited to a single GPU by setting CUDA_VISIBLE_DEVICES=0.
Standalone code to reproduce the issue
&lt;denchmark-code&gt;import tensorflow as tf

N = 100
M = 4

distribute_strategy = tf.distribute.MirroredStrategy()

def op():
  data = tf.random.uniform((N,))
  partitions = tf.random.uniform((N,), maxval=M, dtype=tf.int32)
  return tf.dynamic_partition(data, partitions, M)

distribute_strategy.run(op)
&lt;/denchmark-code&gt;

Other info / logs
Full output of the above code:
&lt;denchmark-code&gt;2020-08-19 12:05:36.898086: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-08-19 12:05:37.828508: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-08-19 12:05:37.900555: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:09:00.0 name: GeForce RTX 2080 computeCapability: 7.5
coreClock: 1.83GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.23GiB/s
2020-08-19 12:05:37.901044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: 
pciBusID: 0000:42:00.0 name: GeForce RTX 2080 computeCapability: 7.5
coreClock: 1.83GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.23GiB/s
2020-08-19 12:05:37.901070: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-08-19 12:05:37.902404: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-08-19 12:05:37.903988: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-08-19 12:05:37.904184: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-08-19 12:05:37.905511: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-08-19 12:05:37.906204: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-08-19 12:05:37.908753: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-08-19 12:05:37.910997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1
2020-08-19 12:05:37.911427: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-08-19 12:05:37.938447: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2994045000 Hz
2020-08-19 12:05:37.941540: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x49ac240 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-08-19 12:05:37.941596: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-08-19 12:05:42.073849: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4a182c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-08-19 12:05:42.073925: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080, Compute Capability 7.5
2020-08-19 12:05:42.073967: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): GeForce RTX 2080, Compute Capability 7.5
2020-08-19 12:05:42.075578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:09:00.0 name: GeForce RTX 2080 computeCapability: 7.5
coreClock: 1.83GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.23GiB/s
2020-08-19 12:05:42.076318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: 
pciBusID: 0000:42:00.0 name: GeForce RTX 2080 computeCapability: 7.5
coreClock: 1.83GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.23GiB/s
2020-08-19 12:05:42.076367: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-08-19 12:05:42.076406: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-08-19 12:05:42.076433: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-08-19 12:05:42.076457: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-08-19 12:05:42.076478: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-08-19 12:05:42.076499: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-08-19 12:05:42.076524: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-08-19 12:05:42.079838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1
2020-08-19 12:05:42.079887: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-08-19 12:05:42.939356: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-08-19 12:05:42.939406: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 1 
2020-08-19 12:05:42.939413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N N 
2020-08-19 12:05:42.939417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 1:   N N 
2020-08-19 12:05:42.941258: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2020-08-19 12:05:42.941298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7252 MB memory) -&gt; physical GPU (device: 0, name: GeForce RTX 2080, pci bus id: 0000:09:00.0, compute capability: 7.5)
2020-08-19 12:05:42.942216: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2020-08-19 12:05:42.942237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 2566 MB memory) -&gt; physical GPU (device: 1, name: GeForce RTX 2080, pci bus id: 0000:42:00.0, compute capability: 7.5)
WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.
2020-08-19 12:05:42.972712: F tensorflow/core/kernels/dynamic_partition_op_gpu.cu.cc:108] Non-OK-status: GpuLaunchKernel(GatherOpKernel&lt;T, int32, true&gt;, config.block_count, config.thread_per_block, 0, d.stream(), params, indices, out, gather_dim_size, indices_size, slice_size, out_size) status: Internal: invalid resource handle
Aborted (core dumped)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='drebain' date='2020-08-20T05:56:44Z'>
		&lt;denchmark-link:https://github.com/drebain&gt;@drebain&lt;/denchmark-link&gt;

I have tried in colab with Tensorflow -GPU version 2.3.0 and i am not seeing any issue.Please, find the gist &lt;denchmark-link:https://colab.research.google.com/gist/ravikyram/1179cb07cd83075aacbfa1dc7835e8f9/untitled267.ipynb&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='2' author='drebain' date='2020-08-20T06:03:07Z'>
		The output includes INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',) which I think indicates that it is only running with a single GPU available. The crash only happens when there are at least two devices present. I don't know if this is possible to test with colab.
		</comment>
		<comment id='3' author='drebain' date='2020-08-22T12:48:51Z'>
		This might have something to do with the version of CUDA, or the type of GPU being used (see &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/30665&gt;similar issue&lt;/denchmark-link&gt;
. Could you try this with the latest version of TF?
!pip install tf-nightly-gpu
		</comment>
		<comment id='4' author='drebain' date='2020-08-23T01:39:43Z'>
		I have encountered the same "invalid resource handle" crash with Tensorflow 2.2.0 and 2.3.0 on machines with RTX 2080 as well as V100 GPUs. I have tried CUDA 10.0, 10.1, and 10.2.
I have just now updated my personal machine to the NVIDIA 450 driver and installed CUDA 11.0 + tf-nightly-gpu:
&lt;denchmark-code&gt;2020-08-22 18:24:52.424497: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2020-08-22 18:24:54.262314: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-08-22 18:24:54.262349: I tensorflow/compiler/jit/xla_gpu_device.cc:69] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-08-22 18:24:54.271839: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-08-22 18:24:54.354837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:09:00.0 name: GeForce RTX 2080 computeCapability: 7.5
coreClock: 1.83GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.23GiB/s
2020-08-22 18:24:54.355405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: 
pciBusID: 0000:42:00.0 name: GeForce RTX 2080 computeCapability: 7.5
coreClock: 1.83GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.23GiB/s
2020-08-22 18:24:54.355431: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2020-08-22 18:24:54.372287: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2020-08-22 18:24:54.383247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-08-22 18:24:54.387885: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-08-22 18:24:54.405073: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-08-22 18:24:54.409284: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11
2020-08-22 18:24:54.411034: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2020-08-22 18:24:54.415315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1
2020-08-22 18:24:54.416756: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-08-22 18:24:54.420115: I tensorflow/compiler/jit/xla_cpu_device.cc:54] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-08-22 18:24:54.420145: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-08-22 18:24:54.664832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:09:00.0 name: GeForce RTX 2080 computeCapability: 7.5
coreClock: 1.83GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.23GiB/s
2020-08-22 18:24:54.665355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: 
pciBusID: 0000:42:00.0 name: GeForce RTX 2080 computeCapability: 7.5
coreClock: 1.83GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.23GiB/s
2020-08-22 18:24:54.665387: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2020-08-22 18:24:54.665407: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2020-08-22 18:24:54.665418: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-08-22 18:24:54.665428: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-08-22 18:24:54.665437: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-08-22 18:24:54.665446: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11
2020-08-22 18:24:54.665459: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2020-08-22 18:24:54.667603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1
2020-08-22 18:24:54.668415: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2020-08-22 18:24:56.003639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-08-22 18:24:56.003708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 1 
2020-08-22 18:24:56.003716: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N N 
2020-08-22 18:24:56.003720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 1:   N N 
2020-08-22 18:24:56.008774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7253 MB memory) -&gt; physical GPU (device: 0, name: GeForce RTX 2080, pci bus id: 0000:09:00.0, compute capability: 7.5)
2020-08-22 18:24:56.012643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 5707 MB memory) -&gt; physical GPU (device: 1, name: GeForce RTX 2080, pci bus id: 0000:42:00.0, compute capability: 7.5)
WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.
2020-08-22 18:24:56.171346: F tensorflow/core/kernels/dynamic_partition_op_gpu.cu.cc:108] Non-OK-status: GpuLaunchKernel(GatherOpKernel&lt;T, int32, true&gt;, config.block_count, config.thread_per_block, 0, d.stream(), params, indices, out, gather_dim_size, indices_size, slice_size, out_size) status: Internal: invalid resource handle
Aborted (core dumped)
&lt;/denchmark-code&gt;

This is the output of the reproduction script above running with tf.__version__ == '2.4.0-dev20200822'
		</comment>
		<comment id='5' author='drebain' date='2020-08-23T08:55:17Z'>
		I think I have found the cause of the issue. The GPU implementation of the dynamic_partition op ends by registering a callback which runs after ComputeAsync() returns, but apparently does not ensure that the correct CUDA device is active when the callback executes. As long as the only GPU being used is the default "GPU:0" this works, but it can fail when attempting to run on a different device. In fact, I have been able to reproduce the crash with an even simpler script that does not require MirroredStrategy:
&lt;denchmark-code&gt;import tensorflow as tf
N = 100
M = 4
with tf.device("GPU:1"):
  data = tf.random.uniform((N,))
  partitions = tf.random.uniform((N,), maxval=M, dtype=tf.int32)
  tf.dynamic_partition(data, partitions, M)
&lt;/denchmark-code&gt;

I have made a &lt;denchmark-link:https://github.com/drebain/tensorflow/commit/113555fab7c1a607991b41cdf3974f5f8e0873ea&gt;simple patch&lt;/denchmark-link&gt;
 for the dynamic_stitch op that makes the above script (as well as the one in the original issue) work by copying some of the code from before  is called, which I think sets up the CUDA device. I doubt this is a robust or correct solution, but hopefully it saves someone some time debugging.
As I mentioned before, I have consistently been able to reproduce this under a variety of conditions, but it's possible that the error is non-deterministic if the scheduling of the callback has an effect.
		</comment>
		<comment id='6' author='drebain' date='2020-08-24T03:29:59Z'>
		Thanks &lt;denchmark-link:https://github.com/drebain&gt;@drebain&lt;/denchmark-link&gt;
 for digging into the root cause! We have been able to repro the issue as well on different GPUs, and do not believe it is specific to GPU or TF version. Assigning to &lt;denchmark-link:https://github.com/sanjoy&gt;@sanjoy&lt;/denchmark-link&gt;
 to take a look further.
		</comment>
		<comment id='7' author='drebain' date='2020-08-24T04:51:18Z'>
		Hi &lt;denchmark-link:https://github.com/drebain&gt;@drebain&lt;/denchmark-link&gt;
,
Your fix looks correct to me; would you be willing to create a PR fixing the bug and adding the test?  You can use &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/09f5609f0fd282943defd4608ee90bb6883a394b/tensorflow/core/kernels/BUILD#L240-L261&gt;collective_nccl_test&lt;/denchmark-link&gt;
 as an example on how to add a multi-GPU test to TensorFlow.
You'll need to switch between ROCm and CUDA though, like we do &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/09f5609f0fd282943defd4608ee90bb6883a394b/tensorflow/core/kernels/segment_reduction_ops_impl.h#L47-L56&gt;here&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='8' author='drebain' date='2020-08-24T05:44:02Z'>
		&lt;denchmark-link:https://github.com/sanjoy&gt;@sanjoy&lt;/denchmark-link&gt;
 Yes, I will add a test that makes sure the op can run when launched on any GPU + the ROCm switch and open a PR.
		</comment>
		<comment id='9' author='drebain' date='2020-08-28T19:25:25Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42500&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42500&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>