<bug id='9502' author='rkeisler' open_date='2017-04-27T21:18:46Z' closed_time='2017-05-05T16:49:45Z'>
	<summary>Internally inconsistent results on GPU, not on CPU</summary>
	<description>
&lt;denchmark-h:h3&gt;System Information&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;Ubuntu 16.04 LTS
CUDA 8.0
cuDNN v5.1
NVIDIA Tesla K80 (11439MiB)
tensorflow 1.0.1 (bug also present in 1.1.0) [pip]
keras 2.0.3 [python setup.py install]
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Problem&lt;/denchmark-h&gt;

I can define a convnet that returns reasonable results with CPU, but returns a mixture of expected and nonsense results with GPU. The problem with the GPU processing is made clear by passing a set of "all ones" images to the net; the output should be identical for all 16 images, yet the outputs for the last one or two images differ drastically from the rest.
I see this problem when using the GPU, but not with CPU.  No exceptions are raised, and there are no warnings beyond the usual TensorFlow library wasn't compiled to use ___ instructions, but these are available on your machine and could speed up CPU computations..
I see this problem using tensorflow as my keras backend, but not when using theano as my backend.  That statement is true whether I set image_data_format to channels_last or channels_first, for either backend.
I've found three changes to the model architecture defined below that make the problem go away: (1) change the number of filters, as described in more detail below, (2) remove the  layer, or (3) remove the   layer.  Given (3), this issue may be related to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/8566&gt;#8566&lt;/denchmark-link&gt;
 .
&lt;denchmark-h:h3&gt;Code&lt;/denchmark-h&gt;

Consider the following convnet.
&lt;denchmark-code&gt;import tensorflow as tf
from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, AveragePooling2D
from keras.models import Model
import numpy as np

def my_model(filters=65):
    np.random.seed(0)
    inputs = Input((None, None, 3))
    conv1 = Conv2D(filters, (5,5), strides=(1,1), padding='same', activation='relu')(inputs)
    conv1 = MaxPooling2D(pool_size=(2, 2), strides=(2,2))(conv1)
    conv2 = Conv2D(filters, (5,5), strides=(1,1), padding='same', activation='relu')(conv1)
    conv2 = MaxPooling2D(pool_size=(2, 2), strides=(2,2))(conv2)
    conv3 = Conv2D(filters, (3,3), strides=(1,1), padding='same', activation='relu')(conv2)
    conv3 = MaxPooling2D(pool_size=(2, 2), strides=(2,2))(conv3)
    conv4 = Conv2D(filters, (3,3), strides=(1,1), padding='same', activation='relu')(conv3)
    conv4 = MaxPooling2D(pool_size=(2, 2), strides=(2,2))(conv4)
    conv5 = Conv2D(filters, (3,3), strides=(1,1), padding='same', activation='relu')(conv4)
    conv5_up = UpSampling2D(size=(8, 8))(conv5)
    conv5_up = AveragePooling2D(pool_size=(8, 8), strides=(1,1), padding='same')(conv5_up)
    return Model(inputs=inputs, outputs=conv5_up)
&lt;/denchmark-code&gt;

We can test the self-consistency of this net by feeding it 16 "all ones" images.  The output features should be identical for each input image, since the input images are simply all ones.
&lt;denchmark-code&gt;def test(filters=65, device='gpu'):
    # Make a batch of 16 "images", ALL ONES.
    img = np.ones((16, 512, 512, 3), dtype='float32')
    with tf.device('/%s:0'%device):
        model = my_model(filters)
        features = model.predict(img)
    # Compare features of 1st and 16th image.
    # Since the input was all ones, they should agree.
    inconsistent = np.any(features[0] != features[15])
    if inconsistent:
        print "Inconsistent!"
&lt;/denchmark-code&gt;

The model architecture is parameterized by a single parameter, filters, the number of filters in each layer.  Interestingly, I see that the model inconsistency cares about powers of 2 in filters:
• consistency for filters=1 through 64,
• inconsistency for filters=65 through 127,
• consistency for filters=128,
• inconsistency for filters=129 through 150.
The smallest model that shows the problem is at filters=65.  Below I show a single feature output for the 1st and 16th input images.  They should agree, but they don't.
Features for 1st image (which is identical for images 1-15):
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/4852957/25504294/33eb3ad6-2b52-11e7-9b1d-636bd576ed23.png&gt;&lt;/denchmark-link&gt;

Features for 16th image:
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/4852957/25504297/3969ab6e-2b52-11e7-87e7-67bed7a8c7b3.png&gt;&lt;/denchmark-link&gt;

As can be seen, the feature map for the 16th image differs dramatically from images 1-15, even though the inputs are identical (all ones).
If I repeat this test with a filters=129 instead of filters=65, the bug appears earlier in the batch; the features for images 1-14 are identical, while the features for images 15 and 16 differ from the rest, and from each other.
	</description>
	<comments>
		<comment id='1' author='rkeisler' date='2017-04-27T21:26:45Z'>
		&lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
 Would you mind commenting on this and &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/8566&gt;#8566&lt;/denchmark-link&gt;
?  Andy asked a while back but didn't get a reply.
		</comment>
		<comment id='2' author='rkeisler' date='2017-05-04T23:31:36Z'>
		I've commented on &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/8566&gt;#8566&lt;/denchmark-link&gt;
, please try setting TF_AVGPOOL_USE_CUDNN=1 and see if it fixes the bug.
		</comment>
		<comment id='3' author='rkeisler' date='2017-05-05T16:46:14Z'>
		Yes, setting TF_AVGPOOL_USE_CUDNN=1 fixes the bug.  Thanks for looking into this.
		</comment>
	</comments>
</bug>