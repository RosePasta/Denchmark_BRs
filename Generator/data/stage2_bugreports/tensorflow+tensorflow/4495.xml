<bug id='4495' author='lingz' open_date='2016-09-20T19:11:51Z' closed_time='2017-01-27T19:56:42Z'>
	<summary>Documentation on how reusable variables sync between CPU and GPU</summary>
	<description>
I haven't been able to find anything online that documents how the sync of reusable variables happens between CPU and GPU. Specifically, I'm interested in if a CPU pinned variable is used multiple times in a GPU computation without being updated, does it sync only once (i.e. use a dirty flag), or will it try sync again each time?
	</description>
	<comments>
		<comment id='1' author='lingz' date='2016-09-22T04:55:18Z'>
		&lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
 Could you take a look? Thanks.
		</comment>
		<comment id='2' author='lingz' date='2016-09-22T05:29:51Z'>
		In general, TensorFlow transfers a tensor between ops across different devices.
If a variable is pinned on CPU, reading it on GPU will cause it to transfer. If your graph reads it several times, it will be transferred several times. The way to avoid that is to read from a tf.identity that is pinned on GPU. In that case, the tensor flowing from the CPU variable to its GPU tf.identity will cause it to be transferred once. The remaining calculation reads from that GPU copy. This gives the model builder the control how the data transfer should happen.
		</comment>
		<comment id='3' author='lingz' date='2016-09-23T08:51:06Z'>
		Thanks &lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
 for your response, so just to clarify things:
For an RNN Cell, whose weights are pinned to CPU but ops executed on GPU, the cell will be called multiple times under dynamic_rnn with tf.python.ops.control_flow_ops.while_loop. In this case, it would try copy the weights on every iteration. The correct approach to this, is to modify the RNNCell class, and modify the weight variables to use an tf.identity on the get_variable, is that correct?
		</comment>
		<comment id='4' author='lingz' date='2016-09-23T17:24:15Z'>
		get_variable already supports that. You can use "caching_device" to specify which device to cache this variable read for.
		</comment>
		<comment id='5' author='lingz' date='2016-09-23T21:39:30Z'>
		Yes but if there are multiple GPU cards and you want to duplicate the same weights for the model on all of them, but allow each to have a local copy of the variable cached. So in this case the beat approach would be to use identity to force a local copy right? Or is there another approach.
		</comment>
		<comment id='6' author='lingz' date='2017-01-27T19:56:42Z'>
		Closing due to lack of recent activity, please reopen if you have further. questions. Also since this involves usage best practices, Stack Overflow is more appropriate for this question.
		</comment>
	</comments>
</bug>