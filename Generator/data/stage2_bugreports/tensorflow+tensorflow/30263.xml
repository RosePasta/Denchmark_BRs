<bug id='30263' author='Slacker-WY' open_date='2019-06-30T21:35:16Z' closed_time='2019-07-07T15:52:17Z'>
	<summary>[TF2.0]: Skipping optimization due to error while loading function</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window 10
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 2.0.0-beta1
Python version: 3.6.0


I'm trying to reproduce the results from the tutorial example about "Text classification with an RNN" provided by Tensorflow at: &lt;denchmark-link:url&gt;https://www.tensorflow.org/beta/tutorials/text/text_classification_rnn&lt;/denchmark-link&gt;

However, this warning message constantly appears that shows "skipping optimization due to error while loading function libraries: Invalid argument: ... "
I tried other optimizers, and LSTM or GRU architectures but nothing changes!
&lt;denchmark-link:https://user-images.githubusercontent.com/52288474/60402198-595ade00-9b5a-11e9-837f-d60f0518f2d0.jpg&gt;&lt;/denchmark-link&gt;

Code to reproduce the issue
from __future__ import absolute_import, division, print_function, unicode_literals
import tensorflow as tf
print(tf.__version__)
import tensorflow_datasets as tfds

# plot result
import matplotlib.pyplot as plt
def plot_graph(histroy, string):
    plt.plot(histroy.history[string])
    plt.plot(histroy.history['val_' + string])
    plt.xlabel('Epochs')
    plt.ylabel('string')
    plt.legend([string, 'val_'+string])
    plt.show()

# See available datasets
# print(tfds.list_builders())
dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True,
                          as_supervised=True)
train_dataset, test_dataset = dataset['train'], dataset['test']
tokenizer = info.features['text'].encoder
# print('Vocabulary size: {}'.format(tokenizer.vocab_size))
# sample_string = 'TensorFlow is cool'
# tokenized_string = tokenizer.encode(sample_string)
# print ('Tokenized string is {}'.format(tokenized_string))
# original_string = tokenizer.decode(tokenized_string)
# print ('The original string: {}'.format(original_string))
# assert original_string == sample_string
# for ts in tokenized_string:
#     print('{} -------&gt; {}'.format(ts, tokenizer.decode([ts])))

BUFFER_SIZE = 10000
BATCH_SIZE = 64
train_dataset = train_dataset.shuffle(BUFFER_SIZE)
train_dataset = train_dataset.padded_batch(BATCH_SIZE, train_dataset.output_shapes)
test_dataset = test_dataset.padded_batch(BATCH_SIZE,test_dataset.output_shapes)

# Build the model
EM_SIZE = 64
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(tokenizer.vocab_size, 64),
    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64)),
    tf.keras.layers.Dense(64, activation= 'relu'),
    tf.keras.layers.Dense(1,activation = 'sigmoid')
])
model.compile(loss= 'mse',
            optimizer= 'sgd',
            metrics=['accuracy'])
history = model.fit(train_dataset, epochs = 1, validation_data=test_dataset)
test_loss, test_acc = model.evaluate(test_dataset)
print('Test Loss: {}'.format(test_loss))
print('Test Accuracy: {}'.format(test_acc))
It seems that many other users are experiencing similar issues on TF2.0-beta
	</description>
	<comments>
		<comment id='1' author='Slacker-WY' date='2019-07-01T09:10:18Z'>
		I am able to execute the code provided by you  without any errors in TF 2.0-beta1 version.Can you check once and let me know is this still an issue?.Thanks!
		</comment>
		<comment id='2' author='Slacker-WY' date='2019-07-01T10:39:11Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
, I tried running the code and I have the following warning/error print-outs at each and every iteration:
&lt;denchmark-code&gt;Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_gru_973_1109_specialized_for_SGD_gradients_bidirectional_StatefulPartitionedCall_1_grad_StatefulPartitionedCall_at___inference_keras_scratch_graph_2955' and '__inference___backward_standard_gru_1969_2558' both implement 'gru_fd36615e-65df-4b45-9bd7-8903b889b94c' but their signatures do not match.
&lt;/denchmark-code&gt;


OS Platform: Linux Mint 19.1
Python version: 3.6.8
TensorFlow version: 2.0-beta1 (installed from pip, with gpu support)
GPU enabled (Nvidia Quadro P1000, with CUDA 10.0, properly detected and used by TensorFlow)

		</comment>
		<comment id='3' author='Slacker-WY' date='2019-07-01T10:41:38Z'>
		Additionally, the loss and accuracy do not seem to change significantly as training goes - it therefore seems that back-propagation is actually not performed as it should.
		</comment>
		<comment id='4' author='Slacker-WY' date='2019-07-01T10:54:28Z'>
		Finally, if I disable the use of the GPU, the E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] constant folding failed: Invalid argument: Unsupported type: 21 error also shows up at each iteration.
So, there definitely is an issue here.
		</comment>
		<comment id='5' author='Slacker-WY' date='2019-07-01T17:56:38Z'>
		Experiencing the same issue, any solution?
		</comment>
		<comment id='6' author='Slacker-WY' date='2019-07-02T09:42:34Z'>
		&lt;denchmark-link:https://github.com/Slacker-WY&gt;@Slacker-WY&lt;/denchmark-link&gt;
 I am able to execute the code provided by you without any errors in TF 2.0-GPU-beta1 version.Please, find the &lt;denchmark-link:https://colab.sandbox.google.com/drive/16APmqQgZQFHg0MZRcJTHmSOwPbbKdm2U#scrollTo=fKivRIWC5x5i&gt;file&lt;/denchmark-link&gt;
 for your reference .Please, let me know if i miss something .
		</comment>
		<comment id='7' author='Slacker-WY' date='2019-07-02T10:26:17Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
 I think you are missing the fact that although it reportedly works on your machine, it does not on a bunch of others. This bug has been experienced by multiple users, and I just ran the exact same code again in TF 2.0-GPU-beta1, which yields the same issue as that first reported by &lt;denchmark-link:https://github.com/Slacker-WY&gt;@Slacker-WY&lt;/denchmark-link&gt;
.
I get that TF is a very large (and, let me stress it out, pretty amazing) project receiving a huge number of Issues, part of which are more about user-related bugs than actual software problems, but systematically answering "it works on my computer" to try and close issues when there are multiple reports of an issue - whose not showing up on your machine actually underlines (in my humble opinion) that there is something tricky to investigate - does not feel like a very good software maintenance policy.
Again, TF 2.0 is a big release with a lot of work and changes, it is bound to include some issues and that's what beta versions are for, but please stop denying that there is something broken here - similar issues have been raised for about a month now, and we are talking about Google's own tutorial code on something "basic" (in terms of use - I totally understand that the underlying mechanics and implementation are not exactly simple) not working (at all) on a number of configurations. It might be due to an unnoticed detail (GPU driver version, support of a given type of CPU instructions... I have honestly no idea), or to an actual software issue (by the way, I tested that the code runs properly - except for a compatibility issue with using test_dataset as validation_data in model.fit - on the same system using TF 1.14-GPU in a different virtual environment, so TF 2.0 really seems to be at fault). At any rate, it really seems worth investigating by TF developers.
		</comment>
		<comment id='8' author='Slacker-WY' date='2019-07-02T13:19:41Z'>
		
I am able to execute the code provided by you without any errors in TF 2.0-beta1 version.Can you check once and let me know is this still an issue?.Thanks!

Hi, still don't work. The issue is still there.
		</comment>
		<comment id='9' author='Slacker-WY' date='2019-07-04T15:54:17Z'>
		Hi all,
apparently seems like the issue (which I had too) is correlated with the activation function of the LSTM/GRU/RNN layer. By changing activation from default 'tanh' to 'sigmoid' the warning disappeared. Hope this will help fixing the problem.
Have a good day!
FYI I run tensorflow-2.0.0-beta1 on CPU
		</comment>
		<comment id='10' author='Slacker-WY' date='2019-07-04T18:14:09Z'>
		
Hi all,
apparently seems like the issue (which I had too) is correlated with the activation function of the LSTM/GRU/RNN layer. By changing activation from default 'tanh' to 'sigmoid' the warning disappeared. Hope this will help fixing the problem.
Have a good day!
FYI I run tensorflow-2.0.0-beta1 on CPU

Thanks for your help. I reran the code as per your suggestion, and the "skipping optimization..." issue does disappear. However, the issue as reported in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/28626&gt;#28626&lt;/denchmark-link&gt;
 still exists.
		</comment>
		<comment id='11' author='Slacker-WY' date='2019-07-05T10:26:18Z'>
		
By changing activation from default 'tanh' to 'sigmoid' the warning disappeared.

Same here, thanks &lt;denchmark-link:https://github.com/Armyke&gt;@Armyke&lt;/denchmark-link&gt;
! This is good news in the sense that not everything is broken, but still something worth looking into for developers...

However, the issue as reported in #28626 still exists.

Again, same for me as for &lt;denchmark-link:https://github.com/Slacker-WY&gt;@Slacker-WY&lt;/denchmark-link&gt;
, and yielding a major slowdown in the training process (&gt; 20 seconds / batch, when the same operation in TF 1.14 on the same computer runs at about 2 seconds / batch).
		</comment>
		<comment id='12' author='Slacker-WY' date='2019-07-06T02:25:52Z'>
		Can also confirm. Switching LSTM activation to "sigmoid" stopped the "skipping optimization" issue, but "Unsupported type: 21" appears in that case.
Further, by switching to sigmoid the model trains about half as fast. However, leaving it at the default tanh does not seem to affect validation convergence (backprop is still working).
I'm running tensorflow-gpu 2.0.0-beta1 on Windows 10 with a 2080 Ti.
This is what prints when I set LSTM activation to tanh (the default):
2019-07-05 19:20:02.138247: W tensorflow/core/grappler/optimizers/implementation_selector.cc:199] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_374_550_specialized_for_Nadam_gradients_recurrent1_StatefulPartitionedCall_grad_StatefulPartitionedCall_at___inference_keras_scratch_graph_2911' and '__inference___backward_standard_lstm_974_1476' both implement 'lstm_3f0d94f5-99f8-4499-8983-151b467a13d9' but their signatures do not match.
		</comment>
		<comment id='13' author='Slacker-WY' date='2019-07-06T02:45:58Z'>
		I am having this issue currently as well with 2.0.0 beta 1 on windows 10. I get both 'Unsupported type 21' and 'skipping optimization'. Strangely this issue occurs in Visual Studio Code, but does not occur in Spyder using same exact script and Conda environment. Maybe this will help solve the issue...
		</comment>
		<comment id='14' author='Slacker-WY' date='2019-07-07T15:52:17Z'>
		Hi all:
This warning message is a red herring and can be ignored. It is raised when we do the implementation selection for RNN based on the availability of GPU for cudnn kernel. It is already suppressed in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/d8379699d3cf5e951e03e70fcc5335726955f260&gt;d837969&lt;/denchmark-link&gt;
.
Btw, you shouldn't change to use other activation function just to avoid this warning.
Constant folding might be some other issue, but should affect how the final graph.
All the warning/error message here is in the graph rewrite stage, which is an optimization process. If the optimazation step failed, it will still use the unoptimized graph, which is still a valid one.
		</comment>
		<comment id='15' author='Slacker-WY' date='2019-07-07T15:52:20Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=30263&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=30263&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='16' author='Slacker-WY' date='2019-07-07T18:25:25Z'>
		
This warning message is a red herring and can be ignored. It is raised when we do the implementation selection for RNN based on the availability of GPU for cudnn kernel. It is already suppressed in d837969.

Hi, I don't think this warning message can be ignored and it directly affects the model training. Because unlike the training results shown in the Tensorflow tutorial example,  the training accuracy of mine running exactly the same code doesn't improve at all even after more than 10 min training (as shown below).
&lt;denchmark-link:https://user-images.githubusercontent.com/52288474/60772310-7e070680-a0c2-11e9-8004-b89c1567d8ad.jpg&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='17' author='Slacker-WY' date='2019-08-13T16:55:36Z'>
		Has this problem been solved?
		</comment>
		<comment id='18' author='Slacker-WY' date='2019-08-13T20:43:24Z'>
		Sort of. The error message no longer shows in the nightly builds, and there has been improvements on the performance dropouts related to not disabling Eager in 2.0. That being said, I personally still cross a lot of situations when disabling Eager (while running in 2.0 to benefit from the API cleanup) makes things smoother (or simply workable in some cases). On the overall, it feels like the beta is, well, a beta, and the 2.0b1 has a lot of issues that are being progressively fixed or improved and incorporated in the nightly builds.
		</comment>
		<comment id='19' author='Slacker-WY' date='2019-08-13T20:45:41Z'>
		&lt;denchmark-link:https://github.com/jiawei6636&gt;@jiawei6636&lt;/denchmark-link&gt;
 You might want to read and follow the newest messages on &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/29525&gt;#29525&lt;/denchmark-link&gt;
 - there appears to be further development as to this specific issue.
		</comment>
		<comment id='20' author='Slacker-WY' date='2019-08-20T14:16:37Z'>
		Hi, I got this warning message:

2019-08-20 21:53:30.063910: W tensorflow/core/grappler/optimizers/implementation_selector.cc:199] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_3015_3517_specialized_for_SGD_gradients_lstm_2_StatefulPartitionedCall_grad_StatefulPartitionedCall_at___inference_keras_scratch_graph_5622' and '__inference___backward_cudnn_lstm_1434_1610' both implement 'lstm_75a56c74-73e4-4a2e-b381-b637dae06737' but their signatures do not match.

2019-08-20 21:59:55.290415: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] constant folding failed: Invalid argument: Unsupported type: 21


System information:

Windows7 64 bit
python version: 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 23:09:28) [MSC v.1916 64 bit (AMD64)]
keras version: 2.2.4
tensorflow 2.0.0-beta1

I found:
if code like this, no 'type:21' appears：

model.add(LSTM(128, activation='relu', input_shape = (dim1, dim2)))

like this, 'type:21' appears：

model.add(LSTM(128, activation='relu', return_sequences=True, input_shape = (dim1, dim2)))
model.add(LSTM(128))

		</comment>
		<comment id='21' author='Slacker-WY' date='2019-08-24T17:23:01Z'>
		
@jiawei6636 You might want to read and follow the newest messages on #29525 - there appears to be further development as to this specific issue.

I install the tensorflow-gpu==2.0.0-rc0, but the issue still exist.
		</comment>
		<comment id='22' author='Slacker-WY' date='2019-08-24T17:26:46Z'>
		
@jiawei6636 You might want to read and follow the newest messages on #29525 - there appears to be further development as to this specific issue.

And some problems occur when I use tf.keras.optimizer.RMSProp() to optimize the LSTM model by a costum training loop like the tutorial. There is no convergence.
		</comment>
		<comment id='23' author='Slacker-WY' date='2019-08-26T15:37:23Z'>
		Same issue here.
		</comment>
		<comment id='24' author='Slacker-WY' date='2019-08-27T12:22:04Z'>
		I get the same warning as &lt;denchmark-link:https://github.com/freetiger20150000&gt;@freetiger20150000&lt;/denchmark-link&gt;
 with CPU-only tensorflow built from the r2.0 source on macOS.
		</comment>
		<comment id='25' author='Slacker-WY' date='2019-09-03T15:06:15Z'>
		Same error following NMT tutorial (copy and paste everything) using tensorflow2.0 RC on CPU
		</comment>
		<comment id='26' author='Slacker-WY' date='2019-09-10T09:29:04Z'>
		I'm facing with the same issue.
		</comment>
		<comment id='27' author='Slacker-WY' date='2019-09-10T16:02:49Z'>
		For any of you who is still facing this issue, please see my previous comment about this error in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/30263#issuecomment-509053940&gt;#30263 (comment)&lt;/denchmark-link&gt;
.
TLDR, this warning message is not an actual error, and can safely ignored. There is also a recent update to just suppress this warning message to avoid the confusion in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/a913689fddb70729dbce45a2cad44f4bd0f03935&gt;a913689&lt;/denchmark-link&gt;

		</comment>
		<comment id='28' author='Slacker-WY' date='2019-09-15T07:59:13Z'>
		
For any of you who is still facing this issue, please see my previous comment about this error in #30263 (comment).
TLDR, this warning message is not an actual error, and can safely ignored. There is also a recent update to just suppress this warning message to avoid the confusion in a913689

Does this issue lower performance? (if you're using Nvidia GPUs) Since I wonder if I would change the GRUs to CUDA optimized, it would speed up my training (which is really slow now).
		</comment>
		<comment id='29' author='Slacker-WY' date='2019-09-17T17:05:43Z'>
		&lt;denchmark-link:https://github.com/DanielWicz&gt;@DanielWicz&lt;/denchmark-link&gt;
, when you see this message, its means the graph has actually being optimized. In TF 2.0, the default GRU/LSTM layer is capable to auto select the CUDNN kernel, and provide better performance on GPU. You don't have to do switch to other CUDA optimized kernel.
		</comment>
		<comment id='30' author='Slacker-WY' date='2019-10-30T16:34:42Z'>
		
There is also a recent update to just suppress this warning message to avoid the confusion in a913689

&lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;
 it seems that your fix has not been incorporated to the released tf2.0.0, see &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/64c3d382cadf7bbe8e7e99884bede8284ff67f56/tensorflow/core/grappler/optimizers/implementation_selector.cc#L310&gt;here in the v2.0.0 tag&lt;/denchmark-link&gt;
. As a result, the warning still shows up. Do you know why your fix isn't in tf2.0.0 and when it will be deployed?
		</comment>
		<comment id='31' author='Slacker-WY' date='2019-10-30T18:32:48Z'>
		&lt;denchmark-link:https://github.com/durandg12&gt;@durandg12&lt;/denchmark-link&gt;
, ah, thanks for the notice. It seems that my change is only made to 1.15 and barely miss the 2.0 branch cut. I guess it will be released in 2.1 which will come out very soon. For now you can safely ignore the warning message.
		</comment>
		<comment id='32' author='Slacker-WY' date='2019-10-31T01:50:26Z'>
		So I was running tensorflow 2.0 in an ubuntu 18.04 docker container on a mac host. I was seeing the very same problem. All I did was I changed my activation from the default (which is tanh? ) to relu, and the optimization worked. I got lucky because relu works good enough for my purposes, but does anyone know a root cause for this issue?
		</comment>
		<comment id='33' author='Slacker-WY' date='2019-10-31T04:10:19Z'>
		&lt;denchmark-link:https://github.com/tommathewXC&gt;@tommathewXC&lt;/denchmark-link&gt;
, you shouldn't change the default activation function. When the default activation function is changed, the alternative cudnn kernel can't be used, which means there isn't any optimization at all. As I stated above, the warning message actually means the optimization is done, and can be safely ignored. I have change it to be a vlog to confuse less people, but it didn't reach 2.0.
		</comment>
		<comment id='34' author='Slacker-WY' date='2019-11-12T10:04:20Z'>
		I am not sure this is directly related, but I also get the 'Skipping optimization due to error while loading function libraries: Invalid argument: Functions(...)' message, and the bug appears to only affect RNN's.
The model trains just fine, however, when calling model.save(), only the initial configuration of the model is saved. That is, all biases are zero, and the weights maintain their original values according to the initializer. It appears as if the link between the weights on GPU and those in memory is broken...?
This only happens when the model contains RNN's. Convolutions, dense layers etc. do not instigate this bug (one RNN layer is enough to break the entire thing).
Setup:
-Win 10
-CUDA v10.0
-cuDNN 7.6
-Visual Studio
&lt;denchmark-link:https://user-images.githubusercontent.com/49571281/68660896-50334200-053a-11ea-8dc7-c723f86ba287.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='35' author='Slacker-WY' date='2020-02-11T15:26:50Z'>
		We had following set up:

cuda 10.0
cudnn 7.6.5
nvidia driver 410.79
tensorflow-gpu 2.0.0
GPU Tesla T4 with 16 GB RAM
CPU with 4 cores and 8 GB RAM (from 5 to 8)

Also, in our case some tensor-flow code were running in above setting while some were not. In our case, this was caused by insufficient RAM, increasing RAM size fixed the issue.
		</comment>
		<comment id='36' author='Slacker-WY' date='2020-05-09T11:02:52Z'>
		this issue is yet un resolved
		</comment>
		<comment id='37' author='Slacker-WY' date='2020-06-25T16:26:20Z'>
		I have the same issue as above (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/30263#issuecomment-509020630&gt;#30263 (comment)&lt;/denchmark-link&gt;
)

Warning: Skipping optimization due to error while loading function libraries....
Training and Val loss stays pretty much constant

Versions: Keras 2.2.4, TF 2.0.0
		</comment>
		<comment id='38' author='Slacker-WY' date='2020-11-09T16:21:17Z'>
		Still have this issue.
		</comment>
		<comment id='39' author='Slacker-WY' date='2020-11-14T00:27:44Z'>
		No CuDNN, but this solution works for me:
lstm_layer = keras.layers.RNN(keras.layers.LSTMCell(units), input_shape=(None, input_dim)
&lt;denchmark-code&gt;# CuDNN is only available at the layer level, and not at the cell level.
# This means `LSTM(units)` will use the CuDNN kernel,
# while RNN(LSTMCell(units)) will run on non-CuDNN kernel.
if allow_cudnn_kernel:
    # The LSTM layer with default options uses CuDNN.
    lstm_layer = keras.layers.LSTM(units, input_shape=(None, input_dim))
else:
    # Wrapping a LSTMCell in a RNN layer will not use CuDNN.
    lstm_layer = keras.layers.RNN(
        keras.layers.LSTMCell(units), input_shape=(None, input_dim)
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>