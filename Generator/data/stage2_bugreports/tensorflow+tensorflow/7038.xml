<bug id='7038' author='yaroslavvb' open_date='2017-01-24T17:06:20Z' closed_time='2017-02-27T18:04:57Z'>
	<summary>multiple dequeue ops are optimized away in latest TF</summary>
	<description>
Multiple ops Dequeue ops from the same queue started getting optimized away in latest TensorFlow:
The following executes dequeue once in Jan17 head, but 3 times in 12.1
sess.run([q.dequeue(), q.dequeue(), q.dequeue()]) 
One gets expected behavior (3 dequeues) when graph optimization is turned off
&lt;denchmark-code&gt;tf.ConfigProto(graph_options=tf.GraphOptions(optimizer_options=tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0)))
sess = tf.Session(config = config)
&lt;/denchmark-code&gt;

Self-contained repro: &lt;denchmark-link:https://github.com/yaroslavvb/stuff/blob/master/parallel_dequeue_test.py&gt;https://github.com/yaroslavvb/stuff/blob/master/parallel_dequeue_test.py&lt;/denchmark-link&gt;

Came up in &lt;denchmark-link:http://stackoverflow.com/questions/41830206/how-to-share-a-queue-containing-variable-length-sequences-batches-between-multip&gt;http://stackoverflow.com/questions/41830206/how-to-share-a-queue-containing-variable-length-sequences-batches-between-multip&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='yaroslavvb' date='2017-01-24T17:23:53Z'>
		cc &lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;
 the queue expert
		</comment>
		<comment id='2' author='yaroslavvb' date='2017-01-24T17:35:58Z'>
		Can you print the GraphDef for that program? In particular, I'd like to know if the FIFOQueueV2 (or some other ...QueueV2) op is being used.
		</comment>
		<comment id='3' author='yaroslavvb' date='2017-01-24T17:39:01Z'>
		Yes, it's using V2 ops for queue. The 12.1 version is using regular
&lt;denchmark-link:http://pastebin.com/gsu7xrU7&gt;http://pastebin.com/gsu7xrU7&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='yaroslavvb' date='2017-01-24T17:53:51Z'>
		&lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
: It looks like the switch to resource-typed queues has caused a subtle bug in some of the queue ops. As far as I can tell, multiple instances of  in the same subgraph are now being treated as common subexpressions, whereas before the fact that they had an incoming ref-typed edge meant they were &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/3570b5de07c8079d1005c7877e235bd8aa0e0ccc/tensorflow/core/graph/optimizer_cse.cc#L162&gt;never candidates for CSE&lt;/denchmark-link&gt;
.
I can see at least two solutions:

Modify the CSE optimizer to reject anything with a resource-typed input as a candidate for elimination.
Modify all of the Queue*V2 accessor ops to be "stateful", which also inhibits the optimization.

		</comment>
		<comment id='5' author='yaroslavvb' date='2017-01-31T19:16:21Z'>
		&lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/06e3aa655506773e49ce9a855f81ba3eae2a6b88#diff-333845d139890198962551b3a1c2a33b&gt;06e3aa6#diff-333845d139890198962551b3a1c2a33b&lt;/denchmark-link&gt;
 should have fixed this issue (it adds a test, even)
		</comment>
		<comment id='6' author='yaroslavvb' date='2017-02-27T18:00:47Z'>
		I just tried in tf1.0 and this behavior seems to still be here back. To reproduce, try this
&lt;denchmark-code&gt;import tensorflow as tf
input1 = tf.train.range_input_producer(10, shuffle=False)
input1_batch = [input1.dequeue(),input1.dequeue()]

config = tf.ConfigProto()
#config = tf.ConfigProto(graph_options=tf.GraphOptions(optimizer_options=tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0)))
input2 = tf.train.batch([input1_batch], batch_size=4, enqueue_many=True)

sess = tf.InteractiveSession(config=config)
tf.train.start_queue_runners()

for i in range(5):
    print(sess.run(input2))
&lt;/denchmark-code&gt;

Result is
&lt;denchmark-code&gt;[0 0 1 1]
[2 2 3 3]
[4 4 5 5]
[6 6 7 7]
[8 8 9 9]
&lt;/denchmark-code&gt;

If you uncomment "#config" line to remove rewriting optimization, then result is as expected
&lt;denchmark-code&gt;[0 1 3 2]
[5 4 6 7]
[8 9 1 0]
[2 3 4 5]
[7 6 9 8]
&lt;/denchmark-code&gt;

Version info:
&lt;denchmark-code&gt;&gt;&gt;&gt; tf.__version__
'1.0.0'

&gt;&gt;&gt; sys.version
'3.5.3 |Continuum Analytics, Inc.| (default, Feb 22 2017, 20:51:01) \n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]'

&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='yaroslavvb' date='2017-02-27T18:04:57Z'>
		It seems like the commit which fixed this bug didn't make the cut into the 1.0 release. master is fixed, and so will 1.1.
		</comment>
		<comment id='8' author='yaroslavvb' date='2017-02-27T18:05:19Z'>
		If you need the fix by itself cherry-pick &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/06e3aa655506773e49ce9a855f81ba3eae2a6b88#diff-333845d139890198962551b3a1c2a33b&gt;06e3aa6#diff-333845d139890198962551b3a1c2a33b&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='yaroslavvb' date='2017-02-27T18:10:59Z'>
		Got it, I forgot that releases are non-linear (it worked in version I built before TF 1.0 was released)
		</comment>
	</comments>
</bug>