<bug id='5543' author='DavidNorman' open_date='2016-11-11T11:50:59Z' closed_time='2017-02-15T10:55:39Z'>
	<summary>Constant folding doesn't remove control edges</summary>
	<description>
I believe that when constant folding takes place, and a section of a graph is replaced by a constant, that only the data output edge of the replaced node is removed.
I believe that I can see that a graph of nodes ending up in a Div (part of the gradient generation bit) is replaced by a Const.  The output of the Div goes to a Mul, and this is changed to the new const correctly.
However, there is a control output from the Div going to a Const (not sure why, but it is).  This is not changed to the Div replacement.  Consequently the dead node pruning doesn't remove the original Div.
Here is some trace:
During the constant folding:
Graph Before #nodes 67 #edges 109
Graph Constant graph #nodes 32 #edges 42
Constant foldable 32 : 67
Replacing {name:'gradients/Mean_grad/truediv' id:28 op device:{/job:localhost/replica:0/task:0/device:ipu:0} def:{gradients/Mean_grad/truediv = Div[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:ipu:0"](gradients/Mean_grad/Tile, gradients/Mean_grad/Cast)}} :: 0 with a constant
Replacing edge to gradients/Square_grad/mul_1:0
During the post constant folding pruning:
PruneForReverseReachability: gradients/Square_grad/mul_1 &lt;- gradients/Mean_grad/truediv/_0__cf__1
PruneForReverseReachability: gradients/Square_grad/mul/x &lt;- gradients/Mean_grad/truediv
After the pruning:
Graph ConstFolding #nodes 68 #edges 110
When the graph is passed to the device, it contains:
Node: gradients/Square_grad/mul/x = Const&lt;denchmark-link:%5Egradients/Mean_grad/truediv&gt;dtype=DT_FLOAT, value=Tensor&lt;type: float shape: [] values: 2&gt;, _device="/job:localhost/replica:0/task:0/device:ipu:0"&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='DavidNorman' date='2016-11-11T13:27:17Z'>
		Example:
&lt;denchmark-code&gt;import tensorflow as tf

# Creates a graph.
a = tf.placeholder(tf.float32, [2,2])
b = tf.placeholder(tf.float32, [2,2])
c = tf.constant([[2.0,2.0],[2.0,2.0]])
d = tf.constant([[2.0,2.0],[2.0,2.0]])
e = tf.add(c, d)
with tf.control_dependencies([e]):
  f = tf.mul(a, a)
  g = tf.mul(b, e)
  o = tf.add(f, g)

sess = tf.Session()

fd = {
  a: [[1.0,2.0],[3.0,4.0]],
  b: [[1.0,0.0],[0.0,1.0]],
}

sess.run(o, feed_dict=fd)
&lt;/denchmark-code&gt;

During constant folding this leads to:
&lt;denchmark-code&gt;Graph Before #nodes 11 #edges 16
|| 
|| () -&gt; () {
||   n2 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:ipu:0", send_device="/job:localhost/replica:0/task:0/cpu:0", send_device_incarnation=1, tensor_name="edge_16__recv_Placeholder_0", tensor_type=float]()
||   n3 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:ipu:0", send_device="/job:localhost/replica:0/task:0/cpu:0", send_device_incarnation=1, tensor_name="edge_17__recv_Placeholder_1_0", tensor_type=float]()
||   n4 = Const[dtype=float, value=Tensor&lt;type: float shape: [2,2] values: 2 2 2...&gt;]()
||   n5 = Const[dtype=float, value=Tensor&lt;type: float shape: [2,2] values: 2 2 2...&gt;]()
||   n6 = Add[T=float](n4, n5)
||   n7 = Mul[T=float](n2, n2) @ n6
||   n8 = Mul[T=float](n3, n6)
||   n9 = Add[T=float](n7, n8)
||   n10 = _Send[T=float, client_terminated=false, recv_device="/job:localhost/replica:0/task:0/cpu:0", send_device="/job:localhost/replica:0/task:0/device:ipu:0", send_device_incarnation=1, tensor_name="edge_5_Add_1"](n9)
|| }
|| 


Graph After #nodes 12 #edges 17
|| 
|| () -&gt; () {
||   n11 = Const[dtype=float, value=Tensor&lt;type: float shape: [2,2] values: 4 4 4...&gt;]()
||   n2 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:ipu:0", send_device="/job:localhost/replica:0/task:0/cpu:0", send_device_incarnation=1, tensor_name="edge_16__recv_Placeholder_0", tensor_type=float]()
||   n3 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:ipu:0", send_device="/job:localhost/replica:0/task:0/cpu:0", send_device_incarnation=1, tensor_name="edge_17__recv_Placeholder_1_0", tensor_type=float]()
||   n4 = Const[dtype=float, value=Tensor&lt;type: float shape: [2,2] values: 2 2 2...&gt;]()
||   n5 = Const[dtype=float, value=Tensor&lt;type: float shape: [2,2] values: 2 2 2...&gt;]()
||   n8 = Mul[T=float](n3, n11)
||   n6 = Add[T=float](n4, n5)
||   n7 = Mul[T=float](n2, n2) @ n6
||   n9 = Add[T=float](n7, n8)
||   n10 = _Send[T=float, client_terminated=false, recv_device="/job:localhost/replica:0/task:0/cpu:0", send_device="/job:localhost/replica:0/task:0/device:ipu:0", send_device_incarnation=1, tensor_name="edge_5_Add_1"](n9)
|| }
|| 
&lt;/denchmark-code&gt;

You can see that there is a constant replacing the Add, but that the Add remains, because it is referred to by the control edge.
I think that the output should be:
&lt;denchmark-code&gt;Graph After #nodes 12 #edges 17
|| 
|| () -&gt; () {
||   n11 = Const[dtype=float, value=Tensor&lt;type: float shape: [2,2] values: 4 4 4...&gt;]()
||   n2 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:ipu:0", send_device="/job:localhost/replica:0/task:0/cpu:0", send_device_incarnation=1, tensor_name="edge_16__recv_Placeholder_0", tensor_type=float]()
||   n3 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:ipu:0", send_device="/job:localhost/replica:0/task:0/cpu:0", send_device_incarnation=1, tensor_name="edge_17__recv_Placeholder_1_0", tensor_type=float]()
||   n8 = Mul[T=float](n3, n11)
||   n7 = Mul[T=float](n2, n2) @ n11
||   n9 = Add[T=float](n7, n8)
||   n10 = _Send[T=float, client_terminated=false, recv_device="/job:localhost/replica:0/task:0/cpu:0", send_device="/job:localhost/replica:0/task:0/device:ipu:0", send_device_incarnation=1, tensor_name="edge_5_Add_1"](n9)
|| }
|| 
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='DavidNorman' date='2016-11-11T13:41:13Z'>
		One more example.  This is the 'getting_started.py' that is in the TF documentation.  I have modified it so that it uses placeholders, rather than allowing the X and Y data to be constants.
&lt;denchmark-code&gt;Graph Before #nodes 67 #edges 109
|| 
|| () -&gt; () {
||   n2 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:ipu:0", send_device="/job:localhost/replica:0/task:0/cpu:0", send_device_incarnation=1, tensor_name="edge_110__recv_Placeholder_1_0", tensor_type=float]()
||   n3 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:ipu:0", send_device="/job:localhost/replica:0/task:0/cpu:0", send_device_incarnation=1, tensor_name="edge_109__recv_Placeholder_0", tensor_type=float]()
||   n4 = Variable[container="", dtype=float, shape=[1], shared_name=""]()
||   n7 = Variable[container="", dtype=float, shape=[1], shared_name=""]()
||   n11 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [0] values: &gt;]()
||   n12 = Const[dtype=float, value=Tensor&lt;type: float shape: [] values: 1&gt;]()
||   n14 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 1&gt;]()
||   n16 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 100&gt;]()
||   n18 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 100&gt;]()
||   n19 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [0] values: &gt;]()
||   n20 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 0&gt;]()
||   n22 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 0&gt;]()
||   n24 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [] values: 1&gt;]()
||   n32 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 100&gt;]()
||   n33 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 100&gt;]()
||   n42 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 100&gt;]()
||   n43 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 1&gt;]()
||   n54 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 1&gt;]()
||   n55 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 100&gt;]()
||   n63 = Const[dtype=float, value=Tensor&lt;type: float shape: [] values: 0.5&gt;]()
||   n5 = Identity[T=float, _class=["loc:@Variable"]](n4)
||   n8 = Identity[T=float, _class=["loc:@Variable_1"]](n7)
||   n13 = Fill[T=float](n11, n12)
||   n21 = Prod[T=int32, Tidx=int32, keep_dims=false](n18, n20)
||   n23 = Prod[T=int32, Tidx=int32, keep_dims=false](n19, n22)
||   n34 = BroadcastGradientArgs[T=int32](n32, n33)
||   n44 = BroadcastGradientArgs[T=int32](n42, n43)
||   n56 = BroadcastGradientArgs[T=int32](n54, n55)
||   n6 = Mul[T=float](n5, n3)
||   n15 = Reshape[T=float, Tshape=int32](n13, n14)
||   n25 = Maximum[T=int32](n23, n24)
||   n9 = Add[T=float](n6, n8)
||   n17 = Tile[T=float, Tmultiples=int32](n15, n16)
||   n26 = Div[T=int32](n21, n25)
||   n10 = Sub[T=float](n9, n2)
||   n27 = Cast[DstT=float, SrcT=int32](n26)
||   n28 = Div[T=float](n17, n27)
||   n29 = Const[dtype=float, value=Tensor&lt;type: float shape: [] values: 2&gt;]() @ n28
||   n30 = Mul[T=float](n29, n10)
||   n31 = Mul[T=float](n28, n30)
||   n35 = Sum[T=float, Tidx=int32, keep_dims=false](n31, n34)
||   n37 = Sum[T=float, Tidx=int32, keep_dims=false](n31, n34:1)
||   n36 = Reshape[T=float, Tshape=int32](n35, n32)
||   n38 = Neg[T=float](n37)
||   n39 = Reshape[T=float, Tshape=int32](n38, n33)
||   n40 = NoOp() @ n36, n39
||   n41 = Identity[T=float, _class=["loc:@gradients/sub_grad/Reshape"]](n36) @ n40
||   n45 = Sum[T=float, Tidx=int32, keep_dims=false](n41, n44)
||   n47 = Sum[T=float, Tidx=int32, keep_dims=false](n41, n44:1)
||   n46 = Reshape[T=float, Tshape=int32](n45, n42)
||   n48 = Reshape[T=float, Tshape=int32](n47, n43)
||   n49 = NoOp() @ n46, n48
||   n50 = Identity[T=float, _class=["loc:@gradients/add_grad/Reshape"]](n46) @ n49
||   n53 = Identity[T=float, _class=["loc:@gradients/add_grad/Reshape_1"]](n48) @ n49
||   n51 = Mul[T=float](n50, n3)
||   n52 = Mul[T=float](n5, n50)
||   n65 = ApplyGradientDescent[T=float, _class=["loc:@Variable_1"], use_locking=false](n7, n63, n53)
||   n57 = Sum[T=float, Tidx=int32, keep_dims=false](n51, n56)
||   n59 = Sum[T=float, Tidx=int32, keep_dims=false](n52, n56:1)
||   n58 = Reshape[T=float, Tshape=int32](n57, n54)
||   n60 = Reshape[T=float, Tshape=int32](n59, n55)
||   n61 = NoOp() @ n58, n60
||   n62 = Identity[T=float, _class=["loc:@gradients/mul_grad/Reshape"]](n58) @ n61
||   n64 = ApplyGradientDescent[T=float, _class=["loc:@Variable"], use_locking=false](n4, n63, n62)
||   n66 = NoOp() @ n64, n65
|| }
|| 


Replacing {name:'gradients/Mean_grad/truediv' id:28 op device:{/job:localhost/replica:0/task:0/device:ipu:0} def:{gradients/Mean_grad/truediv = Div[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:ipu:0"](gradients/Mean_grad/Tile, gradients/Mean_grad/Cast)}} :: 0 with a constant

Graph After #nodes 68 #edges 110
|| 
|| () -&gt; () {
||   n67 = Const[dtype=float, value=Tensor&lt;type: float shape: [100] values: 0.01 0.01 0.01...&gt;]()
||   n2 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:ipu:0", send_device="/job:localhost/replica:0/task:0/cpu:0", send_device_incarnation=1, tensor_name="edge_110__recv_Placeholder_1_0", tensor_type=float]()
||   n3 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:ipu:0", send_device="/job:localhost/replica:0/task:0/cpu:0", send_device_incarnation=1, tensor_name="edge_109__recv_Placeholder_0", tensor_type=float]()
||   n4 = Variable[container="", dtype=float, shape=[1], shared_name=""]()
||   n7 = Variable[container="", dtype=float, shape=[1], shared_name=""]()
||   n11 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [0] values: &gt;]()
||   n12 = Const[dtype=float, value=Tensor&lt;type: float shape: [] values: 1&gt;]()
||   n14 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 1&gt;]()
||   n16 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 100&gt;]()
||   n18 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 100&gt;]()
||   n19 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [0] values: &gt;]()
||   n20 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 0&gt;]()
||   n22 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 0&gt;]()
||   n24 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [] values: 1&gt;]()
||   n32 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 100&gt;]()
||   n33 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 100&gt;]()
||   n42 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 100&gt;]()
||   n43 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 1&gt;]()
||   n54 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 1&gt;]()
||   n55 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 100&gt;]()
||   n63 = Const[dtype=float, value=Tensor&lt;type: float shape: [] values: 0.5&gt;]()
||   n5 = Identity[T=float, _class=["loc:@Variable"]](n4)
||   n8 = Identity[T=float, _class=["loc:@Variable_1"]](n7)
||   n13 = Fill[T=float](n11, n12)
||   n21 = Prod[T=int32, Tidx=int32, keep_dims=false](n18, n20)
||   n23 = Prod[T=int32, Tidx=int32, keep_dims=false](n19, n22)
||   n34 = BroadcastGradientArgs[T=int32](n32, n33)
||   n44 = BroadcastGradientArgs[T=int32](n42, n43)
||   n56 = BroadcastGradientArgs[T=int32](n54, n55)
||   n6 = Mul[T=float](n5, n3)
||   n15 = Reshape[T=float, Tshape=int32](n13, n14)
||   n25 = Maximum[T=int32](n23, n24)
||   n9 = Add[T=float](n6, n8)
||   n17 = Tile[T=float, Tmultiples=int32](n15, n16)
||   n26 = Div[T=int32](n21, n25)
||   n10 = Sub[T=float](n9, n2)
||   n27 = Cast[DstT=float, SrcT=int32](n26)
||   n28 = Div[T=float](n17, n27)
||   n29 = Const[dtype=float, value=Tensor&lt;type: float shape: [] values: 2&gt;]() @ n28
||   n30 = Mul[T=float](n29, n10)
||   n31 = Mul[T=float](n67, n30)
||   n35 = Sum[T=float, Tidx=int32, keep_dims=false](n31, n34)
||   n37 = Sum[T=float, Tidx=int32, keep_dims=false](n31, n34:1)
||   n36 = Reshape[T=float, Tshape=int32](n35, n32)
||   n38 = Neg[T=float](n37)
||   n39 = Reshape[T=float, Tshape=int32](n38, n33)
||   n40 = NoOp() @ n36, n39
||   n41 = Identity[T=float, _class=["loc:@gradients/sub_grad/Reshape"]](n36) @ n40
||   n45 = Sum[T=float, Tidx=int32, keep_dims=false](n41, n44)
||   n47 = Sum[T=float, Tidx=int32, keep_dims=false](n41, n44:1)
||   n46 = Reshape[T=float, Tshape=int32](n45, n42)
||   n48 = Reshape[T=float, Tshape=int32](n47, n43)
||   n49 = NoOp() @ n46, n48
||   n50 = Identity[T=float, _class=["loc:@gradients/add_grad/Reshape"]](n46) @ n49
||   n53 = Identity[T=float, _class=["loc:@gradients/add_grad/Reshape_1"]](n48) @ n49
||   n51 = Mul[T=float](n50, n3)
||   n52 = Mul[T=float](n5, n50)
||   n65 = ApplyGradientDescent[T=float, _class=["loc:@Variable_1"], use_locking=false](n7, n63, n53)
||   n57 = Sum[T=float, Tidx=int32, keep_dims=false](n51, n56)
||   n59 = Sum[T=float, Tidx=int32, keep_dims=false](n52, n56:1)
||   n58 = Reshape[T=float, Tshape=int32](n57, n54)
||   n60 = Reshape[T=float, Tshape=int32](n59, n55)
||   n61 = NoOp() @ n58, n60
||   n62 = Identity[T=float, _class=["loc:@gradients/mul_grad/Reshape"]](n58) @ n61
||   n64 = ApplyGradientDescent[T=float, _class=["loc:@Variable"], use_locking=false](n4, n63, n62)
||   n66 = NoOp() @ n64, n65
|| }
|| 

&lt;/denchmark-code&gt;

Node n29, which has a control dependency on n28 is the problem.  n28 was the Div which was replaced.
		</comment>
		<comment id='3' author='DavidNorman' date='2016-11-11T18:57:06Z'>
		&lt;denchmark-link:https://github.com/keveman&gt;@keveman&lt;/denchmark-link&gt;
 would you please comment? Thanks.
		</comment>
		<comment id='4' author='DavidNorman' date='2016-11-14T09:50:07Z'>
		I should point out that if I bind the nodes to the CPU, then the results are the same.
(these examples are for my own device back end)
		</comment>
		<comment id='5' author='DavidNorman' date='2016-11-14T10:09:01Z'>
		It seems that constant_folding.cc, ReplaceTensorWithConstant, assumes that the node that will be replaced will have multiple outputs, and only replaces those edges that are fed by the one replaced output.
BroadcastGradientArgs is a good example.  It really needs to be replaced by 2 constants, and the code, as it stands, works well for that.
Perhaps it is worth considering what it means to have a node that only has a control output assigned to it?  Is this valid?  If not, then the control output should be eliminated, then then the dead code pruning algorithm will kill off that node and it's upstream nodes.
I've not played with the control flow (loops, conditionals), so I don't know if nodes with only control outputs are useful.   I would expect that loops and conditonals are done with Boolean edges, not control edges.  If this is the case, then I think that adding the control edge removal thing is the correct thing to do.
		</comment>
		<comment id='6' author='DavidNorman' date='2016-11-14T11:17:48Z'>
		I am also wondering what is the rationale behind the memory constraints in the ReplaceTensorWithConstant:
&lt;denchmark-code&gt;  // 1) If the destination tensor is not an int32 tensor, and has HOST_MEMORY
  // constraint, do not replace it.
  // 2) If the destination tensor is an int32 tensor, but has DEVICE_MEMORY
  // constraint, do not replace it.
&lt;/denchmark-code&gt;

Why would you want to keep nodes that perform arithmetic on a device unnecessarily?  I can imagine that you might consider that it is important to do floating point arithmetic on the target platform because of differences in rounding (although on a neural network I would hope that the result would work around that).
In the case of integers, I can't decide why constant folding would always take place. Could you describe why it would be limited to HOST_MEMORY only?
		</comment>
		<comment id='7' author='DavidNorman' date='2016-11-14T11:24:08Z'>
		Here is a diff for trying out 'remove control outputs from nodes that have only a control output after constant folding' change:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/589083/diff.txt&gt;diff.txt&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='DavidNorman' date='2016-11-18T23:36:38Z'>
		Sorry for the late response. Constant folding is conservative (perhaps overly) in what it replaces. In your case, since the original node had an outgoing control dependence, it didn't get replaced. Note that the analysis to decide whether that is safe to do is somewhat non-trivial. We have to determine whether that node is control dependent on a stateful node. We chose to let it be conservative. If you have a real example where the performance is poor because of this conservativeness, I'll be happy to take a look.
		</comment>
		<comment id='9' author='DavidNorman' date='2016-11-21T10:01:45Z'>
		I see the problem.  My case isn't really a performance issue, per se. I am trying to implement a device on TF for our new hardware.  The extra nodes of integer math are getting in the way of the initial bring-up.
If there are stateful nodes in the graph, then why does the constant tree finder consider them acceptable to be within the constant tree?  Isn't the constant that is generated (there is one generated) is not valid after the 1st run through the graph?  In which case, the constant should not replace that section of the graph at all?
		</comment>
		<comment id='10' author='DavidNorman' date='2016-11-24T09:54:21Z'>
		I realize i failed to write sensible language in that last message.
What I intended to say is that replacing a stateful node with a constant doesn't seem right.  If the stateful node changes state, then the constant is invalid. Either the stateful node should not be included in the constant sub-graph, or the constant sub-graph should be re-evaluated whenever the stateful node changes, in which case there doesn't seem to be a lot of point in having it.
		</comment>
		<comment id='11' author='DavidNorman' date='2016-11-24T11:44:57Z'>
		Hi,
In fact, I'm pretty certain that the following code taken from constant_folding.cc will prevent the constant sub-graph from containing any stateful nodes.  In which case, it should be ok to apply the control edge removal.
&lt;denchmark-code&gt;bool IsConstantFoldable(const FunctionLibraryDefinition* flib_def,
                        const Node* n,
                        std::function&lt;bool(const Node*)&gt; consider) {
  if (n-&gt;op_def().is_stateful()) {
    return false;
  }
&lt;/denchmark-code&gt;

		</comment>
		<comment id='12' author='DavidNorman' date='2016-12-08T20:29:37Z'>
		Started looking at this. Will have a fix soon.
		</comment>
		<comment id='13' author='DavidNorman' date='2017-02-15T10:55:39Z'>
		Now that my own device is an XLA device, I am no longer concerned about this.
However this is still a problem as far as I can see.  unless the dead code removal is removing nodes that only have a control output.
		</comment>
	</comments>
</bug>