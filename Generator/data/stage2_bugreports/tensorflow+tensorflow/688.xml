<bug id='688' author='bas-aarts' open_date='2016-01-05T02:49:02Z' closed_time='2016-01-12T20:28:08Z'>
	<summary>deadlock running MNIST example in debug mode on GPU</summary>
	<description>
tensorflow sources at commit &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/d4f3c0a9a5d4bce0752b2883167092a8f5cfb494&gt;d4f3c0a&lt;/denchmark-link&gt;

when building TF for cuda debug:
bazel build  -config=cuda -c dbg --strip=never //tensorflow/tools/pip_package:build_pip_package
running the convolution model results in a hang
[~/tensorflow/tensorflow/models/image/mnist] python convolutional.py
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:909] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:103] Found device 0 with properties:
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:05:00.0
Total memory: 12.00GiB
Free memory: 11.87GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:127] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_init.cc:137] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:702] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:05:00.0)
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Allocating 11.27GiB bytes.
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:52] GPU 0 memory begins at 0xb06c80000 extends to 0xdd853359a
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 16.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 32.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 64.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 128.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 256.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 512.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 16.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 32.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 64.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 128.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 256.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 512.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.00GiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.00GiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.00GiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.00GiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 16.00GiB
Initialized!
Epoch 0.00
Minibatch loss: 10.112, learning rate: 0.010000
Minibatch error: 82.8%
Validation error: 92.4%
attaching a debugger shows the following thread state:
(gdb) i th
Id   Target Id         Frame
22   Thread 0x7f775789e700 (LWP 18189) "python" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
21   Thread 0x7f775709d700 (LWP 18190) "python" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
20   Thread 0x7f775689c700 (LWP 18191) "python" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
19   Thread 0x7f774bca6700 (LWP 18192) "python" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
18   Thread 0x7f774b4a5700 (LWP 18193) "python" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
17   Thread 0x7f774aca4700 (LWP 18194) "python" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
16   Thread 0x7f771eda3700 (LWP 18195) "python" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
15   Thread 0x7f771e5a2700 (LWP 18196) "python" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
14   Thread 0x7f771dda1700 (LWP 18197) "python" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
13   Thread 0x7f771d5a0700 (LWP 18198) "python" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
12   Thread 0x7f771cd9f700 (LWP 18199) "python" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
11   Thread 0x7f771c59e700 (LWP 18200) "python" 0x00007f775a41212d in poll () at ../sysdeps/unix/syscall-template.S:81
10   Thread 0x7f7709bff700 (LWP 18201) "python" pthread_cond_timedwait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_timedwait.S:238
9    Thread 0x7f770931e700 (LWP 18202) "python" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
8    Thread 0x7f7708b1d700 (LWP 18203) "python" 0x00007f775a3e5f3d in nanosleep () at ../sysdeps/unix/syscall-template.S:81
7    Thread 0x7f770831c700 (LWP 18204) "python" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
6    Thread 0x7f7707b1b700 (LWP 18205) "python" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
5    Thread 0x7f770731a700 (LWP 18206) "python" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
4    Thread 0x7f7706b19700 (LWP 18207) "python" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
3    Thread 0x7f76fa9b0700 (LWP 18208) "python" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
2    Thread 0x7f76fa1af700 (LWP 18209) "python" sem_wait () at ../nptl/sysdeps/unix/sysv/linux/x86_64/sem_wait.S:85

1    Thread 0x7f775ab0a740 (LWP 18188) "python" pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185

this state never changes.
building TF w/o GPU support. or in release mode works fine
	</description>
	<comments>
		<comment id='1' author='bas-aarts' date='2016-01-07T00:17:13Z'>
		I can reproduce this problem on my side. The hang only shows up with
external Eigen library. During the hang, one Eigen kernel was stuck on GPU.
It is unclear whether it is a real hang, or it was just the kernel run
extremely slowly.
It is known that there is a recent slowdown after switching to the external
library. Benoit is currently working on a fix.
On Tue, Jan 5, 2016 at 1:35 AM, Martin Wicke &lt;denchmark-link:mailto:notifications@github.com&gt;notifications@github.com&lt;/denchmark-link&gt;

wrote:

Assigned #688 #688 to
@zheng-xq https://github.com/zheng-xq.
â€”
Reply to this email directly or view it on GitHub
#688 (comment).

		</comment>
		<comment id='2' author='bas-aarts' date='2016-01-12T20:28:08Z'>
		I believe this was fixed by &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/38f54b55cdc66e12a2f6ce9d6724440e9738ac6f&gt;38f54b5&lt;/denchmark-link&gt;
. After the upgrade to the latest version of Eigen, I can now run mnist on GPU in debug mode:
bazel run --config=cuda -c dbg --strip=never //tensorflow/models/image/mnist:convolutional
...
Initialized!
Step 0 (epoch 0.00), 23.4 ms
Minibatch loss: 12.054, learning rate: 0.010000
Minibatch error: 90.6%
Validation error: 84.6%
Step 100 (epoch 0.12), 58.6 ms
Minibatch loss: 3.293, learning rate: 0.010000
Minibatch error: 6.2%
Validation error: 7.1%
Step 200 (epoch 0.23), 57.0 ms
Minibatch loss: 3.525, learning rate: 0.010000
Minibatch error: 14.1%
Validation error: 3.9%
...
I have run 2900 steps with no problem so far, so I'm closing this issue. Feel free to reopen if you still experience this deadlock
		</comment>
	</comments>
</bug>