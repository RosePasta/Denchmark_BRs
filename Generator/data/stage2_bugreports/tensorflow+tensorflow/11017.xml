<bug id='11017' author='rubenvereecken' open_date='2017-06-23T17:18:55Z' closed_time='2017-07-07T01:10:24Z'>
	<summary>Tfdbg does not work with Coordinator/QueueRunners</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Mint 18
TensorFlow installed from (source or binary): Binary (pip)
TensorFlow version (use command below): v1.2.0-rc2-21-g12f033d 1.2.0
Bazel version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: N/A

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

The Tensorflow debugger does not seem to be working with Queues; data never seems to be fetched by the QueueRunner threads, be it from a file (using tf.TFRecordReader and tf.parse_single_example) or preloaded (using tf.train.slice_input_producer). Instead, the coordinator.should_stop() is True right away. This is only the case after wrapping the session in a tf.python.debug.LocalCLIDebugWrapperSession. The example should make things clearer.
Moreover, another error occurs at coordinator.join(threads).
I am aware of the &lt;denchmark-link:https://www.tensorflow.org/programmers_guide/debugger&gt;FAQ entry on Threads&lt;/denchmark-link&gt;
, but that does not explain why the data fetching threads would not be working.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

To make it easiest to replicate, I simply took the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/fully_connected_preloaded.py&gt;example on working with preloaded data&lt;/denchmark-link&gt;
, and wrapped the session in there with the debugger. I uploaded the gist with two lines added to &lt;denchmark-link:https://gist.github.com/rubenvereecken/079cdf1abc76866714ff6f752167481d#file-fully_connected_preloaded_debug-py-L92&gt;https://gist.github.com/rubenvereecken/079cdf1abc76866714ff6f752167481d#file-fully_connected_preloaded_debug-py-L92&lt;/denchmark-link&gt;
.
To reproduce, run the file. Once you drop in the debugger, run once. It then exits. The full output is below:
&lt;denchmark-code&gt;Extracting /tmp/data/train-images-idx3-ubyte.gz
Extracting /tmp/data/train-labels-idx1-ubyte.gz
Extracting /tmp/data/t10k-images-idx3-ubyte.gz
Extracting /tmp/data/t10k-labels-idx1-ubyte.gz
Traceback (most recent call last):
  File "ex.py", line 191, in &lt;module&gt;
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File "/home/ruben/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File "ex.py", line 143, in main
    run_training()
  File "ex.py", line 138, in run_training
    coord.join(threads)
  File "/home/ruben/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File "/home/ruben/anaconda3/lib/python3.6/site-packages/six.py", line 686, in reraise
    raise value
  File "/home/ruben/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py", line 233, in _run
    enqueue_callable = sess.make_callable(enqueue_op)
AttributeError: 'LocalCLIDebugWrapperSession' object has no attribute 'make_callable'
&lt;/denchmark-code&gt;

The stacktrace is about coord.join(threads), but this is only possible because coord.should_stop() never seems to be False, which would indicate there is data to load. Without the added debugger lines, the example simply works.
	</description>
	<comments>
		<comment id='1' author='rubenvereecken' date='2017-06-23T18:36:48Z'>
		cc &lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/rubenvereecken&gt;@rubenvereecken&lt;/denchmark-link&gt;
 Thanks for reporting this issue. We are aware of it and will push a fix to it soon.
		</comment>
		<comment id='2' author='rubenvereecken' date='2017-06-23T19:21:33Z'>
		&lt;denchmark-link:https://github.com/caisq&gt;@caisq&lt;/denchmark-link&gt;
 thank you so much, I look forward to it.
		</comment>
		<comment id='3' author='rubenvereecken' date='2017-06-23T19:25:28Z'>
		&lt;denchmark-link:https://github.com/rubenvereecken&gt;@rubenvereecken&lt;/denchmark-link&gt;
 While you wait for the fix, I want to ask you whether you are trying to debug the data input queues, or the training operation on the main thread. If the latter, there is a workaround for that.
		</comment>
		<comment id='4' author='rubenvereecken' date='2017-06-23T23:02:28Z'>
		&lt;denchmark-link:https://github.com/caisq&gt;@caisq&lt;/denchmark-link&gt;
 Ah actually the former, but I'd be working my way towards the latter. Is there a way to debug these training ops while still using data fed from queues? Either way, could you point me at the workaround? Much appreciated!
		</comment>
		<comment id='5' author='rubenvereecken' date='2017-06-23T23:07:52Z'>
		&lt;denchmark-link:https://github.com/rubenvereecken&gt;@rubenvereecken&lt;/denchmark-link&gt;
 The workaround is based on the assumption that the train op runs on the Python main thread, while the data queues run on the child threads, which should usually be the case.
You can just use the thread_name_filter kwarg of the wrapper's constructor to limit the debugging to the train op.
sess = tf_debug.LocalCLIDebugWrapperSession(sess, thread_name_filter="MainThread$")
This is talked about in the FAQ. Doing this doesn't change the source of the input data. They still come from the queues; they just don't break into the TFDBG UI when they run.
		</comment>
		<comment id='6' author='rubenvereecken' date='2017-06-23T23:55:17Z'>
		Oops. I may have given incomplete suggestion. In your code at &lt;denchmark-link:https://gist.github.com/rubenvereecken/079cdf1abc76866714ff6f752167481d#file-fully_connected_preloaded_debug-py-L92&gt;https://gist.github.com/rubenvereecken/079cdf1abc76866714ff6f752167481d#file-fully_connected_preloaded_debug-py-L92&lt;/denchmark-link&gt;

make sure that your wrapped Session object is used only to run the train_op. You can do something like this:
Move the line sess = tf_debug.LocalCLIDebugWrapperSession(sess) after line 100. That makes sure that when the data input queue ops are created, the make_callable() method of the original session, not that of the wrapped session is called. The wrapped session doesn't have a make_callable() method, which was recently added. This was the root cause of the issue you are seeing.
		</comment>
		<comment id='7' author='rubenvereecken' date='2017-07-10T13:14:50Z'>
		ðŸŽ‰
		</comment>
		<comment id='8' author='rubenvereecken' date='2018-01-26T01:23:35Z'>
		For the user of TF-Slim,  The usage of thread_name_filter
&lt;denchmark-code&gt;session_wrapper_main_thread =  functools.partial(
  tf_debug.LocalCLIDebugWrapperSession,
  thread_name_filter="MainThread$")

slim.learning.train(
  ... 
  session_wrapper=session_wrapper_main_thread,
  ... ) 

&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>