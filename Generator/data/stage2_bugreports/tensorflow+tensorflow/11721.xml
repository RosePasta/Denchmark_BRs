<bug id='11721' author='JamesKostas' open_date='2017-07-24T16:34:32Z' closed_time='2017-09-20T17:57:53Z'>
	<summary>Memory Leak from Training Step</summary>
	<description>
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes, although my code is somewhat based on the MNIST deep learning tutorial.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 14.04 VERSION="14.04.5 LTS, Trusty Tahr"
TensorFlow installed from (source or binary):
Installed via the VirtualEnv method
TensorFlow version (use command below):
1.1 and 1.2
Bazel version (if compiling from source):
CUDA/cuDNN version:
CUDA Version 8.0.44
GPU model and memory:
GeForce GTX 780M 4GB
Exact command to reproduce:
self.sess.run(self.train_step, feed_dict={self.x: trainingdata, self.y_true: traininglabels, self.keepratio: self.training_keep_rate})

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

This is very similar to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/9590&gt;the bug report I submitted here&lt;/denchmark-link&gt;
, but is a bit of a slower leak and is present in both TF 1.1 and 1.2.  I have finalized my graph.  Using the architecture described by &lt;denchmark-link:https://arxiv.org/pdf/1311.2901v3.pdf&gt;Zeiler et al., 2013 (ZF Net)&lt;/denchmark-link&gt;
, batch sizes of 64, and 224x224 grayscale (1 channel) input, it leaks approximately 4GB after approximately 3000 batches.  This makes it unworkable, for say, 80 epochs of ImageNet training.  I have confirmed that the leak either does not occur or is much less severe (hard to tell which) if I comment out the training line (i.e. still do all of my preprocessing and loading).
As directed in that last linked issue, I tried to call sess.run with
options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE), run_metadata=run_metadata
and start the program with
env TF_CPP_MIN_VLOG_LEVEL=1 python deep_learning_main.py
but the amount of spew was enormous, and it won't respond to keyboard interrupts (I have to kill the job).  If that info would be helpful, how do I go about recording/saving this information properly to upload and help you all debug?
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/1170372/1_08.zip&gt;1_08.zip&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='JamesKostas' date='2017-07-25T17:57:33Z'>
		Write the log (with TF_CPP_MIN_VLOG_LEVEL=1) to a file and upload it somewhere so that we can download it.
Also, see if you can reproduce the problem with a simpler program. The current program is over 1000 lines, making it hard to debug a memory leak. Additionally, I cannot run the program because you have some hardcoded paths, and the matplotlib dependency makes it hard to run internally. See if synthetic data can be used to remove the dependency on the imagenet data.
		</comment>
		<comment id='2' author='JamesKostas' date='2017-08-01T15:42:04Z'>
		I apologize for not getting back to this and being able to help; I've run out of time and had to move on to a new project.  However, I have had a similar issue with PyTorch; apparently it is an issue with CUDA and maybe my GPU.  Perhaps the TF leak is caused by same issue?  See
&lt;denchmark-link:https://discuss.pytorch.org/t/memory-usage-leak/5645&gt;https://discuss.pytorch.org/t/memory-usage-leak/5645&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='JamesKostas' date='2017-08-01T15:53:03Z'>
		&lt;denchmark-link:https://github.com/reedwm&gt;@reedwm&lt;/denchmark-link&gt;
 btw, regarding using TF_CPP_MIN_VLOG_LEVEL to debug memory leaks, it became less useful between 1.0.1 and 1.1
Some of the deallocation messages started missing allocation ids, so you can't tell how much memory got reclaimed.
ie for some of my scripts I see messages like this in TF 1.1
'2017-05-09 15:31:16.865982: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: "cpu" }'
instead of following in TF 1.0.1
I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 235 allocator_name: "cpu" } 
		</comment>
		<comment id='4' author='JamesKostas' date='2017-08-01T18:44:24Z'>
		&lt;denchmark-link:https://github.com/JamesKostas&gt;@JamesKostas&lt;/denchmark-link&gt;
 That's ok. Respond to this post if you ever get time to upload the logs or reproduce the problem with a simpler program.
&lt;denchmark-link:https://github.com/yaroslavvb&gt;@yaroslavvb&lt;/denchmark-link&gt;
 Can you file a separate bug? allocation_id might be only printed if its not 0, but I'm not sure.
		</comment>
		<comment id='5' author='JamesKostas' date='2017-09-16T20:30:56Z'>
		&lt;denchmark-link:https://github.com/reedwm&gt;@reedwm&lt;/denchmark-link&gt;
 -- filed under &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/13087&gt;#13087&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='JamesKostas' date='2017-09-20T17:57:53Z'>
		Without a reproducible test case, it seems unlikely we are going to make much headway, especially if you are working on a new project. So I am closing for now.
		</comment>
		<comment id='7' author='JamesKostas' date='2017-09-20T17:58:09Z'>
		&lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
, FYI.
		</comment>
	</comments>
</bug>