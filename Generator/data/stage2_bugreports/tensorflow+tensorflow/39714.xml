<bug id='39714' author='BSlience' open_date='2020-05-20T12:59:07Z' closed_time='2020-07-15T01:55:20Z'>
	<summary>Two weird things when I use custom model `train_step` and loss</summary>
	<description>
System information

colab reproduce url: https://colab.research.google.com/drive/15hJPKHgn8aPi1mVi3XFOFvOgj85QcL-Z?usp=sharing
windows 10
TensorFlow version 2.2
Python version:3.7
CUDA/cuDNN version:10.1

import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow.python.eager import backprop
from tensorflow.python.keras.engine import data_adapter

class CustomModel(tf.keras.Model):
    def __init__(self):
        super(CustomModel, self).__init__()
        self.flat = tf.keras.layers.Flatten(input_shape = (28, 28))
        
    def call(self, inputs, training=False, **kwargs):
        x = self.flat(inputs)
        out = (x, x, x)
        return out

    def train_step(self, data):
        data = data_adapter.expand_1d(data)
        x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)

        with backprop.GradientTape() as tape:
            y_pred = self(x, training = True)
            loss0 = tf.reduce_sum(self.losses)
            loss1, loss2, loss3 = self.loss(y, y_pred)
            total_loss = tf.reduce_sum([loss0, loss1, loss2, loss3])
        grads = tape.gradient(total_loss, self.trainable_variables)
        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))
        self.compiled_metrics.update_state(y, y_pred)
        return {m.name: m.result() for m in self.metrics}

class MultiTaskLoss(tf.keras.losses.Loss):
    def __init__(self):
        super(MultiTaskLoss, self).__init__(reduction = tf.keras.losses.Reduction.NONE)

    def call(self, y_true, y_pred):
        tf.print(y_pred[0].shape, y_pred[1].shape, y_pred[2].shape)
        loss1 = tf.reduce_sum(y_pred[0])
        loss2 = tf.reduce_sum(y_pred[1])
        loss3 = tf.reduce_sum(y_pred[2])
        return tf.cast(loss1, tf.float32), tf.cast(loss2, tf.float32), tf.cast(loss3, tf.float32)


tf.config.experimental_run_functions_eagerly(True)

tfds.list_builders()
dataset = tfds.load('mnist', split='train')
dataset = dataset.map(lambda exa: (exa['image'], exa['label']))
dataset = dataset.batch(8)
model = CustomModel()
loss = MultiTaskLoss()
model.compile(loss = loss, optimizer = 'Adam')
model.fit(dataset, epochs=1)
current behavior
1 can't recieve tuple y_pred in custom loss when using self.compiled_loss(y, y_pred)
2 iterating over tf.Tensor is not allowed exception raised when using autograph
Describe the expected behavior
hope everything is ok.

&lt;denchmark-link:https://colab.research.google.com/drive/15hJPKHgn8aPi1mVi3XFOFvOgj85QcL-Z?usp=sharing&gt;https://colab.research.google.com/drive/15hJPKHgn8aPi1mVi3XFOFvOgj85QcL-Z?usp=sharing&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='BSlience' date='2020-05-21T09:56:06Z'>
		Was able to reproduce the issue with &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/bd69b7ad6c3d9fce4938ff3a7f29ff00/39714.ipynb&gt;TF v2.2&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/f9ab32fa296f574d41e7fb9cecfb5048/39714.ipynb&gt;TF-nightly&lt;/denchmark-link&gt;
. Please find the attached gist. Thanks!
		</comment>
		<comment id='2' author='BSlience' date='2020-07-01T00:44:28Z'>
		&lt;denchmark-link:https://github.com/BSlience&gt;@BSlience&lt;/denchmark-link&gt;
 Can yo please make a simple standalone code to reproduce the issue? The current code runs for a long time. Thanks!
		</comment>
		<comment id='3' author='BSlience' date='2020-07-08T01:19:38Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='4' author='BSlience' date='2020-07-15T01:55:18Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='5' author='BSlience' date='2020-07-15T01:55:21Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39714&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39714&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>