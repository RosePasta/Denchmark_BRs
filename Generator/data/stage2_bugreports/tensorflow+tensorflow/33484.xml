<bug id='33484' author='mattbernardini' open_date='2019-10-17T20:20:56Z' closed_time='2019-11-19T02:13:52Z'>
	<summary>Issue with tf.keras.mixed_precision.experimental.LossScaleOptimizer</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 2.0
Python version: 3.6.8
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 10.0, 7.5
GPU model and memory: Titian RTX 24gb

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
When attempting to use tf.keras.mixed_precision.experimental.LossScaleOptimizer it fails to cast a matmul to float16
Describe the expected behavior
Matmul should cast to float16
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
import tensorflow as tf
import numpy as np
from tensorflow.keras.datasets import mnist
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
try:
for gpu in gpus:
tf.config.experimental.set_memory_growth(gpu, True)
logical_gpus = tf.config.experimental.list_logical_devices('GPU')
print(len(gpus), "Physical GPUS,", len(logical_gpus), "Logical GPUS")
except RuntimeError as e:
print(e)
tf.keras.backend.set_floatx('float16')
tf.keras.backend.set_epsilon(1e-4)
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
model.add(tf.keras.layers.MaxPooling2D((2, 2), padding="same"))
model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding="same"))
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(64, activation='relu'))
model.add(tf.keras.layers.Dense(10, activation='softmax'))
model.summary()
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
train_images = train_images.reshape((60000, 28, 28, 1))
train_images = train_images.astype(np.float16) / 255
test_images = test_images.reshape((10000, 28, 28, 1))
test_images = test_images.astype(np.float16) / 255
train_labels = tf.keras.utils.to_categorical(train_labels, dtype='float16')
test_labels = tf.keras.utils.to_categorical(test_labels, dtype='float16')
opt = tf.keras.optimizers.RMSprop()
opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt, "dynamic")
model.compile(optimizer=opt,
loss='categorical_crossentropy',
metrics=['accuracy'])
model.fit(tf.dtypes.cast(train_images, tf.float16), tf.dtypes.cast(train_labels, tf.float16), epochs=50, batch_size=64, steps_per_epoch=200)
test_loss, test_acc = model.evaluate(test_images, test_labels)
test_acc
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='mattbernardini' date='2019-10-18T07:02:14Z'>
		I could reproduce the issue with Tf 2.0. Please take a look at the colab &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/7909678539c398aa1f4b8fafa334807f/untitled208.ipynb&gt;gist&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='mattbernardini' date='2019-11-01T21:37:06Z'>
		&lt;denchmark-link:https://github.com/mattbernardini&gt;@mattbernardini&lt;/denchmark-link&gt;
 The tf.matmul() op does not perform automatic type conversions, so both of its inputs must have the same element type.
Try converting all the inputs to float32 and it should work fine. Please find my github gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/gowthamkpr/2ab5ef2ed2af387abc2e60016a3936bc/untitled208.ipynb&gt;here&lt;/denchmark-link&gt;
.
For more context, you can go through the following &lt;denchmark-link:https://stackoverflow.com/questions/36210887/how-to-fix-matmul-op-has-type-float64-that-does-not-match-type-float32-typeerror&gt;issue&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='3' author='mattbernardini' date='2019-11-06T16:20:30Z'>
		Right, that works for float32, but does not work for float16 which was the original issue.
		</comment>
		<comment id='4' author='mattbernardini' date='2019-11-13T02:24:45Z'>
		&lt;denchmark-link:https://github.com/reedwm&gt;@reedwm&lt;/denchmark-link&gt;
 Can you PTAL? Thanks!
		</comment>
		<comment id='5' author='mattbernardini' date='2019-11-16T14:59:23Z'>
		&lt;denchmark-link:https://github.com/gowthamkpr&gt;@gowthamkpr&lt;/denchmark-link&gt;
 I'm running into the same issue, trying to run float16 training. Not sure why you're casting to float32 in the .fit() method? Thanks!
		</comment>
		<comment id='6' author='mattbernardini' date='2019-11-17T16:08:19Z'>
		I encounter the same problem with float16 training. So far I have figured out that the issue lies in the LossScaleOptimizer, because the issue disappears when you remove it (but then obviously losses are too tiny to be effective).
To be more specific, the error is raised when the LossScaleOptimizer calls its  function which simply multiplies (via python operator *) the loss with the , which is an instance of . &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py#L149&gt;See the code here.&lt;/denchmark-link&gt;

def get_scaled_loss(self, loss):
    loss_scale = self._loss_scale()
    if callable(loss):
      return lambda: loss() * loss_scale
    else:
      return loss * loss_scale
The error says that the y input of the multiplication is float32 and the x input is float16. Since the multiplication in get_scaled_loss() is done like this: loss * loss_scale, this means that loss_scale is a float32. A few lines above we see that loss_scale = self._loss_scale(), and self._loss_scale is an instance of tf.train.experimental.LossScale.
Now, if we look into the source code of  &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/training/experimental/loss_scale.py&gt;here&lt;/denchmark-link&gt;
 we see that the  method of e.g. the DynamicLossScale (which is the one &lt;denchmark-link:https://github.com/mattbernardini&gt;@mattbernardini&lt;/denchmark-link&gt;
 uses) returns . If we then check the initialization of  we see that it is initialized as float32:
self._current_loss_scale = self._add_weight(
    name='current_loss_scale',
    dtype=dtypes.float32,
    initial_value=self._initial_loss_scale)
So this is why the TypeError is thrown when using LossScaleOptimizer in float16 training. I am not exactly sure why LossScales were implemented to only return float32 values. There are also no clear online guides on how to do float16 training in TF 2.0, so it would be nice if someone who knows the details of how Tensorflow implements the optimization process could provide information on proper float16 training, because evidently neither tf.keras.backend.set_floatx('float16') nor     tf.keras.mixed_precision.experimental.set_policy('mixed_float16') seem to work out-of-the-box.
		</comment>
		<comment id='7' author='mattbernardini' date='2019-11-19T02:13:36Z'>
		&lt;denchmark-link:https://github.com/verrannt&gt;@verrannt&lt;/denchmark-link&gt;
, good investigation! In TF 2.1, this will be fixed. Alternatively, you can use the nightly builds with  which has the fix now. This has been fixed by casting the float32 loss scale to the dtype of the loss.
For reference, to do float16 training (typically referred to as mixed precision training), you always want to do the following
&lt;denchmark-code&gt;tf.keras.mixed_precision.experimental.set_policy('mixed_float16')  # Good
&lt;/denchmark-code&gt;

You do not want to set floatx to float16 with the following, unless you also set the policy to 'mixed_float16':
&lt;denchmark-code&gt;tf.keras.backend.set_floatx('float16')  # Bad!
&lt;/denchmark-code&gt;

The reason you want to set the policy instead of floatx is that the policy will keep variables in float32, which is necessary for numeric stability.
As for an online guide to do float16 training, I agree we need one and we will have one in the future. For now, &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/Policy&gt;the Policy API documentation&lt;/denchmark-link&gt;
 has a brief guide on how to do mixed precision training
		</comment>
		<comment id='8' author='mattbernardini' date='2019-11-19T02:13:54Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33484&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33484&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>