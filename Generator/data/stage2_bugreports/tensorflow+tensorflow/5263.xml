<bug id='5263' author='NickShahML' open_date='2016-10-28T14:53:08Z' closed_time='2017-06-16T17:46:42Z'>
	<summary>OOM Error Message Should Show Which GPU is Out of Memory</summary>
	<description>
&lt;denchmark-h:h3&gt;What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?&lt;/denchmark-h&gt;

I've created a few overflow threads in regards to balancing seq2seq memory loads over multiple gpus here:
&lt;denchmark-link:http://stackoverflow.com/questions/39773645/split-rnn-memory-consumption-evenly-between-gpus-in-tensorflow&gt;http://stackoverflow.com/questions/39773645/split-rnn-memory-consumption-evenly-between-gpus-in-tensorflow&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Environment info&lt;/denchmark-h&gt;

Operating System:
Ubuntu 14.04 CUDA 7.5 -- tensorflow 0.11
&lt;denchmark-h:h3&gt;If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)&lt;/denchmark-h&gt;

Simply load two matrices. One incredibly larger one on a gpu0 and a smaller one on gpu1. It would be great if the OOM message could tell you which gpu ran out of memory. This way you can redistribute resources from one gpu to another.
&lt;denchmark-h:h3&gt;What other attempted solutions have you tried?&lt;/denchmark-h&gt;

Currently I look at all tensors assigned per device and how large each tensor it is. I then sum the size of these tensors using the following code:
def calculate_variable_sizes_per_device(batch_size = 64):
  dev_list = get_available_gpus()
  dev_size_list = [[dev_list[i], 0] for i in xrange(len(dev_list))]

  tf.logging.info('calculating variable sizes on respective devices: %d' % len(dev_list))
  print('dev_list', dev_list)
  for eachvar in tf.all_variables():
    device = eachvar.device
    print('device', device)
    var_shape = find_replace_list(eachvar.get_shape().as_list(), '?', batch_size)
    print('varshape',var_shape)
    var_size = np.prod(np.array(var_shape))
    print('varsize', var_size)  

    for i,dev in enumerate(dev_list):
      if dev.replace('/','') in device.lower():
        dev_size_list[i][1] += var_size

  return dev_size_list

def find_replace_list(list_, find, replace):
  for n,i in enumerate(list_):
    if i==find:
      list_[n]=replace
  return list_


def get_available_gpus():
  local_device_protos = device_lib.list_local_devices()
  return [x.name for x in local_device_protos] #if x.device_type == 'GPU']
	</description>
	<comments>
		<comment id='1' author='NickShahML' date='2016-10-28T15:19:09Z'>
		What does the OOM error message currently say for you?  (Including the entire stack trace, not just the error string) ?
		</comment>
		<comment id='2' author='NickShahML' date='2016-10-28T15:40:03Z'>
		Sorry, forgot to add that. The error message seems to tell you which tensors have not been initialized. However, this does not tell you which gpu ran out of memory, because tensors can not be initialized on both gpus. You are still left with not knowing which gpu ran out of memory.
I tensorflow/core/common_runtime/bfc_allocator.cc:696] Sum Total of in-use chunks: 11.27GiB          [1020/1823]
I tensorflow/core/common_runtime/bfc_allocator.cc:698] Stats:
Limit:                 12104795546
InUse:                 12104599040
MaxInUse:              12104599040
NumAllocs:                   50546
MaxAllocSize:            164069376
W tensorflow/core/common_runtime/bfc_allocator.cc:270] *********************************************************
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

W tensorflow/core/common_runtime/bfc_allocator.cc:271] Ran out of memory trying to allocate 384.0KiB.  See logs
for memory state.
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 74370 get requests, put_count=2
3731 evicted_count=1000 eviction_rate=0.042139 and unsatisfied allocation rate=0.695697
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 100 to 110
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 0 get requests, put_count=10010
evicted_count=10000 eviction_rate=0.999001 and unsatisfied allocation rate=0
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 0 get requests, put_count=20010
evicted_count=20000 eviction_rate=0.9995 and unsatisfied allocation rate=0
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 0 get requests, put_count=30010
evicted_count=30000 eviction_rate=0.999667 and unsatisfied allocation rate=0
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 0 get requests, put_count=40010
evicted_count=40000 eviction_rate=0.99975 and unsatisfied allocation rate=0
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 0 get requests, put_count=50010
evicted_count=50000 eviction_rate=0.9998 and unsatisfied allocation rate=0
W tensorflow/core/framework/op_kernel.cc:968] Internal: Dst tensor is not initialized.```
[[Node: vanilla_generator_seq2seq_model/model_with_buckets/embedding_attention_seq2seq_2/embedding_atte
ntion_decoder/Attention_0_60/attn_stack_layer0/Reshape_2/_91437 = _Recv&lt;denchmark-link:&gt;client_terminated=false, recv_device="/j
ob:localhost/replica:0/task:0/gpu:1", send_device="/job:localhost/replica:0/task:0/gpu:0", send_device_incarnati
on=1, tensor_name="edge_356892_vanilla_generator_seq2seq_model/model_with_buckets/embedding_attention_seq2seq_2/
embedding_attention_decoder/Attention_0_60/attn_stack_layer0/Reshape_2", tensor_type=DT_FLOAT, _device="/job:loc
alhost/replica:0/task:0/gpu:1"&lt;/denchmark-link&gt;
]]
[[Node: vanilla_generator_seq2seq_model/model_with_buckets/embedding_attention_seq2seq_2/embedding_atte
ntion_decoder/MultiRNNCell_69/Cell1/LayerNormBasicLSTMCell/forget/forget/moments/sufficient_statistics/Shape/_89
165 = _Recv&lt;denchmark-link:&gt;client_terminated=false, recv_device="/job:localhost/replica:0/task:0/cpu:0", send_device="/job:loca
lhost/replica:0/task:0/gpu:1", send_device_incarnation=1, tensor_name="edge_525863_vanilla_generator_seq2seq_mod
el/model_with_buckets/embedding_attention_seq2seq_2/embedding_attention_decoder/MultiRNNCell_69/Cell1/LayerNormB
asicLSTMCell/forget/forget/moments/sufficient_statistics/Shape", tensor_type=DT_INT32, _device="/job:localhost/r
eplica:0/task:0/cpu:0"&lt;/denchmark-link&gt;
]]
		</comment>
		<comment id='3' author='NickShahML' date='2016-10-28T15:46:06Z'>
		Cool, thanks.


Extending the BFCAllocator to be passed in the 'identity' of the device would be a reasonable feature request that someone from the community could add.


The stack trace does mention that this is "gpu:1" -- if you look at the error message for the node that failed:


&lt;denchmark-code&gt;W tensorflow/core/framework/op_kernel.cc:968] Internal: Dst tensor is not initialized.```
[[Node: vanilla_generator_seq2seq_model/model_with_buckets/embedding_attention_seq2seq_2/embedding_atte
ntion_decoder/Attention_0_60/attn_stack_layer0/Reshape_2/_91437 = _Recvclient_terminated=false, recv_device="/j
ob:localhost/replica:0/task:0/gpu:1", send_device="/job:localhost/replica:0/task:0/gpu:0", send_device_incarnati
on=1, tensor_name="edge_356892_vanilla_generator_seq2seq_model/model_with_buckets/embedding_attention_seq2seq_2/
embedding_attention_decoder/Attention_0_60/attn_stack_layer0/Reshape_2", tensor_type=DT_FLOAT, _device="/job:loc
alhost/replica:0/task:0/gpu:1"]]

&lt;/denchmark-code&gt;

The combination of "Dst tensor" and the "recv_device" being "gpu:1" should at least give you some information in the short term.
		</comment>
		<comment id='4' author='NickShahML' date='2016-10-28T15:55:23Z'>
		Thanks &lt;denchmark-link:https://github.com/vrv&gt;@vrv&lt;/denchmark-link&gt;
 -- The stack trace does also mention gpu0 which is what led to my confusion. I will try re-appropriating more tensors to gpu0 and see what happens and report back.
&lt;denchmark-code&gt;[[Node: vanilla_generator_seq2seq_model/model_with_buckets/embedding_attention_seq2seq_2/embedding_atte
ntion_decoder/Attention_0_60/attn_stack_layer0/Reshape_2/_91437 = _Recvclient_terminated=false, recv_device="/j
ob:localhost/replica:0/task:0/gpu:1", send_device="/job:localhost/replica:0/task:0/gpu:0", send_device_incarnati
on=1, tensor_name="edge_356892_vanilla_generator_seq2seq_model/model_with_buckets/embedding_attention_seq2seq_2/
embedding_attention_decoder/Attention_0_60/attn_stack_layer0/Reshape_2", tensor_type=DT_FLOAT, _device="/job:loc
alhost/replica:0/task:0/gpu:1"]]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='NickShahML' date='2016-10-28T15:57:53Z'>
		Indeed, this error message is telling you that when copying a tensor (Send/Recv) from GPU:0 to GPU:1, that there was a failure to allocate memory on GPU:1.
		</comment>
		<comment id='6' author='NickShahML' date='2016-10-28T16:19:15Z'>
		When I run this network on one titan x, it runs with a batch size of 64. I'm attempting to raise the batch size to 96 with two titan x's on board. I feel that this should be easily done as there is double the GPU memory. Even a batch size of 128 should be doable.
I just placed more of the forward pass on GPU0 in response to your previous message. I'm still getting a memory error. Currently I have:
GPU0 --&gt; stores all vars and handles forward pass
GPU1 --&gt; stores all gradients and handles gradient computations
I will keep trying to change things around, but perhaps I'm approaching this the wrong way. This is turning more into a stack overflow question. Any help is greatly appreciated and I posted the overflow question in regards to this here:
&lt;denchmark-link:http://stackoverflow.com/questions/39773645/split-rnn-memory-consumption-evenly-between-gpus-in-tensorflow&gt;http://stackoverflow.com/questions/39773645/split-rnn-memory-consumption-evenly-between-gpus-in-tensorflow&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>