<bug id='36840' author='OverLordGoldDragon' open_date='2020-02-17T22:34:47Z' closed_time='2020-02-20T19:46:21Z'>
	<summary>Connecting to invalid output X of source node Y which has Z outputs [TF2.1-nightly]</summary>
	<description>
Occurs in Graph execution (not Eager) with a  followed by  (but not either on its own) custom RNN layer; the layer involves  and  ops in , as described &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/36797&gt;here&lt;/denchmark-link&gt;
. The error occurs upon:

model.train_on_batch() -- .fit() -- .save_weights() -- .save()

I tried tf.compat.v1.experimental.output_all_intermediates(True) and False, didn't help. Doesn't occur with built-in LSTM layer.
Any resolution?
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Error trace:
File "C:\DL_code\dev_bn_indrnn\main2.py", line 29, in &lt;module&gt;
  model.train_on_batch(x, y)
File "D:\Anaconda\envs\tf2n_env\lib\site-packages\tensorflow\python\keras\engine\training_v1.py", line 1083, in train_on_batch
  outputs = self.train_function(ins)  # pylint: disable=not-callable
File "D:\Anaconda\envs\tf2n_env\lib\site-packages\tensorflow\python\keras\backend.py", line 3597, in __call__
  session = get_session(inputs)
File "D:\Anaconda\envs\tf2n_env\lib\site-packages\tensorflow\python\keras\backend.py", line 528, in get_session
  _initialize_variables(session)
File "D:\Anaconda\envs\tf2n_env\lib\site-packages\tensorflow\python\keras\backend.py", line 943, in _initialize_variables
  [variables_module.is_variable_initialized(v) for v in candidate_vars])
File "D:\Anaconda\envs\tf2n_env\lib\site-packages\tensorflow\python\client\session.py", line 958, in run
  run_metadata_ptr)
File "D:\Anaconda\envs\tf2n_env\lib\site-packages\tensorflow\python\client\session.py", line 1181, in _run
  feed_dict_tensor, options, run_metadata)
File "D:\Anaconda\envs\tf2n_env\lib\site-packages\tensorflow\python\client\session.py", line 1359, in _do_run
  run_metadata)
File "D:\Anaconda\envs\tf2n_env\lib\site-packages\tensorflow\python\client\session.py", line 1384, in _do_call
  raise type(e)(node_def, op, message)

InvalidArgumentError: Node 'training/Nadam/gradients/gradients/ind_rnn/while_grad/ind_rnn/while_grad': 
Connecting to invalid output 46 of source node ind_rnn/while which has 46 outputs. 
Try using tf.compat.v1.experimental.output_all_intermediates(True).
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Update: while there aren't errors in Eager, the gradients are extremely small: 1e-9 to 1e-19, whereas usually the same layers have 1e-6 to 1e-2. This may or may not be a design rather than a bug problem.
	</description>
	<comments>
		<comment id='1' author='OverLordGoldDragon' date='2020-02-18T14:04:57Z'>
		&lt;denchmark-link:https://github.com/OverLordGoldDragon&gt;@OverLordGoldDragon&lt;/denchmark-link&gt;

Can you please provide simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!
		</comment>
		<comment id='2' author='OverLordGoldDragon' date='2020-02-20T19:46:21Z'>
		I'm no longer working on this, so no example coming anytime soon - will reopen if relevant again.
		</comment>
		<comment id='3' author='OverLordGoldDragon' date='2020-02-20T19:46:23Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36840&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36840&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='OverLordGoldDragon' date='2020-07-17T19:04:04Z'>
		May be it will help someone...
In my case the same issue was caused by the fact that I was setting learning rate BEFORE 1st call of fit(). It is somehow related to not initialized variables...
`
&lt;denchmark-h:h1&gt;TF: 2.2.0&lt;/denchmark-h&gt;

import tensorflow.keras.backend as K
K.set_value( model.optimizer.lr, 0.01 )
model.fit(...)
`
Setting learning rate after fit() or in callback works just fine.
		</comment>
	</comments>
</bug>