<bug id='23473' author='malcolmst' open_date='2018-11-03T04:34:33Z' closed_time='2019-12-09T21:18:39Z'>
	<summary>tf.matmul fails with CUBLAS_STATUS_NOT_SUPPORTED for large matrices when using CUDA 9.1</summary>
	<description>
I am hitting this issue on multiple development machines with different GPUs, and all versions of Tensorflow &gt;= 1.9.0. I have a CUDA 9.1 requirement on the host machine, so downgrading to CUDA 9.0 is not an option for me.
See &lt;denchmark-link:https://stackoverflow.com/questions/50911052/tensorflow-matmul-blas-xgemmbatched-launch-failed/50918250&gt;https://stackoverflow.com/questions/50911052/tensorflow-matmul-blas-xgemmbatched-launch-failed/50918250&lt;/denchmark-link&gt;
 for a possibly related issue, though that involved Tensorflow 1.8.0.
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, a minimum repro is below
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.9.0 to 1.11.0 (does not reproduce in 1.8.0)
Python version: 2.7
Bazel version (if compiling from source): Bazel 0.10.0 for  Tensorflow 1.9.0, Bazel 0.18.0 for Tensorflow 1.10.0 and newer
GCC/Compiler version (if compiling from source): GCC 5.4.0
CUDA/cuDNN version: CUDA 9.1.85 / cuDNN 7.0.5
GPU model and memory: GTX 1080 w/ 7400 MB, GTX 1060 w/ 5600 MB

Describe the current behavior
tf.matmul is failing to multiply matrices above a certain size, with error:
failed to run cuBLAS routine cublasGemmBatchedEx: CUBLAS_STATUS_NOT_SUPPORTED
I have confirmed using nvidia-smi that the GPU is nowhere close to running out of memory.
Describe the expected behavior
The matrix multiplication should complete successfully.
Code to reproduce the issue
This is borrowed from the stackoverflow link above:
import tensorflow as tf
import numpy as np

config = tf.ConfigProto()
config.gpu_options.allow_growth=True
tf.Session(config=config).close()

def calc():
    N = 15 # works for N &lt;= 14
    a = 16
    b = 8
    X = np.random.rand(N, 11520, b, 1).astype(np.float32)
    print(X.nbytes*1e-6, "MB")
    W = np.random.rand(N, 11520, a, b).astype(np.float32)
    print(W.nbytes*1e-6, "MB")
    X_ = tf.constant(X, name="X-constant", dtype=tf.float32)
    W_ = tf.constant(W, name="W-constant", dtype=tf.float32)

    return tf.matmul(W_, X_, name="mymatmul")

tf.reset_default_graph()
a = calc()
sess = tf.Session()
sess.run(tf.global_variables_initializer())
b = sess.run(a)
sess.close()
print(b.shape)
Other info / logs
I found a workaround for this issue is to patch CUDABlas::DoBlasGemmBatchedInternal in tensorflow/stream_extractor/cuda/cuda_blas.cc to disable the #if CUDA_VERSION &gt;= 9010 block which calls wrap::cublasGemmBatchedEx. Downgrading to Tensorflow &lt;= 1.8.0 also resolves the issue for me (that codeblock was added in Tensorflow 1.9.0).
Running the above code (N=15) with Tensorflow 1.11.0:
&lt;denchmark-code&gt;2018-11-02 23:13:48.059128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-11-02 23:13:48.059860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:01:00.0
totalMemory: 7.92GiB freeMemory: 7.40GiB
2018-11-02 23:13:48.059884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-11-02 23:13:48.282603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-02 23:13:48.282631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2018-11-02 23:13:48.282636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2018-11-02 23:13:48.282790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7137 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
(5.529599999999999, 'MB')
(88.47359999999999, 'MB')
2018-11-02 23:13:48.963544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-11-02 23:13:48.963579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-02 23:13:48.963585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2018-11-02 23:13:48.963588: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2018-11-02 23:13:48.963722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7137 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-11-02 23:13:51.146858: E tensorflow/stream_executor/cuda/cuda_blas.cc:652] failed to run cuBLAS routine cublasGemmBatchedEx: CUBLAS_STATUS_NOT_SUPPORTED
2018-11-02 23:13:51.146900: E tensorflow/stream_executor/cuda/cuda_blas.cc:2574] Internal: failed BLAS call, see log for details
Traceback (most recent call last):
  File "./tf_matmul.py", line 25, in &lt;module&gt;
    b = sess.run(a)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 887, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1110, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1286, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1308, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Blas xGEMMBatched launch failed : a.shape=[172800,16,8], b.shape=[172800,8,1], m=16, n=1, k=8, batch_size=172800
	 [[{{node mymatmul}} = BatchMatMul[T=DT_FLOAT, adj_x=false, adj_y=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](W-constant, X-constant)]]

Caused by op u'mymatmul', defined at:
  File "./tf_matmul.py", line 22, in &lt;module&gt;
    a = calc()
  File "./tf_matmul.py", line 19, in calc
    return tf.matmul(W_, X_, name="mymatmul")
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py", line 2015, in matmul
    a, b, adj_x=adjoint_a, adj_y=adjoint_b, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py", line 1245, in batch_mat_mul
    "BatchMatMul", x=x, y=y, adj_x=adj_x, adj_y=adj_y, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 488, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3272, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1768, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): Blas xGEMMBatched launch failed : a.shape=[172800,16,8], b.shape=[172800,8,1], m=16, n=1, k=8, batch_size=172800
	 [[{{node mymatmul}} = BatchMatMul[T=DT_FLOAT, adj_x=false, adj_y=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](W-constant, X-constant)]]
&lt;/denchmark-code&gt;

Reducing the matrix size (N=14):
&lt;denchmark-code&gt;2018-11-02 23:18:29.127555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-11-02 23:18:29.128409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:01:00.0
totalMemory: 7.92GiB freeMemory: 7.39GiB
2018-11-02 23:18:29.128441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-11-02 23:18:29.482292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-02 23:18:29.482346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2018-11-02 23:18:29.482361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2018-11-02 23:18:29.482670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7131 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
(5.160959999999999, 'MB')
(82.57535999999999, 'MB')
2018-11-02 23:18:30.200524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-11-02 23:18:30.200586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-02 23:18:30.200598: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2018-11-02 23:18:30.200605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2018-11-02 23:18:30.200837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7131 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
(14, 11520, 16, 1)

&lt;/denchmark-code&gt;

After patching CUDABlas::DoBlasGemmBatchedInternal (N=15):
&lt;denchmark-code&gt;2018-11-02 23:28:05.667458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-11-02 23:28:05.668033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:01:00.0
totalMemory: 7.92GiB freeMemory: 7.39GiB
2018-11-02 23:28:05.668046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-11-02 23:28:05.866043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-02 23:28:05.866069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2018-11-02 23:28:05.866074: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2018-11-02 23:28:05.866218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7129 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
(5.529599999999999, 'MB')
(88.47359999999999, 'MB')
2018-11-02 23:28:06.452688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-11-02 23:28:06.452722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-02 23:28:06.452727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2018-11-02 23:28:06.452730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2018-11-02 23:28:06.452858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7129 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
(15, 11520, 16, 1)

&lt;/denchmark-code&gt;

Patch for workaround:
diff --git a/tensorflow/stream_executor/cuda/cuda_blas.cc b/tensorflow/stream_executor/cuda/cuda_blas.cc
index ab7091b..50579b6 100644
--- a/tensorflow/stream_executor/cuda/cuda_blas.cc
+++ b/tensorflow/stream_executor/cuda/cuda_blas.cc
@@ -18,6 +18,8 @@ limitations under the License.
 
 #define SE_CUDA_DATA_HALF CUDA_R_16F
 
+#define USE_CUBLAS_GEMM_BATCHED_EX false
+
 #include "tensorflow/stream_executor/cuda/cuda_blas.h"
 
 // Both Eigen Half.h and CUDA cuda_fp16.h provide similar typedef for __half. As
@@ -2482,7 +2484,7 @@ port::Status CUDABlas::DoBlasGemmBatchedInternal(
 
   cudaDataType_t data_type = CUDADataType&lt;T&gt;::type;
 
-#if CUDA_VERSION &gt;= 9010
+#if CUDA_VERSION &gt;= 9010 &amp;&amp; USE_CUBLAS_GEMM_BATCHED_EX
   int cc_major, cc_minor;
   if (stream-&gt;parent()-&gt;GetDeviceDescription().cuda_compute_capability(
           &amp;cc_major, &amp;cc_minor) &amp;&amp;
	</description>
	<comments>
		<comment id='1' author='malcolmst' date='2018-11-07T19:39:14Z'>
		Forwarding this to &lt;denchmark-link:https://github.com/azaks2&gt;@azaks2&lt;/denchmark-link&gt;
 who has access to more CUDA expertise than I do.
		</comment>
		<comment id='2' author='malcolmst' date='2018-12-18T21:29:08Z'>
		I had the same issue with cuda 9.2. Last patch for cuda fixes it.
		</comment>
		<comment id='3' author='malcolmst' date='2018-12-19T00:27:32Z'>
		&lt;denchmark-link:https://github.com/sh1ng&gt;@sh1ng&lt;/denchmark-link&gt;
 Thanks. I believe I was using the latest cuda 9.1 patches but will double-check. If that is the case, maybe this codepath should just be enabled for cuda 9.2 and later.
		</comment>
		<comment id='4' author='malcolmst' date='2019-12-09T21:18:39Z'>
		Closing as stale because we use CUDA 10.1 now.  Please reopen if necessary.
		</comment>
		<comment id='5' author='malcolmst' date='2019-12-09T21:18:41Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23473&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23473&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='malcolmst' date='2020-04-01T12:26:40Z'>
		
(5.529599999999999, 'MB')
(88.47359999999999, 'MB')
2018-11-02 23:28:06.452688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-11-02 23:28:06.452722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-02 23:28:06.452727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0
2018-11-02 23:28:06.452730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N
2018-11-02 23:28:06.452858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7129 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
(15, 11520, 16, 1)

Patch for workaround:

```diff
diff --git a/tensorflow/stream_executor/cuda/cuda_blas.cc b/tensorflow/stream_executor/cuda/cuda_blas.cc
index ab7091b..50579b6 100644
--- a/tensorflow/stream_executor/cuda/cuda_blas.cc
+++ b/tensorflow/stream_executor/cuda/cuda_blas.cc
@@ -18,6 +18,8 @@ limitations under the License.
 
 #define SE_CUDA_DATA_HALF CUDA_R_16F
 
+#define USE_CUBLAS_GEMM_BATCHED_EX false
+
 #include "tensorflow/stream_executor/cuda/cuda_blas.h"
 
 // Both Eigen Half.h and CUDA cuda_fp16.h provide similar typedef for __half. As
@@ -2482,7 +2484,7 @@ port::Status CUDABlas::DoBlasGemmBatchedInternal(
 
   cudaDataType_t data_type = CUDADataType&lt;T&gt;::type;
 
-#if CUDA_VERSION &gt;= 9010
+#if CUDA_VERSION &gt;= 9010 &amp;&amp; USE_CUBLAS_GEMM_BATCHED_EX
   int cc_major, cc_minor;
   if (stream-&gt;parent()-&gt;GetDeviceDescription().cuda_compute_capability(
           &amp;cc_major, &amp;cc_minor) &amp;&amp;


Hi, I have the same problem.
2020-04-01 20:21:43.695815: E tensorflow/stream_executor/cuda/cuda_blas.cc:652] failed to run cuBLAS routine cublasGemmBatchedEx: CUBLAS_STATUS_NOT_SUPPORTED
2020-04-01 20:21:43.695883: E tensorflow/stream_executor/cuda/cuda_blas.cc:2574] Internal: failed BLAS call, see log for details
InternalError (see above for traceback): Blas xGEMMBatched launch failed : a.shape=[245760,8,16], b.shape=[245760,16,16], m=8, n=16, k=16, batch_size=245760
[[node layers/einsum/MatMul (defined at /home/pai/3dfaceRe/RandLA-Net/RandLANet.py:327)  = BatchMatMul[T=DT_FLOAT, adj_x=false, adj_y=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](layers/einsum/Reshape, layers/einsum/Reshape_1)]]
[[{{node optimizer/gradients/layers/Encoder_layer_3LFAmlp2/BiasAdd_grad/BiasAddGrad/_663}} = _Recv&lt;denchmark-link:&gt;client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_6262_...iasAddGrad", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"&lt;/denchmark-link&gt;
]]
I have Tensorflow=1.12, Cudatoolkit=9.2, and Cudnn=7.6.5 in Conda environment. I couldn't find the file of  "tensorflow/stream_executor/cuda/cuda_blas.cc" and only have "tensorflow/stream_executor/cuda/cuda_blas.h"
Does anyone know how to solve this problem?
		</comment>
		<comment id='7' author='malcolmst' date='2020-04-02T01:25:32Z'>
		
I have Tensorflow=1.12, Cudatoolkit=9.2, and Cudnn=7.6.5 in Conda environment.

If at all possible I'd suggest using a more recent version of TF.  1.12 is quite old, and we're not in a position to debug issues with it.
		</comment>
	</comments>
</bug>