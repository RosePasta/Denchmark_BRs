<bug id='33868' author='mjlbach' open_date='2019-10-31T07:59:33Z' closed_time='2019-12-16T18:54:27Z'>
	<summary>Distribution Strategy running variables: NCCL error crashes CUDA</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):
TensorFlow version (use command below): 2.0.0
Python version: 3.7.4
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 10.0/7.6.2
GPU model and memory: Titan XP 12 gb x 8

Describe the current behavior
When using distribution strategy, the following code runs in graph mode, but fails in eager mode resulting in an NCCL error. Since I need persistent Variables to normalize my loss, the loss function was implemented as a callable keras layer, instantiated as a property of the keras model for which is compute the loss.
&lt;denchmark-code&gt;class HierarchicalLoss(tf.keras.layers.Layer):
    def __init__(self):
        super(HierarchicalLoss, self).__init__()

        self.running_N = {}
        self.running_sum = {}
        self.running_mean = {}

    def build(self, input_shapes):
            for layer in input_shapes['nodes']:
                var_scope = ['local_delta_loss' +  "_" + layer]

                self.running_sum[var_scope] = tf.Variable(
                    initial_value = 1.0,
                    dtype = tf.float64,
                    name = var_scope + '_' + 'running_sum',
                    trainable = False,
                    aggregation = tf.VariableAggregation.SUM)

                self.running_N[var_scope] = tf.Variable(
                    initial_value = 1,
                    dtype = tf.int64,
                    name = var_scope + '_' + 'running_N',
                    trainable = False,
                    aggregation = tf.VariableAggregation.SUM)
    def call(self, model_outputs):
        total_loss = 0.0
        for layer in model_output['nodes']:
            loss, sum, N = compute_loss_and_running_values(model_outputs[layer]) 
            self.running_N[layer].assign_add(N)
            self.running_sum[layer].assign_add(sum
            total_loss += 0.0
        return total_loss * self.running_N/self.running_sum
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;2019-10-31 00:57:17.308231: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at nccl_ops.cc:104 : Unknown: Error invoking NCCL: unhandled cuda error
2019-10-31 00:57:17.308341: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Unknown: Error invoking NCCL: unhandled cuda error
         [[{{node NcclAllReduce_7}}]]
         [[NcclAllReduce_5/_26]]
2019-10-31 00:57:17.308471: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Unknown: Error invoking NCCL: unhandled cuda error
         [[{{node NcclAllReduce_7}}]]
&lt;/denchmark-code&gt;

If I change the assign_add call to +=, the model runs in eager mode, but yields the following error in graph mode
&lt;denchmark-code&gt;TypeError: An op outside of the function building code is being passed
a "Graph" tensor. It is possible to have Graph tensors
leak out of the function building context by including a
tf.init_scope in your function building code.
For example, the following function will fail:
  @tf.function
  def has_init_scope():
    my_constant = tf.constant(1.)
    with tf.init_scope():
      added = my_constant * 2
&lt;/denchmark-code&gt;

Is there a preferred way to have persistent variables within your model that are aggregated across multiple GPUs?
Describe the expected behavior
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Other info / logs
NCCL debug info
&lt;denchmark-code&gt;node05-ccncluster:220944:221234 [1] NCCL INFO NET/Socket : Using [0]enp96s0f0:10.102.2.200&lt;0&gt; [1]enp134s0:192.168.4.105&lt;0&gt;
node05-ccncluster:220944:221234 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
node05-ccncluster:220944:221234 [1] NCCL INFO NET/IB : No device found.
NCCL version 2.4.7+cudaCUDA_MAJOR.CUDA_MINOR
node05-ccncluster:220944:224168 [1] NCCL INFO Setting affinity for GPU 1 to 0f,ff000fff
node05-ccncluster:220944:224167 [0] NCCL INFO Setting affinity for GPU 0 to 0f,ff000fff
node05-ccncluster:220944:224167 [0] NCCL INFO Channel 00 :    0   1
node05-ccncluster:220944:224167 [0] NCCL INFO Ring 00 : 0[0] -&gt; 1[1] via P2P/direct pointer
node05-ccncluster:220944:224168 [1] NCCL INFO Ring 00 : 1[1] -&gt; 0[0] via P2P/direct pointer
node05-ccncluster:220944:224167 [0] NCCL INFO Using 256 threads, Min Comp Cap 6, Trees disabled
node05-ccncluster:220944:224167 [0] NCCL INFO comm 0x7f7e38001b30 rank 0 nranks 2 cudaDev 0 nvmlDev 0 - Init COMPLETE
node05-ccncluster:220944:224168 [1] NCCL INFO comm 0x7f7e3c0010b0 rank 1 nranks 2 cudaDev 1 nvmlDev 1 - Init COMPLETE
node05-ccncluster:220944:224164 [0] NCCL INFO Launch mode Group/CGMD
node05-ccncluster:220944:224189 [0] NCCL INFO Setting affinity for GPU 0 to 0f,ff000fff
node05-ccncluster:220944:224190 [0] NCCL INFO Setting affinity for GPU 0 to 0f,ff000fff
node05-ccncluster:220944:224189 [0] NCCL INFO Channel 00 :    0   1
node05-ccncluster:220944:224189 [0] NCCL INFO Ring 00 : 0 -&gt; 1 via P2P/common device
node05-ccncluster:220944:224190 [0] NCCL INFO Ring 00 : 1 -&gt; 0 via P2P/common device
node05-ccncluster:220944:224189 [0] NCCL INFO Using 256 threads, Min Comp Cap 6, Trees disabled
node05-ccncluster:220944:224189 [0] NCCL INFO comm 0x7f7e44001550 rank 0 nranks 2 cudaDev 0 nvmlDev 0 - Init COMPLETE
node05-ccncluster:220944:224190 [0] NCCL INFO comm 0x7f7e30001040 rank 1 nranks 2 cudaDev 0 nvmlDev 0 - Init COMPLETE

node05-ccncluster:220944:224164 [0] external/nccl_archive/src/enqueue.cc:74 NCCL WARN Cuda failure 'invalid device ordinal'
node05-ccncluster:220944:224164 [0] NCCL INFO external/nccl_archive/src/enqueue.cc:175 -&gt; 1
node05-ccncluster:220944:224164 [0] NCCL INFO external/nccl_archive/src/enqueue.cc:437 -&gt; 1
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='mjlbach' date='2019-10-31T09:11:16Z'>
		After some debugging, I think this might have to do with attempting to aggregate the int64?
		</comment>
		<comment id='2' author='mjlbach' date='2019-11-11T02:00:16Z'>
		Hi - can you provide the entire model + training code that you're using above? That will help us reproduce and debug the issue.
The all reduce is most likely triggered from the .assign_add() which will try to aggregate the delta before applying to the mirrored variables. But it's not clear to me why it fails. (because similar things happen for gradient aggregation as well and presumably those are not failing for you?)
When you eager vs graph mode in TF 2.0, what do you mean? perhaps sharing your training code for either case would help. thanks
		</comment>
		<comment id='3' author='mjlbach' date='2019-12-06T21:32:10Z'>
		&lt;denchmark-link:https://github.com/mjlbach&gt;@mjlbach&lt;/denchmark-link&gt;
 Can you please provide the details &lt;denchmark-link:https://github.com/guptapriya&gt;@guptapriya&lt;/denchmark-link&gt;
 is asking?
Please close the issue If the issue was already resolved. Thanks!
		</comment>
		<comment id='4' author='mjlbach' date='2019-12-16T18:54:27Z'>
		I think it was resolved. Closing due to lack of recent activity. Please open new ticket if you see similar issue. Thanks!
		</comment>
		<comment id='5' author='mjlbach' date='2019-12-16T18:54:29Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33868&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33868&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>