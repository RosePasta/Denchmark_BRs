<bug id='39021' author='csxeba' open_date='2020-04-29T11:58:52Z' closed_time='2020-10-30T20:19:55Z'>
	<summary>Buggy behaviour of dataset API</summary>
	<description>
System information

Have I written custom code: yes, see https://colab.research.google.com/drive/1AeVRilpcGp8zb0hZijTIxGWdL9GkzfN_
OS Platform and Distribution: Google Colab (Ubuntu 18.04.3 LTS)
TensorFlow installed from (source or binary): provided by Colab
TensorFlow version (use command below): 2.2.0-rc3
Python version: 3.6.9

Describe the current behavior
At Dataset graph branching points, the node, which is the root of the branching is resampled for each branch during one round of execution. With non-randomized inputs to the Dataset, this does not cause any problems. If the root node is after a .shuffle() call, the branches will receive different inputs in the same computation round.
Describe the expected behavior
Downstream branches should receive the same data even if shuffle() is applied.

&lt;denchmark-link:https://colab.research.google.com/drive/1AeVRilpcGp8zb0hZijTIxGWdL9GkzfN_&gt;https://colab.research.google.com/drive/1AeVRilpcGp8zb0hZijTIxGWdL9GkzfN_&lt;/denchmark-link&gt;

More info:
This behaviour is also present if the dataset is created from a generator, which handles the shuffling implicitly.
Edit: fixed the links here as well
	</description>
	<comments>
		<comment id='1' author='csxeba' date='2020-04-29T12:54:43Z'>
		&lt;denchmark-link:https://github.com/csxeba&gt;@csxeba&lt;/denchmark-link&gt;

The code shared required access permission to view it, please provide with simple standalone code to replicate issue faced.
		</comment>
		<comment id='2' author='csxeba' date='2020-04-30T04:30:55Z'>
		&lt;denchmark-link:https://colab.research.google.com/drive/1AeVRilpcGp8zb0hZijTIxGWdL9GkzfN_&gt;Right link&lt;/denchmark-link&gt;

Sorry, the underscore at the end of the link causes problems with MarkDown
		</comment>
		<comment id='3' author='csxeba' date='2020-04-30T05:49:05Z'>
		i am able to replicate this, please find the &lt;denchmark-link:https://colab.sandbox.google.com/gist/Saduf2019/6b32a8396582948eed6285215435b7af/untitled164.ipynb&gt;gist here&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='csxeba' date='2020-04-30T10:23:23Z'>
		It is also affecting TF 2.0 if that matters.
		</comment>
		<comment id='5' author='csxeba' date='2020-04-30T20:47:28Z'>
		&lt;denchmark-link:https://github.com/aaudiber&gt;@aaudiber&lt;/denchmark-link&gt;
 could you please take a look? thanks
		</comment>
		<comment id='6' author='csxeba' date='2020-04-30T21:18:32Z'>
		Hi &lt;denchmark-link:https://github.com/csxeba&gt;@csxeba&lt;/denchmark-link&gt;
,
This is working as intended. Datasets can be much larger than the memory of a single machine, so Dataset objects act like blueprints for how to produce data (instead of trying to hold the entire dataset at once). Datasets provide a streaming API for consuming data through an iterator. If you want each iterator created on a shuffled dataset to produce elements in the same order, use the reshuffle_each_iteration argument to Datasest.shuffle:
&lt;denchmark-code&gt;index = index.shuffle(buffer_size=len(self.index), reshuffle_each_iteration=False)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='csxeba' date='2020-05-01T12:40:15Z'>
		Hi Audiber, thank you for the clarification!
		</comment>
		<comment id='8' author='csxeba' date='2020-10-30T20:19:56Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39021&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39021&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>