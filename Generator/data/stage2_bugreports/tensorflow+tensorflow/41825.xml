<bug id='41825' author='orangesomethingorange' open_date='2020-07-28T15:19:38Z' closed_time='2020-08-09T11:42:45Z'>
	<summary>Tensorflow-Lite on NDK with C-API fails to provide output - no errors or warnings</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Based on example script
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy S10
TensorFlow Lite version: 2.2
Python version: 3.6
GCC/Compiler version (if compiling from source): clang++ version 7.0

Describe the current behavior
Previously I was running TFLite on Android, with the Java API.
Running the .tflite model from Android's Java API works fine, and I get the expected output.
Now I'm running TFLite on NDK, using the C-API.
Invoking the interpreter in the NDK doesn't seem to do anything.
I can verify the the model is loaded (does not return null) and setting everything up (interpreter / options / input and output tensors / etc) seems OK too.
The input and output are both float32 arrays:

I can verify input array data integrity before TfLiteTensorCopyFromBuffer (all float values are correct and none-zero).
Output array is a float array (in the proper size) initialized with zeros.
Extracting the inference result with TfLiteTensorCopyToBuffer does not change the output array - it's still all zeros.

Describe the expected behavior
Invoking the interpreter should provide an output array, a result.
Standalone code to reproduce the issue
In my native C++ file:
&lt;denchmark-code&gt;// Following the example in c_api.h

// Input
// input is just a single tensor in size
// input is: vector&lt;float&gt; input_float_array;
// accessing input values with input_float_array.at(x), for example,
// provides float32 values - the first few are:
// [ 43.36578, 72.9673, 98.4356, 12.7865, ... ]

// Output
// output is just a single tensor in size
// output is: vector&lt;float&gt; output_float_array;
// output is initialized with zeros:
// [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ... ]
// and is not constant

TfLiteModel * model = TfLiteModelCreateFromFile(
                reinterpret_cast&lt;const char *&gt;(model_file_path));

// checkpoint "model":
// if (model == nullptr)
// result: model is not null - model_file_path found
// in model_file_path there's a .tflite file

TfLiteInterpreterOptions * options = TfLiteInterpreterOptionsCreate();
TfLiteInterpreterOptionsSetNumThreads(options, 2);
TfLiteInterpreter * interpreter = TfLiteInterpreterCreate(model, options);
TfLiteInterpreterAllocateTensors(interpreter);
TfLiteTensor * input_tensor =
        TfLiteInterpreterGetInputTensor(interpreter, 0);

// checkpoint "before"
// input_float_array has legitimate float values, at this point:
// [ 43.36578, 72.9673, 98.4356, 12.7865, ... ]

TfLiteTensorCopyFromBuffer(input_tensor,
        input_float_array.data(),
        input_float_array.size() * sizeof(float));
TfLiteInterpreterInvoke(interpreter);
const TfLiteTensor * output_tensor =
        TfLiteInterpreterGetOutputTensor(interpreter, 0);
TfLiteTensorCopyToBuffer(output_tensor,
        output_float_array.data(),
        output_float_array.size() * sizeof(float));

// checkpoint "after"
// output_float_array remained all zeros, at this point:
// [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ... ]

TfLiteInterpreterDelete(interpreter);
TfLiteInterpreterOptionsDelete(options);
TfLiteModelDelete(model);

// some alternatives to access data and verify integrity:
//
// replacing "input_float_array.data()" with "&amp;(* input_float_array.begin())"
// and "output_float_array.data()" with "&amp;(* output_float_array.begin())"
//
// initializing output_float_array with ones instead of zeros:
// [ 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ... ]

// I also tried to change a few models
// (all with same input / output and characteristics)
&lt;/denchmark-code&gt;

Other info / logs
No errors of any kind or anything to show.
The code runs smoothly, but nothing happens - the output array remains unchanged.
The exact same models, when run with Java API, work just fine.
	</description>
	<comments>
		<comment id='1' author='orangesomethingorange' date='2020-07-29T14:36:24Z'>
		&lt;denchmark-h:h3&gt;Update&lt;/denchmark-h&gt;

To create the original model I used Keras.
Now I've created a new model - this is the same model only I took out 2 operators from the original one:

Conv2DTranspose
BatchNormalization

and I do get results from the model - it's not the expected result values, it's just a mess.
Both operators appear in &lt;denchmark-link:https://www.tensorflow.org/lite/guide/ops_compatibility&gt;TF / TFL ops compatibility&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='2' author='orangesomethingorange' date='2020-07-30T12:59:58Z'>
		&lt;denchmark-link:https://github.com/orangesomethingorange&gt;@orangesomethingorange&lt;/denchmark-link&gt;
 Do you std::cout the TfLiteTensorCopyFromBuffer() and TfLiteTensorCopyToBuffer() return value?They must be  kTfLiteOK.
		</comment>
		<comment id='3' author='orangesomethingorange' date='2020-07-30T14:35:02Z'>
		Hey &lt;denchmark-link:https://github.com/patterson163&gt;@patterson163&lt;/denchmark-link&gt;
 ,
Yes - both  and 's return values are .
		</comment>
		<comment id='4' author='orangesomethingorange' date='2020-07-30T14:53:52Z'>
		&lt;denchmark-h:h3&gt;Update 2&lt;/denchmark-h&gt;

I created 2 more models:

Identity model

&lt;denchmark-code&gt;in = Input(...)
out = in
model = Model(inputs = [in], outputs = [out])
&lt;/denchmark-code&gt;


Single operator (Conv2D) model

&lt;denchmark-code&gt;in = Input(...)
a = in
b = Conv2D(...)(a)
out = b
model = Model(inputs = [in], outputs = [out])
&lt;/denchmark-code&gt;

When running inference with the identity model - the values are again unchanged (output float array was initialized with ones, and stays that way).
When running inference with the single convolution model - I do see results (meaning non-zero float values in the output float array), but again - same as the first update, it's not the expected results but some mess.
		</comment>
		<comment id='5' author='orangesomethingorange' date='2020-07-30T15:00:52Z'>
		Running the same model in Android's Runtime (in Java) produce the expected results.
Isn't Java only an envelope to the C-code?
Are the input / output (byte) arrays expected to be in some unique state?
		</comment>
		<comment id='6' author='orangesomethingorange' date='2020-07-30T16:20:19Z'>
		Would you mind attaching one of the .tflite models that you're using? I'm assuming these results are being generated on the Android device? Did you use our prebuilt C API libs, or did you build from source? If the latter, what build command did you use (and which NDK version)?
We saw a similar issue reported previously (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/40318&gt;#40318&lt;/denchmark-link&gt;
), but it was an issue with using the wrong number of elements in the input/output.
		</comment>
		<comment id='7' author='orangesomethingorange' date='2020-08-03T13:03:22Z'>
		Hey &lt;denchmark-link:https://github.com/jdduke&gt;@jdduke&lt;/denchmark-link&gt;
 ,
Yes, the results are generated on an Android device.
I used the prebuilt libraries (following the instructions &lt;denchmark-link:https://www.tensorflow.org/lite/guide/android#use_tflite_c_api&gt;here&lt;/denchmark-link&gt;
).
I did not build from source, but still - NDK version is .
I only got all zeros on some models.
Following your reference to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/40318&gt;Issue #40318&lt;/denchmark-link&gt;
, I played with the input/output sizes passed to  / . Output was indeed changed, but still not a desired result - still only a mess of values.
When using a model which provided non-zero results, I compared the following calculations:

size_t input_data_size_1 = input_float_array.size() * sizeof(float)
size_t input_data_size_2 = TfLiteTensorByteSize(input_tensor)
size_t input_data_size_3 = input_image_size * input_image_channels * sizeof(float)

(All variables are exactly what you think they are).
** Did the same for the output.
All above data size calculations get the exact same (positive) value.
Increasing and decreasing that size does change the output, but still just a mess of float values.
I'm attaching a link below with some files:

Two .tflite model files: one has a single operator and the other has multiple operators.
Some 512x512 input images (which were translated into input float arrays) that I got from the internet.
The expected results for an output image (in this case the green channel is colored, the result is 1-dim).
The output float array results (translated back into an image) that I got on the Android device.

(*) Note that the input has 12 channels and the output just 1.
See &lt;denchmark-link:https://gofile.io/d/t0HttZ&gt;this link&lt;/denchmark-link&gt;
 for the files.
		</comment>
		<comment id='8' author='orangesomethingorange' date='2020-08-03T19:20:15Z'>
		Thanks for the repro. And you used the prebuilt libs from the &lt;denchmark-link:https://bintray.com/google/tensorflow/tensorflow-lite#files/org%2Ftensorflow%2Ftensorflow-lite%2F2.2.0&gt;TFLite 2.2 release&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='9' author='orangesomethingorange' date='2020-08-04T06:46:20Z'>
		
Thanks for the repro. And you used the prebuilt libs from the TFLite 2.2 release?

Yes
		</comment>
		<comment id='10' author='orangesomethingorange' date='2020-08-04T06:58:18Z'>
		Also, I created the single-operator model again, but I set the input patch-size to 4x4 (instead of 512x512) and it works fine (input sizes of 512x512 / 1200x1600 - I get the expected results).
You've mentioned the memcpy operation in the tensor buffer - does TFLite limit memory allocation by any chance?
		</comment>
		<comment id='11' author='orangesomethingorange' date='2020-08-06T18:00:21Z'>
		I've tried a local repro but don't observe the same issue.

You've mentioned the memcpy operation in the tensor buffer - does TFLite limit memory allocation by any chance?

Here is the implementation: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/c_api.cc#L215&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/c_api.cc#L215&lt;/denchmark-link&gt;
. We just validate that the provided byte length matches the tensor byte length.
One thing you could do is check the TfLiteTensor raw buffer value, that is, the input_tensor-&gt;data.f and output_tensor-&gt;data.f array, just to make sure it's not an issue with the CopyFrom/CopyTo buffer calls.

Some 512x512 input images (which were translated into input float arrays) that I got from the internet.

Can you explain how you loaded the 512x512 RGB input image into a 12-channel (512x512x12) input tensor?

TfLiteInterpreterInvoke(interpreter);

Can you also make sure that the return value for this call was kTfLiteOk?
		</comment>
		<comment id='12' author='orangesomethingorange' date='2020-08-09T11:42:11Z'>
		Hey &lt;denchmark-link:https://github.com/jdduke&gt;@jdduke&lt;/denchmark-link&gt;
 ,
&lt;denchmark-h:h3&gt;Regarding the issue:&lt;/denchmark-h&gt;

The issue was in my design - to speed up performance I'm passing a pointer to the Native environment.
Apparently on Android when encoding / decoding images (which may be passed through a  object) with the same  and  parameters there's a default header shift.
I think this issue is addressed in Android 11's &lt;denchmark-link:https://developer.android.com/ndk/guides/image-decoder&gt;Image Decoder API&lt;/denchmark-link&gt;
 - I was using an older Android API.
This issue did not occur when using Tensorflow-Lite via the Java API.
Currently I can get the inference to run OK, but I have to copy the image (or patches) data, serially, 4 times (2 for the Native environment pointer and 2 for the TF buffer), which slows the whole thing down (which kinda defeats the purpose of running an inference in the Native environment). I'll have to solve the performance issue now.
Also - for the identity model I replaced out = in with out = tensorflow.identity(in), which seem to be helpful.
&lt;denchmark-h:h3&gt;Regarding your questions (for future reference mostly):&lt;/denchmark-h&gt;

The input image (3 channels) object was cloned 4 times and "stacked" one on top of the other, in a way that mimics Numpy arrays behavior (since this is what I used on the Tensorflow model), so each "pixel" (that is, every 3 float values) was cloned 4 times -
For example: every 4 bytes represent a single float value f, and each float value f will represent a single value in a single channel, within one matrix (so for a gray-scale image &lt;f&gt; is the pixel value, and for an RGB image &lt;fa,fb,fc&gt; is the pixel value). If the first pixel is represented by &lt;f0,f1,f2&gt;, the second pixel is represented by &lt;f3,f4,f5&gt;, and so on - then the byte array of the input given to the inference will be bytes representing the data in the following order:
&lt;f0,f1,f2&gt;, &lt;f0,f1,f2&gt;, &lt;f0,f1,f2&gt;, &lt;f0,f1,f2&gt; | &lt;f3,f4,f5&gt;, &lt;f3,f4,f5&gt;, &lt;f3,f4,f5&gt;, &lt;f3,f4,f5&gt; | ... and so on and so forth.
Return values from TfLiteTensorCopyToBuffer / TfLiteTensorCopyFromBuffer we're both kTfLiteOk.
		</comment>
		<comment id='13' author='orangesomethingorange' date='2020-08-09T11:42:45Z'>
		Thanks :)
		</comment>
		<comment id='14' author='orangesomethingorange' date='2020-08-09T11:42:47Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41825&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41825&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='15' author='orangesomethingorange' date='2020-08-10T15:37:10Z'>
		Thanks for the update!
In general, Java-based inference performance should be close to native inference performance when using ByteBuffer (or FloatBuffer) inputs and outputs. We are working on a (formal) way of allowing the user to inject their own CPU buffer in C++ to avoid copying from their source buffer to the Tensor buffer. There will be an update in the release notes for this.
		</comment>
		<comment id='16' author='orangesomethingorange' date='2020-08-11T11:32:26Z'>
		
Thanks for the update!
In general, Java-based inference performance should be close to native inference performance when using ByteBuffer (or FloatBuffer) inputs and outputs. We are working on a (formal) way of allowing the user to inject their own CPU buffer in C++ to avoid copying from their source buffer to the Tensor buffer. There will be an update in the release notes for this.

That's great news! 👍  I'm having issues with my algorithm's runtime, and the buffer-copies are the biggest part of that... Thanks for the info - I'll be sure to check the release notes :)
		</comment>
	</comments>
</bug>