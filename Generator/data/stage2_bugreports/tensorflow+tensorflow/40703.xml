<bug id='40703' author='KTTrev' open_date='2020-06-23T05:24:35Z' closed_time='2020-09-17T10:13:40Z'>
	<summary>[Regression] on_train_batch_begin callbacks with no batch number and size</summary>
	<description>
Yep this should work in TF2.1 as well, here's a full example (had to fix the metric code a bit):
import tensorflow as tf

class Count(tf.keras.metrics.Metric):
  def __init__(self, name=None, dtype=None, **kwargs):
    super(Count, self).__init__(name, dtype, **kwargs)
    self.count = tf.Variable(0)

  def update_state(self, y_true, y_pred, sample_weight=None):
    first_tensor = tf.nest.flatten(y_true)[0]
    batch_size = tf.shape(first_tensor)[0]
    self.count.assign_add(batch_size)

  def result(self):
    return tf.identity(self.count)


class PrintInfo(tf.keras.callbacks.Callback):
  def on_train_batch_end(self, batch, logs):
    print('Batch number: {}'.format(batch))
    print('Samples seen this epoch: {}'.format(logs['counter']))

model = tf.keras.Sequential([tf.keras.layers.Dense(1)])
model.compile(optimizer='sgd', loss='mse', metrics=[Count(name='counter')])
x, y = tf.ones((10, 10)), tf.ones((10, 1))
model.fit(x, y, batch_size=2, callbacks=[PrintInfo()], verbose=2)
Originally posted by @omalleyt12 in #39314 (comment)
I would like to use the same method, but with on_train_batch_begin, and it doesn't work. Actually the logs remain empty even after using the new metric. How can I use the batch size in a callback, with on_train_batch_begin?
	</description>
	<comments>
		<comment id='1' author='KTTrev' date='2020-06-23T05:29:19Z'>
		&lt;denchmark-link:https://github.com/KTTrev&gt;@KTTrev&lt;/denchmark-link&gt;

Please confirm if &lt;denchmark-link:https://colab.research.google.com/gist/Saduf2019/91c05885731c39ce65dbc80e2ae4416c/untitled237.ipynb&gt;this gist&lt;/denchmark-link&gt;
 replicates the issue reported.
		</comment>
		<comment id='2' author='KTTrev' date='2020-06-23T05:52:02Z'>
		The gist is working well. The issue I'm facing is when I change on_train_batch_end with on_train_batch_begin. I want to use the batch size at the beginning of a training, however the logs of on_train_batch_begin is empty. One solution that I found to solve this problem, is using a metric as quoted in  the code I mentioned above. However the code only works for on_train_batch_end. I'm looking for something similar, or another way to use the batch size in on_train_batch_begin.
		</comment>
		<comment id='3' author='KTTrev' date='2020-06-24T01:11:30Z'>
		&lt;denchmark-link:https://github.com/KTTrev&gt;@KTTrev&lt;/denchmark-link&gt;
 Based on the &lt;denchmark-link:https://keras.io/guides/writing_your_own_callbacks/&gt;Keras docs&lt;/denchmark-link&gt;
, I think the dictionary of  is empty when you use . On that page, there is an example also which list the  available to access.
&lt;denchmark-code&gt;Start epoch 0 of training; got log keys: []
...Training: start of batch 0; got log keys: []
...Training: end of batch 0; got log keys: ['loss', 'mean_absolute_error']
...Training: start of batch 1; got log keys: []
...Training: end of batch 1; got log keys: ['loss', 'mean_absolute_error']
...Training: start of batch 2; got log keys: []
...Training: end of batch 2; got log keys: ['loss', 'mean_absolute_error']
...Training: start of batch 3; got log keys: []
...Training: end of batch 3; got log keys: ['loss', 'mean_absolute_error']
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='KTTrev' date='2020-06-24T01:35:52Z'>
		Exactly, but do you have any solution on how I can use the batch size within 'on_train_batch_begin' (which was suppose to be in the logs)?
		</comment>
		<comment id='5' author='KTTrev' date='2020-06-24T01:36:32Z'>
		&lt;denchmark-link:https://github.com/KTTrev&gt;@KTTrev&lt;/denchmark-link&gt;
 Yes. This is regression issue. In TF2.1, you can do this
&lt;denchmark-code&gt;  def on_train_batch_begin(self, batch, logs=None):
    keys = list(logs.keys()) # In TF2.2, this list is empty
    print("...Training: start of batch {}; got log keys: {}".format(batch, keys))
    print('Batch number: {}'.format(batch))
    print('Samples seen this epoch: {}'.format(logs['size']))
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='KTTrev' date='2020-06-25T00:20:57Z'>
		&lt;denchmark-link:https://github.com/KTTrev&gt;@KTTrev&lt;/denchmark-link&gt;
 Thanks for the issue!
on_train_batch_begin no longer receives any keys in the logs, could you explain your use case? I can advise on workarounds
		</comment>
		<comment id='7' author='KTTrev' date='2020-06-25T00:50:13Z'>
		&lt;denchmark-link:https://user-images.githubusercontent.com/47731576/85640784-02b0b300-b68d-11ea-8e37-75b0fd282a17.png&gt;&lt;/denchmark-link&gt;

Here is a screenshot of the model that I'm trying to build. You can notice that I'm using a function  at the output layer, which contain some parameters , , ,  . However these parameters are not static; each image going through the network uses diferent parameters. That's why, I want to update them during training, validation or prediction phases, especially when using mini batches.
		</comment>
		<comment id='8' author='KTTrev' date='2020-09-17T10:13:40Z'>
		&lt;denchmark-link:https://github.com/KTTrev&gt;@KTTrev&lt;/denchmark-link&gt;
  As per the above comment, it is the expected behavior.  Closing this issue as of now. Please feel free to re open the issue if necessary. Thanks!.
		</comment>
		<comment id='9' author='KTTrev' date='2020-09-17T10:13:42Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40703&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40703&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>