<bug id='34521' author='7Z0nE' open_date='2019-11-22T13:43:55Z' closed_time='2020-02-28T19:41:09Z'>
	<summary>keras.Model does not work with keras.Input that was created with `tensor=` kwarg.</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 LTS
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.15.0-rc2
Python version: 3.7.4
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

&lt;denchmark-h:h3&gt;current behavior&lt;/denchmark-h&gt;

keras.Input and keras.InputLayer give the option to pass in a tf.placeholder through the argument tensor=None. When using the output of keras.Input(tensor=some_placeholder) as inputs to the functional keras.Model and subsequently calling model.predict(batch) on the newly created model an error occurs:
&lt;denchmark-code&gt;ValueError: ('Error when checking model input: expected no data, but got:', array([[0., 1.],
       [2., 3.],
       [4., 5.],
       [6., 7.],
       [8., 9.]]))
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;expected behavior&lt;/denchmark-h&gt;

No error occurs and keras.Model correctly feeds the inputs through the graph.
&lt;denchmark-h:h3&gt;Code to reproduce the issue&lt;/denchmark-h&gt;

The following code breaks:
snippet 1
&lt;denchmark-code&gt;import tensorflow as tf
import tensorflow.keras as keras
import numpy as np

print("TF VERSION: ", tf.__version__)

ph = tf.placeholder(shape=[None,2], dtype=tf.float32)

# create input layer from a predefined tensorflow placeholder
inputs = keras.Input(tensor=ph)
# perform some operation
A = tf.constant([[0.5, 0],[0, 0.5]], dtype=tf.float32)
x = tf.linalg.matmul(inputs, A)
outputs = tf.reduce_sum(x, axis=1)

# make a model
model = keras.Model(inputs=inputs, outputs=outputs)

# try to feed a batch through the model
batch = np.linspace(0, 9, 10).reshape(5, 2)
out = model.predict(batch)

print(out)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Explanation&lt;/denchmark-h&gt;

I already debugged through the keras code and I think I spotted the issue:
tesorflow/python/keras/engine/network.py, lines 342-352
snippet 2
&lt;denchmark-code&gt;for i, layer in enumerate(self._input_layers):
      self.input_names.append(layer.name)
      if layer.is_placeholder:
        self._feed_input_names.append(layer.name)
        # Use batch_input_shape here because non-eager composite tensors may not
        # have a shape attribute that's meaningful (sparse, for instance, has
        # a tensor that's non-constant and needs to be fed). This means that
        # input layers that create placeholders will need to have the
        # batch_input_shape attr to allow for input shape validation.
        self._feed_input_shapes.append(layer._batch_input_shape)
        self._feed_inputs.append(layer.input)
&lt;/denchmark-code&gt;

snippet 2 is taken from the Network constructor. As can be seen, only tensors coming from layers with is_placeholder set to true are added to the _feed_inputs array.
In out case, this array is empty, because our InputLayer that was created from a tf.placeholder has is_placeholder set to false.
Looking at the constructor code of InputLayer we can see why this is the case:
tensorflow/python/keras/engine/input_layer.py, lines 115 - 138
snippet 3
&lt;denchmark-code&gt;    if input_tensor is None:
      if input_shape is not None:
        batch_input_shape = (batch_size,) + tuple(input_shape)
      else:
        batch_input_shape = None
      graph = backend.get_graph()
      with graph.as_default():
        input_tensor = backend.placeholder(
            shape=batch_input_shape,
            dtype=dtype,
            name=self.name,
            sparse=sparse,
            ragged=ragged)

      self.is_placeholder = True
      self._batch_input_shape = batch_input_shape
    else:
      if not tf_utils.is_symbolic_tensor(input_tensor):
        raise ValueError('You should not pass an EagerTensor to `Input`. '
                         'For example, instead of creating an '
                         'InputLayer, you should instantiate your model and '
                         'directly call it on your input.')
      self.is_placeholder = False
      self._batch_input_shape = tuple(input_tensor.shape.as_list())
&lt;/denchmark-code&gt;

input_tensor has the value from the tensor= argument. In our case, this holds the placeholder we passed to Input(tensor=ph). Since it is not None self.is_placeholder = False is run in the else-case.
This subsequently results in the layer not being added to the _feed_inputs of the model which then does not expect any inputs to the predict method.
The issue can be worked around by manually setting the is_placeholder field of our input layer.
snippet 4
&lt;denchmark-code&gt;inputs._keras_history[0].is_placeholder = True
&lt;/denchmark-code&gt;

Adding this line of code just below line 10 of snippet 1 resolves the error.
Fixed code:
snippet 5
&lt;denchmark-code&gt;import tensorflow as tf
import tensorflow.keras as keras
import numpy as np

print("TF VERSION: ", tf.__version__)

ph = tf.placeholder(shape=[None,2], dtype=tf.float32)

# create input layer from a predefined tensorflow placeholder
inputs = keras.Input(tensor=ph)
inputs._keras_history[0].is_placeholder=True
# perform some operation
A = tf.constant([[0.5, 0],[0, 0.5]], dtype=tf.float32)
x = tf.linalg.matmul(inputs, A)
outputs = tf.reduce_sum(x, axis=1)

# make a model
model = keras.Model(inputs=inputs, outputs=outputs)

# try to feed a batch through the model
batch = np.linspace(0, 9, 10).reshape(5, 2)
out = model.predict(batch)

print(out)
&lt;/denchmark-code&gt;

Output:
&lt;denchmark-code&gt;TF VERSION:  1.15.0-rc2
[0.5 2.5 4.5 6.5 8.5]
&lt;/denchmark-code&gt;

Since I can not think of any reason, why an InputLayer created from a placeholder should have is_placeholder=False I think this issue can be resolved by removing the respective line of code.
	</description>
	<comments>
		<comment id='1' author='7Z0nE' date='2019-11-25T06:37:34Z'>
		I have tried on colab with TF version 1.15 and was able to reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/bba32c27b2cba874adbb8d2addcb9285/untitled403.ipynb&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='2' author='7Z0nE' date='2020-02-28T19:41:09Z'>
		&lt;denchmark-link:https://github.com/7Z0nE&gt;@7Z0nE&lt;/denchmark-link&gt;
 Thanks for the issue!
Placeholders are removed in TF2, instead you can write the code like this:
import tensorflow as tf
import tensorflow.keras as keras
import numpy as np

print("TF VERSION: ", tf.__version__)


# create input layer from a predefined tensorflow placeholder
inputs = keras.Input(shape=[2,], dtype=tf.float32)
# perform some operation
A = tf.constant([[0.5, 0],[0, 0.5]], dtype=tf.float32)
x = tf.linalg.matmul(inputs, A)
outputs = tf.reduce_sum(x, axis=1)

# make a model
model = keras.Model(inputs=inputs, outputs=outputs)

# try to feed a batch through the model
batch = np.linspace(0, 9, 10).reshape(5, 2)
out = model.predict(batch)

print(out)
Unfortunately we are not able to backport fixes like this to earlier version.
Please try with the latest tf-nightly pip install -U tf-nightly
		</comment>
		<comment id='3' author='7Z0nE' date='2020-02-28T19:41:10Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34521&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34521&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>