<bug id='41328' author='mattroos' open_date='2020-07-13T01:29:43Z' closed_time='2020-07-14T01:25:08Z'>
	<summary>Error: "Data adapters should be mutually exclusive for handling inputs. Found multiple adapters to handle" error when calling `model.fit` with imagenet2012 TFDS.</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.2.0 (also tried 2.3 and tf-nightly)
Python version: 3.6.9
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 10.1 / 7.6.5
GPU model and memory: RTX 2080 Super, 8 GB

Describe the current behavior
When trying to fit a VGG19 model architecture with ImageNet2012 data from TFDS (previously manually downloaded and but with download_and_prepare()), it give the following error:

RuntimeError: Data adapters should be mutually exclusive for handling inputs. Found multiple adapters [&lt;class 'tensorflow.python.keras.engine.data_adapter.GeneratorDataAdapter'&gt;, &lt;class 'tensorflow.python.keras.engine.data_adapter.CompositeTensorDataAdapter'&gt;] to handle input: &lt;class 'tensorflow.python.data.ops.iterator_ops.OwnedIterator'&gt;, &lt;class 'NoneType'&gt;

This is very similar to issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/33811&gt;#33811&lt;/denchmark-link&gt;
. However, the error persists even with newer or nightly versions of TF.
Describe the expected behavior
Successfully start training.
Standalone code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
&lt;denchmark-code&gt;batch_size = 12
n_train = 1281167
ds_train,  ds_train_info = tfds.load(name='imagenet2012', download=True, with_info=True,
                                     data_dir='/Data/tfds/', split='train', as_supervised=True,
                                     download_and_prepare_kwargs={'download_dir':'/hdd/Data/tfds/imagenet2012/',})
ds_train = ds_train.map(lambda x, y: (tf.image.resize(x, [224, 224], method='bilinear'), y), num_parallel_calls=tf.data.experimental.AUTOTUNE)
ds_train = ds_train.batch(batch_size)
ds_train = ds_train.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
ds_train = iter(ds_train)

model = tf.keras.applications.VGG19(include_top=True, weights=None, input_tensor=None, input_shape=None,
                                    pooling=None, classes=1000, classifier_activation='softmax')
optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)
loss_instance = SparseCategoricalCrossentropy(from_logits=False)
model.compile(optimizer=optimizer, loss=loss_instance)

epochs = 500
steps_per_epoch = n_train//batch_size
history = model.fit(x=ds_train, epochs=epochs,
                    steps_per_epoch=steps_per_epoch)

&lt;/denchmark-code&gt;

Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
&lt;denchmark-code&gt;---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
~/scratch/imagenet/train_vgg19.py in &lt;module&gt;
    147 # history = model.fit_generator(data_train, epochs=epochs,
    148 history = model.fit(x=ds_train, epochs=epochs,
--&gt; 149                     steps_per_epoch=steps_per_epoch)
    150                     # callbacks=callbacks,
    151                     # validation_data=ds_val,

~/python_envs/env_p3.6.9_tf2.2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
     64   def _method_wrapper(self, *args, **kwargs):
     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
---&gt; 66       return method(self, *args, **kwargs)
     67 
     68     # Running inside `run_distribute_coordinator` already.

~/python_envs/env_p3.6.9_tf2.2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
    813           workers=workers,
    814           use_multiprocessing=use_multiprocessing,
--&gt; 815           model=self)
    816 
    817       # Container that configures and calls `tf.keras.Callback`s.

~/python_envs/env_p3.6.9_tf2.2/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)
   1097     self._insufficient_data = False
   1098 
-&gt; 1099     adapter_cls = select_data_adapter(x, y)
   1100     self._adapter = adapter_cls(
   1101         x,

~/python_envs/env_p3.6.9_tf2.2/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py in select_data_adapter(x, y)
    967         "handling inputs. Found multiple adapters {} to handle "
    968         "input: {}, {}".format(
--&gt; 969             adapter_cls, _type_name(x), _type_name(y)))
    970   return adapter_cls[0]
    971 

RuntimeError: Data adapters should be mutually exclusive for handling inputs. Found multiple adapters [&lt;class 'tensorflow.python.keras.engine.data_adapter.GeneratorDataAdapter'&gt;, &lt;class 'tensorflow.python.keras.engine.data_adapter.CompositeTensorDataAdapter'&gt;] to handle input: &lt;class 'tensorflow.python.data.ops.iterator_ops.OwnedIterator'&gt;, &lt;class 'NoneType'&gt;
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='mattroos' date='2020-07-13T20:32:02Z'>
		Using imagenet2012 dataset required registration, so used imagenet_resized instead.
Was able to reproduce the issue with &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/d094f9a24f13b64e438e8f5b0d4e9874/41328.ipynb&gt;TF v2.2&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/6341e73feef64c7a4df624fa19b16c53/41328-tf-nightly.ipynb&gt;TF-nightly&lt;/denchmark-link&gt;
. Please find the attached gist. Thanks!
		</comment>
		<comment id='2' author='mattroos' date='2020-07-14T00:53:40Z'>
		&lt;denchmark-link:https://github.com/mattroos&gt;@mattroos&lt;/denchmark-link&gt;
 I changed one line of your code and after that code works without any issue.
The following line was commented.
#ds_train = iter(ds_train)
Please take a look at the &lt;denchmark-link:https://colab.research.google.com/gist/jvishnuvardhan/bf5d39f8da3277601c5e77027720efe2/41328-tf-nightly.ipynb&gt;gist here&lt;/denchmark-link&gt;
.
Please note that I have changed epochs, batch_size etc, to run the code faster. Thanks!
Please verify once and close the issue if this was resolved for you. Thanks!
		</comment>
		<comment id='3' author='mattroos' date='2020-07-14T01:25:04Z'>
		Thanks, &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/amahendrakar&gt;@amahendrakar&lt;/denchmark-link&gt;
! Yes, that resolved the issue.
At some point during my coding I came to think that the tfds.load() was returning a generator, which then had to be converted to an iterator. I can't seem to recreate that condition so perhaps I mixed this up with something else I was coding.
Many Thanks,
Matt
		</comment>
		<comment id='4' author='mattroos' date='2020-07-14T01:25:09Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41328&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41328&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='mattroos' date='2020-09-26T21:48:20Z'>
		&lt;denchmark-link:https://github.com/mattroos&gt;@mattroos&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/amahendrakar&gt;@amahendrakar&lt;/denchmark-link&gt;
, please, re-open the issue. I suggest it to be handled differently:
&lt;denchmark-code&gt;import tensorflow as tf
features =  tf.random.normal(shape=(100, 1, 10))
labels = tf.random.normal((100,1,1))
dataset = tf.data.Dataset.from_tensor_slices((features,labels))
ds_iter = iter(dataset)
x = tf.keras.layers.Input(shape=[10])
y_pred = tf.keras.layers.Dense(1, activation='sigmoid', name="L0")(x)
model = tf.keras.Model(x, y_pred)
model.compile(optimizer='sgd', loss='mse',)
model.fit(ds_iter, epochs=1)
&lt;/denchmark-code&gt;

The code above (like the TS code) triggers error RuntimeError: Data adapters should be mutually exclusive for handling inputs, however this is a good kind of problem to have. There are 2 adapters that claim ability to  handle such iterator: CompositeTensorDataAdapter and GeneratorDataAdapter.
If there is more than single adaptor which can handle certain data - great, just pick the best of them and return it from the function. The adators can be prioritized by changing order in ALL_ADAPTER_CLS.
I suggest not to throw  but instead just return . The change requires deleting &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/data_adapter.py#L958-L96&gt;5 lines&lt;/denchmark-link&gt;
 in 
Later I ran tests with tf.python.keras.engine.data_adapter.select_data_adapter forcing it to choose one of the adapters for tensorflow.python.data.ops.iterator_ops.OwnedIterator:


GeneratorDataAdapter works fine with tensorflow.python.data.ops.iterator_ops.OwnedIterator,


CompositeTensorDataAdapter fails with error AttributeError: 'IteratorSpec' object has no attribute '_to_batched_tensor_list' error stack here Apparently some change is needed either in CompositeTensorDataAdapter.can_handle() or in the handling process itself.


		</comment>
	</comments>
</bug>