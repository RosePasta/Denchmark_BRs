<bug id='37151' author='pingsutw' open_date='2020-02-28T03:21:01Z' closed_time='2020-06-23T03:08:51Z'>
	<summary>Run distributed training mnist failed</summary>
	<description>
My dependencies
&lt;denchmark-code&gt;Python 3.6.8 (default, Aug  7 2019, 17:28:10) 
[GCC 4.8.5 20150623 (Red Hat 4.8.5-39)] on linux
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; print(tf.__version__)
2.2.0-dev20200227
&lt;/denchmark-code&gt;

Here is my code
&lt;denchmark-link:https://gist.github.com/pingsutw/bd4e1b1b2d4055d4e807abcdd2f0d6f4&gt;https://gist.github.com/pingsutw/bd4e1b1b2d4055d4e807abcdd2f0d6f4&lt;/denchmark-link&gt;

I use both ParameterServerStrategy and MultiWorkerMirroredStrategy all failed.
I can successfully run this tutorial locally.
And I follow this &lt;denchmark-link:https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras&gt;link&lt;/denchmark-link&gt;
 to run distributed training, but always fails.
Instead of run multiworker locally, I run on 3 machines.
Below are error messages
&lt;denchmark-code&gt;2020-02-27 21:54:15.735213: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/cloudera/parcels/CDH/lib64:/opt/jdk1.8.0_221/jre/lib/amd64/server
2020-02-27 21:54:15.735272: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)
2020-02-27 21:54:15.735320: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (kevin2): /proc/driver/nvidia/version does not exist
2020-02-27 21:54:15.735784: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-02-27 21:54:15.849834: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2494140000 Hz
2020-02-27 21:54:15.933775: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55fd9c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-02-27 21:54:15.933870: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-02-27 21:54:16.139575: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; kevin2:38518}
2020-02-27 21:54:16.139650: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:43854, 1 -&gt; kevin2:36868}
2020-02-27 21:54:16.157359: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:390] Started server with target: grpc://localhost:43854
2020-02-27 21:54:16.157667: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:394] Server already started (target: grpc://localhost:43854)
WARNING:tensorflow:From /yarn/nm/usercache/root/appcache/application_1582468300204_0090/container_1582468300204_0090_01_000003/tf2.zip/tf2/lib64/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /yarn/nm/usercache/root/appcache/application_1582468300204_0090/container_1582468300204_0090_01_000003/tf2.zip/tf2/lib64/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /yarn/nm/usercache/root/appcache/application_1582468300204_0090/container_1582468300204_0090_01_000003/tf2.zip/tf2/lib64/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:96: DistributedIteratorV1.initialize (from tensorflow.python.distribute.input_lib) is deprecated and will be removed in a future version.
Instructions for updating:
Use the iterator's `initializer` property instead.
WARNING:tensorflow:From /yarn/nm/usercache/root/appcache/application_1582468300204_0090/container_1582468300204_0090_01_000003/tf2.zip/tf2/lib64/python3.6/site-packages/tensorflow_estimator/python/estimator/util.py:96: DistributedIteratorV1.initialize (from tensorflow.python.distribute.input_lib) is deprecated and will be removed in a future version.
Instructions for updating:
Use the iterator's `initializer` property instead.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='pingsutw' date='2020-03-10T03:23:01Z'>
		&lt;denchmark-link:https://github.com/pingsutw&gt;@pingsutw&lt;/denchmark-link&gt;
 Can you post the failure stack trace when running with MultiWorkerMirroredStrategy? The messages that you pasted above are warning messages.
Also, we don't currently support ParameterServer strategy with TF 2 and Keras native APIs so you will probably see an error if you try to use it with Keras compile/fit APIs.
		</comment>
		<comment id='2' author='pingsutw' date='2020-06-23T03:08:51Z'>
		Closing this issue since there's no way to reproduce the error, and as mentioned above ParameterServer strategy is currently not supported for the Keras API.
		</comment>
		<comment id='3' author='pingsutw' date='2020-06-23T03:08:53Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37151&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37151&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>