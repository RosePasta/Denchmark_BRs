<bug id='39542' author='eliorc' open_date='2020-05-14T12:45:09Z' closed_time='2020-05-19T17:23:54Z'>
	<summary>Cannot use TimeDistributed with hub.KerasLayer</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.2
Python version: 3.7
Bazel version (if compiling from source): None
GCC/Compiler version (if compiling from source): None
CUDA/cuDNN version: None
GPU model and memory: None

Describe the current behavior
When trying to apply the tf.keras.layers.TimeDistributed layer on top of a tensorflow_hub.KerasLayer, an exception (and not a useful one) is being raised
Describe the expected behavior
Just like any other tf.keras.layers.Layer, I'd expect one generated by the tensorflow_hub.KerasLayer to work
Standalone code to reproduce the issue
import os

import numpy as np
import tensorflow as tf
import tensorflow_hub as hub

os.environ["TFHUB_CACHE_DIR"] = '/tmp/tfhub'

# Create model
model = tf.keras.Sequential([tf.keras.layers.TimeDistributed(hub.KerasLayer("https://tfhub.dev/google/bit/s-r101x1/1",
                                                                            trainable=False)),
                             tf.keras.layers.LSTM(units=512),
                             tf.keras.layers.Dense(units=128,
                                                   activation=tf.nn.relu),
                             tf.keras.layers.Dense(units=1,
                                                   activation=tf.nn.sigmoid)])

model.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),
              metrics=['accuracy'])

# Fit
model.fit(x=np.random.randint(low=0, high=256, size=(1000, 10, 56, 56, 3)).astype(float),
          y=np.random.randint(low=0, high=2, size=(1000,)).astype(float), epochs=1)
Other info / logs Include any logs or source code that would be helpful to
Traceback:
2020-05-14 15:40:50.293457: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-05-14 15:40:50.314782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-14 15:40:50.315236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:
pciBusID: 0000:09:00.0 name: GeForce GTX 1070 computeCapability: 6.1
coreClock: 1.683GHz coreCount: 15 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 238.66GiB/s
2020-05-14 15:40:50.315361: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory
2020-05-14 15:40:50.315403: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory
2020-05-14 15:40:50.315438: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory
2020-05-14 15:40:50.315474: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory
2020-05-14 15:40:50.315508: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory
2020-05-14 15:40:50.315542: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory
2020-05-14 15:40:50.315577: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory
2020-05-14 15:40:50.315585: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at &lt;denchmark-link:https://www.tensorflow.org/install/gpu&gt;https://www.tensorflow.org/install/gpu&lt;/denchmark-link&gt;
 for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-05-14 15:40:50.315827: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-05-14 15:40:50.337745: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3493215000 Hz
2020-05-14 15:40:50.338512: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f1b94000b60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-05-14 15:40:50.338545: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-05-14 15:40:50.341227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-05-14 15:40:50.341251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]
Traceback (most recent call last):
File ".../site-packages/IPython/core/interactiveshell.py", line 3331, in run_code
exec(code_obj, self.user_global_ns, self.user_ns)
File "", line 27, in 
y=np.random.randint(low=0, high=2, size=(1000,)).astype(float), epochs=1)
File ".../site-packages/tensorflow/python/keras/engine/training.py", line 66, in _method_wrapper
return method(self, *args, **kwargs)
File ".../site-packages/tensorflow/python/keras/engine/training.py", line 848, in fit
tmp_logs = train_function(iterator)
File ".../site-packages/tensorflow/python/eager/def_function.py", line 580, in 
result = self._call(*args, **kwds)
File ".../site-packages/tensorflow/python/eager/def_function.py", line 627, in _call
self._initialize(args, kwds, add_initializers_to=initializers)
File ".../site-packages/tensorflow/python/eager/def_function.py", line 506, in _initialize
*args, **kwds))
File ".../site-packages/tensorflow/python/eager/function.py", line 2446, in _get_concrete_function_internal_garbage_collected
graph_function, _, _ = self._maybe_define_function(args, kwargs)
File ".../site-packages/tensorflow/python/eager/function.py", line 2777, in _maybe_define_function
graph_function = self._create_graph_function(args, kwargs)
File ".../site-packages/tensorflow/python/eager/function.py", line 2667, in _create_graph_function
capture_by_value=self._capture_by_value),
File ".../site-packages/tensorflow/python/framework/func_graph.py", line 981, in func_graph_from_py_func
func_outputs = python_func(*func_args, **func_kwargs)
File ".../site-packages/tensorflow/python/eager/def_function.py", line 441, in wrapped_fn
return weak_wrapped_fn().(*args, **kwds)
File ".../site-packages/tensorflow/python/framework/func_graph.py", line 968, in wrapper
raise e.ag_error_metadata.to_exception(e)
NotImplementedError: in user code:
.../site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *
outputs = self.distribute_strategy.run(
.../site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **
return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
.../site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica
return self._call_for_each_replica(fn, args, kwargs)
.../site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica
return fn(*args, **kwargs)
.../site-packages/tensorflow/python/keras/engine/training.py:531 train_step  **
y_pred = self(x, training=True)
.../site-packages/tensorflow/python/keras/engine/base_layer.py:927 
outputs = call_fn(cast_inputs, *args, **kwargs)
.../site-packages/tensorflow/python/keras/engine/sequential.py:291 call
outputs = layer(inputs, **kwargs)
.../site-packages/tensorflow/python/keras/engine/base_layer.py:927 
outputs = call_fn(cast_inputs, *args, **kwargs)
.../site-packages/tensorflow/python/keras/layers/wrappers.py:246 call
output_shape = self.compute_output_shape(input_shape).as_list()
.../site-packages/tensorflow/python/keras/layers/wrappers.py:190 compute_output_shape
child_output_shape = self.layer.compute_output_shape(child_input_shape)
.../site-packages/tensorflow/python/keras/engine/base_layer.py:699 compute_output_shape
raise NotImplementedError
NotImplementedError:
	</description>
	<comments>
		<comment id='1' author='eliorc' date='2020-05-15T09:23:24Z'>
		I have tried in colab with TF version 2.2 and was able to reproduce the issue.Please. find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/aacbf993c4e2b44c2a30089d2894932e/untitled890.ipynb&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='2' author='eliorc' date='2020-05-18T02:56:16Z'>
		From the error log, it seems that keras wrapper was expecting the tf_hub layer to implement the compute_output_shape() method, but it is not implemented. All the default keras layer has implemented compute_output_shape(), but I think it is missed by tf_hub.KerasLayer.
Adding &lt;denchmark-link:https://github.com/akhorlin&gt;@akhorlin&lt;/denchmark-link&gt;
 from tf-hub side.
		</comment>
		<comment id='3' author='eliorc' date='2020-05-18T02:58:15Z'>
		&lt;denchmark-link:https://github.com/gowthamkpr&gt;@gowthamkpr&lt;/denchmark-link&gt;
 can we transfer this issue to tensorflow/hub?
		</comment>
		<comment id='4' author='eliorc' date='2020-05-18T08:59:13Z'>
		Yes, please make this an issue in tensorflow/hub and assign it to me. hub.KerasLayer should supply its own .compute_output_shape but does not.
That said, I can get &lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
's colab to work by setting  (and reducing the batch size from 1000 to 10). That's because default implementation inherited from the Layer base class for the eager case is good enough (as long as  the override isn't used).
		</comment>
		<comment id='5' author='eliorc' date='2020-05-18T20:51:05Z'>
		Ack, it seems that I somehow don't have the permission to transfer the issue. The dropdown list doesn't contain hub when I click "Transfer issue" on this page. In case this can't be done by any of you, feel free to recreate a new issue on tensorflow/hub.
		</comment>
		<comment id='6' author='eliorc' date='2020-05-19T17:23:52Z'>
		Done. Created &lt;denchmark-link:https://github.com/tensorflow/hub/issues/596&gt;tensorflow/hub#596&lt;/denchmark-link&gt;
, and closing this issue for now.
		</comment>
		<comment id='7' author='eliorc' date='2020-05-19T17:23:56Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39542&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39542&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>