<bug id='6164' author='ericyue' open_date='2016-12-07T16:32:56Z' closed_time='2017-06-28T00:25:36Z'>
	<summary>TFRecords: DataLossError (see above for traceback): corrupted record at XXX</summary>
	<description>
I convert large data from csv to tfrecords using tf.python_io.TFRecordWriter in hadoop
there're some error happens:

BUG 1) if I use zlib or gzip when create tfrecord writer , I can convert the csv successfully. But when I  loop all files to read , it's  stuck at some lines without any error.  when use these tfrecords to train model ,  errors "TFRecords: DataLossError (see above for traceback): corrupted record at XXX"
(I use writer.close() when write done. so I maybe it's  a bug )
BUG 2) When I create tfrecord write without any compress option, the convert process goes well, and when I use the same program loop all the converted files , It all good . BUT when I use these tfrecords to train model in tensorflow , it's report "TFRecords: DataLossError (see above for traceback): corrupted record at XXX" again（some step after, not at the begining, so maybe some part of the tfrecords error） .

my question:

(if solve these bugs are diffcult ) how to skip the corruted records?
OR
how to solve these errors ?

&lt;denchmark-h:h3&gt;Environment info&lt;/denchmark-h&gt;

Operating System:
centos 6 + hadoop
&lt;denchmark-h:h3&gt;If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)&lt;/denchmark-h&gt;

my convert code:
&lt;denchmark-code&gt;          writer = tf.python_io.TFRecordWriter(pb_path)
          example = tf.train.Example(features=tf.train.Features(feature={
              "label":
                  tf.train.Feature(float_list=tf.train.FloatList(value=[label])),
              "ids":
                  tf.train.Feature(int64_list=tf.train.Int64List(value=ids)),
              "values":
                  tf.train.Feature(float_list=tf.train.FloatList(value=values))
          }))

          writer.write(example.SerializeToString())
          writer.close()
&lt;/denchmark-code&gt;

my debug code (for loop all files):
&lt;denchmark-code&gt;  for f in files:
   for serialized_example in tf.python_io.tf_record_iterator(f):
      example = tf.train.Example()
      example.ParseFromString(serialized_example)

      # Read data in specified format
      label = example.features.feature["label"].float_list.value
      ids = example.features.feature["ids"].int64_list.value
      values = example.features.feature["values"].float_list.value
&lt;/denchmark-code&gt;

my train code
&lt;denchmark-code&gt;# Read TFRecords files for training
filename_queue = tf.train.string_input_producer(
    tf.train.match_filenames_once(FLAGS.train),
    num_epochs=epoch_number)
serialized_example = read_and_decode(filename_queue)
batch_serialized_example = tf.train.shuffle_batch(
    [serialized_example],
    batch_size=batch_size,
    num_threads=thread_number,
    capacity=capacity,
    min_after_dequeue=min_after_dequeue)
features = tf.parse_example(
    batch_serialized_example,
    features={
        "label": tf.FixedLenFeature([], tf.float32),
        "ids": tf.VarLenFeature(tf.int64),
        "values": tf.VarLenFeature(tf.float32),
    })
batch_labels = features["label"]
batch_ids = features["ids"]
batch_values = features["values"]
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Logs or other output that would be helpful&lt;/denchmark-h&gt;

(If logs are large, please upload as attachment or provide link).
&lt;denchmark-code&gt;coord stopped
Traceback (most recent call last):
  File "deepcake.py", line 327, in &lt;module&gt;
    coord.join(threads)
  File "/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py", line 386, in join
    six.reraise(*self._exc_info_to_raise)
  File "/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/queue_runner_impl.py", line 234, in _run
    sess.run(enqueue_op)
  File "/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 766, in run
    run_metadata_ptr)
  File "/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 964, in _run
    feed_dict_string, options, run_metadata)
  File "/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1014, in _do_run
    target_list, options, run_metadata)
  File "/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1034, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 2863006
         [[Node: ReaderRead = ReaderRead[_class=["loc:@TFRecordReader", "loc:@input_producer"], _device="/job:localhost/replica:0/task:0/cpu:0"](TFRecordReader, input_producer)]]

Caused by op u'ReaderRead', defined at:
  File "deepcake.py", line 99, in &lt;module&gt;
    serialized_example = read_and_decode(filename_queue)
  File "deepcake.py", line 92, in read_and_decode
    _, serialized_example = reader.read(filename_queue)
  File "/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py", line 265, in read
    return gen_io_ops._reader_read(self._reader_ref, queue_ref, name=name)
  File "/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py", line 213, in _reader_read
    queue_handle=queue_handle, name=name)
  File "/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 759, in apply_op
    op_def=op_def)
  File "/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 2240, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File "/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1128, in __init__
    self._traceback = _extract_stack()

DataLossError (see above for traceback): corrupted record at 2863006
         [[Node: ReaderRead = ReaderRead[_class=["loc:@TFRecordReader", "loc:@input_producer"], _device="/job:localhost/replica:0/task:0/cpu:0"](TFRecordReader, input_producer)]]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='ericyue' date='2016-12-08T01:58:26Z'>
		&lt;denchmark-link:https://github.com/michaelisard&gt;@michaelisard&lt;/denchmark-link&gt;
  how to skip the corrupted record ?
		</comment>
		<comment id='2' author='ericyue' date='2016-12-08T10:05:00Z'>
		&lt;denchmark-link:https://github.com/ericyue&gt;@ericyue&lt;/denchmark-link&gt;
  Is pb_path an HDFS path?
By the way, you can use &lt;denchmark-link:https://github.com/tensorflow/ecosystem/tree/master/hadoop&gt;MapReduce or Spark&lt;/denchmark-link&gt;
 to make the conversion directly.
		</comment>
		<comment id='3' author='ericyue' date='2016-12-08T13:12:07Z'>
		thanks, &lt;denchmark-link:https://github.com/llhe&gt;@llhe&lt;/denchmark-link&gt;
   The pb_path is a local-system path. I am converting the csv to tfrecord in hadoop-streaming reduce process, and "dfs -put" the pb_path file to hdfs , then I get the files downloaded to someplace.
I'll try your link in temporary , also wish to solve this ploblem :)
		</comment>
		<comment id='4' author='ericyue' date='2016-12-08T14:23:28Z'>
		"corrupted record at XXX" means crc checksum failed. Could you make the repro just using the python scripts without hadoop-streaming?
		</comment>
		<comment id='5' author='ericyue' date='2016-12-09T17:20:32Z'>
		thanks &lt;denchmark-link:https://github.com/llhe&gt;@llhe&lt;/denchmark-link&gt;


I think the crc check faild just happened a few lines , if so , how to skip the error record? can i just drop it?  how ?
"using the python scripts without hadoop-streaming?"   you means run the streaming job in a local machine (single node) , just in pipe like cat xxx.dat |python bin/map.py|python bin/reduce.py ?
I have try this , every thing ok, no error .   but the same program running in hadoop failed, I thinks maybe the tfrecords not work well in hdfs ?

		</comment>
		<comment id='6' author='ericyue' date='2016-12-11T07:06:45Z'>
		&lt;denchmark-link:https://github.com/michaelisard&gt;@michaelisard&lt;/denchmark-link&gt;
  is there any progress on this?
		</comment>
		<comment id='7' author='ericyue' date='2016-12-11T19:06:42Z'>
		&lt;denchmark-link:https://github.com/itsmeolivia&gt;@itsmeolivia&lt;/denchmark-link&gt;
 may be able to comment on whether there's any way to track down issues with hdfs.
		</comment>
		<comment id='8' author='ericyue' date='2016-12-16T15:57:31Z'>
		hello, any progress?  :(  &lt;denchmark-link:https://github.com/itsmeolivia&gt;@itsmeolivia&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='ericyue' date='2017-06-16T18:01:38Z'>
		&lt;denchmark-link:https://github.com/jhseu&gt;@jhseu&lt;/denchmark-link&gt;
, Is this still an issue?
Could you comment and/or close this issue?
		</comment>
		<comment id='10' author='ericyue' date='2017-06-28T00:25:36Z'>
		I haven't been able to reproduce this issue, so I'm closing it out. If you feel this is still an issue, please test it out in TF 1.2 and upload the full scripts you're using to &lt;denchmark-link:https://gist.github.com&gt;https://gist.github.com&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='ericyue' date='2017-09-03T05:19:02Z'>
		This just happened to me too and seems the problem is that TFRecord does not recognize that the file is compressed (i.e. GZIP). You need to explicitly configure it using the options, i.e:
&lt;denchmark-code&gt;options = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.GZIP)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='12' author='ericyue' date='2018-06-08T14:17:33Z'>
		I kind of experienced the same error, I used the feature extractor from youtube-8M from google and tried to execute inference.py from google's starter code, and the following message appeared
&lt;denchmark-code&gt;python inference.py --output_file='testpredictions5.csv' --input_data_pattern='/home/estathop/Documents/testmodelfiles/testvideos.csv'  --train_dir='/home/estathop/Documents/features/youtube-8m/MyMoeModel2' --top_k=50
2018-06-08 11:57:52.644482: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-06-08 11:57:52.712548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-06-08 11:57:52.712942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: 
name: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.7845
pciBusID: 0000:01:00.0
totalMemory: 7.93GiB freeMemory: 6.86GiB
2018-06-08 11:57:52.712955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2018-06-08 11:57:52.894855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-06-08 11:57:52.894885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
2018-06-08 11:57:52.894891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
2018-06-08 11:57:52.895076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6611 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)
INFO:tensorflow:number of input files: 1
INFO:tensorflow:loading meta-graph: /home/estathop/Documents/features/youtube-8m/MyMoeModel2/inference_model.meta
INFO:tensorflow:restoring variables from /home/estathop/Documents/features/youtube-8m/MyMoeModel2/inference_model
INFO:tensorflow:Restoring parameters from /home/estathop/Documents/features/youtube-8m/MyMoeModel2/inference_model
INFO:tensorflow:Error reported to Coordinator: &lt;class 'tensorflow.python.framework.errors_impl.DataLossError'&gt;, corrupted record at 0
	 [[Node: input/ReaderReadUpToV2 = ReaderReadUpToV2[_device="/job:localhost/replica:0/task:0/device:CPU:0"](input/TFRecordReaderV2, input/input_producer, input/ReaderReadUpToV2/num_records)]]
	 [[Node: input/ParseExample/ParseExample/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device_incarnation=1, tensor_name="edge_32_input/ParseExample/ParseExample", tensor_type=DT_INT64, _device="/job:localhost/replica:0/task:0/device:GPU:0"]()]]
2018-06-08 11:57:53.285454: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at example_parsing_ops.cc:144 : Invalid argument: Name: &lt;unknown&gt;, Feature: id (data type: string) is required but could not be found.
INFO:tensorflow:Done with inference. The output file was written to testpredictions5.csv
Traceback (most recent call last):
  File "inference.py", line 227, in &lt;module&gt;
    app.run()
  File "/home/estathop/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/platform/app.py", line 126, in run
    _sys.exit(main(argv))
  File "inference.py", line 223, in main
    FLAGS.output_file, FLAGS.batch_size, FLAGS.top_k)
  File "inference.py", line 182, in inference
    coord.join(threads)
  File "/home/estathop/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File "/home/estathop/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/queue_runner_impl.py", line 252, in _run
    enqueue_callable()
  File "/home/estathop/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1244, in _single_operation_run
    self._call_tf_sessionrun(None, {}, [], target_list, None)
  File "/home/estathop/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1409, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 0
	 [[Node: input/ReaderReadUpToV2 = ReaderReadUpToV2[_device="/job:localhost/replica:0/task:0/device:CPU:0"](input/TFRecordReaderV2, input/input_producer, input/ReaderReadUpToV2/num_records)]]
	 [[Node: input/ParseExample/ParseExample/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device_incarnation=1, tensor_name="edge_32_input/ParseExample/ParseExample", tensor_type=DT_INT64, _device="/job:localhost/replica:0/task:0/device:GPU:0"]()]]
``
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>