<bug id='45627' author='IlleQuiProgrammat' open_date='2020-12-12T13:20:21Z' closed_time='2021-01-09T00:44:41Z'>
	<summary>Potential GPU Memory Leak</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab (couldn't find distro from this)
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.3.1
Python version: Python 3.8.5 (64-bit)
CUDA/cuDNN version: 10.1
GPU model and memory: Nvidia Tesla T4 16GB

Describe the current behavior
Half way through an epoch the network appears to run out of memory despite no new memory requirements i.e. nothing extra is being created that isn't deleted (I think). See ipynb below for a detailed error message. Furthermore, when checking the memory free after the program has finished executing (using nvidia-smi) it gives 14599MiB / 15079MiB used and No running processes found.
Describe the expected behavior
The network should continue to train as normal.

&lt;denchmark-link:https://gist.github.com/IlleQuiProgrammat/cd37e1b0767e0c859fef922387284898&gt;https://gist.github.com/IlleQuiProgrammat/cd37e1b0767e0c859fef922387284898&lt;/denchmark-link&gt;

Note: the full error is shown at the bottom
	</description>
	<comments>
		<comment id='1' author='IlleQuiProgrammat' date='2020-12-14T09:33:54Z'>
		&lt;denchmark-link:https://github.com/IlleQuiProgrammat&gt;@IlleQuiProgrammat&lt;/denchmark-link&gt;

I ran the code shared an face a different error, please find the &lt;denchmark-link:https://colab.research.google.com/gist/Saduf2019/f5856ebfa522d94eb93cb30b2ff52624/untitled485.ipynb&gt;gist here&lt;/denchmark-link&gt;
.
Please share all dependencies for us to replicate the issue faced.
		</comment>
		<comment id='2' author='IlleQuiProgrammat' date='2020-12-14T11:46:09Z'>
		Apologies for not explaining more clearly, on my drive I have copied the celeb a img_align data set to  from &lt;denchmark-link:https://drive.google.com/file/d/0B7EVK8r0v71pZjFTYXZWM3FlRnM/view?usp=drivesdk&gt;https://drive.google.com/file/d/0B7EVK8r0v71pZjFTYXZWM3FlRnM/view?usp=drivesdk&lt;/denchmark-link&gt;

And the other error regarding the latest checkpoint can be solved by not running that cell - I haven't trained a model yet it is there for using it in the future for inference.
In addition, you don't appear to be running this on a gpu or colab has missed copying the pip install tensorflow-gpu command from my gist. Running this on cpu would be far too slow and I believe this issue is related to managing the gpu memory but I'm not sure.
		</comment>
		<comment id='3' author='IlleQuiProgrammat' date='2020-12-14T16:40:23Z'>
		&lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;
 mentioning you as, from the previous issues I've seen, tensorflow butler normally removes the awaiting response tag but this hasn't seemed to happen so making sure you've seen my response.
		</comment>
		<comment id='4' author='IlleQuiProgrammat' date='2020-12-28T05:03:01Z'>
		&lt;denchmark-link:https://github.com/IlleQuiProgrammat&gt;@IlleQuiProgrammat&lt;/denchmark-link&gt;

Apologies for the delayed response.
Can you please share a colab gist, i am unable to download from the link shared.
		</comment>
		<comment id='5' author='IlleQuiProgrammat' date='2020-12-28T08:34:07Z'>
		I'm not quite sure how you would like me to attach the dataset to the colab gist as it is very large. If there is a way I don't know, please tell me.
If it is just an issue of the Google drive link not working, you can download the dataset from any of the links on this page: &lt;denchmark-link:http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html&gt;http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html&lt;/denchmark-link&gt;

Thank you in advance.
		</comment>
		<comment id='6' author='IlleQuiProgrammat' date='2021-01-05T19:26:00Z'>
		Since you are hitting OOM error I would recommend &lt;denchmark-link:https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth&gt;limiting gpu memory growth&lt;/denchmark-link&gt;
 and also try reducing your batch size to a smaller value.
# On top of your script 
import tensorflow as tf
gpus = tf.config.experimental.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(gpus[0], True)
# Rest of your code
...
		</comment>
		<comment id='7' author='IlleQuiProgrammat' date='2021-01-08T11:57:18Z'>
		Apologies, I missed your response, I shall try that now and update you to the results.
		</comment>
		<comment id='8' author='IlleQuiProgrammat' date='2021-01-08T18:38:05Z'>
		Thank you, this seems to be working &lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;
 - is there a reason why this is not enabled by default?
		</comment>
		<comment id='9' author='IlleQuiProgrammat' date='2021-01-09T00:44:41Z'>
		Glad it worked. As per the &lt;denchmark-link:https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth&gt;Limiting GPU memory growth&lt;/denchmark-link&gt;
 docs,

By default, TensorFlow maps nearly all of the GPU memory of all GPUs  visible to the process. This is done to more efficiently use the relatively precious GPU memory resources on the devices by reducing memory fragmentation.

		</comment>
		<comment id='10' author='IlleQuiProgrammat' date='2021-01-09T00:44:42Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45627&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45627&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>