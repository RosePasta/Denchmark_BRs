<bug id='42845' author='andrescodas' open_date='2020-08-31T22:10:26Z' closed_time='2020-10-29T22:32:26Z'>
	<summary>AttributeError: 'NoneType' object has no attribute 'python_grad_func' when using @tf.function()</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  dockerhub container 'latest' Digest: 7bc36fe0ca1a051a808122e87f5438614b371263515df4794abef9a78440af8b
TensorFlow version (use command below):
tf.version.GIT_VERSION=v2.3.0-rc2-23-gb36436b087
tf.version.VERSION=2.3.0
Python version: 3.6

Describe the current behavior
Getting AttributeError: 'NoneType' object has no attribute 'python_grad_func' when decorating functions with @tf.function()
Describe the expected behavior
No error
Standalone code to reproduce the issue
import tensorflow as tf
print('tf.version.GIT_VERSION={}'.format(tf.version.GIT_VERSION))
print('tf.version.VERSION={}'.format(tf.version.VERSION))

@tf.function()
def G(x, c):
    xx = tf.transpose(a=x[:, :, None], perm=[0, 2, 1])
    cc = tf.transpose(a=c[:, :, None], perm=[2, 0, 1])
    G = tf.reduce_sum(input_tensor=tf.square(xx - cc), axis=2)
    return G

@tf.function()
def ff(x, c , v):
    with tf.GradientTape(persistent=True) as t1:
        t1.watch(x)
        with tf.GradientTape(persistent=True) as t2:
            t2.watch(x)
            p = tf.reduce_sum(input_tensor=G(x,c) * v, axis=1, keepdims=True)
        f = t2.batch_jacobian(p, x, experimental_use_pfor=False)[:,:,0]
    f_w = t1.batch_jacobian(f, x, experimental_use_pfor=False)[:,:,0]
    return f_w

x = tf.constant([[1.],[2.]])
c = tf.ones((2,1))
v = tf.ones((1,2))

with tf.GradientTape() as tape:
    tape.watch(v)
    o = tf.reduce_sum(tf.square(ff(x, c, v)))
g = tape.gradient(o, v)
print(g)

Other info / logs
&lt;denchmark-code&gt;tf.version.GIT_VERSION=v2.3.0-rc2-23-gb36436b087
tf.version.VERSION=2.3.0
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py", line 2486, in get_attr
    pywrap_tf_session.TF_OperationGetAttrValueProto(self._c_op, name, buf)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Operation 'while_1' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py", line 331, in _MaybeCompile
    xla_compile = op.get_attr("_XlaCompile")
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py", line 2490, in get_attr
    raise ValueError(str(e))
ValueError: Operation 'while_1' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py", line 2486, in get_attr
    pywrap_tf_session.TF_OperationGetAttrValueProto(self._c_op, name, buf)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Operation 'gradient_tape/while/while_grad' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py", line 331, in _MaybeCompile
    xla_compile = op.get_attr("_XlaCompile")
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py", line 2490, in get_attr
    raise ValueError(str(e))
ValueError: Operation 'gradient_tape/while/while_grad' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py", line 607, in _GradientsHelper
    grad_fn = ops.get_gradient_function(op)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py", line 2655, in get_gradient_function
    return _gradient_registry.lookup(op_type)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/registry.py", line 97, in lookup
    "%s registry has no entry for: %s" % (self._name, name))
LookupError: gradient registry has no entry for: PartitionedCall

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/MultiPhaseModel_PINN/tests/issue_xla.py", line 29, in &lt;module&gt;
    o = tf.reduce_sum(tf.square(ff(x, c, v)))
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py", line 780, in __call__
    result = self._call(*args, **kwds)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py", line 846, in _call
    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py", line 1848, in _filtered_call
    cancellation_manager=cancellation_manager)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py", line 1929, in _call_flat
    forward_function, args_with_tangents = forward_backward.forward()
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py", line 1433, in forward
    self._inference_args, self._input_tangents)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py", line 1189, in forward
    self._forward_and_backward_functions(inference_args, input_tangents))
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py", line 1341, in _forward_and_backward_functions
    outputs, inference_args, input_tangents)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py", line 899, in _build_functions_for_outputs
    src_graph=self._func_graph)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py", line 669, in _GradientsHelper
    lambda: grad_fn(op, *out_grads))
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py", line 336, in _MaybeCompile
    return grad_fn()  # Exit early
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py", line 669, in &lt;lambda&gt;
    lambda: grad_fn(op, *out_grads))
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/while_v2.py", line 354, in _WhileGrad
    util.unique_grad_fn_name(body_graph.name), op, maximum_iterations)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/while_v2.py", line 626, in _create_grad_func
    body_graph_inputs, body_graph_outputs))
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py", line 986, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/while_v2.py", line 622, in &lt;lambda&gt;
    lambda *args: _grad_fn(ys, xs, args, body_graph),
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/while_v2.py", line 682, in _grad_fn
    unconnected_gradients="zero")
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py", line 669, in _GradientsHelper
    lambda: grad_fn(op, *out_grads))
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py", line 336, in _MaybeCompile
    return grad_fn()  # Exit early
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py", line 669, in &lt;lambda&gt;
    lambda: grad_fn(op, *out_grads))
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/while_v2.py", line 354, in _WhileGrad
    util.unique_grad_fn_name(body_graph.name), op, maximum_iterations)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/while_v2.py", line 626, in _create_grad_func
    body_graph_inputs, body_graph_outputs))
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py", line 986, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/while_v2.py", line 622, in &lt;lambda&gt;
    lambda *args: _grad_fn(ys, xs, args, body_graph),
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/while_v2.py", line 682, in _grad_fn
    unconnected_gradients="zero")
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py", line 619, in _GradientsHelper
    grad_fn = func_call.python_grad_func
AttributeError: 'NoneType' object has no attribute 'python_grad_func'

Process finished with exit code 1

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='andrescodas' date='2020-09-01T05:26:38Z'>
		I have tried in colab with TF version 2.2, 2.3 and was able to reproduce the issue. However in TF nightly () i am seeing different error message ().Please, find the gist &lt;denchmark-link:https://colab.research.google.com/gist/ravikyram/775fd546727cdba54239b1077da77012/untitled288.ipynb&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='2' author='andrescodas' date='2020-09-25T17:07:10Z'>
		Removing autograph label as the error message persists even when autograph=False
		</comment>
		<comment id='3' author='andrescodas' date='2020-10-01T18:30:32Z'>
		It looks like this is working with experimental_use_pfor=True on both batch_jacobians. It's possible that this is due to recent vectorized_map improvements. Is using that an option for you?
Fixing the control flow gradients may be a bit more complicated, but that we should do too; thank you for the report.
		</comment>
		<comment id='4' author='andrescodas' date='2020-10-01T19:25:00Z'>
		Hi &lt;denchmark-link:https://github.com/allenlavoie&gt;@allenlavoie&lt;/denchmark-link&gt;
 .  Thanks for taking a look.

It looks like this is working with experimental_use_pfor=True on both batch_jacobians. It's possible that this is due to recent vectorized_map improvements. Is using that an option for you?

This is a failing case I managed to reduce from a more complex code.  I shared it with the intention to get more understanding of why it happens, and to make developers aware of it in case they don't know.  I guess you might be tracking it somewhere else, otherwise you may close it.
The actual case I'm trying to optimize for running speed and memory allocation looks similar to what I shared in issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/43252&gt;#43252&lt;/denchmark-link&gt;
, where you provided great help.  I will keep in mind  in my tests.
		</comment>
		<comment id='5' author='andrescodas' date='2020-10-29T22:32:26Z'>
		Looks like the version without pfor is fixed after &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/07b75ffa453b1ec9189371244d21b6684503340b&gt;07b75ff&lt;/denchmark-link&gt;

Thank you for the report.
		</comment>
		<comment id='6' author='andrescodas' date='2020-10-29T22:32:27Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42845&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42845&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>