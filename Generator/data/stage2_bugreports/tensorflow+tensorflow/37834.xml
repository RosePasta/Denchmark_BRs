<bug id='37834' author='w19787' open_date='2020-03-23T14:58:33Z' closed_time='2020-07-24T09:48:12Z'>
	<summary>keras conv layer weight cannot be updated during distribution training mode</summary>
	<description>
OS: Ubuntu 18.04
Tensorflow: 2.1.0
Python: 3.7
GPUs: 4 gpus
trying to train a gan network on multiple GPUs. The network has a spectral normalization layer to update convolution layer weights. The sample code and error are provided as follows:
import tensorflow as tf
strategy = tf.distribute.MirroredStrategy()

with strategy.scope():
    conv = tf.keras.layers.Conv2D(5, 3, 1)
    x = tf.random.normal((1,5, 5, 3))
    conv(x)

@tf.function
def update_w(w, nw):
    w.assign(nw)

x2 = tf.random.normal((3,3, 3, 5))
strategy.experimental_run_v2(update_w, args=(conv.kernel, x2))
&lt;denchmark-link:https://user-images.githubusercontent.com/731496/77329724-5a234800-6d59-11ea-9ee4-a3b976178147.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='w19787' date='2020-03-24T04:52:49Z'>
		i have replicated the error faced on nightly, please find the &lt;denchmark-link:https://colab.sandbox.google.com/gist/Saduf2019/f9a25e7d853d7e41aebf3bfe473aaac0/37834.ipynb&gt;gist here&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='w19787' date='2020-03-27T07:13:44Z'>
		&lt;denchmark-link:https://github.com/w19787&gt;@w19787&lt;/denchmark-link&gt;
 As the error message suggests we currently disallow aggregating a MirroredVariable in replica context since we aggregate the values before assigning (this is to ensure that the MirroredVariables remain in sync).
Would it be possible to work around this by updating the variable in cross replica context like the error message suggests?
		</comment>
		<comment id='3' author='w19787' date='2020-03-28T15:22:02Z'>
		i am unable to work around this issue in cross replica context with merge fn and extended.update. it can work on one gpu but still failed on multiple gpu.
when we create keras layer, i.e Conv2d, there is no argument to specify the aggregation parameter of weights, i think it should be the reason for this issue. I want to confirm whether the above understanding is correct, if so, why we design so? thanks.
		</comment>
		<comment id='4' author='w19787' date='2020-06-04T20:12:59Z'>
		A workaround for this is making a new variable creator and using tf.variable_creator_scope:
&lt;denchmark-code&gt;def variable_creator(next_creator, **kwargs):
    kwargs['aggregation'] = tf.VariableAggregation.ONLY_FIRST_REPLICA
    return next_creator(**kwargs)
with tf.variable_creator_scope(variable_creator):
    conv.build(input_shape)
&lt;/denchmark-code&gt;

Calling build inside your creator's scope will make the variables inside get the aggregation parameter. I chose tf.VariableAggregation.ONLY_FIRST_REPLICA but you may chose the tf.VariableAggregation that fits your use case better
		</comment>
		<comment id='5' author='w19787' date='2020-07-20T19:55:47Z'>
		&lt;denchmark-link:https://github.com/w19787&gt;@w19787&lt;/denchmark-link&gt;
 Sorry for the late response. Recent changes have made it possible to call assign on mirrored variables in replica context(when there is no aggregation). Hopefully that should fix issues like this. However if you want to update your variable in cross replica context you can use something like:
&lt;denchmark-code&gt;strategy = tf.distribute.MirroredStrategy()
print("Devices used ", strategy.extended.worker_devices)
with strategy.scope():
    conv = tf.keras.layers.Conv2D(5, 3, 1)
    x = tf.random.normal((1,5, 5, 3))
    conv(x)

@tf.function
def update_w(w, nw):
    def merge_fn(strategy, w, nw):
      strategy.extended.update(w, lambda v,w:v.assign(w), args=(nw,))
    replica_ctx = tf.distribute.get_replica_context()
    replica_ctx.merge_call(merge_fn, args=(w, nw))

x2 = tf.random.normal((3,3, 3, 5))
print("before update ", conv.kernel[0])
strategy.experimental_run_v2(update_w, args=(conv.kernel, x2))
print("after update ", conv.kernel[0])
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='w19787' date='2020-07-24T09:47:33Z'>
		
@w19787 Sorry for the late response. Recent changes have made it possible to call assign on mirrored variables in replica context(when there is no aggregation). Hopefully that should fix issues like this. However if you want to update your variable in cross replica context you can use something like:
strategy = tf.distribute.MirroredStrategy()
print("Devices used ", strategy.extended.worker_devices)
with strategy.scope():
    conv = tf.keras.layers.Conv2D(5, 3, 1)
    x = tf.random.normal((1,5, 5, 3))
    conv(x)

@tf.function
def update_w(w, nw):
    def merge_fn(strategy, w, nw):
      strategy.extended.update(w, lambda v,w:v.assign(w), args=(nw,))
    replica_ctx = tf.distribute.get_replica_context()
    replica_ctx.merge_call(merge_fn, args=(w, nw))

x2 = tf.random.normal((3,3, 3, 5))
print("before update ", conv.kernel[0])
strategy.experimental_run_v2(update_w, args=(conv.kernel, x2))
print("after update ", conv.kernel[0])


thanks a lot!
		</comment>
		<comment id='7' author='w19787' date='2020-07-24T09:48:14Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37834&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37834&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>