<bug id='30141' author='devstein' open_date='2019-06-25T17:24:24Z' closed_time='2019-06-28T15:40:50Z'>
	<summary>"Iterator::Model::Prefetch::Batch::Shuffle::ParallelInterleaveV2" returned OutOfRange without setting *end_of_sequence</summary>
	<description>
See &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/29060#issuecomment-505539468&gt;#29060 (comment)&lt;/denchmark-link&gt;
 for more details
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX and Linux
TensorFlow installed from (source or binary): pipenv install --pre tensorflow==2.0.0-beta1
TensorFlow version (use command below): 2.0.0-beta1
Python version: 3.6.8

Describe the current behavior
I'm running into this issue when caching before interleave
    filenames_dataset = filenames_dataset.cache("./some_path")

    return filenames_dataset.interleave(
        lambda f: map_file_to_xy_dataset(f, predict_task_fn, params),
        cycle_length=params[CYCLE_LENGTH_KEY],
        block_length=params[BLOCK_LENGTH_KEY],
        num_parallel_calls=tf.data.experimental.AUTOTUNE)
&lt;denchmark-code&gt;Iterator "Iterator::Model::Prefetch::Batch::Shuffle::ParallelInterleaveV2" returned OutOfRange without setting `*end_of_sequence`. This indicates that an error may have occurred. Original message: Attempting to call get_next after iteration should have finished. [Op:IteratorGetNextSync]
&lt;/denchmark-code&gt;

Describe the expected behavior
Able to cache the filenames_dataset
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
See above
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='devstein' date='2019-06-25T20:59:31Z'>
		Thank you for reporting this &lt;denchmark-link:https://github.com/devstein&gt;@devstein&lt;/denchmark-link&gt;
. I am able to reproduce the problem with the below code:
  import tensorflow as tf

  tf.enable_v2_behavior()

  ds = tf.data.Dataset.range(5)
  ds = ds.cache('/tmp/cache_dir' )
  ds = ds.interleave(
      lambda f: tf.data.Dataset.range(f),
      cycle_length=2, num_parallel_calls=1)
  ds = ds.repeat(2)

  for e in ds:
    tf.print(e)
I'm looking into the root cause and will message back when I have a fix.
		</comment>
		<comment id='2' author='devstein' date='2019-06-25T21:44:49Z'>
		The issue is related to using parallel interleave. As a temporary workaround, try leaving num_parallel_calls unset.
		</comment>
		<comment id='3' author='devstein' date='2019-06-25T22:18:35Z'>
		&lt;denchmark-link:https://github.com/aaudiber&gt;@aaudiber&lt;/denchmark-link&gt;
 Appreciate the quick response, I'll try that workaround for now.
		</comment>
		<comment id='4' author='devstein' date='2019-06-28T15:38:00Z'>
		The fix is merged to master in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/3398e887f5daa1e22c59eaa1c6f5ca731698ad2f&gt;3398e88&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='devstein' date='2019-06-28T21:41:01Z'>
		&lt;denchmark-link:https://github.com/aaudiber&gt;@aaudiber&lt;/denchmark-link&gt;
 Thanks!
		</comment>
	</comments>
</bug>