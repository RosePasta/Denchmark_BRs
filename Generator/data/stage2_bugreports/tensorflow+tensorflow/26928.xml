<bug id='26928' author='SumNeuron' open_date='2019-03-20T09:18:49Z' closed_time='2019-03-23T00:30:21Z'>
	<summary>Memory always exceeds 10% system memory</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 LTS
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
TensorFlow installed from (source or binary): conda gpu version
TensorFlow version (use command below): 1.10
Python version: 3.5
Bazel version (if compiling from source): n/a
GCC/Compiler version (if compiling from source): n/a
CUDA/cuDNN version: 10
GPU model and memory: Quadro GP100 16Gb

You can collect some of this information using our environment capture &lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with
python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
Describe the current behavior
In the linked &lt;denchmark-link:https://colab.research.google.com/drive/1oJDfvIWnf7sY5uFcJ7XhTSEdA1n_3Jxv#scrollTo=8pqa0A7BUrG-&gt;colab&lt;/denchmark-link&gt;
 I try to train a WALSModel, with a sparse matrix with shape 
When I try to run it locally on my Quadro I get:
&lt;denchmark-code&gt; Allocator (GPU_0_bfc) ran out of memory trying to allocate 17.32GiB.  Current allocation summary follows.

&lt;/denchmark-code&gt;

Which, ok, I have a 16GB memory card, that makes sense, I guess.
Since this is run on GPU it kills the process.
So I set my session config as follows to use the CPU which has 100Gb of Ram. Surely enough for the 17.32GiB that TF tried to allocate:
# inside train_fn
config = tf.ConfigProto(device_count = {'GPU': 0})
with tf.Session(config=config) as sess:
    #...
and lo and behold:
&lt;denchmark-code&gt;Allocation of 18599850000 exceeds 10% of system memory.
&lt;/denchmark-code&gt;

I get that the sparse matrix is not exactly small, but it is not excessively large either. Storing the coordinate form on disk is &lt;2 gb. so What is with the rampant memory usage?
Please help.
Full logs in last cell of colab
Describe the expected behavior
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='SumNeuron' date='2019-03-23T00:30:21Z'>
		tl;dr: the issue is that the linked script tries to process the entire sparse matrix in one iteration. Batching the sparse input matrix will solve the problem.
Detailed explanation: Suppose the embedding dimension is d, and the number of rows in a batch is n. When processing a batch, the update op needs to solve n linear systems, each involving a different d x d matrix. The combined linear system has size O(nd^2), see the output tensor of the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/e86f5c2b735cd4f3045b41ff6eab14305190176a/tensorflow/contrib/factorization/kernels/wals_solver_ops.cc#L140&gt;WALSComputePartialLhsAndRhsOp kernel&lt;/denchmark-link&gt;

the shape of the output shape is  where  is n and  is d.
In the example colab, d = 150 and n = 210000 (for the row solve), so the kernel allocates n d^2 floats which corresponds roughly to the 18GB seen in the error message.
This problem can be solved by batching the sparse tensor, you can follow the example  in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/f07558116ac7c90858cf0572a1bca1e50e208a37/tensorflow/contrib/factorization/python/ops/wals_test.py#L141&gt;wals_test.py&lt;/denchmark-link&gt;
.
Note that it is not enough to use tf.batch on the sparse tensor, as this will change the row ids (the resulting row id is a 'local' index within the batch instead of the 'global' index within the original matrix). Instead, one should batch then remap the index, this is what the function remap_sparse_tensor_rows  in the example is doing.
		</comment>
		<comment id='2' author='SumNeuron' date='2019-03-23T00:30:22Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=26928&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=26928&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='SumNeuron' date='2019-03-27T14:21:31Z'>
		&lt;denchmark-link:https://github.com/walidk&gt;@walidk&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/agarwal-ashish&gt;@agarwal-ashish&lt;/denchmark-link&gt;
 the linked test code is for the  wrapper of ,  and does not provide sufficient comments / guidance as to how to make and run a sharded . In addition, the documentation for  is far from sufficient. The pseudo-code leads the reader down a rabbit hole of documentation back to the  files, and that is just for setting up the training environment. It is no where addressed in that documentation how to handle sharding the sparse tensor.
in this other &lt;denchmark-link:https://colab.research.google.com/drive/1xGe3lRiKnwPuhc1FfgdZiayEE8rQQwvM&gt;colab&lt;/denchmark-link&gt;

I start to fill in as much of the pseudo-code as possible including using the relevant code linked by &lt;denchmark-link:https://github.com/walidk&gt;@walidk&lt;/denchmark-link&gt;

As it currently stands this issue has only been semi-addressed in that why, when running on a GPU, this error occurs, however this error also occurs when running on a CPU with 100GiB ram. It runs on the CPU, but it still claims to over allocate.
The linked code is appreciated, but it is in sufficient to address how to resolve the issue completely.
In short documentation for WALSModel with sharding is needed.
		</comment>
	</comments>
</bug>