<bug id='42183' author='tianhuat' open_date='2020-08-10T09:21:14Z' closed_time='2020-08-10T18:42:46Z'>
	<summary>Adam optimizer - ValueError: tf.function-decorated function tried to create variables on non-first call</summary>
	<description>
I am using tensorflow 2.3
The code below
&lt;denchmark-code&gt;import  tensorflow as tf

y_N= tf.Variable([1., 2., 3.],name="dd")

@tf.function
def loss():
    return -tf.reduce_mean(input_tensor=tf.reduce_sum(input_tensor=tf.math.log(y_N), axis=0))

@tf.function
def run():
    tf.keras.optimizers.Adam(0.5).minimize(loss, var_list=[y_N])

run()
&lt;/denchmark-code&gt;

gives exception
&lt;denchmark-code&gt;ValueError: tf.function-decorated function tried to create variables on non-first call.
&lt;/denchmark-code&gt;

Problem looks like tf.keras.optimizers.Adam(0.5).minimize(loss, var_list=[y_N]) creates new variable on &gt; first call, while using @tf.function. If I must wrap adam_optimizer under @tf.function, is it possible? looks like a bug?
	</description>
	<comments>
		<comment id='1' author='tianhuat' date='2020-08-10T09:38:49Z'>
		I have tried in colab with TF version 2.3, nightly version() and was able to reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.research.google.com/gist/ravikyram/b8abbdd5f974a0f189fe36cb6c34a421/untitled234.ipynb&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='2' author='tianhuat' date='2020-08-10T13:08:05Z'>
		I found out that  after calling minimize function, it will create an iter variable, by calling self.iterations once.
(This happens in tensorflow/python/keras/optimizer_v2/optimizer_v2.py)
Subsequent call of the self.iterations will not create new variable -
as iter has already been created.
Therefore, by calling it earlier outside the tf.function solve the issue, here's the code that works:
&lt;denchmark-code&gt;import  tensorflow as tf

y_N= tf.Variable([1., 2., 3.],name="dd")

@tf.function
def loss():
    return -tf.reduce_mean(input_tensor=tf.reduce_sum(input_tensor=tf.math.log(y_N), axis=0))

@tf.function
def run(adam):
    adam.minimize(loss, var_list=[y_N])


adam=tf.keras.optimizers.Adam(0.5)
run(adam)
&lt;/denchmark-code&gt;

Though I think the minimize of Adam function could be improved so that we don't need this hack.
		</comment>
		<comment id='3' author='tianhuat' date='2020-08-10T18:42:46Z'>
		Hi &lt;denchmark-link:https://github.com/tianhuat&gt;@tianhuat&lt;/denchmark-link&gt;
, you've run into a fairly common scenario due to some slightly confusing behavior with Keras and tf.function. The &lt;denchmark-link:https://www.tensorflow.org/guide/function#variables&gt;Variables section of the tf.function guide&lt;/denchmark-link&gt;
 explains the error message you've seen as guarding against behavior divergence on repeated calls. In eager mode, a function creates a new variable with each call, but with graph mode a new variable may not be created due to trace reuse. This is essentially what you've noted in your above comment, and the suggested solution is to pass in the optimizer. And for reference, you'll likely run into a similar error message if you try to create the model inside your tf.function, instead of passing it in.
I'll close this issue now as you've found a solution, and there is also an open issue around this error message (with a lot of comments if you're interested in reading more on the details) &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/27120&gt;#27120&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='tianhuat' date='2020-08-10T18:42:48Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42183&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42183&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>