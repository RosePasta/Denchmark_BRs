<bug id='31154' author='jtang7' open_date='2019-07-30T01:52:23Z' closed_time='2019-08-01T00:43:20Z'>
	<summary>Workers are out-of-sync with MultiWorkerMirroredStrategy</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Darwin-18.0.0-x86_64-i386-64bit
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.0.0-dev20190729
Python version:3.6.8
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

I follow the guide in &lt;denchmark-link:https://www.tensorflow.org/beta/tutorials/distribute/multi_worker_with_keras&gt;https://www.tensorflow.org/beta/tutorials/distribute/multi_worker_with_keras&lt;/denchmark-link&gt;
 to try MultiWorkerMirroredStrategy with Keras, as my understanding, training would be synced across workers.
But after I start training as following
python example_tf2_local.py 0 chief
python example_tf2_local.py 0 worker
I found worker/chief are training in different pace, e.g.
if I start chief at first, chief would not wait for worker, it just starts its own training.
If I start worker/chief at the same time, still I saw one would be behind another one a few epochs sometime.
But as document stated " MultiWorkerMirroredStrategy implements synchronous distributed training across multiple workers"
And it's same if I simply starts two workers without chief.
Describe the expected behavior
According to document "MultiWorkerMirroredStrategy implements synchronous distributed training across multiple workers", workers need to be synced during training
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
&lt;denchmark-code&gt;from __future__ import absolute_import, division, print_function, unicode_literals
import datetime
import json
import os
import tensorflow_datasets as tfds
import tensorflow as tf
import subprocess
import shlex
import sys

strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()

tfds.disable_progress_bar()

BUFFER_SIZE = 60000
BATCH_SIZE = 64

NUM_WORKERS = 2
GLOBAL_BATCH_SIZE = NUM_WORKERS * BATCH_SIZE

if __name__ == "__main__":
  worker_addrs = ['localhost:9999']
  chief_addr = ['localhost:9998']
  os.environ['TF_CONFIG'] = json.dumps({
      'cluster': {
          'worker': worker_addrs,
          'chief' : chief_addr
      },
      'task': {'type': sys.argv[2], 'index': int(sys.argv[1])}
  })

  print('TF_CONFIG:' + os.environ['TF_CONFIG'])

  def scale(image, label):
    image = tf.cast(image, tf.float32)
    image /= 255
    return image, label

  def build_and_compile_cnn_model():
    model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),
        tf.keras.layers.MaxPooling2D(),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    model.compile(
        loss=tf.keras.losses.sparse_categorical_crossentropy,
        optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
        metrics=['accuracy'])
    return model

  datasets, info = tfds.load(name='mnist',
                             with_info=True,
                             as_supervised=True)

  train_datasets_unbatched = datasets['train'].map(scale).shuffle(BUFFER_SIZE)

  train_datasets = train_datasets_unbatched.batch(GLOBAL_BATCH_SIZE)

  with strategy.scope():
    multi_worker_model = build_and_compile_cnn_model()
  multi_worker_model.fit(x=train_datasets, epochs=100)
&lt;/denchmark-code&gt;

Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='jtang7' date='2019-07-31T23:49:24Z'>
		Did you try setting your TF_CONFIG as described in the tutorial? From the example you pasted above, it looks different. You don't need to explicitly specify a chief. Simply put all your workers in the worker list, and give each of them a different index. Strategy will automatically pick the first one in the list as the chief.
I think what you're seeing is that both the programs are running independently thinking there is only one worker in its cluster. You need to have both the workers in the "workers" section of TF_CONFIG..
		</comment>
		<comment id='2' author='jtang7' date='2019-08-01T00:11:40Z'>
		I changed code as you suggested, and run
python distribute_run_tf2.py 0
to only start one worker, I assume in sync-training setup, this worker will wait for another worker(like I did with TF1.0 with SyncReplicasOptimizer + parameter server), but worker 0 just start training, here is the log
&lt;denchmark-code&gt;WARNING: Logging before flag parsing goes to stderr.
W0731 17:08:27.104443 4599719360 lazy_loader.py:50] 
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

2019-07-31 17:08:28.003093: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
W0731 17:08:28.004978 4599719360 cross_device_ops.py:1182] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.
TF_CONFIG:{"cluster": {"worker": ["localhost:12345", "localhost:23456"]}, "task": {"type": "worker", "index": 0}}
W0731 17:08:28.090763 4599719360 dataset_builder.py:439] Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.
W0731 17:08:28.167881 4599719360 deprecation.py:506] From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0731 17:08:28.230738 4599719360 distribute_coordinator.py:825] `eval_fn` is not passed in. The `worker_fn` will be used if an "evaluator" task exists in the cluster.
W0731 17:08:28.230838 4599719360 distribute_coordinator.py:829] `eval_strategy` is not passed in. No distribution strategy will be used for evaluation.
W0731 17:08:28.231610 4599719360 cross_device_ops.py:1177] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.
2019-07-31 17:08:28.233445: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:250] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:12345, 1 -&gt; localhost:23456}
2019-07-31 17:08:28.234122: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:12345
2019-07-31 17:08:28.234138: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:369] Server already started (target: grpc://localhost:12345)
W0731 17:08:28.235424 4599719360 cross_device_ops.py:1177] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.
W0731 17:08:28.280784 4599719360 distribute_coordinator.py:825] `eval_fn` is not passed in. The `worker_fn` will be used if an "evaluator" task exists in the cluster.
W0731 17:08:28.280889 4599719360 distribute_coordinator.py:829] `eval_strategy` is not passed in. No distribution strategy will be used for evaluation.
W0731 17:08:28.281712 4599719360 cross_device_ops.py:1177] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.
W0731 17:08:28.282696 4599719360 cross_device_ops.py:1177] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.
W0731 17:08:28.282995 4599719360 distributed_training_utils.py:1082] ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.
Epoch 1/100
2019-07-31 17:08:28.996419: W ./tensorflow/core/framework/model.h:213] Encountered a stop event that was not preceded by a start event.
    333/Unknown - 10s 29ms/step - loss: 2.2656 - acc: 0.2531
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;from __future__ import absolute_import, division, print_function, unicode_literals
import datetime
import json
import os
import tensorflow_datasets as tfds
import tensorflow as tf
import subprocess
import shlex
import sys

strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()

tfds.disable_progress_bar()

BUFFER_SIZE = 60000
BATCH_SIZE = 64

NUM_WORKERS = 2
GLOBAL_BATCH_SIZE = NUM_WORKERS * BATCH_SIZE

if __name__ == "__main__":
  worker_addrs = ['localhost:12345', 'localhost:23456']
  os.environ['TF_CONFIG'] = json.dumps({
      'cluster': {
          'worker': worker_addrs,
      },
      'task': {'type': 'worker', 'index': int(sys.argv[1])}
  })

  print('TF_CONFIG:' + os.environ['TF_CONFIG'])

  def scale(image, label):
    image = tf.cast(image, tf.float32)
    image /= 255
    return image, label

  def build_and_compile_cnn_model():
    model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),
        tf.keras.layers.MaxPooling2D(),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    model.compile(
        loss=tf.keras.losses.sparse_categorical_crossentropy,
        optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
        metrics=['accuracy'])
    return model

  datasets, info = tfds.load(name='mnist',
                             with_info=True,
                             as_supervised=True)

  train_datasets_unbatched = datasets['train'].map(scale).shuffle(BUFFER_SIZE)

  train_datasets = train_datasets_unbatched.batch(GLOBAL_BATCH_SIZE)

  with strategy.scope():
    multi_worker_model = build_and_compile_cnn_model()
  multi_worker_model.fit(x=train_datasets, epochs=100)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='jtang7' date='2019-08-01T00:22:54Z'>
		Can you try by setting the TF_CONFIG another way (for e.g. by actually setting an env variable when you start the process)? I wonder if the strategy is not picking up your TF_CONFIG updates, especially because you create the strategy before TF_CONFIG is modified (and reading the TF_CONFIG happens during initialization of the strategy instance: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/distribute/collective_all_reduce_strategy.py#L104&gt;https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/distribute/collective_all_reduce_strategy.py#L104&lt;/denchmark-link&gt;
)
		</comment>
		<comment id='4' author='jtang7' date='2019-08-01T00:43:20Z'>
		Yes, that works. Thanks so much! it would be nice this dependency could be mentioned somewhere.
Close the issue.
		</comment>
		<comment id='5' author='jtang7' date='2019-08-01T00:43:21Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=31154&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=31154&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>