<bug id='33277' author='ay27' open_date='2019-10-12T06:34:30Z' closed_time='2020-06-09T06:59:16Z'>
	<summary>INT8 calibration error using TrtGraphConverterV2 in TensorFlow2.0</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): pip install tensorflow-gpu
TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38
Python version: 3.6.8
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: cuda10.0
GPU model and memory: TITAN V, 12GB

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
Using TensorRT version in: 5.1.5
When using TrtGraphConverterV2 to convert a saved model and trying to calibrate with INT8 support, it will raise the error shown as follow.
Should be mentioned that, the FP16 and FP32 conversation is work fine but only the INT8 is failed.
&lt;denchmark-code&gt;2019-10-12 06:33:06.015951: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.5
2019-10-12 06:33:06.142792: E tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:838] Calibration failed: Invalid argument: Pad only supports explicit padding on 4 dimensional tensor, at StatefulPartitionedCall/resnet50/conv1_pad/Pad
2019-10-12 06:33:06.142978: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Failed to feed calibration data
         [[{{node TRTEngineOp_0}}]]
Traceback (most recent call last):
  File "calibration.py", line 16, in &lt;module&gt;
    converter.convert(calibration_input_fn=input_fn)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compiler/tensorrt/trt_convert.py", line 984, in convert
    self._converted_func(*map(ops.convert_to_tensor, inp))
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py", line 1081, in __call__
    return self._call_impl(args, kwargs)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py", line 1121, in _call_impl
    return self._call_flat(args, self.captured_inputs, cancellation_manager)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py", line 1224, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py", line 511, in call
    ctx=ctx)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File "&lt;string&gt;", line 3, in raise_from
tensorflow.python.framework.errors_impl.InternalError:  Failed to feed calibration data
         [[node TRTEngineOp_0 (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_pruned_79677]

Function call stack:
pruned

terminate called without an active exception
&lt;/denchmark-code&gt;

Describe the expected behavior
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
from tensorflow.python.compiler.tensorrt import trt_convert as trt
import tensorflow as tf

model = tf.keras.applications.ResNet50(weights=None)
out = model(tf.random.normal((2, 224, 224, 3)))
tf.saved_model.save(model, './saved_model/')
print(model.input_names, model.output_names)

params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(precision_mode='INT8',
                                                    use_calibration=True)
converter = trt.TrtGraphConverterV2(input_saved_model_dir='./saved_model/', conversion_params=params)

def input_fn():
    for i in range(10):
        yield tf.random.normal((1, 224, 224, 3))
converter.convert(calibration_input_fn=input_fn)  #  raise error here
converter.save('./model.trt')
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='ay27' date='2019-10-12T10:14:03Z'>
		I also tried another simple model and found that the existing Conv2D layer will lead to error. Also when change to FP16 or FP32, all the convert works fine!! And no matter the data format is channels_first or channals_last, the results are the same.
Check this code for more details:
from tensorflow.python.compiler.tensorrt import trt_convert as trt
import tensorflow as tf
from tensorflow.python.framework.func_graph import def_function
from tensorflow.python.framework import tensor_spec

class MyModel(tf.keras.Model):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.conv2 = tf.keras.layers.Conv2D(64, (7, 7), strides=(2, 2), padding='valid', data_format='channels_last')
        self.dense = tf.keras.layers.Dense(10, activation='softmax')

    @def_function.function(input_signature=[
        tensor_spec.TensorSpec(shape=[None, 32, 32, 3], dtype=tf.float32)
    ])
    def call(self, inputs, training=True, **kwargs):
        inputs = self.conv2(inputs)  # remove this line, work fine!!
        x = tf.reshape(inputs, [-1, inputs.shape[-1]])
        x = self.dense(x)
        return x

model = MyModel()
out = model(tf.random.normal((2, 32, 32, 3)))
tf.saved_model.save(model, './saved_model/', {'model_key': model.call})

params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(precision_mode='INT8',
                                                    use_calibration=True)
converter = trt.TrtGraphConverterV2(input_saved_model_dir='./saved_model/', conversion_params=params,
                                    input_saved_model_signature_key='model_key')
def input_fn():
    for i in range(10):
        yield tf.random.normal((1, 32, 32, 3))
converter.convert(calibration_input_fn=input_fn)
converter.save('./model.trt')
		</comment>
		<comment id='2' author='ay27' date='2019-10-14T06:12:02Z'>
		&lt;denchmark-link:https://github.com/ay27&gt;@ay27&lt;/denchmark-link&gt;
 ,
When tried executing the given code, &lt;denchmark-link:https://colab.sandbox.google.com/gist/oanush/6b73dc27aaff2332555c403e2712b804/33277.ipynb&gt;colab&lt;/denchmark-link&gt;
 session crashed for both the given code snippets. Can you check and provide gist of colab to reproduce the issue ?Thanks!
		</comment>
		<comment id='3' author='ay27' date='2019-10-14T06:51:34Z'>
		&lt;denchmark-link:https://github.com/oanush&gt;@oanush&lt;/denchmark-link&gt;
 ,
I tried to reproduce in colab, the crash reason is the environment in colab does not have the TensorRT package. You can check the running log for details.
I am not familiar with colab, and don't know how to install the package on colab. Can you reproduce the problem in a local CUDA environment?
&lt;denchmark-link:https://user-images.githubusercontent.com/3942031/66733274-92ad2480-ee91-11e9-875a-a2190bcea9db.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='ay27' date='2019-10-18T09:42:39Z'>
		Does this issue have any progress?
		</comment>
		<comment id='5' author='ay27' date='2019-11-06T15:35:03Z'>
		I'm also interested. To reproduce in colab one has to install TensorRT 5.1.5
!apt-get install -y --no-install-recommends libnvinfer5=5.1.5-1+cuda10.0 libnvinfer-dev=5.1.5-1+cuda10.0
		</comment>
		<comment id='6' author='ay27' date='2019-12-13T21:09:35Z'>
		same problem here
pip3 install tensorflow-gpu==2.0.0
tf: v2.0.0-rc2-26-g64c3d38 2.0.0
dpkg -l | grep nvinfer

ii  libnvinfer-dev                                              6.0.1-1+cuda10.1                                amd64        TensorRT development libraries and headers
ii  libnvinfer5                                                 5.1.5-1+cuda10.1                                amd64        TensorRT runtime libraries
ii  libnvinfer6                                                 6.0.1-1+cuda10.1                                amd64        TensorRT runtime libraries

output:

tiny_yolov3_3l.py", line 100, in init
converter.convert(calibration_input_fn=self.get_calibration_batch)
File "/home/mka/venvs/tf2/lib/python3.6/site-packages/tensorflow_core/python/compiler/tensorrt/trt_convert.py", line 984, in convert
self._converted_func(*map(ops.convert_to_tensor, inp))
File "/home/mka/venvs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 1081, in call
return self._call_impl(args, kwargs)
File "/home/mka/venvs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 1121, in _call_impl
return self._call_flat(args, self.captured_inputs, cancellation_manager)
File "/home/mka/venvs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 1224, in _call_flat
ctx, args, cancellation_manager=cancellation_manager)
File "/home/mka/venvs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 511, in call
ctx=ctx)
File "/home/mka/venvs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py", line 67, in quick_execute
six.raise_from(core._status_to_exception(e.code, message), None)
File "", line 3, in raise_from
tensorflow.python.framework.errors_impl.InternalError:  Failed to feed calibration data
[[node TRTEngineOp_3 (defined at /home/mka/venvs/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_pruned_22003]


Function call stack:
pruned


terminate called without an active exception
Aborted (core dumped)

		</comment>
		<comment id='7' author='ay27' date='2020-01-06T20:53:12Z'>
		&lt;denchmark-link:https://github.com/ay27&gt;@ay27&lt;/denchmark-link&gt;
 could you try letting the input_fn return a tuple?

		</comment>
		<comment id='8' author='ay27' date='2020-01-06T20:58:33Z'>
		After making the above change, the code snippet mentioned by &lt;denchmark-link:https://github.com/ay27&gt;@ay27&lt;/denchmark-link&gt;
 works well for me.
		</comment>
		<comment id='9' author='ay27' date='2020-06-04T19:07:14Z'>
		
@ay27 could you try letting the input_fn return a tuple?
 yield tf.random.normal((1, 224, 224, 3)),

&lt;denchmark-link:https://github.com/ay27&gt;@ay27&lt;/denchmark-link&gt;
,
Is this still an issue? Please feel free to close the issue if resolved. Thanks!
		</comment>
		<comment id='10' author='ay27' date='2020-06-09T06:59:18Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33277&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33277&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='ay27' date='2020-11-29T22:56:59Z'>
		I had a similar problem when trying to convert SSD MobileNet2 to INT8.
The error was :
&lt;denchmark-code&gt;InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  Input shape axis 0 must equal 1, got shape [320,320,3]
	 [[node StatefulPartitionedCall/Preprocessor/unstack (defined at &lt;ipython-input-68-f0b69da9b8ec&gt;:1) ]]
	 [[StatefulPartitionedCall/Postprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/Reshape_11/_38]]
  (1) Invalid argument:  Input shape axis 0 must equal 1, got shape [320,320,3]
	 [[node StatefulPartitionedCall/Preprocessor/unstack (defined at &lt;ipython-input-68-f0b69da9b8ec&gt;:1) ]]
0 successful operations.
0 derived errors ignored. [Op:__inference_pruned_835619]

Function call stack:
pruned -&gt; pruned
&lt;/denchmark-code&gt;

Adding a comma at the end to specify a tuple data format solved the issue :)
		</comment>
	</comments>
</bug>