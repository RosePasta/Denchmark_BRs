<bug id='14480' author='t-fi' open_date='2017-11-11T10:42:22Z' closed_time='2018-01-05T18:01:38Z'>
	<summary>Dataset API batching is slow for numpy arrays</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04
TensorFlow installed from (source or binary):
source
TensorFlow version (use command below):
1.4rc1
Python version:
3.5.2
Bazel version (if compiling from source):
Not sure, think 0.7?
GCC/Compiler version (if compiling from source):
5.4
CUDA/cuDNN version:
8/6
GPU model and memory:
gtx980
Exact command to reproduce:
see attached

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

The tf.data.Dataset.batch() functions seems to be slow when concatenating numpy arrays into batches.
The attached code basically pushes dummy data ([0, 1, 2, ... , NUM_ITEMS] resembling the output of range()) through tensorflow by different approaches.
There are basically two ways to go about this:

Use the tf.data.Dataset.range()
Use custom generators and tf.data.Dataset.from_generator()

Also the data can be batched to increase throughput as it is usually done in deep learning. This can be done either in the generator itself or via tf.data.Dataset.batch(). The latter appears to be ~30x slower for numpy arrays. Somehow this is not the case for 'native' tensorflow generation.
I attached a benchmark to reproduce this. Please note that the behavior for larger amounts of data e.g. images is similar, resulting in only tens of images per second instead of hundreds.
&lt;denchmark-h:h3&gt;Comments to the source code:&lt;/denchmark-h&gt;

gen() is a python generator resembling range().
gen_batch() is also a python generator, but yields batches of 100 numbers instead of single numbers.
These two generators are benchmarked first, yielding millions of elements per second. They should not be a bottleneck.
Then we define a few tf.data.Dataset:
ds_range_single is similar to gen(), using tf.data.Dataset.range()
ds_range_batch is similar to gen_batch(), using tf.data.Dataset.range().batch()
ds_npy_single uses gen() with tf.data.Dataset.from_generator()
ds_npy_batch uses gen() with tf.data.Dataset.from_generator().batch()
ds_npy_single_batch uses uses gen_batch() and will perform much better than ds_npy_batch. In fact it will be close to the 'native' ds_range_batch.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

from time import time
import numpy as np
import tensorflow as tf

NUM_ITEMS = 100000
BATCH_SIZE = 100


def gen():
    for x in np.arange(0, NUM_ITEMS, 1, dtype=np.int64):
        yield x


def gen_batch():
    for x in np.arange(0, NUM_ITEMS, 1, dtype=np.int64).reshape(NUM_ITEMS//BATCH_SIZE, BATCH_SIZE):
        yield x

start = time()
for _ in gen():
    pass
py_single_generator_time = time() - start

start = time()
for _ in gen_batch():
    pass
py_batch_generator_time = time() - start

ds_range_single = tf.data.Dataset.range(NUM_ITEMS)\
    .make_one_shot_iterator().get_next()

ds_range_batch = tf.data.Dataset.range(NUM_ITEMS)\
    .batch(BATCH_SIZE).make_one_shot_iterator().get_next()

ds_npy_single = tf.data.Dataset.from_generator(
    gen, output_types=tf.int64, output_shapes=tf.TensorShape([]))\
    .make_one_shot_iterator().get_next()

ds_npy_batch = tf.data.Dataset.from_generator(
    gen, output_types=tf.int64, output_shapes=tf.TensorShape([]))\
    .batch(BATCH_SIZE).make_one_shot_iterator().get_next()

ds_npy_single_batch = tf.data.Dataset.from_generator(
    gen_batch, output_types=tf.int64, output_shapes=tf.TensorShape([BATCH_SIZE]))\
    .make_one_shot_iterator().get_next()

with tf.Session() as sess:
    start = time()
    for _ in range(NUM_ITEMS):
        sess.run(ds_npy_single)
    npy_single_time = time() - start

    start = time()
    for _ in range(NUM_ITEMS // BATCH_SIZE):
        sess.run(ds_npy_batch)
    npy_batch_time = time() - start

    start = time()
    for _ in range(NUM_ITEMS // BATCH_SIZE):
        sess.run(ds_npy_single_batch)
    npy_single_batch_time = time() - start

    start = time()
    for _ in range(NUM_ITEMS):
        sess.run(ds_range_single)
    range_single_time = time() - start

    start = time()
    for _ in range(NUM_ITEMS // BATCH_SIZE):
        sess.run(ds_range_batch)
    range_batch_time = time() - start


print('Python single generator examples/s :', NUM_ITEMS/py_single_generator_time)
print('Python batch generator examples/s :', NUM_ITEMS/py_batch_generator_time)
print('tf npy single examples/s :', NUM_ITEMS/npy_single_time)
print('tf npy batch examples/s :', NUM_ITEMS/npy_batch_time)
print('tf npy single batch examples/s', NUM_ITEMS/npy_single_batch_time)
print('tf range single examples/s :', NUM_ITEMS/range_single_time)
print('tf range batch examples/s :', NUM_ITEMS/range_batch_time)
&lt;denchmark-h:h3&gt;Prints the following on my machine:&lt;/denchmark-h&gt;

Python single generator examples/s : 3779128.899140432
Python batch generator examples/s : 137113566.52500817
tf npy single examples/s : 2623.528591111384
tf npy batch examples/s : 10184.987886595052
tf npy single batch examples/s 308841.62594729604
tf range single examples/s : 4673.512357184766
tf range batch examples/s : 352137.31184393575
	</description>
	<comments>
		<comment id='1' author='t-fi' date='2017-11-11T18:07:25Z'>
		This question is better asked on  &lt;denchmark-link:http://stackoverflow.com/questions/tagged/tensorflow&gt;StackOverflow&lt;/denchmark-link&gt;
 since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!
		</comment>
		<comment id='2' author='t-fi' date='2017-11-11T23:15:48Z'>
		If it's  much slower than Python, there might be a bug. &lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
 is looking into similar issues in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/13101&gt;#13101&lt;/denchmark-link&gt;
, and this test case is really useful, thanks!
		</comment>
		<comment id='3' author='t-fi' date='2017-12-20T01:25:00Z'>
		It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
		</comment>
		<comment id='4' author='t-fi' date='2017-12-20T18:15:08Z'>
		As per my comment on &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/13101&gt;#13101&lt;/denchmark-link&gt;
, executing  pipelines comes with overhead (moving data between Python and C++, scheduling TensorFlow ops, ...). I agree that the difference in throughput is surprising and will take a closer look at this. Having said that, the  runtime performance is not optimized for this type of pipeline; it is optimized for pipelines that do non-trivial I/O and data transformations which can additionally benefit from parallel processing.
For instance, if the generator in your example would do work that requires one second of CPU time per element, then executing the non-tf.data pipeline for 1000 elements would take ~1000 seconds irrespective of your hardware. In contrast, you can write a tf.data pipeline that does the work inside of tf.data.Dataset.map, which can be parallelized and on a machine with n CPU cores, the pipeline would take ~1000/n seconds to execute.
In other words, the objective of tf.data runtime is not to match the performance of sequential execution of Python code on trivial pipelines. Its objective is to efficiently utilize multicore architectures for non-trivial pipelines.
		</comment>
		<comment id='5' author='t-fi' date='2018-01-04T19:11:42Z'>
		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
		</comment>
		<comment id='6' author='t-fi' date='2019-04-12T20:12:29Z'>
		&lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
 why was this issue closed? Im trying to debug my input pipeline, and to isolate problems with my dataset and reading from disk, I had a similar testcase where I was trying to generate random/dummy input data and noticed this huge delay.
		</comment>
	</comments>
</bug>