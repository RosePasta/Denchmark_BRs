<bug id='41632' author='bjarthur' open_date='2020-07-22T16:20:55Z' closed_time='2020-08-12T16:21:11Z'>
	<summary>ModelCheckpoint save_freq incompatible with Model.fit validation_freq</summary>
	<description>
please see &lt;denchmark-link:https://github.com/keras-team/keras/issues/13689&gt;this issue&lt;/denchmark-link&gt;
 filed on the now unsupported keras repo.  &lt;denchmark-link:https://github.com/gattia&gt;@gattia&lt;/denchmark-link&gt;
 there has it &lt;denchmark-link:https://github.com/keras-team/keras/issues/13689#issuecomment-583758348&gt;figured out&lt;/denchmark-link&gt;
 i believe.  would be nice to have a work around.
in brief, when using ModelCheckpoint(save_freq=&lt;int&gt;... combined with model.fit(validation_freq=&lt;int&gt;... then "WARNING:tensorflow:Can save best model only with val_loss available, skipping." is emitted. even when you've carefully calculated the two to be the same number of epochs.
	</description>
	<comments>
		<comment id='1' author='bjarthur' date='2020-07-29T14:37:50Z'>
		&lt;denchmark-link:https://github.com/bjarthur&gt;@bjarthur&lt;/denchmark-link&gt;

Can you please refer to these links with reported to the error reported;
&lt;denchmark-link:https://stackoverflow.com/questions/59706714/warningtensorflowearly-stopping-conditioned-on-metric-val-binary-accuracy-wh&gt;link&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://stackoverflow.com/questions/53698035/failed-to-get-convolution-algorithm-this-is-probably-because-cudnn-failed-to-in&gt;link1&lt;/denchmark-link&gt;

Please share simple stand alone code [indented with all dependencies] such that we can replicate the issue faced or if possible share a colab gist with the error for us to analyse.
		</comment>
		<comment id='2' author='bjarthur' date='2020-08-05T15:22:59Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='3' author='bjarthur' date='2020-08-12T16:21:09Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='4' author='bjarthur' date='2020-08-12T16:21:12Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41632&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41632&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='bjarthur' date='2020-08-17T23:46:16Z'>
		&lt;denchmark-link:https://github.com/bjarthur&gt;@bjarthur&lt;/denchmark-link&gt;
 can you please reopen the issue?
&lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;
 the problem is not at the links you've presented. Yes, it originally seems like it could be associated with early stopping, but is not, it is a model checkpoint problem.
Attached below is a MWE of the problem. I have tested this on TF 2.0.0. locally and 2.3.0 on colab.
Essentially, if you try to set save_freq in ModelCheckpoint to be any integer it fails producing the warning:
WARNING:tensorflow:Can save best model only with val_loss available, skipping.
each time that it tries to save the model.
The logic and explicit code locations that cause this are outlined in this post:
&lt;denchmark-link:https://github.com/keras-team/keras/issues/13689#issuecomment-662598329&gt;keras-team/keras#13689 (comment)&lt;/denchmark-link&gt;

The gist of that post/the problem is that most people using ModelCheckpoint are benchmarking it against a validation metric (e.g., val_loss), and val_loss or any other validation metric is not calculated until after the epoch is completed (it's calculated when .on_epoch_end  is evoked). However, by nature of the code logic, ModelCheckpoint searches for the val_loss at the end of each batch, which is always before the .on_epoch_end code is invoked. Therefore, when ModelCheckpoint is looking for a val_loss it doesn't exist (and can't/won't) so this error will always appear if someone sets save_freq to be anything other than epoch which is the default and they want the value being monitored to be a validation metric.
The example sets up the save_freq to be at the end of an epoch. This will produce the error once per epoch. Though, it can be any value to produce the warning.
&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.callbacks import ModelCheckpoint
import numpy as np
import tempfile
import os
os.environ['KMP_DUPLICATE_LIB_OK']='True'

# Make Fake Data
x_train = np.random.rand(256, 784).astype("float32")
y_train = np.random.randint(low=0, high=10, size=256)

x_test = np.random.rand(64, 784).astype("float32")
y_test = np.random.randint(low=0, high=10, size=64)

# Build toy model 
inputs = keras.Input(shape=(x_train.shape[1]))
dense = layers.Dense(64, activation="relu")(inputs)
dense = layers.Dense(64, activation="relu")(dense)
outputs = layers.Dense(10)(dense)

model = keras.Model(inputs=inputs, outputs=outputs)

print(model.summary())

model.compile(
    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    optimizer=keras.optimizers.Adam(),
    metrics=["accuracy"],
)

# Set Parameters
batch_size = 32 # divisible by number of examples in train (256)
location_save = tempfile.gettempdir() # Get location of temp directory to not store data permanently

# Align modelcheckpoint to end of epoch based on TF version. 
# Note only tf == 2.0 has been tried, as it was what is available on Conda (recommended install method)
if ((tf.__version__[:3] == '2.0') or
	(tf.__version__[:3] == '2.1')
	):
	save_freq = x_train.shape[0]
elif ((tf.__version__[:3] == '2.2') or
	  (tf.__version__[:3] == '2.3')
	  ):
	save_freq = x_train.shape[0] // batch_size

verbose = 2 # To easily see number of epochs and "warning" print statements from TF. 

# Create model checkpoint
model_checkpoint = ModelCheckpoint(filepath=os.path.join(location_save, 
													     'temp_mnist_weights.h5'),
                                   monitor='val_loss',
                                   verbose=verbose,
                                   save_best_only=True,
                                   save_weights_only=True,
                                   save_freq=save_freq
                                   )

# Fit model 
model.fit(x_train, 
		  y_train,
		  validation_data=(x_test, y_test),
		  batch_size=batch_size, 
		  epochs=2, 
		  callbacks=[model_checkpoint],
		  verbose=verbose)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='bjarthur' date='2020-08-22T15:00:32Z'>
		&lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;
 can you please re-open this issue?  there is now code as you requested.  i don't appear to be able to reopen.  thanks!
		</comment>
		<comment id='7' author='bjarthur' date='2020-08-30T13:52:22Z'>
		The same incompatibility exists with other callbacks like reduce lr on plateau
		</comment>
		<comment id='8' author='bjarthur' date='2020-08-30T14:15:10Z'>
		&lt;denchmark-link:https://github.com/ntakouris&gt;@ntakouris&lt;/denchmark-link&gt;
 thanks for adding that.
&lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;
 can this be reopened? Or should I open a new issue?
		</comment>
	</comments>
</bug>