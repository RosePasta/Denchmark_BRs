<bug id='31214' author='ali-raza-tariq' open_date='2019-08-01T00:06:40Z' closed_time='2019-12-20T22:14:39Z'>
	<summary>INFO:tensorflow:Graph was finalized, Job hangs indefinitely when running with istio-sidecars</summary>
	<description>
Hi guys, i am rather new to tensorflow- and trying to run a distributed tensorflow job (from here &lt;denchmark-link:https://github.com/learnk8s/distributed-tensorflow-on-k8s&gt;https://github.com/learnk8s/distributed-tensorflow-on-k8s&lt;/denchmark-link&gt;
). Its a standard character recognition job using MNIST dataset. I can successfully run the job on locally deployed Kubernetes cluster without any issue. But when i try to deploy the same job in an istio-injection=enabled namespace, the job keeps on running forever. Upon looking at the logs of individual pods ... its both chief-pod &amp; worker-pods are stuck at "INFO:tensorflow:Graph was finalized." step. Which is repeated indefinitely.

&lt;denchmark-code&gt;INFO:tensorflow:Graph was finalized.
INFO:tensorflow:An error was raised while a session was being created. This may be due to a preemption of a connected worker or parameter server. A new session will be created. Error: Socket closed
&lt;/denchmark-code&gt;

System information

OS Platform and Distribution (Linux Ubuntu 18.04):
TensorFlow installed from (pip install):
TensorFlow version:1.8

chief-pod logs
&lt;denchmark-code&gt;WARNING:tensorflow:From /app/main.py:95: load_dataset (from tensorflow.contrib.learn.python.learn.datasets) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.data.
WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/__init__.py:80: load_mnist (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:300: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Please write your own downloading logic.
WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.data to implement this functionality.
WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.data to implement this functionality.
WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: __init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
INFO:tensorflow:TF_CONFIG environment variable: {u'environment': u'cloud', u'cluster': {u'ps': [u'tfjob1-ps-0.istio-test.svc:2222'], u'chief': [u'tfjob1-chief-0.istio-test.svc:2222'], u'worker': [u'tfjob1-worker-0.istio-test.svc:2222', u'tfjob1-worker-1.istio-test.svc:2222']}, u'task': {u'index': 0, u'type': u'chief'}}
INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_session_config': None, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': u'chief', '_train_distribute': None, '_is_chief': True, '_cluster_spec': &lt;tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe6a9ab9190&gt;, '_model_dir': './out/vars', '_num_worker_replicas': 3, '_task_id': 0, '_log_step_count_steps': 100, '_master': u'grpc://tfjob1-chief-0.istio-test.svc:2222', '_save_checkpoints_steps': 20, '_keep_checkpoint_every_n_hours': 10000, '_evaluation_master': '', '_service': None, '_global_id_in_cluster': 0, '_save_summary_steps': 20, '_num_ps_replicas': 1}
INFO:tensorflow:Start Tensorflow server.
2019-07-31 23:54:35.170532: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-31 23:54:35.175567: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job chief -&gt; {0 -&gt; localhost:2222}
2019-07-31 23:54:35.175633: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; tfjob1-ps-0.istio-test.svc:2222}
2019-07-31 23:54:35.175660: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; tfjob1-worker-0.istio-test.svc:2222, 1 -&gt; tfjob1-worker-1.istio-test.svc:2222}
2019-07-31 23:54:35.179473: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:332] Started server with target: grpc://localhost:2222
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:An error was raised while a session was being created. This may be due to a preemption of a connected worker or parameter server. A new session will be created. Error: Socket closed
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:An error was raised while a session was being created. This may be due to a preemption of a connected worker or parameter server. A new session will be created. Error: Socket closed
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:An error was raised while a session was being created. This may be due to a preemption of a connected worker or parameter server. A new session will be created. Error: Socket closed
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:An error was raised while a session was being created. This may be due to a preemption of a connected worker or parameter server. A new session will be created. Error: Socket closed
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:An error was raised while a session was being created. This may be due to a preemption of a connected worker or parameter server. A new session will be created. Error: Socket closed
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:An error was raised while a session was being created. This may be due to a preemption of a connected worker or parameter server. A new session will be created. Error: Socket closed
&lt;/denchmark-code&gt;

I am not an expert in tensorflow, so please provide some insights what might be causing this error. I get the same logs from the worker-pods as well (something regarding socket closed). This only happens when i try to run the job with istio-sidecars, otherwise it runs fine.
thanks!
	</description>
	<comments>
		<comment id='1' author='ali-raza-tariq' date='2019-08-02T11:35:16Z'>
		Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?
Make sure you also include the code snippet to reproduce the issue. If you are unclear what to include see the issue template displayed in the Github new issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/new/choose&gt;template.&lt;/denchmark-link&gt;

We ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!
		</comment>
		<comment id='2' author='ali-raza-tariq' date='2019-08-02T16:59:15Z'>
		Platform: Running on top of 4-node (baremetal) Kubernetes Cluster
Operating System/ Architecture:  Ubuntu 18.04.2 LTS/ x86_64
Tensorflow version: 1.8  (used pip install to setup pod environment)
Kubeflow version: v0.6.0
tfjob.yaml - manifest file to launch the job
&lt;denchmark-code&gt;apiVersion: kubeflow.org/v1beta2
kind: TFJob
metadata:
  name: tfjob1
spec:
  tfReplicaSpecs:
    Chief:
      replicas: 1
      template:
        spec:
          volumes:
            - name: local-pv
              persistentVolumeClaim:
                claimName: local-pvc
          containers:
            - name: tensorflow
              image: learnk8s/mnist:1.0.0
              imagePullPolicy: Always
              args:
                - --model_dir
                - ./out/vars
                - --export_dir
                - ./out/models
              volumeMounts:
                - mountPath: /app/out
                  name: local-pv
          restartPolicy: OnFailure
    Worker:
      replicas: 2
      template:
        spec:
          containers:
            - name: tensorflow
              image: learnk8s/mnist:1.0.0
              imagePullPolicy: Always
          restartPolicy: OnFailure
    PS:
      replicas: 1
      template:
        spec:
          volumes:
            - name: local-pv
              persistentVolumeClaim:
                claimName: local-pvc
          containers:
            - name: tensorflow
              image: learnk8s/mnist:1.0.0
              imagePullPolicy: Always
              volumeMounts:
                - mountPath: /app/out
                  name: local-pv
          restartPolicy: OnFailure
&lt;/denchmark-code&gt;

steps to recreate the issue
Installing kubeflow already installs the istio-system namespace. Create a new namespace to launch the tfjob and enable side-car injection using kubectl label namespace default istio-injection=enabled. Then just launch the job using the attached manifest file. The pods start running and keep on running. Checking the logs of the pods shows the above mentioned error.
Please let me know if i need to elaborate anything else!
		</comment>
		<comment id='3' author='ali-raza-tariq' date='2019-08-06T23:58:25Z'>
		&lt;denchmark-link:https://github.com/ali-raza-tariq&gt;@ali-raza-tariq&lt;/denchmark-link&gt;
 Is it possible for you to try latest TF versions and let us know whether the issue persists? There were lots of performance improvements in the latest versions. Thanks!
		</comment>
		<comment id='4' author='ali-raza-tariq' date='2019-08-08T23:56:26Z'>
		hi &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 i tried with the latest version (tf-v1.14.0) and now receiving this error - 
&lt;denchmark-code&gt;2019-08-08 23:49:47.347991: W tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:157] RPC failed with status = "Unavailable: Socket closed" and grpc_error_string = "{"created":"@1565308187.347861590","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Socket closed","grpc_status":14}", maybe retrying the RPC
2019-08-08 23:49:47.348028: W tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:161] Too many retries, returning last status: Unavailable: Socket closed
I0808 23:49:47.352538 139691432208192 monitored_session.py:1215] An error was raised while a session was being created. This may be due to a preemption of a connected worker or parameter server. A new session will be created. This error may also occur due to a gRPC failure caused by high memory or network bandwidth usage in the parameter servers. If this error occurs repeatedly, try increasing the number of parameter servers assigned to the job. Error: Socket closed
I0808 23:49:47.352852 139691432208192 monitored_session.py:240] Graph was finalized.
&lt;/denchmark-code&gt;

again ... this keeps error keeps repeating! its seems like the same error with a little more information
		</comment>
		<comment id='5' author='ali-raza-tariq' date='2019-08-29T12:55:30Z'>
		Got the same problem with TF 1.14.0. I'm just trying to run this tutorial on TPU v2-8 in the train_and_eval mode: &lt;denchmark-link:https://cloud.google.com/tpu/docs/tutorials/retinanet&gt;https://cloud.google.com/tpu/docs/tutorials/retinanet&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='ali-raza-tariq' date='2019-08-29T14:49:14Z'>
		The problem went away after I fixed my data. I understand that this is a general, catch-all error that would be triggered in case of any problems with your model.
I would recommend switching to GPU or CPU and make sure the model works as expected.
However, it would be beneficial if TF could report the exact error from the remote master.
		</comment>
		<comment id='7' author='ali-raza-tariq' date='2019-09-03T08:54:39Z'>
		&lt;denchmark-link:https://github.com/artyompal&gt;@artyompal&lt;/denchmark-link&gt;
 Could you elaborate what is ? The error message indicates the connection to the remote worker is lost and therefore it doesn't know what happens on that remote worker.
		</comment>
		<comment id='8' author='ali-raza-tariq' date='2019-09-08T17:39:44Z'>
		I mean that in my case, tfrecords had an error and they didn't correspond to the model. Something was missing or in the wrong format (i.e. string instead of int32).

The error message indicates the connection to the remote worker is lost and therefore it doesn't know what happens on that remote worker.
This doesn't necessarily mean it's a network problem. This message would be triggered in case of any TensorFlow error on the remote worker.

		</comment>
		<comment id='9' author='ali-raza-tariq' date='2019-12-20T22:14:39Z'>
		This is a stale issue. Please check the issue with latest TensorFlow. If the issue still persists in the newer version of TF, please feel free to reopen it by providing details about the issue and a standalone code to reproduce the issue. Thanks!
		</comment>
		<comment id='10' author='ali-raza-tariq' date='2019-12-20T22:14:41Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31214&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31214&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>