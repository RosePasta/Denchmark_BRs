<bug id='35574' author='birdmw' open_date='2020-01-03T22:09:07Z' closed_time='2020-07-14T19:55:17Z'>
	<summary>OP_REQUIRES failed due to missing tempstate or invalid argument</summary>
	<description>
System information

Custom implementation of ALBERT for TF2.0
Training on DataBricks
TensorFlow installed from (source or binary): Binary pypi
TensorFlow version (use command below): 2.0-gpu
Python version: 3.7
CUDA/cuDNN version: 10.1
GPU model and memory: Databricks 4xGPU cluster X8

Describe the current behavior
Current behavior: Training proceeds through 3 epochs and barfs on Train Step 399/1330 in the _save_checkpoint routine. We are using ADLSgen2 to store data, and we are mounting to that filesystem. Filesystem works transparently for all tasks so far &amp; we have DataBricks team support. There are many read/writes to this mount point so I am not quick to blame the filesystem - but it's still worth noting.
The actual error is:
2020-01-03 20:51:34.688654: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at save_restore_v2_ops.cc:137 : Invalid argument: /dbfs/mnt/devscoutprototype/eval_out/checkpoint/ctl_step_399.ckpt-3_temp_08f0418651184fbd97263e13fc11b45c/part-00001-of-00002.data-00000-of-00001.tempstate9461307533243821894; Invalid argument
When we navigate to this location we find the folder is genuinely there, however inside that folder there is not this temp file. Instead there are two files:
1)part-00000-of-00002.data-00000-of-00001
2)part-00000-of-00002.index
Describe the expected behavior
I would expect to see a new checkpoint at this step.

This is made following the examples of &lt;denchmark-link:https://github.com/kamalkraj/ALBERT-TF2.0&gt;https://github.com/kamalkraj/ALBERT-TF2.0&lt;/denchmark-link&gt;

If you simply run this example top to bottom on DataBricks you will encounter this error.
Other info / logs
loss = 0.2804690897464752
I0103 20:48:47.790678 140033029736192 model_training_utils.py:346] Train Step: 396/1330  / loss = 0.3561434745788574
I0103 20:48:50.499201 140033029736192 model_training_utils.py:346] Train Step: 397/1330  / loss = 0.26206889748573303
I0103 20:48:53.168039 140033029736192 model_training_utils.py:346] Train Step: 398/1330  / loss = 0.3312344551086426
I0103 20:48:55.838387 140033029736192 model_training_utils.py:346] Train Step: 399/1330  / loss = 0.41013699769973755
2020-01-03 20:51:34.688654: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at save_restore_v2_ops.cc:137 : Invalid argument: /dbfs/mnt/devscoutprototype/eval_out/checkpoint/ctl_step_399.ckpt-3_temp_08f0418651184fbd97263e13fc11b45c/part-00001-of-00002.data-00000-of-00001.tempstate9461307533243821894; Invalid argument
Traceback (most recent call last):
File "/dbfs/mnt/devscoutprototype/ALBERT-TF2.0-master/run_classifer.py", line 455, in 
app.run(main)
File "/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/absl/app.py", line 299, in run
_run_main(main, args)
File "/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/absl/app.py", line 250, in _run_main
sys.exit(main(argv))
File "/dbfs/mnt/devscoutprototype/ALBERT-TF2.0-master/run_classifer.py", line 356, in main
custom_callbacks = custom_callbacks)
File "/dbfs/mnt/devscoutprototype/ALBERT-TF2.0-master/model_training_utils.py", line 354, in run_customized_training_loop
checkpoint_name.format(step=current_step))
File "/dbfs/mnt/devscoutprototype/ALBERT-TF2.0-master/model_training_utils.py", line 33, in _save_checkpoint
saved_path = checkpoint.save(checkpoint_path)
File "/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/util.py", line 1889, in save
file_path = self.write("%s-%d" % (file_prefix, checkpoint_number))
File "/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/util.py", line 1819, in write
output = self._saver.save(file_prefix=file_prefix)
File "/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/util.py", line 1155, in save
file_prefix=file_prefix_tensor, object_graph_tensor=object_graph_tensor)
File "/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/util.py", line 1103, in _save_cached_when_graph_building
save_op = saver.save(file_prefix)
File "/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/training/saving/functional_saver.py", line 230, in save
sharded_saves.append(saver.save(shard_prefix))
File "/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/training/saving/functional_saver.py", line 72, in save
return io_ops.save_v2(file_prefix, tensor_names, tensor_slices, tensors)
File "/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_io_ops.py", line 1933, in save_v2
ctx=_ctx)
File "/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_io_ops.py", line 1970, in save_v2_eager_fallback
ctx=_ctx, name=name)
File "/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py", line 67, in quick_execute
six.raise_from(core._status_to_exception(e.code, message), None)
File "", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: /dbfs/mnt/devscoutprototype/eval_out/checkpoint/ctl_step_399.ckpt-3_temp_08f0418651184fbd97263e13fc11b45c/part-00001-of-00002.data-00000-of-00001.tempstate9461307533243821894; Invalid argument [Op:SaveV2]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

CalledProcessError                        Traceback (most recent call last)
 in 
----&gt; 1 get_ipython().run_cell_magic('sh', '', "\n# Running classifier:\n\nexport GLUE_DIR='/dbfs/mnt/devscoutprototype/dataset/'\nexport ALBERT_DIR='/dbfs/mnt/devscoutprototype/models/base_2/'\nexport TASK_NAME='CoLA'\nexport OUTPUT_DIR='/dbfs/mnt/devscoutprototype/train_out'\nexport MODEL_DIR='/dbfs/mnt/devscoutprototype/eval_out/'\n\npython /dbfs/mnt/devscoutprototype/ALBERT-TF2.0-master/run_classifer.py --train_data_path=${OUTPUT_DIR}/${TASK_NAME}_train.tf_record --eval_data_path=${OUTPUT_DIR}/${TASK_NAME}_eval.tf_record --input_meta_data_path=${OUTPUT_DIR}/${TASK_NAME}_meta_data --init_checkpoint=${ALBERT_DIR}/tf2_model.h5 --spm_model_file=${ALBERT_DIR}/vocab/30k-clean.model --albert_config_file=${ALBERT_DIR}/config.json --output_dir=${MODEL_DIR} --do_train --task_name=${TASK_NAME} --do_eval --custom_training_loop --train_batch_size=64 --learning_rate=1e-5 --num_train_epochs=10\n")
/databricks/python/lib/python3.7/site-packages/IPython/core/interactiveshell.py in run_cell_magic(self, magic_name, line, cell)
2350             with self.builtin_trap:
2351                 args = (magic_arg_s, cell)
-&gt; 2352                 result = fn(*args, **kwargs)
2353             return result
2354
/databricks/python/lib/python3.7/site-packages/IPython/core/magics/script.py in named_script_magic(line, cell)
140             else:
141                 line = script
--&gt; 142             return self.shebang(line, cell)
143
144         # write a basic docstring:
&lt;/databricks/python/lib/python3.7/site-packages/decorator.py:decorator-gen-110&gt; in shebang(self, line, cell)
/databricks/python/lib/python3.7/site-packages/IPython/core/magic.py in (f, *a, **k)
185     # but it's overkill for just that one bit of state.
186     def magic_deco(arg):
--&gt; 187         call = lambda f, *a, **k: f(*a, **k)
188
189         if callable(arg):
/databricks/python/lib/python3.7/site-packages/IPython/core/magics/script.py in shebang(self, line, cell)
243             sys.stderr.flush()
244         if args.raise_error and p.returncode!=0:
--&gt; 245             raise CalledProcessError(p.returncode, cell, output=out, stderr=err)
246
247     def _run_script(self, p, cell, to_close):
CalledProcessError: Command 'b"\n# Running classifier:\n\nexport GLUE_DIR='/dbfs/mnt/devscoutprototype/dataset/'\nexport ALBERT_DIR='/dbfs/mnt/devscoutprototype/models/base_2/'\nexport TASK_NAME='CoLA'\nexport OUTPUT_DIR='/dbfs/mnt/devscoutprototype/train_out'\nexport MODEL_DIR='/dbfs/mnt/devscoutprototype/eval_out/'\n\npython /dbfs/mnt/devscoutprototype/ALBERT-TF2.0-master/run_classifer.py
	</description>
	<comments>
		<comment id='1' author='birdmw' date='2020-02-21T00:01:48Z'>
		To clarify, "Databricks 4xGPU cluster X8" means there are 8 machines each with 4 GPUs attached? Or one machine with 4 GPUs and "cluster X8" refers to something else?
So presumably DBFS doesn't like moving and/or deleting files, which we do when saving checkpoints. S3 has possibly related issues, so you could try waiting for &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/36388&gt;#36388&lt;/denchmark-link&gt;
 to get in and applying that "don't use temporaries" logic to DBFS too.
Or as a workaround you can save to a local directory and copy the resulting checkpoint to DBFS if there's only one machine involved.
		</comment>
		<comment id='2' author='birdmw' date='2020-03-04T01:40:50Z'>
		FYI &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/36388&gt;#36388&lt;/denchmark-link&gt;
 is in if you want to try doing something similar for DBFS. I'm probably not in the best position to test fixes for DBFS.
		</comment>
		<comment id='3' author='birdmw' date='2020-06-30T18:46:54Z'>
		&lt;denchmark-link:https://github.com/birdmw&gt;@birdmw&lt;/denchmark-link&gt;
,
Is this still an issue?
Could you please check &lt;denchmark-link:https://github.com/allenlavoie&gt;@allenlavoie&lt;/denchmark-link&gt;
's workaround and let us know if you are still facing the issue. Thanks!
		</comment>
		<comment id='4' author='birdmw' date='2020-07-07T19:19:37Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='5' author='birdmw' date='2020-07-14T19:55:16Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='6' author='birdmw' date='2020-07-14T19:55:18Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35574&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35574&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>