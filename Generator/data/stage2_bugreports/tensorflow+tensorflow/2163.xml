<bug id='2163' author='altaetran' open_date='2016-04-29T05:14:13Z' closed_time='2016-05-05T05:58:04Z'>
	<summary>Floating point exception when computing gradients</summary>
	<description>
Hi tensorflow developer team!
I just wanted to first let you know that I really appreciate all the work you are putting into this amazing open source system. It has been a terrific system for me so far. However, I have come into a roadblock when using the tf.gather function in gradients.
I am using
&lt;denchmark-h:h3&gt;Environment info&lt;/denchmark-h&gt;

Operating System: CentOS
Installed version of CUDA and cuDNN:
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
CUDA version 7.5
cuDNN version 7.0 (64 bit)
tensorflow/0.8.0-gpu version
&lt;denchmark-h:h3&gt;Steps to reproduce&lt;/denchmark-h&gt;

I obtain a floating point exception when I run the following code
A_ph = tf.placeholder(tf.float32, shape=(None, 2))
ind_ph = tf.placeholder(tf.int32, shape=(None, 2))
gather = tf.gather(A_ph, ind_ph)
redsum = tf.reduce_sum(gather, 1)
l2 = tf.reduce_sum(redsum)
A = np.array([[0,0],[0,0],[0,0]])
ind = np.zeros((0,2))
grad_op = tf.gradients(l2, A_ph)
out = sess.run(grad_op, feed_dict={A_ph:A, ind_ph:ind})
&lt;denchmark-h:h3&gt;What have you tried?&lt;/denchmark-h&gt;

If you replace ind with a non-empty array, such as [[0,1]] it works fine.
I suspect that the output of redsum = [], which makes l2 disconnected from A_ph for the given ind. Fed forward to computing l2, I get the correct value of 0.0 (since there is nothing to be summed). However, for backpropagation of gradients, I think the output of redsum=[] chokes the algorithm, and causes it to evaluate some illegal operation using an empty tensor, instead of just backpropagating 0.0.
Are there any solutions to this problem currently? In my use of this computation, I won't know ahead of time whether or not the intermediate computations produce [](empty tensor), but I also want to be able to get the correct gradients when this happens. Thank you so much for your consideration!
	</description>
	<comments>
		<comment id='1' author='altaetran' date='2016-04-29T16:54:14Z'>
		Thanks for the bug report.
&lt;denchmark-link:https://github.com/girving&gt;@girving&lt;/denchmark-link&gt;
 tf.gradiensts() is generating an unguarded div by 0.  Internal bug opened.
		</comment>
		<comment id='2' author='altaetran' date='2016-04-29T23:14:41Z'>
		Thanks for reporting!  We're going to fix TensorFlow to produce nice exceptions for integer zero divisions.
		</comment>
		<comment id='3' author='altaetran' date='2016-04-29T23:35:30Z'>
		Ah, there are two bugs here: one bug that we crash the process on integer division by zero, and another that we generate a division by zero.
		</comment>
		<comment id='4' author='altaetran' date='2016-04-29T23:46:23Z'>
		Yes, I think there should never be any division by zero in the above operations, at least from what I can tell from the backpropagation formulas. I think I should definitely be able to get a None gradient to indicate that that l2 and A_ph are disconnected, rather than an exception. Is the bug a complex one to fix?
		</comment>
		<comment id='5' author='altaetran' date='2016-04-29T23:49:47Z'>
		&lt;denchmark-link:https://github.com/altaetran&gt;@altaetran&lt;/denchmark-link&gt;
: It's conceptually straightforward: one has to go through and sanitize all the occurrences of  in .  Unfortunately there are a bunch of them for a variety of different ops, and the fixes look different in the different cases.
However, there's no way to get a None gradient, since the variables are connected, just through an empty tensor bottleneck.  Since we don't necessarily know that the shapes are empty until runtime, we can't produce None.
		</comment>
		<comment id='6' author='altaetran' date='2016-04-29T23:50:51Z'>
		&lt;denchmark-link:https://github.com/altaetran&gt;@altaetran&lt;/denchmark-link&gt;
: To make my initial comment make sense: we want to produce nice error messages even for malformed ops, so we want to fix the underlying divisions by zero regardless of whether we fix the gradient code.
		</comment>
		<comment id='7' author='altaetran' date='2016-04-29T23:55:20Z'>
		Oh I see, so those would be two separate tasks. Is there a reason for why an empty bottle neck tensor in the gradients produces division by zero though? Or is that something to be changed as well?
		</comment>
		<comment id='8' author='altaetran' date='2016-04-29T23:56:49Z'>
		As I said: the fixes are straightforward but there are a number of them.  Here is an example of a bad line: 


tensorflow/tensorflow/python/ops/math_grad.py


         Line 36
      in
      5e22e3a






 tile_scaling = input_shape // output_shape_kept_dims 





		</comment>
		<comment id='9' author='altaetran' date='2016-04-30T00:07:08Z'>
		Oh I see ok. So then this is likely something to occur in a future patch?
		</comment>
		<comment id='10' author='altaetran' date='2016-04-30T00:08:52Z'>
		I'm looking at it now, so not too long except for the upcoming weekend. :)
I'll probably fix the gradients first since making integer division safe is slightly more awkward (to do in a reasonably performant manner).
		</comment>
		<comment id='11' author='altaetran' date='2016-05-02T20:07:39Z'>
		CL submitted, so it should be on Github within 24 hours.
		</comment>
		<comment id='12' author='altaetran' date='2016-05-02T20:21:27Z'>
		To clarify: I fixed the gradients not to divide by zero, which should solve your issue.  Integer divide by zero on its own will still crash the process.
		</comment>
		<comment id='13' author='altaetran' date='2016-05-02T21:05:32Z'>
		Great! Been running into a similar problem myself. As a question, is there a local version of the patch I could use? I'm running Tensorflow 0.8.0 on a cluster, and the sysadmins won't rebuild the cluster version before TF 0.9.0 (or the next stable version)
		</comment>
		<comment id='14' author='altaetran' date='2016-05-02T21:10:13Z'>
		&lt;denchmark-link:https://github.com/rbharath&gt;@rbharath&lt;/denchmark-link&gt;
: Do you want source or binaries?  The source version should have the fix tomorrow.  Not sure when the nightly appears (tomorrow or the next day), but the links for those are here: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/README.md&gt;https://github.com/tensorflow/tensorflow/blob/master/README.md&lt;/denchmark-link&gt;

		</comment>
		<comment id='15' author='altaetran' date='2016-05-02T21:14:52Z'>
		&lt;denchmark-link:https://github.com/girving&gt;@girving&lt;/denchmark-link&gt;
 Sorry, should have been clearer. Is the fix python only? If so, would I be able to just use the corrected python module instead of the standard module without rebuilding the rest of the source? (The issue is that our cluster runs with an old CentOs with an old glibc, so the sysadmins have to do a ton of work to get tensorflow to build, so it's infeasible for us to use binaries or rebuild except at stable releases).
		</comment>
		<comment id='16' author='altaetran' date='2016-05-02T21:19:47Z'>
		&lt;denchmark-link:https://github.com/rbharath&gt;@rbharath&lt;/denchmark-link&gt;
: Ah, yes: the fix I just made is Python only.  However, we don't have any setup for using different Python alongside C++, and in any case you'd have to carefully cherry-pick just my change on top of 0.8.0 to have a hope of succeeding.
		</comment>
	</comments>
</bug>