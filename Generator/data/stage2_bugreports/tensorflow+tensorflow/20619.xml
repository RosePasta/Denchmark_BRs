<bug id='20619' author='kenfehling' open_date='2018-07-08T01:10:07Z' closed_time='2019-06-27T20:29:17Z'>
	<summary>ReduceLROnPlateau with native optimizer: 'TFOptimizer' object has no attribute 'lr'</summary>
	<description>
Please go to Stack Overflow for help and support:
&lt;denchmark-link:https://stackoverflow.com/questions/tagged/tensorflow&gt;https://stackoverflow.com/questions/tagged/tensorflow&lt;/denchmark-link&gt;

If you open a GitHub issue, here is our policy:

It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
The form below must be filled out.
It shouldn't be a TensorBoard issue. Those go here.

Here's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;



Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes


OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
macOS 10.12.6


TensorFlow installed from (source or binary):
Binary (pip)


TensorFlow version (use command below):
1.9.0rc2


Python version:
Python 3.6.4 :: Anaconda custom (x86_64)


Bazel version (if compiling from source):
N/A


GCC/Compiler version (if compiling from source):
N/A


CUDA/cuDNN version:
N/A


GPU model and memory:
N/A


Exact command to reproduce:


&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.layers import Dense
from tensorflow.python.training.adam import AdamOptimizer
import numpy as np

model = Sequential()
model.add(Dense(8, input_shape=(2, )))
model.add(Dense(1, activation='softmax'))
model.compile(optimizer=AdamOptimizer(), loss='mse')

lr_schedule = tf.keras.callbacks.ReduceLROnPlateau()

x = np.random.uniform(0, 1, (100, 2))
y = np.random.uniform(0, 1, (100, 1))
model.fit(x=x, y=y, callbacks=[lr_schedule], validation_split=0.2)
&lt;/denchmark-code&gt;

You can collect some of this information using our environment capture script:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&lt;/denchmark-link&gt;

You can obtain the TensorFlow version with
python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

Using a native optimizer (AdamOptimizer) I can't get ReduceLROnPlateau to work, but it does work using an optimizer from tf.keras.optimizers. Only TF native optimizers are supported in Eager mode, so right now I just don't use ReduceLROnPlateau while in eager mode, but I thought this should be reported. Thank you.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;  File "/Users/ken/Documents/Projects/keras-try/src/lr.py", line 16, in &lt;module&gt;
    model.fit(x=x, y=y, callbacks=[lr_schedule], validation_split=0.2)
  File "/Users/ken/anaconda/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py", line 1348, in fit
    validation_steps=validation_steps)
  File "/Users/ken/anaconda/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py", line 277, in fit_loop
    callbacks.on_epoch_end(epoch, epoch_logs)
  File "/Users/ken/anaconda/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py", line 95, in on_epoch_end
    callback.on_epoch_end(epoch, logs)
  File "/Users/ken/anaconda/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py", line 921, in on_epoch_end
    logs['lr'] = K.get_value(self.model.optimizer.lr)
AttributeError: 'TFOptimizer' object has no attribute 'lr'
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='kenfehling' date='2018-07-09T17:19:57Z'>
		I suspect the problem is coming from here:
&lt;denchmark-link:https://github.com/keras-team/keras/blob/master/keras/callbacks.py#L1037&gt;https://github.com/keras-team/keras/blob/master/keras/callbacks.py#L1037&lt;/denchmark-link&gt;

where it asked the optimizer for lr. On the other hand native AdamOptimizer uses _lr:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/python/training/adam.py#L94&gt;https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/python/training/adam.py#L94&lt;/denchmark-link&gt;

I wonder if we could make a check in the callback, if it's keras optimizer, adjust self.lr, if it's tf optimizer, adjust self._lr. else raise error.
		</comment>
		<comment id='2' author='kenfehling' date='2018-07-09T17:39:07Z'>
		as I was checking, tf native optimizers mostly use 1) self._lr, 2) self._learning_rate. Maybe we could consider unifying them.
		</comment>
		<comment id='3' author='kenfehling' date='2018-09-09T18:49:14Z'>
		Review is completed and still working on implementation. It appears to be more challenging in distributed versions.
		</comment>
		<comment id='4' author='kenfehling' date='2018-10-30T04:09:50Z'>
		Any progress?
		</comment>
		<comment id='5' author='kenfehling' date='2018-10-30T14:38:36Z'>
		Yes. We're near to finish and stay tuned.
		</comment>
		<comment id='6' author='kenfehling' date='2018-11-15T02:23:15Z'>
		I faced a very similar problem, but while using SGD optimizer.
The problem for me was in the file [condaEnv]\Lib\site-packages\tensorflow\python\keras\callbacks.py, specifically, in function on_epoch_begin. I changed the code (I know it is bad) to this:
&lt;denchmark-code&gt;def on_epoch_begin(self, epoch, logs=None):
    # if not hasattr(self.model.optimizer, 'lr'):   &lt;=== Original  self.model.optimizer.optimizer._learning_rate
    if not hasattr(self.model.optimizer.optimizer, '_learning_rate'):
      raise ValueError('Optimizer must have a "lr" attribute.')
    try:  # new API
      # lr = float(K.get_value(self.model.optimizer.lr))
      lr = float(self.model.optimizer.optimizer._learning_rate)
      lr = self.schedule(epoch, lr)
    except TypeError:  # Support for old API for backward compatibility
      lr = self.schedule(epoch)
    if not isinstance(lr, (float, np.float32, np.float64)):
      raise ValueError('The output of the "schedule" function '
                       'should be float.')
    # K.set_value(self.model.optimizer.lr, lr)
    self.model.optimizer.optimizer._learning_rate = lr
    if self.verbose &gt; 0:
      print('\nEpoch %05d: LearningRateScheduler reducing learning '
            'rate to %s.' % (epoch + 1, lr))
&lt;/denchmark-code&gt;

It works for me.
		</comment>
		<comment id='7' author='kenfehling' date='2018-12-11T18:04:44Z'>
		I still have this problem when using
from tensorflow.contrib.opt import AdamWOptimizer
and
from tensorflow.keras.callbacks import ReduceLROnPlateau
with tensorflow-gpu==1.12.0
		</comment>
		<comment id='8' author='kenfehling' date='2018-12-29T17:57:39Z'>
		Similar issue with LearningRateScheduler.
		</comment>
		<comment id='9' author='kenfehling' date='2019-01-01T08:16:02Z'>
		I think this may solve the problem &lt;denchmark-link:https://github.com/horovod/horovod/issues/42&gt;horovod/horovod#42&lt;/denchmark-link&gt;
.
This is my code:
&lt;denchmark-code&gt;learning_rate = K.variable(0.001)
adamW = tf.contrib.opt.AdamWOptimizer(weight_decay=1e-4,
                                      learning_rate=learning_rate, 
                                      beta1=0.9, beta2=0.999, 
                                      epsilon=1e-08, name='AdamW')
opt= TFOptimizer(adamW)
opt.lr = learning_rate
model.compile(optimizer=opt, loss=ssd_loss.compute_loss)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='kenfehling' date='2019-01-17T09:28:45Z'>
		&lt;denchmark-link:https://github.com/chuong98&gt;@chuong98&lt;/denchmark-link&gt;

I get this, any ideas?
('Could not interpret optimizer identifier:', &lt;keras.optimizers.TFOptimizer object at 0x7fdebff42828&gt;)
		</comment>
		<comment id='11' author='kenfehling' date='2019-01-17T09:37:38Z'>
		&lt;denchmark-link:https://github.com/hadaev8&gt;@hadaev8&lt;/denchmark-link&gt;
  did you import keras from tf.keras ?
		</comment>
		<comment id='12' author='kenfehling' date='2019-01-17T09:45:52Z'>
		Nope, it raise error then i try to import
No module named 'tensorflow.keras.optimizers.TFOptimizer'
		</comment>
		<comment id='13' author='kenfehling' date='2019-01-17T10:07:23Z'>
		This is the code I got it work:
&lt;denchmark-code&gt;    from keras.optimizers import TFOptimizer
    learning_rate = K.variable(0.001)
    adamW = tf.contrib.opt.AdamWOptimizer(weight_decay=1e-4,
                                      learning_rate=learning_rate,
                                      beta1=0.9, beta2=0.999,
                                      epsilon=1e-08, name='AdamW')
    opt= TFOptimizer(adamW)
    opt.lr = learning_rate
    model.compile(optimizer=opt, loss=ssd_loss.compute_loss)
&lt;/denchmark-code&gt;

Could you compile it sucessfully?
		</comment>
		<comment id='14' author='kenfehling' date='2019-01-17T12:22:28Z'>
		Oh, i get it, all model's layers should be from keras, but not tf.keras
And for tpu keras model i need tf.keras model, unlucky.
		</comment>
		<comment id='15' author='kenfehling' date='2019-01-17T14:22:19Z'>
		had the same issue when i  used tf.train.AdamOptimizer.
i solved the ReduceLR issue by passing 'adam' to model.compile
model.compile(optimizer='adam',loss='mse') 
		</comment>
		<comment id='16' author='kenfehling' date='2019-01-17T15:07:51Z'>
		&lt;denchmark-link:https://github.com/Avimor88&gt;@Avimor88&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/hadaev8&gt;@hadaev8&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/chuong98&gt;@chuong98&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/WillBrennan&gt;@WillBrennan&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/Tzeny&gt;@Tzeny&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/kenfehling&gt;@kenfehling&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/malikaltakrori&gt;@malikaltakrori&lt;/denchmark-link&gt;
 , can you try to sync up with master version (or tf nightly) to see if it works? We published a new set of optimizers to deal with this issue, you can simply use it by:
opt = tf.keras.Adam(learning_rate=0.01)
model.compile(opt, loss=?, metrics=?)
callbacks = [LearningRateScheduler(your_own_schedule)]
model.fit(x,y, num_epochs=?, callbacks=callbacks)
		</comment>
		<comment id='17' author='kenfehling' date='2019-01-17T15:22:14Z'>
		&lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;
  , it worked using opt = tf.keras.optimizers.Adam(lr=0.001) with tensorflow version 1.12 !
Thank you
		</comment>
		<comment id='18' author='kenfehling' date='2019-01-22T01:30:41Z'>
		i had a same issue when using EarlyStopping, ReduceLROnPlateau. (tensorflow version 1.12)
callbacks = [EarlyStopping, ReduceLROnPlateau]
-&gt; it doesn't work:(
when i use optimizer, opt = tf.keras.optimizers.Adam(lr=0.001)
Error message is
'must be an instance of tf.train.Optimizer, not a &lt;class 'tensorflow.python.keras.optimizers.Adam'
so i used optimizer=tf.train.AdamOptimizer(),
but i get this `AttributeError: 'TFOptimizer' object has no attribute 'lr'
any ideas?
		</comment>
		<comment id='19' author='kenfehling' date='2019-01-25T13:21:34Z'>
		
i had a same issue when using EarlyStopping, ReduceLROnPlateau. (tensorflow version 1.12)
callbacks = [EarlyStopping, ReduceLROnPlateau]
-&gt; it doesn't work:(
when i use optimizer, opt = tf.keras.optimizers.Adam(lr=0.001)
Error message is
'must be an instance of tf.train.Optimizer, not a &lt;class 'tensorflow.python.keras.optimizers.Adam'
so i used optimizer=tf.train.AdamOptimizer(),
but i get this `AttributeError: 'TFOptimizer' object has no attribute 'lr'
any ideas?

Same here. This breaks all models compiled with keras_to_tpu and means you can't get any sort of learning rate decay working; pretty big bug imo.
		</comment>
		<comment id='20' author='kenfehling' date='2019-01-25T17:38:07Z'>
		&lt;denchmark-link:https://github.com/SooDevv&gt;@SooDevv&lt;/denchmark-link&gt;

Example:
`import tensorflow as tf
import numpy as np
from tensorflow.python.keras.callbacks import EarlyStopping, ReduceLROnPlateau
reduce_lr = ReduceLROnPlateau(monitor='val_acc', patience=2, verbose=1, factor=0.5, min_lr=0.00001)
earlystop = EarlyStopping(patience=5)
callbacks = [reduce_lr, earlystop]
from tensorflow.keras import models
from tensorflow.keras import layers
from tensorflow.keras import optimizers
model = models.Sequential()
model.add(layers.Dense(10, activation='relu', input_dim=tr_x.shape[1]))
model.add(layers.Dropout(.5))
model.add(layers.Dense(2, activation='sigmoid'))
model.compile(loss=tf.keras.losses.binary_crossentropy,
optimizer=tf.keras.optimizers.Adam(),
metrics=['accuracy'])
history = model.fit(x=tr_x, y=tr_y,
batch_size=5,
epochs=30,
callbacks=callbacks)`
		</comment>
		<comment id='21' author='kenfehling' date='2019-01-25T17:42:55Z'>
		&lt;denchmark-link:https://github.com/hadaev8&gt;@hadaev8&lt;/denchmark-link&gt;
 I cannot edit your colab, can you try changing tf.train.AdamOptimizer to tf.keras.optimizers.Adam?
		</comment>
		<comment id='22' author='kenfehling' date='2019-01-25T18:33:03Z'>
		&lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;

Its not mine colab, its work, couz it copy keras adam parameters to tensorflow adam every step.
It would be better to do it without copying every step.
		</comment>
		<comment id='23' author='kenfehling' date='2019-01-29T10:41:23Z'>
		&lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;

yes, it works when 'not'  tensorflow eager execution mode!
but it's not working when tensorflow eager execution mode.
&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np
from tensorflow.python.keras.callbacks import EarlyStopping, ReduceLROnPlateau

tf.enable_eager_execution() &lt;-- this **

reduce_lr = ReduceLROnPlateau(monitor='val_acc', patience=2, verbose=1, factor=0.5, min_lr=0.00001)
earlystop = EarlyStopping(patience=5)
callbacks = [reduce_lr, earlystop]
from tensorflow.keras import models
from tensorflow.keras import layers
from tensorflow.keras import optimizers

model = models.Sequential()
model.add(layers.Dense(10, activation='relu', input_dim=tr_x.shape[1]))
model.add(layers.Dropout(.5))
model.add(layers.Dense(8, activation='sigmoid'))

model.compile(loss=tf.keras.losses.binary_crossentropy,
optimizer=tf.keras.optimizers.Adam(),
metrics=['accuracy'])

history = model.fit(x=tr_x, y=tr_y,
batch_size=5,
epochs=30,
callbacks=callbacks)
&lt;/denchmark-code&gt;

ValueError: optimizer must be an instance of tf.train.Optimizer, not a &lt;class 'tensorflow.python.keras.optimizers.Adam'&gt;
		</comment>
		<comment id='24' author='kenfehling' date='2019-01-30T09:37:16Z'>
		Another problem with keras model to tpu, keras adam and ReduceLROnPlateau: it doesnt save current lr to checkpoint.
		</comment>
		<comment id='25' author='kenfehling' date='2019-01-30T16:38:46Z'>
		&lt;denchmark-link:https://github.com/SooDevv&gt;@SooDevv&lt;/denchmark-link&gt;
 Oh I see. Thanks for pointing it out. This has been fixed on 12/11/18, but it seems that it's not in the 1.12 release (which is on Nov). There are two options 1) use 1.13 rc which was released a week ago, 2) use tf nightly.
		</comment>
		<comment id='26' author='kenfehling' date='2019-01-30T16:39:57Z'>
		&lt;denchmark-link:https://github.com/hadaev8&gt;@hadaev8&lt;/denchmark-link&gt;
 I suspect if it's also because of 1.12 release -- can you try 1.13 or tf-nightly? In colab is should be just one-line -- !pip install tf-nightly, that should replace 1.12
		</comment>
		<comment id='27' author='kenfehling' date='2019-01-30T20:07:44Z'>
		&lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;

1.13 need cuda 10, right? I have no idea how to update cuda on colab.
		</comment>
		<comment id='28' author='kenfehling' date='2019-02-06T11:51:03Z'>
		Problem still here on tf 1.13
		</comment>
		<comment id='29' author='kenfehling' date='2019-03-15T18:09:40Z'>
		&lt;denchmark-link:https://github.com/hadaev8&gt;@hadaev8&lt;/denchmark-link&gt;
 It works on my 13.1 version code. Which version are you running?
		</comment>
		<comment id='30' author='kenfehling' date='2019-03-23T19:20:07Z'>
		Same
Here is code
&lt;denchmark-link:https://colab.research.google.com/drive/1Px3i0bXFaJdFdMD2F1-WVO3Cp1xM90Iu?authuser=2#scrollTo=BX2SOcy6lRws&gt;https://colab.research.google.com/drive/1Px3i0bXFaJdFdMD2F1-WVO3Cp1xM90Iu?authuser=2#scrollTo=BX2SOcy6lRws&lt;/denchmark-link&gt;

		</comment>
		<comment id='31' author='kenfehling' date='2019-03-23T20:57:51Z'>
		I am having a similar problem. Not able to use ReduceLROnPlateau along with TPU because of the optimizer constraint.
I already tried changing from tf.train.AdamOptimizer to tf.keras.optimizers.Adam . But in this case get TPU exception "TPU ....... not fetchable". Unless I use the tf.train.AdamOptimizer without the ReduceLROnPlateau , TPU gives an error. Kindly suggest the ideal way to use adaptive LR along with TPU.
		</comment>
		<comment id='32' author='kenfehling' date='2019-03-28T10:23:44Z'>
		So I revisited this just now, looks like it is specifically a problem when training using a generator. model.fit() does not have this bug, where model.fit_generator() does.
As a temporary workaround you can just modify the source of the ReduceLROnPlateau:
&lt;denchmark-code&gt;from tensorflow.python.util.tf_export import tf_export
from tensorflow.python.keras.callbacks import Callback

@tf_export('keras.callbacks.ReduceLROnPlateau')
class ReduceLROnPlateauMODIFIED(Callback):
  """Reduce learning rate when a metric has stopped improving.

  Models often benefit from reducing the learning rate by a factor
  of 2-10 once learning stagnates. This callback monitors a
  quantity and if no improvement is seen for a 'patience' number
  of epochs, the learning rate is reduced.

  Example:

  Arguments:
      monitor: quantity to be monitored.
      factor: factor by which the learning rate will
          be reduced. new_lr = lr * factor
      patience: number of epochs with no improvement
          after which learning rate will be reduced.
      verbose: int. 0: quiet, 1: update messages.
      mode: one of {auto, min, max}. In `min` mode,
          lr will be reduced when the quantity
          monitored has stopped decreasing; in `max`
          mode it will be reduced when the quantity
          monitored has stopped increasing; in `auto`
          mode, the direction is automatically inferred
          from the name of the monitored quantity.
      min_delta: threshold for measuring the new optimum,
          to only focus on significant changes.
      cooldown: number of epochs to wait before resuming
          normal operation after lr has been reduced.
      min_lr: lower bound on the learning rate.
  """

  def __init__(self,
               monitor='val_loss',
               factor=0.1,
               patience=10,
               verbose=0,
               mode='auto',
               min_delta=1e-4,
               cooldown=0,
               min_lr=0,
               **kwargs):
    super(ReduceLROnPlateauMODIFIED, self).__init__()

    self.monitor = monitor
    if factor &gt;= 1.0:
      raise ValueError('ReduceLROnPlateau ' 'does not support a factor &gt;= 1.0.')
    if 'epsilon' in kwargs:
      min_delta = kwargs.pop('epsilon')
      logging.warning('`epsilon` argument is deprecated and '
                      'will be removed, use `min_delta` instead.')
    self.factor = factor
    self.min_lr = min_lr
    self.min_delta = min_delta
    self.patience = patience
    self.verbose = verbose
    self.cooldown = cooldown
    self.cooldown_counter = 0  # Cooldown counter.
    self.wait = 0
    self.best = 0
    self.mode = mode
    self.monitor_op = None
    self._reset()

  def _reset(self):
    """Resets wait counter and cooldown counter.
    """
    if self.mode not in ['auto', 'min', 'max']:
      logging.warning('Learning Rate Plateau Reducing mode %s is unknown, '
                      'fallback to auto mode.', self.mode)
      self.mode = 'auto'
    if (self.mode == 'min' or
        (self.mode == 'auto' and 'acc' not in self.monitor)):
      self.monitor_op = lambda a, b: np.less(a, b - self.min_delta)
      self.best = np.Inf
    else:
      self.monitor_op = lambda a, b: np.greater(a, b + self.min_delta)
      self.best = -np.Inf
    self.cooldown_counter = 0
    self.wait = 0

  def on_train_begin(self, logs=None):
    self._reset()

  def on_epoch_end(self, epoch, logs=None):
    logs = logs or {}
#     print("DEBUG")
#     print(self.model.optimizer.optimizer._opt._lr)
#     print(dir(self.model.optimizer.optimizer._opt._lr))
#     print(K.get_value(self.model.optimizer.optimizer._opt._lr))
    logs['lr'] = self.model.optimizer.optimizer._opt._lr
    current = logs.get(self.monitor)
    if current is None:
      logging.warning('Reduce LR on plateau conditioned on metric `%s` '
                      'which is not available. Available metrics are: %s',
                      self.monitor, ','.join(list(logs.keys())))

    else:
      if self.in_cooldown():
        self.cooldown_counter -= 1
        self.wait = 0

      if self.monitor_op(current, self.best):
        self.best = current
        self.wait = 0
      elif not self.in_cooldown():
        self.wait += 1
        if self.wait &gt;= self.patience:
          old_lr = float(self.model.optimizer.optimizer._opt._lr)
          if old_lr &gt; self.min_lr:
            new_lr = old_lr * self.factor
            new_lr = max(new_lr, self.min_lr)
            self.model.optimizer.optimizer._opt._lr = new_lr
            if self.verbose &gt; 0:
              print('\nEpoch %05d: ReduceLROnPlateau reducing learning '
                    'rate to %s.' % (epoch + 1, new_lr))
            self.cooldown_counter = self.cooldown
            self.wait = 0

  def in_cooldown(self):
    return self.cooldown_counter &gt; 0
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;
  callbacks = list()

  callbacks.append(ReduceLROnPlateauMODIFIED(
    monitor='loss',
    factor=0.5,   # lr = lr*factor
    patience=1,  # how many epochs no change
    verbose=1
  ))
    
  return callbacks
&lt;/denchmark-code&gt;

Here is a colab notebook for further clarification: &lt;denchmark-link:https://colab.research.google.com/drive/13L41k6sOvsm-UIm67cpC_kvCDsObqdDZ&gt;https://colab.research.google.com/drive/13L41k6sOvsm-UIm67cpC_kvCDsObqdDZ&lt;/denchmark-link&gt;

		</comment>
		<comment id='33' author='kenfehling' date='2019-04-26T17:08:27Z'>
		&lt;denchmark-link:https://github.com/pradeepelavarasan&gt;@pradeepelavarasan&lt;/denchmark-link&gt;
 If you convert to tf.keras.optimizers.Adam, what is not fetchable? Can you give the full error message, and code snippet as well?
		</comment>
		<comment id='34' author='kenfehling' date='2019-06-27T19:27:14Z'>
		
had the same issue when i used tf.train.AdamOptimizer.
i solved the ReduceLR issue by passing 'adam' to model.compile
model.compile(optimizer='adam',loss='mse') 

This worked. Thanks!
		</comment>
		<comment id='35' author='kenfehling' date='2019-06-27T20:29:18Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=20619&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=20619&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='36' author='kenfehling' date='2019-06-27T20:43:38Z'>
		

had the same issue when i used tf.train.AdamOptimizer.
i solved the ReduceLR issue by passing 'adam' to model.compile
model.compile(optimizer='adam',loss='mse') 

This worked. Thanks!

I don't think solves the original problem, which was about using tf.train.AdamOptimizer. Last I checked, tf.train optimizers were necessary to get TPU training to really work.
Unless the problem of using ReduceLR w/ tf.train optimizers has been fixed or the issue w/ TPU training using Keras optimizers has been fixed, I would still consider this an open issue.
		</comment>
		<comment id='37' author='kenfehling' date='2019-06-27T20:58:05Z'>
		
I don't think solves the original problem, which was about using tf.train.AdamOptimizer. Last I checked, tf.train optimizers were necessary to get TPU training to really work.
Unless the problem of using ReduceLR w/ tf.train optimizers has been fixed or the issue w/ TPU training using Keras optimizers has been fixed, I would still consider this an open issue.

What do you mean Keras optimizer not working with TPU?
		</comment>
		<comment id='38' author='kenfehling' date='2019-06-27T21:19:07Z'>
		I spent quite a bit of time trying to get TPU optimization working with the Keras optimizers about a month ago and it didn't seem to work correctly. I could use tf.train.Adam and the loss would go down (but I couldn't decay the learning rate).  With the Keras optimizer the loss would not go down (I tried various multiples of the learning rate, in case there was a batch sizing issue, but it didn't seem to help).
I have a Colab Notebook I was working with.  It would need a little work to be ready to share, but here are the relevant bits of the final state of my code:
&lt;denchmark-code&gt;from tensorflow.keras.applications.xception import Xception as base_cnn

BASE_LR = 1e-3

USE_KERAS_OPTIMIZER = True

base_model = base_cnn(include_top=False, input_shape=(256,256,3), pooling='avg')
predictions = Dense(N_CLASSES, activation='softmax')(base_model.output)
model = Model(inputs=base_model.inputs,outputs=predictions)
model = add_regularization(model)

print('WARNING: keras optimizer has an 8x scaled learning rate as a hack to see if that is the issue with slow training')

if USE_KERAS_OPTIMIZER:
  opt = tf.keras.optimizers.Adam(lr=1e-3)
  
  def step_decay(epoch):
    initial_lrate = BASE_LR
    drop = 0.1
    epochs_drop = epochs / 3.0
    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))
    return 8.0*lrate
  
  lrate = LearningRateScheduler(step_decay, verbose=True)
  callbacks = [lrate]

else:
  opt = tf.train.AdamOptimizer(learning_rate=BASE_LR, ep)
  callbacks = []
                                      
model.compile(optimizer=opt,
              loss='categorical_crossentropy', 
              metrics=['categorical_accuracy'])

tf.logging.set_verbosity(tf.logging.INFO)

tpu = tf.contrib.cluster_resolver.TPUClusterResolver()
tpu_model = tf.contrib.tpu.keras_to_tpu_model(
    model,
    strategy=tf.contrib.tpu.TPUDistributionStrategy(tpu))

datestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
hist = tpu_model.fit(
    lambda: make_dataset(training=True),
    steps_per_epoch=steps_per_epoch,
    epochs=epochs,
    validation_data=lambda: make_dataset(training=False),
    validation_steps=N_validation_batches, callbacks=callbacks)
&lt;/denchmark-code&gt;

I came to the conclusion that I was asking too much from tf.contrib.tpu.keras_to_tpu_model and gave up.  I would be happy to share a working version of this if someone would be interested in helping me debug.
		</comment>
		<comment id='39' author='kenfehling' date='2019-06-28T10:43:39Z'>
		


had the same issue when i used tf.train.AdamOptimizer.
i solved the ReduceLR issue by passing 'adam' to model.compile
model.compile(optimizer='adam',loss='mse') 

This worked. Thanks!

I don't think solves the original problem, which was about using tf.train.AdamOptimizer. Last I checked, tf.train optimizers were necessary to get TPU training to really work.
Unless the problem of using ReduceLR w/ tf.train optimizers has been fixed or the issue w/ TPU training using Keras optimizers has been fixed, I would still consider this an open issue.

It is true what you are saying. The solution worked for my particular use case but I would still consider that as a hack rather than a solution. Tensorflow provides so many versions of the same optimizer with slightly different parameter names that choosing one over the other causes certain problems in particular use cases. The issue should remain open. Please do correct me if I'm wrong, though.
		</comment>
		<comment id='40' author='kenfehling' date='2019-06-28T14:38:01Z'>
		Using Keras with tensorflow optimizers wasn't the best experience. Instead we merged tensorflow optimizers and keras optimizers, and they're all under keras optimizers now. So by doing model.compile(optimizer='adam') it is by default using the new keras optimizers. This is the right way to do it, not a hack. If you want to specify your own hyperparameters, you can create one by doing tf.keras.optimizers.Adam starting in 1.14.
		</comment>
		<comment id='41' author='kenfehling' date='2019-06-28T20:32:56Z'>
		&lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;

Thank you for the clarification. This helped a lot. So will the other legacy versions of Tensorflow optimizers be deprecated in future and keras optimizers be used or does Tensorflow 1.x API still use it?
		</comment>
		<comment id='42' author='kenfehling' date='2019-09-20T17:22:33Z'>
		
I think this may solve the problem horovod/horovod#42.
This is my code:
learning_rate = K.variable(0.001)
adamW = tf.contrib.opt.AdamWOptimizer(weight_decay=1e-4,
                                      learning_rate=learning_rate, 
                                      beta1=0.9, beta2=0.999, 
                                      epsilon=1e-08, name='AdamW')
opt= TFOptimizer(adamW)
opt.lr = learning_rate
model.compile(optimizer=opt, loss=ssd_loss.compute_loss)


When we need do learning rate decay or ReduceLROnPlateau, we shouldn't use like this. Use the optimizers of tf.keras, not tf.train or tf.contrib.opt.
ReduceLROnPlateau indeed changes the value opt.lr, but it's not the true learning rate of the optimizer, when we see the source code, we can find the true learning rate is opt._lr, so the ReduceLROnPlateau actually doesn't work.
		</comment>
		<comment id='43' author='kenfehling' date='2019-09-20T18:15:23Z'>
		For all -- please use 1.14 and migrate to tf.keras.optimizers so that this wouldn't become an issue for you.
		</comment>
		<comment id='44' author='kenfehling' date='2019-09-20T18:29:38Z'>
		
For all -- please use 1.14 and migrate to tf.keras.optimizers so that this wouldn't become an issue for you.

But some optimizers like AdamW are not implemented in tf.keras.optimizers
		</comment>
		<comment id='45' author='kenfehling' date='2019-09-20T18:58:27Z'>
		

For all -- please use 1.14 and migrate to tf.keras.optimizers so that this wouldn't become an issue for you.

But some optimizers like AdamW are not implemented in tf.keras.optimizers

If this is strongly desired, file a request and either we can make this into tf core or tf addon.
		</comment>
		<comment id='46' author='kenfehling' date='2020-03-20T22:18:18Z'>
		
had the same issue when i used tf.train.AdamOptimizer.
i solved the ReduceLR issue by passing 'adam' to model.compile
model.compile(optimizer='adam',loss='mse') 

yes this work thnaks
		</comment>
		<comment id='47' author='kenfehling' date='2020-03-20T22:18:30Z'>
		yes this works
		</comment>
		<comment id='48' author='kenfehling' date='2020-10-27T06:24:00Z'>
		you should use keras.optimizer not tensorflow.keras.optimizer
		</comment>
	</comments>
</bug>