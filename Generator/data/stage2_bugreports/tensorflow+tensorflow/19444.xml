<bug id='19444' author='ronjet' open_date='2018-05-21T22:14:06Z' closed_time='2018-09-18T19:01:28Z'>
	<summary>Cannot use per process memory fraction in tensorflow distributed</summary>
	<description>
Have I written custom code (as opposed to using a stock example script provided in TensorFlow):

OS Platform and Distribution :  Linux Ubuntu 16.04 (Deep Learning AMI (Ubuntu) Version 8.0 (ami-dff741a0) on AWS)
TensorFlow installed from binary:
TensorFlow version : v1.7.0-13-g99322a92bf 1.7.0
Python version : Python 3.6.4 :: Anaconda, Inc.
CUDA/cuDNN version : 9.0, V9.0.176
GPU model and memory : Tesla K80, 12gb
Bazel version : N/A
Exact command to reproduce:

&lt;denchmark-code&gt;  train_dataset = tf.data.TFRecordDataset([train_file])
  train_dataset = train_dataset.map(lambda x : _parse_function(x), num_parallel_calls = 4)
  train_dataset = train_dataset.batch(train_batch_size)
  #train_dataset = train_dataset.prefetch(buffer_size = 1000)
  train_dataset = train_dataset.repeat()
  train_iterator = train_dataset.make_initializable_iterator()
  next_train_element = train_iterator.get_next()
![screen shot 2018-05-21 at 3 19 59 pm](https://user-images.githubusercontent.com/13630072/40332854-82ac9076-5d0a-11e8-8c81-8ef0f982f41f.png)
![screen shot 2018-05-21 at 3 19 59 pm](https://user-images.githubusercontent.com/13630072/40332868-9198bd8a-5d0a-11e8-9f3c-f40b93687371.png)

cluster = tf.train.ClusterSpec({
            "ps": FLAGS.ps_hosts.split(","),
            "worker": FLAGS.worker_hosts.split(",")})

  server = tf.train.Server(cluster, 
                           job_name = FLAGS.job_name,
                           task_index = FLAGS.task_index)

if FLAGS.job_name == "ps":
    server.join()
  elif FLAGS.job_name == "worker":
    with tf.device(tf.train.replica_device_setter(
                       worker_device="/job:worker/task:%d" % FLAGS.task_index,
                       cluster=cluster)):

      users_input = tf.placeholder(tf.float32, shape = [None, items_size], name = 'User_input')
      items_input = tf.placeholder(tf.float32, shape = [None, users_size], name = 'Item_input')
      ratings_input = tf.placeholder(tf.float32, shape = [None, 1], name = 'Rating_input')
&lt;/denchmark-code&gt;

... model defn
&lt;denchmark-code&gt;init_op = tf.initialize_all_variables()
hooks=[tf.train.StopAtStepHook(last_step=10000)]
sess_config = tf.ConfigProto(allow_soft_placement = True, log_device_placement = True)
sess_config.gpu_options.allow_growth = True
sess_config.gpu_options.per_process_gpu_memory_fraction = 0.33

with tf.train.MonitoredTrainingSession(master=server.target,
                                           is_chief=(FLAGS.task_index == 0),
                                           checkpoint_dir=FLAGS.log_dir,
                                           hooks=hooks,
                                           save_summaries_steps = 10,
                                           config = sess_config) as sess:
&lt;/denchmark-code&gt;

... model training
....
Processes completely utilize the available gpu memory. I further reduced the batch size to 20, input is sensor input values, not images.
&lt;denchmark-link:https://user-images.githubusercontent.com/13630072/40332635-8fa94fd6-5d09-11e8-9bcc-9b0f70946387.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/13630072/40332884-9fa526f2-5d0a-11e8-8a19-12870cd01d2d.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='ronjet' date='2018-05-26T18:42:15Z'>
		Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.
OS Platform and Distribution
Bazel version
		</comment>
		<comment id='2' author='ronjet' date='2018-06-13T22:24:43Z'>
		I did update the issue with the details. And yes, this is still an issue.
		</comment>
		<comment id='3' author='ronjet' date='2018-07-20T19:47:37Z'>
		I'm not clear what the problem is. Is there an error created? It is normal for tensorflow to use all the memory on the gpu. That allows it to use arena allocation which is more efficient for execution time.
		</comment>
		<comment id='4' author='ronjet' date='2018-09-18T19:03:45Z'>
		We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!
		</comment>
	</comments>
</bug>