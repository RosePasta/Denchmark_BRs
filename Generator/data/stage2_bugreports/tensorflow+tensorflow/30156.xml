<bug id='30156' author='devstein' open_date='2019-06-25T23:13:42Z' closed_time='2019-07-03T23:03:11Z'>
	<summary>Memory Leak with tf.Tensor.getitem in tf.data.Dataset</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX and Linux
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): pipenv install --pre tensorflow==2.0.0-beta1
TensorFlow version (use command below): 2.0.0-beta1
Python version: 3.6.8


As described in &lt;denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/slice&gt;tf.slice&lt;/denchmark-link&gt;
, I tried to use  to perform a slice in a more pythonic way, but ran into memory leaks.
def map_to_xy_dataset(csv_dataset, params):
    window_size = params[WINDOW_SIZE_KEY]
    window_shift = params[WINDOW_SHIFT_KEY]
    n_workers = tf.data.experimental.AUTOTUNE

    # MEMORY LEAK IN THIS FUNCTION
    def split_xy(window):
        # For X, select all but the last value and flatten
        range_x = tf.shape(window)[1] - 1
        
        # CAUSES MEMORY LEAK 
        x = tf.reshape(window[:, 0:range_x], [-1])


        # Select a single Y value
        # CAUSES MEMORY LEAK
        y = window[0, -1]

        return x, y

    # Turn the csv dataset row x col tensor
    row_dataset = csv_dataset.map(lambda *x: tf.reshape(x, [len(x)]))

    windowed_dataset = row_dataset.window(
        size=window_size, shift=window_shift,
        drop_remainder=True).flat_map(lambda x: x.batch(window_size))

    xy_dataset = windowed_dataset.map(split_xy, num_parallel_calls=n_workers)

    return xy_dataset
I have  tried to reproduce this behavior with  &lt;denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/slice&gt;tf.slice&lt;/denchmark-link&gt;
. I will update this issue if I do.
Describe the expected behavior
No memory leaks.
Code to reproduce the issue
See above.
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Initial related comment: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/19049#issuecomment-505585437&gt;#19049 (comment)&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='devstein' date='2019-06-26T09:25:17Z'>
		Will it be possible to provide a full code snippet that can replicate the issue.It will indeed help us to move faster. Thanks!
		</comment>
		<comment id='2' author='devstein' date='2019-06-27T02:04:19Z'>
		&lt;denchmark-link:https://github.com/achandraa&gt;@achandraa&lt;/denchmark-link&gt;
  Thanks for looking into this! Does this code snippet suffice? If not, let me know what other details you need.
import tensorflow as tf


def map_to_xy_dataset(csv_dataset, params):
    window_size = 3
    window_shift = 10
    n_workers = tf.data.experimental.AUTOTUNE

    # MEMORY LEAK IN THIS FUNCTION
    def split_xy(window):
        # For X, select all but the last value and flatten
        range_x = tf.shape(window)[1] - 1
        
        # CAUSES MEMORY LEAK 
        x = tf.reshape(window[:, 0:range_x], [-1])


        # Select a single Y value
        # CAUSES MEMORY LEAK
        y = window[0, -1]

        return x, y

    # Turn the csv dataset row x col tensor
    row_dataset = csv_dataset.map(lambda *x: tf.reshape(x, [len(x)]))

    windowed_dataset = row_dataset.window(
        size=window_size, shift=window_shift,
        drop_remainder=True).flat_map(lambda x: x.batch(window_size))

    xy_dataset = windowed_dataset.map(split_xy, num_parallel_calls=n_workers)

    return xy_dataset

def generate_dataset():
    filenames_dataset = tf.data.Dataset.list_files(data_glob)

    dataset = filenames_dataset.flat_map(map_file_to_xy_dataset)

    return dataset


dataset = generate_dataset()

model = ...

model.fit(dataset, ...)
		</comment>
		<comment id='3' author='devstein' date='2019-07-01T10:24:51Z'>
		&lt;denchmark-link:https://github.com/devstein&gt;@devstein&lt;/denchmark-link&gt;
 I have tried to reproduce the issue with the above code but Looks like some of the entities like data_glob, map_file_to_xy_dataset are not defined. Can you help us to reproduce the issue. Thanks!
		</comment>
		<comment id='4' author='devstein' date='2019-07-01T15:38:51Z'>
		&lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
 Here is a more detailed example:
import tensorflow as tf

def map_file_to_xy_dataset(filename):
    window_size = 3
    window_shift = 10
    n_workers = tf.data.experimental.AUTOTUNE

    # Generate a dataset from the filename
    csv_dataset = tf.data.experimental.CsvDataset("Iris.csv",["float32","float32","float32","float32","float32"])

    # Cache at unique file location (i.e filename)
    cached_dataset = csv_dataset.cache("Iris.csv")

    # MEMORY LEAK IN THIS FUNCTION
    def split_xy(window):
        # For X, select all but the last value and flatten
        range_x = tf.shape(window)[1] - 1
        
        # CAUSES MEMORY LEAK 
        x = tf.reshape(window[:, 0:range_x], [-1])


        # Select a single Y value
        # CAUSES MEMORY LEAK
        y = window[0, -1]

        return x, y

    # Turn the csv dataset row x col tensor
    row_dataset = cached_dataset.map(lambda *x: tf.reshape(x, [len(x)]))

    windowed_dataset = row_dataset.window(
        size=window_size, shift=window_shift,
        drop_remainder=True).flat_map(lambda x: x.batch(window_size))

    xy_dataset = windowed_dataset.map(split_xy, num_parallel_calls=n_workers)

    return xy_dataset


def generate_dataset():
    filenames_dataset = tf.data.Dataset.list_files("Iris.csv")

    dataset = filenames_dataset.interleave(map_file_to_xy_dataset, cycle_length=10, block_length=10, num_parallel_calls=tf.data.experimental.AUTOTUNE)

    return dataset

		</comment>
		<comment id='5' author='devstein' date='2019-07-01T18:16:18Z'>
		&lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
 After more experimenting, I don't think  is the root of the issue. This &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/29697&gt;issue&lt;/denchmark-link&gt;
 describes the same behavior but is only caching.
Also, this issue is only persistent during the first epoch. Does tf.data.Dataset.cache cache to memory before it writes to disk?
Edit
I've narrowed it down further. This issue comes from using interleave with a large cycle_length (i.e total number of files) and a small block_length (i.e block_length = 1).
&lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
 How does this relate to caching? Is there an internal memory buffer for each cached dataset while the lock file exists?
		</comment>
		<comment id='6' author='devstein' date='2019-07-02T05:15:37Z'>
		&lt;denchmark-link:https://github.com/devstein&gt;@devstein&lt;/denchmark-link&gt;
 I tried reproducing the issue with the above provided code but it didn't show any error. Please provide us complete code which replicates the reported issue. Thanks!
		</comment>
		<comment id='7' author='devstein' date='2019-07-02T15:30:54Z'>
		&lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
 It causes memory usage climbs, see my last comment. There is no error produced.
		</comment>
		<comment id='8' author='devstein' date='2019-07-03T23:03:11Z'>
		&lt;denchmark-link:https://github.com/devstein&gt;@devstein&lt;/denchmark-link&gt;
 the effect of  with  is that a multiple of  instances of the input pipeline returned by  will be evaluated in parallel. If each instances maintains internal buffers, the memory usage will be multiplied by the  factor.
In your case, it looks like your input pipeline is using window and map with num_parallel_calls. Both of these transformations will maintain an internal buffer.
In other words, the behavior you are experiencing is not a memory leak (at least not based on the evidence you have provided), but high memory usage.
		</comment>
		<comment id='9' author='devstein' date='2019-07-03T23:03:13Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=30156&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=30156&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='devstein' date='2019-07-03T23:12:55Z'>
		&lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
 That makes sense, but why is this only prevalent when using ? Memory usage is significantly lower when using the exact same parameters but not caching each  to disk?
		</comment>
		<comment id='11' author='devstein' date='2019-07-03T23:23:01Z'>
		IIRC, the implementation of cache would uses an internal buffer for batching writes of a tensor. So that would explain why is the problem exacerbated by using cache in your input pipeline.
		</comment>
	</comments>
</bug>