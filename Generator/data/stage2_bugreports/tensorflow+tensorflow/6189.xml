<bug id='6189' author='nasimrahaman' open_date='2016-12-08T10:45:38Z' closed_time='2017-12-21T07:09:52Z'>
	<summary>Inconsistent behavior for tf.variable_scope</summary>
	<description>
&lt;denchmark-h:h4&gt;Problem:&lt;/denchmark-h&gt;

Tensorflow doesn't place ops (e.g. mul) in pre-existing variable scopes (and automatically creates a new scope instead).
&lt;denchmark-h:h4&gt;Minimal Reproducible Example&lt;/denchmark-h&gt;

with tf.variable_scope('layer123'):
    v = tf.get_variable('v', [], initializer=tf.constant_initializer(42., tf.float32))
    w = v * 2
print(w.name)    # Prints 'layer123/mul:0'
However,
with tf.variable_scope('layer123'):
    v = tf.get_variable('v', [], initializer=tf.constant_initializer(42., tf.float32))

with tf.variable_scope('layer123'):
    w = v * 2

print(w.name)    # Prints 'layer123_1/mul:0'
Observe that for the latter, the op w is placed in a different variable scope, auto-named layer123_1.
I've tried the following, to the same effect:
with tf.variable_scope('layer123') as scope:
    v = tf.get_variable('v', [], initializer=tf.constant_initializer(42., tf.float32))

with tf.variable_scope(scope):
    w = v * 2

print(w.name)    # Prints 'layer123_1/mul:0'
with tf.variable_scope('layer123'):
    v = tf.get_variable('v', [], initializer=tf.constant_initializer(42., tf.float32))

with tf.variable_scope('layer123', reuse=True):
    w = v * 2

print(w.name)    # Prints 'layer123_1/mul:0'
&lt;denchmark-h:h4&gt;VersionSpec&lt;/denchmark-h&gt;

Tensorflow version: 0.11.0 (GPU)
OS: Ubuntu 14.04 (w/ CUDA 8)
	</description>
	<comments>
		<comment id='1' author='nasimrahaman' date='2016-12-08T18:11:04Z'>
		Sorry about this confusion. Your third example at least seems to contradict the&lt;denchmark-link:https://www.tensorflow.org/versions/r0.12/how_tos/variable_scope/index.html&gt; variable_scope documentation&lt;/denchmark-link&gt;
: "When opening a variable scope using a captured object instead of a string, we do not alter the current name scope for ops."
&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 can you give some guidance?
		</comment>
		<comment id='2' author='nasimrahaman' date='2016-12-08T18:22:55Z'>
		No problem! But if this is purely a documentation issue, I'd really appreciate any pointers to a workaround (i.e. to guarantee that w.name == 'layer123/mul'). Thank you!
		</comment>
		<comment id='3' author='nasimrahaman' date='2016-12-08T18:26:49Z'>
		I think that &lt;denchmark-link:https://www.tensorflow.org/versions/r0.12/api_docs/python/framework.html#name_scope&gt;tf.name_scope&lt;/denchmark-link&gt;
 is the right thing to use for naming ops rather than variables, but since I know this has changed over time I defer to &lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 for a definitive answer.
		</comment>
		<comment id='4' author='nasimrahaman' date='2016-12-08T18:34:59Z'>
		I believe variable and name scopes are exclusive.  variable_scope and
name_scope change their own scope hierarchies independently.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Thu, Dec 8, 2016 at 10:27 AM, Michael Isard ***@***.***&gt; wrote:
 I think that tf.name_scope
 &lt;https://www.tensorflow.org/versions/r0.12/api_docs/python/framework.html#name_scope&gt;
 is the right thing to use for naming ops rather than variables, but since I
 know this has changed over time I defer to @ebrevdo
 &lt;https://github.com/ebrevdo&gt; for a definitive answer.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#6189 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABtim3asKUKZVRqv6gHwD_W-xGm8dl6Pks5rGEv5gaJpZM4LHrZ6&gt;
 .



		</comment>
		<comment id='5' author='nasimrahaman' date='2016-12-08T18:45:17Z'>
		I ran another test (on &lt;denchmark-link:https://github.com/michaelisard&gt;@michaelisard&lt;/denchmark-link&gt;
 's advice):
with tf.name_scope('layer123') as scope:
    v = tf.Variable(3., name='v')
    
with tf.name_scope('layer123'):
    w = v * 2

print(w.name)    # still prints 'layer123_1/mul:0'
Replacing the first name_scope with variable_scope and Variable with get_variable did not change anything. However, I've found this works:
with tf.name_scope('layer123') as scope:
    v = tf.Variable(3., name='v')

with tf.name_scope(scope):
    w = v * 2

print(w.name)    # prints 'layer123/mul:0' like it should
This is quite unsatisfactory, for obvious reasons (e.g. get_variable functionality is rendered useless, encapsulation is broken, etc.).
		</comment>
		<comment id='6' author='nasimrahaman' date='2016-12-08T19:59:13Z'>
		w is not a variable, it's a Tensor.  w.name gives the Tensor's name, but
v.name returns the variable's name as you would see in the checkpoint.

independently, you can do:

with variable_scope(...) as varscope:
   create_variables

with variable_scope(varscope, reuse=True):
   reuse variables via get_variable
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Thu, Dec 8, 2016 at 10:45 AM, Nasim Rahaman ***@***.***&gt; wrote:
 I ran another test (on @michaelisard &lt;https://github.com/michaelisard&gt; 's
 advice):

 with tf.name_scope('layer123') as scope:
     v = tf.Variable(3., name='v')
     with tf.name_scope('layer123'):
     w = v * 2
 print(w.name)    # still prints 'layer123_1/mul:0'

 Replacing the first name_scope with variable_scope and Variable with
 get_variable did not change anything. However, I've found this works:

 with tf.name_scope('layer123') as scope:
     v = tf.Variable(3., name='v')
 with tf.name_scope(scope):
     w = v * 2
 print(w.name)    # prints 'layer123/mul:0' like it should

 This is quite unsatisfactory, for obvious reasons (e.g. get_variable
 functionality is rendered useless, etc.).

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#6189 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABtimxopoVQzOCx_S29w4DO_llkqZJMnks5rGFBQgaJpZM4LHrZ6&gt;
 .



		</comment>
		<comment id='7' author='nasimrahaman' date='2016-12-08T20:33:47Z'>
		&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 I understand that w is a  resulting from the  op, and that it's not possible to access it via something like . I just need the  op to reside in the same "name-group" as the variable  (such that ), for prettier Tensorboard visualization. Certainly, this is possible when  is defined immediately after  without exiting the first , as shown in the very first example. I was looking for a way for this to work after I had once exited the first .
Also, I've tried the following, which also doesn't work:
with tf.variable_scope('layer123') as varscope:
    v = tf.get_variable('v', [], initializer=tf.constant_initializer(3., tf.float32))

with tf.variable_scope(varscope, reuse=True):
    w = v * 2

print(w.name)    # Prints 'layer123_1/mul:0'
EDIT/CLARIFICATION: I understand that your reply:

w is not a variable, it's a Tensor.  w.name gives the Tensor's name, but
v.name returns the variable's name as you would see in the checkpoint.

was a reply to my statement:

This is quite unsatisfactory, for obvious reasons (e.g. get_variable functionality is rendered useless, encapsulation is broken, etc.).

I was referring to having to use tf.name_scope instead of tf.variable_scope to have consistent naming behaviour, because the former is ignored by tf.get_variable. :)
		</comment>
		<comment id='8' author='nasimrahaman' date='2017-02-20T02:27:11Z'>
		Here's a use-case for re-opening variable scopes without renaming.
Consider a OOP design where a class declares a couple of TensorFlow variables and then provides multiple methods on those variables.
class Foo(object):
    def __init__(self):
        self.x = tf.get_variable('x', [], initializer=tf.constant(0.0))

    def meth(self, foo):
        z = tf.multiply(foo, 3.14, name='z')
        self.x.assign(z)
The easiest to visualize (in TensorBoard) pattern would to be have one variable scope per instance of class and call of method.
f0 = Foo()      # Foo/__init__/x
f1 = Foo()      # Foo_1/__init__/x
f0.meth(1.0)    # Foo/meth/*
f0.meth(2.0)    # Foo/meth_1/*
f1.meth(3.0)    # Foo_1/meth/*
The way that I thought of to implement this is to create a VariableScope on each instance, and then reopen it for each method call before creating another variable scope for the method. The reopening of the variable scope for each method call without changing the naming is currently not possible, so this scheme doesn't work.
		</comment>
		<comment id='9' author='nasimrahaman' date='2017-02-20T03:12:40Z'>
		Thanks &lt;denchmark-link:https://github.com/ppwwyyxx&gt;@ppwwyyxx&lt;/denchmark-link&gt;
 . The main issue here that &lt;denchmark-link:https://github.com/nasimrahaman&gt;@nasimrahaman&lt;/denchmark-link&gt;
 and I are looking for is the naming. Additionally, there's no need for the variable scope just to access the variables as the variables themselves could be saved on the instance (no  call).
For my previous example, the suggested scheme would currently give the names:
f0 = Foo()      # Foo/__init__/x
f1 = Foo()      # Foo_1/__init__/x
f0.meth(1.0)    # Foo_2/meth/*
f0.meth(2.0)    # Foo_3/meth/*
f1.meth(3.0)    # Foo_4/meth/*
which doesn't help at all with distinguishing calls to different instances of Foo.
		</comment>
		<comment id='10' author='nasimrahaman' date='2017-02-20T03:19:46Z'>
		&lt;denchmark-link:https://github.com/eamartin&gt;@eamartin&lt;/denchmark-link&gt;
 My bad earlier. I just found you can do something like this: (from &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/6007&gt;#6007&lt;/denchmark-link&gt;
)
class Foo(object):
    def __init__(self):
        with tf.variable_scope(None, 'Foo'):
            self.sc = tf.get_variable_scope()
            self.x = tf.get_variable('x', initializer=0.0)
            print(self.x)

    def meth(self, foo):
        with tf.name_scope(self.sc.original_name_scope):
            z = tf.multiply(foo, 3.14, name='z')
            print(z)

f0 = Foo()      # Foo/x
f1 = Foo()      # Foo_1/x
f0.meth(1.0)    # Foo/z
f0.meth(2.0)    # Foo/z_1
f1.meth(3.0)    # Foo_1/z
Maybe this will help?
		</comment>
		<comment id='11' author='nasimrahaman' date='2017-02-20T04:39:35Z'>
		Thanks &lt;denchmark-link:https://github.com/ppwwyyxx&gt;@ppwwyyxx&lt;/denchmark-link&gt;
 , this works for my OOP use-case. I wasn't aware of , this could use some docs!
One tricky case with this pattern: If you want to re-open the scope and create new variables, how do you give the appropriate names to the created ops in the re-opened scope? Here's the workaround:
First attempt:
import tensorflow as tf

with tf.variable_scope('foo') as scope:
    x = tf.get_variable('x', initializer=1.0)
    x2 = tf.multiply(x, 2, name='x2')

with tf.variable_scope(scope):
    y = tf.get_variable('y', initializer=2.0)
    y2 = tf.multiply(y, 2, name='y2')

print(x, x2, y, y2)
outputs Tensor("foo/x/read:0", shape=(), dtype=float32) Tensor("foo/x2:0", shape=(), dtype=float32) Tensor("foo/y/read:0", shape=(), dtype=float32) Tensor("foo_1/y2:0", shape=(), dtype=float32). This is wrong for foo_1/y2.
Next (ugly approach): Add a with tf.name_scope(scope.original_name_scope): block inside of the with tf.variable_scope(scope): block. This leads to correct output Tensor("foo/x/read:0", shape=(), dtype=float32) Tensor("foo/x2:0", shape=(), dtype=float32) Tensor("foo/y/read:0", shape=(), dtype=float32) Tensor("foo/y2:0", shape=(), dtype=float32)
		</comment>
		<comment id='12' author='nasimrahaman' date='2017-02-20T04:56:18Z'>
		&lt;denchmark-link:https://github.com/eamartin&gt;@eamartin&lt;/denchmark-link&gt;
 Yes that's a bit tricky. I hope tensorflow can change the default behavior, i.e. when you reenter a variable scope, you should expect to automatically reenter its name scope as well instead of creating a new one.
		</comment>
		<comment id='13' author='nasimrahaman' date='2017-05-12T05:05:26Z'>
		Hi everyone. I am having this problem too. Is there any method to have a consistent variable scope name and the variable names now? I am worry about that Foo/x and Foo_1/x are two independent variables (please correct me if I am wrong). Thank you.
		</comment>
		<comment id='14' author='nasimrahaman' date='2017-12-20T19:29:10Z'>
		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
		</comment>
		<comment id='15' author='nasimrahaman' date='2017-12-20T19:54:36Z'>
		&lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/14390&gt;#14390&lt;/denchmark-link&gt;
 now supports opening a variable scope without a new name scope already. I think this issue is resolved.
		</comment>
		<comment id='16' author='nasimrahaman' date='2017-12-21T07:09:52Z'>
		Great!
		</comment>
		<comment id='17' author='nasimrahaman' date='2018-01-10T02:04:11Z'>
		Hi &lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ppwwyyxx&gt;@ppwwyyxx&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/michaelisard&gt;@michaelisard&lt;/denchmark-link&gt;
 , I think I am facing a similar problem but not sure about it. My TensorFlow version is 1.2.1.
I am trying to finetune &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/resnet_v1.py&gt;resnet_50&lt;/denchmark-link&gt;
 from  package on my own data.
To have the training and validation cycles together I use,
&lt;denchmark-code&gt;        with slim.arg_scope(resnet_arg_scope()):
            self.network_logits,_     = resnet_v1_50(inputs=inputs) 
            self.network_logits_val,_ = resnet_v1_50(inputs=inputs, reuse=True, is_training=False) ## Define a new function for testing but reuse the same model variables
&lt;/denchmark-code&gt;

Note, that my intension is to reuse the variables in the trained model for validation. However, on my TensorBoard I get two seperate resnet_50_v1 and  resnet_50_v1_1 blocks.
&lt;denchmark-link:https://user-images.githubusercontent.com/34502974/34751909-4907825e-f603-11e7-9282-5d9be078ee26.png&gt;&lt;/denchmark-link&gt;

However, when I zoomed in to the block resnet_50_v1_1 (The block used in the validation cycle), I only see a set of operators(i.e. all the variables are passed from resnet_50_v1).
&lt;denchmark-link:https://user-images.githubusercontent.com/34502974/34752040-17de5ecc-f604-11e7-8a17-3b29039ee4bd.jpg&gt;&lt;/denchmark-link&gt;

This is how a similar block looks inside the resnet_50_v1 (The block used in the training cycle),
&lt;denchmark-link:https://user-images.githubusercontent.com/34502974/34752279-5a4aff58-f605-11e7-963c-d285628de0b8.jpg&gt;&lt;/denchmark-link&gt;

I think this sperate blocks appear due to the discussed name_scope issue . If that is the case is my code technically safe? To me this is just a problem of easthetic appearance on tensorboard. Please correct me if I am wrong.
Could you please help me clarify why I am getting two distinct blocks resnet_50_v1 and resnet_50_v1_1? Is my code technically safe from what it appears? Thanks in Advance!
		</comment>
	</comments>
</bug>