<bug id='35317' author='theGOTOguy' open_date='2019-12-20T22:11:02Z' closed_time='2020-06-10T14:46:32Z'>
	<summary>Slicing tensor within a keras.utils.Sequence with multiprocessing=True hangs fit_generator</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04
TensorFlow installed from (source or binary):  binary
TensorFlow version (use command below):  2.0.0
Python version:  3.6.8
CUDA/cuDNN version:  10.2 / 7.6.5.32-1+cuda10.2
GPU model and memory:  NVidia Titan RTX 24218 MiB

Describe the current behavior
When I attempt to slice a tensor inside a keras.util.Sequence from model.fit_generator with multiprocessing=True, TensorFlow hangs forever without reporting any error or using any CPU or GPU cycles.  It works as expected when multiprocessing=False.
Describe the expected behavior
TensorFlow should correctly fit the model just as it does with multiprocessing=False
Code to reproduce the issue
In order to reproduce, substitute my_jpeg for some jpeg on your computer (hopefully with dimension greater than 224px).  Note that if you set use_multiprocessing=False in the example below, then this will correctly train the model.
&lt;denchmark-code&gt;from __future__ import absolute_import, division, print_function, unicode_literals

import tensorflow as tf
from tensorflow import keras

# In order to reproduce, just use whatever random JPEG you have handy here.
# It should be larger than my_crop in the x and y dimension.
my_jpeg = "/home/ben/my_jpeg.jpg"
my_crop = 224

# Generates a single crop for TensorFlow.
class DataGenerator(keras.utils.Sequence):
  def __init__(
      self,
      image_location,
      crop_size=224):
    self._image_location = image_location
    self._crop_size = crop_size

  # Just one single batch will be returned, of just one single image.
  def __len__(self):
    return 1

  # Generate one batch of data.
  def __getitem__(self, index):
    # Where the tensors will be stored.
    X = []
    y = [1]

    # Read it.
    image = tf.io.read_file(self._image_location)

    # Load it.
    image = tf.image.decode_jpeg(image, channels=3)

    assert image.shape[2] == 3  # MUST be RGB.
    height = image.shape[0]
    width = image.shape[1]

    # Just take a trivial crop of the image.
    # This is the offending line operation which hangs forever.
    image = image[0:self._crop_size, 0:self._crop_size, :]

    # This line is equivalent to above, and it also hangs with multiprocessing enabled.
    # image = tf.slice(image, [0, 0, 0], [self._crop_size, self._crop_size, 3])

    X.append(tf.dtypes.cast(image, tf.float32))

    # Tensors are not generally assignable, but we can create them from a number of existing ones.
    X = tf.stack(X)
    y = tf.stack(y)

    # Preprocess it.
    X /= 255.0  # Normalize to [0, 1] range.

    return X, y

generator = DataGenerator(my_jpeg, my_crop)

model = tf.keras.applications.ResNet50(input_shape=(my_crop, my_crop, 3))

model.compile(loss='mse')

# use_multiprocessing=False works.
# use_multiprocessing=True hangs.
model.fit_generator(generator, use_multiprocessing=True, workers=2)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='theGOTOguy' date='2019-12-24T08:03:27Z'>
		I could reproduce the issue with Tf 2.0.
Please take a look at the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/92da8cad7821d73e50e8940f0592c158/untitled318.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='theGOTOguy' date='2020-02-09T06:45:53Z'>
		I noticed when testing this same code with TensorFlow 2.1 instead of 2.0, a helpful warning is generated:
WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.
This warning is absolutely correct.  You can probably close this bug, since this is a known issue and as of TF 2.1 the user is properly warned.
		</comment>
		<comment id='3' author='theGOTOguy' date='2020-06-10T14:36:14Z'>
		&lt;denchmark-link:https://github.com/theGOTOguy&gt;@theGOTOguy&lt;/denchmark-link&gt;

Please,let us know is this still an issue?Please, close this thread if you feel your issue was resolved.Thanks!
		</comment>
		<comment id='4' author='theGOTOguy' date='2020-06-10T14:46:33Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35317&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35317&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>