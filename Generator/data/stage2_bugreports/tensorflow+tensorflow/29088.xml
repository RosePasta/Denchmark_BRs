<bug id='29088' author='thisismygitrepo' open_date='2019-05-28T19:45:01Z' closed_time='2019-07-12T21:41:06Z'>
	<summary>TF 2.0 Beta: custom metric example from documentation is wrong</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):
TensorFlow version (use command below):
Python version:
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

I implemented the custom metric shown in this page (CatgoricalTruePositives) &lt;denchmark-link:https://www.tensorflow.org/alpha/guide/keras/training_and_evaluation&gt;https://www.tensorflow.org/alpha/guide/keras/training_and_evaluation&lt;/denchmark-link&gt;

I think it is full of bugs, it has some comments saying (#TODO: fix this). Any way, here what's particularly wrong about it. The accuracy of the NN approaches 99%, yet, this metric says:
binary_true_positives: 8459.0000
(as shown on the website too). If number of samples in MNIST is 50,000, then at least 45k of them should be true positives.  It is unstable. I messed with it once and I got 49k true positive (which makes total sense). Then I reran it and it returned to 8k.
Describe the expected behavior
The results are shown on the website. For a low loss of 0.03, the true positives should be close to 50k. However they're shown to be 8k only.
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='thisismygitrepo' date='2019-05-29T13:27:21Z'>
		Tried to execute the code and getting loss as 0.066 and categorical_true_positive : ~7863.
		</comment>
		<comment id='2' author='thisismygitrepo' date='2019-05-29T19:15:46Z'>
		Hi. Yes, I got the same thing. Now, if you add one more metric (accuracy) to your fit method, you will notice that the 0.06 loss you got actually correspond to very high accuracy (above 90%), which means that true positives should be well above 40,000. Not 7863. Please notice that we have 50k examples. The metric is way off base.
I suspect that we need to add axis=1 argument to tf.argmax method. (Not sure)
		</comment>
		<comment id='3' author='thisismygitrepo' date='2019-06-12T08:37:14Z'>
		Hi. I have some great news for you. I solved it.
Here what's wrong with your docs:
In the update_state method, you should have this:

  y_pred = tf.reshape(tf.argmax(y_pred, axis=1), shape=(-1, 1))


Currently it is:

    y_pred = tf.argmax(y_pred)


With my fix, the categorical true positives will be close to 50k which makes total sense
50000/50000 [==============================] - 1s 29us/sample - loss: 0.0905 - categorical_true_positives: 48647.0000 - accuracy: 0.9729
Because the accuracy is close to 100%.
Explanation: your argmax should act along axis 1 not axis 0 which is the batch axis. It was returning nonsensical stuff like 49 and 64 which are indices of batch datapoints. Second thing, you need to reshape the result to match the shape of y_pred which is what I did.
		</comment>
		<comment id='4' author='thisismygitrepo' date='2019-06-12T18:00:28Z'>
		@kernelizd Thank you for looking into this. Do you want to send a PR updating the doc?
		</comment>
		<comment id='5' author='thisismygitrepo' date='2019-06-13T03:11:07Z'>
		&lt;denchmark-link:https://github.com/pavithrasv&gt;@pavithrasv&lt;/denchmark-link&gt;

Sure. Will do.
		</comment>
		<comment id='6' author='thisismygitrepo' date='2019-06-14T06:50:01Z'>
		&lt;denchmark-link:https://github.com/pavithrasv&gt;@pavithrasv&lt;/denchmark-link&gt;

Hi, I'm not an expert in github. I've just learned the basics. I made the pull request &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/674&gt;#674&lt;/denchmark-link&gt;

I hope I did it correctly. Let me know.
		</comment>
		<comment id='7' author='thisismygitrepo' date='2019-06-14T17:04:16Z'>
		@kernelizd Thank you for creating a PR, the link you have provided is pointing to a different PR, can you share the entire url may be?
		</comment>
		<comment id='8' author='thisismygitrepo' date='2019-06-14T18:24:04Z'>
		&lt;denchmark-link:https://github.com/pavithrasv&gt;@pavithrasv&lt;/denchmark-link&gt;

I'm sorry. Here is the url you asked for:
&lt;denchmark-link:https://github.com/tensorflow/docs/pull/674&gt;tensorflow/docs#674&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='thisismygitrepo' date='2019-06-17T20:45:26Z'>
		Looks like the diff is more than just this line that has changed. You may need to update your PR.
		</comment>
		<comment id='10' author='thisismygitrepo' date='2019-06-17T21:09:54Z'>
		Hi &lt;denchmark-link:https://github.com/pavithrasv&gt;@pavithrasv&lt;/denchmark-link&gt;

It is exactly one line. No more.
But for some reason, Github says everything got changed. If you investigate it yourself, you see that there is no change at all.
		</comment>
		<comment id='11' author='thisismygitrepo' date='2019-06-17T21:19:21Z'>
		Seems like indentation mismatch.
		</comment>
		<comment id='12' author='thisismygitrepo' date='2019-06-17T21:44:11Z'>
		I guessed as much. Different Jupyter version?
		</comment>
		<comment id='13' author='thisismygitrepo' date='2019-07-12T21:41:05Z'>
		Fixed in PR
		</comment>
	</comments>
</bug>