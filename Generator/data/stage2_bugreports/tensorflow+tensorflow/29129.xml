<bug id='29129' author='alexv1247' open_date='2019-05-29T12:43:29Z' closed_time='2019-05-30T22:11:22Z'>
	<summary>keras lstm cell has no zero state when wrapped with a dropoutwrapper</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Microsoft Windows 10
TensorFlow installed from (source or binary): Anaconda
TensorFlow version (use command below): tensorflow 1.13.1  gpu_py37h83e5d6a_0
Python version: 3.7
CUDA/cuDNN version: 7.3.1
GPU model and memory: GTX 1060 6gb

This problem is very similar to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/25641&gt;#25641&lt;/denchmark-link&gt;
. But that issue was closed and the problem was found in a future version of tensorflow (2.0.0. alpha). This is why I opened a new issue, becuase the current stable release version has the same error.
I want to create a RNN layer with stacked lstm cells. The lstm cells are wrapped with the dropoutwrapper. Since I define no initial state, the RNN tries to get the zero state from the lstm cells. There the error occurs, saying that a lstmcell has no attribute zero_state.
&lt;denchmark-code&gt;    import tensorflow as tf
    with tf.Session() as sess:
    tensor = tf.zeros(shape=(1, 20, 5), dtype=tf.float32)
    inputs = tf.keras.layers.Input(shape=(10, 5), tensor=tensor)
    cell = tf.keras.layers.LSTMCell(10)
    cell = tf.nn.rnn_cell.DropoutWrapper(cell=cell)
    rnn = tf.keras.layers.RNN(cell=cell)(inputs)
&lt;/denchmark-code&gt;

Here is the error message:
&lt;denchmark-code&gt;      File "C:/Users/vaith/PycharmProjects/Bayesian_Neural_Nets/bayesian_lstm_main.py", line 25, in main
       rnn = tf.keras.layers.RNN(cell=cell)(inputs)
       File "C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\keras\layers \recurrent.py", line 701, in __call__
       return super(RNN, self).__call__(inputs, **kwargs)
       File "C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\base_layer.py", line 554, in __call__
       outputs = self.call(inputs, *args, **kwargs)
       File "C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\keras\layers\recurrent.py", line 759, in call
        inputs, initial_state, constants)
       File "C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\keras\layers\recurrent.py", line 863, in _process_inputs
        initial_state = self.get_initial_state(inputs)
       File "C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\keras\layers\recurrent.py", line 679, in get_initial_state
        inputs=None, batch_size=batch_size, dtype=dtype)
       File "C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\rnn_cell_impl.py", line 298, in get_initial_state
        return self.zero_state(batch_size, dtype)
       File "C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\rnn_cell_impl.py", line 1232, in zero_state
        return self._cell.zero_state(batch_size, dtype)
       AttributeError: 'LSTMCell' object has no attribute 'zero_state'
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='alexv1247' date='2019-05-30T10:33:10Z'>
		&lt;denchmark-link:https://github.com/alexv1247&gt;@alexv1247&lt;/denchmark-link&gt;
 Able to reproduce the issue on TF 1.13 GPU and TF nightly
		</comment>
		<comment id='2' author='alexv1247' date='2019-05-30T17:54:24Z'>
		The Keras cells have a dropout argument that you should use directly, rather than passing through the DropoutWrapper. See, for example, &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/keras/layers/recurrent.py#L1022&gt;https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/keras/layers/recurrent.py#L1022&lt;/denchmark-link&gt;
 . + &lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;
 for more guidance.
		</comment>
		<comment id='3' author='alexv1247' date='2019-05-30T22:11:21Z'>
		The keras RNN cell has default dropout support, which mean user probably should just directly using that. Please see API doc param "dropout" and "recurrent_dropout" for any of the RNN layer and cells.
DropoutWrapper is used mostly for legacy TF rnn cell, and some custom cells in tf.contrib.
		</comment>
		<comment id='4' author='alexv1247' date='2019-05-30T22:11:23Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=29129&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=29129&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='alexv1247' date='2019-05-30T22:36:14Z'>
		so will the dropout wrapper work with the cudnnlstm cell from tf contrib?
Sent with GitHawk
		</comment>
		<comment id='6' author='alexv1247' date='2019-05-30T22:40:59Z'>
		In TF 2.0, we merge the cudnn implementation into the normal LSTM/GRU keras layer, so you don't have to use the contrib version anymore.
		</comment>
	</comments>
</bug>