<bug id='27499' author='ikamensh' open_date='2019-04-04T09:11:10Z' closed_time='2019-06-28T18:30:12Z'>
	<summary>optimizer_v2.apply_gradients() - surprising behavior, not explicitly documented</summary>
	<description>
System information

TensorFlow version: 2.0.0-aplha0
Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/optimizers/Optimizer#apply_gradients

I was implementing a DD Policy Gradient for reinforcement learning, and had a bug where my agent would minimize the reward, instead of maximizing it. It turned out optimizer.apply_gradients() follows negative of the gradients I give to it. This is a surprising behavior when using this function separately, outside of .minimize() context.
Documentation does not mention anywhere that negative gradient is followed by this method.
had to use a unit test to clarify the behavior:
&lt;denchmark-code&gt;    opt = tf.optimizers.Adam()
    x = tf.Variable([1], dtype=tf.float32)
    dx = tf.ones([1], dtype=tf.float32)

    opt.apply_gradients( [(dx, x)] )
    assert x.numpy()[0] &gt; 1
&lt;/denchmark-code&gt;

There is also no flag to invert this behavior and follow positive gradient. I have to pass negative gradient of the expected reward to follow it in positive direction.
&lt;denchmark-code&gt;    def train_actor(self, sars):

        obs1, actions, rewards, obs2 = sars
        with tf.GradientTape() as tape:
            would_do_actions = self.actor(obs1)
            score = tf.reduce_mean( self.critic( observations=obs1, actions=would_do_actions ) )
            inverted = - score

        # tf optimizer follows negative of the provided gradients.
        # For this reason we provide negative gradient of the score -
        # it will result in positive gradient being followed.
        grads = tape.gradient( inverted, self.actor.trainable_weights )
        self.optimizer.apply_gradients( zip(grads, self.actor.trainable_weights) )
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='ikamensh' date='2019-04-05T09:58:15Z'>
		&lt;denchmark-link:https://github.com/ikamensh&gt;@ikamensh&lt;/denchmark-link&gt;
 Thanks for bringing this to our notice. We will take a look and resolve it. Thanks!
		</comment>
		<comment id='2' author='ikamensh' date='2019-04-17T18:57:22Z'>
		&lt;denchmark-link:https://github.com/ikamensh&gt;@ikamensh&lt;/denchmark-link&gt;
 This is intended behavior and not 2.0 specific. In general all optimizers follow the algorithm from papers to minimize objective. If the goal is to maximize reward, then the best approach (that I think) is to negate the objective manually.
For example, see this ddpg implementation:
&lt;denchmark-link:https://github.com/openai/spinningup/blob/master/spinup/algos/ddpg/ddpg.py#L157&gt;https://github.com/openai/spinningup/blob/master/spinup/algos/ddpg/ddpg.py#L157&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='ikamensh' date='2019-04-18T14:55:46Z'>
		In reinforcement learning reward is always maximized. Furthermore, Tensorflow can be seen as a broader framework than only machine learning, then it makes sense that "apply gradients" would at least mention that it will follow the negative of the provided gradient. Being specific does not hurt.
		</comment>
		<comment id='4' author='ikamensh' date='2019-06-28T18:30:12Z'>
		Closing this since there is no intermediate plan for improving this. In general I think it's fine because we mostly use neg log prob so it is actually minimizing objective function
		</comment>
		<comment id='5' author='ikamensh' date='2019-07-23T09:33:08Z'>
		&lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;
 as of July 2019, this still has not been documented. In my opinion the documentation for  should mention that it follows the negative gradient.
Independent of the debate whether implementation is good or bad, a better documentation could never hurt.
		</comment>
	</comments>
</bug>