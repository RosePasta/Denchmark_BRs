<bug id='17949' author='denkuzin' open_date='2018-03-23T11:54:29Z' closed_time='2018-08-20T19:44:47Z'>
	<summary>incorrect description of num_sampled parameter in tf.nn.nce_loss() function: num_sampled is number of negative examples per 1 positive example (NOT per batch)</summary>
	<description>
Hi all,
Looks like parameter  in   function defines the number of negative examples per a positive example but not per a batch as described in tensorflow documentation (&lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss&gt;https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss&lt;/denchmark-link&gt;
)
(see the next code)
&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np
# `_compute_sampled_logits` is invoked in nce_loss to generate negative sample and calculate logits
from tensorflow.python.ops.nn_impl import _compute_sampled_logits

embedding_size = 10
words_number = 300
batch_size = 3
num_sampled = 3

graph = tf.Graph()
with graph.as_default():
    # Input data.
    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])
    train_labels = tf.placeholder(tf.int64, shape=[batch_size, 1])

    with tf.device('/cpu:0'):
        embeddings = tf.Variable(
                tf.random_uniform([words_number, embedding_size], -4., 4.))
        embed = tf.nn.embedding_lookup(embeddings, train_inputs)
        nce_weights = tf.Variable(
                tf.random_uniform([words_number, embedding_size], -4., 4.))
        nce_biases = tf.Variable(tf.zeros([words_number]))
        
    logits, labels = _compute_sampled_logits(
                       weights=nce_weights,
                       biases=nce_biases,
                       inputs=embed,
                       labels=train_labels,
                       num_true=1,
                       num_sampled=num_sampled,
                       num_classes=words_number,
                       remove_accidental_hits = False)
    init = tf.global_variables_initializer()


session = tf.InteractiveSession(graph=graph)
init.run(session=session)

batch_inputs = np.array([0,1,2], dtype=np.int32)
batch_labels = np.array([[3],[4],[5]], dtype=np.int32)

feed_dict = {train_inputs : batch_inputs, train_labels : batch_labels}
logits_val, labels_val = session.run([logits, labels], feed_dict=feed_dict)

print ("logits_val = {}".format(logits_val))
print ("labels_val = {}".format(labels_val))

&lt;/denchmark-code&gt;

As a result, _compute_sampled_logits function generated num_sampled examples per 1 positive example:
&lt;denchmark-code&gt;logits_val = [[ -8.18727493   2.02518415  14.18676853   0.51900673]
 [ -8.97232056   5.60003376   4.52866602   3.68161726]
 [ -0.36226368  -5.84330416  -3.39891291   5.58423615]]
labels_val = [[ 1.  0.  0.  0.]
 [ 1.  0.  0.  0.]
 [ 1.  0.  0.  0.]]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='denkuzin' date='2018-03-23T12:02:03Z'>
		Maybe it's not a bug but very significant typo in the documentation
		</comment>
		<comment id='2' author='denkuzin' date='2018-06-29T18:31:52Z'>
		From my limited understanding, I think the documentation is correct. The first logit in your output is computed from all true samples, not just from 1 true sample. The rest 3 logits are for the 3 random samples.
		</comment>
		<comment id='3' author='denkuzin' date='2018-07-21T00:30:23Z'>
		&lt;denchmark-link:https://github.com/denkuzin&gt;@denkuzin&lt;/denchmark-link&gt;
 thank you for pointing this out!
&lt;denchmark-link:https://github.com/MarkDaoust&gt;@MarkDaoust&lt;/denchmark-link&gt;
 would you PTAL?
		</comment>
		<comment id='4' author='denkuzin' date='2018-08-19T18:47:35Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/MarkDaoust&gt;@MarkDaoust&lt;/denchmark-link&gt;
: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='5' author='denkuzin' date='2018-08-20T19:51:01Z'>
		Hi denkuzin@,
Thanks for the report. Here's a fix that I think clarifies what's actually happening here.
		</comment>
	</comments>
</bug>