<bug id='34210' author='Pierrci' open_date='2019-11-12T23:31:26Z' closed_time='2020-01-02T15:09:18Z'>
	<summary>TFLiteConverter - GetOpWithOutput error</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
TensorFlow installed from (source or binary): pip install tf-nightly
TensorFlow version (use command below): 2.1.0-dev20191111
Python version: 3.6.8

Describe the current behavior
I'm trying to convert a BERT keras model imported from the transformers library. Using the convert() method from the TFLiteConverter, the following GetOpWithOutput error happens, no matter the values used passed to allow_custom_ops or target_spec.supported_ops:
tensorflow/lite/toco/tooling_util.cc:935] Check failed: GetOpWithOutput(model, output_array) Specified output array "Identity" is not produced by any op in this graph
Describe the expected behavior
The TFLiteConverter should convert the model and return the tflite version

&lt;denchmark-link:https://colab.research.google.com/drive/16kWs2ji8xrgjSuLftXknDqEh2VixDR4R&gt;https://colab.research.google.com/drive/16kWs2ji8xrgjSuLftXknDqEh2VixDR4R&lt;/denchmark-link&gt;

Other info / logs
The same bert-large-uncased-whole-word-masking-finetuned-squad model imported from the transformers library works perfectly when used directly on a QA task without TFLite conversion.
Also, the conversion of the DistilBERT model from the same library and using the same steps works correctly with converter.target_spec.supported_ops = [tf.lite.OpsSet.SELECT_TF_OPS].
	</description>
	<comments>
		<comment id='1' author='Pierrci' date='2019-11-13T17:43:44Z'>
		Hi,
It's possible that the model doesn't have an output node named "Identity". Can you use netron (&lt;denchmark-link:https://lutzroeder.github.io/netron/&gt;https://lutzroeder.github.io/netron/&lt;/denchmark-link&gt;
) to visualize this graph and see what's the output?
(I'm guessing this is just a tensor naming issue, if not I can take a further look)
		</comment>
		<comment id='2' author='Pierrci' date='2019-11-13T19:37:41Z'>
		Thank you for your answer. I tried to visualize the graph using Netron without success, either with a h5 or SavedModel version.
However, as you can see in &lt;denchmark-link:https://colab.research.google.com/drive/16kWs2ji8xrgjSuLftXknDqEh2VixDR4R#scrollTo=94UoamBP17VO&amp;line=4&amp;uniqifier=1&gt;this cell of the notebook&lt;/denchmark-link&gt;
, it seems there are two outputs  and . Also visible in this Tensorboard graph:
&lt;denchmark-link:https://user-images.githubusercontent.com/5020707/68797525-c1e6bb00-0622-11ea-99ef-b5c1e8454ccc.png&gt;&lt;/denchmark-link&gt;

Does it answer your question &lt;denchmark-link:https://github.com/haozha111&gt;@haozha111&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='3' author='Pierrci' date='2019-11-14T17:39:57Z'>
		That seems like a bug in our old converter.
We have developed a new MLIR-based converter, you can try it out by downloading the tf-nightly pip package, and then set converter.experimental_new_converter = True.
Could you try this out and see if the problem is resolved?
		</comment>
		<comment id='4' author='Pierrci' date='2019-11-15T15:26:37Z'>
		I get this error when trying with converter.experimental_new_converter = True, whatever the value of other options target_spec.supported_ops and allow_custom_ops:
&lt;denchmark-code&gt;2019-11-15 15:18:32.489426: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia
2019-11-15 15:18:34.411116: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:106] Ignored output_format.
2019-11-15 15:18:34.411167: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:112] Ignored drop_control_dependency.
Traceback (most recent call last):
  File "/usr/local/bin/toco_from_protos", line 8, in &lt;module&gt;
    sys.exit(main())
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py", line 93, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File "/usr/local/lib/python3.6/dist-packages/absl/app.py", line 299, in run
    _run_main(main, args)
  File "/usr/local/lib/python3.6/dist-packages/absl/app.py", line 250, in _run_main
    sys.exit(main(argv))
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py", line 56, in execute
    enable_mlir_converter)
Exception: Graph does not contain node: input_1
&lt;/denchmark-code&gt;

But it seems there is an  in the model, as visible in my previous TensorBoard graph and in &lt;denchmark-link:https://colab.research.google.com/drive/16kWs2ji8xrgjSuLftXknDqEh2VixDR4R#scrollTo=94UoamBP17VO&amp;line=4&amp;uniqifier=1&gt;this cell of the notebook&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='5' author='Pierrci' date='2019-11-15T19:24:36Z'>
		I could reproduce the issue now. It seems like a bug in our converter, will take a look and try to fix it soon.
		</comment>
		<comment id='6' author='Pierrci' date='2019-12-25T20:51:18Z'>
		I changed  in the pip command. &lt;denchmark-link:https://colab.research.google.com/drive/1MrPEJWqReZGKWBHAkoYYCrDiDTccmLUb&gt;https://colab.research.google.com/drive/1MrPEJWqReZGKWBHAkoYYCrDiDTccmLUb&lt;/denchmark-link&gt;

Does this help?
		</comment>
		<comment id='7' author='Pierrci' date='2020-01-02T15:09:18Z'>
		I tried again with the latest TF nightly version and it works now using the experimental converter and partial TF operators:
converter.experimental_new_converter = True
converter.target_spec.supported_ops  = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
Specifying  doesn't affect the result since it defaults to the last version available which is  &lt;denchmark-link:https://github.com/ucalyptus&gt;@ucalyptus&lt;/denchmark-link&gt;
.
Closing this, thanks to the TF team for all the continuous improvements! üëè
		</comment>
		<comment id='8' author='Pierrci' date='2020-01-02T15:09:20Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34210&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34210&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='Pierrci' date='2020-04-22T14:04:21Z'>
		Using these options did great! Thanks
converter.experimental_new_converter = True
converter.allow_custom_ops = True
		</comment>
	</comments>
</bug>