<bug id='15269' author='adamcavendish' open_date='2017-12-11T11:05:17Z' closed_time='2018-03-09T03:07:46Z'>
	<summary>tf.layers.Layer.set_scope() problem might cause unexpected duplicate name ValueError</summary>
	<description>

OS Platform and Distribution: Ubuntu 16.04
TensorFlow installed from (source or binary): binary
TensorFlow version: 1.4.1
Python version: 3.5.2
CUDA/cuDNN version: CUDA 8.0, CUDNN 7.0.5
GPU model and memory: GTX1080 (8G)
Exact command to reproduce: python3 test.py
Have I written custom code: True
Bazel version: N/A
GCC version: N/A

Repoduce code:
import tensorflow as tf

_BATCH_NORM_DECAY = 0.997
_BATCH_NORM_EPSILON = 1e-5

def two_batchnorm(inputs):
    with tf.variable_scope('two_batchnorm'):
        inputs = tf.layers.batch_normalization(
            inputs=inputs,
            axis=3,
            momentum=_BATCH_NORM_DECAY,
            epsilon=_BATCH_NORM_EPSILON,
            center=True,
            scale=True,
            training=True,
            fused=True)
        inputs = tf.layers.batch_normalization(
            inputs=inputs,
            axis=3,
            momentum=_BATCH_NORM_DECAY,
            epsilon=_BATCH_NORM_EPSILON,
            center=True,
            scale=True,
            training=True,
            fused=True)
    return inputs

inputs = tf.placeholder(tf.float32, [1, 5, 5, 3])
x = inputs
x = two_batchnorm(x)
x = two_batchnorm(x)
It'll trigger an unexpected ValueError as following:
&lt;denchmark-code&gt;ValueError: Variable two_batchnorm/batch_normalization/gamma already exists, disallowed.
&lt;/denchmark-code&gt;

Removing the variable scope with tf.variable_scope('two_batchnorm') in two_batchnorm will work as expected.
All variables defined in the graph should be (in creation sequense):
&lt;denchmark-code&gt;two_batchnorm/batch_normalization/gamma:0
two_batchnorm/batch_normalization/beta:0
two_batchnorm/batch_normalization/moving_mean:0
two_batchnorm/batch_normalization/moving_variance:0
two_batchnorm/batch_normalization_1/gamma:0
two_batchnorm/batch_normalization_1/beta:0
two_batchnorm/batch_normalization_1/moving_mean:0
two_batchnorm/batch_normalization_1/moving_variance:0
two_batchnorm/batch_normalization_2/gamma:0
two_batchnorm/batch_normalization_2/beta:0
two_batchnorm/batch_normalization_2/moving_mean:0
two_batchnorm/batch_normalization_2/moving_variance:0
two_batchnorm/batch_normalization_3/gamma:0
two_batchnorm/batch_normalization_3/beta:0
two_batchnorm/batch_normalization_3/moving_mean:0
two_batchnorm/batch_normalization_3/moving_variance:0
&lt;/denchmark-code&gt;

However, with tf.layers.Layer's add_variable logics, it'll result in an unexpected value name as following (in creation sequence):
&lt;denchmark-code&gt;two_batchnorm/batch_normalization/gamma:0
two_batchnorm/batch_normalization/beta:0
two_batchnorm/batch_normalization/moving_mean:0
two_batchnorm/batch_normalization/moving_variance:0
two_batchnorm/batch_normalization_1/gamma:0
two_batchnorm/batch_normalization_1/beta:0
two_batchnorm/batch_normalization_1/moving_mean:0
two_batchnorm/batch_normalization_1/moving_variance:0
two_batchnorm/batch_normalization/gamma:0                        &lt;-- ValueError raised here.
two_batchnorm/batch_normalization/beta:0
two_batchnorm/batch_normalization/moving_mean:0
two_batchnorm/batch_normalization/moving_variance:0
two_batchnorm/batch_normalization_1/gamma:0
two_batchnorm/batch_normalization_1/beta:0
two_batchnorm/batch_normalization_1/moving_mean:0
two_batchnorm/batch_normalization_1/moving_variance:0
&lt;/denchmark-code&gt;

One solution might be using self._name to setup Layer's scope, not self._base_name.
	</description>
	<comments>
		<comment id='1' author='adamcavendish' date='2017-12-11T19:02:58Z'>
		Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.
Have I written custom code
Bazel version
		</comment>
		<comment id='2' author='adamcavendish' date='2017-12-11T19:07:09Z'>
		Looks like the butler jumped the gun here.
Thanks for prepping a PR for this. Since it seems like a QOL improvement, I'll mark it as a FR rather than a bug, and wait for input on the PR rather than pinging someone here directly.
		</comment>
		<comment id='3' author='adamcavendish' date='2017-12-11T19:12:30Z'>
		&lt;denchmark-link:https://github.com/tensorflowbutler&gt;@tensorflowbutler&lt;/denchmark-link&gt;
 Fields added.
@angersson Since it might cause someone's wrapping functions, i.e.  in  or  fail to have a function variable scope name, it's not simply a naive issue in my example or so.
		</comment>
		<comment id='4' author='adamcavendish' date='2017-12-12T01:45:16Z'>
		&lt;denchmark-link:https://github.com/adamcavendish&gt;@adamcavendish&lt;/denchmark-link&gt;
 I think you recreate the variable_scope, which introduces the name conflict:
import tensorflow as tf

with tf.variable_scope("a"):
    v = tf.get_variable("v", [1])

with tf.variable_scope("a"):
    v1 = tf.get_variable("v", [1])
Perhaps a better way is to always create new variable_scope when invoked repeatedly,  I mean, use with tf.variable_scope(None, default_name='two_batchnorm'): instead.
By the way, if I remember correctly, self._name starts from one by default, eg: dense_1, dense_2, which might break the existing behavior.
		</comment>
		<comment id='5' author='adamcavendish' date='2017-12-12T02:51:08Z'>
		&lt;denchmark-link:https://github.com/facaiy&gt;@facaiy&lt;/denchmark-link&gt;
 Yep, you're right. It does break the existing behavior. However, we have to break the existing behaviors if we'd like solve the problem.
I just wonder why it should be an expected behavior, I mean if deliberately, in my example, two_bn/bn, two_bn/bn_1, two_bn/bn, two_bn/bn_1 should be accepted rather than two_bn/bn, two_bn/bn_1, two_bn/bn_2, two_bn/bn_3 or two_bn/bn_1, two_bn/bn_2, two_bn/bn_3, two_bn/bn_4.
		</comment>
		<comment id='6' author='adamcavendish' date='2017-12-12T03:41:35Z'>
		Do you try
with tf.variable_scope(None, default_name='two_batchnorm'):
It might resolve your problem.
		</comment>
		<comment id='7' author='adamcavendish' date='2017-12-12T03:43:21Z'>
		&lt;denchmark-link:https://github.com/facaiy&gt;@facaiy&lt;/denchmark-link&gt;
 Yep, however, it'll make incrementations on , but not 
		</comment>
		<comment id='8' author='adamcavendish' date='2017-12-12T04:10:34Z'>
		&lt;denchmark-link:https://github.com/adamcavendish&gt;@adamcavendish&lt;/denchmark-link&gt;
 Yes, increment on variable_scope is intentional: it sees function  as an unit, and we create two ones here: , and .
I'm afraid that &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/15270&gt;#15270&lt;/denchmark-link&gt;
 might also break the  behavior of :
&lt;denchmark-link:https://www.tensorflow.org/versions/master/api_docs/python/tf/layers/batch_normalization&gt; tf.layers.batch_normalization &lt;/denchmark-link&gt;


reuse: Boolean, whether to reuse the weights of a previous layer by the same name.

cc &lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;
 who I know might care the issue.
		</comment>
		<comment id='9' author='adamcavendish' date='2017-12-12T04:45:26Z'>
		I think very probably you're right, but I'd leave this issue and PR open for further discussions.
		</comment>
		<comment id='10' author='adamcavendish' date='2018-01-03T07:35:47Z'>
		It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
		</comment>
		<comment id='11' author='adamcavendish' date='2018-01-04T04:35:51Z'>
		Ping &lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;
 , the authority on this issue.
		</comment>
		<comment id='12' author='adamcavendish' date='2018-01-18T19:08:53Z'>
		Nagging Awaiting TensorFlower: It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
		</comment>
		<comment id='13' author='adamcavendish' date='2018-01-23T22:56:00Z'>
		A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.
		</comment>
		<comment id='14' author='adamcavendish' date='2018-02-21T13:20:27Z'>
		Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='15' author='adamcavendish' date='2018-03-08T19:29:30Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;
: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='16' author='adamcavendish' date='2018-03-09T03:07:45Z'>
		I've been very busy on my work these days, but I'll figure out how to do it afterwards.
		</comment>
	</comments>
</bug>