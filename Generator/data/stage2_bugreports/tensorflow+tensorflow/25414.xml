<bug id='25414' author='EmanueleGhelfi' open_date='2019-02-01T08:18:59Z' closed_time='2019-07-01T16:26:46Z'>
	<summary>Tensorflow 2.0 Preview - tf.function and tensorflow_dataset incompatibility</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
TensorFlow installed from (source or binary): pip
TensorFlow version (use command below): tf-nightly-2.0-preview
Python version: 3.6
CUDA/cuDNN version: 10
GPU model and memory: GTX 1080Ti

Describe the current behavior
Creating a dataset using tensorflow_dataset and passing it to a function decorated with @tf.function gives the error:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/home/emanuele/development/gan-toolbox/try2.py", line 308, in &lt;module&gt;
    main()
  File "/home/emanuele/development/gan-toolbox/try2.py", line 304, in main
    _ = gan.train(dataset, G_opt, D_opt)
  File "/home/emanuele/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py", line 398, in __call__
    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access
  File "/home/emanuele/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 460, in _filtered_call
    (t for t in nest.flatten((args, kwargs))
  File "/home/emanuele/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 507, in _call_flat
    outputs = self._inference_function.call(ctx, args)
  File "/home/emanuele/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 312, in call
    ctx=ctx)
  File "/home/emanuele/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/execute.py", line 66, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File "&lt;string&gt;", line 3, in raise_from
tensorflow.python.framework.errors_impl.NotFoundError: Function Dataset_interleave_TFRecordExampleAdapter.dataset_from_filename_4 is not defined.
	 [[{{node ReduceDataset}}]] [Op:__inference_train_3786]
&lt;/denchmark-code&gt;

Describe the expected behavior
Not encountering the error. I think the problem is related to the magic behind tf.function. The issue is that there isn't a complete guide on how to use correctly tf.function.
Code to reproduce the issue
"""
Implement DCGAN using the new TF 2.0 API.

Also test tensorflow-datasets.

Celeb-A dataset.
"""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from typing import Dict
import tensorflow_datasets as tfds
import tensorflow as tf
from tensorflow import keras as k
from tensorflow.python.ops import control_flow_util


control_flow_util.ENABLE_CONTROL_FLOW_V2 = True


@tf.function
def bce(x, label, label_smoothing=0.0):
    """Returns the discrete binary cross entropy between x and the discrete label
    Args:
        x: a 2D tensor
        label: the discrete label, aka, the distribution to match
        label_smoothing: if greater than zero, smooth the labels

    Returns:
        The binary cros entropy
    """
    return k.losses.BinaryCrossentropy()(tf.ones_like(x) * label, x)


def min_max(positive, negative, label_smoothing=0.0):
    """Returns the discriminator (min max) loss
    Args:
        positive: the discriminator output for the positive class: 2D tensor
        negative: the discriminator output for the negative class: 2D tensor
        smooth: if greater than zero, appiles one-sided label smoothing
    Returns:
        The sum of 2 BCE
    """
    one = tf.constant(1.0)
    zero = tf.constant(0.0)
    d_loss = bce(positive, one, label_smoothing) + bce(negative, zero)
    return d_loss


class Generator(k.Model):
    def __init__(self):
        super(Generator, self).__init__()
        self.fc1 = k.layers.Dense(4 * 4 * 1024)
        self.batchnorm1 = k.layers.BatchNormalization()

        self.conv2 = k.layers.Conv2DTranspose(
            filters=512,
            kernel_size=(5, 5),
            strides=(2, 2),
            padding="same",
            use_bias=False,
        )
        self.batchnorm2 = k.layers.BatchNormalization()

        self.conv3 = k.layers.Conv2DTranspose(
            filters=256,
            kernel_size=(5, 5),
            strides=(2, 2),
            padding="same",
            use_bias=False,
        )
        self.batchnorm3 = k.layers.BatchNormalization()

        self.conv4 = k.layers.Conv2DTranspose(
            filters=128,
            kernel_size=(5, 5),
            strides=(2, 2),
            padding="same",
            use_bias=False,
        )
        self.batchnorm4 = k.layers.BatchNormalization()

        self.conv5 = k.layers.Conv2DTranspose(
            filters=3,
            kernel_size=(5, 5),
            strides=(2, 2),
            padding="same",
            use_bias=False,
        )
        self.batchnorm5 = k.layers.BatchNormalization()

    def call(self, x: tf.Tensor, training: bool = True) -&gt; tf.Tensor:
        x = self.fc1(x)
        x = self.batchnorm1(x, training=training)
        x = tf.nn.relu(x)
        x = tf.reshape(x, shape=(-1, 4, 4, 1024))

        x = self.conv2(x)
        x = self.batchnorm2(x, training=training)
        x = tf.nn.relu(x)

        x = self.conv3(x)
        x = self.batchnorm3(x, training=training)
        x = tf.nn.relu(x)

        x = self.conv4(x)
        x = self.batchnorm4(x, training=training)
        x = tf.nn.relu(x)

        x = self.conv5(x)
        x = self.batchnorm5(x, training=training)

        x = tf.nn.tanh(x)
        return x


class Discriminator(k.Model):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.conv1 = k.layers.Conv2D(128, (5, 5), strides=(2, 2), padding="same")
        self.conv2 = k.layers.Conv2D(256, (5, 5), strides=(2, 2), padding="same")
        self.batchnorm2 = k.layers.BatchNormalization()
        self.conv3 = k.layers.Conv2D(512, (5, 5), strides=(2, 2), padding="same")
        self.batchnorm3 = k.layers.BatchNormalization()
        self.conv4 = k.layers.Conv2D(1024, (5, 5), strides=(2, 2), padding="same")
        self.batchnorm4 = k.layers.BatchNormalization()
        self.flatten = k.layers.Flatten()
        self.fc5 = k.layers.Dense(1)

    def call(self, x, training=True):
        x = self.conv1(x)
        x = tf.nn.leaky_relu(x)

        x = self.conv2(x)
        x = self.batchnorm2(x)
        x = tf.nn.leaky_relu(x)

        x = self.conv3(x)
        x = self.batchnorm3(x)
        x = tf.nn.leaky_relu(x)

        x = self.conv4(x)
        x = self.batchnorm4(x)
        x = tf.nn.leaky_relu(x)

        x = self.flatten(x)
        x = self.fc5(x)
        return x


class GAN:
    def __init__(self, generator, discriminator, encoder=None):
        """
        GAN initializer.

        Args:
            generator: A ``tensorflow.keras.Model`` to use as Generator.
            discriminator: A ``tensorflow.keras.Model`` to use as Discriminator.
            encoder: A ``tensorflow.keras.Model`` to use as Encoder.

        Returns:
            Trained GAN model (?).

        """
        self.G = generator()
        self.D = discriminator()
        # self.E = encoder() if encoder is not None else None
        self.latent_vector_dims = 100

    @tf.function()
    def train(self, dataset, opt1, opt2):
        """
        Train.
        """
        step = 0
        for f in dataset:
            x = f["image"]
            step += 1
            z = tf.random.normal((32, self.latent_vector_dims))

            # We record all the operations in the tape
            with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
                G_z = self.G(z, training=True)

                D_x = self.D(x, training=True)
                D_Gz = self.D(G_z, training=True)

                g_loss = bce(D_Gz, 1.0)
                d_loss = min_max(D_x, D_Gz, label_smoothing=0.0)

            # We retrieve the gradients from our records
            G_grads = gen_tape.gradient(g_loss, self.G.trainable_variables)
            D_grads = disc_tape.gradient(d_loss, self.D.trainable_variables)

            # Optimize and apply the gradients
            opt1.apply_gradients(zip(G_grads, self.G.trainable_variables))
            opt2.apply_gradients(zip(D_grads, self.D.trainable_variables))

            if step % 1 == 0:
                print("--------------------------")
                print("STEP", step)
                print("D_LOSS", d_loss)
                print("G_LOSS:", g_loss)
        return step


class InputPipeline:
    def __init__(
        self, dataset, batch_size, epochs, shuffle_buffer, prefetched_items, size
    ):
        self.batch_size = batch_size
        self.dataset_name = dataset
        self.epochs = epochs
        self.prefetched_items = prefetched_items
        self.shuffle_buffer = shuffle_buffer
        self.size = size

    def get_input_fn(self):
        """Input fn."""
        return self.input_fn

    def load_public_dataset(self):
        """
        Load one of the publicly available datasets, will merge together all the splits.

        Args:
            chosen_dataset: dataset to use.

        Return:
            The chosen dataset as a ``tf.data.Dataset``

        """
        # Construct a tf.data.Dataset
        datasets = tfds.load(name=self.dataset_name, split=tfds.Split.ALL)
        return datasets

    def resize_images(self, features):
        """
        Overwrite the \"image\" feature in order to resize them.

        Args:
            features: features dictionary.
            size: desired target size.

        Returns:
            Features with \"image\" resized to the correct shape.

        """
        features["image"] = tf.image.resize(features["image"], self.size)
        return features

    def input_fn(self):
        dataset = self.load_public_dataset()
        dataset = (
            dataset.map(self.resize_images)
            .shuffle(self.shuffle_buffer)
            .batch(self.batch_size)
            .prefetch(self.prefetched_items)
            .repeat(self.epochs)
        )
        return dataset


def main():
    # See available datasets
    public_datasets = tfds.list_builders()

    gan = GAN(Generator, Discriminator)
    G_opt = k.optimizers.Adam(learning_rate=1e-5, beta_1=0.5)
    D_opt = k.optimizers.Adam(learning_rate=1e-5, beta_1=0.5)

    input_pipeline = InputPipeline(
        dataset="celeb_a",
        batch_size=32,
        epochs=2,
        prefetched_items=1,
        shuffle_buffer=1000,
        size=(64, 64),
    )
    dataset = input_pipeline.input_fn()
    _ = gan.train(dataset, G_opt, D_opt)


if __name__ == "__main__":
    main()
Other info / logs
If I remove the @tf.function annotation the code works as expected.
If I create the dataset inside the annotate function the code works as expected.
Basically in the code below I create a td.data.Dataset using the new package tensorflow_dataset. Then I pass the created dataset to a function annotated with tf.function that should perform the training loop. The errors I get are not informative.
Unfortunately the tensorflow docs do not explain well how to use tf.function and the admitted operations.
	</description>
	<comments>
		<comment id='1' author='EmanueleGhelfi' date='2019-02-01T20:17:23Z'>
		&lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
 I think this is the same function registration issue we've discussed before, right?
		</comment>
		<comment id='2' author='EmanueleGhelfi' date='2019-02-01T20:24:06Z'>
		Correct. It is not a TF 2.0 issue. The same behavior would happen in TF 1.x nightly.
The use program will need to pass a dataset factory into train method so that the dataset is created in the same graph in which it is iterated over.
		</comment>
		<comment id='3' author='EmanueleGhelfi' date='2019-02-01T20:27:08Z'>
		&lt;denchmark-link:https://github.com/EmanueleGhelfi&gt;@EmanueleGhelfi&lt;/denchmark-link&gt;
 Can you try the workaround &lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
  suggested by creating the dataset inside the tf.function? Alternatively you can create the iterator outside.
We're still working on a fix for this so I'd like to leave this open.
		</comment>
		<comment id='4' author='EmanueleGhelfi' date='2019-02-02T09:51:10Z'>
		&lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
 If I create the dataset inside the tf.function the code works as expected. I think this should be at least documented. For me it is not clear how to use correctly tf.function, the things that are allowed and the things not allowed. If I need to use tf.function in every function I call I think it would be nicer to use a global setting.
		</comment>
		<comment id='5' author='EmanueleGhelfi' date='2019-02-04T15:54:53Z'>
		The reason why this is not documented is that this is a bug we're still
working on a fix for (and documenting broken behavior leads to workarounds
persisting for long after the issue has been fixed).
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Sat, Feb 2, 2019 at 1:54 AM Emanuele Ghelfi ***@***.***&gt; wrote:
 @alextp &lt;https://github.com/alextp&gt; @jsimsa &lt;https://github.com/jsimsa&gt;
 If I create the dataset inside the tf.function the code works as expected.
 I think this should be at least documented. For me it is not clear how to
 use correctly tf.function, the things that are allowed and the things not
 allowed. If I need to use tf.function in every function I call I think it
 would be nicer to use a global setting.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#25414 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AAATxSMmfdXlXCR3Em58TmVEJGbk2R0Jks5vJWBNgaJpZM4ad2jI&gt;
 .


-- 
 - Alex

		</comment>
		<comment id='6' author='EmanueleGhelfi' date='2019-02-04T16:02:31Z'>
		Ok, thank you &lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
 for your support.
		</comment>
		<comment id='7' author='EmanueleGhelfi' date='2019-07-01T16:26:47Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=25414&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=25414&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>