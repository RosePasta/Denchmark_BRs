<bug id='38018' author='MaUt89' open_date='2020-03-29T11:56:59Z' closed_time='2020-04-20T06:24:08Z'>
	<summary>tf.estimator.add_metrics ends in Shapes (None, 12) and (None,) are incompatible</summary>
	<description>
System information
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64-bit
TensorFlow installed from (source or binary): PyCharm
TensorFlow version (use command below): 2.0.0
Python version: 3.7.6

I am using a DNNClassifier as my estimator and wanted to add some additional metrics to the estimator. the code I am using is basically the one from the tf.estimator.add_metrics documentation
&lt;denchmark-link:url&gt;https://www.tensorflow.org/api_docs/python/tf/estimator/add_metrics&lt;/denchmark-link&gt;
.
The model works fine without the add_metrics statement. But runs into an ValueError: "Shapes (None, 12) and (None,) are incompatible" when including it. The error occures in the line:
auc_metric.update_state(y_true=labels, y_pred=predictions['logits'])
The line is called by est.evaluate(validation_data).
It is not clear to me why this happens, but it seems like the y_true parameter is not filled correctly. Hence, the label column is not processed correctly to the function. This seems strange since the model works correctly without the additional metric. The training and validation data is created by the following function:
&lt;denchmark-code&gt;def get_dataset_from_tensor_slices(data_input, label_column, n_epochs=None, shuffle=True):
    def get_dataset():
        dataset = tf.data.Dataset.from_tensor_slices((dict(data_input), label_column))
        if shuffle:
            dataset = dataset.shuffle(len(label_column))

        # For training, cycle through dataset as many times as need (n_epochs=None).
        dataset = dataset.repeat(n_epochs)
        # In memory training doesn't use batching.
        dataset = dataset.batch(len(label_column))
        return dataset
    return get_dataset
&lt;/denchmark-code&gt;

Describe the expected behavior
It should be able to add an additional metric to the estimator.
Standalone code to reproduce the issue
&lt;denchmark-code&gt;def my_auc(labels, predictions):
    auc_metric = tf.keras.metrics.AUC(name="my_auc")
    auc_metric.update_state(y_true=labels, y_pred=predictions['logits'])
    return {'auc': auc_metric}


def model_evaluation(features, training_data, validation_data, labels, validation_label_column):
    hidden_layers = len(training_data.__call__().element_spec[0])
    final_layer = len(labels)
    est = tf.estimator.DNNClassifier(feature_columns=features,
                                     hidden_units=[hidden_layers, (hidden_layers / 2), (hidden_layers / 4),
                                                   final_layer],
                                     n_classes=final_layer, label_vocabulary=labels)
    est = tf.estimator.add_metrics(est, my_auc)

    est.train(training_data, max_steps=100)

    result = est.evaluate(validation_data)

&lt;/denchmark-code&gt;

Other info / logs
As far as I debugged it, the problem goes back to the fact that the labels created from the get_dataset_from_tensor_slices method have the shape (None,). --&gt; Thats maybe the problem...how can I fix this?
Whereas the predictions are generated in shape (None, 12) (where 12 is the number of possible labels).
Does anybody know why this happens? Any help is appreciated!
	</description>
	<comments>
		<comment id='1' author='MaUt89' date='2020-03-30T05:49:34Z'>
		&lt;denchmark-link:https://github.com/MaUt89&gt;@MaUt89&lt;/denchmark-link&gt;
, Thanks for reporting this issue, Can you provide the complete code to replicate the reported issue Thanks!
		</comment>
		<comment id='2' author='MaUt89' date='2020-03-31T15:26:24Z'>
		Please find in the following the complete code. The data I'm using consist exclusively of String features so I have only categorical columns. Thank you for your help.
main():
&lt;denchmark-code&gt;from data_preprocessing import data_preprocessing
from model_evaluation import model_evaluation

label_column = '123'
labels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l']

features, training_data, validation_data, validation_label_column = data_preprocessing(label_column)
model_evaluation(features, training_data, validation_data, labels, validation_label_column)
&lt;/denchmark-code&gt;

data_preprocessing():
&lt;denchmark-code&gt;from __future__ import absolute_import, division, print_function, unicode_literals
from operator import is_not
from functools import partial

import numpy as np
import tensorflow as tf
import pandas as pd
import os.path


def data_preprocessing(label_column):
    print(tf.__version__)

    train_file_path = 'Learning Data Input/TrainingDataFromPandas.csv'
    validation_file_path = 'Learning Data Input/ValidationDataFromPandas.csv'

    pandas_data = pd.read_csv(train_file_path, delimiter=';', dtype='string')
    train_label_column = pandas_data.pop(label_column)
    training_data = get_dataset_from_tensor_slices(pandas_data, train_label_column)

    pandas_data = pd.read_csv(validation_file_path, delimiter=';', dtype='string')
    validation_label_column = pandas_data.pop(label_column)
    validation_data = get_dataset_from_tensor_slices(pandas_data, validation_label_column, shuffle=False, n_epochs=1)

    show_batch(training_data.__call__())
    show_batch(validation_data.__call__())

    categorical_columns = get_categorical_feature_columns(training_data.__call__())

    example_batch, labels_batch = next(iter(training_data.__call__()))

    categorical_layer = tf.keras.layers.DenseFeatures(categorical_columns)
    print(categorical_layer(example_batch).numpy()[0])

    return categorical_columns, training_data, validation_data, validation_label_column

def get_dataset_from_tensor_slices(data_input, label_column, n_epochs=None, shuffle=True):
    def get_dataset():
        dataset = tf.data.Dataset.from_tensor_slices((dict(data_input), label_column))
        if shuffle:
            dataset = dataset.shuffle(len(label_column))

        # For training, cycle through dataset as many times as need (n_epochs=None).
        dataset = dataset.repeat(n_epochs)
        # In memory training doesn't use batching.
        dataset = dataset.batch(round(len(label_column)/10))
        return dataset
    return get_dataset


def show_batch(dataset):
    for batch, label in dataset.take(1):
        for key, value in batch.items():
            print("{:20s}: {}".format(key, value.numpy()))
    return


def get_categorical_feature_columns(data):
    categorical_columns = []
    for batch, label in data.take(1):
        for key, value in batch.items():
            # create list of only unique values per key
            unique_values = []
            for unique_value in value:
                # remove eager tensor added characters
                if value.dtype == 'string':
                    unique_value = format(unique_value)[2:-1]
                else:
                    unique_value = format(unique_value)

                if len(unique_values) == 0:
                    unique_values.append(unique_value)
                elif unique_value not in unique_values:
                    unique_values.append(unique_value)
                else:
                    continue

            unique_values = list(filter(partial(is_not, None), unique_values))

            cat_col = tf.feature_column.categorical_column_with_vocabulary_list(key=key,
                                                                                vocabulary_list=unique_values)
            categorical_columns.append(tf.feature_column.indicator_column(cat_col))
    return categorical_columns
&lt;/denchmark-code&gt;

model_evaluation():
&lt;denchmark-code&gt;import numpy as np
import tensorflow as tf
import pandas as pd


def my_auc(labels, predictions):
    auc_metric = tf.keras.metrics.AUC(name="my_auc")
    auc_metric.update_state(y_true=labels, y_pred=predictions['logits'])
    return {'auc': auc_metric}


def model_evaluation(features, training_data, validation_data, labels, validation_label_column):
    hidden_layers = len(training_data.__call__().element_spec[0])
    final_layer = len(labels)
    est = tf.estimator.DNNClassifier(feature_columns=features, hidden_units=[hidden_layers, hidden_layers, hidden_layers], n_classes=final_layer, label_vocabulary=labels)
    est = tf.estimator.add_metrics(est, my_auc)

    # Training
    est.train(training_data, max_steps=1000)

    # Validation
    result = est.evaluate(validation_data)

    # Predicition
    prediction = est.predict(validation_data)

    # Results
    print(pd.Series(result))

    for pred_dict, expec in zip(prediction, validation_label_column):
        class_id = pred_dict['class_ids'][0]
        probability = pred_dict['probabilities'][class_id]

        print('Prediction is "{}" ({:.1f}%), expected "{}"'.format(
            labels[class_id], 100 * probability, expec))

    pred_list = list(est.predict(validation_data))
    probs = pd.Series([max(pred['probabilities']) for pred in pred_list])
    probs.plot(kind='hist', bins=100, title='predicted probabilities')

    return

&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='MaUt89' date='2020-04-02T11:20:28Z'>
		&lt;denchmark-link:https://github.com/MaUt89&gt;@MaUt89&lt;/denchmark-link&gt;
, Can you share the input data csv files. Thanks!
		</comment>
		<comment id='4' author='MaUt89' date='2020-04-03T13:22:01Z'>
		Attached you can find test and validation files. They are really small and only show the structure of the learning input I am using. But the program behaves exactly as described above with these two files.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/4427439/InputData.zip&gt;InputData.zip&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='MaUt89' date='2020-04-08T07:27:53Z'>
		Was able to reproduce the issue with tf2.0.
Please find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/767cb65f1b8267b8c5023e393763da5f/untitled492.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='6' author='MaUt89' date='2020-04-13T07:10:27Z'>
		Hey,
I found the issue...obviousley it is not possible to create a AUC curve for DNNClassifiers with more than two possible outputs. Usually you have:
Prediction -&gt; Expected -&gt; Result
0 -&gt; 0 -&gt; TN
0 -&gt; 1 -&gt; FN
1 -&gt; 1 -&gt; TP
1 -&gt; 0 -&gt; FP
If you have more than two possible outcomes the predicted result is either positive or negative...which means that the AUC curve, which relies on the TN, FN, TP, FP input, cannot be calucluated. Please close this issue and thanks for your help up to that point.
		</comment>
		<comment id='7' author='MaUt89' date='2020-04-20T06:24:08Z'>
		as user confirms the issue is resolved moving this to closed status.
		</comment>
		<comment id='8' author='MaUt89' date='2020-04-20T06:24:10Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38018&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38018&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>