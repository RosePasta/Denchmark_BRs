<bug id='34909' author='remydubois' open_date='2019-12-06T17:43:52Z' closed_time='2019-12-10T13:46:06Z'>
	<summary>Non-deterministic access to Random Number Generator in tf.data.Dataset.map with num_parallel_calls &amp;gt; 1</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 2.0.0
Python version: 3.6.7
CUDA/cuDNN version: 10.0
GPU model and memory: Tesla K80

Describe the current behavior
Currently, when a tf.random.experimental.Generator is passed to an operator used in tf.data.Dataset.map with num_parallel_calls &gt; 1, this random number generator is accessed in a non-deterministic order, which makes that the output of the dataset is non-reproducible (see MVCE below).
Describe the expected behavior
The expected behavior is that even when parallelized inside .map, Random Number Generators are called in a deterministic way, so that the data.Dataset overall pipeline gets fully reproducible.
I do not know whether this is a bug, a desired behavior, or an unavoidable side-effect of parallelizing the operator passed to .map, but the overall consequence is that an operator which leverages random operations (like data augmentation, typically) can not be parallelized over tf.data.dataset pipelines if one wants to keep its experiments reproducible.
I am interested in any trick / workaround which would authorize me to keep both high - performance pipelines and experiments reproducibility.
Code to reproduce the issue
See below a simple MVCE: It is self explanatory.
The test_sequential creates a dummy dataset of length 10, maps an op which pulls 1 number from a RNG  (not parallelized). The 10 resulting values are concatenated into a 10-length vector. It then repeats the overall process and compares the vector from the first draft to the one of the second draft.
The test_parallel does exactly the same. However, the op pulling numbers from the RNG is parallelized over 4 threads. When comparing the two 10-length vectors, they are non equal often.
test_parallel is run 10 times which should be enough to highlight some discrepancies (shuffled values) between the two 10-length generated vectors.
The test_sequential shows that the RNG is indeed reproducible when accessed to in a sequential way.
&lt;denchmark-code&gt;import tensorflow as tf
print('TF Version', tf.__version__)
import numpy as np

def draw_samples(num_parallel_calls):

    def mapper(x, rng):
        return rng.uniform(shape=(), minval=0, maxval=10, dtype=tf.int32)

    seed = 12345
    algo = 1
    state = tf.random.experimental.create_rng_state(seed, 1)
    rng = tf.random.experimental.Generator(state=state, alg=algo)

    ds = tf.data.Dataset.from_tensor_slices(tf.range(10)).map(lambda x: mapper(x, rng), num_parallel_calls=num_parallel_calls).batch(10)

    return next(iter(ds)).numpy()

def test_answer_sequential():
    r = [draw_samples(num_parallel_calls=1) for _ in range(2)]
    print('\nSequential')
    print('\n', r[0].tolist(), '\n', r[1].tolist())


def test_answer_parallel():
    r = [draw_samples(num_parallel_calls=4) for _ in range(2)]
    if not np.allclose(*r):
      print('\nNon deterministic access when spread on 4 threads:')
      print('\tFirst draw ', r[0].tolist(), '\n\tSecond draw', r[1].tolist())
    else:
      print('Ok by chance')

test_answer_sequential()
for _ in range(10):
  test_answer_parallel()
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='remydubois' date='2019-12-09T06:32:48Z'>
		&lt;denchmark-link:https://github.com/remydubois&gt;@remydubois&lt;/denchmark-link&gt;

I have tried on colab with TF version 2.0 . Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/f78cf4e81aa1e4719d5c6d098147897d/untitled450.ipynb&gt;here&lt;/denchmark-link&gt;
. Is this the expected behavior?. Thanks!
		</comment>
		<comment id='2' author='remydubois' date='2019-12-09T08:35:36Z'>
		Hello,
Thanks for looking into this. The notebook you linked indeed showed expected behavior, by chance I guess. The extract I linked below tuns 10 times the test_parallel and should highlight discrepancies between the two vectors output when the num_parallel_calls is set to 4
&lt;denchmark-code&gt;import tensorflow as tf
print('TF Version', tf.__version__)
import numpy as np

def draw_samples(num_parallel_calls):

    def mapper(x, rng):
        return rng.uniform(shape=(), minval=0, maxval=10, dtype=tf.int32)

    seed = 12345
    algo = 1
    state = tf.random.experimental.create_rng_state(seed, 1)
    rng = tf.random.experimental.Generator(state=state, alg=algo)

    ds = tf.data.Dataset.from_tensor_slices(tf.range(10)).map(lambda x: mapper(x, rng), num_parallel_calls=num_parallel_calls).batch(10)

    return next(iter(ds)).numpy()

def test_answer_sequential():
    r = [draw_samples(num_parallel_calls=1) for _ in range(2)]
    print('\nSequential')
    print('\n', r[0].tolist(), '\n', r[1].tolist())


def test_answer_parallel():
    r = [draw_samples(num_parallel_calls=4) for _ in range(2)]
    if not np.allclose(*r):
      print('\nNon deterministic access when spread on 4 threads:')
      print('\tFirst draw ', r[0].tolist(), '\n\tSecond draw', r[1].tolist())
    else:
      print('Ok by chance')

test_answer_sequential()
for _ in range(10):
  test_answer_parallel()

&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='remydubois' date='2019-12-10T09:26:25Z'>
		I have tried on colab with TF version 2.0  and was able to reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/e5cf82bc5c057b7a9419d8a9b033dbb3/untitled459.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='4' author='remydubois' date='2019-12-10T13:46:05Z'>
		Actually this issue is closely related to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/13932&gt;#13932&lt;/denchmark-link&gt;
.
Mrry answered clearly: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/13932#issuecomment-341263301&gt;#13932 (comment)&lt;/denchmark-link&gt;

Basically this is not a bug but an unavoidable behavior, which can not be fixed.
Closing the issue thereof,
Thanks for your time
		</comment>
		<comment id='5' author='remydubois' date='2019-12-10T13:46:08Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34909&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34909&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>