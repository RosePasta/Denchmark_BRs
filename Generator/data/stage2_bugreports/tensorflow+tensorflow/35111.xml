<bug id='35111' author='shazhongcheng' open_date='2019-12-14T08:15:12Z' closed_time='2020-01-07T03:36:07Z'>
	<summary>TypeError: An op outside of the function building code is being passed a "Graph" tensor. In my RNNCell Test.</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): Source
TensorFlow version (use command below): Tensorflow2.0-GPU
Python version: python3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: CUDA10 and cudnn
GPU model and memory: GTX1060 6G

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-165-d59a3afffc48&gt; in &lt;module&gt;
      3 for epoch in range(EPOCHS):
      4     for x, y in db_train:
----&gt; 5         train_step(x, y)
      6 
      7     for test_x, test_y in db_test:

c:\users\sha\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow_core\python\eager\def_function.py in __call__(self, *args, **kwds)
    455 
    456     tracing_count = self._get_tracing_count()
--&gt; 457     result = self._call(*args, **kwds)
    458     if tracing_count == self._get_tracing_count():
    459       self._call_counter.called_without_tracing()

c:\users\sha\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow_core\python\eager\def_function.py in _call(self, *args, **kwds)
    518         # Lifting succeeded, so variables are initialized and we can run the
    519         # stateless function.
--&gt; 520         return self._stateless_fn(*args, **kwds)
    521     else:
    522       canon_args, canon_kwds = \

c:\users\sha\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow_core\python\eager\function.py in __call__(self, *args, **kwargs)
   1821     """Calls a graph function specialized to the inputs."""
   1822     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
-&gt; 1823     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   1824 
   1825   @property

c:\users\sha\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow_core\python\eager\function.py in _filtered_call(self, args, kwargs)
   1139          if isinstance(t, (ops.Tensor,
   1140                            resource_variable_ops.BaseResourceVariable))),
-&gt; 1141         self.captured_inputs)
   1142 
   1143   def _call_flat(self, args, captured_inputs, cancellation_manager=None):

c:\users\sha\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow_core\python\eager\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1222     if executing_eagerly:
   1223       flat_outputs = forward_function.call(
-&gt; 1224           ctx, args, cancellation_manager=cancellation_manager)
   1225     else:
   1226       gradient_name = self._delayed_rewrite_functions.register()

c:\users\sha\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow_core\python\eager\function.py in call(self, ctx, args, cancellation_manager)
    509               inputs=args,
    510               attrs=("executor_type", executor_type, "config_proto", config),
--&gt; 511               ctx=ctx)
    512         else:
    513           outputs = execute.execute_with_cancellation(

c:\users\sha\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow_core\python\eager\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     74           "Inputs to eager execution function cannot be Keras symbolic "
     75           "tensors, but found {}".format(keras_symbolic_tensors))
---&gt; 76     raise e
     77   # pylint: enable=protected-access
     78   return tensors

c:\users\sha\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow_core\python\eager\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     59     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,
     60                                                op_name, inputs, attrs,
---&gt; 61                                                num_outputs)
     62   except core._NotOkStatusException as e:
     63     if name is not None:

TypeError: An op outside of the function building code is being passed
a "Graph" tensor. It is possible to have Graph tensors
leak out of the function building context by including a
tf.init_scope in your function building code.
For example, the following function will fail:
  @tf.function
  def has_init_scope():
    my_constant = tf.constant(1.)
    with tf.init_scope():
      added = my_constant * 2
The graph tensor has name: my_rnn_cell_12/simple_rnn_cell_36/ones_like:0
Describe the expected behavior
It should train fluently.
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
This is the error on &lt;denchmark-link:https://colab.research.google.com/drive/1bLig48fgBRkfihxkU7Sn3bklmVyrrSH-#scrollTo=-RS_curvbwVe&gt;Google Cloab&lt;/denchmark-link&gt;

Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Befor I try experts method with @tf.function，I try model.fit()，but it also run error. After I try to find the solutions from the issues, I find that  can deal this error. But it run very slow. Does it have much better method to deal it?&lt;denchmark-link:https://colab.research.google.com/drive/19waivwxYTpC6S_r5UtPItuks0nW5wufE#scrollTo=HGIY-zFjdnVJ&gt;Google Cloab&lt;/denchmark-link&gt;

Thank you!
	</description>
	<comments>
		<comment id='1' author='shazhongcheng' date='2019-12-16T08:36:55Z'>
		&lt;denchmark-link:https://github.com/shazhongcheng&gt;@shazhongcheng&lt;/denchmark-link&gt;
, Could you provide a access to view the Google colab link. Thanks!
		</comment>
		<comment id='2' author='shazhongcheng' date='2019-12-16T08:39:38Z'>
		&lt;denchmark-link:https://user-images.githubusercontent.com/15049049/70891719-c7209680-2022-11ea-8179-6fd7b4b29f2f.png&gt;&lt;/denchmark-link&gt;

Although I had provided， I paste it following：
&lt;denchmark-link:https://colab.research.google.com/drive/1bLig48fgBRkfihxkU7Sn3bklmVyrrSH-#scrollTo=-RS_curvbwVe&gt;https://colab.research.google.com/drive/1bLig48fgBRkfihxkU7Sn3bklmVyrrSH-#scrollTo=-RS_curvbwVe&lt;/denchmark-link&gt;

&lt;denchmark-link:https://colab.research.google.com/drive/19waivwxYTpC6S_r5UtPItuks0nW5wufE#scrollTo=HGIY-zFjdnVJ&gt;https://colab.research.google.com/drive/19waivwxYTpC6S_r5UtPItuks0nW5wufE#scrollTo=HGIY-zFjdnVJ&lt;/denchmark-link&gt;

Can you look it？
Thank you！
		</comment>
		<comment id='3' author='shazhongcheng' date='2019-12-17T09:07:54Z'>
		&lt;denchmark-link:https://github.com/shazhongcheng&gt;@shazhongcheng&lt;/denchmark-link&gt;
, Still I don't have access to view the Colab links. Could you provide the gist of the colab links. Thanks!
		</comment>
		<comment id='4' author='shazhongcheng' date='2019-12-17T09:15:01Z'>
		&lt;denchmark-link:https://colab.research.google.com/drive/1bLig48fgBRkfihxkU7Sn3bklmVyrrSH-&gt;https://colab.research.google.com/drive/1bLig48fgBRkfihxkU7Sn3bklmVyrrSH-&lt;/denchmark-link&gt;

&lt;denchmark-link:https://colab.research.google.com/drive/19waivwxYTpC6S_r5UtPItuks0nW5wufE&gt;https://colab.research.google.com/drive/19waivwxYTpC6S_r5UtPItuks0nW5wufE&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='shazhongcheng' date='2019-12-17T09:17:39Z'>
		
@shazhongcheng, Still I don't have access to view the Colab links. Could you provide the gist of the colab links. Thanks!

sorry , I didn`t share it! I had shared it now! Can you see it now?
		</comment>
		<comment id='6' author='shazhongcheng' date='2019-12-19T05:57:30Z'>
		&lt;denchmark-link:https://github.com/shazhongcheng&gt;@shazhongcheng&lt;/denchmark-link&gt;
, I could see it now. Thanks!
		</comment>
		<comment id='7' author='shazhongcheng' date='2020-01-07T03:36:06Z'>
		Thanks for the reporting this issue.
The cause of the issue is a combination of directly using RNN cells with dropout and not resetting the dropout mask.
In RNN cell, we cache the dropout mask to achieve the variational dropout (same mask within the batch for different timesteps). Between the boundary of batches, the cached dropout mask need to be reset, and we did that for user in the RNN layers. When user directly using cell, the reset action need to be done by users. Otherwise, from TF runtimes perspective, it is trying to access a cached tensor that belongs to different graph.
You can quickly add following lines in your call() function, and the code should run fine.
&lt;denchmark-code&gt;self.rnn_cell0.reset_dropout_mask()
self.rnn_cell1.reset_dropout_mask()
for word in tf.unstack(x, axis=1): # word: [b, 100]
   ......
&lt;/denchmark-code&gt;

Or if you disable the dropout for the rnn cell, it will work as well.
		</comment>
		<comment id='8' author='shazhongcheng' date='2020-01-07T03:36:09Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35111&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35111&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='shazhongcheng' date='2020-01-07T05:30:04Z'>
		
Thanks for the reporting this issue.
The cause of the issue is a combination of directly using RNN cells with dropout and not resetting the dropout mask.
In RNN cell, we cache the dropout mask to achieve the variational dropout (same mask within the batch for different timesteps). Between the boundary of batches, the cached dropout mask need to be reset, and we did that for user in the RNN layers. When user directly using cell, the reset action need to be done by users. Otherwise, from TF runtimes perspective, it is trying to access a cached tensor that belongs to different graph.
You can quickly add following lines in your call() function, and the code should run fine.
self.rnn_cell0.reset_dropout_mask()
self.rnn_cell1.reset_dropout_mask()
for word in tf.unstack(x, axis=1): # word: [b, 100]
   ......

Or if you disable the dropout for the rnn cell, it will work as well.

OK OK，I have fixed it！ Thank you very much!!!!
		</comment>
	</comments>
</bug>