<bug id='33676' author='DoumanAsh' open_date='2019-10-24T09:47:41Z' closed_time='2019-11-27T19:47:18Z'>
	<summary>TF lite GPU delegates should use the same linker script as main library</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android/Ubuntu
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): Source
TensorFlow version: 1.15.0
Python version: 3.6.8
Installed using virtualenv? pip? conda?: ip
Bazel version (if compiling from source): 0.24.1
GCC/Compiler version (if compiling from source): NDK r17b clang
CUDA/cuDNN version: -
GPU model and memory: -

Describe the problem
Currently libtensorflowlite.so is build with "-Wl,--version-script,$(location //tensorflow/lite:tflite_version_script.lds)"
The same is not done for gpu delegates.
Is there any particular reason for that?
I believe it should be applied to gpu delegate libraries too, as currently you'll need
I don't believe any symbol other than tflite related symbols are necessary.
If visibility=hidden by default is ok (since I think we only need C API from GPU Delegate), then shouldn't it be default flag for the library?
Inconsistent symbol hiding makes it confusing when building both main library and gpu delegates
Would it be acceptable to hide symbols by default or use linker script as in main tflite library?
Provide the exact sequence of commands / steps that you executed before running into the problem
Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='DoumanAsh' date='2019-11-08T18:25:29Z'>
		&lt;denchmark-link:https://github.com/DoumanAsh&gt;@DoumanAsh&lt;/denchmark-link&gt;

Sorry for the late reply; I've been out on a conference.
Hm, I'm not sure what that flag does, and re: your question of:

Is there any particular reason for that?

The people owning TFLite and who delivers the TFLite GPU are different set of people with the latter (I belong here) being more agnostic of what the proper way is :p
Please feel free to send a PR to me and &lt;denchmark-link:https://github.com/jdduke&gt;@jdduke&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='DoumanAsh' date='2019-11-08T18:56:59Z'>
		Hi &lt;denchmark-link:https://github.com/DoumanAsh&gt;@DoumanAsh&lt;/denchmark-link&gt;
, thanks for flagging, we should absolutely be consistent. If you want to propose a PR, feel free to, otherwise I can take a look.
		</comment>
		<comment id='3' author='DoumanAsh' date='2019-11-08T19:19:06Z'>
		Looking over at the delegate API, I see only C API (which is marked visible in code).
So I think the simplest approach would be is to by default build with hidden visibility.
Which doesn't require any extra linking script (this is also what we do internally when we build delegate)
So I'd suggest it as PR with visibility hidden by default for now as all vital APIs are marked with  which resolves to corresponding attribute.
&lt;denchmark-link:https://github.com/jdduke&gt;@jdduke&lt;/denchmark-link&gt;
 does it sound good to you?
		</comment>
		<comment id='4' author='DoumanAsh' date='2019-11-08T19:36:44Z'>
		As a side question.
I noticed that 1.15 brought a new GPU Delegate.
Are there any plans for which is going to be a final one?
		</comment>
		<comment id='5' author='DoumanAsh' date='2019-11-08T19:46:44Z'>
		The new GPU delegate you are referring to probably has the logic of "try to use OpenCL if available,  fallback to OpenGL otherwise".  I'm not 100% certain whether we will kill off the "old" OpenGL delegate, but chances are high.
		</comment>
		<comment id='6' author='DoumanAsh' date='2019-11-08T19:47:13Z'>
		Hi &lt;denchmark-link:https://github.com/DoumanAsh&gt;@DoumanAsh&lt;/denchmark-link&gt;
, the V2 delegate is going to be the default going forward. The pre-compiled GPU delegate (on JCenter/Maven) has already been updated to use this variant, as has our benchmark tooling. We're preparing a blog post, and will likely deprecate the "V1" API at some point.
		</comment>
		<comment id='7' author='DoumanAsh' date='2019-11-08T19:58:07Z'>
		Good to know, I will be shifting our code base to use it then
		</comment>
		<comment id='8' author='DoumanAsh' date='2019-11-27T19:47:20Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33676&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33676&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>