<bug id='38922' author='zacwellmer' open_date='2020-04-26T21:40:17Z' closed_time='2020-04-29T16:31:37Z'>
	<summary>LSTMCell Dropout mask is reset at every call</summary>
	<description>
System information

TensorFlow version (use command below):2.2.0-rc3

Describe the current behavior
The dropout mask is being reset at every call
Describe the expected behavior
The dropout mask should be fixed unless reset is called
Standalone code to reproduce the issue
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Embedding
from tensorflow.keras.layers import LSTM
from tensorflow.keras.datasets import imdb

max_features = 20000
maxlen = 80

(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)
x_train = sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = sequence.pad_sequences(x_test, maxlen=maxlen)

model = Sequential()
model.add(Embedding(max_features, 128))
model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

x_sample = x_train[:1]
a = model(x_sample, training=True)
dp_mask1 = model.layers[1].cell.get_dropout_mask_for_cell(x_sample, training=True, count=4)
rec_dp_mask1 = model.layers[1].cell.get_dropout_mask_for_cell(x_sample, training=True, count=4)

b = model(x_sample, training=True)
dp_mask2 = model.layers[1].cell.get_dropout_mask_for_cell(x_sample, training=True, count=4)
rec_dp_mask2 = model.layers[1].cell.get_dropout_mask_for_cell(x_sample, training=True, count=4)

# check if masks are the same after call
print(np.all([np.all(dp_mask1[i] == dp_mask2[i]) for i in range(len(dp_mask1))]))
print(np.all([np.all(rec_dp_mask1[i] == rec_dp_mask2[i]) for i in range(len(rec_dp_mask1))]))
Jupyter Notebook example &lt;denchmark-link:https://colab.research.google.com/gist/zacwellmer/f56a9e1959e687d03ee89069d78af683/untitled4.ipynb&gt;here&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='zacwellmer' date='2020-04-27T17:04:58Z'>
		Was able to reproduce the issue with TF v2.1, &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/0b74be016ac78170739b6795632135b4/38922-2-2.ipynb&gt;TF v2.2.0rc3&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/404923b89b3d15d7fe3d531ed5f78838/38922-tf-nightly.ipynb&gt;TF-nightly&lt;/denchmark-link&gt;
. Please find the attached gist. Thanks!
		</comment>
		<comment id='2' author='zacwellmer' date='2020-04-28T05:46:57Z'>
		Thanks for reporting the issue. I think resetting the dropout mask for every call is the intended behavior. This is also aligned with the dropout layer in keras. Could u point me to any of the API doc that makes you think the dropout mask is caches across calls?
&lt;denchmark-code&gt;import tensorflow.keras as keras
import numpy as np

dropout = keras.layers.Dropout(0.2)

a = np.arange(10, dtype=np.float32)
d1 = dropout(a, training=True)
d2 = dropout(a, training=True)
print(d1)
print(d2)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='zacwellmer' date='2020-04-28T17:48:24Z'>
		Hi &lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;

The LSTMCell's docs on &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell#reset_dropout_mask&gt;reset_dropout_mask&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell#reset_recurrent_dropout_mask&gt;reset_recurrent_dropout_mask&lt;/denchmark-link&gt;
 led me to believe that the mask was only reset when explicitly called (example: the docs exaggeration on it being used in the call function).
At the very least if it's decided by default to reset the dropout mask at every step there should be a way to specify the mask. Similar to how we can specify the &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN&gt;RNN states via initial_states&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='zacwellmer' date='2020-04-29T16:31:37Z'>
		I see.
The reset_*_mask is there in case user want to use LSTMCell directly. They are called with in LSTM  layer at the start of each batch, but the same mask is used for all the timesteps within the batch. This will ensure same feature get dropped within the sequence (eg same word is dropped in the sentence).
The mask is by default generated by keras framework, if you want to control the mask by yourself, you can call the LSTM or LSTMCell with
&lt;denchmark-code&gt;lstm = tf.keras.layers.LSTM(10)
lstm(inputs, mask=self_managed_mask)
&lt;/denchmark-code&gt;

See more details about mask in &lt;denchmark-link:https://www.tensorflow.org/guide/keras/masking_and_padding#passing_mask_tensors_directly_to_layers&gt;https://www.tensorflow.org/guide/keras/masking_and_padding#passing_mask_tensors_directly_to_layers&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='zacwellmer' date='2020-04-29T16:31:39Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38922&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38922&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='zacwellmer' date='2020-04-30T03:18:26Z'>
		That mask argument does not fully support the use case of rnn dropout with a specific mask (ex: the recurrent dropout mask)
		</comment>
		<comment id='7' author='zacwellmer' date='2020-04-30T04:59:46Z'>
		yes, it doesn't support the recurrent dropout mask, and it is aligned with we do in Dropout. The mask suppose to be a randomly generated filter in this case, and we didn't consider user injection as a common API use case.
		</comment>
	</comments>
</bug>