<bug id='19838' author='yegord' open_date='2018-06-07T16:02:30Z' closed_time='2019-07-10T21:56:45Z'>
	<summary>optimize_for_inference_lib.optimize_for_inference produces an invalid graph</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04
TensorFlow installed from (source or binary):
Source.
TensorFlow version (use command below):
1.8.0
Python version:
2.7.12
Bazel version (if compiling from source):
0.13.0
GCC/Compiler version (if compiling from source):
gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.9)
CUDA/cuDNN version:
9.0.176/7.0.5.15
GPU model and memory:
1080 Ti
Exact command to reproduce:

&lt;denchmark-code&gt;cat &gt; test.py &lt;&lt;EOF &amp;&amp; python test.py
import tensorflow as tf

from collections import namedtuple
from tensorflow.python.tools import optimize_for_inference_lib


def main():
    with tf.Graph().as_default(), tf.Session() as session:
        input = tf.placeholder(shape=[10], dtype=tf.float32)
        output = top_k(input)

        graph_def = session.graph.as_graph_def()

    input_nodes = [input]
    output_nodes = [output.values, output.indices]

    graph_def = tf.graph_util.convert_variables_to_constants(
        session, graph_def, [_get_node_name(t) for t in output_nodes]
    )

    with tf.Graph().as_default():
        tf.import_graph_def(graph_def)  # OK

    graph_def = optimize_for_inference_lib.optimize_for_inference(
        input_graph_def=graph_def,
        input_node_names=[_get_node_name(t) for t in input_nodes],
        output_node_names=[_get_node_name(t) for t in output_nodes],
        placeholder_type_enum=[node.dtype.as_datatype_enum for node in input_nodes]
    )

    with tf.Graph().as_default():
        tf.import_graph_def(graph_def)  # ERROR


TopKResult = namedtuple('TopKResult', ['values', 'indices'])


def top_k(input, k=1, sorted=True, name=None):
    """
    A version of tf.nn.top_k tolerant to k == 0 and k &lt; tf.shape(input)[-1].
    """
    k = tf.minimum(k, tf.shape(input)[-1])

    return tf.cond(
        tf.equal(k, 0),
        lambda: TopKResult(
            values=tf.zeros(
                shape=tf.concat([tf.shape(input)[:-1], [0]], axis=0),
                dtype=input.dtype
            ),
            indices=tf.zeros(
                shape=tf.concat([tf.shape(input)[:-1], [0]], axis=0),
                dtype=tf.int32
            )
        ),
        lambda: TopKResult(**tf.nn.top_k(input, k, sorted, name)._asdict())
    )


def _get_node_name(tensor):
    assert tensor.name.endswith(':0')
    return tensor.name[:-len(':0')]


if __name__ == '__main__':
    main()
EOF
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

optimize_for_inference_lib.optimize_for_inference produces an invalid graph for the graph generated by the above script. The returned GraphDef cannot be imported:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "test.py", line 66, in &lt;module&gt;
    main()
  File "test.py", line 32, in main
    tf.import_graph_def(graph_def)  # ERROR
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 432, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py", line 493, in import_graph_def
    raise ValueError(str(e))
ValueError: NodeDef expected inputs '' do not match 1 inputs specified; Op&lt;name=Const; signature= -&gt; output:dtype; attr=value:tensor; attr=dtype:type&gt;; NodeDef: import/cond/zeros_1/Const = Const[dtype=DT_INT32, value=Tensor&lt;type: int32 shape: [] values: 0&gt;](import/cond/Switch:1)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='yegord' date='2018-08-16T11:03:09Z'>
		I met the same problem when i use the .pb produced by quantize_graph
		</comment>
		<comment id='2' author='yegord' date='2018-08-30T14:25:06Z'>
		same problem with latest tensorflow
		</comment>
		<comment id='3' author='yegord' date='2018-09-14T04:02:08Z'>
		optimized it using optimize_for_inference (works error)
&lt;denchmark-code&gt;${ROOT}/bazel-bin/tensorflow/python/tools/optimize_for_inference  \
--input=$graph \
--output=optimized_for_inference_graph.pb \
--frozen_graph=True \
--input_names="input" \
--output_names="softmax_output"

$ ./summarize_graph.sh optimized_for_inference_graph.pb
Found 1 possible inputs: (name=input, type=float(1), shape=None)
No variables spotted.
Found 1 possible outputs: (name=softmax_output, op=Softmax)
Found 3156821 (3.16M) const parameters, 0 (0) variable parameters, and 0 control_edges
Op types used: 221 Const, 28 Mul, 27 Reshape, 26 Pack, 25 Add, 18 Sub, 14 ConcatV2, 12 Range, 10 StridedSlice, 10 Cast, 8 Transpose, 8 FloorDiv, 8 GatherV2, 8 Less, 7 Enter, 7 RealDiv, 6 Shape, 6 Prod, 6 MatMul, 5 BiasAdd, 5 Fill, 4 Log, 4 Maximum, 4 ExpandDims, 3 ListDiff, 3 Conv2D, 3 Merge, 3 Select, 3 NextIteration, 3 GreaterEqual, 3 Switch, 2 SplitV, 2 Softmax, 2 Size, 2 Rsqrt, 2 FusedBatchNorm, 2 Pad, 2 Neg, 2 LinSpace, 2 TensorArrayV3, 2 Mean, 2 MaxPool, 1 TensorArrayScatterV3, 1 TensorArrayReadV3, 1 TensorArrayGatherV3, 1 TensorArraySizeV3, 1 Sum, 1 TensorArrayWriteV3, 1 Cos, 1 ComplexAbs, 1 Square, 1 Split, 1 Sigmoid, 1 Exit, 1 FloorMod, 1 RandomStandardNormal, 1 RFFT, 1 Placeholder, 1 PadV2, 1 Minimum, 1 LoopCond, 1 LogicalAnd
To use with tensorflow/tools/benchmark:benchmark_model try these arguments:
bazel run tensorflow/tools/benchmark:benchmark_model -- --graph=optimized_for_inference_graph.pb --show_flops --input_layer=input --input_layer_type=float --input_layer_shape= --output_layer=softmax_output

 ./summarize_graph.sh frozen_graph.pb
Found 1 possible inputs: (name=input, type=float(1), shape=[?,480000])
No variables spotted.
Found 1 possible outputs: (name=softmax_output, op=Softmax)
Found 3156821 (3.16M) const parameters, 0 (0) variable parameters, and 129 control_edges
Op types used: 221 Const, 32 Identity, 28 Mul, 27 Reshape, 26 Pack, 25 Add, 18 Sub, 14 ConcatV2, 12 Range, 10 StridedSlice, 10 Cast, 8 FloorDiv, 8 GatherV2, 8 Transpose, 8 Less, 7 Enter, 7 RealDiv, 6 Shape, 6 Prod, 6 MatMul, 5 BiasAdd, 5 Fill, 4 Log, 4 Maximum, 4 ExpandDims, 3 ListDiff, 3 Conv2D, 3 Merge, 3 Select, 3 NextIteration, 3 GreaterEqual, 3 Switch, 2 SplitV, 2 Softmax, 2 Size, 2 Rsqrt, 2 FusedBatchNorm, 2 Pad, 2 Neg, 2 LinSpace, 2 TensorArrayV3, 2 Mean, 2 MaxPool, 1 TensorArrayScatterV3, 1 TensorArrayReadV3, 1 TensorArrayGatherV3, 1 TensorArraySizeV3, 1 Sum, 1 TensorArrayWriteV3, 1 Cos, 1 ComplexAbs, 1 Square, 1 Split, 1 Sigmoid, 1 Exit, 1 FloorMod, 1 RandomStandardNormal, 1 RFFT, 1 Placeholder, 1 PadV2, 1 Minimum, 1 LoopCond, 1 LogicalAnd
To use with tensorflow/tools/benchmark:benchmark_model try these arguments:
&lt;/denchmark-code&gt;

After using optimized_for_inference bin, the input shape changed. tf version 1.9
		</comment>
		<comment id='4' author='yegord' date='2018-10-26T08:22:27Z'>
		I observed a similar problem. The following code works perfectly well with use_optimize_for_inference = False but gives the error
&lt;denchmark-code&gt;tensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef expected inputs '' do not match 1 inputs specified; Op&lt;name=Const; signature= -&gt; output:dtype; attr=value:tensor; attr=dtype:type&gt;; NodeDef: {{node optimized/cond/add/y}} = Const[dtype=DT_FLOAT, value=Tensor&lt;type: float shape: [] values: 1&gt;](optimized/cond/Switch:1)
&lt;/denchmark-code&gt;

if I use the optimize for inference routine. The code:
&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow.python.tools import optimize_for_inference_lib
from tensorflow.python.tools import freeze_graph


use_optimize_for_inference = True

# Set up a simple test graph
a = tf.Variable(tf.ones(3, dtype=tf.float32))
x = tf.placeholder(tf.float32, [3], name='x')
switch = tf.placeholder(tf.bool, name='switch')

y = a * x
y = tf.cond(switch, lambda: y + 1.0, lambda: y)
y = tf.identity(y, 'result')

saver = tf.train.Saver()
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    # Save the graph to disk
    tf.train.write_graph(sess.graph.as_graph_def(), '.', 'model.pbtxt', as_text=True)
    saver.save(sess, './model.ckpt')

    # Freeze the trained model
    output_graph = 'frozen_model.pb'
    freeze_graph.freeze_graph('./model.pbtxt', '', False, './model.ckpt',
                              'result', 'save/restore_all', 'save/Const:0', output_graph, True, '')

    # Reload the frozen graph from disk
    frozen_graph = tf.GraphDef()
    with tf.gfile.Open(output_graph, "rb") as f:
        frozen_graph.ParseFromString(f.read())

    # optimize for inference
    input_names = ['x', 'switch']
    output_names = ['result']
    new_graph = optimize_for_inference_lib.optimize_for_inference(
        frozen_graph, input_names, output_names, [tf.float32.as_datatype_enum, tf.bool.as_datatype_enum])

    # Write the optimized graph to disk
    output_graph_inference = 'frozen_model_inference.pb'
    with tf.gfile.GFile(output_graph_inference, "wb") as f:
        f.write(new_graph.SerializeToString())

    # Attempt loading the frozen graph: This fails if we optimize the graph for inference
    print('Loading the saved graph...')
    optimized_graph_def = tf.GraphDef()
    with tf.gfile.GFile(output_graph_inference if use_optimize_for_inference else output_graph, "rb") as f:
        optimized_graph_def.ParseFromString(f.read())
    with tf.Graph().as_default() as graph:
        tf.import_graph_def(optimized_graph_def, name="optimized")


&lt;/denchmark-code&gt;

The problem is either related to the use of the boolean variable or the conditional. I observed it both in Tensorflow 1.11.0 (Python 3.7) and 1.8.0 (Python 3.6). For now, my solution is to just not use the optimize for inference script, as it doesn't benefit me that much in my use....
		</comment>
		<comment id='5' author='yegord' date='2019-01-03T08:06:53Z'>
		
I met the same problem when i use the .pb produced by quantize_graph

Hi kasyoukin,
I also met the same issue when use the .pb produced by quantize_graph.
Did you find the solution?
		</comment>
		<comment id='6' author='yegord' date='2019-04-11T06:52:17Z'>
		I met exactly the same problem and it seems like that the problem is caused by tf.cond.
		</comment>
		<comment id='7' author='yegord' date='2019-04-21T04:27:16Z'>
		Same problem, looks like the dropout node. I was also trying to use an optimized  graph. It looks like I can used an unoptimized graph just fine.
&lt;denchmark-code&gt;ValueError: NodeDef expected inputs '' do not match 1 inputs specified; Op&lt;name=Const; signature= -&gt; output:dtype; attr=value:tensor; attr=dtype:type&gt;; NodeDef: prefix/dense_0/Dropout/cond/dropout/keep_prob = Const[dtype=DT_FLOAT, value=Tensor&lt;type: float shape: [] values: 0.2&gt;](prefix/dense_0/Dropout/cond/Switch:1)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='yegord' date='2019-04-26T16:24:56Z'>
		I could reproduce the issue with TF1.13.1. Thanks!
Here is the error log
&lt;denchmark-code&gt;INFO:tensorflow:Restoring parameters from ./model.ckpt
INFO:tensorflow:Froze 1 variables.
INFO:tensorflow:Converted 1 variables to const ops.
Loading the saved graph...
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/importer.py in import_graph_def(graph_def, input_map, return_elements, name, op_dict, producer_op_list)
    425         results = c_api.TF_GraphImportGraphDefWithResults(
--&gt; 426             graph._c_graph, serialized, options)  # pylint: disable=protected-access
    427         results = c_api_util.ScopedTFImportGraphDefResults(results)

InvalidArgumentError: NodeDef expected inputs '' do not match 1 inputs specified; Op&lt;name=Const; signature= -&gt; output:dtype; attr=value:tensor; attr=dtype:type&gt;; NodeDef: {{node optimized/cond/add/y}}

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
2 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/importer.py in import_graph_def(graph_def, input_map, return_elements, name, op_dict, producer_op_list)
    428       except errors.InvalidArgumentError as e:
    429         # Convert to ValueError for backwards compatibility.
--&gt; 430         raise ValueError(str(e))
    431 
    432     # Create _DefinedFunctions for any imported functions.

ValueError: NodeDef expected inputs '' do not match 1 inputs specified; Op&lt;name=Const; signature= -&gt; output:dtype; attr=value:tensor; attr=dtype:type&gt;; NodeDef: {{node optimized/cond/add/y}}
&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='yegord' date='2019-05-09T10:02:32Z'>
		In my opinion  it caused by tf.cond(),but without any solutions,unless remove this node.
		</comment>
		<comment id='10' author='yegord' date='2019-06-21T21:56:29Z'>
		Facing the a similar issue when using LSTM in my model. Any leads so far?
ValueError: NodeDef expected inputs '' do not match 1 inputs specified; Op&lt;name=Const; signature= -&gt; output:dtype; attr=value:tensor; attr=dtype:type&gt;; NodeDef: {{node lstm_1/while/add/y}} = Const&lt;denchmark-link:lstm_1/while/Switch:1&gt;_output_shapes=[[]], dtype=DT_INT32, value=Tensor&lt;type: int32 shape: [] values: 1&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='yegord' date='2019-07-10T02:13:20Z'>
		I met the same problem("do not match 1 inputs specified...*") yesterday. The reason seem to be related "tf.graph_util.remove_training_nodes"(or this function wrapped in "tensorflow.python.tools").
		</comment>
		<comment id='12' author='yegord' date='2019-07-10T21:56:45Z'>
		I am unable to repro this in 1.14 for the examples posted by &lt;denchmark-link:https://github.com/yegord&gt;@yegord&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/dvicini&gt;@dvicini&lt;/denchmark-link&gt;
. Please reopen with a repro if this is still not working for you.
		</comment>
		<comment id='13' author='yegord' date='2019-07-10T21:56:46Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=19838&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=19838&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='14' author='yegord' date='2019-07-23T18:15:28Z'>
		
I am unable to repro this in 1.14 for the examples posted by @yegord and @dvicini. Please reopen with a repro if this is still not working for you.

What about Tf 1.13.1  ?
		</comment>
		<comment id='15' author='yegord' date='2019-07-29T07:53:34Z'>
		entity_tagging/bidirectional_rnn/fw/fw/cond/fw/multi_rnn_cell/cell_0/basic_lstm_cell/MatMul/Switch' expects to be colocated with unknown node 'entity_tagging/bidirectional_rnn/fw/multi_rnn_cell/cell_0/basic_lstm_cell/kernel/read'
when use quantize tools cause this problem
solved this problem by delete graph_util.remove_training_nodes function
		</comment>
		<comment id='16' author='yegord' date='2019-07-29T16:06:12Z'>
		reproduct the it:

train a model by tf/models/official/transformer
do freeze_graph for the model, which created at step 1
try to import the graph_def, which created at step 2

&lt;denchmark-code&gt;# step 3 code
input_tensor = tf.placeholder(tf.int64, shape=(1, None), name='input_tensor')
with tf.gfile.GFile(model_file, 'rb') as f:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(f.read())
    with tf.Session() as sess:
        tf.import_graph_def(graph_def,{"input_tensor":input_tensor},name="")
&lt;/denchmark-code&gt;

		</comment>
		<comment id='17' author='yegord' date='2019-08-05T14:30:11Z'>
		I'm having the same issue. For me it seems to occur in tf.keras.layers.BatchNormalization. When I pass the bool training=is_training to the call operator the error dissapears. Any idea on why this happens?
		</comment>
		<comment id='18' author='yegord' date='2019-10-11T02:37:04Z'>
		use tensorflow 1.14.0
		</comment>
		<comment id='19' author='yegord' date='2019-11-29T12:30:51Z'>
		Same issue with TF 1.13.1 caused by batch_norm.
ValueError: NodeDef expected inputs '' do not match 1 inputs specified; Op&lt;name=Const; signature= -&gt; output:dtype; attr=value:tensor; attr=dtype:type&gt;; NodeDef: {{node import/model/conv1/batch_norm/cond/Const}}
Disappear with TF 1.14.0
		</comment>
	</comments>
</bug>