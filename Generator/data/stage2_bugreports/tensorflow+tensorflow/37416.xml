<bug id='37416' author='LeechMuse' open_date='2020-03-07T17:45:17Z' closed_time='2020-03-13T17:30:45Z'>
	<summary>Precision at top_k does not compute the precision *on average* as stated in the API</summary>
	<description>
Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.
The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: &lt;denchmark-link:https://www.tensorflow.org/community/contribute/docs&gt;https://www.tensorflow.org/community/contribute/docs&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;URL(s) with the issue:&lt;/denchmark-h&gt;

Please provide a link to the documentation entry, for example:
&lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision&gt;https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;Description of issue (what needs changing):&lt;/denchmark-h&gt;

This method does not compute the precision on average when top_k is set. This could lead to bad evaluations, especially when the sample_weight is set and used as counts.
&lt;denchmark-h:h3&gt;Clear description&lt;/denchmark-h&gt;

To see the issue, it's enough to run
&lt;denchmark-code&gt;m = tf.keras.metrics.Precision(top_k=2)
m.update_state([0, 0, 1, 1], [1, 1, 1., 1.])

print('Final result: ', m.result().numpy()) # Returns 0 but should return 0.5
&lt;/denchmark-code&gt;

It always computes the precision according to the given order and returns 0. However it should return 0.5 if it's on average.
	</description>
	<comments>
		<comment id='1' author='LeechMuse' date='2020-03-09T06:58:53Z'>
		Was able to reproduce the issue with TF 2.1 and TF-nightly. Please find the Gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/amahendrakar/835c0ddf7b10b0c134967080939efffe/37416.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='LeechMuse' date='2020-03-09T08:31:53Z'>
		&lt;denchmark-link:https://github.com/LeechMuse&gt;@LeechMuse&lt;/denchmark-link&gt;

In the code provided by you  = [0, 0, 1, 1],  = [1, 1, 1., 1.] , top_k = top-k classes to be considered to calculate precision.
If top_k = none it consider all the classes.
Precision is calculated by TP/TP+FP. So in the case top_k =1 it considers the first value only to calculate precision (ie y_true = [0]  and y_pred = [1]) so precision is zero.
Similarly top_k=2 considers first 2 values y_true = [0, 0] and y_pred = [1, 1] so precision is zero.
If top_k=3 then y_true = [0, 0,1] and y_pred = [1,1,1.] so precision is 0.33.
In this example top_k =4 or None is same because it uses all the values. In that case  = [0, 0, 1, 1],  = [1, 1, 1., 1.]  so precision will be 0.5. Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/af71a7be1c9cc0e945f79536d540fbb3/untitled710.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='3' author='LeechMuse' date='2020-03-09T16:30:42Z'>
		Sure, this is true, but in that case it's not on average, and instead you should say precision at top_k (ties are broken by the order in which the values are provided).
If it was on average, it should be expected to break ties according to expectation. So in the case of top_k=1, it should still be 0.5, because under random assignment of order (given that all y_hat are equal), 0.5 of the permutations will have 1 in the first element in the array.
		</comment>
		<comment id='4' author='LeechMuse' date='2020-03-12T08:35:25Z'>
		
@LeechMuse
In the code provided by you y_true = [0, 0, 1, 1], y_pred = [1, 1, 1., 1.] , top_k = top-k classes to be considered to calculate precision.
If top_k = none it consider all the classes.
Precision is calculated by TP/TP+FP. So in the case top_k =1 it considers the first value only to calculate precision (ie y_true = [0] and y_pred = [1]) so precision is zero.
Similarly top_k=2 considers first 2 values y_true = [0, 0] and y_pred = [1, 1] so precision is zero.
If top_k=3 then y_true = [0, 0,1] and y_pred = [1,1,1.] so precision is 0.33.
In this example top_k =4 or None is same because it uses all the values. In that case y_true = [0, 0, 1, 1], y_pred = [1, 1, 1., 1.] so precision will be 0.5. Please, find the gist here. Thanks!

&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
 , I think this example should also be added in the doc, so here it is. PR &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/37528&gt;#37528&lt;/denchmark-link&gt;
 adds this example to doc.
		</comment>
	</comments>
</bug>