<bug id='38457' author='PaulPauls' open_date='2020-04-11T13:50:52Z' closed_time='2020-09-11T22:58:02Z'>
	<summary>Random NaN loss when using float16 dtype and batch size of 1</summary>
	<description>
System information

Platform: Ubuntu Linux (Kernel 5.3) with Python 3.6.9 OR Google Colab
Tested on TensorFlow: TF v2.1.0 and TF v2.2.0rc2

Background
I came across the following bug in one of my Tensorflow projects and was able to successfully reproduce the bug with minimal code in a Google colab. Please see the link to this colab below. The execution of this colab also shows the bug occuring in iteration 248.

see: &lt;denchmark-link:https://colab.research.google.com/drive/1ZzeqGSOKL5qw9j7XPqdlkHQvFAZETU5O&gt;https://colab.research.google.com/drive/1ZzeqGSOKL5qw9j7XPqdlkHQvFAZETU5O&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;import math
import numpy as np
import tensorflow as tf


if __name__ == '__main__':

    x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    y = np.array([[0], [1], [1], [0]])

    loss_function = tf.keras.losses.BinaryCrossentropy()

    for i in range(2000):
        if i % 100 == 0:
            print("Iteration {}".format(i))

        model = tf.keras.Sequential([
            tf.keras.layers.Dense(units=1, activation='tanh', dtype=tf.float16)])

        model.compile(optimizer='sgd', loss=loss_function)
        model.fit(x=x, y=y, epochs=1, batch_size=1)

        loss_result = loss_function(y, model(x))

        if math.isnan(loss_result):
            raise RuntimeError('NAN Error in iteration {}'.format(i))
&lt;/denchmark-code&gt;

Behaviour Description
Seemingly non-deterministic occurence of a NaN result when calculating loss of a very simple Dense Model. The NaN loss seems to happen randomly and can occur on the 60th or 600th iteration. In the supplied Google colab code it happened in the 248th iteration. The bug only seems to occur using a dtype of float16 and batch_size of 1. Debugging the error lead me to see that the models producing a NaN loss seem to have been initialized with a NaN bias and kernel, though I couldn't get to the bottom of why.
	</description>
	<comments>
		<comment id='1' author='PaulPauls' date='2020-04-12T17:27:48Z'>
		i am able to replicate &lt;denchmark-link:https://colab.sandbox.google.com/gist/Saduf2019/145f094f4cab5ef14028a050aa2c53e3/38461.ipynb&gt;this issue&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='PaulPauls' date='2020-09-02T05:15:41Z'>
		&lt;denchmark-link:https://github.com/PaulPauls&gt;@PaulPauls&lt;/denchmark-link&gt;
 I think we need to use &lt;denchmark-link:https://www.tensorflow.org/guide/mixed_precision&gt;mixed_precision&lt;/denchmark-link&gt;
 to avoid any instability issues coming due to mixed precision.
When I added the following line to your code, it doesn't throw the error you are noticing.
&lt;denchmark-code&gt;from tensorflow.keras.mixed_precision import experimental as mixed_precision

policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_policy(policy)
&lt;/denchmark-code&gt;

Please check the &lt;denchmark-link:https://colab.research.google.com/gist/jvishnuvardhan/3b61ebeab8c397349b5c22c2e83f5fe6/38461.ipynb&gt;gist here&lt;/denchmark-link&gt;
 with CPU and &lt;denchmark-link:https://colab.research.google.com/gist/jvishnuvardhan/b445ffe0bccf6a40bf091cb41694402c/untitled27.ipynb&gt;here&lt;/denchmark-link&gt;
 is with GPU. Thanks!
		</comment>
		<comment id='3' author='PaulPauls' date='2020-09-04T18:03:32Z'>
		Thanks for offering a hotfix &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
! I tried out setting the  policy and it did indeed solve the problem, however the performance loss is considerable.
I tried out timing the &lt;denchmark-link:https://colab.research.google.com/drive/19hryWYHCGhDvzNh3DwtGLPFESuMmZ0Tt?usp=sharing&gt;original code&lt;/denchmark-link&gt;
 vs the &lt;denchmark-link:https://colab.research.google.com/drive/1nH4WymavPJKVWTK56oU4_dpfxlXxysgI?usp=sharing&gt;hotfixed code&lt;/denchmark-link&gt;
 and the hotfixed code required nearly twice as much processing time on CPU (152s vs 260s). Similar results when using a GPU: I couldn't get access to a colab GPU as they were all being used but when I timed in on my private machine with a GeForce 2070 the timing results were 92s vs 147s.
Effectively for me this means that it is more sensible to check for the occasional NaN result and recreate the model rather than permanently change the mixed precision policy.
Still, thanks for taking on the issue! I hope others will benefit from this hotfix. =)
		</comment>
		<comment id='4' author='PaulPauls' date='2020-09-11T22:58:02Z'>
		&lt;denchmark-link:https://github.com/PaulPauls&gt;@PaulPauls&lt;/denchmark-link&gt;
 We cannot compare the performance with CPU as the  throws a warning as follows

WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING
The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.
If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once

Regarding GPU performance with mixed_precision API, may be you can open another issue.
I am closing this issue as the original issue was resolved. Please feel free to reopen if I am mistaken. thanks!
		</comment>
		<comment id='5' author='PaulPauls' date='2020-09-11T22:58:04Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38457&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38457&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>