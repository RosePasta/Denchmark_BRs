<bug id='39552' author='vinodganesan' open_date='2020-05-14T18:44:09Z' closed_time='2020-06-02T19:10:55Z'>
	<summary>TensorFlow Hub hosted Mv1/Mv2 fails execution in tflite runtime</summary>
	<description>
The mv1 and mv2 quantized models hosted here
(mv1-224 0.25,0.5,0.75,1.0 and mv2-224)
(&lt;denchmark-link:https://www.tensorflow.org/lite/guide/hosted_models&gt;https://www.tensorflow.org/lite/guide/hosted_models&lt;/denchmark-link&gt;
) fails execution on the tflite runtime on my custom app modified from TensorFlow's demo app, running on a Google Pixel 3a mobile.
The error was,
java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 150528 bytes and a Java Buffer with 602112 bytes.
It seems like there's an int8 vs float32 mismatch somewhere. However, if I manually take an mv2 and use the TFLite Converter (with quantization flag) to convert to a quantized model, it runs perfectly (both the models have the same size (hosted vs converted) hence ruling any issues of Converter not quantizing the model).
Can you please let me know why the hosted model could be erring out?
	</description>
	<comments>
		<comment id='1' author='vinodganesan' date='2020-05-15T07:40:16Z'>
		Also, I am unable to generate/convert (to tflite) models for Mv3-224 (small and large) and the existing model fails similarly.
However, the hosted Mnas models work perfectly well
		</comment>
		<comment id='2' author='vinodganesan' date='2020-05-15T15:29:21Z'>
		&lt;denchmark-link:https://github.com/vinodganesan&gt;@vinodganesan&lt;/denchmark-link&gt;
 Can you please explain what you meant by the following? Give us more details on what you have done taking the hosted models.

The mv1 and mv2 quantized models hosted here
(mv1-224 0.25,0.5,0.75,1.0 and mv2-224)
(https://www.tensorflow.org/lite/guide/hosted_models) fails execution on the tflite runtime on my custom app modified from TensorFlow's demo app, running on a Google Pixel 3a mobile.

My questions are

from the hosted models, did you download saved model or tflite quantized model?
which model you ported into mobile? What are the steps you followed?
what do you mean by "Also, I am unable to generate/convert (to tflite) models for Mv3-224 (small and large) and the existing model fails similarly."

Let's take one at a time and understand the issue so that we can resolve faster. Thanks!
		</comment>
		<comment id='3' author='vinodganesan' date='2020-05-15T15:47:36Z'>
		Hi,
Please find my answer to your queries below


I downloaded the tflite quantized model,


I ported Mobilenet_V1_1.0_224_quant and Mobilenet_V2_1.0_224_quant, using an app modified from the existing tensorflow demo app.. It more or less uses the steps mentioned in this link (https://codelabs.developers.google.com/codelabs/recognize-flowers-with-tensorflow-on-android/)


For Mv3, I took the tflite quantized model provided in https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet and tried running it, it failed too.


However, if I take the float models provided and use TFLiteConverter with OPTIMIZE_FOR_SIZE flag, I get a quantized model with similar size as of that of hosted. However, I was successfully able to run these quantized models that I converted, on the app.
Let me know if that clarifies.
Thanks,
Vinod
		</comment>
		<comment id='4' author='vinodganesan' date='2020-05-19T18:00:22Z'>
		&lt;denchmark-link:https://github.com/vinodganesan&gt;@vinodganesan&lt;/denchmark-link&gt;
 Can you please create a standalone code to reproduce the issue? what are the sizes of your model (that you converted) and the hosted model? Are they both full integer quantization or float quantization models? Thanks!
		</comment>
		<comment id='5' author='vinodganesan' date='2020-05-26T18:26:26Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='6' author='vinodganesan' date='2020-06-02T19:10:54Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='7' author='vinodganesan' date='2020-06-02T19:10:57Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39552&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39552&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>