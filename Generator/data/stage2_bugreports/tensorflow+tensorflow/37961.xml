<bug id='37961' author='zhaozheng09' open_date='2020-03-27T03:59:02Z' closed_time='2020-03-31T18:17:09Z'>
	<summary>I meet a problem with HDFSWritableFile::Append</summary>
	<description>
System information

OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Linux centos 7
TensorFlow installed from (source or
binary): - find in tf_1.10, and recurrent in master.
Python version: find in python2.7, recurrent in python3.6.5
Bazel version :find in 0.15.2, recurrent:2.0.0,
GCC/Compiler version find in 4.8.5, recurrent:7.3.0

I meet a problem with HDFSWritableFile::Append
Background 1ï¼š     I save model and checkpoint in HDFS.
Background 2: 	My users want to add a big dict(30millon data, above 3GB) to graph.
The Problem:		HDFS abort quit when TF saves graph.txt to HDFS.
Part of logs:
&lt;denchmark-code&gt;File "/usr/lib/python2.7/site-packages/tensorflow/python/training/basic_session_run_hooks.py", line 450, in after_create_session
    "graph.pbtxt")
  File "/usr/lib/python2.7/site-packages/tensorflow/python/framework/graph_io.py", line 71, in write_graph
    text_format.MessageToString(graph_def))
  File "/usr/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py", line 434, in atomic_write_string_to_file
    write_string_to_file(temp_pathname, contents)
  File "/usr/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py", line 314, in write_string_to_file
    f.write(file_content)
  File "/usr/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py", line 111, in write
    compat.as_bytes(file_content), self._writable_file, status)
  File "/usr/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py", line 519, in __exit__
    c_api.TF_GetCode(self.status.status))
InvalidArgumentError: viewfs://hadoop-meituan/xxxx01/user/hadoop-waimai/xxxx/model/model//date/graph.pbtxt.tmp04d0a32366f548ec9f3aa629600fa19f; Invalid argument
&lt;/denchmark-code&gt;

I deal with this question by logs, then I get a result that the graph is too big to save.
problem code:
&lt;denchmark-code&gt;Status HDFSWritableFile::Append(StringPiece data) {
    if (libhdfs()-&gt;hdfsWrite(fs_, file_, data.data(),
                             static_cast&lt;tSize&gt;(data.size())) == -1) {
       return IOError(filename_, errno);
}
&lt;/denchmark-code&gt;

data.size() return uint64_t, but hdfsWrite only accept int, so there are some questions when append a big string(len &gt; INT_MAX)
So I change HDFSWritableFile::Append function to solve my question, and successfully solve it.
so I want to make a pull request .
	</description>
	<comments>
		<comment id='1' author='zhaozheng09' date='2020-03-31T18:17:10Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37961&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37961&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>