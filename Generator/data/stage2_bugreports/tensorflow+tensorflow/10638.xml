<bug id='10638' author='liu115' open_date='2017-06-11T21:30:21Z' closed_time='2017-08-16T17:10:25Z'>
	<summary>Might be a bug for contrib.legacy_seq2seq</summary>
	<description>
Hi,
I am using the embedding_attention_seq2seq with output_projection. The document from &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/contrib/legacy_seq2seq/embedding_rnn_decoder&gt;https://www.tensorflow.org/api_docs/python/tf/contrib/legacy_seq2seq/embedding_rnn_decoder&lt;/denchmark-link&gt;
 says,

outputs: A list of the same length as decoder_inputs of 2D Tensors with shape [batch_size x num_decoder_symbols] containing the generated outputs.

But the output seems to be the output before projection when I used it. So I go through the source code from &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py&lt;/denchmark-link&gt;
 ,
embedding_attention_seq2seq is base on the embedding_attention_decoder and attention_decoder. It has to give output_size to attention_decoder, but output_size is set to None when output_projection is not None.
def embedding_attention_seq2seq(encoder_inputs,
                                decoder_inputs,
                                cell,
                                num_encoder_symbols,
                                num_decoder_symbols,
                                embedding_size,
                                num_heads=1,
                                output_projection=None,
                                feed_previous=False,
                                dtype=None,
                                scope=None,
                                initial_state_attention=False):
    ... # skip
    output_size = None
    if output_projection is None:
      cell = core_rnn_cell.OutputProjectionWrapper(cell, num_decoder_symbols)
      output_size = num_decoder_symbols

    if isinstance(feed_previous, bool):
      return embedding_attention_decoder(
          decoder_inputs,
          encoder_state,
          attention_states,
          cell,
          num_decoder_symbols,
          embedding_size,
          num_heads=num_heads,
          output_size=output_size,
          output_projection=output_projection,
          feed_previous=feed_previous,
          initial_state_attention=initial_state_attention)
When output_size is None, the output_size is simply the cell's output_size. And so the shape of output for embedding_attention_seq2seq will be [batch_size x cell's output_size]  rather than [batch_size x num_decoder_symbols]
def attention_decoder(decoder_inputs,
                      initial_state,
                      attention_states,
                      cell,
                      output_size=None,
                      num_heads=1,
                      loop_function=None,
                      dtype=None,
                      scope=None,
                      initial_state_attention=False):
  ... # skip
  if output_size is None:
    output_size = cell.output_size
  ... # skip
      with variable_scope.variable_scope("AttnOutputProjection"):
        output = linear([cell_output] + attns, output_size, True)
      if loop_function is not None:
        prev = output
      outputs.append(output)

  return outputs, state
Thanks.
	</description>
	<comments>
		<comment id='1' author='liu115' date='2017-06-16T00:02:08Z'>
		Please take a look &lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='liu115' date='2017-08-16T13:55:20Z'>
		+1 I am observing this same behavior in embedding_rnn_decoder -- are there any updates or workarounds here? Is there a way to get the output_projection weights and multiple them?
My current workaround is to create a dense later that takes in outputs and converts to the num_decoder_size, but this feels inaccurate.
		</comment>
		<comment id='3' author='liu115' date='2017-08-16T16:11:03Z'>
		I'm not sure what you want to achieve: when output_projection is given, you are passing the variables to the dense layer yourself, right? In that case, num_decoder__symbols is ignored, as I think we specify. Could you explain the problem?
		</comment>
		<comment id='4' author='liu115' date='2017-08-16T16:41:41Z'>
		So I am actually trying to point out the opposite.  When output_projection = None, then num_decoder_symbols should not be ignored and the outputs should be a list of outputs with shape [batch_size x num_decoder_symbols], but what is actually happening is that with output_projection = None the outputs are coming out as [batch_size x cell.output_size] so I am actually being forced to pass the variables to the dense layer myself.  I would like to not have to use output_projection if it can be avoided, but it seems there is no way to avoid it at the moment.
		</comment>
		<comment id='5' author='liu115' date='2017-08-16T16:49:40Z'>
		I can hardly believe this is ignored, it's in the code above:
&lt;denchmark-code&gt;if output_projection is None:
      cell = core_rnn_cell.OutputProjectionWrapper(cell, num_decoder_symbols)
      output_size = num_decoder_symbols
&lt;/denchmark-code&gt;

So if output_projection is None, then we wrap the decoder cell and make sure it outputs num_decoder_symbols. How comes it doesn't work for you?
		</comment>
		<comment id='6' author='liu115' date='2017-08-16T17:02:18Z'>
		I am actually using [embedding_rnn_decoder] (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py#L233&gt;https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py#L233&lt;/denchmark-link&gt;
) (sorry if that caused confusion) so I don't see where num_symbols is passed into the cell?  The line you mentioned is in the embedding_rnn_seq2seq, but the functionality is not replicated in the embedding_rnn_decoder. I think that if the code snippet you mentioned was added before cell was passed then the issue I am seeing would be solved?
		</comment>
		<comment id='7' author='liu115' date='2017-08-16T17:06:34Z'>
		At the very least, the documentation is misleading for embedding_rnn_decoder as it suggests that if output_projection is None that the output projection layer will be wrapped automatically.
		</comment>
		<comment id='8' author='liu115' date='2017-08-16T17:08:20Z'>
		Yes - the functionality isn't replicated in embedding_rnn_decoder because you can just wrap the cell before passing it there. It's also not claimed anywhere that embedding_rnn_decoder does this, it doesn't even have a parameter num_decoder_symbols. So this is solved, I'm still unclear about the original issue, which was clearly with output_projection.
		</comment>
		<comment id='9' author='liu115' date='2017-08-16T17:15:24Z'>
		Maybe we are reading different documentation, because it is claimed that embedding_rnn_decoder does this.
embedding_rnn_decoder has a parameter &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py#L236&gt;num_symbols&lt;/denchmark-link&gt;
 which is the same as num_decoder_symbols and hte &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py#L273&gt;documentation&lt;/denchmark-link&gt;
 says "It is of shape [batch_size x num_decoder_symbols] when output_projection is None." so at the very least it seems that this documentation is in fact unclear.  Would you prefer if I open up a separate issue for this?
		</comment>
	</comments>
</bug>