<bug id='40677' author='prabhat00155' open_date='2020-06-22T14:12:26Z' closed_time='2020-07-08T23:45:29Z'>
	<summary>Unsupported ops when converting keyword spotting model</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Darwin Kernel Version 19.5.0
TensorFlow installed from (source or binary): binary
TensorFlow version (or github SHA if from source): 1.15.2

Command used to run the converter or code if youâ€™re using the Python API
If possible, please share a link to Colab/Jupyter/any notebook.
&lt;denchmark-code&gt;tflite_convert --enable_v1_converter --graph_def_file=tf5/DS_CNN_L.pb --output_file=tflite/DS_CNN_L.tflite --output_format=TFLITE --input_shape=1 --input_arrays=wav_data --output_arrays=labels_softmax --inference_type=FLOAT --input_data_type=STRING
&lt;/denchmark-code&gt;

The output from the converter invocation
&lt;denchmark-code&gt;2020-06-22 19:10:58.286058: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-06-22 19:10:58.298962: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f998703d820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-06-22 19:10:58.298980: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-06-22 19:10:58.334024: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count &gt;= 8, compute capability &gt;= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)
2020-06-22 19:10:58.334124: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-06-22 19:10:58.366223: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize
2020-06-22 19:10:58.366256: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 125 nodes (-57), 125 edges (-57), time = 15.329ms.
2020-06-22 19:10:58.366261: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 125 nodes (0), 125 edges (0), time = 6.637ms.
Traceback (most recent call last):
  File "/Users/prroy/Documents/MachineLearning/onnx_projects/onnxrt_env/bin/tflite_convert", line 11, in &lt;module&gt;
    sys.exit(main())
  File "/Users/prroy/Documents/MachineLearning/onnx_projects/onnxrt_env/lib/python3.6/site-packages/tensorflow_core/lite/python/tflite_convert.py", line 515, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File "/Users/prroy/Documents/MachineLearning/onnx_projects/onnxrt_env/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File "/Users/prroy/Documents/MachineLearning/onnx_projects/onnxrt_env/lib/python3.6/site-packages/absl/app.py", line 299, in run
    _run_main(main, args)
  File "/Users/prroy/Documents/MachineLearning/onnx_projects/onnxrt_env/lib/python3.6/site-packages/absl/app.py", line 250, in _run_main
    sys.exit(main(argv))
  File "/Users/prroy/Documents/MachineLearning/onnx_projects/onnxrt_env/lib/python3.6/site-packages/tensorflow_core/lite/python/tflite_convert.py", line 511, in run_main
    _convert_tf1_model(tflite_flags)
  File "/Users/prroy/Documents/MachineLearning/onnx_projects/onnxrt_env/lib/python3.6/site-packages/tensorflow_core/lite/python/tflite_convert.py", line 199, in _convert_tf1_model
    output_data = converter.convert()
  File "/Users/prroy/Documents/MachineLearning/onnx_projects/onnxrt_env/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py", line 983, in convert
    **converter_kwargs)
  File "/Users/prroy/Documents/MachineLearning/onnx_projects/onnxrt_env/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py", line 449, in toco_convert_impl
    enable_mlir_converter=enable_mlir_converter)
  File "/Users/prroy/Documents/MachineLearning/onnx_projects/onnxrt_env/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py", line 200, in toco_convert_protos
    raise ConverterError("See console for info.\n%s\n%s\n" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: See console for info.
2020-06-22 19:11:00.826624: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: DecodeWav
2020-06-22 19:11:00.826669: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: AudioSpectrogram
2020-06-22 19:11:00.826680: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Mfcc
2020-06-22 19:11:00.828343: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 98 operators, 181 arrays (0 quantized)
2020-06-22 19:11:00.829492: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 98 operators, 181 arrays (0 quantized)
2020-06-22 19:11:00.833039: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 19 operators, 46 arrays (0 quantized)
2020-06-22 19:11:00.833242: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 19 operators, 46 arrays (0 quantized)
2020-06-22 19:11:00.833372: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 19 operators, 46 arrays (0 quantized)
2020-06-22 19:11:00.833621: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 0 bytes, theoretical optimal value: 0 bytes.
2020-06-22 19:11:00.833699: I tensorflow/lite/toco/toco_tooling.cc:454] Number of parameters: 410704
2020-06-22 19:11:00.834066: E tensorflow/lite/toco/toco_tooling.cc:481] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: AVERAGE_POOL_2D, CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, RESHAPE, SOFTMAX, SQUEEZE. Here is a list of operators for which you will need custom implementations: AudioSpectrogram, DecodeWav, Mfcc.
Traceback (most recent call last):
  File "/Users/prroy/Documents/MachineLearning/onnx_projects/onnxrt_env/bin/toco_from_protos", line 11, in &lt;module&gt;
    sys.exit(main())
  File "/Users/prroy/Documents/MachineLearning/onnx_projects/onnxrt_env/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py", line 89, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File "/Users/prroy/Documents/MachineLearning/onnx_projects/onnxrt_env/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File "/Users/prroy/Documents/MachineLearning/onnx_projects/onnxrt_env/lib/python3.6/site-packages/absl/app.py", line 299, in run
    _run_main(main, args)
  File "/Users/prroy/Documents/MachineLearning/onnx_projects/onnxrt_env/lib/python3.6/site-packages/absl/app.py", line 250, in _run_main
    sys.exit(main(argv))
  File "/Users/prroy/Documents/MachineLearning/onnx_projects/onnxrt_env/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py", line 52, in execute
    enable_mlir_converter)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: AVERAGE_POOL_2D, CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, RESHAPE, SOFTMAX, SQUEEZE. Here is a list of operators for which you will need custom implementations: AudioSpectrogram, DecodeWav, Mfcc.
&lt;/denchmark-code&gt;

Also, please include a link to the saved model or GraphDef
&lt;denchmark-code&gt;https://github.com/ARM-software/ML-KWS-for-MCU/tree/master/Pretrained_models/DS_CNN
&lt;/denchmark-code&gt;

Failure details
If the conversion is successful, but the generated model is wrong,
state what is wrong:

Producing wrong results and/or decrease in accuracy
Producing correct results, but the model is slower than expected (model generated from old converter)

RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.
Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='prabhat00155' date='2020-06-24T06:46:04Z'>
		&lt;denchmark-link:https://github.com/thaink&gt;@thaink&lt;/denchmark-link&gt;
 could you take a look?
		</comment>
		<comment id='2' author='prabhat00155' date='2020-06-25T04:55:49Z'>
		Let me add these ops.
		</comment>
		<comment id='3' author='prabhat00155' date='2020-06-25T05:07:18Z'>
		Hi prabhat00155,
You are using a too old version of Tensorflow.
Those ops are supported with flex delegate. Please check it with Tensorflow v2.2.
		</comment>
		<comment id='4' author='prabhat00155' date='2020-06-25T10:37:39Z'>
		Thanks, I have updated my TF version. When I run summarize_graph on the model, I get shape=[]. How do I pass this when using tflite_convert?
&lt;denchmark-code&gt;(onnxrt_env) prroy-Mac:tensorflow prroy$ bazel-bin/tensorflow/tools/graph_transforms/summarize_graph --in_graph=DS_CNN_L.pb 
Found 1 possible inputs: (name=wav_data, type=string(7), shape=[]) 
No variables spotted.
Found 1 possible outputs: (name=labels_softmax, op=Softmax) 
Found 422850 (422.85k) const parameters, 0 (0) variable parameters, and 0 control_edges
Op types used: 70 Const, 57 Identity, 12 BiasAdd, 11 FusedBatchNorm, 11 Relu, 6 Conv2D, 5 DepthwiseConv2dNative, 2 Reshape, 1 AudioSpectrogram, 1 MatMul, 1 Mfcc, 1 Placeholder, 1 DecodeWav, 1 AvgPool, 1 Softmax, 1 Squeeze
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='prabhat00155' date='2020-06-25T12:15:57Z'>
		&lt;denchmark-link:https://github.com/abattery&gt;@abattery&lt;/denchmark-link&gt;
 Do we need to pass --input_shapes in case of string?
		</comment>
		<comment id='6' author='prabhat00155' date='2020-06-25T13:09:42Z'>
		shape=[] means scalar type. You can put one string value via TFLite inference API.
E.g.,
&lt;denchmark-code&gt;model_interpreter.set_tensor(input_details[0]['index'],
                               np.array(['foo'], dtype=np.string_))
&lt;/denchmark-code&gt;

For the inference API, you can refer to the following link: &lt;denchmark-link:https://www.tensorflow.org/lite/guide/inference&gt;https://www.tensorflow.org/lite/guide/inference&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='prabhat00155' date='2020-07-03T21:26:23Z'>
		Thanks, I was able to convert the model to TFLite. Can I use benchmark_model tool on this model?
I built benchmark_model with  --define=tflite_convert_with_select_tf_ops=true.
&lt;denchmark-code&gt;bazel build -c opt tensorflow/lite/tools/benchmark:benchmark_model --define=tflite_convert_with_select_tf_ops=true
&lt;/denchmark-code&gt;

However, I get the following error when running the tool on the converted model:
&lt;denchmark-code&gt;bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model --graph=../models/tflite/wave_model.tflite --num_runs=10 --num_threads=4 --target_ops=TFLITE_BUILTINS,SELECT_TF_OPS --define=tflite_convert_with_select_tf_ops=true
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;Loaded model ../models/tflite/wave_model.tflite
ERROR: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.
ERROR: Node number 0 (FlexDecodeWav) failed to prepare.

Failed to allocate tensors!
Benchmarking failed.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='prabhat00155' date='2020-07-03T21:58:31Z'>
		Could you try "bazel build --config=monolithic -c opt tensorflow/lite/tools/benchmark:benchmark_model_plus_flex" instead?
		</comment>
		<comment id='9' author='prabhat00155' date='2020-07-04T15:20:49Z'>
		&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/4873069/output.txt&gt;output.txt&lt;/denchmark-link&gt;

Thanks, I got some error when running the model with benchmark_model_plus_flex. I have attached the output log file.
&lt;denchmark-code&gt;2020-07-04 16:17:28.726122: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at decode_wav_op.cc:55 : Invalid argument: Header mismatch: Expected RIFF but found we'r
ERROR: Header mismatch: Expected RIFF but found we'r
	 (while executing 'DecodeWav' via Eager)
ERROR: Node number 19 (TfLiteFlexDelegate) failed to invoke.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='prabhat00155' date='2020-07-06T22:17:12Z'>
		&lt;denchmark-link:https://github.com/prabhat00155&gt;@prabhat00155&lt;/denchmark-link&gt;
 please make sure that your inputs are valid to both the TFLite corresponding TF model. According to the error message, TF's DecodeWav op could not handle your input.
		</comment>
		<comment id='11' author='prabhat00155' date='2020-07-08T08:07:33Z'>
		&lt;denchmark-link:https://github.com/prabhat00155&gt;@prabhat00155&lt;/denchmark-link&gt;

Please update as per above comment.
		</comment>
		<comment id='12' author='prabhat00155' date='2020-07-08T23:39:05Z'>
		I am running the benchmark_model_plus_flex tool like this:
&lt;denchmark-code&gt;bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model_plus_flex --graph=wave_model.tflite --num_runs=10 --num_threads=4
&lt;/denchmark-code&gt;

I am not passing any input, the benchmark tool generates random input and then repeatedly runs the model for specified number of runs.
		</comment>
		<comment id='13' author='prabhat00155' date='2020-07-08T23:40:48Z'>
		The wave model could not handle any random data generated from benchmark model since the model needs a valid wav input.
		</comment>
		<comment id='14' author='prabhat00155' date='2020-07-09T10:13:15Z'>
		In that case, is there a way to pass valid data using the benchmark model app?
		</comment>
		<comment id='15' author='prabhat00155' date='2020-07-09T13:23:14Z'>
		The benchmark tool always uses random input. I think you have to write your own inference code.
		</comment>
	</comments>
</bug>