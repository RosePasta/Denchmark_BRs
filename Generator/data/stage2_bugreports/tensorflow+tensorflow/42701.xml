<bug id='42701' author='guillaumelorre28' open_date='2020-08-27T09:55:57Z' closed_time='2020-09-11T19:41:00Z'>
	<summary>Impossible to use non mirrored variables in distributed training</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.3
Python version: 3.7
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: /
GPU model and memory: /

I would like to use a non trainable variable placed on cpu in a multi-gpu training. It is useful for using a memory bank of negative examples for instance in self-supervised learning.
import tensorflow as tf
from tensorflow import keras

layers = tf.keras.layers

strategy = tf.distribute.MirroredStrategy()

class Test(layers.Layer):

    def __init__(self, memory, **kargs):
        super(Test, self).__init__(**kargs)
        
        self.memory = memory
        self.dense = layers.Dense(128)

    def call(self, inputs):
        
        res = tf.matmul(inputs, self.memory, transpose_b=True)
        res = self.dense(res)

        return res
    
def create_model(memory):
    
    input_tensor = layers.Input(
        shape=[128], name="input_tensor"
    )
 
    x = layers.Dense(128)(input_tensor)
    
    output = Test(memory)(x)

    return keras.Model(inputs=input_tensor, outputs=output)
    
with tf.device('/cpu'):
    
    memory = tf.Variable(tf.random.uniform([100, 128]), trainable=False)
    
    
with strategy.scope():
    
    model = create_model(memory)
    
model.compile()

model.summary()

print(memory)
However creating a variable outside strategy.scope() results in an error:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "code_minimal_bug_memory.py", line 44, in &lt;module&gt;
    model.compile()
  File "/opt/conda/envs/tensorflow2.3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py", line 538, in compile
    self._validate_compile(optimizer, metrics, **kwargs)
  File "/opt/conda/envs/tensorflow2.3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py", line 2512, in _validate_compile
    '  model.compile(...)' % (v, strategy))
ValueError: Variable (&lt;tf.Variable 'Variable:0' shape=(100, 128) dtype=float32, numpy=
array([[3.0940974e-01, 9.5359027e-01, 9.3334043e-01, ..., 2.3660135e-01,
        4.1169703e-01, 7.6202512e-02],
       [1.0460985e-01, 2.4617815e-01, 6.7584288e-01, ..., 8.3745670e-01,
        3.0766165e-01, 6.8396461e-01],
       [7.1668625e-04, 6.1034310e-01, 8.1730366e-02, ..., 3.1000912e-01,
        8.4088564e-01, 3.5998344e-02],
       ...,
       [8.0560517e-01, 6.6493630e-02, 5.6640983e-02, ..., 2.0186901e-03,
        4.2683721e-01, 6.6080117e-01],
       [6.1158764e-01, 8.3118451e-01, 2.3782039e-01, ..., 9.0197289e-01,
        3.4783411e-01, 5.6294477e-01],
       [2.9402018e-02, 2.2738779e-01, 5.7802427e-01, ..., 7.3360741e-01,
        8.2319343e-01, 1.2766552e-01]], dtype=float32)&gt;) was not created in the distribution strategy scope of (&lt;tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7fef7d028710&gt;). It is most likely due to not all layers or the model or optimizer being created outside the distribution strategy scope. Try to make sure your code looks similar to the following.
with strategy.scope():
  model=_create_model()
  model.compile(...)
&lt;/denchmark-code&gt;

Creating the variable memory inside the strategy.scope() works but creates a MirroredVariable replicated on every gpu which is not the desired behavior:
import tensorflow as tf
from tensorflow import keras

layers = tf.keras.layers

strategy = tf.distribute.MirroredStrategy()

class Test(layers.Layer):

    def __init__(self, memory, **kargs):
        super(Test, self).__init__(**kargs)
        
        self.memory = memory
        self.dense = layers.Dense(128)

    def call(self, inputs):
        
        res = tf.matmul(inputs, self.memory, transpose_b=True)
        res = self.dense(res)

        return res
    
def create_model(memory):
    
    input_tensor = layers.Input(
        shape=[128], name="input_tensor"
    )
 
    x = layers.Dense(128)(input_tensor)
    
    output = Test(memory)(x)

    return keras.Model(inputs=input_tensor, outputs=output)
    

with strategy.scope():
    
    with tf.device('/cpu'):
    
        memory = tf.Variable(tf.random.uniform([100, 128]), trainable=False)
    
    model = create_model(memory)
    
model.compile()

model.summary()

print(memory)
The output is:
&lt;denchmark-code&gt;MirroredVariable:{
  0: &lt;tf.Variable 'Variable:0' shape=(100, 128) dtype=float32, numpy=
array([[0.9455886 , 0.35135317, 0.03729475, ..., 0.7572125 , 0.16159093,
        0.48011708],
       [0.25856972, 0.1125381 , 0.85940254, ..., 0.21053994, 0.97001517,
        0.52998424],
       [0.23188198, 0.7152246 , 0.6255108 , ..., 0.02806866, 0.82893085,
        0.7379434 ],
       ...,
       [0.4963615 , 0.13400674, 0.4220184 , ..., 0.9512589 , 0.44520867,
        0.5342829 ],
       [0.7703849 , 0.23092055, 0.46073604, ..., 0.81235087, 0.7762462 ,
        0.38599086],
       [0.93219006, 0.04940701, 0.19656885, ..., 0.71836615, 0.56049407,
        0.71457684]], dtype=float32)&gt;,
  1: &lt;tf.Variable 'Variable/replica_1:0' shape=(100, 128) dtype=float32, numpy=
array([[0.9455886 , 0.35135317, 0.03729475, ..., 0.7572125 , 0.16159093,
        0.48011708],
       [0.25856972, 0.1125381 , 0.85940254, ..., 0.21053994, 0.97001517,
        0.52998424],
       [0.23188198, 0.7152246 , 0.6255108 , ..., 0.02806866, 0.82893085,
        0.7379434 ],
       ...,
       [0.4963615 , 0.13400674, 0.4220184 , ..., 0.9512589 , 0.44520867,
        0.5342829 ],
       [0.7703849 , 0.23092055, 0.46073604, ..., 0.81235087, 0.7762462 ,
        0.38599086],
       [0.93219006, 0.04940701, 0.19656885, ..., 0.71836615, 0.56049407,
        0.71457684]], dtype=float32)&gt;,
  2: &lt;tf.Variable 'Variable/replica_2:0' shape=(100, 128) dtype=float32, numpy=
array([[0.9455886 , 0.35135317, 0.03729475, ..., 0.7572125 , 0.16159093,
        0.48011708],
       [0.25856972, 0.1125381 , 0.85940254, ..., 0.21053994, 0.97001517,
        0.52998424],
       [0.23188198, 0.7152246 , 0.6255108 , ..., 0.02806866, 0.82893085,
        0.7379434 ],
       ...,
       [0.4963615 , 0.13400674, 0.4220184 , ..., 0.9512589 , 0.44520867,
        0.5342829 ],
       [0.7703849 , 0.23092055, 0.46073604, ..., 0.81235087, 0.7762462 ,
        0.38599086],
       [0.93219006, 0.04940701, 0.19656885, ..., 0.71836615, 0.56049407,
        0.71457684]], dtype=float32)&gt;,
  3: &lt;tf.Variable 'Variable/replica_3:0' shape=(100, 128) dtype=float32, numpy=
array([[0.9455886 , 0.35135317, 0.03729475, ..., 0.7572125 , 0.16159093,
        0.48011708],
       [0.25856972, 0.1125381 , 0.85940254, ..., 0.21053994, 0.97001517,
        0.52998424],
       [0.23188198, 0.7152246 , 0.6255108 , ..., 0.02806866, 0.82893085,
        0.7379434 ],
       ...,
       [0.4963615 , 0.13400674, 0.4220184 , ..., 0.9512589 , 0.44520867,
        0.5342829 ],
       [0.7703849 , 0.23092055, 0.46073604, ..., 0.81235087, 0.7762462 ,
        0.38599086],
       [0.93219006, 0.04940701, 0.19656885, ..., 0.71836615, 0.56049407,
        0.71457684]], dtype=float32)&gt;
}

&lt;/denchmark-code&gt;

This is a MirroredVariable replicated on each gpu and not a Variable placed on CPU as I would like.
	</description>
	<comments>
		<comment id='1' author='guillaumelorre28' date='2020-08-28T05:01:27Z'>
		I am able to replicate the issue reported, please find the &lt;denchmark-link:https://colab.research.google.com/gist/Saduf2019/59b46c713294c647e379e380b350d91e/untitled397.ipynb&gt;gist here&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='2' author='guillaumelorre28' date='2020-08-28T18:44:00Z'>
		Hi &lt;denchmark-link:https://github.com/guillaumelorre28&gt;@guillaumelorre28&lt;/denchmark-link&gt;
, this functionality is not supported, as you point out all of the variables get mirrored with . Can you provide more information on why you would want the non trainable variables on your CPU instead of having them mirrored across your GPUs? In MirroredStrategy, each GPU gets a copy of the model, and then computes the forwards and backwards pass to compute gradients. The gradients are then aggregated across each GPU and reduced. All of this computation happens efficiently between the GPUs, and the CPU is just responsible for passing new batches of your training data. So if your  variables are stored on CPU, each time your GPUs do the forward pass the  tensors would need to be copied over from the CPU, or that specific computation would have to happen on the CPU, neither of which seem very efficient to do in the middle of each training step.
Alternatively, if you don't want any of the variables mirrored, and want them all stored on your CPU, you could try CentralStorageStrategy which provides synchrnous training with mirroring variables across devices since it's the operations that are replicated across all local GPUs.
		</comment>
		<comment id='3' author='guillaumelorre28' date='2020-09-04T19:16:23Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='4' author='guillaumelorre28' date='2020-09-11T19:40:59Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='5' author='guillaumelorre28' date='2020-09-11T19:41:01Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42701&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42701&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>