<bug id='31331' author='geometrikal' open_date='2019-08-05T11:53:44Z' closed_time='2019-08-20T16:57:29Z'>
	<summary>Error freezing saved model if it contains a tf.keras.layer.BatchNormalisation layer</summary>
	<description>
System information

Have I written custom code: Yes
OS Platform and Distribution: Windows 1903
TensorFlow installed from: pip
TensorFlow version: 1.14.0
Python version: 3.7.3
GPU model and memory: RTX 2080 Ti

Problem
To make a frozen graph, I first create a saved model using tf.saved_model.simple_save and then freeze it using tensorflow.python.tools.freeze_graph.freeze_graph.
If a model contains some tf.keras.layers.BatchNormalisation layers, freezing will fail in TF 1.14.0 with:
ValueError: Tensor name 'batch_normalization/cond/ReadVariableOp/Switch:1' is invalid.
TF 1.13.1 does not give an error
Code to reproduce the issue
&lt;denchmark-code&gt;import tensorflow as tf
import tensorflow.keras.backend as K
import os
import datetime
from tensorflow.python.tools import freeze_graph
from tensorflow.keras.layers import Dense, Flatten, Conv2D, Input, Activation, BatchNormalization
from tensorflow.keras.models import Model

inputs = Input(shape=(128, 128, 1))
x = Conv2D(4, (3, 3))(inputs)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = Flatten()(x)
x = Dense(5, activation='softmax')(x)
model = Model(inputs, x, name='test')
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

K.set_learning_phase(0)
save_dir = "./tmp_{:%Y-%m-%d_%H%M%S}".format(datetime.datetime.now())
tf.saved_model.simple_save(K.get_session(),
                           save_dir,
                           inputs={"input": model.inputs[0]},
                           outputs={"output": model.outputs[0]})

freeze_graph.freeze_graph(None,
                          None,
                          None,
                          None,
                          model.outputs[0].op.name,
                          None,
                          None,
                          os.path.join(save_dir, "frozen_model.pb"),
                          False,
                          "",
                          input_saved_model_dir=save_dir)
&lt;/denchmark-code&gt;

Update:
Seems to be a problem in , in particular &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/0f486fc67070ba888204741c404a55a5f1a41fbc#diff-2d2827fd48cee6884e3587c901ad6952&gt;0f486fc#diff-2d2827fd48cee6884e3587c901ad6952&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/gargn&gt;@gargn&lt;/denchmark-link&gt;

If I change this file back to its 1.13 version there is no more error.
Update 2:
I put K.set_learning_phase(0) before creating the model, then it works. I don't know what effect this has though? Does it just turn off batch normalisation altogether?
Update 3
Final remarks:

Putting K.set_learning_phase(0) before creating the model will let it save, however the Batch Normalisation layer doesn't seem to do anything (updating turned off?) so it is not a solution.
Changing graph_util_impl.py to its 1.13.1 version will let it save without error, however there will be an error ValueError: Input 0 of node batch_normalization/cond/ReadVariableOp/Switch was passed float from batch_normalization/gamma:0 incompatible with expected resource. when loading the frozen graph from the protobuf.
The workaround is to save the model weights, clear the session (so that tensor names are not different because of having two graphs), set learning phase to 0, recreate the model, load the weights, and then freeze (example code in my comment below)

	</description>
	<comments>
		<comment id='1' author='geometrikal' date='2019-08-06T02:32:26Z'>
		Same problem here with tensorflow-gpu 1.14.0, Ubuntu 16.04, Python 3.5.2.
		</comment>
		<comment id='2' author='geometrikal' date='2019-08-06T04:51:09Z'>
		&lt;denchmark-link:https://github.com/wenshuangwang&gt;@wenshuangwang&lt;/denchmark-link&gt;
 I found if I put  before constructing the model, then it works. But I don't know how this affects training?! It seems the accuracy is lower afterwards.
Another way that works is to use normal keras instead of tensorflow.keras... hmmmm
		</comment>
		<comment id='3' author='geometrikal' date='2019-08-06T08:34:20Z'>
		&lt;denchmark-link:https://github.com/geometrikal&gt;@geometrikal&lt;/denchmark-link&gt;
 Thank you very much, it works for me as well. I think it's ok if training and freezing separately.
I know the second way, but I must use tf.keras to use tensorflow_model_optimization.sparsity.
		</comment>
		<comment id='4' author='geometrikal' date='2019-08-06T09:44:12Z'>
		&lt;denchmark-link:https://github.com/wenshuangwang&gt;@wenshuangwang&lt;/denchmark-link&gt;
 Actually, I think putting  before loading the model stops the batch normalisation layer from updating. I can tell a difference - I get worse accuracy (same as when not using batch normalisation) and the accuracy vs epochs graph looks different.
		</comment>
		<comment id='5' author='geometrikal' date='2019-08-06T12:52:27Z'>
		&lt;denchmark-link:https://github.com/wenshuangwang&gt;@wenshuangwang&lt;/denchmark-link&gt;
 I finally found a workaround:

Create the model using a function (do not use K.set_learning_phase(0)):

&lt;denchmark-code&gt;def create_model():
    inputs = Input(...)
    ...
    return model

model = create_model()
&lt;/denchmark-code&gt;


Train model
Save weights:
model.save_weights("weights.h5")
Clear session and set learning phase to 0:

&lt;denchmark-code&gt;K.clear_session()
K.set_learning_phase(0)
&lt;/denchmark-code&gt;


Recreate model and load weights:

&lt;denchmark-code&gt;model = create_model()
model.load_weights("weights.h5")
&lt;/denchmark-code&gt;


Freeze as before

I have confirmed this works with no errors for creating the frozen model protobuf. Also there are no errors when loading the model either (using tf.import_graph_def), and it is working in my production code (using tensorflow java 1.14)
Here is full working example:
&lt;denchmark-code&gt;import tensorflow as tf
import tensorflow.keras.backend as K
import os
import datetime
from tensorflow.python.tools import freeze_graph
from tensorflow.keras.layers import Dense, Flatten, Conv2D, Input, Activation, BatchNormalization, Lambda
from tensorflow.keras.models import Model


def create_model():
    inputs = Input(shape=(128, 128, 1))
    x = Conv2D(4, (3, 3))(inputs)
    x = BatchNormalization()(x)
    # x = Lambda((lambda x: tf.layers.batch_normalization(x)))(x)
    x = Activation('relu')(x)
    x = Flatten()(x)
    x = Dense(5, activation='softmax')(x)
    model = Model(inputs, x, name='test')
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model


model = create_model()

# Training goes here...

model.save_weights("weights.h5")

K.clear_session()
K.set_learning_phase(0)

model = create_model()
model.load_weights("weights.h5")

save_dir = "./tmp_{:%Y-%m-%d_%H%M%S}".format(datetime.datetime.now())
tf.saved_model.simple_save(K.get_session(),
                           save_dir,
                           inputs={"input": model.inputs[0]},
                           outputs={"output": model.outputs[0]})

freeze_graph.freeze_graph(None,
                          None,
                          None,
                          None,
                          model.outputs[0].op.name,
                          None,
                          None,
                          os.path.join(save_dir, "frozen_model.pb"),
                          False,
                          "",
                          input_saved_model_dir=save_dir)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='geometrikal' date='2019-08-07T02:29:46Z'>
		&lt;denchmark-link:https://github.com/geometrikal&gt;@geometrikal&lt;/denchmark-link&gt;
 Yes, this is what I said to train and freeze the network separately. Thank you for explaining patiently.
		</comment>
		<comment id='7' author='geometrikal' date='2019-08-08T00:28:30Z'>
		&lt;denchmark-link:https://github.com/geometrikal&gt;@geometrikal&lt;/denchmark-link&gt;
 Is this resolved by the suggestion of &lt;denchmark-link:https://github.com/wenshuangwang&gt;@wenshuangwang&lt;/denchmark-link&gt;
 . Can we close the issue? Thanks!
		</comment>
		<comment id='8' author='geometrikal' date='2019-08-08T01:21:02Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 I think this is a bug and should be left open.
The bug does not occur in normal keras, and it is clunky to have to create a copy of the graph and transfer the weights across just because you have a batch normalisation layer in there.
In TF 1.13.1 the models would be frozen without error, but this leads to an error when loading the frozen model: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/3628&gt;#3628&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='geometrikal' date='2019-08-08T01:24:15Z'>
		I tested the code in the latest tf-nightly (1.15.0.dev20190807) and the model froze without issue.
		</comment>
		<comment id='10' author='geometrikal' date='2019-08-08T01:51:47Z'>
		&lt;denchmark-link:https://github.com/gargn&gt;@gargn&lt;/denchmark-link&gt;
 I tested as well with 1.15.0-dev20190807 to confirm. It does save without issue, but when loading the frozen graph there is a new error: 
Should I open a new issue for this?
Code for loading graph:
&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow.python.platform import gfile
import tensorflow.keras.backend as K

source = "./freeze_test/frozen_model.pb"

session = K.get_session()
with gfile.Open(source, 'rb') as f:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(f.read())
    session.graph.as_default()
    tf.import_graph_def(graph_def, name='')
&lt;/denchmark-code&gt;

		</comment>
		<comment id='11' author='geometrikal' date='2019-08-15T17:08:18Z'>
		&lt;denchmark-link:https://github.com/geometrikal&gt;@geometrikal&lt;/denchmark-link&gt;
 As &lt;denchmark-link:https://github.com/gargn&gt;@gargn&lt;/denchmark-link&gt;
 mentioned the original issue was resolved in . Please close the issue and open a new issue for the  if it was not already resolved. I think  was also resolved as I could not reproduce the issue. Please check the &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/b3a72cb75dd9b2ed5e05d260ad0247b0/tf_31331_freezing_savedmodel.ipynb&gt;gist here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='12' author='geometrikal' date='2019-08-16T00:30:22Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
  is not resolved - the code you are using in your gist is my workaround for the problem.
I have opened a new issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/31668&gt;#31668&lt;/denchmark-link&gt;
 with a bigger scope to cover the general issue of freezing and loading models with batch normalization.
		</comment>
		<comment id='13' author='geometrikal' date='2019-08-20T16:57:29Z'>
		This should work on the latest nightly. Please reopen if you still have issues with running the script provided above.
		</comment>
		<comment id='14' author='geometrikal' date='2019-08-20T16:57:31Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=31331&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=31331&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='15' author='geometrikal' date='2020-12-17T07:20:31Z'>
		import backend from tf.keras not simply keras, shown below:
from tensorflow.keras import backend as K
		</comment>
	</comments>
</bug>