<bug id='34024' author='bahaelaila7' open_date='2019-11-05T23:59:45Z' closed_time='2019-11-09T03:35:00Z'>
	<summary>Group Convolution not working in TF 2.0.0</summary>
	<description>
The merged PR &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/25818&gt;#25818&lt;/denchmark-link&gt;
 enables group convolution by allowing the input's depth to be multiples of the filter's in_depth parameter rather than exactly equal.
However, as you can see below, whenever I attempt to perform a group convolution, it results in an ambiguous error "No Algorithm Found". Same issue if I make the filter's in_depth = 1.
Making the convolution filter in_depth parameter equal to the channels of the input (=16) it works (regular convolution).
Am I missing something or is this not supported for eager execution?
Tensorflow installed via: pip3 install --upgrade tensorflow-gpu
OS: Ubuntu 18.04.1, kernel: 5.0.0-15 lowlatency.
GTX 1080Ti , cuda: 10.1 , cudnn-7.6.0
(I left tensorflow initialization log in case it provides relevant information)
&lt;denchmark-code&gt;Python 3.7.4 (default, Aug 13 2019, 20:35:49)
[GCC 7.3.0] :: Anaconda, Inc. on linux
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; print(tf.version.GIT_VERSION, tf.version.VERSION)
v2.0.0-rc2-26-g64c3d38 2.0.0
&gt;&gt;&gt; tf.nn.conv2d(tf.random.normal((11,13,17,16)), tf.random.normal((3,5,16//2,7)), padding='SAME',strides=[1,1,1,1]).shape
2019-11-05 18:00:39.800225: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2019-11-05 18:00:39.841613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-05 18:00:39.842211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6705
pciBusID: 0000:1d:00.0
2019-11-05 18:00:39.842279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-05 18:00:39.843083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:1e:00.0
2019-11-05 18:00:39.843414: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-11-05 18:00:39.844684: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-11-05 18:00:39.846034: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-11-05 18:00:39.847021: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-11-05 18:00:39.849473: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-11-05 18:00:39.850970: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-11-05 18:00:39.855126: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-11-05 18:00:39.855289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-05 18:00:39.856005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-05 18:00:39.856903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-05 18:00:39.857518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-05 18:00:39.858304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1
2019-11-05 18:00:39.858525: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-11-05 18:00:39.877459: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3699425000 Hz
2019-11-05 18:00:39.878231: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563f2440c8b0 executing computations on platform Host. Devices:
2019-11-05 18:00:39.878267: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2019-11-05 18:00:40.042173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-05 18:00:40.068620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-05 18:00:40.069774: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563f244af3e0 executing computations on platform CUDA. Devices:
2019-11-05 18:00:40.069799: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1
2019-11-05 18:00:40.069819: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): GeForce GTX 1080 Ti, Compute Capability 6.1
2019-11-05 18:00:40.071588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-05 18:00:40.072539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6705
pciBusID: 0000:1d:00.0
2019-11-05 18:00:40.072670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-05 18:00:40.073607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:1e:00.0
2019-11-05 18:00:40.073652: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-11-05 18:00:40.073666: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-11-05 18:00:40.073678: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-11-05 18:00:40.073688: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-11-05 18:00:40.073699: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-11-05 18:00:40.073710: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-11-05 18:00:40.073720: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-11-05 18:00:40.073771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-05 18:00:40.074334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-05 18:00:40.074891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-05 18:00:40.075465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-05 18:00:40.075960: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1
2019-11-05 18:00:40.075991: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-11-05 18:00:40.077335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-11-05 18:00:40.077350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 1
2019-11-05 18:00:40.077358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N Y
2019-11-05 18:00:40.077364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 1:   Y N
2019-11-05 18:00:40.077488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-05 18:00:40.078794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-05 18:00:40.080681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-05 18:00:40.081210: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10478 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:1d:00.0, compute capability: 6.1)
2019-11-05 18:00:40.081733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-05 18:00:40.082722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10479 MB memory) -&gt; physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:1e:00.0, compute capability: 6.1)
2019-11-05 18:00:40.562956: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-11-05 18:00:41.155546: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at conv_ops.cc:1069 : Not found: No algorithm worked!
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
  File "/home/bahaa/miniconda3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_ops.py", line 1913, in conv2d_v2
    name=name)
  File "/home/bahaa/miniconda3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_ops.py", line 2010, in conv2d
    name=name)
  File "/home/bahaa/miniconda3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_nn_ops.py", line 1039, in conv2d
    _six.raise_from(_core._status_to_exception(e.code, message), None)
  File "&lt;string&gt;", line 3, in raise_from
tensorflow.python.framework.errors_impl.NotFoundError: No algorithm worked! [Op:Conv2D]
&gt;&gt;&gt; tf.nn.conv2d(tf.random.normal((11,13,17,16)), tf.random.normal((3,5,16,7)), padding='SAME',strides=[1,1,1,1]).shape
TensorShape([11, 13, 17, 7])
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='bahaelaila7' date='2019-11-06T10:03:56Z'>
		Could reproduce the issue with TF 2.0 on colab.
Please take a look at the &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/3ea3118c557f04b1f6deda7665b636fd/untitled238.ipynb&gt;gist&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='bahaelaila7' date='2019-11-08T23:49:42Z'>
		I've implemented a GroupConv2D as a tf.keras.layers.Layer by taking inspiration from &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/layers/convolutional.py#L391-L498&gt;the sources of Conv2D&lt;/denchmark-link&gt;
.
This seems to work using tensorflow 2.0.0 (however, only on GPU. On CPU it crashes if groups != 1, because there is no implementation for the group convolutions).
The thing is that tf.nn.conv2d seems to be the only thing that supports it right now.
I also tried to add the group parameter to the base class tf.python.keras.layers.convolutional.Conv, but the conv op that end up being called just crashes with the group-based kernel shape.
import tensorflow as tf
from tensorflow.python.framework import tensor_shape
from tensorflow.python.keras.utils import conv_utils
import tensorflow.keras.activations as activations
import tensorflow.keras.regularizers as regularizers
import tensorflow.keras.initializers as initializers
import tensorflow.keras.constraints as constraints
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import nn


class GroupConvBase(tf.keras.layers.Layer):

    def __init__(self, rank, filters, kernel_size, groups=1, strides=1, padding='VALID', data_format=None,
                 dilation_rate=1,
                 activation=None, use_bias=True, kernel_initializer='glorot_uniform',
                 bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None,
                 kernel_constraint=None, bias_constraint=None, **kwargs):
        super().__init__(activity_regularizer=activity_regularizer, **kwargs)
        if filters % groups != 0:
            raise ValueError("Groups must divide filters evenly, but got {}/{}".format(filters, groups))

        self.filters = filters
        self.groups = groups
        self.kernel_size = conv_utils.normalize_tuple(kernel_size, rank, 'kernel_size')
        self.data_format = data_format
        self.padding = padding
        self.strides = conv_utils.normalize_tuple(strides, rank, 'strides')
        self.dilation_rate = conv_utils.normalize_tuple(dilation_rate, rank, 'dilation_rate')
        self.activation = activations.get(activation)
        self.use_bias = use_bias
        self.kernel_initializer = initializers.get(kernel_initializer)
        self.bias_initializer = initializers.get(bias_initializer)
        self.kernel_regularizer = regularizers.get(kernel_regularizer)
        self.bias_regularizer = regularizers.get(bias_regularizer)
        self.kernel_constraint = constraints.get(kernel_constraint)
        self.bias_constraint = constraints.get(bias_constraint)

    def build(self, input_shape):
        input_shape = tensor_shape.TensorShape(input_shape)
        if conv_utils.normalize_data_format(self.data_format) == 'channels_first':
            channel_axis = 1
        else:
            channel_axis = -1
        if input_shape.dims[channel_axis].value is None:
            raise ValueError('The channel dimension of the inputs '
                             'should be defined. Found `None`.')
        input_dim = int(input_shape[channel_axis])
        kernel_shape = self.kernel_size + (input_dim // self.groups, self.filters)

        self.kernel = self.add_weight(
            name='kernel',
            shape=kernel_shape,
            initializer=self.kernel_initializer,
            regularizer=self.kernel_regularizer,
            constraint=self.kernel_constraint,
            trainable=True,
            dtype=self.dtype)
        if self.use_bias:
            self.bias = self.add_weight(
                name='bias',
                shape=(self.filters,),
                initializer=self.bias_initializer,
                regularizer=self.bias_regularizer,
                constraint=self.bias_constraint,
                trainable=True,
                dtype=self.dtype)
        else:
            self.bias = None

        self.built = True

    def call(self, inputs):
        outputs = tf.nn.conv2d(inputs, self.kernel, strides=self.strides,
                                       data_format=self.data_format, dilations=self.dilation_rate,
                                       name=self.name,
                                       padding=self.padding)

        if self.use_bias:
            if self.data_format == 'channels_first':
                if self.rank == 1:
                    # nn.bias_add does not accept a 1D input tensor.
                    bias = array_ops.reshape(self.bias, (1, self.filters, 1))
                    outputs += bias
                else:
                    outputs = nn.bias_add(outputs, self.bias, data_format='NCHW')
            else:
                outputs = nn.bias_add(outputs, self.bias, data_format='NHWC')

        if self.activation is not None:
            return self.activation(outputs)
        return outputs

    def get_config(self):
        config = {
            'filters': self.filters,
            'kernel_size': self.kernel_size,
            "groups": self.groups,
            'strides': self.strides,
            'padding': self.padding,
            'data_format': self.data_format,
            'dilation_rate': self.dilation_rate,
            'activation': activations.serialize(self.activation),
            'use_bias': self.use_bias,
            'kernel_initializer': initializers.serialize(self.kernel_initializer),
            'bias_initializer': initializers.serialize(self.bias_initializer),
            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),
            'bias_regularizer': regularizers.serialize(self.bias_regularizer),
            'activity_regularizer':
                regularizers.serialize(self.activity_regularizer),
            'kernel_constraint': constraints.serialize(self.kernel_constraint),
            'bias_constraint': constraints.serialize(self.bias_constraint)
        }
        return {list(super(GroupConvBase, self).get_config().items()) + list(config.items())}


class GroupConv2D(GroupConvBase):

    def __init__(self, filters, kernel_size,
                 groups, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None,
                 use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None,
                 bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None,
                 **kwargs):
        super(GroupConv2D, self).__init__(rank=2,
                                          filters=filters, kernel_size=kernel_size, groups=groups, strides=strides,
                                          padding=padding.upper(),
                                          data_format=data_format, dilation_rate=dilation_rate,
                                          activation=activations.get(activation),
                                          use_bias=use_bias, kernel_initializer=initializers.get(kernel_initializer),
                                          bias_initializer=initializers.get(bias_initializer),
                                          kernel_regularizer=regularizers.get(kernel_regularizer),
                                          bias_regularizer=regularizers.get(bias_regularizer),
                                          activity_regularizer=regularizers.get(activity_regularizer),
                                          kernel_constraint=constraints.get(kernel_constraint),
                                          bias_constraint=constraints.get(bias_constraint),
                                          **kwargs)


if __name__ == '__main__':
    gc = GroupConv2D(16, (3, 3), groups=4, padding="same")
    with tf.GradientTape() as tape:
        out = gc(tf.random.normal(shape=(8, 32, 32, 16)))
        summed = tf.reduce_sum(out)
        grad = tape.gradient(summed, gc.variables)
        print(grad)
		</comment>
		<comment id='3' author='bahaelaila7' date='2019-11-09T03:35:00Z'>
		&lt;denchmark-link:https://github.com/RunOrVeith&gt;@RunOrVeith&lt;/denchmark-link&gt;
 Thanks for verifying that it works for GPU.
I realized my mistake, my bad!!
The filter output should also be a multiple of the groups G,  (since any operation will produce G output featuremaps). Therefore,
&lt;denchmark-code&gt;&gt;&gt;&gt; C= 30  #input's depth
&gt;&gt;&gt; G= 5 #a divisor of the input's depth
&gt;&gt;&gt; K= 7 #multiples of G for the ouput featuremaps
&gt;&gt;&gt; tf.nn.conv2d(tf.random.normal((11,13,17,C)), tf.random.normal((3,5,C//G, G*K)), padding='SAME',strides=[1,1]).shape 
TensorShape([11, 13, 17, 35])
&lt;/denchmark-code&gt;

while
&lt;denchmark-code&gt;&gt;&gt;&gt; tf.nn.conv2d(tf.random.normal((11,13,17,C)), tf.random.normal((3,5,C//G, G*K + 1)), padding='SAME',strides=[1,1]).shape 

---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
&lt;ipython-input-27-10c09d6bfa5d&gt; in &lt;module&gt;
      2 G= 5 #a divisor of the input's depth
      3 K= 7 #multiples of G for the ouput featuremaps
----&gt; 4 tf.nn.conv2d(tf.random.normal((11,13,17,C)), tf.random.normal((3,5,C//G, G*K+1)), padding='SAME',strides=[1,1]).shape

~/miniconda3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_ops.py in conv2d_v2(input, filters, strides, padding, data_format, dilations, name)
   1911                 data_format=data_format,
   1912                 dilations=dilations,
-&gt; 1913                 name=name)
   1914 
   1915 

~/miniconda3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_ops.py in conv2d(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name, filters)
   2008                            data_format=data_format,
   2009                            dilations=dilations,
-&gt; 2010                            name=name)
   2011 
   2012 

~/miniconda3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_nn_ops.py in conv2d(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)
   1037       else:
   1038         message = e.message
-&gt; 1039       _six.raise_from(_core._status_to_exception(e.code, message), None)
   1040   # Add nodes to the TensorFlow graph.
   1041   if not isinstance(strides, (list, tuple)):

~/miniconda3.7/lib/python3.7/site-packages/six.py in raise_from(value, from_value)

NotFoundError: No algorithm worked! [Op:Conv2D]
&lt;/denchmark-code&gt;

I think whenever the keras Conv2D gets to support this, there should be checks for the filter output as well (divides by G evenly). Ie, both input's depth and output's depth should divide by G evenly.
Thanks again!
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/25818&gt;#25818&lt;/denchmark-link&gt;
 was only about supporting group convolutions through cuDNN, I am not sure whether I should keep this issue open because the CPU one isn't implemented yet (given the generic issue title).
		</comment>
		<comment id='4' author='bahaelaila7' date='2019-11-09T03:35:02Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34024&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34024&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='bahaelaila7' date='2020-06-04T13:23:12Z'>
		
I've implemented a GroupConv2D as a tf.keras.layers.Layer by taking inspiration from the sources of Conv2D.
This seems to work using tensorflow 2.0.0 (however, only on GPU. On CPU it crashes if groups != 1, because there is no implementation for the group convolutions).
The thing is that tf.nn.conv2d seems to be the only thing that supports it right now.
I also tried to add the group parameter to the base class tf.python.keras.layers.convolutional.Conv, but the conv op that end up being called just crashes with the group-based kernel shape.
import tensorflow as tf
from tensorflow.python.framework import tensor_shape
from tensorflow.python.keras.utils import conv_utils
import tensorflow.keras.activations as activations
import tensorflow.keras.regularizers as regularizers
import tensorflow.keras.initializers as initializers
import tensorflow.keras.constraints as constraints
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import nn


class GroupConvBase(tf.keras.layers.Layer):

    def __init__(self, rank, filters, kernel_size, groups=1, strides=1, padding='VALID', data_format=None,
                 dilation_rate=1,
                 activation=None, use_bias=True, kernel_initializer='glorot_uniform',
                 bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None,
                 kernel_constraint=None, bias_constraint=None, **kwargs):
        super().__init__(activity_regularizer=activity_regularizer, **kwargs)
        if filters % groups != 0:
            raise ValueError("Groups must divide filters evenly, but got {}/{}".format(filters, groups))

        self.filters = filters
        self.groups = groups
        self.kernel_size = conv_utils.normalize_tuple(kernel_size, rank, 'kernel_size')
        self.data_format = data_format
        self.padding = padding
        self.strides = conv_utils.normalize_tuple(strides, rank, 'strides')
        self.dilation_rate = conv_utils.normalize_tuple(dilation_rate, rank, 'dilation_rate')
        self.activation = activations.get(activation)
        self.use_bias = use_bias
        self.kernel_initializer = initializers.get(kernel_initializer)
        self.bias_initializer = initializers.get(bias_initializer)
        self.kernel_regularizer = regularizers.get(kernel_regularizer)
        self.bias_regularizer = regularizers.get(bias_regularizer)
        self.kernel_constraint = constraints.get(kernel_constraint)
        self.bias_constraint = constraints.get(bias_constraint)

    def build(self, input_shape):
        input_shape = tensor_shape.TensorShape(input_shape)
        if conv_utils.normalize_data_format(self.data_format) == 'channels_first':
            channel_axis = 1
        else:
            channel_axis = -1
        if input_shape.dims[channel_axis].value is None:
            raise ValueError('The channel dimension of the inputs '
                             'should be defined. Found `None`.')
        input_dim = int(input_shape[channel_axis])
        kernel_shape = self.kernel_size + (input_dim // self.groups, self.filters)

        self.kernel = self.add_weight(
            name='kernel',
            shape=kernel_shape,
            initializer=self.kernel_initializer,
            regularizer=self.kernel_regularizer,
            constraint=self.kernel_constraint,
            trainable=True,
            dtype=self.dtype)
        if self.use_bias:
            self.bias = self.add_weight(
                name='bias',
                shape=(self.filters,),
                initializer=self.bias_initializer,
                regularizer=self.bias_regularizer,
                constraint=self.bias_constraint,
                trainable=True,
                dtype=self.dtype)
        else:
            self.bias = None

        self.built = True

    def call(self, inputs):
        outputs = tf.nn.conv2d(inputs, self.kernel, strides=self.strides,
                                       data_format=self.data_format, dilations=self.dilation_rate,
                                       name=self.name,
                                       padding=self.padding)

        if self.use_bias:
            if self.data_format == 'channels_first':
                if self.rank == 1:
                    # nn.bias_add does not accept a 1D input tensor.
                    bias = array_ops.reshape(self.bias, (1, self.filters, 1))
                    outputs += bias
                else:
                    outputs = nn.bias_add(outputs, self.bias, data_format='NCHW')
            else:
                outputs = nn.bias_add(outputs, self.bias, data_format='NHWC')

        if self.activation is not None:
            return self.activation(outputs)
        return outputs

    def get_config(self):
        config = {
            'filters': self.filters,
            'kernel_size': self.kernel_size,
            "groups": self.groups,
            'strides': self.strides,
            'padding': self.padding,
            'data_format': self.data_format,
            'dilation_rate': self.dilation_rate,
            'activation': activations.serialize(self.activation),
            'use_bias': self.use_bias,
            'kernel_initializer': initializers.serialize(self.kernel_initializer),
            'bias_initializer': initializers.serialize(self.bias_initializer),
            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),
            'bias_regularizer': regularizers.serialize(self.bias_regularizer),
            'activity_regularizer':
                regularizers.serialize(self.activity_regularizer),
            'kernel_constraint': constraints.serialize(self.kernel_constraint),
            'bias_constraint': constraints.serialize(self.bias_constraint)
        }
        return {list(super(GroupConvBase, self).get_config().items()) + list(config.items())}


class GroupConv2D(GroupConvBase):

    def __init__(self, filters, kernel_size,
                 groups, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None,
                 use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None,
                 bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None,
                 **kwargs):
        super(GroupConv2D, self).__init__(rank=2,
                                          filters=filters, kernel_size=kernel_size, groups=groups, strides=strides,
                                          padding=padding.upper(),
                                          data_format=data_format, dilation_rate=dilation_rate,
                                          activation=activations.get(activation),
                                          use_bias=use_bias, kernel_initializer=initializers.get(kernel_initializer),
                                          bias_initializer=initializers.get(bias_initializer),
                                          kernel_regularizer=regularizers.get(kernel_regularizer),
                                          bias_regularizer=regularizers.get(bias_regularizer),
                                          activity_regularizer=regularizers.get(activity_regularizer),
                                          kernel_constraint=constraints.get(kernel_constraint),
                                          bias_constraint=constraints.get(bias_constraint),
                                          **kwargs)


if __name__ == '__main__':
    gc = GroupConv2D(16, (3, 3), groups=4, padding="same")
    with tf.GradientTape() as tape:
        out = gc(tf.random.normal(shape=(8, 32, 32, 16)))
        summed = tf.reduce_sum(out)
        grad = tape.gradient(summed, gc.variables)
        print(grad)

Do you sure this achieve the goal of grouped convolution?
		</comment>
	</comments>
</bug>