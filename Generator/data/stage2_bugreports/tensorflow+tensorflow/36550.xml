<bug id='36550' author='Flamefire' open_date='2020-02-07T16:52:56Z' closed_time='2020-03-18T22:01:45Z'>
	<summary>TF_CONFIG is used even though a ClusterResolver was passed</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Mint
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.1.0
Python version: 3.7.4

Describe the current behavior
I'm using a MultiWorkerMirroredStrategy with a cluster_resolver to avoid having to define the (to me obscure) TF_CONFIG env variable. But even though TF_CONFIG is used: 


tensorflow/tensorflow/python/distribute/distribute_coordinator.py


         Line 752
      in
      925be10






 tf_config = json.loads(os.environ.get("TF_CONFIG", "{}")) 





This leads to output like:

WARNING:tensorflow:Skipped evaluation since eval_fn is not passed in.

Which can only happen when no cluster-spec was found: 


tensorflow/tensorflow/python/distribute/distribute_coordinator.py


        Lines 778 to 787
      in
      925be10






 if not cluster_spec: 



 # `mode` is ignored in the local case. 



 logging.info("Running local Distribute Coordinator.") 



 _run_single_worker(worker_fn, strategy, None, None, None, session_config, 



 rpc_layer) 



 if eval_fn: 



 _run_single_worker(eval_fn, eval_strategy, None, None, None, 



 session_config, rpc_layer) 



 else: 



 logging.warning("Skipped evaluation since `eval_fn` is not passed in.") 





Describe the expected behavior
There should be no need to define this env variable and it should not be used but everything should come from the cluster resolver
Code to reproduce the issue
&lt;denchmark-link:https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#train_the_model_with_multiworkermirroredstrategy&gt;https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#train_the_model_with_multiworkermirroredstrategy&lt;/denchmark-link&gt;
 but with SlurmClusterResolver
	</description>
	<comments>
		<comment id='1' author='Flamefire' date='2020-03-18T22:01:45Z'>
		This should be &lt;denchmark-link:32aeb9957ede5496aa76a2d9a0b7b202d76a24fc&gt;fixed&lt;/denchmark-link&gt;
 now. Can you try with the latest nightly release to see if you are able to run without issues?
		</comment>
		<comment id='2' author='Flamefire' date='2020-03-18T22:01:46Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36550&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36550&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='Flamefire' date='2020-03-19T08:27:57Z'>
		&lt;denchmark-link:https://github.com/anj-s&gt;@anj-s&lt;/denchmark-link&gt;
 This doesn't look correct. 
 shows that the TF_CONFIG variable takes precedence over the cluster_resolver explicitly passed in. This means different parts of the code might use different information depending on whether they use the TF_CONFIG or the cluster_resolver.
IIRC the strategy (now) always has a cluster resolver and defaults to a resolver using TF_CONFIG. So why not always use the cluster resolver?
Oh and the documentation is incomplete now: 


tensorflow/tensorflow/python/distribute/distribute_coordinator.py


        Lines 703 to 704
      in
      cf46f78






   The `cluster_spec` can be either passed by the argument or parsed from the 



   "TF_CONFIG" environment variable. Example of a TF_CONFIG: 





		</comment>
		<comment id='4' author='Flamefire' date='2020-03-19T17:36:06Z'>
		&lt;denchmark-link:https://github.com/Flamefire&gt;@Flamefire&lt;/denchmark-link&gt;
 Are you setting the TF_CONFIG and also passing a cluster resolver argument to the strategy? We generally expect users to either use TF_CONFIG or use a cluster resolver.
This codepath is used as an entry point for different use cases which is why we need this. For MultiWorkerMirroredStrategy we can default to the cluster resolver as the single source of truth but that is not the case for all use cases of this function. We will be refactoring this soon so hopefully things will look clearer in the coming months.
		</comment>
		<comment id='5' author='Flamefire' date='2020-03-20T15:03:45Z'>
		Currently (2.1.0) yes, because only using TF_CONFIG doesn't allow everything the resolver does, e.g. setting num_gpus.

For MultiWorkerMirroredStrategy we can default to the cluster resolver as the single source of truth but that is not the case for all use cases of this function.

Wouldn't you agree, that if the strategy passed to this function has a cluster resolver, then it should be used?
Additionally: Instead of having multiple sources of truth and multiple parsing of TF_CONFIG why not always get the cluster_resolver from the strategy and default to the TfConfigClusterResolver if none is set? This would make it consistent with MultiWorkerMirroredStrategy and avoids differences in parsing the TF_CONFIG variable.
BTW: What are the different use cases? What strategy would be valid here that does not have a cluster_resolver but requires setting TF_CONFIG? Wouldn't it make sense to require a strategy with a cluster_resolver?
		</comment>
		<comment id='6' author='Flamefire' date='2020-08-31T13:04:49Z'>
		FTR: This was only fully fixed in 2.3.0 with &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/32aeb9957ede5496aa76a2d9a0b7b202d76a24fc&gt;32aeb99&lt;/denchmark-link&gt;
 not in 2.2.0 as indicated here. Hence until 2.3.0 it is required to set TF_CONFIG  to the cluster_resolver
		</comment>
	</comments>
</bug>