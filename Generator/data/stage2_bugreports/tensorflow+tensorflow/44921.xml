<bug id='44921' author='Lasica' open_date='2020-11-17T00:27:03Z' closed_time='2020-11-21T16:05:23Z'>
	<summary>tensorflow.keras.layers.Softmax axis parameter not behaving as described in documentation.</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS: Archlinux
TensorFlow installed from: archlinux repository
TensorFlow version: 2.3.1
Python version: 3.8.6
CUDA/cuDNN version: not relevant
GPU model and memory: not relevant

Current behavior:
The example code raises TypeError: '&lt;=' not supported between instances of 'int' and 'tuple'
Expected behavior
No error in example code. Both softmax layer and activation from softmax should have the same result (within eps).
According to &lt;denchmark-link:https://keras.io/api/layers/activation_layers/softmax/&gt;documentation&lt;/denchmark-link&gt;
 axis can be a single value or list of values. keras.activations.softmax is customly written to handle many axis, while the layer uses tensorflow.nn.softmax which treats axis parameter as a single value. I report it as a bug, since I think it should behave as described in documentation, and not the other way around.
Standalone code to reproduce the issue
import tensorflow as tf
import numpy as np
t = tf.convert_to_tensor(np.random.rand(4,2,2))
assert np.all(tf.keras.layers.Softmax(axis=(1,2))(t) - tf.keras.activations.softmax(t, axis=(1,2)) &lt; tf.keras.backend.epsilon()) 
	</description>
	<comments>
		<comment id='1' author='Lasica' date='2020-11-17T08:39:23Z'>
		&lt;denchmark-link:https://github.com/Lasica&gt;@Lasica&lt;/denchmark-link&gt;

I ran the code shared and face &lt;denchmark-link:https://colab.research.google.com/gist/Saduf2019/f0f3041fb7af9e4325788153148671b4/untitled461.ipynb&gt;this issue&lt;/denchmark-link&gt;
, please confirm.
		</comment>
		<comment id='2' author='Lasica' date='2020-11-17T16:01:53Z'>
		This is weird, I do not have your issue.
Here's what I get in ipython3:
&lt;denchmark-code&gt;In [1]: import tensorflow as tf
   ...: import numpy as np
   ...: t = tf.convert_to_tensor(np.random.rand(4,2,2))
   ...: assert np.all(tf.keras.layers.Softmax(axis=(1,2))(t) - tf.keras.activations.softmax(t, axis=(1,2)) &lt; tf.keras.backend.epsilon())
WARNING:tensorflow:Layer softmax is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-1-58e4e0196866&gt; in &lt;module&gt;
      2 import numpy as np
      3 t = tf.convert_to_tensor(np.random.rand(4,2,2))
----&gt; 4 assert np.all(tf.keras.layers.Softmax(axis=(1,2))(t) - tf.keras.activations.softmax(t, axis=(1,2)) &lt; tf.keras.backend.epsilon())

/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
    983 
    984         with ops.enable_auto_cast_variables(self._compute_dtype_object):
--&gt; 985           outputs = call_fn(inputs, *args, **kwargs)
    986 
    987         if self._activity_regularizer:

/usr/lib/python3.8/site-packages/tensorflow/python/keras/layers/advanced_activations.py in call(self, inputs)
    282 
    283   def call(self, inputs):
--&gt; 284     return K.softmax(inputs, axis=self.axis)
    285 
    286   def get_config(self):

/usr/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)
    199     """Call target, and fall back on dispatchers if there is a TypeError."""
    200     try:
--&gt; 201       return target(*args, **kwargs)
    202     except (TypeError, ValueError):
    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a

/usr/lib/python3.8/site-packages/tensorflow/python/keras/backend.py in softmax(x, axis)
   4607       A tensor.
   4608   """
-&gt; 4609   return nn.softmax(x, axis=axis)
   4610 
   4611 

/usr/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)
    199     """Call target, and fall back on dispatchers if there is a TypeError."""
    200     try:
--&gt; 201       return target(*args, **kwargs)
    202     except (TypeError, ValueError):
    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a

/usr/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)
    505                 'in a future version' if date is None else ('after %s' % date),
    506                 instructions)
--&gt; 507       return func(*args, **kwargs)
    508 
    509     doc = _add_deprecated_arg_notice_to_docstring(

/usr/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py in softmax(logits, axis, name, dim)
   3638   if axis is None:
   3639     axis = -1
-&gt; 3640   return _softmax(logits, gen_nn_ops.softmax, axis, name)
   3641 
   3642 

/usr/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py in _softmax(logits, compute_op, dim, name)
   3561   if isinstance(dim, ops.Tensor):
   3562     dim_val = tensor_util.constant_value(dim)
-&gt; 3563   if dim_val is not None and not -shape.ndims &lt;= dim_val &lt; shape.ndims:
   3564     raise errors_impl.InvalidArgumentError(
   3565         None, None,

TypeError: '&lt;=' not supported between instances of 'int' and 'tuple'

In [2]: tf.version
Out[2]: &lt;module 'tensorflow._api.v2.version' from '/usr/lib/python3.8/site-packages/tensorflow/_api/v2/version/__init__.py'&gt;

In [3]: tf.version.COMPILER_VERSION
Out[3]: '9.3.0'

In [4]: tf.version.VERSION
Out[4]: '2.3.1'
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='Lasica' date='2020-11-17T16:05:59Z'>
		import tensorflow as tf
import numpy as np
t = tf.convert_to_tensor(np.random.rand(4,2,2), dtype=tf.dtypes.float32)
assert np.all(tf.keras.layers.Softmax(axis=(1,2))(t) - tf.keras.activations.softmax(t, axis=(1,2)) &lt; tf.keras.backend.epsilon())
Here's version without the warning that could have caused some issues for you due to default float numpy and epsilon type mismatch.
		</comment>
		<comment id='4' author='Lasica' date='2020-11-17T17:47:06Z'>
		&lt;denchmark-link:https://github.com/Lasica&gt;@Lasica&lt;/denchmark-link&gt;
 Agree with you. Looks like this was a bug in . However, this was resolved in .
&lt;denchmark-link:https://colab.research.google.com/gist/jvishnuvardhan/b3336de39d7121952651948a62c3d868/untitled65.ipynb&gt;Here&lt;/denchmark-link&gt;
 is a gist with  and &lt;denchmark-link:https://colab.research.google.com/gist/jvishnuvardhan/88687303fc215c773e83a99cf597b1fa/untitled.ipynb&gt;here&lt;/denchmark-link&gt;
 is a gist with . Thanks!
Please verify once and close the issue. Thanks!
		</comment>
		<comment id='5' author='Lasica' date='2020-11-21T16:05:21Z'>
		I am closing this issue as this was resolved in tf-nightly. Please feel free to reopen if I am mistaken. Thanks!
		</comment>
		<comment id='6' author='Lasica' date='2020-11-21T16:05:24Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44921&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44921&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>