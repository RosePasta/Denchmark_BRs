<bug id='10730' author='zhaoerchao' open_date='2017-06-15T12:00:24Z' closed_time='2017-12-21T18:29:33Z'>
	<summary>All the graph have an operation _Retval that makes the train loop slow</summary>
	<description>
I have tried to train the model in &lt;denchmark-link:https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py&gt;tf_cnn_benchmarks.py&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py&gt;cifar10_multi_gpu_train.py&lt;/denchmark-link&gt;
.
All the timelines results like the image bellow. There is an operation named '_Retval' in the end of training loop. But as we see, long idle time was leaved in the timeline. How can I make the training faster by getting rid of the operation '_Retval' ? Is there other method to let the gpu be at full power?
&lt;denchmark-link:https://user-images.githubusercontent.com/9522983/27180240-0e26c2f0-5205-11e7-9f7e-98be45a6a2b2.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/9522983/27180242-120350fa-5205-11e7-89f1-5d3fe6261293.png&gt;&lt;/denchmark-link&gt;

The images above were got by the script:
python tf_cnn_benchmarks.py local_parameter_device=cpu --num_gpus=2 --batch_size=64 --model=vgg16 --variable_update=independent --optimizer=sgd --trace_file=/home/zhaoerchao/timeline_benchmark.json --distortions
	</description>
	<comments>
		<comment id='1' author='zhaoerchao' date='2017-06-16T00:00:47Z'>
		&lt;denchmark-link:https://github.com/zhangyaobit&gt;@zhangyaobit&lt;/denchmark-link&gt;
, could you take a look?
		</comment>
		<comment id='2' author='zhaoerchao' date='2017-06-16T23:01:14Z'>
		Could you mark the idle period and  _Retval  in the two images?
Could you also clarify what are the two timelines/images respectively for?  Are they for tf_cnn_benchmarks.py or cifar10_multi_gpu_train.py?
		</comment>
		<comment id='3' author='zhaoerchao' date='2017-06-19T09:52:31Z'>
		Thank you for your apply.
The two images in the question are all for tf_cnn_benchmarks.py
The second one is zoomed in of the first one. The _Retval operation is in the end of the image. The idle period is the period without operation that is obviously.
		</comment>
		<comment id='4' author='zhaoerchao' date='2017-06-20T04:51:22Z'>
		Could you upload the timeline file somewhere, so that someone of us can take a look?
		</comment>
		<comment id='5' author='zhaoerchao' date='2017-06-21T00:56:16Z'>
		Sorry, I am on vacation now.  I will upload the timeline as soon as I come back.
		</comment>
		<comment id='6' author='zhaoerchao' date='2017-06-26T01:30:22Z'>
		&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/1100812/timeline_benchmark.json.txt&gt;timeline_benchmark.json.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/zhangyaobit&gt;@zhangyaobit&lt;/denchmark-link&gt;
 The timeline file is uploaded, looking forward to your apply !
		</comment>
		<comment id='7' author='zhaoerchao' date='2017-06-26T20:10:07Z'>
		&lt;denchmark-link:https://github.com/zhaoerchao&gt;@zhaoerchao&lt;/denchmark-link&gt;
 could you use a python timer to measure the total time of, say 100 steps, and then compute time per step? This will help identify if this is  an issue of Timeline or tf_cnn_benchmarks.
		</comment>
		<comment id='8' author='zhaoerchao' date='2017-06-27T01:16:51Z'>
		The script has measured the total time and the time per step, see &lt;denchmark-link:https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py#L670&gt;time measure code&lt;/denchmark-link&gt;
. I think it will have a significance performance improvement if the problem of the GPU idle time is solved.  Can you help to analysis the result of the script ?
		</comment>
		<comment id='9' author='zhaoerchao' date='2017-06-27T20:19:45Z'>
		What is the time per step measured by the script? If it is the time without gap, it means the gap is an artifact/bug of Timeline; if it is the time including the gap, it means the gap is a performance issue of tf_cnn_benchmarks.py.
		</comment>
		<comment id='10' author='zhaoerchao' date='2017-06-28T01:36:37Z'>
		&lt;denchmark-link:https://github.com/zhangyaobit&gt;@zhangyaobit&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
  Thank you for your suggestion. The time per step measured includes the gap. I have doubted it is a performance issue of tf_cnn_benchmarks.py. But I tried many ways to train the model, all of them have this issue. But when I monitor the GPU usage by , gpu is always 95%+ used during training time. But the timeline still contains the long idle period. And &lt;denchmark-link:https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py&gt;cifar10_multi_gpu_train.py&lt;/denchmark-link&gt;
 also has this issue.
Besides, when I run the script on one single GPU:
python tf_cnn_benchmarks.py local_parameter_device=cpu --num_gpus= 1 --batch_size=64 --model=resnet50 --variable_update=parameter_server --optimizer=sgd
I got the timeline &lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/1107295/timeline_benchmark_origin.json.txt&gt;timeline_benchmark_origin.json.txt&lt;/denchmark-link&gt;
 when the script had no change and &lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/1107297/timeline_benchmark_changed.json.txt&gt;timeline_benchmark_changed.json.txt&lt;/denchmark-link&gt;
 when &lt;denchmark-link:https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py#L933&gt;this line&lt;/denchmark-link&gt;
 was replaced by .
The performance is significantly improved when the operation global_step.assign_add has no dependencies. But the improvement is only useful on the special step:
&lt;denchmark-code&gt;Starting real work at step 10 at time Wed Jun 28 09:49:39 2017
Done warm up
Step    Img/sec loss
1       images/sec: 544.0 +/- 0.0 (jitter = 0.0)        6.776
10      images/sec: 207.7 +/- 33.2 (jitter = 2.1)       5.985
20      images/sec: 201.4 +/- 17.0 (jitter = 1.8)       5.563
30      images/sec: 199.3 +/- 11.4 (jitter = 1.4)       5.343
40      images/sec: 198.2 +/- 8.6 (jitter = 1.0)        5.255
50      images/sec: 197.5 +/- 6.9 (jitter = 1.0)        5.196
60      images/sec: 197.0 +/- 5.8 (jitter = 1.0)        5.167
70      images/sec: 196.6 +/- 5.0 (jitter = 0.9)        5.145
80      images/sec: 196.3 +/- 4.3 (jitter = 0.9)        5.125
90      images/sec: 196.0 +/- 3.9 (jitter = 1.0)        5.111
Finishing real work at step 109 at time Wed Jun 28 09:50:11 2017
----------------------------------------------------------------
total images/sec: 192.95
----------------------------------------------------------------
&lt;/denchmark-code&gt;

May I ask you to clone the repository &lt;denchmark-link:https://github.com/tensorflow/benchmarks&gt;tensorflow high performance model benchmark&lt;/denchmark-link&gt;
 and run it to analysis the reason ?
Thank you very much!
		</comment>
		<comment id='11' author='zhaoerchao' date='2017-06-29T04:52:19Z'>
		&lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/yzhwang&gt;@yzhwang&lt;/denchmark-link&gt;
, could you or re-assign this to someone who is familiar with tf_cnn_benchmarks.py to take a look?
		</comment>
		<comment id='12' author='zhaoerchao' date='2017-07-26T16:24:53Z'>
		Can anyone look into this further? I'm also experiencing this issue, and it seems to me that this is either a serious issue with tensorflow's kernel execution/flow management (unlikely) or a bug of the profiler being unable to log certain operations (more probable). Either way, it is difficult to optimize a tensorflow-written pipeline when information provided from the profiling methods is opaque.
&lt;denchmark-link:https://user-images.githubusercontent.com/14655667/28632229-d6163a1c-71fd-11e7-8ea6-efdc0eb50930.png&gt;&lt;/denchmark-link&gt;

(final op, in green, is _RetVal)
		</comment>
		<comment id='13' author='zhaoerchao' date='2017-07-26T17:10:14Z'>
		Including &lt;denchmark-link:https://github.com/prb12&gt;@prb12&lt;/denchmark-link&gt;
 since it is related to timeline. Paul could you provide some advice here? Thank you!
		</comment>
		<comment id='14' author='zhaoerchao' date='2017-07-27T15:38:47Z'>
		I agree that this looks strange.  The code is clearly waiting for a bunch of operations to complete via control dependencies (group_deps_3), and it's not clear to me from the timeline where those are executing or which one is 'slow'.
The reason for that seems to be that the device level GPU tracing (in the /gpu:0/memcpy  and /gpu:0/stream:N timelines) looks to be truncated after 12ms.  The Tensorflow GPU device is still busy queueing up CUDA kernel launches, so we know there ought to be more activity on the CUDA streams!
&lt;denchmark-link:https://user-images.githubusercontent.com/11547801/28678542-81fd0462-72a5-11e7-8c18-0264b1c24fb3.png&gt;&lt;/denchmark-link&gt;

There is probably a load of work still running on the GPU, followed by some memcpyDtoHs to transfer the results back to the host.  Eventually TF will have to wait for all of these to complete before it can update your parameters or return fetched values to the caller of session.run.
The 'group_dependencies' you commented out are (intentionally) waiting for all of that work to complete.   ops are the ops that return results from a TensorFlow function, and they would also typically be the point where you would need to copy results back from the device and/or sync the GPU.  (I'm not sure why this model is using functions... &lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
 could probably comment here)
So, to me it looks like either something is broken in the GPUTracer, or possibly the version of libcupti or the cuda drivers on your machine.
I would suggest a couple of experiments:

Try capturing a trace using the NVidia tools  -- e.g. nvprof --print_gpu_trace or use nvvp if you prefer.
Turn on some logging in the GPUTracer https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/default/gpu_tracer.cc

Paul
		</comment>
		<comment id='15' author='zhaoerchao' date='2017-07-27T18:01:12Z'>
		I agree nvprof and nvvp provide very useful info.
		</comment>
		<comment id='16' author='zhaoerchao' date='2017-12-20T19:14:48Z'>
		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
		</comment>
		<comment id='17' author='zhaoerchao' date='2017-12-21T18:29:33Z'>
		Closing due to lack of activity, but please reopen if this still needs attention.
		</comment>
	</comments>
</bug>