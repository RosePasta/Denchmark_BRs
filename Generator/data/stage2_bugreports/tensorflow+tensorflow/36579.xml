<bug id='36579' author='maxima120' open_date='2020-02-08T17:28:13Z' closed_time='2020-03-04T19:27:06Z'>
	<summary>TPU proto_buf error after migration from 1.3 to 2.1</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9
TensorFlow installed from (source or binary):  GCP TF 2.1 image
TensorFlow version (use command below):  TF v: 2.1.0 Keras v: 2.2.4-tf
Python version: 3.5.3
TPU software version is 2.1

I tried sample from here and it works: &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy&gt;https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy&lt;/denchmark-link&gt;

When I try to convert my own, perfectly working TPU model, from 1.3 to 2.1 using DistributedStrategy it fails.
When I run the below code in Jupyter, kernel dies when it goes to fit
&lt;denchmark-code&gt;from __future__ import absolute_import, division, print_function, unicode_literals

import tensorflow as tf
import tensorflow.keras as k

print('TF v:', tf.__version__, 'Keras v:', k.__version__)

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://xx.xx.xx.xx:8470')
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)    

strategy = tf.distribute.experimental.TPUStrategy(resolver) 
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;with strategy.scope():
    
    model = k.Sequential()
    model.add(k.layers.Conv1D(filters=16,  kernel_size=2, activation = 'relu', input_shape=(window_size, 1) ))
    model.add(k.layers.Conv1D(filters=32,  kernel_size=2, activation = 'relu'))
    model.add(k.layers.Conv1D(filters=64,  kernel_size=2, activation = 'relu'))
    model.add(k.layers.Conv1D(filters=128, kernel_size=2, activation = 'relu'))
    model.add(k.layers.MaxPooling1D(pool_size=2))
    model.add(k.layers.Flatten())
    model.add(k.layers.Dense(cats, activation='softmax'))
    
    # summary
    print(model.metrics_names)
    print(model.summary())

    print('--')
    model.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
                  metrics=['categorical_accuracy'])
    print('--')
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;model.fit(X, y, batch_size = window_size, shuffle=False, epochs = 5)
&lt;/denchmark-code&gt;

Output:
&lt;denchmark-code&gt;TF v: 2.1.0 Keras v: 2.2.4-tf
INFO:tensorflow:Initializing the TPU system: xxxxxxxxxx:8470
INFO:tensorflow:Initializing the TPU system: xxxxxxxxxx:8470
INFO:tensorflow:Clearing out eager caches
INFO:tensorflow:Clearing out eager caches
INFO:tensorflow:Finished initializing TPU system.
INFO:tensorflow:Finished initializing TPU system.
INFO:tensorflow:Found TPU system:
INFO:tensorflow:Found TPU system:
INFO:tensorflow:*** Num TPU Cores: 8
INFO:tensorflow:*** Num TPU Cores: 8
INFO:tensorflow:*** Num TPU Workers: 1
INFO:tensorflow:*** Num TPU Workers: 1
INFO:tensorflow:*** Num TPU Cores Per Worker: 8
INFO:tensorflow:*** Num TPU Cores Per Worker: 8
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;['loss']
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1d (Conv1D)              (None, 1279, 16)          48        
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 1278, 32)          1056      
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 1277, 64)          4160      
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 1276, 128)         16512     
_________________________________________________________________
max_pooling1d (MaxPooling1D) (None, 638, 128)          0         
_________________________________________________________________
flatten (Flatten)            (None, 81664)             0         
_________________________________________________________________
dense (Dense)                (None, 4)                 326660    
=================================================================
Total params: 348,436
Trainable params: 348,436
Non-trainable params: 0
_________________________________________________________________
None
--
--
&lt;/denchmark-code&gt;

I can see this error in the console though - I am not sure where the proto-buf is coming and why did it work in TF 1.3 - hence I consider this a bug.
&lt;denchmark-code&gt;E0208 17:03:32.001652096    4567 proto_buffer_writer.h:83]   assertion failed: byte_count_ &lt; total_size_
&lt;/denchmark-code&gt;

If I cut data 50 times the fit starts but fails immediately with different error:
&lt;denchmark-code&gt;NotFoundError: No registered 'Identity' OpKernel for 'TPU' devices compatible with node {{node Identity}}
&lt;/denchmark-code&gt;

Full traceback:
&lt;denchmark-code&gt;Train on 27720 samples
Epoch 1/5
    0/27720 [..............................] - ETA: 0s
---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
&lt;ipython-input-14-fae0bbeaa27e&gt; in &lt;module&gt;
----&gt; 1 model.fit(X, y, batch_size = window_size, shuffle=False, epochs = epochs_n)

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    817         max_queue_size=max_queue_size,
    818         workers=workers,
--&gt; 819         use_multiprocessing=use_multiprocessing)
    820 
    821   def evaluate(self,

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    327                 training_data_iter._initializer  # pylint: disable=pointless-statement
    328               else:
--&gt; 329                 training_data_iter = iter(training_dataset)
    330 
    331             training_result = run_one_epoch(

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/distribute/input_lib.py in __iter__(self)
    563 
    564     worker_iterators = _create_iterators_per_worker(self._cloned_datasets,
--&gt; 565                                                     self._input_workers)
    566     iterator = DistributedIterator(self._input_workers, worker_iterators,
    567                                    self._strategy)

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/distribute/input_lib.py in _create_iterators_per_worker(worker_datasets, input_workers)
   1009       worker_devices = input_workers.compute_devices_for_worker(i)
   1010       iterator = _SingleWorkerDatasetIterator(worker_datasets[i], worker,
-&gt; 1011                                               worker_devices)
   1012       iterators.append(iterator)
   1013   return iterators

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/distribute/input_lib.py in __init__(self, dataset, worker, devices)
    862     self._worker = worker
    863     self._devices = devices
--&gt; 864     self._make_iterator()
    865 
    866   def _make_iterator(self):

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/distribute/input_lib.py in _make_iterator(self)
    868     with ops.device(self._worker):
    869       self._iterator = multi_device_iterator_ops.MultiDeviceIterator(
--&gt; 870           self._dataset, self._devices)
    871 
    872   def get_next(self, device, name=None):

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/data/ops/multi_device_iterator_ops.py in __init__(self, dataset, devices, max_buffer_size, prefetch_buffer_size, source_device)
    292                                     self._experimental_slack)
    293         if context.executing_eagerly():
--&gt; 294           self._device_iterators.append(dataset_ops.make_one_shot_iterator(ds))
    295         else:
    296           self._device_iterators.append(

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/data/ops/dataset_ops.py in make_one_shot_iterator(dataset)
   2479     return dataset._make_one_shot_iterator()  # pylint: disable=protected-access
   2480   except AttributeError:
-&gt; 2481     return DatasetV1Adapter(dataset)._make_one_shot_iterator()  # pylint: disable=protected-access
   2482 
   2483 

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/data/ops/dataset_ops.py in _make_one_shot_iterator(self)
   2057   def _make_one_shot_iterator(self):  # pylint: disable=missing-docstring
   2058     if context.executing_eagerly():
-&gt; 2059       return iterator_ops.OwnedIterator(self)
   2060 
   2061     _ensure_same_dataset_graph(self)

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/data/ops/iterator_ops.py in __init__(self, dataset, components, element_spec)
    592           context.context().device_spec.device_type != "CPU"):
    593         with ops.device("/cpu:0"):
--&gt; 594           self._create_iterator(dataset)
    595       else:
    596         self._create_iterator(dataset)

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/data/ops/iterator_ops.py in _create_iterator(self, dataset)
    617               output_types=self._flat_output_types,
    618               output_shapes=self._flat_output_shapes))
--&gt; 619       gen_dataset_ops.make_iterator(ds_variant, self._iterator_resource)
    620       # Delete the resource when this object is deleted
    621       self._resource_deleter = IteratorResourceDeleter(

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/gen_dataset_ops.py in make_iterator(dataset, iterator, name)
   2703         pass  # Add nodes to the TensorFlow graph.
   2704     except _core._NotOkStatusException as e:
-&gt; 2705       _ops.raise_from_not_ok_status(e, name)
   2706   # Add nodes to the TensorFlow graph.
   2707   _, _, _op, _outputs = _op_def_library._apply_op_helper(

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/ops.py in raise_from_not_ok_status(e, name)
   6604   message = e.message + (" name: " + name if name is not None else "")
   6605   # pylint: disable=protected-access
-&gt; 6606   six.raise_from(core._status_to_exception(e.code, message), None)
   6607   # pylint: enable=protected-access
   6608 

/usr/local/lib/python3.5/dist-packages/six.py in raise_from(value, from_value)

NotFoundError: No registered 'Identity' OpKernel for 'TPU' devices compatible with node {{node Identity}}
	 (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_HALF
	.  Registered:  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_HALF, DT_UINT32, DT_UINT64, DT_RESOURCE, DT_VARIANT]
  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_HALF, DT_UINT32, DT_UINT64, DT_RESOURCE, DT_VARIANT]
  device='XLA_TPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_BFLOAT16, DT_UINT32, DT_UINT64, DT_RESOURCE, DT_VARIANT]
  device='XLA_CPU'; T in [DT_UINT8, DT_QUINT8, DT_UINT16, DT_INT8, DT_QINT8, ..., DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_BFLOAT16]
  device='TPU'; T in [DT_INT32, DT_UINT32, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE, DT_BOOL, DT_COMPLEX64, DT_INT64, DT_UINT64]
  device='TPU_SYSTEM'
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_BFLOAT16]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_INT64]
  device='GPU'; T in [DT_UINT16]
  device='GPU'; T in [DT_INT16]
  device='GPU'; T in [DT_UINT8]
  device='GPU'; T in [DT_INT8]
  device='GPU'; T in [DT_COMPLEX64]
  device='GPU'; T in [DT_COMPLEX128]
  device='GPU'; T in [DT_VARIANT]
  device='DEFAULT'; T in [DT_STRING]
  device='DEFAULT'; T in [DT_VARIANT]
  device='DEFAULT'; T in [DT_RESOURCE]
  device='CPU'

	 [[Identity]] [Op:MakeIterator]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='maxima120' date='2020-02-10T22:57:08Z'>
		&lt;denchmark-link:https://github.com/maxima120&gt;@maxima120&lt;/denchmark-link&gt;
 Can you please create a standalone code to reproduce the issue? You could use &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/9d7972363297f592f5a071af6afa885f/untitled814.ipynb&gt;colab&lt;/denchmark-link&gt;
 or any other approach to share a standalone code. Thanks!
		</comment>
		<comment id='2' author='maxima120' date='2020-02-10T23:08:14Z'>
		I promise to get to it in couple days. Thank you for quick response
		</comment>
		<comment id='3' author='maxima120' date='2020-02-20T22:12:54Z'>
		there is any solution?
		</comment>
		<comment id='4' author='maxima120' date='2020-02-24T23:56:19Z'>
		Hi walidayech, did you run into the same issue? Do you have a reproduction?
		</comment>
		<comment id='5' author='maxima120' date='2020-02-28T01:26:30Z'>
		
@maxima120 Can you please create a standalone code to reproduce the issue? You could use colab or any other approach to share a standalone code. Thanks!

&lt;denchmark-link:https://github.com/maxima120&gt;@maxima120&lt;/denchmark-link&gt;
 Any progress from your side? Thanks!
		</comment>
		<comment id='6' author='maxima120' date='2020-03-04T19:27:06Z'>
		Closing for now. Please do re-open if you have a repro for the issue. Thanks!
		</comment>
		<comment id='7' author='maxima120' date='2020-03-04T19:27:08Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36579&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36579&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>