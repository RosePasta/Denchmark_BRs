<bug id='41523' author='Akpadhy' open_date='2020-07-18T06:10:29Z' closed_time='2020-08-31T19:22:21Z'>
	<summary>Train with multi-gpu with MirroredStrategy for dataset issue</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:no
TensorFlow installed from (source or binary):source
TensorFlow version (use command below):2.2.0
Python version:3.7
Bazel version (if compiling from source):no
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: cuda 10.2 and cuDNN 7.6.5
GPU model and memory: Tesla T4 15109MB

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with:

TF 1.0: python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
TF 2.0: python -c "import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"

v2.2.0-rc4-8-g2b96f3662b 2.2.0
GPU details:
[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),
PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),
PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'),
PhysicalDevice(name='/physical_device:XLA_GPU:1', device_type='XLA_GPU'),
PhysicalDevice(name='/physical_device:XLA_GPU:2', device_type='XLA_GPU'),
PhysicalDevice(name='/physical_device:XLA_GPU:3', device_type='XLA_GPU'),
PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),
PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),
PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'),
PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]
Describe the current behavior
I followed the blog for the distributed training with multi gpu.When we train with single gpu this works but with multi gpu i have added mirrored strategy but it throw the error.

Followed this while dev but gpus are not getting used
&lt;denchmark-link:https://www.tensorflow.org/tutorials/distribute/multi_worker_with_estimator#train_and_evaluate_the_model&gt;https://www.tensorflow.org/tutorials/distribute/multi_worker_with_estimator#train_and_evaluate_the_model&lt;/denchmark-link&gt;

Standalone code to reproduce the issue
Notebook and custom transformers are available here:
&lt;denchmark-h:h2&gt;Other info / logs Include any logs or source code that would be helpful to&lt;/denchmark-h&gt;

TypeError                                 Traceback (most recent call last)
 in 
2 eval_spec = tf.estimator.EvalSpec(input_fn = eval_input_fn)
3
----&gt; 4 tf.estimator.train_and_evaluate(estimator, train_spec=train_spec, eval_spec=eval_spec)
/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py in train_and_evaluate(estimator, train_spec, eval_spec)
470         '(with task id 0).  Given task id {}'.format(config.task_id))
471
--&gt; 472   return executor.run()
473
474
/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py in run(self)
611       tf.compat.v1.logging.info(
612           'Running training and evaluation locally (non-distributed).')
--&gt; 613       return self.run_local()
614
615     # Distributed case.
/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py in run_local(self)
712         max_steps=self._train_spec.max_steps,
713         hooks=train_hooks,
--&gt; 714         saving_listeners=saving_listeners)
715
716     eval_result = listener_for_eval.eval_result or _EvalResult(
/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)
347
348       saving_listeners = _check_listeners_type(saving_listeners)
--&gt; 349       loss = self._train_model(input_fn, hooks, saving_listeners)
350       logging.info('Loss for final step: %s.', loss)
351       return self
/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)
1178   def _train_model(self, input_fn, hooks, saving_listeners):
1179     if self._train_distribution:
-&gt; 1180       return self._train_model_distributed(input_fn, hooks, saving_listeners)
1181     else:
1182       return self._train_model_default(input_fn, hooks, saving_listeners)
/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model_distributed(self, input_fn, hooks, saving_listeners)
1240       self._config._train_distribute.configure(self._config.session_config)
1241       return self._actual_train_model_distributed(
-&gt; 1242           self._config._train_distribute, input_fn, hooks, saving_listeners)
1243     # pylint: enable=protected-access
1244
/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py in _actual_train_model_distributed(self, strategy, input_fn, hooks, saving_listeners)
1324                   labels,  # although this will be None it seems
1325                   ModeKeys.TRAIN,
-&gt; 1326                   self.config))
1327           loss = strategy.reduce(
1328               _get_loss_reduce_op_for_reporting(),
/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py in call_for_each_replica(self, fn, args, kwargs)
2288       kwargs = {}
2289     with self._container_strategy().scope():
-&gt; 2290       return self._call_for_each_replica(fn, args, kwargs)
2291
2292   def _call_for_each_replica(self, fn, args, kwargs):
/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py in _call_for_each_replica(self, fn, args, kwargs)
768
769     return _call_for_each_replica(self._container_strategy(), self._devices,
--&gt; 770                                   fn, args, kwargs)
771
772   def _configure(self,
/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py in _call_for_each_replica(distribution, devices, fn, args, kwargs)
199     for t in threads:
200       t.should_run.set()
--&gt; 201     coord.join(threads)
202
203   return values.regroup(tuple(t.main_result for t in threads))
/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/training/coordinator.py in join(self, threads, stop_grace_period_secs, ignore_live_threads)
387       self._registered_threads = set()
388       if self._exc_info_to_raise:
--&gt; 389         six.reraise(*self._exc_info_to_raise)
390       elif stragglers:
391         if ignore_live_threads:
/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/six.py in reraise(tp, value, tb)
691             if value.traceback is not tb:
692                 raise value.with_traceback(tb)
--&gt; 693             raise value
694         finally:
695             value = None
/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/training/coordinator.py in stop_on_exception(self)
295     """
296     try:
--&gt; 297       yield
298     except:  # pylint: disable=bare-except
299       self.request_stop(ex=sys.exc_info())
/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py in run(self)
996               self._var_scope, reuse=self.replica_id &gt; 0), 
997           variable_scope.variable_creator_scope(self.variable_creator_fn):
--&gt; 998         self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
999         self.done = True
1000     finally:
/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)
263       except Exception as e:  # pylint:disable=broad-except
264         if hasattr(e, 'ag_error_metadata'):
--&gt; 265           raise e.ag_error_metadata.to_exception(e)
266         else:
267           raise
TypeError: in user code:
&lt;denchmark-code&gt;/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py:1170 _call_model_fn  *
    model_fn_results = self._model_fn(features=features, **kwargs)
/media/ephemeral0/combined_estimator.py:39 model_fn  *
    if spec.train_op:
/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/autograph/operators/control_flow.py:924 if_stmt
    basic_symbol_names, composite_symbol_names)
/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/autograph/operators/control_flow.py:962 tf_if_stmt
    error_checking_orelse)
/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:507 new_func
    return func(*args, **kwargs)
/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:1177 cond
    return cond_v2.cond_v2(pred, true_fn, false_fn, name)
/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/ops/cond_v2.py:91 cond_v2
    op_return_value=pred)
/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py:981 func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/autograph/operators/control_flow.py:958 error_checking_orelse
    basic_symbol_names + composite_symbol_names)
/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/autograph/operators/control_flow.py:295 _verify_tf_cond_vars
    ' branches.\n\n{}'.format(name, str(e)))

TypeError: "train_op_list" does not have the same nested structure in the TRUE and FALSE branches.

The two structures don't have the same nested structure.

First structure: type=list str=[&lt;tf.Tensor 'Adam/Identity:0' shape=() dtype=int64&gt;]

Second structure: type=NoneType str=None

More specifically: Substructure "type=list str=[&lt;tf.Tensor 'Adam/Identity:0' shape=() dtype=int64&gt;]" is a sequence, while substructure "type=NoneType str=None" is not
Entire first structure:
[.]
Entire second structure:
.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Akpadhy' date='2020-07-23T13:15:28Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41523&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41523&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='Akpadhy' date='2020-07-25T04:34:03Z'>
		The resolution did not help in case of distribution of operations.
So here  i have 3 estimators and i have combined it to one mega estimator.how to use distributed gpu in that case.
		</comment>
		<comment id='3' author='Akpadhy' date='2020-07-27T13:54:39Z'>
		&lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;
 any suggestions here where we have custom estimator how the mirrored strategy can distribute the loss.Any rules to be followed?
		</comment>
		<comment id='4' author='Akpadhy' date='2020-07-27T13:56:08Z'>
		&lt;denchmark-link:https://github.com/guptapriya&gt;@guptapriya&lt;/denchmark-link&gt;
 Need your valuable inputs as well i followed your directions for distributed training and saw that its not working when we use custom estimator.Any inputs?
		</comment>
		<comment id='5' author='Akpadhy' date='2020-07-28T03:39:31Z'>
		&lt;denchmark-link:https://github.com/nikitamaia&gt;@nikitamaia&lt;/denchmark-link&gt;
 any suggestions here?
		</comment>
		<comment id='6' author='Akpadhy' date='2020-07-28T17:00:48Z'>
		Hi &lt;denchmark-link:https://github.com/Akpadhy&gt;@Akpadhy&lt;/denchmark-link&gt;
, please provide minimal reproducible code to help with the debugging process.
Additionally, looking at the stack trace I'm not sure this error is distribution strategy specific. A quick google search for this error message shows several cases on stack overflow that are not related to training with MirroredStrategy.
You are able to run your code without problems if you are not using a distribution strategy?
		</comment>
		<comment id='7' author='Akpadhy' date='2020-07-29T03:20:59Z'>
		&lt;denchmark-link:https://github.com/nikitamaia&gt;@nikitamaia&lt;/denchmark-link&gt;
 Yes this works without any issue when i run on single gpu without distribution strategy.When i use distribution it does not understand optimisers.
git link : &lt;denchmark-link:url&gt;https://github.com/Akpadhy/tensorflow_model/blob/master/GPU_Testing_DL.ipynb&lt;/denchmark-link&gt;

I have given minimal code for reproducibility. There is a git repo includedÂ which is easy to access.
		</comment>
		<comment id='8' author='Akpadhy' date='2020-07-29T23:04:52Z'>
		Here are a couple of things to try:

Firstly try disabling tf.compat.v1.disable_eager_execution() as we don't support custom losses in legacy graph mode (which is what you have if you disable eager mode). I noticed this line in several of your scripts.
As a test, have you tried running with fewer Estimators and seeing what happens? Basically passing in 2 estimators to your CombinedEstimator instead of 7.

Additionally, can you explain what you mean by "it does not understand optimisers" ?
		</comment>
		<comment id='9' author='Akpadhy' date='2020-07-30T01:48:34Z'>
		Thanks for you response &lt;denchmark-link:https://github.com/nikitamaia&gt;@nikitamaia&lt;/denchmark-link&gt;
 .
I have tried that disabling eager mode as you suggested and also tried with 2 estimator and it did not work out same error again.I have made a separate notebook for that.
Here is the link : &lt;denchmark-link:https://github.com/Akpadhy/tensorflow_model/blob/master/GPU_Testing_DL_WOT_EAGER.ipynb&gt;git repo&lt;/denchmark-link&gt;

What i meant when i said it does not understand optimiser because training optimiser spec come different in single and multi gpu.I have tried to print the spec and below are the findings i got training optimiser it is expecting it to be same but single gpu its coming different and multi gpu it is different:
#Single GPU: &lt;class 'tensorflow.python.framework.ops.Operation'&gt;
#Multi GPU: &lt;class 'tensorflow.python.framework.ops.Tensor'&gt;
print(type(spec.train_op))
#Single GPU: name: "Adam/update_0_9/AssignAddVariableOp"
#Multi GPU: Tensor("Adam/Identity:0", shape=(), dtype=int64, device=/replica:0/task:0/device:GPU:0)
		</comment>
		<comment id='10' author='Akpadhy' date='2020-07-31T01:45:50Z'>
		&lt;denchmark-link:https://github.com/nikitamaia&gt;@nikitamaia&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/guptapriya&gt;@guptapriya&lt;/denchmark-link&gt;

Is it the issue if we combine multiple estimator we cant use multi gpu or possible?
		</comment>
		<comment id='11' author='Akpadhy' date='2020-07-31T01:58:10Z'>
		
@nikitamaia @guptapriya
Is it the issue if we combine multiple estimator we cant use multi gpu or possible?


Here are a couple of things to try:

Firstly try disabling tf.compat.v1.disable_eager_execution() as we don't support custom losses in legacy graph mode (which is what you have if you disable eager mode). I noticed this line in several of your scripts.
As a test, have you tried running with fewer Estimators and seeing what happens? Basically passing in 2 estimators to your CombinedEstimator instead of 7.

Additionally, can you explain what you mean by "it does not understand optimisers" ?

Thanks for you response &lt;denchmark-link:https://github.com/nikitamaia&gt;@nikitamaia&lt;/denchmark-link&gt;
 .
I have tried that disabling eager mode as you suggested and also tried with 2 estimator and it did not work out same error again.I have made a separate notebook for that.
Here is the link : git repo
What i meant when i said it does not understand optimiser because training optimiser spec come different in single and multi gpu.I have tried to print the spec and below are the findings i got training optimiser it is expecting it to be same but single gpu its coming different and multi gpu it is different:
#Single GPU: &lt;class 'tensorflow.python.framework.ops.Operation'&gt;
#Multi GPU: &lt;class 'tensorflow.python.framework.ops.Tensor'&gt;
print(type(spec.train_op))
#Single GPU: name: "Adam/update_0_9/AssignAddVariableOp"
#Multi GPU: Tensor("Adam/Identity:0", shape=(), dtype=int64, device=/replica:0/task:0/device:GPU:0)
		</comment>
		<comment id='12' author='Akpadhy' date='2020-08-02T05:54:53Z'>
		&lt;denchmark-link:https://github.com/nikitamaia&gt;@nikitamaia&lt;/denchmark-link&gt;
 Do you have any suggestions after the update you asked to make it.
		</comment>
		<comment id='13' author='Akpadhy' date='2020-08-03T19:18:48Z'>
		Please note that Estimator API support for MirroredStrategy is limited at this point, &lt;denchmark-link:https://www.tensorflow.org/guide/distributed_training#types_of_strategies&gt;as noted here.
&lt;/denchmark-link&gt;

A couple more things to try:

what happens when you pass in just one estimator to CombinedEstimator ?
And what happens when you train a single estimator (removing CombinedEstimator altogether)

		</comment>
		<comment id='14' author='Akpadhy' date='2020-08-10T19:21:04Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='15' author='Akpadhy' date='2020-08-11T04:21:41Z'>
		
Please note that Estimator API support for MirroredStrategy is limited at this point, as noted here. 
A couple more things to try:

what happens when you pass in just one estimator to CombinedEstimator ?
And what happens when you train a single estimator (removing CombinedEstimator altogether)


I tried with single estimator same issue i see here as well.Here is the example notebook.
&lt;denchmark-link:https://github.com/Akpadhy/tensorflow_model/blob/master/GPU_Testing_DL_WOT_EAGER_one_estm.ipynb&gt;https://github.com/Akpadhy/tensorflow_model/blob/master/GPU_Testing_DL_WOT_EAGER_one_estm.ipynb&lt;/denchmark-link&gt;

		</comment>
		<comment id='16' author='Akpadhy' date='2020-08-11T21:49:17Z'>
		I tried to run that notebook but I'm not able to train the estimator with TF 2.3, even without a distribution strategy. I get the following error message, suggesting that there's something wrong with the input.
&lt;denchmark-code&gt;InvalidArgumentError: Input to reshape is a tensor with 256 values, but the requested shape has 1
	 [[node Reshape (defined at /.../tensorflow2.0/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/model_fn.py:338) ]]

Errors may have originated from an input operation.
Input Source operations connected to node Reshape:
 Mean (defined at /.../tensorflow_model-master/model_fns.py:22)

&lt;/denchmark-code&gt;

I'm seeing a variation of this error when I try to run any of the notebooks in that repo without distribution strategy. If I use TF 2.2 I still see a variation of this error message.
		</comment>
		<comment id='17' author='Akpadhy' date='2020-08-12T02:28:17Z'>
		
I tried to run that notebook but I'm not able to train the estimator with TF 2.3, even without a distribution strategy. I get the following error message, suggesting that there's something wrong with the input.
InvalidArgumentError: Input to reshape is a tensor with 256 values, but the requested shape has 1
	 [[node Reshape (defined at /.../tensorflow2.0/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/model_fn.py:338) ]]

Errors may have originated from an input operation.
Input Source operations connected to node Reshape:
 Mean (defined at /.../tensorflow_model-master/model_fns.py:22)

I'm seeing a variation of this error when I try to run any of the notebooks in that repo without distribution strategy. If I use TF 2.2 I still see a variation of this error message.

Please clone the repo now and check
Here is the notebook without distribution:
&lt;denchmark-link:https://github.com/Akpadhy/tensorflow_model/blob/master/GPU_Testing_DL_wot_distribution.ipynb&gt;https://github.com/Akpadhy/tensorflow_model/blob/master/GPU_Testing_DL_wot_distribution.ipynb&lt;/denchmark-link&gt;

		</comment>
		<comment id='18' author='Akpadhy' date='2020-08-12T18:02:13Z'>
		Okay this time I was able to train without distribution, and also reproduce the error message with distribution. However I tried to run the single estimator without using the combined estimator and I was unable to train it successfully, even without distribution.
It's difficult to debug this case since you have a lot of custom code. If your combined estimator is also of type estimator, then I don't see any obvious reasons why it should fail with distributed training. I think you might need to just do some debugging based on the error message.
Looking at the stack trace for the combined estimator case, the message doesn't seem to be be distribution strategy specific. I found a similar message in this issue here &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/33491&gt;#33491&lt;/denchmark-link&gt;

Also this &lt;denchmark-link:https://stackoverflow.com/questions/59585574/autograph-in-tensorflow-1-15-gives-typeerror-in-conditional-statement-with-value&gt;stack overflow post &lt;/denchmark-link&gt;

The message seems very similar to the type error in the docs for &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/nest/assert_same_structure&gt;tf.nest.assert_same_structure &lt;/denchmark-link&gt;

Which only happens when  so I'm guessing that when you run with distribution strategy something causes check_types to get set to True.
Something else I noticed is that when I ran the code in colab with MirroredStrategy and a single GPU, I saw this warning:
&lt;denchmark-code&gt;WARNING: AutoGraph could not transform &lt;function _combine_distributed_scaffold.&lt;locals&gt;.&lt;lambda&gt; at 0x7f7df039bea0&gt; and will run it as-is.
Cause: could not parse the source code:

      lambda scaffold: scaffold.ready_op, args=(grouped_scaffold,))

This error may be avoided by creating the lambda in a standalone statement.

To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
&lt;combined_estimator.CombinedEstimator at 0x7f7df1f08ac8&gt;
&lt;/denchmark-code&gt;

Which makes me think that the problem you're seeing is related to autograph (which you'll notice is referenced in the stack trace also). Hope this helps you to debug.
		</comment>
		<comment id='19' author='Akpadhy' date='2020-08-24T18:25:37Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='20' author='Akpadhy' date='2020-08-31T19:22:20Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='21' author='Akpadhy' date='2020-08-31T19:22:22Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41523&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41523&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>