<bug id='39277' author='Thomas-K-John' open_date='2020-05-07T17:11:04Z' closed_time='2020-05-18T21:56:11Z'>
	<summary>ImageAugmentation using tf.keras.preprocessing.image.ImageDataGenerator and tf.datasets: model.fit() is running infinitely</summary>
	<description>
What I need help with / What I was wondering
I am facing issue while running the fit() function in TensorFlow(v 2.2.0-rc4) with augmented images(using ImageDataGenerator) passed as a dataset. The fit() function is running infinitely without stopping.
What I've tried so far
I tried it with the default code which was shared in Tensorflow documentation.
Please find the code snippet below:
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.layers import Input, Dense
flowers = tf.keras.utils.get_file(
'flower_photos',
'&lt;denchmark-link:https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz&gt;https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz&lt;/denchmark-link&gt;
',
untar=True)
img_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, rotation_range=20)
images, labels = next(img_gen.flow_from_directory(flowers))
print(images.dtype, images.shape)
print(labels.dtype, labels.shape)
train_data_gen = img_gen.flow_from_directory(
batch_size=32,
directory=flowers,
shuffle=True,
target_size=(256, 256),
class_mode='categorical')
ds = tf.data.Dataset.from_generator(lambda: train_data_gen,
output_types=(tf.float32, tf.float32),
output_shapes=([32, 256, 256, 3],
[32, 5])
)
ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
it = iter(ds)
batch = next(it)
print(batch)
def create_model():
model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=images[0].shape))
model.add(Conv2D(32, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.5))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.5))
model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(5, activation='softmax'))
return model
model = create_model()
model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=["accuracy"])
model.fit(ds, verbose=1,  batch_size= 32, epochs =1)
This last line of code - fit() is running infinitly without stopping. I had also tried passing steps_per_epoch = total_no_of_train_records/batch_size.
It would be nice if...
I would like you to confirm whethere this is a bug in the tensorflow datasets package and in which release will this be fixed.
Environment information

System: Google colaborator
Python version: v3.6.9
`tensorflow version: v2.2.0-rc4

	</description>
	<comments>
		<comment id='1' author='Thomas-K-John' date='2020-05-08T09:52:27Z'>
		I have tried in colab with TF version 2.2-rc4 and was able to reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/186467b0cc451d9463b49d5b866a4c70/untitled868.ipynb&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='2' author='Thomas-K-John' date='2020-05-11T10:18:55Z'>
		I confirm this. Its running forever
		</comment>
		<comment id='3' author='Thomas-K-John' date='2020-05-11T20:09:59Z'>
		The ImageDataGenerator returns an infinite number of values, so the epoch would never end unless you specify .
The examples in ImageDataGenerator briefly mention this, but I think the documentation should be clearer about this. So I'll go ahead and update that.
&lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator&gt;https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator&lt;/denchmark-link&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

I'm not sure why setting steps_per_epoch didn't work for you? When I set a value for steps_per_epoch in this colab it ends once it has completed that many steps. Perhaps it's just being slow?
There are ~114 steps in one epoch over the dataset (3670 training images / 32 batch size). 114 * 7 sec / step (in this colab) is close to 15 minutes of runtime.
		</comment>
		<comment id='4' author='Thomas-K-John' date='2020-05-12T06:05:03Z'>
		I tried to use the model.fit() method by passing the steps_per_epoch=3670/32 and I got the following error:
Code used to run the fit() method:
model.fit(ds, verbose=1,  steps_per_epoch=3670/32, epochs =1)
Error:
&lt;denchmark-h:h2&gt;112/114 [============================&gt;.] - ETA: 25s - loss: 1.9140 - accuracy: 0.3262&lt;/denchmark-h&gt;

InvalidArgumentError                      Traceback (most recent call last)
 in ()
----&gt; 1 model.fit(ds, verbose=1,  steps_per_epoch=3670/32, epochs =1)
8 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
58     ctx.ensure_initialized()
59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
---&gt; 60                                         inputs, attrs, num_outputs)
61   except core._NotOkStatusException as e:
62     if name is not None:
InvalidArgumentError:  ValueError: generator yielded an element of shape (22, 256, 256, 3) where an element of shape (32, 256, 256, 3) was expected.
Traceback (most recent call last):
File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py", line 243, in call
ret = func(*args)
File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py", line 309, in wrapper
return func(*args, **kwargs)
File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py", line 821, in generator_py_func
"of shape %s was expected." % (ret_array.shape, expected_shape))
ValueError: generator yielded an element of shape (22, 256, 256, 3) where an element of shape (32, 256, 256, 3) was expected.
&lt;denchmark-code&gt; [[{{node PyFunc}}]]
 [[IteratorGetNext]] [Op:__inference_train_function_1244]
&lt;/denchmark-code&gt;

Function call stack:
train_function
NOTE:  I do understand that when we have 3670 training records, the last step(114th) will contain only 22 records and that should not throw an errors. Please correct me if I am wrong.
		</comment>
		<comment id='5' author='Thomas-K-John' date='2020-05-12T17:12:41Z'>
		So two things because It's also come to my attention that apparently the ImageDataGenerator isn't always supposed to loop forever.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;


When you use tf.data from_generator and specify a shape, the generator must always yield objects of that shape. In your call you're specifying a fixed batch size as part of the shape:

&lt;denchmark-code&gt;ds = tf.data.Dataset.from_generator(lambda: train_data_gen,
output_types=(tf.float32, tf.float32),
output_shapes=([32, 256, 256, 3],
[32, 5])
)
&lt;/denchmark-code&gt;

When it sees a partial batch of 22 on the last step the shapes do not match so it errors.
You can fix this by leaving the batch size unspecified (as None) in from_generator:
&lt;denchmark-code&gt;ds = tf.data.Dataset.from_generator(lambda: train_data_gen,
output_types=(tf.float32, tf.float32),
output_shapes=([None, 256, 256, 3],
[None, 5])
)
&lt;/denchmark-code&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;


Apparently there is a setting where the ImageDataGenerator isn't supposed to loop forever and shouldn't require steps_per_epoch: If you pass the result of flow_from_directory directly to Keras fit without converting it to a dataset yourself. In this specific setting the len information attached to the ImageDataGenerator sequences has historically been used as an implicit steps_per_epoch.

(This does not happen if you manually loop over the generator w/ a for loop, as shown in the documentation of the ImageDataGenerator). This isn't the case for your example code because when you convert it to a dataset manually that cardinality information gets lost.
However it seems we did introduce a regression in that setting at some point. (Though we're not positive which exact version of TF it was introduced in). It did make its way into the TF 2.0 release, and we're currently exploring whether to do a patch release to fix it.
		</comment>
		<comment id='6' author='Thomas-K-John' date='2020-05-13T06:12:33Z'>
		Few points that I found from my testing:


As you suggested I tried by leaving the batch size unspecified (as None) in from_generator. This helped me to successfully train the model by successfully running the model.fit() method. But I was not able to use this model for making any predictions. i.e. model.predict() and model.evaluate() methods were just running infinitely again.


I also tried to pass the validation dataset to my fit() method during the training of the model. The model was again running infinitely. Please find the arguments passed to my fit() method below:
model.fit(train_ds, verbose=1,  steps_per_epoch=3670/32, validation_data=validation_ds, epochs =1)


I would like you to please confirm if these two are bugs. Please let me know if I need to raise any seperate bugs of these issues. I would like you to please suggest me any work around to run the model.predict() and model.evaluate() functions.
		</comment>
		<comment id='7' author='Thomas-K-John' date='2020-05-13T17:01:38Z'>
		The steps argument for evaluate and predict is called steps, not steps_per_epoch. You could go ahead and set that. For validation the argument is validation_steps.
Alternatively, you could put .take(num_steps) at the end of your dataset to force it to have a fixed size.
If we patch the release to handle the case where the generator is directly passed to fit/evaluate/predict it should deal with all of these cases, so you shouldn't need to file any other bugs. We've submitted the code fix already in the TF nightlies, it's just a question of whether it'll go in 2.3 or be patched into 2.2.
		</comment>
		<comment id='8' author='Thomas-K-John' date='2020-05-13T17:11:23Z'>
		Thank you for the details.
		</comment>
		<comment id='9' author='Thomas-K-John' date='2020-05-18T13:16:04Z'>
		The issue appears to be in:
tensorflow.python.keras.engine.data_adapter.py
function:
select_data_adapter()
The function is selecting GeneratorDataAdapter when it should be selecting KerasSequenceAdapter.
If I manually update:
keras_preprocessing.image.iterator.py:
&lt;denchmark-code&gt;class Iterator(IteratorType):
&lt;/denchmark-code&gt;

to:
&lt;denchmark-code&gt;class Iterator(data_utils.Sequence)
&lt;/denchmark-code&gt;

flow_from_directory() works fine with fit() again (no steps_per_epoch argument required).
		</comment>
		<comment id='10' author='Thomas-K-John' date='2020-05-18T15:44:26Z'>
		I hope this fix will also resolve the issue faced with predict() and evaluate() methods.
		</comment>
		<comment id='11' author='Thomas-K-John' date='2020-05-18T21:56:11Z'>
		&lt;denchmark-link:https://github.com/driedler&gt;@driedler&lt;/denchmark-link&gt;
:
We made a roughly equivalent change to the  pip package, but users have to manually upgrade it to get that effect.
We've also submitted this:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/b53ed4d560aaeb7a92185f4fbf2562e5e274456a&gt;b53ed4d&lt;/denchmark-link&gt;

which will affect the next version of tensorflow. We're unlikely to patch this into the existing release, so if you want the fix right away go ahead and update your install of keras_preprocessing to the newest version.
We'll go ahead and close this issue for now.
		</comment>
		<comment id='12' author='Thomas-K-John' date='2020-05-18T21:56:12Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39277&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39277&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='Thomas-K-John' date='2020-08-21T13:55:32Z'>
		Hello everyone. I'm having a similar kind of issue but in my case I'm trying to make an AutoEncoder using the keras ImageDataGenerator() function with flow_from_directory(). Now because I want to build an Autoencoder, I dont require the image labels because the Y_train of my dataset are not labels but images itself.
When I try to fit my model and pass X_train and Y_train it shows an error :
y argument is not supported when using keras.utils.Sequence as input.
How can I get away with this. PLEASE help !
		</comment>
	</comments>
</bug>