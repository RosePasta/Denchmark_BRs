<bug id='7051' author='raphtown' open_date='2017-01-25T00:26:45Z' closed_time='2017-06-06T00:05:10Z'>
	<summary>Bug - tfdbg + multi-gpu gives ValueError: Duplicate node name: 'n/_0'</summary>
	<description>
Hello tensorflow team,
I have been starting to use your tensorflow debugger but have run into the issue that when I try and use it on a multi-gpu model I get ValueError: Duplicate node name: 'n/_0'.
Inspecting things closer, I saw that the issue originated from the run_metadata, whose partition graphs have many _Send and _HostRecv ops with names like 'n/_0'.  These ops are replicated with identical names across my towers which is what is causing the issue.
Looking through the tensorflow code, I believe I tracked where this name is set down to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/graph/graph_partition.cc#L195&gt;graph_partition.cc:195&lt;/denchmark-link&gt;
 where the edge's source name is used as the prefix 'n'.  Unfortunately, I have not been able to figure out why the source's name is only 'n', but that seems to be the root of the issue here.
I should add that I never set any tensor name to 'n' anywhere in my own code.  Plus, I see certain tests in your codebase rely on names such as 'n/_0' which indicates to me the name is being set somewhere internally in the tensorflow code.
Any help you can provide would be much appreciated!
&lt;denchmark-h:h3&gt;What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?&lt;/denchmark-h&gt;

I didn't find any related issues.
&lt;denchmark-h:h3&gt;Environment info&lt;/denchmark-h&gt;

Operating System: Ubuntu 14.04.5 LTS (running in a &lt;denchmark-link:http://singularity.lbl.gov&gt;singularity&lt;/denchmark-link&gt;
 container on a CentOS 6.7 host).
Installed version of CUDA and cuDNN:
I am using CUDA 8.0 with NVIDIA driver 367.48, and cuDNN v5.1 .
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
&lt;denchmark-code&gt;libOpenCL.so
libOpenCL.so.1
libOpenCL.so.1.0
libOpenCL.so.1.0.0
libcublas.so
libcublas.so.8.0
libcublas.so.8.0.45
libcublas_device.a
libcublas_static.a
libcudadevrt.a
libcudart.so
libcudart.so.8.0
libcudart.so.8.0.44
libcudart_static.a
libcudnn.so
libcudnn.so.5
libcudnn.so.5.1.5
libcudnn_static.a
libcufft.so
libcufft.so.8.0
libcufft.so.8.0.44
libcufft_static.a
libcufftw.so
libcufftw.so.8.0
libcufftw.so.8.0.44
libcufftw_static.a
libcuinj64.so
libcuinj64.so.8.0
libcuinj64.so.8.0.44
libculibos.a
libcurand.so
libcurand.so.8.0
libcurand.so.8.0.44
libcurand_static.a
libcusolver.so
libcusolver.so.8.0
libcusolver.so.8.0.44
libcusolver_static.a
libcusparse.so
libcusparse.so.8.0
libcusparse.so.8.0.44
libcusparse_static.a
libnppc.so
libnppc.so.8.0
libnppc.so.8.0.44
libnppc_static.a
libnppi.so
libnppi.so.8.0
libnppi.so.8.0.44
libnppi_static.a
libnppial.so
libnppial.so.8.0
libnppial.so.8.0.44
libnppicc.so
libnppicc.so.8.0
libnppicc.so.8.0.44
libnppicom.so
libnppicom.so.8.0
libnppicom.so.8.0.44
libnppidei.so
libnppidei.so.8.0
libnppidei.so.8.0.44
libnppif.so
libnppif.so.8.0
libnppif.so.8.0.44
libnppig.so
libnppig.so.8.0
libnppig.so.8.0.44
libnppim.so
libnppim.so.8.0
libnppim.so.8.0.44
libnppist.so
libnppist.so.8.0
libnppist.so.8.0.44
libnppisu.so
libnppisu.so.8.0
libnppisu.so.8.0.44
libnppitc.so
libnppitc.so.8.0
libnppitc.so.8.0.44
libnpps.so
libnpps.so.8.0
libnpps.so.8.0.44
libnpps_static.a
libnvToolsExt.so
libnvToolsExt.so.1
libnvToolsExt.so.1.0.0
libnvblas.so
libnvblas.so.8.0
libnvblas.so.8.0.44
libnvgraph.so
libnvgraph.so.8.0
libnvgraph.so.8.0.44
libnvgraph_static.a
libnvrtc-builtins.so
libnvrtc-builtins.so.8.0
libnvrtc-builtins.so.8.0.44
libnvrtc.so
libnvrtc.so.8.0
libnvrtc.so.8.0.44
stubs
&lt;/denchmark-code&gt;


A link to the pip package you installed:
I installed tensorflow using pip install tensorflow-gpu==0.12.1
The output from python -c "import tensorflow; print(tensorflow.__version__)".

&lt;denchmark-code&gt;I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally 
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally 
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
0.12.1 
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;What other attempted solutions have you tried?&lt;/denchmark-h&gt;

The single GPU case works fine.
&lt;denchmark-h:h3&gt;Logs or other output that would be helpful&lt;/denchmark-h&gt;

Here is the dump of some of the problematic nodes.
&lt;denchmark-code&gt;node {
  name: "n/_0"
  op: "_Send"
  input: "__copy_TOWER0/Const_0"
  attr {
    key: "T"
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: "client_terminated"
    value {
      b: false
    }
  }
  attr {
    key: "recv_device"
    value {
      s: "/job:localhost/replica:0/task:0/gpu:0"
    }
  }
  attr {
    key: "send_device"
    value {
      s: "/job:localhost/replica:0/task:0/gpu:0"
    }
  }
  attr {
    key: "send_device_incarnation"
    value {
      i: 0
    }
  }
  attr {
    key: "tensor_name"
    value {
      s: "edge_545___copy_TOWER0/Const_0"
    }
  }
}
node {
  name: "n/_1"
  op: "_HostRecv"
  input: "^n/_0"
  attr {
    key: "client_terminated"
    value {
      b: false
    }
  }
  attr {
    key: "recv_device"
    value {
      s: "/job:localhost/replica:0/task:0/gpu:0"
    }
  }
  attr {
    key: "send_device"
    value {
      s: "/job:localhost/replica:0/task:0/gpu:0"
    }
  }
  attr {
    key: "send_device_incarnation"
    value {
      i: 0
    }
  }
  attr {
    key: "tensor_name"
    value {
      s: "edge_545___copy_TOWER0/Const_0"
    }
  }
  attr {
    key: "tensor_type"
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: "n/_2"
  op: "_Send"
  input: "__copy_TOWER0/Sub_0"
  attr {
    key: "T"
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: "client_terminated"
    value {
      b: false
    }
  }
  attr {
    key: "recv_device"
    value {
      s: "/job:localhost/replica:0/task:0/gpu:0"
    }
  }
  attr {
    key: "send_device"
    value {
      s: "/job:localhost/replica:0/task:0/gpu:0"
    }
  }
  attr {
    key: "send_device_incarnation"
    value {
      i: 0
    }
  }
  attr {
    key: "tensor_name"
    value {
      s: "edge_551___copy_TOWER0/Sub_0"
    }
  }
}

&lt;/denchmark-code&gt;

End of backtrace at crash point
&lt;denchmark-code&gt;  /home/raphtown/.local/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/framework.py(419)run()                 
-&gt; run_end_resp = self.on_run_end(run_end_req)                                                                              
  /home/raphtown/.local/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py(262)on_run_end()  
-&gt; self._dump_root, partition_graphs=partition_graphs)                                                                      
  /home/raphtown/.local/lib/python2.7/site-packages/tensorflow/python/debug/debug_data.py(407)__init__()                    
-&gt; self._load_partition_graphs(partition_graphs)                                                                            
&gt; /home/raphtown/.local/lib/python2.7/site-packages/tensorflow/python/debug/debug_data.py(493)_load_partition_graphs()      
-&gt; raise ValueError("Duplicate node name: '%s'" % node.name)                                                                
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='raphtown' date='2017-01-25T01:20:00Z'>
		&lt;denchmark-link:https://github.com/raphtown&gt;@raphtown&lt;/denchmark-link&gt;
 a small code repro would be nice if you have one.
&lt;denchmark-link:https://github.com/caisq&gt;@caisq&lt;/denchmark-link&gt;
 any idea?
		</comment>
		<comment id='2' author='raphtown' date='2017-01-25T01:47:25Z'>
		Hello &lt;denchmark-link:https://github.com/drpngx&gt;@drpngx&lt;/denchmark-link&gt;
, and thank you for your swift response.  I can try and make a minimal code reproduction if you think it necessary, but it may take a bit for me to do so.  My hunch is that to fix this bug we just need to figure out where these nodes with names 'n' are coming from in the library code.
		</comment>
		<comment id='3' author='raphtown' date='2017-01-25T02:13:28Z'>
		&lt;denchmark-link:https://github.com/raphtown&gt;@raphtown&lt;/denchmark-link&gt;
 Thanks for reporting this bug. As &lt;denchmark-link:https://github.com/drpngx&gt;@drpngx&lt;/denchmark-link&gt;
 said, a short reproduction code will be very helpful. Even if the code can't show the error without a multiple-GPU setup, we may be able to suggest a quick workaround.
		</comment>
		<comment id='4' author='raphtown' date='2017-06-06T00:18:11Z'>
		FYI, this bug about incompatibility between tfdbg and multiple GPUs should have been fixed by the commit above. Please let me know if you see any remaining issues.
		</comment>
		<comment id='5' author='raphtown' date='2018-05-31T13:22:04Z'>
		&lt;denchmark-link:https://github.com/caisq&gt;@caisq&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/aselle&gt;@aselle&lt;/denchmark-link&gt;

If you run  model from  repo: &lt;denchmark-link:https://github.com/tensorflow/tensor2tensor#sentiment-analysis&gt;https://github.com/tensorflow/tensor2tensor#sentiment-analysis&lt;/denchmark-link&gt;

with tfdbg enabled, it will report:
multiple devices with nodes named 'n/_78' but device_name is not specified
My node has 4 v100 gpus. The script I use attached here:
# t2t hparams
PROBLEM=sentiment_imdb
MODEL=transformer_encoder
HPARAMS=transformer_tiny

# t2t dirs
# MODEL-HPARAMS format is mandatory
PROJECT=/project/chli
TRAIN_DIR=$PROJECT/$MODEL/t2t_train/$PROBLEM/$MODEL-$HPARAMS
TMP_DIR=/tmp/t2t_datagen
DATA_DIR=$PROJECT/$MODEL/t2t_data

t2t-trainer \
  --data_dir=$DATA_DIR \
  --tmp_dir=$TMP_DIR \
  --output_dir=$TRAIN_DIR \
  --t2t_usr_dir=$USER_DIR \
  --problems=$PROBLEM \
  --model=$MODEL \
  --hparams_set=$HPARAMS \
  --generate_data=True \
  --train_steps=2000 \
  --worker_gpu=4 \
  --tfdbg
		</comment>
		<comment id='6' author='raphtown' date='2018-08-28T03:04:23Z'>
		Hello &lt;denchmark-link:https://github.com/raphtown&gt;@raphtown&lt;/denchmark-link&gt;
 ,did you solve this problem? I meet with the same problem as you, and I didn't find any useful information about this after looking through this page, I wonder did you solve this problem? and what did you do?
		</comment>
		<comment id='7' author='raphtown' date='2018-08-29T02:10:36Z'>
		Hello &lt;denchmark-link:https://github.com/spacegoing&gt;@spacegoing&lt;/denchmark-link&gt;
  ,did you solve this problem? I meet with the same problem as you, and I didn't find any useful information about this after looking through this page, I wonder did you solve this problem? and what did you do?
		</comment>
		<comment id='8' author='raphtown' date='2018-08-29T02:17:16Z'>
		&lt;denchmark-link:https://github.com/Vamix&gt;@Vamix&lt;/denchmark-link&gt;
  Sorry I didn't. I will let you know if I did:D
		</comment>
	</comments>
</bug>