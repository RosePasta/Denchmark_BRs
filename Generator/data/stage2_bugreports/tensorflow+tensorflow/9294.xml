<bug id='9294' author='kevinashaw' open_date='2017-04-18T18:46:17Z' closed_time='2018-01-16T13:50:27Z'>
	<summary>Tutorial has error: Recurrent Neural Networks</summary>
	<description>
Tutorial URL: &lt;denchmark-link:https://www.tensorflow.org/tutorials/recurrent&gt;https://www.tensorflow.org/tutorials/recurrent&lt;/denchmark-link&gt;

I'm going through the tutorial listed above and I think there is a mistake in the very first code example:
&lt;denchmark-code&gt;lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)
# Initial state of the LSTM memory.
state = tf.zeros([batch_size, lstm.state_size])
&lt;/denchmark-code&gt;

An error is reported for the third line:
&lt;denchmark-code&gt;ValueError: setting an array element with a sequence.
&lt;/denchmark-code&gt;

If one prints the lstm.state_size object (where say, lstm_size = 50) one finds:
&lt;denchmark-code&gt;LSTMStateTuple(c=50, h=50)
&lt;/denchmark-code&gt;

I'm guessing this should be:
&lt;denchmark-code&gt;lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)
# Initial state of the LSTM memory.
state = tf.zeros([batch_size, lstm_size])
&lt;/denchmark-code&gt;

But frankly there are numerous other errors in this tutorial as well, so I'm not sure.  I will continue to report them as I find them.
Version: tensorflow_gpu-1.0.1-cp27-none-linux_x86_64.whl
Running on Ubuntu 14.04
	</description>
	<comments>
		<comment id='1' author='kevinashaw' date='2017-04-18T18:53:23Z'>
		Thanks for reporting! It would be great if you could update us on additional errors as you follow along.
&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 FYI
		</comment>
		<comment id='2' author='kevinashaw' date='2017-04-18T18:59:05Z'>
		Will do.  The API change for TF1.0.1, especially for the RNN classes, has caused confusion and there is a real shortage of working RNN code samples.  The vast majority of the code examples on StackOverflow are from mid-2016 and they no longer work with the new API.  Thanks.
		</comment>
		<comment id='3' author='kevinashaw' date='2017-04-18T19:28:51Z'>
		Indeed, the tutorial has a one line warning:

"The basic pseudocode is as follows:"

we should really just highlight that in red above each code example.

And at the top we should link to the real code:

&lt;denchmark-link:https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb&gt;https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb&lt;/denchmark-link&gt;


that code looks to be up to date.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Tue, Apr 18, 2017 at 11:59 AM, Kevin Shaw ***@***.***&gt; wrote:
 Will do. The API change for TF1.0.1, especially for the RNN classes, has
 caused confusion and there is a real shortage of working RNN code samples.
 The vast majority of the code examples on StackOverflow are from mid-2016
 and they no longer work with the new API. Thanks.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#9294 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABtim1DMKf_TVeAvjH92tx9YW599CEmsks5rxQgOgaJpZM4NAvZe&gt;
 .



		</comment>
		<comment id='4' author='kevinashaw' date='2017-04-18T19:56:07Z'>
		&lt;denchmark-link:https://github.com/kevinashaw&gt;@kevinashaw&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/drpngx&gt;@drpngx&lt;/denchmark-link&gt;
, I just ran across the same problem. My concern is that  might not capture what needs to be captured. In the current API, the  is a tuple of  or , which should have size . Would this then require us to write ?
I am working through this as I type this. So far I have not quite hit on the solution.
Edit: perhaps state = lstm.zero_state(batch_size, dtype=tf.float32), or some other value for dtype? This creates a zeroed-out state tensor given the LSTM's existing state size, shape, etc.
		</comment>
		<comment id='5' author='kevinashaw' date='2017-04-18T20:18:47Z'>
		&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 : Thank you for your help!
I saw the "pseudocode" warning, but those two lines of code look like they should be self-contained.  Also, regardless of highlighting in red, the pseudocode should be solid starting point for working code.  There is so little working TF RNN code after the API change that this one document becomes a critical resource.
I can also confirm that the repo &lt;denchmark-link:https://github.com/tensorflow/models&gt;link&lt;/denchmark-link&gt;
 in the tutorial does not link to the file that you &lt;denchmark-link:https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb&gt;referenced&lt;/denchmark-link&gt;
.  Thank you for providing.  I will note that that code, as a tutorial example, is somewhat opaque.
		</comment>
		<comment id='6' author='kevinashaw' date='2017-04-18T20:39:56Z'>
		&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/drpngx&gt;@drpngx&lt;/denchmark-link&gt;
 : Would you like me to report additional errors on this thread, or on a new thread?
		</comment>
		<comment id='7' author='kevinashaw' date='2017-04-18T20:43:18Z'>
		I wonder if you could just edit the doc and send a PR when done
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Apr 18, 2017 1:41 PM, "Kevin Shaw" ***@***.***&gt; wrote:
 @ebrevdo &lt;https://github.com/ebrevdo&gt; @drpngx &lt;https://github.com/drpngx&gt;
 : Would you like me to report additional errors on this thread, or on a new
 thread?

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#9294 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AT_SbYtcZWmcfMVUuWEC6aTIdx-psZRBks5rxR_2gaJpZM4NAvZe&gt;
 .



		</comment>
		<comment id='8' author='kevinashaw' date='2017-04-18T20:48:47Z'>
		I would be glad to, but I'm not clear on what 'truth' is at this point.
Here is the second code sample:
&lt;denchmark-code&gt;# Placeholder for the inputs in a given iteration.
num_steps  = 20
batch_size = 1000
lstm_size  = 50
words = tf.placeholder(tf.int32, [batch_size, num_steps])

lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)
# Initial state of the LSTM memory.
initial_state = state = tf.zeros([batch_size, lstm.state_size])

for i in range(num_steps):
    # The value of state is updated after processing each batch of words.
    output, state = lstm(words[:, i], state)
&lt;/denchmark-code&gt;

Which yields: ValueError: setting an array element with a sequence. on the tf.zeros line.
I'm working on finding an executable replacement, but haven't found it yet.  I'm not clear on what the proper shape for the state should be.
		</comment>
		<comment id='9' author='kevinashaw' date='2017-04-18T20:53:34Z'>
		Following &lt;denchmark-link:https://github.com/Engineero&gt;@Engineero&lt;/denchmark-link&gt;
's recommendation, I can use  and I then get an error on the last line () with:
&lt;denchmark-code&gt;TypeError: 'Tensor' object is not iterable.
&lt;/denchmark-code&gt;

Which is opaque to me, since it does not look like we are iterating on a Tensor.
		</comment>
		<comment id='10' author='kevinashaw' date='2017-04-18T20:55:28Z'>
		&lt;denchmark-link:https://github.com/kevinashaw&gt;@kevinashaw&lt;/denchmark-link&gt;
 I think it needs to be  or some other value for , depending on the problem. This should create a zeroed-out initial state tensor based on the network's state parameters.
I am still having some other issues, but I think this got me through the initial state declaration.
		</comment>
		<comment id='11' author='kevinashaw' date='2017-04-18T20:58:20Z'>
		&lt;denchmark-link:https://github.com/Engineero&gt;@Engineero&lt;/denchmark-link&gt;
 : Thanks.
I have updated the test code, but it still fails with the same error  message:
&lt;denchmark-code&gt;# Placeholder for the inputs in a given iteration.
num_steps  = 20
batch_size = 1000
lstm_size  = 50
words = tf.placeholder(tf.int32, [batch_size, num_steps])

lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)
# Initial state of the LSTM memory.
initial_state = state = tf.zeros(batch_size, dtype=tf.float32)

for i in range(num_steps):
    # The value of state is updated after processing each batch of words.
    output, state = lstm(words[:, i], state)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='12' author='kevinashaw' date='2017-04-18T21:00:25Z'>
		Not to be too nit-picky about the tutorial, but the example code uses state_is_tuple=False which is now deprecated.  So the sample code should probably be updated.  I, myself, am still in clear on how this changes the code, but I hope to work it out soon.
		</comment>
		<comment id='13' author='kevinashaw' date='2017-04-18T21:02:02Z'>
		&lt;denchmark-link:https://github.com/kevinashaw&gt;@kevinashaw&lt;/denchmark-link&gt;
 that is not what I wrote. You are still using . Should be  I think.
		</comment>
		<comment id='14' author='kevinashaw' date='2017-04-18T21:05:55Z'>
		My bad.  Sorry.  Retrying...
		</comment>
		<comment id='15' author='kevinashaw' date='2017-04-18T21:14:27Z'>
		With the new state initialization line:
&lt;denchmark-code&gt;initial_state = state = lstm.zero_state(batch_size, dtype=tf.float32)
&lt;/denchmark-code&gt;

The new error is thrown on the last line:
&lt;denchmark-code&gt;ValueError: linear is expecting 2D arguments: [TensorShape([Dimension(1000)]), 
    TensorShape([Dimension(1000), Dimension(50)])]
&lt;/denchmark-code&gt;

Here is the full code sample:
&lt;denchmark-code&gt;# Placeholder for the inputs in a given iteration.
num_steps  = 20
batch_size = 1000
lstm_size  = 50
words = tf.placeholder(tf.int32, [batch_size, num_steps])

lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)
# Initial state of the LSTM memory.
initial_state = state = lstm.zero_state(batch_size, dtype=tf.float32)

for i in range(num_steps):
    # The value of state is updated after processing each batch of words.
    output, state = lstm(words[:, i], state)
&lt;/denchmark-code&gt;

If one does the following to determine the shape and type of state:
&lt;denchmark-code&gt;print(tf.shape(state))
print(state)
&lt;/denchmark-code&gt;

the following is printed:
&lt;denchmark-code&gt;Tensor("Shape:0", shape=(3,), dtype=int32)
LSTMStateTuple(c=&lt;tf.Tensor 'zeros_11:0' shape=(1000, 50) dtype=float32&gt;, h=&lt;tf.Tensor 'zeros_12:0' shape=(1000, 50) dtype=float32&gt;)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='16' author='kevinashaw' date='2017-04-18T21:23:11Z'>
		&lt;denchmark-link:https://github.com/kevinashaw&gt;@kevinashaw&lt;/denchmark-link&gt;
 that's where I am now too. It seems like we are not presenting arguments to the  method of  correctly. I am working a different problem, but same issue.
		</comment>
		<comment id='17' author='kevinashaw' date='2017-04-18T21:28:34Z'>
		&lt;denchmark-link:https://github.com/Engineero&gt;@Engineero&lt;/denchmark-link&gt;
 : Here is a dumb question that you may be able to help with.  If TF is capable of auto-unrolling LSTMs, then why do we need to use the  loop (in the sample above) to feed in the words one at a time?  I've never understood this.
		</comment>
		<comment id='18' author='kevinashaw' date='2017-04-18T21:35:59Z'>
		You don't.  We should really change the tutorial to use tf.nn.dynamic_rnn
-- which handles most of this for you.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Tue, Apr 18, 2017 at 2:28 PM, Kevin Shaw ***@***.***&gt; wrote:
 @Engineero &lt;https://github.com/Engineero&gt; : Here is a dumb question that
 you may be able to help with. If TF is capable of auto-unrolling LSTMs,
 then why do we need to use the for loop (in the sample above) to feed in
 the words one at a time? I've never understood this.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#9294 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABtim4IAH-RQE3--SUoWa8SY301zh6GEks5rxSsagaJpZM4NAvZe&gt;
 .



		</comment>
		<comment id='19' author='kevinashaw' date='2017-04-18T21:40:36Z'>
		It seems the unrolling should only be necessary for the training process.  At inference-time, it should function on a sample-by-sample basis.  But I guess it is just a memory vs speed trade-off question.
		</comment>
		<comment id='20' author='kevinashaw' date='2017-04-18T21:41:53Z'>
		&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 : could we get a short snippet of sample code using ?  :-)
		</comment>
		<comment id='21' author='kevinashaw' date='2017-04-18T21:45:12Z'>
		While this tutorial is just a little outdated (they still point to RNNCells
in tf.nn.rnn_cell instead of tf.contrib.rnn), a lot of the examples are
still very good:

&lt;denchmark-link:http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/&gt;http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/&lt;/denchmark-link&gt;


&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Tue, Apr 18, 2017 at 2:42 PM, Kevin Shaw ***@***.***&gt; wrote:
 @ebrevdo &lt;https://github.com/ebrevdo&gt; : could we get a short snippet of
 sample code using tf.nn.dynamic_rnn? :-)

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#9294 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABtim-CBYbG8zxpfR9DY5P_NHWDo-HxPks5rxS43gaJpZM4NAvZe&gt;
 .



		</comment>
		<comment id='22' author='kevinashaw' date='2017-04-18T22:53:50Z'>
		&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 this may be a separate issue, but on that tutorial that you linked, do you have any idea how one should replace  calls? It gives a deprecation warning with the following:

Instructions for updating:
graph_actions.py will be deleted. Use tf.train.* utilities instead. You can use learn/estimators/estimator.py as an example.

The file this sends you to does not seem to be very helpful.
Thanks!
		</comment>
		<comment id='23' author='kevinashaw' date='2017-04-19T00:09:30Z'>
		&lt;denchmark-link:https://github.com/martinwicke&gt;@martinwicke&lt;/denchmark-link&gt;
 what's the replacement for tf.contrib.learn.run_n?
		</comment>
		<comment id='24' author='kevinashaw' date='2017-04-19T00:22:55Z'>
		We have none. It was never used after initial experimentation, so we decided to scrap it altogether. You can get the same behavior using tf.contrib.training.train, if you need feeds, with a Hook to add the feeds.
		</comment>
		<comment id='25' author='kevinashaw' date='2017-04-21T15:34:09Z'>
		&lt;denchmark-link:https://github.com/kevinashaw&gt;@kevinashaw&lt;/denchmark-link&gt;
 For the record, I found &lt;denchmark-link:https://gist.github.com/danijar/61f9226f7ea498abce36187ddaf51ed5&gt;this gist&lt;/denchmark-link&gt;
 to be helpful. He also has &lt;denchmark-link:http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/&gt;a blog&lt;/denchmark-link&gt;
 where he talks about some of the undocumented features of an older version of TensorFlow. You still have to change out  for , but this helped me get something going.
That said, I am now having an issue where my network is not updating at all. I train it for several epochs and get the exact same results every time.
		</comment>
		<comment id='26' author='kevinashaw' date='2017-05-01T23:48:55Z'>
		&lt;denchmark-link:https://github.com/Engineero&gt;@Engineero&lt;/denchmark-link&gt;
 : Thanks!
		</comment>
		<comment id='27' author='kevinashaw' date='2017-05-10T07:12:07Z'>
		Following the discussion in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/9699&gt;#9699&lt;/denchmark-link&gt;
, I've at least found a solution to the initial_state problem.  It would probably make sense to reference it in the new tutorial/documentation.   The problem with the initial_state is not that  produces incorrect results :
&lt;denchmark-code&gt;initial_state = lstm_cells.zero_state(batch_size, tf.float32)
&lt;/denchmark-code&gt;

Its that the batch_size value is a constant.  For the training data, the size of the batch will always match the batch_size.  But for Testing data it usually never will.  Testing is done on a much large set, or atleast a set that has a size that varies from the batch_size.  Hence, for testing data, the initial_state from the test data will still have a size of batch_size but the test data itself will not.
The solution is surprisingly simple, just let batch_size vary according to the data set brought in:
&lt;denchmark-code&gt;batch_size_var  = tf.shape(Xin)[0]
initial_state = lstm_cells.zero_state(batch_size_var, tf.float32) 
&lt;/denchmark-code&gt;

Where Xin is the data input tensor.  Now initial_state will always match the size of the data set.
		</comment>
		<comment id='28' author='kevinashaw' date='2017-05-10T21:27:11Z'>
		Right. tf.nn.dynamic_rnn will call zero_state for you with the right value
of you pass in just the dtype argument and not an initial state.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On May 10, 2017 12:12 AM, "Kevin Shaw" ***@***.***&gt; wrote:
 Following the discussion in #9699
 &lt;#9699&gt;, I've at least
 found a solution to the initial_state problem. It would probably make sense
 to reference it in the new tutorial/documentation. The problem with the
 initial_state is not that zero_state produces incorrect results :

 initial_state = lstm_cells.zero_state(batch_size, tf.float32)

 Its that the batch_size value is a constant. For the training data, the
 size of the batch will *always* match the batch_size. But for Testing
 data it usually never will. Testing is done on a much large set, or atleast
 a set that has a size that varies from the batch_size. Hence, for testing
 data, the initial_state from the test data will still have a size of
 batch_size but the test data itself will not.
 The solution is surprisingly simple, just let batch_size vary according to
 the data set brought in:

 batch_size_var  = tf.shape(Xin)[0]
 initial_state = lstm_cells.zero_state(batch_size_var, tf.float32)

 Where Xin is the data input tensor. Now initial_state will always match
 the size of the data set.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#9294 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABtim6FFXxRWKcofk4unjdfMrJVZ1DmGks5r4WNjgaJpZM4NAvZe&gt;
 .



		</comment>
		<comment id='29' author='kevinashaw' date='2017-05-11T22:30:47Z'>
		Thanks!  The documentation regarding that feature is somewhat thin for tf.nn.dynamic_rnn.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


 On May 10, 2017, at 2:30 PM, ebrevdo ***@***.***&gt; wrote:

 Right. tf.nn.dynamic_rnn will call zero_state for you with the right value
 of you pass in just the dtype argument and not an initial state.

 On May 10, 2017 12:12 AM, "Kevin Shaw" ***@***.***&gt; wrote:

 &gt; Following the discussion in #9699
 &gt; &lt;#9699&gt;, I've at least
 &gt; found a solution to the initial_state problem. It would probably make sense
 &gt; to reference it in the new tutorial/documentation. The problem with the
 &gt; initial_state is not that zero_state produces incorrect results :
 &gt;
 &gt; initial_state = lstm_cells.zero_state(batch_size, tf.float32)
 &gt;
 &gt; Its that the batch_size value is a constant. For the training data, the
 &gt; size of the batch will *always* match the batch_size. But for Testing
 &gt; data it usually never will. Testing is done on a much large set, or atleast
 &gt; a set that has a size that varies from the batch_size. Hence, for testing
 &gt; data, the initial_state from the test data will still have a size of
 &gt; batch_size but the test data itself will not.
 &gt; The solution is surprisingly simple, just let batch_size vary according to
 &gt; the data set brought in:
 &gt;
 &gt; batch_size_var = tf.shape(Xin)[0]
 &gt; initial_state = lstm_cells.zero_state(batch_size_var, tf.float32)
 &gt;
 &gt; Where Xin is the data input tensor. Now initial_state will always match
 &gt; the size of the data set.
 &gt;
 &gt; —
 &gt; You are receiving this because you were mentioned.
 &gt; Reply to this email directly, view it on GitHub
 &gt; &lt;#9294 (comment)&gt;,
 &gt; or mute the thread
 &gt; &lt;https://github.com/notifications/unsubscribe-auth/ABtim6FFXxRWKcofk4unjdfMrJVZ1DmGks5r4WNjgaJpZM4NAvZe&gt;
 &gt; .
 &gt;
 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub &lt;#9294 (comment)&gt;, or mute the thread &lt;https://github.com/notifications/unsubscribe-auth/AGz330YtgDPyVRCxft8ZrEt0wNMS-Rnpks5r4iyFgaJpZM4NAvZe&gt;.



		</comment>
		<comment id='30' author='kevinashaw' date='2017-05-24T19:36:33Z'>
		&lt;denchmark-link:https://github.com/martinwicke&gt;@martinwicke&lt;/denchmark-link&gt;
 , for &lt;denchmark-link:https://github.com/dennybritz/tf-rnn/blob/master/sequence_example.ipynb&gt;https://github.com/dennybritz/tf-rnn/blob/master/sequence_example.ipynb&lt;/denchmark-link&gt;
  as mentioned eariler changed  tf.contrib.learn.run_n(...) to tf.contrib.training.train in the code, but it seems running into the inifinite loop.
		</comment>
		<comment id='31' author='kevinashaw' date='2017-05-25T18:40:43Z'>
		Yes, you'll have to limit the number of steps using a &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/train/StopAtStepHook&gt;StepCounterHook&lt;/denchmark-link&gt;
 or make sure your input function throws an appropriate exception after the training data is depleted (either  or ).
		</comment>
		<comment id='32' author='kevinashaw' date='2017-11-17T13:17:11Z'>
		&lt;denchmark-link:https://github.com/kevinashaw&gt;@kevinashaw&lt;/denchmark-link&gt;
 Did you finally find a solution for the linear problem?
&lt;denchmark-code&gt;ValueError: linear is expecting 2D arguments: [TensorShape([Dimension(1000)]), 
TensorShape([Dimension(1000), Dimension(50)])]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='33' author='kevinashaw' date='2017-12-22T07:31:01Z'>
		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
		</comment>
		<comment id='34' author='kevinashaw' date='2017-12-30T00:28:42Z'>
		&lt;denchmark-link:https://github.com/MarkDaoust&gt;@MarkDaoust&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/mhyttsten&gt;@mhyttsten&lt;/denchmark-link&gt;
, something to consider when thinking about the fate of our tutorials.
		</comment>
		<comment id='35' author='kevinashaw' date='2018-01-13T18:54:26Z'>
		Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='36' author='kevinashaw' date='2018-01-16T13:50:27Z'>
		While it could use some modernization, the tutorial is working fine now.
		</comment>
		<comment id='37' author='kevinashaw' date='2018-04-10T23:01:15Z'>
		Thanks to &lt;denchmark-link:https://github.com/kevinashaw&gt;@kevinashaw&lt;/denchmark-link&gt;
 for pointing out this problem.
As far as I can see &lt;denchmark-link:https://github.com/MarkDaoust&gt;@MarkDaoust&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/drpngx&gt;@drpngx&lt;/denchmark-link&gt;

the tutorial code posted here and at &lt;denchmark-link:https://www.tensorflow.org/tutorials/recurrent&gt;TF site&lt;/denchmark-link&gt;
 still has the same errors after nearly 12 months
I did try making some of the modifications suggested above, they don't appear to solve the issue
&lt;denchmark-code&gt;Xin =tf.random_normal((100,100))

batch_size_var  = tf.shape(Xin)[0]
lstm = tf.nn.rnn_cell.LSTMCell(lstm_size) #with dynamic rnn the reference to .LSTMCell is invalid
hidden_state = lstm_cells.zero_state(batch_size_var, tf.float32) 
current_state = lstm_cells.zero_state(batch_size_var, tf.float32)
&lt;/denchmark-code&gt;

Is there another RNN tutorial anyone can recommend?
		</comment>
		<comment id='38' author='kevinashaw' date='2018-06-10T13:47:56Z'>
		How about try to use lstm.state_size[0] or lstm.state_size[1] instead of just using lstm.state_size
		</comment>
	</comments>
</bug>