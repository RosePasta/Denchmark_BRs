<bug id='30409' author='morgangiraud' open_date='2019-07-04T09:30:57Z' closed_time='2019-09-09T12:33:16Z'>
	<summary>[TF2.0] TensorArray and tf.function</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code: yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX
TensorFlow installed from (source or binary): binary - 2.0.0-beta1
TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1
Python version: 3.6

Describe the current behaviour
The code works in eager mode but does not work when applying the decorator @tf.function.
2 errors happen sequentially:


The first one is about types, Dataset.enumerate() outputs an index of type int64 which is different from a TensorArray inner type (int32).


The second one breaks because of uninitialized tensors.


Describe the expected behaviour
For the first one, It should break in eager mode too.
For the second one, it should work as in eager mode
Code to reproduce the issue
import tensorflow as tf

a = tf.random.uniform([10, 2])
d = tf.data.Dataset.from_tensor_slices(a).batch(2)


def accumulate(d):
    arr = tf.TensorArray(tf.float32, 1, dynamic_size=True)

    for i, t in d.enumerate():
        arr.write(i, t)

    return arr.concat()


@tf.function
def accumulate_f_error1(d):
    arr = tf.TensorArray(tf.float32, 1, dynamic_size=True)

    for i, t in d.enumerate():
        arr.write(i, t)

    return arr.concat()


@tf.function
def accumulate_f_error2(d):
    arr = tf.TensorArray(tf.float32, 1, dynamic_size=True)

    for i, t in d.enumerate():
        j = tf.cast(i, tf.int32)
        arr.write(j, t)

    return arr.concat()


# Works well
data = accumulate(d)
print(data)

# # Breaks because of  a wrong type
# data = accumulate_f_error1(d)
# print(data)

# Breaks due to uninitialized tensors?
data = accumulate_f_error2(d)
print(data)
	</description>
	<comments>
		<comment id='1' author='morgangiraud' date='2019-07-05T07:50:14Z'>
		I am able to reproduce the above mention issue on Colab with Tensorflow 2.0.0.beta1.
		</comment>
		<comment id='2' author='morgangiraud' date='2019-07-07T00:42:54Z'>
		Replace arr.write(j, t) with arr = arr.write(j, t)
The issue is that tf.function executes as a graph. In eager mode the array will be updated (as a convenience), but you're really meant to use the return value to chain operations: &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/TensorArray#returns_6&gt;https://www.tensorflow.org/api_docs/python/tf/TensorArray#returns_6&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='morgangiraud' date='2019-07-07T09:12:22Z'>
		Thanks a lot for your answer, this was not obvious to me.
I've also noticed that this one also return an error when using tf.function:
def accumulate_arg(d, arr):
    for i, t in d.enumerate():
        arr = arr.write(tf.cast(i, tf.int32), t)

    return arr.concat()


@tf.function
def accumulate_graph_arg(d, arr):
    for i, t in d.enumerate():
        arr = arr.write(tf.cast(i, tf.int32), t)

    return arr.concat()


# Works well
arr = tf.TensorArray(tf.float32, 1, dynamic_size=True)
data = accumulate_arg(d, arr)
print("accumulate_arg\n", data)

# breaks
arr = tf.TensorArray(tf.float32, 1, dynamic_size=True)
data = accumulate_graph_arg(d, arr)
print("accumulate_graph_arg\n", data)
But now I'm not sure anymore if this is a bug or not. TF2 is really nice but can clearly be quite strange to figure out if something is not working because of a bug or not.
		</comment>
		<comment id='4' author='morgangiraud' date='2019-07-07T16:59:14Z'>
		In the latter example you're trying to write an int32 where TensorFlow expects a float32. Again, my guess is that eager does some auto casting for convenience; however that generally shouldn't be relied upon.
		</comment>
		<comment id='5' author='morgangiraud' date='2019-07-07T16:59:16Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=30409&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=30409&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='morgangiraud' date='2019-07-07T18:22:49Z'>
		I'm sorry but I don't believe this has anything to do with autocasting in eager mode because it has nothing to do with eager mode.
I should have put the whole example.
import tensorflow as tf


# Data types is float32
a = tf.random.uniform([10, 2])
d = tf.data.Dataset.from_tensor_slices(a).batch(2)


@tf.function
def accumulate_graph(d):
    arr = tf.TensorArray(tf.float32, 1, dynamic_size=True)

    for i, t in d.enumerate():
        arr = arr.write(tf.cast(i, tf.int32), t)

    return arr.concat()


@tf.function
def accumulate_graph_arg(d, arr):
    for i, t in d.enumerate():
        arr = arr.write(tf.cast(i, tf.int32), t)

    return arr.concat()



# Works well (not eager mode)
data = accumulate_graph(d)
print("accumulate_graph\n", data)


# Does not work
# The only thing that changes is that the tensorArray is coming from outside the function

# Error: ValueError: Cannot convert a partially known TensorShape to a Tensor: &lt;unknown&gt;

arr = tf.TensorArray(tf.float32, 1, dynamic_size=True)
data = accumulate_graph_arg(d, arr)
print("accumulate_graph_arg\n", data)
		</comment>
		<comment id='7' author='morgangiraud' date='2019-07-07T18:49:07Z'>
		Ah, I see. Thanks for clarifying. You can fix it by explicitly setting the element shape:
&lt;denchmark-code&gt;arr = tf.TensorArray(tf.float32, 1, dynamic_size=True, infer_shape=False, element_shape=(2, 2))
data = accumulate_graph_arg(d, arr)
&lt;/denchmark-code&gt;

However the fact that the default behavior doesn't cross a tf.function boundary well seems less than ideal. &lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
 Is passing a TensorArray with infer_shape=True to a function a supported use case, or does the side effect (modifying the array when it sees data) preclude that. (In which case a more helpful error message at the tf.function boundary would probably help.)
		</comment>
		<comment id='8' author='morgangiraud' date='2019-07-08T07:34:17Z'>
		This appears to be weird for me because it is explicitly written in the doc that the batch dimension can be different between elements: All of the values must have been written, their ranks must match, and their shapes must all match for all dimensions except the first.
If I'm not mistaken, in the current use case, when the dataset iteration returns not enough elements in the last batch, it will break again.
Also, accumulating tensors is very useful sometimes, is there any better way to do that?
		</comment>
		<comment id='9' author='morgangiraud' date='2019-07-09T09:07:28Z'>
		TensorArray is just a python object as far as tf.function is concerned (i.e. it's not a composite tensor) so I don't understand how the element shape gets dropped.
&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
 could this be related to autograph?
		</comment>
		<comment id='10' author='morgangiraud' date='2019-07-09T10:57:35Z'>
		Nevermind, it's completely unrelated to autograph.
What seems to be happening here is that tf.function treats TensorArray as a python object, but TensorArray has different implementations in graph and eager modes (in graph mode we can use the tensorlist ops and do zero copies, but in eager mode the tensor list ops have to copy, as there are always outstanding references to its inputs; so for eager tensor array we use an implementation backed by a python list) but when we create a tensor array eagerly and pass it into a function we try to use the eager version which can be somewhat confusing.
I don't know if this is the issue, but if it is then we can work around it by avoiding having tensorarrays cross the eager / function boundary, and have a long term fix by maybe making tensorarrays composite tensors and finding a way to cast from the eager representation to the graph representation when passing them to functions.
		</comment>
		<comment id='11' author='morgangiraud' date='2019-09-09T12:33:18Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=30409&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=30409&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='morgangiraud' date='2019-09-09T12:33:26Z'>
		We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!
		</comment>
		<comment id='13' author='morgangiraud' date='2019-11-28T10:43:18Z'>
		I have opened a new issue, related to the end of the discussion here:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/34683&gt;Issue 34683&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>