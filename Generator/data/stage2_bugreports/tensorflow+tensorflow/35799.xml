<bug id='35799' author='faustomorales' open_date='2020-01-12T16:36:32Z' closed_time='2020-05-28T18:22:21Z'>
	<summary>Incompatible shapes when using tf.keras.backend.ctc_decode</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian Buster
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0
Python version: 3.7
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A

Describe the current behavior
When using a tf.keras.backend.ctc_decode with a batch size &lt;  the size of the model input, a ValueError is raised related to failure to broadcast input shapes.
Describe the expected behavior
I expect shapes to be consistent and therefore no ValueError to be raised.
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
import tensorflow as tf
import numpy as np

def CTCDecoder():
    def decoder(y_pred):
        input_shape = tf.keras.backend.shape(y_pred)
        input_length = tf.ones(shape=input_shape[0]) * tf.keras.backend.cast(input_shape[1], 'float32')
        return tf.keras.backend.ctc_decode(y_pred, input_length)[0][0]
    return tf.keras.layers.Lambda(decoder, name='decode')

input_layer = tf.keras.layers.Input((48, 37))
x = CTCDecoder()(input_layer)
model = tf.keras.models.Model(inputs=input_layer, outputs=x)

# This never raises a ValueError. The batch size is equal to the length
# of the input.
y = model.predict(np.random.uniform(size=(100, 48, 37)), batch_size=100)

# This usually raises a ValueError.
y = model.predict(np.random.uniform(size=(100, 48, 37)), batch_size=32)

# This always raises a ValueError.
y = model.predict(np.random.uniform(size=(100, 48, 37)), batch_size=1)
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Here is the full traceback for an example exception.
&lt;denchmark-code&gt;---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-145-9e000cf7055c&gt; in &lt;module&gt;
----&gt; 1 y = model.predict(np.random.uniform(size=(100, 48, 37)), batch_size=1)

/usr/src/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in predict(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)
   1011         max_queue_size=max_queue_size,
   1012         workers=workers,
-&gt; 1013         use_multiprocessing=use_multiprocessing)
   1014 
   1015   def reset_metrics(self):

/usr/src/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in predict(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)
    496         model, ModeKeys.PREDICT, x=x, batch_size=batch_size, verbose=verbose,
    497         steps=steps, callbacks=callbacks, max_queue_size=max_queue_size,
--&gt; 498         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)
    499 
    500 

/usr/src/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in _model_iteration(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)
    473               mode=mode,
    474               training_context=training_context,
--&gt; 475               total_epochs=1)
    476           cbks.make_logs(model, epoch_logs, result, mode)
    477 

/usr/src/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)
    177             batch_outs,
    178             batch_start=step * batch_size,
--&gt; 179             batch_end=step * batch_size + current_batch_size)
    180       cbks.make_logs(model, batch_logs, batch_outs, mode)
    181       step += 1

/usr/src/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py in aggregate(self, batch_outs, batch_start, batch_end)
    345     batch_outs = nest.flatten_up_to(self._structure, batch_outs)
    346     for batch_element, result in zip(batch_outs, self.results):
--&gt; 347       result.aggregate(batch_element, batch_start, batch_end)
    348 
    349   def finalize(self):

/usr/src/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py in aggregate(self, batch_element, batch_start, batch_end)
    278     num_elements = np.prod(batch_element.shape)
    279     if num_elements &lt; self._BINARY_SIZE_THRESHOLD:
--&gt; 280       self.results[batch_start:batch_end] = batch_element
    281     else:
    282       is_finished = threading.Event()

ValueError: could not broadcast input array from shape (1,46) into shape (1,48)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='faustomorales' date='2020-01-12T16:51:36Z'>
		For anyone coming across this issue, I've found that I can work around it by padding the output before returning results.
def CTCDecoder():
    def decoder(y_pred):
        input_shape = tf.keras.backend.shape(y_pred)
        input_length = tf.ones(shape=input_shape[0]) * tf.keras.backend.cast(input_shape[1], 'float32')
        unpadded = tf.keras.backend.ctc_decode(y_pred, input_length)[0][0]
        unpadded_shape = tf.keras.backend.shape(unpadded)
        # I don't *think* I should have to do this but it seems as though I do.
        padded = tf.pad(
            unpadded,
            paddings=[[0, 0], [0, input_shape[1] - unpadded_shape[1]]],
            constant_values=-1
        )
        return padded
    return tf.keras.layers.Lambda(decoder, name='decode')
The following snippet verifies consistency across different batch sizes.
input_layer = tf.keras.layers.Input((48, 37))
x = CTCDecoder()(input_layer)
model = tf.keras.models.Model(inputs=input_layer, outputs=x)

X = np.random.uniform(size=(100, 48, 37))

y100 = model.predict(X, batch_size=100)
y1 = model.predict(X, batch_size=1)
y32 = model.predict(X, batch_size=32)

np.testing.assert_almost_equal(y100, y1)
np.testing.assert_almost_equal(y100, y32)
		</comment>
		<comment id='2' author='faustomorales' date='2020-01-12T17:34:15Z'>
		It seems that small modifications to keras.backend could resolve this.



tensorflow/tensorflow/python/keras/backend.py


        Lines 5817 to 5832
      in
      30b98cc






   """ 



 y_pred = math_ops.log(array_ops.transpose(y_pred, perm=[1, 0, 2]) + epsilon()) 



 input_length = math_ops.cast(input_length, dtypes_module.int32) 



 



 if greedy: 



     (decoded, log_prob) = ctc.ctc_greedy_decoder( 



 inputs=y_pred, sequence_length=input_length) 



 else: 



     (decoded, log_prob) = ctc.ctc_beam_search_decoder( 



 inputs=y_pred, 



 sequence_length=input_length, 



 beam_width=beam_width, 



 top_paths=top_paths) 



 decoded_dense = [ 



 sparse_ops.sparse_to_dense( 



 st.indices, st.dense_shape, st.values, default_value=-1) 





I would have thought that st.dense_shape should be something like (input_shape[0], input_shape[1] where input_shape = tf.keras.backend.shape(y_pred) (assuming you add this line before the transpose at the top of the function). But this causes the first assertion in the associated test (see below) to fail because it assumes that the output has not been padded with -1 values, which seems incorrect to me but I'm really not sure.



tensorflow/tensorflow/python/keras/backend_test.py


        Lines 1779 to 1783
      in
      03c045e






 for i in range(top_paths): 



 self.assertTrue( 



 np.alltrue( 



 decode_truth[i] == keras.backend.eval(decode_pred_tf[i]))) 



 self.assertAllClose(log_prob_truth, log_prob_pred) 





I'm not sure exactly what the desired behavior is here. If the current behavior is correct, then I propose that this issue be closed without action as the workaround above suits my purposes just fine. Otherwise, I'm glad to file a PR with adjustments to the function and associated test.
		</comment>
		<comment id='3' author='faustomorales' date='2020-01-23T20:00:47Z'>
		&lt;denchmark-link:https://github.com/faustomorales&gt;@faustomorales&lt;/denchmark-link&gt;
, would you be open to send out a PR with your modification that fixes the problem? Thanks in advance.
		</comment>
		<comment id='4' author='faustomorales' date='2020-01-24T14:58:45Z'>
		Yes, certainly I can do that. Will try to get it out soon.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Thu, Jan 23, 2020 at 2:00 PM Rick Chao ***@***.***&gt; wrote:
 @faustomorales &lt;https://github.com/faustomorales&gt;, would you be open to
 send out a PR with your modification that fixes the problem? Thanks in
 advance.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#35799?email_source=notifications&amp;email_token=ACKIZ2OI6QV2U2SFF5MJH43Q7HZPNA5CNFSM4KFYS7QKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEJYUXSQ#issuecomment-577850314&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/ACKIZ2PX3BPPYMTIDYORP23Q7HZPNANCNFSM4KFYS7QA&gt;
 .



		</comment>
		<comment id='5' author='faustomorales' date='2020-05-28T18:22:23Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35799&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35799&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>