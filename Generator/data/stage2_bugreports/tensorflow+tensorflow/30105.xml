<bug id='30105' author='devstein' open_date='2019-06-25T00:20:42Z' closed_time='2019-07-03T21:40:28Z'>
	<summary>Dataset.cache() Followed by Dataset.zip() Throws AlreadyExistsError</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX Mojave 10.14.5
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): pipenv install --pre tensorflow==2.0.0-beta1
TensorFlow version (use command below): 2.0.0-beta1
Python version: 3.6.8
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A

1v2.0.0-beta0-16-g1d91213fe7
You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior

Cache a dataset
Derive other datasets from it
Zip the derived datasets into a single dataset
See cache error

&lt;denchmark-code&gt;AlreadyExistsError: There appears to be a concurrent caching iterator running - cache lockfile already exists
&lt;/denchmark-code&gt;

Describe the expected behavior
Only cache the dataset once and not throw an error.
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
import tensorflow as tf


def map_file_to_xy_dataset(filename, params):
    # Generate a dataset from the filename
    csv_dataset = tf.data.experimental.CsvDataset(...)

    # Cache at unique file location (i.e filename)
    cached_dataset = csv_dataset.cache(...)

    # Generate x dataset (all but id column)
    x_dataset = cached_dataset.map(lambda *x: x[0:-1])

    # Generate y dataset (last column of each file)
    y_dataset = cached_dataset.map(lambda *x: x[-1])

    # ZIP the X, Y datasets 
    # This will throw AlreadyExistsError 
    xy_dataset = tf.data.Dataset.zip((x_dataset, y_dataset))

    return xy_dataset


def generate_dataset():
    filenames_dataset = tf.data.Dataset.list_files(data_glob)

    dataset = filenames_dataset.flat_map(map_file_to_xy_dataset)

    return dataset

Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='devstein' date='2019-06-26T12:04:03Z'>
		I have tried to reproduce with below code snippet in Colab with TF CPU version 2.0beta1 and was not able to reproduce the error.
&lt;denchmark-code&gt;import tensorflow as tf

def map_file_to_xy_dataset(filename):
    # Generate a dataset from the filename
    csv_dataset = tf.data.experimental.CsvDataset("Iris.csv",["float32","float32","float32","float32","float32"])

    # Cache at unique file location (i.e filename)
    cached_dataset = csv_dataset.cache("Iris.csv")

    # Generate x dataset (all but id column)
    x_dataset = csv_dataset.map(lambda *x: x[0:-1])

    # Generate y dataset (last column of each file)
    y_dataset = csv_dataset.map(lambda *x: x[-1])

    # ZIP the X, Y datasets 
    # This will throw AlreadyExistsError 
    xy_dataset = tf.data.Dataset.zip((x_dataset, y_dataset))

    return xy_dataset


def generate_dataset():
    filenames_dataset = tf.data.Dataset.list_files("Iris.csv")

    dataset = filenames_dataset.flat_map(map_file_to_xy_dataset)

    return dataset

dataset=generate_dataset()
dataset
&lt;/denchmark-code&gt;

Please let us know if we are missing out something. Thanks!
		</comment>
		<comment id='2' author='devstein' date='2019-06-27T01:56:54Z'>
		&lt;denchmark-link:https://github.com/achandraa&gt;@achandraa&lt;/denchmark-link&gt;
 Thanks for looking into this! Did you iterate over the dataset? I'm still able to reproduce with your code by adding the following:
for x,y in dataset:
    print(x,y)
If this still doesn't work, the only other difference is the filenames_dataset I'm using has multiple files.
		</comment>
		<comment id='3' author='devstein' date='2019-07-01T10:15:33Z'>
		&lt;denchmark-link:https://github.com/devstein&gt;@devstein&lt;/denchmark-link&gt;
 I am not able to reproduce the issue with above code and where to add these two lines
&lt;denchmark-code&gt;for x,y in dataset:
    print(x,y) . 

```Can you help us to reproduce the issue. Thanks!
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='devstein' date='2019-07-01T15:10:16Z'>
		&lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
 Apologies there was a mistake in the demo code, it wasn't mapping over the . Here is the fixed version below:
import tensorflow as tf

def map_file_to_xy_dataset(filename):
    # Generate a dataset from the filename
    csv_dataset = tf.data.experimental.CsvDataset("Iris.csv",["float32","float32","float32","float32","float32"])

    # Cache at unique file location (i.e filename)
    cached_dataset = csv_dataset.cache("Iris.csv")

    # Generate x dataset (all but id column)
    x_dataset = cached_dataset.map(lambda *x: x[0:-1])

    # Generate y dataset (last column of each file)
    y_dataset = cached_dataset.map(lambda *x: x[-1])

    # ZIP the X, Y datasets 
    # This will throw AlreadyExistsError 
    xy_dataset = tf.data.Dataset.zip((x_dataset, y_dataset))

    return xy_dataset


def generate_dataset():
    filenames_dataset = tf.data.Dataset.list_files("Iris.csv")

    dataset = filenames_dataset.flat_map(map_file_to_xy_dataset)

    return dataset

dataset=generate_dataset()
dataset
Otherwise did you try using a tf.data.Dataset.list_files("*.csv") where there are multiples CSV files in the file path? This is the only difference between my code and the example
		</comment>
		<comment id='5' author='devstein' date='2019-07-02T08:14:40Z'>
		&lt;denchmark-link:https://github.com/devstein&gt;@devstein&lt;/denchmark-link&gt;
 Tried reproducing the issue and I received error saying . Thanks!
		</comment>
		<comment id='6' author='devstein' date='2019-07-02T15:29:03Z'>
		&lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
 What line did you receive this error? Did you try
for x,y in dataset:
  print(x,y)
		</comment>
		<comment id='7' author='devstein' date='2019-07-02T21:47:21Z'>
		&lt;denchmark-link:https://github.com/devstein&gt;@devstein&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
 This error message  looks reasonable. It is caused by that multiple datasets are trying to be cached in the same file. You can change the line  to be  so that each cached dataset has a unique file name.
		</comment>
		<comment id='8' author='devstein' date='2019-07-02T22:11:47Z'>
		&lt;denchmark-link:https://github.com/feihugis&gt;@feihugis&lt;/denchmark-link&gt;
 I am caching to a unique location for every file. This is my exact code for generating each filename:
def generate_cache_filename(filename, directory):
    filename_no_slashes = tf.strings.regex_replace(filename, r"\/", "_")

    cache_filename = tf.strings.join([directory, filename_no_slashes],
                                     separator="/")

    return cache_filename
Also for context, this caching code doesn't throw an error unless I call Dataset.zip on the two derived datasets
		</comment>
		<comment id='9' author='devstein' date='2019-07-02T22:36:54Z'>
		&lt;denchmark-link:https://github.com/devstein&gt;@devstein&lt;/denchmark-link&gt;
 Could you print the filenames used for the cache to make sure they are unique?
The following code works for me:
  def map_file_to_xy_dataset(filename):
    # Generate a dataset from the filename
    csv_dataset = tf.data.experimental.CsvDataset(filename,
                                                  ["float32", "float32",
                                                   "float32", "float32",
                                                   "float32"],
                                                  header=True,)

    # Cache at unique file location (i.e filename)
    cached_dataset = csv_dataset.cache("cached_"+filename)

    # Generate x dataset (all but id column)
    x_dataset = csv_dataset.map(lambda *x: x[0:1])

    # Generate y dataset (last column of each file)
    y_dataset = csv_dataset.map(lambda *x: x[-1])

    # ZIP the X, Y datasets
    # This will throw AlreadyExistsError
    xy_dataset = tf.data.Dataset.zip((x_dataset, y_dataset))

    return xy_dataset

  def generate_dataset(file_pattern):
    filenames_dataset = tf.data.Dataset.list_files(file_pattern)
    dataset = filenames_dataset.flat_map(map_file_to_xy_dataset)
    return dataset

  dataset = generate_dataset("/Users/fei/Desktop/csv_test/*.csv")
  for x, y in dataset:
    print(x, y)
		</comment>
		<comment id='10' author='devstein' date='2019-07-02T22:46:35Z'>
		&lt;denchmark-link:https://github.com/feihugis&gt;@feihugis&lt;/denchmark-link&gt;

Can you re-run with the corrected code below? I run into the issue when mapping over the cached_dataset not the csv_dataset
  def map_file_to_xy_dataset(filename):
    # Generate a dataset from the filename
    csv_dataset = tf.data.experimental.CsvDataset(filename,
                                                  ["float32", "float32",
                                                   "float32", "float32",
                                                   "float32"],
                                                  header=True,)

    # Cache at unique file location (i.e filename)
    cached_dataset = csv_dataset.cache("cached_"+filename)

    # Generate x dataset (all but id column)
    x_dataset = cached_dataset.map(lambda *x: x[0:1])

    # Generate y dataset (last column of each file)
    y_dataset = cached_dataset.map(lambda *x: x[-1])

    # ZIP the X, Y datasets
    # This will throw AlreadyExistsError
    xy_dataset = tf.data.Dataset.zip((x_dataset, y_dataset))

    return xy_dataset

  def generate_dataset(file_pattern):
    filenames_dataset = tf.data.Dataset.list_files(file_pattern)
    dataset = filenames_dataset.flat_map(map_file_to_xy_dataset)
    return dataset

  dataset = generate_dataset("/Users/fei/Desktop/csv_test/*.csv")
  for x, y in dataset:
    print(x, y)
Thanks for the help!
		</comment>
		<comment id='11' author='devstein' date='2019-07-02T23:09:51Z'>
		Oh, my bad. Now I got this error tensorflow.python.framework.errors_impl.NotFoundError: cached_/Users/fei/Desktop/csv_test/3.csv_0.lockfile; No such file or directory [Op:IteratorGetNextSync]. It is different from yours.
I guess the problem you met is caused by the cache file name. [After changing cached_dataset = csv_dataset.cache("cached_"+filename) to cached_dataset = csv_dataset.cache("cached_"), I met the same error with yours.]
I am looking into the error I met.
		</comment>
		<comment id='12' author='devstein' date='2019-07-02T23:19:56Z'>
		&lt;denchmark-link:https://github.com/feihugis&gt;@feihugis&lt;/denchmark-link&gt;
 Thanks!
		</comment>
		<comment id='13' author='devstein' date='2019-07-03T10:08:06Z'>
		&lt;denchmark-link:https://github.com/devstein&gt;@devstein&lt;/denchmark-link&gt;
 Were you able to resolve the issue? Thanks!
		</comment>
		<comment id='14' author='devstein' date='2019-07-03T21:00:56Z'>
		&lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
 I am working on the fix. The issue can be reproduced by the simplified code below:
&lt;denchmark-code&gt;  csv_dataset = tf.data.Dataset.range(0, 1000, 1)
  cached_dataset = csv_dataset.cache("cache_test")
  x_dataset = cached_dataset.map(lambda x: x)
  y_dataset = cached_dataset.map(lambda x: x * 2)
  xy_dataset = tf.data.Dataset.zip((x_dataset, y_dataset))

  for x, y in xy_dataset:
    print(x, y)
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/devstein&gt;@devstein&lt;/denchmark-link&gt;
 The workaround solution is to change  to be  which will use the memory cache instead of file cache.
		</comment>
		<comment id='15' author='devstein' date='2019-07-03T21:40:28Z'>
		This behavior is expected because your input pipeline definition is trying to create the same cache twice (once for each component of the zip) and it will not be reused.
Instead, you should do the following (which will also be more efficient):
&lt;denchmark-code&gt;def map_file_to_dataset(filename, params):
    # Generate a dataset from the filename
    dataset = tf.data.experimental.CsvDataset(...)

    return dataset.cache(...).map(lambda *x: (x[0:-1], x[-1]))
&lt;/denchmark-code&gt;

		</comment>
		<comment id='16' author='devstein' date='2019-07-03T21:40:30Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=30105&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=30105&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='17' author='devstein' date='2019-07-03T22:13:58Z'>
		&lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
 I separated the two operations because I window the  afterward and not the , but I'll switch it to split the datasets after I window. Appreciate the help!
		</comment>
	</comments>
</bug>