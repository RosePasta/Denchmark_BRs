<bug id='10519' author='ddtm' open_date='2017-06-08T03:26:09Z' closed_time='2017-06-22T00:34:51Z'>
	<summary>tf.contrib.data: tf-slim training pipeline gets stuck</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;



Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes


OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux leto28 3.16.0-4-amd64 1 SMP Debian 3.16.39-1+deb8u2 (2017-03-07) x86_64 GNU/Linux
VERSION_ID="8"
VERSION="8 (jessie)"


TensorFlow installed from (source or binary):
Binary


TensorFlow version (use command below):
tf.VERSION = 1.2.0-rc2
tf.GIT_VERSION = v1.2.0-rc1-24-gce1d6ec
tf.COMPILER_VERSION = v1.2.0-rc1-24-gce1d6ec


Bazel version (if compiling from source):
None


CUDA/cuDNN version:
8.0/5.1


GPU model and memory:
TITAN X (Pascal), 12189MiB


Exact command to reproduce:
python ./mwe.py


&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

I recently ported my dataset handling to the new dataset API from . Now it seems that the  training pipeline stalls if I request just 1 or 2 CPUs for my job (it used to work just fine with the dataset API provided by ). I does work if I grab 4 CPUs. I tried to come up with a MWE (see below). The interesting thing is that it is not getting stuck if I remove one of the s or  at line 39. I suspect this issue is related to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/10369&gt;#10369&lt;/denchmark-link&gt;
.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

import os
import tensorflow as tf
import tensorflow.contrib.data as tcd
import tensorflow.contrib.slim as slim

from tensorflow.contrib.data.python.ops.dataset_ops import _get_file_names

DATASET_DIR = '/path_to_the_dataset'
FILE_PATTERN = 'shapes_{}_*.tfrecord'
IMAGE_SHAPE = [48, 48, 3]


def _parse_function(example_proto):
    features = {
        "image/encoded": tf.FixedLenFeature(
            (), tf.string, default_value=""),
        'image/annotation/color': tf.FixedLenFeature(
            (), tf.int64, default_value=0),
        'image/annotation/shape': tf.FixedLenFeature(
            (), tf.int64, default_value=0),
    }
    parsed_features = tf.parse_single_example(example_proto, features)
    image_decoded = tf.image.decode_image(parsed_features["image/encoded"])
    color = parsed_features['image/annotation/color']
    shape = parsed_features['image/annotation/shape']

    return image_decoded, color, shape


def get_batch(batch_size=32, group_size=3, split_name='train'):
    file_pattern = os.path.join(
        DATASET_DIR, FILE_PATTERN.format(split_name))

    file_names = _get_file_names(file_pattern, randomize_input=True)

    dataset = tcd.TFRecordDataset(file_names)
    dataset = dataset.map(_parse_function)

    dataset = dataset.map(lambda image, color, shape: image)
    dataset = dataset.shuffle(buffer_size=10000)
    dataset = dataset.repeat().batch(group_size * batch_size)

    iterator = dataset.make_one_shot_iterator()
    images = iterator.get_next()

    images = tf.split(images, group_size, axis=0)
    images = [tf.reshape(x, [batch_size] + IMAGE_SHAPE) for x in images]

    return images


if __name__ == "__main__":
    with tf.Graph().as_default():
        x_1, x_2, x_3 = get_batch(batch_size=32,
                                  group_size=3)

        val = tf.reduce_sum(tf.add_n([x_1, x_2, x_3]))
        val = tf.Print(val, [tf.constant(0)], "I'm alive! ")

        global_step = slim.get_or_create_global_step()
        with tf.control_dependencies([val]):
            update_global_step_op = tf.assign_add(global_step, 1)

        train_op = update_global_step_op

        tf.summary.scalar('Summary 1', val)
        tf.summary.scalar('Summary 2', train_op)

        logdir = 'mwe_logdir'
        slim.learning.train(
            train_op=train_op,
            logdir=logdir,
            number_of_steps=1000000)
	</description>
	<comments>
		<comment id='1' author='ddtm' date='2017-06-08T05:00:43Z'>
		Thanks for reporting this... it definitely looks like a bug. I think I've tracked it down to the "OneShotIterator" op, which internally blocks on this line while a function executes to build the dataset:



tensorflow/tensorflow/core/kernels/iterator_ops.cc


         Line 248
      in
      91cb809






 n.WaitForNotification(); 





That will block one of the inter-op thread pool threads for the (typically short) execution time of the dataset construction function. The number of CPUs determines the default number of threads in that thread pool: when you have only 1 CPU, the system will deadlock as soon as you hit that line (because no more thread pool threads are available to run the function that will unblock it). When you have 2 CPUs, it can work, but slim.learning.train() uses tf.train.Supervisor, which asynchronously runs a background thread... that runs the same "OneShotIterator" op. To ensure that the op only initializes once, the initialization runs under a lock, acquired here:



tensorflow/tensorflow/core/kernels/iterator_ops.cc


         Line 194
      in
      91cb809






 mutex_lock l(mu_); 





The problem is probably starting to become clear: two concurrent executions of the same "OneShotIterator" kernel will potentially block two inter-op thread pool threads, leading to deadlock in a 2-CPU system, because there are no more threads available to run the function that will unblock them.
Anyway, mea culpa, and thanks again for finding the bug. I'll be working on a fix, although it might not make it into the final 1.2 release. In the mean time, there are a couple of workarounds:


Increase the number of threads to more than 2 in the inter-op thread pool. You can do this by passing session_config=tf.ConfigProto(inter_op_parallelism_threads=3) to slim.learning.train().


Use dataset.make_initializable_iterator() instead of dataset.make_one_shot_iterator(). This comes with the additional requirement that you have to run iterator.initializer, which is not completely trivial with slim.learning.train() because you don't have access to the tf.Session. One possibility is to pass local_init_op=tf.group(tf.local_variables_initializer(), tf.tables_initializer(), iterator.initializer) to slim.learning.train().


		</comment>
		<comment id='2' author='ddtm' date='2017-06-08T05:31:29Z'>
		Wow, thanks for the swift response! The first workaround seems to be working perfectly.
On a side note, I'm quite happy with the tf.contrib.data. My code has become way cleaner.
		</comment>
	</comments>
</bug>