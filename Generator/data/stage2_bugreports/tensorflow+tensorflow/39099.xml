<bug id='39099' author='srihari-humbarwadi' open_date='2020-05-02T12:08:30Z' closed_time='2020-05-05T19:56:37Z'>
	<summary>[Regression] Cloud TPU crashes with  UnavailableError: failed to connect to all addresses</summary>
	<description>

TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): tf-nightly/ 2.2.0-dev20200502
Python version: 3.7
TPU model : TPU v3-8 with tf-nightly

Describe the current behavior
I have been using TPU v3-8's with tf-nightly without any issues, however I started  receiving this after upgrading the VM and TPU v3 from 2.2.0-dev20200429 to latest tf-nightly ie. 2.2.0.dev20200502.
Edit [1] : The notebook runs fine on colab with 2.2.0-rc3
Edit [2] :  Not fixed in 2.2.0.dev20200504 aswell
I cannot post my codebase here since it is pretty big, and since the official guide for tpu also triggers the same error, I have attached it here.

&lt;denchmark-link:https://gist.github.com/srihari-humbarwadi/c9bd4b2dec4666252a8d6319ddd87060&gt;notebook to reproduce the bug&lt;/denchmark-link&gt;

Other info / logs
&lt;denchmark-code&gt;UnavailableError                          Traceback (most recent call last)
&lt;ipython-input-10-8b804771207f&gt; in &lt;module&gt;
      1 model.fit(train_dataset,
      2           epochs=5,
----&gt; 3           validation_data=test_dataset)

~/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
     70   def _method_wrapper(self, *args, **kwargs):
     71     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
---&gt; 72       return method(self, *args, **kwargs)
     73 
     74     # Running inside `run_distribute_coordinator` already.

~/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
    893       data_handler._initial_epoch = (  # pylint: disable=protected-access
    894           self._maybe_load_initial_epoch_from_ckpt(initial_epoch))
--&gt; 895       for epoch, iterator in data_handler.enumerate_epochs():
    896         self.reset_metrics()
    897         callbacks.on_epoch_begin(epoch)

~/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py in enumerate_epochs(self)
   1153     """Yields `(epoch, tf.data.Iterator)`."""
   1154     with self._truncate_execution_to_epoch():
-&gt; 1155       data_iterator = iter(self._dataset)
   1156       for epoch in range(self._initial_epoch, self._epochs):
   1157         if self._insufficient_data:  # Set by `catch_stop_iteration`.

~/tf/lib/python3.7/site-packages/tensorflow/python/distribute/input_lib.py in __iter__(self)
    706     worker_iterators = _create_iterators_per_worker(self._cloned_datasets,
    707                                                     self._input_workers,
--&gt; 708                                                     enable_legacy_iterators)
    709     if enable_legacy_iterators:
    710       iterator = DistributedIteratorV1(self._input_workers, worker_iterators,

~/tf/lib/python3.7/site-packages/tensorflow/python/distribute/input_lib.py in _create_iterators_per_worker(worker_datasets, input_workers, enable_legacy_iterators)
   1368       if tf2.enabled() and not enable_legacy_iterators:
   1369         iterator = _SingleWorkerOwnedDatasetIterator(worker_datasets[i], worker,
-&gt; 1370                                                      worker_devices)
   1371       else:
   1372         iterator = _SingleWorkerDatasetIterator(worker_datasets[i], worker,

~/tf/lib/python3.7/site-packages/tensorflow/python/distribute/input_lib.py in __init__(self, dataset, worker, devices, components, element_spec)
   1225         raise ValueError(error_message)
   1226       super(_SingleWorkerOwnedDatasetIterator, self).__init__(dataset, worker,
-&gt; 1227                                                               devices)
   1228 
   1229   def _make_iterator(self):

~/tf/lib/python3.7/site-packages/tensorflow/python/distribute/input_lib.py in __init__(self, dataset, worker, devices)
   1071     self._devices = devices
   1072     self._element_spec = dataset.element_spec
-&gt; 1073     self._make_iterator()
   1074 
   1075   def _make_iterator(self):

~/tf/lib/python3.7/site-packages/tensorflow/python/distribute/input_lib.py in _make_iterator(self)
   1235     with ops.device(self._worker):
   1236       self._iterator = multi_device_iterator_ops.OwnedMultiDeviceIterator(
-&gt; 1237           self._dataset, self._devices, source_device=host_device)
   1238 
   1239   @property

~/tf/lib/python3.7/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py in __init__(self, dataset, devices, max_buffer_size, prefetch_buffer_size, source_device, components, element_spec)
    560                                       incarnation_id, prefetch_buffer_size,
    561                                       experimental_slack)
--&gt; 562           iterator = iter(ds)
    563           self._device_iterators.append(iterator)
    564           iterator_handles.append(iterator._iterator_resource)  # pylint: disable=protected-access

~/tf/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py in __iter__(self)
    407     """
    408     if context.executing_eagerly() or ops.inside_function():
--&gt; 409       return iterator_ops.OwnedIterator(self)
    410     else:
    411       raise RuntimeError("__iter__() is only supported inside of tf.function "

~/tf/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py in __init__(self, dataset, components, element_spec, job_token)
    602           context.context().device_spec.device_type != "CPU"):
    603         with ops.device("/cpu:0"):
--&gt; 604           self._create_iterator(dataset)
    605       else:
    606         self._create_iterator(dataset)

~/tf/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py in _create_iterator(self, dataset)
    628               output_shapes=self._flat_output_shapes))
    629       if self._job_token is None:
--&gt; 630         gen_dataset_ops.make_iterator(ds_variant, self._iterator_resource)
    631       else:
    632         gen_experimental_dataset_ops.make_data_service_iterator(

~/tf/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py in make_iterator(dataset, iterator, name)
   2950       return _result
   2951     except _core._NotOkStatusException as e:
-&gt; 2952       _ops.raise_from_not_ok_status(e, name)
   2953     except _core._FallbackException:
   2954       pass

~/tf/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   6808   message = e.message + (" name: " + name if name is not None else "")
   6809   # pylint: disable=protected-access
-&gt; 6810   six.raise_from(core._status_to_exception(e.code, message), None)
   6811   # pylint: enable=protected-access
   6812 

~/tf/lib/python3.7/site-packages/six.py in raise_from(value, from_value)

UnavailableError: failed to connect to all addresses
Additional GRPC error information:
{"created":"@1588419621.643701261","description":"Failed to pick subchannel","file":"third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc","file_line":3937,"referenced_errors":[{"created":"@1588419621.215578031","description":"failed to connect to all addresses","file":"third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc","file_line":394,"grpc_status":14}]} [Op:MakeIterator]
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/jhseu&gt;@jhseu&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='srihari-humbarwadi' date='2020-05-05T09:55:41Z'>
		I can confirm that this bug has started to occur in the past few days on tf-nightly both in our setup and also when using the example &lt;denchmark-link:https://github.com/tensorflow/models/blob/master/official/nlp/bert/run_classifier.py&gt;run_classifier.py&lt;/denchmark-link&gt;
 script. We have since then moved to TF2.1 (which unfortunately is lacking a few features compared to tf-nightly).
Happy to give additional information to reproduce the bug.
		</comment>
		<comment id='2' author='srihari-humbarwadi' date='2020-05-05T10:17:22Z'>
		
I can confirm that this bug has started to occur in the past few days on tf-nightly both in our setup and also when using the example run_classifier.py script. We have since then moved to TF2.1 (which unfortunately is lacking a few features compared to tf-nightly).
Happy to give additional information to reproduce the bug.

Please do! I'm facing a similar issue but my codebase is too big to publish here.
		</comment>
		<comment id='3' author='srihari-humbarwadi' date='2020-05-05T18:37:20Z'>
		Hi All, the issues should have been fixed in the latest TPU nightly. Can you retry? Thanks!
		</comment>
		<comment id='4' author='srihari-humbarwadi' date='2020-05-05T19:20:28Z'>
		&lt;denchmark-link:https://github.com/rxsang&gt;@rxsang&lt;/denchmark-link&gt;
 confirmed working on 
		</comment>
		<comment id='5' author='srihari-humbarwadi' date='2020-05-05T19:29:00Z'>
		Thanks &lt;denchmark-link:https://github.com/rxsang&gt;@rxsang&lt;/denchmark-link&gt;
! Yep confirmed to be working!
		</comment>
		<comment id='6' author='srihari-humbarwadi' date='2020-05-05T19:56:37Z'>
		Thanks closing this!
		</comment>
		<comment id='7' author='srihari-humbarwadi' date='2020-05-05T19:56:40Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39099&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39099&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='srihari-humbarwadi' date='2020-12-23T01:49:50Z'>
		This bug is still there. I have tried many versions and combinations. When I create dataset using from_tensor_slices, it works fine (but only on small datasets. after that protobuf limitations come into play). But when I try from_generator, or when I'm trying to prepare dataset on th fly (by using tokenization inside of map), it's throwing "failed to connect to all addresses". I think TF team needs to do something with it.
		</comment>
		<comment id='9' author='srihari-humbarwadi' date='2020-12-23T01:59:39Z'>
		Hi fingoldo@,
Sorry for the issue. Dataset.from_generator is expected to not work with TPUs as it uses py_function underneath which is incompatible with Cloud TPU 2VM setup. If you would like to read from large datasets, maybe try to materialize it on disk and use TFRecordDataest instead.
		</comment>
	</comments>
</bug>