<bug id='5987' author='kmalakoff' open_date='2016-11-30T17:19:04Z' closed_time='2018-02-07T23:59:00Z'>
	<summary>Request for documentation on recommended flow in slim for train, validation, and test sets</summary>
	<description>
The examples in the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim&gt;slim README.md&lt;/denchmark-link&gt;
 give basic documentation for training and evaluating models when used separately; however, there is guidance missing on how to do the classic cycle of mini-batch gradient descent using shuffled subsets of the training set, periodically evaluating validation set, and then evaluating on the test set post-training.
Using the &lt;denchmark-link:https://www.tensorflow.org/versions/r0.12/tutorials/mnist/tf/index.html&gt;MNIST tutorial&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/mnuke/tf-slim-mnist&gt;this tutorial&lt;/denchmark-link&gt;
 for reference, the best I came up with was something like this where I'm effectively monkey-patching the train_step_fn to periodically output accuracies:
&lt;denchmark-code&gt;from tensorflow.contrib.slim.python.slim.learning import train_step

graph = tf.Graph()
with graph.as_default():
  image, label = input('train', FLAGS.dataset_dir)
  images, labels = tf.train.shuffle_batch([image, label], batch_size=FLAGS.batch_size, capacity=1000 + 3 * FLAGS.batch_size, min_after_dequeue=1000)
  images_validation, labels_validation = inputs('validation', FLAGS.dataset_dir, 5000)
  images_test, labels_test = inputs('test', FLAGS.dataset_dir, 10000)
 
  with tf.variable_scope("model") as scope:
    predictions = model(images, FLAGS)
    scope.reuse_variables()
    predictions_validation = model(images_validation, FLAGS)
    predictions_test = model(images_test, FLAGS)
    
  slim.losses.softmax_cross_entropy(predictions, labels)
  optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)
  train_op = slim.learning.create_train_op(slim.losses.get_total_loss(), optimizer)

  accuracy_validation = slim.metrics.accuracy(tf.to_int32(tf.argmax(predictions_validation, 1)), tf.to_int32(tf.argmax(labels_validation, 1)))
  accuracy_test = slim.metrics.accuracy(tf.to_int32(tf.argmax(predictions_test, 1)), tf.to_int32(tf.argmax(labels_test, 1)))
    
def train_step_fn(session, *args, **kwargs):
  total_loss, should_stop = train_step(session, *args, **kwargs)

  if train_step_fn.step % FLAGS.validation_check == 0:
    accuracy = session.run(train_step_fn.accuracy_validation)
    print('Step %s - Loss: %.2f Accuracy: %.2f%%' % (str(train_step_fn.step).rjust(6, '0'), total_loss, accuracy * 100))

  if train_step_fn.step == (FLAGS.max_steps - 1):
    accuracy = session.run(accuracy_test)
    print('%s - Loss: %.2f Accuracy: %.2f%%' % ('FINAL TEST', total_loss, accuracy * 100))
    
  train_step_fn.step += 1
  return [total_loss, should_stop]

train_step_fn.step = 0
train_step_fn.accuracy_validation = accuracy_validation

slim.learning.train(
  train_op,
  FLAGS.logs_dir,
  train_step_fn=train_step_fn,
  graph=graph,
  number_of_steps=FLAGS.max_steps
)
&lt;/denchmark-code&gt;

Note: one problem with this implementation is that the final test set is not guaranteed to be run in the case of early exit.
I've posted in the slack channel and Googled around, but haven't been able to find any examples for this basic use case. Accordingly, I would like to propose that an example providing the best practice to periodically evaluate batch trained models using the validate set and the trained model against the test set to be added to the slim README.md.
I think it would really help the community to have a clearer idea on the intentions of the slim team on how the batch training and evaluation paths were designed to be used together during and after training.
	</description>
	<comments>
		<comment id='1' author='kmalakoff' date='2016-12-14T18:59:25Z'>
		&lt;denchmark-link:https://github.com/nathansilberman&gt;@nathansilberman&lt;/denchmark-link&gt;
 any suggestion?
		</comment>
		<comment id='2' author='kmalakoff' date='2016-12-26T11:31:49Z'>
		+1
&lt;denchmark-link:https://github.com/kmalakoff&gt;@kmalakoff&lt;/denchmark-link&gt;
 don't you need to  your batch to placeholders?
		</comment>
		<comment id='3' author='kmalakoff' date='2017-01-03T22:55:45Z'>
		+1
		</comment>
		<comment id='4' author='kmalakoff' date='2017-01-25T13:27:56Z'>
		+1
		</comment>
		<comment id='5' author='kmalakoff' date='2017-02-16T03:20:51Z'>
		Come on guys, the critical use case of training + validating is still — even on r1.0 — completely non-obvious to implement using the slim "lego blocks". We need guidance here, if you please.
		</comment>
		<comment id='6' author='kmalakoff' date='2017-03-07T20:49:28Z'>
		Same here ... I am having trouble with that too!
		</comment>
		<comment id='7' author='kmalakoff' date='2017-03-08T18:27:58Z'>
		Looking for inputs too!
		</comment>
		<comment id='8' author='kmalakoff' date='2017-03-21T23:06:07Z'>
		+1
		</comment>
		<comment id='9' author='kmalakoff' date='2017-04-04T10:23:33Z'>
		+1
		</comment>
		<comment id='10' author='kmalakoff' date='2017-04-11T18:32:20Z'>
		+1
		</comment>
		<comment id='11' author='kmalakoff' date='2017-04-11T21:28:08Z'>
		We have found that running eval on validation dataset in a different process works better.
In most cases, the validation dataset requires several batches, and takes time that could be used to make progress on training.
So the recommended way is to save checkpoints at regular intervals, then use slim.evaluation_loop() to keep evaluating checkpoints while training.
Finally after training, one would run eval on test set.
		</comment>
		<comment id='12' author='kmalakoff' date='2017-04-28T16:45:19Z'>
		&lt;denchmark-link:https://github.com/sguada&gt;@sguada&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/nathansilberman&gt;@nathansilberman&lt;/denchmark-link&gt;
. Has there any update on this? Is there a good way to periodically verify accuracy on training &amp; validation set DURING training instead of two separate processes? Thanks.
		</comment>
		<comment id='13' author='kmalakoff' date='2017-04-29T11:04:47Z'>
		+1
		</comment>
		<comment id='14' author='kmalakoff' date='2017-05-01T15:12:02Z'>
		&lt;denchmark-link:https://github.com/sguada&gt;@sguada&lt;/denchmark-link&gt;

Thanks for your suggestion. I agree that validating in a separate process sounds like a better approach.
There are a number of things that are known during training time: what data file is used, what samples are used for training (as opposed to reserved for validation), what kink was introduced in the code to perform a certain test....etc. These important pieces of information  need to be passed to the other process reliably.
Since it seems to be the standard use case, would it be possible to have, in the repo, an example of such mechanics? slim seems incomplete if it provides many awesome ways to train a network easily, but no easy way to monitor/validate its progress.
Thanks again.
		</comment>
		<comment id='15' author='kmalakoff' date='2017-05-04T00:11:27Z'>
		&lt;denchmark-link:https://github.com/ybsave&gt;@ybsave&lt;/denchmark-link&gt;
 what &lt;denchmark-link:https://github.com/kmalakoff&gt;@kmalakoff&lt;/denchmark-link&gt;
 wrote is a way to compute evaluation metrics while training.
As I mentioned above, that has some problems, like slowing down the training, but could be used for early stopping.
For examples on how to train and evaluate in two different process see:
&lt;denchmark-link:https://github.com/tensorflow/models/blob/master/slim/train_image_classifier.py&gt;https://github.com/tensorflow/models/blob/master/slim/train_image_classifier.py&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/models/blob/master/slim/eval_image_classifier.py&gt;https://github.com/tensorflow/models/blob/master/slim/eval_image_classifier.py&lt;/denchmark-link&gt;

I typically just run train_image_classifier in one process and eval_image_classifier in another.
		</comment>
		<comment id='16' author='kmalakoff' date='2017-05-27T06:50:48Z'>
		In the meantime I've been using the (pre-trained) classification models (vgg, resnet) created by slim, with a regular tensorflow train+val loop. This works well.
When creating the model, for resnet use something like
is_training = tf.Variable(True, name='is_training')
with slim.arg_scope(resnet_v1.resnet_arg_scope(is_training=is_training)):
    net = resnet_v1.resnet_v1_101
    y_est, _ = net(images_in, num_classes=nclasses)
...
At the training part in each epoch run:
sess.run(is_training.assign(True))
and at the validation:
sess.run(is_training.assign(False))
		</comment>
		<comment id='17' author='kmalakoff' date='2017-05-27T15:35:32Z'>
		Since there's no single official procedure, I created a couple of scripts to perform the training/validation in different processes (which we used to participate in a machine learning challenge). If you wanna take a look, they're here: &lt;denchmark-link:https://github.com/learningtitans/isbi2017-part3#deep-learning-component-model-rc30&gt;https://github.com/learningtitans/isbi2017-part3#deep-learning-component-model-rc30&lt;/denchmark-link&gt;
 — the most relevant code is a shell script in &lt;denchmark-link:https://github.com/learningtitans/isbi2017-part3/blob/master/etc/launch_validation_loop.sh&gt;./etc/launch_validation_loop.sh&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='18' author='kmalakoff' date='2017-06-05T02:33:00Z'>
		Seems Slim saves checkpoints at a fixed time interval defined in the save_interval_secs. This seems cause issue if we want to synchronization this with validation set evaluation process. Plus saving checkpoints at fixed time interval (instead of training steps) seems introduce an extra set of uncertainty. I believe override the supervisor can change this behavior but it is kind of messy. Is there a simpler way to do this?
		</comment>
		<comment id='19' author='kmalakoff' date='2017-06-05T02:38:01Z'>
		You can dig into the supervisor and change it. I did the same and customized it a little bit for myself.
		</comment>
		<comment id='20' author='kmalakoff' date='2017-07-18T20:53:01Z'>
		I would like to have an option in slim to be able to periodically run evaluation in the same process as training. Running in separate processes has some downsides for me when training on a single multi-gpu system: 1. A GPU is dedicated to running eval. Training would be faster if I could use all GPUs as "clones" for multi-gpu training, even accounting for doing periodic evaluation. Running eval on the CPU slows down training because eval uses a lot of CPU and then bottlenecks training on image decoding and augmentation. 2. My training process uses most of the system RAM. I can't run eval at the same time without heavy swapping.
		</comment>
		<comment id='21' author='kmalakoff' date='2017-08-28T07:38:52Z'>
		Doesn't  TFSlim update anymore? It's really boring. Convenient setting for validation while trainning is really in need.
		</comment>
		<comment id='22' author='kmalakoff' date='2017-09-12T08:53:27Z'>
		I found the following section really helpful: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim#evaluation-loop&gt;https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim#evaluation-loop&lt;/denchmark-link&gt;

I initially thought the main slim page was:
&lt;denchmark-link:https://github.com/tensorflow/models/tree/master/slim&gt;https://github.com/tensorflow/models/tree/master/slim&lt;/denchmark-link&gt;

I created a PR to make this more explicit here:
&lt;denchmark-link:https://github.com/tensorflow/models/pull/2371&gt;tensorflow/models#2371&lt;/denchmark-link&gt;

		</comment>
		<comment id='23' author='kmalakoff' date='2017-12-22T07:40:43Z'>
		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
		</comment>
		<comment id='24' author='kmalakoff' date='2018-01-05T19:10:04Z'>
		Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='25' author='kmalakoff' date='2018-01-24T13:22:41Z'>
		Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='26' author='kmalakoff' date='2018-02-07T23:58:56Z'>
		Please reopen if &lt;denchmark-link:https://github.com/tensorflow/models/pull/2371&gt;tensorflow/models#2371&lt;/denchmark-link&gt;
 hasn't addressed it.
		</comment>
		<comment id='27' author='kmalakoff' date='2018-02-17T14:56:53Z'>
		As explained above, the solution proposed in &lt;denchmark-link:https://github.com/tensorflow/models/pull/2371&gt;tensorflow/models#2371&lt;/denchmark-link&gt;
 does not address the problem. We are looking for a canonical way to interleave training and evaluation, not for a way to perform evaluation as a separate process. For those of us with few GPUs (or just one!) the current pattern just doesn't work.
		</comment>
		<comment id='28' author='kmalakoff' date='2018-02-27T01:26:46Z'>
		&lt;denchmark-link:https://github.com/liavassif&gt;@liavassif&lt;/denchmark-link&gt;


At the training part in each epoch run:
sess.run(is_training.assign(True))
and at the validation:
sess.run(is_training.assign(False))

this seems like an interesting approach. can you give a more complete example? I'm not sure how to use feed_dict to feed the different train/val inputs into the same model and reuse weights
		</comment>
		<comment id='29' author='kmalakoff' date='2018-03-01T15:14:08Z'>
		&lt;denchmark-link:https://github.com/mixuala&gt;@mixuala&lt;/denchmark-link&gt;

After using sess.run() to set the is_training variable, the rest is just the usual mini-batch approach (at train or at validation).
&lt;denchmark-code&gt;        # is_train should be True when training and False for validation
        sess.run(is_training.assign(is_train))
        for batchid in range(batchN):
            if is_train:
                  cur_batch = get_batch_train(batchid)
            else:
                  cur_batch = get_batch_test(batchid)
            run_results = sess.run(ops_train, feed_dict=cur_batch)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='30' author='kmalakoff' date='2018-03-02T03:46:52Z'>
		&lt;denchmark-link:https://github.com/liavassif&gt;@liavassif&lt;/denchmark-link&gt;
 I keep getting  but I am trying to run this inside  in  . maybe that is not the right way to go...
		</comment>
		<comment id='31' author='kmalakoff' date='2018-03-03T06:54:52Z'>
		&lt;denchmark-link:https://github.com/mixuala&gt;@mixuala&lt;/denchmark-link&gt;
 this approach should be used instead of slim.learning.train(). You should just run the for loop in the example in your main code in order to perform training/val. Similarly as done in the regular Tensorflow MNIST/CIFAR tutorials (well, at least in the old tutorials, now they are using something called an Estimator).
		</comment>
	</comments>
</bug>