<bug id='43271' author='jaltmayerpizzorno' open_date='2020-09-16T17:20:41Z' closed_time='2020-11-04T18:56:07Z'>
	<summary>TFLite converter aborts (dumps core) in BroadcastAdd4DSlow</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Pop! OS 20.04 LTS
TensorFlow installed from (source or binary): binary
TensorFlow version (or github SHA if from source): 2.3.0


If possible, please share a link to Colab/Jupyter/any notebook.
&lt;denchmark-link:https://colab.research.google.com/drive/1rfdr8bu_UfTzpGCXH-tQFVHMIKDreXad?usp=sharing&gt;https://colab.research.google.com/drive/1rfdr8bu_UfTzpGCXH-tQFVHMIKDreXad?usp=sharing&lt;/denchmark-link&gt;

When running on my Pop! OS system under gdb, I get the backtrace:
&lt;denchmark-code&gt;gdb python
[...]
(gdb) run tflite.py
[...]
(gdb) where
#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:50
#1  0x00007ffff7ddb859 in __GI_abort () at abort.c:79
#2  0x00007ffbf82ccb1f in tflite::reference_ops::BroadcastAdd4DSlow(tflite::ArithmeticParams const&amp;, tflite::RuntimeShape const&amp;, float const*, tflite::RuntimeShape const&amp;, float const*, tflite::RuntimeShape const&amp;, float*) ()
   from /home/juan/.local/lib/python3.8/site-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so
#3  0x00007ffbf82d54ec in void tflite::ops::builtin::add::EvalAdd&lt;(tflite::ops::builtin::add::KernelType)2&gt;(TfLiteContext*, TfLiteNode*, TfLiteAddParams*, tflite::ops::builtin::add::OpData const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor*) ()
   from /home/juan/.local/lib/python3.8/site-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so
#4  0x00007ffbf82d846d in TfLiteStatus tflite::ops::builtin::add::Eval&lt;(tflite::ops::builtin::add::KernelType)2&gt;(TfLiteContext*, TfLiteNode*) () from /home/juan/.local/lib/python3.8/site-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so
#5  0x00007ffbf82ab1bf in tflite::optimize::calibration::(anonymous namespace)::LoggingEval(TfLiteContext*, TfLiteNode*) ()
   from /home/juan/.local/lib/python3.8/site-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so
#6  0x00007ffbf850f20b in tflite::impl::Subgraph::Invoke() ()
   from /home/juan/.local/lib/python3.8/site-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so
#7  0x00007ffbf85121c0 in tflite::impl::Interpreter::Invoke() ()
   from /home/juan/.local/lib/python3.8/site-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so
#8  0x00007ffbf827a581 in tflite::calibration_wrapper::CalibrationWrapper::FeedTensor(_object*) ()
   from /home/juan/.local/lib/python3.8/site-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so
#9  0x00007ffbf8274285 in pybind11::cpp_function::initialize&lt;pybind11_init__pywrap_tensorflow_lite_calibration_wrapper(pybind11::module&amp;)::{lambda(tflite::calibration_wrapper::CalibrationWrapper&amp;, pybind11::handle&amp;)#4}, pybind11::object, tflite::calibration_wrapper::CalibrationWrapper&amp;, pybind11::handle&amp;, pybind11::name, pybind11::is_method, pybind11::sibling&gt;(pybind11_init__pywrap_tensorflow_lite_calibration_wrapper(pybind11::module&amp;)::{lambda(tflite::calibration_wrapper::CalibrationWrapper&amp;, pybind11::handle&amp;)#4}&amp;&amp;, pybind11::object (*)(tflite::calibration_wrapper::CalibrationWrapper&amp;, pybind11::handle&amp;), pybind11::name const&amp;, pybind11::is_method const&amp;, pybind11::sibling const&amp;)::{lambda(pybind11::detail::function_call&amp;)#3}::_FUN(pybind11::detail::function_call) ()
   from /home/juan/.local/lib/python3.8/site-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so
#10 0x00007ffbf8271fe7 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) ()
   from /home/juan/.local/lib/python3.8/site-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so
#11 0x00000000005f17e5 in PyCFunction_Call ()
#12 0x00000000005f2406 in _PyObject_MakeTpCall ()
#13 0x000000000050795f in ?? ()
#14 0x000000000056c475 in _PyEval_EvalFrameDefault ()
#15 0x0000000000565972 in _PyEval_EvalCodeWithName ()
#16 0x00000000005f1d85 in _PyFunction_Vectorcall ()
#17 0x00000000005677c7 in _PyEval_EvalFrameDefault ()
#18 0x0000000000565972 in _PyEval_EvalCodeWithName ()
#19 0x00000000005f1d85 in _PyFunction_Vectorcall ()
#20 0x0000000000507729 in ?? ()
#21 0x00000000005f1107 in PyObject_Call ()
#22 0x0000000000568e1f in _PyEval_EvalFrameDefault ()
#23 0x000000000050712e in ?? ()
#24 0x000000000056c475 in _PyEval_EvalFrameDefault ()
#25 0x0000000000565972 in _PyEval_EvalCodeWithName ()
#26 0x00000000005f1d85 in _PyFunction_Vectorcall ()
--Type &lt;RET&gt; for more, q to quit, c to continue without paging--
#27 0x00000000005677c7 in _PyEval_EvalFrameDefault ()
#28 0x0000000000565972 in _PyEval_EvalCodeWithName ()
#29 0x0000000000686053 in PyEval_EvalCode ()
#30 0x00000000006753d1 in ?? ()
#31 0x000000000067544f in ?? ()
#32 0x0000000000675507 in PyRun_FileExFlags ()
#33 0x000000000067758a in PyRun_SimpleFileExFlags ()
#34 0x00000000006ae99e in Py_RunMain ()
#35 0x00000000006aed29 in Py_BytesMain ()
#36 0x00007ffff7ddd0b3 in __libc_start_main (main=0x4ebd20 &lt;main&gt;, argc=2, argv=0x7fffffffe2c8, init=&lt;optimized out&gt;, 
    fini=&lt;optimized out&gt;, rtld_fini=&lt;optimized out&gt;, stack_end=0x7fffffffe2b8) at ../csu/libc-start.c:308
#37 0x00000000005f62ee in _start ()
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='jaltmayerpizzorno' date='2020-09-16T21:17:04Z'>
		&lt;denchmark-link:https://github.com/jaltmayerpizzorno&gt;@jaltmayerpizzorno&lt;/denchmark-link&gt;
 Could you try the tf-nightly version instead?
		</comment>
		<comment id='2' author='jaltmayerpizzorno' date='2020-09-16T22:46:49Z'>
		
@jaltmayerpizzorno Could you try the tf-nightly version instead?

Hi &lt;denchmark-link:https://github.com/abattery&gt;@abattery&lt;/denchmark-link&gt;
, I did earlier, with the same result, but will try again and report back.
		</comment>
		<comment id='3' author='jaltmayerpizzorno' date='2020-09-16T23:35:18Z'>
		&lt;denchmark-link:https://github.com/abattery&gt;@abattery&lt;/denchmark-link&gt;
 It crashed on colab again with .  It also does so using the  (also ) docker image.  On the latter, gdb has the backtrace at:
&lt;denchmark-code&gt;#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51
#1  0x00007fe09246f8b1 in __GI_abort () at abort.c:79
#2  0x00007fdf60088eff in tflite::reference_ops::BroadcastAdd4DSlow(tflite::ArithmeticParams const&amp;, tflite::RuntimeShape const&amp;, float const*, tflite::RuntimeShape const&amp;, float const*, tflite::RuntimeShape const&amp;, float*) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so
#3  0x00007fdf6009136c in void tflite::ops::builtin::add::EvalAdd&lt;(tflite::ops::builtin::add::KernelType)2&gt;(TfLiteContext*, TfLiteNode*, TfLiteAddParams*, tflite::ops::builtin::add::OpData const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor*) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so
#4  0x00007fdf600946d9 in TfLiteStatus tflite::ops::builtin::add::Eval&lt;(tflite::ops::builtin::add::KernelType)2&gt;(TfLiteContext*, TfLiteNode*) () from /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so
#5  0x00007fdf600676bf in tflite::optimize::calibration::(anonymous namespace)::LoggingEval(TfLiteContext*, TfLiteNode*) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so
#6  0x00007fdf60301e6b in tflite::impl::Subgraph::Invoke() ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so
#7  0x00007fdf603049b0 in tflite::impl::Interpreter::Invoke() ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so
#8  0x00007fdf600365c1 in tflite::calibration_wrapper::CalibrationWrapper::FeedTensor(_object*) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so
#9  0x00007fdf6002d5a5 in void pybind11::cpp_function::initialize&lt;pybind11_init__pywrap_tensorflow_lite_calibration_wrapper(pybind11::module&amp;)::{lambda(tflite::calibration_wrapper::CalibrationWrapper&amp;, pybind11::handle&amp;)#4}, pybind11::object, tflite::calibration_wrapper::CalibrationWrapper&amp;, pybind11::handle&amp;, pybind11::name, pybind11::is_method, pybind11::sibling&gt;(pybind11_init__pywrap_tensorflow_lite_calibration_wrapper(pybind11::module&amp;)::{lambda(tflite::calibration_wrapper::CalibrationWrapper&amp;, pybind11::handle&amp;)#4}&amp;&amp;, pybind11::object (*)(tflite::calibration_wrapper::CalibrationWrapper&amp;, pybind11::handle&amp;), pybind11::name const&amp;, pybind11::is_method const&amp;, pybind11::sibling const&amp;)::{lambda(pybind11::detail::function_call&amp;)#3}::_FUN(pybind11::detail::function_call) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so
#10 0x00007fdf6002e279 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='jaltmayerpizzorno' date='2020-09-16T23:38:18Z'>
		Before crashing, it also logs the following... Just passing on in case it matters.
&lt;denchmark-code&gt;see current operation: %174 = "tfl.concatenation"(%173, %128) {axis = 3 : i32, fused_activation_function = "NONE"} : (tensor&lt;?x?x?x256xf32&gt;, tensor&lt;?x38x38x512xf32&gt;) -&gt; tensor&lt;?x38x38x768xf32&gt;
2020-09-16 23:31:50.934601: W tensorflow/compiler/mlir/lite/tf_to_tfl_flatbuffer.cc:138] see current operation: %174 = "tfl.concatenation"(%173, %128) {axis = 3 : i32, fused_activation_function = "NONE"} : (tensor&lt;?x?x?x256xf32&gt;, tensor&lt;?x38x38x512xf32&gt;) -&gt; tensor&lt;?x38x38x768xf32&gt;

see current operation: %193 = "tfl.concatenation"(%192, %77) {axis = 3 : i32, fused_activation_function = "NONE"} : (tensor&lt;?x?x?x128xf32&gt;, tensor&lt;?x76x76x256xf32&gt;) -&gt; tensor&lt;?x76x76x384xf32&gt;
2020-09-16 23:31:50.934633: W tensorflow/compiler/mlir/lite/tf_to_tfl_flatbuffer.cc:138] see current operation: %193 = "tfl.concatenation"(%192, %77) {axis = 3 : i32, fused_activation_function = "NONE"} : (tensor&lt;?x?x?x128xf32&gt;, tensor&lt;?x76x76x256xf32&gt;) -&gt; tensor&lt;?x76x76x384xf32&gt;

see current operation: %216 = "tfl.concatenation"(%212, %213, %215) {axis = 4 : i32, fused_activation_function = "NONE"} : (tensor&lt;?x76x76x3x2xf32&gt;, tensor&lt;?x76x76x3x2xf32&gt;, tensor&lt;?x76x76x3x81xf32&gt;) -&gt; tensor&lt;?x76x76x3x85xf32&gt;
2020-09-16 23:31:50.934644: W tensorflow/compiler/mlir/lite/tf_to_tfl_flatbuffer.cc:138] see current operation: %216 = "tfl.concatenation"(%212, %213, %215) {axis = 4 : i32, fused_activation_function = "NONE"} : (tensor&lt;?x76x76x3x2xf32&gt;, tensor&lt;?x76x76x3x2xf32&gt;, tensor&lt;?x76x76x3x81xf32&gt;) -&gt; tensor&lt;?x76x76x3x85xf32&gt;

see current operation: %231 = "tfl.concatenation"(%219, %222, %226, %230, %215) {axis = 4 : i32, fused_activation_function = "NONE"} : (tensor&lt;?x76x76x3x1xf32&gt;, tensor&lt;?x76x76x3x1xf32&gt;, tensor&lt;?x76x76x3x1xf32&gt;, tensor&lt;?x76x76x3x1xf32&gt;, tensor&lt;?x76x76x3x81xf32&gt;) -&gt; tensor&lt;?x76x76x3x85xf32&gt;
2020-09-16 23:31:50.934655: W tensorflow/compiler/mlir/lite/tf_to_tfl_flatbuffer.cc:138] see current operation: %231 = "tfl.concatenation"(%219, %222, %226, %230, %215) {axis = 4 : i32, fused_activation_function = "NONE"} : (tensor&lt;?x76x76x3x1xf32&gt;, tensor&lt;?x76x76x3x1xf32&gt;, tensor&lt;?x76x76x3x1xf32&gt;, tensor&lt;?x76x76x3x1xf32&gt;, tensor&lt;?x76x76x3x81xf32&gt;) -&gt; tensor&lt;?x76x76x3x85xf32&gt;

see current operation: %242 = "tfl.concatenation"(%238, %239, %241) {axis = 4 : i32, fused_activation_function = "NONE"} : (tensor&lt;?x38x38x3x2xf32&gt;, tensor&lt;?x38x38x3x2xf32&gt;, tensor&lt;?x38x38x3x81xf32&gt;) -&gt; tensor&lt;?x38x38x3x85xf32&gt;
2020-09-16 23:31:50.934666: W tensorflow/compiler/mlir/lite/tf_to_tfl_flatbuffer.cc:138] see current operation: %242 = "tfl.concatenation"(%238, %239, %241) {axis = 4 : i32, fused_activation_function = "NONE"} : (tensor&lt;?x38x38x3x2xf32&gt;, tensor&lt;?x38x38x3x2xf32&gt;, tensor&lt;?x38x38x3x81xf32&gt;) -&gt; tensor&lt;?x38x38x3x85xf32&gt;

see current operation: %257 = "tfl.concatenation"(%245, %248, %252, %256, %241) {axis = 4 : i32, fused_activation_function = "NONE"} : (tensor&lt;?x38x38x3x1xf32&gt;, tensor&lt;?x38x38x3x1xf32&gt;, tensor&lt;?x38x38x3x1xf32&gt;, tensor&lt;?x38x38x3x1xf32&gt;, tensor&lt;?x38x38x3x81xf32&gt;) -&gt; tensor&lt;?x38x38x3x85xf32&gt;
2020-09-16 23:31:50.934677: W tensorflow/compiler/mlir/lite/tf_to_tfl_flatbuffer.cc:138] see current operation: %257 = "tfl.concatenation"(%245, %248, %252, %256, %241) {axis = 4 : i32, fused_activation_function = "NONE"} : (tensor&lt;?x38x38x3x1xf32&gt;, tensor&lt;?x38x38x3x1xf32&gt;, tensor&lt;?x38x38x3x1xf32&gt;, tensor&lt;?x38x38x3x1xf32&gt;, tensor&lt;?x38x38x3x81xf32&gt;) -&gt; tensor&lt;?x38x38x3x85xf32&gt;

see current operation: %268 = "tfl.concatenation"(%264, %265, %267) {axis = 4 : i32, fused_activation_function = "NONE"} : (tensor&lt;?x19x19x3x2xf32&gt;, tensor&lt;?x19x19x3x2xf32&gt;, tensor&lt;?x19x19x3x81xf32&gt;) -&gt; tensor&lt;?x19x19x3x85xf32&gt;
2020-09-16 23:31:50.934705: W tensorflow/compiler/mlir/lite/tf_to_tfl_flatbuffer.cc:138] see current operation: %268 = "tfl.concatenation"(%264, %265, %267) {axis = 4 : i32, fused_activation_function = "NONE"} : (tensor&lt;?x19x19x3x2xf32&gt;, tensor&lt;?x19x19x3x2xf32&gt;, tensor&lt;?x19x19x3x81xf32&gt;) -&gt; tensor&lt;?x19x19x3x85xf32&gt;

see current operation: %283 = "tfl.concatenation"(%271, %274, %278, %282, %267) {axis = 4 : i32, fused_activation_function = "NONE"} : (tensor&lt;?x19x19x3x1xf32&gt;, tensor&lt;?x19x19x3x1xf32&gt;, tensor&lt;?x19x19x3x1xf32&gt;, tensor&lt;?x19x19x3x1xf32&gt;, tensor&lt;?x19x19x3x81xf32&gt;) -&gt; tensor&lt;?x19x19x3x85xf32&gt;
2020-09-16 23:31:50.934717: W tensorflow/compiler/mlir/lite/tf_to_tfl_flatbuffer.cc:138] see current operation: %283 = "tfl.concatenation"(%271, %274, %278, %282, %267) {axis = 4 : i32, fused_activation_function = "NONE"} : (tensor&lt;?x19x19x3x1xf32&gt;, tensor&lt;?x19x19x3x1xf32&gt;, tensor&lt;?x19x19x3x1xf32&gt;, tensor&lt;?x19x19x3x1xf32&gt;, tensor&lt;?x19x19x3x81xf32&gt;) -&gt; tensor&lt;?x19x19x3x85xf32&gt;
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='jaltmayerpizzorno' date='2020-09-30T18:23:36Z'>
		I just confirmed that the issue is still there with TF 2.3.1.
		</comment>
		<comment id='6' author='jaltmayerpizzorno' date='2020-10-19T21:44:01Z'>
		&lt;denchmark-link:https://github.com/abattery&gt;@abattery&lt;/denchmark-link&gt;
 Not sure of this change pertains to Keras/RNN model conversion. Here is a &lt;denchmark-link:https://colab.research.google.com/drive/1lQsF1DH7Ozv2WXYVm0t4HaPfkCAE_G3Z?usp=sharing&gt;colab&lt;/denchmark-link&gt;
 to help you debug.
		</comment>
		<comment id='7' author='jaltmayerpizzorno' date='2020-10-20T05:13:00Z'>
		This issue is related to partial 5D support on both Add and Mul operators. I have working fixes for this problem. I will update the thread when they will arrive.
		</comment>
		<comment id='8' author='jaltmayerpizzorno' date='2020-11-04T04:51:22Z'>
		The fix has merged into master. Could you try again with the tomorrow's nightly version?
		</comment>
		<comment id='9' author='jaltmayerpizzorno' date='2020-11-04T14:59:52Z'>
		Sure, will do.
		</comment>
		<comment id='10' author='jaltmayerpizzorno' date='2020-11-04T18:56:06Z'>
		I re-ran the code on the colab I posted earlier in this ticket; the crash is gone!  Nice! :)
I am getting errors about operations being neither a custom op nor a flex op that I haven't attempted to solve yet, but I assume they are due to what's in my model and not a sign of a bug (well, at least not this bug).
		</comment>
		<comment id='11' author='jaltmayerpizzorno' date='2020-11-04T18:56:08Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43271&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43271&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>