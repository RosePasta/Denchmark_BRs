<bug id='41394' author='patrickyu1111' open_date='2020-07-14T23:42:55Z' closed_time='2020-07-30T20:22:59Z'>
	<summary>InvalidArgumentError: Operation 'TensorListPushBack_489' has no attr named '_XlaCompile'</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script
provided in TensorFlow):
Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
AWS EC2 instance with image of Deep Learning Base AMI (Amazon Linux)
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
happens on a mobile device:
N/A
TensorFlow installed from (source or binary):
Source
TensorFlow version (use command below):
2.2.0
Python version:
3.6.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
cuda = V10.0.130; cudnn =  CUDNN_MAJOR 7; CUDNN_MINOR 5; CUDNN_PATCHLEVEL 1
GPU model and memory:
Tesla T4; 8G
Exact command to reproduce:

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

I am trying to implement a custom model following a paper. This model itself should not matter too much. The problem I encountered is that the code got stuck in  y_pred = self(x, training = True) in the train_step function in the Dynamic_Loss class. It did not return an error. After running for an hour, I interrupted the kernel and it returns the error message saying "InvalidArgumentError: Operation 'TensorListPushBack_489' has no attr named '_XlaCompile'" and "InvalidArgumentError: Operation 'While' has no attr named '_XlaCompile'". I am not sure what the issue is because the code worked before I set persistent = True in tf.Gradient_Tape
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

class Tem_Agg:

	"""
	This class is to aggregate the representations in the temporal dimension using
	an autoencoder architecture
	"""

	## the length of the time stamp
	max_length = 300

	def __init__(self,  length, timestamp, model = "baseline"):


		## the number of timestamp in the batch
		self.timestamp = timestamp
		## the dimension of the vectors to be aggregated
		self.vec_length = length
		## the mode to use: baseline model or dynamic model
		assert model in ["baseline", "dynamic"]
		self.model = model


	def build_model(self):

		"""
		This is an autoecoder model where the input is used to reconstruct the input itself
		and predict the future

		for reference: https://arxiv.org/pdf/1502.04681.pdf
		"""

		## the input layer

		inp = Input(shape = (self.timestamp - 5, self.vec_length))

		if self.model == "dynamic":

			weights = Input(shape = (2, ))

			## mean
			mean_weights = Dense(inp.shape[1], activation = "relu")(weights)
			mean_weights = tf.expand_dims(mean_weights, axis = -1)
			## standard deviation
			std_weights = Dense(inp.shape[1], activation = "relu")(weights)
			std_weights = tf.expand_dims(std_weights, axis = -1)

		## encoder

		enc_first = LSTM(self.vec_length // 2, return_sequences = True)(inp)

		if self.model == "dynamic":
			enc_first = Tem_Agg.FiLM(enc_first, mean_weights, std_weights)


		enc_second = LSTM(self.vec_length // 4, return_sequences = True)(enc_first)


		if self.model == "dynamic":
			enc_second = Tem_Agg.FiLM(enc_second, mean_weights, std_weights)

		## the full representation has the same dimension as the input so that
		## the hierarchy model can use it in different levels;
		enc_second_full = LSTM(self.vec_length, name = "representation")(enc_first)


		## decoder for reconstruction

		dec_first = LSTM(self.vec_length // 2, return_sequences = True)(enc_second)
		recon = LSTM(inp.shape[-1], return_sequences = True)(dec_first)

		## predict the future vectors
		predicted_vec_1 = Dense(inp.shape[-1], activation = "relu")(enc_second_full)
		predicted_vec_2 = Dense(inp.shape[-1], activation = "relu")(predicted_vec_1)
		predicted_vec_3 = Dense(inp.shape[-1], activation = "relu")(predicted_vec_2)
		predicted_vec_4 = Dense(inp.shape[-1], activation = "relu")(predicted_vec_3)
		predicted_vec_5 = Dense(inp.shape[-1], activation = "relu")(predicted_vec_4)


		if self.model == "baseline":
			model = keras.Model(inputs = inp, outputs = [recon, predicted_vec_5])
			model.compile("adam", loss = MeanAbsolutePercentageError())
		else:
		
			model = Dynamic_Loss(inputs = [inp, weights], outputs = [recon, predicted_vec_5, weights])
			

			model.compile("adam")

		return model


	@staticmethod
	def FiLM(tensor, mean_weights, std_weights):
		"""
		This is the Feature-wise Linear Modulation (FiLM) operation from the paper

		Inputs:
			tensor: the tensor from the layer
			mean_weight: weights to be multiplied to the tensor
			std_weights: weights to be added to product of tensor and mean_weight

		Output:
			output: the output tensor after the operation

		"""

		tensor = Multiply()([tensor, mean_weights])
		output = Add()([tensor, std_weights])
		return output


class Dynamic_Loss(keras.Model):

	"""
	This is the class to learn the distribution of the loss for the multi-tasks
	model. We will pass the weight vector in the training and inference stages

	for reference: https://openreview.net/pdf?id=HyxY6JHKwr

	"""

	def train_step(self, data):

		"""
		Overwrite the training loop

		Input:
			data: the data we feed into the fit() function; a tf Dataset object
		"""

		## unpacking the data
		x, y = data

		## unpacking y
		recon, future = y

		## loss
		mape = MeanAbsolutePercentageError()

		trainable_vars = self.trainable_variables


		with tf.GradientTape(persistent = True) as tape_1:

			## the predicted values
			y_pred = self(x, training = True)
			

			recon_pred, future_pred, weights = y_pred
		

			## the two different losses
			recon_loss = mape(recon, recon_pred)
			#recon_gradients = tape_1.gradient(recon_loss, trainable_vars)



			future_loss = mape(future, future_pred)
			

		
		recon_gradients = tape_1.gradient(recon_loss, trainable_vars)
		
		future_gradients = tape_1.gradient(future_loss, trainable_vars)




		## the gradient of the sum is the sum of the gradient
		gradients = weights.numpy()[0] * recon_gradients + weights.numpy()[1] * future_gradients

		## applying gradients
		self.optimizer.apply_gradients(zip(gradients, trainable_vars))
	</description>
	<comments>
		<comment id='1' author='patrickyu1111' date='2020-07-15T14:15:45Z'>
		&lt;denchmark-link:https://github.com/patrickyu1111&gt;@patrickyu1111&lt;/denchmark-link&gt;
,
Issues &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/38524#issuecomment-646133140&gt;#38524&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/34211#issuecomment-599779838&gt;#34211&lt;/denchmark-link&gt;
 with similar error log were fixed in TF v2.3.
Could you please try installing TF v2.3.0rc1 or TF-nightly and check if you are facing the same issue. Thanks!
		</comment>
		<comment id='2' author='patrickyu1111' date='2020-07-15T18:51:35Z'>
		I tried both versions. But the issue is still here
		</comment>
		<comment id='3' author='patrickyu1111' date='2020-07-15T19:49:39Z'>
		
@patrickyu1111,
Issues #38524 and #34211 with similar error log were fixed in TF v2.3.
Could you please try installing TF v2.3.0rc1 or TF-nightly and check if you are facing the same issue. Thanks!

I tried both versions. But the issue is still here.
		</comment>
		<comment id='4' author='patrickyu1111' date='2020-07-16T19:04:50Z'>
		&lt;denchmark-link:https://github.com/patrickyu1111&gt;@patrickyu1111&lt;/denchmark-link&gt;
  From the code snippet you provided I cannot reproduce the behavior you reported. Can you please help us with minimal code reproducer? Thanks!
		</comment>
		<comment id='5' author='patrickyu1111' date='2020-07-23T19:31:54Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='6' author='patrickyu1111' date='2020-07-30T20:22:57Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='7' author='patrickyu1111' date='2020-07-30T20:23:00Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41394&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41394&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='patrickyu1111' date='2020-08-15T11:22:49Z'>
		&lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/patrickyu1111&gt;@patrickyu1111&lt;/denchmark-link&gt;
 Hi, is this issue solved? If yes, can you please provide more information about the solution? I am facing a similar issue
		</comment>
	</comments>
</bug>