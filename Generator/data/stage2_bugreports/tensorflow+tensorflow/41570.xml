<bug id='41570' author='awang27' open_date='2020-07-20T22:33:54Z' closed_time='2020-07-21T06:10:55Z'>
	<summary>TFLite interpreter.invoke() crashes on GPU despite successful TFLite conversion</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 (Google Colab notebook)
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): pip
TensorFlow version (use command below): 2.4.0-dev20200720
Python version: 3.6.9
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: 10.1/7 (Google Colab default GPU)
GPU model and memory: Google Colab default GPU


I converted a simple 3DCNN keras model to TFLite. Upon creating an interpreter from that TFLite model and calling , the Google Colab notebook crashes. This only happens when using a GPU runtime;  works fine on a CPU. The converted TFLite model uses both  and , and I was converting to TFLite in an attempt to apply quantization-aware training and post-training quantization according to &lt;denchmark-link:https://www.tensorflow.org/model_optimization/guide/quantization/training_comprehensive_guide&gt;this guide&lt;/denchmark-link&gt;
.
Describe the expected behavior
I expect to be able to call interpreter.invoke() successfully on a GPU without any crashing.
Standalone code to reproduce the issue

Add this Google Colab notebook and this dataset to your Google Drive.
Open the colab notebook. Set the runtime to GPU (Runtime -&gt; Change Runtime Type -&gt; GPU).
Mount your drive and change "/content/drive/My Drive/full_dataset_vectors.h5" to wherever you're storing the dataset.
Run all cells of the notebook. The actual crash only happens in the last cell, with interpreter.invoke().

Other info / logs
The logs in the Google Colab notebook didn't provide any information about the cause for the crash. To get more info, I tried running the code on an identical local environment and managed to obtain a gdb backtrace. It seems likely to be related to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/35987&gt;this issue&lt;/denchmark-link&gt;
, but the backtrace looks different enough to possibly be a separate bug, so I decided to create a new issue.
&lt;denchmark-code&gt;Thread 1 "python3" received signal SIGSEGV, Segmentation fault.
__memmove_sse2_unaligned_erms () at ../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S:370
370    ../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S: No such file or directory.
(gdb) bt
#0  __memmove_sse2_unaligned_erms () at ../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S:370
#1  0x00007ff291abd70c in tflite::FlexDelegate::CopyFromBufferHandle(TfLiteContext*, int, TfLiteTensor*) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00007ff280226f1d in tflite::impl::Subgraph::Invoke() ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so
#3  0x00007ff280229bd0 in tflite::impl::Interpreter::Invoke() ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so
#4  0x00007ff27ffc8a58 in tflite::interpreter_wrapper::InterpreterWrapper::Invoke() ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so
#5  0x00007ff27ffbe6be in void pybind11::cpp_function::initialize&lt;pybind11_init__pywrap_tensorflow_interpreter_wrapper(pybind11::module&amp;)::{lambda(tflite::interpreter_wrapper::InterpreterWrapper&amp;)#4}, pybind11::object, tflite::interpreter_wrapper::InterpreterWrapper&amp;, pybind11::name, pybind11::is_method, pybind11::sibling&gt;(pybind11_init__pywrap_tensorflow_interpreter_wrapper(pybind11::module&amp;)::{lambda(tflite::interpreter_wrapper::InterpreterWrapper&amp;)#4}&amp;&amp;, pybind11::object (*)(tflite::interpreter_wrapper::InterpreterWrapper&amp;), pybind11::name const&amp;, pybind11::is_method const&amp;, pybind11::sibling const&amp;)::{lambda(pybind11::detail::function_call&amp;)#3}::_FUN(pybind11::detail::function_call)
    () from /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so
#6  0x00007ff27ffbfc19 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) ()
   from /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so
#7  0x00000000005669ac in _PyCFunction_FastCallDict () at ../Objects/methodobject.c:231
#8  0x000000000050a5c3 in call_function.lto_priv () at ../Python/ceval.c:4875
#9  0x000000000050bfb4 in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3335
#10 0x0000000000509758 in PyEval_EvalFrameEx (throwflag=0, 
    f=Frame 0x7ff180002e48, for file /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py, line 524, in invoke (self=&lt;Interpreter(_custom_op_registerers=[], _interpreter=&lt;tensorflow.lite.python.interpreter_wrapper._pywrap_tensorflow_interpreter_wrapper.InterpreterWrapper at remote 0x7ff2779fd110&gt;, _delegates=[]) at remote 0x7ff2779fd710&gt;)) at ../Python/ceval.c:754
#11 _PyFunction_FastCall (globals=&lt;optimized out&gt;, nargs=140675211341384, args=&lt;optimized out&gt;, co=&lt;optimized out&gt;) at ../Python/ceval.c:4933
#12 fast_function.lto_priv () at ../Python/ceval.c:4968
#13 0x000000000050a48d in call_function.lto_priv () at ../Python/ceval.c:4872
#14 0x000000000050bfb4 in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3335
#15 0x0000000000509758 in PyEval_EvalFrameEx (throwflag=0, 
    f=Frame 0x7ff1b4001df8, for file inference.py, line 80, in tflite_inference (interpreter=&lt;Interpreter(_custom_op_registerers=[], _interpreter=&lt;tensorflow.lite.python.interpreter_wrapper._pywrap_tensorflow_interpreter_wrapper.InterpreterWrapper at remote 0x7ff2779fd110&gt;, _delegates=[]) at remote 0x7ff2779fd710&gt;, test_data=[&lt;tensorflow.python.framework.ops.EagerTensor at remote 0x7ff26c0f4938&gt;, &lt;tensorflow.python.framework.ops.EagerTensor at remote 0x7ff26c0f47d8&gt;, &lt;tensorflow.python.framework.ops.EagerTensor at remote 0x7ff26c0f4678&gt;, &lt;tensorflow.python.framework.ops.EagerTensor at remote 0x7ff26c0f4888&gt;, &lt;tensorflow.python.framework.ops.EagerTensor at remote 0x7ff26c0f4e08&gt;, &lt;tensorflow.python.framework.ops.EagerTensor at remote 0x7ff26c0f4bf8&gt;, &lt;tensorflow.python.framework.ops.EagerTensor at remote 0x7ff26c0f4a98&gt;, &lt;tensorflow.python.framework.ops.EagerTensor at remote 0x7ff26c0f4b48&gt;, &lt;tensorflow.python.framework.ops.EagerTensor at remote 0x7ff21c0ad048&gt;, &lt;tensorflow.python.framework.ops.EagerTens...(truncated)) at ../Python/ceval.c:754
#16 _PyFunction_FastCall (globals=&lt;optimized out&gt;, nargs=140676083752440, args=&lt;optimized out&gt;, co=&lt;optimized out&gt;) at ../Python/ceval.c:4933
#17 fast_function.lto_priv () at ../Python/ceval.c:4968
#18 0x000000000050a48d in call_function.lto_priv () at ../Python/ceval.c:4872
#19 0x000000000050bfb4 in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3335
#20 0x0000000000509758 in PyEval_EvalFrameEx (throwflag=0, 
    f=Frame 0x543b138, for file inference.py, line 120, in run_inference at ../Python/ceval.c:754
#21 _PyFunction_FastCall (globals=&lt;optimized out&gt;, nargs=88322360, args=&lt;optimized out&gt;, co=&lt;optimized out&gt;) at ../Python/ceval.c:4933
#22 fast_function.lto_priv () at ../Python/ceval.c:4968
#23 0x000000000050a48d in call_function.lto_priv () at ../Python/ceval.c:4872
#24 0x000000000050bfb4 in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3335
#25 0x0000000000507d64 in PyEval_EvalFrameEx (throwflag=0, f=Frame 0x18c6bd8, for file inference.py, line 150, in &lt;module&gt; ()) at ../Python/ceval.c:754
---Type &lt;return&gt; to continue, or q &lt;return&gt; to quit---
#26 _PyEval_EvalCodeWithName.lto_priv.1820 () at ../Python/ceval.c:4166
#27 0x000000000050ae13 in PyEval_EvalCodeEx (closure=0x0, kwdefs=0x0, defcount=0, defs=0x0, kwcount=0, kws=0x0, argcount=0, args=0x0, locals=&lt;optimized out&gt;, 
    globals=&lt;optimized out&gt;, _co=&lt;optimized out&gt;) at ../Python/ceval.c:4187
#28 PyEval_EvalCode (co=&lt;optimized out&gt;, globals=&lt;optimized out&gt;, locals=&lt;optimized out&gt;) at ../Python/ceval.c:731
#29 0x0000000000634c82 in run_mod () at ../Python/pythonrun.c:1025
#30 0x0000000000634d37 in PyRun_FileExFlags () at ../Python/pythonrun.c:978
#31 0x00000000006384ef in PyRun_SimpleFileExFlags () at ../Python/pythonrun.c:419
#32 0x00000000006386c5 in PyRun_AnyFileExFlags () at ../Python/pythonrun.c:81
#33 0x0000000000639091 in run_file (p_cf=0x7ffece85e63c, filename=&lt;optimized out&gt;, fp=&lt;optimized out&gt;) at ../Modules/main.c:340
#34 Py_Main () at ../Modules/main.c:810
#35 0x00000000004b0d00 in main (argc=2, argv=0x7ffece85e838) at ../Programs/python.c:69
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='awang27' date='2020-07-21T06:10:55Z'>
		FYI, TFLite's GPU supports is different with Tensorflow. It requires Mobile GPU which is available for Android and iOS.
&lt;denchmark-link:https://www.tensorflow.org/lite/performance/gpu&gt;https://www.tensorflow.org/lite/performance/gpu&lt;/denchmark-link&gt;

Your GPU runtime is compatible with Tensorflow not Tensorflow Lite. So please use CPU runtime.
		</comment>
		<comment id='2' author='awang27' date='2020-07-21T06:10:57Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41570&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41570&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='awang27' date='2020-11-02T14:16:30Z'>
		I have the problem with CPU and GPU. My model is a converted mask rcnn in tensorflow2 but my session crashes when I invoke the interpreter. The input is coherent with input details so no problem there.
The following message is from GPU:
&lt;denchmark-code&gt;Timestamp | Level | Message
-- | -- | --
Nov 2, 2020, 3:08:48 PM | WARNING | pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5
Nov 2, 2020, 3:04:57 PM | WARNING | pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5
Nov 2, 2020, 3:02:14 PM | WARNING | pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5
Nov 2, 2020, 3:02:07 PM | WARNING | pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5
Nov 2, 2020, 3:01:00 PM | WARNING | pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5
Nov 2, 2020, 3:00:59 PM | WARNING | pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5
Nov 2, 2020, 2:59:12 PM | INFO | http://172.28.0.2:9000/
Nov 2, 2020, 2:59:12 PM | INFO | google.colab serverextension initialized.
Nov 2, 2020, 3:08:48 PM | WARNING | coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s
Nov 2, 2020, 3:04:57 PM | WARNING | coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s
Nov 2, 2020, 3:02:14 PM | WARNING | coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s
Nov 2, 2020, 3:02:07 PM | WARNING | coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s
Nov 2, 2020, 3:01:00 PM | WARNING | coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s
Nov 2, 2020, 3:00:59 PM | WARNING | coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s
Nov 2, 2020, 2:59:12 PM | INFO | Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret
Nov 2, 2020, 3:10:23 PM | WARNING | WARNING:root:kernel 8e9b15e5-7c66-4b1e-bcbe-1d36d13f7e36 restarted
Nov 2, 2020, 3:05:41 PM | WARNING | WARNING:root:kernel 8e9b15e5-7c66-4b1e-bcbe-1d36d13f7e36 restarted
Nov 2, 2020, 2:59:12 PM | INFO | Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
Nov 2, 2020, 2:59:12 PM | INFO | The Jupyter Notebook is running at:
Nov 2, 2020, 2:59:12 PM | INFO | Serving notebooks from local directory: /
Nov 2, 2020, 3:10:23 PM | INFO | KernelRestarter: restarting kernel (1/5), keep random ports
Nov 2, 2020, 3:05:41 PM | INFO | KernelRestarter: restarting kernel (1/5), keep random ports
Nov 2, 2020, 2:59:56 PM | INFO | Kernel started: 8e9b15e5-7c66-4b1e-bcbe-1d36d13f7e36
Nov 2, 2020, 3:08:49 PM | WARNING | INFO: TfLiteFlexDelegate delegate: 12 nodes delegated out of 586 nodes with 4 partitions.
Nov 2, 2020, 3:04:57 PM | WARNING | INFO: TfLiteFlexDelegate delegate: 12 nodes delegated out of 586 nodes with 4 partitions.


&lt;/denchmark-code&gt;

The kernet seems to restart for unkown reason.
Also, the input detail contain 3 tensors beside the image :
&lt;denchmark-code&gt;[{'dtype': numpy.float32,
  'index': 0,
  'name': 'input_image',
  'quantization': (0.0, 0),
  'quantization_parameters': {'quantized_dimension': 0,
   'scales': array([], dtype=float32),
   'zero_points': array([], dtype=int32)},
  'shape': array([   1, 1024, 1024,    3], dtype=int32),
  'shape_signature': array([  -1, 1024, 1024,    3], dtype=int32),
  'sparsity_parameters': {}},
 {'dtype': numpy.float32,
  'index': 1,
  'name': 'input_image_meta',
  'quantization': (0.0, 0),
  'quantization_parameters': {'quantized_dimension': 0,
   'scales': array([], dtype=float32),
   'zero_points': array([], dtype=int32)},
  'shape': array([ 1, 93], dtype=int32),
  'shape_signature': array([-1, 93], dtype=int32),
  'sparsity_parameters': {}},
 {'dtype': numpy.float32,
  'index': 2,
  'name': 'input_anchors',
  'quantization': (0.0, 0),
  'quantization_parameters': {'quantized_dimension': 0,
   'scales': array([], dtype=float32),
   'zero_points': array([], dtype=int32)},
  'shape': array([     1, 261888,      4], dtype=int32),
  'shape_signature': array([    -1, 261888,      4], dtype=int32),
  'sparsity_parameters': {}}]
&lt;/denchmark-code&gt;

However the original model needs only one tensor. I tried to give all three inputs (image of size (1,1024,1024,3) and 2 zeros tensors with appropriate size but no success.
I need your help on this 👍
		</comment>
		<comment id='4' author='awang27' date='2020-11-02T15:01:54Z'>
		Update
In fact for a model with multiple inputs, you have to specify each one of them at a time like this:
&lt;denchmark-code&gt;interpreter.set_tensor(input_details[0]['index'], image)
interpreter.set_tensor(input_details[1]['index'], A)
interpreter.set_tensor(input_details[2]['index'], B)
&lt;/denchmark-code&gt;

But when I invoke I session crashes again.
run on CPU :
&lt;denchmark-code&gt;
Nov 2, 2020, 3:56:23 PM | WARNING | WARNING:root:kernel 6841efe9-d913-4be1-9de2-c068b02ca452 restarted
-- | -- | --
Nov 2, 2020, 3:56:23 PM | INFO | KernelRestarter: restarting kernel (1/5), keep random ports
Nov 2, 2020, 3:52:45 PM | WARNING | INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 25 nodes with 0 partitions.
Nov 2, 2020, 3:52:45 PM | WARNING | INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.
Nov 2, 2020, 3:52:45 PM | WARNING | INFO: TfLiteFlexDelegate delegate: 12 nodes delegated out of 586 nodes with 4 partitions.
Nov 2, 2020, 3:47:25 PM | WARNING | INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 25 nodes with 0 partitions.
Nov 2, 2020, 3:47:25 PM | WARNING | INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.
Nov 2, 2020, 3:47:25 PM | WARNING | INFO: TfLiteFlexDelegate delegate: 12 nodes delegated out of 586 nodes with 4 partitions.
Nov 2, 2020, 3:46:45 PM | INFO | Adapting to protocol v5.1 for kernel 6841efe9-d913-4be1-9de2-c068b02ca452
Nov 2, 2020, 3:40:47 PM | WARNING | TfLiteFlexDelegate delegate: 0 nodes delegated out of 25 nodes with 0 partitions.
Nov 2, 2020, 3:40:47 PM | WARNING | INFO:
Nov 2, 2020, 3:40:47 PM | WARNING | INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.
Nov 2, 2020, 3:40:47 PM | WARNING | INFO: TfLiteFlexDelegate delegate: 12 nodes delegated out of 586 nodes with 4 partitions.
Nov 2, 2020, 3:38:49 PM | WARNING | 2020-11-02 14:38:49.894074: I tensorflow/lite/tools/optimize/quantize_weights.cc:203] Skipping quantization of tensor arg8 that is not type float.
Nov 2, 2020, 3:38:49 PM | WARNING | 2020-11-02 14:38:49.894064: I tensorflow/lite/tools/optimize/quantize_weights.cc:203] Skipping quantization of tensor mask_rcnn/mrcnn_detection/map/while/strided_slice that is not type float.
Nov 2, 2020, 3:38:49 PM | WARNING | 2020-11-02 14:38:49.894053: I tensorflow/lite/tools/optimize/quantize_weights.cc:211] Skipping quantization of tensor arg7 because it has fewer than 1024 elements (1).
Nov 2, 2020, 3:38:49 PM | WARNING | 2020-11-02 14:38:49.894036: I tensorflow/lite/tools/optimize/quantize_weights.cc:211] Skipping quantization of tensor arg6 because it has fewer than 1024 elements (4).
Nov 2, 2020, 3:38:49 PM | WARNING | 2020-11-02 14:38:49.893950: I tensorflow/lite/tools/optimize/quantize_weights.cc:203] Skipping quantization of tensor arg4 that is not type float.
Nov 2, 2020, 3:38:49 PM | WARNING | 2020-11-02 14:38:49.430175: I tensorflow/lite/tools/optimize/quantize_weights.cc:220] Skipping quantization of tensor mask_rcnn/roi_align_mask/concat because it has no allocated buffer.
Nov 2, 2020, 3:38:49 PM | WARNING | 2020-11-02 14:38:49.430166: I tensorflow/lite/tools/optimize/quantize_weights.cc:203] Skipping quantization of tensor mask_rcnn/roi_align_mask/strided_slice_182 that is not type float.
Nov 2, 2020, 3:38:49 PM | WARNING | 2020-11-02 14:38:49.430151: I tensorflow/lite/tools/optimize/quantize_weights.cc:203] Skipping quantization of tensor mask_rcnn/mrcnn_detection/ArgMax that is not type float.
Nov 2, 2020, 3:38:49 PM | WARNING | 2020-11-02 14:38:49.430142: I tensorflow/lite/tools/optimize/quantize_weights.cc:220] Skipping quantization of tensor mask_rcnn/mrcnn_detection/clipped_boxes because it has no allocated buffer.
Nov 2, 2020, 3:38:49 PM | WARNING | 2020-11-02 14:38:49.430133: I tensorflow/lite/tools/optimize/quantize_weights.cc:211] Skipping quantization of tensor mask_rcnn/mrcnn_detection/GatherNd because it has fewer than 1024 elements (1000).
Nov 2, 2020, 3:38:49 PM | WARNING | 2020-11-02 14:38:49.430124: I tensorflow/lite/tools/optimize/quantize_weights.cc:203] Skipping quantization of tensor mask_rcnn/mrcnn_detection/strided_slice_28 that is not type float.
Nov 2, 2020, 3:38:49 PM | WARNING | 2020-11-02 14:38:49.430114: I tensorflow/lite/tools/optimize/quantize_weights.cc:211] Skipping quantization of tensor mask_rcnn/mrcnn_detection/GatherNd because it has fewer than 1024 elements (1000).


&lt;/denchmark-code&gt;

Your help is much appreciated :D
		</comment>
	</comments>
</bug>