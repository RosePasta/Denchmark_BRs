<bug id='32075' author='rishabhsahrawat' open_date='2019-08-29T07:29:24Z' closed_time='2019-09-09T20:40:13Z'>
	<summary>[TF2.0rc]TFRecord file size bigger than original csv file consisting text data.</summary>
	<description>
I am trying transform a csv file consisting two columns: short_description, label. Description consists sentences from length 1 to 4 and label is just an integer number. The csv file has a size of 740 MBs however on converting it to TFRecord the size of the new file increased to 1.2GB, which I think is not correct since TFRecord files are supposed to have lesser size.
For  (column 1), I am using BytesList and for  (coulmn2) I am using Int64List as was shown here in this &lt;denchmark-link:https://www.tensorflow.org/beta/tutorials/load_data/tf_records&gt;tutorial&lt;/denchmark-link&gt;
. Just to be clear, the data read from the new TFRecord file is fine i.e. no problem. It is just the size of the file. Now I do not understand what the problem is. Please help. Thank you!
	</description>
	<comments>
		<comment id='1' author='rishabhsahrawat' date='2019-08-30T06:14:41Z'>
		Request you to provide standalone code and sample data to reproduce the issue. Thanks!
		</comment>
		<comment id='2' author='rishabhsahrawat' date='2019-08-30T07:05:09Z'>
		Thank you &lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
 . Here is the &lt;denchmark-link:https://drive.google.com/file/d/15bd5zrp-2wSsmHmi9u2SdYJITQYQ2eXv/view?usp=sharing&gt;link&lt;/denchmark-link&gt;
 to the small part of big csv file (428.9KB). Following is the script used for transforming it into TFRecord, giving the tfrecord file of size 710KB.
&lt;denchmark-code&gt;from __future__ import absolute_import, division, print_function, unicode_literals
import sys
import tensorflow as tf
import numpy as np
import pandas as pd

dataset = tf.data.experimental.make_csv_dataset('random.csv', batch_size = 1,select_columns = ['short_description','Label'], label_name = 'Label', num_epochs = 1 ,shuffle = True)# (many examples, many labels) pairs

# creating tfrecord
def serialize_example(string_input, integer_input):# Serializing every element
	string_input = string_input.numpy()
	string_list = tf.train.BytesList(value=string_input)
	feature_strings = tf.train.Feature(bytes_list=string_list)
	int_list = tf.train.Int64List(value = [integer_input])
	feature_int = tf.train.Feature(int64_list = int_list)

	#make Feature value as DICT
	elements_dict = {
	  'strings': feature_strings,
	  'Ints': feature_int
	}
	features_dict = tf.train.Features(feature=elements_dict)
	example = tf.train.Example(features=features_dict)
	return example.SerializeToString()

def tf_serialize_example(f0, f1):
	with tf.device('CPU:0'):
		tf_string = tf.py_function(serialize_example, inp = [f0['short_description'], tf.reshape(f1, [])], Tout = tf.string)
	return tf_string
serialized_dataset = dataset.map(tf_serialize_example)# to serialize every element in the dataset

filename = 'random1.tfrecord'# name of the tfrecord file
writer = tf.data.experimental.TFRecordWriter(filename)# writing the tfrecord file
writer.write(serialized_dataset)
&lt;/denchmark-code&gt;

I used Python 2 and TF2.0rc.
		</comment>
		<comment id='3' author='rishabhsahrawat' date='2019-09-03T13:06:21Z'>
		I have tried on Jupyter notebook with TF2.0rc0 and was able to reproduce the issue.Thanks!
		</comment>
		<comment id='4' author='rishabhsahrawat' date='2019-09-04T16:54:14Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
, could you try reproducing the behavior with TF 1.X, I suspect that this is not specific to TF 2.0.
&lt;denchmark-link:https://github.com/rishabhsahrawat&gt;@rishabhsahrawat&lt;/denchmark-link&gt;
 what leads you to believe that an uncompressed TFRecord file should be smaller than the equivalent csv file? Unlike csv, TFRecord format duplicates the feature key in each record, so I would expect the uncompressed TFRecord file to actually be bigger.
		</comment>
		<comment id='5' author='rishabhsahrawat' date='2019-09-04T22:14:50Z'>
		Hi, according to the docs I got the understanding that TFRecord usually
takes less space. If that is not true then it must be written in the docs.
So, all the cases when it will take more space and less space.
Also, as you are saying that the resulted TFRecord is uncompressed file
then how does one get the compressed file with smaller size and can that be
read or does it need to be first uncompressed and read? Thank you for your
valuable time.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Wednesday, September 4, 2019, Jiri Simsa ***@***.***&gt; wrote:
 @ravikyram &lt;https://github.com/ravikyram&gt;, could you try reproducing the
 behavior with TF 1.X, I suspect that this is not specific to TF 2.0.

 @rishabhsahrawat &lt;https://github.com/rishabhsahrawat&gt; what leads you to
 believe that an uncompressed TFRecord file should be smaller than the
 equivalent csv file? Unlike csv, TFRecord format duplicates the feature key
 in each record, so I would expect the uncompressed TFRecord file to
 actually be bigger.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#32075?email_source=notifications&amp;email_token=ADZMMPHL4FHVUXBOIYA5WPTQH7SV7A5CNFSM4IR62YAKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD54HXVQ#issuecomment-527989718&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ADZMMPFXBNI3TLBHFAPNC23QH7SV7ANCNFSM4IR62YAA&gt;
 .


-- 
Sent from Gmail iPhone

		</comment>
		<comment id='6' author='rishabhsahrawat' date='2019-09-04T22:28:52Z'>
		Could you point me to the documentation that suggests that TFRecord should take less space then a CSV file?
TFRecordWriter supports the same compression types as &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset&gt;TFRecordDataset&lt;/denchmark-link&gt;
 --  no compression, , or  -- specified through the  argument.
		</comment>
		<comment id='7' author='rishabhsahrawat' date='2019-09-05T19:13:27Z'>
		I read about it here (
&lt;denchmark-link:https://medium.com/mostly-ai/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564&gt;https://medium.com/mostly-ai/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564&lt;/denchmark-link&gt;
),
second paragraph. Although I could not find something similar on TF docs.
As you mentioned earlier the size is expected to be bigger than the csv, so
do you think it is still beneficial to use TFRecords data for very large
files?  Thank you!
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Thursday, September 5, 2019, Jiri Simsa ***@***.***&gt; wrote:
 Could you point me to the documentation that suggests that TFRecord should
 take less space then a CSV file?

 TFRecordWriter supports the same compression types as TFRecordDataset
 &lt;https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset&gt; --
 no compression, GZIP, or ZLIB -- specified through the compression_type
 argument.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#32075?email_source=notifications&amp;email_token=ADZMMPC3ZERQWBJB5EJ6R4DQIAZ6JA5CNFSM4IR62YAKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD55G5UI#issuecomment-528117457&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ADZMMPHKBWWZZO3GBGZFFJDQIAZ6JANCNFSM4IR62YAA&gt;
 .


-- 
Sent from Gmail iPhone

		</comment>
		<comment id='8' author='rishabhsahrawat' date='2019-09-05T20:34:28Z'>
		If you compress the file, I would expect it to be smaller. Additional benefits of using TFRecord for large dataset is that you can process the data in a streaming fashion (e.g. with tf.data) as opposed to loading the entire dataset into memory which it might not fit.
		</comment>
		<comment id='9' author='rishabhsahrawat' date='2019-09-06T08:01:38Z'>
		Okay, I understand. So, if I store the compressed TFRecord file then can I
read it using tf.data.tfrecord as it is or do I have to uncompress it?
Processing the data in streaming fashion
I think that will be similar to when loading a csv file (e.g. with
tf.make_csv_dataset function) and processing it with tf.data, except maybe
in terms of reading speed of the data. Please correct me if I wrong.
Before trying loading data from TFRECORD, I used tf.data to load my csv
file. If I want to use .shuffle function on this then it fills up the
shuffle buffer before every epoch even though I have set
‘shuffle_before_iteration=False’ and this buffering also eats out my
device’s RAM which eventually run out of memory. I have raised an issue
since months but still waiting for the solution. Thank you for your
valuable time and inputs.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Thursday, September 5, 2019, Jiri Simsa ***@***.***&gt; wrote:
 If you compress the file, I would expect it to be smaller. Additional
 benefits of using TFRecord for large dataset is that you can process the
 data in a streaming fashion (e.g. with tf.data) as opposed to loading the
 entire dataset into memory which it might not fit.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#32075?email_source=notifications&amp;email_token=ADZMMPEUWOJ67PYIF5N3WQTQIFVFXA5CNFSM4IR62YAKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD6AVRFQ#issuecomment-528570518&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ADZMMPFTIW4TRJWFTWVQYHLQIFVFXANCNFSM4IR62YAA&gt;
 .


-- 
Sent from Gmail iPhone

		</comment>
		<comment id='10' author='rishabhsahrawat' date='2019-09-06T17:51:53Z'>
		You simply need to specify the compression_type argument of TFRecordDataset.
CSV format is amenable to streaming processing but other formats might not be.
As for your  issue, I am not sure which API are referring. Neither &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/data/experimental/CsvDataset&gt;CsvDataset&lt;/denchmark-link&gt;
 nor &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_csv_dataset&gt;make_csv_dataset&lt;/denchmark-link&gt;
 have that argument. Did you file an issue for this? If so, can you please share a link to it here? Thanks.
		</comment>
		<comment id='11' author='rishabhsahrawat' date='2019-09-08T08:44:07Z'>
		Sorry for the confusion, I meant the ‘tf.data.Dataset.shuffle’  has a
parameter ‘reshuffle_each_iteration’ even after setting it to ‘False’ it
still shuffles the dataset before every epoch and fills up the buffer which
at some point leads to OOM error. This error sometimes also comes during
the first time it fills up the buffer or before the beginning of second
epoch. Here is the issue that I have raised.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/30646#event-2516513806&gt;#30646 (comment)&lt;/denchmark-link&gt;

Thank you again!

&lt;denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#shuffle&gt;https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#shuffle&lt;/denchmark-link&gt;

&lt;denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#shuffle&gt;https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#shuffle&lt;/denchmark-link&gt;

&lt;denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#shuffle&gt;https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#shuffle&lt;/denchmark-link&gt;


&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Friday, September 6, 2019, Jiri Simsa ***@***.***&gt; wrote:
 You simply need to specify the compression_type argument of
 TFRecordDataset.

 CSV format is amenable to streaming processing but other formats might not
 be.

 As for your shuffle_before_iteration issue, I am not sure which API are
 referring. Neither CsvDataset
 &lt;https://www.tensorflow.org/api_docs/python/tf/data/experimental/CsvDataset&gt;
 nor make_csv_dataset
 &lt;https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_csv_dataset&gt;
 have that argument. Did you file an issue for this? If so, can you please
 share a link to it here? Thanks.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#32075?email_source=notifications&amp;email_token=ADZMMPCHT5HDCTUJOIONGF3QIKK7ZA5CNFSM4IR62YAKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD6DSQOY#issuecomment-528951355&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ADZMMPFVNWUKIT5VABCBSEDQIKK7ZANCNFSM4IR62YAA&gt;
 .


-- 
Sent from Gmail iPhone

		</comment>
		<comment id='12' author='rishabhsahrawat' date='2019-09-09T20:40:13Z'>
		reshuffle_each_iteration does not control whether elements should only be shuffled every epoch (or only the first epoch). What it controls is whether the order in which elements are shuffled is different (or same) every epoch.
The error on &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/30646&gt;#30646&lt;/denchmark-link&gt;
 does not seem related to tf.data as not allocating memory on GPU. I suggest you create a new issue for your problem with minimal reproducible example and reference it here.
		</comment>
		<comment id='13' author='rishabhsahrawat' date='2019-09-09T20:40:14Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=32075&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=32075&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='14' author='rishabhsahrawat' date='2019-09-10T07:52:22Z'>
		HI &lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/32376&gt;here&lt;/denchmark-link&gt;
 is the new issue. Please let me know if you have any comments or suggestions. Thank you very much!
-Rishabh Sahrawat
		</comment>
	</comments>
</bug>