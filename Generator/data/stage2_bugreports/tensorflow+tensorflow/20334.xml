<bug id='20334' author='Aaron19960821' open_date='2018-06-27T08:03:53Z' closed_time='2018-08-16T17:19:06Z'>
	<summary>Incompatible shapes between op input and calculated input gradient.</summary>
	<description>
When I try to implement a SegNet with tensorflow, I met up with the following issue:
&lt;denchmark-code&gt;Incompatible shapes between op input and calculated input gradient.
&lt;/denchmark-code&gt;

My net structure is here below:
&lt;denchmark-code&gt; def buildNet(self):
        #TODO: Implemente this method
        self.X = tf.placeholder(dtype=tf.float32, shape=(self.batchsize, self.width, self.height, self.channel))
        self.Y = tf.placeholder(dtype=tf.int32, shape=(self.batchsize, self.width, self.height))

        norm = tf.nn.lrn(self.X, depth_radius=5, bias=1.0, alpha=1e-4, beta=0.75)
        # Encoder of segnet
        self.enconv1 = batchnorm(conv2d(norm, [3,3,self.channel,64], [1,1,1,1]))
        self.enconv2 = batchnorm(conv2d(self.enconv1, [3,3,64,64], [1,1,1,1]))
        self.enpool1 = maxpooling2d(self.enconv2, [1,2,2,1], [1,2,2,1])

        self.enconv3 = batchnorm(conv2d(self.enpool1, [3,3,64,128], [1,1,1,1]))
        self.enconv4 = batchnorm(conv2d(self.enconv3, [3,3,128,128], [1,1,1,1]))
        self.enpool2 = maxpooling2d(self.enconv4, [1,2,2,1], [1,2,2,1])

        self.enconv5 = batchnorm(conv2d(self.enpool2, [3,3,128,256], [1,1,1,1]))
        self.enconv6 = batchnorm(conv2d(self.enconv5, [3,3,256,256], [1,1,1,1]))
        self.enpool3 = maxpooling2d(self.enconv6, [1,2,2,1], [1,2,2,1])

        self.enconv7 = batchnorm(conv2d(self.enpool3, [3,3,256,512], [1,1,1,1]))
        self.enconv8 = batchnorm(conv2d(self.enconv7, [3,3,512,512], [1,1,1,1]))
        self.enpool4 = maxpooling2d(self.enconv8, [1,2,2,1], [1,2,2,1])

        self.enconv9 = batchnorm(conv2d(self.enpool4, [3,3,512,512], [1,1,1,1]))
        self.enconv10 = batchnorm(conv2d(self.enconv9, [3,3,512,512], [1,1,1,1]))
        self.enpool5 = maxpooling2d(self.enconv10, [1,2,2,1], [1,2,2,1])

        # Decoder of segnet
        self.depool1 = deconv2d(self.enpool5, [3,3,512,512], self.enconv10.shape, [1,1,1,1])
        self.deconv1 = batchnorm(conv2d(self.depool1, [3,3,512,512], [1,1,1,1]))
        self.deconv2 = batchnorm(conv2d(self.deconv1, [3,3,512,512], [1,1,1,1]))

        self.depool2 = deconv2d(self.deconv2, [3,3,512,512], self.enconv8.shape, [1,1,1,1])
        self.deconv3 = batchnorm(conv2d(self.depool2, [3,3,512,256], [1,1,1,1]))
        self.deconv4 = batchnorm(conv2d(self.deconv3, [3,3,256,256], [1,1,1,1]))

        self.depool3 = deconv2d(self.deconv4, [3,3,256,256], self.enconv6.shape, [1,1,1,1])
        self.deconv5 = batchnorm(conv2d(self.depool3, [3,3,256,128], [1,1,1,1]))
        self.deconv6 = batchnorm(conv2d(self.deconv5, [3,3,128,128], [1,1,1,1]))

        self.depool4 = deconv2d(self.deconv6, [3,3,128,128], self.enconv4.shape, [1,1,1,1])
        self.deconv7 = batchnorm(conv2d(self.depool4, [3,3,128,64], [1,1,1,1]))
        self.deconv8 = batchnorm(conv2d(self.deconv7, [3,3,64,64], [1,1,1,1]))

        self.depool5 = deconv2d(self.deconv8, [3,3,64,64], self.enconv2.shape, [1,1,1,1])
        self.deconv9 = batchnorm(conv2d(self.depool5, [3,3,64,32], [1,1,1,1]))
        self.deconv10 = batchnorm(conv2d(self.deconv9, [3,3,32,self.classes], [1,1,1,1]))

        self.pred = tf.nn.softmax(self.deconv10)
        self.oneHotY = tf.one_hot(self.Y, depth=2, on_value=1.0, off_value=0.0)
        self.loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(onehot_labels=self.oneHotY, logits=self.pred))
        print self.loss.shape
        self.optimizer = tf.train.AdamOptimizer(self.learningrate).minimize(self.loss)
&lt;/denchmark-code&gt;

I think it is the problem of tf.conv2d_trainspose. Does anyone have idea about it?
	</description>
	<comments>
		<comment id='1' author='Aaron19960821' date='2018-06-27T18:46:19Z'>
		Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.
Have I written custom code
OS Platform and Distribution
TensorFlow installed from
TensorFlow version
Bazel version
CUDA/cuDNN version
GPU model and memory
Exact command to reproduce
		</comment>
		<comment id='2' author='Aaron19960821' date='2018-06-27T20:35:33Z'>
		Could you try to simplify your program to the minimum program that exhibits the error?
		</comment>
		<comment id='3' author='Aaron19960821' date='2018-06-28T02:53:31Z'>
		OS Platform: MacOS High Sierra
No GPU, just on CPU, for testing
Tensorflow Version: 1.7
Terminal Output: ValueError: Incompatible shapes between op input and calculated input gradient.  Forward operation: conv2d_transpose_4.  Input index: 2. Original input shape: (4, 256, 256, 64).  Calculated input gradient shape: (4, 512, 512, 64)
		</comment>
		<comment id='4' author='Aaron19960821' date='2018-07-17T07:08:21Z'>
		encountered same issue on older tensorflow version: 1.4
		</comment>
		<comment id='5' author='Aaron19960821' date='2018-07-23T16:50:36Z'>
		Does this still occur in the latest version?
		</comment>
		<comment id='6' author='Aaron19960821' date='2018-07-24T00:38:18Z'>
		U mean tensorflow r1.9? I am not sure and will check it later.
		</comment>
		<comment id='7' author='Aaron19960821' date='2018-07-27T12:17:30Z'>
		I have the same error. My message looks liek this:
ValueError: Incompatible shapes between op input and calculated input gradient.  Forward operation: decoder/conv3d_transpose/conv3d_transpose.  Input index: 2. Original input shape: (30, 149, 149, 1, 8).  Calculated input gradient shape: (30, 298, 298, 1, 8).
It seems that the calculated shape of the gradient does not match with the output shape that one has to give the function conv_transpose. In my case the error only occurs in my code with a stride &gt; 1 and a deconv -&gt; conv  -&gt; deconv -&gt; conv ... architecture for the decoder (For an AE). Iam not sure if this is a bug or If I did something wrong. But since I have a symmetric AE it could be a bug.
EDIT
After further investigation it looks like a bug:
I perform a 2d convolution with stride = 2 and padding ="SAME" and tf reduces the size from 298 to 149. I checked my connections in tensorboard and did nothing wrong. This is the output of the specific node:
&lt;denchmark-code&gt;
`decoder/conv_same_3/Conv2D
Operation: Conv2D
Attributes (6)
T {"type":"DT_FLOAT"}
data_format {"s":"NHWC"}
dilations {"list":{"i":[1,1,1,1]}}
padding {"s":"SAME"}
strides {"list":{"i":[1,2,2,1]}}
use_cudnn_on_gpu {"b":true}
Inputs (2) decoder/conv2d_transpose_3/Relu 30×298×298×8
    default_variable_decoder/conv2d_transpose_3_w/read 2×2×8×8 
Outputs (1) decoder/conv_same_3/BiasAdd 30×149×149×8
&lt;/denchmark-code&gt;

`
		</comment>
		<comment id='8' author='Aaron19960821' date='2018-08-15T02:59:12Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/drpngx&gt;@drpngx&lt;/denchmark-link&gt;
: It has been 18 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='9' author='Aaron19960821' date='2018-08-16T00:13:45Z'>
		&lt;denchmark-link:https://github.com/Aaron19960821&gt;@Aaron19960821&lt;/denchmark-link&gt;
 I think we're at  now.
		</comment>
		<comment id='10' author='Aaron19960821' date='2018-08-16T00:14:47Z'>
		&lt;denchmark-link:https://github.com/2649&gt;@2649&lt;/denchmark-link&gt;
 thanks, that seems strange. Do you have a simpler repro case, maybe with just one call to the conv operator?
		</comment>
		<comment id='11' author='Aaron19960821' date='2018-08-16T11:11:17Z'>
		I extracted the variables of this layer and performed only one convolution. Now I get a different error, but the behaviour is the same. It falsely uses valid padding although I pass "SAME" as padding.
&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np

input_tensor = tf.placeholder(tf.float32, (30, 298, 298, 8))
y_tensor = tf.placeholder(tf.float32, (30, 298, 298, 8))
x = np.random.normal(size=(30, 298, 298, 8))

weight = tf.get_variable("weight", [2, 2, 8, 8])
bias = tf.get_variable("bias", [8])
stride = [1, 2, 2, 1]

conv = tf.nn.conv2d(input_tensor, weight, stride, padding="SAME")

loss = tf.losses.mean_squared_error(y_tensor, conv)
optimizer = tf.train.GradientDescentOptimizer(0.001)
train = optimizer.minimize(loss)

sess = tf.Session()
sess.run(tf.global_variables_initializer())
sess.run(train, 
        feed_dict={
            input_tensor: x,
            y_tensor: x
        })

&lt;/denchmark-code&gt;

The error:
&lt;denchmark-code&gt;---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-23-fe1f6d95be60&gt; in &lt;module&gt;()
     12 conv = tf.nn.conv2d(input_tensor, weight, stride, padding="SAME")
     13 
---&gt; 14 loss = tf.losses.mean_squared_error(y_tensor, conv)
     15 optimizer = tf.train.GradientDescentOptimizer(0.001)
     16 train = optimizer.minimize(loss)

C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\ops\losses\losses_impl.py in mean_squared_error(labels, predictions, weights, scope, loss_collection, reduction)
    627     predictions = math_ops.to_float(predictions)
    628     labels = math_ops.to_float(labels)
--&gt; 629     predictions.get_shape().assert_is_compatible_with(labels.get_shape())
    630     losses = math_ops.squared_difference(predictions, labels)
    631     return compute_weighted_loss(

C:\Program Files\Anaconda3\lib\site-packages\tensorflow\python\framework\tensor_shape.py in assert_is_compatible_with(self, other)
    842     """
    843     if not self.is_compatible_with(other):
--&gt; 844       raise ValueError("Shapes %s and %s are incompatible" % (self, other))
    845 
    846   def most_specific_compatible_shape(self, other):

ValueError: Shapes (30, 149, 149, 8) and (30, 298, 298, 8) are incompatible

&lt;/denchmark-code&gt;

I use version 1.8, but on my workstation 1.6 and it yields the same error.
		</comment>
		<comment id='12' author='Aaron19960821' date='2018-08-16T14:45:54Z'>
		&lt;denchmark-link:https://github.com/drpngx&gt;@drpngx&lt;/denchmark-link&gt;
  Ok I thought about it again and it is not a bug. When using a stride &gt; 1 with same padding the image will automatically scale down. Otherwise the padding must happen between pixels in the image to maintain the original size. I think you can close this thread.
		</comment>
		<comment id='13' author='Aaron19960821' date='2018-08-16T17:19:06Z'>
		Thanks for looping back! Closing.
		</comment>
		<comment id='14' author='Aaron19960821' date='2019-01-18T14:30:55Z'>
		
Ok I thought about it again and it is not a bug. When using a stride &gt; 1 with same padding the image will automatically scale down. Otherwise the padding must happen between pixels in the image to maintain the original size. I think you can close this thread.

&lt;denchmark-link:https://github.com/2649&gt;@2649&lt;/denchmark-link&gt;

did you change padding to "VALID" to fix the shape error?
		</comment>
		<comment id='15' author='Aaron19960821' date='2019-05-16T14:20:02Z'>
		I experience the same issue, already implementing the padding as specified by &lt;denchmark-link:https://github.com/2649&gt;@2649&lt;/denchmark-link&gt;

Have I written custom code
No
OS Platform and Distribution
Windows 10, 64 bit
TensorFlow installed from
Anaconda &lt;denchmark-link:https://anaconda.org/anaconda/tensorflow-gpu&gt;https://anaconda.org/anaconda/tensorflow-gpu&lt;/denchmark-link&gt;

TensorFlow version
1.10.0 (result from print(tf.))
Any help please!
		</comment>
		<comment id='16' author='Aaron19960821' date='2020-09-15T18:56:34Z'>
		&lt;denchmark-link:https://github.com/2649&gt;@2649&lt;/denchmark-link&gt;
 did you solve this issue?
I'm using Python3.6 with TensorFlow 1.5.1 and my issue is [Report Error]ValueError: Incompatible shapes between op input and calculated input gradient. conv2d_transpose any idea how to solve it?
		</comment>
	</comments>
</bug>