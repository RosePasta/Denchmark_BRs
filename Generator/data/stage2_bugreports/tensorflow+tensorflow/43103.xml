<bug id='43103' author='pandrey-fr' open_date='2020-09-10T10:33:15Z' closed_time='2020-09-10T13:59:10Z'>
	<summary>cuDNN error when passing all-masked sequences to RNN layers on GPU</summary>
	<description>
System information

Have I written custom code: yes
OS Platform and Distribution: CentOS 8
TensorFlow installed from: binary (pip)
TensorFlow version: 2.3.0 (v2.3.0-rc2-23-gb36436b087)
Python version: 3.6.8
CUDA/cuDNN version: 10.1 / 7
GPU model and memory: RTX 6000 (24GB)

Describe the current behavior
When passing a boolean mask together with data to a RNN layer running on GPU, a cuDNN error is raised if any row of the mask is made entirely of False values, i.e. if one of the batched input sequences is made entirely of padding values.
The raised error is the following:
&lt;denchmark-code&gt;UnknownError: CUDNN_STATUS_BAD_PARAM
in tensorflow/stream_executor/cuda/cuda_dnn.cc(1521): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&amp;padding_fill)' [Op:CudnnRNNV3]
&lt;/denchmark-code&gt;

This may seem as an edge case, but can be encountered when reshaping a [batch, block, sequence, dimension] tensor with variable-size (therefore zero-padded) blocks of sequences representing e.g. a long text into a [..., sequence, dimension] tensor to be processed at once by a RNN and then reshaped back to [batch, block, rnn_dim].
I was able to implement a work-around using a custom wrapper class, but this does not feel like a righteous solution.
Describe the expected behavior
I would expect the RNN to output some default value (e.g. a vector or zeros) representing the all-padding sequence(s), as it does on CPU.
Standalone code to reproduce the issue
Minimal code to reproduce the error:
import tensorflow as tf

gru = tf.keras.layers.GRU(128)
rng = tf.random.get_global_generator()
inp = rng.normal((8, 64, 256))
msk = tf.concat([tf.ones((4, 64), dtype=tf.bool), tf.zeros((4, 64), dtype=tf.bool)], axis=0)

# works
with tf.device('CPU:0'):
    gru(inp, mask=msk)
# works
with tf.device('GPU:0'):
    gru(inp, mask=tf.ones_like(msk))
# fails
with tf.device('GPU:0'):
    gru(inp, mask=msk)
Workaround I implemented, which "unmasks" all-padding sequences, thus triggering unrequired computations:
import tensorflow as tf

class SafeRNN(tf.keras.layers.Wrapper):
    """Wrapper for keras RNN layers avoiding a mask-caused cuda error."""

    def call(self, inputs, mask=None, **kwargs):
        """Run inputs through the wrapped layer."""
        if mask is not None:
            valid = tf.reduce_any(mask, axis=1, keepdims=True)
            mask = tf.where(valid, mask, tf.ones_like(mask))
        return self.layer(inputs, mask=mask, **kwargs)

    def compute_mask(self, inputs, mask=None):
        """Return an output mask tensor."""
        if mask is None:
            return None
        return tf.reduce_any(mask, axis=1)

gru = SafeRNN(tf.keras.layers.GRU(128))
rng = tf.random.get_global_generator()
inp = rng.normal((8, 64, 256))
msk = tf.concat([tf.ones((4, 64), dtype=tf.bool), tf.zeros((4, 64), dtype=tf.bool)], axis=0)

# works
with tf.device('GPU:0'):
    gru(inp, mask=msk)
Other info
This issue is not consistent from a system to the other; it does not trigger on my Linux Mint 19.1 system, with the same Python, TensorFlow and CUDA/cuDNN versions (but distinct GPU: Quadro P1000).
	</description>
	<comments>
		<comment id='1' author='pandrey-fr' date='2020-09-10T11:12:26Z'>
		&lt;denchmark-link:https://github.com/pandrey-fr&gt;@pandrey-fr&lt;/denchmark-link&gt;

I have tried in colab with TF nightly version() and i am not seeing any issue. Please, find the gist &lt;denchmark-link:https://colab.research.google.com/gist/ravikyram/1b31cd0dc4889e6dcbb817e6517bb76d/untitled334.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='pandrey-fr' date='2020-09-10T13:59:07Z'>
		Indeed, the issue is solved in nightly. I will therefore use my workaround with the current stable release and discard it when I upgrade to 2.4 after a stable release is issued.
It is a bit tiring to constantly have to maintain / upgrade code to get such apparently-small bug fixes which are not back-ported to previous stable releases, but at least the bug seems fixed; thank you!
		</comment>
		<comment id='3' author='pandrey-fr' date='2020-09-10T13:59:12Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43103&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43103&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>