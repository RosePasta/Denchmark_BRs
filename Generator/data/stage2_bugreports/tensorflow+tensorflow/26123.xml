<bug id='26123' author='lenaevans' open_date='2019-02-26T07:41:46Z' closed_time='2019-02-27T17:46:04Z'>
	<summary>For an SSD mobilenetv2 model, tflite quantized_uint8 inference is slower on some android devices than float inference</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No, using the code in the tflite example apps.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac 10.14.2
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy S8 [Snapdragon 835] (API 26), HTC Bolt [Snapdragon 810] (API 24)
TensorFlow installed from (source or binary): Binary (using implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly' on android)
TensorFlow version (use command below): ('v1.12.0-rc0-17-g7b08198113', '1.12.0-rc1')
Python version: 2.7.15
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A

Describe the current behavior
On the HTC Bolt, the performance of the SSD mobilenetv2 model is about the same for a quantized_uint8 model as well as a float model. However, on the Samsung Galaxy S8, the quantized_uint8 model is about twice as fast.
Describe the expected behavior
I would expect the quantized_uint8 model to be faster in every scenario (this is what I observed, for example on iOS).
	</description>
	<comments>
		<comment id='1' author='lenaevans' date='2019-02-27T12:23:16Z'>
		This problem is caused by the specific differences in the CPU of the mobile device.Some Qualcomm Snapdragon processors support the use of DSP to speed up specific operations. If the NNAPI in the system is properly configured, there will be a high speed increase.Maybe the Qualcomm Snapdragon 835's DSP supports acceleration of the INT8, so it gets a higher speed.Specific information needs to visit the Qualcomm Developer website.
This page(&lt;denchmark-link:http://ai-benchmark.com/ranking_processors.html&gt;http://ai-benchmark.com/ranking_processors.html&lt;/denchmark-link&gt;
) also gives some information that might be useful.
		</comment>
		<comment id='2' author='lenaevans' date='2019-02-27T16:14:36Z'>
		I see, NNAPI is only supported on API 27+ right? So is there no way to get speedups on lower APIs (No one updates their android phones)?
		</comment>
		<comment id='3' author='lenaevans' date='2019-02-27T17:19:27Z'>
		This is not just about the API version.NNAPI still has no effect if the device manufacturer does not implement hardware acceleration for NNAPI in the system.For situations where NNAPI is not available, if the device's SOC does support hardware acceleration, you can use the solution provided by the SOC vendor(E.g Qualcomm Neural Processing SDK for AI).But currently Qualcomm Snapdragon
Hisilicon Kirin, MediaTek Helio, and Samsung Exynos all have their own neural network SDKs.These SDKs need to be converted to the model when they are used, sometimes there will be problems.
		</comment>
		<comment id='4' author='lenaevans' date='2019-02-27T17:46:04Z'>
		Ok, thanks for the response!
		</comment>
	</comments>
</bug>