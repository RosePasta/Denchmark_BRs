<bug id='34067' author='m4tts' open_date='2019-11-07T10:04:39Z' closed_time='2020-02-03T07:44:08Z'>
	<summary>TF.distribute.MirroredStrategy() crashes</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Progress Linux 5+ (engywuck-backports) (Linux Debian Buster)
TensorFlow installed from (source or binary): binary (problem happens from source aswell)
TensorFlow version (use command below): v1.12.1-16854-g6778662 2.1.0-dev20191028
Python version: Python 3.7.3
CUDA/cuDNN version: 10.0/7.0
GPU model and memory: 2x Asus GeForxe RTX 2080 Ti, Compute Capability 7.5, No NVLink

Describe the current behavior
Since we are not allowed to share our data, I tried to reproduce our problem with a dataset from tensorflow_datasets.
The current code might not make much sense, but I am able to deliver a reproducible code with it.
Training on only one-gpu  works without a problem, with tf.distribute.MirroredStrategy() it crashes (see dump).
What we already tried:

build tensorflow from source
build tensorflow from source against cuda10.1
using tensorflow via pip: tensorflow-gpu

Describe the expected behavior
Tf.distribute.MirroredStrategy() should lead to similar Results like training on one-gpu only.

I tried to reproduce the problem using google-colab. but since only one gpu is provided, it is not really reproducible.
I tried it with two virtual GPUs, but it didn't lead to similar behavior like our problem.
&lt;denchmark-link:https://colab.research.google.com/drive/1hmqt9KdWoheKajkHl_J6dJgoHneKoJP3&gt;https://colab.research.google.com/drive/1hmqt9KdWoheKajkHl_J6dJgoHneKoJP3&lt;/denchmark-link&gt;

On my set-up i use following code:
# -*- coding: utf-8 -*-

import numpy as np
from tensorflow.keras.applications.resnet50 import ResNet50
import tensorflow as tf
import tensorflow_datasets as tfds

LENGTH_DATASET = 17509
NUM_CLASSES = 9
IMG_SHAPE = (256, 256, 3)
BATCH_SIZE = 32


def mymap_func(features):
    return features["image"], features["label"]


AUTOTUNE = tf.data.experimental.AUTOTUNE

# create input pipeline
dataset = tfds.load(name="deep_weeds", split="train")
dataset = dataset.map(mymap_func,
                      num_parallel_calls=tf.data.experimental.AUTOTUNE)
dataset = dataset.cache()
dataset = dataset.shuffle(buffer_size=LENGTH_DATASET, seed=42,
                          reshuffle_each_iteration=True)
dataset = dataset.batch(batch_size=BATCH_SIZE, drop_remainder=True).repeat()
dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)


# create model
img_width, img_height = 270, 270

shape, classes = (img_width, img_height, 1), 3

strategy = tf.distribute.MirroredStrategy()
print("Number of devices in strategy: {}".format(strategy.num_replicas_in_sync))

with strategy.scope():

  model = ResNet50(include_top=True,
                       weights=None,
                       input_tensor=None,
                       input_shape=IMG_SHAPE,
                       pooling=None,
                       classes=NUM_CLASSES)

  model.compile(optimizer=tf.optimizers.Adam(),
                    loss='sparse_categorical_crossentropy',
                    metrics=["accuracy"])

train_steps = np.ceil(LENGTH_DATASET / BATCH_SIZE)
history = model.fit(
        x=dataset,
        epochs=10,
        verbose=1,
        steps_per_epoch=train_steps,
        use_multiprocessing=False,
        workers=8)
Other info / logs
python dump of above script
python src/test/multi_gpu_training_colab.py 
2019-11-07 10:44:52.905250: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2019-11-07 10:44:52.950697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Found device 0 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:86:00.0
2019-11-07 10:44:52.951317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Found device 1 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:af:00.0
2019-11-07 10:44:52.951554: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-11-07 10:44:52.952809: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-11-07 10:44:52.953947: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-11-07 10:44:52.954263: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-11-07 10:44:52.955764: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-11-07 10:44:52.956986: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-11-07 10:44:52.960430: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-11-07 10:44:52.962770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1674] Adding visible gpu devices: 0, 1
2019-11-07 10:44:52.963103: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-11-07 10:44:53.001056: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2100000000 Hz
2019-11-07 10:44:53.008873: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5460110 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2019-11-07 10:44:53.008905: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2019-11-07 10:44:53.233141: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5521500 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2019-11-07 10:44:53.233177: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5
2019-11-07 10:44:53.233185: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): GeForce RTX 2080 Ti, Compute Capability 7.5
2019-11-07 10:44:53.234101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Found device 0 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:86:00.0
2019-11-07 10:44:53.234646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Found device 1 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:af:00.0
2019-11-07 10:44:53.234685: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-11-07 10:44:53.234699: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-11-07 10:44:53.234711: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-11-07 10:44:53.234723: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-11-07 10:44:53.234736: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-11-07 10:44:53.234748: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-11-07 10:44:53.234760: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-11-07 10:44:53.237620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1674] Adding visible gpu devices: 0, 1
2019-11-07 10:44:53.237669: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-11-07 10:44:53.239881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1087] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-11-07 10:44:53.239900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1093]      0 1 
2019-11-07 10:44:53.239912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1106] 0:   N N 
2019-11-07 10:44:53.239922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1106] 1:   N N 
2019-11-07 10:44:53.242394: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1232] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10312 MB memory) -&gt; physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:86:00.0, compute capability: 7.5)
2019-11-07 10:44:53.243757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1232] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10312 MB memory) -&gt; physical GPU (device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:af:00.0, compute capability: 7.5)
Number of devices in strategy: 2
Train for 548.0 steps
Epoch 1/10
2019-11-07 10:45:11.764680: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-11-07 10:45:13.747993: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-11-07 10:45:15.307911: E tensorflow/stream_executor/cuda/cuda_driver.cc:948] failed to synchronize the stop event: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered
2019-11-07 10:45:15.307949: E tensorflow/stream_executor/gpu/gpu_timer.cc:55] Internal: Error destroying CUDA event: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered
2019-11-07 10:45:15.307957: E tensorflow/stream_executor/gpu/gpu_timer.cc:60] Internal: Error destroying CUDA event: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered
2019-11-07 10:45:15.307992: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 8B (8 bytes) from device: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered
2019-11-07 10:45:15.308001: E tensorflow/stream_executor/stream.cc:5452] Internal: Failed to enqueue async memset operation: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered
2019-11-07 10:45:15.308017: W tensorflow/core/kernels/gpu_utils.cc:68] Failed to check cudnn convolutions for out-of-bounds reads and writes with an error message: 'Failed to load in-memory CUBIN: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered'; skipping this check. This only means that we won't check cudnn for out-of-bounds reads and writes. This message will only be printed once.
2019-11-07 10:45:15.308032: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 8B (8 bytes) from device: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered
2019-11-07 10:45:15.308044: I tensorflow/stream_executor/stream.cc:4963] [stream=0x62f2a40,impl=0x62f1230] did not memzero GPU location; source: 0x7fcf977fbfd0
2019-11-07 10:45:15.308500: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: cuDNN launch failure : input shape([16,3,262,262]) filter shape([7,7,3,64])
         [[{{node replica_1/resnet50/conv1_conv/Conv2D}}]]
         [[loss/mul/_10]]
2019-11-07 10:45:15.308571: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: cuDNN launch failure : input shape([16,3,262,262]) filter shape([7,7,3,64])
         [[{{node replica_1/resnet50/conv1_conv/Conv2D}}]]
         [[metrics/accuracy/div_no_nan/AddN_1/_32]]
2019-11-07 10:45:15.308780: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: cuDNN launch failure : input shape([16,3,262,262]) filter shape([7,7,3,64])
         [[{{node replica_1/resnet50/conv1_conv/Conv2D}}]]
2019-11-07 10:45:15.332853: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2019-11-07 10:45:15.333219: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
  1/548 [..............................] - ETA: 2:39:09Traceback (most recent call last):
  File "src/test/multi_gpu_training_colab.py", line 81, in &lt;module&gt;
    workers=8)
  File "/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py", line 778, in fit
    use_multiprocessing=use_multiprocessing)
  File "/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 338, in fit
    total_epochs=epochs)
  File "/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 128, in run_one_epoch
    batch_outs = execution_function(iterator)
  File "/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py", line 86, in execution_function
    distributed_function(input_fn))
  File "/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 568, in __call__
    result = self._call(*args, **kwds)
  File "/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 632, in _call
    return self._stateless_fn(*args, **kwds)
  File "/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 2339, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File "/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1589, in _filtered_call
    self.captured_inputs)
  File "/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1670, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 521, in call
    ctx=ctx)
  File "/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File "&lt;string&gt;", line 3, in raise_from
tensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.
  (0) Internal:  cuDNN launch failure : input shape([16,3,262,262]) filter shape([7,7,3,64])
         [[node replica_1/resnet50/conv1_conv/Conv2D (defined at usr/lib/python3.7/threading.py:917) ]]
         [[loss/mul/_10]]
  (1) Internal:  cuDNN launch failure : input shape([16,3,262,262]) filter shape([7,7,3,64])
         [[node replica_1/resnet50/conv1_conv/Conv2D (defined at usr/lib/python3.7/threading.py:917) ]]
0 successful operations.
1 derived errors ignored. [Op:__inference_distributed_function_36243]

Function call stack:
distributed_function -&gt; distributed_function

2019-11-07 10:45:15.777040: I tensorflow/stream_executor/stream.cc:1990] [stream=0x5ab9030,impl=0x62f1330] did not wait for [stream=0x62f2a40,impl=0x62f1230]
2019-11-07 10:45:15.777095: I tensorflow/stream_executor/stream.cc:4938] [stream=0x5ab9030,impl=0x62f1330] did not memcpy host-to-device; source: 0x7fcf8007e000
2019-11-07 10:45:15.777129: E tensorflow/stream_executor/stream.cc:332] Error recording event in stream: Error recording CUDA event: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered; not marking stream as bad, as the Event object may be at fault. Monitor for further errors.
2019-11-07 10:45:15.777161: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered
2019-11-07 10:45:15.777181: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1
The device interconnect matrix seems a bit odd, but we don't know if thats an issue for a distributed strategy:
&lt;denchmark-code&gt;2019-11-07 10:44:53.239881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1087] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-11-07 10:44:53.239900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1093]      0 1 
2019-11-07 10:44:53.239912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1106] 0:   N N 
2019-11-07 10:44:53.239922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1106] 1:   N N 
&lt;/denchmark-code&gt;

It seems the low level drivers work fine, see the dumps of nvidia-smi, nvidia-smi topo, cuda deviceQuery and NCCL All reduce test.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3818892/nvidia_smi_topo.txt&gt;nvidia_smi_topo.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3818894/nvidia_smi.txt&gt;nvidia_smi.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3818895/nccl_allreduce.txt&gt;nccl_allreduce.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3818897/cuda_device_query.txt&gt;cuda_device_query.txt&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='m4tts' date='2019-11-10T21:21:09Z'>
		Hi - if the code works with 1 GPU but not 2, perhaps the issue is with how we are handling the gradient all reduce across the 2 GPUs in absence of NVLINK. To test this hypothesis, can you re-run your code with the following change:
&lt;denchmark-code&gt;strategy = tf.distribute.MirroredStrategy(
    cross_device_ops=tf.distribute.ReductionToOneDevice(reduce_to_device="cpu:0"))
&lt;/denchmark-code&gt;

This should force it to do the communication through the CPU.
		</comment>
		<comment id='2' author='m4tts' date='2019-11-10T21:21:19Z'>
		cc &lt;denchmark-link:https://github.com/dubey&gt;@dubey&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/yuefengz&gt;@yuefengz&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='m4tts' date='2019-11-11T07:46:08Z'>
		
Hi - if the code works with 1 GPU but not 2, perhaps the issue is with how we are handling the gradient all reduce across the 2 GPUs in absence of NVLINK. To test this hypothesis, can you re-run your code with the following change:
strategy = tf.distribute.MirroredStrategy(
    cross_device_ops=tf.distribute.ReductionToOneDevice(reduce_to_device="cpu:0"))

This should force it to do the communication through the CPU.

thanks for your reply.
unfortunately it crashes with more or less the same error messages :
Train for 274.0 steps
Epoch 1/10
2019-11-11 08:41:12.723892: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-11-11 08:41:14.713167: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-11-11 08:41:16.401051: E tensorflow/stream_executor/cuda/cuda_driver.cc:948] failed to synchronize the stop event: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered
2019-11-11 08:41:16.401096: E tensorflow/stream_executor/gpu/gpu_timer.cc:55] Internal: Error destroying CUDA event: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered
2019-11-11 08:41:16.401106: E tensorflow/stream_executor/gpu/gpu_timer.cc:60] Internal: Error destroying CUDA event: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered
2019-11-11 08:41:16.401149: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 8B (8 bytes) from device: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered
2019-11-11 08:41:16.401162: E tensorflow/stream_executor/stream.cc:5452] Internal: Failed to enqueue async memset operation: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered
2019-11-11 08:41:16.401182: W tensorflow/core/kernels/gpu_utils.cc:68] Failed to check cudnn convolutions for out-of-bounds reads and writes with an error message: 'Failed to load in-memory CUBIN: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered'; skipping this check. This only means that we won't check cudnn for out-of-bounds reads and writes. This message will only be printed once.
2019-11-11 08:41:16.401197: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 8B (8 bytes) from device: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered
2019-11-11 08:41:16.401209: I tensorflow/stream_executor/stream.cc:4963] [stream=0x5a6a5c0,impl=0x5a68db0] did not memzero GPU location; source: 0x7fbf9bffcfd0
2019-11-11 08:41:16.401718: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: cuDNN launch failure : input shape([32,3,262,262]) filter shape([7,7,3,64])
         [[{{node replica_1/resnet50/conv1_conv/Conv2D}}]]
         [[Adam/AddN_59/_1310]]
2019-11-11 08:41:16.401802: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: cuDNN launch failure : input shape([32,3,262,262]) filter shape([7,7,3,64])
         [[{{node replica_1/resnet50/conv1_conv/Conv2D}}]]
         [[FusedBatchNormGradV3_30/_1010]]
2019-11-11 08:41:16.407012: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: cuDNN launch failure : input shape([32,3,262,262]) filter shape([7,7,3,64])
         [[{{node replica_1/resnet50/conv1_conv/Conv2D}}]]
2019-11-11 08:41:16.407636: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2019-11-11 08:41:16.408077: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
  1/274 [..............................] - ETA: 1:13:30Traceback (most recent call last):
  File "src/test/multi_gpu_training_tf_dataset.py", line 59, in &lt;module&gt;
    workers=8)
  File "/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py", line 778, in fit
    use_multiprocessing=use_multiprocessing)
  File "/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 338, in fit
    total_epochs=epochs)
  File "/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 128, in run_one_epoch
    batch_outs = execution_function(iterator)
  File "/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py", line 86, in execution_function
    distributed_function(input_fn))
  File "/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 568, in __call__
    result = self._call(*args, **kwds)
  File "/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 632, in _call
    return self._stateless_fn(*args, **kwds)
  File "/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 2339, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File "/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1589, in _filtered_call
    self.captured_inputs)
  File "/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1670, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 521, in call
    ctx=ctx)
  File "/home/sam2/workspace/python_venvs/tf-2/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File "&lt;string&gt;", line 3, in raise_from
tensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.
  (0) Internal:  cuDNN launch failure : input shape([32,3,262,262]) filter shape([7,7,3,64])
         [[node replica_1/resnet50/conv1_conv/Conv2D (defined at usr/lib/python3.7/threading.py:917) ]]
  (1) Internal:  cuDNN launch failure : input shape([32,3,262,262]) filter shape([7,7,3,64])
         [[node replica_1/resnet50/conv1_conv/Conv2D (defined at usr/lib/python3.7/threading.py:917) ]]
         [[Adam/AddN_59/_1310]]
0 successful operations.
1 derived errors ignored. [Op:__inference_distributed_function_32351]

Function call stack:
distributed_function -&gt; distributed_function

2019-11-11 08:41:16.724607: I tensorflow/stream_executor/stream.cc:1990] [stream=0x5230f90,impl=0x5a68eb0] did not wait for [stream=0x5a6a5c0,impl=0x5a68db0]
2019-11-11 08:41:16.724650: I tensorflow/stream_executor/stream.cc:4938] [stream=0x5230f90,impl=0x5a68eb0] did not memcpy host-to-device; source: 0x7fc3102069c0
2019-11-11 08:41:16.724695: E tensorflow/stream_executor/stream.cc:332] Error recording event in stream: Error recording CUDA event: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered; not marking stream as bad, as the Event object may be at fault. Monitor for further errors.
2019-11-11 08:41:16.724711: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_INSTRUCTION: an illegal instruction was encountered
2019-11-11 08:41:16.724722: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1
Aborted
i got access to a nvidia dgx workstation over the weekend to run the script. there the script runs without any problems. but it is using an older nvidia-driver and linux kernel version.  i'm guessing our problem is somewhere in the API between kernel / driver / tensorflow. is there any way to debug these interfaces or logs containing more information ?
		</comment>
		<comment id='4' author='m4tts' date='2019-11-11T08:02:16Z'>
		Oh I see. yeah in that case this sounds like a driver/TF mismatch problem. &lt;denchmark-link:https://github.com/zongweiz&gt;@zongweiz&lt;/denchmark-link&gt;
 can someone on your team help?
		</comment>
		<comment id='5' author='m4tts' date='2019-11-11T18:59:51Z'>
		&lt;denchmark-link:https://github.com/chsigg&gt;@chsigg&lt;/denchmark-link&gt;
, any thoughts? Thanks.
		</comment>
		<comment id='6' author='m4tts' date='2019-11-27T14:28:12Z'>
		Any updates? I experienced a similar problem. The training does not start with the following code (Last line is Epoch 1/25 - but there is no error message). The memory of the GPUs remains allocated afterwards (restart must be forced).
&lt;denchmark-code&gt;import numpy as np

import tensorflow as tf
import tensorflow.keras as keras
import tensorflow.keras.layers as layers
import tensorflow.keras.backend as K

NUM_WORKERS = 2

data = np.random.uniform(-1.0, 1.0, (10000, 192, 256, 3))
labels = np.random.uniform(-1.0, 1.0, (10000, 10))


def get_model():
    inputs = keras.Input(shape=(192, 256, 3), name='rgb_input')

    net = layers.Conv2D(16, 9, 2, 'same', activation='relu')(inputs)
    net = layers.Flatten()(net)
    net = layers.Dense(10, activation=K.tanh)(net)

    return keras.Model(inputs=inputs, outputs=net)


strategy = tf.distribute.MirroredStrategy()

with strategy.scope():

    model = get_model()
    model.compile(
        optimizer=keras.optimizers.Adam(3e-4),
        loss=keras.losses.MeanSquaredError(),
        metrics=[keras.metrics.MeanAbsoluteError()])

model.fit(data, labels, batch_size=128 * NUM_WORKERS, epochs=25)
&lt;/denchmark-code&gt;

But if i change the strategy in the following way, then it starts (the utilization is bad, but it works).
&lt;denchmark-code&gt;strategy = tf.distribute.MirroredStrategy(
    cross_device_ops=tf.distribute.ReductionToOneDevice(reduce_to_device="cpu:0"))
&lt;/denchmark-code&gt;

Setup:

Ubuntu 18.04
tensorflow-gpu==2.0.0 (installed via pip)
GeForce GTX 1080 TI
python 3.6 (also tried python 2.7)
CUDA / CuDNN: 10.0/7.6.0

		</comment>
		<comment id='7' author='m4tts' date='2019-12-05T08:46:00Z'>
		&lt;denchmark-link:https://github.com/patrick-schulte&gt;@patrick-schulte&lt;/denchmark-link&gt;
 do you have a device-interconnect matrix with y inside?
		</comment>
		<comment id='8' author='m4tts' date='2019-12-05T09:27:35Z'>
		&lt;denchmark-link:https://github.com/m4tts&gt;@m4tts&lt;/denchmark-link&gt;
 yes
		</comment>
		<comment id='9' author='m4tts' date='2019-12-31T19:27:48Z'>
		&lt;denchmark-link:https://github.com/m4tts&gt;@m4tts&lt;/denchmark-link&gt;
 Does it make a difference if you run with the environment variable  set to ?  I'm wondering if this is a problem with the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/f6216b136033506de2737a16cc29c2ff221decd5/tensorflow/stream_executor/gpu/redzone_allocator.cc#L115&gt;custom PTX&lt;/denchmark-link&gt;
 we use to check for errors in cuDNN.
		</comment>
		<comment id='10' author='m4tts' date='2020-02-03T07:44:08Z'>
		sorry for the late response.
after lots of desparation I found the &lt;denchmark-link:https://github.com/wilicc/gpu-burn&gt;gpu-burn test&lt;/denchmark-link&gt;
.
as it seemed, our GPU was faulty. the test had returned the following :
&lt;denchmark-code&gt;./gpu_burn 120
GPU 0: GeForce RTX 2080 Ti (UUID: [HIDDEN])
GPU 1: GeForce RTX 2080 Ti (UUID: [HIDDEN])
Initialized device 0 with 11019 MB of memory (10788 MB available, using 9709 MB of it), using FLOATS
Initialized device 1 with 11019 MB of memory (10788 MB available, using 9709 MB of it), using FLOATS
10.8%  proc'd: 8456 (11934 Gflop/s) - 6644 (10153 Gflop/s)   errors: 0 - 1052638274  (WARNING!)  temps: 32 C - 39 C 
        Summary at:   Tue Nov 12 09:38:10 CET 2019

21.7%  proc'd: 17516 (11934 Gflop/s) - 14496 (10153 Gflop/s)   errors: 0 - 630154421  (WARNING!)  temps: 32 C - 39 C   
        Summary at:   Tue Nov 12 09:38:23 CET 2019

32.5%  proc'd: 25972 (11519 Gflop/s) - 22348 (10151 Gflop/s)   errors: 0 - -1754308719  (WARNING!)  temps: 60 C - 39 C 
        Summary at:   Tue Nov 12 09:38:36 CET 2019

43.3%  proc'd: 34428 (11513 Gflop/s) - 30200 (10151 Gflop/s)   errors: 0 - 1988913133  (WARNING!)  temps: 60 C - 39 C  
        Summary at:   Tue Nov 12 09:38:49 CET 2019

53.3%  proc'd: 42884 (11387 Gflop/s) - 36844 (10153 Gflop/s)   errors: 0 - 14688317  (WARNING!)  temps: 60 C - 39 C  C 
        Summary at:   Tue Nov 12 09:39:01 CET 2019

64.2%  proc'd: 51944 (11391 Gflop/s) - 44696 (10153 Gflop/s)   errors: 0 - 2095542503  (WARNING!)  temps: 60 C - 39 C  
        Summary at:   Tue Nov 12 09:39:14 CET 2019

75.0%  proc'd: 59796 (11391 Gflop/s) - 52548 (10154 Gflop/s)   errors: 0 - -1156948305  (WARNING!)  temps: 60 C - 39 C 
        Summary at:   Tue Nov 12 09:39:27 CET 2019

85.8%  proc'd: 68252 (11092 Gflop/s) - 60400 (10161 Gflop/s)   errors: 0 - -1474718451  (WARNING!)  temps: 60 C - 39 C 
        Summary at:   Tue Nov 12 09:39:40 CET 2019

96.7%  proc'd: 77312 (11090 Gflop/s) - 67648 (10158 Gflop/s)   errors: 0 - 1678997606  (WARNING!)  temps: 60 C - 39 C  
        Summary at:   Tue Nov 12 09:39:53 CET 2019

100.0%  proc'd: 80332 (11090 Gflop/s) - 70668 (10157 Gflop/s)   errors: 0 - -1670660404  (WARNING!)  temps: 60 C - 39 C 
Killing processes.. done

Tested 2 GPUs:
        GPU 0: OK
        GPU 1: FAULTY
 
&lt;/denchmark-code&gt;

I returned the GPU to the distributor on got a new one. and now everything works like a charm.
&lt;denchmark-link:https://github.com/sanjoy&gt;@sanjoy&lt;/denchmark-link&gt;
 I was not able to test this, since the GPU was allready shipped to the distributor. but it would make sense that there was an error in cuDNN if the GPU was faulty. maybe there would be a way tensorflow could test this or give a hint in that direction?
I thought of every possibility, but it never came to my mind, that the GPU could be faulty because every CUDA and NCCLTest was fine.
		</comment>
		<comment id='11' author='m4tts' date='2020-02-03T07:44:10Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34067&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34067&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='m4tts' date='2020-02-03T16:42:14Z'>
		
maybe there would be a way tensorflow could test this or give a hint in that direction?

I think it is difficult to go from CUDNN_STATUS_INTERNAL_ERROR to "GPU may be faulty" since there are many other reasons why cuDNN will return this error.
However, maybe we should link to the  &lt;denchmark-link:https://github.com/wilicc/gpu-burn&gt;gpu-burn&lt;/denchmark-link&gt;
 test from the TF docs as something users should consider?  CC &lt;denchmark-link:https://github.com/dynamicwebpaige&gt;@dynamicwebpaige&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='m4tts' date='2020-02-05T07:50:17Z'>
		
However, maybe we should link to the gpu-burn test from the TF docs as something users should consider?

I think this is a great idea. I don't know how often GPUs actually break down but in my case, I never really considered it. To have it linked in a troubleshooting or similar section would be really helpful.
		</comment>
		<comment id='14' author='m4tts' date='2020-08-03T20:31:39Z'>
		I had similar problem, but with training only on one GPU without any strategy. The training freezed after tf.data prefetched first batches.
Setting TF_DISABLE_RZ_CHECK=1 helped, I also did gpu-burn, but no errors have shown.
		</comment>
	</comments>
</bug>