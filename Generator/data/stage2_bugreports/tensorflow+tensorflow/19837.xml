<bug id='19837' author='Camuslu' open_date='2018-06-07T15:51:43Z' closed_time='2019-01-23T23:31:51Z'>
	<summary>freeze_graph for inference memory leak: frozen graph size 1.3GB, takes 15GB memory when inference</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes (details below)
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS High Sierra 10.13.3
TensorFlow installed from (source or binary):binary (via pip install)
TensorFlow version (use command below):1.8.0
Python version: 3.6
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce:

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

I want to train a graph, save it, then load it later for inference-only purpose.
The way I'm doing it now:
During training, I save the model with tf.graph_util.convert_variables_to_constants method and tf.train.write_graph which produces a .pb file. Its size is around 1.3 GB.
Then I load the frozen graph and do some inference with some toy data. This causes memory issue by a factor of 10. (In activity monitor, Python takes ~15GB on the first inference)
I've also tried the "saving-loading-inference" procedure with
-saver.save()
-saver.restore()
and it doesn't cause the memory issue.  However, for production purpose on my end, loading a frozen graph would be strongly preferred to the
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

&lt;denchmark-h:h4&gt;Saving graph  (for debugging purpose, I SKIPPED training and save the graph once variables are initialized)&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;## build the graph
x = tf.placeholder(..)
...
some tf operations to build graph
...
pred = tf.nn.xw_plus_b(last_layer, W,b, "predictions")

##save graph
sess = tf.Session()
sess.run(tf.global_variables_initializer()))
graph_as_constants = tf.graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(),
                                      output_node_names=["predictions"])
tf.train.write_graph(graph_as_constants, model_path, as_text=False, name="model_graph")
sess.close()
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Load graph&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;with gfile.FastGFile(model_path, 'rb') as f:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(f.read())
    tf.import_graph_def(graph_def, name='')
graph = tf.get_default_graph()
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Try inference on some toy data:&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;x = graph.get_tensor_by_name("x:0")
pred = graph.get_tensor_by_name("predictions:0")
with tf.Session() as sess:
    for _ in range(10):
        x_test = some np array for testing
        test_result = sess.run(pred, feed_dict={x:x_test})
        time.sleep(30)
&lt;/denchmark-code&gt;

Here in inference, it goes through a loop with 10 iterations. In the first iteration, the sess.run(..) causes the memory leak issue (15 GB). In following iterations, memory falls back to what I would expect with the size of the model loaded.
&lt;denchmark-h:h3&gt;Some details&lt;/denchmark-h&gt;


The issue persists whether I save the frozen graph on CPU machine (macOS High Sierra 10.13.3 ) or a remote GPU machine (Ubuntu 16.04.3 LTS).
Again the graph is frozen once it's initialized. I skipped all training/testing logic for debugging purpose.
For freeze graph,  I also tried freeze_graph in bazel-bin in terminal. It still causes memory issue when loading and doing inference.
I've tried optimize_for_inference(graph_def,...) in the graph loading section. It doesn't solve the issue.

	</description>
	<comments>
		<comment id='1' author='Camuslu' date='2018-07-13T06:49:27Z'>
		I have the same issue! Freezing and optimizing does not help
		</comment>
		<comment id='2' author='Camuslu' date='2018-10-18T12:52:02Z'>
		waiting
		</comment>
		<comment id='3' author='Camuslu' date='2018-12-04T18:51:03Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/jart&gt;@jart&lt;/denchmark-link&gt;
: It has been 46 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='4' author='Camuslu' date='2019-01-09T22:27:02Z'>
		Hello &lt;denchmark-link:https://github.com/Camuslu&gt;@Camuslu&lt;/denchmark-link&gt;
 , There should be some posts on Stack Overflow that address the memory problems the freeze_graph and load model .pb file:
&lt;denchmark-link:http://stackoverflow.com/questions/tagged/tensorflow&gt;Stack Overflow&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='Camuslu' date='2019-01-23T23:31:51Z'>
		Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
	</comments>
</bug>