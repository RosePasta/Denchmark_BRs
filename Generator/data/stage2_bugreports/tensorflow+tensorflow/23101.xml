<bug id='23101' author='fabio12345' open_date='2018-10-19T09:03:22Z' closed_time='2020-07-04T20:46:24Z'>
	<summary>Possible bug in the design of `tf.keras.layers.Conv`</summary>
	<description>
System information

TensorFlow version (you are using): 1.10
Are you willing to contribute it (Yes/No): No (Don't know tf's code base well enough.)

Describe the feature and the current behavior/state.
Current behaviour of tf.layers.Conv2D is that
&lt;denchmark-code&gt;import tensorflow as tf

example = tf.zeros([1, 32, 32, 1])
example2 = tf.zeros([1, 41, 41, 1])

# Using a dilated convolution layer
convolution_op = tf.layers.Conv2D(4, 3, padding="SAME", dilation_rate=4)

print(convolution_op( example ))
print(convolution_op( example2 ))
&lt;/denchmark-code&gt;

causes the following exception
&lt;denchmark-code&gt;ValueError: Dimension size must be evenly divisible by 4 but is 49 for 'conv2d_2/SpaceToBatchND_1' (op: 'SpaceToBatchND') with input shapes: [1,41,41,1], [2], [2,2] and with computed input tensors: input[1] = &lt;4 4&gt;, input[2] = &lt;[4 4][4 4]&gt;.
&lt;/denchmark-code&gt;

The reason for this is that in the  function for  (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/python/keras/layers/convolutional.py&gt;https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/python/keras/layers/convolutional.py&lt;/denchmark-link&gt;
) the convolution operation is created - however, the  convolution operation also contains padding dependent on the size of the first input to .
There's no reason for this, as the padding operation (i.e. the call to ) could simply be different for each of the two s.
Note that with the functional layer API it is still possible to do the reuse of the layers:
&lt;denchmark-code&gt;conv1 = tf.layers.conv2d(example, 4, 3, padding="SAME", dilation_rate=4, name="test")
conv2 = tf.layers.conv2d(example2, 4, 3, padding="SAME", dilation_rate=4, name="test", reuse=True)
&lt;/denchmark-code&gt;

because this causes two separate layer creations. I am proposing that the behaviour should be similar to this.
The consequence of this is that at the moment, to my knowledge, there is no good way of doing weight reuse of dilated convolution weights using tf.keras.layers. I think that the above is possibly a bug in the implementation - the convolution operation should maybe be create in the call function of tf.keras.layers.Conv?
Will this change the current api? How?
This should not raise an error:
&lt;denchmark-code&gt;import tensorflow as tf

example = tf.zeros([1, 32, 32, 1])
example2 = tf.zeros([1, 41, 41, 1])

convolution_op = tf.layers.Conv2D(4, 3, padding="SAME", dilation_rate=4)

print(convolution_op( example ))
print(convolution_op( example2 ))

&lt;/denchmark-code&gt;

Who will benefit with this feature?
Everyone, as this would be a more consistent behavior of the API, and would allow things that right now are not possible.

More information is also in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/23019&gt;#23019&lt;/denchmark-link&gt;
 - I had raised it but it was closed without a relevant reply.
Have I written custom code
as above.
OS Platform and Distribution
N/A
TensorFlow installed from
pip3
Bazel version
N/A
CUDA/cuDNN version
N/A
GPU model and memory
N/A
Exact command to reproduce
as above
Mobile device
N/A
	</description>
	<comments>
		<comment id='1' author='fabio12345' date='2018-10-19T19:19:17Z'>
		Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.
Have I written custom code
OS Platform and Distribution
TensorFlow installed from
Bazel version
CUDA/cuDNN version
GPU model and memory
Exact command to reproduce
Mobile device
		</comment>
		<comment id='2' author='fabio12345' date='2018-10-25T15:18:11Z'>
		Updated.
		</comment>
		<comment id='3' author='fabio12345' date='2018-11-08T04:24:00Z'>
		I'm not sure I understand your question clearly, however I think convolution_op(example) should be invoked only once.
		</comment>
		<comment id='4' author='fabio12345' date='2019-07-19T20:28:07Z'>
		I also ran into this problem (using TF 1.14.0 and 2.0-compatible operations). I would be curious if this behavior is considered intentional by developers.
For others who land on this issue, I've pasted an example workaround for tf.keras.layers.Conv1D:
import functools
import tensorflow._api.v1.compat.v2 as tf


class Conv1D(tf.keras.layers.Conv1D):
    """Prevent keras from caching the input shape when building.
    """
    # pylint: disable=no-name-in-module
    from tensorflow.python.keras.utils import conv_utils
    # pylint: enable=no-name-in-module

    def build(self, input_shape):
        """Use the functional convolution operation instead of an instance 
        of the internal TF Convolution class, which stores and reuses the input shape.
        """
        super().build(input_shape)

        if self.padding == 'causal':
            op_padding = 'valid'
        else:
            op_padding = self.padding
        if not isinstance(op_padding, (list, tuple)):
            op_padding = op_padding.upper()

        # pylint: disable=attribute-defined-outside-init
        self._convolution_op = functools.partial(
            tf.nn.convolution,
            strides=self.strides,
            padding=op_padding,
            data_format=self.conv_utils.convert_data_format(
                self.data_format, self.rank + 2
            ),
            dilations=self.dilation_rate
        )
        # pylint: enable=attribute-defined-outside-init
		</comment>
		<comment id='5' author='fabio12345' date='2020-07-04T20:46:23Z'>
		This is fixed with TF 2.2
convolution_op = tf.keras.layers.Conv2D(4, 3, padding="SAME", dilation_rate=4)
Thanks!
		</comment>
		<comment id='6' author='fabio12345' date='2020-07-04T20:46:25Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23101&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23101&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>