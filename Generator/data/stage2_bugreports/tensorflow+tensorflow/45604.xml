<bug id='45604' author='guillaumelorre28' open_date='2020-12-11T16:09:47Z' closed_time='2020-12-21T21:47:16Z'>
	<summary>No gradients available error</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.3.0
Python version: 3.7

Using a minus sign before the loss in tape.gradient gives a ValueError: No gradients provided for any variable whereas there should be.
import tensorflow as tf
import numpy as np

layers = tf.keras.layers


def _create_mlp(
        hidden_size: int = 1024, output_size: int = 256
):
    return tf.keras.Sequential(
        [
            layers.Dense(hidden_size, activation=None, use_bias=True),
            layers.BatchNormalization(),
            layers.Activation("relu"),
            layers.Dense(hidden_size, activation=None, use_bias=True),
            layers.BatchNormalization(),
            layers.Activation("relu"),
            layers.Dense(output_size, activation=None, use_bias=True)
        ]
    )



class ModelTest(tf.keras.Model):
    def __init__(self):
        super(ModelTest, self).__init__()

        self.generator = _create_mlp(output_size=128)

    def train_step(self, inputs):

        with tf.GradientTape() as gen_tape:

            z = self.generator(inputs)

            final_loss = tf.reduce_mean(z)

        gen_gradients = gen_tape.gradient(-final_loss, self.generator.trainable_variables)

        self.optimizer.apply_gradients(zip(gen_gradients, self.generator.trainable_variables))

        return {}


input_data = np.random.uniform(size=(200, 32))

dataset = tf.data.Dataset.from_tensor_slices(input_data)

dataset = dataset.batch(20)

model = ModelTest()

optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

model.compile(optimizer=optimizer, run_eagerly=True)

model.fit(dataset, epochs=20)
This code gives the following error:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "tests/model_test.py", line 57, in &lt;module&gt;
    model.fit(dataset, epochs=20)
  File "/home/guillaume/miniconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py", line 66, in _method_wrapper
    return method(self, *args, **kwargs)
  File "/home/guillaume/miniconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py", line 848, in fit
    tmp_logs = train_function(iterator)
  File "/home/guillaume/miniconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py", line 572, in train_function
    self.train_step, args=(data,))
  File "/home/guillaume/miniconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py", line 951, in run
    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
  File "/home/guillaume/miniconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py", line 2290, in call_for_each_replica
    return self._call_for_each_replica(fn, args, kwargs)
  File "/home/guillaume/miniconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py", line 2649, in _call_for_each_replica
    return fn(*args, **kwargs)
  File "/home/guillaume/miniconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py", line 282, in wrapper
    return func(*args, **kwargs)
  File "tests/model_test.py", line 40, in train_step
    self.optimizer.apply_gradients(zip(gen_gradients, self.generator.trainable_variables))
  File "/home/guillaume/miniconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py", line 472, in apply_gradients
    grads_and_vars = _filter_grads(grads_and_vars)
  File "/home/guillaume/miniconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py", line 1219, in _filter_grads
    ([v.name for _, v in grads_and_vars],))
ValueError: No gradients provided for any variable: ['sequential/dense/kernel:0', 'sequential/dense/bias:0', 'sequential/batch_normalization/gamma:0', 'sequential/batch_normalization/beta:0', 'sequential/dense_1/kernel:0', 'sequential/dense_1/bias:0', 'sequential/batch_normalization_1/gamma:0', 'sequential/batch_normalization_1/beta:0', 'sequential/dense_2/kernel:0', 'sequential/dense_2/bias:0'].
&lt;/denchmark-code&gt;

If you remove the minus sign before final loss in gen_tape.gradient, the code works fine.
import tensorflow as tf
import numpy as np

layers = tf.keras.layers


def _create_mlp(
        hidden_size: int = 1024, output_size: int = 256
):
    return tf.keras.Sequential(
        [
            layers.Dense(hidden_size, activation=None, use_bias=True),
            layers.BatchNormalization(),
            layers.Activation("relu"),
            layers.Dense(hidden_size, activation=None, use_bias=True),
            layers.BatchNormalization(),
            layers.Activation("relu"),
            layers.Dense(output_size, activation=None, use_bias=True)
        ]
    )



class ModelTest(tf.keras.Model):
    def __init__(self):
        super(ModelTest, self).__init__()

        self.generator = _create_mlp(output_size=128)

    def train_step(self, inputs):

        with tf.GradientTape() as gen_tape:

            z = self.generator(inputs)

            final_loss = tf.reduce_mean(z)

        gen_gradients = gen_tape.gradient(final_loss, self.generator.trainable_variables)

        self.optimizer.apply_gradients(zip(gen_gradients, self.generator.trainable_variables))

        return {}


input_data = np.random.uniform(size=(200, 32))

dataset = tf.data.Dataset.from_tensor_slices(input_data)

dataset = dataset.batch(20)

model = ModelTest()

optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

model.compile(optimizer=optimizer, run_eagerly=True)

model.fit(dataset, epochs=20)
The output is:
&lt;denchmark-code&gt;
10/10 [==============================] - 0s 11ms/step
Epoch 2/20
10/10 [==============================] - 0s 11ms/step
Epoch 3/20
10/10 [==============================] - 0s 11ms/step
Epoch 4/20
10/10 [==============================] - 0s 11ms/step
Epoch 5/20
10/10 [==============================] - 0s 11ms/step
Epoch 6/20
10/10 [==============================] - 0s 11ms/step
Epoch 7/20
10/10 [==============================] - 0s 11ms/step
Epoch 8/20
10/10 [==============================] - 0s 11ms/step
Epoch 9/20
10/10 [==============================] - 0s 11ms/step
Epoch 10/20
10/10 [==============================] - 0s 11ms/step
Epoch 11/20
10/10 [==============================] - 0s 11ms/step
Epoch 12/20
10/10 [==============================] - 0s 11ms/step
Epoch 13/20
10/10 [==============================] - 0s 11ms/step
Epoch 14/20
10/10 [==============================] - 0s 11ms/step
Epoch 15/20
10/10 [==============================] - 0s 11ms/step
Epoch 16/20
10/10 [==============================] - 0s 11ms/step
Epoch 17/20
10/10 [==============================] - 0s 11ms/step
Epoch 18/20
10/10 [==============================] - 0s 11ms/step
Epoch 19/20
10/10 [==============================] - 0s 11ms/step
Epoch 20/20
10/10 [==============================] - 0s 11ms/step

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='guillaumelorre28' date='2020-12-13T20:28:30Z'>
		Why you don't negate the loss inside the gradient tape?
		</comment>
		<comment id='2' author='guillaumelorre28' date='2020-12-14T04:31:21Z'>
		I have tried in colab with TF 2.2, 2.3, nightly version() and was able to reproduce the issue. Please, find the gist &lt;denchmark-link:https://colab.research.google.com/gist/ravikyram/5e41564c21c67bb16b6056d58c88e28c/untitled575.ipynb&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='3' author='guillaumelorre28' date='2020-12-21T05:04:00Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='4' author='guillaumelorre28' date='2020-12-21T18:33:43Z'>
		Without negating the loss in gradient_tape, the code works fine but it raises an error "No gradients provided for any variable" with the minus sign. This behavior is clearly not the one expected.
		</comment>
		<comment id='5' author='guillaumelorre28' date='2020-12-21T18:51:32Z'>
		
Without negating the loss in gradient_tape, the code works fine but it raises an error "No gradients provided for any variable" with the minus sign. This behavior is clearly not the one expected.

Why you don't negate the loss in the gradient_tape?
		</comment>
		<comment id='6' author='guillaumelorre28' date='2020-12-21T21:14:03Z'>
		I don't understand your question. Please elaborate.
		</comment>
		<comment id='7' author='guillaumelorre28' date='2020-12-21T21:27:40Z'>
		I meant to negate ​tf​.​reduce_mean​(​z​)
		</comment>
		<comment id='8' author='guillaumelorre28' date='2020-12-21T21:47:11Z'>
		Ok yes I understand. Because the last operation (multiplication by -1) is done outside of the  "with tf.GradientTape() as gen_tape:", the gradients cannot be computed.
Thanks
		</comment>
		<comment id='9' author='guillaumelorre28' date='2020-12-21T21:47:18Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45604&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45604&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>