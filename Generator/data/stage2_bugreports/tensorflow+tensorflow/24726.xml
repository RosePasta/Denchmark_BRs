<bug id='24726' author='ageron' open_date='2019-01-06T09:29:43Z' closed_time='2019-01-14T16:15:43Z'>
	<summary>PolymorphicFunction.get_concrete_function() creates a different concrete function for each python scalar value</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mac OS X 10.13.6
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
TensorFlow installed from (source or binary):
binary
TensorFlow version (use command below):
VERSION="1.13.0-dev20181226" (this is the TF 2.0-preview)
GIT_VERSION="b'v1.12.0-5133-gc343196842'"
Python version:
3.6.6
Bazel version (if compiling from source):
N/A
GCC/Compiler version (if compiling from source):
N/A
CUDA/cuDNN version:
N/A
GPU model and memory:
N/A

Describe the current behavior
tf.function returns a PolymorphicFunction.  When called, this function creates a concrete function to call for the specific types and shapes of the arguments, by calling PolymorphicFunction.get_concrete_function(), and subsequently, it reuses the same concrete function when it encounters the same types and shapes again.
Unfortunately, when passed python scalars or arrays of scalars, it seems to create a new function for each different value, instead of each different type (see code example below). Similarly, it creates a new function for [1.0, 2.0], and [3.0, 4.0].
I understand that there may be a good technical reason for this, but even if there is, I fear that users will pass Python scalar values or arrays (instead of tensors or NumPy arrays) to @tf.functions and they won't understand why their system blows up (slowed down by all the traces and new concrete functions piling up in RAM all the time).
Describe the expected behavior
I expect PolymorphicFunction.get_concrete_function() to return the same concrete function whether I pass it 1, or 2, or tf.constant(3) or tf.constant(4) as an argument.  I expect a different concrete function to be returned when I pass it [1.0], but this second concrete function should be the same as for the arguments [2.0], or tf.constant([3.0]), or tf.constant([4.0]) or np.array([5.0]), or or np.array([6.0])
Code to reproduce the issue
import tensorflow as tf
import numpy as np

@tf.function
def square(x):
    print("Calling")
    tf.get_logger().warning("Tracing")
    return tf.square(x)

print("Square of int32 scalars")
for i in range(3):
    square(tf.constant(i))
print("OKAY")
    
print()
print("Square of float32 scalars")
for i in range(3):
    square(tf.constant(i, dtype=tf.float32))
print("OKAY")

print()
print("Square of float32 arrays of shape [2]")
for i in range(3):
    square(tf.constant([i, i], dtype=tf.float32))
print("OKAY")

print()
print("Square of float32 NumPy arrays of shape [2]")
for i in range(3):
    square(tf.constant(np.array([i, i]), dtype=tf.float32))
print("OKAY")

print()
print("Square of python integers 1, 2, 3")
for i in range(3):
    square(i)
print("ERROR! Why is there one trace per call?")

print()
print("Square of python integers 1, 2, 3 (AGAIN)")
for i in range(3):
    square(i)
print("ERROR! No traces anymore, which means that one Function was cached for each specific integer 1, 2, 3.")

print()
print("Square of python integers arrays of shape [2]")
for i in range(3):
    square([i, i])
print("ERROR! Why is there one trace per call?")
Other info / logs
Here is the output of this program:
&lt;denchmark-code&gt;Square of int32 scalars
WARNING:tensorflow:Tracing
Calling
Calling
Calling
OKAY

Square of float32 scalars
WARNING:tensorflow:Tracing
Calling
Calling
Calling
OKAY

Square of float32 arrays of shape [2]
WARNING:tensorflow:Tracing
Calling
Calling
Calling
OKAY

Square of float32 NumPy arrays of shape [2]
Calling
Calling
Calling
OKAY

Square of python integers 1, 2, 3
WARNING:tensorflow:Tracing
Calling
WARNING:tensorflow:Tracing
Calling
WARNING:tensorflow:Tracing
Calling
ERROR! Why is there one trace per call?

Square of python integers 1, 2, 3 (AGAIN)
Calling
Calling
Calling
ERROR! No traces anymore, which means that one Function was cached for each specific integer 1, 2, 3.

Square of python integers arrays of shape [2]
WARNING:tensorflow:Tracing
Calling
WARNING:tensorflow:Tracing
Calling
WARNING:tensorflow:Tracing
Calling
ERROR! Why is there one trace per call?
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='ageron' date='2019-01-09T00:41:12Z'>
		Hello &lt;denchmark-link:https://github.com/martinwicke&gt;@martinwicke&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/ageron&gt;@ageron&lt;/denchmark-link&gt;
 has elucidated the polymorphic property of tf.function usage, and pointing to scalar and array arguments throwing warnings. If this intended, can this be noted? Thanks.
		</comment>
		<comment id='2' author='ageron' date='2019-01-09T01:22:58Z'>
		Thanks &lt;denchmark-link:https://github.com/msymp&gt;@msymp&lt;/denchmark-link&gt;
. My code produces the warnings to show when the function is being traced. My point is that ,  and  produce and cache three different concrete functions. This seems wasteful and counterintuitive. If I wrap the integers in , it has the expected behavior (just one concrete function is generated).
		</comment>
		<comment id='3' author='ageron' date='2019-01-12T08:02:40Z'>
		&lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
 I believe this is intentional (for optimization purposes) but I agree it needs to be very clear in the docs.
		</comment>
		<comment id='4' author='ageron' date='2019-01-12T09:18:44Z'>
		Thanks &lt;denchmark-link:https://github.com/martinwicke&gt;@martinwicke&lt;/denchmark-link&gt;
 , I was afraid it might be intentional.  This behavior is really surprising to me, and even now that I know it, I am concerned that I might unintentionally fall into this trap again. Perhaps it should be opt-in, somehow. Can you please give an example of where it might be useful?
		</comment>
		<comment id='5' author='ageron' date='2019-01-14T16:15:43Z'>
		Indeed it is intentional. It's a compromise to support the case where the python scalar is something like "num_layers" which dramatically affects the structure of the generated graph :-/
		</comment>
		<comment id='6' author='ageron' date='2019-01-15T00:42:49Z'>
		Hi &lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
 , oh I see, thanks for the example use case.  This probably deserves a very clear warning in the documentation, wdyt?
		</comment>
		<comment id='7' author='ageron' date='2019-01-15T00:50:52Z'>
		Good point about a warning in the documentation. Do you feel like writing
one, or do you want me to take a stab at it?
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Mon, Jan 14, 2019 at 4:46 PM Aurélien Geron ***@***.***&gt; wrote:
 Hi @alextp &lt;https://github.com/alextp&gt; , oh I see, thanks for the example
 use case. This probably deserves a very clear warning in the documentation,
 wdyt?

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#24726 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AAATxR3--SVROMYHGSh-kRqYfdZUYKLIks5vDSTJgaJpZM4ZyVze&gt;
 .


-- 
 - Alex

		</comment>
		<comment id='8' author='ageron' date='2019-01-15T01:17:31Z'>
		Hi &lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
, I won't have time in the next couple of days, but I can do this at the end of the week.
		</comment>
		<comment id='9' author='ageron' date='2019-01-15T16:18:21Z'>
		Sounds good to me. We're not cutting a release branch between now and then.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Mon, Jan 14, 2019 at 5:21 PM Aurélien Geron ***@***.***&gt; wrote:
 Hi @alextp &lt;https://github.com/alextp&gt;, I won't have time in the next
 couple of days, but I can do this at the end of the week.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#24726 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AAATxVVkwtqT7y8V3VxuoMkVOk9TpseAks5vDSz9gaJpZM4ZyVze&gt;
 .


-- 
 - Alex

		</comment>
		<comment id='10' author='ageron' date='2019-05-29T21:36:36Z'>
		Regarding the "warning in the documentation" discussed here:

https://www.tensorflow.org/api_docs/python/tf/contrib/eager/function doesn't have the warning that was going to be added in 1.13 (which is why I'm commenting on this specific bug);
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/function mentions the problem, but this paragraph (3rd) can't be called a warning. Bold text, color would help. A note that A MISTAKE HERE WILL EAT YOUR COMPUTER would be more explicit than "a new graph will be generated for each distinct value". Also, the section on Retracing is written to suggest TF may eventually become smart about not retracing too much. It should be noted in very clear terms that it is not yet smart at all about it.
https://www.tensorflow.org/alpha/guide/autograph doesn't have a mention of this (and has a nice "Use Python control flow" section, but lacks a "BUT DON'T USE PYTHON VALUES" section;
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/LIMITATIONS.md doesn't mention this either, and yes I can see it contains a different kind of table, but it is linked to by the tf.function tutorial as "Capabilities and Limitations" of tf.function and AutoGraph, and one could assume it is a fair summary of gotchas -- it isn't;
The RFC https://github.com/tensorflow/community/blob/master/rfcs/20180918-functions-not-sessions-20.md mentions the idea of a warning when too many traces are created, but I don't think the warning exists yet.

Also, more fundamentally, could this be changed to something of a tad saner compromise in TF2?
Currently, if you merely forget to wrap in a tensor a Python value, anywhere in your code, hell ensues and you are given no clue whatsoever. Not even: "We've allocated 16GB because we keep compiling your code over and over.. maybe check how function f() is called ... beware Python values ...", or better: some soft threshold on how many times you recompile the same function: "warning... f{} has been recompiled 1001 times ... maybe check for code calling it with non-Tensor argument values...". (Turns out the RFC also thought such a warning would make sense.)
It also makes writing code a pain, when the whole point of tf.function is to make it more Pythonic -- but it does pretty much the exact opposite because Python values should be reserved to very specific (and in practice, with complex code, quite rare) cases.
What about requiring a positive marker on values which should be considered constant with respect to graph instantiation? That is, require wrapping those Python values, and treat others like tensors?
tf.YesIDoWantYouToSpendThreeSecondsRecompilingThisCallAndAllocatingTonsOfRAMDoingSo(3), or maybe tf.GraphConstant(3) or whatever?
		</comment>
		<comment id='11' author='ageron' date='2019-05-29T21:57:10Z'>
		A warning after N retraces of the function (10? 100?) sounds good to me. Want to send a pull request?
Forcing wrapping of other python values doesn't.
I'd still like to find out a better way to inspect code and find out if it'll do something funny with tensor-convertible python values, but I haven't had the time to do this yet.
		</comment>
		<comment id='12' author='ageron' date='2019-05-29T22:56:16Z'>
		To be clear, I mean forcing wrapping of Python values with tf.GraphConstant() that are intended to be compile-time constants, and removing the need for wrapping as tensor every single Python value that changes a bit. I don't have data, but in my code it's 5% true constants, 95% variables.
Another problem with the need to wrap nearly-all Python values as tensors is that this applies only at function call boundaries.
So if your code works, and you say "I'd like to refactor the body of this loop using another tf.function", all of a sudden you better make sure you're indeed using tf.range() not range(), that you don't use integer variables to keep track of stuff, etc... and that's a fair bit of work in a language without type annotations. It's a barrier to code reorg.
Additionally, it's quite backwards: the possibility of optimization shouldn't be at the risk of turning your computer into a pile of molten metal. Hopefully that ship hasn't sailed. I mean, C eventually adopted "restrict" as an explicit keyword, after three decades of "we'll just make silent and subtle assumptions about memory aliasing and break shit – developers will just get it right, after all we have a footnote about this".
I'll submit a PR for a runtime warning in the tracing code and will include a reference to this issue.

I'd still like to find out a better way to inspect code and find out if it'll do something funny with tensor-convertible python values, but I haven't had the time to do this yet.

What do you mean? Do you suggest deriving, through code analysis, that a Python value is clearly not a constant (e.g. return value of range(variable) ...) and therefore do not create a new graph whenever it changes? Generally speaking, through static analysis one could estimate a range / set of possible values for a given variable, and only create new graphs for new values of variables deemed to be nearly-constant, i.e. you can prove that the set of possible values is small. But that immediately introduces the need for a way to override the static analyzer if it gets it wrong. And in this line of work, the static analysis is necessarily conservative. Hence the need for a tf.GraphConstant annotation...
It's also counter-intuitive on this level: the history of compilers is full of constant analysis, and it's not particularly hard, not the other way around.
In any case, this could be done dynamically: instead of a runtime warning, blacklist a function from further retracing if it gets retraced too many times (and maybe still print a warning)? I'll create a second PR for that too, if I can find a sane way to do it.
		</comment>
		<comment id='13' author='ageron' date='2019-05-30T03:16:35Z'>
		I have given a few TensorFlow 2.0 courses, and I can confirm that this behavior surprised every single student (&lt;denchmark-link:https://github.com/ericrannaud&gt;@ericrannaud&lt;/denchmark-link&gt;
 , the "pile of molten metal" made me laugh).
I was also in favor of dynamic-by-default, but I think the ship has sailed, and a warning sounds like the best option, ideally pointing to the problematic argument(s), and perhaps even a pointer to the line of code where tf.function is called. Thanks for your contribution &lt;denchmark-link:https://github.com/ericrannaud&gt;@ericrannaud&lt;/denchmark-link&gt;
. Big warnings in the docs would also be very helpful.
		</comment>
	</comments>
</bug>