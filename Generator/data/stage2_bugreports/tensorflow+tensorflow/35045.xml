<bug id='35045' author='LCorleone' open_date='2019-12-12T05:07:00Z' closed_time='2020-01-13T18:30:27Z'>
	<summary>add_loss bug when using tf.keras, but it is ok using keras.</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04
TensorFlow version (use command below):
tensorflow2.0.0 keras 2.3.1
Python version: 3.6
CUDA/cuDNN version: 10.0
GPU model and memory: 1080ti

Describe the current behavior
When i use keras, the add_loss behavior is all okay. But when i change to tf.keras, the following error
is occured.
I suppose that maybe the eager cause this problem, but it still occurs when i disable the eager.
So, any ideas? Thanks a lot.
2019-12-12 13:02:42.334129: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: You must feed a value for placeholder tensor 'Input-Segment' with dtype float and shape [?,?]
[[{{node Input-Segment}}]]
2019-12-12 13:02:42.334488: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: You must feed a value for placeholder tensor 'Input-Segment' with dtype float and shape [?,?]
[[{{node Input-Segment}}]]
[[Embedding-Token/Cast/_8]]
Traceback (most recent call last):
File "test.py", line 33, in 
model.add_loss(cross_entropy)
File "/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 1132, in add_loss
self._graph_network_add_loss(symbolic_loss)
File "/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py", line 1426, in _graph_network_add_loss
new_nodes, new_layers = _map_subgraph_network(self.inputs, [symbolic_loss])
File "/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py", line 1651, in _map_subgraph_network
base_layer_utils.create_keras_history(outputs)
File "/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py", line 184, in create_keras_history
_, created_layers = _create_keras_history_helper(tensors, set(), [])
File "/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py", line 231, in _create_keras_history_helper
layer_inputs, processed_ops, created_layers)
File "/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py", line 231, in _create_keras_history_helper
layer_inputs, processed_ops, created_layers)
File "/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py", line 231, in _create_keras_history_helper
layer_inputs, processed_ops, created_layers)
File "/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py", line 229, in _create_keras_history_helper
constants[i] = backend.function([], op_input)([])
File "/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py", line 3740, in call
outputs = self._graph_fn(*converted_inputs)
File "/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 1081, in call
return self._call_impl(args, kwargs)
File "/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 1121, in _call_impl
return self._call_flat(args, self.captured_inputs, cancellation_manager)
File "/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 1224, in _call_flat
ctx, args, cancellation_manager=cancellation_manager)
File "/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 511, in call
ctx=ctx)
File "/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py", line 67, in quick_execute
six.raise_from(core._status_to_exception(e.code, message), None)
File "", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError:  You must feed a value for placeholder tensor 'Input-Segment' with dtype float and shape [?,?]
[[node Input-Segment (defined at /home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_keras_scratch_graph_190]
Function call stack:
keras_scratch_graph
Code to reproduce the issue
&lt;denchmark-code&gt;import tensorflow.keras as keras
import tensorflow.keras.backend as K
import tensorflow as tf
tf.compat.v1.disable_eager_execution()
# import keras
# import keras.backend as K
x_in = keras.layers.Input(shape=(None, ), name='Input-Token')
s_in = keras.layers.Input(shape=(None, ), name='Input-Segment')
x, s = x_in, s_in
# Embedding
x = keras.layers.Embedding(input_dim=12,
                           output_dim=12,
                           name='Embedding-Token')(x)
s = keras.layers.Embedding(input_dim=12,
                           output_dim=12,
                           name='Embedding-Segment')(s)
x = keras.layers.Add(name='Embedding-Token-Segment')([x, s])
model = keras.models.Model([x_in, s_in], x)
model.summary()
y_in = model.input[0][:, 1:]
y_mask = model.input[1][:, 1:]
y = model.output[:, :-1]
cross_entropy = K.sparse_categorical_crossentropy(y_in, y)
model.add_loss(cross_entropy)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='LCorleone' date='2019-12-13T06:27:16Z'>
		Issue is replicating with Tf 2.0.
Please find the colab gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/3436d5fac7808a3fec8a598bf2685ec4/untitled301.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='LCorleone' date='2019-12-16T20:26:50Z'>
		You may try to wrap the sparse op in a Lambda layer. Something like;
import tensorflow.keras as keras
import tensorflow.keras.backend as K
import tensorflow as tf
tf.compat.v1.disable_eager_execution()
# import keras
# import keras.backend as K
x_in = keras.layers.Input(shape=(None, ), name='Input-Token')
s_in = keras.layers.Input(shape=(None, ), name='Input-Segment')
x, s = x_in, s_in
# Embedding
x = keras.layers.Embedding(input_dim=12,
                           output_dim=12,
                           name='Embedding-Token')(x)
s = keras.layers.Embedding(input_dim=12,
                           output_dim=12,
                           name='Embedding-Segment')(s)
x = keras.layers.Add(name='Embedding-Token-Segment')([x, s])
model = keras.models.Model([x_in, s_in], x)
model.summary()
y_in = model.input[0][:, 1:]
y_mask = model.input[1][:, 1:]
y = model.output[:, :-1]
cross_entropy_loss = lambda x: K.sparse_categorical_crossentropy(y_in, y)
cross_entropy_loss  = keras.layers.Lambda(cross_entropy)    
model.add_loss(cross_entropy_loss)
		</comment>
		<comment id='3' author='LCorleone' date='2019-12-18T07:54:00Z'>
		&lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;
 thanks a lot.
Your original code cannot compile because the layer need an input. I modify your code like this.
&lt;denchmark-code&gt;import tensorflow.keras as keras
import tensorflow.keras.backend as K
import tensorflow as tf
# tf.compat.v1.disable_eager_execution()
# import keras
# import keras.backend as K

x_in = keras.layers.Input(shape=(None, ), name='Input-Token')
s_in = keras.layers.Input(shape=(None, ), name='Input-Segment')
x, s = x_in, s_in
# Embedding
x = keras.layers.Embedding(input_dim=12,
                           output_dim=12,
                           name='Embedding-Token')(x)
s = keras.layers.Embedding(input_dim=12,
                           output_dim=12,
                           name='Embedding-Segment')(s)
x = keras.layers.Add(name='Embedding-Token-Segment')([x, s])
model = keras.models.Model([x_in, s_in], x)
model.summary()

y_in = model.input[0][:, 1:]
y_mask = model.input[1][:, 1:]
y = model.output[:, :-1]
# cross_entropy = K.sparse_categorical_crossentropy(y_in, y)
def cross_entropy_func(x):
    out = K.sparse_categorical_crossentropy(x[0], x[1])
    return out

cross_entropy_loss = keras.layers.Lambda(cross_entropy_func)([y_in, y])
model.add_loss(cross_entropy_loss)
model.compile(optimizer='adam')
&lt;/denchmark-code&gt;

Although this operation can add loss successfully, but when compile and fit generator, another error comes out.
File "/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py", line 1297, in fit_generator
steps_name='steps_per_epoch')
File "/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_generator.py", line 265, in model_iteration
batch_outs = batch_function(*batch_data)
File "/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py", line 973, in train_on_batch
class_weight=class_weight, reset_metrics=reset_metrics)
File "/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py", line 264, in train_on_batch
output_loss_metrics=model._output_loss_metrics)
File "/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py", line 311, in train_on_batch
output_loss_metrics=output_loss_metrics))
File "/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py", line 268, in _process_single_batch
grads = tape.gradient(scaled_total_loss, trainable_weights)
File "/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py", line 1014, in gradient
unconnected_gradients=unconnected_gradients)
File "/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/eager/imperative_grad.py", line 76, in imperative_grad
compat.as_str(unconnected_gradients.value))
File "/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py", line 138, in _gradient_function
return grad_fn(mock_op, *out_grads)
File "/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py", line 199, in _SumGrad
output_shape_kept_dims = math_ops.reduced_shape(input_shape, op.inputs[1])
File "/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/ops/math_ops.py", line 3490, in reduced_shape
input_shape = input_shape.numpy()
AttributeError: 'Tensor' object has no attribute 'numpy'
I cannot figure it out. I think warp the loss into a Lambda layer may be not a solution.
		</comment>
		<comment id='4' author='LCorleone' date='2020-01-02T17:58:56Z'>
		Apologies for the delay in response. I successfully executed your new code in TF nightly version '2.1.0-dev20200102' . You may use google colab for a quick test. Can you please confirm?
		</comment>
		<comment id='5' author='LCorleone' date='2020-01-13T18:30:29Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35045&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35045&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='LCorleone' date='2020-10-23T11:11:27Z'>
		So, what is the final solution? I have the same problem, too.
		</comment>
		<comment id='7' author='LCorleone' date='2020-12-07T16:05:53Z'>
		
Issue is replicating with Tf 2.0.
Please find the colab gist here. Thanks!

Got the same problem when using customized loss as OP's for VAE by tf 2.3, but OK for tf 1.15.2
and it is OK using Lambda layer for tf 1.15.2 but NOT for tf 2.3
		</comment>
		<comment id='8' author='LCorleone' date='2020-12-07T16:36:27Z'>
		
You may try to wrap the sparse op in a Lambda layer. Something like;
import tensorflow.keras as keras
import tensorflow.keras.backend as K
import tensorflow as tf
tf.compat.v1.disable_eager_execution()
# import keras
# import keras.backend as K
x_in = keras.layers.Input(shape=(None, ), name='Input-Token')
s_in = keras.layers.Input(shape=(None, ), name='Input-Segment')
x, s = x_in, s_in
# Embedding
x = keras.layers.Embedding(input_dim=12,
                           output_dim=12,
                           name='Embedding-Token')(x)
s = keras.layers.Embedding(input_dim=12,
                           output_dim=12,
                           name='Embedding-Segment')(s)
x = keras.layers.Add(name='Embedding-Token-Segment')([x, s])
model = keras.models.Model([x_in, s_in], x)
model.summary()
y_in = model.input[0][:, 1:]
y_mask = model.input[1][:, 1:]
y = model.output[:, :-1]
cross_entropy_loss = lambda x: K.sparse_categorical_crossentropy(y_in, y)
cross_entropy_loss  = keras.layers.Lambda(cross_entropy)    
model.add_loss(cross_entropy_loss)

THIS DOES NOT WORK!
		</comment>
	</comments>
</bug>