<bug id='31842' author='akanyaani' open_date='2019-08-21T14:09:39Z' closed_time='2020-04-06T10:50:06Z'>
	<summary>Tensorflow2.0 Training OOM when tf.function retraces excessively</summary>
	<description>
When I am training my model using tf.function CPU memory is leaking and after some steps of training it is getting killed.
System information

OS Platform: - Linux Ubuntu 16.04):
TensorFlow installed from (source or binary): binary
TensorFlow version: 2.0-beta
Python version: 3.7
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 10.01
GPU model and memory: p100, 16GB

&lt;denchmark-code&gt;@tf.function
def train_step(self, x, step, grad_clip=True, clip_value=1.0):
    with tf.name_scope("input_data"):
        inputs = x[:, :-1]
        targets = x[:, 1:]

     with tf.GradientTape() as tape:
          predictions, _ = model(inputs, training=True)
          loss = self.get_loss(targets, predictions)

      with tf.name_scope("gradients"):
          gradients = tape.gradient(loss, self.trainable_variables)
          if grad_clip:
              gradients = [(tf.clip_by_value(grad, -clip_value, clip_value))
                             for grad in gradients]
          self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))

        accuracy = self.get_padded_accuracy(targets, predictions)
        assert self.train_writer is not None
        with tf.name_scope("summary_writer"):
            with self.train_writer.as_default():
                tf.summary.scalar("loss", loss, step=step)
                tf.summary.scalar("accuracy", accuracy, step=step)
       return loss, accuracy

**Stats of CPU memory
After 100 steps - 5.08 GB
After 500 steps -  14.8 GB
After 1000 steps - 27 GB**
&lt;/denchmark-code&gt;

When is graph mode of training using tf.function CPU memory gets leaked and after some steps, it gets killed but when I train my model using eager mode it works fine, I am using above code for graph mode training as recommended by TensorFlow 2.0 community.
	</description>
	<comments>
		<comment id='1' author='akanyaani' date='2019-08-21T18:04:24Z'>
		Hi &lt;denchmark-link:https://github.com/akanyaani&gt;@akanyaani&lt;/denchmark-link&gt;
, is the shape of  varies between each batch?
		</comment>
		<comment id='2' author='akanyaani' date='2019-08-21T18:11:34Z'>
		Yes, The shape of x varies between each batch.
		</comment>
		<comment id='3' author='akanyaani' date='2019-08-21T18:25:48Z'>
		That's the reason why! Because  will re-trace the graph due to the variable sequence lengths or variable batch sizes, the number of cached graphs will increase over time if it do not see this shape before. Thus OOM occurred. To avoid re-tracing, you might want to specify the . For detailed explanation, please refer to
&lt;denchmark-link:https://www.tensorflow.org/beta/tutorials/eager/tf_function&gt;https://www.tensorflow.org/beta/tutorials/eager/tf_function&lt;/denchmark-link&gt;

&lt;denchmark-link:https://www.tensorflow.org/beta/tutorials/text/transformer&gt;https://www.tensorflow.org/beta/tutorials/text/transformer&lt;/denchmark-link&gt;
 (search )
		</comment>
		<comment id='4' author='akanyaani' date='2019-08-21T18:31:39Z'>
		Thanks for your prompt response.
But for that I was calling this method for prevent tensorflow to create new graph tf.compat.v1.get_default_graph().finalize()
But let me try with the input signature
		</comment>
		<comment id='5' author='akanyaani' date='2019-08-21T19:42:33Z'>
		It tried with @tf.function(input_signature=[tf.TensorSpec(shape=(None, None), dtype=tf.int32)]) with variable sequence length but still, it is happening, but when I run with a fix sequence length it works fine but it will cost extra computation.
		</comment>
		<comment id='6' author='akanyaani' date='2019-08-22T06:15:04Z'>
		&lt;denchmark-link:https://github.com/akanyaani&gt;@akanyaani&lt;/denchmark-link&gt;
 ,
Can you please provide complete code snippet to reproduce the issue reported here.Thanks!
		</comment>
		<comment id='7' author='akanyaani' date='2019-08-23T06:32:08Z'>
		focus on
		</comment>
		<comment id='8' author='akanyaani' date='2019-08-23T06:55:00Z'>
		Hi,
My model has transformer-based encoder-decoder as explained here.
&lt;denchmark-link:url&gt;https://www.tensorflow.org/beta/tutorials/text/transformer&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='akanyaani' date='2019-08-27T05:40:24Z'>
		Hi &lt;denchmark-link:https://github.com/akanyaani&gt;@akanyaani&lt;/denchmark-link&gt;
 , Could you provide the complete code (ideally minimized) that we can run and reproduce?  Thanks.
		</comment>
		<comment id='10' author='akanyaani' date='2019-08-27T06:00:54Z'>
		Also, if you have been passing Python arguments, you can try passing Tensors instead &lt;denchmark-link:https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args&gt;https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='akanyaani' date='2019-08-27T17:35:11Z'>
		Hi, &lt;denchmark-link:https://github.com/kkimdev&gt;@kkimdev&lt;/denchmark-link&gt;

You can produce the error using this repo by making dynamic batch padding.

&lt;denchmark-link:https://github.com/akanyaani/gpt-2-tensorflow2.0&gt;https://github.com/akanyaani/gpt-2-tensorflow2.0&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='akanyaani' date='2019-09-26T15:21:48Z'>
		&lt;denchmark-link:https://github.com/akanyaani&gt;@akanyaani&lt;/denchmark-link&gt;
, it looks like the general suspicion is that subsequent calls to  cause the function to be re-traced (a slow process that should only happen once or twice), likely due to the parameter configuration. For example, it could be because the shape of  varies, but it could also depend on the value of .
I'm not sure I can completely follow your instructions for reproducing it, but here is a way to diagnose this issue: add a print call to train_step, like so:
&lt;denchmark-code&gt;def train_step(self, x, step, grad_clip=True, clip_value=1.0):
    print('tracing', x, step, grad_clip, clip_value)
    with tf.name_scope("input_data"):
      ...
&lt;/denchmark-code&gt;

Since the pure Python functions like print are only called during tracing, it's an effective way to tell whether the function is re-traced too much. If it does, it also helps explain why.
We've just updated the API docs as well with a bit more relevant information, it's worth checking out here: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/eager/def_function.py#L989&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/eager/def_function.py#L989&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='akanyaani' date='2019-10-10T10:00:03Z'>
		Hi &lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
, Sorry for my late response, Actually I tried with one parameter also but the same thing was happening. So I found that the sequence length of x was the culprit. It was working fine with fix sequence length. And I have noticed that if you restart training with different batch size same problem happens.
&lt;denchmark-code&gt;def train_step(self, x):
    with tf.name_scope("input_data"):
&lt;/denchmark-code&gt;

		</comment>
		<comment id='14' author='akanyaani' date='2019-10-10T13:42:41Z'>
		Thanks! Just to double-check, you are still seeing the leak if you try:
&lt;denchmark-code&gt;@tf.function(input_signature=[tf.TensorSpec(shape=(None, None), dtype=tf.int32)])
def train_step(self, x):
    print('tracing')
    with tf.name_scope("input_data"):
&lt;/denchmark-code&gt;

And you see just one "tracing" message at the console?
		</comment>
		<comment id='15' author='akanyaani' date='2019-11-21T05:29:28Z'>
		I met the same problem. Did you solve the same problem?
		</comment>
		<comment id='16' author='akanyaani' date='2019-11-21T07:53:41Z'>
		Hi &lt;denchmark-link:https://github.com/zxk19981227&gt;@zxk19981227&lt;/denchmark-link&gt;
,
Can you give me some idea about your model architecture, then I will be able to give you the answer.
Thanks
		</comment>
		<comment id='17' author='akanyaani' date='2020-03-08T10:30:38Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
 I have use “print” in train_step(), it shows that each epoch has a "print" work. and my code killed because of OOM.
are there some solutions for it?
thank you in advance.
		</comment>
		<comment id='18' author='akanyaani' date='2020-03-09T01:48:08Z'>
		I'm sorry  to hear that. But I solved my problems by turning off auto
trace. Maybe your problem is not the 'print' but the auto trace. You can
try this.
I hope it helps

anna &lt;notifications@github.com&gt; 于2020年3月8日周日 下午6:30写道：
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


 @mdanatg &lt;https://github.com/mdanatg&gt; I have use “print” in train_step(),
 it shows that each epoch has a "print" work. and my code killed because of
 OOM.

 are there some solutions for it?

 thank you in advance.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#31842?email_source=notifications&amp;email_token=AJTMZW4BHJKNCR5OCKDNWXDRGNXVLA5CNFSM4IOHPLUKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEOESGFY#issuecomment-596189975&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/AJTMZW3KTZJJQKQFWOLYSDTRGNXVLANCNFSM4IOHPLUA&gt;
 .



		</comment>
		<comment id='19' author='akanyaani' date='2020-03-19T14:14:09Z'>
		&lt;denchmark-link:https://github.com/akanyaani&gt;@akanyaani&lt;/denchmark-link&gt;
 ,
Could you please update on the above comment, let us know if the issue still exist.
		</comment>
		<comment id='20' author='akanyaani' date='2020-03-29T07:32:31Z'>
		&lt;denchmark-link:https://github.com/akanyaani&gt;@akanyaani&lt;/denchmark-link&gt;
 ,
Could you please update on the above comment
		</comment>
		<comment id='21' author='akanyaani' date='2020-04-06T10:50:06Z'>
		&lt;denchmark-link:https://github.com/akanyaani&gt;@akanyaani&lt;/denchmark-link&gt;
 ,
Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
		<comment id='22' author='akanyaani' date='2020-04-06T10:50:07Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31842&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31842&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='23' author='akanyaani' date='2020-05-24T10:01:16Z'>
		
That's the reason why! Because tf.function will re-trace the graph due to the variable sequence lengths or variable batch sizes, the number of cached graphs will increase over time if it do not see this shape before. Thus OOM occurred. To avoid re-tracing, you might want to specify the input_signature. For detailed explanation, please refer to
https://www.tensorflow.org/beta/tutorials/eager/tf_function
https://www.tensorflow.org/beta/tutorials/text/transformer (search tf.function)

Great, it helps. Could you please add a warning in this function to alert users?
		</comment>
	</comments>
</bug>