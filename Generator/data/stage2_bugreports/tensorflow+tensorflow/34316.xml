<bug id='34316' author='georgesterpu' open_date='2019-11-15T16:46:39Z' closed_time='2019-11-18T21:26:22Z'>
	<summary>tf.function issue after replacing the optimiser</summary>
	<description>
System information

Have I written custom code: no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Manjaro linux testing
TensorFlow installed from (source or binary): pypi binary
TensorFlow version (use command below): v1.12.1-16854-g6778662 2.1.0-dev20191028
Python version: 3.7.4
CUDA/cuDNN version: 10.1.243 / 7.6.4.38
GPU model and memory: GeForce GTX 1080, 8GB

Describe the current behavior
I am training a model using multiple optimisers and am getting an error when the train function is decorated with @tf.function.
Describe the expected behavior
I expected the converted function to be invariant of the optimiser instance.
Code to reproduce the issue
&lt;denchmark-code&gt;import tensorflow as tf


class MyModel(tf.keras.Model):
    def __init__(self):
        super(MyModel, self).__init__()

        self.x = self.add_weight(shape=[1,], dtype=tf.float32)

    def call(self, inputs):
        return inputs * self.x

model = MyModel()
optimiser = tf.keras.optimizers.Adam(0.1)

@tf.function
def train():
    with tf.GradientTape() as tape:
        output = model([1.0])
        loss = output

    grads = tape.gradient(loss, model.trainable_variables)
    optimiser.apply_gradients(zip(grads, model.trainable_variables))


train()
print('First call ok')
optimiser = tf.keras.optimizers.Adam(0.2)
train()
print('Second call ok')
&lt;/denchmark-code&gt;

Other info / logs
&lt;denchmark-code&gt;tensorflow.python.framework.errors_impl.FailedPreconditionError:  Error while reading resource variable Adam/beta_2_33 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/Adam/beta_2_33/N10tensorflow3VarE does not exist.
	 [[node Adam/Cast_3/ReadVariableOp]] [Op:__inference_train_135]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='georgesterpu' date='2019-11-18T10:32:16Z'>
		Could reproduce the error with TF Nightly Version. Here is the &lt;denchmark-link:https://colab.sandbox.google.com/gist/rmothukuru/786d873d44f8e135ac00802e2f16ff69/34316.ipynb&gt;Gist&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='georgesterpu' date='2019-11-18T20:55:00Z'>
		Hi, unfortunately, when @tf.function is traced (the first call), the first optimizer is captured and will still try to use that in the subsequent calls.  Another way to get around is defining tf.function for each.
import tensorflow.compat.v2 as tf
tf.enable_v2_behavior()

class MyModel(tf.keras.Model):
    def __init__(self):
        super(MyModel, self).__init__()

        self.x = self.add_weight(shape=[1,], dtype=tf.float32)

    def call(self, inputs):
        return inputs * self.x

model = MyModel()
optimizer = tf.keras.optimizers.Adam(0.1)

def train():
    with tf.GradientTape() as tape:
        output = model([1.0])
        loss = output

    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))


tf.function(train)()
print('First call ok')
optimizer = tf.keras.optimizers.Adam(0.2)
tf.function(train)()
print('Second call ok')
		</comment>
		<comment id='3' author='georgesterpu' date='2019-11-18T21:26:19Z'>
		I understand, thanks.
Is there a better way of manually changing the learning rate of an optimiser (and optionally resetting the moments) than creating a new instance ? I see a _set_hyper method, but looks protected.
		</comment>
		<comment id='4' author='georgesterpu' date='2019-11-18T21:26:24Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34316&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34316&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='georgesterpu' date='2019-11-19T00:41:03Z'>
		Yes you can also dynamically set it. Here is the code 


tensorflow/tensorflow/python/keras/optimizer_v2/optimizer_v2.py


         Line 556
      in
      bf9c196






 def __setattr__(self, name, value): 





optimizer = tf.keras.optimizers.Adam(0.2)
optimizer.lr = 0.5
optimizer.get_config()
#{'name': 'Adam',
# 'learning_rate': 0.5,
# 'decay': 0.0,
# 'beta_1': 0.9,
# 'beta_2': 0.999,
# 'epsilon': 1e-07,
# 'amsgrad': False}
		</comment>
	</comments>
</bug>