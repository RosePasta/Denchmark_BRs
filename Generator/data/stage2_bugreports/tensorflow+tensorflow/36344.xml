<bug id='36344' author='nmatare' open_date='2020-01-30T18:31:24Z' closed_time='2020-03-05T23:54:58Z'>
	<summary>Allow tf.estimator.Estimators to use CuDNN layers</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
N/A
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
N/A
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
TensorFlow installed from (source or binary):
source
TensorFlow version (use command below):
2.0+
Python version:
3.7
Bazel version (if compiling from source):
N/A
GCC/Compiler version (if compiling from source):
N/A
CUDA/cuDNN version:
N/A
GPU model and memory:
N/A

Describe the current behavior
tf.estimator.Estimators cannot use CuDNN layers
Describe the expected behavior
tf.estimator.Estimators implement CuDNN optimized layers
Code to reproduce the issue
N/A
Other info / logs
Custom tf.estimator.Estimators, those that implement a user defined model_fn, will not allow for the utilization of CuDNN layers -- like the layers found in tf.keras.layers.LSTM
The TensorFlow 2.0 documents state, &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#eager_compatibility&gt;"Calling methods of Estimator will work while eager execution is enabled. However, the model_fn and input_fn is not executed eagerly, Estimator will switch to graph mode before calling all user-provided functions (incl. hooks), so their code has to be compatible with graph mode execution. Note that input_fn code using tf.data generally works in both graph and eager modes."&lt;/denchmark-link&gt;

Thereby, if a user tries to use CuDNN optimized layers, ops.executing_eagerly_outside_functions() will never trigger while the layer is being initialized.



tensorflow/tensorflow/python/keras/layers/recurrent_v2.py


         Line 366
      in
      cf7fcf1






 self.could_use_cudnn = ( 





It would be great to use CuDNN layers with custom estimators, or even with the prebuilt tf.estimator.RNNEstimator class.
As a temporary workaround, I force the self.could_use_cudnn to True after initialization and proceed to build the estimator with CuDNN layers. (Ensuring that all other parameters are rationalized) However, I get the below warning:

[1,0]:2020-01-30 17:39:33.939905: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_6654_7125' and '__inference___backward_cudnn_lstm_with_fallback_6109_6289_specialized_for_training_gradients_lstm_StatefulPartitionedCall_grad_StatefulPartitionedCall_at_tf_graph' both implement 'lstm_b42b1bb1-dad6-4430-b5f1-d964cdf2b5a3' but their signatures do not match.

Aside from the warning report, this works as expected.
	</description>
	<comments>
		<comment id='1' author='nmatare' date='2020-01-31T06:14:41Z'>
		&lt;denchmark-link:https://github.com/nmatare&gt;@nmatare&lt;/denchmark-link&gt;

Can you please provide simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!
		</comment>
		<comment id='2' author='nmatare' date='2020-02-10T12:03:42Z'>
		&lt;denchmark-link:https://github.com/nmatare&gt;@nmatare&lt;/denchmark-link&gt;

Any update on this issue please. Thanks!
		</comment>
		<comment id='3' author='nmatare' date='2020-02-11T00:55:46Z'>
		Apologies on the delay. Please see the below failing assert statement:
&lt;denchmark-code&gt;import tensorflow as tf

def model_fn(features, labels, mode):
        # https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM
	# "The requirements to use the cuDNN implementation are:
	# activation == tanh
	# recurrent_activation == sigmoid
	# recurrent_dropout == 0
	# unroll is False
	# use_bias is True
	# Inputs are not masked or strictly right padded."
	lstm_layer = tf.keras.layers.LSTM(
		units=64,
		activation='tanh',
		recurrent_activation='sigmoid',
		recurrent_dropout=0,
		unroll=False,
		use_bias=True,
	)

	assert lstm_layer.could_use_cudnn is True, 'This should pass'

	logits = tf.keras.layers.Dense(units=1, activation=None)(lstm_layer)

	head = tf.estimator.MultiClassHead()

	optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.001)

	loss = tf.compat.v1.losses.sparse_softmax_cross_entropy(
		labels=labels,
		logits=logits,
		reduction=tf.compat.v1.losses.Reduction.NONE,
	)

	step = tf.compat.v1.train.get_or_create_global_step()

	return head.create_estimator_spec(
		train_op_fn=lambda: optimizer.minimize(loss, step),
		features=features,
		labels=labels,
		mode=mode,
		logits=logits,
	)

# Please see comments from above on line 366 for tensorflow/tensorflow/python/keras/layers/recurrent_v2.py
assert tf.executing_eagerly() is True

def train_input_fn():
	features = tf.constant([[1, 3], [2, 1], [3, 3]])
	labels = tf.constant(['A', 'B', 'A'])
	return tf.data.Dataset.from_tensor_slices((features, labels)) 

classifier = tf.estimator.Estimator(model_fn=model_fn)
classifier.train(input_fn=lambda: train_input_fn(), steps=500)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='nmatare' date='2020-02-11T11:30:06Z'>
		&lt;denchmark-link:https://github.com/nmatare&gt;@nmatare&lt;/denchmark-link&gt;

I tried in colab with TF 2.0 and i am seeing  and I tried in colab with TF 2.2.0-dev20200211 and i am seeing
'PLease, find the gist &lt;denchmark-link:https://colab.research.google.com/gist/ravikyram/f455ca020a66d9d36d2bc60fe4fdb0b6/untitled638.ipynb&gt;here&lt;/denchmark-link&gt;
.Is this the expected behavior. Thanks!
		</comment>
		<comment id='5' author='nmatare' date='2020-02-11T16:58:02Z'>
		It appears the attribute  could_use_cudnn was changed to _could_use_gpu_kernel  for TF 2.2.0. When running the notebook with TF 2.2.0-dev20200211,
please change assert lstm_layer.could_use_cudnn is True, to assert lstm_layer._could_use_gpu_kernel is True, the underlying logic appears to have remained the same.
		</comment>
		<comment id='6' author='nmatare' date='2020-03-05T23:54:58Z'>
		Estimators do not distribute to GPUs natively, and we do not offer support for Estimators on GPU at this time. If you would like to use Keras Layers with GPU, we would encourage you to use Keras models with Distribution Strategies for GPUs: &lt;denchmark-link:https://www.tensorflow.org/guide/distributed_training&gt;https://www.tensorflow.org/guide/distributed_training&lt;/denchmark-link&gt;

It is possible that workarounds exist here, but because we do not explicitly support this use-case, Stack Overflow may be a better place to seek help on this.
		</comment>
		<comment id='7' author='nmatare' date='2020-03-05T23:55:00Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36344&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36344&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>