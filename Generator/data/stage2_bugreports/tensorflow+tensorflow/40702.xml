<bug id='40702' author='tzekid' open_date='2020-06-23T05:14:14Z' closed_time='2020-07-04T20:45:46Z'>
	<summary>Session crashes when I use TFLiteConverter with tf.lite.OpsSet.TFLITE_BUILTINS_INT8 option</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Colaboratory (Ubuntu 18.04.2 LTS)
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
TensorFlow installed from (source or binary): binary, nightly
TensorFlow version (use command below): '2.3.0-dev20200622'
Python version: Python 3.6.7
Bazel version (if compiling from source): NA
GCC/Compiler version (if compiling from source): NA
CUDA/cuDNN version: CUDA v10.0.130, cuDNN 7.6.0
GPU model and memory: K80

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with:


TF 1.0: python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"


TF 2.0: python -c "import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"


AttributeError: module 'tensorflow' has no attribute 'GIT_VERSION'


v1.12.1-34793-g072cf7ee4b 2.3.0-dev20200622


Describe the current behavior
I'm trying to convert and quantize a mask_rcnn_inceptionv2 model from the model zoo for use on a Coral USB Accelerator and the Colab runtime crashes.
Describe the expected behavior
Successful conversion w/o any crashes.
Standalone code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
import tensorflow as tf
import tarfile
import pathlib

model_name = "mask_rcnn_inception_v2_coco_2018_01_28"
base_url = 'http://download.tensorflow.org/models/object_detection/'
model_file = model_name + '.tar.gz'
model_dir = tf.keras.utils.get_file(fname=model_name, origin=base_url + model_file, untar=True)
model_dir = pathlib.Path(model_dir)/"saved_model"
print('Saved to {}'.format(model_dir))

export_dir = "/root/.keras/datasets/mask_rcnn_inception_v2_coco_2018_01_28/saved_model"
converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)
converter.experimental_new_converter=True
converter.optimizations = [tf.lite.Optimize.DEFAULT]
def representative_dataset_gen():
  for _ in range(num_calibration_steps):
    yield [input]
converter.representative_dataset = representative_dataset_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_output_type = tf.int8 
tflite_quant_model = converter.convert() # crashes on this line
open("converted_model_quant.tflite", "wb").write(tflite_quant_model)
Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
&lt;denchmark-code&gt;
Jun 23, 2020, 7:09:41 AM | INFO | Discarding 6 buffered messages for 5081aeef-6fb9-46a7-a45d-3af457c9a931:aed26b5d8fdf4925f295f4d907ca617f
-- | -- | --
Jun 23, 2020, 7:09:41 AM | INFO | Adapting to protocol v5.1 for kernel 5081aeef-6fb9-46a7-a45d-3af457c9a931
Jun 23, 2020, 7:09:39 AM | INFO | Starting buffering for 5081aeef-6fb9-46a7-a45d-3af457c9a931:aed26b5d8fdf4925f295f4d907ca617f
Jun 23, 2020, 7:09:39 AM | WARNING | zmq message arrived on closed channel
Jun 23, 2020, 7:09:11 AM | WARNING | tf.TensorArrayWriteV3 {device = ""}
Jun 23, 2020, 7:09:11 AM | WARNING | tf.TensorArrayV3 {clear_after_read = true, device = "", dtype = i32, dynamic_size = false, element_shape = #tf.shape&lt;*&gt;, identical_element_shapes = true, tensor_array_name = ""}
Jun 23, 2020, 7:09:11 AM | WARNING | tf.TensorArrayV3 {clear_after_read = true, device = "", dtype = f32, dynamic_size = false, element_shape = #tf.shape&lt;*&gt;, identical_element_shapes = true, tensor_array_name = ""}
Jun 23, 2020, 7:09:11 AM | WARNING | tf.TensorArraySizeV3 {device = ""}
Jun 23, 2020, 7:09:11 AM | WARNING | tf.TensorArrayScatterV3 {device = ""}
Jun 23, 2020, 7:09:11 AM | WARNING | tf.TensorArrayReadV3 {device = ""}
Jun 23, 2020, 7:09:11 AM | WARNING | tf.TensorArrayGatherV3 {device = "", element_shape = #tf.shape&lt;?x?x3&gt;}
Jun 23, 2020, 7:09:11 AM | WARNING | tf.TensorArrayGatherV3 {device = "", element_shape = #tf.shape&lt;&gt;}
Jun 23, 2020, 7:09:11 AM | WARNING | tf.TensorArrayGatherV3 {device = "", element_shape = #tf.shape&lt;3&gt;}
Jun 23, 2020, 7:09:11 AM | WARNING | tf.TensorArrayGatherV3 {device = "", element_shape = #tf.shape&lt;100x4&gt;}
Jun 23, 2020, 7:09:11 AM | WARNING | tf.TensorArrayGatherV3 {device = "", element_shape = #tf.shape&lt;100&gt;}
Jun 23, 2020, 7:09:11 AM | WARNING | tf.Size {device = ""}
Jun 23, 2020, 7:09:11 AM | WARNING | tf.NonMaxSuppressionV2 {T = f32, T_threshold = f32, device = ""}
Jun 23, 2020, 7:09:11 AM | WARNING | tf.CropAndResize {T = f32, device = "", extrapolation_value = 0.000000e+00 : f32, method = "bilinear"}
Jun 23, 2020, 7:09:11 AM | WARNING | error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):
Jun 23, 2020, 7:09:10 AM | WARNING | loc(callsite("BatchMultiClassNonMaxSuppression_1/map/while/TensorArrayWrite_2/TensorArrayWriteV3@_functionalize_body_0" at "BatchMultiClassNonMaxSuppression_1/map/while/LoopCond")): error: 'tf.TensorArrayWriteV3' op is neither a custom op nor a flex op
Jun 23, 2020, 7:09:10 AM | WARNING | loc(callsite("BatchMultiClassNonMaxSuppression_1/map/while/TensorArrayWrite_1/TensorArrayWriteV3@_functionalize_body_0" at "BatchMultiClassNonMaxSuppression_1/map/while/LoopCond")): error: 'tf.TensorArrayWriteV3' op is neither a custom op nor a flex op
Jun 23, 2020, 7:09:10 AM | WARNING | loc(callsite("BatchMultiClassNonMaxSuppression_1/map/while/TensorArrayWrite_4/TensorArrayWriteV3@_functionalize_body_0" at "BatchMultiClassNonMaxSuppression_1/map/while/LoopCond")): error: 'tf.TensorArrayWriteV3' op is neither a custom op nor a flex op
Jun 23, 2020, 7:09:10 AM | WARNING | loc(callsite("BatchMultiClassNonMaxSuppression_1/map/while/TensorArrayWrite/TensorArrayWriteV3@_functionalize_body_0" at "BatchMultiClassNonMaxSuppression_1/map/while/LoopCond")): error: 'tf.TensorArrayWriteV3' op is neither a custom op nor a flex op
Jun 23, 2020, 7:09:10 AM | WARNING | loc(callsite("BatchMultiClassNonMaxSuppression_1/map/while/MultiClassNonMaxSuppression/SortByField/Size@_functionalize_body_0" at "BatchMultiClassNonMaxSuppression_1/map/while/LoopCond")): error: 'tf.Size' op is neither a custom op nor a flex op
...
...
...
loc(callsite("BatchMultiClassNonMaxSuppression_1/map/while/MultiClassNonMaxSuppression/non_max_suppression_70/NonMaxSuppressionV2@_functionalize_body_0" at "BatchMultiClassNonMaxSuppression_1/map/while/LoopCond")): error: 'tf.NonMaxSuppressionV2' op is neither a custom op nor a flex op
Jun 23, 2020, 7:09:10 AM | WARNING | : error: 'tf.NonMaxSuppressionV2' op is neither a custom op nor a flex op
Jun 23, 2020, 7:09:10 AM | WARNING | loc(callsite("BatchMultiClassNonMaxSuppression_1/map/while/MultiClassNonMaxSuppression/non_max_suppression_7/NonMaxSuppressionV2@_functionalize_body_0" at "BatchMultiClassNonMaxSuppression_1/map/while/LoopCond"))
...
...
...
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='tzekid' date='2020-06-24T06:48:42Z'>
		I think you need to enable control flow v2 ops.
&lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/compat/v1/enable_control_flow_v2&gt;https://www.tensorflow.org/api_docs/python/tf/compat/v1/enable_control_flow_v2&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='tzekid' date='2020-06-25T18:52:36Z'>
		Tested with calling tf.compat.v1.enable_v2_behavior() before initializing the converter.
It now gives me: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag).
Adding tf.lite.OpsSet.SELECT_TF_OPS by appending it tbo converter.target_spec.supported_ops gives me the same error.
Log:
&lt;denchmark-code&gt;Timestamp | Level | Message
-- | -- | --
Jun 25, 2020, 8:38:39 PM | INFO | Discarding 6 buffered messages for 715bab8b-b522-45ce-9411-e0e24104d8b6:14f1ce01a4e747a4f2ee7e3124a41b41
Jun 25, 2020, 8:38:39 PM | WARNING | stream.close()
Jun 25, 2020, 8:38:39 PM | WARNING | /usr/local/lib/python2.7/dist-packages/notebook/services/kernels/kernelmanager.py:246: UserWarning: Unregistering FD 35 after closing socket. This could result in unregistering handlers for the wrong socket. Please use stream.close() instead of closing the socket directly.
Jun 25, 2020, 8:38:39 PM | WARNING | stream.close()
Jun 25, 2020, 8:38:39 PM | WARNING | /usr/local/lib/python2.7/dist-packages/notebook/services/kernels/kernelmanager.py:246: UserWarning: Unregistering FD 32 after closing socket. This could result in unregistering handlers for the wrong socket. Please use stream.close() instead of closing the socket directly.
Jun 25, 2020, 8:38:39 PM | WARNING | stream.close()
Jun 25, 2020, 8:38:39 PM | WARNING | /usr/local/lib/python2.7/dist-packages/notebook/services/kernels/kernelmanager.py:246: UserWarning: Unregistering FD 29 after closing socket. This could result in unregistering handlers for the wrong socket. Please use stream.close() instead of closing the socket directly.
Jun 25, 2020, 8:38:39 PM | INFO | Adapting to protocol v5.1 for kernel 715bab8b-b522-45ce-9411-e0e24104d8b6
Jun 25, 2020, 8:38:36 PM | INFO | Starting buffering for 715bab8b-b522-45ce-9411-e0e24104d8b6:14f1ce01a4e747a4f2ee7e3124a41b41
Jun 25, 2020, 8:38:36 PM | WARNING | zmq message arrived on closed channel
Jun 25, 2020, 8:38:34 PM | WARNING | tcmalloc: large alloc 1527062528 bytes == 0x5625392e8000 @ 0x7fca003181e7 0x56246cc683ee 0x56246cce7930 0x56246cd128cd 0x56246cdad14e 0x56246cd05bba 0x56246cd032aa 0x56246cd1ef29 0x56246cceee6e 0x56246cdb6661 0x56246cd05bba 0x56246cd032aa 0x56246cd0adce 0x56246cd032aa 0x56246cd0adce 0x56246cd032aa 0x56246cd0adce 0x56246cd0a926 0x56246cd032aa 0x56246cd0b39e 0x56246cd032aa 0x56246cd1f1cc 0x56246cceee6e 0x56246cd08092 0x56246cd032aa 0x56246cd1f1cc 0x56246cceee6e 0x56246cd08092 0x56246cd032aa 0x56246cd0b39e 0x56246cd032aa
Jun 25, 2020, 8:38:28 PM | WARNING | tcmalloc: large alloc 1527062528 bytes == 0x5625ebb40000 @ 0x7fca003181e7 0x56246cce95c2 0x56246cd5009e 0x56246cd786d5 0x56246cceee6e 0x56246cd965b6 0x56246cd05bba 0x56246cd032aa 0x56246cd0b39e 0x56246cd032aa 0x56246cd0adce 0x56246cd032aa 0x56246cd0adce 0x56246cd0a926 0x56246cd032aa 0x56246cd0b39e 0x56246cd032aa 0x56246cd1f1cc 0x56246cceee6e 0x56246cd08092 0x56246cd032aa 0x56246cd1f1cc 0x56246cceee6e 0x56246cd08092 0x56246cd032aa 0x56246cd0b39e 0x56246cd032aa 0x56246cd0b39e 0x56246cd032aa 0x56246cd1f1cc 0x56246cceee6e
Jun 25, 2020, 8:38:26 PM | WARNING | tcmalloc: large alloc 1527062528 bytes == 0x562590aee000 @ 0x7fca003181e7 0x56246cc683ee 0x56246cce7930 0x56246cd128cd 0x56246cdad14e 0x56246cd05bba 0x56246cd032aa 0x56246cd1ef29 0x56246cceee6e 0x56246cdb6661 0x56246cd05bba 0x56246cd032aa 0x56246cd0adce 0x56246cd032aa 0x56246cd0adce 0x56246cd032aa 0x56246cd0adce 0x56246cd0a926 0x56246cd032aa 0x56246cd0b39e 0x56246cd032aa 0x56246cd1f1cc 0x56246cceee6e 0x56246cd08092 0x56246cd032aa 0x56246cd1f1cc 0x56246cceee6e 0x56246cd08092 0x56246cd032aa 0x56246cd0b39e 0x56246cd032aa
Jun 25, 2020, 8:38:08 PM | WARNING | tcmalloc: large alloc 1145217024 bytes == 0xa61b4000 @ 0x7f79204241e7 0x5ab685 0x578518 0x587f87 0x5a6bfb 0x536cf8 0x50a35c 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x507d64 0x509a90 0x50a48d 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x507d64 0x509a90 0x50a48d 0x50bfb4
Jun 25, 2020, 8:38:08 PM | WARNING | tf.TensorArrayWriteV3 {device = ""}
Jun 25, 2020, 8:38:08 PM | WARNING | tf.TensorArrayV3 {clear_after_read = true, device = "", dtype = i32, dynamic_size = false, element_shape = #tf.shape&lt;*&gt;, identical_element_shapes = true, tensor_array_name = ""}
Jun 25, 2020, 8:38:08 PM | WARNING | tf.TensorArrayV3 {clear_after_read = true, device = "", dtype = f32, dynamic_size = false, element_shape = #tf.shape&lt;*&gt;, identical_element_shapes = true, tensor_array_name = ""}
Jun 25, 2020, 8:38:08 PM | WARNING | tf.TensorArraySizeV3 {device = ""}
Jun 25, 2020, 8:38:08 PM | WARNING | tf.TensorArrayScatterV3 {device = ""}
Jun 25, 2020, 8:38:08 PM | WARNING | tf.TensorArrayReadV3 {device = ""}
Jun 25, 2020, 8:38:08 PM | WARNING | tf.TensorArrayGatherV3 {device = "", element_shape = #tf.shape&lt;?x?x3&gt;}
Jun 25, 2020, 8:38:08 PM | WARNING | tf.TensorArrayGatherV3 {device = "", element_shape = #tf.shape&lt;&gt;}
Jun 25, 2020, 8:38:08 PM | WARNING | tf.TensorArrayGatherV3 {device = "", element_shape = #tf.shape&lt;3&gt;}
Jun 25, 2020, 8:38:08 PM | WARNING | tf.TensorArrayGatherV3 {device = "", element_shape = #tf.shape&lt;100x4&gt;}
Jun 25, 2020, 8:38:08 PM | WARNING | tf.TensorArrayGatherV3 {device = "", element_shape = #tf.shape&lt;100&gt;}
Jun 25, 2020, 8:38:08 PM | WARNING | tf.Size {device = ""}
Jun 25, 2020, 8:38:08 PM | WARNING | tf.NonMaxSuppressionV2 {T = f32, T_threshold = f32, device = ""}
Jun 25, 2020, 8:38:08 PM | WARNING | tf.CropAndResize {T = f32, device = "", extrapolation_value = 0.000000e+00 : f32, method = "bilinear"}
Jun 25, 2020, 8:38:08 PM | WARNING | error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):
Jun 25, 2020, 8:38:07 PM | WARNING | loc(callsite("BatchMultiClassNonMaxSuppression_1/map/while/TensorArrayWrite_2/TensorArrayWriteV3@_functionalize_body_3" at "BatchMultiClassNonMaxSuppression_1/map/while/LoopCond")): error: 'tf.TensorArrayWriteV3' op is neither a custom op nor a flex op
...
...
...
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='tzekid' date='2020-06-25T21:44:08Z'>
		Sorry. You need to enable that option before creating a TF graph, the saved model.
		</comment>
		<comment id='4' author='tzekid' date='2020-07-04T19:19:33Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='5' author='tzekid' date='2020-07-04T20:45:46Z'>
		Ok, did that. Final code looks like so:
import tensorflow as tf
import tarfile
import pathlib

tf.compat.v1.enable_v2_behavior()

model_name = "mask_rcnn_inception_v2_coco_2018_01_28"
base_url = 'http://download.tensorflow.org/models/object_detection/'
model_file = model_name + '.tar.gz'
model_dir = tf.keras.utils.get_file(fname=model_name, origin=base_url + model_file, untar=True)
model_dir = pathlib.Path(model_dir)/"saved_model"
print('Saved to {}'.format(model_dir))

saved_model_dir = "/root/.keras/datasets/mask_rcnn_inception_v2_coco_2018_01_28/saved_model"
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.experimental_new_converter = True
converter.optimizations = [tf.lite.Optimize.DEFAULT]
def representative_dataset_gen():
  for _ in range(num_calibration_steps):
    # Get sample input data as a numpy array in a method of your choosing.
    yield [input]
converter.representative_dataset = representative_dataset_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8  # or tf.uint8
converter.inference_output_type = tf.int8  # or tf.uint8
tflite_quant_model = converter.convert()
open("converted_model_quant.tflite", "wb+").write(tflite_quant_model)
And it gives out this error:
&lt;denchmark-code&gt;ValueError: None is only supported in the 1st dimension. Tensor 'image_tensor' has invalid shape '[None, None, None, 3]'.
&lt;/denchmark-code&gt;

I also tried the code again w/o  and it worked. Might have been a problem with Colab itself?
Technically my issue is fixed and now looking at &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/22564&gt;#22564&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='tzekid' date='2020-07-04T20:45:48Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40702&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40702&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>