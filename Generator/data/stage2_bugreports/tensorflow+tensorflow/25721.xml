<bug id='25721' author='RuilongMachineLearning' open_date='2019-02-13T14:34:46Z' closed_time='2019-04-02T01:06:06Z'>
	<summary>convert_variables_to_constants raise issue of while/ReadVariableOp/Enter while dealing with LSTM and GRU</summary>
	<description>
Please make sure that this is a bug. As per our [GitHub Policy]

System information

TensorFlow version:1.12
Python version:3.6
NO GPU, and with GPU(GTX 1080Ti)

Describe the current behavior
convert_variables_to_constants first changes computed variables to constants, then converts ReadVariableOps to Identity node. However, in RNN models such as GRU and LSTM,  some variables are attached to Enter op which are incompatible with the generated constants.
Describe the expected behavior
The model can be successfully frozen and loaded in CNN, however, failed to work in RNN such as LSTM and GRU
Code to reproduce the issue

from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense, LSTM
import numpy as np
import tensorflow as tf
from tensorflow.python.keras import backend as K
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):
    """
    Freezes the state of a session into a pruned computation graph.
    Creates a new computation graph where variable nodes are replaced by
    constants taking their current value in the session. The new graph will be
    pruned so subgraphs that are not necessary to compute the requested
    outputs are removed.
    @param session The TensorFlow session to be frozen.
    @param keep_var_names A list of variable names that should not be frozen,
                          or None to freeze all the variables in the graph.
    @param output_names Names of the relevant graph outputs.
    @param clear_devices Remove the device directives from the graph for better portability.
    @return The frozen graph definition.
    """
    from tensorflow.python.framework.graph_util import convert_variables_to_constants
    graph = session.graph
    with graph.as_default():
        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))
        output_names = output_names or []
        output_names += [v.op.name for v in tf.global_variables()]
        input_graph_def = graph.as_graph_def()
        if clear_devices:
            for node in input_graph_def.node:
                node.device = ""
        frozen_graph = convert_variables_to_constants(session, input_graph_def,
                                                      output_names, freeze_var_names)
        return frozen_graph

def load_graph(frozen_graph_filename):
    # We load the protobuf file from the disk and parse it to retrieve the
    # unserialized graph_def
    with tf.gfile.GFile(frozen_graph_filename, "rb") as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())

    # Then, we import the graph_def into a new Graph and returns it
    with tf.Graph().as_default() as graph:
        # The name var will prefix every op/nodes in your graph
        # Since we load everything in a new graph, this is not needed
        tf.import_graph_def(graph_def, name="prefix")
    return graph


def lstm_model(input_shape):
    model = Sequential()
    model.add(LSTM(256, name='LSTM', return_sequences=False, use_bias=False,  input_shape=(256,1)))
    model.add(Dense(100,activation='relu'))
    model.add(Dense(1, activation='linear', name='output'))
    return model



if __name__ == "__main__":
    #load lstm model
    input_shape = (None, 256)
    model = lstm_model(input_shape)

    x=np.random.rand(1000, 256, 1)
    y = np.random.rand(1000, 1)
    print(x.shape)

    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])

    model.fit(x=x, y=y, epochs=2, validation_split=0.2)

    #freeze the session and save it to a pd.file
    frozen_graph = freeze_session(K.get_session(),
                                  output_names=[out.op.name for out in model.outputs])
    tf.train.write_graph(frozen_graph, './', './model.pb', as_text=False)

    #load the pb file into graph
    frozen_model_filename = './model.pb'
    graph = load_graph(frozen_model_filename)


Other info / logs
The model can be frozen but could not be load. If put LSTM/kernel to black list, the pd file can be load later on, however, cannot be used in c++ and c #.
ValueError: Input 0 of node prefix/LSTM/while/ReadVariableOp/Enter was passed float from prefix/LSTM/kernel:0 incompatible with expected resource.
	</description>
	<comments>
		<comment id='1' author='RuilongMachineLearning' date='2019-02-17T13:15:46Z'>
		I think I'm running into the same issue, when loading my frozen graph from C++:  session-&gt;Create(graph_def) fails with: Invalid argument: Input 0 of node gru/while/ReadVariableOp/Enter was passed float from gru/kernel:0 incompatible with expected resource.
Did you find any solution?
		</comment>
		<comment id='2' author='RuilongMachineLearning' date='2019-02-18T11:25:52Z'>
		
I think I'm running into the same issue, when loading my frozen graph from C++: session-&gt;Create(graph_def) fails with: Invalid argument: Input 0 of node gru/while/ReadVariableOp/Enter was passed float from gru/kernel:0 incompatible with expected resource.
Did you find any solution?

Not yet. It seems this tf function could not freeze any rnn keras models. If your keras models is a sequential model, you might find some other api. Mine keras model has to be a functional model. Have to wait tensorflow or keras to fix this issue.
		</comment>
		<comment id='3' author='RuilongMachineLearning' date='2019-03-28T13:13:59Z'>
		Hi, I am facing the same issue, did any one find a workaround?
		</comment>
		<comment id='4' author='RuilongMachineLearning' date='2019-03-28T17:47:38Z'>
		&lt;denchmark-link:https://github.com/RuilongMachineLearning&gt;@RuilongMachineLearning&lt;/denchmark-link&gt;
 -- can you clarify why you are using graph freezing here? It might be that there's a better approach.
		</comment>
		<comment id='5' author='RuilongMachineLearning' date='2019-03-29T05:08:31Z'>
		&lt;denchmark-link:https://github.com/karmel&gt;@karmel&lt;/denchmark-link&gt;
 , I want to freeze my model (keras model containing LSTM and GRU) so that I can directly load the model using Java API without having to setup python environment on the server.
I have tried freezing small practice models too. It seems this issue comes up everytime LSTM/GRU is used in tf.keras model.
Python = 3.6.0
Tensorflow = 1.13.1
windows 7
		</comment>
		<comment id='6' author='RuilongMachineLearning' date='2019-03-29T05:55:01Z'>
		&lt;denchmark-link:https://github.com/disha3&gt;@disha3&lt;/denchmark-link&gt;
 Maybe you can try our version of convert_variables_to_constants (&lt;denchmark-link:https://github.com/intel-analytics/analytics-zoo/blob/master/pyzoo/zoo/util/tf_graph_util.py#L226&gt;https://github.com/intel-analytics/analytics-zoo/blob/master/pyzoo/zoo/util/tf_graph_util.py#L226&lt;/denchmark-link&gt;
).  This issue should have been fixed there.
(And by the way, if you are using java, you may also want to checkout our tfnet example, which support running tensorflow graph on spark (&lt;denchmark-link:https://github.com/intel-analytics/analytics-zoo/tree/master/zoo/src/main/scala/com/intel/analytics/zoo/examples/tfnet&gt;https://github.com/intel-analytics/analytics-zoo/tree/master/zoo/src/main/scala/com/intel/analytics/zoo/examples/tfnet&lt;/denchmark-link&gt;
) and our java model serving api which also supports loading tensorflow graph (&lt;denchmark-link:https://analytics-zoo.github.io/master/#ProgrammingGuide/inference/&gt;https://analytics-zoo.github.io/master/#ProgrammingGuide/inference/&lt;/denchmark-link&gt;
))
		</comment>
		<comment id='7' author='RuilongMachineLearning' date='2019-03-29T07:01:12Z'>
		&lt;denchmark-link:https://github.com/yangw1234&gt;@yangw1234&lt;/denchmark-link&gt;
, thanks a lot for the solution, it solved the issue. :D
Also, thank you for informing about the java apis for tensorflow.
		</comment>
		<comment id='8' author='RuilongMachineLearning' date='2019-04-02T01:06:07Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=25721&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=25721&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='RuilongMachineLearning' date='2019-06-19T09:18:26Z'>
		
@yangw1234, thanks a lot for the solution, it solved the issue. :D
Also, thank you for informing about the java apis for tensorflow.

hi, guy. how you resolved this problem?
		</comment>
		<comment id='10' author='RuilongMachineLearning' date='2019-07-16T16:00:50Z'>
		&lt;denchmark-link:https://github.com/yangw1234&gt;@yangw1234&lt;/denchmark-link&gt;
 thanks for the solution. I am able to freeze a graph consisting with LSTM node but unable to load the frozen graph using " tf.import_graph_def".  I am trying to load the graph in order to profile it. Do you have any suggestions on how can I profile a TF graph with LSTM loads since freezing and loading doesnt seem to be working.
		</comment>
		<comment id='11' author='RuilongMachineLearning' date='2019-08-06T18:56:24Z'>
		&lt;denchmark-link:https://github.com/GaoQ1&gt;@GaoQ1&lt;/denchmark-link&gt;
 replace your /usr/lib/python3.6/site-packages/tensorflow/python/framework/graph_util_impl.py  file with /pyzoo/zoo/util/tf_graph_util.py
		</comment>
		<comment id='12' author='RuilongMachineLearning' date='2019-08-09T16:36:24Z'>
		*EDIT This change made it possible to export a ProtoBuf file, but that file was not useful for either the Android inference API or converting to Tensorflow Lite.
This ended up working better for me:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/examples/lstm/TensorFlowLite_LSTM_Keras_Tutorial.ipynb&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/examples/lstm/TensorFlowLite_LSTM_Keras_Tutorial.ipynb&lt;/denchmark-link&gt;

Here's hoping all of the different versions and data types get consolidated better soon.
		</comment>
		<comment id='13' author='RuilongMachineLearning' date='2019-08-11T09:28:20Z'>
		Does anyone have an LSTM running on Android?
		</comment>
	</comments>
</bug>