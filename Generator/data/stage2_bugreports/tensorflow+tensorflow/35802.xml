<bug id='35802' author='demmerichs' open_date='2020-01-12T20:13:51Z' closed_time='2020-06-18T00:52:53Z'>
	<summary>recude_max on RaggedTensor fails to propagate gradients</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary(pip)
TensorFlow version (use command below): 2.0.0
Python version: 3.6.8
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 10.0/7.3.1
GPU model and memory:

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
Currently, passing a ragged tensor into a reduce_max an computing the gradient on the result fails. The following code is a minimal example of this:
import numpy as np
import tensorflow as tf

with tf.GradientTape() as tape:
    ragged_t = tf.RaggedTensor.from_row_splits(
        [0.0, 1.0, 2.0, 3.0, 4.0, 5.0], [0, 1, 4, 6]
    )
    tape.watch(ragged_t.values)
    max_t = tf.reduce_max(ragged_t, axis=-1)
    # max_t = tf.reduce_max(ragged_t.to_tensor(default_value=np.nan), axis=-1)
    gradients = tape.gradient(max_t, ragged_t.values)
Describe the expected behavior
As tensorflows main purpose is to compute gradients and reduce operations on ragged tensors is the most basic usage of ragged tensor representations I consider this behavior a bug. The above code shows a workaround in the commented line, but I expect the above example to work without this workaround and without the need of allocating in some cases a substantial amount of memory through the usage of to_tensor.
Other info / logs
The full error trace of the above code example on my system is:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "./misc/reduce_max_ragged_gradients.py", line 13, in &lt;module&gt;
    gradients = tape.gradient(max_t, ragged_t.values)
  File "/lhome/davidj2/code/sync/unsup_flow/.venv/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py", line 1014, in gradient
    unconnected_gradients=unconnected_gradients)
  File "/lhome/davidj2/code/sync/unsup_flow/.venv/lib/python3.6/site-packages/tensorflow_core/python/eager/imperative_grad.py", line 76, in imperative_grad
    compat.as_str(unconnected_gradients.value))
  File "/lhome/davidj2/code/sync/unsup_flow/.venv/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py", line 138, in _gradient_function
    return grad_fn(mock_op, *out_grads)
  File "/lhome/davidj2/code/sync/unsup_flow/.venv/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py", line 455, in _UnsortedSegmentMaxGrad
    return _UnsortedSegmentMinOrMaxGrad(op, grad)
  File "/lhome/davidj2/code/sync/unsup_flow/.venv/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py", line 432, in _UnsortedSegmentMinOrMaxGrad
    _GatherDropNegatives(op.outputs[0], op.inputs[1])
TypeError: 'NoneType' object is not subscriptable
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='demmerichs' date='2020-01-12T20:18:18Z'>
		I have actually had some code using reduce_max on ragged tensors in TF 1.13.0/1 which worked with the static graph gradient computation just fine. So this error seems to be introduced with TF 2.0 either through the eager execution, gradient tape or further changes to the ragged tensors.
		</comment>
		<comment id='2' author='demmerichs' date='2020-01-14T05:44:36Z'>
		@DavidS3141
Can you try both 2.1.0-rc2 and tf-nighly and let us know whether the issue persists with the latest TF versions. I am not seeing any issue with 2.1 and nightly versions.Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/e29ce0aad3912d1db4cfc7a4831b8238/untitled557.ipynb&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='3' author='demmerichs' date='2020-01-17T08:46:35Z'>
		Yes those versions seem to work. I still hope, that TF2.0 gets patched, because TF2.1 seems to require already CUDA10.1 :(
		</comment>
		<comment id='4' author='demmerichs' date='2020-06-03T23:56:10Z'>
		@DavidS3141 I can reproduce the issue with . However it is resolved in recent stable version . Can you please use . &lt;denchmark-link:https://colab.research.google.com/gist/jvishnuvardhan/3d77f0d0f9179ca842983dff0beda51e/untitled557.ipynb&gt;Here&lt;/denchmark-link&gt;
 is a gist with . Thanks
AFAIK only security patches are getting patched with old TF versions. Thanks!
Please close the issue if this was resolved for you. Thaanks!
		</comment>
		<comment id='5' author='demmerichs' date='2020-06-11T00:09:05Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='6' author='demmerichs' date='2020-06-18T00:52:51Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='7' author='demmerichs' date='2020-06-18T00:52:54Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35802&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35802&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>