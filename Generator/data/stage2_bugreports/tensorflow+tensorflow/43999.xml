<bug id='43999' author='usama-ahmedkhan' open_date='2020-10-14T07:08:23Z' closed_time='2020-10-20T08:28:43Z'>
	<summary>Why is loading resnet50 keras model in TF 1.15 on TPU does not work?</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


**Have I written custom code ( No , I am using tf.keras.application.resnet50 ):
OS Platform and Distribution (e.g., Linux Ubuntu 20.04):
TensorFlow version (1.15):
Python version: (3.7)
**TPU *: V2 using TPU estimator API

&lt;denchmark-h:h3&gt;&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;     I am trying to initialize resnet50 as backbone for a model in TF 1.15, and the model is run on google TPU V2. My code is this:
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Source code&lt;/denchmark-h&gt;

import tensorflow.keras as keras
from MBCONV import MBConvBlock
import yaml
import tensorflow.keras.backend as K
import tensorflow as tf
print(tf.version)
tf.compat.v1.enable_eager_execution()
def model():
&lt;denchmark-code&gt;mode=resnet_backbone() // error is in this fuction 

// adding some more layers etc 

return tf.contrib.tpu.TPUEstimatorSpec(mode, loss=loss, train_op=train_op, host_call=host_call,
                                       predictions={"emb": embeddings_layer})
&lt;/denchmark-code&gt;

def resent_backbone():
&lt;denchmark-code&gt;backbone_model=tf.keras.applications.ResNet50(include_top=False, weights='imagenet',pooling=None)
return backbone_model
&lt;/denchmark-code&gt;

def main(unused_argv):
&lt;denchmark-code&gt;tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(FLAGS.tpu)

run_config = tf.contrib.tpu.RunConfig(
    model_dir=FLAGS.model_dir,
    cluster=tpu_cluster_resolver,
    session_config=tf.ConfigProto(
        allow_soft_placement=True, log_device_placement=True),
    tpu_config=tf.contrib.tpu.TPUConfig(FLAGS.iterations),
)

classifier = tf.contrib.tpu.TPUEstimator(
    model_fn=model,
    use_tpu=FLAGS.use_tpu,
    train_batch_size=FLAGS.batch_size,
    eval_batch_size=FLAGS.batch_size,
    predict_batch_size=FLAGS.batch_size,
    config=run_config,
    params={

        "use_tpu": FLAGS.use_tpu,
    })

classifier.train(
    input_fn=lambda params: train_input_fn(params["batch_size"]),
    # input_fn=lambda params: train_input_fn(params["batch_size"]),
    max_steps=FLAGS.train_steps)
&lt;/denchmark-code&gt;

def build_prediction_network():
tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(
FLAGS.tpu)
&lt;denchmark-code&gt;run_config = tf.contrib.tpu.RunConfig(
    model_dir=FLAGS.model_dir,
    cluster=tpu_cluster_resolver,
    session_config=tf.ConfigProto(
        allow_soft_placement=True, log_device_placement=True),
    tpu_config=tf.contrib.tpu.TPUConfig(FLAGS.iterations),
)
classifier = tf.contrib.tpu.TPUEstimator(
    model_fn=model,
    use_tpu=FLAGS.use_tpu,
    train_batch_size=FLAGS.batch_size,
    eval_batch_size=FLAGS.batch_size,
    predict_batch_size=FLAGS.batch_size,
    config=run_config,
    params={

        "use_tpu": FLAGS.use_tpu, })
return classifier
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;I get the following errors&lt;/denchmark-h&gt;

Operation of type Placeholder (input_1) is not supported on the TPU. Execution will fail if this op is used in the graph.
tensorflow.python.framework.errors_impl.InaccessibleTensorError: Operation 'VarIsInitializedOp' has been marked as not fetchable. Typically this happens when it is defined in another function or code block. Use return values,explicit Python locals or TensorFlow collections to access it.
	</description>
	<comments>
		<comment id='1' author='usama-ahmedkhan' date='2020-10-14T11:14:55Z'>
		&lt;denchmark-link:https://github.com/usama-ahmedkhan&gt;@usama-ahmedkhan&lt;/denchmark-link&gt;

Please provide with complete stand alone indented code to replicate the issue faced or if possible share a colab gist with the error reported.
Please note there is no official support for 1.x tf, could you try on 2.x and let us know if you face any issues.
		</comment>
		<comment id='2' author='usama-ahmedkhan' date='2020-10-14T12:52:16Z'>
		yes , I have edited , not possible to work in tf 2.x right now , because of most code is written in the tf 1.x. If i put @tf.function on top of def resent_backbone: , then this error goes away and following error comes:
ValueError: Cannot use 'conv1_conv/kernel/Initializer/random_uniform' as input to 'conv1_conv/kernel/Assign' because 'conv1_conv/kernel/Initializer/random_uniform' is in a while loop. See info log for more details.
		</comment>
		<comment id='3' author='usama-ahmedkhan' date='2020-10-14T18:48:04Z'>
		&lt;denchmark-link:https://github.com/usama-ahmedkhan&gt;@usama-ahmedkhan&lt;/denchmark-link&gt;

Please provide with complete stand alone indented code to replicate the issue faced or if possible share a colab gist with the error reported.
		</comment>
		<comment id='4' author='usama-ahmedkhan' date='2020-10-15T06:38:47Z'>
		I updated it , can you give a guess to what could be the issue , it's just a simple 1 line of code initialization with no input to it .
From what I have searched , following is the closest guess to the error I am getting , it is not possible to fetch the result of an op created inside the while loop's body, because the body might execute 0 or more times, based on the loop condition. To get a value out of the loop, you need to return it from the body function (as one of the loop variables), and its final value after all the iterations will be returned from tf.while_loop(). But this happens inside tf code , not externally , because i am only running a single line code for initialization of a model.
Inside tf.keras implementation code , while loading weights into variable , it checks if there are uninitialized variables and there the error comes , the Varisinitializedop is used there  .
		</comment>
		<comment id='5' author='usama-ahmedkhan' date='2020-10-15T16:45:24Z'>
		In TF 1.x, there are subtle differences between the graph representation of models implemented in Keras (e.g. tf.keras.applications...)  and models implemented in standard TensorFlow.
If you must stick with TF 1, I would highly suggest starting from the &lt;denchmark-link:https://github.com/tensorflow/tpu/blob/master/models/official/resnet/resnet_main.py&gt;reference ResNet model&lt;/denchmark-link&gt;
 that works well on TPUs, but I would highly suggest moving to TF 2.x + Keras which is a lot easier to use. &lt;denchmark-link:https://github.com/tensorflow/models/tree/master/official/vision/image_classification&gt;Here&lt;/denchmark-link&gt;
 is an equivalent reference ResNet model that works well on CPU, GPU and TPUs.
		</comment>
		<comment id='6' author='usama-ahmedkhan' date='2020-10-16T05:19:10Z'>
		thanks for your reply , and yes I am now using reference ResNet model you suggested , it is easy  and flexible to use that resnet model.
		</comment>
		<comment id='7' author='usama-ahmedkhan' date='2020-10-20T05:08:07Z'>
		&lt;denchmark-link:https://github.com/usama-ahmedkhan&gt;@usama-ahmedkhan&lt;/denchmark-link&gt;

Please move the issue to closed status if resolved.
		</comment>
	</comments>
</bug>