<bug id='889' author='ghost(ghost)' open_date='2016-01-26T15:03:42Z' closed_time='2016-04-15T02:11:18Z'>
	<summary>Large Strides for 1x1 Convolutions</summary>
	<description>
When a conv2d operator has a stride greater than the kernel size, the following error is thrown:
ValueError: ('stride must be less than or equal to filter size', 'stride: [2x2] filter: [Dimension(1)xDimension(1)]')
This makes implementing the 1x1 convolutions used to reduce spatial resolution in several papers (MSRA 2015 among others) awkward. Also, a convolution with stride greater than kernel size may be unusual but is it still well defined. Fixing this may be as simple as removing this assertion.
Thanks to everyone who developed TensorFlow, it's a fascinating tool.
	</description>
	<comments>
		<comment id='1' author='ghost(ghost)' date='2016-01-26T18:55:08Z'>
		I think this has been called 'atrous convolution', at least in the Eigen code.  Does anybody know whether cudnn supports this?  &lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
 ?
		</comment>
		<comment id='2' author='ghost(ghost)' date='2016-01-26T19:28:32Z'>
		I don't think Cudnn supports convolutions for stride larger than 1. Not
sure the FFT algorithm they were using would still be correct.
On Tue, Jan 26, 2016 at 10:56 AM, Vijay Vasudevan &lt;denchmark-link:mailto:notifications@github.com&gt;notifications@github.com&lt;/denchmark-link&gt;

wrote:

I think this has been called 'atrous convolution', at least in the Eigen
code. Does anybody know whether cudnn supports this? @zheng-xq
https://github.com/zheng-xq ?
â€”
Reply to this email directly or view it on GitHub
#889 (comment)
.

		</comment>
		<comment id='3' author='ghost(ghost)' date='2016-01-26T20:34:14Z'>
		I stand corrected. The stride larger than 1 should be supported by Cudnn. I was looking at upscalex instead.
		</comment>
		<comment id='4' author='ghost(ghost)' date='2016-01-26T20:42:27Z'>
		it's not just stride &gt; 1, it's stride &gt; the input patch.  E.g., skipping regions of the input.
&lt;denchmark-link:https://github.com/vincentvanhoucke&gt;@vincentvanhoucke&lt;/denchmark-link&gt;
 just corrected me: atrous is introducing holes in the input patch, so the input has 'holes'.  So it's different.
I don't know if our eigen (CPU) implementation supports stride &gt; input_patch.  If it does, and cudnn does too, we can remove this assertion.
		</comment>
		<comment id='5' author='ghost(ghost)' date='2016-02-06T22:08:05Z'>
		FYI last week I did try to look at this and cudnn appeared to give different results than I expected (eigen was correct for both CPU and GPU).  Not yet sure whether this is a bug in cudnn or how we were calling it.
		</comment>
		<comment id='6' author='ghost(ghost)' date='2016-02-19T12:01:29Z'>
		The same error is thrown when calling tf.nn.max_pool or tf.nn.avg_pool with stride greater than filter size.
		</comment>
		<comment id='7' author='ghost(ghost)' date='2016-02-23T19:13:16Z'>
		I hit this bug while trying implement ResNet. I'm working around it by using a 2x2 kernel with all but the top-left zeroed out.
&lt;denchmark-link:https://github.com/ry/tensorflow-resnet/blob/2775805c4c4af3a3a082100c36a2b2f77791df4d/convert.py#L119-L122&gt;https://github.com/ry/tensorflow-resnet/blob/2775805c4c4af3a3a082100c36a2b2f77791df4d/convert.py#L119-L122&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='ghost(ghost)' date='2016-02-23T19:26:42Z'>
		&lt;denchmark-link:https://github.com/ry&gt;@ry&lt;/denchmark-link&gt;
 I used resize_nearest_neighbour to circumvent that problem: &lt;denchmark-link:https://github.com/cesarsalgado/tensorflowed/blob/master/models/residual_nets.py#L46&gt;https://github.com/cesarsalgado/tensorflowed/blob/master/models/residual_nets.py#L46&lt;/denchmark-link&gt;

Or you could also use average pooling for sub-sampling, although it will not be exactly what the paper proposes.
		</comment>
		<comment id='9' author='ghost(ghost)' date='2016-04-08T00:09:40Z'>
		I'm marking this a bug since it really should work. Also contributions welcome. See also the recent duplicate &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/1815&gt;#1815&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='10' author='ghost(ghost)' date='2016-04-08T00:53:03Z'>
		&lt;denchmark-link:https://github.com/martinwicke&gt;@martinwicke&lt;/denchmark-link&gt;
 I'd like to take a look at fixing it because I'm hitting this still. But I've never hacked on TF before, if you could give a rough outline of what's involved it'd help!
		</comment>
		<comment id='11' author='ghost(ghost)' date='2016-04-08T04:29:49Z'>
		&lt;denchmark-link:https://github.com/ry&gt;@ry&lt;/denchmark-link&gt;
: I wish I could still find my code.  I remember Eigen doing something consistent across GPU and CPU and expected and cudnn doing something different (and seemingly wrong), but I might have introduced a bug.
The best thing to start with is via a test: add a simple test to conv_ops_test.py that exercises say, a 1x1 convolution with input depth 1 and output depth 1 for a 1x3x3x1 input with stride 2 (so you get four values: the corners multiplied by a filter).
If you try to run that, you'll first run into a problem doing conv2d shape inference in python (because it checks for stride &lt; filter) -- remove that and make sure that the shape calculation is correct.
Then you'll probably have to make the same fix in the C++ code (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/ops_util.cc#L41&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/ops_util.cc#L41&lt;/denchmark-link&gt;
) to remove that check.
Other than that, I don't think there was anything else I had to do to get the calls to run, but that's when I started seeing different output and didn't have time to debug.  It's possible it has something to do with padding or the shape inference difference between us and cudnn.  Happy to help how I can.
		</comment>
	</comments>
</bug>