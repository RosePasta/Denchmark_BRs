<bug id='41608' author='AlexeyAB' open_date='2020-07-21T19:40:09Z' closed_time='2020-07-22T23:26:24Z'>
	<summary>Converted TFLite-model produces wrong results, while PB-model produces correct result</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS
TensorFlow installed from (source or binary):  pip install tf-nightly
TensorFlow version (or github SHA if from source): 2.4.0-dev20200721

Command used to run the converter or code if youâ€™re using the Python API
Colab (GPU is disabled): &lt;denchmark-link:https://colab.research.google.com/gist/AlexeyAB/07e7aa3c9ab49f1d733153f64d6fd270/onnx_to_tf_to_tflite.ipynb&gt;https://colab.research.google.com/gist/AlexeyAB/07e7aa3c9ab49f1d733153f64d6fd270/onnx_to_tf_to_tflite.ipynb&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;toco --graph_def_file model-f46da743.pb --output_file model-f46da743.tflite --output_format TFLITE --inference_type FLOAT --inference_input_type FLOAT --input_arrays 0 --output_arrays 1195 --enable_v1_converter --target_ops=SELECT_TF_OPS
&lt;/denchmark-code&gt;

The output from the converter invocation
&lt;denchmark-code&gt;2020-07-21 19:15:30.657556: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-07-21 19:15:32.252326: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-07-21 19:15:32.255365: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2020-07-21 19:15:32.255412: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (71fb61714d13): /proc/driver/nvidia/version does not exist
2020-07-21 19:15:32.255749: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-07-21 19:15:32.261434: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2200000000 Hz
2020-07-21 19:15:32.261660: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1aeef40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-21 19:15:32.261724: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
I0721 19:15:35.806807 139688388913024 lite.py:1321] Using experimental converter: If you encountered a problem please file a bug. You can opt-out by setting experimental_new_converter=False
2020-07-21 19:15:36.648004: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:315] Ignored output_format.
2020-07-21 19:15:36.648067: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:318] Ignored drop_control_dependency.
&lt;/denchmark-code&gt;

Also, please include a link to the saved model or GraphDef


model-f46da743.pb


model-f46da743.tflite


Failure details
The conversion is successful, but the generated model is wrong,

Producing wrong results

Any other info / logs
I convert model from ONNX to PB and it works well with , it shows desired :
&lt;denchmark-link:https://user-images.githubusercontent.com/4096485/88097044-43262280-cba0-11ea-8dc9-5b8fff654485.png&gt;&lt;/denchmark-link&gt;

But when I convert PB to TFLITE, it works with , but it shows unexpected :
&lt;denchmark-link:https://user-images.githubusercontent.com/4096485/88097079-4f11e480-cba0-11ea-9ee0-600b5899d687.png&gt;&lt;/denchmark-link&gt;

Reprocudable commands and code (GPU is disabled): &lt;denchmark-link:https://colab.research.google.com/gist/AlexeyAB/07e7aa3c9ab49f1d733153f64d6fd270/onnx_to_tf_to_tflite.ipynb&gt;https://colab.research.google.com/gist/AlexeyAB/07e7aa3c9ab49f1d733153f64d6fd270/onnx_to_tf_to_tflite.ipynb&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='AlexeyAB' date='2020-07-22T05:33:13Z'>
		&lt;denchmark-link:https://github.com/AlexeyAB&gt;@AlexeyAB&lt;/denchmark-link&gt;

Request you to grant me the access for the colab link. Thanks!
		</comment>
		<comment id='2' author='AlexeyAB' date='2020-07-22T05:39:25Z'>
		&lt;denchmark-link:https://github.com/AlexeyAB&gt;@AlexeyAB&lt;/denchmark-link&gt;
 Can't access your colab. I googled a bit, found what the model is for, and created a &lt;denchmark-link:https://github.com/freedomtan/MiDaS/blob/tflite/tf/run_tflite.py&gt;run_tflite&lt;/denchmark-link&gt;
 python script derived from you . It seems the tflite model works.
BTW, any reason why you want --target_ops=SELECT_TF_OPS? TFLite supports all the ops of this MiDaS model. I tried my script with both model with  --target_ops=SELECT_TF_OPS and without. Both of them work.
		</comment>
		<comment id='3' author='AlexeyAB' date='2020-07-22T11:43:31Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;

Yes, there was my little bug, now it works fine: &lt;denchmark-link:https://colab.research.google.com/gist/AlexeyAB/59e8a5e4116e03858bc1fd6c7f1a8d5f/onnx_to_tf_to_tflite.ipynb&gt;https://colab.research.google.com/gist/AlexeyAB/59e8a5e4116e03858bc1fd6c7f1a8d5f/onnx_to_tf_to_tflite.ipynb&lt;/denchmark-link&gt;

TFLite result:
&lt;denchmark-link:https://user-images.githubusercontent.com/4096485/88172347-82e51c80-cc29-11ea-9f8a-99f49f2125d1.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

But why does it work well if GPU is disabled in Colab, but it doesn't work if GPU is enabled?
I use the same PB-file in both cases:  &lt;denchmark-link:https://github.com/intel-isl/MiDaS/releases/download/v2/model-f46da743.pb&gt;https://github.com/intel-isl/MiDaS/releases/download/v2/model-f46da743.pb&lt;/denchmark-link&gt;

Everything is the same, except whether there is a GPU or not:


GPU disabled: (PB-&gt;TFLite) works well, and inference using TFLITE works well https://colab.research.google.com/gist/AlexeyAB/59e8a5e4116e03858bc1fd6c7f1a8d5f/onnx_to_tf_to_tflite.ipynb


GPU enabled: (PB-&gt;TFLite) works well, but inference  using TFLITE failed on line interpreter.invoke() (without any error message) https://colab.research.google.com/gist/AlexeyAB/c1da5acb2be4496455d9433cfe861ef5/onnx_to_tf_to_tflite.ipynb


		</comment>
		<comment id='4' author='AlexeyAB' date='2020-07-22T11:44:37Z'>
		&lt;denchmark-link:https://github.com/freedomtan&gt;@freedomtan&lt;/denchmark-link&gt;
  Thank you for helping me find a typo in my code!
		</comment>
		<comment id='5' author='AlexeyAB' date='2020-07-22T13:11:16Z'>
		&lt;denchmark-link:https://github.com/AlexeyAB&gt;@AlexeyAB&lt;/denchmark-link&gt;
 no problem. I like your YOLOv4.
		</comment>
		<comment id='6' author='AlexeyAB' date='2020-07-22T23:26:26Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41608&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41608&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='AlexeyAB' date='2020-10-07T22:39:02Z'>
		&lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;
 Hi, is there any progress? It seems that even this way now doesn't work :

GPU disabled: (PB-&gt;TFLite) works well, and inference using TFLITE works well https://colab.research.google.com/gist/AlexeyAB/59e8a5e4116e03858bc1fd6c7f1a8d5f/onnx_to_tf_to_tflite.ipynb

		</comment>
	</comments>
</bug>