<bug id='38607' author='joaocaldeira' open_date='2020-04-16T15:37:47Z' closed_time='2020-04-16T16:02:37Z'>
	<summary>Gradients do not exist for variables manually added to wrapper layer</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.3
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
TensorFlow installed from (source or binary): binary
TensorFlow version: v2.1.0-rc2-17-ge5bf8de
Python version: 3.6.9
Bazel version (if compiling from source): n/a
GCC/Compiler version (if compiling from source): n/a
CUDA/cuDNN version: 10.2
GPU model and memory: RTX 2080 TI

Describe the current behavior
When using a Wrapper layer, parameters added using self.add_weight are being ignored by gradients although the outputs depend on those parameters. A regularization term added using self.add_loss also depends on the same parameters.
This is visible as the warnings WARNING:tensorflow:Gradients do not exist for variables ['loss_ignored_3/p_logit:0', 'loss_ignored_4/p_logit:0'] when minimizing the loss. come up.
This happened when I was trying to port the concrete dropout implementation in &lt;denchmark-link:https://github.com/yaringal/ConcreteDropout/blob/master/concrete-dropout-keras.ipynb&gt;https://github.com/yaringal/ConcreteDropout/blob/master/concrete-dropout-keras.ipynb&lt;/denchmark-link&gt;
 to tensorflow 2.
A minimal example with some of the complexity stripped away and not necessarily sensible math is posted below.
Describe the expected behavior
Gradients should exist for those variables, which should be optimized together with the neural network weights.

&lt;denchmark-link:https://colab.research.google.com/drive/1VQHWXn9UtI--aztmrUYuAgMdeOJ5p22u&gt;https://colab.research.google.com/drive/1VQHWXn9UtI--aztmrUYuAgMdeOJ5p22u&lt;/denchmark-link&gt;

An earlier version of this problem was posted in &lt;denchmark-link:https://stackoverflow.com/questions/61164373/loss-added-to-custom-layer-in-tensorflow-2-is-cleared-when-compiling&gt;https://stackoverflow.com/questions/61164373/loss-added-to-custom-layer-in-tensorflow-2-is-cleared-when-compiling&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='joaocaldeira' date='2020-04-16T16:02:37Z'>
		Operations were being carried out "outside of the gradient tape". Hat tip to Tom Charnock for the explanation. Working version in:
&lt;denchmark-link:https://colab.research.google.com/drive/19Q3rERNHQMrV9nM_BNp61byyH0_VE0YP&gt;https://colab.research.google.com/drive/19Q3rERNHQMrV9nM_BNp61byyH0_VE0YP&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='joaocaldeira' date='2020-04-16T16:02:39Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38607&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38607&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>