<bug id='29056' author='hoonkai' open_date='2019-05-27T11:34:59Z' closed_time='2019-05-28T18:17:36Z'>
	<summary>TFLite GPU delegate model inconsistencies: Mobilenet v1 1.0</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 0.0.1-gpu-experimental
Python version: 3.6
Bazel version (if compiling from source): n/a
GCC/Compiler version (if compiling from source): n/a
CUDA/cuDNN version: n/a
GPU model and memory: n/a

Describe the current behavior
The &lt;denchmark-link:https://storage.googleapis.com/download.tensorflow.org/models/tflite/gpu/mobilenet_v1_1.0_224.tflite&gt;mobilenet v1 1.0&lt;/denchmark-link&gt;
 model in the &lt;denchmark-link:https://www.tensorflow.org/lite/performance/gpu&gt;guide&lt;/denchmark-link&gt;
 contains a squeeze operation that isn't supported by GPU, but the &lt;denchmark-link:http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_1.0_224.tgz&gt;mobilenet v1 1.0&lt;/denchmark-link&gt;
 at  &lt;denchmark-link:https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md&gt;tensorflow/models&lt;/denchmark-link&gt;
 does. AFAIK, squeeze isn't a supported operation, but both of them contain at least one. How come the operations are different even when the models are of the same version? Is the one in the guide deliberately modified? If so, would it be a better idea to have it noted in the guide? The page on tensor/models claims 569mil MACs, but the number of MACs this modified 1.0 has is unclear.
What I've found is that the one provided in the guide contains 89 tensors but the one in tensorflow/models 88.
Describe the expected behavior
Models of the same version should be identical. Any modification should be explicitly documented.
	</description>
	<comments>
		<comment id='1' author='hoonkai' date='2019-05-27T17:26:53Z'>
		I've dug a bit deeper. The one in the guide has this additional tensor:
&lt; {'name': 'MobilenetV1/Logits/SpatialSqueeze_shape', 'index': 32, 'shape': array([2], dtype=int32), 'dtype': &lt;class 'numpy.int32'&gt;, 'quantization': (0.0, 0)}
Anyone know why?
Here's the list I got:
&lt;denchmark-code&gt;MobilenetV1/Conv2d_0/weights
MobilenetV1/Conv2d_10_depthwise/depthwise_weights
MobilenetV1/Conv2d_10_pointwise/weights
MobilenetV1/Conv2d_11_depthwise/depthwise_weights
MobilenetV1/Conv2d_11_pointwise/weights
MobilenetV1/Conv2d_12_depthwise/depthwise_weights
MobilenetV1/Conv2d_12_pointwise/weights
MobilenetV1/Conv2d_13_depthwise/depthwise_weights
MobilenetV1/Conv2d_13_pointwise/weights
MobilenetV1/Conv2d_1_depthwise/depthwise_weights
MobilenetV1/Conv2d_1_pointwise/weights
MobilenetV1/Conv2d_2_depthwise/depthwise_weights
MobilenetV1/Conv2d_2_pointwise/weights
MobilenetV1/Conv2d_3_depthwise/depthwise_weights
MobilenetV1/Conv2d_3_pointwise/weights
MobilenetV1/Conv2d_4_depthwise/depthwise_weights
MobilenetV1/Conv2d_4_pointwise/weights
MobilenetV1/Conv2d_5_depthwise/depthwise_weights
MobilenetV1/Conv2d_5_pointwise/weights
MobilenetV1/Conv2d_6_depthwise/depthwise_weights
MobilenetV1/Conv2d_6_pointwise/weights
MobilenetV1/Conv2d_7_depthwise/depthwise_weights
MobilenetV1/Conv2d_7_pointwise/weights
MobilenetV1/Conv2d_8_depthwise/depthwise_weights
MobilenetV1/Conv2d_8_pointwise/weights
MobilenetV1/Conv2d_9_depthwise/depthwise_weights
MobilenetV1/Conv2d_9_pointwise/weights
MobilenetV1/Logits/AvgPool_1a/AvgPool
MobilenetV1/Logits/Conv2d_1c_1x1/BiasAdd
MobilenetV1/Logits/Conv2d_1c_1x1/Conv2D_bias
MobilenetV1/Logits/Conv2d_1c_1x1/weights
MobilenetV1/Logits/SpatialSqueeze
MobilenetV1/Logits/SpatialSqueeze_shape
MobilenetV1/MobilenetV1/Conv2d_0/Conv2D_bias
MobilenetV1/MobilenetV1/Conv2d_0/Relu6
MobilenetV1/MobilenetV1/Conv2d_10_depthwise/Relu6
MobilenetV1/MobilenetV1/Conv2d_10_depthwise/depthwise_bias
MobilenetV1/MobilenetV1/Conv2d_10_pointwise/Conv2D_bias
MobilenetV1/MobilenetV1/Conv2d_10_pointwise/Relu6
MobilenetV1/MobilenetV1/Conv2d_11_depthwise/Relu6
MobilenetV1/MobilenetV1/Conv2d_11_depthwise/depthwise_bias
MobilenetV1/MobilenetV1/Conv2d_11_pointwise/Conv2D_bias
MobilenetV1/MobilenetV1/Conv2d_11_pointwise/Relu6
MobilenetV1/MobilenetV1/Conv2d_12_depthwise/Relu6
MobilenetV1/MobilenetV1/Conv2d_12_depthwise/depthwise_bias
MobilenetV1/MobilenetV1/Conv2d_12_pointwise/Conv2D_bias
MobilenetV1/MobilenetV1/Conv2d_12_pointwise/Relu6
MobilenetV1/MobilenetV1/Conv2d_13_depthwise/Relu6
MobilenetV1/MobilenetV1/Conv2d_13_depthwise/depthwise_bias
MobilenetV1/MobilenetV1/Conv2d_13_pointwise/Conv2D_bias
MobilenetV1/MobilenetV1/Conv2d_13_pointwise/Relu6
MobilenetV1/MobilenetV1/Conv2d_1_depthwise/Relu6
MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise_bias
MobilenetV1/MobilenetV1/Conv2d_1_pointwise/Conv2D_bias
MobilenetV1/MobilenetV1/Conv2d_1_pointwise/Relu6
MobilenetV1/MobilenetV1/Conv2d_2_depthwise/Relu6
MobilenetV1/MobilenetV1/Conv2d_2_depthwise/depthwise_bias
MobilenetV1/MobilenetV1/Conv2d_2_pointwise/Conv2D_bias
MobilenetV1/MobilenetV1/Conv2d_2_pointwise/Relu6
MobilenetV1/MobilenetV1/Conv2d_3_depthwise/Relu6
MobilenetV1/MobilenetV1/Conv2d_3_depthwise/depthwise_bias
MobilenetV1/MobilenetV1/Conv2d_3_pointwise/Conv2D_bias
MobilenetV1/MobilenetV1/Conv2d_3_pointwise/Relu6
MobilenetV1/MobilenetV1/Conv2d_4_depthwise/Relu6
MobilenetV1/MobilenetV1/Conv2d_4_depthwise/depthwise_bias
MobilenetV1/MobilenetV1/Conv2d_4_pointwise/Conv2D_bias
MobilenetV1/MobilenetV1/Conv2d_4_pointwise/Relu6
MobilenetV1/MobilenetV1/Conv2d_5_depthwise/Relu6
MobilenetV1/MobilenetV1/Conv2d_5_depthwise/depthwise_bias
MobilenetV1/MobilenetV1/Conv2d_5_pointwise/Conv2D_bias
MobilenetV1/MobilenetV1/Conv2d_5_pointwise/Relu6
MobilenetV1/MobilenetV1/Conv2d_6_depthwise/Relu6
MobilenetV1/MobilenetV1/Conv2d_6_depthwise/depthwise_bias
MobilenetV1/MobilenetV1/Conv2d_6_pointwise/Conv2D_bias
MobilenetV1/MobilenetV1/Conv2d_6_pointwise/Relu6
MobilenetV1/MobilenetV1/Conv2d_7_depthwise/Relu6
MobilenetV1/MobilenetV1/Conv2d_7_depthwise/depthwise_bias
MobilenetV1/MobilenetV1/Conv2d_7_pointwise/Conv2D_bias
MobilenetV1/MobilenetV1/Conv2d_7_pointwise/Relu6
MobilenetV1/MobilenetV1/Conv2d_8_depthwise/Relu6
MobilenetV1/MobilenetV1/Conv2d_8_depthwise/depthwise_bias
MobilenetV1/MobilenetV1/Conv2d_8_pointwise/Conv2D_bias
MobilenetV1/MobilenetV1/Conv2d_8_pointwise/Relu6
MobilenetV1/MobilenetV1/Conv2d_9_depthwise/Relu6
MobilenetV1/MobilenetV1/Conv2d_9_depthwise/depthwise_bias
MobilenetV1/MobilenetV1/Conv2d_9_pointwise/Conv2D_bias
MobilenetV1/MobilenetV1/Conv2d_9_pointwise/Relu6
MobilenetV1/Predictions/Reshape_1
input
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='hoonkai' date='2019-05-28T18:17:35Z'>
		&lt;denchmark-link:https://github.com/hoonkai&gt;@hoonkai&lt;/denchmark-link&gt;

IIRC the official model includes SQUEEZE, whereby the GPU-compatible model replaces the op with RESHAPE.   There can be multiple ways to represent a certain operation, and the one we present in the GPU page is rewritten to have no GPU-incompatible ops.
I don't think going and updating the official MobileNet release is the scope of the GPU backend, and that may break some other people.
		</comment>
	</comments>
</bug>