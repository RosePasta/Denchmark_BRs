<bug id='38906' author='jakublipinski' open_date='2020-04-26T08:42:01Z' closed_time='2020-06-05T15:53:21Z'>
	<summary>MemoryOptimizer produces broken graph with AlreadyExistsError exception while running GRU layer on Tensorflow 2.2.0rc_3</summary>
	<description>
System information

Custom model built using keras
MacBook Pro, 8-Core Intel Core i9, macOS Catalina 10.15.4
TensorFlow installed from pip in virtual environment
TensorFlow v2.2.0-rc2-77-gaad398b5e9 2.2.0-rc3
Python 3.7.5
Running on CPU

Describe the current behavior
The code snippet listed below outputs multiple tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource warnings and finally exists with tensorflow.python.framework.errors_impl.AlreadyExistsError exception.
Note: the code works correctly if the GRU layer size is decreased from 320 to 80. It also works if TensorFlow is downgraded to version 2.0.1.
The issue is related to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/23780&gt;#23780&lt;/denchmark-link&gt;
 issue reported in 2018. This issue offers code to reproduce it and occurs on the latest version of TensorFlow.
Describe the expected behavior
The code should work without exception.
Standalone code to reproduce the issue
import numpy as np

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Bidirectional, GRU
from tensorflow.keras.layers import Conv1D, MaxPooling1D

x = np.random.rand(1000, 401, 17)
y = np.random.choice([0, 1], size=(1000, 301))

model = Sequential()
model.add(Conv1D(filters=320, kernel_size=26, activation='relu', input_shape=(401, x.shape[2])))
model.add(MaxPooling1D(pool_size=13, strides=13))
model.add(Bidirectional(GRU(320, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))
model.add(Flatten())
model.add(Dense(2000, activation="relu"))
model.add(Dense(301, activation="sigmoid"))
model.compile(loss="binary_crossentropy", optimizer="rmsprop", metrics=["accuracy"])

model.summary()

model.fit(x=x, y=y, epochs=1, verbose=1)
The Google Colab notebook is available &lt;denchmark-link:https://colab.research.google.com/drive/1YohTA6Hxi3H6aOQgFj34C0tKErW2V_rL&gt;here&lt;/denchmark-link&gt;
. The error is reproducible.
Other info / logs
The code above generates following output:
&lt;denchmark-code&gt;Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1d (Conv1D)              (None, 376, 320)          141760    
_________________________________________________________________
max_pooling1d (MaxPooling1D) (None, 28, 320)           0         
_________________________________________________________________
bidirectional (Bidirectional (None, 28, 640)           1232640   
_________________________________________________________________
flatten (Flatten)            (None, 17920)             0         
_________________________________________________________________
dense (Dense)                (None, 2000)              35842000  
_________________________________________________________________
dense_1 (Dense)              (None, 301)               602301    
=================================================================
Total params: 37,818,701
Trainable params: 37,818,701
Non-trainable params: 0
_________________________________________________________________
2020-04-26 10:19:57.349570: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_0/gradient_tape/sequential/bidirectional/backward_gru/while/sequential/bidirectional/backward_gru/while_grad/body/_877/gradients/AddN_8/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE
2020-04-26 10:19:57.363399: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_0/gradient_tape/sequential/bidirectional/backward_gru/while/sequential/bidirectional/backward_gru/while_grad/body/_877/gradients/AddN_7/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE
2020-04-26 10:19:57.377361: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_0/gradient_tape/sequential/bidirectional/backward_gru/while/sequential/bidirectional/backward_gru/while_grad/body/_877/gradients/AddN_8/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE
... (repeated multiple times) ...
2020-04-26 10:19:57.677304: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_0/gradient_tape/sequential/bidirectional/forward_gru/while/sequential/bidirectional/forward_gru/while_grad/body/_577/gradients/AddN_7/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE
Traceback (most recent call last):
  File "alreadyexists_err.py", line 21, in &lt;module&gt;
    model.fit(x=x, y=y, epochs=1, verbose=1)
  File "venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py", line 66, in _method_wrapper
    return method(self, *args, **kwargs)
  File "venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py", line 851, in fit
    tmp_logs = train_function(iterator)
  File "venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py", line 580, in __call__
    result = self._call(*args, **kwds)
  File "venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py", line 644, in _call
    return self._stateless_fn(*args, **kwds)
  File "venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 2420, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File "venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 1665, in _filtered_call
    self.captured_inputs)
  File "venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 1746, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 598, in call
    ctx=ctx)
  File "venv/lib/python3.7/site-packages/tensorflow/python/eager/execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.AlreadyExistsError:  Resource __per_step_0/gradient_tape/sequential/bidirectional/backward_gru/while/sequential/bidirectional/backward_gru/while_grad/body/_877/gradients/AddN_8/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE
	 [[{{node gradient_tape/sequential/bidirectional/backward_gru/while/sequential/bidirectional/backward_gru/while_grad/body/_877/gradients/AddN_8/tmp_var}}]] [Op:__inference_train_function_7551]

Function call stack:
train_function
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='jakublipinski' date='2020-04-27T09:46:15Z'>
		I have tried on colab with TF version 2.2.0-rc3 and was able to reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/8652e1dae44721b792b47b6e9e45052c/untitled828.ipynb&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='2' author='jakublipinski' date='2020-04-27T16:11:55Z'>
		Adding &lt;denchmark-link:https://github.com/saxenasaurabh&gt;@saxenasaurabh&lt;/denchmark-link&gt;
 who works on core ops to provide more information about while loop.
		</comment>
		<comment id='3' author='jakublipinski' date='2020-04-27T16:26:33Z'>
		&lt;denchmark-link:https://github.com/rmlarsen&gt;@rmlarsen&lt;/denchmark-link&gt;
 mentioned &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/23780#issuecomment-498334346&gt;here&lt;/denchmark-link&gt;
 that this may be due to a bug in grappler generating non-unique names. &lt;denchmark-link:https://github.com/rmlarsen&gt;@rmlarsen&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ezhulenev&gt;@ezhulenev&lt;/denchmark-link&gt;
 do you know if this was fixed?
		</comment>
		<comment id='4' author='jakublipinski' date='2020-04-28T11:05:30Z'>
		I dug a bit further into this issue. This is what I found:
It's indeed an optimizer issue, namely the ScopedAllocatorOptimizer optimizer.
Interestingly, you cannot turn this optimizer off using tf.config.optimizer.set_experimental_options(). The scoped_allocator_optimization key is treated as Bool at:



tensorflow/tensorflow/python/eager/context.py


         Line 958
      in
      109fd38






 rewriter_toggle("scoped_allocator_optimization") 





while it's interpreted as int at:



tensorflow/tensorflow/core/grappler/optimizers/meta_optimizer.cc


         Line 292
      in
      8e0eecc






 if (cfg_.scoped_allocator_optimization()) { 





Should I file another issue for that?
		</comment>
		<comment id='5' author='jakublipinski' date='2020-04-29T16:08:15Z'>
		Let me rename this bug to pointing to ScopedAllocatorOptimizer.
		</comment>
		<comment id='6' author='jakublipinski' date='2020-06-05T15:53:21Z'>
		Verified this is fixed with latest tf-nightly.
		</comment>
		<comment id='7' author='jakublipinski' date='2020-06-05T15:53:23Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38906&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38906&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>