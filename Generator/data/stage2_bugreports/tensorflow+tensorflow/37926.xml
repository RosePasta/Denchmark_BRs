<bug id='37926' author='bela127' open_date='2020-03-26T06:12:39Z' closed_time='2020-09-24T23:12:29Z'>
	<summary>ValueError: Unknown floatx type: bfloat16 tf.keras.backend.set_floatx('bfloat16')</summary>
	<description>
System information
Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): No
OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Linux Mint 19.3 (ubuntu)
TensorFlow installed from (source or
binary): binary
TensorFlow version (use command below): tf-nightly
Python version: 3.8 and 3.6
Describe the current behavior
when using layers with bfloat16 they are autocast to float32, and a warning/info gives a hint to use
tf.keras.backend.set_floatx('bfloat16')
when using the function a exception is raised
ValueError: Unknown floatx type: bfloat16
Describe the expected behavior
data type should be recognised
Standalone code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
Other info / logs Include any logs or source code that would be helpful to
Info from Tensorflow:
&lt;denchmark-code&gt;WARNING:tensorflow:Layer compress is casting an input tensor from dtype bfloat16 to the layer's dtype of float16, which is new behavior in TensorFlow 2.  The layer has dtype float16 because its dtype defaults to floatx.

If you intended to run this layer in float16, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype bfloat16 by default, call `tf.keras.backend.set_floatx('bfloat16')`. To change just this layer, pass dtype='bfloat16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.
&lt;/denchmark-code&gt;

Error if tf.keras.backend.set_floatx('bfloat16') is used:
&lt;denchmark-code&gt;2.2.0-dev20200325
Traceback (most recent call last):
  File "/usr/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/usr/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home//.vscode-server/extensions/ms-python.python-2020.3.69010/pythonFiles/lib/python/debugpy/no_wheels/debugpy/__main__.py", line 45, in &lt;module&gt;
    cli.main()
  File "/home//.vscode-server/extensions/ms-python.python-2020.3.69010/pythonFiles/lib/python/debugpy/no_wheels/debugpy/../debugpy/server/cli.py", line 427, in main
    run()
  File "/home//.vscode-server/extensions/ms-python.python-2020.3.69010/pythonFiles/lib/python/debugpy/no_wheels/debugpy/../debugpy/server/cli.py", line 264, in run_file
    runpy.run_path(options.target, run_name="__main__")
  File "/usr/lib/python3.6/runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "/usr/lib/python3.6/runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "/usr/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home//Docker/volumes/3D_Person_Pose_Estimation_from_2D_Singelview_Image_Data/src/ShAReD_Net/training/run_train_train_model.py", line 206, in &lt;module&gt;
    main()
  File "/home//Docker/volumes/3D_Person_Pose_Estimation_from_2D_Singelview_Image_Data/src/ShAReD_Net/training/run_train_train_model.py", line 50, in main
    tf.keras.backend.set_floatx(dtype.name)
  File "/home//Docker/volumes/3D_Person_Pose_Estimation_from_2D_Singelview_Image_Data/src/venv/lib/python3.6/site-packages/tensorflow/python/keras/backend_config.py", line 108, in set_floatx
    raise ValueError('Unknown floatx type: ' + str(value))
ValueError: Unknown floatx type: bfloat16```
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='bela127' date='2020-03-26T10:21:18Z'>
		&lt;denchmark-link:https://github.com/bela127&gt;@bela127&lt;/denchmark-link&gt;
, Please provide the standalone code to reproduce the issue. Thanks
		</comment>
		<comment id='2' author='bela127' date='2020-03-26T16:00:36Z'>
		oh i see, the standalone code is given ;)
its just that small
just run:
tf.keras.backend.set_floatx('bfloat16')
		</comment>
		<comment id='3' author='bela127' date='2020-03-27T12:48:26Z'>
		&lt;denchmark-link:https://github.com/bela127&gt;@bela127&lt;/denchmark-link&gt;
, tf.keras.backend.set_floatx(value), here value should be  or  or . For more read the &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/backend/set_floatx?version=nightly&gt;Tensorflow doc&lt;/denchmark-link&gt;
. Thanks
		</comment>
		<comment id='4' author='bela127' date='2020-03-27T15:14:20Z'>
		then the warning is very misleading, it explicitly tells to use tf.keras.backend.set_floatx('bfloat16') and with this restriction it is not practical to build models with bfloat16. which in fact is necessary for training without numeric instability.
I think the message should be fixed , and/or
should i file a feature request?
		</comment>
		<comment id='5' author='bela127' date='2020-03-30T20:41:55Z'>
		Thank you for this issue. The warning message is incorrect, as you noted.
I can't think of a use case of setting floatx to bfloat16. If you have one, let me know, and we can consider allowing it to be set to bfloat16. Otherwise I'll fix the error message by removing the suggestion to call set_floatx('bfloat16')
		</comment>
		<comment id='6' author='bela127' date='2020-03-31T12:23:35Z'>
		Hello,
I have a verry big model, it only can fit in memory with bfloat16 or float16 on a rtx2080.
Float 16 gives me a lot of Nan because of a early overflow. Bfloat16 would have a larger dynamic with still less memory.
The only solution at the moment is training the model on CPU (a lot of ram is available) but it's very slow.
		</comment>
		<comment id='7' author='bela127' date='2020-03-31T18:47:00Z'>
		Bfloat16 is not supported on Nvidia GPUs, such as the rtx2080, so you have to use float16 or float32. Neither bfloat16 or float16 is supported well on CPUs either, and will probably run slower. If running on the CPU and you have enough RAM, try float32 instead.
Also, most models require &lt;denchmark-link:https://www.tensorflow.org/guide/keras/mixed_precision&gt;mixed precision&lt;/denchmark-link&gt;
 instead of just using float16/bfloat16 for numeric stability. Mixed precision might fix the NaN issue.
		</comment>
		<comment id='8' author='bela127' date='2020-03-31T20:18:31Z'>
		I will try, thanks
		</comment>
		<comment id='9' author='bela127' date='2020-09-16T23:42:43Z'>
		&lt;denchmark-link:https://github.com/bela127&gt;@bela127&lt;/denchmark-link&gt;
 As mentioned by &lt;denchmark-link:https://github.com/reedwm&gt;@reedwm&lt;/denchmark-link&gt;
 using  results in numerical instability issues. There is a note on TF website about that. Please check &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/backend/set_floatx&gt;here&lt;/denchmark-link&gt;
.

Note: It is not recommended to set this to float16 for training, as this will likely cause numeric stability issues. Instead, mixed precision, which is using a mix of float16 and float32, can be used by calling tf.keras.mixed_precision.experimental.set_policy('mixed_float16'). See the mixed precision guide for details.

You can check the example code in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/38457&gt;this issue&lt;/denchmark-link&gt;
 that throws NaN randomly sometimes.
If you plan to use mixed_precision with GPU, add the following lines
&lt;denchmark-code&gt;from tensorflow.keras.mixed_precision import experimental as mixed_precision

policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_policy(policy)
&lt;/denchmark-code&gt;

Please verify once and close the issue if this was resolved for you. Thanks!
If you plan to use mixed_precision with CPU, your code runs slower and code throws clear warning as shown below

WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING
The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.
If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once

		</comment>
		<comment id='10' author='bela127' date='2020-09-24T23:12:29Z'>
		Closing this as this issue was resolved with the above comment. Please feel free to reopen if I am mistaken. Thanks!
		</comment>
		<comment id='11' author='bela127' date='2020-09-24T23:12:31Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37926&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37926&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>