<bug id='41257' author='8bitmp3' open_date='2020-07-10T00:44:41Z' closed_time='2020-11-19T18:55:24Z'>
	<summary>Configuring `TF_CONFIG` in practice + a small "bug" in the Multi-worker training with Keras tutorial</summary>
	<description>
Hey &lt;denchmark-link:https://github.com/lamberta&gt;@lamberta&lt;/denchmark-link&gt;
 and team 
A couple of things here related to the Multi-worker training with Keras tutorial (with tf.distribute.experimental.MultiWorkerMirroredStrategy()):
&lt;denchmark-h:h3&gt;1. Configuring the TF_CONFIG environment variable for multi-worker training in practice&lt;/denchmark-h&gt;

 &lt;denchmark-link:https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#multi-worker_configuration&gt;https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#multi-worker_configuration&lt;/denchmark-link&gt;


There is an important section dedicated to &lt;denchmark-link:https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#multi-worker_configuration&gt;Multi-worker Configuration&lt;/denchmark-link&gt;
, which states that outside of the free Colab environment:

In practice, users would create multiple workers on external IP addresses/ports, and set TF_CONFIG on each worker appropriately.

The current example snippet shows that in the task component of TF_CONFIG, the index value of the worker is set to 0 (task is different on each worker):
# Import necessary libraries (not currently included in the tutorial)
import json
import os

os.environ['TF_CONFIG'] = json.dumps({
    'cluster': {
        'worker': ["localhost:12345", "localhost:23456"]
    },
    'task': {'type': 'worker', 'index': 0}
})
However, this may create some confusion, as demonstrated in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/39922&gt;#39922&lt;/denchmark-link&gt;
. The user tried setting the  environment variable (with ), as shown in the tutorial, but that wouldn't let the rest of the script proceed with execution. If the user runs the experiment with , all is OK.
As mentioned in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/39922&gt;#39922&lt;/denchmark-link&gt;
, you should be following this TensorFlow distributed training &lt;denchmark-link:https://www.tensorflow.org/guide/distributed_training#setting_up_tf_config_environment_variable&gt;guide&lt;/denchmark-link&gt;
, where it says:

One example of TF_CONFIG is:
os.environ["TF_CONFIG"] = json.dumps({
    "cluster": {
        "worker": ["host1:port", "host2:port", "host3:port"],
        "ps": ["host4:port", "host5:port"]
    },
   "task": {"type": "worker", "index": 1}
})

Proposed solution:

Borrow the sample code from the Setting up TF_CONFIG environment variable section from the Distributed training with TensorFlow guide into; and
Add this code sample to the Multi-worker Configuration section of the Multi-worker training with Keras tutorial.

For example:

In this example, we set the task type to "worker" and the task index to 0. This means the machine that has such setting is the first worker...
...
[ORIGINAL CODE SNIPPET]
...
In your use case, you may also set the task type to "worker" and the task index to 1 instead of 0.
# Import necessary libraries (not currently included in the tutorial)
import json
import os
os.environ["TF_CONFIG"] = json.dumps({
    "cluster": {
        "worker": ["host1:port", "host2:port", "host3:port"],
        "ps": ["host4:port", "host5:port"]
    },
   "task": {"type": "worker", "index": 1}
})

&lt;denchmark-h:h3&gt;2. A small "bug" in the Dataset sharding and batch size section&lt;/denchmark-h&gt;

 &lt;denchmark-link:https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#dataset_sharding_and_batch_size&gt;https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#dataset_sharding_and_batch_size&lt;/denchmark-link&gt;

Description: The following sentence in Dataset sharding and batch size looks a bit odd:

If you prefer manual sharding for your training, automatic sharding can be turned off via tf.data.experimental.DistributeOptions api. Concretely,

(this is followed by a code snippet)
&lt;denchmark-h:h3&gt;Submit a pull request?&lt;/denchmark-h&gt;

Yes for (1), once this has been clarified.
Regarding (2), I think it's up to the team.
	</description>
	<comments>
		<comment id='1' author='8bitmp3' date='2020-07-27T04:36:23Z'>
		Hi, apologies for the late response.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/1&gt;#1&lt;/denchmark-link&gt;
 Re: TF_CONFIG - I am not sure how the 2 examples are different. One of them uses example of TF_CONFIG from task 0 and the other one uses task id 1. One is not more correct than the other. If there are 2 workers, the user needs to setup one task with id=0 and one with id=1. What we can probably change in the tutorial is clarify that the task index should be different on both workers. But I don't think the &lt;denchmark-link:https://www.tensorflow.org/guide/distributed_training#setting_up_tf_config_environment_variable&gt;current explanation in the guide&lt;/denchmark-link&gt;
 is any better than the &lt;denchmark-link:https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#choose_the_right_strategy&gt;current one in the multi worker tutorial&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/2&gt;#2&lt;/denchmark-link&gt;
 Sorry, I am not quite sure what "bug" are you referring to. Could you elaborate?
		</comment>
		<comment id='2' author='8bitmp3' date='2020-07-27T22:37:51Z'>
		
Hi, apologies for the late response.

No worries &lt;denchmark-link:https://github.com/guptapriya&gt;@guptapriya&lt;/denchmark-link&gt;
 

What we can probably change in the tutorial is clarify that the task index should be different on both workers. But I don't think the current explanation in the guide is any better than the current one in the multi worker tutorial

It's likely that the Issue another user raised was due to the confusion over the 'index' paramter (set to 0 by default in the original guide).
Perhaps, we can mention that one you (the user) can use the value or 1 and above - WDYT?
For example:

You can also set the task 'index' to 1 instead of 0 [use case explanation].

or something along those lines.

#2 Sorry, I am not quite sure what "bug" are you referring to. Could you elaborate?

Pardon me. The last sentence starts with "Concretely" and it's followed by a comma. Maybe, we could use "For example:" (with a colon)?

If you prefer manual sharding for your training, automatic sharding can be turned off via tf.data.experimental.DistributeOptions API. For example:

options = tf.data.Options()
options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF
dataset_no_auto_shard = multi_worker_dataset.with_options(options)
I've also capitalized "API" in "tf.data.experimental.DistributeOptions API".
Let me know what you think. Thank you!
		</comment>
		<comment id='3' author='8bitmp3' date='2020-08-03T05:32:38Z'>
		
Perhaps, we can mention that one you (the user) can use the value or 1 and above - WDYT?
For example:

You can also set the task 'index' to 1 instead of 0 [use case explanation].

or something along those lines.

I am still not sure why 1 is better than 0. I think the core issue that they should be different on each worker. So if you have 2 workers, it should be 0 and 1. If you have 3 workers, it should be 0, 1, and 2. Is that what you're saying too?


#2 Sorry, I am not quite sure what "bug" are you referring to. Could you elaborate?

Pardon me. The last sentence starts with "Concretely" and it's followed by a comma. Maybe, we could use "For example:" (with a colon)?

If you prefer manual sharding for your training, automatic sharding can be turned off via tf.data.experimental.DistributeOptions API. For example:

options = tf.data.Options()
options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF
dataset_no_auto_shard = multi_worker_dataset.with_options(options)
I've also capitalized "API" in "tf.data.experimental.DistributeOptions API".
Let me know what you think. Thank you!

Sure, those changes sound fine to me üëç
		</comment>
		<comment id='4' author='8bitmp3' date='2020-08-05T20:13:21Z'>
		
I think the core issue that they should be different on each worker. So if you have 2 workers, it should be 0 and 1. If you have 3 workers, it should be 0, 1, and 2. Is that what you're saying too?

&lt;denchmark-link:https://github.com/guptapriya&gt;@guptapriya&lt;/denchmark-link&gt;
 I really like this idea - it's simple and may help with future potential issues, such as &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/39922&gt;#39922&lt;/denchmark-link&gt;
.
What do you think about adding this one sentence at the end (I've included two existing paragraphs for context):
In this example, we set the task `type` to `"worker"` and the task `index` to `0`. 
This means the machine that has such setting is the first worker, which will 
be appointed as the chief worker and do more work than other workers. Note 
that other machines will need to have `TF_CONFIG` environment variable set as well, 
and it should have the same `cluster` dict, but different task `type` or task `index` 
depending on what the roles of those machines are.

For illustration purposes, this tutorial shows how one may set a `TF_CONFIG` with 2 
workers on `localhost`. In practice, users would create multiple workers on external 
IP addresses/ports, and set `TF_CONFIG` on each worker appropriately.
+ So, for example, if you have 2 workers, you should set the task `index` to `0` and `1` separately.
Thank you!
		</comment>
		<comment id='5' author='8bitmp3' date='2020-08-08T02:05:19Z'>
		&lt;denchmark-link:https://github.com/8bitmp3&gt;@8bitmp3&lt;/denchmark-link&gt;
 that SGTM, feel free to send a PR to update the tutorial (maybe in both places). thanks!
		</comment>
		<comment id='6' author='8bitmp3' date='2020-11-19T18:55:25Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41257&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41257&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>