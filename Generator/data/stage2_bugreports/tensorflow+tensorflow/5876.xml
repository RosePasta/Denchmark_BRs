<bug id='5876' author='kohpangwei' open_date='2016-11-27T02:47:29Z' closed_time='2017-02-01T21:20:32Z'>
	<summary>Error in hessians() and _hessian_vector_product() for tf.nn.sparse_softmax_cross_entropy_with_logits()</summary>
	<description>
On a simple model that implements logistic regression, constructing the loss using tf.nn.sparse_softmax_cross_entropy_with_logits() makes both hessians() and _hessian_vector_product() return identically zero vectors, which is incorrect. If I instead write the loss function manually using tf.log, tf.sigmoid, etc., hessians() and _hessian_vector_product return the correct answer. These two versions of the loss function agree on their values and their gradients; however, the Hessian output is different.
Here is some sample output:
&lt;denchmark-code&gt;Using sparse_softmax_cross_entropy_with_logits:
Loss before first step: 0.686726
Loss after first step : 0.686181
Actual diff in grad:
[ 0.000122    0.00014928]
Predicted diff in grad using _hessian_vector_product:
[array([ 0.,  0.], dtype=float32)]
Hessian:
[array([[ 0.,  0.],
       [ 0.,  0.]], dtype=float32)]

Using custom loss function:
Loss before first step: 0.686726
Loss after first step : 0.686181
Actual diff in grad:
[ 0.00012201  0.00014931]
Predicted diff in grad using _hessian_vector_product:
[array([ 0.00012199,  0.0001493 ], dtype=float32)]
Hessian:
[array([[ 0.08229966,  0.        ],
       [ 0.        ,  0.08278375]], dtype=float32)]
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?&lt;/denchmark-h&gt;

None that I can find. The code below uses hessians() and _hessian_vector_product() from &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/a4c8df209d7413068f4ed3e71c43eb798fbd5580/tensorflow/python/ops/gradients_impl.py&gt;https://github.com/tensorflow/tensorflow/blob/a4c8df209d7413068f4ed3e71c43eb798fbd5580/tensorflow/python/ops/gradients_impl.py&lt;/denchmark-link&gt;

Here is the PR that implemented hessians(): &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/5329&gt;#5329&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Environment info&lt;/denchmark-h&gt;

Operating System: Ubuntu 16.04
Installed version of CUDA and cuDNN:
&lt;denchmark-code&gt;-rw-r--r-- 1 root root   558720 Oct  1 00:18 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Oct  1 00:18 /usr/local/cuda/lib64/libcudart.so -&gt; libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Oct  1 00:18 /usr/local/cuda/lib64/libcudart.so.8.0 -&gt; libcudart.so.8.0.44
-rwxr-xr-x 1 root root   415432 Oct  1 00:18 /usr/local/cuda/lib64/libcudart.so.8.0.44
-rw-r--r-- 1 root root   775162 Oct  1 00:18 /usr/local/cuda/lib64/libcudart_static.a
-rwxr-xr-x 1 root root 78065952 Oct  1 16:19 /usr/local/cuda/lib64/libcudnn.so
-rwxr-xr-x 1 root root 78065952 Oct  1 16:19 /usr/local/cuda/lib64/libcudnn.so.5
-rwxr-xr-x 1 root root 78065952 Oct  1 16:19 /usr/local/cuda/lib64/libcudnn.so.5.0.5
-rw-r--r-- 1 root root 68709594 Oct  1 16:19 /usr/local/cuda/lib64/libcudnn_static.a
&lt;/denchmark-code&gt;

The same behavior occurs when running on CPU only.
Installed from: &lt;denchmark-link:https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl&gt;https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl&lt;/denchmark-link&gt;

v0.11.0
&lt;denchmark-h:h3&gt;If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;tf.set_random_seed(0)

### Setup toy data and weights
images_placeholder = tf.placeholder(tf.float32, shape=(3, 2))
labels_placeholder = tf.placeholder(tf.int32, shape=(3))
feed_dict = {
    images_placeholder: np.array([[0, 0], [0, 1], [1, 0]]),
    labels_placeholder: np.array([0, 1, 1]),
}
  
weights = tf.Variable(
  tf.truncated_normal([2],
                      stddev=1.0 / math.sqrt(float(2))),
  name='weights')

### Calculate loss using built-in TF function
weights_with_zeros = tf.pack([tf.zeros([2]), weights], axis=1)
logits = tf.matmul(images_placeholder, weights_with_zeros)
cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels_placeholder)
loss = tf.reduce_mean(cross_entropy)

### Calculate loss using manually constructed TF function
logits2 = tf.matmul(images_placeholder, tf.reshape(weights, [2, 1]))
labels2 = (tf.to_float(labels_placeholder) * 2) - 1
logits_mul_labels = tf.mul(tf.reshape(logits2, [-1]), tf.reshape(labels2, [-1]))
cross_entropy2 = - tf.log(tf.sigmoid(logits_mul_labels))
loss2 = tf.reduce_mean(cross_entropy2)

### Create train_op
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
global_step = tf.Variable(0, name='global_step', trainable=False)
train_op = optimizer.minimize(loss, global_step=global_step)

### Calculate gradients, Hessians, and Hessian-vector products for both versions of loss
grad = tf.gradients(loss, [weights])
grad2 = tf.gradients(loss2, [weights])
v_placeholder = tf.placeholder(tf.float32, shape=weights.get_shape())
hessian_vector = _hessian_vector_product(loss, [weights], [v_placeholder])
hessian_vector2 = _hessian_vector_product(loss2, [weights], [v_placeholder])
hessian = hessians(loss, [weights])
hessian2 = hessians(loss2, [weights])

### Run training for a single step to get the parameters to change.
init = tf.initialize_all_variables()
sess = tf.Session()
sess.run(init)

old_weights_val, old_loss_val, old_grad_val, old_loss2_val, old_grad2_val= sess.run(
  [weights, loss, grad, loss2, grad2], 
  feed_dict=feed_dict)

_ = sess.run(train_op, feed_dict=feed_dict)

new_weights_val, new_loss_val, new_grad_val, new_loss2_val, new_grad2_val = sess.run(
  [weights, loss, grad, loss2, grad2], 
  feed_dict=feed_dict)

hessian_val, hessian2_val = sess.run(
  [hessian, hessian2], 
  feed_dict=feed_dict)

### Calculate the actual difference in gradients before and after the train step,
### and compare with the predicted difference in gradients based on the Hessian.
diff_in_weights = new_weights_val - old_weights_val
actual_diff_in_grad = new_grad_val[0] - old_grad_val[0]
actual_diff_in_grad2 = new_grad2_val[0] - old_grad2_val[0]

feed_dict[v_placeholder] = diff_in_weights
predicted_diff_in_grad = sess.run(hessian_vector, feed_dict=feed_dict)
predicted_diff_in_grad2 = sess.run(hessian_vector2, feed_dict=feed_dict)

print('Diff in weights:\n%s' % diff_in_weights)

print('\nUsing sparse_softmax_cross_entropy_with_logits:')
print('Loss before first step: %s' % old_loss_val)
print('Loss after first step : %s' % new_loss_val)
print('Actual diff in grad:\n%s' % actual_diff_in_grad)
print('Predicted diff in grad using _hessian_vector_product:\n%s' % predicted_diff_in_grad)
print('Hessian:\n%s' % hessian_val)

print('\nUsing custom loss function:')
print('Loss before first step: %s' % old_loss2_val)
print('Loss after first step : %s' % new_loss2_val)
print('Actual diff in grad:\n%s' % actual_diff_in_grad2)
print('Predicted diff in grad using _hessian_vector_product:\n%s' % predicted_diff_in_grad2)
print('Hessian:\n%s' % hessian2_val)

sess.close()

&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;What other attempted solutions have you tried?&lt;/denchmark-h&gt;

Running in CPU or GPU makes no difference.
Using more complicated networks (i.e., adding some non-linear hidden layers before the linear softmax step) makes the Hessian returned from sparse_softmax_cross_entropy_with_logits() non-zero, but the returned value is still wrong in the sense that it does not match the empirical values. In contrast, using the same custom loss function above returns the correct Hessians.
The same problem occurs when using "real" data (e.g., MNIST) or with more examples.
&lt;denchmark-h:h3&gt;Logs or other output that would be helpful&lt;/denchmark-h&gt;

Full output when using CPU:
&lt;denchmark-code&gt;Diff in weights:
[ 0.00148226  0.0018035 ]

Using sparse_softmax_cross_entropy_with_logits:
Loss before first step: 0.686726
Loss after first step : 0.686181
Actual diff in grad:
[ 0.000122    0.00014928]
Predicted diff in grad using _hessian_vector_product:
[array([ 0.,  0.], dtype=float32)]
Hessian:
[array([[ 0.,  0.],
       [ 0.,  0.]], dtype=float32)]

Using custom loss function:
Loss before first step: 0.686726
Loss after first step : 0.686181
Actual diff in grad:
[ 0.00012201  0.00014931]
Predicted diff in grad using _hessian_vector_product:
[array([ 0.00012199,  0.0001493 ], dtype=float32)]
Hessian:
[array([[ 0.08229966,  0.        ],
       [ 0.        ,  0.08278375]], dtype=float32)]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='kohpangwei' date='2016-11-27T20:18:17Z'>
		&lt;denchmark-link:https://github.com/goodfeli&gt;@goodfeli&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/tillahoffmann&gt;@tillahoffmann&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/vrv&gt;@vrv&lt;/denchmark-link&gt;
 This is possibly related to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/5329&gt;#5329&lt;/denchmark-link&gt;
 which you were all involved with.  Could somebody please comment on the above?
		</comment>
		<comment id='2' author='kohpangwei' date='2016-11-27T20:24:32Z'>
		Will have a look tomorrow.
		</comment>
		<comment id='3' author='kohpangwei' date='2016-11-28T09:18:07Z'>
		I won't have time to dig into this properly for a while but here's a hunch: The  presumably indexes the tensor of logits and some of the indexing operations don't have gradients defined (cf. &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/206&gt;#206&lt;/denchmark-link&gt;
). It may be possible to compute the first derivative of a function but the second derivative may not yet be implemented. &lt;denchmark-link:https://github.com/benoitsteiner&gt;@benoitsteiner&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/yuefengz&gt;@yuefengz&lt;/denchmark-link&gt;
 probably have a better understanding of the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_ops.py#L1392&gt;implementation&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='4' author='kohpangwei' date='2016-11-29T00:49:06Z'>
		I agree with &lt;denchmark-link:https://github.com/tillahoffmann&gt;@tillahoffmann&lt;/denchmark-link&gt;
 that this sounds like the gradient of  does not implement the gradient correctly.
If this is indeed caused by an op failing to implement the gradient, it seems like tf.gradients ought to raise a NotImplementedError or issue some other error message, rather than silently returning numerically incorrect values.
Vincent Dumoulin and Alex Kurakin have both told me they have had trouble with ops silently returning zero as their second derivative in the past.
		</comment>
		<comment id='5' author='kohpangwei' date='2016-11-29T01:54:16Z'>
		Thanks for looking into this! On further testing, it seems like the problem is not localized to sparse_softmax_cross_entropy_with_logits. Implementing the same example using softmax_cross_entropy_with_logits (i.e., changing the labels from [0, 1, 1] to [[1, 0], [0, 1], [0, 1]]) has the same issue of a zero second derivative. So the problem might be in the softmax function itself and not the sparse indexing.
		</comment>
		<comment id='6' author='kohpangwei' date='2016-11-29T01:58:47Z'>
		The softmax function shouldn't actually be involved in the implementation of either of those costs (the numerically stable way to implement them involves simplifying log softmax x to x - log sum exp x), so I doubt that it's a problem with the softmax op.
		</comment>
		<comment id='7' author='kohpangwei' date='2016-11-29T02:06:13Z'>
		Oh, right. Sorry, I was sloppy with that statement. I meant to say that the problem is probably in the (presumably shared)  part of the function and not in the additional indexing operations that  does on top of that, which &lt;denchmark-link:https://github.com/tillahoffmann&gt;@tillahoffmann&lt;/denchmark-link&gt;
 suggested was the culprit.
		</comment>
		<comment id='8' author='kohpangwei' date='2016-11-29T09:21:17Z'>
		Hm, I'm computing Hessians of cross entropy losses in my work and don't seem to have a problem with them. Will have a look into a minimum working example and get back to you.
		</comment>
		<comment id='9' author='kohpangwei' date='2016-11-29T16:37:50Z'>
		According to the 0.12 release notes, there are some ops whose second gradients have been fixed, so it might be worth upgrading to 0.12rc0 (just released last night) and see if the problem persists
		</comment>
		<comment id='10' author='kohpangwei' date='2016-11-30T05:30:34Z'>
		Thanks for the tip! I just tested the above example in 0.12rc0 but unfortunately the behavior is unchanged (zeros for Hessians).
		</comment>
		<comment id='11' author='kohpangwei' date='2016-12-07T04:05:24Z'>
		This is unfortunately a known bug and is caused by the fact that the cross_entropy_loss ops are fused: they calculate fw and bprop values at the same time and return them together (the bprop is hidden from the user but is used only during tf.gradients call).  This doesn't work well with TF's gradient registration mechanism when you want the second derivative.  Solutions may be forthcoming, but no promises on the timeline.
		</comment>
		<comment id='12' author='kohpangwei' date='2016-12-07T05:41:07Z'>
		Ok. Thank you for the update!
		</comment>
		<comment id='13' author='kohpangwei' date='2016-12-07T09:24:14Z'>
		&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 Do you have a link to the github issue for this? More generally, is there a way to at least cause a NotImplementedError rather than returning the wrong numbers?
		</comment>
		<comment id='14' author='kohpangwei' date='2016-12-07T16:22:53Z'>
		&lt;denchmark-link:https://github.com/goodfeli&gt;@goodfeli&lt;/denchmark-link&gt;
 this is the only github issue afaik (the other one is internal and doesn't have much more info).
I have one idea about how to get this to fail loudly when differentiating twice, I'll talk to Eugene today and see if it's possible in the short-term.  One workaround for now is to not use the fused cross entropy functions and use their stable, primitive-op expansion.  It will be slower but at least your results will be more correct.
		</comment>
		<comment id='15' author='kohpangwei' date='2017-02-01T21:20:32Z'>
		Okay, with &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/df52532ccb9a6084ed8877a5b11698fa451a8762&gt;df52532&lt;/denchmark-link&gt;
 we now fail loudly when you try to take second derivatives of ops that can't be done that way.  I think we're just going to rely on removing these fused ops via XLA one day as the eventual solution.
		</comment>
		<comment id='16' author='kohpangwei' date='2017-02-02T00:28:42Z'>
		Cool. Thanks!
		</comment>
	</comments>
</bug>