<bug id='29161' author='mwin76' open_date='2019-05-30T10:49:49Z' closed_time='2020-02-28T18:25:02Z'>
	<summary>tf.keras predict stuck with Sequence when using multi-processing</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.13.1
Python version: 3.6.8
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 10.0
GPU model and memory: TITAN

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
Hi,
When using tf.keras with a custom Sequence, the program hangs during predict (with multi-processing).
I was able to reproduce the issue with a simple NN that contains a single Dense layer.
This happens after setting the weights of the layer and running predict with multi-processing.
When commenting the 'set_weights' line or running with multi-threading, the program does not hang.
Issue exists also in 1.14.0-rc0,
Same code works OK with tensorflow 1.12.0 and 2.0.0a0.
Code to reproduce the issue
&lt;denchmark-code&gt;import numpy as np
from tensorflow import keras

INPUT_SIZE = 3
DENSE_OUTPUTS = 2
NUM_OF_SAMPLES = 1000
BATCH_SIZE = 2
NUM_OF_BATCHES = 5


class DummySequence(keras.utils.Sequence):

    def __len__(self):
        return NUM_OF_SAMPLES // BATCH_SIZE

    def __getitem__(self, index):
        data = [np.full(shape=(INPUT_SIZE,), fill_value=(index*BATCH_SIZE + i)) for i in range(BATCH_SIZE)]
        labels = [np.full(shape=(DENSE_OUTPUTS,), fill_value=(index*BATCH_SIZE + i))*INPUT_SIZE for i in range(BATCH_SIZE)]
        return np.stack(data), np.stack(labels)



x = keras.layers.Input(shape=(INPUT_SIZE,))
dense_layer = keras.layers.Dense(DENSE_OUTPUTS)
y = dense_layer(x)
model = keras.Model(x, y)

# remove comment in tf 1.12
#model.compile(optimizer="sgd", loss=keras.losses.mean_squared_error)

shapes = [v.shape for v in dense_layer.weights]
dense_layer.set_weights([np.full(shape=shapes[0], fill_value=1.0), np.full(shape=shapes[1], fill_value=0.0)])

seq = DummySequence()

workers = 5
multiprocessing = True
# works with multi-threaing
#multiprocessing = False
print("running predict with multiprocessing: {}".format(multiprocessing))
res = model.predict(seq, workers=workers, use_multiprocessing=multiprocessing, steps=NUM_OF_BATCHES)
print("predict # of results: {}\nresults:\n{}".format(len(res), res))

&lt;/denchmark-code&gt;

Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='mwin76' date='2019-05-31T04:59:40Z'>
		I was able to reproduce the reported issue on google colab with Tf 1.13.1. Thanks!
		</comment>
		<comment id='2' author='mwin76' date='2019-07-16T15:56:00Z'>
		Hi,
Any update?
Thanks.
		</comment>
		<comment id='3' author='mwin76' date='2019-07-24T20:44:55Z'>
		&lt;denchmark-link:https://github.com/mwin76&gt;@mwin76&lt;/denchmark-link&gt;
 I think it was resolved sometime ago. I have tested in nightly builds (1.15 and 2.0b1) and don't see any issue. Please check the gist with &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/b466a0758214055628c16026d68125d8/tf_29161_tf2p0b1.ipynb&gt;TF2.0&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/220c6d9555b467f171b56ebbe36c6b18/tf_29161_tf1p15.ipynb&gt;tf-nightly&lt;/denchmark-link&gt;
.
I am closing this issue as it was resolved. Please reopen if the issue persists again. Thanks!
		</comment>
		<comment id='4' author='mwin76' date='2019-07-24T20:44:57Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=29161&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=29161&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='mwin76' date='2019-07-25T05:31:07Z'>
		Hi,
I tested it with tf-nightly-gpu and it is stuck.
Thanks,
Mattan.
		</comment>
		<comment id='6' author='mwin76' date='2019-07-25T13:26:29Z'>
		&lt;denchmark-link:https://github.com/mwin76&gt;@mwin76&lt;/denchmark-link&gt;
 Can you check my gists or provide a gist/screenshot showing the issue? Thanks!
		</comment>
		<comment id='7' author='mwin76' date='2019-07-28T05:42:52Z'>
		Hi,
Tried this in colab and it is stuck. the code was the same as the code in your gist just replaced the "pip install tf-nightly" with "pip install tf-nightly-gpu"
When killing the job I get the following stack:
running predict with multiprocessing: True
E0728 05:34:49.872965 140443983652736 ultratb.py:152] Internal Python error in the inspect module.
Below is the traceback from this internal error.
Traceback (most recent call last):
File "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py", line 2882, in run_code
exec(code_obj, self.user_global_ns, self.user_ns)
File "", line 41, in 
res = model.predict(seq, workers=workers, use_multiprocessing=multiprocessing, steps=NUM_OF_BATCHES)
File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py", line 920, in predict
use_multiprocessing=use_multiprocessing)
File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_generator.py", line 648, in predict
use_multiprocessing=use_multiprocessing)
File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_generator.py", line 221, in model_iteration
batch_data = _get_next_batch(generator)
File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_generator.py", line 363, in _get_next_batch
generator_output = next(generator)
File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/utils/data_utils.py", line 779, in get
inputs = self.queue.get(block=True).get()
File "/usr/lib/python3.6/multiprocessing/pool.py", line 638, in get
self.wait(timeout)
File "/usr/lib/python3.6/multiprocessing/pool.py", line 635, in wait
self._event.wait(timeout)
File "/usr/lib/python3.6/threading.py", line 551, in wait
signaled = self._cond.wait(timeout)
File "/usr/lib/python3.6/threading.py", line 295, in wait
waiter.acquire()
KeyboardInterrupt
Thanks,
Mattan.
		</comment>
		<comment id='8' author='mwin76' date='2019-07-28T09:57:14Z'>
		Hi &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
. I tested your gist with 1.15.0-dev20190728 and it still doesn't work. Let me know if you need more information.
Thanks
		</comment>
		<comment id='9' author='mwin76' date='2019-07-30T17:06:59Z'>
		&lt;denchmark-link:https://github.com/mwin76&gt;@mwin76&lt;/denchmark-link&gt;
 I could reproduce the issue with  but TF2.0 has no issues. We will try to find the root-cause of the issue in TF1.x nightly. Thanks
		</comment>
		<comment id='10' author='mwin76' date='2019-09-08T10:29:55Z'>
		Is there any progress on the issue?
Thanks
		</comment>
		<comment id='11' author='mwin76' date='2019-10-18T12:58:54Z'>
		I faced the same issue and It seem to resolve it with adding this :
&lt;denchmark-code&gt;from tensorflow.python.keras import backend as K
import tensorflow as tf

config = tf.ConfigProto()
config.gpu_options.allow_growth = True
session = tf.Session(config=config)
K.set_session(session)
&lt;/denchmark-code&gt;

It seem to be unrelated problem but it's works.
		</comment>
		<comment id='12' author='mwin76' date='2019-11-13T16:48:04Z'>
		Hi,
Thanks.
Using config.gpu_options.allow_growth works however if you clear the session and reload the model it gets stuck
&lt;denchmark-code&gt;import numpy as np
from tensorflow import keras
#import keras

INPUT_SIZE = 3
DENSE_OUTPUTS = 2
NUM_OF_SAMPLES = 1000
BATCH_SIZE = 2
NUM_OF_BATCHES = 5


class DummySequence(keras.utils.Sequence):

    def __len__(self):
        return NUM_OF_SAMPLES // BATCH_SIZE

    def __getitem__(self, index):
        data = [np.full(shape=(INPUT_SIZE,), fill_value=(index*BATCH_SIZE + i)) for i in range(BATCH_SIZE)]
        labels = [np.full(shape=(DENSE_OUTPUTS,), fill_value=(index*BATCH_SIZE + i))*INPUT_SIZE for i in range(BATCH_SIZE)]
        return np.stack(data), np.stack(labels)


class CountBatchesCallback(keras.callbacks.Callback):

    def __init__(self):
        super(CountBatchesCallback, self).__init__()

        self.batches = 0

    def on_batch_begin(self, batch, logs=None):
        self.batches += 1


def get_model():
    x = keras.layers.Input(shape=(INPUT_SIZE,))
    dense_layer = keras.layers.Dense(DENSE_OUTPUTS)
    y = dense_layer(x)
    model = keras.Model(x, y)
    model.compile(optimizer="sgd", loss=keras.losses.mean_squared_error)
    shapes = [v.shape for v in dense_layer.weights]
    dense_layer.set_weights([np.full(shape=shapes[0], fill_value=1.0), np.full(shape=shapes[1], fill_value=0.0)])
    return model


def run_fit_and_predict(model):
    seq = DummySequence()
    steps = 5
    batch_counter_callback = CountBatchesCallback()
    use_multiprocessing = True
    workers = 5
    print("running fit with {} steps".format(steps))
    model.fit_generator(
        seq,
        epochs=1,
        steps_per_epoch=steps,
        use_multiprocessing=use_multiprocessing,
        # workers=workers,
        callbacks=[batch_counter_callback]
    )
    print("batches processed: {}".format(batch_counter_callback.batches))
    results = model.predict_generator(
        seq,
        use_multiprocessing=use_multiprocessing,
        # workers=workers,
        steps=steps
    )
    print("\npredict\nexpected number of results: {}.\nactual number of results: {}.\npredictions:\n{}".format(
        steps * BATCH_SIZE, len(results), results)
    )


if __name__ == '__main__':
    model = get_model()
    print("\n************************ Running run_fit_and_predict first ************************")
    run_fit_and_predict(model)
    print("\n************************ clear session ************************")
    keras.backend.clear_session()
    model = get_model()
    print("\n************************ Running run_fit_and_predict second ************************")
    run_fit_and_predict(model)


&lt;/denchmark-code&gt;

		</comment>
		<comment id='13' author='mwin76' date='2020-02-16T07:22:05Z'>
		test_predict = model.predict_generator(
test_generator,
max_queue_size=64,
workers=4,
use_multiprocessing=True)
Exception in thread Thread-24:
Traceback (most recent call last):
File "D:\Anaconda\envs\tfgpu\lib\threading.py", line 926, in _bootstrap_inner
self.run()
File "D:\Anaconda\envs\tfgpu\lib\threading.py", line 870, in run
self._target(*self._args, **self._kwargs)
File "D:\Anaconda\envs\tfgpu\lib\site-packages\tensorflow\python\keras\utils\data_utils.py", line 619, in _run
with closing(self.executor_fn(_SHARED_SEQUENCES)) as executor:
File "D:\Anaconda\envs\tfgpu\lib\site-packages\tensorflow\python\keras\utils\data_utils.py", line 600, in pool_fn
workers, initializer=init_pool_generator, initargs=(seqs, None))
File "D:\Anaconda\envs\tfgpu\lib\multiprocessing\context.py", line 119, in Pool
context=self.get_context())
File "D:\Anaconda\envs\tfgpu\lib\multiprocessing\pool.py", line 176, in init
self._repopulate_pool()
File "D:\Anaconda\envs\tfgpu\lib\multiprocessing\pool.py", line 241, in _repopulate_pool
w.start()
File "D:\Anaconda\envs\tfgpu\lib\multiprocessing\process.py", line 112, in start
self._popen = self._Popen(self)
File "D:\Anaconda\envs\tfgpu\lib\multiprocessing\context.py", line 322, in _Popen
return Popen(process_obj)
File "D:\Anaconda\envs\tfgpu\lib\multiprocessing\popen_spawn_win32.py", line 89, in init
reduction.dump(process_obj, to_child)
File "D:\Anaconda\envs\tfgpu\lib\multiprocessing\reduction.py", line 60, in dump
ForkingPickler(file, protocol).dump(obj)
TypeError: can't pickle _thread.lock objects
		</comment>
		<comment id='14' author='mwin76' date='2020-02-28T18:25:02Z'>
		&lt;denchmark-link:https://github.com/mwin76&gt;@mwin76&lt;/denchmark-link&gt;
 Thanks for the issue!
This looks like it is fixed in the latest tf-nightly, please give it a try: pip install -U tf-nightly
		</comment>
		<comment id='15' author='mwin76' date='2020-02-28T18:25:04Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29161&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29161&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='16' author='mwin76' date='2020-03-18T13:19:47Z'>
		&lt;denchmark-link:https://github.com/omalleyt12&gt;@omalleyt12&lt;/denchmark-link&gt;
 I'm facing the exact same issue on v2.1.0.
I plan on giving tf-nightly a try. But will it impact my previously trained models if I upgrade?
		</comment>
		<comment id='17' author='mwin76' date='2020-03-24T17:56:42Z'>
		Hi, I am facing the exact same issue with tf-nightly==2.2.0.dev20200324. A sequential multiprocessing model woudn't start re-training.
&lt;denchmark-code&gt;File "/home/sumit/.conda/envs/sopi/lib/python3.7/threading.py", line 296, in wait waiter.acquire()
&lt;/denchmark-code&gt;

		</comment>
		<comment id='18' author='mwin76' date='2020-04-29T10:02:22Z'>
		Any news? I'm facing the same problem here.
		</comment>
		<comment id='19' author='mwin76' date='2020-12-01T11:46:20Z'>
		Any updates on this? I am also facing a similar issue while trying to do model.predict() with multi-processing. I made sure that the tensorflow graph and tensorflow session are unique for each process.
		</comment>
	</comments>
</bug>