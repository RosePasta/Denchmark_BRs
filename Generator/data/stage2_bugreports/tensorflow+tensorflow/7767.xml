<bug id='7767' author='alsrgv' open_date='2017-02-22T08:28:12Z' closed_time='2017-02-27T19:14:01Z'>
	<summary>ParameterServer restart crashes distributed training process</summary>
	<description>
I've tried running distributed TensorFlow with Supervisor and new MonitoredTrainingSession and I observe following behavior if Parameter Server is restarted.  Code is available at &lt;denchmark-link:http://pastebin.com/HBUicRp2&gt;http://pastebin.com/HBUicRp2&lt;/denchmark-link&gt;
.  I'm using TensorFlow freshly built from r1.0.
At first, this error happens which is OK:
&lt;denchmark-code&gt;INFO:tensorflow:Error reported to Coordinator: &lt;class 'tensorflow.python.framework.errors_impl.UnavailableError'&gt;, {"created":"@1487751897.191020037","description":"EOF","file":"ex
ternal/grpc/src/core/lib/iomgr/tcp_posix.c","file_line":235,"grpc_status":14}
W tensorflow/core/framework/op_kernel.cc:993] Unavailable: {"created":"@1487751897.191020037","description":"EOF","file":"external/grpc/src/core/lib/iomgr/tcp_posix.c","file_line":
235,"grpc_status":14}
         [[Node: Assign_1_S295 = _Recv[_start_time=0, client_terminated=false, recv_device="/job:worker/replica:0/task:0/cpu:0", send_device="/job:ps/replica:0/task:0/cpu:0", send_
device_incarnation=8036561443230364107, tensor_name="edge_30_Assign_1", tensor_type=DT_INT64, _device="/job:worker/replica:0/task:0/cpu:0"]()]]
W tensorflow/core/framework/op_kernel.cc:993] Unavailable: {"created":"@1487751897.191020037","description":"EOF","file":"external/grpc/src/core/lib/iomgr/tcp_posix.c","file_line":
235,"grpc_status":14}
         [[Node: Assign_1_S295 = _Recv[_start_time=0, client_terminated=false, recv_device="/job:worker/replica:0/task:0/cpu:0", send_device="/job:ps/replica:0/task:0/cpu:0", send_
device_incarnation=8036561443230364107, tensor_name="edge_30_Assign_1", tensor_type=DT_INT64, _device="/job:worker/replica:0/task:0/cpu:0"]()]]
W tensorflow/core/framework/op_kernel.cc:993] Unavailable: {"created":"@1487751897.191020037","description":"EOF","file":"external/grpc/src/core/lib/iomgr/tcp_posix.c","file_line":
235,"grpc_status":14}
         [[Node: Assign_1_S295 = _Recv[_start_time=0, client_terminated=false, recv_device="/job:worker/replica:0/task:0/cpu:0", send_device="/job:ps/replica:0/task:0/cpu:0", send_
device_incarnation=8036561443230364107, tensor_name="edge_30_Assign_1", tensor_type=DT_INT64, _device="/job:worker/replica:0/task:0/cpu:0"]()]]
&lt;/denchmark-code&gt;

However, after Parameter Server is restarted, I see this error in logs and training worker crashes:
&lt;denchmark-code&gt;I tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000729. Possibly, this worker just restarted.
I tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000727. Possibly, this worker just restarted.
I tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000731. Possibly, this worker just restarted.
I tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Aborted: Graph handle is not found: 0000000000000723. Possibly, this worker just restarted.
I tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Aborted: Graph handle is not found: 000000000000072b. Possibly, this worker just restarted.
Traceback (most recent call last):
  File "tf_dist_mnist.py", line 140, in &lt;module&gt;
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File "tf_dist_mnist.py", line 107, in main
    mon_sess.run(train_op, feed_dict={image: image_, label: label_})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py", line 478, in __exit__
    self._close_internal(exception_type)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py", line 511, in _close_internal
    self._sess.close()
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py", line 739, in close
    self._sess.close()
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py", line 827, in close
    self._coord.join()
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py", line 386, in join
    six.reraise(*self._exc_info_to_raise)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/queue_runner_impl.py", line 234, in _run
    sess.run(enqueue_op)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 767, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 965, in _run
    feed_dict_string, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1015, in _do_run
    target_list, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnavailableError: {"created":"@1487751897.191020037","description":"EOF","file":"external/grpc/src/core/lib/iomgr/tcp_posix.c","file_line":235,"grpc_status":14}
&lt;/denchmark-code&gt;

I expect it to recover from latest checkpoint instead.
	</description>
	<comments>
		<comment id='1' author='alsrgv' date='2017-02-22T08:29:16Z'>
		&lt;denchmark-link:https://github.com/jhseu&gt;@jhseu&lt;/denchmark-link&gt;
 we discussed this on TF Dev Summit and you recommended raising issue.
		</comment>
		<comment id='2' author='alsrgv' date='2017-02-22T18:14:08Z'>
		Thanks! Adding to my todo list (but may not get to it for a little bit).
		</comment>
		<comment id='3' author='alsrgv' date='2017-02-22T21:22:54Z'>
		Sent out a fix internally.
		</comment>
		<comment id='4' author='alsrgv' date='2017-02-22T23:24:28Z'>
		Will you backport to Supervisor's managed_session as well?  It's still listed on TensorFlow documentation website as one of the recommended ways to do simplified session management - &lt;denchmark-link:https://www.tensorflow.org/programmers_guide/supervisor&gt;https://www.tensorflow.org/programmers_guide/supervisor&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='alsrgv' date='2017-02-22T23:32:21Z'>
		My understanding is that managed_session never did recovery for failed workers. Also, Supervisor is eventually going away. (I never use Supervisor, so I'm probably wrong.)
		</comment>
		<comment id='6' author='alsrgv' date='2017-02-22T23:39:52Z'>
		AFAIK it is has similar level of recoverability as MonitoredTrainingSession (before your fix).  Here's my observations for Supervisor's managed_session:

Parameter Server failure.  Training throws UnavailableError on all workers.  All workers crash.  After restart of failed PS and all workers, training continues from the last checkpoint.
Chief worker failure.  Training freezes.  After chief worker is restarted, training continues from the last checkpoint.
Non-Chief worker failure.  Training freezes.  After failed worker is restarted, training continues from the last iteration.
Backup worker failure.  Training continues, with potential slowdown.  After failed backup worker is restarted, training continues with original pace.

Since it's still widely advertised a lot of people may jump to use it.
		</comment>
		<comment id='7' author='alsrgv' date='2018-02-24T02:33:22Z'>
		&lt;denchmark-link:https://github.com/alsrgv&gt;@alsrgv&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jhseu&gt;@jhseu&lt;/denchmark-link&gt;
 Hi, sorry to comment on this closed issue.
But I'm wondering will training freezes if non-chief workers failure in the latest release (v1.5.0)?
		</comment>
	</comments>
</bug>