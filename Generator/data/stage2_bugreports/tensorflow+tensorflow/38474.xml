<bug id='38474' author='sayakpaul' open_date='2020-04-12T15:53:22Z' closed_time='2020-05-30T09:47:40Z'>
	<summary>Embedding a preprocessing function inside a tf.keras model for serving</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Colab
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device:
TensorFlow installed from (source or
binary): - TensorFlow version (use command below):
Python version: - Bazel
version (if compiling from source):
GCC/Compiler version (if compiling from
source):
CUDA/cuDNN version: - GPU model and memory:

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
I am trying to embed a simple image preprocessing function inside an already trained tf.keras model. This is a useful feature to have because it can help us reduce a lot of boilerplate code needed while using any model for serving purposes. With this capability, you get a lot more flexibility and modularity to your model.
So after training my model, I am first defining a preprocessing function like so -
def preprocess_image_cv2(image_path):
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    img = cv2.resize(img, (28, 28)).astype("float32")
    img = img / 255
    img = np.expand_dims(img, 0)
    img = tf.convert_to_tensor(img)
    return img
I am then using it to create another model class along with the trained model -
# Define the model for predcition purpose
class ExportModel(tf.keras.Model):
    def __init__(self, preproc_func, model):
        super().__init__(self)
        self.preproc_func = preproc_func
        self.model = model

    @tf.function
    def my_serve(self, image_path):
        print("Inside")
        preprocessed_image = self.preproc_func(image_path) # Preprocessing
        probabilities = self.model(preprocessed_image, training=False) # Model prediction
        class_id = tf.argmax(probabilities[0], axis=-1) # Postprocessing
        return {"class_index": class_id}
I am then able to run inference on a sample image with this setting:
# Now initialize a dummy model and fill its parameters with that of
# the model we trained
restored_model = get_training_model()
restored_model.set_weights(apparel_model.get_weights())

# Now use this model, preprocessing function, and the same image
# for checking if everything is working
serving_model = ExportModel(preprocess_image_cv2, restored_model)
class_index = serving_model.my_serve("sample_image.png")
CLASSES[class_index["class_index"].numpy()] # prints Dress
But I am unable to export this model for serving. I am doing the following for exporting -
# Make sure we are *not* letting the model to train
tf.keras.backend.set_learning_phase(0)

# Serialize model
export_path = "model_preprocessing_func"
tf.saved_model.save(serving_model, export_path, signatures={"serving_default": serving_model.my_serve})
This yields -
&lt;denchmark-code&gt;---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-97-9e2616e04da9&gt; in &lt;module&gt;()
      1 export_path = "model_preprocessing_func"
----&gt; 2 tf.saved_model.save(serving_model, export_path, signatures={"serving_default": serving_model.my_serve})

2 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/save.py in save(obj, export_dir, signatures, options)
    949 
    950   _, exported_graph, object_saver, asset_info = _build_meta_graph(
--&gt; 951       obj, export_dir, signatures, options, meta_graph_def)
    952   saved_model.saved_model_schema_version = constants.SAVED_MODEL_SCHEMA_VERSION
    953 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/save.py in _build_meta_graph(obj, export_dir, signatures, options, meta_graph_def)
   1009 
   1010   signatures, wrapped_functions = (
-&gt; 1011       signature_serialization.canonicalize_signatures(signatures))
   1012   signature_serialization.validate_saveable_view(checkpoint_graph_view)
   1013   signature_map = signature_serialization.create_signature_map(signatures)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/signature_serialization.py in canonicalize_signatures(signatures)
    110           ("Expected a TensorFlow function to generate a signature for, but "
    111            "got {}. Only `tf.functions` with an input signature or "
--&gt; 112            "concrete functions can be used as a signature.").format(function))
    113 
    114     wrapped_functions[original_function] = signature_function = (

ValueError: Expected a TensorFlow function to generate a signature for, but got &lt;tensorflow.python.eager.def_function.Function object at 0x7fd5b646ea58&gt;. Only `tf.functions` with an input signature or concrete functions can be used as a signature.
&lt;/denchmark-code&gt;

I am able to interpret the last part of the error but I am unable to figure out what steps should I take to resolve it.
Describe the expected behavior

One can reproduce the issue with this &lt;denchmark-link:https://colab.research.google.com/drive/1QuJ7MLgtgNtJ7E_r4gpCokNxukNxay1O&gt;Colab Notebook&lt;/denchmark-link&gt;
. Help is appreciated.
Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='sayakpaul' date='2020-04-13T10:42:19Z'>
		I was able to reproduce the issue with Tf 2.2rc2.
Please find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/f3a2b12faf72261fccc1367fe10450af/untitled501.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='sayakpaul' date='2020-04-13T20:38:25Z'>
		&lt;denchmark-link:https://github.com/sayakpaul&gt;@sayakpaul&lt;/denchmark-link&gt;
 You are not providing the right input signature, so saving the model is not possible. The signature that you are providing  doesn't have any input signature or neither its a concrete function
The input signature should contain tensorinfo(name, dtype and tensor shape) but not in this case , hence the reason for your error.
		</comment>
		<comment id='3' author='sayakpaul' date='2020-04-14T07:52:26Z'>
		&lt;denchmark-link:https://github.com/gowthamkpr&gt;@gowthamkpr&lt;/denchmark-link&gt;
 could you provide a minimal working example?
		</comment>
		<comment id='4' author='sayakpaul' date='2020-05-30T07:52:03Z'>
		&lt;denchmark-link:https://github.com/sayakpaul&gt;@sayakpaul&lt;/denchmark-link&gt;

@tf.function
---&gt;
@tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string)])
		</comment>
		<comment id='5' author='sayakpaul' date='2020-05-30T09:47:42Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38474&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38474&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='sayakpaul' date='2020-08-21T14:22:36Z'>
		Hi, &lt;denchmark-link:https://github.com/sayakpaul&gt;@sayakpaul&lt;/denchmark-link&gt;
 could you resolve this only by adding an input signature? I am trying to do almost the same, but I can't access the python string from inside the . I have tried the following:


Send the picture path as a tf.string tensor and run .eval() on it. It did not work because when running .eval() with the function input, it asks for a "feed_dict for the placeholder", so I guess TF creates a placeholder according the input signature and is that what we can access from inside the tf.function, not the tensor we forward (which is a constant and should return the path in bytes, so it could be decoded in a python string with .decode('utf-8').


Using the path of the image as the name of the tensor I forward to the tf.function (i.e. image_path_tensor = tf.constant('test_images/IMG_1097.JPG', dtype=tf.string, name='test_images/IMG_1097.JPG')). Again, since I can not (or do not know how to) access the tensor I input from inside the tf.function, I could not just run input.name and get the path from there.


Is there another way I can use SavedModel to store the inferrence and some other python processing to be able to run it without a Python interpreter? As, for example, with TF C++?
		</comment>
		<comment id='7' author='sayakpaul' date='2020-08-21T14:56:43Z'>
		&lt;denchmark-link:https://github.com/jmtc7&gt;@jmtc7&lt;/denchmark-link&gt;
 here's what I ended up with: &lt;denchmark-link:https://sayak.dev/tf.keras/preprocessing/2020/04/13/embedding-image-preprocessing-functions.html&gt;https://sayak.dev/tf.keras/preprocessing/2020/04/13/embedding-image-preprocessing-functions.html&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>