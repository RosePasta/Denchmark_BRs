<bug id='43584' author='JosepLeder' open_date='2020-09-26T10:04:32Z' closed_time='2020-10-20T17:22:56Z'>
	<summary>tf.summary.create_file_writer does not write any event file and tensorboard show nothing</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

OS Platform and Distribution : Linux Ubuntu 18.04
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.2.0
Python version: 3.8.3
CUDA/cuDNN version: 10.1/7.6
GPU model and memory: RTX 2070super 8GB


I have tried the example of &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/summary&gt;https://www.tensorflow.org/api_docs/python/tf/summary&lt;/denchmark-link&gt;

import tensorflow as tf

writer = tf.summary.create_file_writer("/tmp/mylogs")
with writer.as_default():
  for step in range(100):
    # other model code would go here
    tf.summary.scalar("my_metric", 0.5, step=step)
    writer.flush()
and it works fine. However, when I migrate some codes from tf 1.X to tf 2.2 using tf_upgrade_v2 script, and add some codes to store some data in files, the summary_writer doesn't create any folder or event files.
#!/usr/bin/env python
import tensorflow as tf

tf.compat.v1.disable_eager_execution()
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        # Currently, memory growth needs to be the same across GPUs
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        logical_gpus = tf.config.experimental.list_logical_devices('GPU')
        print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")
    except RuntimeError as e:
        # Memory growth must be set before GPUs have been initialized
        print(e)

def start_experiment():
    file_writer = tf.summary.create_file_writer("/tmp/{}_{}".format('none', 0))

    trainer = Trainer(file_writer=file_writer)
    trainer.train()
    file_writer.close()


class Trainer(object):
    def __init__(self, file_writer):
        self.file_writer = file_writer

    def train(self):
        info = {'tcount': 0, 'int_rew': 1, 'eprew':1, 'opt_tot':1}
        while True:
            # record data to tensorboard
            with self.file_writer.as_default():
                x = int(info['tcount'])
                tf.summary.scalar("int_reward", info['int_rew'], step=x)
                tf.summary.scalar("ext_reward", info['eprew'], step=x)
                tf.summary.scalar("total_loss", info['opt_tot'], step=x)
                self.file_writer.flush()
            info['tcount'] += 1
            if info['tcount'] &gt; 100:
                break


if __name__ == '__main__':
    start_experiment()
I removed some detail codes to make it more clear. The training part of codes works fine, and it just doesn't record anything.
Describe the expected behavior
It should create a folder in /tmp/none_0 and write some event file, but it did nothing.
	</description>
	<comments>
		<comment id='1' author='JosepLeder' date='2020-09-26T12:31:49Z'>
		Can you minimize the code/imports?
It will make easy for us to copy, paste and run your code example to reproduce your issue.
		</comment>
		<comment id='2' author='JosepLeder' date='2020-09-26T15:36:50Z'>
		
Can you minimize the code/imports?
It will make easy for us to copy, paste and run your code example to reproduce your issue.

I updated my code to remove all the unnecessary codes and imports as above. It still records nothing. The folder "/tmp/none_0" is not created. And there are no event files. If you would like to run my code and help me find the problem, that would be really helpful. Thanks.
		</comment>
		<comment id='3' author='JosepLeder' date='2020-09-28T12:19:38Z'>
		&lt;denchmark-link:https://github.com/JosepLeder&gt;@JosepLeder&lt;/denchmark-link&gt;
 I think that there is a note in the documentation for running in the graph:

Default writers do not (yet) propagate across the @tf.function execution boundary - they are only detected when the function is traced - so best practice is to call writer.as_default() within the function body, and to ensure that the writer object continues to exist as long as the @tf.function is being used

As you see in the graph function execution case is still inside tf.function perimeter. So please be aware about tf.compat.v1.disable_eager_execution():
&lt;denchmark-code&gt;writer = tf.summary.create_file_writer("/tmp/mylogs/tf_function")

@tf.function
def my_func(step):
  with writer.as_default():
    # other model code would go here
    tf.summary.scalar("my_metric", 0.5, step=step)

for step in tf.range(100, dtype=tf.int64):
  my_func(step)
  writer.flush()
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='JosepLeder' date='2020-09-30T10:34:18Z'>
		&lt;denchmark-link:https://github.com/Joseph-Rance&gt;@Joseph-Rance&lt;/denchmark-link&gt;

Please update as per above comment, if possible could you try on tf-nightly and let us know if the issue exist, please share a colab gist with the issue reported.
		</comment>
		<comment id='5' author='JosepLeder' date='2020-09-30T15:55:11Z'>
		&lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;
 Not sure you meant to @ me in that last comment?
		</comment>
		<comment id='6' author='JosepLeder' date='2020-09-30T17:00:22Z'>
		&lt;denchmark-link:https://github.com/JosepLeder&gt;@JosepLeder&lt;/denchmark-link&gt;

Is this still an issue, can you follow bhack comment and let us know.
		</comment>
		<comment id='7' author='JosepLeder' date='2020-10-01T02:52:44Z'>
		
@JosepLeder I think that there is a note in the documentation for running in the graph:

Default writers do not (yet) propagate across the @tf.function execution boundary - they are only detected when the function is traced - so best practice is to call writer.as_default() within the function body, and to ensure that the writer object continues to exist as long as the @tf.function is being used

As you see in the graph function execution case is still inside tf.function perimeter. So please be aware about tf.compat.v1.disable_eager_execution():
writer = tf.summary.create_file_writer("/tmp/mylogs/tf_function")

@tf.function
def my_func(step):
  with writer.as_default():
    # other model code would go here
    tf.summary.scalar("my_metric", 0.5, step=step)

for step in tf.range(100, dtype=tf.int64):
  my_func(step)
  writer.flush()


I add @tf.function to my functions as follows:
#!/usr/bin/env python
import tensorflow as tf

tf.compat.v1.disable_eager_execution()
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        # Currently, memory growth needs to be the same across GPUs
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        logical_gpus = tf.config.experimental.list_logical_devices('GPU')
        print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")
    except RuntimeError as e:
        # Memory growth must be set before GPUs have been initialized
        print(e)

@tf.function
def start_experiment():
    trainer = Trainer()
    trainer.train()


class Trainer(object):

    def __init__(self):
        pass

    @tf.function
    def train(self):
        file_writer = tf.summary.create_file_writer("/tmp/{}_{}".format('none', 0))
        info = {'tcount': 0, 'int_rew': 1, 'eprew':1, 'opt_tot':1}
        while True:
            # record data to tensorboard
            with file_writer.as_default():
                x = int(info['tcount'])
                tf.summary.scalar("int_reward", info['int_rew'], step=x)
                tf.summary.scalar("ext_reward", info['eprew'], step=x)
                tf.summary.scalar("total_loss", info['opt_tot'], step=x)
                file_writer.flush()
            info['tcount'] += 1
            if info['tcount'] &gt; 100:
                break
        file_writer.close()


if __name__ == '__main__':
    start_experiment()
But it still doesn't work. If I remove tf.compat.v1.disable_eager_execution(), it works as the document said. However, I have to change lots of code using placeholder after using tf_upgrade_v2 if I remove tf.compat.v1.disable_eager_execution(). I am not sure if there is a way  I can use the writer without remove that code.
		</comment>
		<comment id='8' author='JosepLeder' date='2020-10-01T06:59:27Z'>
		You can try to take a look at &lt;denchmark-link:https://www.tensorflow.org/tensorboard/migrate&gt;https://www.tensorflow.org/tensorboard/migrate&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='JosepLeder' date='2020-10-13T16:45:50Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='10' author='JosepLeder' date='2020-10-20T17:22:55Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='11' author='JosepLeder' date='2020-10-20T17:22:58Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43584&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43584&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>