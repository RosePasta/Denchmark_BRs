<bug id='31832' author='rishabhsahrawat' open_date='2019-08-21T09:01:02Z' closed_time='2020-04-11T21:20:13Z'>
	<summary>[TF 1.14] [TPU] Errors while training LSTM model on Colab TPU</summary>
	<description>
I worked a bit with TF2.0 but new to TF1.x working. I want o use Google TPUs and for that I am making a LSTM model in TF1.14 without eager execution. I am reusing code from one of the Tensorflow &lt;denchmark-link:https://www.tensorflow.org/beta/tutorials/load_data/text#split_the_dataset_into_text_and_train_batches&gt;tutorials&lt;/denchmark-link&gt;
. Below is the code I am executing in Colab while setting the runtime environment to  and .
On running it in colab, I am getting error 
and if I run it again (without restarting runtime) it throws following error 
THANK YOU!
&lt;denchmark-code&gt;import tensorflow as tf
import tensorflow_datasets as tfds
import os
import pprint
import tensorflow as tf
import time
import numpy as np
from tensorflow import keras

TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']

DIRECTORY_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'
FILE_NAMES = ['cowper.txt', 'derby.txt', 'butler.txt']

for name in FILE_NAMES:
  text_dir = tf.keras.utils.get_file(name, origin=DIRECTORY_URL+name)
  
parent_dir = os.path.dirname(text_dir)

print (parent_dir)
def labeler(example, index):
  return example, tf.cast(index, tf.int32)  

labeled_data_sets = []

for i, file_name in enumerate(FILE_NAMES):
  lines_dataset = tf.data.TextLineDataset(os.path.join(parent_dir, file_name))
  labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))
  labeled_data_sets.append(labeled_dataset)

BUFFER_SIZE = 50000
BATCH_SIZE = 64
TAKE_SIZE = 5000
all_labeled_data = labeled_data_sets[0]
for labeled_dataset in labeled_data_sets[1:]:
  all_labeled_data = all_labeled_data.concatenate(labeled_dataset)
  
all_labeled_data = all_labeled_data.shuffle(
    BUFFER_SIZE, reshuffle_each_iteration=False)
print type(all_labeled_data)
tokenizer = tfds.features.text.Tokenizer()

vocabulary_set = set()
my_iterator = all_labeled_data.make_initializable_iterator()
text_tensor, _ = my_iterator.get_next()
with tf.Session() as sess1: 
  sess1.run(my_iterator.initializer)
  try:
    while True:
      text_string = sess1.run(text_tensor)
      #print text_string
      some_tokens = tokenizer.tokenize(text_string)
      vocabulary_set.update(some_tokens)
  except tf.errors.OutOfRangeError:
    pass
vocab_size = len(vocabulary_set)
print 'Length of Vocab',len(vocabulary_set)
print 'Done Tokenizing!'

encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)
with tf.Session() as sess2:
  sess2.run(my_iterator.initializer)
  try:
    while True:
      text_string = sess2.run(text_tensor)
      encoded_example = encoder.encode(text_string)
      #print encoded_example
  except tf.errors.OutOfRangeError:
    pass

def encoder_fn(text_tensor, label):
  #print type(text_tensor)
  #print text_tensor
  encoded_text = encoder.encode(text_tensor)
  padded_text = encoded_text[:4]# limiting the length to 4
  padded_text = np.asarray(padded_text, dtype=np.int32)# chnaging type for tensor type compatibility with TF
  return padded_text, label

def encode_map_fn(text, label):
  return tf.py_func(encoder_fn, inp = [text, tf.reshape(label, [])], Tout = [tf.int32, tf.int32])

all_encoded_data= all_labeled_data.map(encode_map_fn)
train_data = all_encoded_data.padded_batch(BATCH_SIZE, padded_shapes=([-1],[]), drop_remainder= True)
my_iterator1 = train_data.make_initializable_iterator()
text_tensor, lab = my_iterator1.get_next()
with tf.Session() as sess2:
  sess2.run(my_iterator1.initializer)
  text_string = sess2.run(text_tensor)
  lab = sess2.run(lab)
  print text_string
  print lab
print 'Printing an example data done!.'

all_encoded_data = all_encoded_data.repeat()
train_data = all_encoded_data.skip(TAKE_SIZE).shuffle(BUFFER_SIZE)
train_data = train_data.padded_batch(BATCH_SIZE, padded_shapes=([-1],[]), drop_remainder= True)
#train_data= train_data.batch(64)
train_data = train_data.prefetch(BATCH_SIZE)
test_data = all_encoded_data.take(TAKE_SIZE)
test_data = test_data.padded_batch(BATCH_SIZE, padded_shapes=([-1],[]), drop_remainder = True)
test_data = test_data.prefetch(BATCH_SIZE)



vocab_size += 1

tf.keras.backend.clear_session()
resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)
tf.contrib.distribute.initialize_tpu_system(resolver)
strategy = tf.contrib.distribute.TPUStrategy(resolver)

with strategy.scope():
  model = tf.keras.Sequential()
  model.add(tf.keras.layers.Embedding(vocab_size, 64))
  model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, activation= 'tanh', recurrent_activation= 'sigmoid', recurrent_dropout = 0, unroll = False, use_bias= True)))
  # One or more dense layers.
  # Edit the list in the `for` line to experiment with layer sizes.
  for units in [64, 64]:
    model.add(tf.keras.layers.Dense(units, activation='relu'))

  # Output layer. The first argument is the number of labels.
  model.add(tf.keras.layers.Dense(3, activation='softmax'))
  model.compile(optimizer='adam',
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy'])
###Strategy  end###
steps = 50000/64
print model.summary()
print (steps)

model.fit(train_data, steps_per_epoch = steps,epochs=10)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='rishabhsahrawat' date='2019-08-22T12:57:04Z'>
		I have tried on colab with TF 1.14 and was able to reproduce the issue.Please, find the &lt;denchmark-link:https://colab.research.google.com/drive/15MX0aH_W8nyU02743Xt7JlyfziwWhUw3&gt;gist &lt;/denchmark-link&gt;
here.I tried in nightly versions and i am getting the below error Thanks!
		</comment>
		<comment id='2' author='rishabhsahrawat' date='2019-08-22T13:59:25Z'>
		Thank you for confirming the issue. &lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='rishabhsahrawat' date='2020-04-11T21:20:12Z'>
		Sorry for the long delay. Please re-open this issue if this is still reproducible on the latest versions of TensorFlow &amp; Cloud TPUs. Thanks!
		</comment>
		<comment id='4' author='rishabhsahrawat' date='2020-04-11T21:20:14Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31832&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31832&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>