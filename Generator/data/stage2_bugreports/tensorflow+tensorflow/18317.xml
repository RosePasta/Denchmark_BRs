<bug id='18317' author='qiuxin2012' open_date='2018-04-08T01:38:40Z' closed_time='2018-10-21T19:02:56Z'>
	<summary>[bug] FtrlOptimizer with l2_shrinkage_regularization_strength is incorrect</summary>
	<description>
The result of accum in FtrlOptimizer with l2_shrinkage_regularization_strength seems incorrect.
Test code:
&lt;denchmark-code&gt;import tensorflow as tf

from tensorflow.python.framework import constant_op
from tensorflow.python.ops import variables
from tensorflow.python.training import ftrl

dtype = "float32"

with tf.Session() as sess:
  var0 = variables.Variable([1.0], dtype=dtype)
  grads0 = constant_op.constant([0.1], dtype=dtype)
  opt = ftrl.FtrlOptimizer(
      3.0,
      initial_accumulator_value=0.0,
      l1_regularization_strength=0.0,
      l2_regularization_strength=0.0,
      l2_shrinkage_regularization_strength=0.5)
  update = opt.apply_gradients(zip([grads0], [var0]))
  variables.global_variables_initializer().run()

  v0_val = sess.run(var0)

  update.run()

  v0_val = sess.run(var0)
  print (sess.run(opt._slots))
  print (v0_val)
&lt;/denchmark-code&gt;

My result is
&lt;denchmark-code&gt;{'accum': {(&lt;tensorflow.python.framework.ops.Graph object at 0x7f1d1221a990&gt;, u'Variable'): array([3.61], dtype=float32)}, 'linear': {(&lt;tensorflow.python.framework.ops.Graph object at 0x7f1d1221a990&gt;, u'Variable'): array([0.73333335], dtype=float32)}}
[-2.]
&lt;/denchmark-code&gt;

OS Platform and Distribution
OS: Ubuntu 16.04
Tensorflow version: 1.7.0
CPU: i7 4790
TensorFlow installed from
pypi
According to comments of apply_ftrl_v2 in gen_training_ops.py
&lt;denchmark-code&gt;grad_with_shrinkage = grad + 2 * l2_shrinkage * var
accum_new = accum + grad_with_shrinkage * grad_with_shrinkage
linear += grad_with_shrinkage +
      (accum_new^(-lr_power) - accum^(-lr_power)) / lr * var
quadratic = 1.0 / (accum_new^(lr_power) * lr) + 2 * l2
var = (sign(linear) * l1 - linear) / quadratic if |linear| &gt; l1 else 0.0
accum = accum_new
&lt;/denchmark-code&gt;

grad_with_shrinkage should be [0.1] + 2 * 0.5 * [1.0] = 1.1
accum_new should be [0.0] + [1.1] * [1.1] = [1.21]
so accum should be [1.21], but the result of accum from the test code is [3.61] ???
	</description>
	<comments>
		<comment id='1' author='qiuxin2012' date='2018-04-08T12:26:25Z'>
		Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.
Have I written custom code
OS Platform and Distribution
TensorFlow installed from
Bazel version
CUDA/cuDNN version
GPU model and memory
Exact command to reproduce
		</comment>
		<comment id='2' author='qiuxin2012' date='2018-04-16T01:49:29Z'>
		&lt;denchmark-link:https://github.com/rohan100jain&gt;@rohan100jain&lt;/denchmark-link&gt;
 Any updateï¼Ÿ
		</comment>
		<comment id='3' author='qiuxin2012' date='2018-04-18T08:27:33Z'>
		
Have I written custom code
No.
OS Platform and Distribution
OS: Ubuntu 16.04
CPU: i7 4790
TensorFlow installed from
pypi
TensorFlow version
1.7.0
Bazel version
N/A
CUDA/cuDNN version
N/A
GPU model and memory
N/A
Exact command to reproduce

&lt;denchmark-code&gt;import tensorflow as tf

from tensorflow.python.framework import constant_op
from tensorflow.python.ops import variables
from tensorflow.python.training import ftrl

dtype = "float32"

with tf.Session() as sess:
  var0 = variables.Variable([1.0], dtype=dtype)
  grads0 = constant_op.constant([0.1], dtype=dtype)
  opt = ftrl.FtrlOptimizer(
      3.0,
      initial_accumulator_value=0.0,
      l1_regularization_strength=0.0,
      l2_regularization_strength=0.0,
      l2_shrinkage_regularization_strength=0.5)
  update = opt.apply_gradients(zip([grads0], [var0]))
  variables.global_variables_initializer().run()

  v0_val = sess.run(var0)

  update.run()

  v0_val = sess.run(var0)
  print (sess.run(opt._slots))
  print (v0_val)
&lt;/denchmark-code&gt;

According to comments of apply_ftrl_v2 in gen_training_ops.py
&lt;denchmark-code&gt;grad_with_shrinkage = grad + 2 * l2_shrinkage * var
accum_new = accum + grad_with_shrinkage * grad_with_shrinkage
linear += grad_with_shrinkage +
      (accum_new^(-lr_power) - accum^(-lr_power)) / lr * var
quadratic = 1.0 / (accum_new^(lr_power) * lr) + 2 * l2
var = (sign(linear) * l1 - linear) / quadratic if |linear| &gt; l1 else 0.0
accum = accum_new
&lt;/denchmark-code&gt;

grad_with_shrinkage should be [0.1] + 2 * 0.5 * [1.0] = 1.1
accum_new should be [0.0] + [1.1] * [1.1] = [1.21]
so accum should be [1.21], but the result of accum from the test code is [3.61] ???
		</comment>
		<comment id='4' author='qiuxin2012' date='2018-04-25T09:17:28Z'>
		The root cause is when &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/15a9de7763aa736e97d7e636d9487c0dde372d4b/tensorflow/core/kernels/training_ops.cc#L217&gt;this line&lt;/denchmark-link&gt;
 or &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/15a9de7763aa736e97d7e636d9487c0dde372d4b/tensorflow/core/kernels/training_ops.cc#L224&gt;this line&lt;/denchmark-link&gt;
 update value, the  is also changed, although it is defined before those lines. So when update accum, the accum_new is not the expected one.
From &lt;denchmark-link:https://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf&gt;this paper&lt;/denchmark-link&gt;
, the  should be the sum of gradient square in equation (2). So looks like the update of the  is not correct.
		</comment>
		<comment id='5' author='qiuxin2012' date='2018-07-26T18:27:24Z'>
		&lt;denchmark-link:https://github.com/yangli2&gt;@yangli2&lt;/denchmark-link&gt;
 to take a look
		</comment>
		<comment id='6' author='qiuxin2012' date='2018-10-15T12:48:37Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/rohan100jain&gt;@rohan100jain&lt;/denchmark-link&gt;
: It has been 80 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='7' author='qiuxin2012' date='2018-10-21T19:02:56Z'>
		&lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/20d5683b826be03776978af3b8108fc3b5dc9cb8#diff-fee33779233fb6ff133bbb93c7830880&gt;20d5683#diff-fee33779233fb6ff133bbb93c7830880&lt;/denchmark-link&gt;
 should have fixed the bug
		</comment>
	</comments>
</bug>