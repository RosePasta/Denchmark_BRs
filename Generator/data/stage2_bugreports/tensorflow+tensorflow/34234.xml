<bug id='34234' author='georgesterpu' open_date='2019-11-13T15:55:45Z' closed_time='2019-11-18T18:05:36Z'>
	<summary>Checkpoint managers overwrite not owned checkpoints</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Manjaro linux testing
TensorFlow installed from: pypi binary
TensorFlow version (use command below): v1.12.1-16854-g6778662 2.1.0-dev20191028
Python version: 3.7.4

Describe the current behavior
A tf.train.CheckpointManager erases checkpoints in its directory path that were not written by it.
This is a different behaviour from tf.train.Saver in TF1.
Describe the expected behavior
Do not remove checkpoints that are not owned by a CheckpointManager instance.
In the code below, I expected to have checkpoints 1, 2, 4, 5 at the end. Instead, only 4 and 5 survived.
Code to reproduce the issue
&lt;denchmark-code&gt;import tensorflow as tf

var = tf.Variable(initial_value=12)

checkpoint = tf.train.Checkpoint(var=var)
manager = tf.train.CheckpointManager(
    checkpoint=checkpoint,
    directory='./delete_me/',
    max_to_keep=2)

manager.save(0)
manager.save(1)
manager.save(2)

manager2 = tf.train.CheckpointManager(
    checkpoint=checkpoint,
    directory='./delete_me/',
    max_to_keep=2)

manager2.save(3)
manager2.save(4)
manager2.save(5)
#  Why were 1 and 2 deleted by manager2 ?
&lt;/denchmark-code&gt;

Other info / logs
	</description>
	<comments>
		<comment id='1' author='georgesterpu' date='2019-11-14T06:46:37Z'>
		&lt;denchmark-link:https://github.com/georgesterpu&gt;@georgesterpu&lt;/denchmark-link&gt;
, In the , change the value of .
max_to_keep shows the number of checkpoints to keep. Please refer this &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/train/CheckpointManager&gt;link&lt;/denchmark-link&gt;
 for more information. Thanks!
		</comment>
		<comment id='2' author='georgesterpu' date='2019-11-14T09:57:51Z'>
		Hi &lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;

Thanks for the reply.
My issue is slightly different. I think that  should not erase checkpoints written by , or in general by any other checkpoint manager.
The aim is to keep up to N checkpoints from a set of M managers writing K checkpoints each, where K &gt;&gt; N. You cannot control this with the max_to_keep parameter alone, unless it is set to a very large value of the order of M*K.
I expected each checkpoint manager to keep a list of all the checkpoints it has written so far, and to do the sweep operation only on this list. Instead, it appears that a checkpoint manager inspects its working directory (which can be shared by other managers) and is able to sweep any checkpoint inside, leaving up to N checkpoints there. With tf.train.Saver in TensorFlow 1, that directory would contain N * M checkpoints.
Please let me know if this explanation is more clear.
		</comment>
		<comment id='3' author='georgesterpu' date='2019-11-15T05:02:16Z'>
		Issue is replicating on colab with TF 2.1.0.dev20191028.
Please see gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/458b64b41d927e2e18318900b306d959/untitled258.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='4' author='georgesterpu' date='2019-11-15T10:40:29Z'>
		&lt;denchmark-link:https://github.com/georgesterpu&gt;@georgesterpu&lt;/denchmark-link&gt;
,
As per &lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
's comment, by using , all the Checkpoints are retained. Please find the &lt;denchmark-link:https://colab.sandbox.google.com/gist/rmothukuru/729f03581c84f22332f4eb2ff29b47f5/untitled258.ipynb#scrollTo=m9Ca1XPakRK9&gt;Gist&lt;/denchmark-link&gt;
. Please let me know if you need more information. Thanks!
		</comment>
		<comment id='5' author='georgesterpu' date='2019-11-15T10:56:57Z'>
		Thanks, &lt;denchmark-link:https://github.com/rmothukuru&gt;@rmothukuru&lt;/denchmark-link&gt;
,
Sorry, I think that I haven't explained this clear enough, please have a look at another snippet below.
&lt;denchmark-code&gt;import tensorflow as tf

var = tf.Variable(initial_value=12)

checkpoint = tf.train.Checkpoint(var=var)
manager = tf.train.CheckpointManager(
    checkpoint=checkpoint,
    directory='./delete_me/',
    max_to_keep=2)

for i in range(1000):
    manager.save(i)

manager2 = tf.train.CheckpointManager(
    checkpoint=checkpoint,
    directory='./delete_me/',
    max_to_keep=2)

for i in range(1000, 2000):
    manager2.save(i)
&lt;/denchmark-code&gt;

What I would like to find in the checkpoint directory is the following list of checkpoints:
ckpt-998, ckpt-999, ckpt-1998, ckpt-1999, or the last two checkpoints saved by each manager.
Instead, there are only ckpt-1998, ckpt-1999. You are suggesting that the solution to this is to change max to keep to 1000, which will save an unnecessary amount of checkpoints.
		</comment>
		<comment id='6' author='georgesterpu' date='2019-11-15T11:38:29Z'>
		Could reproduce the issue with TF Version 2.0. Here is the &lt;denchmark-link:https://colab.sandbox.google.com/gist/rmothukuru/c350687aa6389346a38ac2e7595c2c7b/untitled262.ipynb&gt;Gist&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='7' author='georgesterpu' date='2019-11-18T17:48:28Z'>
		The checkpoint directory should be viewed as a single unit of work, and in general, we would not advise using the same checkpoint dir for multiple checkpoint managers, for this reason and others. In this case, please use separate directories for each Manager, or adjust params to match the expectations of the checkpoint dir rather than each manager alone.
		</comment>
		<comment id='8' author='georgesterpu' date='2019-11-18T18:05:36Z'>
		Thanks, &lt;denchmark-link:https://github.com/karmel&gt;@karmel&lt;/denchmark-link&gt;
, I will update my code in that case.
I have an use case where a new Manager resumes training from the latest checkpoint of a previous Manager, but also need to store the last checkpoint of each manager. Some checkpoint copying script will do it.
		</comment>
		<comment id='9' author='georgesterpu' date='2019-11-18T18:05:37Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34234&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34234&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>