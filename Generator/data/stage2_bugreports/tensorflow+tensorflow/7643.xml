<bug id='7643' author='daniellevy' open_date='2017-02-18T07:17:02Z' closed_time='2017-03-03T23:38:23Z'>
	<summary>Interaction between tf.map_fn and tf.gradients</summary>
	<description>
Hi,
I am using Tensorflow v0.11 and I have tried on Mac OS X and Centos 6
I am running into an error when running the following code:
&lt;denchmark-code&gt;W = tf.get_variable('W', (5, 3))

x = tf.placeholder(tf.float32, shape=(None, 5))

h = tf.matmul(x, W)

grads = tf.map_fn(lambda x: tf.gradients(x, W)[0], h)
&lt;/denchmark-code&gt;

I basically want to have the following but without a fixed batch size:
grads = [tf.gradients(h[t], W)[0] for t in range(batch_size)]
My error is:
&lt;denchmark-code&gt;Invalid argument: TensorArray map/TensorArray_1@map/while/gradients: Could not write to TensorArray index 3 because it has already been read.
[...]
tensorflow.python.framework.errors.InvalidArgumentError: TensorArray map/TensorArray_1@map/while/gradients: Could not write to TensorArray index 3 because it has already been read.
	 [[Node: map/while/gradients/map/while/TensorArrayRead_grad/TensorArrayWrite = TensorArrayWrite[T=DT_FLOAT, _class=["loc:@map/TensorArray"], _device="/job:localhost/replica:0/task:0/cpu:0"](map/while/gradients/map/while/TensorArrayRead_grad/TensorArrayGrad/TensorArrayGrad, map/while/Identity, map/while/gradients/Fill, map/while/gradients/map/while/TensorArrayRead_grad/TensorArrayGrad/gradient_flow)]]
&lt;/denchmark-code&gt;

I have tried the following workaround using scan instead of map_fn with a zero initializer but to no avail:
&lt;denchmark-code&gt;initializer = np.zeros((5, 3)).astype('float32')
grads = tf.scan(
	lambda a, x: tf.gradients(x, W)[0],
	h,
	initializer)
&lt;/denchmark-code&gt;

Is this a know issue?
	</description>
	<comments>
		<comment id='1' author='daniellevy' date='2017-02-18T22:30:04Z'>
		&lt;denchmark-link:https://github.com/yuanbyu&gt;@yuanbyu&lt;/denchmark-link&gt;
, do you have any ideas? &lt;denchmark-link:https://github.com/daniellevy&gt;@daniellevy&lt;/denchmark-link&gt;
 , could you try 1.0, please and see if this is still a problem.
		</comment>
		<comment id='2' author='daniellevy' date='2017-02-19T01:06:48Z'>
		Yes the problem still occurs with v1.0 with the same error message.
		</comment>
		<comment id='3' author='daniellevy' date='2017-02-19T05:41:07Z'>
		It looks like this is a known issue and this issue is probably a duplicate of &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/3972&gt;#3972&lt;/denchmark-link&gt;
. There &lt;denchmark-link:https://github.com/yuanbyu&gt;@yuanbyu&lt;/denchmark-link&gt;
 suggests using tf.while_loop instead of tf.map_fn. Give that a try and let us know if that works. Thanks!
		</comment>
		<comment id='4' author='daniellevy' date='2017-02-23T05:26:23Z'>
		I think this works
&lt;denchmark-code&gt;max_seq_len = 10
x = tf.placeholder(tf.float32, [None, 5])
W = tf.get_variable("W", [5, 3])
h = tf.matmul(x, W)

def body(old_g, t):
    g = tf.gradients([h[t]], [W])[0]
    new_g = tuple(tf.select(tf.equal(ti, t), g, old_g[ti]) for ti in range(len(old_g)))
    return new_g, t + 1

def cond(_, t):
    return tf.less(t, tf.shape(h)[0])

grads = tf.while_loop(cond, body, [(tf.zeros_like(W),)*max_seq_len, tf.constant(0)])

with tf.Session() as sess:
    tf.global_variables_initializer().run()
    grads_out = sess.run(grads, feed_dict={x: np.random.randn(2*5).reshape(2, 5)})
&lt;/denchmark-code&gt;

You basically just build a giant empty tuple and fill it in as you go. dynmaic_rnn does something similar to this.
		</comment>
		<comment id='5' author='daniellevy' date='2017-03-03T23:38:23Z'>
		Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
	</comments>
</bug>