<bug id='9545' author='ethanluoyc' open_date='2017-04-29T21:43:08Z' closed_time='2019-08-06T22:17:04Z'>
	<summary>Duplicate variable shown in Tensorboard expected?</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
MacOS Sierra 12.12.4
TensorFlow installed from (source or binary):
pip
TensorFlow version (use command below):
1.1.0 (CPU)

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

I am trying to implement E2C (available from &lt;denchmark-link:https://arxiv.org/pdf/1506.07365.pdf&gt;https://arxiv.org/pdf/1506.07365.pdf&lt;/denchmark-link&gt;
). Basically it is a neural network that is used for learning a transition model using neural networks. In the training set I have data of the form (X_t, X_t+1) where both X_t and X_t+1 needs to be transformed by an encoding network (e.g. a variational autoencoder). I use the following snippet for creating the encoding network (adapted from &lt;denchmark-link:https://github.com/ericjang/e2c&gt;https://github.com/ericjang/e2c&lt;/denchmark-link&gt;
):
    def encode(self, x, share=None):
        fc = tf.contrib.layers.fully_connected
        with tf.variable_scope('Encoder', reuse=share):
            l1 = fc(x, 400, weights_initializer=tf.orthogonal_initializer(),
                    activation_fn=tf.nn.relu)
            l2 = fc(l1, 100, weights_initializer=tf.orthogonal_initializer(),
                    activation_fn=tf.nn.relu)
            return l2

    def decode(self, z, share=None):
        fc = tf.contrib.layers.fully_connected
        with tf.variable_scope("Decoder", reuse=share):
            l1 = fc(z, 100, weights_initializer=tf.orthogonal_initializer(1.1),
                    activation_fn=tf.nn.relu)
            l2 = fc(l1, 400, weights_initializer=tf.orthogonal_initializer(1.1),
                    activation_fn=tf.nn.relu)

            return fc(l2, self.x_dim,
                      weights_initializer=tf.orthogonal_initializer(1.1),
                      activation_fn=tf.nn.sigmoid)
Then I would use something like
h_enc_t = encoder(X_t)
h_enc_t_next = encoder(X_{t+1}, share=True)
to create the encoded output for the model.
The problem is that when visualizing this on Tensorboard, while it is sharing the variables by setting share=True for the variable scope, on the graph visulisation you will have Encoder and Encoder_1 instead of just a Decoder scope. Of course they took different input since we need to transform X_t and X_t+1, but shouldn't the network be wrapped in the same scope since underneath we are reusing the same weights? I wonder if it is a feature to have Encoder_1 and Encoder separately or it is a limitation of the variable scoping. The problem is illustrated in the screenshot below, you will see duplicates for 'Encoder' 'SampleQPhi" etc:
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/6040760/25559064/98fecc5a-2d2b-11e7-8669-00b1227abf17.png&gt;&lt;/denchmark-link&gt;

However, I would expect something like this (as appeared in the paper) to be a more reasonable visualization (h_enc) with input x_t and x_t+1 are the same network:
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/6040760/25565322/fc04ab8a-2dbc-11e7-9876-5e823a3bf47b.png&gt;&lt;/denchmark-link&gt;

Many thanks in advance!
	</description>
	<comments>
		<comment id='1' author='ethanluoyc' date='2017-05-01T02:24:37Z'>
		&lt;denchmark-link:https://github.com/dandelionmane&gt;@dandelionmane&lt;/denchmark-link&gt;
, could you please take a look?
		</comment>
		<comment id='2' author='ethanluoyc' date='2017-05-04T22:42:22Z'>
		I need to take a look more closely, but I think calling tf.scope creates a new namespace in general, appending _[Some int] to the name if necessary for uniqueness.
This might deviate from your intention, but what if you move the Encoder scope outside of encode like this?
&lt;denchmark-code&gt;with tf.variable_scope('Encoder'):
  h_enc_t = encoder(X_t)
  h_enc_t_next = encoder(X_{t+1}, share=True)
&lt;/denchmark-code&gt;

And then
&lt;denchmark-code&gt;def encode(self, x, share=None):
  fc = tf.contrib.layers.fully_connected
  with tf.variable_scope('encode', reuse=share):
    l1 = fc(x, 400, weights_initializer=tf.orthogonal_initializer(),
            activation_fn=tf.nn.relu)
    l2 = fc(l1, 100, weights_initializer=tf.orthogonal_initializer(),
            activation_fn=tf.nn.relu)
    return l2
&lt;/denchmark-code&gt;

?
		</comment>
		<comment id='3' author='ethanluoyc' date='2017-05-05T17:08:32Z'>
		&lt;denchmark-link:https://github.com/chihuahua&gt;@chihuahua&lt;/denchmark-link&gt;
 Nope this won't work. You would obtain something that's even weirder.
This is the graph:
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/6040760/25756242/f0754a2c-31bd-11e7-8951-2ae16760e318.png&gt;&lt;/denchmark-link&gt;

A closer look:
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/6040760/25756118/81e85bee-31bd-11e7-9ad3-016893b18d14.png&gt;&lt;/denchmark-link&gt;

The reason I think this is not the intended behavior is because. In some settings, it's really common to take a part of the computation graph as a module and take input from different sources. It turns out that Tensorboard does not recognize that the same part of the graph is used for different inputs. Thus it fails to use a common scope for them.

I tried to use &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/make_template&gt;make_template&lt;/denchmark-link&gt;
 to see this would work in that setting. But apparently it is not....
		</comment>
		<comment id='4' author='ethanluoyc' date='2017-07-27T09:37:01Z'>
		Reusing a lot of layers I encounter the same messy graphs in TensorBoard.
It would be nice to have the two namespaces merged if they contain the same variables!
		</comment>
		<comment id='5' author='ethanluoyc' date='2017-11-24T16:45:05Z'>
		Have same issue
		</comment>
		<comment id='6' author='ethanluoyc' date='2017-12-22T07:58:25Z'>
		Any updates on this in recent releases?
		</comment>
		<comment id='7' author='ethanluoyc' date='2018-01-11T19:56:30Z'>
		Why does a tf.VariableScope open up a name scope?
I am confused why in the end I have a node called ns/s4_1/concat and one called s4/concat.
I would expect it to be ns/s4/concat and ns/s4/concat_1 or something
import tensorflow as tf

with tf.name_scope('ns'):
  s4 = tf.VariableScope(tf.AUTO_REUSE, name='s4')

  with tf.variable_scope(s4, tf.AUTO_REUSE):
    print(tf.get_variable('f', []))

  with tf.variable_scope(s4, True):
    print(tf.get_variable('f', []))
    print(tf.concat([tf.constant([1]), tf.constant([1])], 0))

with tf.variable_scope(s4, True):
    print(tf.concat([tf.constant([1]), tf.constant([1])], 0))

session = tf.Session()

tf.summary.FileWriter('./tmp/scope/', session.graph)
		</comment>
		<comment id='8' author='ethanluoyc' date='2018-01-25T01:19:58Z'>
		Maybe &lt;denchmark-link:https://github.com/jart&gt;@jart&lt;/denchmark-link&gt;
 has some ideas?
		</comment>
		<comment id='9' author='ethanluoyc' date='2018-03-13T14:27:32Z'>
		A workaround to prevent variable scope from opening new namescopes is the following
from tensorflow.python.ops import variable_scope as var_scope

def simple_variable_scope(name_or_scope, reuse=None):
  """Creates a variable scope without also creating a name scope."""
  return var_scope.variable_scope(name_or_scope, reuse=reuse,
                                  auxiliary_name_scope=False)
		</comment>
		<comment id='10' author='ethanluoyc' date='2018-03-27T19:39:38Z'>
		&lt;denchmark-link:https://github.com/dsmilkov&gt;@dsmilkov&lt;/denchmark-link&gt;
 Is this a tensorboard-side issue?
		</comment>
		<comment id='11' author='ethanluoyc' date='2018-03-28T13:59:35Z'>
		I still observe that even when a variable scope does not create an extra name scope tensorboard still treats parts of the graph that receive varying inputs as multiple/duplicate nodes.
		</comment>
		<comment id='12' author='ethanluoyc' date='2018-06-06T05:03:34Z'>
		Does this Problem by any chance take up excess gpu memory ?
		</comment>
		<comment id='13' author='ethanluoyc' date='2018-06-07T14:38:03Z'>
		I would hope not, but feel free to create a test case.
		</comment>
		<comment id='14' author='ethanluoyc' date='2018-07-18T16:00:21Z'>
		I had the same issue issue: each time the variable_scope is closed and reopened, it creates a new scope name by appending some integer at the end. As a consequence, the variables are duplicated (as we can witness on tensorboard).
My (unelegant) solution to the problem: add everything that has to rest under the same scope in one go and close the scope afterward. I am aware that this might not be a solution in a lot of cases but I hope that it will help at least some!
		</comment>
		<comment id='15' author='ethanluoyc' date='2018-07-19T16:25:53Z'>
		/cc &lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;
 Probably related to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/16468#issuecomment-385662734&gt;#16468 (comment)&lt;/denchmark-link&gt;

		</comment>
		<comment id='16' author='ethanluoyc' date='2018-07-19T16:45:16Z'>
		Anybody tested if this has any memory impact?
		</comment>
		<comment id='17' author='ethanluoyc' date='2018-07-19T21:28:12Z'>
		&lt;denchmark-link:https://github.com/ewilderj&gt;@ewilderj&lt;/denchmark-link&gt;
 What is the default policy to handle these cases?
The assignee was more the 1 year inactive (without comments) on this and was labeled as bug.
Every time someone make a comment the bot reset the Nagging Assignee counter.
How the TF team could be aware of the status of issues like this one (it is not the only one with a similar status/history)?
I think that the &lt;denchmark-link:https://github.com/tensorflowbutler&gt;@tensorflowbutler&lt;/denchmark-link&gt;
 needs to count only TF team no activity days.
/cc &lt;denchmark-link:https://github.com/av8ramit&gt;@av8ramit&lt;/denchmark-link&gt;
  What do you think?
		</comment>
		<comment id='18' author='ethanluoyc' date='2018-07-19T22:24:42Z'>
		&lt;denchmark-link:https://github.com/aselle&gt;@aselle&lt;/denchmark-link&gt;
 thoughts?
		</comment>
		<comment id='19' author='ethanluoyc' date='2018-07-19T22:45:42Z'>
		&lt;denchmark-link:https://github.com/av8ramit&gt;@av8ramit&lt;/denchmark-link&gt;
 sorry I didn't want to bother you. Was just for the &lt;denchmark-link:https://github.com/tensorflowbutler&gt;@tensorflowbutler&lt;/denchmark-link&gt;
 counting behavior   I think that the nagging assignee counter doesn't need to have a reset after non TF team members comments.
		</comment>
		<comment id='20' author='ethanluoyc' date='2018-07-19T22:48:24Z'>
		&lt;denchmark-link:https://github.com/qmeeus&gt;@qmeeus&lt;/denchmark-link&gt;
 If you really want to reuse name scopes then you can do this:
def absolute_name_scope(scope):
  """Builds an absolute tf.name_scope relative to the current_scope.
  This is helpful to reuse nested name scopes.

  E.g. The following will happen when using regular tf.name_scope:

    with tf.name_scope('outer'):
      with tf.name_scope('inner'):
        print(tf.constant(1)) # Will print outer/inner/Const:0
    with tf.name_scope('outer'):
      with tf.name_scope('inner'):
        print(tf.constant(1)) # Will print outer/inner_1/Const:0

  With absolute_name_scope:

    with absolute_name_scope('outer'):
      with absolute_name_scope('inner'):
        print(tf.constant(1)) # Will print outer/inner/Const:0
    with absolute_name_scope('outer'):
      with absolute_name_scope('inner'):
        print(tf.constant(1)) # Will print outer/inner/Const_1:0
  """
  current_scope = tf.get_default_graph().get_name_scope()
  if not current_scope:
    if scope.endswith('/'):
      name_scope = tf.name_scope(scope)
    else:
      name_scope = tf.name_scope('{}/'.format(scope))
  else:
    name_scope = tf.name_scope('{}/{}/'.format(current_scope, scope))
  return name_scope
Tensorflow team, I can also add this as a PR if desired. I find this rather useful to get clean scopes in Tensorboard.
		</comment>
		<comment id='21' author='ethanluoyc' date='2018-07-20T00:16:02Z'>
		It is not a problem at all &lt;denchmark-link:https://github.com/bhack&gt;@bhack&lt;/denchmark-link&gt;
 and absolutely no bother. I'm always here to help the community. I'll default to Andy on this one.
		</comment>
		<comment id='22' author='ethanluoyc' date='2018-07-20T08:00:02Z'>
		Im not sure if it does have an impact on memory usage but I was training a model similar to segnet with image input size as 640x1024 and, at max I could only set my batch size to 1 image which is kind of strange (not sure if it should take that much memory) i was using a titanX (12gb ) and it ended up using the whole 12 gb.
(if I tried increasing batch size to anything greater than 1 I got CUDA out of memory error)
Is there any way to check how much memory a particular model should be taking ?
		</comment>
		<comment id='23' author='ethanluoyc' date='2018-07-20T08:36:21Z'>
		&lt;denchmark-link:https://github.com/rajnunes&gt;@rajnunes&lt;/denchmark-link&gt;
 Have you tried to check memory with &lt;denchmark-link:https://www.tensorflow.org/guide/graph_viz#runtime_statistics&gt;https://www.tensorflow.org/guide/graph_viz#runtime_statistics&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='24' author='ethanluoyc' date='2018-07-20T13:12:27Z'>
		thanks &lt;denchmark-link:https://github.com/sleighsoft&gt;@sleighsoft&lt;/denchmark-link&gt;
, appreciate it!
&lt;denchmark-link:https://github.com/bhack&gt;@bhack&lt;/denchmark-link&gt;
 the memory use was the thing that tipped me off. Indeed, I think that the duplication impacts memory, as I was getting those "More than 10% of total memory allocated" warnings and eventually, the program crashed... Those warnings and crashed disappeared after modifying my code
		</comment>
		<comment id='25' author='ethanluoyc' date='2018-07-20T13:28:35Z'>
		&lt;denchmark-link:https://github.com/qmeeus&gt;@qmeeus&lt;/denchmark-link&gt;
 I don't know if it is strictly related to the  handling.
In cases like &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/16468#issuecomment-385662734&gt;#16468 (comment)&lt;/denchmark-link&gt;
:
I am not using an explicit variable scope but seems that has some impact on TB grouping/rendering.
The runtime statistics on the Tensorboard tell that  and  has its own memory allocated and on the TB seems a duplicated allocation.
		</comment>
		<comment id='26' author='ethanluoyc' date='2018-07-20T20:37:58Z'>
		&lt;denchmark-link:https://github.com/bhack&gt;@bhack&lt;/denchmark-link&gt;
 if there are systemic issues with the bug nagging/updating, then let's file this as a bug itself so we can track it, assign it, and collect the multiple instances of where it's happening. Thanks for highlighting this!
		</comment>
		<comment id='27' author='ethanluoyc' date='2018-07-20T21:28:44Z'>
		&lt;denchmark-link:https://github.com/ewilderj&gt;@ewilderj&lt;/denchmark-link&gt;


the @tensorflowbutler counting behavior 😄 I think that the nagging assignee counter doesn't need to have a reset after non TF team members comments.

This could help to have a better rapresentation of MIA
		</comment>
		<comment id='28' author='ethanluoyc' date='2018-11-05T23:56:00Z'>
		The normal TensorFlow way to handle using the same scope in two places is:
&lt;denchmark-code&gt;    with tf.name_scope('outer'):
      with tf.name_scope('inner') as scope:
        print(tf.constant(1)) # Will print outer/inner/Const:0
    with tf.name_scope(scope):
      print(tf.constant(1)) # Will print outer/inner/Const_1:0
&lt;/denchmark-code&gt;

		</comment>
		<comment id='29' author='ethanluoyc' date='2018-11-06T08:55:15Z'>
		AFAIK, this bug is not about shared scopes but having variables abnormally duplicated in tensorboard plots when said variable is used at different locations in the code.
		</comment>
		<comment id='30' author='ethanluoyc' date='2018-11-22T15:49:42Z'>
		I am facing the same issue. Its a problem with tensor-board creating extra instances of shared layers.
&lt;denchmark-code&gt;def inference():
    with tf.variable_scope("inner", reuse=tf.AUTO_REUSE):
        x = tf.placeholder(dtype = tf.float32, shape=(None, 30,30,3))
        h = tf.layers.conv2d(x, 5, [3, 3], [1, 1], padding='SAME')
        print(tf.get_default_graph().get_name_scope()) 
        return h

with tf.variable_scope("outer"):
    x = inference() _# will print outer/inner_
    x2 = inference() _# will print outer/inner_1_
&lt;/denchmark-code&gt;

But output of the model summary shows that the variables are being shared.
&lt;denchmark-code&gt;outer/inner/conv2d/kernel:0 (float32_ref 3x3x3x5) [135, bytes: 540]
outer/inner/conv2d/bias:0 (float32_ref 5) [5, bytes: 20]
Total size of variables: 140
Total bytes of variables: 560
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/sleighsoft&gt;@sleighsoft&lt;/denchmark-link&gt;
's PR is still in review. Is there any suggested workaround?
		</comment>
		<comment id='31' author='ethanluoyc' date='2019-08-06T22:17:04Z'>
		This is a stale issue. Please check the issue with latest TensorFlow. If the issue still persists in the newer version of TF, please feel free to reopen it by providing details about the issue and a standalone code to reproduce the issue. Thanks!
		</comment>
		<comment id='32' author='ethanluoyc' date='2019-08-06T22:17:05Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=9545&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=9545&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='33' author='ethanluoyc' date='2020-01-05T16:31:47Z'>
		The graph associated to this model:
import tensorflow as tf

# Meaningless model
input_A = tf.keras.Input(shape=(1,2), name='Input_A')
input_B = tf.keras.Input(shape=(1,2), name='Input_B')

inner_block = tf.keras.Sequential(
    [ tf.keras.layers.Dense(1) for i in range(50) ])

output = inner_block(input_A) + inner_block(input_B)

model = tf.keras.Model((input_A,input_B), output)

# Write
tf.keras.callbacks.TensorBoard('.', write_graph=True).set_model(model)
is:
&lt;denchmark-link:https://user-images.githubusercontent.com/25563883/71782839-f2255700-2fde-11ea-9efd-3296200885fc.png&gt;&lt;/denchmark-link&gt;

All layers and namespaces are working as expected, however the data flow is made more complex by dependencies of ReadVariableOp inside sequential_1. In more complex models, the real inputs become the thinner connections and they are hard to follow. Also, shared layers appear with different colors or gray (probably due to the different ops inside the original and reused variables). For example, in
&lt;denchmark-link:https://user-images.githubusercontent.com/25563883/71783501-e5593100-2fe7-11ea-8ffa-632e212b3c99.png&gt;&lt;/denchmark-link&gt;

the three generators are the same layer, called three times, and the thicker arrows are propagating variables.
I don't want to force Tf to merge namescopes, as this would make each block accept multiple inputs, giving a wrong visual hint (multiple dependencies, not runs).
A nice toggle in TensorBoard to exclude inputs of ReadVariableOp (and why not, control dependencies) would simplify graphs a lot.
Details: Tensorflow 2.1.0-rc0 built from source.
		</comment>
		<comment id='34' author='ethanluoyc' date='2020-01-07T15:13:45Z'>
		Maybe you should open this as a separate issue at /tensorboard.
		</comment>
		<comment id='35' author='ethanluoyc' date='2020-01-07T16:40:10Z'>
		Posted &lt;denchmark-link:https://github.com/tensorflow/tensorboard/issues/3118&gt;tensorflow/tensorboard#3118&lt;/denchmark-link&gt;
, thanks
		</comment>
	</comments>
</bug>