<bug id='46334' author='arsenal-2004' open_date='2021-01-11T09:44:55Z' closed_time='2021-01-13T06:38:09Z'>
	<summary>Error when compiling a fully-integer quantized model for the EdgeTPU</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
TensorFlow installed from (source or binary): Binary
TensorFlow version (or github SHA if from source): tf-nightly

Command used to run the converter or code if youâ€™re using the Python API
If possible, please share a link to Colab/Jupyter/any notebook.
&lt;denchmark-code&gt;import cv2
import tensorflow as tf
import numpy as np

def representative_dataset_gen():
	for image in raw_image_paths:
		image = cv2.imread(image)
		image = cv2.resize(image, (128, 256))
		image = image[..., ::-1]
		image = image * (1 / 255)
		image[0, ...] = (image[0, ...] - 0.485) / 0.229
		image[1, ...] = (image[1, ...] - 0.456) / 0.224
		image[2, ...] = (image[2, ...] - 0.406) / 0.225
		image = tf.expand_dims(tf.convert_to_tensor(image, dtype = tf.float32), axis = 0)
		yield [image]

with open('selected_files.txt') as f:
	raw_image_paths = f.read().split('\n')[:-1]

# Full Integer Quantization - Input/Output=float32
converter = tf.lite.TFLiteConverter.from_saved_model('/home/parth/Internships/Clutterbot/Trial_4/saved_model')
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
converter.allow_custom_ops = True
converter.representative_dataset = representative_dataset_gen
tflite_quant_model = converter.convert()
&lt;/denchmark-code&gt;

The output from the converter invocation
&lt;denchmark-code&gt;No issues here
&lt;/denchmark-code&gt;


The Zip file contains the Saved_Model, as well as the full-integer quantized TFLite model.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/5794906/osnet_x0_25_msmt17_batch_1.zip&gt;osnet_x0_25_msmt17_batch_1.zip&lt;/denchmark-link&gt;

Failure details
If the conversion is successful, but the generated model is wrong,
state what is wrong:

The conversion is successful
The issue arises when trying to compile the TFLite model for the EdgeTPU.

&lt;denchmark-code&gt;Edge TPU Compiler version 15.0.340273435
loc("model/depthwise_conv2d_3/depthwise"): error: Invalid argument: Quantized tensors must have non-zero scales
error: could not translate function : Quantized tensors must have non-zero scales

Internal compiler error. Aborting! 
&lt;/denchmark-code&gt;


I have also mentioned the error in the issue &lt;denchmark-link:https://github.com/google-coral/edgetpu/issues/290&gt;here&lt;/denchmark-link&gt;
.
	</description>
	<comments>
		<comment id='1' author='arsenal-2004' date='2021-01-11T10:04:08Z'>
		&lt;denchmark-link:https://github.com/jingpu&gt;@jingpu&lt;/denchmark-link&gt;
 could you take a look at this?
		</comment>
		<comment id='2' author='arsenal-2004' date='2021-01-11T17:49:36Z'>
		&lt;denchmark-link:https://github.com/arsenal-2004&gt;@arsenal-2004&lt;/denchmark-link&gt;
 Could you attached the converted TFLite model?
		</comment>
		<comment id='3' author='arsenal-2004' date='2021-01-11T18:29:30Z'>
		&lt;denchmark-link:https://github.com/jingpu&gt;@jingpu&lt;/denchmark-link&gt;
 I have attached the TFLite model as part of the Zip file.

osnet_x0_25_msmt17_batch_1.zip

Thank you for your help :)
		</comment>
		<comment id='4' author='arsenal-2004' date='2021-01-11T18:40:38Z'>
		&lt;denchmark-link:https://github.com/jingpu&gt;@jingpu&lt;/denchmark-link&gt;
 I have also attached the original Saved_Model, in case you want to convert it with some different settings.
		</comment>
		<comment id='5' author='arsenal-2004' date='2021-01-11T19:07:39Z'>
		I looked at the converted TFLite model. The weight of the "model/depthwise_conv2d_3/depthwise" node is INT8 type but doesn't have quantization parameter, which causes the Edge TPU compiler to raise the error.
&lt;denchmark-link:https://github.com/abattery&gt;@abattery&lt;/denchmark-link&gt;
 Could you take a look if the conversion is correct?
		</comment>
		<comment id='6' author='arsenal-2004' date='2021-01-11T19:14:20Z'>
		&lt;denchmark-link:https://github.com/jingpu&gt;@jingpu&lt;/denchmark-link&gt;
 How did you check that? I used Netron and it does show a quantization parameter, although it's quite small.
&lt;denchmark-link:https://user-images.githubusercontent.com/32266223/104227304-2e4f8b00-546f-11eb-8dd9-38c73c7b9d80.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='arsenal-2004' date='2021-01-11T19:22:26Z'>
		The input and output do have quantization parameters, but the weights and bias don't.
&lt;denchmark-link:https://user-images.githubusercontent.com/2155368/104228115-3e2f8680-53ff-11eb-8182-35aab1b5225e.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='arsenal-2004' date='2021-01-11T19:25:54Z'>
		Oh okay. I guess I misunderstood. I thought that since even the Conv2D nodes do not have a quantization parameter for the 'filter' and 'bias', none of the nodes would have it
		</comment>
		<comment id='9' author='arsenal-2004' date='2021-01-11T19:43:07Z'>
		I dug a little more. It seems like the weights have quantization parameter but it is using per-axis quantization and Netron fails to visualize it.
And so the real issue is for one of the axis the quantization scale is zero, which I think it is a bug in the quantizer of the TFLite Converter.
		</comment>
		<comment id='10' author='arsenal-2004' date='2021-01-11T20:05:47Z'>
		Oh okay. That seems like it would take a while to resolve. Thank you though.
If I may ask, do you know of any person-reid models that have been tested on the Coral? I have been looking a lot but couldn't get anything like that.
		</comment>
		<comment id='11' author='arsenal-2004' date='2021-01-11T23:41:27Z'>
		Parth, you can set converter._experimental_new_quantizer=True to avoid the zero scale issue. The new quantizer will set the scale to fixed small value and hopefully the edge tpu compiler can handle that.
		</comment>
		<comment id='12' author='arsenal-2004' date='2021-01-12T04:26:28Z'>
		&lt;denchmark-link:https://github.com/liufengdb&gt;@liufengdb&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/abattery&gt;@abattery&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jingpu&gt;@jingpu&lt;/denchmark-link&gt;
 Thank you guys so much. Using the new quantizer worked for the compilation :)
I'll try out the new compiled model today and see if the results are as expected. If it works out, I'll close this issue in the evening
		</comment>
		<comment id='13' author='arsenal-2004' date='2021-01-13T06:38:11Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46334&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46334&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>