<bug id='6635' author='adamsyu' open_date='2017-01-04T13:11:04Z' closed_time='2018-02-07T23:52:03Z'>
	<summary>Incorrect gradient when using tf.dynamic_stitch and tf.gather?</summary>
	<description>
In Tensorflow 0.12, I find the discrepancy of gradients in two mathematically equivalent training procedures of LSTM, probably due to the use of tf.gather and tf.dynamic_stitch.  One is the normal procedure using the whole batch of training examples to unroll the LSTM in each step. The other first uses tf.gather to select ALL the examples of the whole batch in each step, then unroll the LSTM with those examples and finally use tf.dynamic_stitch to update the corresponding states and outputs.
These two procedures should be equivalent as they both essentially use the whole batch. However, the gradients of the same variables are significantly different.
The code is as follows (the core parts are essentially # 1. and  # 2.):
batch_size = 2
num_timesteps = 10
vocab_size = 10
num_embedding_nodes = 32
hidden_size = 128
n_class = 2
learning_rate = 0.001
inputs = tf.placeholder(tf.int64, [batch_size, num_timesteps])
targets = tf.placeholder(tf.int64, [batch_size])
embedding = tf.get_variable("embedding", [vocab_size, num_embedding_nodes])
x = tf.nn.embedding_lookup(embedding, inputs)
w_predict = tf.get_variable("w_predict", [hidden_size, n_class])
b_predict = tf.get_variable("b_predict", [n_class])

with tf.variable_scope('lstm') as lstm_scope:
  cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0, state_is_tuple=False)
  state = cell.zero_state(batch_size, dtype=tf.float32)
  state1 = cell.zero_state(batch_size, dtype=tf.float32)
  for t in range(num_timesteps):
    if t == 0:
      output, state = cell(x[:, t, :], state)
      lstm_scope.reuse_variables()
      output1, state1 = cell(x[:, t, :], state1)
    else:
      lstm_scope.reuse_variables()
      # 1. normal lstm 
      output, state = cell(x[:, t, :], state)
      # 2. lstm using tf.gather and tf.dynamic_stitch to select all samples from batch
      idx_select = tf.range(batch_size)
      tmp_output, tmp_state = cell(tf.gather(x[:, t, :], idx_select), tf.gather(state1, idx_select))
      output1 = tf.dynamic_stitch([tf.range(batch_size), idx_select], [output1, tmp_output])
      state1 = tf.dynamic_stitch([tf.range(batch_size), idx_select], [state1, tmp_state])
logits = tf.nn.xw_plus_b(output, w_predict, b_predict)
cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, targets, name=None)
logits1 = tf.nn.xw_plus_b(output1, w_predict, b_predict)
cross_entropy1 = tf.nn.sparse_softmax_cross_entropy_with_logits(logits1, targets, name=None)
trainable = tf.trainable_variables()
grad = tf.gradients(cross_entropy, trainable)
grad1 = tf.gradients(cross_entropy1, trainable)

######## gradient log
cg = []
for g in grad:
  if g is not None:
    cg += [g]
cg1 = []
for g in grad1:
  if g is not None:
    cg1 += [g]

optimizer = tf.train.AdamOptimizer(learning_rate)
train_op = optimizer.apply_gradients(zip(grad, trainable))
init_op = tf.initialize_all_variables()


""" Training """
## arbitrary synthetic data
# we use two training examples, each with length 10
my_input = np.array([[3,4,6,8,3,5,8,9,2,4], [4,2,3,8,5,2,2,3,6,1]])
my_target = np.array([0,1])
sess = tf.Session()
sess.run(init_op)

epoch = 0
while epoch &lt; 5:
  epoch += 1
  fetches = [train_op, cg, cg1]
  outputs = sess.run(fetches, feed_dict={inputs: my_input, targets: my_target})
  gradients = outputs[1] 
  gradients1 = outputs[2] 
  print 'epoch %d:' % epoch 
    for i, g in enumerate(gradients):
      if i &gt; 0:
	print('norm of gradient of var %d: %f' % (i, LA.norm(gradients[i])))
	print('norm of gradient1 of var %d: %f' % (i, LA.norm(gradients1[i])))
The sample output is:
&lt;denchmark-code&gt;epoch 1:
norm of gradient of var 1: 0.644620
norm of gradient1 of var 1: 0.644620
norm of gradient of var 2: 0.393020
norm of gradient1 of var 2: 0.393020
norm of gradient of var 3: 0.838759
norm of gradient1 of var 3: 102.815338
norm of gradient of var 4: 0.435841
norm of gradient1 of var 4: 44.867126
epoch 2:
norm of gradient of var 1: 0.613848
norm of gradient1 of var 1: 0.611423
norm of gradient of var 2: 0.355387
norm of gradient1 of var 2: 0.351987
norm of gradient of var 3: 0.797761
norm of gradient1 of var 3: 96.391121
norm of gradient of var 4: 0.397020
norm of gradient1 of var 4: 39.937107
epoch 3:
norm of gradient of var 1: 0.603118
norm of gradient1 of var 1: 0.603118
norm of gradient of var 2: 0.318260
norm of gradient1 of var 2: 0.318260
norm of gradient of var 3: 0.773661
norm of gradient1 of var 3: 93.290131
norm of gradient of var 4: 0.366684
norm of gradient1 of var 4: 36.636879
epoch 4:
norm of gradient of var 1: 0.607643
norm of gradient1 of var 1: 0.607643
norm of gradient of var 2: 0.280101
norm of gradient1 of var 2: 0.280101
norm of gradient of var 3: 0.763007
norm of gradient1 of var 3: 92.295441
norm of gradient of var 4: 0.340769
norm of gradient1 of var 4: 33.630474
epoch 5:
norm of gradient of var 1: 0.622874
norm of gradient1 of var 1: 0.619443
norm of gradient of var 2: 0.239731
norm of gradient1 of var 2: 0.235509
norm of gradient of var 3: 0.757205
norm of gradient1 of var 3: 93.335388
norm of gradient of var 4: 0.312203
norm of gradient1 of var 4: 30.536301
&lt;/denchmark-code&gt;

We can see that the gradient and gradient1 of var3 have significantly different norms in every epoch, which should be the same. So is var4. Those two are the trainable variables of LSTM. In fact, if the sequence length is 50 instead of 10, the discrepancy is even much larger.
Could anybody tell me why it is the case?
&lt;denchmark-h:h3&gt;Environment info&lt;/denchmark-h&gt;

Operating System: ubuntu 14.04
	</description>
	<comments>
		<comment id='1' author='adamsyu' date='2017-01-06T17:53:35Z'>
		Is this on CPU?
		</comment>
		<comment id='2' author='adamsyu' date='2017-01-06T23:31:54Z'>
		&lt;denchmark-link:https://github.com/michaelisard&gt;@michaelisard&lt;/denchmark-link&gt;
 , yes, on CPU.
		</comment>
		<comment id='3' author='adamsyu' date='2017-01-07T00:27:16Z'>
		&lt;denchmark-link:https://github.com/rmlarsen&gt;@rmlarsen&lt;/denchmark-link&gt;
 any idea why this might be happening?
		</comment>
		<comment id='4' author='adamsyu' date='2017-06-16T17:00:06Z'>
		&lt;denchmark-link:https://github.com/rmlarsen&gt;@rmlarsen&lt;/denchmark-link&gt;
 Were you planning to respond based on the tag you added?  Someone should probably track this down.
		</comment>
		<comment id='5' author='adamsyu' date='2017-12-20T19:26:26Z'>
		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
		</comment>
		<comment id='6' author='adamsyu' date='2018-01-04T19:17:48Z'>
		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
		</comment>
		<comment id='7' author='adamsyu' date='2018-01-23T23:12:09Z'>
		A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.
		</comment>
		<comment id='8' author='adamsyu' date='2018-02-07T13:50:32Z'>
		Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='9' author='adamsyu' date='2018-02-07T23:52:03Z'>
		Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
	</comments>
</bug>