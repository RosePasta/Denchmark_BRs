<bug id='41135' author='hoangcuong2011' open_date='2020-07-06T20:07:27Z' closed_time='2020-07-24T05:38:56Z'>
	<summary>Possible bug(?): tensorflow.python.eager.core._SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [&amp;lt;tf.Tensor X&amp;gt;]</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS High Sierra
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): &gt;= 2.0
Python version: 3.6
Running on: CPUs (But I guess it happens on GPUs as well)

The following issue is very similar to this &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/41111&gt;one&lt;/denchmark-link&gt;
 I posted before. The difference here is that I use eager execution in the code and it produces an error of Keras symbolic tensors. To be fair I am not sure this is a bug or the error is on purpose. Here is the code to produce the error:
&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow.keras import backend as K
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

class MyWordEmbedding(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(MyWordEmbedding, self).__init__(**kwargs)

    def build(self, input_shape):
        self.kernel = self.add_weight(shape=(300, 512), dtype='float32')
        super(MyWordEmbedding, self).build(input_shape)  # Be sure to call this at the end
    
    def call(self, inputs):
        return tf.nn.embedding_lookup(params=self.kernel, ids=inputs[0])

class EncoderLayer(tf.keras.layers.Layer):
    def __init__(self, mask_para, **kwargs):
        self.mask_para = mask_para
        super(EncoderLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.Qdense = self.add_weight(name='Qdense', shape=(512, 512))
        super(EncoderLayer, self).build(input_shape)

    def call(self, x):
        Qoutput = tf.einsum('aij,jk-&gt;aik', x[0], self.Qdense)
        Koutput =  tf.einsum('aij,jk-&gt;aik', x[0], self.Qdense)
        Voutput =  tf.einsum('aij,jk-&gt;aik', x[0], self.Qdense)
        a = tf.einsum('ajk,afk-&gt;ajf', Qoutput, Koutput) * tf.tile(K.expand_dims(self.mask_para, axis=1), [1, 64, 1])
        a = tf.matmul(a, Voutput)
        print(a)
        return a

    def compute_mask(self, inputs, mask):
        return mask

    def compute_output_shape(self, input_shape):
        return input_shape[0]

def create_encoder_model():
    word_ids_fr = tf.keras.layers.Input(dtype='int32', shape=(None,))
    a = MyWordEmbedding()([word_ids_fr])
    a = EncoderLayer(K.cast(K.not_equal(0, word_ids_fr), dtype='float32'))([a])
    model = tf.keras.models.Model(inputs=[word_ids_fr], outputs=a)
    return model

def create_model():
    word_ids_en = tf.keras.layers.Input(dtype='int32', shape=(None,))
    a = tf.keras.layers.Input(shape=(None, 512,))
    b = MyWordEmbedding()([word_ids_en])
    b = b + a
    model = tf.keras.models.Model(inputs=[word_ids_en, a], outputs=b)
    return model
    
def evaluate():
    source_sequence_ids = pad_sequences(np.random.randint(5, size=(3, 64)), maxlen=64, padding='pre')
    output = decoder_model.predict([pad_sequences(np.random.randint(5, size=(3, 64)), maxlen=64, padding='post'), encoder_model(source_sequence_ids, training=False)], steps=1, verbose=1, batch_size=256)

decoder_model = create_model()
encoder_model = create_encoder_model()
evaluate()
&lt;/denchmark-code&gt;

Error:
tensorflow.python.eager.core._SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [&lt;tf.Tensor 'MatMul:0' shape=(3, 64, 512) dtype=float32&gt;]
Meanwhile, I also provide a way to fix this as follows (just a simple modification):
&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow.keras import backend as K
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

class MyWordEmbedding(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(MyWordEmbedding, self).__init__(**kwargs)

    def build(self, input_shape):
        self.kernel = self.add_weight(shape=(300, 512), dtype='float32')
        super(MyWordEmbedding, self).build(input_shape)  # Be sure to call this at the end
    
    def call(self, inputs):
        return tf.nn.embedding_lookup(params=self.kernel, ids=inputs[0])

class EncoderLayer(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(EncoderLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.Qdense = self.add_weight(name='Qdense', shape=(512, 512))
        super(EncoderLayer, self).build(input_shape)

    def call(self, x):
        mask_para = x[1]
        Qoutput = tf.einsum('aij,jk-&gt;aik', x[0], self.Qdense)
        Koutput =  tf.einsum('aij,jk-&gt;aik', x[0], self.Qdense)
        Voutput =  tf.einsum('aij,jk-&gt;aik', x[0], self.Qdense)
        a = tf.einsum('ajk,afk-&gt;ajf', Qoutput, Koutput) * tf.tile(K.expand_dims(mask_para, axis=1), [1, 64, 1])
        a = tf.matmul(a, Voutput)
        print(a)
        return a

    def compute_mask(self, inputs, mask):
        return mask

    def compute_output_shape(self, input_shape):
        return input_shape[0]

def create_encoder_model():
    word_ids_fr = tf.keras.layers.Input(dtype='int32', shape=(None,))
    a = MyWordEmbedding()([word_ids_fr])
    a = EncoderLayer()([a, K.cast(K.not_equal(0, word_ids_fr), dtype='float32')])
    model = tf.keras.models.Model(inputs=[word_ids_fr], outputs=a)
    return model

def create_model():
    word_ids_en = tf.keras.layers.Input(dtype='int32', shape=(None,))
    a = tf.keras.layers.Input(shape=(None, 512,))
    b = MyWordEmbedding()([word_ids_en])
    b = b + a
    model = tf.keras.models.Model(inputs=[word_ids_en, a], outputs=b)
    return model
    
def evaluate():
    source_sequence_ids = pad_sequences(np.random.randint(5, size=(3, 64)), maxlen=64, padding='pre')
    output = decoder_model.predict([pad_sequences(np.random.randint(5, size=(3, 64)), maxlen=64, padding='post'), encoder_model(source_sequence_ids, training=False)], steps=1, verbose=1, batch_size=256)

decoder_model = create_model()
encoder_model = create_encoder_model()
evaluate()
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='hoangcuong2011' date='2020-07-07T09:24:41Z'>
		&lt;denchmark-link:https://github.com/hoangcuong2011&gt;@hoangcuong2011&lt;/denchmark-link&gt;

I have tried in colab with TF version 2.2, nightly versions and i am seeing the error message).Please, find the gist &lt;denchmark-link:https://colab.research.google.com/gist/ravikyram/7e2356466b56bc3dcb896afcab13cb76/untitled90.ipynb&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='2' author='hoangcuong2011' date='2020-07-07T11:18:33Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
: Interesting. If you run it with TF 2.0, you got a different error - the one that I reported (see the gist &lt;denchmark-link:https://colab.research.google.com/drive/1LfiWeoYiWtGK8pjuG9xHOKfxdVDInPqp?usp=sharing&gt;here&lt;/denchmark-link&gt;
)
in any case this is very likely a bug. Any thoughts on this?
		</comment>
		<comment id='3' author='hoangcuong2011' date='2020-07-16T10:29:23Z'>
		I have a same problem, but runs well without BiLSTM:
&lt;denchmark-link:https://github.com/AlucardNosferatu/LostXmas/issues/2#issue-658065152&gt;AlucardNosferatu/LostXmas#2 (comment)&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='hoangcuong2011' date='2020-07-16T10:43:09Z'>
		
I have a same problem, but runs well without BiLSTM:
AlucardNosferatu/LostXmas#2 (comment)

Solved
		</comment>
		<comment id='5' author='hoangcuong2011' date='2020-07-24T00:01:08Z'>
		Re-assigning to someone who's more familiar with Keras tensors.
		</comment>
		<comment id='6' author='hoangcuong2011' date='2020-07-24T05:38:55Z'>
		Hi &lt;denchmark-link:https://github.com/hoangcuong2011&gt;@hoangcuong2011&lt;/denchmark-link&gt;

This unfortunately isn't the most helpful of error messages, but it is behaving as expected.
Keras symbolic tensors cannot be passed to a layer's constructor, it is only valid to pass them into a layer's call. Your first example passes the symbolic tensors to the constructor and tries to use them, raising the error message. Your second example makes sure to pass all symbolic tensors as arguments to the layer calls, so it works.
We can look into trying to provide a more meaningful error message for this in the future once an upcoming refactoring of the internals lands.
		</comment>
		<comment id='7' author='hoangcuong2011' date='2020-07-24T05:38:58Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41135&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41135&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='hoangcuong2011' date='2020-07-26T00:25:17Z'>
		&lt;denchmark-link:https://github.com/tomerk&gt;@tomerk&lt;/denchmark-link&gt;
 Thanks. I did not know that we should not pass Keras symbolic tensors to a layer's constructor? May I know why it is designed that way? (Why we should the tensor to a layer's call instead of constructor?)
		</comment>
		<comment id='9' author='hoangcuong2011' date='2020-07-31T22:07:31Z'>
		Hi &lt;denchmark-link:https://github.com/hoangcuong2011&gt;@hoangcuong2011&lt;/denchmark-link&gt;
 , it's this way because models contain layer objects that they reuse, rather than re-creating the layers each time you call the model. These layers may get called in different settings (individually, as part of the larger model, in a tf.function, totally eagerly, etc.
We can handle Functional model definition via layer calls because we define __call__ in Keras and have users just override call. So, we can track a variety of metadata under the hood. We use this metadata whenever you call the constructed model to get the model working in the above settings. (e.g. we keep track of what partially-created values need to be forwarded to what)
But, arbitrary layer constructors for custom layers don't give us the same ability to track model structure metadata. Arguably there's maybe stuff we could do with python's __new__ in certain circumstances, but it would be very unreliable and would make for a poor user experience. So, it's the most straightforward to disallow it altogether.
It's generally fairly easy to make sure a layer takes all tensor inputs directly in call as opposed to in the constructor.
		</comment>
		<comment id='10' author='hoangcuong2011' date='2020-07-31T22:21:27Z'>
		&lt;denchmark-link:https://github.com/tomerk&gt;@tomerk&lt;/denchmark-link&gt;
  Your answer is thoughtful and helpful to me. Thank you!
		</comment>
	</comments>
</bug>