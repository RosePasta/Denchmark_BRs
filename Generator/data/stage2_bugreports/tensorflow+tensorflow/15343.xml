<bug id='15343' author='agrinh' open_date='2017-12-13T18:15:07Z' closed_time='2017-12-14T19:52:25Z'>
	<summary>Iterator on cached tf.data Dataset cannot be reinitialized</summary>
	<description>
Found a likely bug when trying to use a reinitializable iterator to read from two cached datasets, one for validation and one for training. The iterator can however only be initialized once per cached dataset. Seems to me like the iterator should remove the lock file when being reinitialized, it is not in my case and that is why I get this issue. Here's a minimal example with only one cached dataset.
(basic system information below)
&lt;denchmark-h:h3&gt;Example&lt;/denchmark-h&gt;

import os

import numpy as np
import tensorflow as tf


data = np.random.rand(10, 3).astype(np.float32)
dataset = tf.data.Dataset.from_tensor_slices(data)
batches = dataset.shuffle(10).repeat().batch(5)

config = tf.ConfigProto(device_count = {'GPU': 0})
sess = tf.Session(config=config)

cache_dir = os.path.join(os.getcwd(), 'cache_dir')
try:
    os.makedirs(cache_dir)
except OSError:
    print('Cache directory already exists')

cached = batches.cache(os.path.join(cache_dir, 'cache'))
iterator = tf.data.Iterator.from_structure(output_types=tf.float32, output_shapes=(5, 3))
batch = iterator.get_next()

init1 = iterator.make_initializer(cached)
init2 = iterator.make_initializer(batches)

sess.run(init1)
sess.run(batch)

array([[ 0.11960778,  0.3081578 ,  0.96522039],
[ 0.90339011,  0.12458269,  0.30650312],
[ 0.58160347,  0.55877644,  0.50363588],
[ 0.2350398 ,  0.33509603,  0.4165386 ],
[ 0.76757395,  0.50134581,  0.93601096]], dtype=float32)

sess.run(init2)
sess.run(batch)

array([[ 0.76757395,  0.50134581,  0.93601096],
[ 0.2350398 ,  0.33509603,  0.4165386 ],
[ 0.90339011,  0.12458269,  0.30650312],
[ 0.13266359,  0.82675195,  0.26691398],
[ 0.58160347,  0.55877644,  0.50363588]], dtype=float32)

sess.run(init1)
sess.run(batch)

AlreadyExistsError (see above for traceback): There appears to be a concurrent caching iterator running - cache lockfile already exists ('/home/ubuntu/ai_notebooks/notebooks/projects/deep-purple/cache_dir/cache.lockfile'). If you are sure no other running TF computations are using this cache prefix, delete the lockfile and re-initialize the iterator. Lockfile contents: Created at: 1513187725
[[Node: IteratorGetNext = IteratorGetNextoutput_shapes=[[5,3]], output_types=[DT_FLOAT], _device="/job:localhost/replica:0/task:0/device:CPU:0"]]

&lt;denchmark-h:h3&gt;Sytem information&lt;/denchmark-h&gt;

Tensorflow version: v1.4.0-rc1-11-g130a514 1.4.0 (installed from pip)
Python version: 3.5.2
OS: Linux Ubuntu 16.04.3
CUDA: 8.0.61
cuDNN: 6
	</description>
	<comments>
		<comment id='1' author='agrinh' date='2017-12-13T18:41:41Z'>
		Brennan, can you please take a look?
		</comment>
		<comment id='2' author='agrinh' date='2017-12-13T18:58:54Z'>
		As a temporary fix I tried deleting the lockfile before reinitializing. When I do I keep getting new cache files after reinit. Here's an example from a cache directory with a larger dataset:
1199681441 Dec 13 19:41 cache.data-00000-of-00001.tempstate12238922573150074415
1199681441 Dec 13 19:48 cache.data-00000-of-00001.tempstate18257683081874428073
1199681441 Dec 13 19:45 cache.data-00000-of-00001.tempstate5944011296618694888
...
Double checked and all have the same content.
		</comment>
		<comment id='3' author='agrinh' date='2017-12-14T02:35:05Z'>
		Hey &lt;denchmark-link:https://github.com/agrinh&gt;@agrinh&lt;/denchmark-link&gt;

If you have not finished reading the underlying input data, then we do not release the lock. Can you read the entire underlying dataset before switching between the validation / training datasets?
All the best,
-Brennan
		</comment>
		<comment id='4' author='agrinh' date='2017-12-14T19:44:25Z'>
		&lt;denchmark-link:https://github.com/saeta&gt;@saeta&lt;/denchmark-link&gt;
 Thanks for helping out, that seemed to solve that particular problem.
I was applying the repeat() before caching (both in the example and in my real data loader), so I assume the cache releases the lock and produces the index file on tf.errors.OutOfRangeError. When I move the repeat to after the caching and make sure to run through all data this example works.
Perhaps this behaviour could be documented. Some interactions (e.g. repeat - cache, shuffle - cache etc.) are pretty non-obvious to me and it would save me a huge amount of time to get at least some of those details listed.
Thanks again,
Agrin
		</comment>
		<comment id='5' author='agrinh' date='2017-12-14T19:52:24Z'>
		&lt;denchmark-link:https://github.com/agrinh&gt;@agrinh&lt;/denchmark-link&gt;
 Funny you should ask. I and &lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
 have been working on a datasets performance guide we hope to publish soon, and we cover recommended order-of-operations. I'll make sure  vs  and  vs  are included. :-)
Nice sleuthing and getting it all working!!
		</comment>
		<comment id='6' author='agrinh' date='2017-12-15T09:18:28Z'>
		Awesome, looking forward to it, thanks again &lt;denchmark-link:https://github.com/saeta&gt;@saeta&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='agrinh' date='2018-01-16T14:05:58Z'>
		&lt;denchmark-link:https://github.com/saeta&gt;@saeta&lt;/denchmark-link&gt;
 Sorry for hijacking the thread, is there a release date for the new performance guide? and does it include FP16 performance recommendations?
		</comment>
		<comment id='8' author='agrinh' date='2018-01-16T16:14:18Z'>
		Although it's not yet complete, the new dataset performance guide is available at: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/performance/datasets_performance.md&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/performance/datasets_performance.md&lt;/denchmark-link&gt;
 It doesn't include FP16 performance recommendations as that usually occurs on the accelerator device (i.e. GPU/TPU), while the input pipelines run on the attached CPU. Hope this helps!
		</comment>
		<comment id='9' author='agrinh' date='2018-01-16T16:20:25Z'>
		Thanks &lt;denchmark-link:https://github.com/saeta&gt;@saeta&lt;/denchmark-link&gt;
!
		</comment>
		<comment id='10' author='agrinh' date='2018-04-03T12:05:32Z'>
		&lt;denchmark-link:https://github.com/sata&gt;@sata&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;
 Seems that  with cache doesn't work instead  works.
Docs tells that is the &lt;denchmark-link:https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/data/shuffle_and_repeat&gt;same call&lt;/denchmark-link&gt;
. Is there any different internal details that invalid the iterator of  combined with cache?
		</comment>
		<comment id='11' author='agrinh' date='2018-04-03T16:04:47Z'>
		&lt;denchmark-link:https://github.com/bhack&gt;@bhack&lt;/denchmark-link&gt;
 Please open a new issue with complete details of the program you're running and the expected and observed behavior.
		</comment>
		<comment id='12' author='agrinh' date='2018-04-05T10:20:39Z'>
		&lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;
 What is the correct sequence with cache?  and then ?
		</comment>
		<comment id='13' author='agrinh' date='2018-04-05T14:13:36Z'>
		&lt;denchmark-link:https://github.com/bhack&gt;@bhack&lt;/denchmark-link&gt;
 Please open a new issue with complete details of the program you're running and the expected and observed behavior.
		</comment>
		<comment id='14' author='agrinh' date='2018-04-05T14:28:53Z'>
		&lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;
 We will isolate the code and open an new issue but generally documenting a little bit more about the command sequence could in  could help.
		</comment>
		<comment id='15' author='agrinh' date='2018-04-05T16:02:39Z'>
		&lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;
 We start with &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/18266&gt;#18266&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>