<bug id='29542' author='srihari-humbarwadi' open_date='2019-06-07T18:12:51Z' closed_time='2019-06-25T17:42:21Z'>
	<summary>Loss of shape information when using dilation_rate != 1 in Conv layers</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/a
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.0.0-beta0
Python version: 3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 10.0, 7.5
GPU model and memory: 11gb, GTX1080Ti

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
I was using the code below with 1.12, 1.13.1 and tf2.0 alpha. But it fails to run in the latest tf2.0 beta. As you can see there is nothing fancy going on in the code. This is the block of code that produces this error
&lt;denchmark-code&gt;def ASPP(tensor):
    '''atrous spatial pyramid pooling'''
    dims = K.int_shape(tensor)

    y_pool = AveragePooling2D(pool_size=(
        dims[1], dims[2]), name='average_pooling')(tensor)
    y_pool = Conv2D(filters=256, kernel_size=1, padding='same',
                    kernel_initializer='he_normal', name='pool_1x1conv2d', use_bias=False)(y_pool)
    y_pool = BatchNormalization(name=f'bn_1')(y_pool)
    y_pool = Activation('relu', name=f'relu_1')(y_pool)

    y_pool = Upsample(tensor=y_pool, size=[dims[1], dims[2]])

    y_1 = Conv2D(filters=256, kernel_size=1, dilation_rate=1, padding='same',
                 kernel_initializer='he_normal', name='ASPP_conv2d_d1', use_bias=False)(tensor)
    y_1 = BatchNormalization(name=f'bn_2')(y_1)
    y_1 = Activation('relu', name=f'relu_2')(y_1)

    y_6 = Conv2D(filters=256, kernel_size=3, dilation_rate=6, padding='same',
                 kernel_initializer='he_normal', name='ASPP_conv2d_d6', use_bias=False)(tensor)
    y_6 = BatchNormalization(name=f'bn_3')(y_6)
    y_6 = Activation('relu', name=f'relu_3')(y_6)

    y_12 = Conv2D(filters=256, kernel_size=3, dilation_rate=12, padding='same',
                  kernel_initializer='he_normal', name='ASPP_conv2d_d12', use_bias=False)(tensor)
    y_12 = BatchNormalization(name=f'bn_4')(y_12)
    y_12 = Activation('relu', name=f'relu_4')(y_12)

    y_18 = Conv2D(filters=256, kernel_size=3, dilation_rate=18, padding='same',
                  kernel_initializer='he_normal', name='ASPP_conv2d_d18', use_bias=False)(tensor)
    y_18 = BatchNormalization(name=f'bn_5')(y_18)
    y_18 = Activation('relu', name=f'relu_5')(y_18)

    y = concatenate([y_pool, y_1, y_6, y_12, y_18], name='ASPP_concat')

    y = Conv2D(filters=256, kernel_size=1, dilation_rate=1, padding='same',
               kernel_initializer='he_normal', name='ASPP_conv2d_final', use_bias=False)(y)
    y = BatchNormalization(name=f'bn_final')(y)
    y = Activation('relu', name=f'relu_final')(y)
    return y
&lt;/denchmark-code&gt;

Strangely shapes of  y_pool, y_1 are correctly inferred , complete code to reproduce the issue available below
&lt;denchmark-code&gt;y = concatenate([y_pool, y_1, y_6, y_12, y_18], name='ASPP_concat')
&lt;/denchmark-code&gt;

Describe the expected behavior
The code should work as it did in the earlier release ie tf2.0 alpha
Code to reproduce the issue
&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow.keras import backend as K
from tensorflow.keras.models import Model
from tensorflow.keras.layers import AveragePooling2D, Lambda, Conv2D, Conv2DTranspose, Activation, Reshape, concatenate, Concatenate, BatchNormalization, ZeroPadding2D
from tensorflow.keras.applications.resnet50 import ResNet50


def Upsample(tensor, size):
    '''bilinear upsampling'''
    name = tensor.name.split('/')[0] + '_upsample'

    def bilinear_upsample(x, size):
        resized = tf.image.resize(
            images=x, size=size)
        return resized
    y = Lambda(lambda x: bilinear_upsample(x, size),
               output_shape=size, name=name)(tensor)
    return y


def ASPP(tensor):
    '''atrous spatial pyramid pooling'''
    dims = K.int_shape(tensor)

    y_pool = AveragePooling2D(pool_size=(
        dims[1], dims[2]), name='average_pooling')(tensor)
    y_pool = Conv2D(filters=256, kernel_size=1, padding='same',
                    kernel_initializer='he_normal', name='pool_1x1conv2d', use_bias=False)(y_pool)
    y_pool = BatchNormalization(name=f'bn_1')(y_pool)
    y_pool = Activation('relu', name=f'relu_1')(y_pool)

    y_pool = Upsample(tensor=y_pool, size=[dims[1], dims[2]])

    y_1 = Conv2D(filters=256, kernel_size=1, dilation_rate=1, padding='same',
                 kernel_initializer='he_normal', name='ASPP_conv2d_d1', use_bias=False)(tensor)
    y_1 = BatchNormalization(name=f'bn_2')(y_1)
    y_1 = Activation('relu', name=f'relu_2')(y_1)

    y_6 = Conv2D(filters=256, kernel_size=3, dilation_rate=6, padding='same',
                 kernel_initializer='he_normal', name='ASPP_conv2d_d6', use_bias=False)(tensor)
    y_6 = BatchNormalization(name=f'bn_3')(y_6)
    y_6 = Activation('relu', name=f'relu_3')(y_6)

    y_12 = Conv2D(filters=256, kernel_size=3, dilation_rate=12, padding='same',
                  kernel_initializer='he_normal', name='ASPP_conv2d_d12', use_bias=False)(tensor)
    y_12 = BatchNormalization(name=f'bn_4')(y_12)
    y_12 = Activation('relu', name=f'relu_4')(y_12)

    y_18 = Conv2D(filters=256, kernel_size=3, dilation_rate=18, padding='same',
                  kernel_initializer='he_normal', name='ASPP_conv2d_d18', use_bias=False)(tensor)
    y_18 = BatchNormalization(name=f'bn_5')(y_18)
    y_18 = Activation('relu', name=f'relu_5')(y_18)

    y = concatenate([y_pool, y_1, y_6, y_12, y_18], name='ASPP_concat')

    y = Conv2D(filters=256, kernel_size=1, dilation_rate=1, padding='same',
               kernel_initializer='he_normal', name='ASPP_conv2d_final', use_bias=False)(y)
    y = BatchNormalization(name=f'bn_final')(y)
    y = Activation('relu', name=f'relu_final')(y)
    return y


def DeepLabV3Plus(img_height, img_width):
    base_model = ResNet50(input_shape=(
        img_height, img_width, 3), weights='imagenet', include_top=False)

    image_features = base_model.get_layer('activation_39').output
    x_a = ASPP(image_features)
    x_a = Upsample(tensor=x_a, size=[img_height // 4, img_width // 4])

    x_b = base_model.get_layer('activation_9').output
    x_b = Conv2D(filters=48, kernel_size=1, padding='same',
                 kernel_initializer='he_normal', name='low_level_projection', use_bias=False)(x_b)
    x_b = BatchNormalization(name=f'bn_low_level_projection')(x_b)
    x_b = Activation('relu', name='low_level_activation')(x_b)

    x = concatenate([x_a, x_b], name='decoder_concat')

    x = Conv2D(filters=256, kernel_size=3, padding='same', activation='relu',
               kernel_initializer='he_normal', name='decoder_conv2d_1', use_bias=False)(x)
    x = BatchNormalization(name=f'bn_decoder_1')(x)
    x = Activation('relu', name='activation_decoder_1')(x)

    x = Conv2D(filters=256, kernel_size=3, padding='same', activation='relu',
               kernel_initializer='he_normal', name='decoder_conv2d_2', use_bias=False)(x)
    x = BatchNormalization(name=f'bn_decoder_2')(x)
    x = Activation('relu', name='activation_decoder_2')(x)
    x = Upsample(x, [img_height, img_width])

    x = Conv2D(1, (1, 1), name='output_layer')(x)
    x = Activation('sigmoid')(x)
    model = Model(inputs=base_model.input, outputs=x, name='DeepLabV3_Plus')
    return model
&lt;/denchmark-code&gt;

Other info / logs
&lt;denchmark-code&gt;ValueError                                Traceback (most recent call last)
&lt;ipython-input-20-0876af914f24&gt; in &lt;module&gt;()
----&gt; 1 DeepLabV3Plus(512, 512)

6 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/merge.py in build(self, input_shape)
    389                        'inputs with matching shapes '
    390                        'except for the concat axis. '
--&gt; 391                        'Got inputs shapes: %s' % (input_shape))
    392 
    393   def _merge_function(self, inputs):

ValueError: A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 32, 32, 256), (None, 32, 32, 256), (None, None, None, 256), (None, None, None, 256), (None, None, None, 256)]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='srihari-humbarwadi' date='2019-06-07T18:18:02Z'>
		&lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/dynamicwebpaige&gt;@dynamicwebpaige&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='srihari-humbarwadi' date='2019-06-07T18:21:19Z'>
		Interesting thing I found out was, this happens to all Conv2D layers with dilation_rate != 1
Explicitly setting shapes to the tensors seems to be a dirty workaround as of now
&lt;denchmark-code&gt;y_6.set_shape([None, 32, 32, 256])
y_6.shape
&lt;/denchmark-code&gt;

Output
&lt;denchmark-code&gt;TensorShape([None, 32, 32, 256])
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='srihari-humbarwadi' date='2019-06-08T04:57:18Z'>
		Did you use tf.function decorator?
		</comment>
		<comment id='4' author='srihari-humbarwadi' date='2019-06-08T05:40:59Z'>
		here is colab notebook showing the problem
&lt;denchmark-link:https://colab.research.google.com/drive/1UtkZRNyBBRJqDgDo3VMMP2otxSJXcv88&gt;https://colab.research.google.com/drive/1UtkZRNyBBRJqDgDo3VMMP2otxSJXcv88&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='srihari-humbarwadi' date='2019-06-08T05:41:18Z'>
		&lt;denchmark-link:https://github.com/llan-ml&gt;@llan-ml&lt;/denchmark-link&gt;
 no I did not use it
		</comment>
		<comment id='6' author='srihari-humbarwadi' date='2019-06-09T16:51:07Z'>
		&lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;

Problem in compute_output_shape or should the set_shape be inside call?
		</comment>
		<comment id='7' author='srihari-humbarwadi' date='2019-06-13T08:47:23Z'>
		Duplicate of issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/28400&gt;#28400&lt;/denchmark-link&gt;

The regression happened somewhere between tf-nightly-gpu-2.0-preview==2.0.0.dev20190410  and tf-nightly-gpu-2.0-preview==2.0.0.dev20190504.
It also affects 1.14 RC0 when Eager mode is enabled.
Dilations worked fine in 2.0-Alpha
		</comment>
		<comment id='8' author='srihari-humbarwadi' date='2019-06-23T09:06:13Z'>
		&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/29843&gt;#29843&lt;/denchmark-link&gt;
 has some debugging information.
		</comment>
		<comment id='9' author='srihari-humbarwadi' date='2019-06-23T09:06:43Z'>
		I will close the other two as duplicates.
		</comment>
		<comment id='10' author='srihari-humbarwadi' date='2019-06-23T19:37:01Z'>
		Perhaps related to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/26797&gt;#26797&lt;/denchmark-link&gt;
, where the shape in dilated conv is not just lost but wrong.
		</comment>
		<comment id='11' author='srihari-humbarwadi' date='2019-06-25T16:22:56Z'>
		are there some news/updates ?
		</comment>
		<comment id='12' author='srihari-humbarwadi' date='2019-06-25T17:00:48Z'>
		&lt;denchmark-link:https://github.com/k-w-w&gt;@k-w-w&lt;/denchmark-link&gt;
 is working on a fix.
		</comment>
		<comment id='13' author='srihari-humbarwadi' date='2019-06-25T17:42:20Z'>
		Thanks for finding this bug! The fix has been submitted (waiting for the changes to be copied in).
For now, to get around the shape issue, run the code below after the Conv layer has just been created/before calling the layer:
&lt;denchmark-code&gt;from tensorflow.python.keras import backend
with backend.get_graph().as_default():
  layer.build(tensor.shape) 
&lt;/denchmark-code&gt;

		</comment>
		<comment id='14' author='srihari-humbarwadi' date='2019-06-25T17:42:22Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=29542&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=29542&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='15' author='srihari-humbarwadi' date='2019-06-27T18:16:46Z'>
		how/where can i see your merge request to know when it gets merged and a nightly is usable?
		</comment>
		<comment id='16' author='srihari-humbarwadi' date='2019-06-28T18:36:48Z'>
		Here's the commit: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/0b4c19a3c1d1d3e7c016760aeea099fecb5fa544&gt;0b4c19a&lt;/denchmark-link&gt;

		</comment>
		<comment id='17' author='srihari-humbarwadi' date='2019-06-29T08:16:18Z'>
		thanks
		</comment>
	</comments>
</bug>