<bug id='44865' author='JiahaoYao' open_date='2020-11-14T01:29:24Z' closed_time='2020-12-01T19:27:23Z'>
	<summary>tensorflow gradient update does not match torch gradient update?</summary>
	<description>
import os
import sys

os.environ[
    "KMP_DUPLICATE_LIB_OK"
] = "True"  # uncomment this line if omp error occurs on OSX for python 3
os.environ["MKL_NUM_THREADS"] = "1"  # set number of MKL threads to run in parallel
os.environ["OMP_NUM_THREADS"] = "4"

import numpy as np
import tensorflow as tf
import torch
from tensorflow.keras import Sequential, datasets, layers, metrics, optimizers

# parameters
N, D_in, H, D_out = 40, 120, 1, 25
learning_rate = 0.1
opt = "adam"  # 'adam'
# opt = 'sgd'


# helper utilies
def detach(tensor):
    if isinstance(tensor, torch.Tensor):
        if tensor.device.type == "cuda":
            tensor = tensor.cpu()
        if tensor.requires_grad:
            tensor = tensor.detach()
        tensor = tensor.numpy()

    return tensor


# Create random Tensors to hold inputs and outputs
x = torch.randn(N, D_in)
y = torch.randn(N, D_out)

x_tf = tf.convert_to_tensor(detach(x))
y_tf = tf.convert_to_tensor(detach(y))

####### tensorflow
network = Sequential([layers.Dense(D_out, activation="relu")])
network.build(input_shape=(None, D_in))
network.summary()


if opt == "adam":
    optimizer = optimizers.Adam(lr=learning_rate)
elif opt == "sgd":
    optimizer = optimizers.SGD(lr=learning_rate)
else:
    raise NotImplementedError(opt + " not implemented.")


####### torch
# Use the nn package to define our model and loss function.
model = torch.nn.Sequential(
    torch.nn.Linear(D_in, D_out),
    torch.nn.ReLU(),
)
loss_fn = torch.nn.MSELoss(reduction="mean")

if opt == "adam":
    optimizer_th = torch.optim.Adam(model.parameters(), lr=learning_rate)
elif opt == "sgd":
    optimizer_th = torch.optim.SGD(model.parameters(), lr=learning_rate)
else:
    raise NotImplementedError(opt + " not implemented.")


params_torch = list(model.parameters())

weights = params_torch[0]
biases = params_torch[1]

network.layers[0].set_weights([detach(weights).T, detach(biases)])


for t in range(10):
    with tf.GradientTape() as tape:

        out = network(x_tf)
        loss = tf.square(out - y_tf)
        loss = tf.reduce_sum(loss) / N / D_out

    grads = tape.gradient(loss, network.trainable_variables)
    optimizer.apply_gradients(zip(grads, network.trainable_variables))

    # torch
    optimizer_th.zero_grad()
    # Forward pass: compute predicted y by passing x to the model.
    y_pred = model(x)

    # Compute and print loss.
    loss_th = loss_fn(y_pred, y)

    loss_th.backward()

    params_torch = list(model.parameters())

    weights = params_torch[0]
    biases = params_torch[1]

    weights_grad = weights.grad
    biases_grad = biases.grad
    optimizer_th.step()

    print(
        "check output diff:", np.linalg.norm(y_pred.detach().numpy() - np.asarray(out))
    )

    print(t, "loss: diff = {0:0.14f}.".format(loss_th.item() - np.asarray(loss)))

    print(
        "check weights/biases grads diff:",
        np.linalg.norm(weights_grad.detach().numpy() - np.asarray(grads[0]).T),
        np.linalg.norm(biases_grad.detach().numpy() - np.asarray(grads[1])),
    )

    print()
This is my comparison between torch and tensorflow but they don't match.
check output diff: 2.6182365e-06
0 loss: diff = 0.00000000000000.
check weights/biases grads diff: 1.1457902e-07 2.150125e-08

check output diff: 0.17770502
1 loss: diff = 0.00141370296478.
check weights/biases grads diff: 0.031174246 0.002777245

check output diff: 1.8508035
2 loss: diff = 0.00122940540314.
check weights/biases grads diff: 0.0800667 0.0038902021

check output diff: 1.8711921
3 loss: diff = -0.00223851203918.
check weights/biases grads diff: 0.05721609 0.0050597424

check output diff: 1.6562314
4 loss: diff = 0.00020694732666.
check weights/biases grads diff: 0.031922814 0.0016808284

check output diff: 1.6612757
5 loss: diff = -0.00040757656097.
check weights/biases grads diff: 0.03356902 0.0019494247

check output diff: 1.9143565
6 loss: diff = -0.00022947788239.
check weights/biases grads diff: 0.055081423 0.005770285

check output diff: 2.6084518
7 loss: diff = 0.00054198503494.
check weights/biases grads diff: 0.032326505 0.0040451903

check output diff: 2.745288
8 loss: diff = -0.00023376941681.
check weights/biases grads diff: 0.030059196 0.002589117

check output diff: 2.4303644
9 loss: diff = 0.00073283910751.
check weights/biases grads diff: 0.018728718 0.0017236237

	</description>
	<comments>
		<comment id='1' author='JiahaoYao' date='2020-11-17T06:08:08Z'>
		Was able to reproduce the issue with &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/3607cdfd623ed9443bdb7c20f7a10517/44865.ipynb&gt;TF v2.3&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/9563f297f291525dcaa9f9b18a5a3c34/44865-tf-nightly.ipynb&gt;TF-nightly&lt;/denchmark-link&gt;
. Please find the attached gist. Thanks!
		</comment>
		<comment id='2' author='JiahaoYao' date='2020-11-23T21:44:29Z'>
		&lt;denchmark-link:https://github.com/JiahaoYao&gt;@JiahaoYao&lt;/denchmark-link&gt;
 are you making sure the two models have identical initializations? They may receive different random values otherwise.
		</comment>
		<comment id='3' author='JiahaoYao' date='2020-11-23T22:24:42Z'>
		yes, i extracted the initialization from one to initialize the other and ensured that they started with the same parameters.
		</comment>
		<comment id='4' author='JiahaoYao' date='2020-11-24T14:41:09Z'>
		It seems that the two optimizers have different values for epsilon: keras.optimizers.Adam uses 1e-7, while torch.optim.Adam uses 1e-8. I'd also recommend using SGD which has fewer moving parts. I think that one shows smaller differences. I don't know if they're small enough to attribute to the order of computations, though, so it would be interesting to dig deeper there.
		</comment>
		<comment id='5' author='JiahaoYao' date='2020-11-24T16:33:46Z'>
		Oh i see, thanks &lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='JiahaoYao' date='2020-12-01T01:37:37Z'>
		Feel free to close this issue if its solved. Thanks!
		</comment>
		<comment id='7' author='JiahaoYao' date='2020-12-01T19:27:23Z'>
		Cool, thanks!
		</comment>
		<comment id='8' author='JiahaoYao' date='2020-12-01T19:27:24Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44865&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44865&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>