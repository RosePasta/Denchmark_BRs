<bug id='31187' author='DavidNorman' open_date='2019-07-31T06:20:35Z' closed_time='2019-12-20T18:06:20Z'>
	<summary>Failed to compile 'tensorflow/lite/experimental/ruy/pack_avx512.cc'</summary>
	<description>
System information

Have I written custom code - YES, but not in the failing part
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): master branch
Python version: 3.6
Bazel version (if compiling from source): 0.21
GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
CUDA/cuDNN version: N/A
GPU model and memory:N/A

&lt;denchmark-code&gt;tensorflow/lite/experimental/ruy/pack_avx512.cc: In function 'void ruy::{anonymous}::HalfPackFloatAvx512(const float*, const float*, int, int, int, float*, float*)':
tensorflow/lite/experimental/ruy/pack_avx512.cc:343:41: error: cannot convert '__m512 {aka __vector(16) float}' to '__m512i {aka __vector(8) long long int}' in assignment
         t0 = LoaduTwo(src_ptr0, src_ptr4);
                                         ^
tensorflow/lite/experimental/ruy/pack_avx512.cc:344:41: error: cannot convert '__m512 {aka __vector(16) float}' to '__m512i {aka __vector(8) long long int}' in assignment
         t1 = LoaduTwo(src_ptr1, src_ptr5);
                                         ^
tensorflow/lite/experimental/ruy/pack_avx512.cc:345:41: error: cannot convert '__m512 {aka __vector(16) float}' to '__m512i {aka __vector(8) long long int}' in assignment
         t2 = LoaduTwo(src_ptr2, src_ptr6);
                                         ^
tensorflow/lite/experimental/ruy/pack_avx512.cc:346:41: error: cannot convert '__m512 {aka __vector(16) float}' to '__m512i {aka __vector(8) long long int}' in assignment
         t3 = LoaduTwo(src_ptr3, src_ptr7);
                                         ^
tensorflow/lite/experimental/ruy/pack_avx512.cc:363:9: error: '_mm256_storeu_epi32' was not declared in this scope
         _mm256_storeu_epi32(packed_ptr + 0 * 16, _mm512_castsi512_si256(r0));
         ^~~~~~~~~~~~~~~~~~~
tensorflow/lite/experimental/ruy/pack_avx512.cc:363:9: note: suggested alternative: '_mm256_store_epi64'
         _mm256_storeu_epi32(packed_ptr + 0 * 16, _mm512_castsi512_si256(r0));
         ^~~~~~~~~~~~~~~~~~~
         _mm256_store_epi64
tensorflow/lite/experimental/ruy/pack_avx512.cc:382:55: error: cannot convert '__m512 {aka __vector(16) float}' to '__m512i {aka __vector(8) long long int}' in assignment
         t0 = MaskLoaduTwo(row_mask, src_ptr0, src_ptr4);
                                                       ^
tensorflow/lite/experimental/ruy/pack_avx512.cc:383:55: error: cannot convert '__m512 {aka __vector(16) float}' to '__m512i {aka __vector(8) long long int}' in assignment
         t1 = MaskLoaduTwo(row_mask, src_ptr1, src_ptr5);
                                                       ^
tensorflow/lite/experimental/ruy/pack_avx512.cc:384:55: error: cannot convert '__m512 {aka __vector(16) float}' to '__m512i {aka __vector(8) long long int}' in assignment
         t2 = MaskLoaduTwo(row_mask, src_ptr2, src_ptr6);
                                                       ^
tensorflow/lite/experimental/ruy/pack_avx512.cc:385:55: error: cannot convert '__m512 {aka __vector(16) float}' to '__m512i {aka __vector(8) long long int}' in assignment
         t3 = MaskLoaduTwo(row_mask, src_ptr3, src_ptr7);
                                                       ^
tensorflow/lite/experimental/ruy/pack_avx512.cc:402:9: error: '_mm256_storeu_epi32' was not declared in this scope
         _mm256_storeu_epi32(trailing_buf + 0 * 16, _mm512_castsi512_si256(r0));
         ^~~~~~~~~~~~~~~~~~~
tensorflow/lite/experimental/ruy/pack_avx512.cc:402:9: note: suggested alternative: '_mm256_store_epi64'
         _mm256_storeu_epi32(trailing_buf + 0 * 16, _mm512_castsi512_si256(r0));
         ^~~~~~~~~~~~~~~~~~~
         _mm256_store_epi64
tensorflow/lite/experimental/ruy/pack_avx512.cc: In function 'void ruy::Pack8bitAvx512(const int8_t*, int8_t, const int8_t*, int, int, int, int8_t*, int32_t*)':
tensorflow/lite/experimental/ruy/pack_avx512.cc:465:3: error: 'memset' was not declared in this scope
   memset(trailing_buf, 0, kTrailingBufSize * sizeof(std::int8_t));
   ^~~~~~
tensorflow/lite/experimental/ruy/pack_avx512.cc:465:3: note: suggested alternative: 'Offset'
   memset(trailing_buf, 0, kTrailingBufSize * sizeof(std::int8_t));
   ^~~~~~
   Offset
tensorflow/lite/experimental/ruy/pack_avx512.cc:500:5: error: 'memcpy' was not declared in this scope
     memcpy(packed_ptr + Layout::kCols * non_trailing_rows, trailing_buf,
     ^~~~~~
tensorflow/lite/experimental/ruy/pack_avx512.cc:500:5: note: suggested alternative: '_m_empty'
     memcpy(packed_ptr + Layout::kCols * non_trailing_rows, trailing_buf,
     ^~~~~~
     _m_empty
tensorflow/lite/experimental/ruy/pack_avx512.cc: In function 'void ruy::PackFloatAvx512(const float*, const float*, int, int, int, float*)':
tensorflow/lite/experimental/ruy/pack_avx512.cc:516:5: error: 'memset' was not declared in this scope
     memset(trailing_buf, 0, sizeof(trailing_buf));
     ^~~~~~
tensorflow/lite/experimental/ruy/pack_avx512.cc:516:5: note: suggested alternative: 'Offset'
     memset(trailing_buf, 0, sizeof(trailing_buf));
     ^~~~~~
     Offset
tensorflow/lite/experimental/ruy/pack_avx512.cc:524:5: error: 'memcpy' was not declared in this scope
     memcpy(packed_ptr + 16 * non_trailing_rows, trailing_buf,
     ^~~~~~
tensorflow/lite/experimental/ruy/pack_avx512.cc:524:5: note: suggested alternative: '_m_empty'
     memcpy(packed_ptr + 16 * non_trailing_rows, trailing_buf,
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='DavidNorman' date='2019-08-01T09:48:51Z'>
		&lt;denchmark-link:https://github.com/DavidNorman&gt;@DavidNorman&lt;/denchmark-link&gt;

In order to expedite the trouble-shooting process, please provide a full code snippet to reproduce the issue reported here. Thanks!
		</comment>
		<comment id='2' author='DavidNorman' date='2019-08-05T03:29:21Z'>
		the code is in the repo as it stands.  i expect that if you try to compile the code with the default options, using this compiler: gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0, then you will get the error.
		</comment>
		<comment id='3' author='DavidNorman' date='2019-08-05T05:00:17Z'>
		further info:  setting the tflite_with_ruy_explicit_false config variable does not disable compilation of the file tensorflow/lite/experimental/ruy/kernel_avx512.cc.
		</comment>
		<comment id='4' author='DavidNorman' date='2019-08-05T05:00:54Z'>
		more further info: we set the following arch flags : '-march=skylake'
		</comment>
		<comment id='5' author='DavidNorman' date='2019-08-06T16:12:45Z'>
		-&gt; &lt;denchmark-link:https://github.com/jalexstark&gt;@jalexstark&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='DavidNorman' date='2019-08-08T16:54:01Z'>
		I was having the same issue today compiling the v2 API on a skylake architecture system.
Is there, (or can there please soon be), a compile option for completely disabling tf lite when building the pip wheel. When building on the average desktop/laptop tf lite being there makes sense for completeness. But when building from source code for a high end workstation or cluster node this entire portion of tensorflow is never going to be used and only serves to break the build.
I have had similar issues with tf lite when building tensorflow on aarch64 platforms where again tf lite will never be used and is not needed.
If we need tf lite, then we'd take our model building code and pretrained weights and use a seperate script to convert on any machine, even a laptop. There is no need to have tf lite host capability on cluster nodes and embedded devices and should be able to be disabled during compilation.
Hoping for a quick fix to the main issue. :) Thanks
		</comment>
		<comment id='7' author='DavidNorman' date='2019-08-10T23:05:26Z'>
		I just tried to build the r2.0 branch and have exactly the same error.  I am running Ubuntu 18.04.  My architecture is skylake too (Intel 7820x CPU).  My GPU is an RTX 2080 Ti.  It's a desktop computer not a high end workstation.  I ran the following:
./configure
Accepted defaults initially
When asked for CUDA support, typed "y"
When asked for TensorRT support, typed "y"
When asked for compute capabilities, typed 7.5
Accepted defaults thereafter
bazel build --config=opt --config=cuda --config=v2 --cxxopt="-D_GLIBCXX_USE_CXX11_ABI=0" //tensorflow/tools/pip_package:build_pip_package
I get the exact same error.
Also hoping for a fix or the ability to exclude tf lite.  Would you even need tf lite on a desktop computer?
:) Also hoping for a fix. Thanks
		</comment>
		<comment id='8' author='DavidNorman' date='2019-08-11T03:54:30Z'>
		I can confirm that this error does not occur when building r2.0 branch on my laptop with a Coffeelake Intel  i7-9750H CPU.  The laptop is set up exactly the same as my Skylake desktop (which has a build error).  They are both Ubuntu 18.04, same options in ./configure, and same bazel build command (see my previous post immediately above for details).  I was able to pip install the .whl created on my laptop successfully on both my Coffeelake laptop and my Skylake desktop.
		</comment>
		<comment id='9' author='DavidNorman' date='2019-08-12T15:04:51Z'>
		Would you be able to post the compiler errors? I want to sort this out promptly. Lots of fixes have gone in to get this working with various compilers. It is mostly C++ fragility (transitive includes) and uneven type checking.
I apologize for the pain, and really appreciate your patience. I hope the longer-run code health, and the performance of the new GEMM kernels as we roll them out, will bring benefits.
		</comment>
		<comment id='10' author='DavidNorman' date='2019-08-13T11:39:42Z'>
		&lt;denchmark-link:https://github.com/jalexstark&gt;@jalexstark&lt;/denchmark-link&gt;

Hi Alex,
The attached files represent (1) bazel --verbose_explanations and (2) the entire cleaned capture of the output to the terminal when the following command's were issued:
`
TERM=dump
script capture.txt
./configure
bazel build --explain=verbose_explanations.txt --verbose_explanations --verbose_failures --subcommands=pretty_print --config=opt --config=cuda --config=v2 --cxxopt="-D_GLIBCXX_USE_CXX11_ABI=0" //tensorflow/tools/pip_package:build_pip_package
col -b &lt; capture.txt &gt; cleaned-capture.txt
exit
`
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3496750/verbose_explanations.txt&gt;verbose_explanations.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3496713/cleaned-capture.txt&gt;cleaned-capture.txt&lt;/denchmark-link&gt;

cleaned-capture.txt caused gedit on my machine to go black because it's a big file.  I was able to load it in to LibreOffice 6 and read it without difficulty.  It just takes about 30 seconds to load in to LibreOffice.
The build ended with the same error detailed on this page.  It was run on my Skylake Intel i7-7820x running Unbuntu 18.04 and bazel 0.26.0.  I accepted the defaults in ./configure except for selecting support for CUDA and TensorRT and specifying compute capability 7.5 (it has an NVIDIA RTX 2080 Ti).
I hope this helps.
Cheers,
Dan
		</comment>
		<comment id='11' author='DavidNorman' date='2019-08-14T20:05:00Z'>
		&lt;denchmark-link:https://github.com/jalexstark&gt;@jalexstark&lt;/denchmark-link&gt;

Hi,
Have you had a chance to look at the files in the previous post?  Was that the information you were looking for or do you need me to run some other commands and get error info?
All the best,
Dan
		</comment>
		<comment id='12' author='DavidNorman' date='2019-08-15T14:16:11Z'>
		Thanks for the details.
I will check to see if these can be fixed. They might have already.
Some of the errors are "incorrect" in that the statements do not match Intel documentation. That might just be weak C++ errors.
In the light of the inability of the computer engineering community to get even roughly consistent compilation together, which makes it very difficult to reproduce issues, I disabled AVX enhancements entirely outside of Clang+Linux. When the code is being developed less intensely, we will look into opening up to GCC and Apple.
Thanks again for taking the time to get the logs: they really do help, and confirm that we have at least fixed the basic bugs.
		</comment>
		<comment id='13' author='DavidNorman' date='2019-08-15T22:32:38Z'>
		I did a git pull to update to recent bug fixes.
The build still fails around //tensorflow/lite/experimental/ruy
I produced these output files for this new build using the same method as before:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3507388/cleaned-capture.txt&gt;cleaned-capture.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3507389/verbose_explanations.txt&gt;verbose_explanations.txt&lt;/denchmark-link&gt;

Cheers,
Dan
		</comment>
		<comment id='14' author='DavidNorman' date='2019-08-16T14:19:05Z'>
		Would you check which code you have?
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/ruy/pack_avx512.cc#L352&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/ruy/pack_avx512.cc#L352&lt;/denchmark-link&gt;

cleaned-capture.txt has
tensorflow/lite/experimental/ruy/pack_avx512.cc: In function ‘void ruy::{anonymous}::HalfPackFloatAvx512(const float*, const float*, int, int, int, float*, float*)’:
tensorflow/lite/experimental/ruy/pack_avx512.cc:352:42: error: cannot convert ‘__m512 {aka __vector(16) float}’ to ‘__m512i {aka __vector(8) long long int}’ for argument ‘1’ to ‘__m512i _mm512_unpacklo_epi32(__m512i, __m512i)’
r0 = _mm512_unpacklo_epi32(t0, t1);
(If my browser is showing me the correct file)
This issue should have been fixed.
		</comment>
		<comment id='15' author='DavidNorman' date='2019-08-17T01:20:46Z'>
		I had the same issue building   with  from {commit &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/65e6355ad9d74359a46827f87e76fa311ebf7714&gt;65e6355&lt;/denchmark-link&gt;
 (HEAD -&gt; r2.0, origin/r2.0)} inside the  Docker container ()
Latest r2.0 "commit &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/b0fee96b1dd893fe3fea6b746cf1965c8ca9f114&gt;b0fee96&lt;/denchmark-link&gt;
 (HEAD -&gt; r2.0, origin/r2.0)" does not work either.
I'll try master now.
OK, master does not have --config=v2
		</comment>
		<comment id='16' author='DavidNorman' date='2019-08-18T02:55:27Z'>
		Doing a simple 'git pull' was not enough to update "pack_avx512.cc".  I deleted and cloned the repository again then did:
git checkout r2.0
I get a different set of build errors:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3512651/cleaned-capture.txt&gt;cleaned-capture.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3512653/verbose_explanations.txt&gt;verbose_explanations.txt&lt;/denchmark-link&gt;

		</comment>
		<comment id='17' author='DavidNorman' date='2019-08-19T22:37:57Z'>
		Obviously I am flying almost blind here, and I appreciate the logs. Do feel free to chase down as best you can: don't feel you have to wait for suggestions.
So, I do see
&lt;denchmark-link:https://github.com/gcc-mirror/gcc/blob/master/gcc/config/i386/avx512bwintrin.h#L385&gt;https://github.com/gcc-mirror/gcc/blob/master/gcc/config/i386/avx512bwintrin.h#L385&lt;/denchmark-link&gt;

In one search, but I cannot tell where your headers files came from.
The above linked file does seem to suggest how a bug could have crept in. Notice how there are masked loadu_epi8 instructions, but not the as-it-were unmasked ones.  According to Intel (link provided earlier), _mm512_loadu_epi8 should be available for AVX512BW.
I would suggest grepping for loadu_epi8 in your include directories to hunt down the function prototype. The inclusion and guarding should match the support in
&lt;denchmark-link:https://software.intel.com/sites/landingpage/IntrinsicsGuide/#expand=3373,3373,2422,2452,2455,2186,103,2186,2204,87,4008,3534,2197,2192,2201,5008,2201,6098,245,3171,2458,2201,3505,5205,4550,94,1548,1383,3533,3533,3505,2984,3021,3263,3518,3956,3992,4029,6042,1383,429,1192,3395,3396&amp;avx512techs=AVX512F,AVX512BW,AVX512CD,AVX512DQ,AVX512VL&amp;techs=MMX,SSE,SSE2,SSE3,SSSE3,SSE4_1,SSE4_2,AVX,AVX2,FMA&amp;text=loadu_epi8&gt;https://software.intel.com/sites/landingpage/IntrinsicsGuide/#expand=3373,3373,2422,2452,2455,2186,103,2186,2204,87,4008,3534,2197,2192,2201,5008,2201,6098,245,3171,2458,2201,3505,5205,4550,94,1548,1383,3533,3533,3505,2984,3021,3263,3518,3956,3992,4029,6042,1383,429,1192,3395,3396&amp;avx512techs=AVX512F,AVX512BW,AVX512CD,AVX512DQ,AVX512VL&amp;techs=MMX,SSE,SSE2,SSE3,SSSE3,SSE4_1,SSE4_2,AVX,AVX2,FMA&amp;text=loadu_epi8&lt;/denchmark-link&gt;

		</comment>
		<comment id='18' author='DavidNorman' date='2019-08-20T03:36:15Z'>
		There are only masked loadu_epi8 instructions on my system:
grep "loadu_epi8" /usr/lib/gcc/x86_64-linux-gnu/7/include/avx512bwintrin.h
_mm512_mask_loadu_epi8 (__m512i __W, __mmask64 __U, void const *__P)
_mm512_maskz_loadu_epi8 (__mmask64 __U, void const *__P)
I did a grep of all include directories for gcc, clang 6 and clang 9.  Clang 9 includes an unmasked instruction.  Would I have to reconfigure my whole toolchain to use Clang 9 with bazel?  I've tried looking up how to do that and it seems beyond my skill set.  Or do I need to patch or update gcc?  Or can your code be updated to use masked instructions?
Cheers,
Dan
		</comment>
		<comment id='19' author='DavidNorman' date='2019-08-20T04:06:12Z'>
		The entire output of the grep commands on include directories for gcc, clang 9 and clang 6 in that order was:
(tfgpu) daniel@linuxcorsair:~/tensorflow$ grep -r "loadu_epi8" /usr/lib/gcc/x86_64-linux-gnu/7/include/
/usr/lib/gcc/x86_64-linux-gnu/7/include/avx512vlbwintrin.h:_mm256_mask_loadu_epi8 (__m256i __W, __mmask32 __U, void cons
t *__P)
/usr/lib/gcc/x86_64-linux-gnu/7/include/avx512vlbwintrin.h:_mm256_maskz_loadu_epi8 (__mmask32 __U, void const *__P)
/usr/lib/gcc/x86_64-linux-gnu/7/include/avx512vlbwintrin.h:_mm_mask_loadu_epi8 (__m128i __W, __mmask16 __U, void const *
__P)
/usr/lib/gcc/x86_64-linux-gnu/7/include/avx512vlbwintrin.h:_mm_maskz_loadu_epi8 (__mmask16 __U, void const *__P)
/usr/lib/gcc/x86_64-linux-gnu/7/include/avx512bwintrin.h:_mm512_mask_loadu_epi8 (__m512i __W, __mmask64 __U, void const
*__P)
/usr/lib/gcc/x86_64-linux-gnu/7/include/avx512bwintrin.h:_mm512_maskz_loadu_epi8 (__mmask64 __U, void const *__P)
(tfgpu) daniel@linuxcorsair:~/tensorflow$ grep -r "loadu_epi8" /usr/lib/llvm-9/lib/clang/9.0.0/include/
/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512vbmi2intrin.h:_mm512_mask_expandloadu_epi8(__m512i __S, __mmask64 __U, voi
d const *__P)
/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512vbmi2intrin.h:_mm512_maskz_expandloadu_epi8(__mmask64 __U, void const *__P
)
/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512vlvbmi2intrin.h:_mm_mask_expandloadu_epi8(__m128i __S, __mmask16 __U, void
const *__P)
/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512vlvbmi2intrin.h:_mm_maskz_expandloadu_epi8(__mmask16 __U, void const *__P)
/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512vlvbmi2intrin.h:_mm256_mask_expandloadu_epi8(__m256i __S, __mmask32 __U, v
oid const *__P)
/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512vlvbmi2intrin.h:_mm256_maskz_expandloadu_epi8(__mmask32 _U, void const *
_P)
/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512vlbwintrin.h:_mm_loadu_epi8 (void const __P)
/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512vlbwintrin.h:  struct __loadu_epi8 {
/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512vlbwintrin.h:  return ((struct __loadu_epi8)__P)-&gt;__v;
/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512vlbwintrin.h:_mm_mask_loadu_epi8 (__m128i __W, __mmask16 __U, void const *
__P)
/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512vlbwintrin.h:_mm_maskz_loadu_epi8 (__mmask16 __U, void const *__P)
/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512vlbwintrin.h:_mm256_loadu_epi8 (void const __P)
/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512vlbwintrin.h:  struct __loadu_epi8 {
/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512vlbwintrin.h:  return ((struct __loadu_epi8)__P)-&gt;__v;
/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512vlbwintrin.h:_mm256_mask_loadu_epi8 (__m256i __W, __mmask32 __U, void cons
t *__P)
/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512vlbwintrin.h:_mm256_maskz_loadu_epi8 (__mmask32 __U, void const *__P)
/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512bwintrin.h:_mm512_loadu_epi8 (void const __P)
/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512bwintrin.h:  struct __loadu_epi8 {
/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512bwintrin.h:  return ((struct __loadu_epi8)__P)-&gt;__v;
/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512bwintrin.h:_mm512_mask_loadu_epi8 (__m512i __W, __mmask64 __U, void const
*__P)
/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512bwintrin.h:_mm512_maskz_loadu_epi8 (__mmask64 __U, void const *__P)
(tfgpu) daniel@linuxcorsair:~/tensorflow$ grep -r "loadu_epi8" /usr/lib/llvm-6.0/lib/clang/6.0.0/include/
/usr/lib/llvm-6.0/lib/clang/6.0.0/include/avx512vbmi2intrin.h:_mm512_mask_expandloadu_epi8(__m512i __S, __mmask64 __U, v
oid const *__P)
/usr/lib/llvm-6.0/lib/clang/6.0.0/include/avx512vbmi2intrin.h:_mm512_maskz_expandloadu_epi8(__mmask64 _U, void const *
_P)
/usr/lib/llvm-6.0/lib/clang/6.0.0/include/avx512vlvbmi2intrin.h:_mm128_mask_expandloadu_epi8(__m128i __S, __mmask16 __U,
void const *__P)
/usr/lib/llvm-6.0/lib/clang/6.0.0/include/avx512vlvbmi2intrin.h:_mm128_maskz_expandloadu_epi8(__mmask16 __U, void const
*__P)
/usr/lib/llvm-6.0/lib/clang/6.0.0/include/avx512vlvbmi2intrin.h:_mm256_mask_expandloadu_epi8(__m256i __S, __mmask32 __U,
void const *__P)
/usr/lib/llvm-6.0/lib/clang/6.0.0/include/avx512vlvbmi2intrin.h:_mm256_maskz_expandloadu_epi8(__mmask32 __U, void const
*__P)
/usr/lib/llvm-6.0/lib/clang/6.0.0/include/avx512vlbwintrin.h:_mm_mask_loadu_epi8 (__m128i __W, __mmask16 __U, void const
*__P)
/usr/lib/llvm-6.0/lib/clang/6.0.0/include/avx512vlbwintrin.h:_mm_maskz_loadu_epi8 (__mmask16 __U, void const *__P)
/usr/lib/llvm-6.0/lib/clang/6.0.0/include/avx512vlbwintrin.h:_mm256_mask_loadu_epi8 (__m256i __W, __mmask32 __U, void co
nst *__P)
/usr/lib/llvm-6.0/lib/clang/6.0.0/include/avx512vlbwintrin.h:_mm256_maskz_loadu_epi8 (__mmask32 __U, void const *__P)
/usr/lib/llvm-6.0/lib/clang/6.0.0/include/avx512bwintrin.h:_mm512_mask_loadu_epi8 (__m512i __W, __mmask64 __U, void cons
t *__P)
/usr/lib/llvm-6.0/lib/clang/6.0.0/include/avx512bwintrin.h:_mm512_maskz_loadu_epi8 (__mmask64 __U, void const *__P)
I tried making replacing gcc with clang (sudo update-alternatives --install /usr/bin/gcc gcc /usr/lib/llvm-9/bin/clang 20) and also replacing g++ with clang++ and replacing ld with lld.  However that just causes the compiler to not recognize gcc options a few seconds in to the build.  I restored gcc, g++ and ld after that.  I gathered such a crude approach was insufficient to get bazel to properly use the clang toolchain.  Is there a docker image where the toolchain is set up to build with clang?
Should I try building in:
docker pull tensorflow/tensorflow:devel-gpu-py3
Cheers,
Dan
		</comment>
		<comment id='20' author='DavidNorman' date='2019-08-20T05:01:46Z'>
		I just tried building in the docker image tensorflow/tensorflow:devel-gpu-py3.
So the compiler setup should be standard.
I ran ./configure and chose not to use clang as the CUDA compiler.
bazel build --config=opt --config=cuda --config=v2 --cxxopt="-D_GLIBCXX_USE_CXX11_ABI=0" //tensorflow/tools/pip_package:build_pip_package
I got the same error message.
I then did "bazel clean"
I ran ./configure, and this time chose clang as CUDA compiler, electing to download a fresh release.
bazel build --config=opt --config=v2 --cxxopt="-D_GLIBCXX_USE_CXX11_ABI=0" //tensorflow/tools/pip_package:build_pip_package
Same error again.
All these trials are being done with the BIOS  set to factory defaults.  No overclocking or XMP.
		</comment>
		<comment id='21' author='DavidNorman' date='2019-08-21T18:19:48Z'>
		Various.
I did notice that one of the earlier logs showed an error in a line in (I think it was) kernel.h that no longer existed / exists. But I am assuming that the problems with the recent runs are with entirely fresh checkouts.
The current version can only compile the code with the problematic intrinsics if under clang.
The grep output shows that you have the illogical version of intrinsics for
/usr/lib/gcc/x86_64-linux-gnu/7/include/
/usr/lib/llvm-6.0/lib/clang/6.0.0/include/
but working version for
/usr/lib/llvm-9/lib/clang/9.0.0/include/
Please take a look at XXX/tensorflow/lite/experimental/ruy/platform.h
You can disable the problematic features at line 81 of
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/ruy/platform.h#L81&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/ruy/platform.h#L81&lt;/denchmark-link&gt;

If you do manage to figure out a version-based control of the logic at line 81, please let me know. Now that I have some idea of what is out in the wild, I might be able to take a stab, but the illogical intrinsics were, as you can see, propagated for a while before they were fixed.
"Illogical" is not meant to be critical or judgemental. Rather they are just internally nonsensical. The Intel chipsets added masked versions of the CPU instructions as superset enhancements. They are logical supersets, and so having them without the non-masked versions is illogical.
		</comment>
		<comment id='22' author='DavidNorman' date='2019-08-21T18:35:26Z'>
		Thanks again for the grepping, BTW. I will probably add a guard such as clang_major &gt;= 7
		</comment>
		<comment id='23' author='DavidNorman' date='2019-08-21T22:25:55Z'>
		I tried:
CC=/usr/lib/llvm-9/bin/clang CXX=/usr/lib/llvm-9/bin/clang++ bazel build --config=opt --define=using_clang=true --define=using_cuda_clang=true --config=v2 --cxxopt="-D_GLIBCXX_USE_CXX11_ABI=0" //tensorflow/tools/pip_package:build_pip_package
This built for longer and seemed to get past the error we're talking about here.  However, due to incompatibilities with clang and gcc (I think) it eventually failed to build with an unrelated error.
You can get a successful build if you use comments in:
~/tensorflow/tensorflow/lite/experimental/ruy/platform.h
Comment out:
`
// TODO(b/138433137) Select AVX-512 at runtime rather than via compile options.
// #if defined(AVX512F) &amp;&amp; defined(AVX512DQ) &amp;&amp; defined(AVX512CD) &amp;&amp; 
defined(AVX512BW) &amp;&amp; defined(AVX512VL)
// #define RUY_DONOTUSEDIRECTLY_AVX512 1
// #else
#define RUY_DONOTUSEDIRECTLY_AVX512 0
// #endif~/tensorflow/tensorflow/lite/experimental/ruy/platform.h
`
i.e. Always: #define RUY_DONOTUSEDIRECTLY_AVX512 0
then the build completes successfully.
I'm no C developer.  I fear that tensorflow's kernel_avx512.cc is trying to accomplish with AVX-512 on a Skylake is not possible with the compatible gcc (version 7).  This incompatibility (support for unmasked instructions) does not appear to have been updated on the latest source of gcc.  I don't think it is easy to compile all of Tensorflow with clang-9 as it is incompatible in other ways out of the box.  My limited knowledge suggests the only 3 solutions are:

Re-writing gcc intrinsics support.  I submitting patches to the gcc git.
Giving up on AVX-512 (#define RUY_DONOTUSEDIRECTLY_AVX512 0).
Making a flag that you include in "bazel build" that would make the whole build compatible with clang-9.  This would require re-writing areas of the code where clang-9 causes errors because it is not a straight-out replacement for gcc.

		</comment>
		<comment id='24' author='DavidNorman' date='2019-08-21T23:46:51Z'>
		When I restore platform.h to its original state and run:
CC=/usr/lib/llvm-9/bin/clang CXX=/usr/lib/llvm-9/bin/clang++ bazel build --config=opt --config=cuda --config=v2 --cxxopt="-D_GLIBCXX_USE_CXX11_ABI=0" //tensorflow/tools/pip_package:build_pip_package
I get the same build error we're talking about (in tensorflow/lite/experimental/ruy/pack_avx512.cc).
		</comment>
		<comment id='25' author='DavidNorman' date='2019-08-22T14:19:43Z'>
		Recent versions of platform.h have the following lines:
#if RUY_PLATFORM(X86_ENHANCEMENTS) &amp;&amp; RUY_PLATFORM(X86) &amp;&amp;                    
defined(AVX512F) &amp;&amp; defined(AVX512DQ) &amp;&amp; defined(AVX512CD) &amp;&amp; 
defined(AVX512BW) &amp;&amp; defined(AVX512VL)
#define RUY_DONOTUSEDIRECTLY_AVX512 1
#else
#define RUY_DONOTUSEDIRECTLY_AVX512 0
#endif
Your file looks different. Would you check out the source / confirm which version that you are using?
		</comment>
		<comment id='26' author='DavidNorman' date='2019-08-22T20:38:22Z'>
		Yes, I did a git clone and tried out building 25 hours ago.  Then I was using a different version of platform.h to the one available now.  I just now did a new fresh git clone and have the new verion of platform.h.
I still get the same error at _mm512_storeu_epi32.  I did not use clang as cuda compiler in ./configure.  I built using:
bazel build --config=opt --config=cuda --config=v2 --cxxopt="-D_GLIBCXX_USE_CXX11_ABI=0" //tensorflow/tools/pip_package:build_pip_package
Cheers,
Daniel
		</comment>
		<comment id='27' author='DavidNorman' date='2019-08-22T21:05:53Z'>
		Just a moment - now at _mm512_storeu_epi32()?
That is more different that it might at first appear. Which line?
I think that cuda means clang. There is a recent change (may not have propagated yet) that disables for Clang &lt; version 8.
Nonetheless, it would be useful to know the exact error. I have a way of working around the other mess, but that might not help for epi32. There used to be an actual bug relating to the types in the call to that function, but it was fixed a while back. So it would be good to know if something else is not working together.
		</comment>
		<comment id='28' author='DavidNorman' date='2019-08-22T23:19:29Z'>
		Sorry I pasted the end of the traceback.  The start of the error message
incluedes: "‘_mm512_loadu_epi8’ was not declared in this scope" early on,
which is the same error:  The whole error message is:

ERROR:
/home/daniel/tensorflow/tensorflow/lite/experimental/ruy/BUILD:271:1: C++
compilation of rule '//tensorflow/lite/experimental/ruy:kernel' failed
(Exit 1)
In file included from
tensorflow/lite/experimental/ruy/kernel_avx512.cc:18:0:
./tensorflow/lite/experimental/ruy/kernel.h:613:9: warning: multi-line
comment [-Wcomment]
 #endif  // (RUY_PLATFORM(NEON_64) || RUY_PLATFORM(NEON_32) || \
         ^
In file included from external/gemmlowp/fixedpoint/fixedpoint.h:895:0,
                 from ./tensorflow/lite/experimental/ruy/kernel.h:22,
                 from tensorflow/lite/experimental/ruy/kernel_avx512.cc:18:
external/gemmlowp/fixedpoint/./fixedpoint_sse.h:43:39: warning: ignoring
attributes on template argument ‘__m128i {aka __vector(2) long long int}’
[-Wignored-attributes]
 struct FixedPointRawTypeTraits&lt;__m128i&gt; {
                                       ^
In file included from
tensorflow/lite/experimental/ruy/kernel_avx512.cc:18:0:
./tensorflow/lite/experimental/ruy/kernel.h: In function ‘void
ruy::MakeKernelParamsFloat(const ruy::PackedMatrix&lt;float&gt;&amp;, const
ruy::PackedMatrix&lt;float&gt;&amp;, const ruy::BasicSpec&lt;float, float&gt;&amp;, int, int,
int, int, ruy::Matrix&lt;float&gt;*, ruy::KernelParamsFloat&lt;LhsCols, RhsCols&gt;*)’:
./tensorflow/lite/experimental/ruy/kernel.h:456:53: warning: typedef ‘using
Params = struct ruy::KernelParamsFloat&lt;LhsCols, RhsCols&gt;’ locally defined
but not used [-Wunused-local-typedefs]
   using Params = KernelParamsFloat&lt;LhsCols, RhsCols&gt;;
                                                     ^
tensorflow/lite/experimental/ruy/kernel_avx512.cc: In function ‘void
ruy::Kernel8bitAvx512(const ruy::KernelParams8bit&lt;16, 16&gt;&amp;)’:
tensorflow/lite/experimental/ruy/kernel_avx512.cc:111:34: error:
‘_mm512_loadu_epi8’ was not declared in this scope
         const __m512i lhs_data = _mm512_loadu_epi8(lhs_ptr);
                                  ^~~~~~~~~~~~~~~~~
tensorflow/lite/experimental/ruy/kernel_avx512.cc:111:34: note: suggested
alternative: ‘_mm512_add_epi8’
         const __m512i lhs_data = _mm512_loadu_epi8(lhs_ptr);
                                  ^~~~~~~~~~~~~~~~~
                                  _mm512_add_epi8
tensorflow/lite/experimental/ruy/kernel_avx512.cc:161:32: error:
‘_mm512_loadu_epi32’ was not declared in this scope
                                _mm512_loadu_epi32(&amp;params.lhs_sums[row]));
                                ^~~~~~~~~~~~~~~~~~
tensorflow/lite/experimental/ruy/kernel_avx512.cc:161:32: note: suggested
alternative: ‘_mm512_load_epi32’
                                _mm512_loadu_epi32(&amp;params.lhs_sums[row]));
                                ^~~~~~~~~~~~~~~~~~
                                _mm512_load_epi32
tensorflow/lite/experimental/ruy/kernel_avx512.cc:170:32: error:
‘_mm512_loadu_epi32’ was not declared in this scope
                                _mm512_loadu_epi32(&amp;params.rhs_sums[col]));
                                ^~~~~~~~~~~~~~~~~~
tensorflow/lite/experimental/ruy/kernel_avx512.cc:170:32: note: suggested
alternative: ‘_mm512_load_epi32’
                                _mm512_loadu_epi32(&amp;params.rhs_sums[col]));
                                ^~~~~~~~~~~~~~~~~~
                                _mm512_load_epi32
tensorflow/lite/experimental/ruy/kernel_avx512.cc:277:13: error:
‘_mm_storeu_epi8’ was not declared in this scope
             _mm_storeu_epi8(tmp_ptr,
_mm512_cvtepi32_epi8(accum_data_v[j]));
             ^~~~~~~~~~~~~~~
tensorflow/lite/experimental/ruy/kernel_avx512.cc:277:13: note: suggested
alternative: ‘_mm_store_epi64’
             _mm_storeu_epi8(tmp_ptr,
_mm512_cvtepi32_epi8(accum_data_v[j]));
             ^~~~~~~~~~~~~~~
             _mm_store_epi64
tensorflow/lite/experimental/ruy/kernel_avx512.cc:293:13: error:
‘_mm_storeu_epi8’ was not declared in this scope
             _mm_storeu_epi8(tmp_ptr,
_mm512_cvtepi32_epi8(accum_data_v[j]));
             ^~~~~~~~~~~~~~~
tensorflow/lite/experimental/ruy/kernel_avx512.cc:293:13: note: suggested
alternative: ‘_mm_store_epi64’
             _mm_storeu_epi8(tmp_ptr,
_mm512_cvtepi32_epi8(accum_data_v[j]));
             ^~~~~~~~~~~~~~~
             _mm_store_epi64
tensorflow/lite/experimental/ruy/kernel_avx512.cc:309:13: error:
‘_mm256_storeu_epi16’ was not declared in this scope
             _mm256_storeu_epi16(tmp_ptr,
             ^~~~~~~~~~~~~~~~~~~
tensorflow/lite/experimental/ruy/kernel_avx512.cc:309:13: note: suggested
alternative: ‘_mm256_store_epi64’
             _mm256_storeu_epi16(tmp_ptr,
             ^~~~~~~~~~~~~~~~~~~
             _mm256_store_epi64
tensorflow/lite/experimental/ruy/kernel_avx512.cc:326:13: error:
‘_mm512_storeu_epi32’ was not declared in this scope
             _mm512_storeu_epi32(tmp_ptr, accum_data_v[j]);
             ^~~~~~~~~~~~~~~~~~~
tensorflow/lite/experimental/ruy/kernel_avx512.cc:326:13: note: suggested
alternative: ‘_mm512_store_epi32’
             _mm512_storeu_epi32(tmp_ptr, accum_data_v[j]);
             ^~~~~~~~~~~~~~~~~~~
             _mm512_store_epi32
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Fri, 23 Aug 2019 at 07:13, Alex Stark ***@***.***&gt; wrote:
 Just a moment - now at _mm512_storeu_epi32()?

 That is more different that it might at first appear. Which line?

 I think that cuda means clang. There is a recent change (may not have
 propagated yet) that disables for Clang &lt; version 8.

 Nonetheless, it would be useful to know the exact error. I have a way of
 working around the other mess, but that might not help for epi32. There
 *used* to be an actual bug relating to the types in the call to that
 function, but it was fixed a while back. So it would be good to know if
 something else is not working together.

 —
 You are receiving this because you commented.
 Reply to this email directly, view it on GitHub
 &lt;#31187?email_source=notifications&amp;email_token=AAB26QRVHWE2ITYP4SVJ773QF36OVA5CNFSM4IIDGWLKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD46MTLQ#issuecomment-524077486&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AAB26QRPKC4LCGQ63LNKVLDQF36OVANCNFSM4IIDGWLA&gt;
 .



		</comment>
		<comment id='29' author='DavidNorman' date='2019-08-23T17:16:23Z'>
		I'm still not understanding.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/ruy/kernel.h&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/ruy/kernel.h&lt;/denchmark-link&gt;

does not have a line 613, so how can there be this error?
ERROR:
/home/daniel/tensorflow/tensorflow/lite/experimental/ruy/BUILD:271:1: C++
compilation of rule '//tensorflow/lite/experimental/ruy:kernel' failed
(Exit 1)
In file included from
tensorflow/lite/experimental/ruy/kernel_avx512.cc:18:0:
./tensorflow/lite/experimental/ruy/kernel.h:613:9: warning: multi-line
comment [-Wcomment]
#endif  // (RUY_PLATFORM(NEON_64) || RUY_PLATFORM(NEON_32) || 
^
		</comment>
		<comment id='30' author='DavidNorman' date='2019-08-24T05:07:37Z'>
		I just now did:
git clone &lt;denchmark-link:https://github.com/tensorflow/tensorflow.git&gt;https://github.com/tensorflow/tensorflow.git&lt;/denchmark-link&gt;

git checkout r2.0
tensorflow/tensorflow/lite/experimental/ruy/kernel.h
has a line 613:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3536878/kernel.h.txt&gt;kernel.h.txt&lt;/denchmark-link&gt;

I am on r2.0 not master:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/lite/experimental/ruy/kernel.h&gt;https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/lite/experimental/ruy/kernel.h&lt;/denchmark-link&gt;

		</comment>
		<comment id='31' author='DavidNorman' date='2019-08-24T19:02:13Z'>
		Master branch builds successfully.  Maybe you guys just need to use the code in master for /tensorflow/lite/experimental/ruy/kernel.h in r2.0.
		</comment>
		<comment id='32' author='DavidNorman' date='2019-08-24T21:01:12Z'>
		&lt;denchmark-link:https://github.com/dbonner&gt;@dbonner&lt;/denchmark-link&gt;
 glad to here compilation is possible, have you looked at the diff between branches for that file?
The work around I used to disable tf lite from building was to comment out the following lines in the pip package BUILD file (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/tools/pip_package/BUILD#L64-L66&gt;https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/tools/pip_package/BUILD#L64-L66&lt;/denchmark-link&gt;
) which lets branch r2.0 build successfully on Skylake at the loss of tf lite functionality which I did not need.
		</comment>
		<comment id='33' author='DavidNorman' date='2019-08-26T12:46:47Z'>
		&lt;denchmark-link:https://github.com/JossWhittle&gt;@JossWhittle&lt;/denchmark-link&gt;
 The files between the 2 branches are completely different.  One is about 25Kb (r2.0) and the other is only 2-3Kb (master) from memory.
Yes I have a similar workaround (commenting out AVX512 support for tf lite but not disabling TF lite entirely) to get r2.0 to build.
&lt;denchmark-link:https://github.com/jalexstark&gt;@jalexstark&lt;/denchmark-link&gt;
 Hi, hadn't heard from you in a bit.  I'm sorry I got you thinking I was on the master branch.  My problem has always been r2.0 since that's what I'm interested in learning to code in.  All those files I dumped were for building r2.0.  I just tried again and it is still failing.  Are you able to help with the skylake building for r2.0?  I don't know if you can just port the way they have fixed this issue from master to r2.0?  Or should I start a new thread for the r2.0 issue since this thread was started by someone who was building master?
The error right now on r2.0  is (I think it's still the same):
ERROR: /home/daniel/tensorflow/tensorflow/lite/experimental/ruy/BUILD:271:1: C++ compilation of rule '//tensorflow/lite/experimental/ruy:kernel' failed (Exit 1)
In file included from tensorflow/lite/experimental/ruy/kernel_avx512.cc:18:0:
./tensorflow/lite/experimental/ruy/kernel.h:613:9: warning: multi-line comment [-Wcomment]
#endif  // (RUY_PLATFORM(NEON_64) || RUY_PLATFORM(NEON_32) || 
^
In file included from external/gemmlowp/fixedpoint/fixedpoint.h:895:0,
from ./tensorflow/lite/experimental/ruy/kernel.h:22,
from tensorflow/lite/experimental/ruy/kernel_avx512.cc:18:
external/gemmlowp/fixedpoint/./fixedpoint_sse.h:43:39: warning: ignoring attributes on template argument ‘__m128i {aka __vector(2) long long int}’ [-Wignored-attributes]
struct FixedPointRawTypeTraits&lt;__m128i&gt; {
^
In file included from tensorflow/lite/experimental/ruy/kernel_avx512.cc:18:0:
./tensorflow/lite/experimental/ruy/kernel.h: In function ‘void ruy::MakeKernelParamsFloat(const ruy::PackedMatrix&amp;, const ruy::PackedMatrix&amp;, const ruy::BasicSpec&lt;float, float&gt;&amp;, int, int, int, int, ruy::Matrix, ruy::KernelParamsFloat&lt;LhsCols, RhsCols&gt;)’:
./tensorflow/lite/experimental/ruy/kernel.h:456:53: warning: typedef ‘using Params = struct ruy::KernelParamsFloat&lt;LhsCols, RhsCols&gt;’ locally defined but not used [-Wunused-local-typedefs]
using Params = KernelParamsFloat&lt;LhsCols, RhsCols&gt;;
^
tensorflow/lite/experimental/ruy/kernel_avx512.cc: In function ‘void ruy::Kernel8bitAvx512(const ruy::KernelParams8bit&lt;16, 16&gt;&amp;)’:
tensorflow/lite/experimental/ruy/kernel_avx512.cc:111:34: error: ‘_mm512_loadu_epi8’ was not declared in this scope
const __m512i lhs_data = _mm512_loadu_epi8(lhs_ptr);
^~~~~~~~~~~~~~~~~
tensorflow/lite/experimental/ruy/kernel_avx512.cc:111:34: note: suggested alternative: ‘_mm512_add_epi8’
const __m512i lhs_data = _mm512_loadu_epi8(lhs_ptr);
^~~~~~~~~~~~~~~~~
_mm512_add_epi8
tensorflow/lite/experimental/ruy/kernel_avx512.cc:161:32: error: ‘_mm512_loadu_epi32’ was not declared in this scope
_mm512_loadu_epi32(&amp;params.lhs_sums[row]));
^~~~~~~~~~~~~~~~~~
tensorflow/lite/experimental/ruy/kernel_avx512.cc:161:32: note: suggested alternative: ‘_mm512_load_epi32’
_mm512_loadu_epi32(&amp;params.lhs_sums[row]));
^~~~~~~~~~~~~~~~~~
_mm512_load_epi32
tensorflow/lite/experimental/ruy/kernel_avx512.cc:170:32: error: ‘_mm512_loadu_epi32’ was not declared in this scope
_mm512_loadu_epi32(&amp;params.rhs_sums[col]));
^~~~~~~~~~~~~~~~~~
tensorflow/lite/experimental/ruy/kernel_avx512.cc:170:32: note: suggested alternative: ‘_mm512_load_epi32’
_mm512_loadu_epi32(&amp;params.rhs_sums[col]));
^~~~~~~~~~~~~~~~~~
_mm512_load_epi32
tensorflow/lite/experimental/ruy/kernel_avx512.cc:277:13: error: ‘_mm_storeu_epi8’ was not declared in this scope
_mm_storeu_epi8(tmp_ptr, _mm512_cvtepi32_epi8(accum_data_v[j]));
^~~~~~~~~~~~~~~
tensorflow/lite/experimental/ruy/kernel_avx512.cc:277:13: note: suggested alternative: ‘_mm_store_epi64’
_mm_storeu_epi8(tmp_ptr, _mm512_cvtepi32_epi8(accum_data_v[j]));
^~~~~~~~~~~~~~~
_mm_store_epi64
tensorflow/lite/experimental/ruy/kernel_avx512.cc:293:13: error: ‘_mm_storeu_epi8’ was not declared in this scope
_mm_storeu_epi8(tmp_ptr, _mm512_cvtepi32_epi8(accum_data_v[j]));
^~~~~~~~~~~~~~~
tensorflow/lite/experimental/ruy/kernel_avx512.cc:293:13: note: suggested alternative: ‘_mm_store_epi64’
_mm_storeu_epi8(tmp_ptr, _mm512_cvtepi32_epi8(accum_data_v[j]));
^~~~~~~~~~~~~~~
_mm_store_epi64
tensorflow/lite/experimental/ruy/kernel_avx512.cc:309:13: error: ‘_mm256_storeu_epi16’ was not declared in this scope
_mm256_storeu_epi16(tmp_ptr,
^~~~~~~~~~~~~~~~~~~
tensorflow/lite/experimental/ruy/kernel_avx512.cc:309:13: note: suggested alternative: ‘_mm256_store_epi64’
_mm256_storeu_epi16(tmp_ptr,
^~~~~~~~~~~~~~~~~~~
_mm256_store_epi64
tensorflow/lite/experimental/ruy/kernel_avx512.cc:326:13: error: ‘_mm512_storeu_epi32’ was not declared in this scope
_mm512_storeu_epi32(tmp_ptr, accum_data_v[j]);
^~~~~~~~~~~~~~~~~~~
tensorflow/lite/experimental/ruy/kernel_avx512.cc:326:13: note: suggested alternative: ‘_mm512_store_epi32’
_mm512_storeu_epi32(tmp_ptr, accum_data_v[j]);
^~~~~~~~~~~~~~~~~~~
_mm512_store_epi32
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 94.519s, Critical Path: 13.68s
INFO: 1041 processes: 1041 local.
FAILED: Build did NOT complete successfully
		</comment>
		<comment id='34' author='DavidNorman' date='2019-08-27T09:22:49Z'>
		&lt;denchmark-link:https://github.com/jalexstark&gt;@jalexstark&lt;/denchmark-link&gt;
 Hi, please indicate whether I should start a new post to get Skylake building fixed for r2.0?  Many thanks, Daniel.
		</comment>
		<comment id='35' author='DavidNorman' date='2019-08-27T13:46:33Z'>
		I would suggest going ahead, making clear that it is a case of updating r2.0 with existing fixes, so that it doesn't get triaged as a bug needing a development fix.
Thanks.
		</comment>
		<comment id='36' author='DavidNorman' date='2019-08-27T20:35:03Z'>
		OK thanks, I have submitted the issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/32026&gt;#32026&lt;/denchmark-link&gt;

		</comment>
		<comment id='37' author='DavidNorman' date='2019-09-11T17:28:30Z'>
		Vanilla code from r2.0 branch still doesn't work for me on Skylake.

i.e. Always: #define RUY_DONOTUSEDIRECTLY_AVX512 0

But this workaround helps.
		</comment>
		<comment id='38' author='DavidNorman' date='2019-12-20T18:06:20Z'>
		Should be fixed in r2.1.
		</comment>
		<comment id='39' author='DavidNorman' date='2019-12-20T18:06:22Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31187&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31187&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>