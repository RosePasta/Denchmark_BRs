<bug id='26132' author='Keepmoving-ZXY' open_date='2019-02-26T11:28:23Z' closed_time='2019-03-12T21:25:33Z'>
	<summary>Tensorflow hang when specify 'nccl/xring' as all reduce alg</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No
TensorFlow installed from (source or binary):binary
TensorFlow version (use command below):1.13.0-rc0
Python version:2.7.12
Bazel version (if compiling from source):0.20.0
GCC/Compiler version (if compiling from source):5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.11)
CUDA/cuDNN version:CUDA version:9.0.176 and cuDNN version:7.2.1.38
GPU model and memory:Tesla V100 , 32480MiB

Describe the current behavior
I train NeuMF model in distribute environment, and I find that TensorFlow hang all the time when training finish.
Below is some detail about distributed training:
Model : NeuMF
Dataset : ml-1m
Num_worker : 2
Num_gpu_each_worker: 4
Strategy : tf.contrib.distribute.MirrorStrategy
Reduce_alg : replace pscpu/pscpu with nccl/xring
Nccl version : 2.3.7-1+cuda9.0
The distributed training run 120 steps then tensorflow quit when use default all reduce alg pscpu/pscpu of MirrorStrategy. And when replace default all reduce alg with nccl/xring , distributed training run 120 steps, but TensorFlow hang all the time. Below is screen output of two conditions:
log when normal exit:
&lt;denchmark-code&gt;I0226 10:04:21.481378 140388122244864 basic_session_run_hooks.py:247] loss = 0.3570031, step = 120 (1.436 sec)
I0226 10:04:21.483242 140388122244864 basic_session_run_hooks.py:680] global_step/sec: 6.96639
I0226 10:04:22.065148 140388122244864 util.py:168] Finalize strategy.
I0226 10:04:22.066411 140388122244864 basic_session_run_hooks.py:594] Saving checkpoints for 125 into /tmp/ncf_model/model.ckpt.
I0226 10:04:22.903732 140388122244864 estimator.py:359] Loss for final step: 0.35619634.
I0226 10:04:22.906255 140388122244864 data_preprocessing.py:391] Shutting down train data creation subprocess.
&lt;/denchmark-code&gt;

log when hang:
&lt;denchmark-code&gt;I0226 10:13:19.793405 140644590810880 basic_session_run_hooks.py:247] loss = 0.360583, step = 100 (1.755 sec)
I0226 10:13:19.795331 140644590810880 basic_session_run_hooks.py:680] global_step/sec: 5.70165
I0226 10:13:21.573970 140644590810880 basic_session_run_hooks.py:247] loss = 0.35865137, step = 110 (1.781 sec)
I0226 10:13:21.576019 140644590810880 basic_session_run_hooks.py:680] global_step/sec: 5.616
I0226 10:13:23.344115 140644590810880 basic_session_run_hooks.py:247] loss = 0.35676146, step = 120 (1.770 sec)
I0226 10:13:23.347656 140644590810880 basic_session_run_hooks.py:680] global_step/sec: 5.64505
&lt;/denchmark-code&gt;

Compare two log,  TensorFlow do not continue to save checkpoints and output final loss when use nccl/xring all reduce alg. How to solve this problem, thanks.
Code to reproduce the issue
I get NeuMF model source code from official models repo, and original code only support local training with estimator, and I add below code to support distributed training:
&lt;denchmark-code&gt;distribution = tf.contrib.distribute.MirroredStrategy(
    num_gpus_per_worker=num_gpus)
    
run_config = tf.estimator.RunConfig(
    model_dir=model_dir,
    log_step_count_steps=10,
    train_distribute=distribution,Â                
    eval_distribute=distribution)
    
 estimator = tf.estimator.Estimator(
    model_fn=model_fn,
    model_dir=model_dir,
    config=run_config, 
    params=params)
    
 train_spec = tf.estimator.TrainSpec(
    train_input_fn,max_steps=5000,hooks=train_hooks)
 eval_spec = tf.estimator.EvalSpec(eval_input_fn,steps=100)
 tf.estimator.train_and_evaluate(estimator,train_spec,eval_spec)
&lt;/denchmark-code&gt;

The cross device ops implement of MirrorStrategy  is MultiWorkerAllReduce  which is defined in tensorflow/python/distribute/cross_device_ops.py, I change value of all_reduce_alg  parameter of __init__  of class MultiworkerAllReduce from pscpu/pscpu to nccl/xring.
Other info / logs
When hang, stack of two worker is:
&lt;denchmark-code&gt;#0  0x00007f990858ba13 in epoll_wait () at ../sysdeps/unix/syscall-template.S:84
#1  0x00007f988f250d98 in pollset_work () from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00007f988f270dcf in cq_pluck(grpc_completion_queue*, void*, gpr_timespec, void*) () from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00007f988f27119b in grpc_completion_queue_pluck () from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00007f988f1dc898 in grpc::CoreCodegen::grpc_completion_queue_pluck(grpc_completion_queue*, void*, gpr_timespec, void*) ()
   from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00007f988882ad02 in grpc::CompletionQueue::Pluck(grpc::internal::CompletionQueueTag*) () from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#6  0x00007f988882f056 in grpc::internal::BlockingUnaryCallImpl&lt;tensorflow::RunStepRequest, tensorflow::RunStepResponse&gt;::BlockingUnaryCallImpl(grpc::ChannelInterface*, grpc::internal::RpcMethod const&amp;, grpc::ClientContext*, tensorflow::RunStepRequest const&amp;, tensorflow::RunStepResponse*) () from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#7  0x00007f988882f26d in tensorflow::grpc::MasterService::Stub::RunStep(grpc::ClientContext*, tensorflow::RunStepRequest const&amp;, tensorflow::RunStepResponse*) ()
   from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#8  0x00007f98886b3f6d in tensorflow::GrpcRemoteMaster::RunStep(tensorflow::CallOptions*, tensorflow::RunStepRequestWrapper*, tensorflow::MutableRunStepResponseWrapper*) ()
   from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#9  0x00007f98886adfc9 in tensorflow::GrpcSession::RunProto(tensorflow::CallOptions*, tensorflow::MutableRunStepRequestWrapper*, tensorflow::MutableRunStepResponseWrapper*) ()
   from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#10 0x00007f98886aec52 in tensorflow::GrpcSession::RunHelper(tensorflow::RunOptions const&amp;, std::vector&lt;std::pair&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, tensorflow::Tensor&gt;, std::allocator&lt;std::pair&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, tensorflow::Tensor&gt; &gt; &gt; const&amp;, std::vector&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, std::allocator&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;, std::vector&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, std::allocator&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;, std::vector&lt;tensorflow::Tensor, std::allocator&lt;tensorflow::Tensor&gt; &gt;*, tensorflow::RunMetadata*, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;) ()
   from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#11 0x00007f98886af3e0 in tensorflow::GrpcSession::Run(tensorflow::RunOptions const&amp;, std::vector&lt;std::pair&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, tensorflow::Tensor&gt;, std::allocator&lt;std::pair&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, tensorflow::Tensor&gt; &gt; &gt; const&amp;, std::vector&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, std::allocator&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;, std::vector&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, std::allocator&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;, std::vector&lt;tensorflow::Tensor, std::allocator&lt;tensorflow::Tensor&gt; &gt;*, tensorflow::RunMetadata*) () from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#12 0x00007f9888693f42 in tensorflow::SessionRef::Run(tensorflow::RunOptions const&amp;, std::vector&lt;std::pair&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, tensorflow::Tensor&gt;, std::allocator&lt;std::pair&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, tensorflow::Tensor&gt; &gt; &gt; const&amp;, std::vector&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, std::allocator&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;, std::vector&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, std::allocator&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;, std::vector&lt;tensorflow::Tensor, std::allocator&lt;tensorflow::Tensor&gt; &gt;*, tensorflow::RunMetadata*) () from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#13 0x00007f98888997a1 in TF_Run_Helper(tensorflow::Session*, char const*, TF_Buffer const*, std::vector&lt;std::pair&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, tensorflow::Tensor&gt;, std::allocator&lt;std::pair&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, tensorflow::Tensor&gt; &gt; &gt; const&amp;, std::vector&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, std::allocator&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;, TF_Tensor**, std::vector&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, std::allocator&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;, TF_Buffer*, TF_Status*) [clone .constprop.664] ()
   from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#14 0x00007f9888899f9e in TF_SessionRun () from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#15 0x00007f988868fbc9 in tensorflow::TF_SessionRun_wrapper_helper(TF_Session*, char const*, TF_Buffer const*, std::vector&lt;TF_Output, std::allocator&lt;TF_Output&gt; &gt; const&amp;, std::vector&lt;_object*, std::allocator&lt;_object*&gt; &gt; const&amp;, std::vector&lt;TF_Output, std::allocator&lt;TF_Output&gt; &gt; const&amp;, std::vector&lt;TF_Operation*, std::allocator&lt;TF_Operation*&gt; &gt; const&amp;, TF_Buffer*, TF_Status*, std::vector&lt;_object*, std::allocator&lt;_object*&gt; &gt;*) () from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#16 0x00007f988868fc62 in tensorflow::TF_SessionRun_wrapper(TF_Session*, TF_Buffer const*, std::vector&lt;TF_Output, std::allocator&lt;TF_Output&gt; &gt; const&amp;, std::vector&lt;_object*, std::allocator&lt;_object*&gt; &gt; const&amp;, std::vector&lt;TF_Output, std::allocator&lt;TF_Output&gt; &gt; const&amp;, std::vector&lt;TF_Operation*, std::allocator&lt;TF_Operation*&gt; &gt; const&amp;, TF_Buffer*, TF_Status*, std::vector&lt;_object*, std::allocator&lt;_object*&gt; &gt;*) ()
   from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#17 0x00007f988864a70a in _wrap_TF_SessionRun_wrapper () from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#18 0x00000000004bc4aa in PyEval_EvalFrameEx ()
#19 0x00000000004b9b66 in PyEval_EvalCodeEx ()
#20 0x00000000004c1f56 in PyEval_EvalFrameEx ()
#21 0x00000000004b9b66 in PyEval_EvalCodeEx ()
#22 0x00000000004d5669 in ?? ()
#23 0x00000000004a587e in PyObject_Call ()
#24 0x00000000004be51e in PyEval_EvalFrameEx ()
---Type &lt;return&gt; to continue, or q &lt;return&gt; to quit---
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;90      pthread_join.c: No such file or directory.
bt
#0  0x00007f65d1baf98d in pthread_join (threadid=140057709098752, thread_return=0x0) at pthread_join.c:90
#1  0x00007f6540377b97 in std::thread::join() () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00007f654ef477a0 in tensorflow::(anonymous namespace)::StdThread::~StdThread() () from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#3  0x00007f6551b6c032 in tensorflow::GrpcServer::Join() () from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00007f6551be4a8c in TF_ServerJoin () from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00007f655198d756 in _wrap_TF_ServerJoin () from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#6  0x00000000004bc4aa in PyEval_EvalFrameEx ()
#7  0x00000000004b9b66 in PyEval_EvalCodeEx ()
#8  0x00000000004c1f56 in PyEval_EvalFrameEx ()
#9  0x00000000004b9b66 in PyEval_EvalCodeEx ()
#10 0x00000000004c17c6 in PyEval_EvalFrameEx ()
#11 0x00000000004b9b66 in PyEval_EvalCodeEx ()
#12 0x00000000004c17c6 in PyEval_EvalFrameEx ()
#13 0x00000000004b9b66 in PyEval_EvalCodeEx ()
#14 0x00000000004c1f56 in PyEval_EvalFrameEx ()
#15 0x00000000004b9b66 in PyEval_EvalCodeEx ()
#16 0x00000000004c1f56 in PyEval_EvalFrameEx ()
#17 0x00000000004b9b66 in PyEval_EvalCodeEx ()
#18 0x00000000004c1f56 in PyEval_EvalFrameEx ()
#19 0x00000000004b9b66 in PyEval_EvalCodeEx ()
#20 0x00000000004c1f56 in PyEval_EvalFrameEx ()
#21 0x00000000004b9b66 in PyEval_EvalCodeEx ()
#22 0x00000000004c17c6 in PyEval_EvalFrameEx ()
#23 0x00000000004b9b66 in PyEval_EvalCodeEx ()
#24 0x00000000004eb69f in ?? ()
#25 0x00000000004e58f2 in PyRun_FileExFlags ()
#26 0x00000000004e41a6 in PyRun_SimpleFileExFlags ()
#27 0x00000000004938ce in Py_Main ()
#28 0x00007f65d17fd830 in __libc_start_main (main=0x493370 &lt;main&gt;, argc=30, argv=0x7ffdb8177748, init=&lt;optimized out&gt;, fini=&lt;optimized out&gt;, rtld_fini=&lt;optimized out&gt;, stack_end=0x7ffdb8177738)
    at ../csu/libc-start.c:291
#29 0x0000000000493299 in _start ()
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Keepmoving-ZXY' date='2019-02-27T01:16:26Z'>
		Could you try with CollectiveAllReduceStrategy?  We have very limited support for MirroredStrategy running in distributed fashion.
		</comment>
		<comment id='2' author='Keepmoving-ZXY' date='2019-02-27T03:26:53Z'>
		CollectiveAllReduceStrategy is ok, but I think nccl is the best for local reduction, so there is an idea that use nccl in a worker and other method based on network for reduction between workers and nccl/xring achieve this idea but has problem. So does the CollectiveAllReduceStrategy also achieve this idea, does CollectiveAllReduceStrategy also use nccl for reduction in a worker?
		</comment>
		<comment id='3' author='Keepmoving-ZXY' date='2019-02-27T06:43:51Z'>
		Yeah, we will enable nccl implementation shortly in CollectiveAllReduceStrategy.
		</comment>
		<comment id='4' author='Keepmoving-ZXY' date='2019-02-27T09:03:35Z'>
		ok, thank you.
		</comment>
		<comment id='5' author='Keepmoving-ZXY' date='2019-03-12T21:25:33Z'>
		You can now use tf.distribute.MultiWorkerMirroredStrategy(communication=tf.distribute.experimental.CollectiveCommunication.NCCL) to use nccl on multiple nodes.
		</comment>
		<comment id='6' author='Keepmoving-ZXY' date='2019-03-13T01:56:20Z'>
		thank you, I find time to have a try  and I find this problem seems be related to NCCL version in these day.
		</comment>
	</comments>
</bug>