<bug id='43125' author='JamesMBartlett' open_date='2020-09-10T23:11:54Z' closed_time='2020-09-22T06:35:53Z'>
	<summary>tf.data.experimental.service throws error when using with TPUs</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes, I put an example to reproduce the issue below


OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
$ uname -a Linux james-tpu 4.19.0-10-cloud-amd64 #1 SMP Debian 4.19.132-1 (2020-07-24) x86_64 GNU/Linux


Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A


TensorFlow installed from (source or binary):
binary


TensorFlow version (use command below):
v2.3.0-rc2-23-gb36436b087 2.3.0


Python version:
3.7.3


Bazel version (if compiling from source):
N/A


GCC/Compiler version (if compiling from source):
N/A


CUDA/cuDNN version:
N/A


GPU model and memory:
N/A


Describe the current behavior
Errors out with 2020-09-10 22:59:13.294374: E tensorflow/core/common_runtime/eager/context.cc:678] Failed to register function remotely due to Failed to register dataset: failed to connect to all addresses see full log below
Describe the expected behavior
I wouldn't expect an error for this code.
Standalone code to reproduce the issue

Code
import logging

import tensorflow as tf
import numpy as np

logging.getLogger('tensorflow').setLevel(logging.DEBUG)


def get_dataset(dispatcher, batch_size):
  fake_data = np.zeros((128, 256, 256), dtype=np.float32)
  ds = tf.data.Dataset.from_tensor_slices(fake_data)
  ds = ds.repeat()
  ds = ds.batch(batch_size)
  ds = ds.apply(tf.data.experimental.service.distribute('parallel_epochs', dispatcher.target, job_name='data_job'))
  return ds

def run(strategy, batch_size):
  data_dispatcher = tf.data.experimental.service.DispatchServer(port=0)
  dispatcher_address = data_dispatcher.target.split("://")[1]
  worker = tf.data.experimental.service.WorkerServer(
    port=0, dispatcher_address=dispatcher_address)

  per_replica_batch_size = batch_size // strategy.num_replicas_in_sync
  dataset = strategy.experimental_distribute_datasets_from_function(
    lambda _: get_dataset(
      data_dispatcher,
      per_replica_batch_size,
    )
  )
  ds_iterator = iter(dataset)

  @tf.function(input_signature=[tf.TensorSpec([256, 256], dtype=tf.float32)])
  def step_fn(inputs):
    return

  @tf.function
  def train_step(iterator):
    strategy.run(step_fn, args=(next(iterator),))

  for _ in range(10):
    train_step(ds_iterator)


def setup_strategy(tpu=False):
  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='james-tpu')
  tf.config.experimental_connect_to_cluster(resolver)
  tf.tpu.experimental.initialize_tpu_system(resolver)
  return tf.distribute.TPUStrategy(resolver)


if __name__ == '__main__':
  strategy = setup_strategy()
  batch_size = 64
  run(strategy, batch_size)


Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Logs
2020-09-10 22:59:07.877313: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-10 22:59:07.882757: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2299995000 Hz
2020-09-10 22:59:07.883041: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3aa2460 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-10 22:59:07.883156: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-10 22:59:07.889580: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; 10.184.89.74:8470}
2020-09-10 22:59:07.889626: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -&gt; {0 -&gt; localhost:31299}
2020-09-10 22:59:07.904477: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; 10.184.89.74:8470}
2020-09-10 22:59:07.904533: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -&gt; {0 -&gt; localhost:31299}
2020-09-10 22:59:07.905018: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:405] Started server with target: grpc://localhost:31299
INFO:tensorflow:Initializing the TPU system: james-tpu
INFO:tensorflow:Initializing the TPU system: james-tpu
INFO:tensorflow:Clearing out eager caches
INFO:tensorflow:Clearing out eager caches
INFO:tensorflow:Finished initializing TPU system.
INFO:tensorflow:Finished initializing TPU system.
INFO:tensorflow:Found TPU system:
INFO:tensorflow:Found TPU system:
INFO:tensorflow:*** Num TPU Cores: 8
INFO:tensorflow:*** Num TPU Cores: 8
INFO:tensorflow:*** Num TPU Workers: 1
INFO:tensorflow:*** Num TPU Workers: 1
INFO:tensorflow:*** Num TPU Cores Per Worker: 8
INFO:tensorflow:*** Num TPU Cores Per Worker: 8
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
2020-09-10 22:59:13.294374: E tensorflow/core/common_runtime/eager/context.cc:678] Failed to register function remotely due to Failed to register dataset: failed to connect to all addresses
This shouldn't happen, please file a bug to tensorflow team.
Traceback (most recent call last):
  File "/usr/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/usr/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/jamesbartlett/train/pixieml/pixieml/models/transformer/minimal_example.py", line 54, in &lt;module&gt;
    run(strategy, batch_size)
  File "/home/jamesbartlett/train/pixieml/pixieml/models/transformer/minimal_example.py", line 30, in run
    ds_iterator = iter(dataset)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py", line 1199, in __iter__
    enable_legacy_iterators)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py", line 1752, in _create_iterators_per_worker
    worker_devices)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py", line 1609, in __init__
    devices)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py", line 1448, in __init__
    self._make_iterator()
  File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py", line 1619, in _make_iterator
    self._dataset, self._devices, source_device=host_device)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py", line 547, in __init__
    dataset.element_spec)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py", line 54, in __init__
    init_func_concrete = _init_func.get_concrete_function()
  File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py", line 2939, in get_concrete_function
2020-09-10 22:59:13.297630: W tensorflow/core/distributed_runtime/eager/remote_tensor_handle_data.cc:76] Unable to destroy remote tensor handles. If you are running a tf.function, it usually indicates some op in the graph gets an error: Failed to register dataset: failed to connect to all addresses
    *args, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py", line 2906, in _get_concrete_function_garbage_collected
    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py", line 3213, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py", line 3075, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py", line 991, in func_graph_from_py_func
    expand_composites=True)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py", line 635, in map_structure
    structure[0], [func(*x) for x in entries],
  File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py", line 635, in &lt;listcomp&gt;
    structure[0], [func(*x) for x in entries],
  File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py", line 942, in convert
    x = ops.convert_to_tensor_or_composite(x)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py", line 1622, in convert_to_tensor_or_composite
    value=value, dtype=dtype, name=name, as_ref=False)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py", line 1661, in internal_convert_to_tensor_or_composite
    accepted_result_types=(Tensor, composite_tensor.CompositeTensor))
  File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py", line 1467, in convert_to_tensor
    return graph.capture(value, name=name)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py", line 624, in capture
    return self.capture_eager_tensor(tensor, name)
  File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py", line 721, in capture_eager_tensor
    graph_const = constant_op.constant(tensor.numpy(), dtype=tensor.dtype,
  File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py", line 1063, in numpy
    maybe_arr = self._numpy()  # pylint: disable=protected-access
  File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py", line 1031, in _numpy
    six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access
  File "&lt;string&gt;", line 3, in raise_from
tensorflow.python.framework.errors_impl.UnavailableError: Failed to register dataset: failed to connect to all addresses
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/tpu_strategy.py", line 540, in async_wait
    context.async_wait()
  File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py", line 2319, in async_wait
    context().sync_executors()
  File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py", line 658, in sync_executors
    pywrap_tfe.TFE_ContextSyncExecutors(self._context_handle)
tensorflow.python.framework.errors_impl.UnavailableError: Failed to register dataset: failed to connect to all addresses
2020-09-10 22:59:13.375822: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 30, Output num: 0
Additional GRPC error information from remote target /job:worker/replica:0/task:0:
:{"created":"@1599778753.375752373","description":"Error received from peer ipv4:10.184.89.74:8470","file":"external/com_github_grpc_grpc/src/core/lib/surface/call.cc","file_line":1056,"grpc_message":"Unable to find the relevant tensor remote_handle: Op ID: 30, Output num: 0","grpc_status":3}


	</description>
	<comments>
		<comment id='1' author='JamesMBartlett' date='2020-09-11T09:59:17Z'>
		Was able to reproduce the issue with TF v2.3. Please find the gist of it &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/a9bf81fba516d0dfee51bcfca6507d6f/43125.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='JamesMBartlett' date='2020-09-11T23:56:34Z'>
		Hi &lt;denchmark-link:https://github.com/JamesMBartlett&gt;@JamesMBartlett&lt;/denchmark-link&gt;
, to help isolate if this is an issue with tf.data or with tpu, can you confirm that your code runs without TPUs using the default strategy  ?
		</comment>
		<comment id='3' author='JamesMBartlett' date='2020-09-12T00:09:03Z'>
		Yeah the error goes away with the default strategy, although there's a bug with my minimal example where I specified the wrong input shape, so you get the following error:
&lt;denchmark-code&gt;ValueError: Python inputs incompatible with input_signature:
      inputs: (
        Tensor("IteratorGetNext:0", shape=(None, 256, 256), dtype=float32))
      input_signature: (
        TensorSpec(shape=(256, 256), dtype=tf.float32, name=None))
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='JamesMBartlett' date='2020-09-12T00:18:16Z'>
		Can you provide a fix so that the code runs without error when using the default strategy?
		</comment>
		<comment id='5' author='JamesMBartlett' date='2020-09-12T00:48:03Z'>
		This code errors if you run with variable tpu = True on line 59, and doesn't error if tpu = False:
&lt;denchmark-code&gt;import logging

import tensorflow as tf
import numpy as np

logging.getLogger('tensorflow').setLevel(logging.DEBUG)


def get_dataset(dispatcher, batch_size):
  fake_data = np.zeros((128, 256, 256), dtype=np.float32)
  ds = tf.data.Dataset.from_tensor_slices(fake_data)
  ds = ds.repeat()
  ds = ds.batch(batch_size)
  ds = ds.apply(tf.data.experimental.service.distribute('parallel_epochs', dispatcher.target, job_name='data_job'))
  return ds

def run(strategy, batch_size):
  data_dispatcher = tf.data.experimental.service.DispatchServer(port=0)
  dispatcher_address = data_dispatcher.target.split("://")[1]
  worker = tf.data.experimental.service.WorkerServer(
    port=0, dispatcher_address=dispatcher_address)

  per_replica_batch_size = batch_size // strategy.num_replicas_in_sync
  dataset = strategy.experimental_distribute_datasets_from_function(
    lambda _: get_dataset(
      data_dispatcher,
      per_replica_batch_size,
    )
  )
  ds_iterator = iter(dataset)

  @tf.function(input_signature=[tf.TensorSpec([None, 256, 256], dtype=tf.float32)])
  def step_fn(inputs):
    pass

  @tf.function
  def train_step(iterator):
    strategy.run(step_fn, args=(next(iterator),))

  for _ in range(10):
    train_step(ds_iterator)

  print('Finished, stopping data workers...')
  del ds_iterator
  del dataset
  data_dispatcher._stop()
  worker._stop()
  print('Stopped')


def setup_tpu_strategy():
  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='james-tpu')
  tf.config.experimental_connect_to_cluster(resolver)
  tf.tpu.experimental.initialize_tpu_system(resolver)
  return tf.distribute.TPUStrategy(resolver)


if __name__ == '__main__':
  tpu = False
  if tpu:
    strategy = setup_tpu_strategy()
  else:
    strategy = tf.distribute.get_strategy()
  batch_size = 64
  run(strategy, batch_size)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='JamesMBartlett' date='2020-09-16T06:17:44Z'>
		&lt;denchmark-link:https://github.com/aaudiber&gt;@aaudiber&lt;/denchmark-link&gt;
 do you have any thoughts here? The error is "Failed to register function remotely due to Failed to register dataset".
		</comment>
		<comment id='7' author='JamesMBartlett' date='2020-09-16T06:29:39Z'>
		&lt;denchmark-link:https://github.com/rxsang&gt;@rxsang&lt;/denchmark-link&gt;
 is that the full error? The real cause should be included as "Failed to register dataset: &lt;cause&gt;" (error message created &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/8e789c38727f955fd9735e41c0155a536f0d30b6/tensorflow/core/data/service/data_service.cc#L123&gt;here&lt;/denchmark-link&gt;
/&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/8e789c38727f955fd9735e41c0155a536f0d30b6/tensorflow/core/data/service/grpc_util.cc#L34-L35&gt;here&lt;/denchmark-link&gt;
)
		</comment>
		<comment id='8' author='JamesMBartlett' date='2020-09-16T07:05:21Z'>
		I think the full error is 2020-09-10 22:59:13.294374: E tensorflow/core/common_runtime/eager/context.cc:678] Failed to register function remotely due to Failed to register dataset: failed to connect to all addresses This shouldn't happen, please file a bug to tensorflow team.
However it is not that informative.
		</comment>
		<comment id='9' author='JamesMBartlett' date='2020-09-16T22:22:35Z'>
		&lt;denchmark-link:https://github.com/JamesMBartlett&gt;@JamesMBartlett&lt;/denchmark-link&gt;
 I think the problem is in the target passed in
&lt;denchmark-code&gt;ds = ds.apply(tf.data.experimental.service.distribute('parallel_epochs', dispatcher.target, job_name='data_job'))
&lt;/denchmark-code&gt;

dispatcher.target will return something like "grpc://localhost:39263". This works when the client runs on the local machine (non-TPU case), but when the TPU tries to register the dataset with "localhost:39263", it hits "Failed to register dataset: failed to connect to all addresses". Can you try setting a specific port and using an IP address accessible from the TPU?
Something like
&lt;denchmark-code&gt;data_dispatcher = tf.data.experimental.service.DispatchServer(port=5050)
...
ds = ds.apply(tf.data.experimental.service.distribute('parallel_epochs', "grpc://&lt;ip_address&gt;:5050, job_name='data_job'))
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='JamesMBartlett' date='2020-09-16T23:08:08Z'>
		I can confirm that if I use grpc://&lt;ip_address&gt;:5050 then my minimal example works.
		</comment>
		<comment id='11' author='JamesMBartlett' date='2020-09-16T23:16:08Z'>
		Thanks for confirming James! I will submit a change to include the dispatcher address in the error message, to help diagnose this type of issue in the future.
		</comment>
		<comment id='12' author='JamesMBartlett' date='2020-09-22T06:35:54Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43125&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43125&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>