<bug id='33059' author='wuhy08' open_date='2019-10-04T23:21:29Z' closed_time='2019-10-14T15:21:17Z'>
	<summary>Size and CombinedNMS Op support request for tflite</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
TensorFlow installed from (source or binary): binary (pip)
TensorFlow version (or github SHA if from source): 2.0.0

Provide the text output from tflite_convert
&lt;denchmark-code&gt;2019-10-04 16:17:28.794407: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2019-10-04 16:17:28.798220: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2019-10-04 16:17:28.798242: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: WD-AI-lab
2019-10-04 16:17:28.798246: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: WD-AI-lab
2019-10-04 16:17:28.798282: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.26.0
2019-10-04 16:17:28.798299: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 396.51.0
2019-10-04 16:17:28.798303: E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 396.51.0 does not match DSO version 430.26.0 -- cannot find working devices in this configuration
2019-10-04 16:17:28.798415: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-10-04 16:17:28.821484: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 4008000000 Hz
2019-10-04 16:17:28.822257: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5563ee4031d0 executing computations on platform Host. Devices:
2019-10-04 16:17:28.822268: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2019-10-04 16:17:32.842974: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count &gt;= 8, compute capability &gt;= 0.0): 0
2019-10-04 16:17:32.843029: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2019-10-04 16:17:32.858437: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize
2019-10-04 16:17:32.858454: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.003ms.
2019-10-04 16:17:32.858458: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0ms.
2019-10-04 16:17:35.240170: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count &gt;= 8, compute capability &gt;= 0.0): 0
2019-10-04 16:17:35.240264: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2019-10-04 16:17:35.995629: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize
2019-10-04 16:17:35.995652: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 1001 nodes (-358), 2553 edges (-358), time = 346.923ms.
2019-10-04 16:17:35.995656: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 1001 nodes (0), 2553 edges (0), time = 114.777ms.
Traceback (most recent call last):
  File "/home/wd_ai/git-pkgs/yolov3-tf2/convert_tflite.py", line 29, in &lt;module&gt;
    app.run(main)
  File "/home/wd_ai/miniconda3/envs/yolov3-tf2-gpu/lib/python3.7/site-packages/absl/app.py", line 299, in run
    _run_main(main, args)
  File "/home/wd_ai/miniconda3/envs/yolov3-tf2-gpu/lib/python3.7/site-packages/absl/app.py", line 250, in _run_main
    sys.exit(main(argv))
  File "/home/wd_ai/git-pkgs/yolov3-tf2/convert_tflite.py", line 24, in main
    tflite_model = converter.convert()
  File "/home/wd_ai/miniconda3/envs/yolov3-tf2-gpu/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py", line 446, in convert
    **converter_kwargs)
  File "/home/wd_ai/miniconda3/envs/yolov3-tf2-gpu/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py", line 449, in toco_convert_impl
    enable_mlir_converter=enable_mlir_converter)
  File "/home/wd_ai/miniconda3/envs/yolov3-tf2-gpu/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py", line 200, in toco_convert_protos
    raise ConverterError("See console for info.\n%s\n%s\n" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: See console for info.
2019-10-04 16:17:37.744297: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size
2019-10-04 16:17:37.744348: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size
2019-10-04 16:17:37.744519: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size
2019-10-04 16:17:37.744542: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size
2019-10-04 16:17:37.744676: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size
2019-10-04 16:17:37.744683: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size
2019-10-04 16:17:37.744759: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: CombinedNonMaxSuppression
2019-10-04 16:17:37.757406: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 743 operators, 1376 arrays (0 quantized)
2019-10-04 16:17:37.767372: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 743 operators, 1376 arrays (0 quantized)
2019-10-04 16:17:38.148616: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 284 operators, 533 arrays (0 quantized)
2019-10-04 16:17:38.152237: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 284 operators, 533 arrays (0 quantized)
2019-10-04 16:17:38.155833: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 284 operators, 533 arrays (0 quantized)
2019-10-04 16:17:38.158553: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 284 operators, 533 arrays (0 quantized)
2019-10-04 16:17:38.164271: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 66560128 bytes, theoretical optimal value: 44408960 bytes.
2019-10-04 16:17:38.165392: E tensorflow/lite/toco/toco_tooling.cc:466] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DIV, EXP, EXPAND_DIMS, FILL, LEAKY_RELU, LOGISTIC, MUL, PACK, PAD, RESHAPE, RESIZE_NEAREST_NEIGHBOR, SHAPE, SPLIT_V, STRIDED_SLICE, SUB. Here is a list of operators for which you will need custom implementations: CombinedNonMaxSuppression, Size.
Traceback (most recent call last):
  File "/home/wd_ai/miniconda3/envs/yolov3-tf2-gpu/bin/toco_from_protos", line 10, in &lt;module&gt;
    sys.exit(main())
  File "/home/wd_ai/miniconda3/envs/yolov3-tf2-gpu/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py", line 89, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File "/home/wd_ai/miniconda3/envs/yolov3-tf2-gpu/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File "/home/wd_ai/miniconda3/envs/yolov3-tf2-gpu/lib/python3.7/site-packages/absl/app.py", line 299, in run
    _run_main(main, args)
  File "/home/wd_ai/miniconda3/envs/yolov3-tf2-gpu/lib/python3.7/site-packages/absl/app.py", line 250, in _run_main
    sys.exit(main(argv))
  File "/home/wd_ai/miniconda3/envs/yolov3-tf2-gpu/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py", line 52, in execute
    enable_mlir_converter)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DIV, EXP, EXPAND_DIMS, FILL, LEAKY_RELU, LOGISTIC, MUL, PACK, PAD, RESHAPE, RESIZE_NEAREST_NEIGHBOR, SHAPE, SPLIT_V, STRIDED_SLICE, SUB. Here is a list of operators for which you will need custom implementations: CombinedNonMaxSuppression, Size.

&lt;/denchmark-code&gt;

Also, please include a link to a GraphDef or the model if possible.
Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='wuhy08' date='2019-10-09T05:44:47Z'>
		&lt;denchmark-link:https://github.com/wuhy08&gt;@wuhy08&lt;/denchmark-link&gt;
 ,  as an op is supported by our &lt;denchmark-link:https://www.tensorflow.org/lite/guide/ops_select&gt;Select TF ops&lt;/denchmark-link&gt;
 feature - assuming you are okay with a few MBs of increase in binary size. However, CombinedNonMaxSuppression is not.
As part of some upcoming TFLite converter changes, we are planning to support the usage of NonMaxSuppressionV4 &amp; NonMaxSuppressionV5 in TFLite graphs - is there any way you could modify your network to get these instead? Are you using TF's object detection API?
		</comment>
		<comment id='2' author='wuhy08' date='2019-10-09T17:12:37Z'>
		Hi &lt;denchmark-link:https://github.com/srjoglekar246&gt;@srjoglekar246&lt;/denchmark-link&gt;

I am using tf.image.combined_non_max_suppression, I think it uses tensorflow::ops::CombinedNonMaxSuppression on the backend, correct?
If I switch to tf.image.non_max_suppression, will it call tensorflow::ops::NonMaxSuppressionV3?
Which front end API calls V4 or V5?
Thank you!
		</comment>
		<comment id='3' author='wuhy08' date='2019-10-10T02:30:11Z'>
		One of the following should yield the required ops in the graph:

tf.image.non_max_suppression_padded
tf.image.non_max_suppression_with_scores

The first is just a slight modification to , while the latter one generalizes it to 'soft' NMS. Which ops are generated by each function based on the params can be seen in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/ops/image_ops_impl.py&gt;this file&lt;/denchmark-link&gt;
.
Once you have these ops in, using &lt;denchmark-link:https://www.tensorflow.org/lite/guide/ops_select&gt;Select TF ops&lt;/denchmark-link&gt;
 should correctly convert your model. Later this year, we will be making some converter updates that make NMS ops even easier to support natively in TFLite :-)
		</comment>
		<comment id='4' author='wuhy08' date='2019-10-10T18:13:00Z'>
		Thank you, &lt;denchmark-link:https://github.com/srjoglekar246&gt;@srjoglekar246&lt;/denchmark-link&gt;
 !
		</comment>
		<comment id='5' author='wuhy08' date='2019-10-14T15:21:17Z'>
		Closing this for now. Feel free to re-open if you are still stuck. Thanks!
		</comment>
		<comment id='6' author='wuhy08' date='2019-10-14T15:21:18Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33059&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33059&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>