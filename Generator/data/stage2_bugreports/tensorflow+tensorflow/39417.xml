<bug id='39417' author='Flamefire' open_date='2020-05-11T16:07:02Z' closed_time='2020-05-18T18:05:00Z'>
	<summary>experimental_run_v2 throws AttributeError with MultiWorkerMirroredStrategy</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.1 and 2.2 affected

Describe the current behavior
Using strategy.experimental_run_v2 (or strategy.run for TF 2.2) with MultiWorkerMirroredStrategy throws AttributeError: 'CollectiveAllReduceExtended' object has no attribute '_cfer_fn_cache' when passing it a tf.function
This is caused by the access at &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/distribute/mirrored_strategy.py#L743&gt;https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/distribute/mirrored_strategy.py#L743&lt;/denchmark-link&gt;
 due to  not calling the  function which creates that dictionary at &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/distribute/mirrored_strategy.py#L472&gt;https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/distribute/mirrored_strategy.py#L472&lt;/denchmark-link&gt;

I noted that the relevant code was removed in current master by &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/b16d24a342c5de1384dcb9ee408a74f206d332b2&gt;b16d24a&lt;/denchmark-link&gt;
 but wanted to make sure it is included in the next release or in a patch release. Also  is not mentioned in the commit, so it might be a good idea to include something like this as a test case to avoid regressions. Looking at the commit I guess this is fixed too.
Standalone code to reproduce the issue
&lt;denchmark-code&gt;import tensorflow as tf

strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()

with strategy.scope():
    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
    x_train, x_test = x_train / 255.0, x_test / 255.0
    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)

    model = tf.keras.models.Sequential([
        tf.keras.layers.Flatten(input_shape=(28, 28)),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(10),
    ])

    model.compile(
        optimizer=tf.keras.optimizers.SGD(),
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=['accuracy'])


@tf.function
def train_step(model, data, target):
    with tf.GradientTape() as tape:
        predictions = model(data, training=True)
        loss = model.loss(target, predictions)

    gradients = tape.gradient(loss, model.trainable_variables)
    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    return loss


def distributed_train_step(strategy, model, x, y):
    strategy.experimental_run_v2(train_step, args=(model, x, y))


for x, y in train_dataset:
    distributed_train_step(strategy, model, x, y)

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Flamefire' date='2020-05-12T10:22:05Z'>
		Was able to reproduce the issue with TF v2.1 and &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/316a940c03a42bbb52b1d8ce4db15d43/39417.ipynb&gt;TF v2.2&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/948f56175057fb095317cd2c91fff970/39417-tf-nightly.ipynb#scrollTo=vA2ulWhPWgOe&gt;TF-nightly&lt;/denchmark-link&gt;
. Please find the attached gist. Thanks!
		</comment>
		<comment id='2' author='Flamefire' date='2020-05-18T18:05:00Z'>
		The issue should already be fixed at HEAD. As a workaround for 2.2, you can wrap distributed_train_step with @tf.function.
		</comment>
		<comment id='3' author='Flamefire' date='2020-05-18T18:05:02Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39417&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39417&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='Flamefire' date='2020-05-19T06:56:04Z'>
		&lt;denchmark-link:https://github.com/crccw&gt;@crccw&lt;/denchmark-link&gt;
 The suggested workaround does not work in neither TF 2.1 or TF 2.2: &lt;denchmark-link:https://colab.research.google.com/drive/1563oBryExvogSUexRlR64C4nuK52WMR5?usp=sharing&gt;https://colab.research.google.com/drive/1563oBryExvogSUexRlR64C4nuK52WMR5?usp=sharing&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='Flamefire' date='2020-05-20T20:54:52Z'>
		Try only wrap distributed_train_step, not train_step.
		</comment>
		<comment id='6' author='Flamefire' date='2020-06-08T09:32:11Z'>
		Dear &lt;denchmark-link:https://github.com/Flamefire&gt;@Flamefire&lt;/denchmark-link&gt;

Maybe you can help to reproduce the &lt;denchmark-link:https://github.com/brianwa84&gt;@brianwa84&lt;/denchmark-link&gt;
 's &lt;denchmark-link:https://colab.research.google.com/github/tensorflow/probability/blob/master/discussion/examples/cross_gpu_logprob.ipynb&gt;example&lt;/denchmark-link&gt;
 on Slurm machines (below the code). The main idea is to execute log probability calculations in MCMC algorithm across GPUs.
I've tested lots of combinations (as you can see in the commented code below) but none of them work :(
The first attempt was to use the same configuration as &lt;denchmark-link:https://github.com/brianwa84&gt;@brianwa84&lt;/denchmark-link&gt;
 has in his &lt;denchmark-link:https://colab.research.google.com/github/tensorflow/probability/blob/master/discussion/examples/cross_gpu_logprob.ipynb&gt;notebook&lt;/denchmark-link&gt;
. It worked, but I want to use physical GPUs in the machine (several nodes), and not to create logical GPUs as he did. But when I try to use physical GPUs then MirroredStrategy  does not execute  calculations, the GPUs do not show activity (volatility 0%). Due to this behaviour my second attempt was try MultiWorkerMirroredStrategy but throws the same errors discussed here

Please could you help me how to reproduce the &lt;denchmark-link:https://github.com/brianwa84&gt;@brianwa84&lt;/denchmark-link&gt;
 's &lt;denchmark-link:https://colab.research.google.com/github/tensorflow/probability/blob/master/discussion/examples/cross_gpu_logprob.ipynb&gt;notebook&lt;/denchmark-link&gt;
 using Slurm queues?
#!/apps/PYTHON/3.7.4_ML/bin/python 
from __future__ import absolute_import, division, print_function, unico 
import sys 
import os 
# os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID" 
# os.environ['CUDA_VISIBLE_DEVICES'] = "0,1,2,3" 
import numpy as np 
import tensorflow as tf 
import tensorflow_probability as tfp 
tfb, tfd = tfp.bijectors, tfp.distributions 
print("TF version: ", tf.__version__) 
print("TFP: ", tfp.__version__) 
from slurm_cluster_resolver import SlurmClusterResolver 
# Sanity check to observe which device has calculations 
tf.debugging.set_log_device_placement(True) 
# ------------------------ 
# Setting up physical GPUs 
# ------------------------ 
# 
# Nothing to do with previos os.eviron['CUDA...] the system recognize t 
physical_gpus = tf.config.list_physical_devices('GPU') 
print(len(physical_gpus), "Physical GPUs") 
resolver = tf.distribute.cluster_resolver.SlurmClusterResolver() 
st = tf.distribute.experimental.MultiWorkerMirroredStrategy(resolver) 
# st = tf.distribute.MirroredStrategy() 
# strategy = tf.distribute.OneDeviceStrategy(device="/gpu:0") 
# ----------------------- 
# Setting up logical GPUs 
# ----------------------- 
# 
# GPU_SIZE = 6 # (G)  MemorySIZE per GPU 
# for gpu in physical_gpus: 
#    tf.config.experimental.set_virtual_device_configuration( 
#        gpu, 
#        [tf.config.experimental.VirtualDeviceConfiguration(memory_limi 
# logical_gpus = tf.config.list_logical_devices('GPU') 
# print(len(logical_gpus), "Logical GPUs") 
# st = tf.distribute.MirroredStrategy( 
#    devices=tf.config.list_logical_devices('GPU')) 
# resolver = tf.distribute.cluster_resolver.SlurmClusterResolver() 
# st = tf.distribute.experimental.MultiWorkerMirroredStrategy(cluster_r 
# Sanity check 
print ('Number of devices to do the strategy: {}'.format(st.num_replica 
print('Woker devices: {}', st.extended.worker_devices) 
# sys.exit() 

# Model and dataset 
# ----------------- 
# 
# Draw samples from an MVN, then sort them. This way we can easily visu 
# verify the correct partition ends up on the correct GPUs. 
ndim = 3 
def model(): 
    Root = tfd.JointDistributionCoroutine.Root 
    loc = yield Root(tfb.Shift(.5)(tfd.MultivariateNormalDiag(loc=tf.ze 
    scale_tril = yield Root(tfb.FillScaleTriL()(tfd.MultivariateNormalD 
 + 1) // 2])))) 
    yield tfd.MultivariateNormalTriL(loc=loc, scale_tril=scale_tril) 

dist = tfd.JointDistributionCoroutine(model) 

tf.random.set_seed(1) 
loc, scale_tril, _ = dist.sample(seed=2) 
samples = dist.sample(value=([loc] * 1024, scale_tril, None), seed=3)[2 
samples = tf.round(samples * 1000) / 1000 
for dim in reversed(range(ndim)): 
  samples = tf.gather(samples, tf.argsort(samples[:,dim])) 

# print(len(samples)) 
# print(loc) 
# print(scale_tril) 
# print(tf.reduce_mean(samples, 0)) 
# sys.exit() 

def dataset_fn(ctx): 
    batch_size = ctx.get_per_replica_batch_size(len(samples)) 
    d = tf.data.Dataset.from_tensor_slices(samples).batch(batch_size) 
    return d.shard(ctx.num_input_pipelines, ctx.input_pipeline_id) 
ds = st.experimental_distribute_datasets_from_function(dataset_fn) 

# Sanity check playing with dataset from function 
# iterator = iter(ds) 
# def replica_fn(arg): 
#    return tf.reduce_mean(arg, 0) 
# for batch in ds: 
#    replica1 = st.run(replica_fn, args=(batch,)) 
# print(replica1) 

# Toy 2 
# iterator = iter(ds) 
# @tf.function(input_signature=[iterator.element_spec]) 
# def replica_fn2(arg): 
#    return tf.reduce_mean(arg, 0) 
# replica2 = st.run(replica_fn2, args=(next(iterator),)) 
# print(replica2) 
# replicas_sum = st.reduce(tf.distribute.ReduceOp.SUM, replica1) 
# print(replicas_sum) 
# sys.exit() 
observations = next(iter(ds)) 
# print(observations) 

@tf.function(autograph=False) 
def log_prob_and_grad(loc, scale_tril, observations): 
    ctx = tf.distribute.get_replica_context() 
    with tf.GradientTape() as tape: 
        tape.watch((loc, scale_tril)) 
        lp = tf.reduce_sum(dist.log_prob(loc, scale_tril, observations)) / len(samples) 
    grad = tape.gradient(lp, (loc, scale_tril)) 
    return ctx.all_reduce('sum', lp), [ctx.all_reduce('sum', g) for g in grad] 

@tf.function(autograph=False) 
@tf.custom_gradient 
def target_log_prob(loc, scale_tril): 
    lp, grads = st.run(log_prob_and_grad, (loc, scale_tril, observations)) 
    return lp.values[0], lambda grad_lp: [grad_lp * g.values[0] for g in grads] 

singleton_vals = tfp.math.value_and_gradient(target_log_prob, (loc, scale_tril)) 
print(singleton_vals) 
print("*"*50) 
# sys.exit() 

kernel = tfp.mcmc.HamiltonianMonteCarlo(target_log_prob, step_size=.35, num_leapfrog_steps=2) 
kernel = tfp.mcmc.TransformedTransitionKernel(kernel, bijector=[tfb.Identity(), tfb.FillScaleTriL()])

@tf.function(autograph=False) 
def sample_chain(): 
    return tfp.mcmc.sample_chain( 
        num_results=200, num_burnin_steps=100, 
        current_state=[tf.ones_like(loc), tf.linalg.eye(scale_tril.shape[-1])], 
        kernel=kernel, trace_fn=lambda _, kr: kr.inner_results.is_accepted) 

samps, is_accepted = sample_chain() 
print(f'accept rate: {np.mean(is_accepted)}') 
print(f'ess: {tfp.mcmc.effective_sample_size(samps)}') 
print(tf.reduce_mean(samps[0], axis=0)) 
print(tf.reduce_mean(samps[1], axis=0)) 
&lt;denchmark-code&gt;#!/bin/bash 
#SBATCH --job-name='test_cross_gpu' 
#SBATCH --qos=debug 
# 
#SBATCH --nodes=1 
#SBATCH --ntasks=1 
#SBATCH --cpus-per-task=160 
#SBATCH --gres=gpu:4 
#SBATCH --time=00:15:00 
#------- I/O ------- 
#SBATCH -D . 
#SBATCH --output=test_cross_gpu_%j.out 
#SBATCH --error=test_cross_gpu_%j.err 
#------- modules ------- 
module purge 
module load gcc/8.3.0 cuda/10.1 cudnn/7.6.4 nccl/2.4.8 tensorrt/6.0.1 openmpi/4.0.1 atlas/3.10.3 scalapack/2.0.2 fftw/3.3.8 szip/2.1.1 ffmpeg/4.2.1 opencv/4.1.1 python/3.7.4_ML 
echo "== Starting run at $(date)" 
echo "== Job ID: ${SLURM_JOBID}" 
echo "== Job NPROCS: ${SLURM_NPROCS}" 
echo "== Job NNODES: ${SLURM_NNODES}" 
echo "== Node list: ${SLURM_NODELIST}" 
echo "== Submit dir. : ${SLURM_SUBMIT_DIR}" 
#------- srun ------ 
srun test_cross_gpu_logprob.py 
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='Flamefire' date='2020-06-08T18:46:00Z'>
		Which Tensorflow version are you using? Could you upgrade to 2.2?
		</comment>
		<comment id='8' author='Flamefire' date='2020-06-08T22:07:37Z'>
		I use TF 2.2.0 and TFP 0.9.0
		</comment>
		<comment id='9' author='Flamefire' date='2020-06-08T22:15:07Z'>
		Sorry the fix was not in 2.2. Could you try use nightly? If it's not possible, please wrap strategy.run() in a tf.function and pass a non tf.function to strategy.run
		</comment>
		<comment id='10' author='Flamefire' date='2020-06-08T22:23:11Z'>
		Sorry, I can't update to nightly. It's difficult to me understand your last sentence. Please, could you tell me how to do that using the code above?
		</comment>
		<comment id='11' author='Flamefire' date='2020-06-10T19:14:58Z'>
		Many thanks &lt;denchmark-link:https://github.com/crccw&gt;@crccw&lt;/denchmark-link&gt;
 !
&lt;denchmark-link:https://github.com/brianwa84&gt;@brianwa84&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/Flamefire&gt;@Flamefire&lt;/denchmark-link&gt;
 in CC.
In previous code I have commented @tf.function(autograph=False)  for  def_log_prob_and_grad. I have not commented @tf.funtcion in  def target_log_prob(loc, scale_tril): which contains strategy.run()
Now, I have not errors, but the process freezes. This also happens using MirroredStrategy with one machine and 4 GPUs.
Any clue?
# @tf.function(autograph=False) 
def log_prob_and_grad(loc, scale_tril, observations): 
    ctx = tf.distribute.get_replica_context() 
    with tf.GradientTape() as tape: 
        tape.watch((loc, scale_tril)) 
        lp = tf.reduce_sum(dist.log_prob(loc, scale_tril, observations)) / len(samples) 
    grad = tape.gradient(lp, (loc, scale_tril)) 
    return ctx.all_reduce('sum', lp), [ctx.all_reduce('sum', g) for g in grad] 

@tf.function(autograph=False) 
@tf.custom_gradient 
def target_log_prob(loc, scale_tril): 
    lp, grads = st.run(log_prob_and_grad, (loc, scale_tril, observations)) 
    return lp.values[0], lambda grad_lp: [grad_lp * g.values[0] for g in grads] 
singleton_vals = tfp.math.value_and_gradient(target_log_prob, (loc, scale_tril)) 
print(singleton_vals) 
print("*"*50) 
sys.exit() 

kernel = tfp.mcmc.HamiltonianMonteCarlo(target_log_prob, step_size=.35, num_leapfrog_steps=2) 
kernel = tfp.mcmc.TransformedTransitionKernel(kernel, bijector=[tfb.Identity(), tfb.FillScaleTriL()]) 

@tf.function(autograph=False) 
def sample_chain(): 
    return tfp.mcmc.sample_chain( 
        num_results=200, num_burnin_steps=100, 
        current_state=[tf.ones_like(loc), tf.linalg.eye(scale_tril.shape[-1])], 
        kernel=kernel, trace_fn=lambda _, kr: kr.inner_results.is_accepted) 

samps, is_accepted = sample_chain() 

print(f'accept rate: {np.mean(is_accepted)}') 
print(f'ess: {tfp.mcmc.effective_sample_size(samps)}') 
print(tf.reduce_mean(samps[0], axis=0)) 
print(tf.reduce_mean(samps[1], axis=0)) 
&lt;denchmark-code&gt;+-----------------------------------------------------------------------------+
| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000004:04:00.0 Off |                    0 |
| N/A   38C    P0    51W / 300W |  15360MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-SXM2...  On   | 00000004:05:00.0 Off |                    0 |
| N/A   40C    P0    50W / 300W |  15220MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-SXM2...  On   | 00000035:03:00.0 Off |                    0 |
| N/A   37C    P0    50W / 300W |  15220MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-SXM2...  On   | 00000035:04:00.0 Off |                    0 |
| N/A   40C    P0    55W / 300W |  15220MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0    129007      C   /apps/PYTHON/3.7.4_ML/bin/python           15349MiB |
|    1    129007      C   /apps/PYTHON/3.7.4_ML/bin/python           15209MiB |
|    2    129007      C   /apps/PYTHON/3.7.4_ML/bin/python           15209MiB |
|    3    129007      C   /apps/PYTHON/3.7.4_ML/bin/python           15209MiB |
+-----------------------------------------------------------------------------+
&lt;/denchmark-code&gt;

		</comment>
		<comment id='12' author='Flamefire' date='2020-06-11T15:54:22Z'>
		I am not experienced w/ slurm, though I might be interested enough to learn and get a GCP example set up. I don't expect to have cycles to do so imminently though.
Meantime you might find fellow users with experience in &lt;denchmark-link:https://groups.google.com/forum/#!forum/google-cloud-slurm-discuss&gt;https://groups.google.com/forum/#!forum/google-cloud-slurm-discuss&lt;/denchmark-link&gt;
 It seems GCP does support slurm with GPUs. &lt;denchmark-link:https://cloud.google.com/blog/products/compute/hpc-made-easy-announcing-new-features-for-slurm-on-gcp&gt;https://cloud.google.com/blog/products/compute/hpc-made-easy-announcing-new-features-for-slurm-on-gcp&lt;/denchmark-link&gt;
 so I'd think there may be a user on there with the relevant experience.
		</comment>
		<comment id='13' author='Flamefire' date='2020-06-15T07:29:19Z'>
		Many thanks &lt;denchmark-link:https://github.com/brianwa84&gt;@brianwa84&lt;/denchmark-link&gt;
 !
For me this is frustrating. I've spent three weeks trying to run your &lt;denchmark-link:https://colab.research.google.com/github/tensorflow/probability/blob/master/discussion/examples/cross_gpu_logprob.ipynb&gt;notebook&lt;/denchmark-link&gt;
 using multiple GPUs, and I get no advance. I'm afraid I'm not asking myself the right questions.
I'll ask to slurm-discuss forum.
		</comment>
	</comments>
</bug>