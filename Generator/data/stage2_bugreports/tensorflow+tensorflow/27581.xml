<bug id='27581' author='tarrade' open_date='2019-04-06T18:56:13Z' closed_time='2019-05-16T13:58:03Z'>
	<summary>keras model.predict_on_batch/model.test_on_batch with tf.dataset: dataset.make_initializable_iterator is not supported when eager execution is enabled.</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): yes
TensorFlow version (use command below): 2.0.0-alpha0
Python version: 3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

Describe the current behavior
best_model.test_on_batch(testing_dataset)
and
best_model.predict_on_batch(training_dataset)
crashing with the following error messages
"RuntimeError: dataset.make_initializable_iterator is not supported when eager execution is enabled."
but best_model.evaluate(testing_dataset,) is wokring.
Describe the expected behavior
Exact same code working with Tensorflow 1.12

All the code and llogs are here:
&lt;denchmark-link:https://github.com/tarrade/proj_DL_models_and_pipelines_with_GCP/blob/master/notebook/TF_2.0/05-Mnist_model_validation_and_interpretation.ipynb&gt;https://github.com/tarrade/proj_DL_models_and_pipelines_with_GCP/blob/master/notebook/TF_2.0/05-Mnist_model_validation_and_interpretation.ipynb&lt;/denchmark-link&gt;

Other info / logs
Here the full logs:
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

AttributeError                            Traceback (most recent call last)
~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py in make_initializable_iterator(dataset, shared_name)
1852     # some datasets (e.g. for prefetching) override its behavior.
-&gt; 1853     return dataset._make_initializable_iterator(shared_name)  # pylint: disable=protected-access
1854   except AttributeError:
AttributeError: 'PrefetchDataset' object has no attribute '_make_initializable_iterator'
During handling of the above exception, another exception occurred:
RuntimeError                              Traceback (most recent call last)
 in 
1 #!! Bug TF 2.0
----&gt; 2 score = best_model.test_on_batch(testing_dataset)
3
4 # print test accuracy
5 print('Loss:')
~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in test_on_batch(self, x, y, sample_weight, reset_metrics)
1310     # Validate and standardize user data.
1311     x, y, sample_weights = self._standardize_user_data(
-&gt; 1312         x, y, sample_weight=sample_weight, extract_tensors_from_dataset=True)
1313
1314     if self.run_eagerly:
~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)
2439       if extract_tensors_from_dataset:
2440         # We do this for train_on_batch/etc.
-&gt; 2441         x, y, sample_weight = training_utils.extract_tensors_from_dataset(x)
2442     elif isinstance(x, iterator_ops.Iterator):
2443       # Graph mode iterator. We extract the symbolic tensors.
~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py in extract_tensors_from_dataset(dataset)
1382     Tuple of tensors x, y, weights. y and weights entry may be None.
1383   """
-&gt; 1384   iterator = get_iterator(dataset)
1385   inputs, targets, sample_weight = unpack_iterator_input(iterator)
1386   return inputs, targets, sample_weight
~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py in get_iterator(dataset)
1362 def get_iterator(dataset):
1363   """Create and initialize an iterator from a dataset."""
-&gt; 1364   iterator = dataset_ops.make_initializable_iterator(dataset)
1365   initialize_iterator(iterator)
1366   return iterator
~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py in make_initializable_iterator(dataset, shared_name)
1853     return dataset._make_initializable_iterator(shared_name)  # pylint: disable=protected-access
1854   except AttributeError:
-&gt; 1855     return DatasetV1Adapter(dataset)._make_initializable_iterator(shared_name)  # pylint: disable=protected-access
1856
1857
~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py in _make_initializable_iterator(self, shared_name)
1495     if context.executing_eagerly():
1496       raise RuntimeError(
-&gt; 1497           "dataset.make_initializable_iterator is not supported when eager "
1498           "execution is enabled.")
1499     _ensure_same_dataset_graph(self)
RuntimeError: dataset.make_initializable_iterator is not supported when eager execution is enabled.
	</description>
	<comments>
		<comment id='1' author='tarrade' date='2019-04-09T17:55:34Z'>
		Did you upgrade your &lt;denchmark-link:https://www.tensorflow.org/alpha/guide/upgrade&gt;TF 1.X code to TensorFlow 2.0&lt;/denchmark-link&gt;
 before executing it in TF 2.0?
		</comment>
		<comment id='2' author='tarrade' date='2019-04-09T19:18:38Z'>
		&lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;
 yes, I migrated my python code using the script tf_upgrade_v2 and I use the tool tf2up.ml to help migrating the notebook. I will update the logs with the error because it was corrupted after I deleted some flags (the error messgae stay the same)
I am not saying that I didn't do something wrong on my side but do you see why the following will work:
best_model.evaluate(testing_dataset,)
and not
best_model.predict_on_batch(training_dataset)
both use the same best_model keras and the tf.dataset (migrated to TF 2.0).
&lt;denchmark-link:https://user-images.githubusercontent.com/12021701/55828465-cb4bfa00-5b0c-11e9-92a7-7358216a8322.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='tarrade' date='2019-04-10T11:14:25Z'>
		&lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;
 here some modifided code from Tensorflow 2.0 where you can easily reproduce the issue:
pip install tensorflow==2.0.0-alpha0
pip install tensorflow-datasets
the usage I am doing from "predict_on_btach" if conform to what is written in the doc:
&lt;denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model&gt;https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;import tensorflow as tf
import tensorflow_datasets as tfds
from absl import logging

logging.set_verbosity(logging.INFO)
# Define the estimator's input_fn
STEPS_PER_EPOCH = 5
#BUFFER_SIZE = 10 # Use a much larger value for real code. 
BATCH_SIZE = 64
NUM_EPOCHS = 5


def input_fn():
    datasets, ds_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
    mnist_train, mnist_test = datasets['train'], datasets['test']

    BUFFER_SIZE = 10000
    BATCH_SIZE = 64

    def scale(image, label):
        image = tf.cast(image, tf.float32)
        image /= 255
    
        return image, label[..., tf.newaxis]

    train_data = mnist_train.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
    return train_data.repeat()


def make_model():
    return tf.keras.Sequential([
        tf.keras.layers.Conv2D(32, 3, activation='relu',
                               kernel_regularizer=tf.keras.regularizers.l2(0.02),
                               input_shape=(28, 28, 1)),
        tf.keras.layers.MaxPooling2D(),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dropout(0.1),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(10, activation='softmax')
    ])

model = make_model()

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

print(model.summary())

training_dataset=input_fn()

print("train")
model.fit(training_dataset,
          steps_per_epoch=5,
          epochs=10,
          verbose = 1)

print("evaluate")
model.evaluate(training_dataset,
              steps=1)

print("predict on batch")
model.predict_on_batch(training_dataset)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='tarrade' date='2019-04-19T18:54:45Z'>
		&lt;denchmark-link:https://github.com/tarrade&gt;@tarrade&lt;/denchmark-link&gt;
 Apologies for the delay in response. Thanks for the minimal code snippet. I was able to reproduce your behavior. It executes successfully in TF 1.13.1 and fails at  with TF 2.0 alpha.
		</comment>
		<comment id='5' author='tarrade' date='2019-05-16T13:58:02Z'>
		&lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;
 I tested today with
tf-nightly-2.0-preview==2.0.0.dev20190516
and the issues are fixed for both :
model.predict_on_batch(training_dataset)
and
model.test_on_batch(training_dataset)
closing this ticket
		</comment>
		<comment id='6' author='tarrade' date='2019-05-16T13:58:04Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=27581&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=27581&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>