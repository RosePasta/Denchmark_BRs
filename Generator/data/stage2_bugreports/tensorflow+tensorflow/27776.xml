<bug id='27776' author='matthen' open_date='2019-04-12T06:36:44Z' closed_time='2019-07-09T20:48:34Z'>
	<summary>Cannot compute gradients in graphs using RaggedTensors</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac 10.14
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
TensorFlow installed from (source or binary): pip install tensorflow==1.13.1
TensorFlow version (use command below): 1.13.1
Python version: 3.6
Bazel version (if compiling from source): n/a
GCC/Compiler version (if compiling from source): n/a
CUDA/cuDNN version: n/a
GPU model and memory: n/a

Describe the current behavior
Computing gradients for some graphs involving RaggedTensors causes an error
Describe the expected behavior
The gradients should work. This looks similar to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/26015&gt;#26015&lt;/denchmark-link&gt;

Code to reproduce the issue
num_buckets = 10
embedding_size = 3

texts = tf.constant(["hello how", "are you ?"])
ragged = tf.RaggedTensor.from_sparse(tf.strings.split(texts))
ragged_ids = tf.ragged.map_flat_values(
    tf.string_to_hash_bucket,
    ragged,
    num_buckets
)
embeddings = tf.get_variable("embeddings", shape=[num_buckets, embedding_size])
ragged_embeddings = tf.ragged.map_flat_values(
  tf.nn.embedding_lookup, embeddings, ragged_ids
)

variable = tf.get_variable(
    "variable",
    [1, 1, embedding_size]
)

ragged_embeddings_1 = tf.RaggedTensor.from_row_splits(
    values=(ragged_embeddings + variable),
    row_splits=ragged_embeddings.row_splits,
)

ragged_embeddings_2 = tf.concat([ragged_embeddings, ragged_embeddings], 1)


sess.run(tf.global_variables_initializer())

def test_grad(ragged_tensor):
  sess.run(ragged_tensor)
  grad, = tf.gradients([ragged_tensor.to_tensor()], [embeddings])
  sess.run(grad)

# No error
test_grad(ragged_embeddings)

# Raises InvalidArgumentError: Operation 'RaggedTile/Tile' has no attr named '_XlaCompile'.
test_grad(ragged_embeddings_1)

# Raises LookupError: gradient registry has no entry for: RaggedGather
test_grad(ragged_embeddings_2)
Other info / logs
Full error for grad of ragged_embeddings_1:
&lt;denchmark-code&gt;/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in get_attr(self, name)
   2408       with c_api_util.tf_buffer() as buf:
-&gt; 2409         c_api.TF_OperationGetAttrValueProto(self._c_op, name, buf)
   2410         data = c_api.TF_GetBuffer(buf)

InvalidArgumentError: Operation 'RaggedTile/Tile' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py in _MaybeCompile(scope, op, func, grad_fn)
    414     try:
--&gt; 415       xla_compile = op.get_attr("_XlaCompile")
    416       xla_separate_compiled_gradients = op.get_attr(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in get_attr(self, name)
   2412       # Convert to ValueError for backwards compatibility.
-&gt; 2413       raise ValueError(str(e))
   2414     x = attr_value_pb2.AttrValue()

ValueError: Operation 'RaggedTile/Tile' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    454                 preferred_dtype=default_dtype,
--&gt; 455                 as_ref=input_arg.is_ref)
    456             if input_arg.number_attr and len(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in internal_convert_n_to_tensor(values, dtype, name, as_ref, preferred_dtype, ctx)
   1239             preferred_dtype=preferred_dtype,
-&gt; 1240             ctx=ctx))
   1241   return ret

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors)
   1174     if ret is None:
-&gt; 1175       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1176 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _TensorTensorConversionFunction(t, dtype, name, as_ref)
    976         "Tensor conversion requested dtype %s for Tensor with dtype %s: %r" %
--&gt; 977         (dtype.name, t.dtype.name, str(t)))
    978   return t

ValueError: Tensor conversion requested dtype int64 for Tensor with dtype int32: 'Tensor("gradients/RaggedTile/Tile_grad/Shape:0", shape=(2,), dtype=int32)'

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
&lt;ipython-input-38-299d0ab83362&gt; in &lt;module&gt;()
     38   sess.run(grad)
     39 
---&gt; 40 test_grad(ragged_embeddings_1)

&lt;ipython-input-38-299d0ab83362&gt; in test_grad(ragged_tensor)
     35 def test_grad(ragged_tensor):
     36   sess.run(ragged_tensor)
---&gt; 37   grad, = tf.gradients([ragged_tensor.to_tensor()], [embeddings])
     38   sess.run(grad)
     39 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)
    662     return _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops,
    663                             gate_gradients, aggregation_method, stop_gradients,
--&gt; 664                             unconnected_gradients)
    665 
    666 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)
    963                 # functions.
    964                 in_grads = _MaybeCompile(grad_scope, op, func_call,
--&gt; 965                                          lambda: grad_fn(op, *out_grads))
    966               else:
    967                 # For function call ops, we add a 'SymbolicGradient'

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py in _MaybeCompile(scope, op, func, grad_fn)
    418       xla_scope = op.get_attr("_XlaScope").decode()
    419     except ValueError:
--&gt; 420       return grad_fn()  # Exit early
    421 
    422   if not xla_compile:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py in &lt;lambda&gt;()
    963                 # functions.
    964                 in_grads = _MaybeCompile(grad_scope, op, func_call,
--&gt; 965                                          lambda: grad_fn(op, *out_grads))
    966               else:
    967                 # For function call ops, we add a 'SymbolicGradient'

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py in _TileGrad(op, grad)
    579   #   axes = [0, 2, 4]
    580   split_shape = array_ops.reshape(
--&gt; 581       array_ops.transpose(array_ops.stack([op.inputs[1], input_shape])), [-1])
    582   axes = math_ops.range(0, array_ops.size(split_shape), 2)
    583   # Sum reduces grad along the first dimension for IndexedSlices

/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)
    178     """Call target, and fall back on dispatchers if there is a TypeError."""
    179     try:
--&gt; 180       return target(*args, **kwargs)
    181     except (TypeError, ValueError):
    182       # Note: convert_to_eager_tensor currently raises a ValueError, not a

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py in stack(values, axis, name)
   1003                                                       expanded_num_dims))
   1004 
-&gt; 1005   return gen_array_ops.pack(values, axis=axis, name=name)
   1006 
   1007 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py in pack(values, axis, name)
   5446   axis = _execute.make_int(axis, "axis")
   5447   _, _, _op = _op_def_lib._apply_op_helper(
-&gt; 5448         "Pack", values=values, axis=axis, name=name)
   5449   _result = _op.outputs[:]
   5450   _inputs_flat = _op.inputs

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    481                                 (prefix, dtype.name))
    482               else:
--&gt; 483                 raise TypeError("%s that don't all match." % prefix)
    484             else:
    485               raise TypeError(

TypeError: Tensors in list passed to 'values' of 'Pack' Op have types [int64, int32] that don't all match.
&lt;/denchmark-code&gt;

Full error for ragged_embeddings_2:
&lt;denchmark-code&gt;LookupError                               Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)
    906           try:
--&gt; 907             grad_fn = ops.get_gradient_function(op)
    908           except LookupError:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in get_gradient_function(op)
   2545     op_type = op.type
-&gt; 2546   return _gradient_registry.lookup(op_type)
   2547 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/registry.py in lookup(self, name)
     93       raise LookupError(
---&gt; 94           "%s registry has no entry for: %s" % (self._name, name))

LookupError: gradient registry has no entry for: RaggedGather

During handling of the above exception, another exception occurred:

LookupError                               Traceback (most recent call last)
&lt;ipython-input-39-8a8bafd157d6&gt; in &lt;module&gt;()
     38   sess.run(grad)
     39 
---&gt; 40 test_grad(ragged_embeddings_2)

&lt;ipython-input-39-8a8bafd157d6&gt; in test_grad(ragged_tensor)
     35 def test_grad(ragged_tensor):
     36   sess.run(ragged_tensor)
---&gt; 37   grad, = tf.gradients([ragged_tensor.to_tensor()], [embeddings])
     38   sess.run(grad)
     39 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)
    662     return _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops,
    663                             gate_gradients, aggregation_method, stop_gradients,
--&gt; 664                             unconnected_gradients)
    665 
    666 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)
    921               raise LookupError(
    922                   "No gradient defined for operation '%s' (op type: %s)" %
--&gt; 923                   (op.name, op.type))
    924         if loop_state:
    925           loop_state.EnterGradWhileContext(op, before=False)

LookupError: No gradient defined for operation 'RaggedConcat/RaggedGather/RaggedGather' (op type: RaggedGather)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='matthen' date='2019-04-13T13:32:13Z'>
		I found that this has been fixed in the nightly builds.
		</comment>
		<comment id='2' author='matthen' date='2019-04-13T14:23:48Z'>
		In 1.14.1-dev20190413, ragged_embeddings_1 is fixed, but ragged_embeddings_2 is not. That is the _XlaCompile error goes away, but apparently RaggedGather still has no registered gradient
		</comment>
		<comment id='3' author='matthen' date='2019-04-13T16:02:25Z'>
		Yes, you're right, the second error still exists. I've had to avoid using tf.concat on ragged tensors.
		</comment>
		<comment id='4' author='matthen' date='2019-04-15T13:26:43Z'>
		Also been having trouble with this issue. I have written a quick patch for the RaggedGather:
&lt;denchmark-code&gt;@tf.RegisterGradient("RaggedGather")
def _ragged_gather_grad(gather_op, unused,out_grads):
    *params_nested_splits, params_dense_values, ragged_indices = list(
        gather_op.inputs
    )
    if len(params_nested_splits) &gt; 1:
        raise NotImplementedError(
            'Backward pass for nested RaggedTensors not supported'
        )
    output_nested_splits, output_dense_values = gather_op.outputs

    # Turn the ragged indices into flat indices
    flat_indices = tf.RaggedTensor.from_nested_row_splits(
        tf.range(tf.shape(grads)[0]), [output_nested_splits]
    )
    flat_indices = tf.gather(flat_indices, ragged_indices).flat_values

    return [
        None,
        tf.IndexedSlices(
            values=grads, indices=flat_indices,
            dense_shape=tf.shape(params_dense_values)
        ),
        None
    ]
&lt;/denchmark-code&gt;

The ragged_gather op is found in tensorflow/python/ops/gen_ragged_array_ops.py
This is working for concats along a ragged dimension. I've only tested with ragged_rank=1
Happy to work this into a future PR.
		</comment>
		<comment id='5' author='matthen' date='2019-06-03T00:57:17Z'>
		Thanks &lt;denchmark-link:https://github.com/coopie&gt;@coopie&lt;/denchmark-link&gt;
!
output_ragged = tf.gather(input_ragged, indices)
Your code works perfect for input_ragged ragged_rank is 1 and indices is 1D.
Just a typo want to remind, all grads inside the function should be out_grads
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Updated: model is able to running now, but compared to convert it to tensor and then gather, RaggedGather does not converge.
&lt;denchmark-code&gt;output_tensor = tf.gather(input_ragged.to_tensor(), indices)
&lt;/denchmark-code&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Updated: I figured it out, there is a bug here
    # Turn the ragged indices into flat indices
    flat_indices = tf.RaggedTensor.from_nested_row_splits(
        tf.range(tf.shape(grads)[0]), [output_nested_splits]
    )
which should be
    # Turn the ragged indices into flat indices
    flat_indices = tf.RaggedTensor.from_nested_row_splits(
        tf.range(tf.shape(out_grads)[0]), [params_nested_splits[0]]
    )
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

the whole code
@tf.RegisterGradient("RaggedGather")
def _ragged_gather_grad(gather_op, unused, out_grads):
    *params_nested_splits, params_dense_values, ragged_indices = list(
        gather_op.inputs
    )
    if len(params_nested_splits) &gt; 1:
        raise NotImplementedError(
            'Backward pass for nested RaggedTensors not supported'
        )
    output_nested_splits, output_dense_values = gather_op.outputs

    # Turn the ragged indices into flat indices
    flat_indices = tf.RaggedTensor.from_nested_row_splits(
        tf.range(tf.shape(out_grads)[0]), [params_nested_splits[0]]
    )
    flat_indices = tf.gather(flat_indices, ragged_indices).flat_values

    return [
        None,
        tf.IndexedSlices(
            values=out_grads, indices=flat_indices,
            dense_shape=tf.shape(params_dense_values)
        ),
        None
    ]
then it could converge
&lt;denchmark-code&gt;Epoch 1/5
289/288 [==============================] - 171s 591ms/step - loss: 0.2502 - rmse: 302.2978 - val_loss: 0.0024 - val_rmse: 20.7268
Epoch 2/5
289/288 [==============================] - 85s 295ms/step - loss: 0.0015 - rmse: 17.3415 - val_loss: 0.0013 - val_rmse: 15.1368
Epoch 3/5
289/288 [==============================] - 85s 295ms/step - loss: 9.2807e-04 - rmse: 13.5158 - val_loss: 8.3251e-04 - val_rmse: 12.1445
Epoch 4/5
289/288 [==============================] - 84s 291ms/step - loss: 6.6616e-04 - rmse: 11.4514 - val_loss: 6.6404e-04 - val_rmse: 10.8465
Epoch 5/5
289/288 [==============================] - 88s 303ms/step - loss: 5.0972e-04 - rmse: 10.0171 - val_loss: 4.9283e-04 - val_rmse: 9.3444
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='matthen' date='2019-07-09T20:48:34Z'>
		Fixed in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/f7415d1efbe544c03107141eb3cc73714c5276bf&gt;f7415d1&lt;/denchmark-link&gt;
.  Gradient is now defined for RaggedGather (for any inputs, including ragged_rank&gt;1 and indices.ndims&gt;1).
		</comment>
		<comment id='7' author='matthen' date='2019-07-09T20:48:36Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=27776&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=27776&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>