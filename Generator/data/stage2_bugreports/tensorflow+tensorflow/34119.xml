<bug id='34119' author='moberweger' open_date='2019-11-09T07:53:17Z' closed_time='2020-03-24T08:54:52Z'>
	<summary>TF2.0 Calling set_session before fit_generator causes training to freeze when using multiprocessing</summary>
	<description>
System information

Have I written custom code: No
OS Platform and Distribution: Linux Ubuntu 16.04
Mobile device if the issue happens on mobile device: N/A
TensorFlow installed from: binary
TensorFlow version: v2.0.0-rc2-26-g64c3d38 2.0.0
Python version: 3.6
Bazel version: N/A
GCC/Compiler version: N/A
CUDA/cuDNN version: 10.0 / 7
GPU model and memory: GTX 1080Ti


I am trying to modify TF session parameters in combination with Keras (e.g. the allowed GPU memory) and use  to store these parameters. However, if  is called  before , it causes the training to freeze when using multiprocessing. To reproduce the error use the following code. The Colab gist to reproduce the error can be found &lt;denchmark-link:https://colab.research.google.com/gist/moberweger/2553560a5deeb2eaa5e3cfc23516ef34/untitled217.ipynb&gt;here&lt;/denchmark-link&gt;
. The colab execution of the code in question is not responding, so I stopped executing after 10 min, which ultimately gives me the error. This problems seems to occur also in 1.14, but not 1.15 !? as indicated in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/33973&gt;#33973&lt;/denchmark-link&gt;
. The gist might work when executed the first time, but the error can be reproduce after running the script a few times.
Describe the expected behavior
No freeze should occur.
Code to reproduce the issue
import numpy as np
import tensorflow as tf
import tensorflow.keras.backend as K
from tensorflow.keras.layers import Input, Conv2D
from tensorflow.keras.models import Model
from tensorflow.keras.utils import Sequence


def get_model(input_shape, output_shape, compile_batch_size):
    assert K.image_data_format() == 'channels_last'
    m_input = Input(shape=input_shape, batch_size=compile_batch_size)
    m_output = Conv2D(output_shape[-1], 1, activation=None)(m_input)
    model = Model(inputs=m_input, outputs=m_output)
    return model


class DataGenerator(Sequence):
    def __init__(self, batch_size, image_size, num_batches):
        assert K.image_data_format() == 'channels_last'

        self.batch_size = batch_size
        self.image_size = image_size
        self.num_batches = num_batches
        self.on_epoch_end()

    def __len__(self):
        assert self.num_batches &gt; 0
        return self.num_batches

    def __getitem__(self, index):
        return np.zeros((self.batch_size,) + self.image_size).astype('float32'), np.zeros(
            (self.batch_size,) + self.image_size).astype('float32')

    def on_epoch_end(self):
        pass


if __name__ == '__main__':
    print(tf.version.GIT_VERSION, tf.version.VERSION)

    # set to True to trigger infinite wait
    if True:
        # limit GPU memory
        config = tf.compat.v1.ConfigProto()
        config.gpu_options.allow_growth = True
        config.gpu_options.per_process_gpu_memory_fraction = 0.5
        tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))

    inp_shape = (128, 64, 2)
    outp_shape = (128, 64, 2)
    batch_size = 16

    gen = DataGenerator(batch_size, inp_shape, 10)
    model = get_model(inp_shape, outp_shape, batch_size)
    model.summary()
    model.compile(loss=["mse"], optimizer="adam", metrics=["accuracy"])
    model.fit_generator(gen, steps_per_epoch=10, epochs=5, workers=2, use_multiprocessing=True)
Other info / logs
&lt;denchmark-code&gt;Traceback (most recent call last)

&lt;ipython-input-4-26fa57f6f7f8&gt; in &lt;module&gt;()
     55     model.summary()
     56     model.compile(loss=["mse"], optimizer="adam", metrics=["accuracy"])
---&gt; 57     model.fit_generator(gen, steps_per_epoch=10, epochs=5, workers=2, use_multiprocessing=True)

7 frames

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1295         shuffle=shuffle,
   1296         initial_epoch=initial_epoch,
-&gt; 1297         steps_name='steps_per_epoch')
   1298 
   1299   def evaluate_generator(self,

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)
    219     step = 0
    220     while step &lt; target_steps:
--&gt; 221       batch_data = _get_next_batch(generator)
    222       if batch_data is None:
    223         if is_dataset:

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_generator.py in _get_next_batch(generator)
    361   """Retrieves the next batch of input data."""
    362   try:
--&gt; 363     generator_output = next(generator)
    364   except (StopIteration, errors.OutOfRangeError):
    365     return None

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/utils/data_utils.py in get(self)
    777     try:
    778       while self.is_running():
--&gt; 779         inputs = self.queue.get(block=True).get()
    780         self.queue.task_done()
    781         if inputs is not None:

/usr/lib/python3.6/multiprocessing/pool.py in get(self, timeout)
    636 
    637     def get(self, timeout=None):
--&gt; 638         self.wait(timeout)
    639         if not self.ready():
    640             raise TimeoutError

/usr/lib/python3.6/multiprocessing/pool.py in wait(self, timeout)
    633 
    634     def wait(self, timeout=None):
--&gt; 635         self._event.wait(timeout)
    636 
    637     def get(self, timeout=None):

/usr/lib/python3.6/threading.py in wait(self, timeout)
    549             signaled = self._flag
    550             if not signaled:
--&gt; 551                 signaled = self._cond.wait(timeout)
    552             return signaled
    553 

/usr/lib/python3.6/threading.py in wait(self, timeout)
    293         try:    # restore state no matter what (e.g., KeyboardInterrupt)
    294             if timeout is None:
--&gt; 295                 waiter.acquire()
    296                 gotit = True
    297             else:
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='moberweger' date='2020-03-23T21:22:20Z'>
		&lt;denchmark-link:https://github.com/moberweger&gt;@moberweger&lt;/denchmark-link&gt;
 I think this was resolved in recent .  I ran your code with  and I cannot reproduce the error you had faced with . Please check the &lt;denchmark-link:https://colab.research.google.com/gist/jvishnuvardhan/951378d5d7eabb359453dd5557d75dab/untitled37.ipynb&gt;gist here&lt;/denchmark-link&gt;
.
There is a warning by the code that suggest you to update data pipelines with tf.data for better performance. Thanks!
WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.
Please close the issue if this was resolved for you. Thanks!
		</comment>
		<comment id='2' author='moberweger' date='2020-03-24T08:54:52Z'>
		Thanks &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 for checking.
Although your notebook does not show the specific error, it is still deadlocked, ie stuck at .
Seems that there is now a valid warning, and the Model.fit_generator interface is deprecated. So assuming there is no impact of this problem after the interface got removed in the future, I close this issue.
		</comment>
		<comment id='3' author='moberweger' date='2020-03-24T08:54:54Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34119&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34119&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='moberweger' date='2020-03-24T14:51:22Z'>
		&lt;denchmark-link:https://github.com/moberweger&gt;@moberweger&lt;/denchmark-link&gt;
 Code was taking longer time. So I cancelled after some time. Please open a new bug if you face any issues in updating input pipeline. Thanks!
		</comment>
	</comments>
</bug>