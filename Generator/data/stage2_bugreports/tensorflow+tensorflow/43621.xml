<bug id='43621' author='anth2o' open_date='2020-09-28T14:09:58Z' closed_time='2020-09-29T23:19:18Z'>
	<summary>Tensorflow 2.3 doesn't log batch metrics and learning rate in Tensorboard</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.14 or Ubuntu 18.04.3 (bug happens on both platforms)
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Nope
TensorFlow installed from (source or binary): pip
TensorFlow version (use command below): 2.3 (bug) / 2.2 (good behavior)
Python version: 3.6.8
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: None
GPU model and memory: None

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with:

TF 1.0: python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
TF 2.0: python -c "import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"

Describe the current behavior
I used to make custom training loops to fit my models, with a tensorboard callback to log metrics, losses and learning rate using the following command:
    callbacks.append(tf.keras.callbacks.TensorBoard(
        tensorboard_dir,
        profile_batch=0,
        update_freq=2, # write in tensorboard metrics and losses every 2 batches
        write_graph=False)
    )
Now, after upgrading tensorflow from 2.2 to 2.3, the same code doesn't log in tensorboard the metrics and losses one every 2 batches, and also the learning rate for each epoch (that I manage either by using tf.keras.callbacks.LearningRateScheduler
or tf.keras.callbacks.ReduceLROnPlateau).
I shared a link below to a Colab with a minimal example, and the tensorboard for a model trained in 2.2 and in 2.3.
Describe the expected behavior
Same behavior in tf 2.3 as in tf 2.2.
Standalone code to reproduce the issue
&lt;denchmark-link:https://colab.research.google.com/drive/16G5b0wfcG0SuNWrbQbv1pQfPbk6Ix9qU#scrollTo=7tuJE8iW_-NY&gt;https://colab.research.google.com/drive/16G5b0wfcG0SuNWrbQbv1pQfPbk6Ix9qU#scrollTo=7tuJE8iW_-NY&lt;/denchmark-link&gt;

Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='anth2o' date='2020-09-28T16:17:10Z'>
		It seems the behavior has been introduced by this commit: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/69565ec4003902794bc94e10ba5fe9469a0b3ae4&gt;69565ec&lt;/denchmark-link&gt;

The self._log_metrics(logs, prefix='batch_', step=train_batches) has been removed. For epoch logging this method def _log_epoch_metrics(self, epoch, logs) has been introduced.
It seems that there should have an automatic update with:
if self.update_freq == 'epoch':
      should_record = False
      writer = None
    else:
      should_record = lambda: math_ops.equal(step % self.update_freq, 0)

    summary_state.is_recording = should_record
    summary_state.writer = writer
    # TODO(b/151339474): Fix deadlock when not using .value() here.
    summary_ops_v2.set_step(step.value())
However, I really don't know how it works.
&lt;denchmark-link:https://github.com/omalleyt12&gt;@omalleyt12&lt;/denchmark-link&gt;
 do you have any information about this issue ?
Thanks
		</comment>
		<comment id='2' author='anth2o' date='2020-09-28T16:42:13Z'>
		Was able to reproduce the issue with TF v2.3 and TF-nightly, please find the gist of it &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/5151b55dfb1341da1329f858c3c49a8d/43621-tf-nightly.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='3' author='anth2o' date='2020-09-29T23:19:17Z'>
		
I used to make custom training loops to fit my models, with a tensorboard callback to log metrics, losses and learning rate using the following command:

Ah I understand now, the TensorBoard callback is designed for use with Model.fit. It still logs batch-level metrics when used in Model.fit. For a custom training loop, you should use lower-level tf.summary APIs to log batch-level summaries. The change from 2.2 to 2.3 made the batch-level summaries part of the Model.train_function rather than something that the TensorBoard callback creates itself. This resulted in a 2x improvement in speed for many small models in Model.fit, but it does have the side effect that calling TensorBoard.on_train_batch_end(my_batch, my_metrics) in a custom training loop will no longer log batch-level metrics
		</comment>
		<comment id='4' author='anth2o' date='2020-09-29T23:19:20Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43621&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43621&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>