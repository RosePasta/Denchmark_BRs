<bug id='27455' author='LynnHo' open_date='2019-04-03T09:07:50Z' closed_time='2019-04-09T21:52:57Z'>
	<summary>TF2.0 gradient problem of using tf.nn.relu in tf.keras.Model.</summary>
	<description>
System information

OS Platform and Distribution: Linux Ubuntu 18.04
TensorFlow installed from: binary
TensorFlow version: 2.0.0-alpha0
Python version: 3.6.8

Describe the current behavior
I built a keras model with only a tf.nn.relu, but the gradient seems to be None after being decorated by @tf.function
Code to reproduce the issue

tf.nn.relu + tf.keras.Model + @tf.function (this is the only case that produce None gradient)

import tensorflow as tf

z = tf.keras.Input(())
h = tf.nn.relu(z)
m = tf.keras.Model(z, h)

@tf.function
def f(x):  # with @tf.function
    with tf.GradientTape() as t:
        t.watch(x)
        z = m(x ** 2)
    return t.gradient(z, x)

print(f(tf.convert_to_tensor(10.0)))

&gt;&gt;&gt; None
1.2 tf.nn.relu + tf.keras.Model without @tf.function
def f(x):  # without @tf.function
    with tf.GradientTape() as t:
        t.watch(x)
        z = m(x ** 2)
    return t.gradient(z, x)

print(f(tf.convert_to_tensor(10.0)))

&gt;&gt;&gt; tf.Tensor(20.0, shape=(), dtype=float32)

tf.keras.layers.ReLU() + tf.keras.Model + @tf.function

import tensorflow as tf

z = tf.keras.Input(())
h = tf.keras.layers.ReLU()(z)
m = tf.keras.Model(z, h)

@tf.function
def f(x):  # with @tf.function
    with tf.GradientTape() as t:
        t.watch(x)
        z = m(x ** 2)
    return t.gradient(z, x)

print(f(tf.convert_to_tensor(10.0)))

&gt;&gt;&gt; tf.Tensor(20.0, shape=(), dtype=float32)
2.2 tf.keras.layers.ReLU() + tf.keras.Model without @tf.function
def f(x):  # without @tf.function
    with tf.GradientTape() as t:
        t.watch(x)
        z = m(x ** 2)
    return t.gradient(z, x)

print(f(tf.convert_to_tensor(10.0)))

&gt;&gt;&gt; tf.Tensor(20.0, shape=(), dtype=float32)

only tf.nn.relu

import tensorflow as tf
m = tf.nn.relu

@tf.function
def f(x):  # with @tf.function
    with tf.GradientTape() as t:
        t.watch(x)
        z = m(x ** 2)
    return t.gradient(z, x)

print(f(tf.convert_to_tensor(10.0)))

&gt;&gt;&gt; tf.Tensor(20.0, shape=(), dtype=float32)
So, I think its the problem between tf.nn.relu and tf.keras.Model? Besides, tf.nn.tanh has the same problem.
	</description>
	<comments>
		<comment id='1' author='LynnHo' date='2019-04-05T21:28:52Z'>
		&lt;denchmark-link:https://github.com/tomerk&gt;@tomerk&lt;/denchmark-link&gt;
 I think something is broken with the keras graph here since the tape isn't seeing it.
Can you take a look, or help triage to the right person?
		</comment>
		<comment id='2' author='LynnHo' date='2019-04-08T23:10:25Z'>
		I have a fix for this that will be submitted soon.
		</comment>
		<comment id='3' author='LynnHo' date='2019-04-09T21:52:58Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=27455&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=27455&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>