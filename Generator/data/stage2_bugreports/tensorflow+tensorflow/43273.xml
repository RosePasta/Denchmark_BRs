<bug id='43273' author='nlp-sudo' open_date='2020-09-16T18:05:43Z' closed_time='2020-09-17T09:56:58Z'>
	<summary>FP16 not working with NHNet</summary>
	<description>
I am able to run NHNet code with batch size 8 but the accuracy numbers are no way near to what are reported in the paper. So, I thought increasing the batch size might help. But due to GPU limitation, I am not able to do it. So I tried using Mixed Precision to train the model for larger batch. I tried the following 2 things:

Using tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt,"dynamic")
But this gives the following error.
AttributeError: 'LossScaleOptimizer' object has no attribute '_hypers_created'

2.I also tried using:  tf.keras.mixed_precision.experimental.set_policy('mixed_float16')
But that gives this error:
TypeError: Tensors in list passed to 'inputs' of 'Einsum' Op have types [float16, float32] that don't all match.
System information

Used exact same code from NHNet repo
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
TensorFlow installed from (source or binary): tf-nightly==2.4.0.dev20200724
TensorFlow version (use command below): v1.12.1-31004-g203aa8b634 2.2.0-dev20200501
Python version: 3.7.7
CUDA/cuDNN version: 10.1/7
GPU model and memory: TITAN RTX 24GB

Describe the current behavior
As soon as epoch 1 starts the code throws error as mentioned before.
Describe the expected behavior
The training should happen.
Standalone code to reproduce the issue
Exact same code mentioned here: &lt;denchmark-link:https://github.com/tensorflow/models/blob/master/official/nlp/nhnet/trainer.py&gt;https://github.com/tensorflow/models/blob/master/official/nlp/nhnet/trainer.py&lt;/denchmark-link&gt;

Just add one line after line no 146:
opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt,"dynamic")
	</description>
	<comments>
		<comment id='1' author='nlp-sudo' date='2020-09-16T23:23:51Z'>
		You need some adaptation also for the train_step. See:
&lt;denchmark-link:https://www.tensorflow.org/guide/mixed_precision?hl=en#training_the_model_with_a_custom_training_loop&gt;https://www.tensorflow.org/guide/mixed_precision?hl=en#training_the_model_with_a_custom_training_loop&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='nlp-sudo' date='2020-09-17T08:48:37Z'>
		The code actually uses model.fit and hence I didn't change anything. But I changed it now, but still its not helping.
My code looks like this:
&lt;denchmark-code&gt;class Trainer(tf.keras.Model):
  """A training only model."""

  def __init__(self, model, params):
    super(Trainer, self).__init__()
    self.model = model
    self.params = params
    self._num_replicas_in_sync = tf.distribute.get_strategy(
    ).num_replicas_in_sync

  def call(self, inputs, mode="train"):
    return self.model(inputs, mode)

  def train_step(self, inputs):
    """The logic for one training step."""
    with tf.GradientTape() as tape:
      logits, _, _ = self(inputs, mode="train", training=True)
      targets = models.remove_sos_from_seq(inputs["target_ids"],
                                           self.params.pad_token_id)
      loss = transformer_metrics.transformer_loss(logits, targets,
                                                  self.params.label_smoothing,
                                                  self.params.vocab_size)
      # Scales the loss, which results in using the average loss across all
      # of the replicas for backprop.
      scaled_loss = self.optimizer.get_scaled_loss(loss) / self._num_replicas_in_sync

    tvars = self.trainable_variables
    grads = self.optimizer.get_unscaled_gradients(tape.gradient(scaled_loss, tvars))
    self.optimizer.apply_gradients(list(zip(grads, tvars)))
    return {
        "training_loss": loss,
        "learning_rate": self.optimizer._decayed_lr(var_dtype=tf.float32)
    }


def train(params, strategy, dataset=None):
  """Runs training."""

  if not dataset:
    dataset = input_pipeline.get_input_dataset(
        FLAGS.train_file_pattern,
        FLAGS.train_batch_size,
        params,
        is_training=True,
        strategy=strategy)

  with strategy.scope():
    model = models.create_model(
        FLAGS.model_type, params, init_checkpoint=FLAGS.init_checkpoint)
    opt = tf.keras.optimizers.Adam(learning_rate=params.learning_rate)
    opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt,"dynamic")
    trainer = Trainer(model, params)
    model.global_step = opt.iterations

    trainer.compile(
        optimizer=opt,
        experimental_steps_per_execution=FLAGS.steps_per_loop)
    summary_dir = os.path.join(FLAGS.model_dir, "summaries")
    summary_callback = tf.keras.callbacks.TensorBoard(
        summary_dir, update_freq=max(100, FLAGS.steps_per_loop))
    checkpoint = tf.train.Checkpoint(model=model, optimizer=opt)
    checkpoint_manager = tf.train.CheckpointManager(
        checkpoint,
        directory=FLAGS.model_dir,
        max_to_keep=10,
        step_counter=model.global_step,
        checkpoint_interval=FLAGS.checkpoint_interval)
    if checkpoint_manager.restore_or_initialize():
      logging.info("Training restored from the checkpoints in: %s",
                   FLAGS.model_dir)
    checkpoint_callback = keras_utils.SimpleCheckpoint(checkpoint_manager)

  # Trains the model.
  steps_per_epoch = min(FLAGS.train_steps, FLAGS.checkpoint_interval)
  epochs = FLAGS.train_steps // steps_per_epoch
  history = trainer.fit(
      x=dataset,
      steps_per_epoch=steps_per_epoch,
      epochs=epochs,
      callbacks=[summary_callback, checkpoint_callback],
      verbose=2)
  train_hist = history.history
  # Gets final loss from training.
  stats = dict(training_loss=float(train_hist["training_loss"][-1]))
  return stats
&lt;/denchmark-code&gt;

This still gives the same error:  AttributeError: 'LossScaleOptimizer' object has no attribute '_hypers_created.
If I remove "learning_rate": self.optimizer._decayed_lr(var_dtype=tf.float32) in the return, it throws this error:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "trainer.py", line 237, in &lt;module&gt;
    app.run(main)
  File "/home/rishabh/.local/lib/python3.7/site-packages/absl/app.py", line 299, in run
    _run_main(main, args)
  File "/home/rishabh/.local/lib/python3.7/site-packages/absl/app.py", line 250, in _run_main
    sys.exit(main(argv))
  File "trainer.py", line 231, in main
    stats = run()
  File "trainer.py", line 210, in run
    stats = train(params, strategy)
  File "trainer.py", line 176, in train
    verbose=2)
  File "/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py", line 72, in _method_wrapper
    return method(self, *args, **kwargs)
  File "/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py", line 907, in fit
    tmp_logs = train_function(iterator)
  File "/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py", line 766, in __call__
    result = self._call(*args, **kwds)
  File "/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py", line 809, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File "/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py", line 688, in _initialize
    *args, **kwds))
  File "/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 2902, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File "/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 3232, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File "/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 3121, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File "/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py", line 981, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py", line 595, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File "/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py", line 964, in wrapper
    user_requested=True,
  File "/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py", line 585, in converted_call
    return _fall_back_unconverted(f, args, kwargs, options, e)
  File "/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py", line 393, in _fall_back_unconverted
    return _call_unconverted(f, args, kwargs, options)
  File "/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py", line 343, in _call_unconverted
    return f(*args, **kwargs)
  File "/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py", line 627, in train_function
    for _ in math_ops.range(self._steps_per_execution - 1):
  File "/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/framework/ops.py", line 552, in __iter__
    self._disallow_iteration()
  File "/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/framework/ops.py", line 545, in _disallow_iteration
    self._disallow_when_autograph_enabled("iterating over `tf.Tensor`")
  File "/home/rishabh/.conda/envs/condaenvNH/lib/python3.7/site-packages/tensorflow/python/framework/ops.py", line 523, in _disallow_when_autograph_enabled
    " decorating it directly with @tf.function.".format(task))
tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.
&lt;/denchmark-code&gt;

Also beyond this, I don't think FP16 will work directly as the NHNet model has many custom layers which are not getting converted. When I run the code I get following warnings:
&lt;denchmark-code&gt;W0917 14:16:15.056954 140018629465920 ag_logging.py:146] AutoGraph could not transform &lt;bound method TransformerDecoder.call of &lt;official.nlp.nhnet.decoder.TransformerDecoder object at 0x7f573c702350&gt;&gt; and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform &lt;bound method TransformerDecoderLayer.call of &lt;official.nlp.modeling.layers.transformer.TransformerDecoderLayer object at 0x7f573c67f750&gt;&gt; and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0917 14:16:15.077228 140018629465920 ag_logging.py:146] AutoGraph could not transform &lt;bound method TransformerDecoderLayer.call of &lt;official.nlp.modeling.layers.transformer.TransformerDecoderLayer object at 0x7f573c67f750&gt;&gt; and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform &lt;bound method CachedAttention.call of &lt;official.nlp.modeling.layers.attention.CachedAttention object at 0x7f573c63ead0&gt;&gt; and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0917 14:16:15.085375 140018629465920 ag_logging.py:146] AutoGraph could not transform &lt;bound method CachedAttention.call of &lt;official.nlp.modeling.layers.attention.CachedAttention object at 0x7f573c63ead0&gt;&gt; and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform &lt;bound method MultiChannelAttention.call of &lt;official.nlp.modeling.layers.multi_channel_attention.MultiChannelAttention object at 0x7f573c60af90&gt;&gt; and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0917 14:16:15.138240 140018629465920 ag_logging.py:146] AutoGraph could not transform &lt;bound method MultiChannelAttention.call of &lt;official.nlp.modeling.layers.multi_channel_attention.MultiChannelAttention object at 0x7f573c60af90&gt;&gt; and will run it as-is.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='nlp-sudo' date='2020-09-17T09:01:04Z'>
		&lt;denchmark-link:https://github.com/nlp-sudo&gt;@nlp-sudo&lt;/denchmark-link&gt;

i see few issues with similar error, could you please refer to them if they are helpful.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/37144&gt;#37144&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/35101&gt;#35101&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://stackoverflow.com/questions/60268178/error-creating-universal-sentence-encoder-embeddings-using-beam-tf-transform&gt;link&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='nlp-sudo' date='2020-09-17T09:05:36Z'>
		I will handle the warning part later, that's not a big deal. It would be really helpful if you can help me out to solve the error:
AttributeError: 'LossScaleOptimizer' object has no attribute '_hypers_created.
I checked the optimizer code of keras too, it does have _hypers_created but I don't know why the error is occurring. And as I said, if I ignore the decaying of learning rate, the other error is coming. Solution to anyone of it would be really helpful.
		</comment>
		<comment id='5' author='nlp-sudo' date='2020-09-17T09:18:10Z'>
		Can you share a very, very minimal but complete runnable example or Colab (better) to reproduce this?
		</comment>
		<comment id='6' author='nlp-sudo' date='2020-09-17T09:23:39Z'>
		Also I see some explicit  in the model definition at &lt;denchmark-link:https://github.com/tensorflow/models/blob/master/official/nlp/nhnet/models.py&gt;https://github.com/tensorflow/models/blob/master/official/nlp/nhnet/models.py&lt;/denchmark-link&gt;
.
So probably it is better that you open a ticket directly in &lt;denchmark-link:https://github.com/tensorflow/models/&gt;https://github.com/tensorflow/models/&lt;/denchmark-link&gt;
  repository
		</comment>
		<comment id='7' author='nlp-sudo' date='2020-09-17T09:36:19Z'>
		I thought the issue was related to FP16 in Keras and hence opened it here. No issues, I will open a issue in that repo. Thanks for help.
		</comment>
		<comment id='8' author='nlp-sudo' date='2020-09-17T09:44:12Z'>
		&lt;denchmark-link:https://github.com/nlp-sudo&gt;@nlp-sudo&lt;/denchmark-link&gt;

PLease move the issue to closed status, once open in mentioned repo.
		</comment>
		<comment id='9' author='nlp-sudo' date='2020-09-17T09:56:58Z'>
		Opened an issue here: &lt;denchmark-link:https://github.com/tensorflow/models/issues/9262&gt;tensorflow/models#9262&lt;/denchmark-link&gt;

Closing this one.
		</comment>
		<comment id='10' author='nlp-sudo' date='2020-09-17T09:57:00Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43273&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43273&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>