<bug id='27949' author='cottrell' open_date='2019-04-18T09:52:17Z' closed_time='2019-06-24T18:08:00Z'>
	<summary>Tensorflow 2.0: No gradients provided for any variable but only when tf.math.square AND tf.function is used?</summary>
	<description>
I posted this in SO
&lt;denchmark-link:https://stackoverflow.com/questions/55730930/tensorflow-2-0-no-gradients-provided-for-any-variable-but-only-when-tf-math-squ&gt;https://stackoverflow.com/questions/55730930/tensorflow-2-0-no-gradients-provided-for-any-variable-but-only-when-tf-math-squ&lt;/denchmark-link&gt;

but now am wondering if it is actually a bug.
&lt;denchmark-code&gt;In [176]: sys.version
Out[176]: '3.7.3 (default, Mar 27 2019, 16:54:48) \n[Clang 4.0.1 (tags/RELEASE_401/final)]'

In [177]: tf.__version__
Out[177]: '2.0.0-alpha0'
&lt;/denchmark-code&gt;

import tensorflow as tf
import tensorflow.keras as keras
import tensorflow.keras.backend as K
import numpy as np

m = 1000
n = 1
X = np.random.randn(m, n).astype(np.float32)
y = (3 + 0 * np.random.randn(m)).astype(np.float32)

def create_model():
    a_input = keras.layers.Input(shape=(n,), dtype=np.float32)
    a = K.expand_dims(a_input, axis=2)
    q = keras.layers.Conv1D(1, 1)(a)
    q = - tf.math.square(q) # this breaks things, but only when using tf.function
    model = keras.models.Model(inputs=a_input, outputs=q)
    return model

model = create_model()
model.predict(X)

class Trainer():
    def __init__(self, epochs=10):
        self.epochs = epochs
        self.model = create_model()
        self.optimizer = tf.optimizers.Adam()
        self.step = 0
    def train(self, X, y, epochs=10):
        X = tf.convert_to_tensor(X, dtype=tf.float32)
        y = tf.convert_to_tensor(y, dtype=tf.float32)
        for epoch in range(epochs):
            l = self._train_one_step(X, y)
        return l
    @tf.function
    def _train_one_step(self, X, y):
        with tf.GradientTape() as tape:
            yp = self.model(X)
            loss = tf.reduce_mean(tf.math.square(y - yp))
        gradients = tape.gradient(loss, self.model.trainable_variables)
        l = self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))
        d = dict(loss=loss)
        tf.print(yp[0], loss)
        self.step += 1

trainer = Trainer()
l = trainer.train(X, y, epochs=100)
&lt;denchmark-code&gt;lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in _filter_grads(grads_and_vars)
    922   if not filtered:
    923     raise ValueError("No gradients provided for any variable: %s." %
--&gt; 924                      ([v.name for _, v in grads_and_vars],))
    925   if vars_with_empty_grads:
    926     logging.warning(

ValueError: No gradients provided for any variable: ['conv1d_47/kernel:0', 'conv1d_47/bias:0'].
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='cottrell' date='2019-04-30T23:13:52Z'>
		&lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;
 I think what's happening here is that the tf.math.square not wrapped in a lambda layer is messing the functional model's ability to be called, so self.model(X) leaves unconnected inputs and outputs.
		</comment>
		<comment id='2' author='cottrell' date='2019-05-13T15:42:16Z'>
		I had the same issue with the tf.math.pow function as well. But the issue disappears if you put the tf.math operation inside the function with the @tf.function annotation.
		</comment>
		<comment id='3' author='cottrell' date='2019-05-13T16:46:49Z'>
		&lt;denchmark-link:https://github.com/robieta&gt;@robieta&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/omalleyt12&gt;@omalleyt12&lt;/denchmark-link&gt;
 I think this is related to another issue you're working on, can you check?
		</comment>
		<comment id='4' author='cottrell' date='2019-05-13T21:01:26Z'>
		&lt;denchmark-link:https://github.com/cottrell&gt;@cottrell&lt;/denchmark-link&gt;
 could you try this with the 2.0 nightly? i think this should be fixed
		</comment>
		<comment id='5' author='cottrell' date='2019-05-21T07:38:15Z'>
		&lt;denchmark-link:https://github.com/omalleyt12&gt;@omalleyt12&lt;/denchmark-link&gt;
 Could you please tell me how to install the 2-0 nightly? Should I build it from source or I can just install it just by  ?
		</comment>
		<comment id='6' author='cottrell' date='2019-05-21T10:28:36Z'>
		&lt;denchmark-link:https://github.com/jiarenyf&gt;@jiarenyf&lt;/denchmark-link&gt;
 I think:
&lt;denchmark-link:https://pypi.org/project/tf-nightly-2.0-preview/&gt;https://pypi.org/project/tf-nightly-2.0-preview/&lt;/denchmark-link&gt;

but it might not yet work in new python 3.7 and up. See posts like this one if you hit problems in your python version
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/28691&gt;#28691&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='cottrell' date='2019-05-21T11:49:49Z'>
		&lt;denchmark-link:https://github.com/cottrell&gt;@cottrell&lt;/denchmark-link&gt;
 Thank you, I could install the  in python 3.7 and ubuntu 18.04 and solve the problem of .
		</comment>
		<comment id='8' author='cottrell' date='2019-05-30T15:26:08Z'>
		I have having same issue with tf.math.sqrt along with MirroredStrategy and @tf.function
		</comment>
		<comment id='9' author='cottrell' date='2019-05-30T17:24:06Z'>
		Hi &lt;denchmark-link:https://github.com/mdasadul&gt;@mdasadul&lt;/denchmark-link&gt;
 is this issue occurring with the latest nightly?
		</comment>
		<comment id='10' author='cottrell' date='2019-05-30T17:35:24Z'>
		&lt;denchmark-link:https://github.com/omalleyt12&gt;@omalleyt12&lt;/denchmark-link&gt;
  yes. I intalled tf nightly-gpu (2.0.0-dev20190522). And my case is quite interesting. It's happening only when I am using mirrored-strategy. I think it's something to do with mirrored strategy and @tf.function
		</comment>
		<comment id='11' author='cottrell' date='2019-05-30T17:52:09Z'>
		&lt;denchmark-link:https://github.com/mdasadul&gt;@mdasadul&lt;/denchmark-link&gt;
 thanks for reporting this, could you provide a minimal example that reproduces this?
		</comment>
		<comment id='12' author='cottrell' date='2019-05-30T21:10:13Z'>
		I was trying to create a minimal example but couldn't recreate the issue with minimal example. I can share my code if you share your email address
Thanks
		</comment>
		<comment id='13' author='cottrell' date='2019-06-04T13:14:38Z'>
		&lt;denchmark-link:https://github.com/omalleyt12&gt;@omalleyt12&lt;/denchmark-link&gt;
 Here is what the minimal example looks like although I am getting   when using distributed strategy.
The model looks like this
&lt;denchmark-code&gt;!pip install -q tf-nightly-gpu-2.0-preview
from __future__ import absolute_import, division, print_function, unicode_literals

import tensorflow as tf

from tensorflow.keras.layers import Dense, Flatten, Conv2D
from tensorflow.keras import Model
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

# Add a channels dimension
x_train = x_train[..., tf.newaxis]
x_test = x_test[..., tf.newaxis]

train_ds = tf.data.Dataset.from_tensor_slices(
    (x_train, y_train)).shuffle(10000).batch(32)
test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)


class MyModel(Model):
  def __init__(self):
    super(MyModel, self).__init__()
    self.conv1 = Conv2D(32, 3, activation='relu')
    self.flatten = Flatten()
    self.d1 = Dense(128, activation='relu')
    self.d2 = Dense(10, activation='softmax')

  def call(self, x):
    x = self.conv1(x)
    x = self.flatten(x)
    x = self.d1(x)
    return self.d2(x)

&lt;/denchmark-code&gt;

The training class without distributed strategy looks like as follows and I was able to save the model without distributed strategy
&lt;denchmark-code&gt;class Train(object):
  def __init__(self,epochs, enable_function, batch_size, per_replica_batch_size):
    self.epochs = epochs
    self.enable_function = enable_function
    self.batch_size = batch_size
    self.per_replica_batch_size = per_replica_batch_size
    self.learning_rate =  CustomSchedule(10)
    self.model = MyModel()
    self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
     reduction=tf.keras.losses.Reduction.SUM)
    self.optimizer = tf.keras.optimizers.Adam()
    self.train_loss = tf.keras.metrics.Mean(name='train_loss')
    self.train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')

    self.test_loss = tf.keras.metrics.Mean(name='test_loss')
    self.test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')
    
  
  def loss_function(self, real, pred):
      mask = tf.math.logical_not(tf.math.equal(real, 0))
      loss_ = self.loss_object(real, pred)
      mask = tf.cast(mask, dtype=loss_.dtype)
      loss_ *= mask
      return tf.reduce_sum(loss_) * 1./self.batch_size

  def train_step(self, inputs):
    images, labels = inputs
    with tf.GradientTape() as tape:
      predictions = self.model(images)
      loss = self.loss_function(labels, predictions)
    gradients = tape.gradient(loss, self.model.trainable_variables)
    self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))

    self.train_loss(loss)
    self.train_accuracy(labels, predictions)
    
  def test_step(self, inputs):
    images, labels = inputs
    predictions = self.model(images)
    t_loss = self.loss_function(labels, predictions)

    self.test_loss(t_loss)
    self.test_accuracy(labels, predictions)
    
  def training_loop(self, train_dataset, test_dataset):
    if self.enable_function:
      self.train_step=tf.function(self.train_step)
      self.test_step=tf.function(self.test_step)
    for epoch in range(self.epochs):
      self.train_loss.reset_states()
      self.test_loss.reset_states()
      self.train_accuracy.reset_states()
      self.test_accuracy.reset_states()

      for images, labels in train_dataset:
        self.train_step((images, labels))
      for test_images,test_labels in test_dataset:
        self.test_step((test_images, test_labels))

      template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'
      print (template.format(epoch+1,
                             self.train_loss.result(),
                             self.train_accuracy.result()*100,
                             self.test_loss.result(),
                             self.test_accuracy.result()*100))
      
      
      
 
epochs=5
enable_function=True
batch_size=128
per_replica_batch_size=128
train_obj=Train(epochs, enable_function, batch_size, per_replica_batch_size)
train_obj.training_loop(train_ds, test_ds)
tf.saved_model.save(train_obj.model,'model')
&lt;/denchmark-code&gt;

When I am adding distributed strategy as follows I am getting ** NotImplementedError**
&lt;denchmark-code&gt;class DistributedTrain(Train):
  def __init__(self,epochs, enable_function, batch_size, per_replica_batch_size):
    Train.__init__(self,epochs, enable_function, batch_size, per_replica_batch_size)
      
  def training_loop(self, train_iterator, test_iterator, 
                   num_train_steps_per_epoch, num_test_steps_per_epoch,
                   strategy):
    def distributed_train():
      return strategy.experimental_run(self.train_step, train_iterator)

    def distributed_test():
      return strategy.experimental_run(self.test_step, test_iterator)

    if self.enable_function:
      distributed_train = tf.function(distributed_train)
      distributed_test = tf.function(distributed_test)

    template = 'Epoch: {}, Train Loss: {}, Test Loss: {}'
    for epoch in range(self.epochs):
      self.train_loss.reset_states()
      self.test_loss.reset_states()
      self.train_accuracy.reset_states()
      self.test_accuracy.reset_states()

      train_iterator.initialize()
      for _ in range(num_train_steps_per_epoch):
        distributed_train()

      test_iterator.initialize()
      for _ in range(num_test_steps_per_epoch):
        distributed_test()

      template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'
      print (template.format(epoch+1,
                               self.train_loss.result(),
                               self.train_accuracy.result()*100,
                               self.test_loss.result(),
                               self.test_accuracy.result()*100))


strategy = tf.distribute.MirroredStrategy()
num_replicas = strategy.num_replicas_in_sync

with strategy.scope():
  num_train_steps_per_epoch = tf.data.experimental.cardinality(train_ds)
  num_test_steps_per_epoch = tf.data.experimental.cardinality(test_ds)

  train_iterator = strategy.make_dataset_iterator(train_ds)
  test_iterator = strategy.make_dataset_iterator(test_ds)
  
  train_obj= DistributedTrain(epochs, enable_function, batch_size, per_replica_batch_size)
  train_obj.training_loop(train_iterator,
                          test_iterator,
                                   num_train_steps_per_epoch,
                                   num_test_steps_per_epoch,
                                   strategy)
  tf.saved_model.save(train_obj.model,'dist-model')
&lt;/denchmark-code&gt;

Thanks so much
		</comment>
		<comment id='14' author='cottrell' date='2019-06-05T20:57:03Z'>
		mdasadul@ can you post the stack trace of the error you are seeing?
		</comment>
		<comment id='15' author='cottrell' date='2019-06-06T11:36:25Z'>
		&lt;denchmark-link:https://github.com/anj-s&gt;@anj-s&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/omalleyt12&gt;@omalleyt12&lt;/denchmark-link&gt;
  Here is the stack trace
&lt;denchmark-code&gt;NotImplementedError                       Traceback (most recent call last)
&lt;ipython-input-33-a2c05e070639&gt; in &lt;module&gt;()
     55                                    num_test_steps_per_epoch,
     56                                    strategy)
---&gt; 57   tf.saved_model.save(train_obj.model,'dist-model')

4 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/save.py in save(obj, export_dir, signatures)
    833   object_saver = util.TrackableSaver(checkpoint_graph_view)
    834   asset_info, exported_graph = _fill_meta_graph_def(
--&gt; 835       meta_graph_def, saveable_view, signatures)
    836   saved_model.saved_model_schema_version = (
    837       constants.SAVED_MODEL_SCHEMA_VERSION)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/save.py in _fill_meta_graph_def(meta_graph_def, saveable_view, signature_functions)
    529   resource_initializer_ops = []
    530   with exported_graph.as_default():
--&gt; 531     object_map, resource_map, asset_info = saveable_view.map_resources()
    532     for resource_initializer_function in resource_initializer_functions:
    533       asset_dependencies = []

/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/save.py in map_resources(self)
    250         self.captured_tensor_node_ids[obj.resource_handle] = node_id
    251       elif resource_variable_ops.is_resource_variable(obj):
--&gt; 252         new_variable = resource_variable_ops.copy_to_graph_uninitialized(obj)
    253         object_map[obj] = new_variable
    254         resource_map[obj.handle] = new_variable.handle

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py in copy_to_graph_uninitialized(var)
   1794       dtype=var.dtype,
   1795       name=var._shared_name,
-&gt; 1796       synchronization=var.synchronization,
   1797       aggregation=var.aggregation,
   1798       extra_handle_data=var.handle)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py in synchronization(self)
    512   @property
    513   def synchronization(self):
--&gt; 514     raise NotImplementedError
    515 
    516   @property

NotImplementedError: 
&lt;/denchmark-code&gt;

		</comment>
		<comment id='16' author='cottrell' date='2019-06-07T17:55:23Z'>
		Thanks mdasadul@ seemuch@ is working on adding support for saving a model under distribution strategy scope using the save_model.save() API.  seemuch@ Can you update this issue once the fix is in? Thanks!
		</comment>
		<comment id='17' author='cottrell' date='2019-06-24T13:48:29Z'>
		&lt;denchmark-link:https://github.com/anj-s&gt;@anj-s&lt;/denchmark-link&gt;
 can you provide workaround to save model in distribution mode for current tf version? My main goal is convert tf model (subclassed model of tf.keras.Model which was trained using custom loop) to tf lite. How can I do it currently without tf.saved_model.save API?
		</comment>
		<comment id='18' author='cottrell' date='2019-06-24T17:59:07Z'>
		I believe this issue is fixed in the latest 2.0 nightly.
Although, the tf.saved_model.save(train_obj.model,'dist-model') line should be called out side of the distribution scope.
		</comment>
		<comment id='19' author='cottrell' date='2019-06-24T18:08:00Z'>
		I am going to close this issue for now, since it should be fixed.
If there is any more updates, feel free to let me know.
		</comment>
		<comment id='20' author='cottrell' date='2019-06-24T18:08:01Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=27949&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=27949&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='21' author='cottrell' date='2019-06-24T19:10:34Z'>
		&lt;denchmark-link:https://github.com/seemuch&gt;@seemuch&lt;/denchmark-link&gt;
 thank for update! If you can, please, look at this issue &lt;denchmark-link:https://stackoverflow.com/questions/56739596/how-to-convert-trained-in-custom-loop-subclassed-tf-keras-model-to-tflite&gt;https://stackoverflow.com/questions/56739596/how-to-convert-trained-in-custom-loop-subclassed-tf-keras-model-to-tflite&lt;/denchmark-link&gt;
 ? I can't understand how I can convert subclassed tf.keras.Model to SavedModel due to the following warning: 
		</comment>
		<comment id='22' author='cottrell' date='2019-06-24T19:22:06Z'>
		
Although, the tf.saved_model.save(train_obj.model,'dist-model') line should be called out side of the distribution scope.

It's not the true, because I got the next error:
RuntimeError: Need to be inside "with strategy.scope()" for &lt;tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7fd50d505ef0&gt;
		</comment>
		<comment id='23' author='cottrell' date='2019-08-29T21:45:36Z'>
		Hitting a similar issue with
ValueError: Num gradients 0 generated for op name: "nvp_layer_5/StatefulPartitionedCall"
op: "StatefulPartitionedCall"
input: "S"
input: "B"
input: "nvp_layer_5/StatefulPartitionedCall/args_2"
input: "nvp_layer_5/StatefulPartitionedCall/args_3"
input: "nvp_layer_5/StatefulPartitionedCall/args_4"
attr {
key: "Tin"
value {
list {
type: DT_FLOAT
type: DT_FLOAT
type: DT_RESOURCE
type: DT_RESOURCE
type: DT_RESOURCE
}
}
}
attr {
key: "Tout"
value {
list {
type: DT_FLOAT
}
}
}
attr {
key: "_gradient_op_type"
value {
s: "PartitionedCall-4641"
}
}
attr {
key: "config"
value {
s: ""
}
}
attr {
key: "config_proto"
value {
s: "\n\007\n\003CPU\020\001\n\007\n\003GPU\020\0002\002J\0008\001"
}
}
attr {
key: "executor_type"
value {
s: ""
}
}
attr {
key: "f"
value {
func {
name: "__inference_call_4640"
}
}
}
Is there some easy workaround beside not using tf.function? I've tried constructing most things inside tf.function call but can't seem to get around this one. I have a feeling some update of tf rc0 or tfp broke it.
I can try to open a new issue with a minimal example if it will help but I think things might be changing in the versions so not sure it is worth it.
		</comment>
		<comment id='24' author='cottrell' date='2019-08-29T21:58:50Z'>
		This is a separate issue. Can you file a separate bug with instructions to
reproduce?
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Thu, Aug 29, 2019 at 2:52 PM David Cottrell ***@***.***&gt; wrote:
 Hitting a similar issue with

 ValueError: Num gradients 0 generated for op name:
 "nvp_layer_5/StatefulPartitionedCall"
 op: "StatefulPartitionedCall"
 input: "S"
 input: "B"
 input: "nvp_layer_5/StatefulPartitionedCall/args_2"
 input: "nvp_layer_5/StatefulPartitionedCall/args_3"
 input: "nvp_layer_5/StatefulPartitionedCall/args_4"
 attr {
 key: "Tin"
 value {
 list {
 type: DT_FLOAT
 type: DT_FLOAT
 type: DT_RESOURCE
 type: DT_RESOURCE
 type: DT_RESOURCE
 }
 }
 }
 attr {
 key: "Tout"
 value {
 list {
 type: DT_FLOAT
 }
 }
 }
 attr {
 key: "_gradient_op_type"
 value {
 s: "PartitionedCall-4641"
 }
 }
 attr {
 key: "config"
 value {
 s: ""
 }
 }
 attr {
 key: "config_proto"
 value {
 s: "\n\007\n\003CPU\020\001\n\007\n\003GPU\020\0002\002J\0008\001"
 }
 }
 attr {
 key: "executor_type"
 value {
 s: ""
 }
 }
 attr {
 key: "f"
 value {
 func {
 name: "__inference_call_4640"
 }
 }
 }

 Is there some easy workaround beside not using tf.function? I've tried
 constructing most things inside tf.function call but can't seem to get
 around this one. I have a feeling some update of tf rc0 or tfp broke it.

 I can try to open a new issue with a minimal example if it will help but I
 think things might be changing in the versions so not sure it is worth it.

 —
 You are receiving this because you were assigned.
 Reply to this email directly, view it on GitHub
 &lt;#27949?email_source=notifications&amp;email_token=AAABHRKJVUBUS2XG2IFJHS3QHBALJA5CNFSM4HG3NY3KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD5P5NJA#issuecomment-526374564&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AAABHRMDZ3IZLRLP3YRBU4LQHBALJANCNFSM4HG3NY3A&gt;
 .


-- 
 - Alex

		</comment>
		<comment id='25' author='cottrell' date='2019-10-11T13:25:32Z'>
		I am getting this error again, I have one encoder model and output of that I am passing to three different decoders and adding loss of all three and then doing backward propagation. But I am getting this error
`:61 train_step  *
optimizer.apply_gradients(zip(gradients, variables))
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:427 apply_gradients
grads_and_vars = _filter_grads(grads_and_vars)
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:1025 _filter_grads
([v.name for _, v in grads_and_vars],))
&lt;denchmark-code&gt;ValueError: No gradients provided for any variable: ['encoder/embedding/embeddings:0', 'encoder/gru/kernel:0', 'encoder/gru/recurrent_kernel:0', 'encoder/gru/bias:0', 'decoder_a/embedding_1/embeddings:0', 'decoder_a/gru_1/kernel:0', 'decoder_a/gru_1/recurrent_kernel:0', 'decoder_a/gru_1/bias:0', 'decoder_a/dense_3/kernel:0', 'decoder_a/dense_3/bias:0', 'decoder_a/bahdanau_attention_1/dense_4/kernel:0', 'decoder_a/bahdanau_attention_1/dense_4/bias:0', 'decoder_a/bahdanau_attention_1/dense_5/kernel:0', 'decoder_a/bahdanau_attention_1/dense_5/bias:0', 'decoder_a/bahdanau_attention_1/dense_6/kernel:0', 'decoder_a/bahdanau_attention_1/dense_6/bias:0', 'decoder_b/embedding_2/embeddings:0', 'decoder_b/gru_2/kernel:0', 'decoder_b/gru_2/recurrent_kernel:0', 'decoder_b/gru_2/bias:0', 'decoder_b/dense_7/kernel:0', 'decoder_b/dense_7/bias:0', 'decoder_b/bahdanau_attention_2/dense_8/kernel:0', 'decoder_b/bahdanau_attention_2/dense_8/bias:0', 'decoder_b/bahdanau_attention_2/dense_9/kernel:0', 'decoder_b/bahdanau_attention_2/dense_9/bias:0', 'decoder_b/bahdanau_attention_2/dense_10/kernel:0', 'decoder_b/bahdanau_attention_2/dense_10/bias:0', 'decoder_c/embedding_3/embeddings:0', 'decoder_c/gru_3/kernel:0', 'decoder_c/gru_3/recurrent_kernel:0', 'decoder_c/gru_3/bias:0', 'decoder_c/dense_11/kernel:0', 'decoder_c/dense_11/bias:0', 'decoder_c/bahdanau_attention_3/dense_12/kernel:0', 'decoder_c/bahdanau_attention_3/dense_12/bias:0', 'decoder_c/bahdanau_attention_3/dense_13/kernel:0', 'dec...`
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/seemuch&gt;@seemuch&lt;/denchmark-link&gt;

		</comment>
		<comment id='26' author='cottrell' date='2019-10-18T17:20:20Z'>
		I have same problem.
ValueError: No gradients provided for any variable: ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'].
I have a tf.pow() in the computation. Others are just tf.add, tf.matmul, tf.sigmoid. So I guess this tf.pow() or tf.math.pow() causes the problem. The gradient chain is broken.
		</comment>
		<comment id='27' author='cottrell' date='2019-10-21T09:37:47Z'>
		I am also getting the same issue while writing custom layer functionality
		</comment>
		<comment id='28' author='cottrell' date='2019-10-29T20:27:13Z'>
		&lt;denchmark-link:https://github.com/hansong999&gt;@hansong999&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/UdiBhaskar&gt;@UdiBhaskar&lt;/denchmark-link&gt;
 can you open separate issues with the instructions to reproduce your problems?
		</comment>
		<comment id='29' author='cottrell' date='2019-10-31T12:37:33Z'>
		I have the same problem, my model consists of three layers of dnn, and uses a cross-entropy loss for multi-tasks, finally, add these multi-tasks loss for train, but i had just got an error like that:### ValueError: No gradients provided for any variable
class model():
.........
def loss(self, outputs, task_labels):
losses = []
ctr_label = tf.cast(task_labels[..., 0], tf.float32)
ctr = tf.sigmoid(outputs[..., 0])
ctr_loss = tf.keras.losses.binary_crossentropy(ctr_label, ctr)
losses.append(ctr_loss)
for i in range(1, self.num_classes):
task_label = tf.cast(task_labels[..., i], tf.float32) * ctr_label
cvr = tf.sigmoid(outputs[..., i])
task_loss = tf.keras.losses.binary_crossentropy(task_label, ctr * cvr)
losses.append(task_loss)
return losses
tasks_losses = model.loss(tasks_outputs, train_task_labels)
def loss_all_func():
return tf.add_n(tasks_losses)
join_op=tf.keras.optimizers.Adamax(0.001).minimize(loss_all_func,var_list=tf.compat.v1.trainable_variables())
		</comment>
		<comment id='30' author='cottrell' date='2020-04-17T15:30:09Z'>
		Please file a new bug for your issue.
		</comment>
	</comments>
</bug>