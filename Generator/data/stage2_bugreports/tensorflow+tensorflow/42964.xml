<bug id='42964' author='keithm-xmos' open_date='2020-09-04T17:16:47Z' closed_time='2020-09-10T20:42:48Z'>
	<summary>MicroInterpreter::tensors_size() always returns zero</summary>
	<description>
@tensorflow/micro
System information

Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora 32
TensorFlow installed from (source or binary): source
Tensorflow version (commit SHA if source): 238981a91a9b780ab4449829469a71c5d668a273
Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): x86

Describe the problem
The tensors_size() method of the MicroInterpreter class always returns zero.
Please provide the exact sequence of commands/steps when you ran into the problem
To demonstrate, insert the following line:
TF_LITE_MICRO_EXPECT_GT(static_cast&lt;size_t&gt;(0), interpreter.tensors_size());
here: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/micro_interpreter_test.cc#L87&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/micro_interpreter_test.cc#L87&lt;/denchmark-link&gt;

And run:
$ make -f tensorflow/lite/micro/tools/make/Makefile TARGET="x86" test_micro_interpreter_test
	</description>
	<comments>
		<comment id='1' author='keithm-xmos' date='2020-09-08T06:10:01Z'>
		&lt;denchmark-link:https://github.com/advaitjain&gt;@advaitjain&lt;/denchmark-link&gt;
 Could you take a look at this issue?
		</comment>
		<comment id='2' author='keithm-xmos' date='2020-09-09T15:25:48Z'>
		&lt;denchmark-link:https://github.com/keithm-xmos&gt;@keithm-xmos&lt;/denchmark-link&gt;
 I think you are using a bad TARGET="x86". Can you try and remove that?
make -f tensorflow/lite/micro/tools/make/Makefile test_micro_interpreter_test
That passes for me on Darwin x86_64.
		</comment>
		<comment id='3' author='keithm-xmos' date='2020-09-09T15:33:21Z'>
		&lt;denchmark-link:https://github.com/nkreeger&gt;@nkreeger&lt;/denchmark-link&gt;
 Did you make the modification to micro_interpreter_test.cc before rebuilding and running your suggestion?  It fails on x86 and xcore for me.  I think you will find that it will fail on any platform.
		</comment>
		<comment id='4' author='keithm-xmos' date='2020-09-09T15:45:29Z'>
		&lt;denchmark-link:https://github.com/keithm-xmos&gt;@keithm-xmos&lt;/denchmark-link&gt;
 ah sorry i missed that - our macro is a little confusing and the test code isn't the cleanest at the moment.
Try this:
TF_LITE_MICRO_EXPECT_GT(interpreter.tensors_size(), static_cast&lt;size_t&gt;(0));
That works for me - the macro is checking that |0 &gt; tensors_size| which is always false.
		</comment>
		<comment id='5' author='keithm-xmos' date='2020-09-09T16:54:53Z'>
		&lt;denchmark-link:https://github.com/nkreeger&gt;@nkreeger&lt;/denchmark-link&gt;
 I applied your suggestion and I get the following:
&lt;denchmark-code&gt;Testing TestInterpreter
interpreter.tensors_size() &gt; static_cast&lt;size_t&gt;(0) failed at tensorflow/lite/micro/micro_interpreter_test.cc:87
Output tensors not at index 0 are allocated from the persistent memory arena. Repeat calls will cause excess allocation!
Testing TestMultiTenantInterpreter
Testing TestKernelMemoryPlanning
Output tensors not at index 0 are allocated from the persistent memory arena. Repeat calls will cause excess allocation!
Output tensors not at index 0 are allocated from the persistent memory arena. Repeat calls will cause excess allocation!
Output tensors not at index 0 are allocated from the persistent memory arena. Repeat calls will cause excess allocation!
Output tensors not at index 0 are allocated from the persistent memory arena. Repeat calls will cause excess allocation!
Output tensors not at index 0 are allocated from the persistent memory arena. Repeat calls will cause excess allocation!
Output tensors not at index 0 are allocated from the persistent memory arena. Repeat calls will cause excess allocation!
Testing TestVariableTensorReset
Testing TestIncompleteInitialization
Testing InterpreterWithProfilerShouldProfileOps
Testing TestIncompleteInitializationAllocationsWithSmallArena
Failed to allocate tail memory. Requested: 240, available 208, missing: 32
Failed to allocate memory for context-&gt;eval_tensors, 240 bytes required
Failed starting model allocation.

Testing TestInterpreterDoesNotAllocateUntilInvoke
[RecordingMicroAllocator] Arena allocation total 768 bytes
[RecordingMicroAllocator] Arena allocation head 32 bytes
[RecordingMicroAllocator] Arena allocation tail 736 bytes
[RecordingMicroAllocator] 'TfLiteEvalTensor data' used 240 bytes with alignment overhead (requested 240 bytes for 10 allocations)
[RecordingMicroAllocator] 'Persistent TfLiteTensor data' used 0 bytes with alignment overhead (requested 0 bytes for 0 tensors)
[RecordingMicroAllocator] 'Persistent TfLiteTensor quantization data' used 0 bytes with alignment overhead (requested 0 bytes for 0 allocations)
[RecordingMicroAllocator] 'TfLiteTensor variable buffer data' used 40 bytes with alignment overhead (requested 12 bytes for 3 allocations)
[RecordingMicroAllocator] 'NodeAndRegistration struct' used 168 bytes with alignment overhead (requested 168 bytes for 3 NodeAndRegistration structs)
[RecordingMicroAllocator] 'Operator runtime data' used 0 bytes with alignment overhead (requested 0 bytes for 0 OpData structs)
7/8 tests passed
~~~SOME TESTS FAILED~~~

Interpreter has 0 tensors and 2 nodes
Inputs: 0
Outputs: 2 3


Node   0 Operator Custom Name mock_custom
  Inputs: 0 1
  Outputs: 2
Node   1 Operator Custom Name mock_custom
  Inputs: 0 1
  Outputs: 3
make: *** [tensorflow/lite/micro/tools/make/Makefile:368: test_micro_interpreter_test] Error 1
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='keithm-xmos' date='2020-09-09T16:55:48Z'>
		I ran with

make -f tensorflow/lite/micro/tools/make/Makefile test_micro_interpreter_test

Here's my diff
&lt;denchmark-code&gt;diff --git a/tensorflow/lite/micro/micro_interpreter_test.cc b/tensorflow/lite/micro/micro_interpreter_test.cc
index a4a4143a2a..c5a13af8da 100644
--- a/tensorflow/lite/micro/micro_interpreter_test.cc
+++ b/tensorflow/lite/micro/micro_interpreter_test.cc
@@ -84,6 +84,7 @@ TF_LITE_MICRO_TEST(TestInterpreter) {
     TF_LITE_MICRO_EXPECT_LE(interpreter.arena_used_bytes(), 928 + 100);
     TF_LITE_MICRO_EXPECT_EQ(static_cast&lt;size_t&gt;(1), interpreter.inputs_size());
     TF_LITE_MICRO_EXPECT_EQ(static_cast&lt;size_t&gt;(2), interpreter.outputs_size());
+    TF_LITE_MICRO_EXPECT_GT(interpreter.tensors_size(), static_cast&lt;size_t&gt;(0));
 
     TfLiteTensor* input = interpreter.input(0);
     TF_LITE_MICRO_EXPECT_NE(nullptr, input);

&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='keithm-xmos' date='2020-09-10T13:17:24Z'>
		&lt;denchmark-link:https://github.com/keithm-xmos&gt;@keithm-xmos&lt;/denchmark-link&gt;
 sorry I had a bad vim buffer - yes this was accidentally regressed. I just opened &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/43109&gt;#43109&lt;/denchmark-link&gt;
 to fix this -
Hopefully this isn't breaking you on anything right now - this API is mostly around for unit tests and inspecting of a model. I'd advise against looping through the graph on that API. Those values are now allocated from the "persistent" memory area of the arena. I have some documentation coming on those changes.
		</comment>
		<comment id='8' author='keithm-xmos' date='2020-09-10T20:42:50Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42964&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42964&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='keithm-xmos' date='2020-10-09T13:08:34Z'>
		Should be fixed with this change: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/ba071d416fc3d00b79021658c8d666b40b3fcb77&gt;ba071d4&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>