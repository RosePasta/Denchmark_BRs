<bug id='43293' author='RomanGirin' open_date='2020-09-17T11:48:42Z' closed_time='2020-09-20T12:12:04Z'>
	<summary>Usage of intermediate layer causes an exception</summary>
	<description>
System information
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution: Windows 10
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.3
Python version: 3.7.5
CUDA/cuDNN version: 10.1
GPU model and memory: RTX 2080 Ti
Describe the current behavior
In toy example (provided below) there is an example of usage intermediate NN layer in loss function and in accuracy validation logic. On TF 2.3 it causes the following exeption:
Traceback (most recent call last): File "C:/PyProjects/TF_Issue/main.py", line 53, in &lt;module&gt; hits += tf.reduce_sum(tf.where((output - target_batch + x_slice) &lt; 0.01, 1, 0)).numpy() AttributeError: 'Tensor' object has no attribute 'numpy'
I marked the line which causes the error. Without usage of intermediate layer it works.
Please, note these are two lines in the script below:
&lt;denchmark-code&gt;hits += tf.reduce_sum(tf.where((output - target_batch + x_slice) &lt; 0.01, 1, 0)).numpy() # this line causes the error
 # hits += tf.reduce_sum(tf.where(output - target_batch &lt; 0.01, 1, 0)).numpy() # if x_slice is omitted script works fine
&lt;/denchmark-code&gt;

Commenting-out the first one and uncomment the second to get the script working.
Standalone code to reproduce the issue
&lt;denchmark-code&gt;import tensorflow as tf

batch_size = 128

input = tf.keras.Input(shape=(None, 1))
x = tf.keras.layers.Dense(1)(input)
output = tf.keras.layers.Dense(1)(x)

model = tf.keras.Model(inputs=input, outputs=output)

# A toy dataset of points around 3 * x + 2
NUM_EXAMPLES = 2000
inputs = tf.random.normal([NUM_EXAMPLES])
noise = tf.random.normal([NUM_EXAMPLES])
outputs = inputs * 3 + 2 + noise

training_inputs = tf.reshape(inputs[:1500], (1500, 1))
training_outputs = tf.reshape(outputs[:1500], (1500, 1))
training_inputs = tf.data.Dataset.from_tensor_slices(training_inputs).batch(batch_size)
training_outputs = tf.data.Dataset.from_tensor_slices(training_outputs).batch(batch_size)
test_inputs = tf.reshape(inputs[1500:], (500, 1))
test_outputs = tf.reshape(outputs[1500:], (500, 1))
test_inputs = tf.data.Dataset.from_tensor_slices(test_inputs).batch(batch_size)
test_outputs = tf.data.Dataset.from_tensor_slices(test_outputs).batch(batch_size)


def loss(model, inputs, targets):
  outputs = model(inputs)
  output = outputs[:, 0]  # take the first output (in general model can have several outputs)
  global x
  x_slice = x[:, 0]
  error = output - targets + x_slice
  return tf.reduce_mean(tf.square(error))


optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)
epoch = 3
for i in range(epoch):
  for input_batch, target_batch in zip(training_inputs, training_outputs):
    with tf.GradientTape() as tape:
      loss_value = loss(model, input_batch, target_batch)
      grads = tape.gradient(loss_value, model.trainable_variables)
      optimizer.apply_gradients(zip(grads, model.trainable_variables))
  print('epoch #:', i)


hits = 0
total = 0
for input_batch, target_batch in zip(test_inputs, test_outputs):
  outputs = model(input_batch)
  output = outputs[:, 0]  # take the first output (in general model can have several outputs)
  x_slice = x[:, 0]
  hits += tf.reduce_sum(tf.where((output - target_batch + x_slice) &lt; 0.01, 1, 0)).numpy() # this line causes the error
  # hits += tf.reduce_sum(tf.where(output - target_batch &lt; 0.01, 1, 0)).numpy() # if x_slice is omitted script works fine
  total += input_batch.shape[0]

print(hits)
print('Accuracy: ', hits/total)

&lt;/denchmark-code&gt;

Any ideas how to fix the issues?
Thanks in advance!!!
	</description>
	<comments>
		<comment id='1' author='RomanGirin' date='2020-09-17T23:03:01Z'>
		It is not a bug. It is by design and your effort is similar to:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/37522#issuecomment-598648414&gt;#37522 (comment)&lt;/denchmark-link&gt;

Cause also tf.data.Dataset runs in graph mode.
		</comment>
		<comment id='2' author='RomanGirin' date='2020-09-20T12:12:04Z'>
		&lt;denchmark-link:https://github.com/bhack&gt;@bhack&lt;/denchmark-link&gt;
 thank you for the reply!
yes, you are right!
More details (and fixed version of the example above) in comment in related issue here &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/41200#issuecomment-695779647&gt;#41200 (comment)&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='RomanGirin' date='2020-09-20T12:12:06Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43293&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43293&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>