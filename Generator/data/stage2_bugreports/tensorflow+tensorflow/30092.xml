<bug id='30092' author='keurcien' open_date='2019-06-24T15:46:46Z' closed_time='2019-06-27T23:22:16Z'>
	<summary>AttributeError: module 'tensorflow._api.v2.train' has no attribute 'FtrlOptimizer'</summary>
	<description>
Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.
The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: &lt;denchmark-link:https://www.tensorflow.org/community/contribute/docs&gt;https://www.tensorflow.org/community/contribute/docs&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;URL(s) with the issue:&lt;/denchmark-h&gt;

Please provide a link to the documentation entry, for example:
&lt;denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/estimator/LinearClassifier&gt;https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/estimator/LinearClassifier&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;Description of issue (what needs changing):&lt;/denchmark-h&gt;

FtrlOptimizer is not accessible from tf.train:
# Or estimator using the FTRL optimizer with regularization.
estimator = LinearClassifier(
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b],
    optimizer=tf.train.FtrlOptimizer(
      learning_rate=0.1,
      l1_regularization_strength=0.001
    )
    ### should be optimizer=tf.keras.optimizers.Ftrl(...)
)

&gt; AttributeError: module 'tensorflow._api.v2.train' has no attribute 'FtrlOptimizer'
&lt;denchmark-h:h3&gt;Clear description&lt;/denchmark-h&gt;

N/A
&lt;denchmark-h:h3&gt;Correct links&lt;/denchmark-h&gt;

N/A
&lt;denchmark-h:h3&gt;Parameters defined&lt;/denchmark-h&gt;

N/A
&lt;denchmark-h:h3&gt;Returns defined&lt;/denchmark-h&gt;

N/A
&lt;denchmark-h:h3&gt;Raises listed and defined&lt;/denchmark-h&gt;

N/A
&lt;denchmark-h:h3&gt;Usage example&lt;/denchmark-h&gt;

N/A
&lt;denchmark-h:h3&gt;Request visuals, if applicable&lt;/denchmark-h&gt;

N/A
&lt;denchmark-h:h3&gt;Submit a pull request?&lt;/denchmark-h&gt;

N/A
	</description>
	<comments>
		<comment id='1' author='keurcien' date='2019-06-25T09:47:41Z'>
		Can you please provide your full code snippet to reproduce the issue.Thanks!
		</comment>
		<comment id='2' author='keurcien' date='2019-06-25T10:15:54Z'>
		Sure, here's my MWE:
import tensorflow as tf
print(tf.__version__)

&gt; 2.0.0-beta1
from tensorflow.estimator import LinearClassifier

feature_columns = []

for feature_name in ['some_feature_a', 'some_feature_b']:
    feature_columns.append(
        tf.feature_column.numeric_column(feature_name, dtype=tf.float32)
    )

estimator = LinearClassifier(
    feature_columns=feature_columns,
    optimizer=tf.train.FtrlOptimizer(
      learning_rate=0.1,
      l1_regularization_strength=0.001
    )
)

&gt; AttributeError: module 'tensorflow._api.v2.train' has no attribute 'FtrlOptimizer'
The Ftrl optimizer (or any other optimizer) can be called from tf.optimizers or tf.keras.optimizers:
estimator = LinearClassifier(
    feature_columns=feature_columns,
    optimizer=tf.optimizers.Ftrl(
      learning_rate=0.1,
      l1_regularization_strength=0.001
    )
)
		</comment>
		<comment id='3' author='keurcien' date='2019-06-25T20:03:04Z'>
		I have similar issue:
Version: tensorflow._api.v2.version
tensorflow-gpu==2.0Beta1
Code:
&lt;denchmark-code&gt;	example = tf.train.example(features=tf.train.features(feature={
		'height': _int64_feature(rows),
		'width': _int64_feature(cols),
		'depth': _int64_feature(depth),
		'label_raw': _bytes_feature(label_raw),
		'image_raw': _bytes_feature(image_raw)}))
&lt;/denchmark-code&gt;

Error:
Traceback (most recent call last):
File "train_v1.1.py", line 236, in 
main(sys.argv)
File "train_v1.1.py", line 226, in main
convert_to(train_images, train_labels, datafolder)
File "train_v1.1.py", line 136, in convert_to
example = tf.train.example(features=tf.train.features(feature={
AttributeError: module 'tensorflow._api.v2.train' has no attribute 'example'
		</comment>
		<comment id='4' author='keurcien' date='2019-06-26T17:16:42Z'>
		I found a way to get past this issue.
ef _parse_function(example_proto):
feature_description = {
'height': tf.io.FixedLenFeature([], tf.int64),
'width':  tf.io.FixedLenFeature([], tf.int64),
'depth':  tf.io.FixedLenFeature([], tf.int64),
'label_raw':  tf.io.FixedLenFeature([], tf.string),
'image_raw':  tf.io.FixedLenFeature([], tf.string),
}
parsed_features = tf.io.parse_single_example(example_proto, feature_description)
return parsed_features["height"], parsed_features["width"], parsed_features["depth"], parsed_features["image_raw"], parsed_features["label_raw"]
...
...
serialized_example = tf.data.TFRecordDataset(filenames)
serialized_example = serialized_example.map(_parse_function)
Calling tf.io.parse_single_example inside tf.Data.map() function as above works without error.
Thank you.
		</comment>
		<comment id='5' author='keurcien' date='2019-06-27T23:27:56Z'>
		&lt;denchmark-link:https://github.com/keurcien&gt;@keurcien&lt;/denchmark-link&gt;
 Thanks for catching this. Sent a PR to address this issue.
		</comment>
		<comment id='6' author='keurcien' date='2019-07-09T20:35:27Z'>
		First, tf.train.FtrlOptimizer is not exposed in TF 2.0, and you may use tf.compat.v1 to access it. Secondly, LinearClassifier in TF 2.0 supports tf.keras.optimziers.* (and tf.optimizers.*) only, and you cannot use tf.train.*Optimizer here.
So this is the intended behavior, not an actual issue.
		</comment>
		<comment id='7' author='keurcien' date='2019-07-09T20:38:25Z'>
		Ah, just notice this is about the doc string issue. Yes, that's right. Thank you!
		</comment>
		<comment id='8' author='keurcien' date='2019-10-22T04:05:34Z'>
		hi yhliang2018, do u know how to use tf.contrib.keras.optimizers.Adamax?
		</comment>
		<comment id='9' author='keurcien' date='2019-10-22T04:55:07Z'>
		&lt;denchmark-link:https://github.com/xiNA2019&gt;@xiNA2019&lt;/denchmark-link&gt;
 Hi, Adamax optimizer has been exposed in TF 2.0. Please check &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adamax&gt;https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adamax&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='keurcien' date='2019-10-23T08:33:04Z'>
		I had the same problem here:
&lt;denchmark-code&gt;!pip install scikit-learn
!pip install tensorflow-gpu
!pip install tensorflow-hub
!pip install bert-tensorflow
&lt;/denchmark-code&gt;

from sklearn.model_selection import train_test_split
import pandas as pd
import tensorflow as tf
import tensorflow_hub as hub
from datetime import datetime

import bert
from bert import run_classifier
from bert import optimization
from bert import tokenization
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-9-1ccb11d8dffa&gt; in &lt;module&gt;
      1 import bert
----&gt; 2 from bert import run_classifier
      3 from bert import optimization
      4 from bert import tokenization

~/anaconda3/envs/py37/lib/python3.7/site-packages/bert/run_classifier.py in &lt;module&gt;
     23 import os
     24 from bert import modeling
---&gt; 25 from bert import optimization
     26 from bert import tokenization
     27 import tensorflow as tf

~/anaconda3/envs/py37/lib/python3.7/site-packages/bert/optimization.py in &lt;module&gt;
     85 
     86 
---&gt; 87 class AdamWeightDecayOptimizer(tf.train.Optimizer):
     88   """A basic Adam optimizer that includes "correct" L2 weight decay."""
     89 

AttributeError: module 'tensorflow_core._api.v2.train' has no attribute 'Optimizer'
while using
&lt;denchmark-code&gt;!pip install tensorflow-gpu==1.15.0
&lt;/denchmark-code&gt;

I get a deprecation warning only
&lt;denchmark-code&gt;WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.
&lt;/denchmark-code&gt;

see this notebook: &lt;denchmark-link:https://github.com/loretoparisi/bert-movie-reviews-sentiment-classifier/blob/master/src/bert_sentiment_classifier.ipynb&gt;https://github.com/loretoparisi/bert-movie-reviews-sentiment-classifier/blob/master/src/bert_sentiment_classifier.ipynb&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='keurcien' date='2019-10-23T17:03:11Z'>
		Are you in TF 1.x or TF 2.0? In general,  has been deprecated in TF 2.0, and you need to use &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/Optimizer&gt;tf.compat.v1.Optimizer&lt;/denchmark-link&gt;
 (then the deprecation message shows up but it's a warning only). In TF 2.0,  the Keras optimziers &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/optimizers&gt;tf.keras.optimizers.*&lt;/denchmark-link&gt;
 are recommended to use.
		</comment>
		<comment id='12' author='keurcien' date='2019-10-24T23:43:53Z'>
		&lt;denchmark-link:https://github.com/loretoparisi&gt;@loretoparisi&lt;/denchmark-link&gt;
 I'm getting a similar error trying to run the Bert example here in my own Jupyter notebook running on EC2 (using TF2.0 on an Ubuntu deep learning AMI):
&lt;denchmark-link:https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb#scrollTo=IhJSe0QHNG7U&gt;https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb#scrollTo=IhJSe0QHNG7U&lt;/denchmark-link&gt;

from bert import tokenization
&lt;denchmark-code&gt;AttributeError                            Traceback (most recent call last)
&lt;ipython-input-24-f9c3f4b9c6d5&gt; in &lt;module&gt;()
----&gt; 1 from bert import optimization

~/bert/optimization.py in &lt;module&gt;()
     85 
     86 
---&gt; 87 class AdamWeightDecayOptimizer(tf.train.Optimizer):
     88   """A basic Adam optimizer that includes "correct" L2 weight decay."""
     89 

AttributeError: module 'tensorflow_core._api.v2.train' has no attribute 'Optimizer'
&lt;/denchmark-code&gt;

Even running the code in the Colab notebook gives a similar warning (although not an error):
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead. 
		</comment>
		<comment id='13' author='keurcien' date='2019-12-12T12:27:07Z'>
		Thank you!
		</comment>
		<comment id='14' author='keurcien' date='2019-12-12T12:30:13Z'>
		
@loretoparisi I'm getting a similar error trying to run the Bert example here in my own Jupyter notebook running on EC2 (using TF2.0 on an Ubuntu deep learning AMI):
https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb#scrollTo=IhJSe0QHNG7U
from bert import tokenization
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-24-f9c3f4b9c6d5&gt; in &lt;module&gt;()
----&gt; 1 from bert import optimization

~/bert/optimization.py in &lt;module&gt;()
     85 
     86 
---&gt; 87 class AdamWeightDecayOptimizer(tf.train.Optimizer):
     88   """A basic Adam optimizer that includes "correct" L2 weight decay."""
     89 

AttributeError: module 'tensorflow_core._api.v2.train' has no attribute 'Optimizer'

Even running the code in the Colab notebook gives a similar warning (although not an error):
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead. 

EvanZ, try to change the code in the 87 line,
tf.train.Optimizer
-&gt;  tf.keras.optimizers.Optimizer
		</comment>
		<comment id='15' author='keurcien' date='2019-12-28T14:28:55Z'>
		Any updates on this one?
		</comment>
		<comment id='16' author='keurcien' date='2020-01-02T17:36:53Z'>
		&lt;denchmark-link:https://github.com/senthilmk&gt;@senthilmk&lt;/denchmark-link&gt;
 What is your question? This issue is already answered and closed. Please open a new one if necessary.
		</comment>
	</comments>
</bug>