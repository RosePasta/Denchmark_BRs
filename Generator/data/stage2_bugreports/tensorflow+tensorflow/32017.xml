<bug id='32017' author='Victor-Almeida' open_date='2019-08-27T17:59:14Z' closed_time='2019-09-19T04:01:37Z'>
	<summary>Training stalls after saving checkpoint 0</summary>
	<description>
Hello.
I'm trying to run the LibriSpeech problem using tensor2tensor on Google Colab's GPU runtime, but the training stalls after saving checkpoint 0 and opening dynamic library libcublas.so.10.0. There is no error message, it just stops there forever.
I'm posting it here because the stalling point happens on Tensorflow's packages.
Python's version : 3.6.8
Tensorflow's version : 1.14.0
tensor2tensor's version : 1.14.0
CUDA's version : 10.1
OS : Ubuntu 18.04
This is the code
&lt;denchmark-code&gt;from tensor2tensor import models
from tensor2tensor.utils import registry

!t2t-trainer \
    --tmp_dir='/content/gdrive/My Drive/TCC/T2T LibriSpeech/tmp/' \
    --problem='librispeech_clean_small' \
    --model='transformer' \
    --train_steps=10 \
    --hparams_set='transformer_librispeech' \
    --data_dir='/content/gdrive/My Drive/TCC/T2T LibriSpeech/data/' \
    --output_dir='/content/gdrive/My Drive/TCC/T2T LibriSpeech/output/' \
    --worker-gpu=0
&lt;/denchmark-code&gt;

And here's the output :
&lt;denchmark-code&gt;WARNING: Logging before flag parsing goes to stderr.
W0827 17:43:33.747592 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/expert_utils.py:68: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

W0827 17:43:35.111425 139969908836224 lazy_loader.py:50] 
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

W0827 17:43:36.899833 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/adafactor.py:27: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

W0827 17:43:36.900365 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/multistep_optimizer.py:32: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

W0827 17:43:36.911696 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/mesh_tensorflow/ops.py:4237: The name tf.train.CheckpointSaverListener is deprecated. Please use tf.estimator.CheckpointSaverListener instead.

W0827 17:43:36.911862 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/mesh_tensorflow/ops.py:4260: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.

W0827 17:43:36.928164 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/models/research/neural_stack.py:38: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.

W0827 17:43:36.975095 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/rl/gym_utils.py:235: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.

W0827 17:43:36.993708 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/trainer_lib.py:111: The name tf.OptimizerOptions is deprecated. Please use tf.compat.v1.OptimizerOptions instead.

W0827 17:43:37.006869 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensorflow_gan/python/contrib_utils.py:305: The name tf.estimator.tpu.TPUEstimator is deprecated. Please use tf.compat.v1.estimator.tpu.TPUEstimator instead.

W0827 17:43:37.007008 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensorflow_gan/python/contrib_utils.py:310: The name tf.estimator.tpu.TPUEstimatorSpec is deprecated. Please use tf.compat.v1.estimator.tpu.TPUEstimatorSpec instead.

W0827 17:43:38.449517 139969908836224 deprecation_wrapper.py:119] From /usr/local/bin/t2t-trainer:32: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.

W0827 17:43:38.449705 139969908836224 deprecation_wrapper.py:119] From /usr/local/bin/t2t-trainer:32: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.

W0827 17:43:38.449807 139969908836224 deprecation_wrapper.py:119] From /usr/local/bin/t2t-trainer:33: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.

I0827 17:43:38.450179 139969908836224 t2t_trainer.py:155] Found unparsed command-line arguments. Checking if any start with --hp_ and interpreting those as hparams settings.
W0827 17:43:38.450768 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/bin/t2t_trainer.py:165: The name tf.logging.warn is deprecated. Please use tf.compat.v1.logging.warn instead.

W0827 17:43:38.450837 139969908836224 t2t_trainer.py:165] Found unknown flag: --worker-gpu=0
W0827 17:43:38.451183 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/hparams_lib.py:49: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.

W0827 17:43:38.451832 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/trainer_lib.py:839: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

W0827 17:43:38.452693 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/trainer_lib.py:123: The name tf.GraphOptions is deprecated. Please use tf.compat.v1.GraphOptions instead.

W0827 17:43:38.452859 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/trainer_lib.py:129: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

W0827 17:43:38.453019 139969908836224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/trainer_lib.py:242: RunConfig.__init__ (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.
Instructions for updating:
When switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.
I0827 17:43:38.453181 139969908836224 trainer_lib.py:265] Configuring DataParallelism to replicate the model.
I0827 17:43:38.453252 139969908836224 devices.py:76] schedule=continuous_train_and_eval
I0827 17:43:38.453314 139969908836224 devices.py:77] worker_gpu=1
I0827 17:43:38.453381 139969908836224 devices.py:78] sync=False
W0827 17:43:38.453437 139969908836224 devices.py:141] Schedule=continuous_train_and_eval. Assuming that training is running on a single machine.
I0827 17:43:38.453504 139969908836224 devices.py:170] datashard_devices: ['gpu:0']
I0827 17:43:38.453559 139969908836224 devices.py:171] caching_devices: None
I0827 17:43:38.454001 139969908836224 devices.py:172] ps_devices: ['gpu:0']
I0827 17:43:38.454567 139969908836224 estimator.py:209] Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': &lt;tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4cda9f8438&gt;, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_experimental_max_worker_delay_secs': None, '_device_fn': None, '_tf_config': gpu_options {
  per_process_gpu_memory_fraction: 1.0
}
, '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': None, '_log_step_count_steps': 100, '_protocol': None, '_session_config': gpu_options {
  per_process_gpu_memory_fraction: 0.95
}
allow_soft_placement: true
graph_options {
  optimizer_options {
    global_jit_level: OFF
  }
}
isolate_session_state: true
, '_save_checkpoints_steps': 1000, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/content/gdrive/My Drive/TCC/T2T LibriSpeech/output/', 'use_tpu': False, 't2t_device_info': {'num_async_replicas': 1}, 'data_parallelism': &lt;tensor2tensor.utils.expert_utils.Parallelism object at 0x7f4cda9f84a8&gt;}
W0827 17:43:38.454751 139969908836224 model_fn.py:630] Estimator's model_fn (&lt;function T2TModel.make_estimator_model_fn.&lt;locals&gt;.wrapping_model_fn at 0x7f4cda9e7ae8&gt;) includes params argument, but params are not passed to Estimator.
W0827 17:43:38.454877 139969908836224 trainer_lib.py:783] ValidationMonitor only works with --schedule=train_and_evaluate
W0827 17:43:38.455530 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/bin/t2t_trainer.py:328: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.

W0827 17:43:38.458196 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/bin/t2t_trainer.py:344: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.

I0827 17:43:38.487565 139969908836224 estimator_training.py:186] Not using Distribute Coordinator.
I0827 17:43:38.487942 139969908836224 training.py:612] Running training and evaluation locally (non-distributed).
I0827 17:43:38.488237 139969908836224 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1000 or save_checkpoints_secs None.
W0827 17:43:38.493283 139969908836224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0827 17:43:38.502703 139969908836224 problem.py:644] Reading data files from /content/gdrive/My Drive/TCC/T2T LibriSpeech/data/librispeech_clean_small-train*
I0827 17:43:38.543926 139969908836224 problem.py:670] partition: 0 num_data_files: 100
W0827 17:43:38.545797 139969908836224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/data_generators/problem.py:680: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0827 17:43:38.581830 139969908836224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/layers/common_audio.py:92: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
W0827 17:43:38.823341 139969908836224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/layers/common_audio.py:115: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
W0827 17:43:38.987241 139969908836224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/data_reader.py:275: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.
Instructions for updating:
Use eager execution and: 
`tf.data.TFRecordDataset(path)`
W0827 17:43:40.327878 139969908836224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/data_reader.py:395: DatasetV1.output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.data.get_output_shapes(dataset)`.
W0827 17:43:40.328149 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/data_reader.py:398: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.

W0827 17:43:40.328256 139969908836224 data_reader.py:399] Shapes are not fully defined. Assuming batch_size means tokens.
W0827 17:43:40.374079 139969908836224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/experimental/ops/grouping.py:193: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0827 17:43:40.414666 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/data_reader.py:231: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

I0827 17:43:40.470206 139969908836224 estimator.py:1145] Calling model_fn.
I0827 17:43:40.481091 139969908836224 t2t_model.py:2248] Setting T2TModel mode to 'train'
W0827 17:43:40.552857 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/t2t_model.py:244: The name tf.summary.text is deprecated. Please use tf.compat.v1.summary.text instead.

I0827 17:43:41.160171 139969908836224 api.py:255] Using variable initializer: uniform_unit_scaling
I0827 17:43:41.531091 139969908836224 t2t_model.py:2248] Transforming feature 'inputs' with speech_recognition_modality.bottom
W0827 17:43:41.532868 139969908836224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/layers/modalities.py:439: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
I0827 17:43:41.922302 139969908836224 t2t_model.py:2248] Transforming feature 'targets' with symbol_modality_256_384.targets_bottom
I0827 17:43:42.037450 139969908836224 t2t_model.py:2248] Building model body
W0827 17:43:42.094394 139969908836224 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/models/transformer.py:96: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
W0827 17:43:42.130389 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/layers/common_layers.py:3077: The name tf.layers.Dense is deprecated. Please use tf.compat.v1.layers.Dense instead.

W0827 17:43:42.473380 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/layers/common_attention.py:1249: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.

I0827 17:43:49.011597 139969908836224 t2t_model.py:2248] Transforming body output with symbol_modality_256_384.top
W0827 17:43:49.118912 139969908836224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/learning_rate.py:120: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.

I0827 17:43:49.120072 139969908836224 learning_rate.py:29] Base learning rate: 2.000000
I0827 17:43:49.131614 139969908836224 optimize.py:338] Trainable Variables Total size: 70343552
I0827 17:43:49.131888 139969908836224 optimize.py:338] Non-trainable variables Total size: 5
I0827 17:43:49.132170 139969908836224 optimize.py:193] Using optimizer adam
I0827 17:43:59.596418 139969908836224 estimator.py:1147] Done calling model_fn.
I0827 17:43:59.597772 139969908836224 basic_session_run_hooks.py:541] Create CheckpointSaverHook.
I0827 17:44:03.685569 139969908836224 monitored_session.py:240] Graph was finalized.
2019-08-27 17:44:03.685968: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-08-27 17:44:03.708726: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-08-27 17:44:03.898700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-08-27 17:44:03.899340: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x207fb80 executing computations on platform CUDA. Devices:
2019-08-27 17:44:03.899389: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5
2019-08-27 17:44:03.901408: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz
2019-08-27 17:44:03.901570: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x207ea00 executing computations on platform Host. Devices:
2019-08-27 17:44:03.901594: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;
2019-08-27 17:44:03.901797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-08-27 17:44:03.902276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59
pciBusID: 0000:00:04.0
2019-08-27 17:44:03.902614: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-08-27 17:44:03.907500: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-08-27 17:44:03.908556: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-08-27 17:44:03.911851: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-08-27 17:44:03.916549: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-08-27 17:44:03.917606: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-08-27 17:44:03.925044: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-08-27 17:44:03.925147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-08-27 17:44:03.925681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-08-27 17:44:03.926137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-08-27 17:44:03.926182: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-08-27 17:44:03.927269: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-08-27 17:44:03.927290: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-08-27 17:44:03.927300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-08-27 17:44:03.927408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-08-27 17:44:03.927907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-08-27 17:44:03.928376: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2019-08-27 17:44:03.928411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14325 MB memory) -&gt; physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)
2019-08-27 17:44:07.049904: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
I0827 17:44:09.037412 139969908836224 session_manager.py:500] Running local_init_op.
I0827 17:44:09.280463 139969908836224 session_manager.py:502] Done running local_init_op.
I0827 17:44:18.882892 139969908836224 basic_session_run_hooks.py:606] Saving checkpoints for 0 into /content/gdrive/My Drive/TCC/T2T LibriSpeech/output/model.ckpt.
2019-08-27 17:44:39.361151: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Victor-Almeida' date='2019-08-28T06:55:37Z'>
		Same problem. When setting TF_CPP_MIN_VLOG_LEVEL=2, it printing this:
&lt;denchmark-code&gt;[...]
session_manager.py:500] Running local_init_op.
session_manager.py:502] Done running local_init_op.
basic_session_run_hooks.py:606] Saving checkpoints for 0 into ~/trainoutput/librispeech/model.ckpt.
tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
tensorflow/core/framework/model.cc:440] Starting optimization of tunable parameters
tensorflow/core/framework/model.cc:482] Number of tunable parameters: 0
tensorflow/core/kernels/data/model_dataset_op.cc:172] Waiting for 20480 ms.
tensorflow/core/framework/model.cc:440] Starting optimization of tunable parameters
tensorflow/core/framework/model.cc:482] Number of tunable parameters: 0
tensorflow/core/kernels/data/model_dataset_op.cc:172] Waiting for 40960 ms.
tensorflow/core/framework/model.cc:440] Starting optimization of tunable parameters
tensorflow/core/framework/model.cc:482] Number of tunable parameters: 0
tensorflow/core/kernels/data/model_dataset_op.cc:172] Waiting for 60000 ms.
tensorflow/core/framework/model.cc:440] Starting optimization of tunable parameters
tensorflow/core/framework/model.cc:482] Number of tunable parameters: 0
tensorflow/core/kernels/data/model_dataset_op.cc:172] Waiting for 60000 ms.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='Victor-Almeida' date='2019-08-28T08:37:33Z'>
		&lt;denchmark-link:https://github.com/Victor-Almeida&gt;@Victor-Almeida&lt;/denchmark-link&gt;
, Will it possible to provide Google colab link to expedite the trouble-shooting process. Thanks!
		</comment>
		<comment id='3' author='Victor-Almeida' date='2019-08-28T12:38:51Z'>
		I encounter the same problem when executing tensor2tensor on my local machine.
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.6 LTS
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.14.0
Python version: Python 2.7.12
Bazel version (if compiling from source): 0.24.1
GCC/Compiler version (if compiling from source): 5.4.0
CUDA/cuDNN version: CUDA 10.1 / CUDNN 7.5
GPU model and memory: Nvidia 1080Ti 11GB

Describe the current behavior
&lt;denchmark-code&gt;[...]
session_manager.py:500] Running local_init_op.
session_manager.py:502] Done running local_init_op.
basic_session_run_hooks.py:606] Saving checkpoints for 0 into ~/trainoutput/librispeech/model.ckpt.
tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10
tensorflow/core/framework/model.cc:440] Starting optimization of tunable parameters
tensorflow/core/framework/model.cc:482] Number of tunable parameters: 0
tensorflow/core/kernels/data/model_dataset_op.cc:172] Waiting for 20480 ms.
tensorflow/core/framework/model.cc:440] Starting optimization of tunable parameters
tensorflow/core/framework/model.cc:482] Number of tunable parameters: 0
tensorflow/core/kernels/data/model_dataset_op.cc:172] Waiting for 40960 ms.
tensorflow/core/framework/model.cc:440] Starting optimization of tunable parameters
tensorflow/core/framework/model.cc:482] Number of tunable parameters: 0
tensorflow/core/kernels/data/model_dataset_op.cc:172] Waiting for 60000 ms.
tensorflow/core/framework/model.cc:440] Starting optimization of tunable parameters
tensorflow/core/framework/model.cc:482] Number of tunable parameters: 0
tensorflow/core/kernels/data/model_dataset_op.cc:172] Waiting for 60000 ms.
&lt;/denchmark-code&gt;

Code to reproduce the issue
Install tensor2tensor (Release v1.14.0)
&lt;denchmark-code&gt;export TF_CPP_MIN_VLOG_LEVEL=2
t2t-trainer --problem=librispeech --model=transformer --data_dir=~/datasets/t2t/librispeech/ --output_dir=~/trainoutput/librispeech/ --hparams_set=transformer_librispeech --worker_gpu=1
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='Victor-Almeida' date='2019-08-28T15:30:05Z'>
		&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3551652/Training_Speech_Recognition_Model_with_tensor2tensor.zip&gt;Here&lt;/denchmark-link&gt;
 it is.
		</comment>
		<comment id='5' author='Victor-Almeida' date='2019-08-30T18:12:26Z'>
		I'm experiencing the same issue.  If I export TF_CPP_MIN_VLOG_LEVEL=2 before running t2t-trainer I get the following error, just prior to the stall.  It might be unique to my custom librispeech-like --problem -- but curious if others are seeing that too?  Perhaps related, I've been fighting with a similar message when attempting to load an older t2t saved model into a current TFServing.
&lt;denchmark-code&gt;2019-08-30 17:54:05.447740: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 3917 allocator_name: "cpu" }
2019-08-30 17:54:05.447758: I tensorflow/core/common_runtime/executor.cc:2240] [/job:localhost/replica:0/task:0/device:CPU:0] Executor start aborting: Unimplemented: Generic conv implementation only supports NHWC tensor format for now.
	 [[{{node Conv2D}}]]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='Victor-Almeida' date='2019-09-04T14:46:59Z'>
		&lt;denchmark-link:https://github.com/Victor-Almeida&gt;@Victor-Almeida&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tenghaha&gt;@tenghaha&lt;/denchmark-link&gt;
 Do you specifically need those versions of cuDNN and CUDA? If not the official tested configuration for tf1.14_gpu looks like cuDNN=7.4, CUDA=10.0, see &lt;denchmark-link:https://www.tensorflow.org/install/source#tested_build_configurations&gt;this&lt;/denchmark-link&gt;
. And you might have an easier time debugging it in comparison to something that works in CPU mode as well as in comparison to a trivially simple t2t problem and model.
		</comment>
		<comment id='7' author='Victor-Almeida' date='2019-09-04T23:19:39Z'>
		I'm seeing the same issues running from the official 1.14.0-gpu docker image.  As well as the nightly-gpu.  The training works fine with the simple models (eg --problem=image_mnist --model=shake_shake).
Seems to be some issue with the interaction between the T2T Librispeech problem and the new tensorflow versions.
		</comment>
		<comment id='8' author='Victor-Almeida' date='2019-09-04T23:44:17Z'>
		&lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
, could you triage? I'm guessing I was assigned due to the "checkpoint" keyword, but it looks like checkpointing completes and this is either input pipeline optimization related or GPU related.
		</comment>
		<comment id='9' author='Victor-Almeida' date='2019-09-05T17:25:10Z'>
		Can you collect a stacktrace at the time your program hangs a share a link to it?
		</comment>
		<comment id='10' author='Victor-Almeida' date='2019-09-05T19:07:57Z'>
		Can you clarify what you're looking for?  Since the process does not halt, there is no stacktrace.  A representative end-of-logging is shown in &lt;denchmark-link:https://github.com/cantwbr&gt;@cantwbr&lt;/denchmark-link&gt;
 post (github.com/&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/32017&gt;/issues/32017&lt;/denchmark-link&gt;
#issuecomment-525726395).
With TF_CPP_MIN_VLOG_LEVEL=2 enabled the logging is extremely verbose and probably not practical to post here.
		</comment>
		<comment id='11' author='Victor-Almeida' date='2019-09-05T19:48:24Z'>
		Here is a snip of the last ~100 lines from a run of the basic problem, run via:
t2t-trainer --problem=librispeech_clean_small --model=transformer --output_dir=/models/JUNK --data_dir=/data/ --save_checkpoints_secs=1800 --schedule=train --hparams_set=transformer_tiny
&lt;denchmark-code&gt;2019-09-05 19:38:13.932318: W tensorflow/core/common_runtime/executor.cc:2428]     Input 147: Tensor&lt;type: float shape: [0], bytes: 0&gt;
2019-09-05 19:38:13.932329: W tensorflow/core/common_runtime/executor.cc:2428]     Input 148: Tensor&lt;type: int32 shape: [], bytes: 4&gt;
2019-09-05 19:38:13.932341: W tensorflow/core/common_runtime/executor.cc:2428]     Input 149: Tensor&lt;type: float shape: [0], bytes: 0&gt;
2019-09-05 19:38:13.932349: W tensorflow/core/common_runtime/executor.cc:2428]     Input 150: Tensor&lt;type: int32 shape: [1], bytes: 4&gt;
2019-09-05 19:38:13.932357: W tensorflow/core/common_runtime/executor.cc:2428]     Input 151: Tensor&lt;type: int32 shape: [1], bytes: 4&gt;
2019-09-05 19:38:13.932366: W tensorflow/core/common_runtime/executor.cc:2428]     Input 152: Tensor&lt;type: int32 shape: [1], bytes: 4&gt;
2019-09-05 19:38:13.932373: W tensorflow/core/common_runtime/executor.cc:2428]     Input 153: Tensor&lt;type: float shape: [0], bytes: 0&gt;
2019-09-05 19:38:13.932382: W tensorflow/core/common_runtime/executor.cc:2428]     Input 154: Tensor&lt;type: float shape: [], bytes: 4&gt;
2019-09-05 19:38:13.932390: W tensorflow/core/common_runtime/executor.cc:2428]     Input 155: Tensor&lt;type: float shape: [0], bytes: 0&gt;
2019-09-05 19:38:13.932396: W tensorflow/core/common_runtime/executor.cc:2428]     Input 156: Tensor&lt;type: int32 shape: [], bytes: 4&gt;
2019-09-05 19:38:13.932401: W tensorflow/core/common_runtime/executor.cc:2428]     Input 157: Tensor&lt;type: int32 shape: [], bytes: 4&gt;
2019-09-05 19:38:13.932406: W tensorflow/core/common_runtime/executor.cc:2428]     Input 158: Tensor&lt;type: float shape: [0], bytes: 0&gt;
2019-09-05 19:38:13.932412: W tensorflow/core/common_runtime/executor.cc:2428]     Input 159: Tensor&lt;type: float shape: [0], bytes: 0&gt;
2019-09-05 19:38:13.932417: W tensorflow/core/common_runtime/executor.cc:2428]     Input 160: Tensor&lt;type: float shape: [0], bytes: 0&gt;
2019-09-05 19:38:13.932425: W tensorflow/core/common_runtime/executor.cc:2428]     Input 161: Tensor&lt;type: float shape: [0], bytes: 0&gt;
2019-09-05 19:38:13.932435: W tensorflow/core/common_runtime/executor.cc:2428]     Input 162: Tensor&lt;type: float shape: [0], bytes: 0&gt;
2019-09-05 19:38:13.932443: W tensorflow/core/common_runtime/executor.cc:2428]     Input 163: Tensor&lt;type: float shape: [0], bytes: 0&gt;
2019-09-05 19:38:13.932448: W tensorflow/core/common_runtime/executor.cc:2428]     Input 164: Tensor&lt;type: float shape: [0], bytes: 0&gt;
2019-09-05 19:38:13.932456: W tensorflow/core/common_runtime/executor.cc:2428]     Input 165: Tensor&lt;type: float shape: [250,80,3], bytes: 240000&gt;
2019-09-05 19:38:13.932461: W tensorflow/core/common_runtime/executor.cc:2428]     Input 166: Tensor&lt;type: int32 shape: [], bytes: 4&gt;
2019-09-05 19:38:13.932466: W tensorflow/core/common_runtime/executor.cc:2428]     Input 167: Tensor&lt;type: int64 shape: [1], bytes: 8&gt;
2019-09-05 19:38:13.932470: W tensorflow/core/common_runtime/executor.cc:2428]     Input 168: Tensor&lt;type: float shape: [0], bytes: 0&gt;
2019-09-05 19:38:13.932475: W tensorflow/core/common_runtime/executor.cc:2428]     Input 169: Tensor&lt;type: int64 shape: [201], bytes: 1608&gt;
2019-09-05 19:38:13.932480: W tensorflow/core/common_runtime/executor.cc:2428]     Input 170: Tensor&lt;type: float shape: [0], bytes: 0&gt;
2019-09-05 19:38:13.932486: W tensorflow/core/common_runtime/executor.cc:2436]     Total bytes 729120
2019-09-05 19:38:13.932497: I tensorflow/core/framework/log_memory.cc:34] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 25835 allocator_name: "cpu" }
2019-09-05 19:38:13.932512: I tensorflow/core/common_runtime/executor.cc:1868] Synchronous kernel done: 102 step -8435589056233180824 {{node Conv2D}} = Conv2D[T=DT_FLOAT, data_format="NCHW", dilations=[1, 1, 1, 1], explicit_paddings=[], padding="SAME", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/device:CPU:0"](Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer/_4, Conv2D/filter) device: /job:localhost/replica:0/task:0/device:CPU:0
2019-09-05 19:38:13.932520: I tensorflow/core/framework/log_memory.cc:34] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 1501 allocator_name: "gpu_host_bfc" }
2019-09-05 19:38:13.932527: I tensorflow/core/common_runtime/bfc_allocator.cc:526] DeallocateRaw gpu_host_bfc 487360
2019-09-05 19:38:13.932536: I tensorflow/core/common_runtime/executor.cc:2249] [/job:localhost/replica:0/task:0/device:CPU:0] Executor start aborting: Unimplemented: The Conv2D op currently only supports the NHWC tensor format on the CPU. The op was given the format: NCHW
	 [[{{node Conv2D}}]]
2019-09-05 19:38:13.932547: W tensorflow/core/common_runtime/executor.cc:2007] 0x7f0368013e90 Compute status: Unimplemented: The Conv2D op currently only supports the NHWC tensor format on the CPU. The op was given the format: NCHW
	 [[{{node Conv2D}}]]
	 [[Shape_3-0-0-DataFormatVecPermuteNCHWToNHWC-LayoutOptimizer/_10]]
2019-09-05 19:38:13.932557: I tensorflow/core/common_runtime/executor.cc:1777] Async kernel done: 110 step -8435589056233180824 {{node Shape_3-0-0-DataFormatVecPermuteNCHWToNHWC-LayoutOptimizer/_10}} = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=-2425652197422393730, tensor_name="edge_154_Shape_3-0-0-DataFormatVecPermuteNCHWToNHWC-LayoutOptimizer", tensor_type=DT_INT32, _device="/job:localhost/replica:0/task:0/device:CPU:0"]() device: /job:localhost/replica:0/task:0/device:CPU:0
2019-09-05 19:38:13.932567: W tensorflow/core/common_runtime/executor.cc:2007] 0x7f0368013e90 Compute status: Unimplemented: The Conv2D op currently only supports the NHWC tensor format on the CPU. The op was given the format: NCHW
	 [[{{node Conv2D}}]]
	 [[mul_2-0-0-TransposeNCHWToNHWC-LayoutOptimizer/_14]]
2019-09-05 19:38:13.932576: I tensorflow/core/common_runtime/executor.cc:1777] Async kernel done: 117 step -8435589056233180824 {{node mul_2-0-0-TransposeNCHWToNHWC-LayoutOptimizer/_14}} = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=-2425652197422393730, tensor_name="edge_168_mul_2-0-0-TransposeNCHWToNHWC-LayoutOptimizer", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]() device: /job:localhost/replica:0/task:0/device:CPU:0
2019-09-05 19:38:13.932586: W tensorflow/core/common_runtime/executor.cc:2007] 0x7f03680133b0 Compute status: Unimplemented: The Conv2D op currently only supports the NHWC tensor format on the CPU. The op was given the format: NCHW
	 [[{{node Conv2D}}]]
	 [[mul_2/_12]]
2019-09-05 19:38:13.932592: W tensorflow/core/common_runtime/executor.cc:2442] Dumping state
2019-09-05 19:38:13.932597: W tensorflow/core/common_runtime/executor.cc:2444] 
2019-09-05 19:38:13.932602: W tensorflow/core/common_runtime/executor.cc:2448]   Iteration:
2019-09-05 19:38:13.932611: W tensorflow/core/common_runtime/executor.cc:2366]     Pending Node: {name:'Shape_3-0-0-DataFormatVecPermuteNCHWToNHWC-LayoutOptimizer' id:10 op device:{} def:{{{node Shape_3-0-0-DataFormatVecPermuteNCHWToNHWC-LayoutOptimizer}} = DataFormatVecPermute[T=DT_INT32, _kernel="host", dst_format="NHWC", src_format="NCHW", _device="/job:localhost/replica:0/task:0/device:GPU:0"](Shape_3/_8)}}
2019-09-05 19:38:13.932618: W tensorflow/core/common_runtime/executor.cc:2371]       Input 0: Tensor&lt;type: float shape: [0]&gt;
2019-09-05 19:38:13.932628: W tensorflow/core/common_runtime/executor.cc:2366]     Pending Node: {name:'Shape_3-0-0-DataFormatVecPermuteNCHWToNHWC-LayoutOptimizer/_9' id:11 op device:{} def:{{{node Shape_3-0-0-DataFormatVecPermuteNCHWToNHWC-LayoutOptimizer/_9}} = _HostSend[T=DT_INT32, client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=-2425652197422393730, tensor_name="edge_154_Shape_3-0-0-DataFormatVecPermuteNCHWToNHWC-LayoutOptimizer", _device="/job:localhost/replica:0/task:0/device:GPU:0"](Shape_3-0-0-DataFormatVecPermuteNCHWToNHWC-LayoutOptimizer)}}
2019-09-05 19:38:13.932635: W tensorflow/core/common_runtime/executor.cc:2371]       Input 0: Tensor&lt;type: float shape: [0]&gt;
2019-09-05 19:38:13.932642: W tensorflow/core/common_runtime/executor.cc:2366]     Pending Node: {name:'mul_2-0-0-TransposeNCHWToNHWC-LayoutOptimizer' id:13 op device:{} def:{{{node mul_2-0-0-TransposeNCHWToNHWC-LayoutOptimizer}} = Transpose[T=DT_FLOAT, Tperm=DT_INT32, _device="/job:localhost/replica:0/task:0/device:GPU:0"](mul_2/_12, mul_2-0-0-PermConstNCHWToNHWC-LayoutOptimizer)}}
2019-09-05 19:38:13.932648: W tensorflow/core/common_runtime/executor.cc:2371]       Input 0: Tensor&lt;type: float shape: [0]&gt;
2019-09-05 19:38:13.932653: W tensorflow/core/common_runtime/executor.cc:2371]       Input 1: Tensor&lt;type: int32 shape: [4]&gt;
2019-09-05 19:38:13.932661: W tensorflow/core/common_runtime/executor.cc:2366]     Pending Node: {name:'mul_2-0-0-TransposeNCHWToNHWC-LayoutOptimizer/_13' id:14 op device:{} def:{{{node mul_2-0-0-TransposeNCHWToNHWC-LayoutOptimizer/_13}} = _Send[T=DT_FLOAT, client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=-2425652197422393730, tensor_name="edge_168_mul_2-0-0-TransposeNCHWToNHWC-LayoutOptimizer", _device="/job:localhost/replica:0/task:0/device:GPU:0"](mul_2-0-0-TransposeNCHWToNHWC-LayoutOptimizer)}}
2019-09-05 19:38:13.932668: W tensorflow/core/common_runtime/executor.cc:2371]       Input 0: Tensor&lt;type: float shape: [0]&gt;
2019-09-05 19:38:13.932676: W tensorflow/core/common_runtime/executor.cc:2385]     Active Node: {name:'Shape_3/_8' id:9 op device:{} def:{{{node Shape_3/_8}} = _HostRecv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device_incarnation=-3160500913623757191, tensor_name="edge_151_Shape_3", tensor_type=DT_INT32, _device="/job:localhost/replica:0/task:0/device:GPU:0"]()}}
2019-09-05 19:38:13.932685: W tensorflow/core/common_runtime/executor.cc:2385]     Active Node: {name:'mul_2/_12' id:12 op device:{} def:{{{node mul_2/_12}} = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device_incarnation=-3160500913623757191, tensor_name="edge_166_mul_2", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:GPU:0"]()}}
2019-09-05 19:38:13.932692: W tensorflow/core/common_runtime/executor.cc:2428]     Input 0: Tensor&lt;type: float shape: [0], bytes: 0&gt;
2019-09-05 19:38:13.932697: W tensorflow/core/common_runtime/executor.cc:2428]     Input 1: Tensor&lt;type: float shape: [0], bytes: 0&gt;
2019-09-05 19:38:13.932702: W tensorflow/core/common_runtime/executor.cc:2428]     Input 2: Tensor&lt;type: float shape: [0], bytes: 0&gt;
2019-09-05 19:38:13.932708: W tensorflow/core/common_runtime/executor.cc:2428]     Input 3: Tensor&lt;type: float shape: [0], bytes: 0&gt;
2019-09-05 19:38:13.932715: W tensorflow/core/common_runtime/executor.cc:2428]     Input 4: Tensor&lt;type: float shape: [0], bytes: 0&gt;
2019-09-05 19:38:13.932721: W tensorflow/core/common_runtime/executor.cc:2428]     Input 5: Tensor&lt;type: float shape: [0], bytes: 0&gt;
2019-09-05 19:38:13.932726: W tensorflow/core/common_runtime/executor.cc:2428]     Input 6: Tensor&lt;type: float shape: [0], bytes: 0&gt;
2019-09-05 19:38:13.932731: W tensorflow/core/common_runtime/executor.cc:2428]     Input 7: Tensor&lt;type: int32 shape: [4], bytes: 16&gt;
2019-09-05 19:38:13.932735: W tensorflow/core/common_runtime/executor.cc:2428]     Input 8: Tensor&lt;type: float shape: [0], bytes: 0&gt;
2019-09-05 19:38:13.932741: W tensorflow/core/common_runtime/executor.cc:2436]     Total bytes 16
2019-09-05 19:38:13.932748: I tensorflow/core/common_runtime/executor.cc:1777] Async kernel done: 12 step -8435589056233180824 {{node mul_2/_12}} = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device_incarnation=-3160500913623757191, tensor_name="edge_166_mul_2", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:GPU:0"]() device: /job:localhost/replica:0/task:0/device:GPU:0
2019-09-05 19:38:13.932755: I tensorflow/core/common_runtime/executor.cc:2249] [/job:localhost/replica:0/task:0/device:GPU:0] Executor start aborting: Unimplemented: The Conv2D op currently only supports the NHWC tensor format on the CPU. The op was given the format: NCHW
	 [[{{node Conv2D}}]]
	 [[mul_2/_12]]
2019-09-05 19:38:13.932766: W tensorflow/core/common_runtime/executor.cc:2007] 0x7f03680133b0 Compute status: Unimplemented: The Conv2D op currently only supports the NHWC tensor format on the CPU. The op was given the format: NCHW
	 [[{{node Conv2D}}]]
	 [[Shape_3/_8]]
2019-09-05 19:38:13.932775: I tensorflow/core/common_runtime/executor.cc:1777] Async kernel done: 9 step -8435589056233180824 {{node Shape_3/_8}} = _HostRecv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device_incarnation=-3160500913623757191, tensor_name="edge_151_Shape_3", tensor_type=DT_INT32, _device="/job:localhost/replica:0/task:0/device:GPU:0"]() device: /job:localhost/replica:0/task:0/device:GPU:0
2019-09-05 19:38:13.932795: I tensorflow/core/common_runtime/process_function_library_runtime.cc:927] Component function execution failed: Unimplemented: The Conv2D op currently only supports the NHWC tensor format on the CPU. The op was given the format: NCHW
	 [[{{node Conv2D}}]]
2019-09-05 19:38:13.932797: I tensorflow/core/common_runtime/process_function_library_runtime.cc:927] Component function execution failed: Unimplemented: The Conv2D op currently only supports the NHWC tensor format on the CPU. The op was given the format: NCHW
	 [[{{node Conv2D}}]]
	 [[mul_2/_12]]
2019-09-05 19:38:13.932842: I tensorflow/core/framework/log_memory.cc:34] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 4294 allocator_name: "cpu" }
2019-09-05 19:38:13.932864: I tensorflow/core/framework/log_memory.cc:34] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 4295 allocator_name: "cpu" }
2019-09-05 19:38:16.281181: I tensorflow/core/framework/model.cc:892] Starting optimization of tunable parameters with HillClimb
2019-09-05 19:38:16.281277: I tensorflow/core/framework/model.cc:943] Number of tunable parameters: 0
2019-09-05 19:38:16.281328: I tensorflow/core/kernels/data/model_dataset_op.cc:192] Waiting for 10240 ms.
2019-09-05 19:38:26.521615: I tensorflow/core/framework/model.cc:892] Starting optimization of tunable parameters with HillClimb
2019-09-05 19:38:26.521725: I tensorflow/core/framework/model.cc:943] Number of tunable parameters: 0
2019-09-05 19:38:26.521775: I tensorflow/core/kernels/data/model_dataset_op.cc:192] Waiting for 20480 ms.
2019-09-05 19:38:47.002024: I tensorflow/core/framework/model.cc:892] Starting optimization of tunable parameters with HillClimb
2019-09-05 19:38:47.002125: I tensorflow/core/framework/model.cc:943] Number of tunable parameters: 0
2019-09-05 19:38:47.002175: I tensorflow/core/kernels/data/model_dataset_op.cc:192] Waiting for 40960 ms.
2019-09-05 19:39:27.962480: I tensorflow/core/framework/model.cc:892] Starting optimization of tunable parameters with HillClimb
2019-09-05 19:39:27.962617: I tensorflow/core/framework/model.cc:943] Number of tunable parameters: 0
2019-09-05 19:39:27.962692: I tensorflow/core/kernels/data/model_dataset_op.cc:192] Waiting for 60000 ms.```
&lt;/denchmark-code&gt;

		</comment>
		<comment id='12' author='Victor-Almeida' date='2019-09-05T20:30:08Z'>
		It does not halt, but it does stalls and it would be good to understand what is each thread of the program doing (e.g. if this is a deadlock). The log itself does not provide enough information to form a hypothesis about what could be going on and the first thing I would do to debug further is to use gdb to connect to running process to see what is each thread doing. As far as I know, this will not be possible running the program in a colab though.
&lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;
 could you please loop in someone from the trainer2trainer project as I am not familiar with that project.
		</comment>
		<comment id='13' author='Victor-Almeida' date='2019-09-05T21:08:57Z'>
		I've attached a gdb to the 'stalled' process, would you want to see a bt ?
		</comment>
		<comment id='14' author='Victor-Almeida' date='2019-09-05T21:22:51Z'>
		Also, FWIW, the lead of the Tensor2Tensor team referred this to Tensorflow support in: &lt;denchmark-link:https://github.com/tensorflow/tensor2tensor/issues/1643&gt;tensorflow/tensor2tensor#1643&lt;/denchmark-link&gt;

		</comment>
		<comment id='15' author='Victor-Almeida' date='2019-09-05T21:34:24Z'>
		Could you run thread apply all bt and post the result in pastebin and share a link to it here? Thanks.
		</comment>
		<comment id='16' author='Victor-Almeida' date='2019-09-06T02:36:33Z'>
		&lt;denchmark-link:https://pastebin.com/w3ZevdSs&gt;https://pastebin.com/w3ZevdSs&lt;/denchmark-link&gt;

The full trace was 1.6MB in size, but most of the higher number threads were identical.  I've tried to edit out the mostly redundant traces.
		</comment>
		<comment id='17' author='Victor-Almeida' date='2019-09-06T17:47:23Z'>
		Thank you Michael. At least some of the threads seem to be waiting in tf.data. &lt;denchmark-link:https://github.com/rachellim&gt;@rachellim&lt;/denchmark-link&gt;
 could you please try to reproduce and investigate this issue? Thank you.
		</comment>
		<comment id='18' author='Victor-Almeida' date='2019-09-06T18:36:33Z'>
		Thank you!
&lt;denchmark-link:https://github.com/rachellim&gt;@rachellim&lt;/denchmark-link&gt;
 please let me know if you have any trouble reproducing the issue (on gpu).
My setup is via Nvidia Docker Hub

tensorflow/tensorflow:1.14.0-gpu
or
tensorflow/tensorflow:nightly-gpu
and
pip2 install tensorflow-hub &amp;&amp; pip2 install tensor2tensor
apt-get update &amp;&amp; apt-get install sox

t2t-trainer --problem=librispeech_clean_small --model=transformer --output_dir=/models/JUNK --data_dir=/data/ --save_checkpoints_secs=1800 --schedule=train --hparams_set=transformer_librispeech
Note: you'll need to include the  --generate_data flag the first time you run to dl and prep the dataset.
		</comment>
		<comment id='19' author='Victor-Almeida' date='2019-09-09T15:53:25Z'>
		Did a little more testing to try to narrow down in which version of tensorflow this issue crops up...
Testing with T2T 1.13.4:
TF 1.13.2  works
TF 1.14.0+ hangs
		</comment>
		<comment id='20' author='Victor-Almeida' date='2019-09-13T21:33:52Z'>
		I'm having trouble reproducing this with the following setup:
tensorflow-gpu: 1.14.0
tensor2tensor: 1.14.0
$ t2t-trainer --problem=librispeech_clean_small --model=transformer --output_dir=/tmp/t2t_output --data_dir=/tmp/t2t_data/ --save_checkpoints_secs=1800 --schedule=train --hparams_set=transformer_librispeech
(It runs multiple steps without hanging)
Are you encountering this issue with non-gpu tensorflow as well, or just tensorflow-gpu?
		</comment>
		<comment id='21' author='Victor-Almeida' date='2019-09-13T21:55:03Z'>
		Hmm, just setup a non-gpu container and it is running w/o issue there.
Of course for production training we need to utilize the GPUs...
		</comment>
		<comment id='22' author='Victor-Almeida' date='2019-09-13T22:03:04Z'>
		Yup -- just trying to further narrow down the possible sources of this issue. Let me dig into this a little further. Thanks!
		</comment>
		<comment id='23' author='Victor-Almeida' date='2019-09-13T22:07:39Z'>
		Thanks!
In case it is helpful, the host NVIDIA GPU driver I currently have loaded is: 430.34 (I see the current version is 430.50).
UPDATE: installed 430.50 gpu driver and got same stall behavior
		</comment>
		<comment id='24' author='Victor-Almeida' date='2019-09-13T22:18:13Z'>
		Could you guys check if you can resume training from checkpoint? I am
getting the checkpoint not valid error with this same setup on normal
runtime.

Em sex, 13 de set de 2019 19:14, Michael Schonwetter &lt;
notifications@github.com&gt; escreveu:
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


 Thanks!

 In case it is helpful, the host NVIDIA GPU driver I currently have loaded
 is: 430.34 (I see the current version is 430.50).

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#32017?email_source=notifications&amp;email_token=AJ6VEEHJVI5AWQZ5M2QHVKLQJQGD5A5CNFSM4IQHWXEKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD6WJZ6A#issuecomment-531406072&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AJ6VEEBDXBCVINGXZU2FAYTQJQGD5ANCNFSM4IQHWXEA&gt;
 .



		</comment>
		<comment id='25' author='Victor-Almeida' date='2019-09-13T22:49:20Z'>
		&lt;denchmark-link:https://github.com/Victor-Almeida&gt;@Victor-Almeida&lt;/denchmark-link&gt;
, just tested per your request, with the -gpu version I was able to save a checkpoint, and resume from that checkpoint w/o error. (TF1.14.0, T2T1.14.0)
		</comment>
		<comment id='26' author='Victor-Almeida' date='2019-09-14T16:10:26Z'>
		In case it helps, I ran with  enabled, and here is the very end of the log (where it hangs):
&lt;denchmark-link:https://pastebin.com/xvxzyXkS&gt;https://pastebin.com/xvxzyXkS&lt;/denchmark-link&gt;

		</comment>
		<comment id='27' author='Victor-Almeida' date='2019-09-17T20:22:13Z'>
		&lt;denchmark-link:https://github.com/rachellim&gt;@rachellim&lt;/denchmark-link&gt;
 When you are getting a working test:

Are you testing with the tensorflow/tensorflow:1.14.0-gpu docker image?
What NVidia GPU are you using?

Do you have any other suggestions for how I could help isolate the cause of the hang?
		</comment>
		<comment id='28' author='Victor-Almeida' date='2019-09-17T21:08:30Z'>
		I managed to get a repro, now trying to get to the bottom of it! (I'm using a GTX 1080 with driver version 430.34)
		</comment>
		<comment id='29' author='Victor-Almeida' date='2019-09-19T03:17:35Z'>
		Just adding some more findings here.
Environments:

tensorflow: 1.14.0
tensor2tensor: tried 1.11, 1.13, 1.14, results were the same.

When I use the default hparams set (transformer_librispeech), the training will hang.
I explored the problem specific hparams (defined in tensor2tensor/data_generators/speech_recognition.py) and tried the below changes which can avoid the hang:

set audio_add_delta_deltas from True to False
OR
set audio_preproc_in_bottom from False to True (this moves the feature generation process from the preprocessing stage to the bottom of the transformer encoder)

It looks like the hang is due to some audio feature generation operations in preprocessing stage.  When I removed them from preprocessing, the hang is gone.  Hope this helps in pinpointing the cause.
In summary, either of the below can make the training started:

Use tensorflow-gpu 1.13.2.
OR
Use tensorflow-gpu 1.14.0, with the above mentioned changes in hparams.

		</comment>
		<comment id='30' author='Victor-Almeida' date='2019-09-19T04:01:37Z'>
		Thanks for flagging it and the repro instructions. I found the issue in . Fix: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/6274f037d4acc9d04cd4aafbda7547a3d89e5674&gt;6274f03&lt;/denchmark-link&gt;

		</comment>
		<comment id='31' author='Victor-Almeida' date='2019-09-19T04:01:38Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=32017&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=32017&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='32' author='Victor-Almeida' date='2019-09-19T04:04:53Z'>
		The cause of the hanging here is that parallel_interleave_dataset_op.cc doesn't handle iterator creation errors correctly when the sloppy=True param is set. This fix above makes it handle the error more gracefully (i.e. it raises an error instead of hanging), but the actual dataset iterator creation error here is:
&lt;denchmark-code&gt;  (0) Unimplemented:  The Conv2D op currently only supports the NHWC tensor format on the CPU. The op was given the format: NCHW
         [[{{node Conv2D}}]]
  (1) Unimplemented:  The Conv2D op currently only supports the NHWC tensor format on the CPU. The op was given the format: NCHW
         [[{{node Conv2D}}]]
         [[Shape_3/_8]]
&lt;/denchmark-code&gt;

in the preprocessing function in the t2t code.
		</comment>
		<comment id='33' author='Victor-Almeida' date='2019-09-19T13:06:51Z'>
		&lt;denchmark-link:https://github.com/rachellim&gt;@rachellim&lt;/denchmark-link&gt;
 Thank you very much for isolating the cause of the hang!
Can you clarify if the Unimplemented Conv2D errors are still present after applying fix 6274f03?
Specifically does this (Unimplemented issue) still need to get fixed in the T2T code?
UPDATE:  By setting sloppy=False I can replicate the halt on Conv2D errors.  Do you suggest a new issue to resolve why the same T2T code works fine in TF 1.13.2 but crashes in TF 1.14.0?
		</comment>
	</comments>
</bug>