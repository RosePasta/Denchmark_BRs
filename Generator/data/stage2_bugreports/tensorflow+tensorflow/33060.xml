<bug id='33060' author='pooyadavoodi' open_date='2019-10-04T23:29:29Z' closed_time='2019-11-05T12:16:15Z'>
	<summary>Possible corruption in Load or freeze in TF2.0</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): custom code
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary 2.0.0
TensorFlow version (use command below): 2.0
Python version: 3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 7.6.4
GPU model and memory: Volta/Turing

Describe the current behavior
When I run load and freeze in two different python functions, I get a crash that says:
AssertionError: Called a function referencing variables which have been deleted. This likely means that function-local variables were created and not referenced elsewhere in the program. This is generally a mistake; consider storing variables in an object attribute on first call.
But when I run both of load and freeze in the same python function, then it works as expected.
Describe the expected behavior
Calling load and freeze in different python functions should work unless there is some hidden assumption in TF 2.0. My guess is that there is a leak or some dependency between the two APIs.
Code to reproduce the issue
&lt;denchmark-code&gt;import argparse
import numpy as np
import tensorflow as tf
from tensorflow.python.saved_model import signature_constants
from tensorflow.python.saved_model import tag_constants
from tensorflow.python.framework import convert_to_constants

def get_dataset(batch_size, input_size):
  features = np.random.normal(
      loc=112, scale=70,
      size=(batch_size, input_size, input_size, 3)).astype(np.float32)
  features = np.clip(features, 0.0, 255.0)
  features = tf.convert_to_tensor(value=tf.compat.v1.get_variable(
      "features", dtype=tf.float32, initializer=tf.constant(features)))
  dataset = tf.data.Dataset.from_tensor_slices([features])
  dataset = dataset.repeat()
  return dataset


def run_func(saved_model_dir):

  def load_model():
    saved_model_loaded = tf.saved_model.load(
        saved_model_dir, tags=[tag_constants.SERVING])
    graph_func = saved_model_loaded.signatures[
        signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY]
    return graph_func
  graph_func = load_model()

# Replace load_model function and its call with the following to make it work
#  saved_model_loaded = tf.saved_model.load(
#      saved_model_dir, tags=[tag_constants.SERVING])
#  graph_func = saved_model_loaded.signatures[
#      signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY]

  frozen_func = convert_to_constants.convert_variables_to_constants_v2(graph_func)
  def wrap_func(*args, **kwargs):
    return frozen_func(*args, **kwargs)[0]
  inference_graph_func = wrap_func
  dataset = get_dataset(batch_size=8, input_size=224)
  for i, (batch_feats) in enumerate(dataset):
    batch_preds = inference_graph_func(batch_feats).numpy()
    print(batch_preds)


if __name__ == '__main__':
  parser = argparse.ArgumentParser(description='Evaluate model')
  parser.add_argument('--saved_model_dir', type=str, default=None,
                      help='Directory containing a particular saved model.')
  args = parser.parse_args()

  run_func(saved_model_dir=args.saved_model_dir)
&lt;/denchmark-code&gt;

Other info / logs
Command line to run: python tfv2_load_issue.py --saved_model_dir path_to_saved_model
&lt;denchmark-code&gt;2019-10-04 20:35:48.913525: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:734] Optimization results for grappler item: graph_to_optimize
2019-10-04 20:35:48.913553: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:736]   function_optimizer: Graph size after: 1314 nodes (1044), 2670 edges (2400), time = 23.171ms.
2019-10-04 20:35:48.913557: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:736]   function_optimizer: function_optimizer did nothing. time = 0.371ms.
Traceback (most recent call last):
  File "run.py", line 51, in &lt;module&gt;
    run_func(saved_model_dir=args.saved_model_dir)
  File "run.py", line 35, in run_func
    frozen_func = convert_to_constants.convert_variables_to_constants_v2(graph_func)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/convert_to_constants.py", line 411, in convert_variables_to_constants_v2
    tensor_data = _get_tensor_data(func)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/convert_to_constants.py", line 182, in _get_tensor_data
    for var in func.graph.variables:
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py", line 435, in variables
    "Called a function referencing variables which have been deleted. "
AssertionError: Called a function referencing variables which have been deleted. This likely means that function-local variables were created and not referenced elsewhere in the program. This is generally a mistake; consider storing variables in an object attribute on first call.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='pooyadavoodi' date='2019-10-09T08:42:56Z'>
		&lt;denchmark-link:https://github.com/pooyadavoodi&gt;@pooyadavoodi&lt;/denchmark-link&gt;
,
I have executed the code you provided, with the Exported Model of Iris Data and I have executed the command line argument provided by you but I didn't get any error. It printed the values. Can you please help me reproduce the issue. Thanks!
		</comment>
		<comment id='2' author='pooyadavoodi' date='2019-10-21T12:48:48Z'>
		&lt;denchmark-link:https://github.com/pooyadavoodi&gt;@pooyadavoodi&lt;/denchmark-link&gt;
,
Can you please let us know if your issue is resolved. Else, can you please help us reproduce the issue. Thanks!
		</comment>
		<comment id='3' author='pooyadavoodi' date='2019-11-05T12:16:14Z'>
		Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
		<comment id='4' author='pooyadavoodi' date='2019-11-05T12:16:18Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33060&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33060&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='pooyadavoodi' date='2019-11-05T12:33:41Z'>
		It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue?
		</comment>
		<comment id='6' author='pooyadavoodi' date='2020-03-15T16:24:58Z'>
		I am facing the same issue . When returning a tf.saved_model.load object inside a function and then try to use it, it is not working.
I am having a file sample.py
&lt;denchmark-code&gt;#### sample.py

import tensorflow as tf
def load_model(model_dir):

    # Load Model
    loaded = tf.saved_model.load(model_dir)
    model = loaded.signatures['serving_default']
    print("Model Loaded")
    return model

&lt;/denchmark-code&gt;

When I am executing main.py
&lt;denchmark-code&gt;from sample import load_model

model_dir = 'som_path of a saved model'
model1 = load_model(model_dir)
&lt;/denchmark-code&gt;

If I print model.variables I am getting following error
&lt;denchmark-code&gt;AssertionError: Called a function referencing variables which have been deleted. This likely means that function-local variables were created and not referenced elsewhere in the program. This is generally a mistake; consider storing variables in an object attribute on first call.

&lt;/denchmark-code&gt;

But. If load the model with same code inside the function, but not using the function it works fine
&lt;denchmark-code&gt;#### main.py
loaded = tf.saved_model.load(model_dir)
model = loaded.signatures['serving_default']
&lt;/denchmark-code&gt;

If I print model.variables, its working as expected.
		</comment>
		<comment id='7' author='pooyadavoodi' date='2020-07-01T16:00:30Z'>
		&lt;denchmark-link:https://github.com/s4sarath&gt;@s4sarath&lt;/denchmark-link&gt;
 encountered the same issue, it seems that that the function loading the model must return  not just an inference function obtained by referencing  property , otherwise the inference fails
		</comment>
		<comment id='8' author='pooyadavoodi' date='2020-07-02T13:56:58Z'>
		Glad someone mentioned it. It's very critical right.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Wed, Jul 1, 2020, 9:30 PM etsygankov ***@***.***&gt; wrote:
 @s4sarath &lt;https://github.com/s4sarath&gt; encountered the same issue, it
 seems that that the function loading the model must return
 tf.saved_model.load(path) not just an inference function obtained by
 referencing signature property
 tf.saved_model.load(path).signatures['serving_default'].

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#33060 (comment)&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/ACRE6KGYUUJIODAOYL2P2U3RZNMTBANCNFSM4I5VKZCA&gt;
 .



		</comment>
	</comments>
</bug>