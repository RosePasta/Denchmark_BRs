<bug id='37226' author='stefanondisponibile' open_date='2020-03-02T09:23:21Z' closed_time='2020-07-14T14:52:38Z'>
	<summary>RaggedTensor are converted to Tensors inside exported signature.</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
TensorFlow installed from (source or binary): no
TensorFlow version (use command below): 2.0.1
Python version: 3.7.3
Bazel version (if compiling from source): -
GCC/Compiler version (if compiling from source): -
CUDA/cuDNN version: -
GPU model and memory: -


As &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/function#args_2&gt;properly documented&lt;/denchmark-link&gt;
, it is currently possible to pass a possibly nested sequence of &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/TensorSpec&gt;tf.TensorSpec&lt;/denchmark-link&gt;
 objects as a   argument.
However, if one attempts to pass a tf.RaggedTensorSpec inside an input_signature sequence, everything still works, but the RaggedTensorSpec gets converted in its "dense" values + row_splits form.
Describe the expected behavior
I would expect that if one specifies a tf.RaggedTensorSpec element inside the input_signature sequence it should either be exported as a tf.RaggedTensorSpec, or raise an error.
Of course the firs option sounds more desiderable, since it's pretty common to have ragged input sequences. In this case tf.RaggedTensorSpec would be also lacking of a name argument, but this is easier to solve.

&lt;denchmark-link:https://colab.research.google.com/drive/1mEXNfmIiEU9XNL7niKCC_LRccfW5ZDob&gt;Colab Notebook&lt;/denchmark-link&gt;

import tensorflow as tf

class SomeModule(tf.Module):

  @tf.function(input_signature=[
    tf.RaggedTensorSpec(shape=[None, None], dtype=tf.string)
  ])
  def return_ragged_inputs(self, inputs: tf.RaggedTensor):
    return inputs

ragged_inputs = tf.ragged.constant([["foo"], ["bar"], ["foo", "bar"]], dtype=tf.string)

some_module = SomeModule()
return_ragged_inputs_result = some_module.return_ragged_inputs(ragged_inputs)
print(f"ragged_inputs =&gt; {ragged_inputs}\n")
print("We can use the method with ragged inputs:")
print(f"return_ragged_inputs =&gt; {return_ragged_inputs_result}\n")
print("Saving some_module...")
tf.saved_model.save(some_module, export_dir="foobar")
some_module = tf.saved_model.load("foobar")
print(f"\nWe can pass and return ragged inputs even by saving and reloading the module, if we use it 'eagerly':")
print(some_module.return_ragged_inputs(ragged_inputs))
print("\nBut the graph signature tell us that our exported signature expects tf.Tensor values instead:")
exported_model_graph_inputs = some_module.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY].inputs
exported_model_graph_outputs = some_module.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY].outputs
print(f"Expected inputs  : {exported_model_graph_inputs}")
print(f"Expected outputs : {exported_model_graph_outputs}")
print("\nThis means we couldn't use the method as we would expect in (e.g.) TensorFlow Serving.")

This issue follows up &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/37049&gt;#37049&lt;/denchmark-link&gt;
 , as suggested by &lt;denchmark-link:https://github.com/gowthamkpr&gt;@gowthamkpr&lt;/denchmark-link&gt;
 .
	</description>
	<comments>
		<comment id='1' author='stefanondisponibile' date='2020-03-03T22:36:33Z'>
		&lt;denchmark-link:https://github.com/stefanondisponibile&gt;@stefanondisponibile&lt;/denchmark-link&gt;
 I was able to feed in ragged input into the loaded model and it works but if I don't feed in the ragged tensor, it fails. Here is the &lt;denchmark-link:https://colab.sandbox.google.com/gist/gowthamkpr/f8f851ab2ec82a39cfc3e4216d063acc/untitled32.ipynb&gt;gist&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='stefanondisponibile' date='2020-03-03T23:03:26Z'>
		I believe this is due to  not being a first-class type inside GraphDef - internally, it is represented as a &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/composite_tensor.py#L31&gt;CompositeTensor&lt;/denchmark-link&gt;
, which in turn is lowered to a list of , by  when the function is compiled.
When loading a saved model, that lowering becomes visible.
As a workaround, you should be able to build inputs compatible with the loaded model by flattening the ragged tensor: tf.nest.flatten(ragged_inputs, expand_composites=True).
I think we can eventually handle them correctly inside SavedModel, by placing the necessary type spec into the graph metadata and reconstructing it upon import, but I'll let &lt;denchmark-link:https://github.com/k-w-w&gt;@k-w-w&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/edloper&gt;@edloper&lt;/denchmark-link&gt;
 give more expert insights.
Nit: in the colab, did you mean the last two lines to be "exported", not "expected"?
		</comment>
		<comment id='3' author='stefanondisponibile' date='2020-03-03T23:18:48Z'>
		So, &lt;denchmark-link:https://github.com/stefanondisponibile&gt;@stefanondisponibile&lt;/denchmark-link&gt;
 is expecting it to be a ragged tensor but its not the case as seen in the gist.
		</comment>
		<comment id='4' author='stefanondisponibile' date='2020-03-03T23:44:56Z'>
		&lt;denchmark-link:https://github.com/gowthamkpr&gt;@gowthamkpr&lt;/denchmark-link&gt;
 yes, for what I've seen so far, and as explained by &lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
 this difference between Ragged and normal Tensors pops up when the function gets converted to a Graph. I can't do it in Colab, but if you tried to load that servable with tf serving you would see you would have to pass two different dense Tensors in place of the Ragged Tensor.
&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
 thanks for the explanation! I am currently working around this by actually passing two dense Tensors at inference time, one with the values and the other with the splits, and building them up inside the function itself via tf.RaggedTensor.from_row_splits. So those last two lines were as "expected", since I was expecting to pass RaggedTensors to the served model, but I end up passing dense ones, building the RaggedTensor inside the function.
		</comment>
		<comment id='5' author='stefanondisponibile' date='2020-03-04T04:29:42Z'>
		Loaded model signatures currently flatten any structured input into a flat list of tensors.  This is true for RaggedTensor, but also for nested structure, such as a dictionary of tensors.  E.g., if we define a module whose function takes and returns a dict of tensors, then the model signature inputs &amp; outputs are currently flat lists of tensors:
&lt;denchmark-code&gt;class SomeModule(tf.Module):

  @tf.function(input_signature=[{'foo': tf.TensorSpec(None, tf.int32),
                                 'bar': tf.TensorSpec(None, tf.string)}
  ])
  def return_dict(self, inputs):
    return inputs

some_module = SomeModule()
tf.saved_model.save(some_module, export_dir="foobar2")
some_module = tf.saved_model.load("foobar2")
print(some_module.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY].inputs)
# prints: [&lt;tf.Tensor 'inputs:0' shape=&lt;unknown&gt; dtype=string&gt;, &lt;tf.Tensor 'inputs_1:0' shape=&lt;unknown&gt; dtype=int32&gt;]
print( some_module.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY].outputs)
# prints: [&lt;tf.Tensor 'Identity:0' shape=&lt;unknown&gt; dtype=string&gt;, &lt;tf.Tensor 'Identity_1:0' shape=&lt;unknown&gt; dtype=int32&gt;]
&lt;/denchmark-code&gt;

Hopefully we can improve on this, but the problem is a general issue with any structured inputs &amp; outputs, and is not specific to ragged tensors.
		</comment>
		<comment id='6' author='stefanondisponibile' date='2020-07-14T14:21:15Z'>
		&lt;denchmark-link:https://github.com/stefanondisponibile&gt;@stefanondisponibile&lt;/denchmark-link&gt;

Please let us know if this is still an issue.
		</comment>
		<comment id='7' author='stefanondisponibile' date='2020-07-14T14:52:31Z'>
		Hey &lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;
 , sorry for not having replied yet! I would say that , this still is an issue, but as &lt;denchmark-link:https://github.com/edloper&gt;@edloper&lt;/denchmark-link&gt;
 kindly explained is not specific to ragged tensors, moreover there are workarounds/better-ways for tackling this, so I'll close this specific issue. Thanks for you help!
		</comment>
		<comment id='8' author='stefanondisponibile' date='2020-07-14T14:52:39Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37226&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37226&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>