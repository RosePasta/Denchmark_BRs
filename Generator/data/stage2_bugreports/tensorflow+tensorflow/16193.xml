<bug id='16193' author='Pelups' open_date='2018-01-17T13:39:19Z' closed_time='2018-01-22T09:01:17Z'>
	<summary>Performance issues with TF1.5 on CPU</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.5.0-rc1
Python version: 2.7
Bazel version (if compiling from source): 0.5.4
GCC/Compiler version (if compiling from source): 5.4.0
CUDA/cuDNN version: NA
GPU model and memory: NA
Exact command to reproduce:

Hello,
I'm facing performance issues with the last releases of TF using a CPU.
I'm using the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/benchmark/benchmark_model.cc&gt;benchmark tool&lt;/denchmark-link&gt;
 to calculate mean inference time of a model.
For example, in order to evaluate mobilenet (trained on a custom dataset), I'm using this command :
bazel-bin/tensorflow/tools/benchmark/benchmark_model --graph="path to mobilenet graph" --input_layer="input" --input_layer_shape="1,224,224,3" --input_layer_type="float" --output_layer="MobilenetV1/Predictions/Reshape_1"
After setting CUDA_VISIBLE_DEVICES to "" in order to run on CPU.
With TF 1.4.1, I obtain a mean inference time equals to 26ms (13ms if I compile with optimization flags).
Using tf 1.5.*, I obtain a mean inference time equals to 51ms (45ms if I compile with optimization flags).
The loss is very important, so I'm wondering if it's a known issue and how I can improve this.
I tried with tags/v1.5.0-rc0, tags/v1.5.0-rc1 and master, and the problem is the same.
Thank you
	</description>
	<comments>
		<comment id='1' author='Pelups' date='2018-01-17T21:15:04Z'>
		&lt;denchmark-link:https://github.com/tfboyd&gt;@tfboyd&lt;/denchmark-link&gt;
, could you or somebody you know take a look? This seems somewhat worrying.
		</comment>
		<comment id='2' author='Pelups' date='2018-01-18T17:05:54Z'>
		Taking a look, I do not doubt the results but I am reproducing them this morning.
		</comment>
		<comment id='3' author='Pelups' date='2018-01-19T01:18:18Z'>
		&lt;denchmark-link:https://github.com/Pelups&gt;@Pelups&lt;/denchmark-link&gt;
 :  Could you either share the exact graph file you used and/or test with the graph I used below?  I am getting contradictory results and while I try different setups it would help if I was testing the exact same setup.  Also what CPU setup are you using, the CPU model code is the easiest way to share that.
I am still investigating and it is possible I am running the test incorrectly, there are a lot of results from the script and I believe I am interpreting them correctly but maybe not.  On an AWS m4.4xlarge 16 vCPUs at 2.3 Ghz (broadwell) I am seeing different results and I want to keep looking as I find it odd my results are so different.  If you have the exact graph you were using that would help.   Here is the test that I ran.
I used the following graphs:

mobilenetv1
inception

I ran mobilenet with:
./benchmark_model --graph=/data/mobilenet_v1_1_224/mobilenet_v1_1.0_224/frozen_graph.pb --input_layer="input" --output_layer="MobilenetV1/Predictions/Reshape_1"
For Inception there was no difference between 1.4.1 and 1.5RC1

1.4.1: 32.7ms  2018-01-18 22:30:25.591497: I tensorflow/core/util/stat_summarizer.cc:468] Timings (microseconds): count=450 first=32455 curr=32522 min=30880 max=36377 avg=32680.5 std=703
1.5RC1: 32.5ms   2018-01-18 22:43:14.819298: I tensorflow/core/util/stat_summarizer.cc:468] Timings (microseconds): count=451 first=33216 curr=33187 min=31072 max=37290 avg=32510 std=768

For mobilenetv1 the results were perplexing and I am digging in.  1.5RC1 was faster.  I double checked that I did not compile in AVX by looking at the logs.
All Threads (16)

1.4.1: 32.4ms 2018-01-19 00:01:13.230477: I tensorflow/core/util/stat_summarizer.cc:468] Timings (microseconds): count=308 first=34539 curr=32567 min=30796 max=34958 avg=32374.9 std=650
1.5RC1: 24.9ms  2018-01-18 23:57:49.456252: I tensorflow/core/util/stat_summarizer.cc:468] Timings (microseconds): count=400 first=24113 curr=24401 min=23805 max=26506 avg=24910.6 std=524
1.5.RC1  17.8ms AVX2 compiled with broadwell running on the Docker.  Weird and repeated as outside Docker AVX2 is consistent 21ms.

1 Thread

1.4.1: 179ms  2018-01-19 00:42:10.455322: I tensorflow/core/util/stat_summarizer.cc:468] Timings (microseconds): count=56 first=179066 curr=178621 min=177166 max=180046 avg=178665 std=574
1.5RC1: 106ms  2018-01-19 00:12:49.710419: I tensorflow/core/util/stat_summarizer.cc:468] Timings (microseconds): count=94 first=106362 curr=106541 min=105246 max=108088 avg=106450 std=356
1.5.RC1  52ms AVX2 compiled with broadwell running on the Docker.

Testing outside docker (still testing)
mobilenetv1:
All Threads (16):

1.5.RC1  21ms AVX2 compiled with broadwell (consistent)
1.4.1 31ms SSE3

One Thread:

1.5.RC1  56.3ms AVX2 compiled with broadwell
1.4.1 178ms SSE3

Extra info for tracking:

I used Bazel 0.5.4 for 1.4.1 and Bazel 0.9 for 1.5RC1.  I doubt the version of bazel matters
I built for CPU only in a Docker.  bazel --config=opt
I have the log files so I have everything that was output
I ran the tests a few times and did not average them I just confirmed the results were not jumping around and took one.  Not 100% scientific.

We are currently holding the 1.5 build as we investigate further.
		</comment>
		<comment id='4' author='Pelups' date='2018-01-20T11:16:00Z'>
		Hello! May I know what CPU are you currently using??
		</comment>
		<comment id='5' author='Pelups' date='2018-01-20T23:38:36Z'>
		&lt;denchmark-link:https://github.com/asp143&gt;@asp143&lt;/denchmark-link&gt;
  Just in case you were referencing me.  I am using AWS m4.4xlarge 16 vCPUs which is a  2.3 Ghz (broadwell) is CPU model  Intel Xeon E5-2686 v4 (Broadwell).
		</comment>
		<comment id='6' author='Pelups' date='2018-01-21T04:27:34Z'>
		&lt;denchmark-link:https://github.com/tfboyd&gt;@tfboyd&lt;/denchmark-link&gt;
 Did you recreate the problem? I think this has something with the intel kernel issue
		</comment>
		<comment id='7' author='Pelups' date='2018-01-22T09:01:17Z'>
		Hello !
Thank you &lt;denchmark-link:https://github.com/tfboyd&gt;@tfboyd&lt;/denchmark-link&gt;
  for your answer.
I really don't understand what happened when I ran my evaluation, because after runing it again, I obtain quite the same results than yours.
I did exactly the same installation/compilation steps than previously ...
Maybe a process was runing on the computer's cpu, lowering the performances of the evaluation.
So, thank you for your answer, sorry for your time.
		</comment>
		<comment id='8' author='Pelups' date='2018-01-22T16:34:30Z'>
		&lt;denchmark-link:https://github.com/Pelups&gt;@Pelups&lt;/denchmark-link&gt;
   Not a problem at all and I mean that completely.  Your work showed you tried a few things or I would not have taken you seriously.  We also should have a nightly/hourly tests with this script on desktop CPUs, we have a bunch on android devices.  If it was python I would setup the test myself in my "rogue" test suite.  Thank you for retesting so quickly.  Please do not hesitate to report issue.
		</comment>
	</comments>
</bug>