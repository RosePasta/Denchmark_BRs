<bug id='32622' author='CJMenart' open_date='2019-09-18T15:35:07Z' closed_time='2019-09-24T21:23:35Z'>
	<summary>[TF 2.0] tf.assign fails with int32 tensors using a multi-device distribution strategy</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.0rc0
Python version: 3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 10/7
GPU model and memory: 2 GPUs

Describe the current behavior
If you create a distribution strategy like MultiWorkerMirroredDistributionStrategy, then create a tf.int32 variable var, then call var.assign(some_int32_tensor_to_assign), TF will crash. It crashes in values.py, line 1220,
&lt;denchmark-code&gt; # To preserve the sum across save and restore, we have to divide the
        # total across all devices when restoring a variable that was summed
        # when saving.
        tensor = args[0]
  if self._aggregation == vs.VariableAggregation.SUM:
          tensor *= 1. / len(self.devices)
&lt;/denchmark-code&gt;

So what's happening here appears to be that TF tries to divide the int32 tensor by a float with no cast and blows up.
Describe the expected behavior
It seems like being able to assign integers to tensors while using distributed training is something that should be possible, given the goal of abstracting away distribution as much as possible. I know the distribution strategies may be kind of in flux right now, and I'm mainly just posting to make more knowledgeable people away of the issue; I'm just going to make all my tensors floats in the meantime :). I don't have the context to know how this should be approached anyway. Cast everything to floats? Make it so that the tensors don't always have to be divided? Divide integer tensors by assigning everything to one worker and giving zeros to everyone else? Who knows.
	</description>
	<comments>
		<comment id='1' author='CJMenart' date='2019-09-24T21:23:33Z'>
		I believe this has been fixed by (in one way, perhaps not the best): &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/438fc52e81ead89799067a2b961f6990d9b4f5f6#diff-580627c9b7904095019167ef005a72de&gt;438fc52#diff-580627c9b7904095019167ef005a72de&lt;/denchmark-link&gt;
. Please re-open if not, and thank you for filing!
		</comment>
		<comment id='2' author='CJMenart' date='2019-09-24T21:23:36Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=32622&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=32622&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>