<bug id='33744' author='loretoparisi' open_date='2019-10-26T11:12:24Z' closed_time='2020-06-10T09:28:43Z'>
	<summary>Tensor zero_padding2d_1_input:0, specified in either feed_devices or fetch_devices was not found in the Graph</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
custom
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):

&lt;denchmark-code&gt;PRETTY_NAME="Debian GNU/Linux 10 (buster)"
NAME="Debian GNU/Linux"
VERSION_ID="10"
VERSION="10 (buster)"
VERSION_CODENAME=buster
ID=debian
&lt;/denchmark-code&gt;


Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
none
TensorFlow installed from (source or binary):
binary
TensorFlow version (use command below):
v1.14.0-rc1-22-gaf24dc9 1.14.0
Python version:
Python 3.7.4
Bazel version (if compiling from source):
none
GCC/Compiler version (if compiling from source):
none
CUDA/cuDNN version:
none
GPU model and memory:
CPU model

Describe the current behavior
This issue happens when running the Tensorflow model inference in a Tornado server started as a Thread.
class WebServer(threading.Thread):
    def run(self):
        application.listen(8888)
        #asyncio.set_event_loop(asyncio.new_event_loop())
        asyncio.set_event_loop(AnyThreadEventLoopPolicy())
        tornado.ioloop.IOLoop.instance().start()
WebServer.start()
Describe the expected behavior
Code to reproduce the issue
Inference running without any issue. The issue does not occur when it starts running on the main thread instead:
application.listen(8888)
application.listen(8888)
tornado.ioloop.IOLoop.instance().start()
Other info / logs
This error happens on both Tensorflow models I run, so I assume it's a threading problem when running the Graph in a thread that it is not the main thread.
The only error logging I get is
 Tensor zero_padding2d_1_input:0, specified in either feed_devices or fetch_devices was not found in the Graph
	</description>
	<comments>
		<comment id='1' author='loretoparisi' date='2019-10-26T11:58:59Z'>
		
According to &lt;denchmark-link:https://stackoverflow.com/questions/54652536/keras-tensorflow-backend-error-tensor-input-10-specified-in-either-feed-de&gt;here&lt;/denchmark-link&gt;
, I have tried to run  in my session so I did
    with session.as_default():
        with session.graph.as_default():
            labels = np.round(model.predict(X)[0])
        
            prediction = []

            for i in range(0, len(labels)):
                if labels[i] == 1:
                    prediction.append(labels_group[i])

            return prediction
but now getting a
&lt;denchmark-code&gt;Error while reading resource variable block4_conv2/kernel from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/block4_conv2/kernel)
	 [[{{node sequential_1_3/block4_conv2/convolution/ReadVariableOp}}]]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='loretoparisi' date='2019-10-26T12:25:28Z'>
		
I was able to solve this issue creating a new  and then setting it to keras before loading the model as stated in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/28287#issuecomment-495005162&gt;#28287 (comment)&lt;/denchmark-link&gt;
:
   def load_tensorflow_shared_session(self):
        """ Load a Tensorflow/Keras shared session """
        # LP: create a config by gpu cpu backend
        if os.getenv('HAS_GPU', '0') == '1':
            config = tf.ConfigProto(
                device_count={'GPU': 1},
                intra_op_parallelism_threads=1,
                allow_soft_placement=True
            )
            config.gpu_options.allow_growth = True
            config.gpu_options.per_process_gpu_memory_fraction = 0.6
        else:
            config = tf.ConfigProto(
                intra_op_parallelism_threads=1,
                allow_soft_placement=True
            )
        # LP: create session by config
        self.tf_session = tf.Session(config=config)

        return self.tf_session
and then passing this shared session to all TF /Keras models:
def load__model(MODEL_PATH, session):
    '''
        Load the model
    '''

    # IMPORTANT: models have to be loaded AFTER SETTING THE SESSION for keras! 
    # Otherwise, their weights will be unavailable in the threads after the session there has been set
    set_session(session)
    model = keras.models.load_model(MODEL_PATH)
    model._make_predict_function()
Instead of using global session object I'm using a singleton object that retains this.tf_session, by the way it works in both ways.
		</comment>
		<comment id='3' author='loretoparisi' date='2020-01-15T22:22:26Z'>
		&lt;denchmark-link:https://github.com/loretoparisi&gt;@loretoparisi&lt;/denchmark-link&gt;
 Is this still an issue or was it resolved? Please close the issue if it was resolved already. Thanks!
		</comment>
		<comment id='4' author='loretoparisi' date='2020-01-16T07:24:13Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 please let me check, thank you.
		</comment>
		<comment id='5' author='loretoparisi' date='2020-01-16T18:02:14Z'>
		&lt;denchmark-link:https://github.com/loretoparisi&gt;@loretoparisi&lt;/denchmark-link&gt;
 When you check, can you also check with latest versions in TF1.x and TF2.x? Thanks!
		</comment>
		<comment id='6' author='loretoparisi' date='2020-01-16T23:40:26Z'>
		So for sure we were talking about of version TF 1.x.
		</comment>
		<comment id='7' author='loretoparisi' date='2020-06-01T21:14:01Z'>
		&lt;denchmark-link:https://github.com/loretoparisi&gt;@loretoparisi&lt;/denchmark-link&gt;
 Can you please check with the latest versions in TF1.15.3 and let us know whether the issue persists? Thanks!
Please feel free to close the issue if the issue was resolved for you. Thanks!
		</comment>
		<comment id='8' author='loretoparisi' date='2020-06-03T09:46:40Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 ok I will confirm.
		</comment>
		<comment id='9' author='loretoparisi' date='2020-06-03T19:10:03Z'>
		&lt;denchmark-link:https://github.com/loretoparisi&gt;@loretoparisi&lt;/denchmark-link&gt;
 Are you confirming that this is an issue with  or your are confirming that this is already resolved for you? Thanks!
		</comment>
		<comment id='10' author='loretoparisi' date='2020-06-10T09:28:43Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 I confirm that this has been resolved. I have investigated the solution. The problem was related to a wrong handling of the TF session through different models loaded on the same server (multi-processing application server so with the same shared session). The solution was a common shared session, passed through all loaded models.
		</comment>
		<comment id='11' author='loretoparisi' date='2020-06-10T09:28:45Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33744&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33744&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>