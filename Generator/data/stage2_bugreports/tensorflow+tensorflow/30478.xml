<bug id='30478' author='morgangiraud' open_date='2019-07-08T07:24:50Z' closed_time='2019-07-13T19:03:18Z'>
	<summary>[TF2.0] Dataset iteration, dynamic TensorArray and reduce operations</summary>
	<description>
System information

Have I written custom code: yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX
TensorFlow installed from (source or binary): binary - 2.0.0-beta1
TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1
Python version: 3.6

Describe the current behaviour
I'm trying to apply a reduce operation over the result of TensorArray concatenation.
The concatenation happens in a for loop generated by iteration over a dataset.
The resulting value of the reduce operation is malformed tensor:

the shape is ()
the actual value is [float32] of shape (1,)

This makes the resulting tensor effectively unusable because TF will then fail either because of shape information or because of the actual value of the tensor
Remark:

If the for loop is generated from the tf.range operation, everything works as expected.

Describe the expected behaviour
Getting a valid result from the different reduce operations when applied to the result of a TensorArray concatenation operation when this one is filled in a for loop generated by iteration over a dataset.
Code to reproduce the issue
import tensorflow as tf


mean = tf.keras.metrics.Mean()

a = tf.random.uniform([10, 2])
d = tf.data.Dataset.from_tensor_slices(a).batch(2)


@tf.function
def compute(mean, dataset):
    # I don't use the dataset at all
    arr = tf.TensorArray(tf.float32, 1, dynamic_size=True)
    for i in tf.range(10):  # Simple for loop
        real_logits = tf.random.normal([5, 1])
        arr = arr.write(tf.cast(i, tf.int32), real_logits)
    all_real_logits = arr.concat()

    score = tf.reduce_mean(all_real_logits)
    tf.print(tf.shape(score), score)  # -&gt; [], 0.0653904751
    mean.update_state(score)
    return mean.result()


@tf.function
def compute_error(mean, dataset):
    # I use the dataset only to get the index
    arr = tf.TensorArray(tf.float32, 1, dynamic_size=True)
    for i, _ in dataset.enumerate():  # Dataset for loop with enumerate
        real_logits = tf.random.normal([5, 1])
        arr = arr.write(tf.cast(i, tf.int32), real_logits)
    all_real_logits = arr.concat()

    score = tf.reduce_mean(all_real_logits)
    tf.print(tf.shape(score), score)  # -&gt; [], [-0.256373167] brackets!
    mean.update_state(score)
    return mean.result()


@tf.function
def compute_error2(mean, dataset):
    # I only use the dataset to simulate a for loop
    arr = tf.TensorArray(tf.float32, 1, dynamic_size=True)
    i = tf.constant(0, tf.int32)
    for _ in dataset:  # Dataset for loop
        real_logits = tf.random.normal([5, 1])
        arr = arr.write(tf.cast(i, tf.int32), real_logits)
        i = i + 1
    all_real_logits = arr.concat()

    # score = tf.reduce_mean(all_real_logits)
    score = tf.reduce_sum(all_real_logits)
    tf.print(tf.shape(score), score)  # -&gt; [], [-0.256373167] brackets!
    mean.update_state(score)
    return mean.result()


# Works well
print(compute(mean, d))

# Breaks because the shape is wrong for the score var
# We have shape=() and the actual value is [float] of shape (1,)
# Yet we can't do score[0] because the shape is ()
# In the end the score var becomes unusable

# Error: Cannot update variable with shape [] using a Tensor with shape [1], shapes must be equal.
print(compute_error(mean, d))

# it seems that the error still occurs as long as the call to
# arr.write is inside a for loop generated by iteration on a
# dataset
# Switching the reduce operation does not change the behaviour
print(compute_error2(mean, d))
	</description>
	<comments>
		<comment id='1' author='morgangiraud' date='2019-07-09T12:31:20Z'>
		Was able to reproduce the issue on Colab with Tensorflow version 2.0.0.beta1.
		</comment>
		<comment id='2' author='morgangiraud' date='2019-07-12T08:39:00Z'>
		&lt;denchmark-link:https://github.com/morgangiraud&gt;@morgangiraud&lt;/denchmark-link&gt;
 thank you for the detail instructions for how to reproduce the issue.
When tf.function is used to decorate a function that uses Python-style iteration over a dataset, Autograph is used to rewrite the loop to a graph equivalent. It seems that there is a bug in how TensorArray objects used inside of the loop are handled.
I am cc-ing &lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
, an expert on Autograph to take a closer look.
		</comment>
		<comment id='3' author='morgangiraud' date='2019-07-12T11:38:29Z'>
		Thanks for taking the time to look into it. üëçüèª
Also, it might be unrelated but I've been playing with autograph, tf.function,  datasets and TensorArray quite a lot and I found a bunch of strange behaviours:

#30409
#29996

Just wanted to gather everything here as it might be useful.
		</comment>
		<comment id='4' author='morgangiraud' date='2019-07-13T19:03:17Z'>
		Indeed, we seem to be dealing with an inconsistent Tensor object here, one whose static shape does not match its actual value.
This seems to be caused by a combination of issues, but ultimately the smoking gun seems to be tf.reduce_mean.
To clarify how AutoGraph transforms these pieces of code:

in the first instance (compute), a simple tf.while_loop is used
in both of the second instances (compute_error, compute_error2), Datset.reduce is used instead

The two should be entirely consistent, but in reality they are not:

in the case of tf.while_loop, the TensorArray retains a static shape on its elements, so that ultimately, the result of concat() is a shape [None, 1]
in the case of Dataset.reduce however, the TensorArray seems to lose its inferred shape (this shape is memorized after the first call to write; this in turn causes concat() to generate an entirely unknown (that is, dynamic) shape and rank

The consequence of this inconsistency highlights a bug in tf.reduce_mean, which in the former case returns a scalar (correctly), but in the latter case it returns a size-1 vector (incorrectly). What's more, in both cases it reports a static shape of ().
This code highlights both the inconsistency and the bug. It's based on the original code, but contains the equivalent code that autograph generates, to highlight the differences:
&lt;denchmark-code&gt;@tf.function(autograph=False)
def compute():
    arr = tf.TensorArray(tf.float32, 1, dynamic_size=True)
    def body(i, arr):
        real_logits = tf.random.normal([5, 1])
        arr = arr.write(tf.cast(i, tf.int32), real_logits)
        i += 1
        return i, arr
    def cond(i, arr):
      return i &lt; 10
    _, arr = tf.while_loop(cond, body, (0, arr))

    c = arr.concat()
    m = tf.reduce_mean(c)
    tf.print('TensortArray.concat() shape:', c.shape, 'rank:', c.shape.rank)
    tf.print('Reported shape of tf.reduce_mean(TensortArray.concat()):', m.shape)
    tf.print('Actual value of tf.reduce_mean(TensortArray.concat()):', m)
    return m

@tf.function(autograph=False)
def compute_ds():
    arr = tf.TensorArray(tf.float32, 1, dynamic_size=True)
    def body(state, _):
        i, arr = state
        real_logits = tf.random.normal([5, 1])
        arr = arr.write(tf.cast(i, tf.int32), real_logits)
        i += 1
        return i, arr
    en_ds = tf.data.Dataset.range(10).enumerate()
    _, arr = en_ds.reduce((0, arr), body)

    c = arr.concat()
    # Making the shape known makes them inconsistency once more:
    # c.set_shape([None, 1])
    m = tf.reduce_mean(c)
    tf.print('TensortArray.concat() shape:', c.shape, 'rank:', c.shape.rank)
    tf.print('Reported shape of tf.reduce_mean(TensortArray.concat()):', m.shape)
    tf.print('Actual value of tf.reduce_mean(TensortArray.concat()):', m)
    return m

print('*** With tf.while_loop')
_ = compute()
print()
print('*** With tf.Dataset.reduce')
_ = compute_ds()
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;*** With tf.while_loop
TensortArray.concat() shape: TensorShape([None, 1]) rank: 2
Reported shape of tf.reduce_mean(TensortArray.concat()): TensorShape([])
Actual value of tf.reduce_mean(TensortArray.concat()): 0.266083568

*** With tf.Dataset.reduce
TensortArray.concat() shape: TensorShape(None) rank: None
Reported shape of tf.reduce_mean(TensortArray.concat()): TensorShape([])
Actual value of tf.reduce_mean(TensortArray.concat()): [0.0591446124]
&lt;/denchmark-code&gt;

Note, in the above output, the differences in shape ([None, 1] vs. None) and rank (2 vs. None). Also note the mismatch between static shape and actual value (TensorShape([]) vs. [0.0591446124]).
Filed &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/30685&gt;#30685&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/30686&gt;#30686&lt;/denchmark-link&gt;
 to track each of these issues separately. Please re-open this issue if I missed anything not captured in there!
		</comment>
		<comment id='5' author='morgangiraud' date='2019-07-13T19:03:20Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=30478&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=30478&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='morgangiraud' date='2019-07-14T16:08:58Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
  Cool, thanks for taking the time to do this work.
One thing that might have been forgotten: it does not only happens with tf.reduce_mean but also with tf.reduce_sum and possibly with every tf.reduce_ operations. (I didn't check that)
		</comment>
		<comment id='7' author='morgangiraud' date='2019-07-15T12:55:18Z'>
		Thanks, that's useful to know - added note to the issue thread.
		</comment>
	</comments>
</bug>