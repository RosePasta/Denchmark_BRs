<bug id='34850' author='niranjan8129' open_date='2019-12-05T04:40:21Z' closed_time='2020-01-08T05:22:29Z'>
	<summary>Error: slice index 0 of dimension 0 out of bounds</summary>
	<description>
&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;

Hi, I am working on tensorflow 2.0  and getting an error at line 3 while running the model

lstm_cell =tf.keras.layers.LSTM(units=hidden_unit)
lstm_cell = tf.nn.RNNCellDropoutWrapper(lstm_cell, output_keep_prob=self.dropout_keep_prob)
self._initial_state = lstm_cell.get_initial_state(128, tf.float32)

Got ERROR at line 3
ValueError: slice index 0 of dimension 0 out of bounds. for 'strided_slice' (op: 'StridedSlice') with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = &lt;0&gt;, input[2] = &lt;1&gt;, input[3] = &lt;1&gt;.
may I know why I am getting this error?
	</description>
	<comments>
		<comment id='1' author='niranjan8129' date='2019-12-06T05:12:23Z'>
		lstm_cell =tf.keras.layers.LSTMCell(units=hidden_unit)
lstm_cell = tf.nn.RNNCellDropoutWrapper(lstm_cell, output_keep_prob=self.dropout_keep_prob)
self._initial_state = lstm_cell.get_initial_state(128, tf.float32)
Got ERROR at line 3
ValueError: slice index 0 of dimension 0 out of bounds. for 'strided_slice' (op: 'StridedSlice') with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = &lt;0&gt;, input[2] = &lt;1&gt;, input[3] = &lt;1&gt;.
may I know why I am getting this error?
		</comment>
		<comment id='2' author='niranjan8129' date='2019-12-06T06:53:02Z'>
		Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?
Please, share simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. If you are unclear what to include see the issue template displayed in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/new/choose&gt;the Github new issue template&lt;/denchmark-link&gt;
.
We ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!
		</comment>
		<comment id='3' author='niranjan8129' date='2019-12-06T06:58:42Z'>
		Running against Tensorflow 2.0 and getting error below
lstm_cell =tf.keras.layers.LSTMCell(units=hidden_unit)
lstm_cell = tf.nn.RNNCellDropoutWrapper(lstm_cell, output_keep_prob=self.dropout_keep_prob)
self._initial_state = lstm_cell.get_initial_state(self.batch_size, tf.float32)
inputs = [tf.squeeze(input_, [1]) for input_ in tf.split(pooled_concat,num_or_size_splits=int(reduced),axis=1)]
return_state =tf.keras.layers.RNN(lstm_cell, inputs, initial_state=self._initial_state,return_sequences=True, return_state=True)
output = return_state[0]
with tf.compat.v1.variable_scope('Output'):
tf.compat.v1.get_variable_scope().reuse_variables()
one = tf.ones([1, hidden_unit], tf.float32)
for i in range(1,len(outputs)):
ind = self.real_len &lt; (i+1)
ind = tf.cast(ind, dtype=tf.float32)
ind = tf.expand_dims(ind, -1)
mat = tf.matmul(ind, one)
output = tf.add(tf.multiply(output, mat),tf.multiply(outputs[i], 1.0 - mat))
Got ERROR at bullet point line
ValueError: slice index 0 of dimension 0 out of bounds. for 'strided_slice' (op: 'StridedSlice') with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = &lt;0&gt;, input[2] = &lt;1&gt;, input[3] = &lt;1&gt;.
may I know why I am getting this error?
		</comment>
		<comment id='4' author='niranjan8129' date='2019-12-06T08:46:20Z'>
		&lt;denchmark-link:https://github.com/niranjan8129&gt;@niranjan8129&lt;/denchmark-link&gt;

Looks like code is incomplete.I am seeing below error.(NameError: name 'hidden_unit' is not defined). Can you please help me with the reproducible code in our environment or colab link. It helps us in localizing the issue faster. Thanks!
		</comment>
		<comment id='5' author='niranjan8129' date='2019-12-06T08:59:26Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
 ravikyram
{
"batch_size": 128,
"dropout_keep_prob": 0.5,
"embedding_dim": 300,
"evaluate_every": 200,
"filter_sizes": "3,4,5",
"hidden_unit": 300,
"l2_reg_lambda": 0.0,
"max_pool_size": 4,
"non_static": false,
"num_epochs": 1,
"num_filters": 32
}
		</comment>
		<comment id='6' author='niranjan8129' date='2019-12-06T09:04:13Z'>
		FULL code  for Tensorflow 2.0 version
def init(self, embedding_mat, non_static, hidden_unit, sequence_length, max_pool_size,
num_classes, embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):
&lt;denchmark-code&gt;	self.input_x = tf.compat.v1.placeholder(tf.int32, [None, sequence_length], name='input_x')
	self.input_y = tf.compat.v1.placeholder(tf.float32, [None, num_classes], name='input_y')
	self.dropout_keep_prob = tf.compat.v1.placeholder(tf.float32, name='dropout_keep_prob')
	self.batch_size = tf.compat.v1.placeholder(tf.int32, [])
	self.pad = tf.compat.v1.placeholder(tf.float32, [None, 1, embedding_size, 1], name='pad')
	self.real_len = tf.compat.v1.placeholder(tf.int32, [None], name='real_len')

	l2_loss = tf.constant(0.0)

	with tf.device('/cpu:0'), tf.compat.v1.name_scope('embedding'):
		if not non_static:
			W = tf.constant(embedding_mat, name='W')
		else:
			W = tf.Variable(embedding_mat, name='W')
		self.embedded_chars = tf.nn.embedding_lookup(params=W, ids=self.input_x)
		emb = tf.expand_dims(self.embedded_chars, -1)

	pooled_concat = []
	reduced = np.int32(np.ceil((sequence_length) * 1.0 / max_pool_size))
	
	for i, filter_size in enumerate(filter_sizes):
		with tf.compat.v1.name_scope('conv-maxpool-%s' % filter_size):

			# Zero paddings so that the convolution output have dimension batch x sequence_length x emb_size x channel
			num_prio = (filter_size-1) // 2
			num_post = (filter_size-1) - num_prio
			pad_prio = tf.concat([self.pad] * num_prio,1)
			pad_post = tf.concat([self.pad] * num_post,1)
			emb_pad = tf.concat([pad_prio, emb, pad_post],1)

			filter_shape = [filter_size, embedding_size, 1, num_filters]
			W = tf.Variable(tf.random.truncated_normal(filter_shape, stddev=0.1), name='W')
			b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name='b')
			conv = tf.nn.conv2d(input=emb_pad, filters=W, strides=[1, 1, 1, 1], padding='VALID', name='conv')

			h = tf.nn.relu(tf.nn.bias_add(conv, b), name='relu')

			# Maxpooling over the outputs
			pooled = tf.nn.max_pool2d(input=h, ksize=[1, max_pool_size, 1, 1], strides=[1, max_pool_size, 1, 1], padding='SAME', name='pool')
			pooled = tf.reshape(pooled, [-1, reduced, num_filters])
			pooled_concat.append(pooled)

	pooled_concat = tf.concat(pooled_concat,2)
	pooled_concat = tf.nn.dropout(pooled_concat, 1 - (self.dropout_keep_prob))


	lstm_cell =tf.keras.layers.LSTMCell(units=hidden_unit)
	lstm_cell = tf.nn.RNNCellDropoutWrapper(lstm_cell, output_keep_prob=self.dropout_keep_prob)

	self._initial_state = lstm_cell.get_initial_state(self.batch_size, tf.float32)
	
	inputs = [tf.squeeze(input_, [1]) for input_ in tf.split(pooled_concat,num_or_size_splits=int(reduced),axis=1)]
	return_state =tf.keras.layers.RNN(lstm_cell, inputs, initial_state=self._initial_state,return_sequences=True, return_state=True)
	# Collect the appropriate last words into variable output (dimension = batch x embedding_size)
	output = return_state[0]
	with tf.compat.v1.variable_scope('Output'):
		tf.compat.v1.get_variable_scope().reuse_variables()
		one = tf.ones([1, hidden_unit], tf.float32)
		for i in range(1,len(outputs)):
			ind = self.real_len &lt; (i+1)
			ind = tf.cast(ind, dtype=tf.float32)
			ind = tf.expand_dims(ind, -1)
			mat = tf.matmul(ind, one)
			output = tf.add(tf.multiply(output, mat),tf.multiply(outputs[i], 1.0 - mat))

	with tf.compat.v1.name_scope('output'):
		self.W = tf.Variable(tf.random.truncated_normal([hidden_unit, num_classes], stddev=0.1), name='W')
		b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name='b')
		l2_loss += tf.nn.l2_loss(W)
		l2_loss += tf.nn.l2_loss(b)
		self.scores = tf.compat.v1.nn.xw_plus_b(output, self.W, b, name='scores')
		self.predictions = tf.argmax(input=self.scores, axis=1, name='predictions')

	with tf.compat.v1.name_scope('loss'):
		losses = tf.nn.softmax_cross_entropy_with_logits(labels = tf.stop_gradient( self.input_y), logits = self.scores) #  only named arguments accepted            
		self.loss = tf.reduce_mean(input_tensor=losses) + l2_reg_lambda * l2_loss

	with tf.compat.v1.name_scope('accuracy'):
		correct_predictions = tf.equal(self.predictions, tf.argmax(input=self.input_y, axis=1))
		self.accuracy = tf.reduce_mean(input_tensor=tf.cast(correct_predictions, "float"), name='accuracy')

	with tf.compat.v1.name_scope('num_correct'):
		correct = tf.equal(self.predictions, tf.argmax(input=self.input_y, axis=1))
		self.num_correct = tf.reduce_sum(input_tensor=tf.cast(correct, 'float'))
&lt;/denchmark-code&gt;

Got ERROR at bullet point line
ValueError: slice index 0 of dimension 0 out of bounds. for 'strided_slice' (op: 'StridedSlice') with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = &lt;0&gt;, input[2] = &lt;1&gt;, input[3] = &lt;1&gt;.
may I know why I am getting this error?
		</comment>
		<comment id='7' author='niranjan8129' date='2019-12-06T16:41:12Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
   here you can clone code and test
!git clone &lt;denchmark-link:https://github.com/niranjan8129/classification_tensorflow&gt;https://github.com/niranjan8129/classification_tensorflow&lt;/denchmark-link&gt;

!pip install tensorflow-gpu==2.0.0-beta1
from google.colab import drive
dirpath=drive.mount('/classification_tensorflow')
!chmod 777 classification_tensorflow
!python ./classification_tensorflow/train.py ./classification_tensorflow/data/train.csv.zip ./classification_tensorflow/training_config.json
		</comment>
		<comment id='8' author='niranjan8129' date='2019-12-10T08:51:05Z'>
		I have tried on colab with TF version 2.0 beta, 2.0 and was able to reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/304d34ec814dbbf13ed63b5a23416ab1/untitled458.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='9' author='niranjan8129' date='2019-12-13T12:31:36Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;
  Do you how to overcome this issue ?
issue at line :  self._initial_state = lstm_cell.get_initial_state(self.batch_size, tf.float32)
		</comment>
		<comment id='10' author='niranjan8129' date='2020-01-08T05:22:26Z'>
		Hi &lt;denchmark-link:https://github.com/niranjan8129&gt;@niranjan8129&lt;/denchmark-link&gt;
.
get_inital_state() method takes 3 named param, (inputs, batch_size, dtype) respectively. Since you are trying to provide batch_size and dtype, you should use
&lt;denchmark-code&gt;initial_state = lstm_cell.get_initial_state(batch_size=self.batch_size, dtype=tf.float32)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='11' author='niranjan8129' date='2020-01-08T05:22:31Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34850&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34850&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='niranjan8129' date='2020-01-08T06:06:58Z'>
		&lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;

I modified the code but still getting error below.
lstm_cell =tf.keras.layers.GRUCell(units=hidden_unit)
lstm_cell = tf.nn.RNNCellDropoutWrapper(lstm_cell,output_keep_prob=self.dropout_keep_prob)
self._initial_state = lstm_cell.get_initial_state( inputs=lstm_cell, batch_size=self.batch_size, dtype= tf.float32)
inputs = [tf.squeeze(input_, [1]) for input_ in tf.split(pooled_concat,num_or_size_splits=int(reduced),axis=1)]
return_state =tf.keras.layers.RNN(cell=lstm_cell, inputs=inputs,initial_state=self._initial_state, return_sequences=True, return_state=True, dtype=tf.float32)
&lt;denchmark-h:h1&gt;Error&lt;/denchmark-h&gt;

During handling of the above exception, another exception occurred:
self._initial_state = lstm_cell.get_initial_state( inputs=lstm_cell, batch_size=self.batch_size, dtype= tf.float32)
File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/recurrent.py", line 1748, in get_initial_state     return _generate_zero_filled_state_for_cell(self, inputs, batch_size, dtype)
TypeError: Expected binary or unicode string, got &lt;tensorflow.python.keras.layers.recurrent_v2.GRUCell object at 0x7f76c350b3c8&gt;
TypeError: Expected binary or unicode string, got &lt;tensorflow.python.keras.layers.recurrent_v2.GRUCell object at 0x7f76c350b3c8&gt;
		</comment>
		<comment id='13' author='niranjan8129' date='2020-01-08T06:09:24Z'>
		You shouldn't provide inputs param to get_initial_state(), only batch_size and dtype are the params you have.
		</comment>
		<comment id='14' author='niranjan8129' date='2020-01-08T06:12:38Z'>
		If  I am using below code,getting below error
self._initial_state = lstm_cell.get_initial_state(self.batch_size, tf.float32)
Got ERROR
ValueError: slice index 0 of dimension 0 out of bounds. for 'strided_slice' (op: 'StridedSlice') with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = &lt;0&gt;, input[2] = &lt;1&gt;, input[3] = &lt;1&gt;.
may I know why I am getting this error?
		</comment>
		<comment id='15' author='niranjan8129' date='2020-01-08T06:13:17Z'>
		Also, you are calling the RNN layer incorrectly. The tensor params should be called with RNN layer, rather than pass in as init param.
&lt;denchmark-code&gt;rnn= tf.keras.layers.RNN(cell=lstm_cell, return_sequences=True, return_state=True, dtype=tf.float32)
output = rnn(inputs)
&lt;/denchmark-code&gt;

Also, the default init state will be inferred with all zeros. You don't need to call get_initial_state and pass it to the rnn layer. The initial state is optional.
		</comment>
		<comment id='16' author='niranjan8129' date='2020-01-08T06:16:36Z'>
		get_initial_state() takes named argument, you can't just feed your param as the normal positional argument to it.
		</comment>
		<comment id='17' author='niranjan8129' date='2020-01-08T06:19:12Z'>
		&lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;
  kindly correct me if I wrong .. I use below code
lstm_cell =tf.keras.layers.GRUCell(units=hidden_unit)
lstm_cell = tf.nn.RNNCellDropoutWrapper(lstm_cell, output_keep_prob=self.dropout_keep_prob)
self._initial_state = lstm_cell.get_initial_state( self.batch_size,  tf.float32)
inputs = [tf.squeeze(input_, [1]) for input_ in tf.split(pooled_concat,num_or_size_splits=int(reduced),axis=1)]
rnn= tf.keras.layers.RNN(cell=lstm_cell,initial_state=self._initial_state, return_sequences=True, return_state=True, dtype=tf.float32)
output = rnn(inputs)
		</comment>
		<comment id='18' author='niranjan8129' date='2020-01-08T06:21:05Z'>
		The correct code should be
&lt;denchmark-code&gt;self._initial_state = lstm_cell.get_initial_state(batch_size=self.batch_size, dtype=tf.float32)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='19' author='niranjan8129' date='2020-01-08T06:46:35Z'>
		&lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;
  I am very close to .. Thank you a lot.  if I use initial_state below places, I get two kinds of error .  is that the places i use correctly?



rnn= tf.keras.layers.RNN(cell=lstm_cell,initial_state=self._initial_state,return_sequences=True, return_state=True, dtype=tf.float32)
&lt;denchmark-h:h1&gt;Error:&lt;/denchmark-h&gt;

TypeError: ('Keyword argument not understood:', 'initial_state')

outputs = rnn(inputs ,initial_state=self._initial_state  )

&lt;denchmark-h:h1&gt;Error:&lt;/denchmark-h&gt;

assert initial_state is None and constants is None
AssertionError
		</comment>
		<comment id='20' author='niranjan8129' date='2020-02-13T11:42:08Z'>
		i a, working with same code and when try to run train.py getting     scale /= max(1., (fan_in + fan_out) / 2.)
TypeError: unsupported operand type(s) for +: 'NoneType' and 'int'
		</comment>
	</comments>
</bug>