<bug id='33254' author='VoVAllen' open_date='2019-10-11T15:33:20Z' closed_time='2019-10-16T05:56:06Z'>
	<summary>Share memory between numpy and tensorflow doesn't work</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Python
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): pip
TensorFlow version (use command below): 2.0.0
Python version: 3.6.8

Describe the current behavior
As described in introduction, tf will try to share memory between tf and numpy when possible. However I couldn't figure out how to do this.
a = tf.constant([3, 4]).cpu()
b = tf.numpy()
b[0] = 1
print(a)
# [3, 4]
print(b)
# [1, 4]
Describe the expected behavior
a = tf.constant([3, 4]).cpu()
b = tf.numpy()
b[0] = 1
print(a)
# [1, 4]
print(b)
# [1, 4]
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='VoVAllen' date='2019-10-14T07:17:04Z'>
		&lt;denchmark-link:https://github.com/VoVAllen&gt;@VoVAllen&lt;/denchmark-link&gt;
 ,
When tried executing the given code is faced, kindly find the &lt;denchmark-link:https://colab.sandbox.google.com/gist/oanush/067537c18ac24ca5884996792c77085e/33254.ipynb&gt;gist&lt;/denchmark-link&gt;
 of colab. Thanks!
		</comment>
		<comment id='2' author='VoVAllen' date='2019-10-14T07:44:43Z'>
		&lt;denchmark-link:https://github.com/oanush&gt;@oanush&lt;/denchmark-link&gt;
 Sorry I mistakenly write the code, it should be  instead of 
The following code should work
import tensorflow as tf
import numpy as np
print(tf.__version__) # 2.0.0
tf.executing_eagerly() # True

a = tf.constant([3, 4]).cpu()
b = a.numpy()
b[0] = 1
print(a)
# [3, 4], Expected to be [1, 4] if share memory with numpy array
print(b)
# [1, 4]
		</comment>
		<comment id='3' author='VoVAllen' date='2019-10-14T07:46:47Z'>
		As described in &lt;denchmark-link:https://www.tensorflow.org/tutorials/customization/basics#numpy_compatibility&gt;https://www.tensorflow.org/tutorials/customization/basics#numpy_compatibility&lt;/denchmark-link&gt;
, they should share the underlying memorys.
		</comment>
		<comment id='4' author='VoVAllen' date='2019-10-14T11:46:56Z'>
		Issue replicating with TF-2.0, kindly find the &lt;denchmark-link:https://colab.sandbox.google.com/gist/oanush/3554926482eac5992e49569227accc10/33254.ipynb&gt;gist&lt;/denchmark-link&gt;
 of collab. Thanks!
		</comment>
		<comment id='5' author='VoVAllen' date='2019-10-14T13:09:40Z'>
		&lt;denchmark-link:https://github.com/VoVAllen&gt;@VoVAllen&lt;/denchmark-link&gt;
,
Below Text from the &lt;denchmark-link:https://www.tensorflow.org/tutorials/customization/basics#numpy_compatibility&gt;Tutorial&lt;/denchmark-link&gt;
 doesn't say that Numpy Array and Tensors share the Underlying Memory always, but it was a conditional statement ("if possible" was mentioned).

These conversions are typically cheap since the array and tf.Tensor share the underlying memory representation, if possible. However, sharing the underlying representation isn't always possible since the tf.Tensor may be hosted in GPU memory while NumPy arrays are always backed by host memory, and the conversion involves a copy from GPU to host memory.

It is evident from the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/afa2418f90f7cc15e3ada12e7760c47b79404a23/tensorflow/python/framework/ops.py#L923#L939&gt;Implementation of Tensor.numpy()&lt;/denchmark-link&gt;
]  as well that Sharing of Underlying Memory is not Mandatory.
Below sentence states that,

TODO(ashankar,agarwal): Perhaps this should NOT reference the underlying
buffer but instead always explicitly copy? Note that currently it may or may
not copy based on whether the numpy data is properly aligned or not.


Considering all these observations, as per my understanding, this behavior is expected. Please let me know your thoughts. Thanks!
		</comment>
		<comment id='6' author='VoVAllen' date='2019-10-14T15:40:46Z'>
		Hi,
Thanks for your reply.
Actually I think I found the bug here. At here 


tensorflow/tensorflow/python/framework/ops.py


         Line 939
      in
      afa2418






 return maybe_arr.copy() if isinstance(maybe_arr, np.ndarray) else maybe_arr 




,
If it is a numpy.ndarray, why not directly return maybe_arr? The logic here says return a copy of numpy.ndarray, or directly return something which is not a numpy.ndarray. It's weird.
And I checked maybe_arr shares memory with original tensor, which as expected.
By the following code I got what I expected:
import tensorflow as tf
import numpy as np
print(tf.__version__) # 2.0.0
tf.executing_eagerly() # True

a = tf.constant([3, 4]).cpu()
b = a._numpy() # Change from numpy() to _numpy()
b[0] = 1
print(a)
# [1, 4]
print(b)
# [1, 4]
At least there're some inconsistency between the doc and the behavior,
		</comment>
		<comment id='7' author='VoVAllen' date='2019-10-14T15:58:05Z'>
		I checked some history commit (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/7caec689acd6a4d173a5dd99aa0da9961063058a&gt;7caec68&lt;/denchmark-link&gt;
), the copy behavior seems intentional. The comment at  function seems outdated. I couldn't find way that share memory between tensorflow and numpy, no matter from numpy to tf or from tf to numpy.
		</comment>
		<comment id='8' author='VoVAllen' date='2019-10-14T16:18:45Z'>
		At &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blame/5278b8509e2cd1b2847315db46fc0f958824cfce/tensorflow/python/framework/ops.py#L709&gt;https://github.com/tensorflow/tensorflow/blame/5278b8509e2cd1b2847315db46fc0f958824cfce/tensorflow/python/framework/ops.py#L709&lt;/denchmark-link&gt;
, numpy is zero-copyed from tf.
The next commit (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/ca1b54a83ae352c41bb285f0a6ecace20f706ac1?diff=split&gt;ca1b54a?diff=split&lt;/denchmark-link&gt;
), fixed that the  operation should not be recorded. To fix this, this author copied tensor to create a new one which is not recorded. This fix seems not ideal, since avoiding recording the operation doesn't have to copy a new one. From here, it becomes the copy behavior.
Later at 
, &lt;denchmark-link:https://github.com/superbobry&gt;@superbobry&lt;/denchmark-link&gt;
 mentioned that it has to copy to avoid reference counting problem on the data memory.
&lt;denchmark-link:https://github.com/superbobry&gt;@superbobry&lt;/denchmark-link&gt;
 Sorry to bother you. Could you comment a bit on this issue, that at what condition tf tensor can share memory with numpy? Also could you say a bit more about the reference counting problem mentioned above? Many thanks!
		</comment>
		<comment id='9' author='VoVAllen' date='2019-10-15T19:58:18Z'>
		NumPy arrays are mutable whereas tensors aren't, therefore .numpy() has to copy to avoid breaking the runtime's invariants. In theory, a NumPy array could be flagged as non-writable (via a.flags["WRITABLE"]), but unfortunately a lot of internal (and I assume external) users already rely on the mutability of .numpy().
Two caveats:
Tensor supports the buffer interface, so you could get a readonly NumPy array via
&gt;&gt;&gt; t = tf.constant([42])
&gt;&gt;&gt; a = np.asarray(memoryview(t))
&gt;&gt;&gt; a
array([42], dtype=int32)
Passing a CPU tensor directly to any NumPy APIs is zero-copy, because NumPy uses buffer interface behind the scenes (but NumPy will allocate a new array for the result):
&gt;&gt;&gt; np.square(t)
array([1764], dtype=int32)
		</comment>
		<comment id='10' author='VoVAllen' date='2019-10-16T05:56:06Z'>
		Thanks a lot for the explanation. This makes much more sense to me.
		</comment>
		<comment id='11' author='VoVAllen' date='2019-10-16T05:56:07Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33254&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33254&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>