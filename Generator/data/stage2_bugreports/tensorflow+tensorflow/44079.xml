<bug id='44079' author='car1hsiung' open_date='2020-10-16T09:34:31Z' closed_time='2020-12-01T04:47:35Z'>
	<summary>Getting errors when converting a simple convs model to INT8 TFLite</summary>
	<description>
System information

OS Platform and Distribution: Ubuntu 18.04
TensorFlow installed from binary
TensorFlow version: tf-nightly (2.4.0-dev20201015) and tf-2.3.0
TensorFlow Model Optimization version: 0.5.0

Command used to run the converter or code if youâ€™re using the Python API
FP32 TFLite is work, but INT8 TFLite is not work with tf-nightly.
Please check the &lt;denchmark-link:https://colab.research.google.com/drive/14glMvUN1H2PZHB2VPOwX9sZX9E2RNu0J?usp=sharing&gt;gist&lt;/denchmark-link&gt;
 (tf-nightly).


Can not convert quantization aware model to INT8 TFLite.


converter.inference_input_type = tf.int8 and converter.inference_output_type = tf.int8 are not work.


FP32 TFLite is work.


Item 2 has been fixed in 2.3.0 (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/36024&gt;#36024&lt;/denchmark-link&gt;
), but still has another error.
Please check the &lt;denchmark-link:https://colab.research.google.com/drive/1ejLyom6qeGJlxkk0jL62OF7ybrzzX3Wu?usp=sharing&gt;gist&lt;/denchmark-link&gt;
 (tf-2.3.0)
The output from the converter invocation
Item 1:
&lt;denchmark-code&gt;RuntimeError: tensorflow/lite/kernels/quantize.cc:113 affine_quantization-&gt;scale-&gt;size == 1 was not true.Node number 0 (QUANTIZE) failed to prepare.
&lt;/denchmark-code&gt;

Item 2:
&lt;denchmark-code&gt;ValueError: The inference_input_type and inference_output_type must be tf.float32.
&lt;/denchmark-code&gt;

Failure details
Get errors in tensorflow
Thanks in advance.
	</description>
	<comments>
		<comment id='1' author='car1hsiung' date='2020-10-19T06:54:25Z'>
		Was able to reproduce the issue with &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/c9fe5970e900520fadff7c515c18a13e/44079_2.ipynb&gt;TF v2.3&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/26edd3e78722dfa86b4bccf93d21a8fc/44079_2-nightly.ipynb&gt;TF-nightly&lt;/denchmark-link&gt;
. Please find the attached gist. Thanks!
		</comment>
		<comment id='2' author='car1hsiung' date='2020-11-03T03:59:17Z'>
		System information

OS Platform and Distribution: Ubuntu 18.04
TensorFlow installed from binary
TensorFlow version: tf-nightly (2.5.0.dev20201029)
TensorFlow Model Optimization version: 0.5.0.dev20201103 (installed from github)

Command used to run the converter or code
It can convert model to INT8 TFLite successfully with latest tf-nightly,
but fail to convert quantized model with tensorflow_model_optimization.
Please check the &lt;denchmark-link:https://colab.research.google.com/drive/1UeEnd2Hh3Xc2fLxuBEsy2G_qBRrP0hdN?usp=sharing&gt;gist&lt;/denchmark-link&gt;
.
The output from the converter invocation
&lt;denchmark-code&gt;RuntimeError: Quantization not yet supported for op: 'DEQUANTIZE'.
&lt;/denchmark-code&gt;

Might be similar to issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/42082&gt;#42082&lt;/denchmark-link&gt;

Thanks in advance.
		</comment>
		<comment id='3' author='car1hsiung' date='2020-11-13T05:56:49Z'>
		&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/42082&gt;#42082&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='car1hsiung' date='2020-11-18T07:39:44Z'>
		Issue of the last comments was solved in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/44882&gt;#44882&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/MeghnaNatraj&gt;@MeghnaNatraj&lt;/denchmark-link&gt;
 , is there a rough estimate at which point  will work for quantization-aware models?
		</comment>
		<comment id='5' author='car1hsiung' date='2020-11-18T22:14:40Z'>
		&lt;denchmark-link:https://github.com/hhass&gt;@hhass&lt;/denchmark-link&gt;
 It is supported in all 2.4 release candidates and the final yet-to-be released versions (2.4.0-rc0 .... 2.4). For details refer to this &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/42082#issue-673921045&gt;comment&lt;/denchmark-link&gt;

The changes were made in this &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/5832fa2&gt;commit&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='car1hsiung' date='2020-12-01T04:47:35Z'>
		
System information
* OS Platform and Distribution: Ubuntu 18.04

* TensorFlow installed from binary

* TensorFlow version: tf-nightly (2.5.0.dev20201029)

* TensorFlow Model Optimization version: 0.5.0.dev20201103 (installed from github)

Command used to run the converter or code
It can convert model to INT8 TFLite successfully with latest tf-nightly,
but fail to convert quantized model with tensorflow_model_optimization.
Please check the gist.
The output from the converter invocation
RuntimeError: Quantization not yet supported for op: 'DEQUANTIZE'.

Might be similar to issue #42082
Thanks in advance.

Close the issue, because the error is duplicate of 42082.
		</comment>
		<comment id='7' author='car1hsiung' date='2020-12-01T04:47:37Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44079&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44079&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='car1hsiung' date='2020-12-16T01:51:15Z'>
		Thanks for  MeghnaNatraj's &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/42082#issuecomment-745541910&gt;example&lt;/denchmark-link&gt;
.
I can convert QAT models successfully. My &lt;denchmark-link:https://colab.research.google.com/drive/1P4tnoZ4Sb9Q_uwRr2CxwcAtCM8rtzlFo?usp=sharing&gt;gist&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>