<bug id='34380' author='ahmedelmahy' open_date='2019-11-18T16:53:07Z' closed_time='2020-04-04T16:04:07Z'>
	<summary>Why is autoencoder with tensorflow 2.0 is performing very bad compared to the same code in keras?</summary>
	<description>
I am training an autoencoder on the mnist data With keras the validation loss is great (starting from 0.2687 to .1) While with tensorflow(version 2.0).keras validation loss is stuck at (.6) Even though I am using the same code.
Below is the code with keras (&lt;denchmark-link:https://colab.research.google.com/drive/1AD5Z_pXsaM6Ubn6Yfg0uD230J9o8AfRS&gt;you can test it in colab&lt;/denchmark-link&gt;
) and followed by the code with tf.keras (&lt;denchmark-link:https://colab.research.google.com/drive/1Hj3-_Bjfkp3-dmd38-1CmIR4VLOV98hB&gt;you can test it in colab&lt;/denchmark-link&gt;
)
&lt;denchmark-code&gt;from keras.layers import Input, Dense
from keras.models import Model, Sequential
from keras.datasets import mnist
import numpy as np

#Import the MNIST data, only take the images since we don't need the targets
(x_train, y_train), (x_test, y_test) = mnist.load_data()
#Normalize and reshape images to vectors
x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.
x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))
x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))
print (x_train.shape)
print (x_test.shape)


input = Input(shape=(784,))
y = Dense(64, activation='relu')(input)
z = Dense(784, activation='sigmoid')(y)
ae = Model(input, z)
encoder = Model(input, y)
input_decoder = Input(shape = (64,))
decoder_layer = ae.layers[-1]
decoder = Model(input_decoder, decoder_layer(input_decoder))
ae.compile(optimizer='adadelta', loss = 'binary_crossentropy')
ae.fit(x_train, x_train, epochs = 50, batch_size=256, shuffle=False, validation_data=(x_test, x_test))
&lt;/denchmark-code&gt;

60000/60000 [==============================] - 5s 88us/step - loss: 0.3494 - val_loss: 0.2688 Epoch 2/50 60000/60000 [==============================] - 4s 74us/step - loss: 0.2578 - val_loss: 0.2445 Epoch 3/50
&lt;denchmark-code&gt;from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.datasets import mnist
import numpy as np

#Import the MNIST data, only take the images since we don't need the targets
(x_train, y_train), (x_test, y_test) = mnist.load_data()
#Normalize and reshape images to vectors
x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.
x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))
x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))
print (x_train.shape)
print (x_test.shape)


input = Input(shape=(784,))
y = Dense(64, activation='relu')(input)
z = Dense(784, activation='sigmoid')(y)
ae = Model(input, z)
encoder = Model(input, y)
input_decoder = Input(shape = (64,))
decoder_layer = ae.layers[-1]
decoder = Model(input_decoder, decoder_layer(input_decoder))
ae.compile(optimizer='adadelta', loss = 'binary_crossentropy')
ae.fit(x_train, x_train, epochs = 50, batch_size=256, shuffle=False, validation_data=(x_test, x_test))

&lt;/denchmark-code&gt;

Train on 60000 samples, validate on 10000 samples Epoch 1/50 60000/60000 [==============================] - 4s 59us/sample - loss: 0.6941 - val_loss: 0.6939 Epoch 2/50 60000/60000 [==============================] - 3s 47us/sample - loss: 0.6937 - val_loss: 0.6936
	</description>
	<comments>
		<comment id='1' author='ahmedelmahy' date='2019-11-18T17:10:31Z'>
		It turns out it's a behaviour from the optimizer. Could it be a bug? or just different initialization parameters
with tf.keras it works very well and reaches val_loss less than 1 with adam optimizer
&lt;denchmark-code&gt;
#from keras import backend as K
from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.utils import plot_model, to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.datasets import mnist
from tensorflow.keras import regularizers
import tensorflow.keras as keras
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import tensorflow as tf

#Import the MNIST data, only take the images since we don't need the targets
(x_train, y_train), (x_test, y_test) = mnist.load_data()
#Normalize and reshape images to vectors
x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.
x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))
x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))
print (x_train.shape)
print (x_test.shape)


input = Input(shape=(784,))
y = Dense(64, activation='relu')(input)
z = Dense(784, activation='sigmoid')(y)
ae = Model(input, z)
encoder = Model(input, y)
input_decoder = Input(shape = (encoding_size,))
decoder_layer = ae.layers[-1]
decoder = Model(input_decoder, decoder_layer(input_decoder))
ae.compile(optimizer='adam', loss = 'binary_crossentropy')
ae.fit(x_train, x_train, epochs = 50, batch_size=256, shuffle=False, validation_data=(x_test, x_test))

&lt;/denchmark-code&gt;

Train on 60000 samples, validate on 10000 samples
Epoch 1/50
60000/60000 [==============================] - 2s 28us/sample - loss: 0.2540 - val_loss: 0.1771
Epoch 2/50
60000/60000 [==============================] - 1s 20us/sample - loss: 0.1578 - val_loss: 0.1418
		</comment>
		<comment id='2' author='ahmedelmahy' date='2019-11-19T06:08:36Z'>
		&lt;denchmark-link:https://github.com/ahmedelmahy&gt;@ahmedelmahy&lt;/denchmark-link&gt;

I tried the code with keras using TF 1.15 and validation loss is converging  (starting from 0.2687 to .1). However in TF 2.0 using keras gives the below error.
tf.keras.
I tried the given code using tf.keras using TF 2.0 against adadelta and adam and yes 'adadelta' has much worse performance compared to adam (as you can see &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/abf3f37d69beb90346d2aa7a05f88f70/ae-in-tf-keras.ipynb&gt;here&lt;/denchmark-link&gt;
)and this can be explained &lt;denchmark-link:https://www.reddit.com/r/MachineLearning/comments/3y84hr/how_does_adam_compare_to_adadelta/&gt;here&lt;/denchmark-link&gt;
.Please correct me if you think otherwise.Thanks!
		</comment>
		<comment id='3' author='ahmedelmahy' date='2019-11-25T11:16:19Z'>
		&lt;denchmark-link:https://github.com/ahmedelmahy&gt;@ahmedelmahy&lt;/denchmark-link&gt;

Any update on this issue please?. Thanks!
		</comment>
		<comment id='4' author='ahmedelmahy' date='2019-11-25T16:15:45Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
  thanks for the mention. That seems correct that adam could be faster than adadelta
&lt;denchmark-link:https://www.reddit.com/r/MachineLearning/comments/3y84hr/how_does_adam_compare_to_adadelta/&gt;reddit&lt;/denchmark-link&gt;

But I still kind of don't understand why "adadelta" is behaving in a different way in tf.keras as compared to the separate keras  package.
For now I am using Adam and things are working fine.
		</comment>
		<comment id='5' author='ahmedelmahy' date='2020-04-03T18:06:47Z'>
		&lt;denchmark-link:https://github.com/ahmedelmahy&gt;@ahmedelmahy&lt;/denchmark-link&gt;
 I think this is probably due to we changed the default learning rate (unfortunately) from 1.0 to 0.001. Can you try it and let us know if it fixes for you?
		</comment>
		<comment id='6' author='ahmedelmahy' date='2020-04-04T16:04:07Z'>
		That works like a charm &lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;
  . &lt;denchmark-link:https://colab.research.google.com/drive/1Hj3-_Bjfkp3-dmd38-1CmIR4VLOV98hB&gt;Setting the learning rate &lt;/denchmark-link&gt;
 to 1.0 produces the same results as with keras.
		</comment>
		<comment id='7' author='ahmedelmahy' date='2020-04-04T16:04:09Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34380&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34380&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>