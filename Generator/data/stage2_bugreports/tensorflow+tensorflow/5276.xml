<bug id='5276' author='timmeinhardt' open_date='2016-10-29T11:35:20Z' closed_time='2017-06-16T21:32:44Z'>
	<summary>TensorBoard histograms NaN value handling</summary>
	<description>
Recently I had to fight a lot with exploding gradients and thereby weights turning NaN after only several iterations event hough I was using tf.contrib.layers.variance_scaling_initializer correctly. To monitor the problem I started saving weight histograms which seemed totally messed up. I thought this was due to me using the initializer wrongly and only after some debugging I realised my initialization was totally fine. The NaN values appearing in iteration n seem to mess with the visualization of values in previous iterations which seems like a very misleading behaviour to me. Histograms and distributions should be visualized correctly until this is not really possible because of ill values.
I can easily provide tensorflow summary data to reproduce this behaviour.
	</description>
	<comments>
		<comment id='1' author='timmeinhardt' date='2016-10-29T16:43:18Z'>
		&lt;denchmark-link:https://github.com/danmane&gt;@danmane&lt;/denchmark-link&gt;
 not sure what the proper thing to do with NaN is, maybe report them in a separate counter or ignore?
		</comment>
		<comment id='2' author='timmeinhardt' date='2016-10-29T16:52:07Z'>
		Yes, &lt;denchmark-link:https://github.com/timmeinhardt&gt;@timmeinhardt&lt;/denchmark-link&gt;
 please attach summary data in case we want to repro.
		</comment>
		<comment id='3' author='timmeinhardt' date='2016-10-30T08:48:39Z'>
		Yes, of course. Here is some example summary data. All weights were initialised with the default parameters for tf.contrib.layers.variance_scaling_initializer.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/560359/events.out.tfevents.1477816723.Tims-MacBook-Pro.local.zip&gt;events.out.tfevents.1477816723.Tims-MacBook-Pro.local.zip&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='timmeinhardt' date='2017-06-16T21:32:44Z'>
		I migrated this issue to &lt;denchmark-link:https://github.com/tensorflow/tensorboard/issues/87&gt;tensorflow/tensorboard#87&lt;/denchmark-link&gt;
 because TensorBoard has moved into its own, separate repo. Lets continue discussion there.
		</comment>
		<comment id='5' author='timmeinhardt' date='2020-09-13T23:07:14Z'>
		The weights being written to the histogram needs to be clipped.
Adding this line in def _log_weights(self, epoch) should help:
weight = tf.clip_by_value(weight, -1e12, 1e12)
		</comment>
	</comments>
</bug>