<bug id='27780' author='rootkitchao' open_date='2019-04-12T07:52:36Z' closed_time='2020-08-07T22:41:13Z'>
	<summary>DepthwiseConv mixed precision train super slow(Caused byDepthwiseConv2dNativeBackpropFilter )</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):MS Windows10 X64 1809 build 17763
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):binary(pip)
TensorFlow version (use command below):1.13.1
Python version:3.6
Bazel version (if compiling from source):N/A
GCC/Compiler version (if compiling from source):N/A
CUDA/cuDNN version:10.0/7.5.0.56
GPU model and memory:NVIDIA Geforce RTX2080TI 11GB


I am trying to port mnasnet's TPU implementation to the GPU.It works fine when using FP32.But when using mixed precision training, the speed is very slow.By using the Profiler to track the training loop, I found that DepthwiseConv2dNativeBackpropFilter consumed too much time.I tried adjusting the loss_scale but it didn't work.I want to know if Depthwise convolution does not support backpropagation at half precision, or there are performance issues.
&lt;denchmark-link:https://user-images.githubusercontent.com/40640909/56020277-59a4c380-5d39-11e9-9811-3166dfec6978.png&gt;&lt;/denchmark-link&gt;

Describe the expected behavior

&lt;denchmark-link:https://github.com/rootkitchao/mnasnet_temp&gt;https://github.com/rootkitchao/mnasnet_temp&lt;/denchmark-link&gt;

python mnasnet_main.py --data_dir=D:\dataset\imagenet_tfrecord\ --model_dir=D:\tf_project\mnasnet\  --model_name=mnasnet-a1 --use_bfloat16=True  --use_keras=False --mode=train --train_batch_size=96 --num_gpus=1 --train_steps=2335456 --steps_per_eval=33363
(Temporary version.The code does not really use bfloat16, use float16 instead.)
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
N/A
	</description>
	<comments>
		<comment id='1' author='rootkitchao' date='2019-04-13T21:33:51Z'>
		I tried to upgrade the graphics driver and disabled the IOMMU of AMD ThreadRipper. It seems that it is useless.
		</comment>
		<comment id='2' author='rootkitchao' date='2019-05-01T17:52:35Z'>
		Apologies for the delay in response. Can you please provide a minimal code snippet to reproduce the issue? This can help us to reduce the troubleshooting time. Thanks!
		</comment>
		<comment id='3' author='rootkitchao' date='2019-05-01T21:07:06Z'>
		
Apologies for the delay in response. Can you please provide a minimal code snippet to reproduce the issue? This can help us to reduce the troubleshooting time. Thanks!

This is the code used to reproduce this issue:&lt;denchmark-link:https://github.com/rootkitchao/mnasnet_temp&gt;https://github.com/rootkitchao/mnasnet_temp&lt;/denchmark-link&gt;
.
This code is ported from &lt;denchmark-link:https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet&gt;tensorflow/tpu&lt;/denchmark-link&gt;
.The --use_bfloat16 flag does not mean using bfloat16, but float16.
To run this code, use the following command:

Due to another known issue (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/27392&gt;#27392&lt;/denchmark-link&gt;
), it cannot be run on multiple GPUs.I think this issue is caused by DepthwiseConv2D. If you change the DepthwiseConv2D in the code to Conv2D, the training speed is OK.
&lt;denchmark-link:https://user-images.githubusercontent.com/40640909/57042719-1b475800-6c98-11e9-91d6-a326ebcd47a7.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='rootkitchao' date='2019-05-02T22:53:05Z'>
		Can you replicate this with a simpler example, ideally limited to a comparison of DepthwiseConv2D and Conv2D? Can you reproduce directly with the ops in question? It's hard for us to help debug with larger models/implementations, as so much is at play.
		</comment>
		<comment id='5' author='rootkitchao' date='2019-05-04T23:22:49Z'>
		
Can you replicate this with a simpler example, ideally limited to a comparison of DepthwiseConv2D and Conv2D? Can you reproduce directly with the ops in question? It's hard for us to help debug with larger models/implementations, as so much is at play.

Thanks for reply and apologize for the delayed response.I have rewritten a test code.But this code encountered a new problem in the current tensorflow version (1.13).
If I use tf.contrib.mixed_precision.LossScaleOptimizer, the training does not work properly and displays a warning when using depthwise converge .But the same code works fine when using conv2d.If I don't use it, I get a NaN error when using mixed precision training.It seems impossible to avoid NaN errors by adjusting loss_scale.
But by testing with MNASNET code, I found that this seems to be related to the usage of video memory.If I reduce the batch_size from 96 to 64, the speed of mixing precision training and the speed of training with fp32 will be very close.Further reduce the batch_size, the speed of the mixing precision training will be slightly faster than the training using fp32.But when training with fp32, the speed of batch_size=96 is normal.
So I may not understand the reason for this problem correctly. It seems that the current method of mixing precision training will consume more gpu memory than using fp32.So when the batch_size is large, the gpu memory has overflowed, and some variables are stored in the cpu memory.Eventually, memory access operations across devices cause significant performance degradation.And using depthwise conv is more likely to cause this problem.
I noticed that tensorflow recently added a new api &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/9ce7b4a0a0757eb4019fb373ab238e12addc29d5/tensorflow/python/training/experimental/mixed_precision.py&gt;tf.train.experimental.enable_mixed_precision_graph_rewrite&lt;/denchmark-link&gt;
.This seems to solve the current problem.I will try this method after the official release of tensorflow 1.14.
Thanks again to everyone who tried to help this issue.
import numpy as np
import tensorflow as tf
import functools
from absl import app
from tensorflow.core.protobuf import rewriter_config_pb2
from tensorflow.python.estimator import estimator

def conv_kernel_initializer(shape, dtype=None, partition_info=None):

    del partition_info
    kernel_height, kernel_width, _, out_filters = shape
    fan_out = int(kernel_height * kernel_width * out_filters)
    return tf.random_normal(
        shape, mean=0.0, stddev=np.sqrt(2.0 / fan_out), dtype=dtype)
def dense_kernel_initializer(shape, dtype=None, partition_info=None):

    del partition_info
    init_range = 1.0 / np.sqrt(shape[1])
    return tf.random_uniform(shape, -init_range, init_range, dtype=dtype)

class FakeImageDataInput(object):

    def __init__(self,image_size=64,dataset_size=128*64,use_float16=True):
        self._image_size = image_size
        self._use_float16 = use_float16
        self._dataset_size = dataset_size

    def set_shapes(self, batch_size, images, labels):
        """Statically set the batch_size dimension."""
        images.set_shape(images.get_shape().merge_with(
            tf.TensorShape([batch_size, None, None, None])))
        labels.set_shape(labels.get_shape().merge_with(
            tf.TensorShape([batch_size])))
        return images, labels

    def _get_random_input(self, data):
        return tf.zeros([self._image_size, self._image_size, 3], tf.float16
                                if self._use_float16 else tf.float32)

    def make_source_dataset(self):
        """See base class."""
        return tf.data.Dataset.range(self._dataset_size).repeat().map(self._get_random_input)
    def dataset_parser(self, value):
        #random = tf.random.uniform(shape=[1],dtype=tf.int32,minval=0,maxval=1000 - 1)
        random = np.random.randint(0,999)
        return value, tf.constant(int(random), tf.int32)

    def input_fn(self, params):

        batch_size = params['batch_size']

        dataset = self.make_source_dataset()
        
        dataset = dataset.apply(
            tf.contrib.data.map_and_batch(
                self.dataset_parser, batch_size=batch_size,
                num_parallel_batches=4, drop_remainder=True))

        dataset = dataset.map(functools.partial(self.set_shapes, batch_size))
        dataset = dataset.prefetch(tf.contrib.data.AUTOTUNE)
        return dataset


class TestBlock(object):
    
    def __init__(self,
                 kernel_size,
                 input_filters,
                 output_filters,
                 stride=1,
                 use_depthwise_conv=True):
        self._use_depthwise_conv = use_depthwise_conv
        self._kernel_size = kernel_size
        self._input_filters = input_filters
        self._output_filters = output_filters
        self._stride = stride
        self._build()
    
    def _build(self):
        self._expand_conv = tf.layers.Conv2D(filters=self._input_filters * 6,
                                             kernel_size=[1,1],
                                             strides=[self._stride,self._stride],
                                             kernel_initializer=conv_kernel_initializer,
                                             padding='same',
                                             use_bias=False)
        self._bn0 = tf.layers.BatchNormalization(axis=-1,
                                                 momentum=0.99,
                                                 epsilon=1e-3,
                                                 fused=True)
        if self._use_depthwise_conv:
            self._depthwise_conv = tf.keras.layers.DepthwiseConv2D(
                kernel_size=self._kernel_size,
                strides=[self._stride,self._stride],
                depthwise_initializer=conv_kernel_initializer,
                padding='same',
                use_bias=False)
        else:
            self._depthwise_conv = tf.layers.Conv2D(
                filters=self._input_filters * 6,
                kernel_size=self._kernel_size,
                strides=[self._stride,self._stride],
                kernel_initializer=conv_kernel_initializer,
                padding='same',
                use_bias=False)
        self._bn1 = tf.layers.BatchNormalization(axis=-1,
                                                 momentum=0.99,
                                                 epsilon=1e-3,
                                                 fused=True)
        self._project_conv = tf.layers.Conv2D(filters=self._output_filters,
                                             kernel_size=[1,1],
                                             strides=[self._stride,self._stride],
                                             kernel_initializer=conv_kernel_initializer,
                                             padding='same',
                                             use_bias=False)
        self._bn2 = tf.layers.BatchNormalization(axis=-1,
                                                 momentum=0.99,
                                                 epsilon=1e-3,
                                                 fused=True)
    def call(self,inputs,training=True):
        x = tf.nn.relu(self._bn0(self._expand_conv(inputs), training=training))
        x = tf.nn.relu(self._bn1(self._depthwise_conv(x), training=training))
        x = self._bn2(self._project_conv(x), training=training)
        return x

class TestModel(tf.keras.Model):
    
    def __init__(self,use_depthwise_conv=True):
        super(TestModel,self).__init__()
        self._use_depthwise_conv = use_depthwise_conv
        self._build()
    def _custom_dtype_getter(self, getter, name, shape=None, dtype=tf.float32,
                           *args, **kwargs):
        if dtype is tf.float16:
            var = getter(name, shape, tf.float32, *args, **kwargs)
            return tf.cast(var, dtype=dtype, name=name + '_cast')
        else:
            return getter(name, shape, dtype, *args, **kwargs)

    def _model_variable_scope(self,scope):
        return tf.variable_scope(scope,custom_getter=self._custom_dtype_getter)
    
    def _build(self):
        self._conv_stem = tf.layers.Conv2D(filters=32,
                                           kernel_size=[3,3],
                                           strides=[2,2],
                                           kernel_initializer=conv_kernel_initializer,
                                           padding='same',
                                           use_bias=False)
        self._bn0 = tf.layers.BatchNormalization(axis=-1,
                                                 momentum=0.99,
                                                 epsilon=1e-3,
                                                 fused=True)
        self._conv_head = tf.layers.Conv2D(filters=1280,
                                           kernel_size=[1,1],
                                           strides=[1,1],
                                           kernel_initializer=conv_kernel_initializer,
                                           padding='same',
                                           use_bias=False)
        self._bn1 = tf.layers.BatchNormalization(axis=-1,
                                                 momentum=0.99,
                                                 epsilon=1e-3,
                                                 fused=True)
        self._avg_pooling = tf.keras.layers.GlobalAveragePooling2D(
            data_format='channels_last')
        self._fc = tf.layers.Dense(
            1000,
            kernel_initializer=dense_kernel_initializer)
        self._blocks = []
        self._blocks.append(TestBlock(kernel_size=[3,3],input_filters=32,output_filters=16,stride=1,use_depthwise_conv=self._use_depthwise_conv))
        self._blocks.append(TestBlock(kernel_size=[3,3],input_filters=16,output_filters=24,stride=1,use_depthwise_conv=self._use_depthwise_conv))
        self._blocks.append(TestBlock(kernel_size=[3,3],input_filters=24,output_filters=24,stride=2,use_depthwise_conv=self._use_depthwise_conv))
        self._blocks.append(TestBlock(kernel_size=[3,3],input_filters=24,output_filters=40,stride=1,use_depthwise_conv=self._use_depthwise_conv))
        self._blocks.append(TestBlock(kernel_size=[3,3],input_filters=40,output_filters=40,stride=1,use_depthwise_conv=self._use_depthwise_conv))
        self._blocks.append(TestBlock(kernel_size=[3,3],input_filters=40,output_filters=40,stride=2,use_depthwise_conv=self._use_depthwise_conv))
        #self._blocks.append(TestBlock(kernel_size=[3,3],input_filters=40,output_filters=80,stride=1,use_depthwise_conv=self._use_depthwise_conv))
        #self._blocks.append(TestBlock(kernel_size=[3,3],input_filters=40,output_filters=80,stride=1,use_depthwise_conv=self._use_depthwise_conv))
        self._blocks.append(TestBlock(kernel_size=[3,3],input_filters=40,output_filters=80,stride=1,use_depthwise_conv=self._use_depthwise_conv))
        self._blocks.append(TestBlock(kernel_size=[3,3],input_filters=40,output_filters=80,stride=2,use_depthwise_conv=self._use_depthwise_conv))
        self._blocks.append(TestBlock(kernel_size=[3,3],input_filters=80,output_filters=112,stride=1,use_depthwise_conv=self._use_depthwise_conv))
        self._blocks.append(TestBlock(kernel_size=[3,3],input_filters=112,output_filters=160,stride=2,use_depthwise_conv=self._use_depthwise_conv))
        self._blocks.append(TestBlock(kernel_size=[3,3],input_filters=160,output_filters=320,stride=1,use_depthwise_conv=self._use_depthwise_conv))


    def call(self,inputs,training=True):
        with self._model_variable_scope('testmode'):
            outputs = None
            self.endpoints = {}
            with tf.variable_scope('testmode_stem'):
                outputs = tf.nn.relu(
                    self._bn0(self._conv_stem(inputs), training=training))
            self.endpoints['stem'] = outputs

            for idx, block in enumerate(self._blocks):
                with tf.variable_scope('mnas_blocks_%s' % idx):
                    outputs = block.call(outputs, training=training)
                    self.endpoints['block_%s' % idx] = outputs
            self.endpoints['global_pool'] = outputs
            with tf.variable_scope('testmode_head'):
                outputs = tf.nn.relu(self._bn1(self._conv_head(outputs), training=training))
                outputs = self._avg_pooling(outputs)
                outputs = self._fc(outputs)
                self.endpoints['head'] = outputs
        return outputs,self.endpoints


def test_model_fn(features, labels, mode, params):
    
    model = TestModel(use_depthwise_conv=params['use_depthwise_conv'])
    if params['use_float16']:
        logits, _  = model(features,training=True)
        logits = tf.cast(logits, tf.float32)
    else:
        logits, _  = model(features,training=True)
    

    weight_decay = 1e-5
    one_hot_labels = tf.one_hot(labels, 1000)
    cross_entropy = tf.losses.softmax_cross_entropy(
        logits=logits,
        onehot_labels=one_hot_labels,
        label_smoothing=0.1)
    loss = cross_entropy + weight_decay * tf.add_n([
      tf.nn.l2_loss(tf.cast(v,tf.float32))
      for v in tf.trainable_variables()
      if 'batch_normalization' not in v.name
    ])

    learning_rate = 0.01
    optimizer = tf.train.MomentumOptimizer(
        learning_rate=learning_rate, momentum=0.9)

    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
    global_step = tf.train.get_global_step()
    
    if params['use_float16']:
      
      #loss_scale_manager = tf.contrib.mixed_precision.FixedLossScaleManager(4096)
      #loss_scale_optimizer = tf.contrib.mixed_precision.LossScaleOptimizer(optimizer, loss_scale_manager)
      #train_op = loss_scale_optimizer.minimize(loss,tf.train.get_global_step())
      loss_scale = 128
      scaled_grad_vars = optimizer.compute_gradients(loss * loss_scale)
      unscaled_grad_vars = [(grad / loss_scale, var)
                            for grad, var in scaled_grad_vars]
      minimize_op = optimizer.apply_gradients(unscaled_grad_vars, global_step)
      train_op = tf.group(minimize_op, update_ops)

    else: 
      with tf.control_dependencies(update_ops):
        train_op = optimizer.minimize(loss, tf.train.get_global_step())


    tf.summary.scalar('cross_entropy',cross_entropy)
    tf.summary.scalar('loss',loss)

    return tf.estimator.EstimatorSpec(
      mode=mode,
      loss=loss,
      train_op=train_op,
      eval_metric_ops=None,
      scaffold=None)

def main(unused_argv):
    use_float16 = True
    use_depthwise_conv = True
    model_dir = 'D:/tf_project/depthwiseconv_mixpt/model/'
    batch_size = 64
    train_steps = 100000
    distribution_strategy = None

    gpu_options = tf.GPUOptions(allow_growth=True)
    config = tf.estimator.RunConfig(
      model_dir=model_dir,
      train_distribute=distribution_strategy,
      save_checkpoints_steps=1000,
      log_step_count_steps=1,
      session_config=tf.ConfigProto(
          graph_options=tf.GraphOptions(
              rewrite_options=rewriter_config_pb2.RewriterConfig(
                  disable_meta_optimizer=True)),
          #gpu_options=gpu_options
          ),
    )
    params = dict(
        use_depthwise_conv=use_depthwise_conv,
        steps_per_epoch=64,
        use_float16=use_float16,
        batch_size=batch_size
        )
    testmode_est = tf.estimator.Estimator(
      model_fn=test_model_fn,
      config=config,
      params=params
    )

    fake_imagenet_train = FakeImageDataInput(image_size=224,dataset_size= 128 * batch_size,use_float16=use_float16)
    current_step = load_global_step_from_checkpoint_dir(  # pylint: disable=protected-access
        model_dir)

    tf.logging.info(
        'Training for %d steps (%.2f epochs in total). Current'
        ' step %d.', train_steps,
        train_steps / 64, current_step)
    testmode_est.train(
        input_fn=fake_imagenet_train.input_fn,
        max_steps=train_steps
    )

def load_global_step_from_checkpoint_dir(checkpoint_dir):
    try:
        checkpoint_reader = tf.train.NewCheckpointReader(
            tf.train.latest_checkpoint(checkpoint_dir))
        return checkpoint_reader.get_tensor(tf.GraphKeys.GLOBAL_STEP)
    except:  # pylint: disable=bare-except
        return 0
if __name__ == '__main__':
  tf.logging.set_verbosity(tf.logging.INFO)
  app.run(main)
		</comment>
		<comment id='6' author='rootkitchao' date='2019-06-27T07:52:21Z'>
		I met the same problem, it's much slow compared to float32 precision.
		</comment>
		<comment id='7' author='rootkitchao' date='2019-07-01T18:29:34Z'>
		&lt;denchmark-link:https://github.com/rootkitchao&gt;@rootkitchao&lt;/denchmark-link&gt;
  did you try "tf.train.experimental.enable_mixed_precision_graph_rewrite"? Does it work?
Thanks!
		</comment>
		<comment id='8' author='rootkitchao' date='2019-07-17T00:48:33Z'>
		Actually under my experiments, just doing inference (without backprop), depthwise conv is still slower in fp16 than fp32. So the problem can't just be unsupported half precision in back-propagation.
		</comment>
		<comment id='9' author='rootkitchao' date='2019-12-02T03:29:06Z'>
		any update?
		</comment>
		<comment id='10' author='rootkitchao' date='2020-02-13T14:14:33Z'>
		Seems like this is still super slow
		</comment>
		<comment id='11' author='rootkitchao' date='2020-02-13T14:19:49Z'>
		This is fixed with &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/31597&gt;#31597&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/33836&gt;#33836&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='12' author='rootkitchao' date='2020-02-18T08:58:54Z'>
		
This is fixed with #31597 and #33836.

In which version of tensorflow is this fixed? In the latest nightly 2.2.0.dev20200217, I still experience very slow DepthwiseConv when using mixed precision.
		</comment>
		<comment id='13' author='rootkitchao' date='2020-02-18T10:38:14Z'>
		You need to build TF from source using latest cuDNN.
		</comment>
		<comment id='14' author='rootkitchao' date='2020-02-18T11:25:23Z'>
		Any idea whether this fix will make it into the next release?
		</comment>
		<comment id='15' author='rootkitchao' date='2020-02-19T10:24:11Z'>
		&lt;denchmark-link:https://github.com/urimerhav&gt;@urimerhav&lt;/denchmark-link&gt;
 Both features are included in the 2.1 release tarballs. Note that the change is only enabled with compile-time cuDNN version check, and &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/tools/dockerfiles/partials/ubuntu/nvidia.partial.Dockerfile#L8&gt;TensorFlow 2.1.0 on PyPI is built with cuDNN 7.6.2&lt;/denchmark-link&gt;
. Note that in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/1af66b1e18ca9b8739c68956001b23b0f4ed1bff#diff-1a6332ad6f785f73b7526d275744490aR31&gt;1af66b1#diff-1a6332ad6f785f73b7526d275744490aR31&lt;/denchmark-link&gt;
 it was updated to 7.6.4, so you could expect TF 2.2.0 PyPI package to have this feature enabled.
		</comment>
		<comment id='16' author='rootkitchao' date='2020-02-19T19:37:11Z'>
		&lt;denchmark-link:https://github.com/byronyi&gt;@byronyi&lt;/denchmark-link&gt;
 I checked the git history of v2.1.0 and seems &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/31597&gt;#31597&lt;/denchmark-link&gt;
 is in but &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/33836&gt;#33836&lt;/denchmark-link&gt;
 is not.
		</comment>
		<comment id='17' author='rootkitchao' date='2020-02-20T13:22:26Z'>
		I have just tried with v2.1.0 build for CUDA/CuDNN 10.2.89_441.22/7.6.5.32 and DepthwiseConv is still slow when using mixed precision. The fix does indeed not seem to be included in 2.1.
If I build nightly from source, should it be fixed then?
		</comment>
		<comment id='18' author='rootkitchao' date='2020-02-20T13:35:03Z'>
		
In the latest nightly 2.2.0.dev20200217, I still experience very slow DepthwiseConv when using mixed precision.

I think this is expected for mixed precision, since depthwise convolutions are not whitelisted in the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/7966f2df6b6405a3d4672c868f6b301246ab9870/tensorflow/core/grappler/optimizers/auto_mixed_precision_lists.h#L76-L80&gt;auto mixed-precision optimizer&lt;/denchmark-link&gt;
, or am I missing something &lt;denchmark-link:https://github.com/reedwm&gt;@reedwm&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/benbarsdell&gt;@benbarsdell&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='19' author='rootkitchao' date='2020-02-20T17:36:55Z'>
		Try using mixed precision manually, not the auto mixed precision optimizer.
		</comment>
		<comment id='20' author='rootkitchao' date='2020-02-20T20:01:01Z'>
		I am going to use regular Conv for now, that one is much faster in mixed precision. I am sorry, I don't have time to lose on this any longer. But thanks for the help.
I hope DepthwiseConv for mixed precision will be supported in the next release.
		</comment>
		<comment id='21' author='rootkitchao' date='2020-06-01T03:05:56Z'>
		Any advances on support this? this has a huge impact on the MobileNetV2 provided by tensorflow.keras.applications.
		</comment>
		<comment id='22' author='rootkitchao' date='2020-08-07T22:41:13Z'>
		This should be fixed by &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/201d45cea27c1792a86b3fc7eb688fb2dd1d0df1&gt;201d45c&lt;/denchmark-link&gt;
. DepthwiseConv2dNativeBackpropFilter should not be significantly slower anymore in float16. Sorry for the long delay.
If anyone has any other performance issues with depthwise convolutions, please file a new issue with a self-contained (ideally short) example.
		</comment>
		<comment id='23' author='rootkitchao' date='2020-08-07T22:41:14Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27780&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27780&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='24' author='rootkitchao' date='2020-08-17T08:54:56Z'>
		Thanks. Any idea when this fix will make it into the next release?
		</comment>
		<comment id='25' author='rootkitchao' date='2020-08-17T17:23:56Z'>
		This change will be in TensorFlow 2.4, but I'm unsure when it will be released
		</comment>
	</comments>
</bug>