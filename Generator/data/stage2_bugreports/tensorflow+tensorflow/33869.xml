<bug id='33869' author='aefuimn' open_date='2019-10-31T08:01:33Z' closed_time='2020-04-24T03:51:54Z'>
	<summary>How to save model, multi-worker training with Keras</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary):  binary（pip install tensorflow-gpu）
TensorFlow version (use command below):  2.0.0
Python version: 3.7.4
CUDA/cuDNN version:  CUDA-10.0/cuDNN 7
GPU model and memory: 1X1080TI+2X1070Ti    11GB   8GB

Describe the current behavior
Failure to save model
Describe the expected behavior
Save model
Code to reproduce the issue
`
&lt;denchmark-code&gt;# multi worker strategy
strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()
with strategy.scope():
    train_dataset, validation_dataset, test_dataset = load_data()
    multi_worker_model = Alexnet()
    multi_worker_model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,
                               optimizer=tf.keras.optimizers.Adam(learning_rate=1e-2),
                               metrics=['accuracy'])
# fit
multi_worker_model.fit(train_dataset,
                       epochs=EPOCHS,
                       validation_steps=6,
                       validation_data=validation_dataset,
                       steps_per_epoch=45000//BATCH_SIZE,
                       callbacks=callbacks,
                       verbose=2)

# eval
multi_worker_model.evaluate(test_dataset)

# save model
# multi_worker_model.save(os.path.join(OUTPUT_PATH, 'model'))
tf.saved_model.save(multi_worker_model, os.path.join(OUTPUT_PATH, 'model'))
&lt;/denchmark-code&gt;

`
Other info / logs
Traceback (most recent call last):
File "training_cifar10_with_multi-woker.py", line 197, in 
tf.saved_model.save(multi_worker_model, os.path.join(OUTPUT_PATH, 'model'))
File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py", line 893, in save
meta_graph_def, saveable_view, signatures, options.namespace_whitelist)
File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py", line 596, in _fill_meta_graph_def
saver_def = saver.to_proto()
File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/saving/functional_saver.py", line 150, in to_proto
save_tensor = self._traced_save(filename_tensor)
File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 457, in call
result = self._call(*args, **kwds)
File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 503, in _call
self._initialize(args, kwds, add_initializers_to=initializer_map)
File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 408, in _initialize
*args, **kwds))
File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1848, in _get_concrete_function_internal_garbage_collected
graph_function, _, _ = self._maybe_define_function(args, kwargs)
File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 2150, in _maybe_define_function
graph_function = self._create_graph_function(args, kwargs)
File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 2041, in _create_graph_function
capture_by_value=self._capture_by_value),
File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py", line 915, in func_graph_from_py_func
func_outputs = python_func(*func_args, **func_kwargs)
File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 358, in wrapped_fn
return weak_wrapped_fn().wrapped(*args, **kwds)
File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 2653, in bound_method_wrapper
return wrapped_fn(weak_instance(), *args, **kwargs)
File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/saving/functional_saver.py", line 162, in _traced_save
save_op = self.save(file_prefix)
File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/saving/functional_saver.py", line 230, in save
sharded_saves.append(saver.save(shard_prefix))
File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/saving/functional_saver.py", line 69, in save
tensors.append(spec.tensor)
File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/saving/saveable_object.py", line 52, in tensor
return self._tensor() if callable(self._tensor) else self._tensor
File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/values.py", line 1151, in tensor
return strategy.extended.read_var(sync_on_read_variable)
File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py", line 736, in read_var
return replica_local_var._get_cross_replica()  # pylint: disable=protected-access
File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/values.py", line 1237, in _get_cross_replica
self, axis=None)
File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py", line 805, in reduce
return self._extended._reduce(reduce_op, value)  # pylint: disable=protected-access
File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py", line 1436, in _reduce
device_util.current() or "/device:CPU:0"))[0]
File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/collective_all_reduce_strategy.py", line 490, in _reduce_to
reduce_op, value, destinations=destinations)
File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/cross_device_ops.py", line 282, in reduce
destinations)
File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/cross_device_ops.py", line 1025, in reduce_implementation
all_reduced = self._batch_all_reduce(reduce_op, [per_replica_value])[0]
File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/cross_device_ops.py", line 1091, in _batch_all_reduce
dense_results = self._do_batch_all_reduce_dense(reduce_op, dense_values)
File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/cross_device_ops.py", line 1120, in _do_batch_all_reduce_dense
"Id")
File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/cross_device_utils.py", line 345, in build_collective_reduce
group_key = collective_keys.get_group_key(devices)
File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/cross_device_utils.py", line 295, in get_group_key
names = sorted(['%s:%d' % (d.device_type, d.device_index) for d in parsed])
File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/cross_device_utils.py", line 295, in 
names = sorted(['%s:%d' % (d.device_type, d.device_index) for d in parsed])
TypeError: %d format: a number is required, not NoneType
	</description>
	<comments>
		<comment id='1' author='aefuimn' date='2019-11-01T20:56:39Z'>
		&lt;denchmark-link:https://github.com/aefuimn&gt;@aefuimn&lt;/denchmark-link&gt;
 Can you provide a standalone code to reproduce the issue? Thanks!
		</comment>
		<comment id='2' author='aefuimn' date='2019-11-03T11:47:27Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33869&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33869&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='aefuimn' date='2019-11-04T03:13:28Z'>
		
@aefuimn Can you provide a standalone code to reproduce the issue? Thanks!

# -*- coding: utf-8 -*-
from __future__ import absolute_import, division, print_function, unicode_literals

import os
import json
import pickle
import tensorflow as tf

os.environ['TF_CONFIG'] = json.dumps({
    'cluster': {
        'worker': ['10.10.2.37:12345', '10.10.2.38:12345']
    },
    'task': {'type': 'worker', 'index': 0}
})


def image_enhancement(image, label):
    image = tf.dtypes.cast(image, tf.dtypes.float32)
    image = tf.reshape(image, [3, 32, 32])
    image = tf.transpose(image, [1, 2, 0])

    # image = tf.image.random_crop(image, [28, 28, 3])
    image = tf.image.random_flip_left_right(image)
    image = tf.image.random_brightness(image, max_delta=63)
    image = tf.image.random_contrast(image, lower=0.2, upper=1.8)
    image = tf.image.per_image_standardization(image)
    return image, label


def test_image(image, label):
    image = tf.dtypes.cast(image, tf.dtypes.float32)
    image = tf.reshape(image, [3, 32, 32])
    image = tf.transpose(image, [1, 2, 0])
    image = tf.image.per_image_standardization(image)
    return image, label


def load_data():
    train_data = {b'data': [], b'labels': []}

    for i in range(5):
        with open("cifar-10-batches-py/data_batch_" + str(i+1), mode='rb') as f:
            data = pickle.load(f, encoding="bytes")
            train_data[b'data'] += list(data[b'data'])
            train_data[b'labels'] += data[b'labels']

    with open("cifar-10-batches-py/test_batch", mode='rb') as file:
        test_data = pickle.load(file, encoding='bytes')

    train_ds = tf.data.Dataset.from_tensor_slices(
        (train_data[b'data'][:45000], train_data[b'labels'][:45000]))
    train_ds = train_ds.map(image_enhancement, num_parallel_calls=10)
    train_ds = train_ds.shuffle(10000).batch(BATCH_SIZE).repeat()
    validation_ds = tf.data.Dataset.from_tensor_slices(
        (train_data[b'data'][45000:], train_data[b'labels'][45000:]))
    validation_ds = validation_ds.map(image_enhancement, num_parallel_calls=10)
    validation_ds = validation_ds.batch(BATCH_SIZE)
    test_ds = tf.data.Dataset.from_tensor_slices((test_data[b'data'], test_data[b'labels']))
    test_ds = test_ds.map(test_image, num_parallel_calls=10)
    test_ds = test_ds.batch(BATCH_SIZE)
    return train_ds, validation_ds, test_ds


class Alexnet(tf.keras.Model):
    def __init__(self):
        super(Alexnet, self).__init__()
        self.conv1 = tf.keras.layers.Conv2D(64, 3, activation='relu', name='conv1')
        self.conv2 = tf.keras.layers.Conv2D(128, 3, activation='relu', name='conv2')
        self.conv3 = tf.keras.layers.Conv2D(256, 3, activation='relu', name='conv3')
        self.pool1 = tf.keras.layers.MaxPool2D(2, 2, name='pool1')
        self.pool2 = tf.keras.layers.MaxPool2D(2, 2, name='pool2')
        self.pool3 = tf.keras.layers.MaxPool2D(2, 2, name='pool3')
        self.batch_normalization1 = tf.keras.layers.BatchNormalization(name='bn1')
        self.batch_normalization2 = tf.keras.layers.BatchNormalization(name='bn2')
        self.batch_normalization3 = tf.keras.layers.BatchNormalization(name='bn3')
        self.flatten = tf.keras.layers.Flatten(name='f1')
        self.dropout = tf.keras.layers.Dropout(0.5)
        self.d1 = tf.keras.layers.Dense(1024, activation='relu', name='d1')
        self.d2 = tf.keras.layers.Dense(512, activation='relu', name='d2')
        self.d3 = tf.keras.layers.Dense(256, activation='relu', name='d3')
        self.d4 = tf.keras.layers.Dense(10, activation='softmax', name='out')

    def call(self, x):
        # conv1
        x = self.conv1(x)
        x = self.pool1(x)
        x = self.batch_normalization1(x)

        # conv2
        x = self.conv2(x)
        x = self.pool2(x)
        x = self.batch_normalization2(x)

        # conv3
        x = self.conv3(x)
        x = self.pool3(x)
        x = self.batch_normalization3(x)

        # Flatten
        x = self.flatten(x)
        x = self.dropout(x)

        # Dense
        x = self.d1(x)
        x = self.d2(x)
        x = self.d3(x)
        return self.d4(x)


def decay(epoch):
    if epoch &lt; 30:
        return 1e-2
    elif epoch &gt;= 30 and epoch &lt; 50:
        return 5e-3
    elif epoch &gt;= 50 and epoch &lt; 80:
        return 1e-3
    elif epoch &gt;= 80 and epoch &lt; 120:
        return 1e-4
    else:
        return 1e-5


class PrintLR(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        print('\nLearning rate for epoch {} is {}'.format(epoch + 1,
                                                          multi_worker_model.optimizer.lr.numpy()))


if __name__ == "__main__":

    BATCH_SIZE_PER_WORKER = 384
    NUM_WORKERS = 2
    BATCH_SIZE = BATCH_SIZE_PER_WORKER * NUM_WORKERS
    OUTPUT_PATH = '/home/output'
    EPOCHS = 2

    checkpoint_dir = os.path.join(OUTPUT_PATH, 'train_checkpoints')
    checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt_{epoch}")

    callbacks = [
        tf.keras.callbacks.TensorBoard(log_dir=os.path.join(OUTPUT_PATH, 'logs')),
        tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,
                                           save_weights_only=True),
        tf.keras.callbacks.LearningRateScheduler(decay),
        PrintLR()
    ]

    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()

    with strategy.scope():
        train_dataset, validation_dataset, test_dataset = load_data()
        multi_worker_model = Alexnet()
        multi_worker_model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,
                                   optimizer=tf.keras.optimizers.Adam(learning_rate=1e-2),
                                   metrics=['accuracy'])

    multi_worker_model.fit(train_dataset,
                           epochs=EPOCHS,
                           validation_steps=6,
                           validation_data=validation_dataset,
                           steps_per_epoch=45000//BATCH_SIZE,
                           callbacks=callbacks,
                           verbose=2)

    multi_worker_model.evaluate(test_dataset)

    multi_worker_model.save(os.path.join(OUTPUT_PATH, 'model'))
The dataset is cifar-10-batches-py
		</comment>
		<comment id='4' author='aefuimn' date='2019-11-11T01:52:44Z'>
		I think this should be fixed by &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/c82df39b0cfd6fee460c1e942a3322061bc6f8bb#diff-39620d083041d43ea80ef9ef7246652b&gt;c82df39#diff-39620d083041d43ea80ef9ef7246652b&lt;/denchmark-link&gt;
 which was committed couple days ago.
Are you able to test with a nightly version of tensorflow (&lt;denchmark-link:https://pypi.org/project/tf-nightly-gpu/&gt;https://pypi.org/project/tf-nightly-gpu/&lt;/denchmark-link&gt;
)
		</comment>
		<comment id='5' author='aefuimn' date='2019-11-11T06:16:56Z'>
		
I think this should be fixed by c82df39#diff-39620d083041d43ea80ef9ef7246652b which was committed couple days ago.
Are you able to test with a nightly version of tensorflow (https://pypi.org/project/tf-nightly-gpu/)

This error occurred in my code on TF version 2.1.0-dev20191110
&lt;denchmark-code&gt;[[{{node IteratorGetNext}}]]
2019-11-11 14:08:07.039360: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_1}}]]
	 [[replica_1/metrics/accuracy/AssignAddVariableOp_1/_53]]
2019-11-11 14:08:07.039467: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_1}}]]
	 [[allreduce_1/CollectiveReduce/_56]]
2019-11-11 14:08:07.039615: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
     14/Unknown - 1s 59ms/step - loss: 2.7934 - accuracy: 0.16942019-11-11 14:08:07.871879: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
W1111 14:08:08.245472 140200689571584 deprecation.py:506] From /home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2019-11-11 14:08:08.564973: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-11-11 14:08:08.564980: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-11-11 14:08:08.565051: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-11-11 14:08:08.565059: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-11-11 14:08:08.565173: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at collective_ops.cc:253 : Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-11-11 14:08:08.565190: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-11-11 14:08:08.565219: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-11-11 14:08:08.565174: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at collective_ops.cc:253 : Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-11-11 14:08:08.565231: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
	 [[CollectiveReduce]]
2019-11-11 14:08:08.565261: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at collective_ops.cc:253 : Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-11-11 14:08:08.565324: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
	 [[CollectiveReduce_2]]
2019-11-11 14:08:08.565374: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
	 [[CollectiveReduce_1]]
Traceback (most recent call last):
  File "training_cifar10_with_multi-woker.py", line 196, in &lt;module&gt;
    multi_worker_model.save(os.path.join(OUTPUT_PATH, 'model'))
  File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py", line 1008, in save
    signatures, options)
  File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py", line 115, in save_model
    signatures, options)
  File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save.py", line 78, in save
    save_lib.save(model, filepath, signatures, options)
  File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py", line 920, in save
    object_saver.save(utils_impl.get_variables_path(export_dir))
  File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/util.py", line 1168, in save
    file_prefix=file_prefix_tensor, object_graph_tensor=object_graph_tensor)
  File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/util.py", line 1116, in _save_cached_when_graph_building
    save_op = saver.save(file_prefix)
  File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/saving/functional_saver.py", line 230, in save
    sharded_saves.append(saver.save(shard_prefix))
  File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/saving/functional_saver.py", line 69, in save
    tensors.append(spec.tensor)
  File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/saving/saveable_object.py", line 52, in tensor
    return self._tensor() if callable(self._tensor) else self._tensor
  File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/values.py", line 1252, in tensor
    return strategy.extended.read_var(sync_on_read_variable)
  File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py", line 763, in read_var
    return replica_local_var._get_cross_replica()  # pylint: disable=protected-access
  File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/values.py", line 1347, in _get_cross_replica
    self, axis=None)
  File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py", line 808, in reduce
    return self._extended._reduce(reduce_op, value)  # pylint: disable=protected-access
  File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py", line 1449, in _reduce
    device_util.current() or "/device:CPU:0"))[0]
  File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/collective_all_reduce_strategy.py", line 528, in _reduce_to
    reduce_op, value, destinations=destinations)
  File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/cross_device_ops.py", line 282, in reduce
    destinations)
  File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/cross_device_ops.py", line 1038, in reduce_implementation
    all_reduced = self._batch_all_reduce(reduce_op, [per_replica_value])[0]
  File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/cross_device_ops.py", line 1118, in _batch_all_reduce
    dense_results = self._do_batch_all_reduce_dense(reduce_op, dense_values)
  File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/cross_device_ops.py", line 1160, in _do_batch_all_reduce_dense
    "Id", communication_hint)
  File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/cross_device_utils.py", line 368, in build_collective_reduce
    return collective_all_reduce()
  File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 568, in __call__
    result = self._call(*args, **kwds)
  File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 638, in _call
    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access
  File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1589, in _filtered_call
    self.captured_inputs)
  File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1670, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 523, in call
    ctx=ctx)
  File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File "&lt;string&gt;", line 3, in raise_from
tensorflow.python.framework.errors_impl.OutOfRangeError:  [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
	 [[CollectiveReduce]] [Op:__inference_collective_all_reduce_11819]
&lt;/denchmark-code&gt;

I upgraded my cuda, now:
cuda version: 10.1
cudnn version: 7.6.5.32-1+cuda10.1
libnvinfer version: 6.0.1-1+cuda10.1
		</comment>
		<comment id='6' author='aefuimn' date='2020-04-02T22:11:49Z'>
		&lt;denchmark-link:https://github.com/aefuimn&gt;@aefuimn&lt;/denchmark-link&gt;
 Can you please check whether this was resolved for you. Thanks!
		</comment>
		<comment id='7' author='aefuimn' date='2020-04-09T02:19:47Z'>
		&lt;denchmark-link:https://github.com/aefuimn&gt;@aefuimn&lt;/denchmark-link&gt;
 Can you please check whether this was resolved for you. Thanks!
		</comment>
		<comment id='8' author='aefuimn' date='2020-04-24T03:51:54Z'>
		Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
		<comment id='9' author='aefuimn' date='2020-04-24T03:51:56Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33869&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33869&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>