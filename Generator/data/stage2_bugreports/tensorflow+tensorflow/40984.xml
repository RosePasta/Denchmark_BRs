<bug id='40984' author='borundev' open_date='2020-07-01T11:35:22Z' closed_time='2020-07-06T14:54:34Z'>
	<summary>A function that is wrapped with tf.function gives wrong gradients on loss functions that are themselves wrapped by the same decorator</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No
TensorFlow installed from (source or binary):
Binary
TensorFlow version (use command below):
2.2.0
Python version:
3.6.9
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with:

TF 1.0: python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
TF 2.0: python -c "import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"

Describe the current behavior
I have two versions of loss function, one decorated with tf.function and the other not,  and I have two functions that compute the gradients using these two loss functions, one decorate with tf.function and the other not . When I use the gradient function that is not decorate I get the same results for both the loss function. However, when I use the gradient function that is decorated I get different results and in particular I get a wrong result for the loss function that is decorated.
Describe the expected behavior
I would expect all of the above to give the same result.
Standalone code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
&lt;denchmark-link:https://colab.research.google.com/drive/1erNOChp355OTiCwWiizPdjA1OT1s3HYV?usp=sharing&gt;https://colab.research.google.com/drive/1erNOChp355OTiCwWiizPdjA1OT1s3HYV?usp=sharing&lt;/denchmark-link&gt;

Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='borundev' date='2020-07-01T15:28:40Z'>
		Was able to reproduce the issue with TF v2.2 and TF-nightly. Please find the gist of it &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/6a0afaa83051077734f18b5032f8ef1c/40984.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='borundev' date='2020-07-02T20:18:21Z'>
		Hi &lt;denchmark-link:https://github.com/borundev&gt;@borundev&lt;/denchmark-link&gt;
, you mentioned that when you use the gradient function that is decorated, you get the wrong result for the loss function that is decorated.
I'm running your colab and seeing that the different result is for the loss function that is not decorated. Can you double check and let me know if I'm reading this correctly?
		</comment>
		<comment id='3' author='borundev' date='2020-07-02T22:43:09Z'>
		Hi &lt;denchmark-link:https://github.com/nikitamaia&gt;@nikitamaia&lt;/denchmark-link&gt;
 you are right. I must have mistyped it here. When I asked this on &lt;denchmark-link:https://stackoverflow.com/questions/62428590/gradienttape-gives-different-gradients-depending-on-loss-function-being-decorate&gt;stackoverflow &lt;/denchmark-link&gt;
 I said

Thus, when my outer function is decorated I only get the correct answer from the inner function that is decorated as well. I was under the impression that decorating the outer one (which is the training loop in many applications) is sufficient but here we see its not.

I apologize for the confusion.
		</comment>
		<comment id='4' author='borundev' date='2020-07-03T15:25:47Z'>
		No worries!
Using from_logits=True in your loss function is more numerically stable (see &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/keras/losses.py#L554&gt;note in docs here&lt;/denchmark-link&gt;
) and recommended. If you do that and also remove the softmax from your output layer you should see the same result in all cases.
		</comment>
		<comment id='5' author='borundev' date='2020-07-03T20:51:15Z'>
		&lt;denchmark-link:https://github.com/nikitamaia&gt;@nikitamaia&lt;/denchmark-link&gt;
 So this is not considered a bug?.
		</comment>
		<comment id='6' author='borundev' date='2020-07-06T14:54:34Z'>
		Hi &lt;denchmark-link:https://github.com/borundev&gt;@borundev&lt;/denchmark-link&gt;
 you are correct, it is a bug since the behavior is surprising and from_logits=False does not error out. And I believe this is a duplicate of issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/32895&gt;#32895&lt;/denchmark-link&gt;
. I will close this issue now so we can track this bug in one place, but feel free to reopen if you don't think it's a duplicate.
		</comment>
		<comment id='7' author='borundev' date='2020-07-06T14:54:36Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40984&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40984&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>