<bug id='25858' author='Kublai-Jing' open_date='2019-02-19T01:44:04Z' closed_time='2019-04-26T17:35:06Z'>
	<summary>Tensorflow Parameter Server Hangs when doing distributed training with Estimator</summary>
	<description>
Please make sure that this is a documentation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template
System information

TensorFlow version:1.5, 1.8, 1.12 all the same results
Doc Link:  https://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig , https://www.tensorflow.org/guide/distribute_strategy#example_with_estimator_api

Describe the documentation issue
Is parameter server expected to be killed after the training is done when using tf.estimator.Estimator for distributed training ? or it is expected behavior that ps hangs forever ?
I am trying simple mnist example on localhost to try to get distributed training with estimator work but not able to do so.
Here is the complete code (code I downloaded and modified from &lt;denchmark-link:https://github.com/yu-iskw/tensorflow-serving-example/blob/master/python/train/mnist_custom_estimator.py&gt;https://github.com/yu-iskw/tensorflow-serving-example/blob/master/python/train/mnist_custom_estimator.py&lt;/denchmark-link&gt;
).
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function


import simplejson 
import numpy as np
import tensorflow as tf

tf.logging.set_verbosity(tf.logging.INFO)

tf.app.flags.DEFINE_integer('steps', 100, 'The number of steps to train a model')
tf.app.flags.DEFINE_string('job_name', 'master', 'job_name')
tf.app.flags.DEFINE_string('task_index', '0', 'task_index')
tf.app.flags.DEFINE_string('model_dir', './models/ckpt/', 'Dir to save a model and checkpoints')
FLAGS = tf.app.flags.FLAGS

INPUT_FEATURE = 'image'
NUM_CLASSES = 10


def model_fn(features, labels, mode):
    # Input Layer
    input_layer = features[INPUT_FEATURE]
    # Logits layer
    logits = tf.layers.dense(inputs=input_layer, units=NUM_CLASSES)

    predictions = {
        # Generate predictions (for PREDICT and EVAL mode)
        "classes": tf.argmax(input=logits, axis=1),
        # Add `softmax_tensor` to the graph. It is used for PREDICT and by the
        # `logging_hook`.
        "probabilities": tf.nn.softmax(logits, name="softmax_tensor")
    }

    # PREDICT mode
    if mode == tf.estimator.ModeKeys.PREDICT:
        return tf.estimator.EstimatorSpec(
            mode=mode,
            predictions=predictions,
            export_outputs={
                'predict': tf.estimator.export.PredictOutput(predictions)
            })

    # Calculate Loss (for both TRAIN and EVAL modes)
    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)

    # Configure the Training Op (for TRAIN mode)
    if mode == tf.estimator.ModeKeys.TRAIN:
        optimizer = tf.train.AdamOptimizer()
        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())
        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)

    # Add evaluation metrics (for EVAL mode)
    eval_metric_ops = {
        "accuracy": tf.metrics.accuracy(labels=labels, predictions=predictions["classes"])
    }
    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)


def main(_):
    # Load training and eval data
    mnist = tf.contrib.learn.datasets.load_dataset("mnist")
    train_data = mnist.train.images  # Returns np.array
    train_labels = np.asarray(mnist.train.labels, dtype=np.int32)
    eval_data = mnist.test.images  # Returns np.array
    eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)

    # reshape images
    # To have input as an image, we reshape images beforehand.
    train_data = train_data.reshape(train_data.shape[0], 28 * 28)
    eval_data = eval_data.reshape(eval_data.shape[0], 28 * 28)

    # Create the Estimator
    training_config = tf.estimator.RunConfig(
        model_dir=FLAGS.model_dir,
        save_summary_steps=20,
        save_checkpoints_steps=20)
    classifier = tf.estimator.Estimator(
        model_fn=model_fn,
        model_dir=FLAGS.model_dir,
        config=training_config)

    # Set up logging for predictions
    # Log the values in the "Softmax" tensor with label "probabilities"
    tensors_to_log = {"probabilities": "softmax_tensor"}
    logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=50)

    # Train the model
    train_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={INPUT_FEATURE: train_data},
        y=train_labels,
        batch_size=FLAGS.steps,
        num_epochs=1,
        shuffle=True)

    # Evaluate the model and print results
    eval_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={INPUT_FEATURE: eval_data},
        y=eval_labels,
        num_epochs=1,
        shuffle=False)
    train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn)
    # setup eval spec evaluating ever n seconds
    eval_spec = tf.estimator.EvalSpec(input_fn = eval_input_fn)
    tf.estimator.train_and_evaluate(classifier, train_spec, eval_spec)
def make_tf_training_config(args):
    """
    Returns TF_CONFIG that can be used to set the environment variable necessary for distributed training
    See https://github.com/clusterone/clusterone-tutorials/blob/master/tf-estimator/mnist.py
    """
    worker_hosts = ['localhost:2222']
    ps_hosts = ['localhost:2224']
    tf_config = {
        'task': {
            'type': FLAGS.job_name,
            'index': FLAGS.task_index
        },
        'cluster': {
            'master': [worker_hosts[0]],
            'ps': ps_hosts
        },
        'environment': 'cloud'
    }

    return tf_config

import os
if __name__ == "__main__":
    print("@@@ Version: {}".format(tf.__version__))
    tf_config = make_tf_training_config(None)
    os.environ['TF_CONFIG'] = simplejson.dumps(tf_config)
    tf.app.run()
Here is how I launch one master job and one ps job:
# launch master worker job
python test.py 
# launch ps job
python test.py --job_name ps
Here is the job log for master worker job (only last few lines as this job succeeded and exit):
INFO:tensorflow:Finished evaluation at 2019-02-19-01:40:18
INFO:tensorflow:Saving dict for global step 3301: accuracy = 0.9918, global_step = 3301, loss = 0.025573652
INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3301: ./models/ckpt/model.ckpt-3301
INFO:tensorflow:Loss for final step: 0.02294134.
Here is the complete job log for ps job:
@@@ Version: 1.12.0
WARNING:tensorflow:From test.py:118: load_dataset (from tensorflow.contrib.learn.python.learn.datasets) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.data.
WARNING:tensorflow:From /Users/hjing/miniconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/__init__.py:80: load_mnist (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
WARNING:tensorflow:From /Users/hjing/miniconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:300: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
WARNING:tensorflow:From /Users/hjing/miniconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Please write your own downloading logic.
WARNING:tensorflow:From /Users/hjing/miniconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.data to implement this functionality.
Extracting MNIST-data/train-images-idx3-ubyte.gz
WARNING:tensorflow:From /Users/hjing/miniconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.data to implement this functionality.
Extracting MNIST-data/train-labels-idx1-ubyte.gz
Extracting MNIST-data/t10k-images-idx3-ubyte.gz
Extracting MNIST-data/t10k-labels-idx1-ubyte.gz
WARNING:tensorflow:From /Users/hjing/miniconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: __init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
INFO:tensorflow:TF_CONFIG environment variable: {u'environment': u'cloud', u'cluster': {u'ps': [u'localhost:2224'], u'master': [u'localhost:2222']}, u'task': {u'index': u'0', u'type': u'ps'}}
INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_session_config': device_filters: "/job:ps"
device_filters: "/job:worker"
device_filters: "/job:master"
allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_task_type': u'ps', '_train_distribute': None, '_is_chief': False, '_cluster_spec': &lt;tensorflow.python.training.server_lib.ClusterSpec object at 0x127a47550&gt;, '_model_dir': './models/ckpt/', '_protocol': None, '_save_checkpoints_steps': 20, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 1, '_tf_random_seed': None, '_save_summary_steps': 20, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 1, '_master': u'grpc://localhost:2224'}
INFO:tensorflow:Not using Distribute Coordinator.
INFO:tensorflow:Start Tensorflow server.
2019-02-18 17:38:15.844761: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-02-18 17:38:15.846148: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:222] Initialize GrpcChannelCache for job master -&gt; {0 -&gt; localhost:2222}
2019-02-18 17:38:15.846171: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:222] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; localhost:2224}
2019-02-18 17:38:15.846704: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:381] Started server with target: grpc://localhost:2224

And the ps hangs forever.
I would expect estimator, being such a high-level API should handle closing the parameter server when training is done. If that is not the case, it should be clear in the documentation.
Thanks !
We welcome contributions by users. Will you be able to update submit a PR (use the doc style guide) to fix the doc Issue?
	</description>
	<comments>
		<comment id='1' author='Kublai-Jing' date='2019-03-08T22:52:27Z'>
		For 1.x tensorflow, can you follow this instruction to set up parameter server and worker?
&lt;denchmark-link:https://github.com/tensorflow/examples/blob/master/community/en/docs/deploy/distributed.md&gt;https://github.com/tensorflow/examples/blob/master/community/en/docs/deploy/distributed.md&lt;/denchmark-link&gt;

I think the server.join() should get terminated once job is done.
		</comment>
		<comment id='2' author='Kublai-Jing' date='2019-03-08T23:03:10Z'>
		&lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;
 server.join() blocks forever based on :
&lt;denchmark-code&gt;   def join(self):
    Blocks until the server has shut down.

    This method currently blocks forever.

    Raises:
      tf.errors.OpError: Or one of its subclasses if an error occurs while
        joining the TensorFlow server.
    
    with errors.raise_exception_on_not_ok_status() as status:
      pywrap_tensorflow.PyServer_Join(self._server, status)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='Kublai-Jing' date='2019-03-09T00:58:26Z'>
		Oh then you probably have to ps aux and then kill it manually. Seems this has been an issue for quite some time. (Contributions welcome)
		</comment>
		<comment id='4' author='Kublai-Jing' date='2019-03-09T01:16:19Z'>
		The cluster orchestrator (Yarn, K8s, or the poor programmer himself) is expected to terminate PS processes when the job is done, along with handling failing workers by respawning processes.
		</comment>
		<comment id='5' author='Kublai-Jing' date='2019-04-26T17:35:07Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=25858&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=25858&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>