<bug id='42498' author='Eric-Le-Ge' open_date='2020-08-19T18:30:55Z' closed_time='2020-10-19T18:22:59Z'>
	<summary>ConnectionResetByPeer using MultiWorkerMirroredStrategy with Native Keras BERT Model</summary>
	<description>
Describe the current behavior
I am trying to use the following script to run multi-worker distributed training on a Google Kubernetes Engine (GKE) cluster. The script completes and uploads the trained model when there is only one worker. However, it fails with a connection reset by peer error after some arbitrary number of steps when I use multiple nodes.
Standalone code to reproduce the issue
Training script: &lt;denchmark-link:https://gist.github.com/Eric-Le-Ge/21fe4df4c8e361c60d20a683b08aa743&gt;https://gist.github.com/Eric-Le-Ge/21fe4df4c8e361c60d20a683b08aa743&lt;/denchmark-link&gt;

Original example: &lt;denchmark-link:https://github.com/tensorflow/tfx/tree/master/tfx/examples/bert/cola&gt;https://github.com/tensorflow/tfx/tree/master/tfx/examples/bert/cola&lt;/denchmark-link&gt;

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Container-Optimized OS (cos) (default)
TensorFlow installed from (source or binary): pip
TensorFlow version (use command below): 2.3.0
Python version: 3.6.9
Docker Image: tensorflow/tensorflow:latest-gpu
GPU model and memory: NVIDIA K80


Pod execution logs:
&lt;denchmark-link:https://gist.github.com/Eric-Le-Ge/693640219f54cb6350e2ea1cc4d97547&gt;https://gist.github.com/Eric-Le-Ge/693640219f54cb6350e2ea1cc4d97547&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='Eric-Le-Ge' date='2020-08-20T22:49:55Z'>
		Hi &lt;denchmark-link:https://github.com/Eric-Le-Ge&gt;@Eric-Le-Ge&lt;/denchmark-link&gt;
, looking at the logs it's not clear to me that the training started. Where are you seeing that?
This seems like it will be a bit tricky to reproduce. For starters, are you seeing this happen every time you try to kick off multi worker training on GKE? If so, can you try out a simpler model to help determine if the problem is with TF or GKE?
Additionally, MultiWorkerMirroredStrategy can be run on a single machine without extra setup. Can you confirm that you're able to train successfully on each machine individually? If so, can you provide the logs for the successful training?
		</comment>
		<comment id='2' author='Eric-Le-Ge' date='2020-08-24T17:22:52Z'>
		Hi &lt;denchmark-link:https://github.com/nikitamaia&gt;@nikitamaia&lt;/denchmark-link&gt;
 , thank you for following up on this issue. I've tried training another model with the same setup and it ran successfully, so I think the issue is related to this specific model. In addition, I have successfully trained this model using a single node with the same script, and here are the logs from the worker:
&lt;denchmark-link:https://gist.github.com/Eric-Le-Ge/48380f2c84afa635989dfc82d175d03a&gt;https://gist.github.com/Eric-Le-Ge/48380f2c84afa635989dfc82d175d03a&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='Eric-Le-Ge' date='2020-08-25T15:53:17Z'>
		So to clarify, every time you try and run this model with two nodes you get this exact same error Aborting RingReduce with Unavailable: Connection reset by peer?
For the model that you were able to train successfully, was it also from tf hub?
I can try to repro this on my end. Can you provide complete instructions with data and how to run the script?
		</comment>
		<comment id='4' author='Eric-Le-Ge' date='2020-08-25T15:58:00Z'>
		Actually one more question -- for the single case it looks like the collective communication was set to RING. And it was AUTO when you trained with two nodes. This shouldn't make a difference, but just to sanity check can you also confirm that the two nodes fail with RING set as well.
		</comment>
		<comment id='5' author='Eric-Le-Ge' date='2020-08-29T07:57:49Z'>
		Thanks for following up! Yes, every time I run the script I get the exact same error, this also happens with 4 nodes as well. The other model is directly imported from tf.keras.applications.MobileNet and this model does not have the collective communication error.
Also, when I got the original error the collective communication was set to AUTO. I tried setting it to RING and NCCL and both fail as well.
I think the error should be relatively easy to reproduce, but it requires setup for running TFX pipelines. The original example is here: &lt;denchmark-link:https://github.com/tensorflow/tfx/tree/master/tfx/examples/bert/cola&gt;https://github.com/tensorflow/tfx/tree/master/tfx/examples/bert/cola&lt;/denchmark-link&gt;
. The fastest way to reproduce this is to:

Run the pipeline locally and collect the data artifacts from the output of the Transform component.
Pack the data artifacts with the training script into a container image.
Start a GKE cluster with 2 nodes, and set up TF Config for the nodes to communicate with each other.
Run the run_fn in both containers where fn_args contains the path to the transformed data.

Let me know if there is anything I can provide. Thank you!
		</comment>
		<comment id='6' author='Eric-Le-Ge' date='2020-09-02T22:04:49Z'>
		Do you face the same problem when running the original bert model with MWMS?
		</comment>
		<comment id='7' author='Eric-Le-Ge' date='2020-09-05T05:06:18Z'>
		Not sure since I have only tried running the BERT model provided in the example.
		</comment>
		<comment id='8' author='Eric-Le-Ge' date='2020-09-09T00:43:37Z'>
		To clarify, you are using the same bert_cola_pipeline.py and bert_cola_pipeline_e2e_test.py scripts from the tfx/tfx/examples/bert/cola/ repo, and just swapping out the bert_cola_utils.py file? If so, I think trying to run the original COLA model with MWMS would be worthwhile. If that trains okay then it will help narrow down where the issue is, and if it fails then we can reach out to the maintainers of that code.
		</comment>
		<comment id='9' author='Eric-Le-Ge' date='2020-09-21T04:10:03Z'>
		The original bert_cola_pipeline.py file runs on a single node so I only followed this pipeline up to the trainer part. By original COLA model do you mean the one provided in bert_cola_utils.py? If so the modified bert_cola_utils.py should be still running the same model.
		</comment>
		<comment id='10' author='Eric-Le-Ge' date='2020-09-23T18:54:03Z'>
		
I think the error should be relatively easy to reproduce, but it requires setup for running TFX pipelines. The original example is here: https://github.com/tensorflow/tfx/tree/master/tfx/examples/bert/cola. The fastest way to reproduce this is to:

Run the pipeline locally and collect the data artifacts from the output of the Transform component.
Pack the data artifacts with the training script into a container image.
Start a GKE cluster with 2 nodes, and set up TF Config for the nodes to communicate with each other.
Run the run_fn in both containers where fn_args contains the path to the transformed data.


I'm still a little confused by your above instructions to reproduce the issue. You linked to the original example but it seems that is not needed to reproduce, correct? All I need is &lt;denchmark-link:https://gist.github.com/Eric-Le-Ge/21fe4df4c8e361c60d20a683b08aa743&gt;this bert_cola.utils.py&lt;/denchmark-link&gt;
 script from your repo?
		</comment>
		<comment id='11' author='Eric-Le-Ge' date='2020-09-28T04:57:54Z'>
		Yes, the original example is not needed to reproduce this issue, but the training steps requires transformed data and running the original example up to the Transform component would produce this data.
		</comment>
		<comment id='12' author='Eric-Le-Ge' date='2020-10-01T21:49:16Z'>
		I suspect this is caused by instance preemption. MultiworkerMirroredStrategy requires all workers to be up to do gradient aggregation. Can you try using &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/experimental/BackupAndRestore&gt;BackupAndRestore&lt;/denchmark-link&gt;
 callback, and wraps  with a loop and try/catch &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/errors/UnavailableError&gt;UnavailableError&lt;/denchmark-link&gt;
?
Alternatively, just restart all the instances should also work.
		</comment>
		<comment id='13' author='Eric-Le-Ge' date='2020-10-12T17:45:48Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='14' author='Eric-Le-Ge' date='2020-10-19T18:22:57Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='15' author='Eric-Le-Ge' date='2020-10-19T18:23:00Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42498&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42498&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>