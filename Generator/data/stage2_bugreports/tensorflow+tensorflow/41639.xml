<bug id='41639' author='bsugerman' open_date='2020-07-22T20:49:06Z' closed_time='2020-08-06T12:23:01Z'>
	<summary>Using learning-rate decay schedule and verbose=1 creates fatal TypeError</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 and Windows 10
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO
TensorFlow installed from (source or binary): Binary (pip3)
TensorFlow version (use command below): 2.2.0
Python version: 3.6
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A

Describe the current behavior
In the following minimal example, setting verbose=1 in the fitting creates a fatal error. I realize that ReduceLROnPlateau isn't needed when also using ExponentialDecay but the fact that this error happens if verbose=1 and not when verbose=0 in training shows there is a deeper problem.
&lt;denchmark-code&gt;# minimum keras example
import tensorflow as tf
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.optimizers.schedules import ExponentialDecay
from tensorflow.keras.layers import Dense
from tensorflow.keras import Sequential
from tensorflow.keras.callbacks import TerminateOnNaN,EarlyStopping,ReduceLROnPlateau

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8,input_shape=(16,)))
model.add(tf.keras.layers.Dense(1))
callbacks=[#TerminateOnNaN(),EarlyStopping(monitor='val_loss'),
          ReduceLROnPlateau(monitor='val_loss')]
optimizer=SGD(learning_rate=ExponentialDecay(initial_learning_rate=1.e-3,
                                            decay_steps=2,decay_rate=0.5))
model.compile(optimizer=optimizer, loss='mse')
x=tf.ones((32,16))
y=tf.ones((32,1))
v=tf.ones((8,1))
model.fit(x, y, batch_size=4, epochs=8, callbacks=callbacks,
         validation_split=0.2,steps_per_epoch=4,verbose=1)
&lt;/denchmark-code&gt;

yields the following error:
&lt;denchmark-code&gt;Epoch 1/8
1/4 [======&gt;.......................] - ETA: 0s - loss: 1.4363

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-138-2120310fffef&gt; in &lt;module&gt;
     19 v=tf.ones((8,1))
     20 model.fit(x, y, batch_size=4, epochs=8, callbacks=callbacks,
---&gt; 21          validation_split=0.2,steps_per_epoch=4,verbose=1)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py in _method_wrapper(self, *args, **kwargs)
     64   def _method_wrapper(self, *args, **kwargs):
     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
---&gt; 66       return method(self, *args, **kwargs)
     67 
     68     # Running inside `run_distribute_coordinator` already.

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
    874           epoch_logs.update(val_logs)
    875 
--&gt; 876         callbacks.on_epoch_end(epoch, epoch_logs)
    877         if self.stop_training:
    878           break

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\keras\callbacks.py in on_epoch_end(self, epoch, logs)
    363     logs = self._process_logs(logs)
    364     for callback in self.callbacks:
--&gt; 365       callback.on_epoch_end(epoch, logs)
    366 
    367   def on_train_batch_begin(self, batch, logs=None):

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\keras\callbacks.py in on_epoch_end(self, epoch, logs)
    892 
    893   def on_epoch_end(self, epoch, logs=None):
--&gt; 894     self._finalize_progbar(logs)
    895 
    896   def on_test_end(self, logs=None):

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\keras\callbacks.py in _finalize_progbar(self, logs)
    933       self.progbar.target = self.seen
    934     logs = logs or {}
--&gt; 935     self.progbar.update(self.seen, list(logs.items()), finalize=True)
    936 
    937 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\keras\utils\generic_utils.py in update(self, current, values, finalize)
    568         value_base = max(current - self._seen_so_far, 1)
    569         if k not in self._values:
--&gt; 570           self._values[k] = [v * value_base, value_base]
    571         else:
    572           self._values[k][0] += v * value_base

TypeError: unsupported operand type(s) for *: 'ExponentialDecay' and 'int'
&lt;/denchmark-code&gt;

Describe the expected behavior
Model should fit without any errors. Here is what happens if I don't use the callbacks:
&lt;denchmark-code&gt;Epoch 1/8
4/4 [==============================] - 0s 20ms/step - loss: 6.3209 - val_loss: 4.6678
Epoch 2/8
4/4 [==============================] - 0s 10ms/step - loss: 4.4087 - val_loss: 4.0870
Epoch 3/8
4/4 [==============================] - 0s 9ms/step - loss: 4.0231 - val_loss: 3.9547
Epoch 4/8
4/4 [==============================] - 0s 10ms/step - loss: 3.9385 - val_loss: 3.9224
Epoch 5/8
4/4 [==============================] - 0s 9ms/step - loss: 3.9185 - val_loss: 3.9143
Epoch 6/8
4/4 [==============================] - 0s 9ms/step - loss: 3.9131 - val_loss: 3.9123
Epoch 7/8
4/4 [==============================] - 0s 9ms/step - loss: 3.9121 - val_loss: 3.9118
Epoch 8/8
4/4 [==============================] - 0s 11ms/step - loss: 3.9117 - val_loss: 3.9117
&lt;/denchmark-code&gt;

Standalone code to reproduce the issue
see above.
Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='bsugerman' date='2020-07-23T11:54:59Z'>
		&lt;denchmark-link:https://github.com/bsugerman&gt;@bsugerman&lt;/denchmark-link&gt;
,
I was able to reproduce the issue with TF v2.2. However, the issue seems to be fixed in the latest TF-nightly. Please find the gist of it &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/697836288a6d02f048d11e08b7044079/41639-tf-nightly.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='bsugerman' date='2020-07-30T12:22:54Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='3' author='bsugerman' date='2020-08-06T12:23:00Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='4' author='bsugerman' date='2020-08-06T12:23:03Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41639&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41639&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>