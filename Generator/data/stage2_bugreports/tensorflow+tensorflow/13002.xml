<bug id='13002' author='kudchikarsk' open_date='2017-09-12T19:19:44Z' closed_time='2017-09-15T18:44:05Z'>
	<summary>1_notmnist assignment1 zero mean equation in code</summary>
	<description>
Okay in tutorial its say to make zero mean do r-128/128 but in code its r-128/255
Below is the code provided in ipython notebook
image_size = 28  # Pixel width and height.
pixel_depth = 255.0  # Number of levels per pixel.
def load_letter(folder, min_num_images):
"""Load the data for a single letter label."""
image_files = os.listdir(folder)
dataset = np.ndarray(shape=(len(image_files), image_size, image_size),
dtype=np.float32)
print(folder)
num_images = 0
for image in image_files:
image_file = os.path.join(folder, image)
try:
image_data = (ndimage.imread(image_file).astype(float) -
pixel_depth / 2) / pixel_depth
if image_data.shape != (image_size, image_size):
raise Exception('Unexpected image shape: %s' % str(image_data.shape))
dataset[num_images, :, :] = image_data
num_images = num_images + 1
except IOError as e:
print('Could not read:', image_file, ':', e, '- it's ok, skipping.')
	</description>
	<comments>
		<comment id='1' author='kudchikarsk' date='2017-09-14T16:44:37Z'>
		I'm sorry, @shadmanssk, but I don't understand what you're asking. Is there a bug or a feature request you'd like to make? Are you asking that we change or fix the tutorial? Or is there some aspect of the tutorial that you'd like explained? Also, which tutorial are you referring to?
		</comment>
		<comment id='2' author='kudchikarsk' date='2017-09-15T05:45:03Z'>
		​
Tutorial: Deep Learning
Lesson 1 Concept 19 and Assignment 1
Link: &lt;denchmark-link:https://classroom.udacity.com/courses/ud730/lessons/6370362152/concepts/71191606550923&gt;https://classroom.udacity.com/courses/ud730/lessons/6370362152/concepts/71191606550923&lt;/denchmark-link&gt;


Assignment :
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/1_notmnist.ipynb&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/1_notmnist.ipynb&lt;/denchmark-link&gt;


Issue:
Above equation of normalizing pixel inputs are not followed in code

Code (in above ipynb) :

image_size = 28  # Pixel width and height.pixel_depth = 255.0  #
Number of levels per pixel.


def load_letter(folder, min_num_images):
  """Load the data for a single letter label."""
  image_files = os.listdir(folder)
  dataset = np.ndarray(shape=(len(image_files), image_size, image_size),
                         dtype=np.float32)
  print(folder)
  num_images = 0
  for image in image_files:
    image_file = os.path.join(folder, image)
    try:
      image_data = (ndimage.imread(image_file).astype(float) -
                    pixel_depth / 2) / pixel_depth
      if image_data.shape != (image_size, image_size):
        raise Exception('Unexpected image shape: %s' % str(image_data.shape))
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Thu, Sep 14, 2017 at 10:17 PM, Cliff Young ***@***.***&gt; wrote:
 I'm sorry, @shadmanssk &lt;https://github.com/shadmanssk&gt;, but I don't
 understand what you're asking. Is there a bug or a feature request you'd
 like to make? Are you asking that we change or fix the tutorial? Or is
 there some aspect of the tutorial that you'd like explained? Also, which
 tutorial are you referring to?

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#13002 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AS-E5NvwVceCBUu8YfVWLQyp7VuFPhr-ks5siVidgaJpZM4PVH1k&gt;
 .



		</comment>
		<comment id='3' author='kudchikarsk' date='2017-09-15T18:44:05Z'>
		You're quite right that the Udacity slides and the code in the ipynb don't match up exactly. But I don't think it matters for educational purposes.
I think there are two points in what Vincent is saying:

Having a zero mean makes it easier for the optimizer to work.
Normalizing different axes to have the same scale (so circular, or standard deviations, rather than oval, as shown in slides) also makes it easier for the optimizer to work.

I think the code gets both right. The -128 moves the mean of the [0,255] image data close to zero.
The /255 rescales so that values will be in the range [-.5,.5]. The formulas that Vincent showed would have scaled to [-1,1]. But as long as everything rescales to the same ranges (so you get circularity across dimensions, not ovals), then the optimizer will still have an easier time. It's not the absolute magnitude of the rescaling that matters; it's the relative scales across the dimensions.
Not sure if the course has gotten to batch normalization at this point, but that strives to rescale a standard deviation to have value about 1.0. Then you don't know the absolute max/min bounds, but you do have some statistical beliefs that your data are now spread out similarly in multiple dimensions.
Does that make sense? &lt;denchmark-link:https://github.com/vincentvanhoucke&gt;@vincentvanhoucke&lt;/denchmark-link&gt;
, feel free to amend or comment, but I'll close for now.
		</comment>
		<comment id='4' author='kudchikarsk' date='2017-09-16T14:02:12Z'>
		yeah even I realised that so I totally get your point still, it was
confusing at first but thanks for clearing out this thing.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Sat, Sep 16, 2017 at 12:20 AM, Cliff Young ***@***.***&gt; wrote:
 You're quite right that the Udacity slides and the code in the ipynb don't
 match up exactly. But I don't think it matters for educational purposes.

 I think there are two points in what Vincent is saying:

    1. Having a zero mean makes it easier for the optimizer to work.
    2. Normalizing different axes to have the same scale (so circular, or
    standard deviations, rather than oval, as shown in slides) also makes it
    easier for the optimizer to work.

 I think the code gets both right. The -128 moves the mean of the [0,255]
 image data close to zero.
 The /255 rescales so that values will be in the range [-.5,.5]. The
 formulas that Vincent showed would have scaled to [-1,1]. But as long as
 everything rescales to the same ranges (so you get circularity across
 dimensions, not ovals), then the optimizer will still have an easier time.
 It's not the absolute magnitude of the rescaling that matters; it's the
 relative scales across the dimensions.

 Not sure if the course has gotten to batch normalization at this point,
 but that strives to rescale a standard deviation to have value about 1.0.
 Then you don't know the absolute max/min bounds, but you do have some
 statistical beliefs that your data are now spread out similarly in multiple
 dimensions.

 Does that make sense? @vincentvanhoucke
 &lt;https://github.com/vincentvanhoucke&gt;, feel free to amend or comment, but
 I'll close for now.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#13002 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AS-E5OT3O6vzZDOnRiMT2xyaXAlivBPiks5siscEgaJpZM4PVH1k&gt;
 .



		</comment>
	</comments>
</bug>