<bug id='30486' author='LukeBolly' open_date='2019-07-08T10:05:09Z' closed_time='2020-01-09T23:30:45Z'>
	<summary>Keras TimeDistributed on a Model creates duplicate layers, and is inconsistent with TimeDistributed on a Layer</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
TensorFlow installed from (source or binary): Binary
TensorFlow version (use command below): tensorflow-gpu 1.14.0
Python version: 3.6.7
CUDA/cuDNN version: 10
GPU model and memory: GTX1060M


Wrapping a model in a TimeDistributed layer creates duplicate nodes in the graph. If we follow the docs &lt;denchmark-link:https://keras.io/getting-started/functional-api-guide/#all-models-are-callable-just-like-layers&gt;(link)&lt;/denchmark-link&gt;
 and create a simple Dense Model wrapped in a TD Layer:
&lt;denchmark-code&gt;    inner_input = keras.layers.Input((2,))
    dense = keras.layers.Dense(2, activation='relu')(inner_input)
    inner_model = keras.Model(inputs=inner_input, outputs=dense)

    full_input = keras.layers.Input((2,2))
    td_2 = keras.layers.TimeDistributed(inner_model)(full_input)
    model = keras.models.Model(full_input, td_2)
    model.compile('SGD', 'mse')
&lt;/denchmark-code&gt;

You end up with this:
&lt;denchmark-link:https://user-images.githubusercontent.com/24449147/60799985-d4e60c00-a1a6-11e9-84e8-e0b6ddd2a789.png&gt;&lt;/denchmark-link&gt;

Firstly, if you follow the documentation approach you end up with an additional Dense Layer (Bottom Left). This eats up memory, and happens because you build the inner model then rebuild it again when you build the TimeDistributed model. This can be avoided by parameterizing your model &lt;denchmark-link:https://www.tensorflow.org/beta/guide/keras/custom_layers_and_models#building_models&gt;(link)&lt;/denchmark-link&gt;
, but it can be a very painful workaround if your model is complex. But for demonstrations sake, here's the model as an object:
&lt;denchmark-code&gt;    class InnerModel(keras.Model):
        def __init__(self):
            super(InnerModel, self).__init__()

            self.dense = keras.layers.Dense(2, activation='relu')

        def call(self, inputs, training=None, mask=None):
            out = self.dense(inputs)
            return out

        def compute_output_shape(self, input_shape):
            return (input_shape[0], 2)

    input = keras.layers.Input((2,2))
    td_model = InnerModel()
    time_dist = keras.layers.TimeDistributed(td_model)(input)
    model = keras.models.Model(input, time_dist)
    model.compile('SGD', 'mse')
&lt;/denchmark-code&gt;

Here's the improved graph:
&lt;denchmark-link:https://user-images.githubusercontent.com/24449147/60801864-58552c80-a1aa-11e9-9550-24068fffc43c.png&gt;&lt;/denchmark-link&gt;

So the additional Dense layer is gone, but there are still two dense layers inside. They have different contents too.
&lt;denchmark-link:https://user-images.githubusercontent.com/24449147/60801763-25ab3400-a1aa-11e9-9721-00482828ed58.png&gt;&lt;/denchmark-link&gt;

Describe the expected behavior
If you compare to what you get if you just wrap the Dense layer itself:
&lt;denchmark-code&gt;    full_input = keras.layers.Input((2,2,2))
    td_2 = keras.layers.TimeDistributed(keras.layers.Dense(2, activation='relu'))(full_input)
    model = keras.models.Model(full_input, td_2)
    model.compile('SGD', 'mse')
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/24449147/60800184-5c337f80-a1a7-11e9-8dd3-901054a537d9.png&gt;&lt;/denchmark-link&gt;

I'd expect wrapping a model should result in a very similar looking graph to wrapping a layer

Full code for creating the graphs:
&lt;denchmark-link:https://gist.github.com/LukeBolly/0efeca3db275dee97c5f0fbf1f400b5a&gt;https://gist.github.com/LukeBolly/0efeca3db275dee97c5f0fbf1f400b5a&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='LukeBolly' date='2019-07-09T04:33:09Z'>
		If you define your custom model as a Layer instead of a Model, it produces the expected graph without the additional nodes
		</comment>
		<comment id='2' author='LukeBolly' date='2019-08-12T03:59:14Z'>
		&lt;denchmark-link:https://github.com/LukeBolly&gt;@LukeBolly&lt;/denchmark-link&gt;
 this seems to be fixed in the latest nightly, could you please try updating? Otherwise it may be an issue with TensorBoard, but the layers in the outer Model look good to me as far as training in Keras
		</comment>
		<comment id='3' author='LukeBolly' date='2020-01-09T23:30:45Z'>
		This is fixed with tf-nightly '2.1.0-dev20200109' version. Thanks!
		</comment>
		<comment id='4' author='LukeBolly' date='2020-01-09T23:30:47Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30486&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30486&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>