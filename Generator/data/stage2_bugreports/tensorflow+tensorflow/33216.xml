<bug id='33216' author='angeliand' open_date='2019-10-10T16:55:09Z' closed_time='2019-11-01T19:07:37Z'>
	<summary>[TF 2.0.0] Training keras Model on tf.data.Dataset causes small bug in logging</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.0.0
Python version: 3.6.x
CUDA/cuDNN version: 10.0/7.6.1
GPU model and memory:

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Describe the current behavior
When fitting (.fit) a keras Model on a tf.data.Dataset, the dataset size is not inferred. Because of this, when setting verbose=1, during the first epoch the log becomes current_step/Unknown. Also the following is thrown (though it does not cause crashing):
&lt;denchmark-code&gt;[[{{node IteratorGetNext}}]]
	 [[IteratorGetNext/_2]]
2019-10-10 18:41:50.728985: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext}}]]
&lt;/denchmark-code&gt;

Describe the expected behavior
I would expect to see the number of samples/batches/etc.
Code to reproduce the issue
I created a small Colab notebook to demonstrate the issue: &lt;denchmark-link:https://colab.research.google.com/drive/1-S787cE6BWhXJ_0BeAb6EGq4GaXAFwmu&gt;https://colab.research.google.com/drive/1-S787cE6BWhXJ_0BeAb6EGq4GaXAFwmu&lt;/denchmark-link&gt;

I recommend downloading the .py file and running it in command line (so colab logging doesn't interfere), because after the epoch is done, the correct batch number is found. The problem is during the epoch.
Other info / logs
I found that the dataset size inferring is actually run, but the returned value is not stored or used anywhere, it is only to throw a warning if the initialization made by the user is faulty in some way.
I am referring to this line: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/engine/training_v2.py#L247&gt;https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/engine/training_v2.py#L247&lt;/denchmark-link&gt;

The above problem would be eliminated with something like this (or the like):
&lt;denchmark-code&gt;steps_per_epoch = training_utils.infer_steps_for_dataset(training_dataset, steps_per_epoch, 
                                                                                 steps_name='steps_per_epoch',epochs=0) 
                                                                            if steps_per_epoch is None else steps_per_epoch
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='angeliand' date='2019-10-11T06:20:04Z'>
		&lt;denchmark-link:https://github.com/angeliand&gt;@angeliand&lt;/denchmark-link&gt;

I am able to successfully execute Colab notebook provided by you with a warning message.Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/d23afdb38149eeb3fc179a6e30d20721/untitled254.ipynb&gt;here&lt;/denchmark-link&gt;
.Please, let me know is this still an issue?.Thanks!
		</comment>
		<comment id='2' author='angeliand' date='2019-10-11T07:09:40Z'>
		As I have said, the problem is with what gets logged during the epoch. Not after. In your notebook you did not interrupt the training to view the printed log during the epoch. If interrupted the described behaviour can still be seen.
Also, I recommend running the script in command line (or anywhere, where the output is printed line by line), where the problem can be viewed without stopping the training.
		</comment>
		<comment id='3' author='angeliand' date='2019-10-11T09:52:22Z'>
		&lt;denchmark-link:https://github.com/angeliand&gt;@angeliand&lt;/denchmark-link&gt;

I tried running in command line and i am able to execute the .py file successfully. Please find the log file in the attachment.Is this the expected output?
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3716797/text.txt.tar.gz&gt;text.txt.tar.gz&lt;/denchmark-link&gt;
.I could reproduce the issue in colab when i interrupted during training.Thanks!
		</comment>
		<comment id='4' author='angeliand' date='2019-10-11T10:17:29Z'>
		As I have stated in the original post, the bug does not cause crashing, but it is still inconvenient and can be avoided easily with a minor fix (I have also offered a solution).
If logging is line by line (so when it doesn’t refresh) eg. in PyCharm or when logging to file, the problem can be viewed easier. It appears during the first epoch and ceases when that is done. (Probably because after the first epoch, we know the number of steps.) This is why training has to be interrupted in colab to view the issue (or you can check the runtime logs in colab!).
Also the IteratorGetNext warning appears at the end of the first epoch before validating steps. This is because the training loop does not know the number of steps to take (—&gt; times to “query” the dataset) and the dataset runs out.
All in all, this is not huge but still not what would be the expected behaviour.
I will provide a log file from PyCharm in a few hours.
		</comment>
		<comment id='5' author='angeliand' date='2019-10-11T12:48:23Z'>
		PyCharm output is line-by-line, so the problem can be seen better. Here is the output without the proposed solution:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3717600/keras_bug_op.txt&gt;keras_bug_op.txt&lt;/denchmark-link&gt;

And here is the correct (expected) output after using the proposed solution:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3717605/keras_bug_sol.txt&gt;keras_bug_sol.txt&lt;/denchmark-link&gt;

Hope this helps.
		</comment>
		<comment id='6' author='angeliand' date='2019-10-14T16:58:49Z'>
		&lt;denchmark-link:https://github.com/angeliand&gt;@angeliand&lt;/denchmark-link&gt;
 Are you willing to contribute through PR to update relevant codes? Thanks!
		</comment>
		<comment id='7' author='angeliand' date='2019-10-16T07:51:47Z'>
		Sure. I'm busy in the next few days, but I will do it after that.
		</comment>
		<comment id='8' author='angeliand' date='2019-10-31T12:34:35Z'>
		It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue?
		</comment>
		<comment id='9' author='angeliand' date='2019-11-01T19:07:38Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33216&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33216&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>