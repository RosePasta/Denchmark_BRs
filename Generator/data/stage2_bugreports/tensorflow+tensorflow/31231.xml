<bug id='31231' author='etsygankov' open_date='2019-08-01T10:13:02Z' closed_time='2019-08-15T18:37:40Z'>
	<summary>DistributionStrategy is not supported by tf.keras.models.Model.fit_generator</summary>
	<description>
Hi! Recently I've encountered a  while trying to apply a fit_generator method of a  with a . It is almost a year since this handlers were added to the code ( &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/9541ce3475ea70fd8eb9552f60de462127f15440#diff-de9b96ac2d81503324cbbbe21732031f&gt;9541ce3#diff-de9b96ac2d81503324cbbbe21732031f&lt;/denchmark-link&gt;
 ) and I'm wondering whether to expect an implementation to be added any time soon? (with the release of TF2.0 for example)
Making efforts to find a workaround I've tried to transform a generator to TF Dataset by tf.data.Dataset.from_generator to replace the fit_generator by fit but encountered similar problem. The obtained object has type DatasetV1Adapterwhich is also incompatible with distribution strategies
I dare to assume that for a wide society of TF users and for me in particular this functionality would be of a great interest. Dealing with large, domain specific data sets that doesn't fit into memory, one  often has no choice other than writing a custom data generator. When big data is involved the distributed training might be crucial.
I would highly appreciate any information on the current state of the problem or possible workarounds from the TensorFlow developers team. Thanks in advance!
System information

TensorFlow version (you are using): 2.0.0.dev20190729
Are you willing to contribute it (Yes/No): No

	</description>
	<comments>
		<comment id='1' author='etsygankov' date='2019-08-02T09:20:08Z'>
		&lt;denchmark-link:https://github.com/etsygankov&gt;@etsygankov&lt;/denchmark-link&gt;
 Will it be possible to provide the minimal code snippet which replicates the reported issue. Thanks!
		</comment>
		<comment id='2' author='etsygankov' date='2019-08-05T14:54:57Z'>
		Getting NotImplementedError is rather trivial
&lt;denchmark-code&gt;import numpy as np
import tensorflow as tf
# strategy = tf.distribute.MirroredStrategy()
strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()


def generator(batch_size=100):
    while True:
        data = np.random.random((100, 1))
        yield {'inp': data}, 3 * data


with strategy.scope():
    model = tf.keras.models.Sequential()
    model.add(tf.keras.layers.Dense(1))
    model.compile('Adam', 'mae')
    model.fit_generator(generator(), steps_per_epoch=1000, epochs=10)

Traceback (most recent call last):
  File "&lt;input&gt;", line 1, in &lt;module&gt;
  File "/snap/pycharm-professional/147/helpers/pydev/_pydev_bundle/pydev_umd.py", line 197, in runfile
    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script
  File "/snap/pycharm-professional/147/helpers/pydev/_pydev_imps/_pydev_execfile.py", line 18, in execfile
    exec(compile(contents+"\n", file, 'exec'), glob, loc)
  File "/home/tsygankov/Development/next/bin/tmp1.py", line 17, in &lt;module&gt;
    model.fit_generator(generator(), steps_per_epoch=1000, epochs=10)
  File "/home/tsygankov/Development/next/.vtf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py", line 1277, in fit_generator
    raise NotImplementedError('`fit_generator` is not supported for '
NotImplementedError: `fit_generator` is not supported for models compiled with tf.distribute.Strategy.
&lt;/denchmark-code&gt;

Experimenting further with TF Data API and a fit method I found the bug in my own code, so finally  tf.data.Dataset.from_generator works as expected, at least with MirroredStrategy.
		</comment>
		<comment id='3' author='etsygankov' date='2019-08-06T05:22:34Z'>
		&lt;denchmark-link:https://github.com/etsygankov&gt;@etsygankov&lt;/denchmark-link&gt;
 Thanks for sharing the code.
I could able to reproduce the issue on Colab with Tensorflow 2.0.0.dev20190729. Please take a look at gist &lt;denchmark-link:https://colab.research.google.com/drive/1V6n4HIVDPJNZNQjmthrk6s1Xm2R0PS-0&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='4' author='etsygankov' date='2019-08-12T19:57:03Z'>
		Model.fit() was recently made work with generators and distribution strategies.  Could you try the latest version of TF2?  Does it provide a work-around for you?
This works:
import numpy as np
import tensorflow as tf
#strategy = tf.distribute.MirroredStrategy()
strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()


def generator():
      while True:
        yield np.ones([10, 10], np.float32), np.ones([10, 1], np.float32)


with strategy.scope():
  model = tf.keras.models.Sequential()
  model.add(tf.keras.layers.Dense(1, input_shape=(10,), activation="relu"))
  model.compile('Adam', 'mae')
  model.fit(generator(), steps_per_epoch=1000, epochs=10)
		</comment>
		<comment id='5' author='etsygankov' date='2019-08-15T18:37:40Z'>
		Please reopen if it doesn't provide a workaround.
		</comment>
		<comment id='6' author='etsygankov' date='2019-08-15T18:37:42Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=31231&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=31231&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='etsygankov' date='2019-09-10T13:17:48Z'>
		Hi,
I have found that the workaround those not work if the model has multiple inputs. The following code fails:
import numpy as np
import tensorflow as tf
#strategy = tf.distribute.MirroredStrategy()
strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()


def generator():
    while True:
        yield [np.ones([10, 10], np.float32), np.ones([10, 10], np.float32)], np.ones([10, 1], np.float32)


with strategy.scope():
    inputA = tf.keras.layers.Input(shape=(10,))
    inputB = tf.keras.layers.Input(shape=(10,))

    output = tf.keras.layers.Concatenate()([inputA, inputB])
    output = tf.keras.layers.Dense(1, input_shape=(10,), activation="relu")(output)
    model = tf.keras.models.Model(inputs=[inputA, inputB], outputs=output)
    model.compile('Adam', 'mae')
    model.fit(generator(), steps_per_epoch=1000, epochs=10)
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "scripts/distributed_strategy_test/fit_test_multiple_inputs.py", line 20, in &lt;module&gt;
    model.fit(generator(), steps_per_epoch=1000, epochs=10)
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py", line 734, in fit
    use_multiprocessing=use_multiprocessing)
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 224, in fit
    distribution_strategy=strategy)
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 547, in _process_training_inputs
    use_multiprocessing=use_multiprocessing)
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 605, in _process_inputs
    use_multiprocessing=use_multiprocessing)
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/data_adapter.py", line 550, in __init__
    reassemble, nested_dtypes, output_shapes=nested_shape)
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py", line 540, in from_generator
    output_types, tensor_shape.as_shape, output_shapes)
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/data/util/nest.py", line 471, in map_structure_up_to
    results = [func(*tensors) for tensors in zip(*all_flattened_up_to)]
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/data/util/nest.py", line 471, in &lt;listcomp&gt;
    results = [func(*tensors) for tensors in zip(*all_flattened_up_to)]
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py", line 1216, in as_shape
    return TensorShape(shape)
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py", line 776, in __init__
    self._dims = [as_dimension(d) for d in dims_iter]
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py", line 776, in &lt;listcomp&gt;
    self._dims = [as_dimension(d) for d in dims_iter]
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py", line 718, in as_dimension
    return Dimension(value)
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py", line 193, in __init__
    self._value = int(value)
TypeError: int() argument must be a string, a bytes-like object or a number, not 'tuple'
&lt;/denchmark-code&gt;

Should I open a new thread for this issue?
		</comment>
		<comment id='8' author='etsygankov' date='2019-09-14T04:03:34Z'>
		&lt;denchmark-link:https://github.com/etsygankov&gt;@etsygankov&lt;/denchmark-link&gt;
 hi I meet the same problem with 1.14 tf version, can u share ur solution with tf.data.Dataset.from_generator?
		</comment>
		<comment id='9' author='etsygankov' date='2019-09-19T10:49:33Z'>
		I have the same error:
TypeError: int() argument must be a string, a bytes-like object or a number, not 'tuple'
with tensorflow 2.0.0-rc0 and a generator based on keras.utils.Sequence
		</comment>
		<comment id='10' author='etsygankov' date='2019-09-23T12:53:13Z'>
		&lt;denchmark-link:https://github.com/AHEADer&gt;@AHEADer&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/kielnino&gt;@kielnino&lt;/denchmark-link&gt;
 According to my experience the generator must return a tuple of length two while objects inside it may be python dicts. Say, your model has two inputs  'x1', 'x2' and a single output. Respective generator may be defined as follows:
&lt;denchmark-code&gt;def generator():
    while True:
        data = np.random.random((100, 1))
        yield {'x1': data, 'x2': 2 * data}, 3 * data

gn = tf.data.Dataset.from_generator(
    generator, ({'x1': tf.float32, 'x2': tf.float32}, tf.float32),
    ({'x1': tf.TensorShape([100, 1]), 'x2': tf.TensorShape([100, 1])}, tf.TensorShape([100, 1]))),

&lt;/denchmark-code&gt;

Please note that shape and type of the data produced by generator must be specified while calling from_generator though the shape might be not fully defined
		</comment>
		<comment id='11' author='etsygankov' date='2019-09-29T16:09:27Z'>
		I tried every workaround proposed here with Colab using TF 1.14 and it didn't work, even though it works on a GPU without any issue, using both tf.keras.utils.Sequence or a python generator.
Is there any workaround for people using a tf.keras with a generator, TPU and TF 1.14?
		</comment>
		<comment id='12' author='etsygankov' date='2019-10-09T14:39:03Z'>
		Hi isaprykin, thanks again for this clean workaround using tf.distribute.experimental.MultiWorkerMirroredStrategy, but it does not perfectly replace tf.distribute.MirroredStrategy since we cannot set a devices list as param.
		</comment>
		<comment id='13' author='etsygankov' date='2019-10-18T13:01:09Z'>
		I have the same error,

NotImplementedError: fit_generator is not supported for models compiled with tf.distribute.Strategy.

with TPUStrategy.
I am trying to run a keras model on TPU with significant CPU preprocessing of data that I want to be done in parallel with running batches on TPU.
The strategy, TPUStrategy is a simple one that distribute everything to one TPU core.
There is no reason why it cannot run preprocessing on CPU in parallel.
I shall have to switch to fit from fit_generator and run everything sequentially.
I suggest to reopen this issue.
		</comment>
		<comment id='14' author='etsygankov' date='2019-10-31T19:12:43Z'>
		any optimal solution for this issue?
		</comment>
		<comment id='15' author='etsygankov' date='2019-11-29T09:26:04Z'>
		
Hi,
I have found that the workaround those not work if the model has multiple inputs. The following code fails:
import numpy as np
import tensorflow as tf
#strategy = tf.distribute.MirroredStrategy()
strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()


def generator():
    while True:
        yield [np.ones([10, 10], np.float32), np.ones([10, 10], np.float32)], np.ones([10, 1], np.float32)


with strategy.scope():
    inputA = tf.keras.layers.Input(shape=(10,))
    inputB = tf.keras.layers.Input(shape=(10,))

    output = tf.keras.layers.Concatenate()([inputA, inputB])
    output = tf.keras.layers.Dense(1, input_shape=(10,), activation="relu")(output)
    model = tf.keras.models.Model(inputs=[inputA, inputB], outputs=output)
    model.compile('Adam', 'mae')
    model.fit(generator(), steps_per_epoch=1000, epochs=10)
Traceback (most recent call last):
  File "scripts/distributed_strategy_test/fit_test_multiple_inputs.py", line 20, in &lt;module&gt;
    model.fit(generator(), steps_per_epoch=1000, epochs=10)
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py", line 734, in fit
    use_multiprocessing=use_multiprocessing)
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 224, in fit
    distribution_strategy=strategy)
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 547, in _process_training_inputs
    use_multiprocessing=use_multiprocessing)
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 605, in _process_inputs
    use_multiprocessing=use_multiprocessing)
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/data_adapter.py", line 550, in __init__
    reassemble, nested_dtypes, output_shapes=nested_shape)
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py", line 540, in from_generator
    output_types, tensor_shape.as_shape, output_shapes)
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/data/util/nest.py", line 471, in map_structure_up_to
    results = [func(*tensors) for tensors in zip(*all_flattened_up_to)]
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/data/util/nest.py", line 471, in &lt;listcomp&gt;
    results = [func(*tensors) for tensors in zip(*all_flattened_up_to)]
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py", line 1216, in as_shape
    return TensorShape(shape)
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py", line 776, in __init__
    self._dims = [as_dimension(d) for d in dims_iter]
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py", line 776, in &lt;listcomp&gt;
    self._dims = [as_dimension(d) for d in dims_iter]
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py", line 718, in as_dimension
    return Dimension(value)
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py", line 193, in __init__
    self._value = int(value)
TypeError: int() argument must be a string, a bytes-like object or a number, not 'tuple'

Should I open a new thread for this issue?

did you solve this?
		</comment>
		<comment id='16' author='etsygankov' date='2019-12-01T11:11:55Z'>
		Any update to this? Particularly curious about tf.keras.utils.Sequence
		</comment>
		<comment id='17' author='etsygankov' date='2019-12-08T10:28:29Z'>
		

Hi,
I have found that the workaround those not work if the model has multiple inputs. The following code fails:
import numpy as np
import tensorflow as tf
#strategy = tf.distribute.MirroredStrategy()
strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()


def generator():
    while True:
        yield [np.ones([10, 10], np.float32), np.ones([10, 10], np.float32)], np.ones([10, 1], np.float32)


with strategy.scope():
    inputA = tf.keras.layers.Input(shape=(10,))
    inputB = tf.keras.layers.Input(shape=(10,))

    output = tf.keras.layers.Concatenate()([inputA, inputB])
    output = tf.keras.layers.Dense(1, input_shape=(10,), activation="relu")(output)
    model = tf.keras.models.Model(inputs=[inputA, inputB], outputs=output)
    model.compile('Adam', 'mae')
    model.fit(generator(), steps_per_epoch=1000, epochs=10)
Traceback (most recent call last):
  File "scripts/distributed_strategy_test/fit_test_multiple_inputs.py", line 20, in &lt;module&gt;
    model.fit(generator(), steps_per_epoch=1000, epochs=10)
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py", line 734, in fit
    use_multiprocessing=use_multiprocessing)
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 224, in fit
    distribution_strategy=strategy)
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 547, in _process_training_inputs
    use_multiprocessing=use_multiprocessing)
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 605, in _process_inputs
    use_multiprocessing=use_multiprocessing)
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/data_adapter.py", line 550, in __init__
    reassemble, nested_dtypes, output_shapes=nested_shape)
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py", line 540, in from_generator
    output_types, tensor_shape.as_shape, output_shapes)
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/data/util/nest.py", line 471, in map_structure_up_to
    results = [func(*tensors) for tensors in zip(*all_flattened_up_to)]
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/data/util/nest.py", line 471, in &lt;listcomp&gt;
    results = [func(*tensors) for tensors in zip(*all_flattened_up_to)]
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py", line 1216, in as_shape
    return TensorShape(shape)
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py", line 776, in __init__
    self._dims = [as_dimension(d) for d in dims_iter]
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py", line 776, in &lt;listcomp&gt;
    self._dims = [as_dimension(d) for d in dims_iter]
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py", line 718, in as_dimension
    return Dimension(value)
  File "/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py", line 193, in __init__
    self._value = int(value)
TypeError: int() argument must be a string, a bytes-like object or a number, not 'tuple'

Should I open a new thread for this issue?

did you solve this?

Try returning tuples instead of lists.
&lt;denchmark-code&gt;import numpy as np
import tensorflow as tf
#strategy = tf.distribute.MirroredStrategy()
strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()


def generator():
    while True:
        yield (np.ones([10, 10], np.float32), np.ones([10, 10], np.float32)), np.ones([10, 1], np.float32)


with strategy.scope():
    inputA = tf.keras.layers.Input(shape=(10,))
    inputB = tf.keras.layers.Input(shape=(10,))

    output = tf.keras.layers.Concatenate()([inputA, inputB])
    output = tf.keras.layers.Dense(1, input_shape=(10,), activation="relu")(output)
    model = tf.keras.models.Model(inputs=[inputA, inputB], outputs=output)
    model.compile('Adam', 'mae')
    model.fit(generator(), steps_per_epoch=1000, epochs=10)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='18' author='etsygankov' date='2020-01-28T14:55:45Z'>
		Had the same problem. Then I referred to the &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit&gt;tf2.0.0 documentation for fit()&lt;/denchmark-link&gt;
, where it says:

x: Input data. It could be:

generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample weights).  more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below.
y: Target data. Like the input data x, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator, or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from x).


This leads me to the following solution:
&lt;denchmark-code&gt;# define your DataGenerator() like
class DataGenerator(tf.keras.utils.Sequence):
    ...
    return X, tf.keras.utils.to_categorical(y, num_classes=self.n_classes)


# when fitting the model
model.fit(x = training_generator, 
    validation_data = validation_generator, 
    use_multiprocessing = True, 
    workers = num_cores,
    epochs= epochs, 
    verbose = 2)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='19' author='etsygankov' date='2020-02-15T18:47:48Z'>
		This issue has been resolved with TF v2.1.0 by replacing model.fit_generator() with model.fit()
		</comment>
		<comment id='20' author='etsygankov' date='2020-04-30T19:20:35Z'>
		Neither the model.fit() or model.fit_generator() or disabling the eager computation tf.compat.v1.disable_eager_execution() have solved the problem for me.
Here is my code:
strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()
with strategy.scope():
parallel_model = Sequential([
Dense(n, input_dim=n, kernel_initializer='normal', activation='relu'),
Dense(17, kernel_initializer='normal', activation='relu'),
Dense(4, kernel_initializer='normal', activation='relu'),
Dense(1, kernel_initializer='normal')])
parallel_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mae', 'mse'])
parallel_model.fit(X, y, epochs=20, batch_size=128)
and here is the error I am getting:
&lt;denchmark-h:h2&gt;INFO:tensorflow:Using MirroredStrategy with devices ('/device:GPU:0', '/device:GPU:1', '/device:GPU:2', '/device:GPU:3')
INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:GPU:0', '/device:GPU:1', '/device:GPU:2', '/device:GPU:3'), communication = CollectiveCommunication.AUTO
Model: "sequential_3"&lt;/denchmark-h&gt;

NotImplementedError                       Traceback (most recent call last)
 in ()
11
12 parallel_model.summary()
---&gt; 13 parallel_model.fit(X, y, epochs=20, batch_size=128)
~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
1211         else:
1212             fit_inputs = x + y + sample_weights
-&gt; 1213         self._make_train_function()
1214         fit_function = self.train_function
1215
~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/engine/training.py in _make_train_function(self)
314                     training_updates = self.optimizer.get_updates(
315                         params=self._collected_trainable_weights,
--&gt; 316                         loss=self.total_loss)
317                 updates = self.updates + training_updates
318
~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/legacy/interfaces.py in wrapper(*args, **kwargs)
89                 warnings.warn('Update your ' + object_name + ' call to the ' +
90                               'Keras 2 API: ' + signature, stacklevel=2)
---&gt; 91             return func(*args, **kwargs)
92         wrapper._original_function = func
93         return wrapper
~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in symbolic_fn_wrapper(*args, **kwargs)
73         if _SYMBOLIC_SCOPE.value:
74             with get_graph().as_default():
---&gt; 75                 return func(*args, **kwargs)
76         else:
77             return func(*args, **kwargs)
~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/optimizers.py in get_updates(self, loss, params)
548
549             # Apply constraints.
--&gt; 550             if getattr(p, 'constraint', None) is not None:
551                 new_p = p.constraint(new_p)
552
~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py in constraint(self)
569       Can be None if no constraint was passed.
570     """
--&gt; 571     raise NotImplementedError
572
573   def assign(self, value, use_locking=False, name=None, read_value=True):
NotImplementedError:
When I try your generator example given above:
import numpy as np
import tensorflow as tf
strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()
def generator():
while True:
yield (np.ones([10, 10], np.float32), np.ones([10, 10], np.float32)), np.ones([10, 1], np.float32)
with strategy.scope():
inputA = tf.keras.layers.Input(shape=(10,))
inputB = tf.keras.layers.Input(shape=(10,))
&lt;denchmark-code&gt;output = tf.keras.layers.Concatenate()([inputA, inputB])
output = tf.keras.layers.Dense(1, input_shape=(10,), activation="relu")(output)
model = tf.keras.models.Model(inputs=[inputA, inputB], outputs=output)
model.compile('Adam', 'mae')
model.fit(generator(), steps_per_epoch=1000, epochs=10)
&lt;/denchmark-code&gt;

I am getting this Assertion error.
INFO:tensorflow:Using MirroredStrategy with devices ('/device:GPU:0', '/device:GPU:1', '/device:GPU:2', '/device:GPU:3')
INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:GPU:0', '/device:GPU:1', '/device:GPU:2', '/device:GPU:3'), communication = CollectiveCommunication.AUTO
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

AssertionError                            Traceback (most recent call last)
 in ()
18     model = tf.keras.models.Model(inputs=[inputA, inputB], outputs=output)
19     model.compile('Adam', 'mae')
---&gt; 20     model.fit(generator(), steps_per_epoch=1000, epochs=10)
~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
817         max_queue_size=max_queue_size,
818         workers=workers,
--&gt; 819         use_multiprocessing=use_multiprocessing)
820
821   def evaluate(self,
~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_distributed.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
617         validation_split=validation_split,
618         shuffle=shuffle,
--&gt; 619         epochs=epochs)
620     if not dist_utils.is_distributing_by_cloning(model):
621       with model._distribution_strategy.scope():
~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py in _distribution_standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, validation_split, shuffle, epochs, allow_partial_batch)
2240         x = ds.batch(batch_size, drop_remainder=drop_remainder)
2241       else:
-&gt; 2242         assert isinstance(x, dataset_ops.DatasetV2)
2243         training_utils.validate_dataset_input(x, y, sample_weight,
2244                                               validation_split)
AssertionError:
Can we please reopen this issue, I do not think it is solved, especially for Keras Sequential Model users?
		</comment>
		<comment id='21' author='etsygankov' date='2020-04-30T19:28:34Z'>
		
Model.fit() was recently made work with generators and distribution strategies. Could you try the latest version of TF2? Does it provide a work-around for you?
This works:
import numpy as np
import tensorflow as tf
#strategy = tf.distribute.MirroredStrategy()
strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()


def generator():
      while True:
        yield np.ones([10, 10], np.float32), np.ones([10, 1], np.float32)


with strategy.scope():
  model = tf.keras.models.Sequential()
  model.add(tf.keras.layers.Dense(1, input_shape=(10,), activation="relu"))
  model.compile('Adam', 'mae')
  model.fit(generator(), steps_per_epoch=1000, epochs=10)

It does not, can we please reopen this issue?
		</comment>
		<comment id='22' author='etsygankov' date='2020-05-01T16:07:14Z'>
		I have just run a colab notebook and it still works.
&lt;denchmark-link:https://colab.research.google.com/drive/1lmr3vS6q8OkAtpgogIGBvmBK-VV53GAY&gt;https://colab.research.google.com/drive/1lmr3vS6q8OkAtpgogIGBvmBK-VV53GAY&lt;/denchmark-link&gt;

		</comment>
		<comment id='23' author='etsygankov' date='2020-06-14T10:21:31Z'>
		&lt;denchmark-link:https://github.com/etsygankov&gt;@etsygankov&lt;/denchmark-link&gt;


According to my experience the generator must return a tuple of length two while objects inside it may be python dicts. Say, your model has two inputs 'x1', 'x2' and a single output.......

Thank you for this comment. It helped me very much when I had a similar issue.
		</comment>
	</comments>
</bug>