<bug id='37587' author='eduardofv' open_date='2020-03-14T01:58:57Z' closed_time='2020-03-17T01:13:11Z'>
	<summary>EarlyStopping Callback not working with Multiple Callbacks</summary>
	<description>
System information

Have I written custom code
OS Platform and Distribution: Google Colab
TensorFlow version: 2.1.0
Keras: 2.2.4-tf
Python version: 3
CUDA/cuDNN version: No GPU but happens also with it

Describe the current behavior
When a CustomCallback  uses model.predict or model.evaluate as part of on_epoch_end,  to for example, track a custom evaluation metric on an test dataset, model.stop_training is reset to False even if it was previously set to True by Early Stopping. Training does not stop. Current workaround is putting EarlyStopping as the last callback of the list.
Describe the expected behavior
Regardless the order in which callbacks are called, if one of the sets the model to stop training it should stop even if a later callback resets the stop flag.

Check this colab with the full example: &lt;denchmark-link:https://colab.research.google.com/drive/1lw943Ggwkp_wvGxVX-5XqaEJ5qXmrHlA&gt;https://colab.research.google.com/drive/1lw943Ggwkp_wvGxVX-5XqaEJ5qXmrHlA&lt;/denchmark-link&gt;

In short:
&lt;denchmark-code&gt;class MyCallback(keras.callbacks.Callback):
    def __init__(self, test_data):
        super(MyCallback, self).__init__()
        self.test_data = test_data

    def on_epoch_end(self, epoch, logs=None):
        print(f"\n--------- pre-predict stop_training={self.model.stop_training}\n")
        #The problem is in the prediction: if commented ES works fine
        predictions = self.model.predict(self.test_data.batch(512))
        print(f"\n--------- post-predict stop_training={self.model.stop_training}\n")
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;es = keras.callbacks.EarlyStopping(patience=2)
myc = MyCallback(test_data)

#This causes EarlyStop not to stop
my_callbacks = [es, myc]
#Either of these works fine
#my_callbacks = [myc, es]
#my_callbacks = [es]
...
model.fit(train_data.batch(512),
          validation_data=validation_data.batch(512),
          epochs=100,
          callbacks=my_callbacks,
          verbose=1)
&lt;/denchmark-code&gt;

This issue is refered also in the keras repository: &lt;denchmark-link:https://github.com/keras-team/keras/issues/13381&gt;keras-team/keras#13381&lt;/denchmark-link&gt;

Some thoughts: I'm not sure if the correct solution would be to discourage the use of predict or evaluate inside a training loop since there may be some other side effects of running one of those to the model.
Anyway, I'm opening the issue and submitting a pull request to fix this.
	</description>
	<comments>
		<comment id='1' author='eduardofv' date='2020-03-16T07:30:41Z'>
		I am able to reproduce the issue with Tf 2.1.
Please find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/756228eeeca347ffa8f4a40cd662d7f1/untitled457.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='eduardofv' date='2020-03-17T01:13:11Z'>
		As suggested on the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/37588&gt;pull request&lt;/denchmark-link&gt;
 it's now fixed on v2.2rc0 and current master. Closing the issue. Thanks!
		</comment>
		<comment id='3' author='eduardofv' date='2020-03-17T01:13:12Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37587&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37587&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>