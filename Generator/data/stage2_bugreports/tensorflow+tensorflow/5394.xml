<bug id='5394' author='passerbyd' open_date='2016-11-04T04:29:08Z' closed_time='2016-11-04T17:21:17Z'>
	<summary>Distributed training hangs</summary>
	<description>
As mentioned in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/4651&gt;#4651&lt;/denchmark-link&gt;
, during training with syncreplicasoptimizer, tensorflow must run into hanging(after thousands steps). This does not happen with simple network setup.
Both workers and pses are still "alive", no errors/exceptions are reported. The saver thread keeps working.
Environment info
the cluster is set on 2 servers, one ps and one worker on each. And each worker works on 4 gpu cards.
Operating System:
Centos (customized)
Installed version of CUDA and cuDNN:
CUDA 7.5
cnDNN 5.1
installed from source,
build on branch r0.11, and r0.10
trace log after hanging
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/570595/ps0_pstack.txt&gt;ps0_pstack.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/570597/ps0_strace.txt&gt;ps0_strace.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/570598/ps1_pstack.txt&gt;ps1_pstack.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/570602/ps1_strace.txt&gt;ps1_strace.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/570601/worker0_pstack.txt&gt;worker0_pstack.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/570600/worker0_strace.txt&gt;worker0_strace.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/570596/worker1_pstack.txt&gt;worker1_pstack.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/570599/worker1_strace.txt&gt;worker1_strace.txt&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='passerbyd' date='2016-11-04T04:34:51Z'>
		If you need the setup to reproduce this issue, please message me your email address.
		</comment>
		<comment id='2' author='passerbyd' date='2016-11-04T16:58:44Z'>
		I'm sorry, but I'm closing this for now for these reasons.

Providing reproduction instructions was why #4651 was closed, and you still have not provided them. This type of problem cannot be solved by simple inspection of the stack trace (@mrry, you don't have any magical solution do you).
Providing them for email removes visibility to your issue which is at odds with the idea of community support.
CentOS is an unsupported platform. @jart's example of someone getting help was on an Ubuntu platform.

		</comment>
		<comment id='3' author='passerbyd' date='2016-11-04T17:20:12Z'>
		See &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/5394&gt;#5394&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='passerbyd' date='2016-11-05T00:09:55Z'>
		From &lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;
: Maybe the queue runner on the chief wasn't started, which is required for synchronous updates. No way to verify without the code.
		</comment>
		<comment id='5' author='passerbyd' date='2017-11-14T03:29:20Z'>
		The problem does not happen any more on current version of RDMA. Seems like an issue on GRPC.
		</comment>
		<comment id='6' author='passerbyd' date='2017-11-14T06:32:03Z'>
		Maybe it's an instance of the bug fixed by &lt;denchmark-link:https://github.com/grpc/grpc/pull/12648&gt;grpc/grpc#12648&lt;/denchmark-link&gt;
? We should get an updated ref to gRPC soon, which might fix this.
		</comment>
	</comments>
</bug>