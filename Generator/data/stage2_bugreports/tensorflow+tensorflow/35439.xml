<bug id='35439' author='gsimko' open_date='2019-12-27T03:39:17Z' closed_time='2020-02-22T04:17:01Z'>
	<summary>tf.metrics.Mean* metrics miscalculated</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac os
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): pip3
TensorFlow version (use command below): 2.0.0
Python version: 3.7.4
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
tf.metrics.MeanAbsoluteError and others compute the mean of means.
Describe the expected behavior
They should compute the mean of all the data to support iterating over evaluation datasets.
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
import tensorflow as tf
m=tf.metrics.MeanAbsoluteError()
m.update_state(y_true=[0,0], y_pred=[1,1])
m.update_state(y_true=[0], y_pred=[2])
assert m.result() == 2
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
The fix is simple, instead of the mean_squared_error function use a (new) sum_squared_error function in the metric and pass that to MeanMetricWrapper.
	</description>
	<comments>
		<comment id='1' author='gsimko' date='2019-12-27T05:05:23Z'>
		The behavior seems expected according to source code since  accumulates (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/keras/metrics.py#L213&gt;[1]&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanAbsoluteError?version=stable#update_state&gt;[2]&lt;/denchmark-link&gt;
) the errors. You can verify this by checking the value of  variable after executing each  statement. I have done so by running the following
&lt;denchmark-code&gt;import tensorflow as tf
m=tf.metrics.MeanAbsoluteError()
m.update_state(y_true=[0,0], y_pred=[1,1])
print(m.total.numpy())
m.update_state(y_true=[0], y_pred=[2])
print(m.total.numpy())
print(m.result().numpy())
&lt;/denchmark-code&gt;

Then since state count is 2 and total is 3, the result returned is 1.5. Source code of  is referred from &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/metrics.py#L361-L371&gt;here&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='2' author='gsimko' date='2019-12-27T19:47:13Z'>
		Of course the result matches the source code :) However, I believe the source code does not match the intent.
If you do an eval loop like this where the data is sequential hence label is (batch,step,1)-shaped, it won't calculate the MAE/MSE properly:
&lt;denchmark-code&gt;for feature, label in eval_dataset:
   pred = model.predict(feature)
   metric.update_state(pred, label)
&lt;/denchmark-code&gt;

The reason is that "mean of means of squares" != "mean of squares" as long as in the first term the dimension varies.
		</comment>
		<comment id='3' author='gsimko' date='2019-12-28T03:06:52Z'>
		Oh now I got it. Sorry for misinterpreting your question the first time. &lt;denchmark-link:https://github.com/gsimko&gt;@gsimko&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='gsimko' date='2020-01-18T01:07:26Z'>
		To avoid metric accumulation you may try  method.
See &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Mean?version=nightly#reset_states&gt;https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Mean?version=nightly#reset_states&lt;/denchmark-link&gt;

import tensorflow as tf
m=tf.metrics.MeanAbsoluteError()
m.update_state(y_true=[0,0], y_pred=[1,1])
m.reset_states()
m.update_state(y_true=[0], y_pred=[2])
assert m.result() == 2
		</comment>
		<comment id='5' author='gsimko' date='2020-01-27T22:33:38Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35439&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35439&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='gsimko' date='2020-01-28T18:31:40Z'>
		Resetting the state does not solve the problem - during evals one needs to accumulate the metrics. Why was this issue closed?
		</comment>
		<comment id='7' author='gsimko' date='2020-02-02T02:37:50Z'>
		&lt;denchmark-link:https://github.com/gsimko&gt;@gsimko&lt;/denchmark-link&gt;
 Thank you for the question. For metrics and losses, we expect that labels and predictions are at least 2D.
&lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanAbsoluteError?version=nightly#update_state&gt;https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanAbsoluteError?version=nightly#update_state&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except
        sparse loss functions such as sparse categorical crossentropy where
        shape = `[batch_size, d0, .. dN-1]`
y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`
&lt;/denchmark-code&gt;

For example, let's say y_pred and y_true are of the shape [batch_size, d0], we compute the mean across the last axis in a sample -&gt; this will give us one value per-sample -&gt; [batch_size].  The second mean is computed across all samples across all batches.
		</comment>
		<comment id='8' author='gsimko' date='2020-02-21T18:50:19Z'>
		It has been 19 days with no activity and the awaiting response label was assigned. Is this still an issue?
		</comment>
		<comment id='9' author='gsimko' date='2020-02-22T04:17:01Z'>
		&lt;denchmark-link:https://github.com/pavithrasv&gt;@pavithrasv&lt;/denchmark-link&gt;
: thanks for the response, that answered it! I also just realized that my original assertion was off (it should have been 4/3 instead of 2), and by following what you described (adding an inner dimension) I get that answer.
If someone else finds this problem, this works as expected:
import tensorflow as tf
m=tf.metrics.MeanAbsoluteError()
m.update_state(y_true=[[0],[0]], y_pred=[[1],[1]])
m.update_state(y_true=[[0]], y_pred=[[2]])
assert (m.result() - 4/3)&lt;1e-6
		</comment>
		<comment id='10' author='gsimko' date='2020-02-22T04:17:03Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35439&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35439&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>