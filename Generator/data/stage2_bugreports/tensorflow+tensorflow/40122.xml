<bug id='40122' author='djshen' open_date='2020-06-03T13:18:27Z' closed_time='2020-11-09T03:22:15Z'>
	<summary>Incorrect result of _MKLMaxPoolGrad</summary>
	<description>
System information

OS Platform and Distribution: Arch Linux 5.5.2-arch1-1  x86_64
TensorFlow installed from: source
TensorFlow version: v1.12.1-33097-g83eb4048ba 2.2.0 and v2.2.0-0-g2b96f3662b 2.2.0
Python version: Python 3.6.10
Bazel version: 3.0.0 for master, 2.0.0 for r2.2
GCC/Compiler version: GCC 9.3.0

The package was built with the commands:
bazel build --config=mkl //tensorflow/tools/pip_package:build_pip_package
# For master (commit #83eb40)
./bazel-bin/tensorflow/tools/pip_package/build_pip_package --nightly_flag ./master-83eb40
# For r2.2
bazel build --config=mkl //tensorflow/tools/pip_package:build_pip_package
Describe the current behavior
The gradient of the max pooling 2D is wrong.
Code:
import numpy as np
import tensorflow as tf
tf.compat.v1.disable_v2_behavior()

x = np.array([
    [3, 0, 0, 2, 3],
    [0, 0, 0, 0, 1],
    [0, 0, 0, 1, 3],
    [0, 0, 0, 0, 0],
    [1, 1, 3, 8, 6]
]).astype(np.float32).reshape([1, 5, 5, 1])

x_t = tf.compat.v1.placeholder(tf.float32, shape=[1, 5, 5, 1])
w = np.array([1]).reshape([1, 1, 1, 1]).astype(np.float32)
conv_t = tf.nn.conv2d(x_t, w, [1, 1, 1, 1], 'SAME')
pool_t = tf.nn.max_pool(conv_t, [1, 2, 2, 1], [1, 2, 2, 1], 'VALID')
grad_t = tf.gradients(ys=pool_t, xs=conv_t)

tensors = [conv_t, pool_t, grad_t]
tensors = [tf.squeeze(t, [-1]) for t in tensors]

with tf.compat.v1.Session() as sess:
    conv, pool, grad = sess.run(tensors, feed_dict={x_t: x})
    print('conv\n', conv, '\npool\n', pool, '\ngrad\n', grad)
Output:
&lt;denchmark-code&gt;conv
 [[[3. 0. 0. 2. 3.]
  [0. 0. 0. 0. 1.]
  [0. 0. 0. 1. 3.]
  [0. 0. 0. 0. 0.]
  [1. 1. 3. 8. 6.]]]
pool
 [[[3. 2.]
  [0. 1.]]]
grad
 [[[[1. 0. 1. 0. 0.]
   [0. 0. 0. 0. 0.]
   [1. 0. 1. 0. 0.]
   [0. 0. 0. 0. 0.]
   [0. 0. 0. 0. 0.]]]]
&lt;/denchmark-code&gt;

Describe the expected behavior
If we run the code with TF_DISABLE_MKL=1, the gradient will be
&lt;denchmark-code&gt; [[[[1. 0. 0. 1. 0.]
   [0. 0. 0. 0. 0.]
   [1. 0. 0. 1. 0.]
   [0. 0. 0. 0. 0.]
   [0. 0. 0. 0. 0.]]]]
&lt;/denchmark-code&gt;

Note that the positions of the second 1's in the first and the third rows are different.
Other info / logs
If I directly feed the input to max pooling, the result is correct.
import numpy as np
import tensorflow as tf
tf.compat.v1.disable_v2_behavior()


x = np.array([
    [3, 0, 0, 2, 3],
    [0, 0, 0, 0, 1],
    [0, 0, 0, 1, 3],
    [0, 0, 0, 0, 0],
    [1, 1, 3, 8, 6]
]).astype(np.float32).reshape([1, 5, 5, 1])

x_t = tf.compat.v1.placeholder(tf.float32, shape=[1, 5, 5, 1])
pool_t = tf.nn.max_pool(x_t, [1, 2, 2, 1], [1, 2, 2, 1], 'VALID')
grad_t = tf.gradients(ys=pool_t, xs=x_t)

tensors = [x_t, pool_t, grad_t]
tensors = [tf.squeeze(t, [-1]) for t in tensors]

with tf.compat.v1.Session() as sess:
    x, pool, grad = sess.run(tensors, feed_dict={x_t: x})
    print('x\n', x, '\npool\n', pool, '\ngrad\n', grad)
Output:
&lt;denchmark-code&gt;x
 [[[3. 0. 0. 2. 3.]
  [0. 0. 0. 0. 1.]
  [0. 0. 0. 1. 3.]
  [0. 0. 0. 0. 0.]
  [1. 1. 3. 8. 6.]]]
pool
 [[[3. 2.]
  [0. 1.]]]
grad
 [[[[1. 0. 0. 1. 0.]
   [0. 0. 0. 0. 0.]
   [1. 0. 0. 1. 0.]
   [0. 0. 0. 0. 0.]
   [0. 0. 0. 0. 0.]]]]
&lt;/denchmark-code&gt;

In addition, if the I replace the conv2d with relu, which will be also rewritten, the result is also correct.
import numpy as np
import tensorflow as tf
tf.compat.v1.disable_v2_behavior()


x = np.array([
    [3, 0, 0, 2, 3],
    [0, 0, 0, 0, 1],
    [0, 0, 0, 1, 3],
    [0, 0, 0, 0, 0],
    [1, 1, 3, 8, 6]
]).astype(np.float32).reshape([1, 5, 5, 1])

x_t = tf.compat.v1.placeholder(tf.float32, shape=[1, 5, 5, 1])
relu_t = tf.nn.relu(x_t)
pool_t = tf.nn.max_pool(relu_t, [1, 2, 2, 1], [1, 2, 2, 1], 'VALID')
grad_t = tf.gradients(ys=pool_t, xs=relu_t)

tensors = [relu_t, pool_t, grad_t]
tensors = [tf.squeeze(t, [-1]) for t in tensors]

with tf.compat.v1.Session() as sess:
    relu, pool, grad = sess.run(tensors, feed_dict={x_t: x})
    print('relu\n', relu, '\npool\n', pool, '\ngrad\n', grad)
Output:
&lt;denchmark-code&gt;relu
 [[[3. 0. 0. 2. 3.]
  [0. 0. 0. 0. 1.]
  [0. 0. 0. 1. 3.]
  [0. 0. 0. 0. 0.]
  [1. 1. 3. 8. 6.]]]
pool
 [[[3. 2.]
  [0. 1.]]]
grad
 [[[[1. 0. 0. 1. 0.]
   [0. 0. 0. 0. 0.]
   [1. 0. 0. 1. 0.]
   [0. 0. 0. 0. 0.]
   [0. 0. 0. 0. 0.]]]]
&lt;/denchmark-code&gt;

I checked the log with TF_CPP_MIN_VLOG_LEVEL=1 and confirmed that the OP was rewritten.
It seems that the result is affected by the convolution.
	</description>
	<comments>
		<comment id='1' author='djshen' date='2020-06-04T03:09:49Z'>
		&lt;denchmark-link:https://github.com/djshen&gt;@djshen&lt;/denchmark-link&gt;

I have tried in colab with TF version 2.2 and i am not seeing any issue here.Please, find the gist &lt;denchmark-link:https://colab.research.google.com/gist/ravikyram/b89f9c38e637d8507935e6aeb6fb8a88/untitled47.ipynb&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='2' author='djshen' date='2020-06-04T06:29:29Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;

I think the official release is not compiled with MKL. This issue does not exist when I install TF from PyPI.
I try to install TF from &lt;denchmark-link:https://tensorflow-ci.intel.com/job/tensorflow-mkl-build-whl-nightly/&gt;https://tensorflow-ci.intel.com/job/tensorflow-mkl-build-whl-nightly/&lt;/denchmark-link&gt;

and can reproduce the result.
Please find the &lt;denchmark-link:https://colab.research.google.com/gist/djshen/2dde585e30b2da59342cb8e0a3de6c3b/untitled47.ipynb&gt;gist&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='3' author='djshen' date='2020-06-12T23:11:07Z'>
		we are looking at this issue and will respond once we reproduce
		</comment>
		<comment id='4' author='djshen' date='2020-06-17T02:11:41Z'>
		&lt;denchmark-link:https://github.com/djshen&gt;@djshen&lt;/denchmark-link&gt;
 We are checking this issue now.
		</comment>
		<comment id='5' author='djshen' date='2020-06-17T09:12:09Z'>
		&lt;denchmark-link:https://github.com/djshen&gt;@djshen&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/pre&gt;@pre&lt;/denchmark-link&gt;

I reproduce the issue in TF 2.2 (Installed by Conda: tensorflow  2.2.0   mkl_py36h5a57954_0).
There is no such problem in TF2.1.
TF2.2 use DNNL 1.2.2 release.
After check the DNNL log, it's the fault of DNNL (MKLDNN).
I have reported the issue to Intel DNNL team as high level.
Hope get the confirm and fix as soon!
		</comment>
		<comment id='6' author='djshen' date='2020-06-19T06:00:46Z'>
		DNNL team confirm it's not DNNL issue. I have reported to Intel Tensorflow team as high level.
		</comment>
		<comment id='7' author='djshen' date='2020-06-26T17:46:19Z'>
		&lt;denchmark-link:https://github.com/penpornk&gt;@penpornk&lt;/denchmark-link&gt;
 Internally we are also working to solve this bug. Just a heads up. No milestone set for TF 2.3 yet. But making a mention for your internal tracking.
		</comment>
		<comment id='8' author='djshen' date='2020-06-26T17:52:56Z'>
		&lt;denchmark-link:https://github.com/nammbash&gt;@nammbash&lt;/denchmark-link&gt;
 Got it. Thank you!
		</comment>
		<comment id='9' author='djshen' date='2020-11-05T08:33:43Z'>
		&lt;denchmark-link:https://github.com/djshen&gt;@djshen&lt;/denchmark-link&gt;

This issue is fixed in Tf 2.4.
Could you check it?
		</comment>
		<comment id='10' author='djshen' date='2020-11-06T05:25:19Z'>
		&lt;denchmark-link:https://github.com/NeoZhangJianyu&gt;@NeoZhangJianyu&lt;/denchmark-link&gt;

I compiled TF v2.4.0-rc0 from source with --config=mkl and confirmed that the result is correct.
		</comment>
		<comment id='11' author='djshen' date='2020-11-09T02:50:36Z'>
		&lt;denchmark-link:https://github.com/djshen&gt;@djshen&lt;/denchmark-link&gt;

It's great news!
Could you close this issue? :)
		</comment>
		<comment id='12' author='djshen' date='2020-11-09T03:22:15Z'>
		&lt;denchmark-link:https://github.com/NeoZhangJianyu&gt;@NeoZhangJianyu&lt;/denchmark-link&gt;

Sure!
		</comment>
		<comment id='13' author='djshen' date='2020-11-09T03:22:17Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40122&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40122&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='14' author='djshen' date='2020-11-09T03:23:42Z'>
		&lt;denchmark-link:https://github.com/djshen&gt;@djshen&lt;/denchmark-link&gt;

Thank you very much!
		</comment>
	</comments>
</bug>