<bug id='33190' author='paanguin' open_date='2019-10-10T02:38:05Z' closed_time='2020-09-17T18:41:15Z'>
	<summary>A bug in 'MoveBinaryOperatorBeforeReshape' graph_transformations  during tflite_convert.</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution: Linux Ubuntu 16.04
TensorFlow installed from source
TensorFlow version (use command below): ('v2.0.0-0-g64c3d38', '2.0.0')
Python version: 2.7.12
Bazel version (if compiling from source): 0.24.1
GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
CUDA/cuDNN version: 10.0 / 7.5
GPU model and memory: GeForce GTX TITAN

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
The outputs of the tflite model (*.tflite), converted by the tflite_convert, are totally different from the original pre-trained saved_model.
When I tries to fix that, I finally got the right outputs by suppressing 'MoveBinaryOperatorBeforeReshape' of the graph_transformations (tensorflow/lite/toco/graph_transformations).  To do this, I removed the operations from parts of the source codes (tensorflow/lite/toco/BUILD, tensorflow/lite/toco/toco_tooling.cc, ,tensorflow/lite/toco/graph_transformations/graph_transformations.h) and built a package from it.
I compared the two versions of the tflite models, by using the visualization tool (bazel run //tensorflow/lite/tools:visualize model.tflite visualized_model.html). I found out that when the MoveBinaryOperatorBeforeReshape is turned on, the tflite_convert removes a specific ADD op and its one constant input tensor from my graph. Then it was the very reason I got the different output.
This seems a bug to me so that I want to report this.
Since my graph is too big, and I cannot share my codes, models nor figures, I want to share the responsible part of the visualization results. I strongly recommend to draw graphs for better understanding.
Consider following as tables,  columns are seperated by '|'.
Original Graph (which gives wrong output)
Operations
&lt;denchmark-code&gt;index | inputs | outputs | builtin_options | opcode_index
69 | [175, 43, 4154] | [185] | {u'fused_activation_function':   u'NONE', u'stride_h': 1, u'padding': u'VALID', u'stride_w': 1,   u'dilation_h_factor': 1, u'dilation_w_factor': 1} | CONV_2D (2)
100 | [185, 186] | [190] | {u'new_shape': [1, 600, 512]} | RESHAPE (5)
126 | [190, 1518] | [1517] | {u'keep_dims': True} | MEAN (10)
127 | [190, 1517] | [1529] | {u'fused_activation_function':   u'NONE'} | SUB (11)
128 | [190, 1517] | [1528] | {u'fused_activation_function':   u'NONE'} | SUB (11)
341 | [1539, 190] | [1516] | {u'fused_activation_function':   u'NONE'} | ADD (0)
&lt;/denchmark-code&gt;

Tensors
&lt;denchmark-code&gt;index | name | type | shape | buffer | quantization
43 | Identity_16 | FLOAT32 | [512, 1, 1, 240] | 2051 | {u'quantized_dimension': 0,   u'details_type': 0}
175 | body/model/parallel_0/body/ExpandDims_2 | FLOAT32 | [1, 600, 1, 240] | 1816 | {u'quantized_dimension': 0,   u'details_type': 0}
4154 | body/model/parallel_0/body/prepare_transform_before_single/Conv2D_bias | FLOAT32 | [512] | 2053 | {u'quantized_dimension': 0,   u'details_type': 0}
185 | body/model/parallel_0/body/Squeeze | FLOAT32 | [1, 600, 1, 512] | 2338 | {u'quantized_dimension': 0,   u'details_type': 0}
186 | body/model/parallel_0/body/Squeeze_shape | INT32 | [3] | 2110 | {u'quantized_dimension': 0,   u'details_type': 0}
190 | body/model/parallel_0/body/add_1 | FLOAT32 | [1, 600, 512] | 1098 | {u'quantized_dimension': 0,   u'details_type': 0}
&lt;/denchmark-code&gt;

Fixed Graph ('MoveBinaryOperatorBeforeReshape' suppressed and gives the right outputs)
Operations
&lt;denchmark-code&gt;index | inputs | outputs | builtin_options | opcode_index
69 | [175, 43, 4156] | [4155] | {u'fused_activation_function':   u'NONE', u'stride_h': 1, u'padding': u'VALID', u'stride_w': 1,   u'dilation_h_factor': 1, u'dilation_w_factor': 1} | CONV_2D (2)
98 | [4155, 186] | [185] | {u'new_shape': [1, 600, 512]} | RESHAPE (5)
101 | [190, 185] | [191] | {u'fused_activation_function':   u'NONE'} | ADD (0)
127 | [191, 1519] | [1518] | {u'keep_dims': True} | MEAN (10)
128 | [191, 1518] | [1530] | {u'fused_activation_function':   u'NONE'} | SUB (11)
129 | [191, 1518] | [1529] | {u'fused_activation_function':   u'NONE'} | SUB (11)
342 | [1540, 191] | [1517] | {u'fused_activation_function':   u'NONE'} | ADD (0)
&lt;/denchmark-code&gt;

Tensors
&lt;denchmark-code&gt;index | name | type | shape | buffer | quantization
43 | Identity_16 | FLOAT32 | [512, 1, 1, 240] | 2055 | {u'quantized_dimension': 0,   u'details_type': 0}
175 | body/model/parallel_0/body/ExpandDims_2 | FLOAT32 | [1, 600, 1, 240] | 1819 | {u'quantized_dimension': 0,   u'details_type': 0}
4156 | body/model/parallel_0/body/prepare_transform_before_single/Conv2D_bias | FLOAT32 | [512] | 2057 | {u'quantized_dimension': 0,   u'details_type': 0}
4155 | body/model/parallel_0/body/prepare_transform_before_single/BiasAdd | FLOAT32 | [1, 600, 1, 512] | 1547 | {u'quantized_dimension': 0,   u'details_type': 0}
186 | body/model/parallel_0/body/Squeeze_shape | INT32 | [3] | 2114 | {u'quantized_dimension': 0,   u'details_type': 0}
190 | body/model/parallel_0/body/add | FLOAT32 | [1, 600, 512] | 1845 | {u'quantized_dimension': 0,   u'details_type': 0}
185 | body/model/parallel_0/body/Squeeze | FLOAT32 | [1, 600, 512] | 2342 | {u'quantized_dimension': 0,   u'details_type': 0}
191 | body/model/parallel_0/body/add_1 | FLOAT32 | [1, 600, 512] | 1098 | {u'quantized_dimension': 0,   u'details_type': 0}
&lt;/denchmark-code&gt;

Rest of the operations are the same. Total number of graphs are different only by 1. In the fixed graph, the ADD op 101 has one constant tensor (with none-zero values) and one variable tensor input. It is missing from the original graph, I don't understand why the tflite_convert removes it.
Code to reproduce the issue
I could not find reproducing code.
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='paanguin' date='2020-09-03T17:51:57Z'>
		&lt;denchmark-link:https://github.com/paanguin&gt;@paanguin&lt;/denchmark-link&gt;
,
Is this still an issue? Could you please update TensorFlow to v2.3 and check if you are facing the same issue?

Since my graph is too big, and I cannot share my codes, models nor figures, I want to share the responsible part of the &gt;visualization results. I strongly recommend to draw graphs for better understanding.

Also, without a reproducible code it would be difficult for us to debug the issue. Could you please provide a minimal working example to reproduce the issue on our end, so that we can take a look at it. Thanks!
		</comment>
		<comment id='2' author='paanguin' date='2020-09-10T18:16:25Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='3' author='paanguin' date='2020-09-17T18:41:12Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='4' author='paanguin' date='2020-09-17T18:41:17Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33190&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33190&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>