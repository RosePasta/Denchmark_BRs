<bug id='33369' author='venuswu' open_date='2019-10-15T06:27:44Z' closed_time='2019-11-13T01:42:43Z'>
	<summary>Dose the data division for multi-gpu destruct the accuracy?</summary>
	<description>
Data division for multi-gpu after tf1.11 seems to have the same shape of data across all gpus, which will result in a bigger variance of loss between different steps. For example, for the transformer model, when in one step, all the gpus are feeded with the data with length near 8, but in the next step, all the gpus are feeded with the data with length near 32, which will result in a bigger variance of loss.
In practice, I got a lower accuracy trained with tf2.0 than tf1.11.
&lt;denchmark-link:https://github.com/guptapriya&gt;@guptapriya&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/yuefengz&gt;@yuefengz&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/zongweiz&gt;@zongweiz&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/sgpyc&gt;@sgpyc&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='venuswu' date='2019-10-15T16:03:35Z'>
		Hi,

Thank you for your question. Can you elaborate on your example? Why is the
data length 8 vs 32 in subsequent steps? Do you mean you have dynamic input
per step?

We have been able to train transformer to the state of art accuracy with
2.0 and 1.15, but we havent tested with 1.11. can you share your setup?
Which API are you you using, what are the batch sizes etc?
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Tue, Oct 15, 2019, 2:32 AM venuswu ***@***.***&gt; wrote:
 Data division for multi-gpu after tf1.11 seems to have the same shape of
 data across all gpus, which will result in a bigger variance of loss
 between different steps. For example, for the transformer model, when in
 one step, all the gpus are feeded with the data with length near 8, but in
 the next step, all the gpus are feeded with the data with length near 32,
 which will result in a bigger variance of loss.

 In practice, I got a lower accuracy trained with tf2.0 than tf1.11.

 @guptapriya &lt;https://github.com/guptapriya&gt; @yuefengz
 &lt;https://github.com/yuefengz&gt; @zongweiz &lt;https://github.com/zongweiz&gt;
 @sgpyc &lt;https://github.com/sgpyc&gt;

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#33369?email_source=notifications&amp;email_token=ADLTSF4IFVKOA3AHVZHYSV3QOVPW3A5CNFSM4JAX3ZAKYY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4HRYXD4A&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/ADLTSF5JAK4TUE6MUJFV2ADQOVPW3ANCNFSM4JAX3ZAA&gt;
 .



		</comment>
		<comment id='2' author='venuswu' date='2019-10-16T09:57:42Z'>
		Yes, I used dynamic batchsize to train the official/transformer.
&lt;denchmark-link:https://github.com/tensorflow/models/blob/master/official/transformer/v2/data_pipeline.py&gt;https://github.com/tensorflow/models/blob/master/official/transformer/v2/data_pipeline.py&lt;/denchmark-link&gt;

Here is the data pipeline of 2.0 same as 1.11, for which batchsize means the number of tokens.  For example in tf1.11, I set the batchsize to 4096 which means 4096 tokens for every replica, for a replica       the length of the tokens seem to be as near as possible(padding fills up the gap), for example a replica can get a batch with the shape of [1024,4], and another get a shape of [ 128,32], in the same step, the shape of the batch may vary between different replica. But there is similar situation between steps.
For the later version like tf1.14 and tf2.0, the parameter batchsize means total tokens of all the replicas, so for example the batchsize 4096*8 may have a shape of [8192,4] and autoshard across all the replicas, every replica get the same shape of [1024,4], but for the next step, it may read a shape of [1024, 32], and every replica get the same shape of [128,32], which may result in a variance between different steps.
I have read a paper &lt;denchmark-link:https://arxiv.org/pdf/1806.00187.pdf&gt;https://arxiv.org/pdf/1806.00187.pdf&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/3830256/66908721-27548580-f03e-11e9-85fc-e4ffb4eced36.png&gt;&lt;/denchmark-link&gt;

in which it is said that it my destruct the accuracy with all replica feeded with the same shape of data.
I don't kown whether I have described the question clearly.
&lt;denchmark-link:https://github.com/guptapriya&gt;@guptapriya&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='venuswu' date='2019-10-18T04:12:01Z'>
		
Here is the command line.
&lt;denchmark-link:https://github.com/guptapriya&gt;@guptapriya&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='venuswu' date='2019-10-18T11:28:31Z'>
		Is there a option for Backward compatibility? &lt;denchmark-link:https://github.com/guptapriya&gt;@guptapriya&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='venuswu' date='2019-10-29T22:01:46Z'>
		&lt;denchmark-link:https://github.com/venuswu&gt;@venuswu&lt;/denchmark-link&gt;
 Is there a way that you can provide us a minimal reproducible example ? Also can you please explain it clearly as I am unable to understand what you are trying to convey? Whats the root cause of the issue?
		</comment>
		<comment id='6' author='venuswu' date='2019-11-13T01:42:43Z'>
		Clsoing this issue as it has been inactive for more than 14 days. Please add additional details and we can open this issue again
		</comment>
		<comment id='7' author='venuswu' date='2019-11-13T01:42:45Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33369&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33369&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>