<bug id='39101' author='Dmitriy90' open_date='2020-05-02T14:24:14Z' closed_time='2020-05-10T06:22:19Z'>
	<summary>TFLite Interpreter fails to load quantized model on iPhone</summary>
	<description>
Similar issue: #28163
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): stock MobilenetV2
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone 5s
TensorFlow installed from (source or binary): pip install tensorflow==2.1
TensorFlow version (use command below): 2.1.0
Python version: 3.7.4
CUDA/cuDNN version: 10.1
GPU model and memory: Geforce GTX 1650, 4 GB

Describe the current behavior
I have created a new tflite model based on MobilenetV2.
tf.keras.applications.MobileNetV2(input_shape=[SIZE, SIZE, 3], include_top=False)
It works well without quantization using CPU on iOS. I should say that TensorFlow team did a great job, many thanks.
Unfortunately there is a problem with latency.  I have read TF documentation related to optimization (post trainig quantization) and workerd with dynamic range quantization.
I executed the following python code:
&lt;denchmark-code&gt;tflite_models_dir = pathlib.Path("/tmp/mnist_tflite_models/")
tflite_models_dir.mkdir(exist_ok=True, parents=True)
converter = tf.lite.TFLiteConverter.from_saved_model('C:\Work\Python\NN\MobileNet_v2_128')
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_quant_model = converter.convert()
tflite_model_quant_file = tflite_models_dir/"MobileNet_v2_128_quant.tflite"
tflite_model_quant_file.write_bytes(tflite_quant_model)
&lt;/denchmark-code&gt;

After this model was added to XCode project on MAC. Pod file contains the following framework:
pod 'TensorFlowLite', '~&gt; 1.13.1'
Error:
tflite::InterpreterBuilder return this error: "Didn't find op for builtin opcode 'CONV_2D' version '2'"
Describe the expected behavior
I suppose this should work faster without errors.
Other info / logs
I also tried to use float16 quantization.
Python code:
&lt;denchmark-code&gt;converter = tf.lite.TFLiteConverter.from_saved_model('C:\Work\Python\NN\MobileNet_v2_128')
converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
tflite_quant_model = converter.convert()
open("MobileNet_v2_128_qua_float16.tflite", "wb").write(tflite_quant_model)
&lt;/denchmark-code&gt;

In swift code I used MetalDelegate
With pod 'TensorFlowLiteSwift', '0.0.1-nightly'
I have no errors, but model doesnâ€™t work
With pod 'TensorFlowLiteSwift', '2.1.0'
I have the following error:
2020-05-01 21:36:13.578369+0300 TFL Segmentation[6367:330410] Initialized TensorFlow Lite runtime. 2020-05-01 21:36:20.877393+0300 TFL Segmentation[6367:330397] Execution of the command buffer was aborted due to an error during execution. Caused GPU Hang Error (IOAF code 3)
Is it possible to use MobilenetV2.tflite quantized model in XCode Swift project?
Best regards,
Dmitriy
	</description>
	<comments>
		<comment id='1' author='Dmitriy90' date='2020-05-05T05:55:23Z'>
		Just want to confirm that the workflow works without converter.optimizations = [tf.lite.Optimize.DEFAULT]? 
		</comment>
		<comment id='2' author='Dmitriy90' date='2020-05-05T16:31:17Z'>
		
Is it possible to use MobilenetV2.tflite quantized model in XCode Swift project?

With the 2.1 build, does it work if you run without the Metal delegate? Using the CPU? In general, quantized models don't work with GPU (Metal) acceleration.
		</comment>
		<comment id='3' author='Dmitriy90' date='2020-05-05T17:50:45Z'>
		

Is it possible to use MobilenetV2.tflite quantized model in XCode Swift project?

With the 2.1 build, does it work if you run without the Metal delegate? Using the CPU? In general, quantized models don't work with GPU (Metal) acceleration.

Yes. It works well with CPU (without delegate).
But is it possible to use  quantized MobilenetV2 model in XCode for Swift project? Maybe I need to use another approach/library?
		</comment>
		<comment id='4' author='Dmitriy90' date='2020-05-05T17:52:11Z'>
		
Just want to confirm that the workflow works without converter.optimizations = [tf.lite.Optimize.DEFAULT]? 

I'm trying to understand... is it possible to to use quantized MobilenetV2 model in XCode for Swift project?
		</comment>
		<comment id='5' author='Dmitriy90' date='2020-05-05T17:54:59Z'>
		
is it possible to to use quantized MobilenetV2 model in XCode for Swift project?

Yes, this is possible, however, that is independent of the issue with using Metal delegation on a quantized model (which isn't yet supported). But note that you need the latest pod (2.1.0) to use the latest set of quantized operators for your model.
		</comment>
		<comment id='6' author='Dmitriy90' date='2020-05-05T18:09:09Z'>
		

is it possible to to use quantized MobilenetV2 model in XCode for Swift project?

Yes, this is possible, however, that is independent of the issue with using Metal delegation on a quantized model (which isn't yet supported). But note that you need the latest pod (2.1.0) to use the latest set of quantized operators for your model.

Thanks for answer.
Do you mean pod 'TensorFlowLiteSwift', '2.1.0'?
I tried to use pod 'TensorFlowLiteSwift', '2.1.0' and float16 quantization.
But I have the following error:
2020-05-01 21:36:13.578369+0300 TFL Segmentation[6367:330410] Initialized TensorFlow Lite runtime. 2020-05-01 21:36:20.877393+0300 TFL Segmentation[6367:330397] Execution of the command buffer was aborted due to an error during execution. Caused GPU Hang Error (IOAF code 3)
P.S. I will check this again tomorrow...
		</comment>
		<comment id='7' author='Dmitriy90' date='2020-05-06T15:13:06Z'>
		

is it possible to to use quantized MobilenetV2 model in XCode for Swift project?

Yes, this is possible, however, that is independent of the issue with using Metal delegation on a quantized model (which isn't yet supported). But note that you need the latest pod (2.1.0) to use the latest set of quantized operators for your model.

I have changed a code in this example: &lt;denchmark-link:https://github.com/tensorflow/examples/tree/master/lite/examples/image_segmentation/ios&gt;https://github.com/tensorflow/examples/tree/master/lite/examples/image_segmentation/ios&lt;/denchmark-link&gt;

and used pod 'TensorFlowLiteSwift', '2.1.0'
MobileNetV2  with quantization and without quantization.
And one more question...
As I understand I cannot use GPU acceleration with MobilenetV2.tflite . Because we can use the following list of models with GPU: &lt;denchmark-link:https://www.tensorflow.org/lite/performance/gpu#supported_models_and_ops&gt;https://www.tensorflow.org/lite/performance/gpu#supported_models_and_ops&lt;/denchmark-link&gt;
.
Is it correct?
		</comment>
		<comment id='8' author='Dmitriy90' date='2020-05-06T19:44:16Z'>
		
As I understand I cannot use GPU acceleration with MobilenetV2.tflite .

Are you referring to the quantized version? The floating point version should work, but the quantized version will not. If you're unable to use the GPU with the floating point version, that's a bug that we should address.
		</comment>
		<comment id='9' author='Dmitriy90' date='2020-05-07T15:50:10Z'>
		

As I understand I cannot use GPU acceleration with MobilenetV2.tflite .

Are you referring to the quantized version? The floating point version should work, but the quantized version will not. If you're unable to use the GPU with the floating point version, that's a bug that we should address.

Today I tried to use GPU for my model using this example: &lt;denchmark-link:https://github.com/tensorflow/examples/tree/master/lite/examples/image_segmentation/ios&gt;https://github.com/tensorflow/examples/tree/master/lite/examples/image_segmentation/ios&lt;/denchmark-link&gt;

I have tested stock MobilenetV2.tflite model with pod 'TensorFlowLiteSwift', '2.1.0' and Metal delegate. It works, but slower then CPU.
CPU: preprocessing - 114ms, postprocessing - 201ms.
GPU: preprocessing - 130ms, postprocessing - 212ms
I thought I can implement float16 quantization to use GPU, but it's not possible with 'TensorFlowLiteSwift', '2.1.0'.
GPU approach work slower than CPU (probably because it's try to convert Float32 to Float16)
So I have only one way to speed up segmentation process.

Use CPU.
Implement Quantization aware training.

Is it correct?
		</comment>
		<comment id='10' author='Dmitriy90' date='2020-05-08T04:03:30Z'>
		&lt;denchmark-link:https://github.com/Dmitriy90&gt;@Dmitriy90&lt;/denchmark-link&gt;
 I think you should be comparing the  time instead, when using the image_segmentation example.
The pre-/post-processing logic is run outside of the TFLite inference, so whether or not you use the Metal delegate is irrelevant. The difference you were seeing was likely within the typical margin of error.
Could you check again whether the Metal delegate helps shortening the model inference time? Also please make sure that you're &lt;denchmark-link:https://www.tensorflow.org/lite/performance/gpu#step_5_release_mode&gt;using the release build&lt;/denchmark-link&gt;
 when testing these.
Another interesting point is that you're using the iPhone 5s model, on which we haven't tested our Metal delegate. The difference in hw specs compared to more recent models might also be one of the reasons if you're not seeing enough speed boost with GPU.
		</comment>
		<comment id='11' author='Dmitriy90' date='2020-05-09T13:04:18Z'>
		Hi &lt;denchmark-link:https://github.com/yyoon&gt;@yyoon&lt;/denchmark-link&gt;

Thanks for answer. I have implemented the following recomendations: &lt;denchmark-link:https://www.tensorflow.org/lite/performance/gpu#step_5_release_mode&gt;https://www.tensorflow.org/lite/performance/gpu#step_5_release_mode&lt;/denchmark-link&gt;

CPU: And it works very fast (40 - 60ms) for different MobileNetV2.tflite versions (input image sizes 128x128, 160x160, 192x192 and 224x224)
GPU: works a bit faster, but for MobileNetV2.tflite with input 128x128.
For 160x160, 192x192 and 224x224 I see the following error:
"Ececution of the command buffer was aborted due to an error during execution. Caused GPU Hang Error (IOAF code 3)"
Anyway it's fast enough for me. Thanks for help. Tensorflow support team is amaising.
		</comment>
		<comment id='12' author='Dmitriy90' date='2020-05-10T06:22:20Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39101&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39101&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>