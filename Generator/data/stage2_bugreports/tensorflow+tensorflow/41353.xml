<bug id='41353' author='lriuui0x0' open_date='2020-07-13T21:51:35Z' closed_time='2020-07-15T19:18:55Z'>
	<summary>tf.nn.ctc_loss not returning expected value</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
TensorFlow installed from (source or binary): Binary
TensorFlow version (use command below):  v2.2.0-rc4-8-g2b96f3662b 2.2.0
Python version: 3.7.7
Bazel version (if compiling from source): No
GCC/Compiler version (if compiling from source): No

Describe the current behavior
I run the following code:
&lt;denchmark-code&gt;import tensorflow as tf

label = [1, 2, 1]
logits = [[0.0, 1.0, 0.0],
          [0.0, 0.0, 1.0],
          [0.0, 1.0, 0.0]]
labels_length = 3
logits_length = 3

labels_tensor = tf.convert_to_tensor([label], dtype=tf.int32)
logits_tensor = tf.convert_to_tensor([logits], dtype=tf.float32)
labels_length_tensor = tf.convert_to_tensor([labels_length], dtype=tf.int32)
logits_length_tensor = tf.convert_to_tensor([logits_length], dtype=tf.int32)

loss = tf.nn.ctc_loss(labels_tensor, logits_tensor, labels_length_tensor, logits_length_tensor, logits_time_major=False)
print(loss.numpy()[0])
&lt;/denchmark-code&gt;

Basically there're 3 timestamps, and there're two characters a, b and the GT is aba ([1, 2, 1]) which only has one representation in CTC. And I'm making the logits have the probability of 1 for aba only. Therefore I expect the loss is 0, however the loss is some random value 1.6543342.
Either there's a bug, or I seriously misunderstand the documentation - in which case I think the &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/nn/ctc_loss&gt;documentation on this topic&lt;/denchmark-link&gt;
 isn't very good. It would be better to have some example, as suggested in another issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/35063&gt;#35063&lt;/denchmark-link&gt;
.
Describe the expected behavior
Expect the loss to be 0.
	</description>
	<comments>
		<comment id='1' author='lriuui0x0' date='2020-07-14T06:55:06Z'>
		I have tried in colab with TF versions 2.2, 2.3-rc1,nightly versions() and was able to reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.research.google.com/gist/ravikyram/4ae1e375a30ac4a1d887525a71b50611/untitled119.ipynb&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='2' author='lriuui0x0' date='2020-07-14T08:53:38Z'>
		Hi &lt;denchmark-link:https://github.com/lriuui0x0&gt;@lriuui0x0&lt;/denchmark-link&gt;
 , I think the value is not expected to zero in the implementation. If we follow the fomula in the paper, the neg log probability is of course zero (see last two cell in the colab link). However, in most of frameworks, in order for numerical stability, the product of probability will become the addition of log probability, and thus, the input probability will be transformed via log softmax first, and do the computation in logarithm domain. You can see the such transformation in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/ops/ctc_ops.py#L651&gt;this line&lt;/denchmark-link&gt;
.
In this kind of implementation (see the last cell in the colab link), the produced loss value is indeed 1.6543342. Note that pytorch (and cudnn backend) also follows the same implementation for numerical stability
&lt;denchmark-link:https://colab.research.google.com/drive/1DofmAw8wgsYYN166n-s80PPKA092U6F6?usp=sharing&gt;https://colab.research.google.com/drive/1DofmAw8wgsYYN166n-s80PPKA092U6F6?usp=sharing&lt;/denchmark-link&gt;

&lt;denchmark-link:https://www.cs.toronto.edu/~graves/icml_2006.pdf&gt;https://www.cs.toronto.edu/~graves/icml_2006.pdf&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='lriuui0x0' date='2020-07-14T14:59:54Z'>
		&lt;denchmark-link:https://github.com/lriuui0x0&gt;@lriuui0x0&lt;/denchmark-link&gt;
 As mentioned in the above comment, this is an expected behavior. Log probability is just added for stability so you can ignore it.
		</comment>
		<comment id='4' author='lriuui0x0' date='2020-07-14T20:31:26Z'>
		&lt;denchmark-link:https://github.com/WindQAQ&gt;@WindQAQ&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/gowthamkpr&gt;@gowthamkpr&lt;/denchmark-link&gt;
 Thanks for the response. The value is quite off from the actual value . Is it due to  being inaccurate?
		</comment>
		<comment id='5' author='lriuui0x0' date='2020-07-14T20:46:29Z'>
		
@WindQAQ @gowthamkpr Thanks for the response. The value is quite off from the actual value 0. Is it due to np.logaddexp being inaccurate?

No, it is due to log softmax will assign some small probability to zero probability. For example, in your case [0.0, 1.0, 0.0] after log softmax will become [-1.55144471 -0.55144471 -1.55144471]. Obviously, if you take exponent on it (convert it back to probability), they are all non-zero. If you use this probability (e^[-1.55144471 -0.55144471 -1.55144471]) to compute ctc loss, you can also obtain 1.6543342.
So to conclude,

Using log softmax to compute prob in logarithm domain.
Log softmax assigns some small value to zero probability. Think about [0.0, 1.0] -&gt; log_softmax([0.0, 1.0]) = [1 / (1 + e), e / (1 + e)] -&gt; take exponent -&gt; both non zero.

		</comment>
		<comment id='6' author='lriuui0x0' date='2020-07-14T21:12:52Z'>
		&lt;denchmark-link:https://github.com/WindQAQ&gt;@WindQAQ&lt;/denchmark-link&gt;
 OK so it's because of the way  is impelemented, it results in  instead of , which is mathematically more correct, but is numerically unstable / slower.
OK that makes sense then, do you know any reference on the theory behind why log softmax is implemented in that way?
		</comment>
		<comment id='7' author='lriuui0x0' date='2020-07-14T21:50:02Z'>
		
@WindQAQ OK so it's because of the way tf.nn.log_softmax is impelemented, it results in [-1.55144471 -0.55144471 -1.55144471] instead of [-inf 0 -inf], which is mathematically more correct, but is numerically unstable / slower.
OK that makes sense then, do you know any reference on the theory behind why log softmax is implemented in that way?

Umm, I am not sure about that, but it's not about how log_softmax is implemented, it is implemented as described in its name (take softmax and then take log) :-)
		</comment>
		<comment id='8' author='lriuui0x0' date='2020-07-14T23:22:15Z'>
		&lt;denchmark-link:https://github.com/WindQAQ&gt;@WindQAQ&lt;/denchmark-link&gt;
 If it's really implementing softmax then  log, why would  results in ?
		</comment>
		<comment id='9' author='lriuui0x0' date='2020-07-14T23:57:48Z'>
		Denote z = [0, 1, 0].
Let's first compute the numerator of softmax, that is, e^{z_i}.
We can have e^z = [1, e, 1]. The denominator is then sum_{i=0}^2 e^{z_i} = 2 + e. Thus, the softmax value is softmax(z) = [1 / (2 + e), e / (2 + e), 1 / (2 + e)]. Finally, taking the log on softmax yields log_softmax(z) = [-1.55144471, -0.55144471, -1.55144471].
		</comment>
		<comment id='10' author='lriuui0x0' date='2020-07-15T19:18:54Z'>
		&lt;denchmark-link:https://github.com/WindQAQ&gt;@WindQAQ&lt;/denchmark-link&gt;
 Oh yes, I forgot it's calculating softmax once again on top of softmax result. Thanks, we can close the issue now!
		</comment>
		<comment id='11' author='lriuui0x0' date='2020-07-15T19:18:56Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41353&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41353&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>