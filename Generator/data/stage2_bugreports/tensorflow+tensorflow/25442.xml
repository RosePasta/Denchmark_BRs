<bug id='25442' author='xudong2019' open_date='2019-02-01T23:52:27Z' closed_time='2019-03-10T01:41:32Z'>
	<summary>dynamic_decode reports error under eager_execution ? “ValueError: The inequality of unknown TensorShapes is undefined."</summary>
	<description>

TensorFlow version (use command below): 1.2

I have encountered a weird problem when transforming a usual seq2seq code into eager execution mode. After changing the placeholder input to numpy array,  by calling tf.contrib.seq2seq.dynamic_decode(training_decoder,impute_finished=True,maximum_iterations=max_target_sequence_length)
gives error “ValueError: The inequality of unknown TensorShapes is undefined."
Without activating eager execution error, everything is fine.
Code to reproduce the issue
The code requires two files from github:
&lt;denchmark-link:https://github.com/udacity/deep-learning/blob/master/seq2seq/data/letters_source.txt&gt;https://github.com/udacity/deep-learning/blob/master/seq2seq/data/letters_source.txt&lt;/denchmark-link&gt;

and
&lt;denchmark-link:https://github.com/udacity/deep-learning/tree/master/seq2seq/data/letters_target.txt&gt;https://github.com/udacity/deep-learning/tree/master/seq2seq/data/letters_target.txt&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;import tensorflow as tf
import tensorflow.contrib.eager as tfe
tfe.enable_eager_execution()

from distutils.version import LooseVersion
from tensorflow.python.layers.core import Dense

assert LooseVersion(tf.__version__) &gt;= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'
print('TensorFlow Version: {}'.format(tf.__version__))
import numpy as np
import time
import tensorflow as tf

with open('data/letters_source.txt', 'r', encoding='utf-8') as f:
    source_data = f.read()

with open('data/letters_target.txt', 'r', encoding='utf-8') as f:
    target_data = f.read()

def extract_character_vocab(data):
    special_words = ['&lt;PAD&gt;', '&lt;UNK&gt;', '&lt;GO&gt;',  '&lt;EOS&gt;']
    set_words = list(set([character for line in data.split('\n') for character in line]))
    int_to_vocab = {idx: word for idx, word in enumerate(special_words + set_words)}
    vocab_to_int = {word: idx for idx, word in int_to_vocab.items()}
    return int_to_vocab, vocab_to_int

source_int_to_letter, source_letter_to_int = extract_character_vocab(source_data)
target_int_to_letter, target_letter_to_int = extract_character_vocab(target_data)

source_int = [[source_letter_to_int.get(letter, source_letter_to_int['&lt;UNK&gt;']) 
               for letter in line] for line in source_data.split('\n')]
target_int = [[target_letter_to_int.get(letter, target_letter_to_int['&lt;UNK&gt;']) 
               for letter in line] + [target_letter_to_int['&lt;EOS&gt;']] for line in target_data.split('\n')] 

def get_batches(targets, sources, batch_size, source_pad_int, target_pad_int):
    for batch_i in range(0, len(sources)//batch_size):
        start_i = batch_i * batch_size
        sources_batch = sources[start_i:start_i + batch_size]
        targets_batch = targets[start_i:start_i + batch_size]
        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))
        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))
        pad_targets_lengths = []
        for target in pad_targets_batch:
            pad_targets_lengths.append(len(target))
        pad_source_lengths = []
        for source in pad_sources_batch:
            pad_source_lengths.append(len(source))
        yield pad_targets_batch, pad_sources_batch, pad_targets_lengths, pad_source_lengths
        
def pad_sentence_batch(sentence_batch, pad_int):
    max_sentence = max([len(sentence) for sentence in sentence_batch])
    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]

display_step = 50 # 每隔50轮输出loss
epochs = 60
batch_size = 128
rnn_size = 50
num_layers = 2
encoding_embedding_size = 15
decoding_embedding_size = 15
learning_rate = 0.001
checkpoint = "trained_model.ckpt" 
train_source = source_int[batch_size:]
train_target = target_int[batch_size:]
valid_source = source_int[:batch_size]
valid_target = target_int[:batch_size]
(valid_targets_batch, valid_sources_batch, valid_targets_lengths, valid_sources_lengths) = next(get_batches(valid_target, valid_source, batch_size,
                           source_letter_to_int['&lt;PAD&gt;'],target_letter_to_int['&lt;PAD&gt;']))
sess = tf.InteractiveSession()
tf.global_variables_initializer()

epoch_i = 1
batch_i = 0
(targets_batch, sources_batch, targets_lengths, sources_lengths) = next(get_batches(train_target, train_source, batch_size,source_letter_to_int['&lt;PAD&gt;'],target_letter_to_int['&lt;PAD&gt;']))

input_data = sources_batch
targets = targets_batch
lr = learning_rate
target_sequence_length = targets_lengths
source_sequence_length= sources_lengths
max_target_sequence_length = tf.reduce_max(target_sequence_length, name='max_target_len')

#Encoder
source_vocab_size = len(source_letter_to_int)
target_vocab_size = len(target_letter_to_int)
def get_lstm_cell(rnn_size):
    lstm_cell = tf.contrib.rnn.LSTMCell(rnn_size, initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))
    return lstm_cell

encoder_embed_input = tf.contrib.layers.embed_sequence(input_data, source_vocab_size, encoding_embedding_size)
cell = tf.contrib.rnn.MultiRNNCell([get_lstm_cell(rnn_size) for _ in range(num_layers)])
encoder_output, encoder_state = tf.nn.dynamic_rnn(cell, encoder_embed_input, sequence_length=source_sequence_length, dtype=tf.float32)

ending = tf.strided_slice(targets, [0, 0], [batch_size, -1], [1, 1])
decoder_input = tf.concat([tf.fill([batch_size, 1], target_letter_to_int['&lt;GO&gt;']), ending], 1)

target_vocab_size   = len(target_letter_to_int)
decoder_embeddings  = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))
decoder_embed_input = tf.nn.embedding_lookup(decoder_embeddings, decoder_input) 
def get_decoder_cell(rnn_size):
    decoder_cell = tf.contrib.rnn.LSTMCell(rnn_size,initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))
    return decoder_cell
cell         = tf.contrib.rnn.MultiRNNCell([get_decoder_cell(rnn_size) for _ in range(num_layers)])
output_layer = Dense(target_vocab_size,kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))

with tf.variable_scope("decode"):
    training_helper  = tf.contrib.seq2seq.TrainingHelper(inputs=decoder_embed_input,sequence_length=target_sequence_length,time_major=False)
    training_decoder = tf.contrib.seq2seq.BasicDecoder(cell,training_helper,encoder_state,output_layer) 
    training_decoder_output, _,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,impute_finished=True,maximum_iterations=max_target_sequence_length)
with tf.variable_scope("decode", reuse=True):
    start_tokens       = tf.tile(tf.constant([target_letter_to_int['&lt;GO&gt;']], dtype=tf.int32), [batch_size], name='start_tokens')
    predicting_helper  = tf.contrib.seq2seq.GreedyEmbeddingHelper(decoder_embeddings,start_tokens,target_letter_to_int['&lt;EOS&gt;'])
    predicting_decoder = tf.contrib.seq2seq.BasicDecoder(cell,predicting_helper,encoder_state,output_layer)
    predicting_decoder_output, _,_ = tf.contrib.seq2seq.dynamic_decode(predicting_decoder,impute_finished=True,maximum_iterations=max_target_sequence_length)

&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/2823741/letters_source.txt&gt;letters_source.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/2823742/letters_target.txt&gt;letters_target.txt&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='xudong2019' date='2019-02-13T10:56:44Z'>
		I have same problem, and I can add that if we use decoder manually: initialize and step, all is ok.
		</comment>
		<comment id='2' author='xudong2019' date='2019-02-22T23:20:09Z'>
		&lt;denchmark-link:https://github.com/xudong2019&gt;@xudong2019&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/MihailSalnikov&gt;@MihailSalnikov&lt;/denchmark-link&gt;
 Same problem. I checked the stack trace and decided that this might be a problem with the shape checking in _EagerTensorArray.
# python_ops.tensor_array_ops _EagerTensorArray
  def __init__(self,
               dtype,
               size=None,
               dynamic_size=None,
               clear_after_read=None,
               tensor_array_name=None,
               handle=None,
               flow=None,
               infer_shape=True,
               element_shape=None,
               colocate_with_first_write_call=True,
               name=None):
You need to modify the library file of tensorflow 1.12:
# seq2seq.python.ops.decoder
def dynamic_decode(......):
    def _create_ta(s, d):
      return tensor_array_ops.TensorArray(
          dtype=d,
          size=0 if dynamic_size else maximum_iterations,
          dynamic_size=dynamic_size,
          element_shape=_shape(decoder.batch_size, s),
          infer_shape=False # Add THIS LINE HERE
      )
		</comment>
		<comment id='3' author='xudong2019' date='2019-02-22T23:34:11Z'>
		Does this still happen in nightly?
If it does, I'll happily accept a pull request with the workaround proposed here.
		</comment>
		<comment id='4' author='xudong2019' date='2019-03-09T18:43:45Z'>
		It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue?
		</comment>
		<comment id='5' author='xudong2019' date='2019-03-10T01:41:31Z'>
		Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
		<comment id='6' author='xudong2019' date='2019-04-06T04:20:07Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 I also ran into this, please reopen.
		</comment>
		<comment id='7' author='xudong2019' date='2019-04-08T15:55:14Z'>
		&lt;denchmark-link:https://github.com/andportnoy&gt;@andportnoy&lt;/denchmark-link&gt;
 do you get this error on nightly or the 2.0 alpha preview? Instead of reopening this issue can you file a new one with instructions to reproduce if you do indeed get it on nightly?
		</comment>
	</comments>
</bug>