<bug id='18443' author='jiazhe0909' open_date='2018-04-12T03:16:33Z' closed_time='2020-01-24T22:35:16Z'>
	<summary>In Matmul, is FP16 x FP16 accumulated to FP16 or FP32?</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04
TensorFlow installed from (source or binary):
binary
TensorFlow version (use command below):
1.7
Python version:
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
9.0
GPU model and memory:
V100 16GB
Exact command to reproduce:

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
It is not clear in documents that, in Matmul, FP16xFP16 is accumulated to FP16 or FP32.
This choice affects not only training accuracy, but also TensorCore computing performance on Volta GPU.
	</description>
	<comments>
		<comment id='1' author='jiazhe0909' date='2018-04-27T18:41:18Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/jart&gt;@jart&lt;/denchmark-link&gt;
: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='2' author='jiazhe0909' date='2018-05-12T18:30:19Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/jart&gt;@jart&lt;/denchmark-link&gt;
: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='3' author='jiazhe0909' date='2018-05-15T00:52:26Z'>
		Thank you for the feedback on doc improvement. We'd welcome a contribution that makes our docs more clear on this matter.
		</comment>
		<comment id='4' author='jiazhe0909' date='2020-01-24T21:29:27Z'>
		&lt;denchmark-link:https://github.com/reedwm&gt;@reedwm&lt;/denchmark-link&gt;
 do you know the answer?
		</comment>
		<comment id='5' author='jiazhe0909' date='2020-01-24T22:35:16Z'>
		On GPUs, fp16 Matmul accumulation is done in fp32.
		</comment>
	</comments>
</bug>