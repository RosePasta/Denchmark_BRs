<bug id='32159' author='robertnishihara' open_date='2019-09-02T18:06:39Z' closed_time='2020-06-20T21:56:02Z'>
	<summary>TF 2.0 regression: cloudpickle cannot serialize tf.keras.Sequential.</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes (code included below in the issue)
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.3
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): pip
TensorFlow version (use command below): v2.0.0-beta1-5101-gc75bb66a99 2.0.0-rc0
Python version: Python 3.6.7 :: Anaconda, Inc.
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A

Using cloudpickle to serialize a Python function that uses tf.keras.Sequential fails with a recursion error.
Note that this works with tensorflow==1.14.0.
I imagine it also fails with other things, not just tf.keras.Sequential.
import cloudpickle  # cloudpickle.__version__ == '1.2.1'
import tensorflow as tf  # tf.__version__ == '2.0.0-rc0'

def f():
    tf.keras.Sequential

cloudpickle.loads(cloudpickle.dumps(f))  # This fails.
The last line fails with
&lt;denchmark-code&gt;---------------------------------------------------------------------------
RecursionError                            Traceback (most recent call last)
&lt;ipython-input-23-25cc307e6227&gt; in &lt;module&gt;
----&gt; 1 cloudpickle.loads(cloudpickle.dumps(f))

~/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py in __getattr__(self, item)
     48 
     49   def __getattr__(self, item):
---&gt; 50     module = self._load()
     51     return getattr(module, item)
     52 

~/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py in _load(self)
     42   def _load(self):
     43     """Import the target module and insert it into the parent's namespace."""
---&gt; 44     module = _importlib.import_module(self.__name__)
     45     self._parent_module_globals[self._local_name] = module
     46     self.__dict__.update(module.__dict__)

... last 2 frames repeated, from the frame below ...

~/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py in __getattr__(self, item)
     48 
     49   def __getattr__(self, item):
---&gt; 50     module = self._load()
     51     return getattr(module, item)
     52 

RecursionError: maximum recursion depth exceeded while calling a Python object
&lt;/denchmark-code&gt;

See &lt;denchmark-link:https://stackoverflow.com/questions/57750920/ray-tensorflow-gpu-2-0-recursionerror/57761034#57761034&gt;https://stackoverflow.com/questions/57750920/ray-tensorflow-gpu-2-0-recursionerror/57761034#57761034&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='robertnishihara' date='2019-09-03T05:46:16Z'>
		I have tried on colab with TF 1.14 and able to execute the code.However i am able to reproduce the issue with TF 2.0.0-rc0 and 2.0 nightly versions.Please, find the &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/af3b48253cc090e9c2048db71024d890/untitled143.ipynb&gt;gist &lt;/denchmark-link&gt;
here.Thanks!
		</comment>
		<comment id='2' author='robertnishihara' date='2019-09-03T18:47:16Z'>
		What is the goal of serialization here? We have several saving methods that allow you to save and revive a keras model that might be more appropriate here.
		</comment>
		<comment id='3' author='robertnishihara' date='2019-09-03T19:49:17Z'>
		&lt;denchmark-link:https://github.com/karmel&gt;@karmel&lt;/denchmark-link&gt;
 Note that this issue is not actually about serializing a keras model, but rather about serializing a function that creates a keras model. The APIs you refer to don't help in this case, I think.
Cloudpickle is the standard when it comes to general purpose serialization of arbitrary Python objects (including functions and classes). This is used by many distributed computing frameworks (like Ray, PySpark, Dask, IPython Parallel) that serialize arbitrary user-defined functions and ship them to remote worker processes to be executed.
As long as TensorFlow plays nicely with cloudpickle, then cloudpickle will be able to serialize arbitrary functions/classes that use TensorFlow. Serializing arbitrary functions/classes is most likely out of scope for TensorFlow, and so it makes sense to have cloudpickle handle that.
		</comment>
		<comment id='4' author='robertnishihara' date='2019-09-03T20:36:44Z'>
		&lt;denchmark-link:https://github.com/yifeif&gt;@yifeif&lt;/denchmark-link&gt;
 Can you take a look at the cloud pickle issue? It looks like it's getting caught up in an infinite recursion loop in LazyLoader
		</comment>
		<comment id='5' author='robertnishihara' date='2019-09-03T22:03:50Z'>
		Looks like we might need to handle ,  for the LazyLoader at virtual pip level? cc &lt;denchmark-link:https://github.com/mihaimaruseac&gt;@mihaimaruseac&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='robertnishihara' date='2019-09-03T22:14:41Z'>
		Seems that that is the case, __getstate__, __setstate__, __getinitargs__ and __getnewargs__.
I will send a fix later today/tomorrow.
		</comment>
		<comment id='7' author='robertnishihara' date='2019-09-04T18:02:42Z'>
		Update: the issue comes from the unpickling part, as shown from the script below:
_p = print
import cloudpickle  # cloudpickle.__version__ == '1.2.1'
import tensorflow as tf  # tf.__version__ == '2.0.0-rc0'

def f():
  _p("f() called")
  tf.keras.Sequential
  _p("f() ending")

_p("Dumping...")
s = cloudpickle.dumps(f)
_p("dumped, loading...")
cloudpickle.loads(s)
_p("done")
This outputs:
Dumping...
dumped, loading...
Traceback (most recent call last):
  File "test.py", line 13, in &lt;module&gt;
    cloudpickle.loads(s)
  File "/tmp/gh/1/lib/python3.6/site-packages/tensorflow/__init__.py", line 51, in __getattr__
    _p("{}.__getattr__({})".format(self._local_name, item))
  File "/tmp/gh/1/lib/python3.6/site-packages/tensorflow/__init__.py", line 51, in __getattr__
    _p("{}.__getattr__({})".format(self._local_name, item))
  File "/tmp/gh/1/lib/python3.6/site-packages/tensorflow/__init__.py", line 51, in __getattr__
    _p("{}.__getattr__({})".format(self._local_name, item))
  [Previous line repeated 330 more times]
RecursionError: maximum recursion depth exceeded while calling a Python object
Further investigation reveals that during unpickling __setattr__ needs to be called (equivalently, __setstate__ could be called but it needs at least one __setattr__ to store the new state on the module's __dict__). However, the lazy loading approach we're using assumes read only modules, we cannot add new attributes. Even defining an emtpy __setattr__ results in infinite recursion at the import tensorflow line.
		</comment>
		<comment id='8' author='robertnishihara' date='2019-09-04T18:31:03Z'>
		Another update:
tf.keras.Sequential, tf.keras and tf.estimator all result in the infinite recursion errors.
tf.math.sin doesn't.
		</comment>
		<comment id='9' author='robertnishihara' date='2019-09-04T19:14:33Z'>
		We cannot fix this in time for TF 2.0 final release. In fact, we cannot really fix this unless we give up Python 2 support, so we're looking at a fix that should come up by start of next year or so.
Sorry for the delay, but as we didn't support serialization via pickling we never tested if this functionality would get broken by our changes. We'll fix this in the future
		</comment>
		<comment id='10' author='robertnishihara' date='2019-09-05T17:07:09Z'>
		Thanks for the update &lt;denchmark-link:https://github.com/mihaimaruseac&gt;@mihaimaruseac&lt;/denchmark-link&gt;
. Fixing this in the future would be great. Out of curiosity, why does fixing it mean giving up support for Python 2?
		</comment>
		<comment id='11' author='robertnishihara' date='2019-09-05T17:10:15Z'>
		We are using a custom lazy loader object to mimic functionality that is present only in Python3.5 and later to create some modules on the fly.
		</comment>
		<comment id='12' author='robertnishihara' date='2019-10-15T23:29:12Z'>
		&lt;denchmark-link:https://github.com/mihaimaruseac&gt;@mihaimaruseac&lt;/denchmark-link&gt;
 I saw &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/4675891bd3c9e9ee7a57552486ec5bdc40379787&gt;4675891&lt;/denchmark-link&gt;
 . Is it relevant to this issue?
		</comment>
		<comment id='13' author='robertnishihara' date='2019-10-15T23:59:14Z'>
		I'll have to check this, as it is on a different path.
		</comment>
		<comment id='14' author='robertnishihara' date='2019-10-17T10:42:28Z'>
		Is there any type of workaround for this? Running Ray and TF 2.0 and now facing this issue. Would be great to see a fix for this any time soon, rather than next year.
		</comment>
		<comment id='15' author='robertnishihara' date='2019-10-23T15:07:00Z'>
		If you are just blocked by some framework that ships the serialized function you could bypass the tensorflow serialization by using importlib.import_module and then during de-serialization make sure the module you use is shipped/available in the PYTHONPATH.
Something like:
&lt;denchmark-code&gt;mymodule.py
def tf_fn():
   tf.keras.Sequential

def f():
    module = importlib.import_module("mymodule")
    return module.tf_fn()
&lt;/denchmark-code&gt;

In our use case to run distributed TensorFlow on Hadoop we provide a &lt;denchmark-link:https://github.com/criteo/tf-yarn/blob/master/tf_yarn/__init__.py#L566&gt;safe_experiment function &lt;/denchmark-link&gt;
 function and then we upload the TensorFlow functions inside a module to the cluster. This works as a workaround for the moment with tf2.
		</comment>
		<comment id='16' author='robertnishihara' date='2019-10-23T16:37:42Z'>
		&lt;denchmark-link:https://github.com/jharaldson&gt;@jharaldson&lt;/denchmark-link&gt;
 the easiest workaround might be the one described in &lt;denchmark-link:https://github.com/ray-project/ray/issues/5614#issuecomment-527292289&gt;ray-project/ray#5614 (comment)&lt;/denchmark-link&gt;
.
Another workaround is described in &lt;denchmark-link:https://stackoverflow.com/a/57761034/7858504&gt;https://stackoverflow.com/a/57761034/7858504&lt;/denchmark-link&gt;

		</comment>
		<comment id='17' author='robertnishihara' date='2019-10-23T17:02:46Z'>
		Coming back to the example at &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/32159#issuecomment-528016376&gt;#32159 (comment)&lt;/denchmark-link&gt;

In python2 all works
(py2) mihaimaruseac@ankh:/tmp/pickle/py2$ python test.py
Dumping...
dumped, loading...
done
In python3.5 the error is from PyCapsule objects:
(py35) mihaimaruseac@ankh:/tmp/pickle/py35$ python test.py
Dumping...
Traceback (most recent call last):
  File "test.py", line 11, in &lt;module&gt;
    s = cloudpickle.dumps(f)
  File "/tmp/pickle/py35/lib/python3.5/site-packages/cloudpickle/cloudpickle.py", line 1125, in dumps
    cp.dump(obj)
  File "/tmp/pickle/py35/lib/python3.5/site-packages/cloudpickle/cloudpickle.py", line 482, in dump
    return Pickler.dump(self, obj)
  File "/usr/lib/python3.5/pickle.py", line 408, in dump
    self.save(obj)
  File "/usr/lib/python3.5/pickle.py", line 475, in save
    f(self, obj) # Call unbound method with explicit self
  File "/tmp/pickle/py35/lib/python3.5/site-packages/cloudpickle/cloudpickle.py", line 556, in save_function
    return self.save_function_tuple(obj)
  File "/tmp/pickle/py35/lib/python3.5/site-packages/cloudpickle/cloudpickle.py", line 758, in save_function_tuple
    save(state)
  File "/usr/lib/python3.5/pickle.py", line 475, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/lib/python3.5/pickle.py", line 814, in save_dict
    self._batch_setitems(obj.items())
  File "/usr/lib/python3.5/pickle.py", line 840, in _batch_setitems
    save(v)
  File "/usr/lib/python3.5/pickle.py", line 475, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/lib/python3.5/pickle.py", line 774, in save_list
    self._batch_appends(obj)
  File "/usr/lib/python3.5/pickle.py", line 801, in _batch_appends
    save(tmp[0])
  File "/usr/lib/python3.5/pickle.py", line 520, in save
    self.save_reduce(obj=obj, *rv)
  File "/usr/lib/python3.5/pickle.py", line 627, in save_reduce
    save(state)
  File "/usr/lib/python3.5/pickle.py", line 475, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/lib/python3.5/pickle.py", line 814, in save_dict
    self._batch_setitems(obj.items())
  File "/usr/lib/python3.5/pickle.py", line 840, in _batch_setitems
    save(v)
  File "/usr/lib/python3.5/pickle.py", line 475, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/lib/python3.5/pickle.py", line 814, in save_dict
    self._batch_setitems(obj.items())
  File "/usr/lib/python3.5/pickle.py", line 840, in _batch_setitems
    save(v)
  File "/usr/lib/python3.5/pickle.py", line 495, in save
    rv = reduce(self.proto)
TypeError: can't pickle PyCapsule objects
In Python3.7 the error is from _LazyLoader
(py37) mihaimaruseac@ankh:/tmp/pickle/py37$ python test.py
Dumping...
Traceback (most recent call last):
  File "test.py", line 11, in &lt;module&gt;
    s = cloudpickle.dumps(f)
  File "/tmp/pickle/py37/lib/python3.7/site-packages/cloudpickle/cloudpickle.py", line 1125, in dumps
    cp.dump(obj)
  File "/tmp/pickle/py37/lib/python3.7/site-packages/cloudpickle/cloudpickle.py", line 482, in dump
    return Pickler.dump(self, obj)
  File "/usr/lib/python3.7/pickle.py", line 437, in dump
    self.save(obj)
  File "/usr/lib/python3.7/pickle.py", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File "/tmp/pickle/py37/lib/python3.7/site-packages/cloudpickle/cloudpickle.py", line 556, in save_function
    return self.save_function_tuple(obj)
  File "/tmp/pickle/py37/lib/python3.7/site-packages/cloudpickle/cloudpickle.py", line 758, in save_function_tuple
    save(state)
  File "/usr/lib/python3.7/pickle.py", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/lib/python3.7/pickle.py", line 856, in save_dict
    self._batch_setitems(obj.items())
  File "/usr/lib/python3.7/pickle.py", line 882, in _batch_setitems
    save(v)
  File "/usr/lib/python3.7/pickle.py", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/lib/python3.7/pickle.py", line 816, in save_list
    self._batch_appends(obj)
  File "/usr/lib/python3.7/pickle.py", line 843, in _batch_appends
    save(tmp[0])
  File "/usr/lib/python3.7/pickle.py", line 524, in save
    rv = reduce(self.proto)
TypeError: can't pickle _LazyLoader objects
Can't test python3.6 anymore due to an issue in my system.
Manually applying the fix from &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/4675891bd3c9e9ee7a57552486ec5bdc40379787&gt;4675891&lt;/denchmark-link&gt;
 to the other codepath makes that test pass in all 3 instances.
I'm running the change through more tests and will submit a fix.
		</comment>
		<comment id='18' author='robertnishihara' date='2019-10-23T17:37:49Z'>
		Let's wait for a new tf-nightly and test this again. Or, you can build from source, with 353b8a1
		</comment>
		<comment id='19' author='robertnishihara' date='2019-11-01T13:44:22Z'>
		Seems the fix works with Ray. However if we use custom layers with functions decorated with @tf.function there are still pickling issues. As a workaround for that I figured one could save the model as a "savedmodel" on a distributed storage and then have the ray worker load the model from the distributed storage, but this throws an error.
Note: Removing the LSTM layer does not result in an error, which would suggest that this error is related to the while operation (as the error suggests).
&lt;denchmark-code&gt;LookupError: No gradient defined for operation 'while' (op type: While)
&lt;/denchmark-code&gt;

Code to reproduce
&lt;denchmark-code&gt;import tensorflow as tf
import ray 
import numpy as np

ray.init()

def build_save_model():
    lstm_in = tf.keras.Input(shape=(24,1))
    lstm_out = tf.keras.layers.LSTM(6)(lstm_in)
    dense_out = tf.keras.layers.Dense(24)(lstm_out)
    model = tf.keras.Model([lstm_in], dense_out)
    model.save('/path/in/common/storage/lstm_model')

@ray.remote
class Worker():
    def __init__(self):
        self.model = tf.keras.models.load_model('/path/in/common/storage/lstm_model')
        self.model.compile(optimizer=tf.keras.optimizers.Adam(1e-1), loss=tf.keras.losses.mse)
        self.data = np.arange(24).reshape(1,24,1)
        self.label = np.arange(24).reshape(1,24)
        
    def train(self):
        history = self.model.fit(self.data, self.label, epochs=10)
        return history.history
        
build_save_model()
lstm_worker = Worker.remote()
w = ray.get(lstm_worker.train.remote())
&lt;/denchmark-code&gt;

Error
&lt;denchmark-code&gt;---------------------------------------------------------------------------
RayTaskError                              Traceback (most recent call last)
&lt;ipython-input-3-a18941ca631a&gt; in &lt;module&gt;
     22 build_save_model()
     23 lstm_worker = Worker.remote()
---&gt; 24 w = ray.get(lstm_worker.train.remote())

/opt/conda/lib/python3.6/site-packages/ray/worker.py in get(object_ids)
   2245             if isinstance(value, RayError):
   2246                 last_task_error_raise_time = time.time()
-&gt; 2247                 raise value
   2248 
   2249         # Run post processors.

RayTaskError: ray_worker (pid=1397, host=thesis-clustering-7dfb7867df-pk5fc)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 2326, in get_attr
    c_api.TF_OperationGetAttrValueProto(self._c_op, name, buf)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Operation 'StatefulPartitionedCall' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

ray_worker (pid=1397, host=thesis-clustering-7dfb7867df-pk5fc)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py", line 331, in _MaybeCompile
    xla_compile = op.get_attr("_XlaCompile")
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 2330, in get_attr
    raise ValueError(str(e))
ValueError: Operation 'StatefulPartitionedCall' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

ray_worker (pid=1397, host=thesis-clustering-7dfb7867df-pk5fc)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 2326, in get_attr
    c_api.TF_OperationGetAttrValueProto(self._c_op, name, buf)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Operation 'StatefulPartitionedCall' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

ray_worker (pid=1397, host=thesis-clustering-7dfb7867df-pk5fc)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py", line 331, in _MaybeCompile
    xla_compile = op.get_attr("_XlaCompile")
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 2330, in get_attr
    raise ValueError(str(e))
ValueError: Operation 'StatefulPartitionedCall' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

ray_worker (pid=1397, host=thesis-clustering-7dfb7867df-pk5fc)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py", line 607, in _GradientsHelper
    grad_fn = ops.get_gradient_function(op)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 2495, in get_gradient_function
    return _gradient_registry.lookup(op_type)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/registry.py", line 97, in lookup
    "%s registry has no entry for: %s" % (self._name, name))
LookupError: gradient registry has no entry for: While

During handling of the above exception, another exception occurred:

ray_worker (pid=1397, host=thesis-clustering-7dfb7867df-pk5fc)
  File "&lt;ipython-input-3-a18941ca631a&gt;", line 19, in train
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py", line 785, in fit
    use_multiprocessing=use_multiprocessing)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 337, in fit
    total_epochs=epochs)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 127, in run_one_epoch
    batch_outs = execution_function(iterator)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py", line 86, in execution_function
    distributed_function(input_fn))
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", line 568, in __call__
    result = self._call(*args, **kwds)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", line 615, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", line 497, in _initialize
    *args, **kwds))
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 2366, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 2675, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 2565, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py", line 974, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", line 439, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py", line 73, in distributed_function
    per_replica_function, args=(x, y, sample_weights))
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py", line 763, in experimental_run_v2
    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py", line 1819, in call_for_each_replica
    return self._call_for_each_replica(fn, args, kwargs)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py", line 2164, in _call_for_each_replica
    return fn(*args, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py", line 292, in wrapper
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py", line 264, in train_on_batch
    output_loss_metrics=model._output_loss_metrics)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py", line 312, in train_on_batch
    output_loss_metrics=output_loss_metrics))
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py", line 269, in _process_single_batch
    grads = tape.gradient(scaled_total_loss, trainable_weights)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py", line 1029, in gradient
    unconnected_gradients=unconnected_gradients)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/imperative_grad.py", line 77, in imperative_grad
    compat.as_str(unconnected_gradients.value))
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 766, in _backward_function
    return self._rewrite_forward_and_call_backward(call_op, *args)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 685, in _rewrite_forward_and_call_backward
    forward_function, backwards_function = self.forward_backward(len(doutputs))
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 594, in forward_backward
    forward, backward = self._construct_forward_backward(num_doutputs)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 642, in _construct_forward_backward
    func_graph=backwards_graph)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py", line 974, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 632, in _backprop_function
    src_graph=self._func_graph)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py", line 669, in _GradientsHelper
    lambda: grad_fn(op, *out_grads))
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py", line 336, in _MaybeCompile
    return grad_fn()  # Exit early
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py", line 669, in &lt;lambda&gt;
    lambda: grad_fn(op, *out_grads))
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 685, in _rewrite_forward_and_call_backward
    forward_function, backwards_function = self.forward_backward(len(doutputs))
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 594, in forward_backward
    forward, backward = self._construct_forward_backward(num_doutputs)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 642, in _construct_forward_backward
    func_graph=backwards_graph)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py", line 974, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 632, in _backprop_function
    src_graph=self._func_graph)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py", line 669, in _GradientsHelper
    lambda: grad_fn(op, *out_grads))
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py", line 336, in _MaybeCompile
    return grad_fn()  # Exit early
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py", line 669, in &lt;lambda&gt;
    lambda: grad_fn(op, *out_grads))
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 685, in _rewrite_forward_and_call_backward
    forward_function, backwards_function = self.forward_backward(len(doutputs))
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 594, in forward_backward
    forward, backward = self._construct_forward_backward(num_doutputs)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 642, in _construct_forward_backward
    func_graph=backwards_graph)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py", line 974, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 632, in _backprop_function
    src_graph=self._func_graph)
  File "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py", line 623, in _GradientsHelper
    (op.name, op.type))
LookupError: No gradient defined for operation 'while' (op type: While)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='20' author='robertnishihara' date='2019-11-01T15:33:41Z'>
		Can you run a model with while but without pickling/unpickling? Afaik, while doesn't have gradients (maybe with gradient tape but then I don't know if those get pickled anyway)
		</comment>
		<comment id='21' author='robertnishihara' date='2019-11-02T12:01:36Z'>
		Two examples that runs without errors:


the above example commenting out the @ray.remote decorator and call the train function without the remote call.


the above example adding a return statement to the build_save_model() to return the built model. We swap out tf.keras.models.load_model() in the Worker to self.model = build_save_model() and call train()


One example that runs with error:

we build and save the model in the Worker (as part of remote call) and tries to load the saved model in the main python session (not remote)

		</comment>
		<comment id='22' author='robertnishihara' date='2020-03-19T22:10:15Z'>
		Any updates on this?
		</comment>
		<comment id='23' author='robertnishihara' date='2020-04-20T14:38:58Z'>
		Are there any updates regarding this issue? Has there been a fix (such as &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/353b8a1adcb471a48ef9b1c5cbfc6097d036473e&gt;353b8a1&lt;/denchmark-link&gt;
) applied to tf 1.15?
		</comment>
		<comment id='24' author='robertnishihara' date='2020-04-20T17:35:30Z'>
		No, but if you want to make a cherry-pick we can merge it if and when we do a new patch release on 1.15
		</comment>
		<comment id='25' author='robertnishihara' date='2020-04-29T18:22:47Z'>
		&lt;denchmark-link:https://github.com/mihaimaruseac&gt;@mihaimaruseac&lt;/denchmark-link&gt;
 I opened &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/39034&gt;#39034&lt;/denchmark-link&gt;
 for this.
		</comment>
		<comment id='26' author='robertnishihara' date='2020-06-20T21:56:02Z'>
		I think this can be closed now as it has been solved and backported to 1.15 too.
		</comment>
		<comment id='27' author='robertnishihara' date='2020-06-20T21:56:04Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32159&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32159&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>