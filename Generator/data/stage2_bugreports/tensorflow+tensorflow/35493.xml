<bug id='35493' author='ChunHsinWang' open_date='2019-12-30T09:38:05Z' closed_time='2020-06-11T02:01:07Z'>
	<summary>tf.recompute_grad() throws dimension mismatched error when concatenating tensors with different number of channels</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.0
Python version: 3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 10.0 / 7.6
GPU model and memory: GTX 1080 / 8GB

Describe the current behavior
My model is a segmentation model with DenseNet like structure. There are many concatenate operations between the encoder and decoder tensors. I want to recompute the gradients on these concatenate layers during backpropagation to limit the amount of GPU memory usage. A similar approach can be found here : &lt;denchmark-link:https://github.com/joeyearsley/efficient_densenet_tensorflow&gt;https://github.com/joeyearsley/efficient_densenet_tensorflow&lt;/denchmark-link&gt;
.  I tried to use tf.recompute_grad() on a wrapper function which has a concatenate layer inside but it would raise an error when the channel dimensions of input tensors are not matched.
Describe the expected behavior
The concatenate layer should not raise an error when concatenating inputs with different number of channels.

Provide a reproducible test case that is the bare minimum necessary to generate the problem.
&lt;denchmark-link:https://colab.research.google.com/drive/1d7zSGbYmocnupUpDIJLnlXt-3vH5bi83#scrollTo=xcPT-MWZAuvK&amp;uniqifier=1&gt;https://colab.research.google.com/drive/1d7zSGbYmocnupUpDIJLnlXt-3vH5bi83#scrollTo=xcPT-MWZAuvK&amp;uniqifier=1&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;Other info / logs&lt;/denchmark-h&gt;

InvalidArgumentError                      Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)
1609   try:
-&gt; 1610     c_op = c_api.TF_FinishOperation(op_desc)
1611   except errors.InvalidArgumentError as e:
InvalidArgumentError: Dimension 3 in both shapes must be equal, but are 16 and 32. Shapes are [?,30,30,16] and [?,30,30,32].
From merging shape 0 with other shapes. for 'packed_7' (op: 'Pack') with input shapes: [?,30,30,16], [?,30,30,32].
During handling of the above exception, another exception occurred:
ValueError                                Traceback (most recent call last)
16 frames
 in ()
----&gt; 1 model = test_model()
2 history = model.fit(train_images, train_labels, epochs=10,
3                     validation_data=(test_images, test_labels))
 in test_model()
4     x_list.append(layers.Conv2D(16, (3,3), activation='relu')(x_in))
5     x_list.append(layers.Conv2D(32, (3,3), activation='relu')(x_in))
----&gt; 6     x = efficient_concat(x_list)
7     x = layers.Flatten()(x)
8     x = layers.Dense(10, activation='softmax')(x)
 in efficient_concat(input_list)
4         return x
5     wraper = tf.recompute_grad(wraper)
----&gt; 6     return wraper(input_list)
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/custom_gradient.py in decorated(*args, **kwargs)
164     """Decorated function with custom gradient."""
165     if context.executing_eagerly():
--&gt; 166       return _eager_mode_decorator(f, *args, **kwargs)
167     else:
168       return _graph_mode_decorator(f, *args, **kwargs)
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/custom_gradient.py in _eager_mode_decorator(f, *args, **kwargs)
333
334   input_tensors = [ops.convert_to_tensor(x) for x
--&gt; 335                    in list(args) + list(variables)]
336   arg_count = len(args)
337   def actual_grad_fn(*result_grads):
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/custom_gradient.py in (.0)
332   flat_result = [gen_array_ops.identity(x) for x in flat_result]
333
--&gt; 334   input_tensors = [ops.convert_to_tensor(x) for x
335                    in list(args) + list(variables)]
336   arg_count = len(args)
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype, dtype_hint)
1182   preferred_dtype = deprecation.deprecated_argument_lookup(
1183       "dtype_hint", dtype_hint, "preferred_dtype", preferred_dtype)
-&gt; 1184   return convert_to_tensor_v2(value, dtype, preferred_dtype, name)
1185
1186
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)
1240       name=name,
1241       preferred_dtype=dtype_hint,
-&gt; 1242       as_ref=False)
1243
1244
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_composite_tensors)
1294
1295     if ret is None:
-&gt; 1296       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
1297
1298     if ret is NotImplemented:
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py in _autopacking_conversion_function(v, dtype, name, as_ref)
1276   elif dtype != inferred_dtype:
1277     v = nest.map_structure(_cast_nested_seqs_to_dtype(dtype), v)
-&gt; 1278   return _autopacking_helper(v, dtype, name or "packed")
1279
1280
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py in _autopacking_helper(list_or_tuple, dtype, name)
1182     # checking.
1183     if all(ops.is_dense_tensor_like(elem) for elem in list_or_tuple):
-&gt; 1184       return gen_array_ops.pack(list_or_tuple, name=name)
1185   must_pack = False
1186   converted_elems = []
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_array_ops.py in pack(values, axis, name)
6302   axis = _execute.make_int(axis, "axis")
6303   _, _, _op = _op_def_lib._apply_op_helper(
-&gt; 6304         "Pack", values=values, axis=axis, name=name)
6305   _result = _op.outputs[:]
6306   _inputs_flat = _op.inputs
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
791         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,
792                          input_types=input_types, attrs=attr_protos,
--&gt; 793                          op_def=op_def)
794       return output_structure, op_def.is_stateful, op
795
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py in create_op(failed resolving arguments)
546     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access
547         op_type, inputs, dtypes, input_types, name, attrs, op_def,
--&gt; 548         compute_device)
549
550   def capture(self, tensor, name=None):
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in _create_op_internal(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)
3427           input_types=input_types,
3428           original_op=self._default_original_op,
-&gt; 3429           op_def=op_def)
3430       self._create_op_helper(ret, compute_device=compute_device)
3431     return ret
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in init(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)
1771           op_def, inputs, node_def.attr)
1772       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,
-&gt; 1773                                 control_input_ops)
1774     # pylint: enable=protected-access
1775
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)
1611   except errors.InvalidArgumentError as e:
1612     # Convert to ValueError for backwards compatibility.
-&gt; 1613     raise ValueError(str(e))
1614
1615   return c_op
ValueError: Dimension 3 in both shapes must be equal, but are 16 and 32. Shapes are [?,30,30,16] and [?,30,30,32].
From merging shape 0 with other shapes. for 'packed_7' (op: 'Pack') with input shapes: [?,30,30,16], [?,30,30,32].
	</description>
	<comments>
		<comment id='1' author='ChunHsinWang' date='2019-12-31T05:47:53Z'>
		Issue replicating when tried replicating for code given in the gist of colab for TF-2.0 and tf-nightly. Thanks!
		</comment>
		<comment id='2' author='ChunHsinWang' date='2020-01-03T23:22:38Z'>
		&lt;denchmark-link:https://github.com/ChunHsinWang&gt;@ChunHsinWang&lt;/denchmark-link&gt;
 Correct me if I am wrong but The weights of the model depend on the number of channels. Changing channels is changing weights. Changing weights is having a completely new model.
		</comment>
		<comment id='3' author='ChunHsinWang' date='2020-01-04T05:52:25Z'>
		&lt;denchmark-link:https://github.com/gowthamkpr&gt;@gowthamkpr&lt;/denchmark-link&gt;
 thanks for the reply, but concatenate layer does not change the number of channels, it merges channels from the list of input tensors, so the total number of weights in the model does not change.  To my understanding, TensorFlow's concatenate layer will always allocate new memory and copy from the input tensors, and the memory consumption grows quadratically I think. What I'm trying to achieve is to use tf.recompute_grad for having concatenate layers to use a shared memory space, so the output of every concatenate layer does not allocate new memory. During the backward pass, these outputs will have to be recalculated for calculating the gradients. The following figure from this paper &lt;denchmark-link:https://arxiv.org/abs/1707.06990&gt;https://arxiv.org/abs/1707.06990&lt;/denchmark-link&gt;
 demonstrates the process.
&lt;denchmark-link:https://user-images.githubusercontent.com/20900499/71760456-fe36e900-2ef8-11ea-8b07-a57e14e127d8.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='ChunHsinWang' date='2020-04-17T06:43:32Z'>
		Issue replicating when tried replicating for code given in the gist of colab for TF 2.2.0-rc3.Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/6baee6feddff579b09550fef5cb6bb4a/untitled782.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='5' author='ChunHsinWang' date='2020-06-10T14:06:50Z'>
		&lt;denchmark-link:https://github.com/ChunHsinWang&gt;@ChunHsinWang&lt;/denchmark-link&gt;

I am not seeing any issue with nightly version ().Please, find the gist &lt;denchmark-link:https://colab.research.google.com/gist/ravikyram/95ec843df15f1fdfc3a88992db8f7dec/untitled56.ipynb&gt;here&lt;/denchmark-link&gt;
.Please, verify once and close the issue.Thanks!
		</comment>
		<comment id='6' author='ChunHsinWang' date='2020-06-11T02:01:06Z'>
		The issue seems to be fixed, so I will close it. Thanks.
		</comment>
		<comment id='7' author='ChunHsinWang' date='2020-06-11T02:01:08Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35493&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35493&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>