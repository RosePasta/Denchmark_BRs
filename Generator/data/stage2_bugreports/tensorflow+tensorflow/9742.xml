<bug id='9742' author='sirfz' open_date='2017-05-07T20:20:30Z' closed_time='2017-06-05T16:23:17Z'>
	<summary>Optimized compiled Tensorflow uses almost 2x more memory than non-optimized binary</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): Both
TensorFlow version (use command below): ('v1.0.0-65-g4763edf-dirty', '1.0.1') (compiled at the same git commit of the binary version)
Bazel version (if compiling from source): 0.4.5
CUDA/cuDNN version: N/A (CPU only)
GPU model and memory: N/A
Exact command to reproduce:

import pdb
import numpy as np
import tensorflow as tf

def get_layer(input_size, output_size, name):
    # W_val is loaded from a file using numpy.load
    W_val = np.random.normal(scale=0.1, size=(input_size, output_size)).astype(np.float32)
    W = tf.get_variable(name='W_{}'.format(name), shape=(input_size, output_size),
                        initializer=tf.constant_initializer(value=W_val, dtype=tf.float32),
                        dtype=tf.float32)
    b = tf.get_variable(name='b_{}'.format(name), shape=(output_size,),
                        initializer=tf.constant_initializer(value=0.1, dtype=tf.float32),
                        dtype=tf.float32)
    return W, b

sessions = []
for i in range(3):
    g = tf.Graph()
    with g.as_default():
        W1, b1 = get_layer(158238, 900, '1')
        W2, b2 = get_layer(900, 1000, '2')
        W3, b3 = get_layer(1000, 1, '3')

        init = tf.global_variables_initializer()
    session = tf.Session(graph=g)
    session.run(init)
    print 'Loaded {}'.format(i)
    sessions.append(session)

pdb.set_trace()
&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

After running the code snippet under the non-optimized binary Tensorflow installation, the used memory is ~6GB. However, when the same snippet is run with Tensorflow compiled with -c opt --copt=-march=native directives, the memory usage is ~11GB (1.8x larger).
Note that there's no memory-usage difference when I use tf.truncated_normal(stddev=0.1...) instead of tf.constant_initializer with a numpy array.
Not sure if this is a bug, or a side-effect of the optimized version or if there's something I can do to optimize memory usage?
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

I can attach chrome traces if necessary.
	</description>
	<comments>
		<comment id='1' author='sirfz' date='2017-05-07T23:14:29Z'>
		&lt;denchmark-link:https://github.com/tfboyd&gt;@tfboyd&lt;/denchmark-link&gt;
 is there anyone who could take a look?
		</comment>
		<comment id='2' author='sirfz' date='2017-05-07T23:16:17Z'>
		Could you try to disable jemalloc?
		</comment>
		<comment id='3' author='sirfz' date='2017-05-07T23:22:12Z'>
		Which CPU are you using?  If we repro, I would like to compile with the same optimizations.  Thank you.  I ask because if I repo with AVX but the issue is with AVX2 for example I would not see it.
		</comment>
		<comment id='4' author='sirfz' date='2017-05-08T14:19:30Z'>
		Haven't tried with jemalloc disabled yet but to answer the question about CPU it's an Amazon ec2 instance with Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz.
		</comment>
		<comment id='5' author='sirfz' date='2017-05-08T14:51:30Z'>
		Just re-compiled with TF_NEED_JEMALLOC environment variable set to 0, memory usage is the same (i.e. equal to the compiled version with jemalloc enabled).
		</comment>
		<comment id='6' author='sirfz' date='2017-05-08T14:56:37Z'>
		What if you run with tcmalloc?

sudo apt-get install google-perftools
export LD_PRELOAD="/usr/lib/libtcmalloc.so.4"
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Mon, May 8, 2017 at 7:53 AM, Fayez ***@***.***&gt; wrote:
 Just re-compiled with TF_NEED_JEMALLOC environment variable set to 0,
 memory usage is the same (i.e. equal to the compiled version with jemalloc
 enabled).

 —
 You are receiving this because you are subscribed to this thread.
 Reply to this email directly, view it on GitHub
 &lt;#9742 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AYoxVP-OdUbvKvzohzA4BcwrxC0AgK-mks5r3yxmgaJpZM4NTSLO&gt;
 .



		</comment>
		<comment id='7' author='sirfz' date='2017-05-08T15:24:30Z'>
		Tested with tcmalloc on both jemalloc enabled and disabled versions, memory usage is the same as well. Here's the output log with tcmalloc:
&lt;denchmark-code&gt;~# LD_PRELOAD="/usr/lib/libtcmalloc.so.4" python test.py
tcmalloc: large alloc 1139318784 bytes == 0x3488000 @  0x7f268db1fd1b 0x7f268be9835c 0x7f268becfe73 0x7f268bed2b19 0x7f268bf4b1e8 0x7f263b52ece5 0x7f263b571c00 0x4cada2 0x4c9d8f 0x4c2765 0x4c2509 0x4f1def 0x4ec652 0x4eae31 0x49e14a 0x7f268d527830 0x49d9d9 (nil)
Loaded 0
Loaded 1
Loaded 2
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='sirfz' date='2017-05-08T16:56:52Z'>
		Great! Could you follow the &lt;denchmark-link:http://goog-perftools.sourceforge.net/doc/heap_profiler.html&gt;pprof instructions&lt;/denchmark-link&gt;
 to understand what does the big allocations?
		</comment>
		<comment id='9' author='sirfz' date='2017-05-08T19:27:38Z'>
		I ran the profiler tool against the pip version and the compiled version. First, here's the printed messages for each:
&lt;denchmark-h:h3&gt;Pip version&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;~# LD_PRELOAD="/usr/lib/libtcmalloc.so.4" HEAPPROFILE=/root/tf_profile python test.py
Starting tracking the heap
Starting tracking the heap
tcmalloc: large alloc 1139318784 bytes == 0x4326000 @  0x7fa0319cfd1b 0x7fa02fa7835c 0x7fa02faafe73 0x7fa02fab2b19 0x7fa02fb2b1e8 0x7f9fdef7ece5 0x7f9fdefc1c00 0x4cada2 0x4c9d8f 0x4c2765 0x4c2509 0x4f1def 0x4ec652 0x4eae31 0x49e14a 0x7fa0313d7830 0x49d9d9 (nil)
Dumping heap profile to /root/tf_profile.0001.heap (1387 MB allocated cumulatively, 1105 MB currently in use)
Dumping heap profile to /root/tf_profile.0002.heap (1649 MB currently in use)
Dumping heap profile to /root/tf_profile.0003.heap (3016 MB allocated cumulatively, 1649 MB currently in use)
Dumping heap profile to /root/tf_profile.0004.heap (2192 MB currently in use)
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Dumping heap profile to /root/tf_profile.0005.heap (4676 MB allocated cumulatively, 1657 MB currently in use)
Dumping heap profile to /root/tf_profile.0006.heap (5769 MB allocated cumulatively, 1657 MB currently in use)
Dumping heap profile to /root/tf_profile.0007.heap (Exiting, 561 MB in use)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Compiled version&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;~# LD_PRELOAD="/usr/lib/libtcmalloc.so.4" HEAPPROFILE=/root/tf_profile python test.py
Starting tracking the heap
Starting tracking the heap
tcmalloc: large alloc 1139318784 bytes == 0x3c70000 @  0x7f4c16a35d1b 0x7f4c14ade35c 0x7f4c14b15e73 0x7f4c14b18b19 0x7f4c14b911e8 0x7f4bc3fd4ce5 0x7f4bc4017c00 0x4cada2 0x4c9d8f 0x4c2765 0x4c2509 0x4f1def 0x4ec652 0x4eae31 0x49e14a 0x7f4c1643d830 0x49d9d9 (nil)
Dumping heap profile to /root/tf_profile.0001.heap (1390 MB allocated cumulatively, 1105 MB currently in use)
Dumping heap profile to /root/tf_profile.0002.heap (1649 MB currently in use)
Dumping heap profile to /root/tf_profile.0003.heap (3020 MB allocated cumulatively, 1649 MB currently in use)
Dumping heap profile to /root/tf_profile.0004.heap (2192 MB currently in use)
Dumping heap profile to /root/tf_profile.0005.heap (4680 MB allocated cumulatively, 1657 MB currently in use)
Dumping heap profile to /root/tf_profile.0006.heap (5777 MB allocated cumulatively, 2754 MB currently in use)
Dumping heap profile to /root/tf_profile.0007.heap (6867 MB allocated cumulatively, 2751 MB currently in use)
Dumping heap profile to /root/tf_profile.0008.heap (7960 MB allocated cumulatively, 2751 MB currently in use)
Dumping heap profile to /root/tf_profile.0009.heap (Exiting, 561 MB in use)
&lt;/denchmark-code&gt;

Up until heap 4 in both cases, memory usage is the same and both graphs (generated by google-pprof tool) are exactly the same based on a quick look.
At heap 5, the memory usage is still the same but in the compiled version's case I see some parallelization in the graph where there are 2 nodes with ~540MB each instead of 1 node with 1090MB in the pip version's case.
Heap 6 is where the memory usage doubles in the compiled version's case. In the pip version there's 1 big unnamed node with 1637.6MB. However, in the compiled version there are 3 nodes with ~500MB each (initcmath + 2 other unnamed nodes) + a fourth large node called tensorflow PartialRunSetupRequest MergePartialFromCodedStream with 1093.8MB usage. I didn't see this particular node in any of the heap files of the pip-version. I can attach a screenshot but it becomes unreadable when I zoom out enough to view the whole thing. Let me know if you'd like me to attach the heap files.
In heap 7 of the compiled version, that same node appears with double the memory as well (2183.7MB).
I used a simplified version of my original snippet to generate the heap files:
import numpy as np
import tensorflow as tf

def get_layer(input_size, output_size, name):
    # supposedly loaded from a saved file
    W_val = np.random.normal(scale=0.1, size=(input_size, output_size)).astype(np.float32)
    W = tf.get_variable(name='W_{}'.format(name), shape=(input_size, output_size),
                        initializer=tf.constant_initializer(value=W_val, dtype=tf.float32),
                        dtype=tf.float32)
    b = tf.get_variable(name='b_{}'.format(name), shape=(output_size,),
                        initializer=tf.constant_initializer(value=0.1, dtype=tf.float32),
                        dtype=tf.float32)
    return W, b

g = tf.Graph()
with g.as_default():
    W1, b1 = get_layer(158238, 900, '1')
    W2, b2 = get_layer(900, 1000, '2')
    W3, b3 = get_layer(1000, 1, '3')
    init = tf.global_variables_initializer()
session = tf.Session(graph=g)
session.run(init)
session.close()
		</comment>
		<comment id='10' author='sirfz' date='2017-05-08T22:59:42Z'>
		Could it be that one of the version is using Python implementation of
protobuf instead of cpp?

Try this
python -c "from google.protobuf.internal import api_implementation;
print(api_implementation._default_implementation_type)"

It should print "cpp"
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Mon, May 8, 2017 at 12:30 PM, Fayez ***@***.***&gt; wrote:
 I ran the profiler tool against the pip version and the compiled version.
 First, here's the printed messages for each:
 Pip version

 ~# LD_PRELOAD="/usr/lib/libtcmalloc.so.4" HEAPPROFILE=/root/tf_profile python test.py
 Starting tracking the heap
 Starting tracking the heap
 tcmalloc: large alloc 1139318784 bytes == 0x4326000 @  0x7fa0319cfd1b 0x7fa02fa7835c 0x7fa02faafe73 0x7fa02fab2b19 0x7fa02fb2b1e8 0x7f9fdef7ece5 0x7f9fdefc1c00 0x4cada2 0x4c9d8f 0x4c2765 0x4c2509 0x4f1def 0x4ec652 0x4eae31 0x49e14a 0x7fa0313d7830 0x49d9d9 (nil)
 Dumping heap profile to /root/tf_profile.0001.heap (1387 MB allocated cumulatively, 1105 MB currently in use)
 Dumping heap profile to /root/tf_profile.0002.heap (1649 MB currently in use)
 Dumping heap profile to /root/tf_profile.0003.heap (3016 MB allocated cumulatively, 1649 MB currently in use)
 Dumping heap profile to /root/tf_profile.0004.heap (2192 MB currently in use)
 W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
 W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
 W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
 W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
 W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
 W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
 Dumping heap profile to /root/tf_profile.0005.heap (4676 MB allocated cumulatively, 1657 MB currently in use)
 Dumping heap profile to /root/tf_profile.0006.heap (5769 MB allocated cumulatively, 1657 MB currently in use)
 Dumping heap profile to /root/tf_profile.0007.heap (Exiting, 561 MB in use)

 Compiled version

 ~# LD_PRELOAD="/usr/lib/libtcmalloc.so.4" HEAPPROFILE=/root/tf_profile python test.py
 Starting tracking the heap
 Starting tracking the heap
 tcmalloc: large alloc 1139318784 bytes == 0x3c70000 @  0x7f4c16a35d1b 0x7f4c14ade35c 0x7f4c14b15e73 0x7f4c14b18b19 0x7f4c14b911e8 0x7f4bc3fd4ce5 0x7f4bc4017c00 0x4cada2 0x4c9d8f 0x4c2765 0x4c2509 0x4f1def 0x4ec652 0x4eae31 0x49e14a 0x7f4c1643d830 0x49d9d9 (nil)
 Dumping heap profile to /root/tf_profile.0001.heap (1390 MB allocated cumulatively, 1105 MB currently in use)
 Dumping heap profile to /root/tf_profile.0002.heap (1649 MB currently in use)
 Dumping heap profile to /root/tf_profile.0003.heap (3020 MB allocated cumulatively, 1649 MB currently in use)
 Dumping heap profile to /root/tf_profile.0004.heap (2192 MB currently in use)
 Dumping heap profile to /root/tf_profile.0005.heap (4680 MB allocated cumulatively, 1657 MB currently in use)
 Dumping heap profile to /root/tf_profile.0006.heap (5777 MB allocated cumulatively, 2754 MB currently in use)
 Dumping heap profile to /root/tf_profile.0007.heap (6867 MB allocated cumulatively, 2751 MB currently in use)
 Dumping heap profile to /root/tf_profile.0008.heap (7960 MB allocated cumulatively, 2751 MB currently in use)
 Dumping heap profile to /root/tf_profile.0009.heap (Exiting, 561 MB in use)

 Up until heap 4 in both cases, memory usage is the same and both graphs
 (generated by google-pprof tool) are exactly the same based on a quick
 look.

 At heap 5, the memory usage is still the same but in the compiled
 version's case I see some parallelization in the graph where there are 2
 nodes with ~540MB each instead of 1 node with 1090MB in the pip version's
 case.

 Heap 6 is where the memory usage doubles in the compiled version's case.
 In the pip version there's 1 big unnamed node with 1637.6MB. However, in
 the compiled version there are 3 nodes with ~500MB each (initcmath + 2
 other unnamed nodes) + a fourth large node called tensorflow
 PartialRunSetupRequest MergePartialFromCodedStream with 1093.8MB usage. I
 didn't see this particular node in any of the heap files of the
 pip-version. I can attach a screenshot but it becomes unreadable when I
 zoom out enough to view the whole thing. Let me know if you'd like me to
 attach the heap files.

 In heap 7 of the compiled version, that same node appears with double the
 memory as well (2183.7MB).

 I used a simplified version of my original snippet to generate the heap
 files:

 import numpy as npimport tensorflow as tf
 def get_layer(input_size, output_size, name):
     # supposedly loaded from a saved file
     W_val = np.random.normal(scale=0.1, size=(input_size, output_size)).astype(np.float32)
     W = tf.get_variable(name='W_{}'.format(name), shape=(input_size, output_size),
                         initializer=tf.constant_initializer(value=W_val, dtype=tf.float32),
                         dtype=tf.float32)
     b = tf.get_variable(name='b_{}'.format(name), shape=(output_size,),
                         initializer=tf.constant_initializer(value=0.1, dtype=tf.float32),
                         dtype=tf.float32)
     return W, b

 g = tf.Graph()with g.as_default():
     W1, b1 = get_layer(158238, 900, '1')
     W2, b2 = get_layer(900, 1000, '2')
     W3, b3 = get_layer(1000, 1, '3')
     init = tf.global_variables_initializer()
 session = tf.Session(graph=g)
 session.run(init)
 session.close()

 —
 You are receiving this because you commented.
 Reply to this email directly, view it on GitHub
 &lt;#9742 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AYoxVHu9SJzrRAiqslcLQwpRRaD2Ku51ks5r321OgaJpZM4NTSLO&gt;
 .



		</comment>
		<comment id='11' author='sirfz' date='2017-05-09T13:55:18Z'>
		I ran the command in both environments (compiled and pip version - Docker containers) and the output is "cpp"
		</comment>
		<comment id='12' author='sirfz' date='2017-05-17T19:30:58Z'>
		After seeing &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/9823&gt;#9823&lt;/denchmark-link&gt;
 (not sure if related to this), I tried the memory_util script suggested by &lt;denchmark-link:https://github.com/yaroslavvb&gt;@yaroslavvb&lt;/denchmark-link&gt;
 but the output is pretty much the same for both environments:
&lt;denchmark-h:h3&gt;Binary&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 32
I tensorflow/core/common_runtime/direct_session.cc:83] Direct session inter op parallelism threads: 32
        0                Unknown (from Proto)(1-cpu)   569656800   569656800 cpu
        1                Unknown (from Proto)(1-cpu)  -569656800           0 cpu
        2               W_1/Initializer/Const(2-cpu)   569656800   569656800 cpu
        3                Unknown (from Proto)(3-cpu)        3600   569660400 cpu
        4                Unknown (from Proto)(3-cpu)       -3600   569656800 cpu
        5               b_1/Initializer/Const(4-cpu)        3600   569660400 cpu
        6                Unknown (from Proto)(5-cpu)     3600000   573260400 cpu
        7                Unknown (from Proto)(5-cpu)    -3600000   569660400 cpu
        8               W_2/Initializer/Const(6-cpu)     3600000   573260400 cpu
        9                Unknown (from Proto)(7-cpu)        4000   573264400 cpu
       10                Unknown (from Proto)(7-cpu)       -4000   573260400 cpu
       11               b_2/Initializer/Const(8-cpu)        4000   573264400 cpu
       12                Unknown (from Proto)(9-cpu)        4000   573268400 cpu
       13                Unknown (from Proto)(9-cpu)       -4000   573264400 cpu
       14              W_3/Initializer/Const(10-cpu)        4000   573268400 cpu
       25               Unknown (from Proto)(13-cpu)   569656800  1142925200 cpu
       26               Unknown (from Proto)(13-cpu)  -569656800   573268400 cpu
       28               Unknown (from Proto)(14-cpu)        3600   573272000 cpu
       29               Unknown (from Proto)(14-cpu)       -3600   573268400 cpu
       31                         b_1/Assign(16-cpu)        3600   573272000 cpu
       32                         W_1/Assign(17-cpu)   569656800  1142928800 cpu
       34               Unknown (from Proto)(15-cpu)     3600000  1146528800 cpu
       35               Unknown (from Proto)(15-cpu)    -3600000  1142928800 cpu
       37               Unknown (from Proto)(18-cpu)        4000  1142932800 cpu
       38               Unknown (from Proto)(18-cpu)       -4000  1142928800 cpu
       40               Unknown (from Proto)(19-cpu)        4000  1142932800 cpu
       41               Unknown (from Proto)(19-cpu)       -4000  1142928800 cpu
       48                         W_2/Assign(22-cpu)     3600000  1146528800 cpu
       49                         b_2/Assign(23-cpu)        4000  1146532800 cpu
       52                         W_3/Assign(24-cpu)        4000  1146536800 cpu
I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 2 allocator_name: "cpu" }
I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 4 allocator_name: "cpu" }
I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 6 allocator_name: "cpu" }
I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 8 allocator_name: "cpu" }
I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 10 allocator_name: "cpu" }
I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 12 allocator_name: "cpu" }
I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 17 allocator_name: "cpu" }
I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 16 allocator_name: "cpu" }
I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 23 allocator_name: "cpu" }
I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 21 allocator_name: "cpu" }
I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 22 allocator_name: "cpu" }
I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 24 allocator_name: "cpu" }
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Compiled&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;I tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 32
I tensorflow/core/common_runtime/direct_session.cc:83] Direct session inter op parallelism threads: 32
        0                Unknown (from Proto)(1-cpu)   569656800   569656800 cpu
        1                Unknown (from Proto)(1-cpu)  -569656800           0 cpu
        2               W_1/Initializer/Const(2-cpu)   569656800   569656800 cpu
        3                Unknown (from Proto)(3-cpu)        3600   569660400 cpu
        4                Unknown (from Proto)(3-cpu)       -3600   569656800 cpu
        5               b_1/Initializer/Const(4-cpu)        3600   569660400 cpu
        6                Unknown (from Proto)(5-cpu)     3600000   573260400 cpu
        7                Unknown (from Proto)(5-cpu)    -3600000   569660400 cpu
        8               W_2/Initializer/Const(6-cpu)     3600000   573260400 cpu
        9                Unknown (from Proto)(7-cpu)        4000   573264400 cpu
       10                Unknown (from Proto)(7-cpu)       -4000   573260400 cpu
       11               b_2/Initializer/Const(8-cpu)        4000   573264400 cpu
       12                Unknown (from Proto)(9-cpu)        4000   573268400 cpu
       13                Unknown (from Proto)(9-cpu)       -4000   573264400 cpu
       14              W_3/Initializer/Const(10-cpu)        4000   573268400 cpu
       25               Unknown (from Proto)(13-cpu)   569656800  1142925200 cpu
       26               Unknown (from Proto)(13-cpu)  -569656800   573268400 cpu
       28               Unknown (from Proto)(14-cpu)        3600   573272000 cpu
       29               Unknown (from Proto)(14-cpu)       -3600   573268400 cpu
       31                         b_1/Assign(16-cpu)        3600   573272000 cpu
       32                         W_1/Assign(17-cpu)   569656800  1142928800 cpu
       34               Unknown (from Proto)(15-cpu)     3600000  1146528800 cpu
       35               Unknown (from Proto)(15-cpu)    -3600000  1142928800 cpu
       37               Unknown (from Proto)(18-cpu)        4000  1142932800 cpu
       38               Unknown (from Proto)(18-cpu)       -4000  1142928800 cpu
       40               Unknown (from Proto)(19-cpu)        4000  1142932800 cpu
       41               Unknown (from Proto)(19-cpu)       -4000  1142928800 cpu
       48                         W_2/Assign(22-cpu)     3600000  1146528800 cpu
       49                         b_2/Assign(23-cpu)        4000  1146532800 cpu
       51                         W_3/Assign(24-cpu)        4000  1146536800 cpu
I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 2 allocator_name: "cpu" }
I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 4 allocator_name: "cpu" }
I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 6 allocator_name: "cpu" }
I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 8 allocator_name: "cpu" }
I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 10 allocator_name: "cpu" }
I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 12 allocator_name: "cpu" }
I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 21 allocator_name: "cpu" }
I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 24 allocator_name: "cpu" }
I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 22 allocator_name: "cpu" }
I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 17 allocator_name: "cpu" }
I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 23 allocator_name: "cpu" }
I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 16 allocator_name: "cpu" }
&lt;/denchmark-code&gt;

This is the script I used for testing:
import memory_util
memory_util.vlog(1)
import numpy as np
import tensorflow as tf


def get_layer(input_size, output_size, name):
    # supposedly loaded from a saved file
    W_val = np.random.normal(scale=0.1, size=(input_size, output_size)).astype(np.float32)
    W = tf.get_variable(name='W_{}'.format(name), shape=(input_size, output_size),
                        initializer=tf.constant_initializer(value=W_val, dtype=tf.float32),
                        dtype=tf.float32)
    b = tf.get_variable(name='b_{}'.format(name), shape=(output_size,),
                        initializer=tf.constant_initializer(value=0.1, dtype=tf.float32),
                        dtype=tf.float32)
    return W, b


g = tf.Graph()
with g.as_default():
    W1, b1 = get_layer(158238, 900, '1')
    W2, b2 = get_layer(900, 1000, '2')
    W3, b3 = get_layer(1000, 1, '3')

    init = tf.global_variables_initializer()
session = tf.Session(graph=g)
with memory_util.capture_stderr() as stderr:
    session.run(init)
memory_util.print_memory_timeline(stderr, ignore_less_than_bytes=1000)
Are there any updates on this issue? Has anyone been able to reproduce it?
		</comment>
		<comment id='13' author='sirfz' date='2017-05-17T19:53:40Z'>
		memory_util only captures memory allocated by TF runtime, so this suggests the problem is elsewhere. Earlier you said that the extra memory comes from memory allocated by MergePartialFromCodedStream which could suggests that extra memory was allocated by protobuf library.
Note that you are initializing your layers with huge constants. Those constants get serialized into GraphDef protobufs. There might be a bug somewhere which causes memory allocated by this serialization not to be freed.
As a work-around, you could try not using constant nodes to initialize your variables. You can initialize them directly from numpy arrays by using tf.placeholder and providing value through feed_dict, for examples see &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/9821&gt;#9821&lt;/denchmark-link&gt;

		</comment>
		<comment id='14' author='sirfz' date='2017-05-18T12:12:54Z'>
		&lt;denchmark-link:https://github.com/yaroslavvb&gt;@yaroslavvb&lt;/denchmark-link&gt;
 wow, the difference is huge. Initializing the variables now takes less than 3GB of memory, that's 2x less than what the binary version was taking and 4x less than the compiled one (using ). That's a huge difference! (edit: It's also way faster)
However, the placeholder initialization workaround is unpractical in my case as we have wrapper classes that do the loading for us which we just initialize using global_variables_initializer so it'll require some design changes which isn't very feasible right now.
I understand this is an issue with protobuf but how come when loading the variables in the compiled version of tensorflow we get so much more memory usage? Even though it's the same protobuf version used in both cases? Also should I file an issue with the protobuf team regarding this or are there ways to tune it?
In case someone needs it, here's the code I used:
import numpy as np
import tensorflow as tf

def get_layer(input_size, output_size, name):
    # supposedly loaded from a saved file
    W_pl = tf.placeholder(tf.float32, (input_size, output_size))
    W_val = np.random.normal(scale=0.1, size=(input_size, output_size)).astype(np.float32)
    W = tf.get_variable(name='W_{}'.format(name),
                        initializer=W_pl,
                        dtype=tf.float32)
    b = tf.get_variable(name='b_{}'.format(name), shape=(output_size,),
                        initializer=tf.constant_initializer(value=0.1, dtype=tf.float32),
                        dtype=tf.float32)
    return (W_pl, W_val), W, b

sessions = []
for i in range(3):
    g = tf.Graph()
    with g.as_default():
        W1_pl, W1, b1 = get_layer(158238, 900, '1')
        W2_pl, W2, b2 = get_layer(900, 1000, '2')
        W3_pl, W3, b3 = get_layer(1000, 1, '3')
        init = tf.global_variables_initializer()
    session = tf.Session(graph=g)
    session.run(init, feed_dict=dict([W1_pl, W2_pl, W3_pl]))
    print 'Loaded {}'.format(i)
    sessions.append(session)
		</comment>
		<comment id='15' author='sirfz' date='2017-05-18T14:04:36Z'>
		BTW, putting huge constants into Graph has performance implications besides higher memory, accessing data is slower too. Also there's a 2GB limit which includes graph inlined constants, once you hit it, you won't be able to add any more operations to the graph.
I get around design issues by keeping global init_dict with links to all numpy arrays and passing it to global_variables_initializer.
If you really wanted to find out why there's a difference you could try to narrow problem further.
For instance, does it happen with Python protobufs as well or just with cpp?
To find out which protobuf you are running:
python -c "from google.protobuf.internal import api_implementation; print(api_implementation._default_implementation_type)"
Does it happen when the constant in question are getting read or is just the serialization step leaking memory? You can create large constant + small constant, and do "sess.run(small_constant)"
		</comment>
		<comment id='16' author='sirfz' date='2017-05-18T14:31:37Z'>
		Thanks for the suggestion &lt;denchmark-link:https://github.com/yaroslavvb&gt;@yaroslavvb&lt;/denchmark-link&gt;
, sounds fair enough (and worth it given the significant memory/performance improvement).
Not sure it's worth it to look into the difference between the environments given that I found the main bottleneck now which is in the protobuf serialization. For what it's worth the protobuf implementation type in my case is cpp.
As for your last question, I already tested with tf.random_normal_initializer instead of tf.constant_initializer with a numpy array and there's no issue at all.
		</comment>
		<comment id='17' author='sirfz' date='2017-06-05T16:23:17Z'>
		It sounds as if the right thing to do is follow up with protobuf to investigate further, so I am closing this for now but please reopen if there are more TF issues.
		</comment>
	</comments>
</bug>