<bug id='36612' author='dd1923' open_date='2020-02-10T05:09:31Z' closed_time='2020-07-01T21:11:10Z'>
	<summary>ReduceLROnPlateau callback crashed when learning rate is of type tf.keras.optimizers.schedules</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): pip3
TensorFlow version (use command below): 2.0.0
Python version: 3.6

Describe the current behavior
When using tf.keras.callbacks.ReduceLROnPlateau in callbacks in model.fit(), it fails with an error if model.optimizer.lr is of type  tf.keras.optimizers.schedules. *.
For example, one can use tf.keras.optimizers.schedules.ExponentialDecay in an optimizer. This would cause the following line to fail:



tensorflow/tensorflow/python/keras/callbacks.py


         Line 2021
      in
      6dd3403






 old_lr = float(K.get_value(self.model.optimizer.lr)) 





Above line assumes lr to be a float.
The error it gives in this case is :
TypeError: float() argument must be a string or a number, not 'ExponentialDecay'
Describe the expected behavior
ReduceLROnPlateau should also work with learning rate whose type is of tf.keras.optimizers.schedules
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='dd1923' date='2020-02-10T11:22:40Z'>
		&lt;denchmark-link:https://github.com/dd1923&gt;@dd1923&lt;/denchmark-link&gt;

Will it be possible to share simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!
		</comment>
		<comment id='2' author='dd1923' date='2020-02-10T16:46:24Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
 Here is the &lt;denchmark-link:https://colab.research.google.com/drive/1bULTBS7TYFrSOtYbLoqaHj0zM0Tr2zU3&gt;colab link&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='3' author='dd1923' date='2020-02-11T10:02:23Z'>
		I have tried on colab with TF version 2.1.0, 2.2.0-dev20200211 and was able to reproduce the issue. Thanks!
		</comment>
		<comment id='4' author='dd1923' date='2020-04-29T00:17:17Z'>
		Any updates?
		</comment>
		<comment id='5' author='dd1923' date='2020-05-15T21:45:22Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 ? &lt;denchmark-link:https://github.com/rchao&gt;@rchao&lt;/denchmark-link&gt;
 ?
		</comment>
		<comment id='6' author='dd1923' date='2020-06-26T21:43:09Z'>
		Same here. ReduceLRonPlateau does not play well with ExponentialDecay. Providing lr as a simple constant works.
tf.__version__
2.2.0

tf.keras.__version__
2.3.0-tf
		</comment>
		<comment id='7' author='dd1923' date='2020-06-26T22:32:30Z'>
		Doesn't look like anyone from tf team is interested in fixing any bugs. I have had a few others open for a long time now.
		</comment>
		<comment id='8' author='dd1923' date='2020-06-27T13:30:18Z'>
		&lt;denchmark-link:https://github.com/dd1923&gt;@dd1923&lt;/denchmark-link&gt;
 it's open-source, so you can only complain if you have an MR fixing the bug, but no one reviews it. And even then, you have the liberty to fork the repo and maintain a better fork.
Especially, this issue seems to be easily fixable with just python knowledge.
		</comment>
		<comment id='9' author='dd1923' date='2020-06-27T16:56:36Z'>
		&lt;denchmark-link:https://github.com/dd1923&gt;@dd1923&lt;/denchmark-link&gt;
 Extremely sorry for missing this issue. I could reproduce the issue with . However, I ran your code with  and I cannot reproduce the issue. The callback is working without any error. Please check the &lt;denchmark-link:https://colab.research.google.com/gist/jvishnuvardhan/f8a79e46abbbcb2b5aa25636d25f0433/reducelronplateau-bug.ipynb&gt;gist here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='10' author='dd1923' date='2020-07-01T21:11:10Z'>
		I am closing this issue as this was resolved in tf-nightly. If you want to use stable version, then please wait for some time. Stable TF2.3 will be released in near future. Thanks!
		</comment>
		<comment id='11' author='dd1923' date='2020-07-01T21:11:12Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36612&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36612&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='dd1923' date='2020-08-03T15:01:15Z'>
		I updated my Tensorflow version to 2.3.0, However, the behavior still persists on my machine. I am trying to combine tf.keras.optimizers.schedules.ExponentialDecaywith tf.keras.callbacks.ReduceLROnPlateau with no luck. Thank you for your help.
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catalina 10.15.6
TensorFlow installed from (source or binary): pip3
TensorFlow version (use command below): 2.3.0
Python version: 3.7
		</comment>
		<comment id='13' author='dd1923' date='2020-08-03T16:26:04Z'>
		&lt;denchmark-link:https://github.com/dcleres&gt;@dcleres&lt;/denchmark-link&gt;
 Can you please create a new issue with a simple standalone code to reproduce the issue? Thanks!
		</comment>
	</comments>
</bug>