<bug id='45441' author='DachuanZhao' open_date='2020-12-07T09:20:29Z' closed_time='2020-12-08T14:32:10Z'>
	<summary>mixed_precision make train and predict very slow when only using cpu</summary>
	<description>
tensorflow : 2.3
Here is the &lt;denchmark-link:https://colab.research.google.com/drive/1glypA7wC988kivuGe07Qfk_XHuq1j9aS?usp=sharing&gt;colab&lt;/denchmark-link&gt;

You can see that it costs 3s to train a epoch while costs 187s to train the same epoch using 
&lt;denchmark-code&gt;Epoch 1/5
1875/1875 [==============================] - 3s 1ms/step - loss: 0.2968 - accuracy: 0.9134
Epoch 2/5
1875/1875 [==============================] - 3s 1ms/step - loss: 0.1448 - accuracy: 0.9575
Epoch 3/5
1875/1875 [==============================] - 2s 1ms/step - loss: 0.1073 - accuracy: 0.9678
Epoch 4/5
1875/1875 [==============================] - 3s 1ms/step - loss: 0.0861 - accuracy: 0.9730
Epoch 5/5
1875/1875 [==============================] - 3s 1ms/step - loss: 0.0734 - accuracy: 0.9769
&lt;/denchmark-code&gt;

vs
&lt;denchmark-code&gt;Epoch 1/5
1875/1875 [==============================] - 187s 100ms/step - loss: 0.2936 - accuracy: 0.9141
Epoch 2/5
1179/1875 [=================&gt;............] - ETA: 1:11 - loss: 0.1455 - accuracy: 0.9555
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='DachuanZhao' date='2020-12-07T09:58:40Z'>
		&lt;denchmark-link:https://github.com/DachuanZhao&gt;@DachuanZhao&lt;/denchmark-link&gt;

We see that the issue template has not been filled, could you please do so as it helps us analyse the issue [steps followed before you ran into this error or stand alone code to reproduce the issue faced, if possible colab gist with the error]
		</comment>
		<comment id='2' author='DachuanZhao' date='2020-12-07T10:04:11Z'>
		
@DachuanZhao
We see that the issue template has not been filled, could you please do so as it helps us analyse the issue [steps followed before you ran into this error or stand alone code to reproduce the issue faced, if possible colab gist with the error]

I think this bug is a common bug which means it may exist in many versions . So I only give the tensroflow version .
I have post a colab , and you can run the code in the colab .
		</comment>
		<comment id='3' author='DachuanZhao' date='2020-12-07T10:20:18Z'>
		Why using mixed precision with CPU? As said in the &lt;denchmark-link:https://www.tensorflow.org/guide/mixed_precision&gt;doc&lt;/denchmark-link&gt;
,

As mentioned before, the mixed_float16 policy will most significantly improve performance on NVIDIA GPUs with compute capability of at least 7.0. The policy will run on other GPUs and CPUs but may not improve performance. For TPUs, the mixed_bfloat16 policy should be used instead.

This slowdown is pretty much expected.
		</comment>
		<comment id='4' author='DachuanZhao' date='2020-12-07T10:25:43Z'>
		
This slowdown is pretty much expected.
it may not improve performance.

In fact , it leads to worse performance on cpu ......
		</comment>
		<comment id='5' author='DachuanZhao' date='2020-12-07T10:26:34Z'>
		I don't think it is a bug. This feature is not meant to accelerate your model on regular CPUs.
		</comment>
		<comment id='6' author='DachuanZhao' date='2020-12-07T10:34:04Z'>
		
I don't think it is a bug. This feature is not meant to accelerate your model on regular CPUs.

But at least it should't make the performance  worse ...... We often train a model with gpu and deploy it with cpu . In this situation, the bug has a big impact ......
		</comment>
		<comment id='7' author='DachuanZhao' date='2020-12-07T10:36:00Z'>
		

I don't think it is a bug. This feature is not meant to accelerate your model on regular CPUs.

But at least it should't make the performance worse ...... We often train a model with gpu and deploy it with cpu . In this situation, the bug has a big impact ......

There is nothing prevent you deploying a model trained in mixed precision with full precision. The model itself is always stored in full precision no matter using mixed precision or not.
		</comment>
		<comment id='8' author='DachuanZhao' date='2020-12-07T10:48:51Z'>
		


I don't think it is a bug. This feature is not meant to accelerate your model on regular CPUs.

But at least it should't make the performance worse ...... We often train a model with gpu and deploy it with cpu . In this situation, the bug has a big impact ......

There is nothing prevent you deploying a model trained in mixed precision with full precision. The model itself is always stored in full precision.

The strange thing is that when you deploy a model which is trained with mixed_precision on tf-serving-cpu , you will find the model is slower than the model trained without mixed_precision ......
		</comment>
		<comment id='9' author='DachuanZhao' date='2020-12-07T11:03:08Z'>
		I've clearly stated that mixed precision training has nothing to do with serving your model in full precision. Even in mixed precision training your model is saved in full precision.
		</comment>
		<comment id='10' author='DachuanZhao' date='2020-12-07T11:04:06Z'>
		



I don't think it is a bug. This feature is not meant to accelerate your model on regular CPUs.

But at least it should't make the performance worse ...... We often train a model with gpu and deploy it with cpu . In this situation, the bug has a big impact ......

There is nothing prevent you deploying a model trained in mixed precision with full precision. The model itself is always stored in full precision.

The strange thing is that when you deploy a model which is trained with mixed_precision on tf-serving-cpu , you will find the model is slower than the model trained without mixed_precision ......

If you actually meet a bug, which is unlikely IMHO, you could raise an issue on tensorflow/serving.
		</comment>
		<comment id='11' author='DachuanZhao' date='2020-12-07T11:09:50Z'>
		




I don't think it is a bug. This feature is not meant to accelerate your model on regular CPUs.

But at least it should't make the performance worse ...... We often train a model with gpu and deploy it with cpu . In this situation, the bug has a big impact ......

There is nothing prevent you deploying a model trained in mixed precision with full precision. The model itself is always stored in full precision.

The strange thing is that when you deploy a model which is trained with mixed_precision on tf-serving-cpu , you will find the model is slower than the model trained without mixed_precision ......

If you actually meet a bug, which is unlikely IMHO, you could raise an issue on tensorflow/serving.

Have you run the code in my colab ?  It's acceptable that training a model with mixed_precision on cpu is 60 times slower than training it without mixed_precision  ?
		</comment>
		<comment id='12' author='DachuanZhao' date='2020-12-07T12:24:39Z'>
		





I don't think it is a bug. This feature is not meant to accelerate your model on regular CPUs.

But at least it should't make the performance worse ...... We often train a model with gpu and deploy it with cpu . In this situation, the bug has a big impact ......

There is nothing prevent you deploying a model trained in mixed precision with full precision. The model itself is always stored in full precision.

The strange thing is that when you deploy a model which is trained with mixed_precision on tf-serving-cpu , you will find the model is slower than the model trained without mixed_precision ......

If you actually meet a bug, which is unlikely IMHO, you could raise an issue on tensorflow/serving.

Have you run the code in my colab ? It's acceptable that training a model with mixed_precision on cpu is 60 times slower than training it without mixed_precision ?

"The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16."
Mixed precision is not meant to accelerate training on CPU. It's expected to impact performance.
		</comment>
		<comment id='13' author='DachuanZhao' date='2020-12-07T12:31:15Z'>
		





I don't think it is a bug. This feature is not meant to accelerate your model on regular CPUs.

But at least it should't make the performance worse ...... We often train a model with gpu and deploy it with cpu . In this situation, the bug has a big impact ......

There is nothing prevent you deploying a model trained in mixed precision with full precision. The model itself is always stored in full precision.

The strange thing is that when you deploy a model which is trained with mixed_precision on tf-serving-cpu , you will find the model is slower than the model trained without mixed_precision ......

If you actually meet a bug, which is unlikely IMHO, you could raise an issue on tensorflow/serving.

Have you run the code in my colab ? It's acceptable that training a model with mixed_precision on cpu is 60 times slower than training it without mixed_precision ?

float16 are accelerated by hardware, not software. Software enables such acceleration with supported hardware, while regular CPUs do not have fp16 capability. I am surprised it actually runs btw.
		</comment>
		<comment id='14' author='DachuanZhao' date='2020-12-08T09:28:19Z'>
		&lt;denchmark-link:https://github.com/byronyi&gt;@byronyi&lt;/denchmark-link&gt;

is this still an issue.
		</comment>
		<comment id='15' author='DachuanZhao' date='2020-12-08T14:21:55Z'>
		
@byronyi
is this still an issue.

I don't think so and this issue could be closed.
		</comment>
		<comment id='16' author='DachuanZhao' date='2020-12-08T14:32:10Z'>
		Moving this to closed status with confirmation.
		</comment>
		<comment id='17' author='DachuanZhao' date='2020-12-08T14:32:12Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45441&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45441&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='18' author='DachuanZhao' date='2020-12-09T03:22:55Z'>
		

@byronyi
is this still an issue.

I don't think so and this issue could be closed.

Why not ???
You say : "I am surprised it actually runs btw." , but you don't think it's a bug ???
		</comment>
		<comment id='19' author='DachuanZhao' date='2020-12-09T03:25:14Z'>
		
Moving this to closed status with confirmation.

I suggest you run the code in my colab and you decide whether it's a bug or not by yourself ......
		</comment>
		<comment id='20' author='DachuanZhao' date='2020-12-09T03:31:14Z'>
		
accelerate

So the conclusion is that "It's acceptable that training or predicting a model with mixed_precision on cpu is slower than without mixed_precision" ?
		</comment>
		<comment id='21' author='DachuanZhao' date='2020-12-09T03:36:09Z'>
		You should really consult your CPU vendor. There is nothing TF can do to fix this for you except for disabling mixed precision on unsupported hardware all together.
		</comment>
		<comment id='22' author='DachuanZhao' date='2020-12-09T03:36:47Z'>
		cc &lt;denchmark-link:https://github.com/reedwm&gt;@reedwm&lt;/denchmark-link&gt;

		</comment>
		<comment id='23' author='DachuanZhao' date='2020-12-09T03:46:04Z'>
		
You should really consult your CPU vendor. There is nothing TF can do to fix this for you except for disabling mixed precision on unsupported hardware all together.

This problem can reproduce at google colab . You think it may be a common problem on different types of CPUs so that TF can't fix it ?
		</comment>
		<comment id='24' author='DachuanZhao' date='2020-12-09T20:44:49Z'>
		As &lt;denchmark-link:https://github.com/byronyi&gt;@byronyi&lt;/denchmark-link&gt;
 has stated, CPUs do not have hardware support for float16 and so will be slower with mixed_float16. I'll clarify in the tutorial that mixed precision can actually significantly slow down a model on CPUs, instead of just not speeding it up.
As for the TF serving issue: A float32 and a mixed_float16 tf.train.Checkpoint are identical for the same model, as checkpoints do not store the dtype of computations. On the other hand, a float32 and mixed_float16 SavedModel are different, as SavedModels store the graph of computations, which includes the dtype of computations. Using a mixed_float16 SavedModel with TF-Serving on a device that does not support mixed precision will be slow. As a workaround, checkpoints can be used instead, then when a SavedModel is required, the checkpoint can be loaded into a float32 model and a float32 SavedModel can be generated. I'll talk people working on SavedModel to work on a better solution.
		</comment>
	</comments>
</bug>