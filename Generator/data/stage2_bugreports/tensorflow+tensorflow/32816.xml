<bug id='32816' author='SanthoshRajendiran' open_date='2019-09-25T15:47:59Z' closed_time='2020-08-23T19:25:37Z'>
	<summary>huge runtime increase for keras converted tflite on some android devices</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Honor Play (COR-AL00) and Poco F1
TensorFlow installed from (source or binary): Source
TensorFlow version (use command below): Tested on various versions(1.14, 1.15rc1, 2.0rc1)
Python version: 3.6
Bazel version (if compiling from source): NIL
GCC/Compiler version (if compiling from source): NIL
CUDA/cuDNN version: NIL
GPU model and memory: NIL

Describe the current behavior
Honor Play and POCO F1 phones give nearly same performance for TFLite models converted using TensorFlow (.pb) to TFLite whereas TFLite model converted using Keras (h5) to TFLite is behaving strange i.e. runtime on HonorPlay(COR-AL00) is 3 times higher than POCOF1.
Example:
Mobilenet-Unet (converted from keras(h5) to tflite)
Average model runtime of mobilenet-unet(Poco): 60ms
Average model runtime of mobilenet-unet(Honor): 180ms
Simple - Unet (converted from tensorflow(pb) to tflite)
Average model runtime of unet(Poco): 50ms
Average model runtime of unet(Honor): 60ms
Describe the expected behavior
Same runtime for models with same architecture, no matter whether it is keras(h5) converted to tflite, or pb converted to tflite.
Code to reproduce the issue
Official Benchmark tool test and android integration test (Benchmark version- 1.14, and Android(tflite gpu delegate-nightly)
Other info / logs
The base keras(h5) model has been created using pure keras, not tf-keras.
### Benchmark tool results for sample model (google drive link attached):
### HONOR PLAY RESULTS
STARTING!
Min num runs: [50]
Min runs duration (seconds): [1]
Inter-run delay (seconds): [-1]
Num threads: [1]
Benchmark name: []
Output prefix: []
Min warmup runs: [1]
Min warmup runs duration (seconds): [0.5]
Graph: [/data/local/tmp/testing_mobilenet_sigmoid_divtry_tf15_1.tflite]
Input layers: []
Input shapes: []
Use nnapi : [0]
Use legacy nnapi : [0]
Use gpu : [1]
Allow fp16 : [0]
Enable op profiling: [0]
Loaded model /data/local/tmp/testing_mobilenet_sigmoid_divtry_tf15_1.tflite
resolved reporter
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for GPU.
ERROR: Next operations are not supported by GPU delegate:
CAST: Operation is not supported.
DIV: Expected 2 input tensor(s), but node has 1 runtime input(s).
RESHAPE:
First 93 operations will run on the GPU, and the remaining 3 on the CPU.
Applied GPU delegate.
Initialized session in 894.605ms
Running benchmark for at least 1 iterations and at least 0.5 seconds
count=3 first=227955 curr=174845 min=174845 max=227955 avg=204373 std=22086
Running benchmark for at least 50 iterations and at least 1 seconds
count=50 first=148625 curr=101260 min=97334 max=148625 avg=103508 std=7440
Average inference timings in us: Warmup: 204373, Init: 894605, no stats: 103508
### POCO F1 RESULTS
STARTING!
Min num runs: [50]
Min runs duration (seconds): [1]
Inter-run delay (seconds): [-1]
Num threads: [1]
Benchmark name: []
Output prefix: []
Min warmup runs: [1]
Min warmup runs duration (seconds): [0.5]
Graph: [/data/local/tmp/testing_mobilenet_sigmoid_divtry_tf15_1.tflite]
Input layers: []
Input shapes: []
Use nnapi : [0]
Use legacy nnapi : [0]
Use gpu : [1]
Allow fp16 : [0]
Enable op profiling: [0]
Loaded model /data/local/tmp/testing_mobilenet_sigmoid_divtry_tf15_1.tflite
resolved reporter
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for GPU.
ERROR: Next operations are not supported by GPU delegate:
CAST: Operation is not supported.
DIV: Expected 2 input tensor(s), but node has 1 runtime input(s).
RESHAPE:
First 93 operations will run on the GPU, and the remaining 3 on the CPU.
Applied GPU delegate.
Initialized session in 612.655ms
Running benchmark for at least 1 iterations and at least 0.5 seconds
count=15 first=75811 curr=30928 min=30909 max=75811 avg=34068.6 std=11156
Running benchmark for at least 50 iterations and at least 1 seconds
count=50 first=31179 curr=31509 min=30836 max=32212 avg=31148.3 std=261
Average inference timings in us: Warmup: 34068.6, Init: 612655, no stats: 31148.3

&lt;denchmark-link:https://drive.google.com/file/d/1PvvoIzfrzLY8VNw5wTdpA6hxOFw_Ri9B/view?usp=sharing&gt;https://drive.google.com/file/d/1PvvoIzfrzLY8VNw5wTdpA6hxOFw_Ri9B/view?usp=sharing&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='SanthoshRajendiran' date='2019-09-26T15:05:58Z'>
		Can you attach both the .tflite model converted from Keras and that converted from the frozen graph (.pb)? As well as the frozen graph itself? Thanks.
		</comment>
		<comment id='2' author='SanthoshRajendiran' date='2019-10-01T06:04:13Z'>
		Attaching with this, the mobilenet-unet keras model and tflite for 2 class segmentation..
An update to the issue, this issue does not serve the same for all keras models. Some other models are performing nearly the same speed. For reference, that file is also attached with this issue..
Both models are keras trained. They have been named according to their behavior on the mobiles. The same speed giving model is a simple unet encoder-decoder model. The model giving speed issue is a keras trained mobilenet-encoder and unet-decoder model.
Models ( check out the google drive link below)
&lt;denchmark-link:https://drive.google.com/file/d/16yNeKx94Oh3JxTQaLmK2q_rARAMjrWfm/view?usp=sharing&gt;https://drive.google.com/file/d/16yNeKx94Oh3JxTQaLmK2q_rARAMjrWfm/view?usp=sharing&lt;/denchmark-link&gt;

The speed details are listed below.
unet_mobilenet_not_same_speed.tflite
Honor Play : 154ms (Average)
Poco F1: 47ms (Average)
working_nearly_same_speed.tflite
Honor Play: 18.9 ms (Average)
Poco F1: 24ms ( Average)
Android Benchmark Tool Logs:
Poco F1:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3674658/PocoF1_same_speed_model.txt&gt;PocoF1_same_speed_model.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3674659/PocoF1_unet_mobilenet_not_same_speed_model.txt&gt;PocoF1_unet_mobilenet_not_same_speed_model.txt&lt;/denchmark-link&gt;

Honor Play:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3674660/HonorPlay_same_speed_model.txt&gt;HonorPlay_same_speed_model.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3674661/HonorPlay_unet_mobilenet_not_same_speed_model.txt&gt;HonorPlay_unet_mobilenet_not_same_speed_model.txt&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='SanthoshRajendiran' date='2020-03-23T11:33:01Z'>
		&lt;denchmark-link:https://github.com/SanthoshRajendiran&gt;@SanthoshRajendiran&lt;/denchmark-link&gt;

please let us know if the issue still persist
		</comment>
		<comment id='4' author='SanthoshRajendiran' date='2020-08-16T18:27:29Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='5' author='SanthoshRajendiran' date='2020-08-23T19:25:36Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='6' author='SanthoshRajendiran' date='2020-08-23T19:25:38Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32816&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32816&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>