<bug id='33397' author='wuhy08' open_date='2019-10-15T22:14:25Z' closed_time='2020-03-09T05:22:10Z'>
	<summary>LEAKY_RELU not supported in INT8 quantization</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
TensorFlow installed from (source or binary): binary
TensorFlow version (or github SHA if from source): 2.0

Provide the text output from tflite_convert
&lt;denchmark-code&gt;&lt;ipython-input-54-6f97bfc97449&gt; in &lt;module&gt;
----&gt; 1 workflow(False, 10, 'int8', True, True)

&lt;ipython-input-53-3889d2128b41&gt; in workflow(dw, n_conv, dtype, v1, leaky)
     11     model.save(keras_file_name)
     12     if v1:
---&gt; 13         tflite_model = convert2tflitev1(keras_file_name, qmode=dtype)
     14     else:
     15         tflite_model = convert2tflite(model, qmode=dtype)

&lt;ipython-input-27-637084f295d5&gt; in convert2tflitev1(model_file, qmode)
     19     elif qmode == 'float32':
     20         pass
---&gt; 21     tflite_model = converter.convert()
     22 
     23     return tflite_model

~/miniconda3/envs/tf2-n-pytorch/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py in convert(self)
    991     if self._is_calibration_quantize():
    992       result = self._calibrate_quantize_model(result, inference_input_type,
--&gt; 993                                               inference_output_type)
    994 
    995     return result

~/miniconda3/envs/tf2-n-pytorch/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py in _calibrate_quantize_model(self, result, inference_input_type, inference_output_type)
    237     return calibrate_quantize.calibrate_and_quantize(
    238         self.representative_dataset.input_gen, inference_input_type,
--&gt; 239         inference_output_type, allow_float)
    240 
    241   def _get_base_converter_args(self):

~/miniconda3/envs/tf2-n-pytorch/lib/python3.7/site-packages/tensorflow_core/lite/python/optimize/calibrator.py in calibrate_and_quantize(self, dataset_gen, input_type, output_type, allow_float)
     76     return self._calibrator.QuantizeModel(
     77         np.dtype(input_type.as_numpy_dtype()).num,
---&gt; 78         np.dtype(output_type.as_numpy_dtype()).num, allow_float)

~/miniconda3/envs/tf2-n-pytorch/lib/python3.7/site-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py in QuantizeModel(self, input_py_type, output_py_type, allow_float)
    113 
    114     def QuantizeModel(self, input_py_type, output_py_type, allow_float):
--&gt; 115         return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_QuantizeModel(self, input_py_type, output_py_type, allow_float)
    116 CalibrationWrapper_swigregister = _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_swigregister
    117 CalibrationWrapper_swigregister(CalibrationWrapper)

RuntimeError: Quantization not yet supported for op: LEAKY_RELU
&lt;/denchmark-code&gt;

Also, please include a link to a GraphDef or the model if possible.
Model definition
&lt;denchmark-code&gt;def build_lrelu_model(n_layers):
    layer_list = []
    for idx in range(n_layers):
        conv_name = f'conv{idx}'
        if idx == 0:
            conv = layers.Conv2D(filters=32, 
                                 kernel_size=3, 
                                 padding='same', 
                                 name=conv_name, 
                                 input_shape=(28, 28, 3))
        else:
            conv = layers.Conv2D(filters=32, 
                                 kernel_size=3, 
                                 padding='same', 
                                 name=conv_name)
        bn = layers.BatchNormalization(name=f'bn{idx}')
        activation = layers.LeakyReLU(alpha=0.1, name=f'lrelu{idx}')
        layer_list.extend([conv, bn, activation])
    model = tf.keras.models.Sequential(layer_list)
    model.summary()
    return model
&lt;/denchmark-code&gt;

Conversion options:
&lt;denchmark-code&gt;        converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(model_file)
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
        converter.inference_input_type = tf.uint8
        converter.inference_output_type = tf.uint8
&lt;/denchmark-code&gt;

Any other info / logs
I saw there were some PR about the implementation of Leaky_ReLU in tflite and quantization. But I still cannot get it to work.
Ref PR: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/27061&gt;#27061&lt;/denchmark-link&gt;

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='wuhy08' date='2019-10-16T15:31:39Z'>
		BTW, I am not using tflite_convert.py
I am using TFLiteConverter, as shown in the code snippet above.
		</comment>
		<comment id='2' author='wuhy08' date='2019-10-16T20:35:01Z'>
		I am not sure if I am looking at the right place.
But the error might be thrown from here: 


tensorflow/tensorflow/lite/tools/optimize/quantize_model.cc


         Line 590
      in
      265126c






 error_reporter-&gt;Report("Quantization not yet supported for op: %s", 





which calls  (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/d08bf910b3ea5d8895a07553379436ad543f2da7/tensorflow/lite/tools/optimize/operator_property.cc&gt;link&lt;/denchmark-link&gt;
), where LEAKY_RELU and RESIZE_NEAREST_NEIGHBOUR are not defined.
		</comment>
		<comment id='3' author='wuhy08' date='2020-02-20T19:00:46Z'>
		Fixed in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/36876&gt;#36876&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='wuhy08' date='2020-02-20T19:00:48Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33397&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33397&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='wuhy08' date='2020-02-28T11:25:59Z'>
		Hi &lt;denchmark-link:https://github.com/wuhy08&gt;@wuhy08&lt;/denchmark-link&gt;

After adding Leaky_Relu into operator property, I can generate the INT8 model which include Leaky_Relu . But the int8 implementation of Leaky_Relu is still missing, right? When I called invoked() tflite model, it will return fail. Did you meet the same problem?
Error detail:
ValueError: Didn't find op for builtin opcode 'LEAKY_RELU' version '2'
Registration failed.
Thanks
Chunjue
		</comment>
		<comment id='6' author='wuhy08' date='2020-02-28T21:20:22Z'>
		Hi &lt;denchmark-link:https://github.com/cjtang&gt;@cjtang&lt;/denchmark-link&gt;

There was a PR about Leaky_RELU_INT8 a while ago (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/27061&gt;#27061&lt;/denchmark-link&gt;
). I believe that part of code was never touched since nobody successfully quantized a Leaky_RELU op. Let me investigate a little.
		</comment>
		<comment id='7' author='wuhy08' date='2020-02-28T23:48:44Z'>
		It looks the error comes from this line:



tensorflow/tensorflow/lite/core/api/op_resolver.cc


         Line 41
      in
      3a8c575






 "Didn't find op for builtin opcode '%s' version '%d'\n", 





Will continue to investigate.
		</comment>
		<comment id='8' author='wuhy08' date='2020-03-04T05:57:19Z'>
		A PR is created for this issue: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/37279&gt;#37279&lt;/denchmark-link&gt;
. Will wait for approval.
		</comment>
		<comment id='9' author='wuhy08' date='2020-03-04T06:19:46Z'>
		Great, I also added my version of INT8 Leaky_RELU implementation, but the result is always not good. Now I know that is because model doesn't restrict the same scale for input and output. Thank you for pointing that out. Let's wait for approval.
		</comment>
		<comment id='10' author='wuhy08' date='2020-03-09T05:22:10Z'>
		&lt;denchmark-link:https://github.com/cjtang&gt;@cjtang&lt;/denchmark-link&gt;

The PR has just been merged. Expect a few days before it is in tf-nightly.
Thanks.
		</comment>
		<comment id='11' author='wuhy08' date='2020-03-09T05:22:12Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33397&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33397&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>