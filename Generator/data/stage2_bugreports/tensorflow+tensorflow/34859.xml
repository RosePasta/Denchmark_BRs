<bug id='34859' author='sharvil' open_date='2019-12-05T09:09:28Z' closed_time='2020-01-13T09:09:44Z'>
	<summary>tf.train.AdamOptimizer doesn't work with custom TPU training loop</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.15
Python version: 3.x
Bazel version (if compiling from source): n/a
GCC/Compiler version (if compiling from source): n/a
CUDA/cuDNN version: n/a
GPU model and memory: n/a


Run this Colab notebook with a TPU accelerator: &lt;denchmark-link:https://colab.research.google.com/drive/1bsgSNK3aK9sETlplIPVpAa-yc4q1S3sA&gt;https://colab.research.google.com/drive/1bsgSNK3aK9sETlplIPVpAa-yc4q1S3sA&lt;/denchmark-link&gt;

When running the above notebook with tf.train.AdamOptimizer, we get:
&lt;denchmark-code&gt;ValueError: in converted code:

    &lt;ipython-input-22-807c7cf92c68&gt;:21 simple_model_fn  *
        train_op = tf.train.AdamOptimizer().minimize(y)
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/optimizer.py:413 minimize
        name=name)
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/optimizer.py:569 apply_gradients
        self._distributed_apply, args=(grads_and_vars, global_step, name))
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py:1940 merge_call
        return self._merge_call(merge_fn, args, kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py:1947 _merge_call
        return merge_fn(self._strategy, *args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/optimizer.py:717 _distributed_apply
        non_slot_devices, finish, args=(self, update_ops), group=False)
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py:1577 update_non_slot
        return self._update_non_slot(colocate_with, fn, args, kwargs, group)
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/tpu_strategy.py:580 _update_non_slot
        result = fn(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/optimizer.py:713 finish
        return self._finish(update_ops, "update")
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/adam.py:228 _finish
        beta1_power, beta2_power = self._get_beta_accumulators()
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/adam.py:115 _get_beta_accumulators
        return (self._get_non_slot_variable("beta1_power", graph=graph),
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/optimizer.py:868 _get_non_slot_variable
        if hasattr(non_slot, "_distributed_container"):
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/values.py:827 __getattr__
        return super(TPUVariableMixin, self).__getattr__(name)
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/values.py:389 __getattr__
        return getattr(self.get(), name)
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/values.py:834 get
        return super(TPUVariableMixin, self).get(device=device)
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/values.py:324 get
        return self._device_map.select_for_device(self._values, device)
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/values.py:219 select_for_device
        (device, self._devices, device_util.current()))

    ValueError: Device /job:worker/replica:0/task:0/device:CPU:0 not found in ('/job:worker/replica:0/task:0/device:TPU:0', '/job:worker/replica:0/task:0/device:TPU:1', '/job:worker/replica:0/task:0/device:TPU:2', '/job:worker/replica:0/task:0/device:TPU:3', '/job:worker/replica:0/task:0/device:TPU:4', '/job:worker/replica:0/task:0/device:TPU:5', '/job:worker/replica:0/task:0/device:TPU:6', '/job:worker/replica:0/task:0/device:TPU:7') (current device /job:worker/replica:0/task:0/device:CPU:0)
&lt;/denchmark-code&gt;

This code runs just fine with tf.train.MomentumOptimizer and tf.keras.optimizers.Adam (run same code with the optimizer_type form variable set to KerasAdam or Momentum).
Describe the expected behavior
Code should run without error using tf.train.AdamOptimizer, just like it does for the other optimizers.

&lt;denchmark-link:https://colab.research.google.com/drive/1bsgSNK3aK9sETlplIPVpAa-yc4q1S3sA&gt;https://colab.research.google.com/drive/1bsgSNK3aK9sETlplIPVpAa-yc4q1S3sA&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='sharvil' date='2019-12-06T06:46:31Z'>
		I have tried on colab with TF version 1.15 and was able to reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/340328a62bf9432601cbe8ac5d08cc9e/broken-adamoptimizer-tpu-custom-loop.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='sharvil' date='2019-12-06T19:22:40Z'>
		Is there any reason not to use tf.keras.optimizers.Adam?
		</comment>
		<comment id='3' author='sharvil' date='2019-12-06T20:14:50Z'>
		The primary reason, for us, is that Keras' optimizers throw on missing gradients instead of returning None.
It's often the case that we have multiple losses in a model. Not all variables are connected to all losses in the graph. Computing the set of variables that are connected to each loss function a priori is somewhat onerous, so we let TF do it for us. We compute the gradients of each loss with respect to all trainable variables and pull out the gradient/variable pairs that have a non-None gradient. Keras optimizers will throw an exception as soon as they encounter a disconnected variable/loss pair so we can't compute any gradients this way.
While we're on the topic of Keras optimizers, the docs refer to  being incremented if it's non-None (&lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam#apply_gradients&gt;https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam#apply_gradients&lt;/denchmark-link&gt;
). As far as I can tell, none of the arguments of any method take  and the optimizers don't update the  tensor created by . What am I missing here?
		</comment>
		<comment id='4' author='sharvil' date='2019-12-06T20:30:19Z'>
		optimizer.iterations is the global_step for each optimizer.
The apply_gradients for v2 optimizer (TF2) should just print a warning for variables without gradients. See: &lt;denchmark-link:https://github.com/tensorflow/models/blob/master/official/modeling/model_training_utils.py#L258&gt;https://github.com/tensorflow/models/blob/master/official/modeling/model_training_utils.py#L258&lt;/denchmark-link&gt;

I indeed have some variables that are not connected to loss inside bert.
minimize() will throw an error if there is None.
		</comment>
		<comment id='5' author='sharvil' date='2019-12-06T20:50:22Z'>
		&lt;denchmark-link:https://github.com/saberkun&gt;@saberkun&lt;/denchmark-link&gt;
 thanks for the tip on optimizer.iterations.
The exception is thrown when computing gradients, not when applying them. You're right that apply_gradients is just fine, but tf.keras.optimizers.Adam.get_gradients will throw. That's not the case for gradients computed through tf.train.*Optimizer.compute_gradients.
		</comment>
		<comment id='6' author='sharvil' date='2019-12-06T21:54:36Z'>
		Yes, we have reported this difference. &lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;

It is very common that variables without gradients and users may not want to take extra effort to organize which loss should take which set of trainable variables.
tf.train.*Optimizer does not work because distribution strategy takes a different way to handle cross-replica sum internally. To correctly use TPU, you need CrossSharedOptimizer, but it cannot be used for keras v2 optimizer: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/tpu/tpu_optimizer.py#L58&gt;https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/tpu/tpu_optimizer.py#L58&lt;/denchmark-link&gt;

In my experience, grads = tape.gradient(loss, training_vars) will not assert error for None gradients.
Does it fit to your case?
Here is the TF 1.x customized training example:
&lt;denchmark-link:https://github.com/tensorflow/tpu/blob/master/models/experimental/resnet50_keras/resnet50_ctl_tf1.py#L131&gt;https://github.com/tensorflow/tpu/blob/master/models/experimental/resnet50_keras/resnet50_ctl_tf1.py#L131&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='sharvil' date='2019-12-07T00:50:33Z'>
		I suppose we could use GradientTape to compute gradients instead of the optimizer's methods. It's less convenient, and more work on our part to switch everything over but it will work in general. Thanks for the suggestion.
Going back to the original bug report, it's still the case that AdamOptimizer can't be used with custom TPU training loops whereas other tf.train.*Optimizer classes can. It would be really nice to have consistency both in the semantics of gradient computations across APIs (i.e. anything that computes gradients should return None if the xs are not connected to the ys) and implementation behavior (i.e. optimizers can be swapped without introducing failures). Are these reasonable expectations to have of TensorFlow?
		</comment>
		<comment id='8' author='sharvil' date='2019-12-08T13:54:35Z'>
		I am working with TensorFlow r1.15 and am able to reproduce the same issue.
The custom TPU training loop works only with tf.train.GradientDescentOptimizer but not with any other optimizer.
Also, tf.keras.optimizers.Adam, RMSprop, Adagrad, etc. reports the same error!
		</comment>
		<comment id='9' author='sharvil' date='2019-12-12T02:21:51Z'>
		&lt;denchmark-link:https://github.com/sharvil&gt;@sharvil&lt;/denchmark-link&gt;
 -- in this case, the behavior of the v2 tf.keras.optimizers are consistent with the behavior of v1 tf.keras.optimizers, which also threw errors for missing gradients. Ideally, we would not break consistency with the tf.train optimizers, but in this case we had to break one way or the other. Given that we had the capacity to raise informative error messages, and that requiring filtering on the user side seems safer than potentially silently ignoring missing grads in user code, we decided to maintain consistency with the v1 tf.keras.optimizers.
		</comment>
		<comment id='10' author='sharvil' date='2019-12-14T21:18:59Z'>
		&lt;denchmark-link:https://github.com/karmel&gt;@karmel&lt;/denchmark-link&gt;
, thanks for the explanation. Ultimately, it's the TF team's decision on the direction to take with the APIs. As a user, my feedback is that the Keras v2 optimizers API is less convenient in this particular case and that friction is why we're choosing to stick with .
Back to this bug: I've experimented with other v1 optimizers including MomentumOptimizer, RMSPropOptimizer, and AdagradOptimizer. All of those work just fine. It's only AdamOptimizer that seems to run into issues (see Colab notebook I posted in the initial bug report for reproducible examples).
		</comment>
		<comment id='11' author='sharvil' date='2019-12-14T21:57:35Z'>
		Hello,
Thanks &lt;denchmark-link:https://github.com/karmel&gt;@karmel&lt;/denchmark-link&gt;
 for posting the colab notebooks. I'm looking into the same.
Actually, I'm facing similar issues and am curious to know what causes missing gradients to occur in code at the first place.
Any help in this regard of handling missing gradients in code would be appreciated.
Just to be specific, (TensorFlow r1.15, Debian Linux) I'm trying to train two models simultaneously inside the TPU custom training loop and both of them were constructed using simple tf.keras layers only.
tf.train.GradientDescentOptimizer works just fine but not others.
		</comment>
		<comment id='12' author='sharvil' date='2019-12-16T16:52:40Z'>
		&lt;denchmark-link:https://github.com/sharvil&gt;@sharvil&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/swghosh&gt;@swghosh&lt;/denchmark-link&gt;
 when you call get_gradients, it should return error information regarding which variable is missing the gradient I think?
		</comment>
		<comment id='13' author='sharvil' date='2019-12-17T07:25:22Z'>
		Hello &lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;
!
tf.gradients function is not able to provide any information regarding missing gradients. Also, when used with unconnected_gradients argument,
tf.gradients(loss_value, model.trainable_variables, unconnected_gradients='zero')
the same error is raised with with tf.train.AdamOptimizer.
		</comment>
		<comment id='14' author='sharvil' date='2019-12-19T06:46:22Z'>
		Can we move the discussion about gradients into another topic / bug? I was responding to the original question &lt;denchmark-link:https://github.com/martinwicke&gt;@martinwicke&lt;/denchmark-link&gt;
 asked about why we're not using Keras optimizers. That discussion doesn't seem to contribute to the issue at hand, i.e.  not working with custom TPU training loops.
		</comment>
		<comment id='15' author='sharvil' date='2019-12-21T05:19:52Z'>
		&lt;denchmark-link:https://github.com/sharvil&gt;@sharvil&lt;/denchmark-link&gt;
 we provided tf.keras optimizers for use of eager mode, distribution strategy, etc. Those things are bundled through 2.0. We probably don't have plans to fix this for tf.train optimizer anymore, given there will be no more 1.x major versions to be released.
That said, if you believe the None gradient is truly a hassle, we can consider making it warning instead of error. But might need help on provide concrete examples / use cases before we make make the decision
		</comment>
		<comment id='16' author='sharvil' date='2019-12-21T05:21:07Z'>
		On the other hand, using GradientTape is the way to go in 2.0. So filtering out variables with None gradient doesn't seem too bad
		</comment>
		<comment id='17' author='sharvil' date='2019-12-29T19:26:15Z'>
		&lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;
, the specific use-case is what I describe in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/34859#issuecomment-562721861&gt;my comment&lt;/denchmark-link&gt;
 and a concrete example of it is a BERT model that &lt;denchmark-link:https://github.com/saberkun&gt;@saberkun&lt;/denchmark-link&gt;
 describes in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/34859#issuecomment-562726594&gt;their comment&lt;/denchmark-link&gt;
.
I understand that there are no more TF 1.x releases, but the tf.train APIs are still available in TF 2.x  through tf.compat.v1.train. Will bugs in v1.compat still be fixed in new releases of TF?
		</comment>
		<comment id='18' author='sharvil' date='2020-01-13T09:09:41Z'>
		&lt;denchmark-link:https://github.com/sharvil&gt;@sharvil&lt;/denchmark-link&gt;
 Unfortunately we don't have further 1.x releases (i.e., 1.16), so please keep using keras optimizers. If filtering gradients for not connected variables is desired, which does make sense, maybe file another issue for it. Meanwhile closing this for now.
		</comment>
		<comment id='19' author='sharvil' date='2020-01-13T09:09:45Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34859&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34859&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='20' author='sharvil' date='2020-01-13T17:36:04Z'>
		&lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;
, I don't understand your last comment. I agree that there are no more 1.x releases but this API is also exposed in TensorFlow 2.x and fails unexpectedly, as reported in this bug.
Does your comment mean that bugs in v1.compat will not be fixed in newer TF 2.x releases?
		</comment>
		<comment id='21' author='sharvil' date='2020-01-13T17:43:12Z'>
		New features (such as TPU support outside of TPUEstimator) will not
necessarily be supported for old (compat.v1) APIs. I believe this is such a
case.
		</comment>
		<comment id='22' author='sharvil' date='2020-01-13T17:50:51Z'>
		Thanks for clarifying, &lt;denchmark-link:https://github.com/martinwicke&gt;@martinwicke&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='23' author='sharvil' date='2020-02-10T22:38:54Z'>
		Do we know what's the issue with AdamOptimizer? I'd like to fork it and fix it, because I do need it. keras.optimizers.Adam doesn't work for me. All other tf.train.*Optimizers work.
Here's a simple example where keras's optimizer doesn't work: &lt;denchmark-link:https://gist.github.com/sherjilozair/db9578005a1520d357e7bd8d4ddc5168&gt;https://gist.github.com/sherjilozair/db9578005a1520d357e7bd8d4ddc5168&lt;/denchmark-link&gt;

&lt;denchmark-link:https://hastebin.com/atidadubux.sql&gt;https://hastebin.com/atidadubux.sql&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>