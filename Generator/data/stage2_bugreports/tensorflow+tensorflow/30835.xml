<bug id='30835' author='ssable' open_date='2019-07-18T11:03:09Z' closed_time='2019-07-29T21:42:07Z'>
	<summary>Keras Adam optimizer unsupported by GPU</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): preinstalled in colab
TensorFlow version (use command below): 1.14.0
Python version: 3.6.8
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory: GPU on Colab

Describe the current behavior
I run a Keras Adam optimizer with a CNN network. The code works fine with CPU. If I turn on GPU in the notebook, and rerun the same code, I get an exception.
Describe the expected behavior
No exception
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Activate GPU.
&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow import keras

print(tf.version.VERSION)

training_samples = 100
input_shape = (16, 512, 1)

dataset = tf.data.Dataset.from_tensor_slices((tf.random_uniform([32, 16, 512, 1], dtype=tf.float32), tf.random_uniform([32], dtype=tf.float32)))
dataset = dataset.shuffle(32).repeat()

strategy = tf.distribute.MirroredStrategy()

with strategy.scope():
    initializer = 'he_uniform'
    nb_filts = [8, 16, 32, 400]
    out_size = 1
    model = tf.keras.models.Sequential()
    model.add(keras.layers.Conv2D(nb_filts[0], kernel_size=(3, 3),
                activation='relu', padding='same',
                kernel_initializer=initializer,
                bias_initializer=initializer, input_shape=input_shape))
    model.add(keras.layers.Conv2D(nb_filts[0], kernel_size=(3, 3),
                activation='relu', padding='same',
                kernel_initializer=initializer,
                bias_initializer=initializer))
    model.add(keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), 
                padding='same'))
    model.add(keras.layers.Conv2D(nb_filts[1], kernel_size=(3, 3),
                activation='relu', padding='same',
                kernel_initializer=initializer,
                bias_initializer=initializer))
    model.add(keras.layers.Conv2D(nb_filts[1], kernel_size=(3, 3),
                activation='relu', padding='same',
                kernel_initializer=initializer,
                bias_initializer=initializer))
    model.add(keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), 
                padding='same'))
    model.add(keras.layers.Conv2D(nb_filts[2], kernel_size=(3, 3),
                activation='relu', padding='same',
                kernel_initializer=initializer,
                bias_initializer=initializer))
    model.add(keras.layers.Conv2D(nb_filts[2], kernel_size=(3, 3),
                activation='relu', padding='same',
                kernel_initializer=initializer,
                bias_initializer=initializer))
    model.add(keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), 
                padding='same'))
    model.add(keras.layers.Flatten())
    model.add(keras.layers.Dense(1024, activation='relu',
                kernel_initializer=initializer,
                bias_initializer=initializer))
    model.add(keras.layers.Dense(nb_filts[3], activation='relu',
                kernel_initializer=initializer,
                bias_initializer=initializer))
    model.add(keras.layers.Dense(out_size))

    optimizer = tf.keras.optimizers.Adam(1e-3)
    model.compile(optimizer=optimizer, loss='mean_absolute_error', metrics=['mean_squared_error', 'mean_absolute_error'])

with strategy.scope():
    batch_size = 32
    nb_epochs = 1
    history = model.fit(dataset.batch(batch_size, drop_remainder=True), epochs=nb_epochs, steps_per_epoch=training_samples // batch_size, verbose=1)
&lt;/denchmark-code&gt;

Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
&lt;denchmark-code&gt;---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1355     try:
-&gt; 1356       return fn(*args)
   1357     except errors.OpError as e:

14 frames
InvalidArgumentError: No OpKernel was registered to support Op 'NcclAllReduce' used by {{node Adam/NcclAllReduce}}with these attrs: [shared_name="c0", T=DT_FLOAT, num_devices=1, reduction="sum"]
Registered devices: [CPU, GPU, XLA_CPU, XLA_GPU]
Registered kernels:
  &lt;no registered kernels&gt;

	 [[Adam/NcclAllReduce]]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
InvalidArgumentError: No OpKernel was registered to support Op 'NcclAllReduce' used by node Adam/NcclAllReduce (defined at &lt;ipython-input-8-7f03033581ef&gt;:4) with these attrs: [shared_name="c0", T=DT_FLOAT, num_devices=1, reduction="sum"]
Registered devices: [CPU, GPU, XLA_CPU, XLA_GPU]
Registered kernels:
  &lt;no registered kernels&gt;

	 [[Adam/NcclAllReduce]]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='ssable' date='2019-07-19T05:48:34Z'>
		&lt;denchmark-link:https://github.com/ssable&gt;@ssable&lt;/denchmark-link&gt;
 , I tried executing the given code on Colab with Tensorflow 1.14.0 with GPU activate. I didn't get any error. Can you check once and let us know, is still an issue. Thanks!
		</comment>
		<comment id='2' author='ssable' date='2019-07-19T09:42:16Z'>
		Thank you for your feedback.
I tried again and indeed the code works fine. Sorry for the confusion.
I modified the example above to add MirroredStrategy() which is what is causing the issue. You should be able to reproduce the bug this time.
		</comment>
		<comment id='3' author='ssable' date='2019-07-22T09:39:04Z'>
		&lt;denchmark-link:https://github.com/ssable&gt;@ssable&lt;/denchmark-link&gt;
 I tried reproducing the issue on Colab with the updated code and i didn't see any error. Let us know the expected behavior. Thanks!
		</comment>
		<comment id='4' author='ssable' date='2019-07-25T10:47:23Z'>
		I just tried creating a new notebook, activating GPU, putting the code above in the notebook and running and... I still get the exception above.
I am not sure what can be the difference between our 2 environments.
Here is the full callstack:
&lt;denchmark-code&gt;---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1355     try:
-&gt; 1356       return fn(*args)
   1357     except errors.OpError as e:

16 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1338       # Ensure any changes to the graph are reflected in the runtime.
-&gt; 1339       self._extend_graph()
   1340       return self._call_tf_sessionrun(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _extend_graph(self)
   1373     with self._graph._session_run_lock():  # pylint: disable=protected-access
-&gt; 1374       tf_session.ExtendSession(self._session)
   1375 

InvalidArgumentError: No OpKernel was registered to support Op 'NcclAllReduce' used by {{node Adam/NcclAllReduce}}with these attrs: [reduction="sum", shared_name="c0", T=DT_FLOAT, num_devices=1]
Registered devices: [CPU, GPU, XLA_CPU, XLA_GPU]
Registered kernels:
  &lt;no registered kernels&gt;

	 [[Adam/NcclAllReduce]]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
&lt;ipython-input-1-f5513cfe82bc&gt; in &lt;module&gt;()
     62     batch_size = 32
     63     nb_epochs = 1
---&gt; 64     history = model.fit(dataset.batch(batch_size, drop_remainder=True), epochs=nb_epochs, steps_per_epoch=training_samples // batch_size, verbose=1)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    647             steps_per_epoch=steps_per_epoch,
    648             validation_steps=validation_steps,
--&gt; 649             validation_freq=validation_freq)
    650 
    651     batch_size = self._validate_or_infer_batch_size(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_distributed.py in fit_distributed(model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq)
    141         validation_steps=validation_steps,
    142         validation_freq=validation_freq,
--&gt; 143         steps_name='steps_per_epoch')
    144 
    145 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)
    155 
    156   # Get step function and loop type.
--&gt; 157   f = _make_execution_function(model, mode)
    158   use_steps = is_dataset or steps_per_epoch is not None
    159   do_validation = val_inputs is not None

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py in _make_execution_function(model, mode)
    529   """Makes function to run one step of model execution."""
    530   if model._distribution_strategy:
--&gt; 531     return distributed_training_utils._make_execution_function(model, mode)
    532   return model._make_execution_function(mode)
    533 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/distribute/distributed_training_utils.py in _make_execution_function(model, mode)
    778   """Makes or reuses function to run one step of distributed model execution."""
    779   if is_distributing_by_cloning(model):
--&gt; 780     return _make_execution_function_with_cloning(model, mode)
    781 
    782   distributed_function = get_distributed_function(model, mode)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/distribute/distributed_training_utils.py in _make_execution_function_with_cloning(model, mode)
    862     distributed_function = _make_eager_execution_function(model, mode)
    863   else:
--&gt; 864     distributed_function = _make_graph_execution_function(model, mode)
    865 
    866   # We cache the distributed execution function on the model since creating

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/distribute/distributed_training_utils.py in _make_graph_execution_function(model, mode)
    889     # needed. This method does initialization or waiting for initialization
    890     # according to the context object of distribute coordinator.
--&gt; 891     init_restore_or_wait_for_variables()
    892 
    893     # Unwrap all the per device values returned from `call_for_each_replica`.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/distribute/distributed_training_utils.py in init_restore_or_wait_for_variables()
    368   if not worker_context or worker_context.experimental_should_init:
    369     # TODO(yuefengz): if checkpoints exist, restore from checkpoint.
--&gt; 370     K._initialize_variables(session)  # pylint: disable=protected-access
    371   else:
    372     _wait_for_variable_initialization(session)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in _initialize_variables(session)
    877     # marked as initialized.
    878     is_initialized = session.run(
--&gt; 879         [variables_module.is_variable_initialized(v) for v in candidate_vars])
    880     uninitialized_vars = []
    881     for flag, v in zip(is_initialized, candidate_vars):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    948     try:
    949       result = self._run(None, fetches, feed_dict, options_ptr,
--&gt; 950                          run_metadata_ptr)
    951       if run_metadata:
    952         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1171     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1172       results = self._do_run(handle, final_targets, final_fetches,
-&gt; 1173                              feed_dict_tensor, options, run_metadata)
   1174     else:
   1175       results = []

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1348     if handle is None:
   1349       return self._do_call(_run_fn, feeds, fetches, targets, options,
-&gt; 1350                            run_metadata)
   1351     else:
   1352       return self._do_call(_prun_fn, handle, feeds, fetches)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1368           pass
   1369       message = error_interpolation.interpolate(message, self._graph)
-&gt; 1370       raise type(e)(node_def, op, message)
   1371 
   1372   def _extend_graph(self):

InvalidArgumentError: No OpKernel was registered to support Op 'NcclAllReduce' used by node Adam/NcclAllReduce (defined at &lt;ipython-input-1-f5513cfe82bc&gt;:64) with these attrs: [reduction="sum", shared_name="c0", T=DT_FLOAT, num_devices=1]
Registered devices: [CPU, GPU, XLA_CPU, XLA_GPU]
Registered kernels:
  &lt;no registered kernels&gt;

	 [[Adam/NcclAllReduce]]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='ssable' date='2019-07-26T06:56:25Z'>
		I could able to reproduce the issue on Colab with TF 1.14.0. Look at the gist &lt;denchmark-link:https://colab.research.google.com/drive/1ryWl00ykTXykIO01Ne45eyjHW4W1XaOb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='6' author='ssable' date='2019-07-29T21:42:07Z'>
		Hi, this has been fixed already so if you use the latest versions of TF, you should not see this issue. I just tested with latest tf nightly (&lt;denchmark-link:https://pypi.org/project/tf-nightly-gpu/&gt;https://pypi.org/project/tf-nightly-gpu/&lt;/denchmark-link&gt;
) and it works fine.
		</comment>
		<comment id='7' author='ssable' date='2019-07-29T21:42:08Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=30835&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=30835&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>