<bug id='26107' author='antonsoch' open_date='2019-02-26T00:30:59Z' closed_time='2020-01-02T17:31:35Z'>
	<summary>Big memory consumption conv2d vs conv3d</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
VERSION="16.04.5 LTS (Xenial Xerus)"
TensorFlow installed from:
Binary
TensorFlow version:
1.12.0
Python version:
Python 3.5.2
CUDA/cuDNN version:
CUDA: 9.0, V9.0.176
cuDNN: 7.4.2
GPU model and memory:
NVIDIA Tesla V100 SXM2, 32GB
Driver Version: 384.145

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

Conv2D layer consumes a lot of memory comparing to the same operation performed by Conv3D.
I have 2 independent graphs:
1)
Input (shape=[16, 224, 224, 4])
conv2d (padding = SAME, filter=[3, 3, 4, 16])
bias_add (shape=[16])
2)
Input (shape=[1,16, 224, 224, 4])
conv3d (padding = SAME, filter=[1, 3, 3, 4, 16])
bias_add (shape=[16])
TF profiler reports that conv2d operation in first graph consumes 2900MB whereas conv3d that I presume should perform the same operation (cause leading dimensions of filter is 1) consumes 269.2MB which is an order of magnitude less.
Also for the graph with conv2d I see 2 additional layers are injected:
Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer consumes 16.78MB
Conv2D-0-0-TransposeNCHWToNHWC-LayoutOptimizer consumes 67.11MB
these layers are disappeared if I remove bias_add operation but memory consumption still stays the same.
Am i missing something obvious or my expectations regarding conv2d vs conv3d doing the same in my case are wrong?
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import os, time
os.environ["CUDA_VISIBLE_DEVICES"] = os.environ.get("CUDA_VISIBLE_DEVICES", "1")

import tensorflow as tf 
import numpy as np

def test_conv2d():
    input_ = tf.placeholder(tf.float32, shape=[16, 224, 224, 4], name="input_2d")
    filter_ = tf.get_variable(dtype=tf.float32, shape=[3, 3, 4, 16], name="filter_2d")
    conv = tf.nn.conv2d(input_, filter_, strides=(1,1,1,1), padding='SAME', dilations=(1,1,1,1))

    vBias1 = tf.get_variable(name='bias_2d', shape=[16], dtype=tf.float32)
    lBias1 = tf.nn.bias_add(conv, vBias1)
    return lBias1

def test_conv3d():
    input_ = tf.placeholder(tf.float32, shape=[1, 16, 224, 224, 4], name="input_3d")
    filter_ = tf.get_variable(dtype=tf.float32, shape=[1, 3, 3, 4, 16], name="filter_3d")
    conv = tf.nn.conv3d(input_, filter_, strides=(1,1,1,1,1), padding='SAME', dilations=(1,1,1,1,1))

    vBias1 = tf.get_variable(name='bias_3d', shape=[16], dtype=tf.float32)
    lBias1 = tf.nn.bias_add(conv, vBias1)
    return lBias1

gpu_options = tf.GPUOptions(allow_growth=True, visible_device_list=str(0))
config = tf.ConfigProto(gpu_options=gpu_options)

run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
run_metadata = tf.RunMetadata()

inputs = [
    {
        "l":test_conv2d(), 
        "i":{"input_2d:0": np.random.random([16, 224, 224, 4])}
    },
    {
        "l":test_conv3d(), 
        "i":{"input_3d:0": np.random.random([1, 16, 224, 224, 4])}
    }
]
outputs = []
for input in inputs:
    with tf.Session(config=config) as sess:
        sess.run(tf.global_variables_initializer())
        sess.run(
            input["l"], 
            input["i"],
            options = run_options,
            run_metadata = run_metadata)
        
        tf.profiler.profile(tf.get_default_graph(),
                                run_meta=run_metadata,
                                cmd='op',
                                options=tf.profiler.ProfileOptionBuilder.time_and_memory())
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Output&lt;/denchmark-h&gt;




Profile:
node name | requested bytes | total execution time | accelerator execution time | cpu execution time
Conv2D                      2900.02MB (100.00%, 97.19%),      2.70sec (100.00%, 99.83%),      14.11ms (100.00%, 97.40%),      2.68sec (100.00%, 99.84%)
Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer        16.78MB (2.81%, 0.56%),          4.23ms (0.17%, 0.16%),            48us (2.60%, 0.33%),          4.18ms (0.16%, 0.16%)
BiasAdd                               0B (0.00%, 0.00%),           207us (0.02%, 0.01%),           168us (2.27%, 1.16%),            39us (0.00%, 0.00%)
Conv2D-0-0-TransposeNCHWToNHWC-LayoutOptimizer        67.11MB (2.25%, 2.25%),           202us (0.01%, 0.01%),           160us (1.10%, 1.10%),            42us (0.00%, 0.00%)
VariableV2                        2.56KB (0.00%, 0.00%),            20us (0.00%, 0.00%),             0us (0.00%, 0.00%),            20us (0.00%, 0.00%)



Profile:
node name | requested bytes | total execution time | accelerator execution time | cpu execution time
Conv3D                      269.20MB (100.00%, 100.00%),     137.06ms (100.00%, 99.85%),      64.03ms (100.00%, 99.74%),      73.03ms (100.00%, 99.94%)
BiasAdd                               0B (0.00%, 0.00%),           197us (0.15%, 0.14%),           168us (0.26%, 0.26%),            29us (0.06%, 0.04%)
VariableV2                        2.56KB (0.00%, 0.00%),            12us (0.01%, 0.01%),             0us (0.00%, 0.00%),            12us (0.02%, 0.02%)
	</description>
	<comments>
		<comment id='1' author='antonsoch' date='2019-02-27T19:24:37Z'>
		Update.
On Ubuntu 18.04, CUDA: 10.0, V10.0.130, cuDNN: 7.5, Driver Version: 410.79, Python 3.6.7, tensorflow-gpu 1.13.1 I got the same result.
However on Windows 10 it's not reproduced on CPU:
â€¢	CUDA: 10.0, V10.0.130, cuDNN: 7.3.1, GeForce RTX 2080 Ti, Driver Version: 417.01, Python 3.7.2, tensorflow 1.13.1
CONV2D graph
Profile:
node name | requested bytes | total execution time | accelerator execution time | cpu execution time
*BiasAdd                      51.38MB (100.00%, 100.00%),      56.93ms (100.00%, 99.98%),             0us (0.00%, 0.00%),      56.93ms (100.00%, 99.98%)
VariableV2                        2.37KB (0.00%, 0.00%),            13us (0.02%, 0.02%),             0us (0.00%, 0.00%),            13us (0.02%, 0.02%)
*Here BiasAdd is fused with Conv2D. Also no Transpose layers.
CONV3D graph
Profile:
node name | requested bytes | total execution time | accelerator execution time | cpu execution time
Conv3D                       51.38MB (100.00%, 100.00%),      62.21ms (100.00%, 96.43%),             0us (0.00%, 0.00%),      62.21ms (100.00%, 96.43%)
BiasAdd                               0B (0.00%, 0.00%),          2.29ms (3.57%, 3.55%),             0us (0.00%, 0.00%),          2.29ms (3.57%, 3.55%)
VariableV2                        2.37KB (0.00%, 0.00%),            11us (0.02%, 0.02%),             0us (0.00%, 0.00%),            11us (0.02%, 0.02%)
		</comment>
		<comment id='2' author='antonsoch' date='2019-12-31T23:06:13Z'>
		This does not reproduce on 1.14.
Profile with reproducer run as-is: &lt;denchmark-link:https://gist.github.com/sanjoy/8eee43352c6498b8e62f70f425cb097a&gt;https://gist.github.com/sanjoy/8eee43352c6498b8e62f70f425cb097a&lt;/denchmark-link&gt;

Profile without the bias-add node: &lt;denchmark-link:https://gist.github.com/sanjoy/f4fed522dd9bebf5d7b55e048e740f18&gt;https://gist.github.com/sanjoy/f4fed522dd9bebf5d7b55e048e740f18&lt;/denchmark-link&gt;

Looks like we always use 51.38MB now.
		</comment>
		<comment id='3' author='antonsoch' date='2019-12-31T23:06:14Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26107&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26107&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='antonsoch' date='2019-12-31T23:14:34Z'>
		I'm sorry, my previous &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/26107#issuecomment-570003319&gt;comment&lt;/denchmark-link&gt;
 is incorrect.  I'm able to reproduce this in 1.14 (the logs I pasted were from a CPU run).
		</comment>
		<comment id='5' author='antonsoch' date='2020-01-02T17:31:35Z'>
		This seems to have been fixed in TF 2.0.
With &lt;denchmark-link:https://gist.github.com/sanjoy/b4fe8791a2f71625758ffbce5ad89e83&gt;https://gist.github.com/sanjoy/b4fe8791a2f71625758ffbce5ad89e83&lt;/denchmark-link&gt;
 I see:
&lt;denchmark-code&gt;TF 2.0, P100
Conv3D                      228.18MB (100.00%, 100.00%),    941.59ms (100.00%, 100.00%),             0us (0.00%, 0.00%),    941.59ms (100.00%, 100.00%)
Conv2D                      201.93MB (100.00%, 100.00%),     20.96ms (100.00%, 100.00%),             0us (0.00%, 0.00%),     20.96ms (100.00%, 100.00%)

TF 1.14, P100
Conv3D                      228.18MB (100.00%, 100.00%),    927.60ms (100.00%, 100.00%),             0us (0.00%, 0.00%),    927.60ms (100.00%, 100.00%)
Conv2D                     1550.98MB (100.00%, 100.00%),      15.96ms (100.00%, 99.89%),             0us (0.00%, 0.00%),      15.96ms (100.00%, 99.89%)

TF 2.0, Titan-V
Conv3D                      235.65MB (100.00%, 100.00%),     1.43sec (100.00%, 100.00%),             0us (0.00%, 0.00%),     1.43sec (100.00%, 100.00%)
Conv2D                      201.93MB (100.00%, 100.00%),     33.63ms (100.00%, 100.00%),             0us (0.00%, 0.00%),     33.63ms (100.00%, 100.00%)

TF 1.14, Titan-V
Conv3D                      235.65MB (100.00%, 100.00%),     1.46sec (100.00%, 100.00%),             0us (0.00%, 0.00%),     1.46sec (100.00%, 100.00%)
Conv2D                     2900.02MB (100.00%, 100.00%),      20.61ms (100.00%, 99.85%),             0us (0.00%, 0.00%),      20.61ms (100.00%, 99.85%)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='antonsoch' date='2020-01-02T17:31:37Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26107&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26107&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>