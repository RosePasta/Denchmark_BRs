<bug id='37932' author='nistrup' open_date='2020-03-26T08:13:25Z' closed_time='2020-03-27T16:54:11Z'>
	<summary>CUDNN_STATUS_INTERNAL_ERROR after hours of HyperParameter optimization.</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): I'm using completely standard Tensorflow / Keras implementations, see code below.
OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Windows 10
TensorFlow installed from (source or
binary): - TensorFlow version (use command below): tensorflow==2.1.0
Python version: - Bazel
version (if compiling from source): Python 3.7
CUDA/cuDNN version: - GPU model and memory: Nvidia GTX 1070 8GB, Cuda compilation tools release 10.1, V10.1.243

Describe the current behavior
I was training and tuning HyperParameters using "Keras Tuner" for about ~2 hours until I noticed that no new trials where created in the "Tuner project" folder, this is my training setup:
&lt;denchmark-link:https://user-images.githubusercontent.com/17565925/77623713-e17ee000-6f40-11ea-9d7b-bc9e2fad9b5f.png&gt;&lt;/denchmark-link&gt;

And this is my dynamic model:
&lt;denchmark-link:https://user-images.githubusercontent.com/17565925/77623844-24d94e80-6f41-11ea-831c-55d1eb62ee31.png&gt;&lt;/denchmark-link&gt;

After I noticed the stand-still I checked my terminal and got the following error:
&lt;denchmark-link:https://user-images.githubusercontent.com/17565925/77624487-3111db80-6f42-11ea-84e4-70a111ff2478.png&gt;&lt;/denchmark-link&gt;

Describe the expected behavior
I would expect the training to keep running. It completed 48 permutations training each one 3 times, i.e. 144 models without error, so the sudden interruption and kernel restart is weird to me!
Standalone code to reproduce the issue
In addition to the code referenced above, I've pretty much followed the setup in this Tensorflow tutorial: &lt;denchmark-link:https://www.tensorflow.org/tutorials/structured_data/time_series#single_step_model&gt;Time series forecasting&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='nistrup' date='2020-03-26T10:56:22Z'>
		&lt;denchmark-link:https://github.com/nistrup&gt;@nistrup&lt;/denchmark-link&gt;

please refer to these issues as per the error faced  #&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/29632&gt;#29632&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/33848&gt;#33848&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/28254&gt;#28254&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/24496&gt;comments&lt;/denchmark-link&gt;

let us know if it &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/34695&gt;helps&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='nistrup' date='2020-03-26T14:23:11Z'>
		&lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;
 Thank you!
I seem to get a different error now after training several models successfully I get this in JupyterLab:
&lt;denchmark-code&gt;InternalError:  [_Derived_]  Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 8, 128, 1, 90, 64, 128] 
	 [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]
	 [[StatefulPartitionedCall_2]] [Op:__inference_distributed_function_353504]

Function call stack:
distributed_function -&gt; distributed_function -&gt; distributed_function
&lt;/denchmark-code&gt;

And this in my terminal window:
&lt;denchmark-code&gt;2020-03-26 15:14:33.877175: E tensorflow/stream_executor/dnn.cc:596] CUDNN_STATUS_INTERNAL_ERROR
in tensorflow/stream_executor/cuda/cuda_dnn.cc(1921): 'cudnnRNNBackwardData( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, output_desc.handles(), output_data.opaque(), output_desc.handles(), output_backprop_data.opaque(), output_h_desc.handle(), output_h_backprop_data.opaque(), output_c_desc.handle(), output_c_backprop_data.opaque(), rnn_desc.params_handle(), params.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), input_desc.handles(), input_backprop_data-&gt;opaque(), input_h_desc.handle(), input_h_backprop_data-&gt;opaque(), input_c_desc.handle(), input_c_backprop_data-&gt;opaque(), workspace.opaque(), workspace.size(), reserve_space_data-&gt;opaque(), reserve_space_data-&gt;size())'
2020-03-26 15:14:33.911329: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at cudnn_rnn_ops.cc:1922 : Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 8, 128, 1, 90, 64, 128]
2020-03-26 15:14:33.938015: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 8, 128, 1, 90, 64, 128]
         [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]
2020-03-26 15:14:33.959686: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: {{function_node __inference___backward_cudnn_lstm_with_fallback_350144_350320_specialized_for_StatefulPartitionedCall_2_at___inference_distributed_function_353504}} {{function_node __inference___backward_cudnn_lstm_with_fallback_350144_350320_specialized_for_StatefulPartitionedCall_2_at___inference_distributed_function_353504}} Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 8, 128, 1, 90, 64, 128]
         [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]
         [[StatefulPartitionedCall_2]]
&lt;/denchmark-code&gt;

Any ideas?
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

EDIT: I've tried limiting the memory usage, using the nightly build and setting the growth to True (gpu = tf.config.experimental.list_physical_devices('GPU'), tf.config.experimental.set_memory_growth(gpu[0], True) to see if it's an issue with GPU memory but I still get the following even when limiting to 2GB:
In Jupyter:
&lt;denchmark-code&gt;[_Derived_]  Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 56, 64, 1, 90, 64, 64] 
	 [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]
	 [[PartitionedCall_2]] [Op:__inference_train_function_47260]

Function call stack:
train_function -&gt; train_function -&gt; train_function
&lt;/denchmark-code&gt;

In my terminal window:
&lt;denchmark-code&gt;2020-03-26 21:34:20.415064: E tensorflow/stream_executor/dnn.cc:613] CUDNN_STATUS_INTERNAL_ERROR
in tensorflow/stream_executor/cuda/cuda_dnn.cc(1986): 'cudnnRNNBackwardData( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, output_desc.handles(), output_data.opaque(), output_desc.handles(), output_backprop_data.opaque(), output_h_desc.handle(), output_h_backprop_data.opaque(), output_c_desc.handle(), output_c_backprop_data.opaque(), rnn_desc.param
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='nistrup' date='2020-03-27T09:54:49Z'>
		&lt;denchmark-link:https://github.com/nistrup&gt;@nistrup&lt;/denchmark-link&gt;

with reference to the error shared please find these example issues, let us know if it helps:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/35950&gt;#35950&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/33536&gt;#33536&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/33924&gt;#33924&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='nistrup' date='2020-03-27T11:48:28Z'>
		
@nistrup
with reference to the error shared please find these example issues, let us know if it helps:
#35950 #33536 #33924

I'm afraid none of this has worked. I'll spend some time trying to do a completely clean install and maybe try to downgrade my GPU drivers as well. So far nothing has worked. The models train completely fine using CPU only and breaks seemingly at random when using GPU, like in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/37942&gt;#37942&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='nistrup' date='2020-03-27T12:45:56Z'>
		Hi &lt;denchmark-link:https://github.com/nistrup&gt;@nistrup&lt;/denchmark-link&gt;
 ,
please check my issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/37942&gt;#37942&lt;/denchmark-link&gt;
. I might have found a solution for your problem as well. 
		</comment>
		<comment id='6' author='nistrup' date='2020-03-27T16:23:03Z'>
		
Hi @nistrup ,
please check my issue #37942. I might have found a solution for your problem as well. ðŸ™‚

Thanks! It seems to have fixed one of my issues at least! Now I'm faced with this instead:
After following the solutions suggested like:

Allowing GPU Memory Growth
Using batch_input_shape instead of input_shape
Using drop_remainder=True when creating batches

I'm faced with the following after the first successfully trained model:
&lt;denchmark-code&gt;2020-03-27 17:21:28.275596: F .\tensorflow/core/util/gpu_launch_config.h:129] Check failed: work_element_count &gt; 0 (0 vs. 0)
[I 17:21:29.843 LabApp] KernelRestarter: restarting kernel (1/5), keep random ports
kernel 2bac517a-c195-47ca-952b-c25881cf0757 restarted
&lt;/denchmark-code&gt;

And the Jupyter Kernel crashes. Any ideas? :)
		</comment>
		<comment id='7' author='nistrup' date='2020-03-27T16:24:37Z'>
		What is your batch_size? What value have you chosen?
(As part of batch_input_shape=(batch_size, n_timesteps, n_features))?
		</comment>
		<comment id='8' author='nistrup' date='2020-03-27T16:27:54Z'>
		
What is your batch_size?
(As part of batch_input_shape=(batch_size, n_timesteps, n_features))?

Currently 256, I'm using a setup where each HyperParameter permutation model is trained 3 times to give me an average score, the first model completes all 3 runs but then the kernel seems to die when starting the first run of the 2nd model.
Edit:
I should add that this is how I construct my training and validation data, where BATCH_SIZE = 256:
&lt;denchmark-code&gt;train_data_single = tf.data.Dataset.from_tensor_slices((x_train_single, y_train_single))
train_data_single = train_data_single.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).repeat()

val_data_single = tf.data.Dataset.from_tensor_slices((x_val_single, y_val_single))
val_data_single = val_data_single.batch(BATCH_SIZE, drop_remainder=True).repeat()
&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='nistrup' date='2020-03-27T16:29:37Z'>
		I also had this issues with such a high batch_size. 197 for me, for instance, did not work out and it lead to crashes with the same error.
Currently I use some value between 30 and 130 as batch_size and it works. Remember that it needs to be a divisor of len(X). ðŸ˜„
		</comment>
		<comment id='10' author='nistrup' date='2020-03-27T16:33:26Z'>
		
I also had this issues with such a high batch_size. 197 for me, for instance, did not work out and it lead to crashes with the same error.
Currently I use some value between 60 and 160 as batch_size and it works. Remember that it needs to be a divisor of len(X). ðŸ˜„

I just tried with BATCH_SIZE = 64 and the exact same thing happens, i.e. kernel crashes with Check failed: work_element_count &gt; 0 (0 vs. 0)
Regarding the fact that it needs to be a divisor of len(x_train), I think this should fix "itself" when constructing training data using the .batch(BATCH_SIZE, drop_remainder=True)
		</comment>
		<comment id='11' author='nistrup' date='2020-03-27T16:37:21Z'>
		Concerning drop_remainder I would think so too.
I just checked and found that my error was slightly different:
&lt;denchmark-code&gt;2020-03-27 15:01:57.982960: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2020-03-27 15:01:57.983072: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1
&lt;/denchmark-code&gt;

So my advice maybe (lowering the batch_size) was not right.
I fear, that you have gotten off the path that we shared, as your setup starts beginning to be different than mine. I really am not so happy about the current state of GPU-accelerated in Tensorflow/Keras, it is really tough to use. (Especially, as the CPU-equivalents just work.) As I could not reproduce my problem with Google Colab, I really hope, that it is just an issue with my platform.
Godspeed!
		</comment>
		<comment id='12' author='nistrup' date='2020-03-27T16:42:39Z'>
		I also think our paths have diverged slightly at this point! :)
Thanks a bunch for trying to help me out though, thoroughly appreciated!
		</comment>
		<comment id='13' author='nistrup' date='2020-03-27T16:54:11Z'>
		I think I will close this issue and create a new one related to my current issue instead since this seems to be unrelated at this point. Thanks! :)
		</comment>
		<comment id='14' author='nistrup' date='2020-03-27T16:54:12Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37932&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37932&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='15' author='nistrup' date='2020-05-01T10:28:27Z'>
		&lt;denchmark-link:https://github.com/nistrup&gt;@nistrup&lt;/denchmark-link&gt;

Could you elaborate something? As I understood - when setting the  then you are forcing yourself to use the input data of given dimensions. So, then, if I later wanted to predict the outcome based on the input of batch size = 1 - that wouldn't be possible, is that correct?
In my current implementation I use a batch size of 2048 for training, and I want to be able to use a batch size of 1 for predictions later on. Will it still be possible to use your proposed solution? (sorry I am new in ML)
On a side note, I would like to ask whether there is a difference between using a dataset with a given batch size and how is it different/how does it work with conjunction with the back-size that we define for the model layers and fit method..?
i.e.
&lt;denchmark-code&gt;batch_size = 2048
epochs = 100
train_size = TRAIN_X.shape[0]
validation_size = VALIDATION_X.shape[0]

compute_steps_per_epoch = lambda x: int(math.ceil(1. * x / batch_size))
	
steps_per_epoch = compute_steps_per_epoch(train_size)
val_steps = compute_steps_per_epoch(validation_size)
&lt;/denchmark-code&gt;

what's the difference between
&lt;denchmark-code&gt;features = tf.convert_to_tensor(TRAIN_X, dtype=tf.float32)
labels = tf.convert_to_tensor(TRAIN_y, dtype=tf.float32)

features2 = tf.convert_to_tensor(VALIDATION_X, dtype=tf.float32)
labels2 = tf.convert_to_tensor(VALIDATION_y, dtype=tf.float32)

# Train model
history = model.fit(
	x=features,
	y=labels,
	steps_per_epoch=steps_per_epoch,
	epochs=epochs,
	validation_data=(features2, labels2),
	validation_steps=val_steps,
	callbacks=[tensor_board, post_processing_callback, checkpoint],
	class_weight=d_class_weights
)
&lt;/denchmark-code&gt;

and explicitly generating batches:
&lt;denchmark-code&gt;buffer_size_train = train_size
train_data_ds = tf.data.Dataset.from_tensor_slices((TRAIN_X, TRAIN_y))
train_data_batch = train_data_ds\
	.shuffle(buffer_size_train)\
	.batch(batch_size, drop_remainder=True)\
	.repeat()

buffer_size_validation = validation_size
validation_data_ds = tf.data.Dataset.from_tensor_slices((VALIDATION_X, VALIDATION_y))
validation_data_batch = validation_data_ds\
	.shuffle(buffer_size_validation)\
	.batch(batch_size, drop_remainder=True)\
	.repeat()

steps_per_epoch = compute_steps_per_epoch(train_size)
val_steps = compute_steps_per_epoch(validation_size)

history = model.fit(
	x=train_data_batch,
	steps_per_epoch=steps_per_epoch,
	epochs=epochs,

	validation_data=validation_data_batch,
	validation_steps=val_steps,

	callbacks=[tensor_board, post_processing_callback, checkpoint],
	class_weight=d_class_weights
)
&lt;/denchmark-code&gt;

edit: don't know if it currently matters but the model is built either:
&lt;denchmark-code&gt;model.add(LSTM(128, input_shape=(TRAIN_X.shape[1], TRAIN_X.shape[2]), return_sequences=False))
&lt;/denchmark-code&gt;

or
&lt;denchmark-code&gt;
model.add(LSTM(128, batch_input_shape=(batch_size, TRAIN_X.shape[1], TRAIN_X.shape[2]), return_sequences=False))

model.add(Dense(32, activation='relu'))
model.add(Dropout(0.2))

&lt;/denchmark-code&gt;

I am trying to fix the issue mentioned in the subject :)
		</comment>
		<comment id='16' author='nistrup' date='2020-10-12T09:51:30Z'>
		I have recently had the exact same problem on Windows 10 and I figured out a solution.
The machine used has got a dual graphic card, one of them is an NVidia Quadro P5200 (16GB) and the second one a standard Intel 630 used for display. I am running Keras 2.3.1 (on top of TensorFlow GPU 2.1.0) with Python 3.7.7.
The problem mentionned above was occurring regardless of the batch_size (it also failed when batch_size was set to 1).
What worked for me was to switch the NVidia card to TCC mode by running the following command (you need to Run As Admin).
"C:\Program Files\NVIDIA Corporation\NVSMI\nvidia-smi.exe" -g 0 -dm 1
Then reboot.
Then go to the Device Manager and re-enable the device (I am still unsure why this steps is required, but it seems to be).
After this, confirm the NVidia card has switched to TCC mode by running (you need to Run As Admin).
"C:\Program Files\NVIDIA Corporation\NVSMI\nvidia-smi.exe"
which should output something like this:
&lt;denchmark-code&gt;+-----------------------------------------------------------------------------+
| NVIDIA-SMI 451.77       Driver Version: 451.77       CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Quadro P5200        TCC  | 00000000:01:00.0 Off |                  N/A |
| N/A   51C    P8     5W /  N/A |  15705MiB / 16296MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+
&lt;/denchmark-code&gt;

(Note that once the card has switched to TCC mode, it disappear from the Task Manager performance tab.
I hope this helps.
		</comment>
	</comments>
</bug>