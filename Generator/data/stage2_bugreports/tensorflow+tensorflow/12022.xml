<bug id='12022' author='justinshapiro' open_date='2017-08-03T22:08:10Z' closed_time='2018-04-18T18:21:27Z'>
	<summary>Error exporting TensorForestEstimator model for serving</summary>
	<description>
&lt;denchmark-h:h3&gt;Problem&lt;/denchmark-h&gt;

I am trying to host a TensorForestEstimator model on Google Cloud's ML Engine. Everything works right, but at the very end the model fails to export with stack trace:
&lt;denchmark-code&gt;Traceback (most recent call last):
[...]
File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.py", line 502, in train_and_evaluate
  export_results = self._maybe_export(eval_result)
File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.py", line 597, in _maybe_export
  eval_result=eval_result))
File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/export_strategy.py", line 87, in export
  return self.export_fn(estimator, export_path, **kwargs)
File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py", line 412, in export_fn
  checkpoint_path=checkpoint_path)
File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py", line 1280, in export_savedmodel
  actual_default_output_alternative_key)
File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py", line 252, in build_all_signature_defs
  for input_key, inputs in input_alternatives.items()
File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py", line 254, in &lt;dictcomp&gt;
  in output_alternatives.items()}
File "/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py", line 119, in build_standardized_signature_def
  input_tensors, output_tensors)
File "/root/.local/lib/python2.7/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py", line 146, in predict_signature_def
  signature_constants.PREDICT_METHOD_NAME)
File "/root/.local/lib/python2.7/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py", line 45, in build_signature_def
  signature_def.outputs[item].CopyFrom(outputs[item])
TypeError: None has type NoneType, but expected one of: bytes, unicode
&lt;/denchmark-code&gt;

Based on that trace, I'm thinking the error is in the make_export_strategy function with default_output_alternative_key=None. So what I did is set default_output_alternative_key='default' but then got the error:
&lt;denchmark-code&gt;ValueError: Requested default_output_alternative: default, but available output_alternatives are: [None]
&lt;/denchmark-code&gt;

So this shows that there are no output alternatives and my model is single-headed. Here is the code:
&lt;denchmark-code&gt;def serving_input_fn():
    feature_placeholders = {
    column['name']: tf.placeholder(dtype=column['dtype'], shape=[None])
    for column in columns_list if column['derived'] == 'N' and column['column_role'] != 'label'
    }

    features = {
        key: tf.expand_dims(tensor, -1)
        for key, tensor in feature_placeholders.items()
    }

    return InputFnOps(
        features=features,
        labels=None,
        default_inputs=feature_placeholders
    )

def get_experiment_fn(args):
    def _experiment(run_config, hparams):
        return Experiment(
            estimator=TensorForestEstimator(
                params=ForestHParams(
                    num_trees=args.num_trees,
                    max_nodes=10000,
                    min_split_samples=2,
                    num_features=7,
                    num_classes=args.num_projections,
                    regression=True
                ),
                model_dir=args.job_dir,
                graph_builder_class=RandomForestGraphs,
                config=run_config,
                report_feature_importances=True,
            ),
            train_input_fn=get_input_fn(
                project_name=args.project,
                data_location=args.train_data,
                dataset_size=args.train_size,
                batch_size=args.train_batch_size
            ),
            train_steps=args.train_steps,
            eval_input_fn=get_input_fn(
                project_name=args.project,
                data_location=args.eval_data,
                dataset_size=args.eval_size,
                batch_size=args.eval_batch_size
            ),
            eval_steps=args.eval_steps,
            eval_metrics=get_eval_metrics(),
            export_strategies=[
                make_export_strategy(
                    serving_input_fn,
                    default_output_alternative_key=None,
                    exports_to_keep=1
                )
            ]
        )
    return _experiment


def main():
    args = get_arg_parser().parse_args()

    learn_runner.run(
        experiment_fn=get_experiment_fn(args),
        run_config=RunConfig(model_dir=args.job_dir),
        hparams=HParams(**args.__dict__)
    )

if __name__ == '__main__':
    main()
&lt;/denchmark-code&gt;

This seems like a bug, but I could be wrong.
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;== cat /etc/issue ===============================================
Darwin mbmagenic12 16.5.0 Darwin Kernel Version 16.5.0: Fri Mar  3 16:52:33 PST 2017; root:xnu-3789.51.2~3/RELEASE_X86_64 x86_64
Mac OS X 10.12.4

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 8.1.0 (clang-802.0.42)
Target: x86_64-apple-darwin16.5.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin

== uname -a =====================================================
Darwin mbmagenic12 16.5.0 Darwin Kernel Version 16.5.0: Fri Mar  3 16:52:33 PST 2017; root:xnu-3789.51.2~3/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy (1.13.1)
protobuf (3.1.0.post1)
tensorflow (1.2.1)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.2.1
tf.GIT_VERSION = v1.2.0-5-g435cdfc
tf.COMPILER_VERSION = v1.2.0-5-g435cdfc
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='justinshapiro' date='2017-08-24T23:36:12Z'>
		We ran into this issue as well. I don't think your problem is related to default_output_alternative_key being None rather, it looks like TensorForestEstimator currently populates model_ops.predictions only if a keys_column is provided[1]. Without model_ops.predictions populated, default_outputs in saved_model_export will lack a default key[2]. As a result, this will make the model end up with no heads and eventually the export will fail.
TLDR: Populating keys_column in TensorForestEstimator should fix your problem. That being said, it's definitely a bug if an optional argument is really required for the model to function properly. So I would leave this issue open.
As a side: I did not really understand why requiring keys_name[3] to be part of the incomingfeature_columns list (after all, it's not necessary a feature) and then pop it?
[1]
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensor_forest/client/random_forest.py#L235&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensor_forest/client/random_forest.py#L235&lt;/denchmark-link&gt;

[2]
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py#L217&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py#L217&lt;/denchmark-link&gt;

[3]
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensor_forest/client/random_forest.py#L157&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensor_forest/client/random_forest.py#L157&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='justinshapiro' date='2017-08-24T23:37:08Z'>
		Should also mention that the workaround (populating key_column) only helps with tensorflow 1.3.
		</comment>
		<comment id='3' author='justinshapiro' date='2017-08-26T04:58:53Z'>
		It looks like upgrading to tensorflow 1.3 fixed the issue. I did not have to even populate keys_name, the graph was saved as normal after training/evaluation.
However, I need a fix for tensorflow 1.2 since I am using Google Cloud ML Engine and 1.2 is the highest version supported.
		</comment>
		<comment id='4' author='justinshapiro' date='2017-08-28T03:34:05Z'>
		&lt;denchmark-link:https://github.com/caisq&gt;@caisq&lt;/denchmark-link&gt;
, are you able to take a look?
		</comment>
		<comment id='5' author='justinshapiro' date='2017-12-07T18:58:02Z'>
		&lt;denchmark-link:https://github.com/justinshapiro&gt;@justinshapiro&lt;/denchmark-link&gt;
: Are you able to resolve it now? Any temporary wayout?
		</comment>
		<comment id='6' author='justinshapiro' date='2017-12-22T19:01:13Z'>
		It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
		</comment>
		<comment id='7' author='justinshapiro' date='2018-01-06T18:54:49Z'>
		Nagging Awaiting TensorFlower: It has been 14 days with no activityand the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
		</comment>
		<comment id='8' author='justinshapiro' date='2018-01-24T13:12:35Z'>
		Nagging Awaiting TensorFlower: It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
		</comment>
		<comment id='9' author='justinshapiro' date='2018-02-08T19:31:21Z'>
		Nagging Awaiting TensorFlower: It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
		</comment>
		<comment id='10' author='justinshapiro' date='2018-02-23T14:09:22Z'>
		Nagging Awaiting TensorFlower: It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
		</comment>
		<comment id='11' author='justinshapiro' date='2018-03-10T13:21:04Z'>
		Nagging Awaiting TensorFlower: It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
		</comment>
		<comment id='12' author='justinshapiro' date='2018-03-25T12:39:20Z'>
		Nagging TensorFlower: It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
		</comment>
		<comment id='13' author='justinshapiro' date='2018-04-10T12:36:31Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/skye&gt;@skye&lt;/denchmark-link&gt;
: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='14' author='justinshapiro' date='2018-04-18T18:21:27Z'>
		I'm assuming this is resolved now. Please comment or reopen if I'm mistaken.
		</comment>
	</comments>
</bug>