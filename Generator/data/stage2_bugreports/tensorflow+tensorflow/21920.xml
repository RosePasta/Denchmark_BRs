<bug id='21920' author='weiwancheng' open_date='2018-08-28T11:46:07Z' closed_time='2018-10-25T15:35:49Z'>
	<summary>BeamSearchDecoder bug？</summary>
	<description>
it seems that the beam search not end with 'eos' symbol. So, there are same sentences generated by beam search. who can slove this problem?
code:
                    inference_decoder = BeamSearchDecoder( cell=self.decoder_cell, embedding=embed_and_input_proj, start_tokens=start_tokens, end_token=end_token, initial_state=self.decoder_initial_state, beam_width=self.beam_width, output_layer=self.decoder_output_projection, )
beam_size=5,result:
sentence1： 给您带来不好的体验了，小曦也是十分抱歉的 eos unk unk unk unk unk
sentence2： 给您带来不好的体验了，非常抱歉 eos  eos  eos eos eos eos eos eos eos eos eos
sentence3： 给您带来不好的体验了，非常抱歉 eos eos eos eos eos eos eos eos eos eos unk
sentence4： 给您带来不好的体验了，非常抱歉 eos eos eos eos eos eos eos unk unk unk unk
sentence5： 给您带来不好的体验了，非常抱歉 eos unk unk unk unk unk unk unk unk unk unk
above. sentence2,sentence3,sentence4,sentence5 is alike.
	</description>
	<comments>
		<comment id='1' author='weiwancheng' date='2018-08-30T03:35:28Z'>
		Any comments? &lt;denchmark-link:https://github.com/bmabey&gt;@bmabey&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='weiwancheng' date='2018-08-30T11:43:31Z'>
		&lt;denchmark-link:https://github.com/georgesterpu&gt;@georgesterpu&lt;/denchmark-link&gt;
 can you have a look?
		</comment>
		<comment id='3' author='weiwancheng' date='2018-09-03T22:16:06Z'>
		Hello &lt;denchmark-link:https://github.com/weiwancheng&gt;@weiwancheng&lt;/denchmark-link&gt;
,
From your post it is unclear whether you are using a well tested framework or your own. Could you please give us more details ?
I have to assume that you are looking at the outputs (predicted_ids) of the seq2seq.dynamic_decode method, instantiated with the beam search decoder you mentioned, and that you translated the numerical ids back into symbols from your vocabulary. dynamic_decode will also return you the final sequence lengths. Looking at my own results, they are typically the lengths up to the first occurrence of the eos token (but I can also see that the decoder sometimes predict a few more extra characters when eos is fed back as input)). In your case, the lengths should be [9, 4, 4, 4, 4], and you can ignore the rest.
Unfortunately I did not study the implementation of this function to help you further. I assume it depends a lot on the task that you are trying to solve, and also on the checkpoint you are loading (is it a well trained model ?). At least in my case, all the beams appear to be unique (just sampling a few of them, nothing rigorous). I have also seen that the inputs and the outputs of the decoder have to be properly formatted in order to obtain sound results (e.g. where you place the GO and EOS in the sequence, ensuring that the provided sequence length also includes these symbols). If it helps you, I will open-source our seq2seq ASR projects this week to see an example, yet the nmt tutorial might be an even better resource.
		</comment>
		<comment id='4' author='weiwancheng' date='2018-09-04T06:52:52Z'>
		&lt;denchmark-link:https://github.com/georgesterpu&gt;@georgesterpu&lt;/denchmark-link&gt;
 Thank you for your reply.
Yes,I have done something as your said. About beam search API, the main part in my code as follow :

....

I feel very strange, I did not find any problems. This problem bothering me many days. I have trained the model for other tasks, but this problem still existed. There were still repeated sentences in the prediction, and the encounter with eos does not stop. But It stopped when the length is set length.
Anyway,Thank you very much. you are very kindful. I am very looking forward to your projects to open.
		</comment>
		<comment id='5' author='weiwancheng' date='2018-09-08T21:30:59Z'>
		&lt;denchmark-link:https://github.com/weiwancheng&gt;@weiwancheng&lt;/denchmark-link&gt;
 Here it is: &lt;denchmark-link:https://github.com/georgesterpu/Sigmedia-AVSR&gt;https://github.com/georgesterpu/Sigmedia-AVSR&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='weiwancheng' date='2018-09-09T04:08:09Z'>
		&lt;denchmark-link:https://github.com/georgesterpu&gt;@georgesterpu&lt;/denchmark-link&gt;
 OK,Thanks
		</comment>
		<comment id='7' author='weiwancheng' date='2018-10-25T12:41:13Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='8' author='weiwancheng' date='2018-10-25T15:35:49Z'>
		Marking this as 'fixed'.
		</comment>
		<comment id='9' author='weiwancheng' date='2018-11-08T03:21:18Z'>
		
@georgesterpu Thank you for your reply.
Yes,I have done something as your said. About beam search API, the main part in my code as follow :
inference_decoder= BeamSearchDecoder( cell=self.decoder_cell, embedding=embed_and_input_proj, start_tokens=start_tokens, end_token=end_token, initial_state=self.decoder_initial_state, beam_width=self.beam_width, output_layer=self.decoder_output_projection, )
....
(self.decoder_outputs_decode,self.final_state,_ ) = (seq2seq.dynamic_decode(decoder=inference_decoder,output_time_major=self.time_major,maximum_iterations=max_decode_step,swap_memory=True,scope=decoder_scope))
I feel very strange, I did not find any problems. This problem bothering me many days. I have trained the model for other tasks, but this problem still existed. There were still repeated sentences in the prediction, and the encounter with eos does not stop. But It stopped when the length is set length.
Anyway,Thank you very much. you are very kindful. I am very looking forward to your projects to open.

I meet the same problem. How do you solve it?
		</comment>
		<comment id='10' author='weiwancheng' date='2018-11-08T08:39:00Z'>
		&lt;denchmark-link:https://github.com/peinbill&gt;@peinbill&lt;/denchmark-link&gt;
 sorry, i have not  solved this problem. i only change to use greedy search. This problem seems that tensorflow's bug.  if you make progress on this problem, we can communicate more.
		</comment>
		<comment id='11' author='weiwancheng' date='2018-11-11T13:30:40Z'>
		
@peinbill sorry, i have not solved this problem. i only change to use greedy search. This problem seems that tensorflow's bug. if you make progress on this problem, we can communicate more.

all right，thanks a lot.
		</comment>
	</comments>
</bug>