<bug id='31893' author='csachs' open_date='2019-08-22T13:45:42Z' closed_time='2019-08-30T20:22:44Z'>
	<summary>[TF2] tf.saved_model.save fails on models re-using other models</summary>
	<description>
System information

Have I written custom code: Yes.
OS Platform and Distribution: Ubuntu 18.04
TensorFlow installed from (source or binary): binary
TensorFlow version: 2.0.0-dev20190821
Python version: 3.7.3
CUDA/cuDNN version: 10.0
GPU model and memory: GTX 1080 Ti

This bug might be related to the behavior observed by &lt;denchmark-link:https://github.com/cysmnl&gt;@cysmnl&lt;/denchmark-link&gt;
 in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/28923#issuecomment-514736847&gt;#28923 (comment)&lt;/denchmark-link&gt;
 . However I think the bug present is distinct from the original bug described in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/28923&gt;#28923&lt;/denchmark-link&gt;
.
Describe the current behavior
The minimal working example below fails with the following exception:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "mwe.py", line 33, in &lt;module&gt;
    tf.saved_model.save(second_convolution_model, '/tmp/model2')  # does NOT work
  File ".../python3.7/site-packages/tensorflow_core/python/saved_model/save.py", line 860, in save
    meta_graph_def, saveable_view, signatures)
  File ".../python3.7/site-packages/tensorflow_core/python/saved_model/save.py", line 590, in _fill_meta_graph_def
    signatures = _generate_signatures(signature_functions, resource_map)
  File ".../python3.7/site-packages/tensorflow_core/python/saved_model/save.py", line 464, in _generate_signatures
    function, mapped_inputs, resource_map)
  File ".../python3.7/site-packages/tensorflow_core/python/saved_model/save.py", line 416, in _call_function_with_mapped_captures
    function.graph.captures, resource_map)
  File ".../python3.7/site-packages/tensorflow_core/python/saved_model/save.py", line 338, in _map_captures_to_created_tensors
    .format(interior))
AssertionError: Tried to export a function which references untracked object Tensor("StatefulPartitionedCall/args_1:0", shape=(), dtype=resource).TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.
&lt;/denchmark-code&gt;

As a side note, the error message misses a space after the period: 


tensorflow/tensorflow/python/saved_model/save.py


         Line 334
      in
      d1583ca






 ("Tried to export a function which references untracked object {}." 





Describe the expected behavior
Possibility to save models with tf.saved_model.save as they would be saveable with model.save.
Code to reproduce the issue
(This is example is nonsensical, but shows the problem. I've boiled down the problem to a MWE.)
import tensorflow as tf
from tensorflow.python.keras import Model
from tensorflow.python.keras.layers import Input, Conv1D, Lambda

first_input = Input(shape=(1,))
first_result = Conv1D(filters=1, kernel_size=1)(first_input[..., tf.newaxis])
first_convolution_model = Model(inputs=[first_input], outputs=[first_result])


def inner_loop(tensor):
    the_len = tf.shape(tensor)[0]
    collector = tf.TensorArray(tf.float32, size=the_len)

    _, collector = tf.while_loop(
        cond=lambda i, _: i &lt; the_len,
        body=lambda i, c_: (i+1, c_.write(i, first_convolution_model(tensor[i:i + 1]))),
        loop_vars=(0, collector)
    )

    return collector.stack()


second_input = Input(shape=(1,))
second_result = Lambda(inner_loop)(second_input)[..., 0]
second_convolution_model = Model(inputs=[second_input], outputs=[second_result])

print(first_convolution_model.predict([1, 2, 3]))
first_convolution_model.save('/tmp/model1.h5')  # works
tf.saved_model.save(first_convolution_model, '/tmp/model1')  # works

print(second_convolution_model.predict([1, 2, 3]))
first_convolution_model.save('/tmp/model2.h5')  # works
tf.saved_model.save(second_convolution_model, '/tmp/model2')  # does NOT work
print("(not reached)")
	</description>
	<comments>
		<comment id='1' author='csachs' date='2019-08-22T22:36:08Z'>
		Was able to reproduce the issue. Here's the colab &lt;denchmark-link:https://colab.research.google.com/gist/gowthamkpr/ba0632fc201dd096c95a70c6586a06f8/untitled106.ipynb&gt;notebook&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='csachs' date='2019-08-28T18:55:32Z'>
		Lambda layers are not supposed to contain any state (e.g. variables, which the first model has).  Creating a custom layer would work:
&lt;denchmark-code&gt;class InnerLoopLayer(tf.keras.layers.Layer):
  def __init__(self, model):
    self.model = model
    super(InnerLoopLayer, self).__init__()

  def call(self, tensor):
    the_len = tf.shape(tensor)[0]
    print(the_len)
    collector = tf.TensorArray(tf.float32, size=the_len)

    _, collector = tf.while_loop(
        cond=lambda i, _: i &lt; the_len,
        body=lambda i, c_: (i+1, c_.write(i, self.model(tensor[i:i + 1]))),
        loop_vars=(0, collector)
    )

    return collector.stack()


second_input = Input(shape=(1,), batch_size=3)
second_result = InnerLoopLayer(first_convolution_model)(second_input)[..., 0]
second_convolution_model = Model(inputs=[second_input], outputs=[second_result])
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='csachs' date='2019-08-28T18:55:34Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=31893&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=31893&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='csachs' date='2019-08-28T18:58:34Z'>
		Actually, reopening because there appears to be an issue when loading the model. Looking into this.
		</comment>
		<comment id='5' author='csachs' date='2019-08-29T08:09:52Z'>
		Thanks for the response! Interesting, I was puzzled because it worked at runtime …
however when I save a model like this, it still seems to be in an invalid state:
Actually, I'm using tf.saved_model.save because I want to use the Keras models with TensorFlow serving. When I serve the model generated using your InnerLoopLayer, I however get the following error:
# curl -X POST -H "Content-Type: application/json" -d '{"inputs":[[1,2,3]]}' http://localhost:8501/v1/models/1:predict
{ "error": "2 root error(s) found.\n  (0) Invalid argument: PartialTensorShape: Incompatible shapes during merge: [?,1,1] vs. [1,3,1]\n\t [[{{node model_1/inner_loop_layer/TensorArrayV2Stack/TensorListStack}}]]\n\t [[StatefulPartitionedCall/_33]]\n  (1) Invalid argument: PartialTensorShape: Incompatible shapes during merge: [?,1,1] vs. [1,3,1]\n\t [[{{node model_1/inner_loop_layer/TensorArrayV2Stack/TensorListStack}}]]\n0 successful operations.\n0 derived errors ignored." }
(Same call works for the first model).
		</comment>
		<comment id='6' author='csachs' date='2019-08-30T11:34:36Z'>
		
Thanks for the response! Interesting, I was puzzled because it worked at runtime …
however when I save a model like this, it still seems to be in an invalid state:
Actually, I'm using tf.saved_model.save because I want to use the Keras models with TensorFlow serving. When I serve the model generated using your InnerLoopLayer, I however get the following error:
# curl -X POST -H "Content-Type: application/json" -d '{"inputs":[[1,2,3]]}' http://localhost:8501/v1/models/1:predict
{ "error": "2 root error(s) found.\n  (0) Invalid argument: PartialTensorShape: Incompatible shapes during merge: [?,1,1] vs. [1,3,1]\n\t [[{{node model_1/inner_loop_layer/TensorArrayV2Stack/TensorListStack}}]]\n\t [[StatefulPartitionedCall/_33]]\n  (1) Invalid argument: PartialTensorShape: Incompatible shapes during merge: [?,1,1] vs. [1,3,1]\n\t [[{{node model_1/inner_loop_layer/TensorArrayV2Stack/TensorListStack}}]]\n0 successful operations.\n0 derived errors ignored." }
(Same call works for the first model).

Actually, I was calling it wrong apparently, I've retried today and it works as expected even with TensorFlow Serving, furthermore, with your suggestion I could even fix the problem in my own, larger code … thanks a lot!
		</comment>
		<comment id='7' author='csachs' date='2019-08-30T20:22:44Z'>
		That's great! I've submitted a fix internally that resolves the bug when loading this model back into python, so re-closing this issue.
		</comment>
		<comment id='8' author='csachs' date='2019-08-30T20:22:45Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=31893&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=31893&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>