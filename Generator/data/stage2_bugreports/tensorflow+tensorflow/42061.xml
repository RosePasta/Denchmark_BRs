<bug id='42061' author='bergfita' open_date='2020-08-05T14:05:56Z' closed_time='2020-08-27T17:25:45Z'>
	<summary>Fail to export function containing tf.lookup.StaticHashTable</summary>
	<description>
System information

Ubuntu 18.04
TensorFlow installed from (source or binary): binary (`pip install tensorflow)
TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0
Python version: 3.6.9

Describe the current behavior
Trying to save a SavedModel with tf.saved_model.save, tf.Module fails if it includes tf.lookup.StaticHashTable.
Describe the expected behavior
No failure.
Standalone code to reproduce the issue
The following code
&lt;denchmark-code&gt;import tensorflow as tf
class M(tf.Module):
    @tf.function
    def __call__(self, a):       
        keys_tensor = tf.constant([1, 2])
        vals_tensor = tf.constant([3, 4])
        input_tensor = tf.constant([1, 5])
        tf.lookup.StaticHashTable(tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor), -1)

        return a

m = M()
cf = m.__call__.get_concrete_function(a=tf.TensorSpec([None], tf.float32))

tf.saved_model.save(m, "~/output_dir", signatures={"serving_default": cf})
&lt;/denchmark-code&gt;

yields the error:
&lt;denchmark-code&gt;AssertionError: Tried to export a function which references untracked object Tensor("178:0", shape=(), dtype=resource).TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.
&lt;/denchmark-code&gt;

Note that the following modification (assigning the return value of tf.lookup.StaticHashTable to a class attribute) fails with the same Exception
&lt;denchmark-code&gt;import tensorflow as tf
class M(tf.Module):
    def __init__(self, **kwargs):
        super(M, self).__init__()
        self.table = None
        
    @tf.function
    def __call__(self, a):       
        keys_tensor = tf.constant([1, 2])
        vals_tensor = tf.constant([3, 4])
        input_tensor = tf.constant([1, 5])
        self.table = tf.lookup.StaticHashTable(tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor), -1)
        
        return a
    
m = M()
concrete_fn = m.__call__.get_concrete_function(a=tf.TensorSpec([None], tf.float32))

tf.saved_model.save(m, "~/some_output_dir", signatures={"serving_default": concrete_fn})
&lt;/denchmark-code&gt;

Other info / logs Include any logs or source code that would be helpful to
Full trace:
&lt;denchmark-code&gt;---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
&lt;ipython-input-19-401fea85b3de&gt; in &lt;module&gt;
     17 concrete_fn = m.__call__.get_concrete_function(a=tf.TensorSpec([None], tf.float32))
     18 
---&gt; 19 tf.saved_model.save(m, "~/some_output_dir", signatures={"serving_default": concrete_fn})

~/py3env/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py in save(obj, export_dir, signatures, options)
    974 
    975   _, exported_graph, object_saver, asset_info = _build_meta_graph(
--&gt; 976       obj, export_dir, signatures, options, meta_graph_def)
    977   saved_model.saved_model_schema_version = constants.SAVED_MODEL_SCHEMA_VERSION
    978 

~/py3env/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py in _build_meta_graph(obj, export_dir, signatures, options, meta_graph_def)
   1064   asset_info, exported_graph = _fill_meta_graph_def(meta_graph_def,
   1065                                                     saveable_view, signatures,
-&gt; 1066                                                     options.namespace_whitelist)
   1067   if options.function_aliases:
   1068     function_aliases = meta_graph_def.meta_info_def.function_aliases

~/py3env/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py in _fill_meta_graph_def(meta_graph_def, saveable_view, signature_functions, namespace_whitelist)
    651 
    652   with exported_graph.as_default():
--&gt; 653     signatures = _generate_signatures(signature_functions, resource_map)
    654     for concrete_function in saveable_view.concrete_functions:
    655       concrete_function.add_to_graph()

~/py3env/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py in _generate_signatures(signature_functions, resource_map)
    517                                                   signature_key, function.name))
    518     outputs = _call_function_with_mapped_captures(
--&gt; 519         function, mapped_inputs, resource_map)
    520     signatures[signature_key] = signature_def_utils.build_signature_def(
    521         _tensor_dict_to_tensorinfo(exterior_argument_placeholders),

~/py3env/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py in _call_function_with_mapped_captures(function, args, resource_map)
    469   """Calls `function` in the exported graph, using mapped resource captures."""
    470   export_captures = _map_captures_to_created_tensors(function.graph.captures,
--&gt; 471                                                      resource_map)
    472   # Calls the function quite directly, since we have new captured resource
    473   # tensors we need to feed in which weren't part of the original function

~/py3env/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py in _map_captures_to_created_tensors(original_captures, resource_map)
    392            "be tracked by assigning them to an attribute of a tracked object "
    393            "or assigned to an attribute of the main object directly."
--&gt; 394           ).format(interior))
    395     export_captures.append(mapped_resource)
    396   return export_captures

AssertionError: Tried to export a function which references untracked object Tensor("836:0", shape=(), dtype=resource).TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='bergfita' date='2020-08-06T00:10:45Z'>
		&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/39064&gt;#39064&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='bergfita' date='2020-08-06T07:50:09Z'>
		Thanks, it seems that the following works:
&lt;denchmark-code&gt;import tensorflow as tf
class M(tf.Module):
    def __init__(self, **kwargs):
        super(M, self).__init__()
        self.table = None
        
    def f(self):
        @tf.function
        def inner(a):
            keys_tensor = tf.constant([1, 2])
            vals_tensor = tf.constant([3, 4])
            input_tensor = tf.constant([1, 5])
            self.table = tf.lookup.StaticHashTable(tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor), -1)
        
            return a
        return inner
    
m = M()
concrete_fn = m.f().get_concrete_function(a=tf.TensorSpec([None], tf.float32))

tf.saved_model.save(m, "~/some_output_dir", signatures={"serving_default": concrete_fn})
&lt;/denchmark-code&gt;

Is there an explanation why this works and my original code does not? I'd like to understand the underlying issue a bit more (if at all possible). Is this expected behavior? The docs seem to encourage decorating the class methods, e.g. &lt;denchmark-link:https://www.tensorflow.org/guide/saved_model#saving_a_custom_model&gt;https://www.tensorflow.org/guide/saved_model#saving_a_custom_model&lt;/denchmark-link&gt;
, thus I'm a bit surprised that this actually makes a difference.
Best regards
		</comment>
		<comment id='3' author='bergfita' date='2020-08-10T19:33:12Z'>
		&lt;denchmark-link:https://github.com/bergfita&gt;@bergfita&lt;/denchmark-link&gt;

please move the issue to closed status if resolved.
		</comment>
		<comment id='4' author='bergfita' date='2020-08-13T16:20:08Z'>
		&lt;denchmark-link:https://github.com/bergfita&gt;@bergfita&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/bhack&gt;@bhack&lt;/denchmark-link&gt;
 : I'm having similar issue. I'm using  as in one Lambda layer after the output layer of my tf.keras model. It's quite simple actually: I've a text classification models and I'm adding a simple lambda layer that takes the  and convert the model_id to more general labels. I can save this version of model with model.save(... as H5 format..) without any issue, and can load it back and use it without any problem.
Issue is, when I try to export my TF2.2.0 model for TF-Serving, I can't find how I can export it. Here is what I can do with TF1.X or with TF2.X + tf.compat.v1.disable_eager_execution()
tf.compat.v1.disable_eager_execution()
version = 1
name = 'tmp_model'
export_path = f'/opt/tf_serving/{name}/{version}'
builder = saved_model_builder.SavedModelBuilder(export_path)

model_signature = tf.compat.v1.saved_model.predict_signature_def(
    inputs={
        'input': model.input
    }, 
    outputs={
        'output': model.output
    }
)

with tf.compat.v1.keras.backend.get_session() as sess:
    builder.add_meta_graph_and_variables(
        sess=sess,
        tags=[tf.compat.v1.saved_model.tag_constants.SERVING],
        signature_def_map={
            'predict': model_signature
        },
        # For initializing Hashtables
        main_op=tf.compat.v1.tables_initializer()
    )
    builder.save()
This will save my models with TF1.X format for serving and I can use it without any issue. Things is, I'm using LSTM layer and I want to use my model on GPU. By the documentation, if I disable the eager mode, I can't use the GPU-version of LSTM with TF2.2. And without going through above mentioned code, I can't save my model for serving wrt TF2.2 standard and StaticHashTables.
Here is how I'm trying to export my TF2.2 model which is using StaticHashTables in final layer; and which is giving me same error as above:
class MyModule(tf.Module):

    def __init__(self, model):
        super(MyModule, self).__init__()
        self.model = model
    
    @tf.function(input_signature=[tf.TensorSpec(shape=(None, 16), dtype=tf.int32, name='input')])
    def predict(self, input):
        result = self.model(input)
        return {"output": result}

version = 1
name = 'tmp_model'
export_path = f'/opt/tf_serving/{name}/{version}'

module = MyModule(model)
tf.saved_model.save(module, export_path, signatures={"predict": module.predict.get_concrete_function()})
Any suggestion or am I missing anything on exporting TF2.2 model which is using the StaticHashTables in final Lambda layer for TensorFlow Serving?
Thanks!
		</comment>
		<comment id='5' author='bergfita' date='2020-08-20T16:27:38Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='6' author='bergfita' date='2020-08-27T17:25:44Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='7' author='bergfita' date='2020-08-27T17:25:47Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42061&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42061&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='bergfita' date='2020-08-28T23:47:08Z'>
		More information here on how save model with StatcHashTable: &lt;denchmark-link:https://github.com/tensorflow/serving/issues/1719&gt;tensorflow/serving#1719&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>