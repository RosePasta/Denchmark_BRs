<bug id='36024' author='jinay1991' open_date='2020-01-18T20:41:22Z' closed_time='2020-03-11T22:07:01Z'>
	<summary>Tensorflow Lite New Converter does not allow to use inference_input_type and inference_output_type with V2 APIs</summary>
	<description>
System information

OS Platform and Distribution (Linux, MacOS):
TensorFlow installed from (official python wheel):
TensorFlow version (v2.1.0):

    converter = tf.lite.TFLiteConverter.from_keras_model(model)

    converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
    converter.experimental_new_converter = True
    converter.experimental_new_quantizer = True
    converter.representative_dataset = representative_data_gen
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
    converter.inference_input_type = tf.uint8
    converter.inference_output_type = tf.uint8

    tflite_model = converter.convert()
Above code segment is expected to produce uint8 inference_input_type and inference_output_type, yet there is no uint8 conversion of the input layers and it ended up in identifying as floating_model when used in example &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/6ef62c6d2e90675eed0bb6ed10d8c5761ab365c1/tensorflow/lite/examples/label_image/label_image.cc#L226&gt;label_image.cc&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;...
mobilenetv2_1.00_224/global_average_pooling2d/Mean &lt;type 'numpy.int8'&gt;
mobilenetv2_1.00_224/global_average_pooling2d/Mean/reduction_indices &lt;type 'numpy.int32'&gt;
mobilenetv2_1.00_224/out_relu/Relu &lt;type 'numpy.int8'&gt;
input_1 &lt;type 'numpy.float32'&gt;
Identity &lt;type 'numpy.float32'&gt;
&lt;/denchmark-code&gt;

Model used: &lt;denchmark-link:https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224_quant.tgz&gt;mobilenet_v2_1.0_224_quant.tgz&lt;/denchmark-link&gt;

Failure details
If the conversion is successful, but the generated model is wrong,
state what is wrong:

Unable to produce true uint8 input/output layers.

	</description>
	<comments>
		<comment id='1' author='jinay1991' date='2020-03-03T22:35:55Z'>
		I used &lt;denchmark-link:https://github.com/lutzroeder/netron&gt;netron&lt;/denchmark-link&gt;
 to visualize your model. Please take a look.
&lt;denchmark-link:https://user-images.githubusercontent.com/42785357/75826347-2fe2f980-5d5c-11ea-857e-e04abbeaace3.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='jinay1991' date='2020-03-11T22:07:01Z'>
		Closing this issue since it's resolved. Feel free to reopen if necessary. Thanks!
		</comment>
		<comment id='3' author='jinay1991' date='2020-03-11T22:07:03Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36024&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36024&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='jinay1991' date='2020-09-29T17:02:44Z'>
		No clear what was the solution.
		</comment>
		<comment id='5' author='jinay1991' date='2020-09-29T18:53:59Z'>
		
No clear what was the solution.

As I have noticed, this has been fixed and included in Release &lt;denchmark-link:https://github.com/tensorflow/tensorflow/releases/tag/v2.3.0&gt;v2.3.0&lt;/denchmark-link&gt;
 -&gt; Checkout Release Notes (tflite section)
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;tf.lite: (Snapshot of Release Note section)&lt;/denchmark-h&gt;

&lt;denchmark-h:h4&gt;Converter&lt;/denchmark-h&gt;



Restored inference_input_type and inference_output_type flags in TF 2.x TFLiteConverter (backward compatible with TF 1.x) to support integer (tf.int8, tf.uint8) input and output types in post training full integer quantized models.


Added support for converting and resizing models with dynamic (placeholder) dimensions. Previously, there was only limited support for dynamic batch size, and even that did not guarantee that the model could be properly resized at runtime.


Enabled experimental support for a new quantization mode with 16-bit activations and 8-bit weights. See lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8.


		</comment>
	</comments>
</bug>