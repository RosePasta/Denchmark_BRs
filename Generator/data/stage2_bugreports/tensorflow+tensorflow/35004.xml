<bug id='35004' author='chahld' open_date='2019-12-10T18:42:12Z' closed_time='2019-12-12T22:21:16Z'>
	<summary>BinaryCrossentropy incorrect partial reduction of loss when reduction='none'</summary>
	<description>
System information
custom code

OS Platform and Distribution
ubuntu 18.04
TensorFlow installed from (source or binary):
pip install tensorflow-gpu
TensorFlow version (use command below):
v2.0.0-rc2-26-g64c3d38 2.0.0
Python version:
3.6.9

Describe the current behavior
BinaryCrossentropy does a reduction on the last dimension even if you pass in reduction='none'
Describe the expected behavior
It should not do any reduction
Workaround
use tf.nn.sigmoid_cross_entropy_with_logits()
Code to reproduce the issue
&lt;denchmark-code&gt;import numpy as np

import tensorflow as tf

def main():
    y = np.array([0., 0., 1., 1.]).reshape((1, 1, 1, -1))
    x = np.array([1., 1., 1., 0.]).reshape((1, 1, 1, -1))
    print(y.shape)
    print(x.shape)

    bce_reduce = tf.keras.losses.BinaryCrossentropy()
    loss = bce_reduce(y, x)
    print('correct: fully reduced loss (reduction=default)', loss.numpy())

    bce = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)
    loss = bce(y, x)
    print('incorrect: should be not be reduced (reduction=none): ', loss.numpy())  # Loss: 11.522857
    print('incorrect: reduced along last dimension:', loss.shape)

    # should be the same as:
    correct_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=x)
    print('correct: unreduced_loss (using sigmoid_cross_entropy_with_logits)', correct_loss)
    print('correct: unreduced_loss_shape', correct_loss.shape)


main()
&lt;/denchmark-code&gt;

output:
&lt;denchmark-code&gt;correct: fully reduced loss (reduction=default) 11.568711280822754
incorrect: should be not be reduced (reduction=none):  [[[11.56871128]]]
incorrect: reduced along last dimension: (1, 1, 1)
correct: unreduced_loss (using sigmoid_cross_entropy_with_logits) tf.Tensor([[[[1.31326169 1.31326169 0.31326169 0.69314718]]]], shape=(1, 1, 1, 4), dtype=float64)
correct: unreduced_loss_shape (1, 1, 1, 4)

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='chahld' date='2019-12-12T06:51:39Z'>
		I have tried on colab with TF version 2.0 ,2.1.0-rc0 and was able to reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/9f1986fd00ea719785ffd0f790eb7330/untitled469.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='chahld' date='2019-12-12T22:21:13Z'>
		&lt;denchmark-link:https://github.com/chahld&gt;@chahld&lt;/denchmark-link&gt;
 The behavior is as expected. Please see the docs here: &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy#returns&gt;https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy#returns&lt;/denchmark-link&gt;
. The inputs are always reduced by one dimension.  indicates whether you are reducing the per sample losses - For example if you input is 2D (samples, features), if reduction is none, you will get (samples,) values - ie. one loss value per sample.
		</comment>
		<comment id='3' author='chahld' date='2019-12-12T22:21:18Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35004&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35004&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='chahld' date='2019-12-13T14:56:20Z'>
		Sorry I misread the documentation.
In my use case I'm trying to weight the loss per filter (setting it to zero in some circumstances).
What is the use case for this partial reduction? I understand if the output is dense it would still allow me to weight the loss differently for each sample. But I wonder why it is useful for the convolutional case where it returns the loss per pixel per sample?
Some further suggestions (all somewhat minor):

it would be useful to have a 'really none' option.
the option called 'none' is misnamed and should be something like "filter'.
Also the return value documentation says the reduction axis is 'usually' -1, but for this function there is no way to pass in an axis, so it is always -1.

		</comment>
	</comments>
</bug>