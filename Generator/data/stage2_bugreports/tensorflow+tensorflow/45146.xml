<bug id='45146' author='jponf' open_date='2020-11-24T09:05:29Z' closed_time='2020-12-25T03:33:55Z'>
	<summary>Missing call to model.optimizer._clip_gradients(grads)?</summary>
	<description>
Hi,
I've been having problems training a model and, after reading the keras training loop code, it seems that the call to
model.optimizer._clip_gradients(grads) introduced in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/69da929ad4d5ba605507efa1f52b382a55b6a969&gt;69da929&lt;/denchmark-link&gt;
 is missing again.
Has it been moved somewhere else?



tensorflow/tensorflow/python/keras/engine/training_eager_v1.py


         Line 278
      in
      ab953ab






 grads = model.optimizer.get_unscaled_gradients(grads) 





	</description>
	<comments>
		<comment id='1' author='jponf' date='2020-11-24T10:17:30Z'>
		&lt;denchmark-link:https://github.com/jponf&gt;@jponf&lt;/denchmark-link&gt;

We see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]
		</comment>
		<comment id='2' author='jponf' date='2020-11-25T06:33:22Z'>
		Hi, the behavior was already explained on a previous issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/33929&gt;#33929&lt;/denchmark-link&gt;
, were it is mentioned that TF 2.2 fixed it. But it seems that it is happening again in TF 2.3.0

TF version: 2.3.0
TF Keras version: 2.4.0

I was training a CRNN and tried to apply clipnorm to avoid exploding gradients, but the issue persisted after adding the parameter to the optimizer. Then I just sub-classed tf.keras.Model and modified the train_step method to apply tf.clip_by_global_norm before calling the optimizer and so far it seems the exploding gradients problem is gone.
		</comment>
		<comment id='3' author='jponf' date='2020-11-26T19:26:28Z'>
		&lt;denchmark-link:https://github.com/jponf&gt;@jponf&lt;/denchmark-link&gt;

I ran the code on tf-nightly and &lt;denchmark-link:https://colab.research.google.com/gist/Saduf2019/ccbeccee471ec883c4deb875538cc173/untitled476.ipynb&gt;tf 2.3&lt;/denchmark-link&gt;
 do not see the nan values, please refer to this &lt;denchmark-link:https://colab.research.google.com/gist/Saduf2019/77fb086287b1846cf61302ad3a2f3f93/untitled472.ipynb&gt;gist here&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='4' author='jponf' date='2020-11-27T06:28:14Z'>
		In my particular case it wasn't NaNs what I found, but the running mean of the batchnorm layers that skyrocketed to values between 1000 and 300000, even though the input was normalized between 0 and 1.
A complete rewrite of the model in PyTorch (with gradient clipping) did not has this issue, and as I said, a custom train_step with tf.clip_by_global_norm before applying the gradients didn't show that behavior either. That is what lead me to the issue mentioned in my previous comment.
If my schedule allows me to, I'll try to explore this issue in more detail.
In the meanwhile, what I found is that line 277, were gradient clipping is applied, in
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/69da929ad4d5ba605507efa1f52b382a55b6a969/tensorflow/python/keras/engine/training_eager.py&gt;https://github.com/tensorflow/tensorflow/blob/69da929ad4d5ba605507efa1f52b382a55b6a969/tensorflow/python/keras/engine/training_eager.py&lt;/denchmark-link&gt;

is gone in:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training_eager_v1.py&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training_eager_v1.py&lt;/denchmark-link&gt;

Which may be completely unrelated to my problem but so far is the only thing that I know may explain the behavior that I observed.
		</comment>
		<comment id='5' author='jponf' date='2020-12-11T02:36:19Z'>
		&lt;denchmark-link:https://github.com/jponf&gt;@jponf&lt;/denchmark-link&gt;
 Can you please share a simple standalone code to reproduce the issue? Thanks!
		</comment>
		<comment id='6' author='jponf' date='2020-12-18T03:03:59Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='7' author='jponf' date='2020-12-25T03:33:54Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='8' author='jponf' date='2020-12-25T03:33:56Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45146&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45146&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>