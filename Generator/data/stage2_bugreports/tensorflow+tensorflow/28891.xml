<bug id='28891' author='Frank1993' open_date='2019-05-21T01:28:07Z' closed_time='2019-06-24T21:19:21Z'>
	<summary>tf.contrib.distribute.CollectiveAllReduceStrategy always failed for OS error or socket closed</summary>
	<description>
When using tf.contrib.distribute.CollectiveAllReduceStrategy to distribute train on 16 workers with 2 GPU each, my job always failed with OS error or socket closed after running for several thousands of steps. I also tried 8 workers and different batch size, the failure always reproduce. And when using parameter server strategy, all my jobs can run successfully and I believe this was not caused by my cluster.
Thank you so much for helping investigating this problem.
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
using estimator and train_and_evaluate
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
when training
TensorFlow installed from (source or binary):
source
TensorFlow version (use command below):
tf 1.13
Python version:
python 3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
CUDA 10
GPU model and memory:
P100, 16G

Describe the current behavior
When using tf.contrib.distribute.CollectiveAllReduceStrategy to distribute train on 16 workers with 2 GPU each, my job always failed with OS error or socket closed after running for several thousands of steps. I also tried 8 workers and different batch size, the failure always reproduce.
Describe the expected behavior
The job should keep training without failure
Code to reproduce the issue
Other info / logs


2019-05-19T15:07:45.079Z: [1,13]:INFO:tensorflow:loss = 3.7297182, step = 10300 (50.597 sec)
2019-05-19T15:07:45.080Z: [1,14]:INFO:tensorflow:loss = 3.7297182, step = 10300 (50.608 sec)
2019-05-19T15:07:45.081Z: [1,14]:INFO:tensorflow:global_step/sec: 1.97598
2019-05-19T15:08:13.129Z: [1,4]:2019-05-19 15:08:13.128241: E tensorflow/core/common_runtime/ring_reducer.cc:369] Aborting RingReduce with Unavailable: Socket closed
2019-05-19T15:08:13.129Z: [1,4]:2019-05-19 15:08:13.128323: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Unavailable: Socket closed
2019-05-19T15:08:14.047Z: [1,1]:Segmentation fault (core dumped)
or os error
2019-05-20T12:05:35.816Z: [1,12]:INFO:tensorflow:loss = 5.7242765, step = 3400 (57.129 sec)
2019-05-20T12:05:35.817Z: [1,14]:INFO:tensorflow:loss = 5.7242765, step = 3400 (57.107 sec)
2019-05-20T12:05:41.291Z: [1,7]:2019-05-20 12:05:41.289749: E tensorflow/core/common_runtime/ring_reducer.cc:369] Aborting RingReduce with Unavailable: OS Error
2019-05-20T12:05:41.291Z: [1,7]:2019-05-20 12:05:41.289831: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Unavailable: OS Error
2019-05-20T12:05:41.774Z: [1,1]:Segmentation fault (core dumped)
	</description>
	<comments>
		<comment id='1' author='Frank1993' date='2019-05-21T01:32:42Z'>
		Ping &lt;denchmark-link:https://github.com/dubey&gt;@dubey&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='Frank1993' date='2019-06-05T20:29:58Z'>
		There was a bug in grpc which has been fixed recently. Could you try a newer version of TF?
		</comment>
		<comment id='3' author='Frank1993' date='2019-06-20T12:36:11Z'>
		It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue?
		</comment>
		<comment id='4' author='Frank1993' date='2019-06-24T21:19:21Z'>
		Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!
		</comment>
		<comment id='5' author='Frank1993' date='2019-06-24T21:19:22Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=28891&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=28891&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='Frank1993' date='2019-11-27T22:50:04Z'>
		&lt;denchmark-link:https://github.com/yuefengz&gt;@yuefengz&lt;/denchmark-link&gt;
 One question for your answer 'There was a bug in grpc which has been fixed recently. Could you try a newer version of TF?': Does TF 1.14 contains this bug fix?
		</comment>
	</comments>
</bug>