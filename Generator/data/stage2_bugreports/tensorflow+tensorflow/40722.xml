<bug id='40722' author='FlorentijnD' open_date='2020-06-23T17:16:53Z' closed_time='2020-06-26T08:43:02Z'>
	<summary>Segmentation fault interpreter-&amp;gt;SetNumThreads CPP</summary>
	<description>
@tensorflow/micro

Hardware : Freescale i.MX6 Quad/DualLite
Processor: ARMv7 Processor rev 10 (v71)
OS Platform and Distribution: Yocto built Linux distribution (kernel 4.9.4+)
The tf-lite library was built with common options and using default makefile: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/lite/tools/make/Makefile&gt;https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/lite/tools/make/Makefile&lt;/denchmark-link&gt;

API : CPP
Describe the problem
Cross compiling the minimal TF Lite interpreter cpp code where I added the line to set the number of threads to use results in a segmentation error when running the program.
The relevant code :
&lt;denchmark-code&gt;int main(int argc, char* argv[]) {
  if (argc != 2) {
    fprintf(stderr, "minimal &lt;tflite model&gt;\n");
    return 1;
  }
  const char* filename = argv[1];
  int startt, endd;
  startt = clock();

  // Load model
  // Load model
  std::unique_ptr&lt;tflite::FlatBufferModel&gt; model =
      tflite::FlatBufferModel::BuildFromFile(filename);
  TFLITE_MINIMAL_CHECK(model != nullptr);

  // Build the interpreter
  tflite::ops::builtin::BuiltinOpResolver resolver;
  InterpreterBuilder builder(*model, resolver);
  std::unique_ptr&lt;Interpreter&gt; interpreter;
  std::cout &lt;&lt; "allocating one number of threads" &lt;&lt; std::endl;
  int numthreads=1;
  interpreter-&gt;SetNumThreads(numthreads);
  std::cout &lt;&lt; "threads allocated" &lt;&lt; std::endl;

  builder(&amp;interpreter);
  TFLITE_MINIMAL_CHECK(interpreter != nullptr);
  endd = clock();

  // Allocate tensor buffers.
  TFLITE_MINIMAL_CHECK(interpreter-&gt;AllocateTensors() == kTfLiteOk);
  printf("=== Pre-invoke Interpreter State ===\n");
  tflite::PrintInterpreterState(interpreter.get());

  double time_taken = double(endd-startt)/double(CLOCKS_PER_SEC);
  std::cout &lt;&lt; "4 threads should be selected" &lt;&lt; std::endl;
  std::cout &lt;&lt; "Time taken to load in the model in tflite using CPP API :" &lt;&lt; fixed &lt;&lt; time_taken &lt;&lt; std::setprecision(5);
  std::cout &lt;&lt; " sec " &lt;&lt; std::endl;
}
&lt;/denchmark-code&gt;

The model I've been using is a custom ENet I trained in TensorFlow and converted to TFLite using both TFLite_builtin_ops and TF_OPS, though I believe the problem has nothing to do with the model itself.

Running gdb and checking the stacktrace results in the following:
&lt;denchmark-link:https://user-images.githubusercontent.com/29673343/85434212-fee63900-b585-11ea-860f-b3af1b4748bc.png&gt;&lt;/denchmark-link&gt;

I also cross compiled a standard cpp program using multiple threads which runs just fine. So that excludes problems concerning firmware. For the record, the multithreading program I got running is the following :
&lt;denchmark-code&gt;#include &lt;iostream&gt;
#include &lt;cstdlib&gt;
#include &lt;pthread.h&gt;

using namespace std;

#define NUM_THREADS 4

void *PrintHello(void *threadid) {
   long tid;
   tid = (long)threadid;
   cout &lt;&lt; "Hello World! Thread ID, " &lt;&lt; tid &lt;&lt; endl;
   pthread_exit(NULL);
}

int main () {
   pthread_t threads[NUM_THREADS];
   int rc;
   int i;

   for( i = 0; i &lt; NUM_THREADS; i++ ) {
      cout &lt;&lt; "main() : creating thread, " &lt;&lt; i &lt;&lt; endl;
      rc = pthread_create(&amp;threads[i], NULL, PrintHello, (void *)i);

      if (rc) {
         cout &lt;&lt; "Error:unable to create thread," &lt;&lt; rc &lt;&lt; endl;
         exit(-1);
      }
   }
   pthread_exit(NULL);
}
&lt;/denchmark-code&gt;

How do I go about this problem? What is causing this?
	</description>
	<comments>
		<comment id='1' author='FlorentijnD' date='2020-06-23T20:48:44Z'>
		The problem might be related to that you are not allocating memory for Interpreter object? I would expect to see below;
&lt;denchmark-code&gt;std::unique_ptr&lt;Interpreter&gt; interpreter = std::make_unique&lt;Interpreter&gt;();  //Make_unique missing here on your impl.
std::cout &lt;&lt; "allocating one number of threads" &lt;&lt; std::endl;
int numthreads=1;
interpreter-&gt;SetNumThreads(numthreads);
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='FlorentijnD' date='2020-06-24T08:26:07Z'>
		&lt;denchmark-link:https://github.com/cngzhnp&gt;@cngzhnp&lt;/denchmark-link&gt;
 Indeed! Good find. This means that the documentation is not on point. I got my code from the following &lt;denchmark-link:https://www.tensorflow.org/lite/guide/inference&gt;link&lt;/denchmark-link&gt;
.
Now that we have that sorted out, I see that running inference on test data results in the use of only one core, even though I am explicitly initlizalizing 4 threads:
&lt;denchmark-link:https://user-images.githubusercontent.com/29673343/85520934-dbfa6a00-b603-11ea-8313-5d946e41a3f3.png&gt;&lt;/denchmark-link&gt;

The code I am now using (in full) is stated below:
&lt;denchmark-code&gt;/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
    http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/


#include &lt;cstdio&gt;
#include &lt;iostream&gt;
#include &lt;iomanip&gt;
#include "tensorflow/lite/interpreter.h"
#include "tensorflow/lite/kernels/register.h"
#include "tensorflow/lite/model.h"
#include "tensorflow/lite/optional_debug_tools.h"
#include &lt;time.h&gt;

// This is an example that is minimal to read a model
// from disk and perform inference. There is no data being loaded
// that is up to you to add as a user.
//
// NOTE: Do not add any dependencies to this that cannot be built with
// the minimal makefile. This example must remain trivial to build with
// the minimal build tool.
//
// Usage: minimal &lt;tflite model&gt;

using namespace tflite;
using namespace std;

#define TFLITE_MINIMAL_CHECK(x)                              \
  if (!(x)) {                                                \
    fprintf(stderr, "Error at %s:%d\n", __FILE__, __LINE__); \
    exit(1);                                                 \
  }

int main(int argc, char* argv[]) {
  if (argc != 2) {
    fprintf(stderr, "minimal &lt;tflite model&gt;\n");
    return 1;
  }
  const char* filename = argv[1];
  int startt, endd;
  startt = clock();

  // Load model
  // Load model
  std::unique_ptr&lt;tflite::FlatBufferModel&gt; model =
      tflite::FlatBufferModel::BuildFromFile(filename);
  TFLITE_MINIMAL_CHECK(model != nullptr);

  // Build the interpreter
  tflite::ops::builtin::BuiltinOpResolver resolver;
  InterpreterBuilder builder(*model, resolver);
  std::unique_ptr&lt;Interpreter&gt; interpreter(new Interpreter());
  std::cout &lt;&lt; "allocating a number of threads" &lt;&lt; std::endl;
  int numthreads=4;
  interpreter-&gt;SetNumThreads(numthreads);
  std::cout &lt;&lt; "threads allocated" &lt;&lt; std::endl;

  builder(&amp;interpreter);
  TFLITE_MINIMAL_CHECK(interpreter != nullptr);
  endd = clock();

  // Allocate tensor buffers.
  TFLITE_MINIMAL_CHECK(interpreter-&gt;AllocateTensors() == kTfLiteOk);
  printf("=== Pre-invoke Interpreter State ===\n");
  //tflite::PrintInterpreterState(interpreter.get());

  double time_taken = double(endd-startt)/double(CLOCKS_PER_SEC);
  std::cout &lt;&lt; "a number of threads should be selected" &lt;&lt; std::endl;
  std::cout &lt;&lt; "Time taken to load in the model in tflite using CPP API :" &lt;&lt; fixed &lt;&lt; time_taken &lt;&lt; std::setprecision(5);
  std::cout &lt;&lt; " sec " &lt;&lt; std::endl;


  // Fill input buffers
  // TODO(user): Insert code to fill input tensors
  int startinput = clock();
  int input = interpreter-&gt;inputs()[0];
  float* input_data_ptr = interpreter-&gt;typed_tensor&lt;float&gt;(input);
  float valuefloat = 0.5;
  std::cout &lt;&lt; "add dummy image data of 3x480x640" &lt;&lt; std::endl;
  for (int k=0; k&lt;3; k++){
    for (int i=0; i&lt;480; ++i){
        for (int j=0; j&lt;640; j++){
            *(input_data_ptr)=valuefloat;
            input_data_ptr++;
            }
        }
  }


  int stopinput = clock();
  double time_taken2 = double(stopinput-startinput)/double(CLOCKS_PER_SEC);
  std::cout &lt;&lt; "Time taken to load data :" &lt;&lt; fixed &lt;&lt; time_taken2 &lt;&lt; std::setprecision(5);
  std::cout &lt;&lt; " sec " &lt;&lt; std::endl;

   // Run inference
  std::cout &lt;&lt; "start interpreting" &lt;&lt; std::endl;
  int interpretstart = clock();
  TFLITE_MINIMAL_CHECK(interpreter-&gt;Invoke() == kTfLiteOk);
  int interpretstop = clock();
  double time_taken3 = double(interpretstop-interpretstart)/double(CLOCKS_PER_SEC);
  std::cout &lt;&lt; "Interpreting took :"&lt;&lt; time_taken3 &lt;&lt; std::setprecision(5);
  std::cout &lt;&lt; " sec " &lt;&lt; std::endl;

  printf("\n\n=== Post-invoke Interpreter State ===\n");
  tflite::PrintInterpreterState(interpreter.get());

  // Read output buffers
  // TODO(user): Insert getting data out code.
  int output_idx = interpreter-&gt;outputs()[0];
  float* output = interpreter-&gt;typed_tensor&lt;float&gt;(output_idx);
  std::cout &lt;&lt; "OUTPUT: " &lt;&lt; *output &lt;&lt; std::endl;


  return 0;
}
&lt;/denchmark-code&gt;

the screenshot is taken after the line "start interpreting" was printed to console, so during model inference.
		</comment>
		<comment id='3' author='FlorentijnD' date='2020-06-24T09:22:55Z'>
		&lt;denchmark-link:https://github.com/FlorentijnD&gt;@FlorentijnD&lt;/denchmark-link&gt;
 you're missing here I guess, in this link Interpreter pointer is allocated like below:
&lt;denchmark-code&gt;std::unique_ptr&lt;tflite::Interpreter&gt; interpreter;
tflite::InterpreterBuilder(*model, resolver)(&amp;interpreter);  //(&amp;interpreter) is missing on your part again.
&lt;/denchmark-code&gt;

In your example, you did not give interpreter object as a parameter to InterpreterBuilder class. There is no missing point on documentation.
		</comment>
		<comment id='4' author='FlorentijnD' date='2020-06-24T09:45:06Z'>
		Thanks for your quick reply! Six lines after instantiating the interpreter I am allocating the Interpreter pointer:
builder(&amp;interpreter);
For reference: I got my code from &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/minimal.cc&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/minimal.cc&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='FlorentijnD' date='2020-06-24T11:30:08Z'>
		Yes, I would expect that it should work.
		</comment>
		<comment id='6' author='FlorentijnD' date='2020-06-24T11:37:11Z'>
		I would too :) but setting 4 threads results in only one core being used for me. I'll await the assignee's response
		</comment>
		<comment id='7' author='FlorentijnD' date='2020-06-25T19:18:34Z'>
		Only the InterpreterBuilder should be used to create a new Interpreter instance. You're right that the documentation for this is pretty poor.
&lt;denchmark-code&gt;std::unique_ptr&lt;Interpreter&gt; interpreter(new Interpreter());
interpreter-&gt;SetNumThreads(numthreads);
builder(&amp;interpreter);
&lt;/denchmark-code&gt;

should be
&lt;denchmark-code&gt;std::unique_ptr&lt;Interpreter&gt; interpreter;
builder(&amp;interpreter);
interpreter-&gt;SetNumThreads(numthreads);
&lt;/denchmark-code&gt;

or better yet
&lt;denchmark-code&gt;std::unique_ptr&lt;Interpreter&gt; interpreter;
builder(&amp;interpreter, numthreads);
&lt;/denchmark-code&gt;

The fact that we have a public no-arg constructor is a mistake, and I believe is only used for testing. I'll look into restricting its visibility.
		</comment>
		<comment id='8' author='FlorentijnD' date='2020-06-26T08:43:02Z'>
		This solves my problem. Thanks for looking into this
		</comment>
		<comment id='9' author='FlorentijnD' date='2020-06-26T08:43:04Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40722&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40722&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>