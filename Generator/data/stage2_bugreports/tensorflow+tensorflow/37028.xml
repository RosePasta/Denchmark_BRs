<bug id='37028' author='rzaratx' open_date='2020-02-24T19:32:55Z' closed_time='2020-04-30T16:20:00Z'>
	<summary>jupyter notebook kernel dies when running a UNet with tensorflow=2.1.0</summary>
	<description>
System information

OS - Linux RedHat 8
conda install tensorflow-gpu - tensorflow version = 2.1.0
Python version: 3.7.4
Jupyter Notebook
CUDA/cuDNN version: - 10.2
CPU: AMD Threadripper 3960x
GPU: 2x RTX Titans
Memory: 128 GB Corsair 3200MHz

I recently upgraded to tensorflow=2.1.0 from tensorflow=2.0.0 and now my code will not run. Before the first epoch ends I get a message saying 'The kernel appears to have died. It will restart automatically.'. The code is using MirroredStrategy() to distribute the UNet to the GPUs. I don't get this error when I remove MirroredStrategy() and run on a single-gpu.
Originally I was running my code with tensorflow=2.0.0 using jupyter notebook and it would worked fine except that when I would try to load my model and make a prediction I get an error at model.predict() saying AttributeError: 'Model' object has no attribute 'loss'. I called model.summary() after loading the model and it showed the model was empty. So I upgraded to tensorflow=2.1.0 and called model.summary() this time it showed a readout. However I still get the same error AttributeError: 'Model' object has no attribute 'loss'
Is there currently an incompatibility with MirroredStrategy() and tensorflow=2.1.0?
&lt;denchmark-code&gt;def get_model(optimizer, loss_metric, metrics, lr=1e-4):
    with tf.device('/job:localhost/replica:0/task:0/device:GPU:0'):
        inputs = Input((sample_width, sample_height, sample_depth, 1))
        conv1 = Conv3D(32, (3, 3, 3), activation='relu', padding='same')(inputs)
        conv1 = Conv3D(32, (3, 3, 3), activation='relu', padding='same')(conv1)
        pool1 = MaxPooling3D(pool_size=(2, 2, 2))(conv1)
        drop1 = Dropout(0.5)(pool1)
        conv2 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(drop1)
        conv2 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(conv2)
        pool2 = MaxPooling3D(pool_size=(2, 2, 2))(conv2)
        drop2 = Dropout(0.5)(pool2)
        conv3 = Conv3D(128, (3, 3, 3), activation='relu', padding='same')(drop2)
        conv3 = Conv3D(128, (3, 3, 3), activation='relu', padding='same')(conv3)
        pool3 = MaxPooling3D(pool_size=(2, 2, 2))(conv3)
        drop3 = Dropout(0.3)(pool3)
        conv4 = Conv3D(256, (3, 3, 3), activation='relu', padding='same')(drop3)
        conv4 = Conv3D(256, (3, 3, 3), activation='relu', padding='same')(conv4)
        pool4 = MaxPooling3D(pool_size=(2, 2, 2))(conv4)
        drop4 = Dropout(0.3)(pool4)
        conv5 = Conv3D(512, (3, 3, 3), activation='relu', padding='same')(drop4)
        conv5 = Conv3D(512, (3, 3, 3), activation='relu', padding='same')(conv5)
    with tf.device('/job:localhost/replica:0/task:0/device:GPU:1'): 
        up6 = concatenate([Conv3DTranspose(256, (2, 2, 2), strides=(2, 2, 2), padding='same')(conv5), conv4], axis=4)
        conv6 = Conv3D(256, (3, 3, 3), activation='relu', padding='same')(up6)
        conv6 = Conv3D(256, (3, 3, 3), activation='relu', padding='same')(conv6)
        up7 = concatenate([Conv3DTranspose(128, (2, 2, 2), strides=(2, 2, 2), padding='same')(conv6), conv3], axis=4)
        conv7 = Conv3D(128, (3, 3, 3), activation='relu', padding='same')(up7)
        conv7 = Conv3D(128, (3, 3, 3), activation='relu', padding='same')(conv7)
        up8 = concatenate([Conv3DTranspose(64, (2, 2, 2), strides=(2, 2, 2), padding='same')(conv7), conv2], axis=4)
        conv8 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(up8)
        conv8 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(conv8)
        up9 = concatenate([Conv3DTranspose(32, (2, 2, 2), strides=(2, 2, 2), padding='same')(conv8), conv1], axis=4)
        conv9 = Conv3D(32, (3, 3, 3), activation='relu', padding='same')(up9)
        conv9 = Conv3D(32, (3, 3, 3), activation='relu', padding='same')(conv9)
        conv10 = Conv3D(1, (1, 1, 1), activation='sigmoid')(conv9)
        model = Model(inputs=[inputs], outputs=[conv10])
        model.compile(optimizer=optimizer(lr=lr), loss=loss_metric, metrics=metrics)
        return model

smooth = 1.
def dice_coef(y_true, y_pred):
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    intersection = K.sum(y_true_f * y_pred_f)
    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)
def dice_coef_loss(y_true, y_pred):
    return -dice_coef(y_true, y_pred)

mirrored_strategy = tf.distribute.MirroredStrategy()
with mirrored_strategy.scope():
    model = get_model(optimizer=Adam, loss_metric=dice_coef_loss, metrics=[dice_coef], lr=1e-4)
observe_var = 'dice_coef'
strategy = 'max' # greater dice_coef is better
model_checkpoint = ModelCheckpoint('{epoch:04}model', monitor=observe_var, save_best_only=True)
model.fit(train_x, train_y, batch_size = 2, epochs= 1000, verbose=1, shuffle=True, validation_split=.2, callbacks=[model_checkpoint])
model.save( 'finalmodel')
&lt;/denchmark-code&gt;

model.summary():
&lt;denchmark-code&gt;Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         multiple                  0         
_________________________________________________________________
conv3d (Conv3D)              multiple                  896       
_________________________________________________________________
conv3d_1 (Conv3D)            multiple                  27680     
_________________________________________________________________
max_pooling3d (MaxPooling3D) multiple                  0         
_________________________________________________________________
dropout (Dropout)            multiple                  0         
_________________________________________________________________
conv3d_2 (Conv3D)            multiple                  55360     
_________________________________________________________________
conv3d_3 (Conv3D)            multiple                  110656    
_________________________________________________________________
max_pooling3d_1 (MaxPooling3 multiple                  0         
_________________________________________________________________
dropout_1 (Dropout)          multiple                  0         
_________________________________________________________________
conv3d_4 (Conv3D)            multiple                  221312    
_________________________________________________________________
conv3d_5 (Conv3D)            multiple                  442496    
_________________________________________________________________
max_pooling3d_2 (MaxPooling3 multiple                  0         
_________________________________________________________________
dropout_2 (Dropout)          multiple                  0         
_________________________________________________________________
conv3d_6 (Conv3D)            multiple                  884992    
_________________________________________________________________
conv3d_7 (Conv3D)            multiple                  1769728   
_________________________________________________________________
max_pooling3d_3 (MaxPooling3 multiple                  0         
_________________________________________________________________
dropout_3 (Dropout)          multiple                  0         
_________________________________________________________________
conv3d_8 (Conv3D)            multiple                  3539456   
_________________________________________________________________
conv3d_9 (Conv3D)            multiple                  7078400   
_________________________________________________________________
conv3d_transpose (Conv3DTran multiple                  1048832   
_________________________________________________________________
concatenate (Concatenate)    multiple                  0         
_________________________________________________________________
conv3d_10 (Conv3D)           multiple                  3539200   
_________________________________________________________________
conv3d_11 (Conv3D)           multiple                  1769728   
_________________________________________________________________
conv3d_transpose_1 (Conv3DTr multiple                  262272    
_________________________________________________________________
concatenate_1 (Concatenate)  multiple                  0         
_________________________________________________________________
conv3d_12 (Conv3D)           multiple                  884864    
_________________________________________________________________
conv3d_13 (Conv3D)           multiple                  442496    
_________________________________________________________________
conv3d_transpose_2 (Conv3DTr multiple                  65600     
_________________________________________________________________
concatenate_2 (Concatenate)  multiple                  0         
_________________________________________________________________
conv3d_14 (Conv3D)           multiple                  221248    
_________________________________________________________________
conv3d_15 (Conv3D)           multiple                  110656    
_________________________________________________________________
conv3d_transpose_3 (Conv3DTr multiple                  16416     
_________________________________________________________________
concatenate_3 (Concatenate)  multiple                  0         
_________________________________________________________________
conv3d_16 (Conv3D)           multiple                  55328     
_________________________________________________________________
conv3d_17 (Conv3D)           multiple                  27680     
_________________________________________________________________
conv3d_18 (Conv3D)           multiple                  33        
=================================================================
Total params: 22,575,329
Trainable params: 22,575,329
Non-trainable params: 0
&lt;/denchmark-code&gt;

Prediction Code:
&lt;denchmark-code&gt;norm_images='imagedata\'
model = load_model('finalmodel',compile = False)
print(model.summary())
prediction = model.predict(norm_images, verbose = 1)
&lt;/denchmark-code&gt;

Attribute Error:
&lt;denchmark-code&gt;AttributeError                            Traceback (most recent call last)
&lt;ipython-input-2-06140bdcec2b&gt; in &lt;module&gt;
     75 
     76 print('Predicting the labels')
---&gt; 77 prediction = model.predict(norm_images, verbose = 1)
     78 # prediction = tf.keras.Model.predict(norm_images)
     79 

~\.conda\envs\gputest\lib\site-packages\tensorflow_core\python\keras\engine\training.py in predict(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)
   1011         max_queue_size=max_queue_size,
   1012         workers=workers,
-&gt; 1013         use_multiprocessing=use_multiprocessing)
   1014 
   1015   def reset_metrics(self):

~\.conda\envs\gputest\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py in predict(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)
    496         model, ModeKeys.PREDICT, x=x, batch_size=batch_size, verbose=verbose,
    497         steps=steps, callbacks=callbacks, max_queue_size=max_queue_size,
--&gt; 498         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)
    499 
    500 

~\.conda\envs\gputest\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py in _model_iteration(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)
    424           max_queue_size=max_queue_size,
    425           workers=workers,
--&gt; 426           use_multiprocessing=use_multiprocessing)
    427       total_samples = _get_total_number_of_samples(adapter)
    428       use_sample = total_samples is not None

~\.conda\envs\gputest\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py in _process_inputs(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)
    644     standardize_function = None
    645     x, y, sample_weights = standardize(
--&gt; 646         x, y, sample_weight=sample_weights)
    647   elif adapter_cls is data_adapter.ListsOfScalarsDataAdapter:
    648     standardize_function = standardize

~\.conda\envs\gputest\lib\site-packages\tensorflow_core\python\keras\engine\training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)
   2358     is_compile_called = False
   2359     if not self._is_compiled and self.optimizer:
-&gt; 2360       self._compile_from_inputs(all_inputs, y_input, x, y)
   2361       is_compile_called = True
   2362 

~\.conda\envs\gputest\lib\site-packages\tensorflow_core\python\keras\engine\training.py in _compile_from_inputs(self, all_inputs, target, orig_inputs, orig_target)
   2609     self.compile(
   2610         optimizer=self.optimizer,
-&gt; 2611         loss=self.loss,
   2612         metrics=self._compile_metrics,
   2613         weighted_metrics=self._compile_weighted_metrics,
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='rzaratx' date='2020-03-23T19:18:07Z'>
		Can you remove the device scope in the get_model function and give a complete reproducible example?
Can you also clarify the issues you had with the respective TF versions?
In TF 2.0 - you had issues with calling model.predict?
In TF 2.1 - you had a problem with the kernel dying? Where were you running this?
		</comment>
		<comment id='2' author='rzaratx' date='2020-04-29T23:32:02Z'>
		I a having the same exact problem with this, as there is no get_model.
It is jupyter notebook and TF 2.1.
def build_model2():
model = Sequential([
Dense(n, input_dim=n, kernel_initializer='normal', activation='relu'),
Dense(17, kernel_initializer='normal', activation='relu'),
Dense(4, kernel_initializer='normal', activation='relu'),
Dense(1, kernel_initializer='normal')])
model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mae', 'mse'])
return model
strategy = tf.distribute.MirroredStrategy(devices=["/gpu:2"])
with strategy.scope():
parallel_model = build_model2()
parallel_model.compile(loss='mean_absolute_error', optimizer='adam')
--&gt; Kernel is dying at this cell and I am not able to run the below cells.
parallel_model.summary()
parallel_model.fit(x, y, epochs=20, batch_size=256)
		</comment>
		<comment id='3' author='rzaratx' date='2020-04-30T16:19:57Z'>
		I solved my problem by downgrading to tensorflow-gpu=2.0.0 and had to add the .h5 extension when saving my final model and when saving my models at my checkpoint.
		</comment>
		<comment id='4' author='rzaratx' date='2020-04-30T16:20:01Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37028&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37028&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='rzaratx' date='2020-09-09T03:33:25Z'>
		
I a having the same exact problem with this, as there is no get_model.
It is jupyter notebook and TF 2.1.
def build_model2():
model = Sequential([
Dense(n, input_dim=n, kernel_initializer='normal', activation='relu'),
Dense(17, kernel_initializer='normal', activation='relu'),
Dense(4, kernel_initializer='normal', activation='relu'),
Dense(1, kernel_initializer='normal')])
model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mae', 'mse'])
return model
strategy = tf.distribute.MirroredStrategy(devices=["/gpu:2"])
with strategy.scope():
parallel_model = build_model2()
parallel_model.compile(loss='mean_absolute_error', optimizer='adam')
--&gt; Kernel is dying at this cell and I am not able to run the below cells.
parallel_model.summary()
parallel_model.fit(x, y, epochs=20, batch_size=256)

&lt;denchmark-link:https://github.com/BanuSelinTosun&gt;@BanuSelinTosun&lt;/denchmark-link&gt;
 I had the same issue and I was solve it by moving 'with strategy.scope():' line into the function, just like how &lt;denchmark-link:https://github.com/rzaratx&gt;@rzaratx&lt;/denchmark-link&gt;
 did, but I am not sure why it doesn't work in their case.
Best,
Emre
		</comment>
	</comments>
</bug>