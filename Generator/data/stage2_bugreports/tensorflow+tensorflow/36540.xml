<bug id='36540' author='jorgeraulgomez' open_date='2020-02-07T14:18:54Z' closed_time='2020-02-12T20:27:51Z'>
	<summary>Can't compute distributed gradient in 2 computers with GradientTape</summary>
	<description>

TensorFlow version 2.0.0

I am training 2 cascading models, model1 and model2, to be used in 2 different computers. Currently I am simulating it in the same computer. If I train the models in the same gradient tape, it trains perfectly, computing the gradients as:
&lt;denchmark-code&gt;    import tensorflow as tf
    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

    with tf.GradientTape(persistent=True) as tape:
        features = model1(x_batch_train)
        logits = model2(features)
        loss_value = loss_fn(y_batch_train, logits)

    # Computing gradients for model 2
    grads2 = tape.gradient(loss_value, model2.trainable_variables)
    # Computing gradients to pass to model 1
    grads_pass = tape.gradient(loss_value, features)
    # Computing gradients for model 1
    grads1 = tape.gradient(features, model1.trainable_variables, output_gradients=grads_pass)
&lt;/denchmark-code&gt;

However, if I want to split the training into 2 different computers, I should have a gradient tape in each one of them, but then the results computed are different!
&lt;denchmark-code&gt;    import tensorflow as tf
    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

    with tf.GradientTape() as tape1:
        features = model1(x_batch_train)
    
    with tf.GradientTape(persistent=True) as tape2:
        logits = model2(features)
        loss_value = loss_fn(y_batch_train, logits)

    # Computing gradients for model 2
    grads2 = tape2.gradient(loss_value, model2.trainable_variables)
    # Computing gradients to pass to model 1
    grads_pass = tape2.gradient(loss_value, features)
    # Computing gradients for model 1
    grads1 = tape1.gradient(features, model1.trainable_variables, output_gradients=grads_pass)
&lt;/denchmark-code&gt;

It would be very helpful to have a way to be able to compute gradients in different tapes synchronizing them somehow.
Thanks in advance
	</description>
	<comments>
		<comment id='1' author='jorgeraulgomez' date='2020-02-10T08:48:12Z'>
		&lt;denchmark-link:https://github.com/jorgeraulgomez&gt;@jorgeraulgomez&lt;/denchmark-link&gt;

Can you please provide us colab link or simple stand alone code with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!
		</comment>
		<comment id='2' author='jorgeraulgomez' date='2020-02-10T14:53:24Z'>
		Yes, sure
&lt;denchmark-link:https://drive.google.com/file/d/1uCGFklj4EoyrlltgXeOGudMsEGtULbXm/view?usp=sharing&gt;https://drive.google.com/file/d/1uCGFklj4EoyrlltgXeOGudMsEGtULbXm/view?usp=sharing&lt;/denchmark-link&gt;

Thank you very much!
		</comment>
		<comment id='3' author='jorgeraulgomez' date='2020-02-11T22:12:02Z'>
		&lt;denchmark-link:https://github.com/jorgeraulgomez&gt;@jorgeraulgomez&lt;/denchmark-link&gt;
 Please take a look at this &lt;denchmark-link:https://github.com/tensorflow/examples/blob/master/community/en/docs/deploy/distributed.md&gt;doc&lt;/denchmark-link&gt;
 where it explains how to use tensorflow in a distributed mode. Thanks!
		</comment>
		<comment id='4' author='jorgeraulgomez' date='2020-02-12T09:03:59Z'>
		Thank you for your answer. It is a very interesting document.
However, I don't think it solves my problem. I am currently executing it in the same notebook, so I am not distributing it yet. This will be a problem of my future me, and I will address it, but for now I would need to be able to execute it in the same notebook.
Regards
		</comment>
		<comment id='5' author='jorgeraulgomez' date='2020-02-12T20:27:50Z'>
		&lt;denchmark-link:https://github.com/jorgeraulgomez&gt;@jorgeraulgomez&lt;/denchmark-link&gt;
 If you are planning to work on the distributed mode then you have to follow the Distributed Tensorflow concept.
Anyways I am going to close this issue for now and if you encounter any issues in future, create a new issue. Thanks!
		</comment>
		<comment id='6' author='jorgeraulgomez' date='2020-02-12T20:27:53Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36540&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36540&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>