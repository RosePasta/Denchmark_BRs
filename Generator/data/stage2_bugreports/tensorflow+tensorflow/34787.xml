<bug id='34787' author='Tenyn' open_date='2019-12-03T08:18:25Z' closed_time='2019-12-17T23:48:04Z'>
	<summary>Bug: tensors built by tf.keras layers cannot use numpy() to obtain the array.</summary>
	<description>
Bug: tensors built by tf.keras layers cannot use numpy() to obtain the array. So can i get the value of such tensors??
	</description>
	<comments>
		<comment id='1' author='Tenyn' date='2019-12-04T08:20:04Z'>
		Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?
Request you to provide simple standalone code to reproduce the issue in our environment. If you are unclear what to include see the issue template displayed in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/new/choose&gt;the Github new issue template&lt;/denchmark-link&gt;
.
We ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!
		</comment>
		<comment id='2' author='Tenyn' date='2019-12-04T08:54:56Z'>
		Operating system is windows 10, and TensorFlow version s 2.0.0.
I want to obtain the value of my custom layer, and fail by using numpy() which raises 'Tensor' object has no attribute 'numpy' error.
Eager execution is used automatically.
The code is here, using an example in the tensorflow tutorials.
import tensorflow as tf
from tensorflow.keras import *

print("TensorFlow version: {}".format(tf.__version__))
print("Eager execution: {}".format(tf.executing_eagerly()))

mnist = tf.keras.datasets.mnist

(x_train, y_train),(x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

import tensorflow.keras.backend as K

class MyDenseLayer(tf.keras.layers.Layer):
    def __init__(self, num_outputs):
        super(MyDenseLayer, self).__init__()
        self.num_outputs = num_outputs
        self.a = None
  
    def build(self, input_shape):
        self.kernel = self.add_weight("kernel",
                                        shape=[int(input_shape[-1]), self.num_outputs])
  
    def call(self, inputs):
        self.a = K.dot(inputs, self.kernel)
        return K.dot(inputs, self.kernel)

layer1 = MyDenseLayer(64)
input_tensor = Input(shape=(28, 28))
x = layers.Flatten()(input_tensor)
x = layer1(x)
output_tensor = layers.Dense(10, activation='softmax')(x)

model = Model(input_tensor,  output_tensor)

model.summary()

adam = tf.keras.optimizers.Adam()
model.compile(optimizer=adam,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

for epoch in range(2):
    model.fit(x_train, y_train, batch_size=32, epochs=1)

layer = model.layers[2]
print('--------------------------------------------------')
print(layer.a)

layer.a.numpy()
		</comment>
		<comment id='3' author='Tenyn' date='2019-12-04T09:44:23Z'>
		I have tried on colab with TF version 2.0 ,2.1.0-dev20191203 and was able to reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/de006b8bfcd8c04b89f483a0eaececac/untitled446.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='4' author='Tenyn' date='2019-12-05T07:33:23Z'>
		Even though the boolean flag "tf.executing_eagerly()" is True, while using the Keras model you are essentially creating a graph(DAGs) of layers. So in the flow through the graph, tensors created are of the type "tensorflow.python.framework.ops.Tensor" and not "tensorflow.python.framework.ops.EagerTensor".
Hence the error "'Tensor' object has no attribute 'numpy' error.".
		</comment>
		<comment id='5' author='Tenyn' date='2019-12-08T09:36:00Z'>
		
Even though the boolean flag "tf.executing_eagerly()" is True, while using the Keras model you are essentially creating a graph(DAGs) of layers. So in the flow through the graph, tensors created are of the type "tensorflow.python.framework.ops.Tensor" and not "tensorflow.python.framework.ops.EagerTensor".
Hence the error "'Tensor' object has no attribute 'numpy' error.".

How to change tensorflow.python.framework.ops.Tensor into numpy?
		</comment>
		<comment id='6' author='Tenyn' date='2019-12-08T12:10:11Z'>
		You have to explicitly declare as EagerMode to run it eagerly
tf.config.experimental_run_functions_eagerly(True).
Please note that there will be significant performance hits when this is set to true. This is suggested only in cases like debugging etc.
Even after decorating the  method of  MyDenseLayer with tf.function. there is only slight performance gain.
Please find the gist here &lt;denchmark-link:https://colab.research.google.com/drive/16hPlAHon5Lwr2uy1Nl3OSAOiBXLqSe4A&gt;https://colab.research.google.com/drive/16hPlAHon5Lwr2uy1Nl3OSAOiBXLqSe4A&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='Tenyn' date='2019-12-17T23:48:04Z'>
		&lt;denchmark-link:https://github.com/Tenyn&gt;@Tenyn&lt;/denchmark-link&gt;
 thanks for the issue!
As &lt;denchmark-link:https://github.com/Athul8raj&gt;@Athul8raj&lt;/denchmark-link&gt;
 mentioned, this is expected behavior for Tensor created inside a Functional Model. These Tensors are created during the execution of a , and as such are Symbolic Tensors, which cannot be converted to a value
You could assign layer.a to a Variable if you need to inspect it's value after execution, the Variables have values that live on after tf.function execution. Something like this should work:
class MyDenseLayer(tf.keras.layers.Layer):
    def __init__(self, num_outputs):
        super(MyDenseLayer, self).__init__()
        self.num_outputs = num_outputs
        self.a = tf.Variable([0]*num_outputs, dtype=tf.dtypes.float32)
  
    def build(self, input_shape):
        self.kernel = self.add_weight("kernel",
                                        shape=[int(input_shape[-1]), self.num_outputs])
  
    def call(self, inputs):
        self.a.assign(K.dot(inputs, self.kernel))
        return K.dot(inputs, self.kernel)

layer1 = MyDenseLayer(64)
input_tensor = Input(shape=(28, 28))
x = layers.Flatten()(input_tensor)
x = layer1(x)
output_tensor = layers.Dense(10, activation='softmax')(x)

model = Model(input_tensor,  output_tensor)

model.summary()

adam = tf.keras.optimizers.Adam()
model.compile(optimizer=adam,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

for epoch in range(2):
    model.fit(x_train, y_train, batch_size=32, epochs=1)

layer = model.layers[2]
print('--------------------------------------------------')
print(layer.a)

layer.a.numpy()
		</comment>
		<comment id='8' author='Tenyn' date='2019-12-17T23:48:06Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34787&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34787&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>