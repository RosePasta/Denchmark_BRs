<bug id='27190' author='matgad' open_date='2019-03-27T09:26:57Z' closed_time='2019-08-06T00:34:26Z'>
	<summary>Losses with Reduction.NONE do not keep the input shape (the result is averaged along the last axis)</summary>
	<description>
System information

OS Platform and Distribution: Linux Manjaro
TensorFlow installed from: binary
TensorFlow version (use command below): 2.0.0-dev20190326 (nightly build)
Python version: 3.7.2

Describe the current behavior
Currently, when a loss object is created with reduction=tf.losses.Reduction.NONE, the result is averaged along the last axis. This happens with all the losses in the tf.losses module.

When a loss object is created with reduction=tf.losses.Reduction.NONE, the output shape should match the input shape (y_true and y_pred shape).
This is also reported in the doc: &lt;denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/losses/Reduction&gt;tf.losses.Reduction&lt;/denchmark-link&gt;

Moreover, when a   parameter (&lt;denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/losses/BinaryCrossentropy&gt;BinaryCrossentropy&lt;/denchmark-link&gt;
) is provided with the same shape of y_pred, it throws an exception.
Code to reproduce the issue
Loss function with Reduction.NONE
&lt;denchmark-code&gt;bce = tf.losses.BinaryCrossentropy(reduction=tf.losses.Reduction.NONE)
y_true = [1., 1., 1., 1.]
y_pred = [1., 1., 0.5, 0.5]
loss = bce(y_true, y_pred)
print('Loss: ', loss.numpy())
&lt;/denchmark-code&gt;

CURRENT RESULT: 0.34657347
EXPECTED RESULT: [0., 0., 0.69314694, 0.69314694]
Loss function with Reduction.NONE and sample_weight
&lt;denchmark-code&gt;bce = tf.losses.BinaryCrossentropy(reduction=tf.losses.Reduction.NONE)
y_true = [1., 1., 1., 1.]
y_pred = [1., 1., 0.5, 0.5]
sample_weight = [1, 1, 2, 1]
loss = bce(y_true, y_pred, sample_weight=sample_weight)
print('Loss: ', loss.numpy())
&lt;/denchmark-code&gt;

CURRENT RESULT: Exception
EXPECTED RESULT: [-0., -0., 1.3862939, 0.69314694]
Other info / logs
Currently, to achieve the expected result, an additional "fake" dimension must be added to y_true and y_pred, as in the following example.
&lt;denchmark-code&gt;bce = tf.losses.BinaryCrossentropy(reduction=tf.losses.Reduction.NONE)
y_true = [1., 1., 1., 1.]
y_pred = [1., 1., 0.5, 0.5]
sample_weight = [1, 1, 2, 1]
loss = bce(tf.expand_dims(y_true, -1), tf.expand_dims(y_pred, -1), sample_weight=sample_weight)
print('Loss: ', loss.numpy())
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='matgad' date='2019-04-10T18:31:33Z'>
		&lt;denchmark-link:https://github.com/amitsrivastava78&gt;@amitsrivastava78&lt;/denchmark-link&gt;
 Can you please take a look at this issue? Thanks!
		</comment>
		<comment id='2' author='matgad' date='2019-04-11T06:26:35Z'>
		&lt;denchmark-link:https://github.com/matgad&gt;@matgad&lt;/denchmark-link&gt;
 , actually BinaryCrossentropy uses the function nn.sigmoid_cross_entropy_with_logits which takes into account the reduction and  applies applying the below formula :-
_

max(x, 0) - x * z + log(1 + exp(-abs(x)))
where in your case y_pred = x and y_true = z.

_
So based on this the computed size is same as that of input, but there is a twist in this before the actual result is passed it is done a reduce_mean with axis as -1 which means it will add the axis with input dimension and do the mean on tat axes, which s the result you are getting : -
0.34657347
Hope this clears things.
Regards
Amit
		</comment>
		<comment id='3' author='matgad' date='2019-04-11T08:47:12Z'>
		Hi &lt;denchmark-link:https://github.com/amitsrivastava78&gt;@amitsrivastava78&lt;/denchmark-link&gt;
, thanks for the answer. The behavior that you described is the default one. But when you define a loss with the optional parameter , the last reduce_mean (or any other reduce) should be skipped. This issue only applies with None reduction.
 is just an example, the same behavior should be extended for any loss function.
		</comment>
		<comment id='4' author='matgad' date='2019-04-11T11:48:18Z'>
		&lt;denchmark-link:https://github.com/matgad&gt;@matgad&lt;/denchmark-link&gt;
 ,i see your point, i think this is a BUG, irrespective of the reduction, the tensorflow is applying the reduce_mean to everything, in the coming days i will try to raise one PR to fix this issue.
Regards
Amit
		</comment>
		<comment id='5' author='matgad' date='2019-04-11T12:00:21Z'>
		Great, many thanks. Please also take a look on the exception that is thrown when sample_weight has the same shape of the input (see the example in my first post for details), I think the two issues are related.
		</comment>
		<comment id='6' author='matgad' date='2019-04-12T09:58:31Z'>
		&lt;denchmark-link:https://github.com/matgad&gt;@matgad&lt;/denchmark-link&gt;
 , i have raised the PR and included your cases as unit test cases as well, Lets wait for this PR to merge, in the mean time you can  use this PR in case you are blocked.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/27784&gt;#27784&lt;/denchmark-link&gt;

Regards
Amit
		</comment>
		<comment id='7' author='matgad' date='2019-04-12T11:46:42Z'>
		Wonderful, thank you.
		</comment>
		<comment id='8' author='matgad' date='2019-04-26T23:43:49Z'>
		&lt;denchmark-link:https://github.com/matgad&gt;@matgad&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/amitsrivastava78&gt;@amitsrivastava78&lt;/denchmark-link&gt;
  The loss functions actually do not perform any reduction by default. The expectation is that the input passed to the functions will be atleast 2D. These functions return one loss value per sample as output.
Eg if y_true and y_pred have the shape (3, 3), return value will be of shape (3,). =&gt; we get one loss value per sample
if y_true and y_pred have the shape (3, 2, 4), return value will be of shape (3, 2) =&gt; we get one loss value per sample, per timestep etc.
Sorry that the functions do not have any documentation about this currently. We are working on adding that, please feel free to contribute to the documentation if you are interested.
I just replied on the PR with this as well.
		</comment>
		<comment id='9' author='matgad' date='2019-04-26T23:46:36Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=27190&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=27190&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='matgad' date='2019-04-29T01:16:08Z'>
		Why would issue be closed before updating the documentation? I just ran into the problem because documentation specifically said reduction=Reduction.NONE would give me same shape as I send in.
		</comment>
		<comment id='11' author='matgad' date='2019-04-29T17:14:55Z'>
		&lt;denchmark-link:https://github.com/pat-coady&gt;@pat-coady&lt;/denchmark-link&gt;
 sure we can leave this issue open until we have documentation updated.
		</comment>
		<comment id='12' author='matgad' date='2019-04-29T19:24:23Z'>
		&lt;denchmark-link:https://github.com/pavithrasv&gt;@pavithrasv&lt;/denchmark-link&gt;
 If it is helpful, I can update docstrings in appropriate places and submit the PR. I haven't contributed to the project before, but am reviewing Contributing Guidelines now.
		</comment>
		<comment id='13' author='matgad' date='2019-04-29T20:43:00Z'>
		&lt;denchmark-link:https://github.com/pat-coady&gt;@pat-coady&lt;/denchmark-link&gt;
 Oh yes absolutely. You are welcome to make contributions anytime :) As mentioned in my comment above we will need to add documentation for all standalone functions in losses.py (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/losses.py#L752&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/losses.py#L752&lt;/denchmark-link&gt;
) indicating that the input needs to be at least 2D.
		</comment>
		<comment id='14' author='matgad' date='2019-07-07T14:12:39Z'>
		Submitted PR:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/30463&gt;#30463&lt;/denchmark-link&gt;

		</comment>
		<comment id='15' author='matgad' date='2019-07-08T17:41:41Z'>
		Great thank you!
		</comment>
		<comment id='16' author='matgad' date='2019-08-06T00:34:25Z'>
		Automatically closing this out since I understand it to be resolved by the PR &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/30463&gt;#30463&lt;/denchmark-link&gt;
 (merged already), but please let me know if I'm mistaken.Thanks!
		</comment>
		<comment id='17' author='matgad' date='2019-08-06T00:34:27Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=27190&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=27190&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='18' author='matgad' date='2020-06-22T23:22:02Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 This still is NOT resolved even with TF2.2.
&lt;denchmark-code&gt;a = np.array([[.3, .4], [.5, .6]])
b = np.array([[.312, .433], [.555, .66]])

print('incorrect output shape:', tf.keras.losses.BinaryCrossentropy(reduction=tf.losses.Reduction.NONE)(a, b))
print('+'*10)
print('correct output shape:', tf.nn.sigmoid_cross_entropy_with_logits(a, b))

incorrect output shape: tf.Tensor([0.64322317 0.69003338], shape=(2,), dtype=float64)
++++++++++
correct output shape: tf.Tensor(
[[0.76766615 0.75970248]
 [0.73116606 0.6806367 ]], shape=(2, 2), dtype=float64)

&lt;/denchmark-code&gt;

reduction='none' needs to mean reduction equals none...not sure why this would still not be the case. :(
		</comment>
		<comment id='19' author='matgad' date='2020-06-23T00:16:55Z'>
		&lt;denchmark-link:https://github.com/pGit1&gt;@pGit1&lt;/denchmark-link&gt;
 This is expected and this is what &lt;denchmark-link:https://github.com/pavithrasv&gt;@pavithrasv&lt;/denchmark-link&gt;
 described in detail &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/27190#issuecomment-487233022&gt;here&lt;/denchmark-link&gt;
.
Please check the usage examples and description &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy?version=nightly&gt;here&lt;/denchmark-link&gt;
.
Please let me know if there is anything I am missing. Thanks!
		</comment>
		<comment id='20' author='matgad' date='2020-06-23T17:02:00Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 I see that but it still doesn't make sense to me. I discovered the tf.nn.sigmoid_cross_entropy by accident and it behaves  when there is no reduction (i.e. element-wise loss scalars). &lt;denchmark-link:https://github.com/pavithrasv&gt;@pavithrasv&lt;/denchmark-link&gt;
 explanation doesn't makes sense to me and the documentation doesn't either. It's is weird to me that no reduction actually leads to a loss in tensor dimension...I just cannot wrap my head around on what "sample" means in &lt;denchmark-link:https://github.com/pavithrasv&gt;@pavithrasv&lt;/denchmark-link&gt;
 explanation or the official documentation.
If I am comparing two 3x3 tensors with no reduction I have no idea why I am not getting a 3x3 loss tensor out. I really may be just misunderstanding what "sample" means here. It almost seems like by sample you mean row in y_pred and y_true tensors. Its as if you actually do compute an element-wise loss and once this is done you take the mean along the -1 dimension. But the fact that a mean (or sum) is taken means there is a reduction by definition. :(
I really dislike this inflexibility. It would be nice if reduction='none' actually meant no reduction in tensor dimension. This behavior means of all the useful losses that tf.losses already provides would need to be re-implemented or do the adding an extra dimension workaround that &lt;denchmark-link:https://github.com/matgad&gt;@matgad&lt;/denchmark-link&gt;
 pointed out. Again reduction equals none should not mean the software takes an average for me. I've updated and confirmed my example above:
&lt;denchmark-code&gt;a = np.array([[.3, .4], [.5, .6]])
b = np.array([[.312, .433], [.555, .66]])

print('incorrect output shape:', tf.keras.losses.BinaryCrossentropy(reduction=tf.losses.Reduction.NONE,
                                                                    from_logits=True)(a, b))
print('+'*10)
print('correct output shape:', tf.nn.sigmoid_cross_entropy_with_logits(a, b))
v = tf.nn.sigmoid_cross_entropy_with_logits(a, b).numpy()
print(f"confirming naughty behavior of reduction='none':{v.mean(axis=-1)} ")

incorrect output shape: tf.Tensor([0.76368433 0.70590138], shape=(2,), dtype=float64)
++++++++++
correct output shape: tf.Tensor(
[[0.76766615 0.75970248]
 [0.73116606 0.6806367 ]], shape=(2, 2), dtype=float64)
++++++++++
confirming naughty behavior of reduction='none':[0.76368431 0.70590138] 
&lt;/denchmark-code&gt;

No reduction should mean no reduction. Even if the documentation points out the behavior it is still incredibly inflexible and frustrating.
		</comment>
		<comment id='21' author='matgad' date='2020-06-23T20:31:20Z'>
		&lt;denchmark-link:https://github.com/pGit1&gt;@pGit1&lt;/denchmark-link&gt;
 Sorry about that. I understand this can be confusing. This is by design and has been since the beginning of Keras, so any changes to this will be a big breaking change. If your data is 1D, we request that this be converted to 2D as losses expect 2D data. You will get 1D data back ie. one loss value per example when reduction is None.
		</comment>
		<comment id='22' author='matgad' date='2020-06-24T18:03:59Z'>
		&lt;denchmark-link:https://github.com/pavithrasv&gt;@pavithrasv&lt;/denchmark-link&gt;
 I guess adding an extra dimension solution isnt so bad but I still think its weird. Thanks for clarifying.
		</comment>
		<comment id='23' author='matgad' date='2020-07-01T19:37:43Z'>
		&lt;denchmark-link:https://github.com/pavithrasv&gt;@pavithrasv&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;

Quick question by the way while on this topic. I wonder what tf.keras is actually doing with  tensors in general because if I use a tf.keras.Reshape layer I can literally reshape the layer to anything I want and it will still work. Also I can have an output layer for a model of some fixed dimension and pass it a tensor of different dimension during training and it still works. Why is that?? What is going on now? Im using TF 2.2 by the way. Also tf.reshape behaves as it should, i.e. it breaks when I try to arbitrarily reshape a tensor with more or less elements than input tensor...
		</comment>
	</comments>
</bug>