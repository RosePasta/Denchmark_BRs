<bug id='40157' author='aselva-eb' open_date='2020-06-04T16:01:38Z' closed_time='2020-09-22T03:54:05Z'>
	<summary>TFLite: Cannot run inference on TF Lite Model: "Regular TensorFlow ops are not supported by this interpreter."</summary>
	<description>
System information

OSX
TF 2.3.0-dev20200602

Command used to run the converter or code if youâ€™re using the Python API
If possible, please share a link to Colab/Jupyter/any notebook.
Conversion code:
&lt;denchmark-code&gt;    converter = tf.lite.TFLiteConverter.from_saved_model(curr_dir + "saved_model")
    tflite_model = converter.convert()

    # Save the TF Lite model.
    with tf.io.gfile.GFile(curr_dir + '/model.tflite', 'wb') as f:
        f.write(tflite_model)
&lt;/denchmark-code&gt;

Inference code:
&lt;denchmark-code&gt;    # Compare Inference
    import tensorflow as tf

    # Load the TFLite model and allocate tensors.
    interpreter = tf.lite.Interpreter(model_path="./model.tflite")
    interpreter.allocate_tensors()

    # Get input and output tensors.
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
&lt;/denchmark-code&gt;

The model I'm trying to convert to tflite and run inference on is SSDLite_MobileNetV2, obtained rom the Model Zoo:
&lt;denchmark-link:http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz&gt;http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz&lt;/denchmark-link&gt;

Failure details
Conversion is successful, however I cannot run inference: Here is the error that I run into:
&lt;denchmark-code&gt;RuntimeError: Regular TensorFlow ops are not supported by this interpreter. 
Make sure you apply/link the Flex delegate before inference.Node number 3 (FlexTensorArrayV3) failed to prepare.
&lt;/denchmark-code&gt;

I've been playing around with converter settings with no luck
i.e. combinations of:
&lt;denchmark-code&gt;    # converter.optimizations = [tf.lite.Optimize.DEFAULT]
    # converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,
    #                                        tf.lite.OpsSet.SELECT_TF_OPS]
&lt;/denchmark-code&gt;

With none of the settings above set, or the supported_ops set, I can convert the model but cannot run inference, with a similar error as above.
With optimizations set to default, it gives me an error in conversion
	</description>
	<comments>
		<comment id='1' author='aselva-eb' date='2020-06-05T03:58:46Z'>
		Using flex delegate in python is not yet supported &lt;denchmark-link:https://www.tensorflow.org/lite/guide/ops_select#python_pip_package&gt;https://www.tensorflow.org/lite/guide/ops_select#python_pip_package&lt;/denchmark-link&gt;

However, the feature will be landed really soon, might be sometimes next week, so please wait a little bit.
		</comment>
		<comment id='2' author='aselva-eb' date='2020-06-05T15:27:18Z'>
		Thanks for the information. I'll keep a watch
		</comment>
		<comment id='3' author='aselva-eb' date='2020-06-05T15:27:20Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40157&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40157&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='aselva-eb' date='2020-06-12T16:26:05Z'>
		Is there an approximate ETA on Python support for flex delegate?
		</comment>
		<comment id='5' author='aselva-eb' date='2020-06-15T02:14:02Z'>
		The PR got all required approval. I think it won't take too long.
		</comment>
		<comment id='6' author='aselva-eb' date='2020-06-16T08:51:15Z'>
		The feature is delivered at the HEAD of master. aselva-eb you can try it now.
		</comment>
		<comment id='7' author='aselva-eb' date='2020-06-17T09:56:44Z'>
		&lt;denchmark-link:https://github.com/thaink&gt;@thaink&lt;/denchmark-link&gt;
 Would you please post a sample code of how to use this in ?
		</comment>
		<comment id='8' author='aselva-eb' date='2020-06-17T16:06:40Z'>
		Thank you &lt;denchmark-link:https://github.com/thaink&gt;@thaink&lt;/denchmark-link&gt;
 for continuing to give me updates on this - I appreciate it very much!
I tried to update my Tensorflow (using tf-nightly to get latest HEAD of master) and run inference on a model with both ops - however I still run into the same error. Perhaps there's a flag in Interpreter that needs to be set to enable flex ops?
Here's the code I'm using for inference:
&lt;denchmark-code&gt;# Load the TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_path="./model.tflite")
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

&lt;/denchmark-code&gt;

Error happens on allocate_tensors() function:
Here is the error:
Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 3 (FlexTensorArrayV3) failed to prepare.
		</comment>
		<comment id='9' author='aselva-eb' date='2020-06-19T04:07:24Z'>
		Hi aselva-eb,
There was a regression with the PR so it got rolled back and just re-submitted yesterday.
Can you give it a try again?
There should be no additional step to use Flex delegate.
		</comment>
		<comment id='10' author='aselva-eb' date='2020-06-19T12:37:08Z'>
		&lt;denchmark-link:https://github.com/thaink&gt;@thaink&lt;/denchmark-link&gt;
 still no luck. Was it rolled-back again by any chance? I'm on: 
		</comment>
		<comment id='11' author='aselva-eb' date='2020-06-22T06:50:35Z'>
		aselva-eb,
The PR was not rolled back anymore.
Could you send me your converted model so I can do a check.
		</comment>
		<comment id='12' author='aselva-eb' date='2020-06-22T13:41:40Z'>
		This is absolutely fabulous! First tests on tf-nightly 06-22  seem to work perfectly. I have not evaluated the results of the converted models, but FlexDelegate loads automatically for models that need it.
		</comment>
		<comment id='13' author='aselva-eb' date='2020-06-23T06:35:53Z'>
		Great to hear that.
		</comment>
		<comment id='14' author='aselva-eb' date='2020-06-23T12:43:32Z'>
		&lt;denchmark-link:https://github.com/thaink&gt;@thaink&lt;/denchmark-link&gt;
 - I've attached the converted model:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/4819404/model.tflite.zip&gt;model.tflite.zip&lt;/denchmark-link&gt;

		</comment>
		<comment id='15' author='aselva-eb' date='2020-06-25T04:09:35Z'>
		
Thank you @thaink for continuing to give me updates on this - I appreciate it very much!
I tried to update my Tensorflow (using tf-nightly to get latest HEAD of master) and run inference on a model with both ops - however I still run into the same error. Perhaps there's a flag in Interpreter that needs to be set to enable flex ops?
Here's the code I'm using for inference:
# Load the TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_path="./model.tflite")
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

Error happens on allocate_tensors() function:
Here is the error:
Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 3 (FlexTensorArrayV3) failed to prepare.

have you fixed it yet? I have the same issue :((((
		</comment>
		<comment id='16' author='aselva-eb' date='2020-06-25T07:24:02Z'>
		In my test today. It is working.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/4829629/test_flex.ipynb.zip&gt;test_flex.ipynb.zip&lt;/denchmark-link&gt;

		</comment>
		<comment id='17' author='aselva-eb' date='2020-06-25T14:37:52Z'>
		
In my test today. It is working.
test_flex.ipynb.zip

Hi &lt;denchmark-link:https://github.com/thaink&gt;@thaink&lt;/denchmark-link&gt;

I tried your notebook with a fresh environment and wasn't able to get it to work. Still get the same issue:
&lt;denchmark-code&gt;RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 3 (FlexTensorArrayV3) failed to prepare.
&lt;/denchmark-code&gt;

Version of tf-nightly: 2.3.0-dev20200625 running with Python 3.7
This very odd...
Can you re-try with a fresh environment and let me know?
		</comment>
		<comment id='18' author='aselva-eb' date='2020-06-29T14:30:06Z'>
		Hi &lt;denchmark-link:https://github.com/thaink&gt;@thaink&lt;/denchmark-link&gt;
 just wanted to follow up with ^. Are you able to reproduce your results with a fresh environment?
		</comment>
		<comment id='19' author='aselva-eb' date='2020-06-30T14:32:58Z'>
		Hi aselva-eb,
I tried a fresh environment with venv today and it is still working fine.
Can you try VenV? Are you getting the error at interpreter.allocate_tensors()?
		</comment>
		<comment id='20' author='aselva-eb' date='2020-06-30T14:34:11Z'>
		
Hi aselva-eb,
I tried a fresh environment with venv today and it is still working fine.
Can you try VenV? Are you getting the error at interpreter.allocate_tensors()?

Oh really??? No I'm using Conda..
Yes, my error is on allocate_tensors()
		</comment>
		<comment id='21' author='aselva-eb' date='2020-09-08T08:20:50Z'>
		
@thaink can you give me an eta on the removal of the ops?

Flex-ops only models can support this ops so it will not be removed.
		</comment>
		<comment id='22' author='aselva-eb' date='2020-09-13T06:02:26Z'>
		&lt;denchmark-link:https://github.com/thaink&gt;@thaink&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/terryheo&gt;@terryheo&lt;/denchmark-link&gt;

Hi
i convert my frozen graph to tflite. when inference it, the inference code is
interpreter = tf.lite.Interpreter(model_path="converted_model.tflite")
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
input_shape = input_details[0]['shape']
input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)
interpreter.invoke()
output_data = interpreter.get_tensor(output_details[0]['index'])
print(output_data)
i get the below error:
RuntimeError: Container __per_step_0 does not exist. (Could not find resource:__per_step_0/_tensor_arraysbidirectional_rnn/bw/bw/dynamic_rnn/input_0_1)
(while executing 'TensorArrayScatterV3' via Eager)Node number 91 (TfLiteFlexDelegate) failed to invoke.
can you help me ?
		</comment>
		<comment id='23' author='aselva-eb' date='2020-09-13T08:23:28Z'>
		&lt;denchmark-link:https://github.com/saeedkhanehgir&gt;@saeedkhanehgir&lt;/denchmark-link&gt;
 TensorArrayScatterV3 does not works with flex delegate in most case.
For now, you'll need to remove it from the mode.
		</comment>
		<comment id='24' author='aselva-eb' date='2020-09-13T08:51:16Z'>
		&lt;denchmark-link:https://github.com/thaink&gt;@thaink&lt;/denchmark-link&gt;

thanks. For your answer
Could you please explain further where I should delete it?
		</comment>
		<comment id='25' author='aselva-eb' date='2020-09-13T08:58:26Z'>
		&lt;denchmark-link:https://github.com/saeedkhanehgir&gt;@saeedkhanehgir&lt;/denchmark-link&gt;
 I think you have to replace it in the training code. Then train the model and convert it again.
		</comment>
		<comment id='26' author='aselva-eb' date='2020-09-13T09:07:01Z'>
		&lt;denchmark-link:https://github.com/thaink&gt;@thaink&lt;/denchmark-link&gt;

Do I have no other solution?
		</comment>
		<comment id='27' author='aselva-eb' date='2020-09-13T09:10:31Z'>
		&lt;denchmark-link:https://github.com/thaink&gt;@thaink&lt;/denchmark-link&gt;

can i build a custom TensorArrayScatterV3 op?
		</comment>
		<comment id='28' author='aselva-eb' date='2020-09-13T10:10:58Z'>
		&lt;denchmark-link:https://github.com/thaink&gt;@thaink&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/terryheo&gt;@terryheo&lt;/denchmark-link&gt;

I think this error was because of lstm layer . if lstm layer was created by keras, this error was solved.
		</comment>
		<comment id='29' author='aselva-eb' date='2020-09-14T00:55:15Z'>
		&lt;denchmark-link:https://github.com/saeedkhanehgir&gt;@saeedkhanehgir&lt;/denchmark-link&gt;
 Great info. Thanks for letting us know.
		</comment>
		<comment id='30' author='aselva-eb' date='2020-09-17T13:46:33Z'>
		Hi &lt;denchmark-link:https://github.com/thaink&gt;@thaink&lt;/denchmark-link&gt;
,
I train and save .pb model with TensorFlow 1.15.0 and  try to convert .pb file to .tflite file with tf-nightly
Main codes:
coverter = tf.compat.v1.lite.TFliteConverter.from_frozen_graph()
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT]
converter.experimental_new_converter = True
Error:
RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. Node  number 3 (FlexSign) failed to prepare.
Seems like it's the problem with tf.sign and I DO have tf.sign in my model and there is BiLSTM layer in my model. So does it mean I have to implement tf.sign op on my own or are there other solutions?
Thank you.
		</comment>
		<comment id='31' author='aselva-eb' date='2020-09-17T15:05:22Z'>
		&lt;denchmark-link:https://github.com/le8888e&gt;@le8888e&lt;/denchmark-link&gt;
 Sign is supported by flex delegate so the conversion must be fine.
I think the problem here is a typo.
could you use tf.lite.OpsSet.SELECT_TF_OPS instead of tf.lite.OpsSet.SELECT
		</comment>
		<comment id='32' author='aselva-eb' date='2020-09-17T15:11:08Z'>
		Hi &lt;denchmark-link:https://github.com/thaink&gt;@thaink&lt;/denchmark-link&gt;
,
I am using tf.lite.OpsSet.SELECT_TF_OPS, sorry for my mistake. And I'm trying tf-nightly 2.4.0 for  flex delegate.
Another question,  I'm using tf 15.0 to define and train the model. In the step of defining model, is 'tf.enable_control_flow_v2()' a must?
Without this, there are errors like
â€˜converting unsupported op Enter and TensorArrayV3â€™
Thank you
		</comment>
		<comment id='33' author='aselva-eb' date='2020-09-18T00:23:37Z'>
		&lt;denchmark-link:https://github.com/le8888e&gt;@le8888e&lt;/denchmark-link&gt;
 I don't think define the model in a version and convert an a different version is a good practice.
Could you try to define the model with nightly? You can just try to convert it, no need to train.
		</comment>
		<comment id='34' author='aselva-eb' date='2020-09-18T01:03:51Z'>
		&lt;denchmark-link:https://github.com/thaink&gt;@thaink&lt;/denchmark-link&gt;
 OK, I will give it a try. Is tf-nightly 1.* available and where can I get them? Since code between tf 1.* and 2.* is a little different, I want to try tf-nightly 1.*
Thank you
		</comment>
		<comment id='35' author='aselva-eb' date='2020-09-18T01:30:38Z'>
		I don't think we have nightly for 1*
You can try disable_v2_behavior on nightly and see if it is what you expect.
		</comment>
		<comment id='36' author='aselva-eb' date='2020-09-18T05:57:18Z'>
		Hi &lt;denchmark-link:https://github.com/saeedkhanehgir&gt;@saeedkhanehgir&lt;/denchmark-link&gt;
,
I came up the same problem with you. In my case there are
tf.contirb.cnn.BasicLSTMCell()
and
tf.nn.bidirectional_dynamic_rnn()
in my model and there are other layers. Do I only need to implement these two layers by keras and keep other layers unchangeï¼Ÿ The tf version Iâ€˜m using is tf 1.15.0
Thank you.
		</comment>
		<comment id='37' author='aselva-eb' date='2020-09-21T07:24:33Z'>
		Hi  &lt;denchmark-link:https://github.com/thaink&gt;@thaink&lt;/denchmark-link&gt;
,
After implementing BiLSTM layer by Keras, I successfully convert and inference .tflite model.
I want to serve .tflite model by tf-serving on PC with python api. Is this feasible and is there any guidance for this?
THANK YOU
		</comment>
		<comment id='38' author='aselva-eb' date='2020-09-21T07:32:13Z'>
		&lt;denchmark-link:https://github.com/le8888e&gt;@le8888e&lt;/denchmark-link&gt;
 You implemented the BiLSTM for Tensorflow?
Is it in C++ or python?
		</comment>
		<comment id='39' author='aselva-eb' date='2020-09-21T07:38:13Z'>
		&lt;denchmark-link:https://github.com/thaink&gt;@thaink&lt;/denchmark-link&gt;
 Just replace TF layer declaration by Keras
Original:
outputs, _ = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, embedded_chars, dtype=tf.float32)
New:
outputs = tf.keras.layers.Birdirectional(lstm_cell, merge_mode="concat")(embedded_chars)
Then errors are gone.
		</comment>
		<comment id='40' author='aselva-eb' date='2020-09-21T07:47:12Z'>
		&lt;denchmark-link:https://github.com/le8888e&gt;@le8888e&lt;/denchmark-link&gt;
 Are you able to run the tflite model?
		</comment>
		<comment id='41' author='aselva-eb' date='2020-09-21T07:51:38Z'>
		&lt;denchmark-link:https://github.com/thaink&gt;@thaink&lt;/denchmark-link&gt;
 Yes, I can run .tflite model by
interpreter = tf.lite.Interpreter(model_path)
interpreter.allocate_tensors()
...
...
interpreter.invoke()
The outputs of .pb and .tflite are exactly the same.
I'm wondering if I can serve .tflite model with TF Serving.
Thank you
		</comment>
		<comment id='42' author='aselva-eb' date='2020-09-22T03:54:04Z'>
		There is an on-going effort of supporting tflite on TF Serving.
&lt;denchmark-link:https://github.com/tensorflow/serving/blob/master/tensorflow_serving/servables/tensorflow/tflite_session.cc&gt;https://github.com/tensorflow/serving/blob/master/tensorflow_serving/servables/tensorflow/tflite_session.cc&lt;/denchmark-link&gt;

It's not ready yet but it'll be available soon.
		</comment>
		<comment id='43' author='aselva-eb' date='2020-09-22T03:54:06Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40157&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40157&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='44' author='aselva-eb' date='2020-09-22T09:06:51Z'>
		Hi &lt;denchmark-link:https://github.com/terryheo&gt;@terryheo&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/thaink&gt;@thaink&lt;/denchmark-link&gt;
 ,
I was building tf-nightly viac pip. Would XNNPACK built by default? And how can I check it?
Thank you
		</comment>
		<comment id='45' author='aselva-eb' date='2020-09-22T09:52:54Z'>
		It's not enabled by default.
You can enable it with "--use_xnnpack=true" option.
&lt;denchmark-link:https://blog.tensorflow.org/2020/07/accelerating-tensorflow-lite-xnnpack-integration.html&gt;https://blog.tensorflow.org/2020/07/accelerating-tensorflow-lite-xnnpack-integration.html&lt;/denchmark-link&gt;

You can see "Created TensorFlow Lite XNNPACK delegate for CPU." from log.
		</comment>
	</comments>
</bug>