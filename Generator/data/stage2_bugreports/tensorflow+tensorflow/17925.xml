<bug id='17925' author='mrosenkranz' open_date='2018-03-22T13:50:21Z' closed_time='2018-12-06T23:15:56Z'>
	<summary>MaskedAutoregressiveFlow example (tf.contrib.distributions) raises ValueError</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab (also tried on a Windows 10 machine with TF 1.6)
TensorFlow installed from (source or binary): defaults from Colab
TensorFlow version (use command below): 1.6.0
Python version: 3.6.3 (default, Oct  3 2017, 21:45:48)
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: see attached file

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

I'm new to tf.contrib.distributions. I've just copied the example for MaskedAutoregressiveFlow from &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/contrib/distributions/bijectors/MaskedAutoregressiveFlow&gt;https://www.tensorflow.org/api_docs/python/tf/contrib/distributions/bijectors/MaskedAutoregressiveFlow&lt;/denchmark-link&gt;
. Running the example fails with a ValueError at . See the attached file and error log below.  Running  in the session doesn't solve it either. It looks like  expects a tensor with  but  passes a tensor with .
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/1837741/masked_autoregressive_issue.txt&gt;masked_autoregressive_issue.txt&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-3-646c58f4e818&gt; in &lt;module&gt;()
     12 sess.run(tf.global_variables_initializer())
     13 
---&gt; 14 x = maf.sample()  # Expensive; uses `tf.while_loop`, no Bijector caching.
     15 maf.log_prob(x)   # Almost free; uses Bijector caching.
     16 maf.log_prob(0.)  # Cheap; no `tf.while_loop` despite no Bijector caching.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/distributions/distribution.py in sample(self, sample_shape, seed, name)
    687       samples: a `Tensor` with prepended dimensions `sample_shape`.
    688     """
--&gt; 689     return self._call_sample_n(sample_shape, seed, name)
    690 
    691   def _log_prob(self, value):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/distributions/transformed_distribution.py in _call_sample_n(self, sample_shape, seed, name, **kwargs)
    411       # work, it is imperative that this is the last modification to the
    412       # returned result.
--&gt; 413       y = self.bijector.forward(x, **kwargs)
    414       y = self._set_sample_static_shape(y, sample_shape)
    415 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/distributions/bijector_impl.py in forward(self, x, name)
    618       NotImplementedError: if `_forward` is not implemented.
    619     """
--&gt; 620     return self._call_forward(x, name)
    621 
    622   def _inverse(self, y):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/distributions/bijector_impl.py in _call_forward(self, x, name, **kwargs)
    599       if mapping.y is not None:
    600         return mapping.y
--&gt; 601       mapping = mapping.merge(y=self._forward(x, **kwargs))
    602       self._cache(mapping)
    603       return mapping.y

/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/distributions/python/ops/bijectors/masked_autoregressive.py in _forward(self, x)
    245     y0 = array_ops.zeros_like(x, name="y0")
    246     # call the template once to ensure creation
--&gt; 247     _ = self._shift_and_log_scale_fn(y0)
    248     def _loop_body(index, y0):
    249       """While-loop body for autoregression calculation."""

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/template.py in __call__(self, *args, **kwargs)
    358           custom_getter=self._custom_getter) as vs:
    359         self._variable_scope = vs
--&gt; 360         result = self._call_func(args, kwargs)
    361         return result
    362 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/template.py in _call_func(self, args, kwargs)
    300       trainable_at_start = len(
    301           ops.get_collection(ops.GraphKeys.TRAINABLE_VARIABLES))
--&gt; 302       result = self._func(*args, **kwargs)
    303 
    304       if self._variables_created:

/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/distributions/python/ops/bijectors/masked_autoregressive.py in _fn(x)
    478             activation=activation,
    479             *args,
--&gt; 480             **kwargs)
    481       x = masked_dense(
    482           inputs=x,

/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/distributions/python/ops/bijectors/masked_autoregressive.py in masked_dense(inputs, units, num_blocks, exclusive, kernel_initializer, reuse, name, *args, **kwargs)
    386         *args,
    387         **kwargs)
--&gt; 388     return layer.apply(inputs)
    389 
    390 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py in apply(self, inputs, *args, **kwargs)
    807       Output tensor(s).
    808     """
--&gt; 809     return self.__call__(inputs, *args, **kwargs)
    810 
    811   def _add_inbound_node(self,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)
    671 
    672           # Check input assumptions set before layer building, e.g. input rank.
--&gt; 673           self._assert_input_compatibility(inputs)
    674           if input_list and self._dtype is None:
    675             try:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py in _assert_input_compatibility(self, inputs)
   1195                            ', found ndim=' + str(ndim) +
   1196                            '. Full shape received: ' +
-&gt; 1197                            str(x.get_shape().as_list()))
   1198       # Check dtype.
   1199       if spec.dtype is not None:

ValueError: Input 0 of layer dense_1 is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [5]

originally defined at:
  File "&lt;ipython-input-3-646c58f4e818&gt;", line 10, in &lt;module&gt;
    hidden_layers=[512,512])),
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/distributions/python/ops/bijectors/masked_autoregressive.py", line 499, in masked_autoregressive_default_template
    "masked_autoregressive_default_template", _fn)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/template.py", line 152, in make_template
    **kwargs)
&lt;/denchmark-code&gt;

edit: fixed link to TF doc
	</description>
	<comments>
		<comment id='1' author='mrosenkranz' date='2018-03-26T07:48:43Z'>
		It looks like changing the base distribution to have shape [1,1] eliminates the exception:
distribution=tfd.Normal(loc=[[0.]], scale=[[1.]]).
I feel silly realising this only now but it seems the general semantics of tf.layers expects the first dimension of inputs to be batch_size. If this is the case, may I suggest that this could be made clearer in the tf.layers documentation and/or reflected in the tf.contrib.distributions examples?
		</comment>
		<comment id='2' author='mrosenkranz' date='2018-03-26T17:32:36Z'>
		&lt;denchmark-link:https://github.com/jvdillon&gt;@jvdillon&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/langmore&gt;@langmore&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 : Mind taking a look?
		</comment>
		<comment id='3' author='mrosenkranz' date='2018-03-26T18:09:41Z'>
		Thanks for reporting! Ill take a look ASAP.
		</comment>
		<comment id='4' author='mrosenkranz' date='2018-12-05T18:56:39Z'>
		Nagging Assignees &lt;denchmark-link:https://github.com/jvdillon&gt;@jvdillon&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
: It has been 253 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='5' author='mrosenkranz' date='2018-12-06T23:15:55Z'>
		Fixed by &lt;denchmark-link:https://github.com/tensorflow/probability/commit/50f38f3b0184c1a441921fb7826d9c76672ca44f&gt;tensorflow/probability@50f38f3&lt;/denchmark-link&gt;
. Switch over to the bijector in that repository for the fix.
		</comment>
	</comments>
</bug>