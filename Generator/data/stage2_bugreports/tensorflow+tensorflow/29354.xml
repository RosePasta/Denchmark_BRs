<bug id='29354' author='grahamgower' open_date='2019-06-03T09:43:49Z' closed_time='2019-10-09T21:25:39Z'>
	<summary>Suggested KMP_AFFINITY settings harm performance</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes (actually keras, using tensorflow backend)
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7.6
TensorFlow installed from (source or binary): binary (via anaconda)
TensorFlow version (use command below): 1.13.1 (mkl_py37h54b294f_0)
Python version: 3.7.3
CUDA/cuDNN version: N/A


Set  as per the performance guidelines.
&lt;denchmark-link:https://www.tensorflow.org/guide/performance/overview&gt;https://www.tensorflow.org/guide/performance/overview&lt;/denchmark-link&gt;

With OMP_NUM_THREADS=4, I train a model and see the following output on a 44 core (88 hyperthreads) system:
&lt;denchmark-code&gt;OMP: Info #250: KMP_AFFINITY: pid 372422 tid 372422 thread 0 bound to OS proc set 0
OMP: Info #250: KMP_AFFINITY: pid 372420 tid 372420 thread 0 bound to OS proc set 0
OMP: Info #250: KMP_AFFINITY: pid 372423 tid 372423 thread 0 bound to OS proc set 0
OMP: Info #250: KMP_AFFINITY: pid 372424 tid 372424 thread 0 bound to OS proc set 0
OMP: Info #250: KMP_AFFINITY: pid 372425 tid 372425 thread 0 bound to OS proc set 0
OMP: Info #250: KMP_AFFINITY: pid 372426 tid 372426 thread 0 bound to OS proc set 0
OMP: Info #250: KMP_AFFINITY: pid 372427 tid 372427 thread 0 bound to OS proc set 0
OMP: Info #250: KMP_AFFINITY: pid 372421 tid 372421 thread 0 bound to OS proc set 0
OMP: Info #250: KMP_AFFINITY: pid 372414 tid 372414 thread 0 bound to OS proc set 0
OMP: Info #250: KMP_AFFINITY: pid 372415 tid 372415 thread 0 bound to OS proc set 0
OMP: Info #250: KMP_AFFINITY: pid 372416 tid 372416 thread 0 bound to OS proc set 0
OMP: Info #250: KMP_AFFINITY: pid 372417 tid 372417 thread 0 bound to OS proc set 0
OMP: Info #250: KMP_AFFINITY: pid 372418 tid 372418 thread 0 bound to OS proc set 0
OMP: Info #250: KMP_AFFINITY: pid 372419 tid 372419 thread 0 bound to OS proc set 0
OMP: Info #250: KMP_AFFINITY: pid 372429 tid 372429 thread 0 bound to OS proc set 0
OMP: Info #250: KMP_AFFINITY: pid 372413 tid 372413 thread 0 bound to OS proc set 0
OMP: Info #250: KMP_AFFINITY: pid 372378 tid 372396 thread 1 bound to OS proc set 4
OMP: Info #250: KMP_AFFINITY: pid 372378 tid 372460 thread 2 bound to OS proc set 8
OMP: Info #250: KMP_AFFINITY: pid 372378 tid 372461 thread 3 bound to OS proc set 10
OMP: Info #250: KMP_AFFINITY: pid 372378 tid 372462 thread 4 bound to OS proc set 6
&lt;/denchmark-code&gt;

There are a bunch of processes and threads here. Some are worker processes for feeding in data. Only the final 4 are "training" threads, which are pinned appropriately. The other procs/threads are erroneously pinned to a single hardware thread context.
Using KMP_AFFINITY="verbose", I get the following output (and substantially greater performance):
&lt;denchmark-code&gt;OMP: Info #250: KMP_AFFINITY: pid 373414 tid 373414 thread 0 bound to OS proc set 0-87
OMP: Info #250: KMP_AFFINITY: pid 373416 tid 373416 thread 0 bound to OS proc set 0-87
OMP: Info #250: KMP_AFFINITY: pid 373420 tid 373420 thread 0 bound to OS proc set 0-87
OMP: Info #250: KMP_AFFINITY: pid 373425 tid 373425 thread 0 bound to OS proc set 0-87
OMP: Info #250: KMP_AFFINITY: pid 373424 tid 373424 thread 0 bound to OS proc set 0-87
OMP: Info #250: KMP_AFFINITY: pid 373418 tid 373418 thread 0 bound to OS proc set 0-87
OMP: Info #250: KMP_AFFINITY: pid 373422 tid 373422 thread 0 bound to OS proc set 0-87
OMP: Info #250: KMP_AFFINITY: pid 373427 tid 373427 thread 0 bound to OS proc set 0-87
OMP: Info #250: KMP_AFFINITY: pid 373415 tid 373415 thread 0 bound to OS proc set 0-87
OMP: Info #250: KMP_AFFINITY: pid 373432 tid 373432 thread 0 bound to OS proc set 0-87
OMP: Info #250: KMP_AFFINITY: pid 373426 tid 373426 thread 0 bound to OS proc set 0-87
OMP: Info #250: KMP_AFFINITY: pid 373421 tid 373421 thread 0 bound to OS proc set 0-87
OMP: Info #250: KMP_AFFINITY: pid 373423 tid 373423 thread 0 bound to OS proc set 0-87
OMP: Info #250: KMP_AFFINITY: pid 373419 tid 373419 thread 0 bound to OS proc set 0-87
OMP: Info #250: KMP_AFFINITY: pid 373429 tid 373429 thread 0 bound to OS proc set 0-87
OMP: Info #250: KMP_AFFINITY: pid 373417 tid 373417 thread 0 bound to OS proc set 0-87
OMP: Info #250: KMP_AFFINITY: pid 373298 tid 373399 thread 1 bound to OS proc set 0-87
OMP: Info #250: KMP_AFFINITY: pid 373298 tid 373441 thread 2 bound to OS proc set 0-87
OMP: Info #250: KMP_AFFINITY: pid 373298 tid 373442 thread 3 bound to OS proc set 0-87
OMP: Info #250: KMP_AFFINITY: pid 373298 tid 373443 thread 4 bound to OS proc set 0-87
&lt;/denchmark-code&gt;

Describe the expected behavior
Don't pin non-OMP worker processes to a single hardware thread context. Perhaps the documentation could suggest starting with KMP_AFFINITY="disabled", as a baseline?

Other performance related issues in the tracker are likely to have been raised because of this problem. E.g. &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/29008&gt;#29008&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/23238&gt;#23238&lt;/denchmark-link&gt;
 are currently open. There are closed issues that are likely caused by this, but have been closed without proper resolution (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/15320&gt;#15320&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/22212&gt;#22212&lt;/denchmark-link&gt;
, but probably many others). At least suggesting  in such issues might be warranted in the future.
	</description>
	<comments>
		<comment id='1' author='grahamgower' date='2019-06-07T02:50:47Z'>
		Hi @ &lt;denchmark-link:https://github.com/grahamgower&gt;@grahamgower&lt;/denchmark-link&gt;

Could you paste all the KMP information?
when you set
&lt;denchmark-code&gt;KMP_AFFINITY="granularity=fine,verbose,compact,1,0"
OMP_NUM_THREADS=4
&lt;/denchmark-code&gt;

such as
&lt;denchmark-code&gt;OMP: Info #212: KMP_AFFINITY: decoding x2APIC ids.
OMP: Info #210: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info
OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-25,52-77
OMP: Info #156: KMP_AFFINITY: 52 available OS procs
OMP: Info #157: KMP_AFFINITY: Uniform topology
OMP: Info #179: KMP_AFFINITY: 1 packages x 26 cores/pkg x 2 threads/core (26 total cores)
OMP: Info #214: KMP_AFFINITY: OS proc to physical thread map:
OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 52 maps to package 0 core 0 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 53 maps to package 0 core 1 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 2 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 54 maps to package 0 core 2 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 3 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 55 maps to package 0 core 3 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 4 maps to package 0 core 4 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 56 maps to package 0 core 4 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 5 maps to package 0 core 5 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 57 maps to package 0 core 5 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 6 maps to package 0 core 6 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 58 maps to package 0 core 6 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 7 maps to package 0 core 8 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 59 maps to package 0 core 8 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 8 maps to package 0 core 9 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 60 maps to package 0 core 9 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 9 maps to package 0 core 10 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 61 maps to package 0 core 10 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 10 maps to package 0 core 11 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 62 maps to package 0 core 11 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 11 maps to package 0 core 12 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 63 maps to package 0 core 12 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 12 maps to package 0 core 13 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 64 maps to package 0 core 13 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 13 maps to package 0 core 16 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 65 maps to package 0 core 16 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 14 maps to package 0 core 17 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 66 maps to package 0 core 17 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 15 maps to package 0 core 18 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 67 maps to package 0 core 18 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 16 maps to package 0 core 19 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 68 maps to package 0 core 19 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 17 maps to package 0 core 20 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 69 maps to package 0 core 20 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 18 maps to package 0 core 21 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 70 maps to package 0 core 21 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 19 maps to package 0 core 22 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 71 maps to package 0 core 22 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 20 maps to package 0 core 24 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 72 maps to package 0 core 24 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 21 maps to package 0 core 25 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 73 maps to package 0 core 25 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 22 maps to package 0 core 26 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 74 maps to package 0 core 26 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 23 maps to package 0 core 27 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 75 maps to package 0 core 27 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 24 maps to package 0 core 28 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 76 maps to package 0 core 28 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 25 maps to package 0 core 29 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 77 maps to package 0 core 29 thread 1
OMP: Info #250: KMP_AFFINITY: pid 64740 tid 64869 thread 0 bound to OS proc set 0
OMP: Info #250: KMP_AFFINITY: pid 64740 tid 64869 thread 1 bound to OS proc set 1
OMP: Info #250: KMP_AFFINITY: pid 64740 tid 64869 thread 2 bound to OS proc set 2
OMP: Info #250: KMP_AFFINITY: pid 64740 tid 66062 thread 3 bound to OS proc set 3
OMP: Info #250: KMP_AFFINITY: pid 64740 tid 66063 thread 4 bound to OS proc set 4
OMP: Info #250: KMP_AFFINITY: pid 64740 tid 66065 thread 6 bound to OS proc set 6
OMP: Info #250: KMP_AFFINITY: pid 64740 tid 66064 thread 5 bound to OS proc set 5
OMP: Info #250: KMP_AFFINITY: pid 64740 tid 66066 thread 7 bound to OS proc set 7
OMP: Info #250: KMP_AFFINITY: pid 64740 tid 66067 thread 8 bound to OS proc set 8
OMP: Info #250: KMP_AFFINITY: pid 64740 tid 66068 thread 9 bound to OS proc set 9
OMP: Info #250: KMP_AFFINITY: pid 64740 tid 66069 thread 10 bound to OS proc set 10
OMP: Info #250: KMP_AFFINITY: pid 64740 tid 66070 thread 11 bound to OS proc set 11
OMP: Info #250: KMP_AFFINITY: pid 64740 tid 66071 thread 12 bound to OS proc set 12
OMP: Info #250: KMP_AFFINITY: pid 64740 tid 66072 thread 13 bound to OS proc set 13
OMP: Info #250: KMP_AFFINITY: pid 64740 tid 66074 thread 15 bound to OS proc set 15
OMP: Info #250: KMP_AFFINITY: pid 64740 tid 66073 thread 14 bound to OS proc set 14
OMP: Info #250: KMP_AFFINITY: pid 64740 tid 66075 thread 16 bound to OS proc set 16
OMP: Info #250: KMP_AFFINITY: pid 64740 tid 66076 thread 17 bound to OS proc set 17
OMP: Info #250: KMP_AFFINITY: pid 64740 tid 66077 thread 18 bound to OS proc set 18
OMP: Info #250: KMP_AFFINITY: pid 64740 tid 66078 thread 19 bound to OS proc set 19
OMP: Info #250: KMP_AFFINITY: pid 64740 tid 66080 thread 21 bound to OS proc set 21
OMP: Info #250: KMP_AFFINITY: pid 64740 tid 66079 thread 20 bound to OS proc set 20
OMP: Info #250: KMP_AFFINITY: pid 64740 tid 66082 thread 23 bound to OS proc set 23
OMP: Info #250: KMP_AFFINITY: pid 64740 tid 66081 thread 22 bound to OS proc set 22
OMP: Info #250: KMP_AFFINITY: pid 64740 tid 66084 thread 25 bound to OS proc set 25
OMP: Info #250: KMP_AFFINITY: pid 64740 tid 66083 thread 24 bound to OS proc set 24
OMP: Info #250: KMP_AFFINITY: pid 64740 tid 66085 thread 26 bound to OS proc set 52
OMP: Info #250: KMP_AFFINITY: pid 64740 tid 66086 thread 27 bound to OS proc set 53
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='grahamgower' date='2019-06-07T06:36:25Z'>
		This is what I see with
&lt;denchmark-code&gt;NUM_PARALLEL_EXEC_UNITS=4
os.environ['OMP_NUM_THREADS'] = str(NUM_PARALLEL_EXEC_UNITS)
os.environ["KMP_AFFINITY"]= "granularity=fine,verbose,compact,1,0"

...

config = tf.ConfigProto(intra_op_parallelism_threads=NUM_PARALLEL_EXEC_UNITS,
                        inter_op_parallelism_threads=1,
                        allow_soft_placement=True,
                        device_count={'CPU': NUM_PARALLEL_EXEC_UNITS })
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;OMP: Info #212: KMP_AFFINITY: decoding x2APIC ids.
OMP: Info #210: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info
OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-87
OMP: Info #156: KMP_AFFINITY: 88 available OS procs
OMP: Info #157: KMP_AFFINITY: Uniform topology
OMP: Info #179: KMP_AFFINITY: 2 packages x 22 cores/pkg x 2 threads/core (44 total cores)
OMP: Info #214: KMP_AFFINITY: OS proc to physical thread map:
OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 44 maps to package 0 core 0 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 4 maps to package 0 core 1 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 48 maps to package 0 core 1 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 8 maps to package 0 core 2 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 52 maps to package 0 core 2 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 10 maps to package 0 core 3 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 54 maps to package 0 core 3 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 6 maps to package 0 core 4 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 50 maps to package 0 core 4 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 5 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 46 maps to package 0 core 5 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 12 maps to package 0 core 8 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 56 maps to package 0 core 8 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 16 maps to package 0 core 9 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 60 maps to package 0 core 9 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 20 maps to package 0 core 10 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 64 maps to package 0 core 10 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 18 maps to package 0 core 11 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 62 maps to package 0 core 11 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 14 maps to package 0 core 12 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 58 maps to package 0 core 12 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 24 maps to package 0 core 16 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 68 maps to package 0 core 16 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 28 maps to package 0 core 17 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 72 maps to package 0 core 17 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 32 maps to package 0 core 18 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 76 maps to package 0 core 18 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 30 maps to package 0 core 19 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 74 maps to package 0 core 19 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 26 maps to package 0 core 20 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 70 maps to package 0 core 20 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 22 maps to package 0 core 21 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 66 maps to package 0 core 21 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 36 maps to package 0 core 24 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 80 maps to package 0 core 24 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 40 maps to package 0 core 25 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 84 maps to package 0 core 25 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 42 maps to package 0 core 26 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 86 maps to package 0 core 26 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 38 maps to package 0 core 27 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 82 maps to package 0 core 27 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 34 maps to package 0 core 28 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 78 maps to package 0 core 28 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 1 core 0 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 45 maps to package 1 core 0 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 5 maps to package 1 core 1 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 49 maps to package 1 core 1 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 9 maps to package 1 core 2 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 53 maps to package 1 core 2 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 11 maps to package 1 core 3 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 55 maps to package 1 core 3 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 7 maps to package 1 core 4 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 51 maps to package 1 core 4 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 1 core 5 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 47 maps to package 1 core 5 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 13 maps to package 1 core 8 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 57 maps to package 1 core 8 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 17 maps to package 1 core 9 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 61 maps to package 1 core 9 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 21 maps to package 1 core 10 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 65 maps to package 1 core 10 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 19 maps to package 1 core 11 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 63 maps to package 1 core 11 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 15 maps to package 1 core 12 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 59 maps to package 1 core 12 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 25 maps to package 1 core 16 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 69 maps to package 1 core 16 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 29 maps to package 1 core 17 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 73 maps to package 1 core 17 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 33 maps to package 1 core 18 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 77 maps to package 1 core 18 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 31 maps to package 1 core 19 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 75 maps to package 1 core 19 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 27 maps to package 1 core 20 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 71 maps to package 1 core 20 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 23 maps to package 1 core 21 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 67 maps to package 1 core 21 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 37 maps to package 1 core 24 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 81 maps to package 1 core 24 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 41 maps to package 1 core 25 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 85 maps to package 1 core 25 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 43 maps to package 1 core 26 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 87 maps to package 1 core 26 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 39 maps to package 1 core 27 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 83 maps to package 1 core 27 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 35 maps to package 1 core 28 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 79 maps to package 1 core 28 thread 1 
OMP: Info #250: KMP_AFFINITY: pid 208694 tid 208694 thread 0 bound to OS proc set 0
OMP: Info #250: KMP_AFFINITY: pid 210004 tid 210004 thread 0 bound to OS proc set 0
OMP: Info #250: KMP_AFFINITY: pid 210005 tid 210005 thread 0 bound to OS proc set 0
OMP: Info #250: KMP_AFFINITY: pid 210002 tid 210002 thread 0 bound to OS proc set 0
OMP: Info #250: KMP_AFFINITY: pid 210003 tid 210003 thread 0 bound to OS proc set 0
OMP: Info #250: KMP_AFFINITY: pid 209998 tid 209998 thread 0 bound to OS proc set 0
OMP: Info #250: KMP_AFFINITY: pid 210001 tid 210001 thread 0 bound to OS proc set 0
OMP: Info #250: KMP_AFFINITY: pid 209999 tid 209999 thread 0 bound to OS proc set 0
OMP: Info #250: KMP_AFFINITY: pid 210000 tid 210000 thread 0 bound to OS proc set 0
OMP: Info #250: KMP_AFFINITY: pid 208694 tid 209280 thread 1 bound to OS proc set 4
OMP: Info #250: KMP_AFFINITY: pid 208694 tid 220311 thread 2 bound to OS proc set 8
OMP: Info #250: KMP_AFFINITY: pid 208694 tid 220312 thread 3 bound to OS proc set 10
OMP: Info #250: KMP_AFFINITY: pid 208694 tid 220313 thread 4 bound to OS proc set 6
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='grahamgower' date='2019-06-07T10:54:14Z'>
		Hi &lt;denchmark-link:https://github.com/grahamgower&gt;@grahamgower&lt;/denchmark-link&gt;

From your logs, I see lots of different process(different pid numbers).
Such as 208694, 209998-21005.
However, in the major process 208694, different threads are binding to different procs and the binding id means KMP setting works.
I am curious why would you have different pids?
		</comment>
		<comment id='4' author='grahamgower' date='2019-06-12T11:51:02Z'>
		After some more investigation, I have a minimal working example (see below). Additional processes are created by keras because I'm asking for them via fit_generator(). In my case, there appears to be a bad interaction with skimage.transform.resize(). I didn't check the details of what this function does that causes the problem, but I guess it also uses a pool of threads which respects KMP_AFFINITY.
This behaviour can be worked around by setting KMP_AFFINITY to "noverbose" or "none" in each worker process. In my initial report, I suggested KMP_AFFINITY="disabled" - but that is wrong, as that disables multithreading completely. "none" appears to be the default behaviour: to retain multithreading, without any cpu affinity. I humbly suggest that the tensorflow documentation should recommend "none" as the default.
&lt;denchmark-code&gt;#!/usr/bin/env python3

import os

NUM_PARALLEL_EXEC_UNITS = 4
os.environ['OMP_NUM_THREADS'] = str(NUM_PARALLEL_EXEC_UNITS)
os.environ["KMP_AFFINITY"] = "granularity=fine,verbose,compact,1,0"
#os.environ["KMP_AFFINITY"] = "verbose" # no affinity
#os.environ["KMP_AFFINITY"] = "none" # no affinity
#os.environ["KMP_AFFINITY"] = "disabled" # completely disable thread pools

import numpy as np
import keras
import tensorflow as tf
from keras import backend as K
from keras import models, layers
import skimage

config = tf.ConfigProto(intra_op_parallelism_threads=NUM_PARALLEL_EXEC_UNITS,
                        inter_op_parallelism_threads=1,
                        allow_soft_placement=True,
                        device_count={'CPU': NUM_PARALLEL_EXEC_UNITS })
session = tf.Session(config=config)

K.set_session(session)

class MySeq(keras.utils.Sequence):
    def __init__(self, files, dim, batch_size=32):
        self.batch_size = batch_size
        self.files = files
        self.dim = dim

        # Workaround to avoid pinning all worker processes to one cpu.
        #os.environ["KMP_AFFINITY"]= "none"

    def __len__(self):
        return len(self.files) // self.batch_size

    def __getitem__(self, idx):
        files = self.files[idx*self.batch_size:(idx+1)*self.batch_size]

        x = np.empty((self.batch_size, *self.dim), dtype=np.float16)
        y = np.empty(self.batch_size, dtype=np.uint8)

        for i, fn in enumerate(files):
            # load files ... (just make dummy data here)
            y[i] = np.random.randint(2)
            img = (10+100*y[i]) * np.random.rand(128,128)

            # reduce image size
            if True:
                # This uses a thread pool, which is affected by KMP_AFFINITY.
                x[i] = skimage.transform.resize(img, self.dim, preserve_range=True)
            else:
                # This is unaffected by KMP_AFFINITY.
                x[i,:,:,0] = img[:self.dim[0], :self.dim[1]]

        return x, y


dim = (32,32,1)
filelist = [str(x) for x in range(10000)] # placeholder, not real files
itrain = int(0.9 * len(filelist))

train_seq = MySeq(filelist[:itrain], dim)
test_seq = MySeq(filelist[itrain:], dim)

model = models.Sequential([
                layers.Flatten(input_shape=dim),
                layers.Dense(1, activation='softmax'),
                ])

model.summary()

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.fit_generator(generator=train_seq,
                        validation_data=test_seq,
                        epochs=1,
                        use_multiprocessing=True,
                        workers=4)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='grahamgower' date='2019-08-30T19:16:57Z'>
		&lt;denchmark-link:https://github.com/grahamgower&gt;@grahamgower&lt;/denchmark-link&gt;
  Thanks for your suggestions and reporting your findings.
One of the crucial BKMs that allows us to maximize CPU usage is the KMP_AFFINITY params, typically around training and inference compute operations. Setting KMP_AFFINITY to CPU affinity has displayed major benefits on critical workloads in Xeon architecture. Each param that goes in the KMP_AFFINITY has its own significance. Please do take a read at the draft &lt;denchmark-link:https://software.intel.com/en-us/articles/maximize-tensorflow-performance-on-cpu-considerations-and-recommendations-for-inference&gt;here&lt;/denchmark-link&gt;
 that illustrates the benefits of these params . With that said, I'll try to have a section added about your custom use case as part of the document. We appreciate trying TF w/ MKL build
		</comment>
		<comment id='6' author='grahamgower' date='2019-09-03T19:20:55Z'>
		&lt;denchmark-link:https://github.com/grahamgower&gt;@grahamgower&lt;/denchmark-link&gt;
 Please close this issue and reopen if you have similar findings in future and have additional suggesion that needs to be addressed
		</comment>
		<comment id='7' author='grahamgower' date='2019-10-09T21:25:41Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29354&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29354&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>