<bug id='473' author='noisychannel' open_date='2015-12-11T00:08:48Z' closed_time='2016-06-06T18:52:22Z'>
	<summary>Occupies GPU even if it is not compatible</summary>
	<description>
As the title states, even if a GPU is not CUDA compatible, Tensorflow will occupy the GPU and then proceed to ignore it.
Here's a use case :
CUDA_VISIBLE_DEVICES=3 python convolutional.py
....
I tensorflow/core/common_runtime/gpu/gpu_device.cc:669] Ignoring gpu device (device: 0, name: Tesla K10.G2.8GB, pci bus id: 0000:45:00.0) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.
....
From nvidia-dmi
|   3  Tesla K10.G2.8GB    Off  | 0000:45:00.0     Off |                    0 |
| N/A   34C    P0    43W / 117W |     50MiB /  3583MiB |      0%    E. Thread |
.....
|    3     26078    C   python                                          37MiB |
+-----------------------------------------------------------------------------+
	</description>
	<comments>
		<comment id='1' author='noisychannel' date='2016-03-08T01:01:41Z'>
		&lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/noisychannel&gt;@noisychannel&lt;/denchmark-link&gt;
: Looks like this fell through the cracks.  Is it still an issue?
		</comment>
		<comment id='2' author='noisychannel' date='2016-06-06T18:52:22Z'>
		Closing due to lack of response.
		</comment>
	</comments>
</bug>