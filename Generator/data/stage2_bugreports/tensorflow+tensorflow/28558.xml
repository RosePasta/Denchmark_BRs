<bug id='28558' author='sleighsoft' open_date='2019-05-09T16:34:31Z' closed_time='2019-06-19T15:00:16Z'>
	<summary>Tensorflow v2 Variable name uniquification for Keras Layers in eager is inconsistent</summary>
	<description>
Tensorflow v2.0a
When creating e.g. keras models I would assume, that when I run make_generator_model twice in eager mode that the trainable_variable names are identical.
Why would I assume this?
Because the tf.train.Checkpoint and Checkpointable api makes you believe that variables are coupled with their corresponding object/class and uniquification of variables would be no longer necessary. And indeed, this is the case when creating a variable with the same name twice (as can be seen at the end of the code)
What do I get instead?
In the below example the variables of the second make_generator_model() call will be uniquified.
&lt;denchmark-code&gt;# First call
['dense/kernel:0', 'batch_normalization_v2/gamma:0', 'batch_normalization_v2/beta:0', 'conv2d_transpose/kernel:0', 'batch_normalization_v2_1/gamma:0', 'batch_normalization_v2_1/beta:0', 'conv2d_transpose_1/kernel:0', 'batch_normalization_v2_2/gamma:0', 'batch_normalization_v2_2/beta:0', 'conv2d_transpose_2/kernel:0']

# Second
['dense_1/kernel:0', 'batch_normalization_v2_3/gamma:0', 'batch_normalization_v2_3/beta:0', 'conv2d_transpose_3/kernel:0', 'batch_normalization_v2_4/gamma:0', 'batch_normalization_v2_4/beta:0', 'conv2d_transpose_4/kernel:0', 'batch_normalization_v2_5/gamma:0', 'batch_normalization_v2_5/beta:0', 'conv2d_transpose_5/kernel:0']

# Third
['dense/kernel:0', 'batch_normalization_v2/gamma:0', 'batch_normalization_v2/beta:0', 'conv2d_transpose/kernel:0', 'batch_normalization_v2_1/gamma:0', 'batch_normalization_v2_1/beta:0', 'conv2d_transpose_1/kernel:0', 'batch_normalization_v2_2/gamma:0', 'batch_normalization_v2_2/beta:0', 'conv2d_transpose_2/kernel:0']

# Fourth
['dense/kernel:0', 'batch_normalization_v2/gamma:0', 'batch_normalization_v2/beta:0', 'conv2d_transpose/kernel:0', 'batch_normalization_v2_1/gamma:0', 'batch_normalization_v2_1/beta:0', 'conv2d_transpose_1/kernel:0', 'batch_normalization_v2_2/gamma:0', 'batch_normalization_v2_2/beta:0', 'conv2d_transpose_2/kernel:0']

# Manual Creation
&lt;tf.Variable 'test:0' shape=() dtype=int32, numpy=1&gt;
&lt;tf.Variable 'test:0' shape=() dtype=int32, numpy=1&gt;
&lt;/denchmark-code&gt;

import tensorflow as tf
from tensorflow.keras import layers

def make_generator_model():
    model = tf.keras.Sequential()
    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Reshape((7, 7, 256)))
    assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size

    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))
    assert model.output_shape == (None, 7, 7, 128)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    assert model.output_shape == (None, 14, 14, 64)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))
    assert model.output_shape == (None, 28, 28, 1)

    return model


m1 = make_generator_model()
noise = tf.random.normal([1, 100])
generated_image = m1(noise, training=False)
print([v.name for v in m1.trainable_variables])

m2 = make_generator_model()
noise = tf.random.normal([1, 100])
generated_image = m2(noise, training=False)
print([v.name for v in m2.trainable_variables])

with tf.Graph().as_default():
    m1 = make_generator_model()
    noise = tf.random.normal([1, 100])
    generated_image = m1(noise, training=False)
    print([v.name for v in m1.trainable_variables])

with tf.Graph().as_default():
    m2 = make_generator_model()
    noise = tf.random.normal([1, 100])
    generated_image = m2(noise, training=False)
    print([v.name for v in m2.trainable_variables])

a = tf.Variable(1, name='test')
b = tf.Variable(1, name='test')
print(a)
print(b)
	</description>
	<comments>
		<comment id='1' author='sleighsoft' date='2019-05-10T13:00:50Z'>
		&lt;denchmark-link:https://github.com/sleighsoft&gt;@sleighsoft&lt;/denchmark-link&gt;
 : I was able to get the output mentioned above when I tried running the code snippet with TensorFlow 2.0.0-alpha on Colab.
		</comment>
		<comment id='2' author='sleighsoft' date='2019-05-10T15:04:14Z'>
		Ideally I would like to get the same functionality as is currently available when in graph mode.
So when calling make_generator_model() I have the choice to generate a new one "in a new graph so to speak" (the with tf.Graph().as_default() example) or to just use the same weights as in the existing scope (the a, b example).
I found this to be useful when having a training "graph" and an evaluation "graph". Or is this just me thinking the "old (tf1)" way?
		</comment>
		<comment id='3' author='sleighsoft' date='2019-05-10T15:48:43Z'>
		
The "problem" is here: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/engine/base_layer.py#L1372&gt;https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/engine/base_layer.py#L1372&lt;/denchmark-link&gt;

There seems to be a global name space for layers. Which is a little confusing considering tf-2 wants to drop most/all of the global states.
Wouldn't it be better if e.g. tf.keras.Sequential would handle name collisions instead of the layers themselves? And if the user wants, alternative names can be provided to each layer anyway.
		</comment>
		<comment id='4' author='sleighsoft' date='2019-05-15T23:51:05Z'>
		This is intended behavior where most users won't pass in name, while Sequential can do that, functional or subclass cannot.
		</comment>
		<comment id='5' author='sleighsoft' date='2019-05-16T08:26:40Z'>
		I assumed it to be intended. That's why I put incosistent in the name.
Also, why is it possible to scope variables with tf.name_scope but not tf.keras.layers.
This makes working with subclassed tf.keras.Models a nightmare as behavior is not as one would expect.
import tensorflow as tf


dense1 = tf.keras.layers.Dense(10)
var1 = tf.Variable([10])

with tf.name_scope('scoped'):
    dense2 = tf.keras.layers.Dense(15)
    var2 = tf.Variable([15])

dense1(tf.ones([10, 15]))
dense2(tf.ones([10, 20]))

print(dense1.variables[0].name)
print(var1.name)
print(dense2.variables[0].name)
print(var2.name)
This prints
&lt;denchmark-code&gt;dense/kernel:0
Variable:0
dense_1/kernel:0
scoped/Variable:0
&lt;/denchmark-code&gt;

I would expect the dense2 to be scoped/dense/kernel:0
		</comment>
		<comment id='6' author='sleighsoft' date='2019-05-16T14:17:47Z'>
		Sorry to hear that. The issue you're seeing here is because your layer is not called, thus the variable is deferred created. What can be done here is to do extra layer.build((15,).
I admit that this can be surprising behavior, and I can make it eagerly created if that's what you prefer.
		</comment>
		<comment id='7' author='sleighsoft' date='2019-05-16T19:15:03Z'>
		I find it confusing that for the dense2 case the tf.name_scope has no effect. If creating it eagerly solves that problem that's nice.
But: If you would turn it into being eagerly created, will it be consistent when the code is run in graph mode instead?
Having a behavior that stays the same no matter if the code is executed in eager or graph is the best option.
		</comment>
		<comment id='8' author='sleighsoft' date='2019-05-16T20:25:43Z'>
		I'm not sure why this is not graph/eager mode agnostic?
		</comment>
		<comment id='9' author='sleighsoft' date='2019-05-19T18:22:07Z'>
		Updated example
I hope this shows the confusion a little better.
import tensorflow as tf


dense = tf.keras.layers.Dense(10)
var1 = tf.Variable([10])

with tf.name_scope('scoped'):
    dense1 = tf.keras.layers.Dense(15)
    var2 = tf.Variable([15])
    dense2 = tf.keras.layers.Dense(15)
    dense3 = tf.keras.layers.Dense(15)
    dense3.build((15,))

dense(tf.ones([10, 15]))
dense1(tf.ones([10, 20]))

with tf.name_scope('scoped'):
    dense2(tf.ones([10, 20]))

print(dense.variables[0].name)
print(var1.name)
print(dense1.variables[0].name) # Created in tf.name_scope('scoped') &amp; First called in root name_scope
print(var2.name)
print(dense2.variables[0].name) # Created in tf.name_scope('scoped') &amp; First called in tf.name_scope('scoped')
print(dense3.variables[0].name) # Called .build on it within tf.name_scope('scoped')
Prints
&lt;denchmark-code&gt;dense/kernel:0                # Ok
Variable:0                    # Ok
dense_1/kernel:0              # Why is it called dense_1 ?        Expected: scoped/dense/kernel:0
scoped/Variable:0             # Ok
scoped/dense_2/kernel:0       # Why is it called scoped/dense_2 ? Expected: scoped/dense_1/kernel:0
scoped/kernel:0               # Where did the layer naming go?    Expected: scoped/dense_2/kernel:0
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='sleighsoft' date='2019-05-20T15:09:48Z'>
		&lt;denchmark-link:https://github.com/sleighsoft&gt;@sleighsoft&lt;/denchmark-link&gt;
 as mentioned in early reply, Dense(15) does not create variable for you yet. You'd need to either call layer.build(input_shape) or y = layer(x) in order to create the variables.
		</comment>
		<comment id='11' author='sleighsoft' date='2019-05-21T09:20:17Z'>
		&lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;
 I understand what is happening. Though, I do not understand why it has to be that way.
For example, why is dense3 called scoped/kernel:0? Why did keras not add dense in the name, even though I called .build() on it.
I feel like we are talking past each other.
		</comment>
		<comment id='12' author='sleighsoft' date='2019-06-03T07:25:29Z'>
		&lt;denchmark-link:https://github.com/sleighsoft&gt;@sleighsoft&lt;/denchmark-link&gt;
 You should look through this function


		</comment>
		<comment id='13' author='sleighsoft' date='2019-06-03T07:40:47Z'>
		&lt;denchmark-link:https://github.com/llan-ml&gt;@llan-ml&lt;/denchmark-link&gt;
 Can you be a little more specific, please?
		</comment>
		<comment id='14' author='sleighsoft' date='2019-06-03T08:18:45Z'>
		The command layer = Dense(10) only invokes __init__ of the class Dense.
However, the variables for Dense are created in Dense.build 


tensorflow/tensorflow/python/keras/layers/core.py


         Line 998
      in
      e1c98ee






 def build(self, input_shape): 





which is automatically called when we call layer(inputs) within the scope self._name_scope()



tensorflow/tensorflow/python/keras/engine/base_layer.py


        Lines 610 to 614
      in
      e1c98ee






 with graph.as_default(), backend.name_scope(self._name_scope()): 



 # Build layer if applicable (if the `build` method has been 



 # overridden). 



 self._maybe_build(inputs) 



 





As for your example
with tf.name_scope('scoped'):
    dense2(tf.ones([10, 20]))

since all related variables are created during the first call layer(inputs), any subsequent calls do not invoke dense2.build to create new variables and the scope scoped does not change the name of already created variables.
		</comment>
		<comment id='15' author='sleighsoft' date='2019-06-03T09:31:21Z'>
		Thank you for elaborating your answer.
I still fail to see how this resolves the confusing naming.
dense = tf.keras.layers.Dense(15)
dense(tf.ones([10, 20]))

with tf.name_scope('first'):
    denseN = tf.keras.layers.Dense(15)

with tf.name_scope('scoped'):
    denseN(tf.ones([10, 20]))
    
print(dense.variables[0].name)
print(denseN.variables[0].name)
Prints
&lt;denchmark-code&gt;dense/kernel:0
scoped/dense_1/kernel:0
&lt;/denchmark-code&gt;

Expected
&lt;denchmark-code&gt;dense/kernel:0
scoped/dense/kernel:0
&lt;/denchmark-code&gt;

		</comment>
		<comment id='16' author='sleighsoft' date='2019-06-03T11:06:43Z'>
		&lt;denchmark-link:https://github.com/sleighsoft&gt;@sleighsoft&lt;/denchmark-link&gt;
 Use  and .
If you do not explicitly pass the argument name, keras will automatically use the class name and count how many times it has been used, regardless of its parent scopes. It appends _1 to the name of denseN.
Also, if you only pass name="dense" to dense = tf.keras.layers.Dense(15, name="dense"), the output is still what you expect, because the background counter does not count.
		</comment>
		<comment id='17' author='sleighsoft' date='2019-06-04T09:23:26Z'>
		&lt;denchmark-link:https://github.com/llan-ml&gt;@llan-ml&lt;/denchmark-link&gt;
 That does not really help in making the behavior more understandable but it at least offers a "solution".
Thank you for your help :)
Much appreciated!
I still believe this should be reworked but it would break current code.
So if any tensorflower thinks this issue should be closed, do so. Otherwise I'll let it stay open.
		</comment>
		<comment id='18' author='sleighsoft' date='2019-06-18T12:31:43Z'>
		This makes restoring a keras model for evaluation in the same program where the training graph/model exists impossible without overriding the training model.
Like so:
train_model = MyModel()

train_one_epoch(train_model)
checkpoint_path = save_checkpoint(train_model)

eval_model = MyModel()
restore(checkpoint_path, eval_model) # This will fail due to the second call to MyModel() which creates different "unique" layer names.
eval(eval_model)

train_one_epoch(train_model)
		</comment>
		<comment id='19' author='sleighsoft' date='2019-06-18T12:40:35Z'>
		&lt;denchmark-link:https://github.com/llan-ml&gt;@llan-ml&lt;/denchmark-link&gt;
 Your solution should be the default behavior because that resembles the current design principle of tensorflow v2.
		</comment>
		<comment id='20' author='sleighsoft' date='2019-06-18T14:33:48Z'>
		The reason is that when you create the layer, no input shape is passed, variables are not created, so the name scope you put over there does not impact variable naming.
An option is to enforce the input shape and make sure variables are created when layer is created, but users of Sequential model without passing in input shape would not like it.
		</comment>
		<comment id='21' author='sleighsoft' date='2019-06-18T15:02:57Z'>
		I see a global name space as a remnant of TF1, which is confusing to have in an object based, encapsulated, TF2.
I understand your reluctance to change it as it might break a couple of things but it is definitely not understandable/beautiful code as is.
This has nothing to do with when the layer gets created, even though we always seem to circle back to that argument.
		</comment>
		<comment id='22' author='sleighsoft' date='2019-06-18T16:05:08Z'>
		&lt;denchmark-link:https://github.com/sleighsoft&gt;@sleighsoft&lt;/denchmark-link&gt;
 Re "This has nothing to do with when the layer gets created" -- not true. See this:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/base_layer.py#L1615-L1621&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/base_layer.py#L1615-L1621&lt;/denchmark-link&gt;

This is called when layer is created. If you don't pass in "name", something will be created for you and be used for the lifetime of this layer.
		</comment>
		<comment id='23' author='sleighsoft' date='2019-06-19T08:55:51Z'>
		So the question becomes the following.
Should calling, for example tf.keras.layers.Dense(), twice in the same scope give you a layer that references the same underlying variables or not.
I can understand both options. But then it should be more clear in the documentation what the implication of not specifying a name in the constructor does.
Because this does not tell the whole story.
&lt;denchmark-link:https://user-images.githubusercontent.com/9438971/59751428-c7f98b00-9280-11e9-96ae-dfde065bd2c8.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='24' author='sleighsoft' date='2019-06-19T15:00:16Z'>
		To answer your question, it should not. Here's the general conclusion:

name_scope isn't object oriented. This might go away in the long term.
layer names are made unique today by global state, which is not idea as well. Many of these global and uniqueness has mainly to do with two things: a) that variable name cannot duplicate, b) that Tensorboard requires naming to group things.

So I would suggest working with tf.keras.layers.Dense(name='your own layer name'), not name scope.
		</comment>
		<comment id='25' author='sleighsoft' date='2019-06-19T15:00:18Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=28558&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=28558&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='26' author='sleighsoft' date='2019-06-20T08:54:36Z'>
		&lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;
 Will do so. Thank you for explaining everything :)
		</comment>
	</comments>
</bug>