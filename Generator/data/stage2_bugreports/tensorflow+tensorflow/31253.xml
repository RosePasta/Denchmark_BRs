<bug id='31253' author='talipini' open_date='2019-08-02T00:30:16Z' closed_time='2019-08-15T16:25:38Z'>
	<summary>Using class_weights in fit_generator causes continuous increase in CPU memory usage until depletion. OOM</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2 LTS
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not Relevant.
TensorFlow installed from (source or binary):
TensorFlow version (use command below): tf-nightly-gpu           1.15.0.dev20190728
Python version: Python 3.7.4
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: release 10.0, V10.0.130
NVIDIA-SMI 418.43       Driver Version: 418.43       CUDA Version: 10.1
GPU model and memory:
GeForce RTX 2080Ti 11GB

Describe the current behavior
When using class_weights in fit_generator causes the training process to continuously consume more and more CPU RAM until depletion. There is a stepped increased in memory usage after each epoch. See below for the reproducible example.  To keep the reproducible example small, I decreased the size of the dataset and batch size, which shows the trend of increasing memory. While training with my actual data, it depletes the full 128GB RAM by 70 EPOCS.
In the code below, if you comment out the class weights, the program trains without depleting memory.
Describe the expected behavior
Not leak memory.
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
&lt;denchmark-code&gt;import tensorflow as tf
tf.enable_eager_execution()
import numpy as np
 
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import CuDNNLSTM, Dense
from tensorflow.keras.optimizers import Adadelta


feature_count = 25
batch_size = 16
look_back = 5
target_groups = 10

def random_data_generator( ):
    x_data_size =(batch_size, look_back, feature_count) # batches, lookback, features
    x_data = np.random.uniform(low=-1.0, high=5, size=x_data_size)
 
    y_data_size = (batch_size, target_groups)
    Y_data = np.random.randint(low=1, high=21, size=y_data_size)
    
    return x_data, Y_data
 
def get_simple_Dataset_generator():        
    while True:
        yield random_data_generator()

def build_model():
    model = Sequential()
    model.add(CuDNNLSTM(feature_count,
                    batch_input_shape=(batch_size,look_back, feature_count),
                    stateful=False))  
    model.add(Dense(target_groups, activation='softmax'))
    optimizer = Adadelta(learning_rate=1.0, epsilon=None) 
    model.compile(loss='categorical_crossentropy', optimizer=optimizer) 
    return model


def run_training():
   
    model = build_model()
    train_generator = get_simple_Dataset_generator()
    validation_generator = get_simple_Dataset_generator()
    class_weights = {0:2, 1:8, 2:1, 3:4, 4:8, 5:35, 6:30, 7:4, 8:5, 9:3}

    model.fit_generator(generator = train_generator,
            steps_per_epoch=1,
            epochs=1000,            
            verbose=2,
            validation_data=validation_generator,
            validation_steps=20,
            max_queue_size = 10,
            workers = 0, 
            use_multiprocessing = False,
            class_weight = class_weights
            )

if __name__ == '__main__': 
    run_training()
 
&lt;/denchmark-code&gt;

Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
&lt;denchmark-h:h1&gt;Memory Usage When class_weights are used&lt;/denchmark-h&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/46456896/62335750-aea84900-b492-11e9-9b23-72b29c6f2499.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-h:h1&gt;Memory Without class_weights&lt;/denchmark-h&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/46456896/62335751-aea84900-b492-11e9-9f2a-ef9c296850ee.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='talipini' date='2019-08-02T11:55:12Z'>
		Was able to reproduce the issue on Colab with Tf-nightly-gpu version. Please have a look at Colab &lt;denchmark-link:https://colab.research.google.com/drive/1udDjMNZ0AkbApQLy43fvQpXTvIih4Qlf&gt;link&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='talipini' date='2019-08-06T14:21:35Z'>
		Any workarounds or updates this? Thanks
		</comment>
		<comment id='3' author='talipini' date='2019-08-14T00:13:49Z'>
		I'm not able to reproduce this - can you update to the latest tf-nightly-gpu package and try again? Thanks!
		</comment>
		<comment id='4' author='talipini' date='2019-08-15T16:25:38Z'>
		&lt;denchmark-link:https://github.com/talipini&gt;@talipini&lt;/denchmark-link&gt;
 This was resolved in the 
Please check the &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/20aeb317e65ecb8a0ec3e15812c41b10/tf31253_keras_memory_issue.ipynb&gt;gist here&lt;/denchmark-link&gt;
.
I am closing the issue here. Please feel free to open the issue if it persists again. Thanks!
		</comment>
		<comment id='5' author='talipini' date='2019-08-15T16:25:39Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=31253&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=31253&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='talipini' date='2019-08-18T13:03:50Z'>
		Looks like new nightly build has fixed this issue. But training time is significantly longer (about 30%) in my case between 1.15.0.dev20190728 and 1.15.0.dev20190813. Same codebase takes 30% longer to train in the newer nightly build but it definitely seems to solve the memory issue though. I will try to create a reproducible example and open an new issue. Thanks for checking into this.
		</comment>
		<comment id='7' author='talipini' date='2019-08-18T15:03:56Z'>
		&lt;denchmark-link:https://github.com/talipini&gt;@talipini&lt;/denchmark-link&gt;
 Sure. Thanks for your support.
		</comment>
		<comment id='8' author='talipini' date='2019-10-06T19:43:20Z'>
		I'm still having the same memory issue when using class_weights with Keras model.fit_generator. Running the stable version of TF 2.0.
		</comment>
		<comment id='9' author='talipini' date='2019-10-07T17:09:02Z'>
		&lt;denchmark-link:https://github.com/TKassis&gt;@TKassis&lt;/denchmark-link&gt;
 Please open a new issue with details about your issue, platform details and a standalone code to reproduce the issue. Thanks!
		</comment>
		<comment id='10' author='talipini' date='2019-10-14T17:56:22Z'>
		
@TKassis Please open a new issue with details about your issue, platform details and a standalone code to reproduce the issue. Thanks!

Thanks, it seems multiple people have reported the issue already.
		</comment>
		<comment id='11' author='talipini' date='2020-02-29T18:43:14Z'>
		&lt;denchmark-link:https://github.com/TKassis&gt;@TKassis&lt;/denchmark-link&gt;
 do you have a link to the other issues? I am not fining them. Thank you!
		</comment>
		<comment id='12' author='talipini' date='2020-02-29T18:45:22Z'>
		&lt;denchmark-link:https://github.com/talipini&gt;@talipini&lt;/denchmark-link&gt;
 Thanks for the issue!
Can you please try this again with the latest nightly (pip install -U tf-nightly) an report back? I believe this issue should be fixed in the latest nightly
		</comment>
	</comments>
</bug>