<bug id='34381' author='bmiselis' open_date='2019-11-18T17:18:31Z' closed_time='2019-11-19T20:32:38Z'>
	<summary>Segmentation fault (core dumped) tf 1.12.0-gpu-py3 image (works with cuda:9.0-cudnn7-devel-ubuntu16.04 and manual tf install)</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no (code from https://keras.io/examples/mnist_cnn/)
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu Pop!_OS 19.10
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): docker image 1.12.0-gpu-py
TensorFlow version (use command below): 1.12.0
Python version: 3.5.2
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: from Tensorflow image
GPU model and memory: GeForce RTX 2070 | 8GB

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
When running the aforementioned example inside the Tensorflow 1.12.0-gpu-py3 container I get the following output:
docker run -it tensorflow/tensorflow:1.12.0-gpu-py3 bash
root@da57df222465:/notebooks# cd /export/home/bartosz.miselis/code/repos/cicd/
root@da57df222465:/export/home/bartosz.miselis/code/repos/cicd# python mnist.py
2019-11-18 16:57:56.234513: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-11-18 16:57:56.335561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-18 16:57:56.335832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.71
pciBusID: 0000:01:00.0
totalMemory: 7.79GiB freeMemory: 7.34GiB
2019-11-18 16:57:56.335847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-11-18 16:57:56.519252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-11-18 16:57:56.519282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0
2019-11-18 16:57:56.519287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N
2019-11-18 16:57:56.519367: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7056 MB memory) -&gt; physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
x_train shape: (60000, 28, 28, 1)
60000 train samples
10000 test samples
Train on 60000 samples, validate on 10000 samples
Epoch 1/12
Segmentation fault (core dumped)
Describe the expected behavior
The training should run smoothly.
Code to reproduce the issue
As stated above, the code is directly accessible through the URL.
Other info / logs
nvidia-smi output:
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 440.31       Driver Version: 440.31       CUDA Version: 10.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce RTX 2070    Off  | 00000000:01:00.0 Off |                  N/A |
|  0%   41C    P8    19W / 185W |    363MiB /  7981MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
Additionally, running the code inside cuda:9.0-cudnn7-devel-ubuntu16.04 container with manual tensorflow-gpu==1.12.0 package via pip makes it run properly.
Even more strangely, the code runs perfectly fine when using 1.12.3-gpu-py3 container. I'm limited to using 1.12.0 currently (strict project requirements).
	</description>
	<comments>
		<comment id='1' author='bmiselis' date='2019-11-19T20:32:38Z'>
		Thanks for your report. I'm afraid this falls outside of our official support matrix, especially because the issue appears to be fixed in the 1.12.3 container.
		</comment>
		<comment id='2' author='bmiselis' date='2019-11-19T20:32:40Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34381&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34381&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='bmiselis' date='2020-10-29T08:54:57Z'>
		I met  too with  and docker image . When I move to docker image , it seems failed to use the GPU ( usage in the output of ).
But in another machine, with  and , it works well.
(Both running the SAME codes, which can be found &lt;denchmark-link:https://github.com/lelan-li/SSAH&gt;here&lt;/denchmark-link&gt;
)
		</comment>
		<comment id='4' author='bmiselis' date='2020-11-27T18:52:25Z'>
		If this issue only occurs in the official 1.12.0 TF image, then it would be nice to have updated to fix this issue... I don't think this is happening... I managed to make it work using the 1.14.0-gpu-py3 image. Thankfully I can still go back to running the final model that was trained using the previous version, since the device that I'm using is limited to the cuda version 9, and so I have to use this older TF...
		</comment>
	</comments>
</bug>