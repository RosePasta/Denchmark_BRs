<bug id='20346' author='benbotto' open_date='2018-06-27T15:24:30Z' closed_time='2019-02-05T19:09:02Z'>
	<summary>Tensorflow Program Hangs on pthread_cond_wait</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.4 LTS, xenial
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below):  tensorflow-gpu (1.8.0)
Python version: Python 3.5.2
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 9.0.176/7.1.3.16
GPU model and memory: GeForce GTX 850M with 2GB of memory
Exact command to reproduce:

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

I have a program using Tensorflow and it consistently hangs after a week or so, arbitrarily.  The GPU utilization goes to 0%, and the program stops training.  A stack trace shows that the program is stuck waiting on pthread_cond_wait.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

I don't know that the source code will be of much use since it's not a concise example that reproduces the problem, but the code is available here: &lt;denchmark-link:https://github.com/benbotto/bsy-dqn-atari/tree/breakout-best&gt;https://github.com/benbotto/bsy-dqn-atari/tree/breakout-best&lt;/denchmark-link&gt;
  My code is single threaded.
When last the program hung I took a stack trace.  That's available here: &lt;denchmark-link:https://pastebin.com/LiPhz2CE&gt;https://pastebin.com/LiPhz2CE&lt;/denchmark-link&gt;

This looks similar to this report: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/1947&gt;#1947&lt;/denchmark-link&gt;

Let me know if more information is needed.
	</description>
	<comments>
		<comment id='1' author='benbotto' date='2018-07-19T03:51:37Z'>
		the same problem, no solution now.
		</comment>
		<comment id='2' author='benbotto' date='2018-10-13T03:50:26Z'>
		Looked at the pastebin and nothing obvious jumped out to me. &lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;
 any ideas?
If the problem happened again, could you paste a newer version of the stack trace?
		</comment>
		<comment id='3' author='benbotto' date='2018-10-16T20:27:52Z'>
		The hang happens reliably on one of my machines.  I have another machine that's running the same software on the same version of Tensorflow--albeit different hardware--and I haven't had any hangs on that one.  The hang is sporadic, though, and often takes a week or more.  If I really need to I can take another stack trace, but that will take some time.
		</comment>
		<comment id='4' author='benbotto' date='2018-10-31T22:39:11Z'>
		Looking at this again, and there's no obvious answer in the stack trace, but I have a few suggestions:

Can you try creating your tf.Session with config=tf.ConfigProto(inter_op_parallelism_threads=1, intra_op_parallelism_threads=1)? This shouldn't fix anything... if anything it makes it more likely that buggy code will deadlock sooner... but it should give a more compact thread dump to analyze.
Are you using tf.py_func() in your code? It looks like "Thread 1" (which appears twice in the pastebin link) is blocked in some NumPy code, in what appears to be OpenBLAS (exec_blas_async_wait()). It's possible that if your other machine has a different version of NumPy, then it might have different behavior for this method.
Related to (2), experimenting with the OMP_NUM_THREADS variable (e.g. setting it to 1?) might reveal something about the behavior.

If you can reproduce the hang and capture another (ideally smaller) stack trace, that would be useful!
		</comment>
		<comment id='5' author='benbotto' date='2018-10-31T23:04:58Z'>
		It could be related to this issue in OpenBLAS: &lt;denchmark-link:https://github.com/xianyi/OpenBLAS/issues/660&gt;xianyi/OpenBLAS#660&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/xianyi/OpenBLAS/issues/660#issuecomment-277033954&gt;This comment&lt;/denchmark-link&gt;
 suggests that setting the environment variable  is a possible workaround, but may slow things down.
		</comment>
		<comment id='6' author='benbotto' date='2018-11-01T17:55:20Z'>
		Thanks for the response, &lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;
.

Right now I'm not actually creating a session explicitly as I'm using tf.keras which evidently creates a session behind the scenes.  It looks like Keras has the option of using a manually defined session (tf.keras.backend.set_session), so I'll try your config recommendation and see if I can get a better stack dump.
I'm not using tf.py_func anywhere, no.  As far as I know both machines are using identical version of everything (Tensorflow, Keras, numpy, and all other dependencies).  I'm using virtualenv, and I set up both machines simultaneously.  I'll double check the np version, though.

I wish I knew how to set up a smaller test case that would reproduce the issue--as a programmer I certainly understand how important that is--but I'm not sure how in this instance.  I don't think my code is doing anything out of the ordinary: I'm generating images, doing some basic processing on those (crop and grayscale and such), then feeding them through a rather trivial network.  I'm not directly using any threading in my code or anything else that would cause a deadlock.  All that is to say I wish I could be of more help (and furthermore I wish the code would run reliably on all my machines!).
Anyway, I'll try again with the session configured as recommended and see if I can get another stack trace.  It may take a few weeks to lock up again.
		</comment>
		<comment id='7' author='benbotto' date='2018-11-16T09:28:38Z'>
		&lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;
 The similar problem occurs at the end of an iteration when using  api, while the old  based data api is fine.
TF version: 1.12 build from source
Linux kernel: 3.10.0
Distributed running on yarn, only cpu. Every time, several random nodes will hang forever.
&lt;denchmark-code&gt;Thread 217 (Thread 0x7fce01b3e700 (LWP 100195)):
#0  0x00007fce7ea14995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007fcdf8b3082c in std::condition_variable::wait(std::unique_lock&lt;std::mutex&gt;&amp;) () from /lib64/libstdc++.so.6
#2  0x00007fce5be86d30 in Eigen::NonBlockingThreadPoolTempl&lt;tensorflow::thread::EigenEnvironment&gt;::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /data9/hadoop/yarn/nm-local-dir/usercache/hadoop-user/appcache/application_1538038835235_2238029/container_e10_1538038835235_2238029_01_000361/tf.tgz/tf/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#3  0x00007fce5be87784 in Eigen::NonBlockingThreadPoolTempl&lt;tensorflow::thread::EigenEnvironment&gt;::WorkerLoop(int) () from /data9/hadoop/yarn/nm-local-dir/usercache/hadoop-user/appcache/application_1538038835235_2238029/container_e10_1538038835235_2238029_01_000361/tf.tgz/tf/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#4  0x00007fce5be86482 in std::_Function_handler&lt;void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function&lt;void ()&gt;)::{lambda()#1}&gt;::_M_invoke(std::_Any_data const&amp;) () from /data9/hadoop/yarn/nm-local-dir/usercache/hadoop-user/appcache/application_1538038835235_2238029/container_e10_1538038835235_2238029_01_000361/tf.tar.gz/tf/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#5  0x00007fcdf8b34070 in ?? () from /lib64/libstdc++.so.6
#6  0x00007fce7ea10e25 in start_thread () from /lib64/libpthread.so.0
#7  0x00007fce7e031bad in clone () from /lib64/libc.so.6
Thread 3 (Thread 0x7fc99ffff700 (LWP 104238)):
#0  0x00007fce7ea14995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007fcd7b5aa187 in gpr_cv_wait () from /data9/hadoop/yarn/nm-local-dir/usercache/hadoop-user/appcache/application_1538038835235_2238029/container_e10_1538038835235_2238029_01_000361/tf.tgz/tf/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00007fcd7b58680c in executor_thread(void*) () from /data9/hadoop/yarn/nm-local-dir/usercache/hadoop-user/appcache/application_1538038835235_2238029/container_e10_1538038835235_2238029_01_000361/tf.tgz/tf/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00007fcd7b5aae8c in grpc_core::(anonymous namespace)::ThreadInternalsPosix::ThreadInternalsPosix(char const*, void (*)(void*), void*, bool*)::{lambda(void*)#1}::_FUN(void*) () from /data9/hadoop/yarn/nm-local-dir/usercache/hadoop-user/appcache/application_1538038835235_2238029/container_e10_1538038835235_2238029_01_000361/tf.tgz/tf/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00007fce7ea10e25 in start_thread () from /lib64/libpthread.so.0
#5  0x00007fce7e031bad in clone () from /lib64/libc.so.6
Thread 2 (Thread 0x7fc99f7fe700 (LWP 119076)):
#0  0x00007fce7e02bec9 in syscall () from /lib64/libc.so.6
#1  0x00007fcd7b4625d4 in nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) () from /data9/hadoop/yarn/nm-local-dir/usercache/hadoop-user/appcache/application_1538038835235_2238029/container_e10_1538038835235_2238029_01_000361/tf.tgz/tf/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00007fcd7b461db1 in nsync::nsync_sem_wait_with_cancel_(nsync::waiter*, timespec, nsync::nsync_note_s_*) () from /data9/hadoop/yarn/nm-local-dir/usercache/hadoop-user/appcache/application_1538038835235_2238029/container_e10_1538038835235_2238029_01_000361tf.tgz/tf/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00007fcd7b45f2e4 in nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) () from /data9/hadoop/yarn/nm-local-dir/usercache/hadoop-user/appcache/application_1538038835235_2238029/container_e10_1538038835235_2238029_01_000361/tf.tgz/tf/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00007fcd7b45f805 in nsync::nsync_cv_wait_with_deadline(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*, timespec, nsync::nsync_note_s_*) () from /data9/hadoop/yarn/nm-local-dir/usercache/hadoop-user/appcache/application_1538038835235_2238029/container_e10_1538038835235_2238029_01_000361/tf.tgz/tf/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00007fcd78af538b in tensorflow::BlockingCounter::Wait() () from /data9/hadoop/yarn/nm-local-dir/usercache/hadoop-user/appcache/application_1538038835235_2238029/container_e10_1538038835235_2238029_01_000361/tf.tgz/tf/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#6  0x00007fcd78b02a66 in tensorflow::Status tensorflow::MasterSession::ReffedClientGraph::RunPartitionsHelper&lt;std::vector&lt;std::string, std::allocator&lt;std::string&gt; &gt;, tensorflow::RunStepRequestWrapper, tensorflow::MutableRunStepResponseWrapper&gt;(std::unordered_map&lt;absl::string_view, unsigned long, tensorflow::hash&lt;absl::string_view, void&gt;, std::equal_to&lt;absl::string_view&gt;, std::allocator&lt;std::pair&lt;absl::string_view const, unsigned long&gt; &gt; &gt; const&amp;, std::vector&lt;std::string, std::allocator&lt;std::string&gt; &gt; const&amp;, tensorflow::MasterEnv const*, long long, long long, tensorflow::MasterSession::PerStepState*, tensorflow::CallOptions*, tensorflow::RunStepRequestWrapper const&amp;, tensorflow::MutableRunStepResponseWrapper*, tensorflow::CancellationManager*, bool) () from /data9/hadoop/yarn/nm-local-dir/usercache/hadoop-user/appcache/application_1538038835235_2238029/container_e10_1538038835235_2238029_01_000361/tf.tgz/tf/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#7  0x00007fcd78b03a5b in tensorflow::MasterSession::ReffedClientGraph::RunPartitions(tensorflow::MasterEnv const*, long long, long long, tensorflow::MasterSession::PerStepState*, tensorflow::CallOptions*, tensorflow::RunStepRequestWrapper const&amp;, tensorflow::MutableRunStepResponseWrapper*, tensorflow::CancellationManager*, bool) () from /data9/hadoop/yarn/nm-local-dir/usercache/hadoop-user/appcache/application_1538038835235_2238029/container_e10_1538038835235_2238029_01_000361/tf.tgz/tf/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#8  0x00007fcd78b0c530 in tensorflow::MasterSession::DoRunWithLocalExecution(tensorflow::CallOptions*, tensorflow::RunStepRequestWrapper const&amp;, tensorflow::MutableRunStepResponseWrapper*) () from /data9/hadoop/yarn/nm-local-dir/usercache/hadoop-user/appcache/application_1538038835235_2238029/container_e10_1538038835235_2238029_01_000361/tf.tgz/tf/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#9  0x00007fcd78b0d93e in tensorflow::MasterSession::Run(tensorflow::CallOptions*, tensorflow::RunStepRequestWrapper const&amp;, tensorflow::MutableRunStepResponseWrapper*) () from /data9/hadoop/yarn/nm-local-dir/usercache/hadoop-user/appcache/application_1538038835235_2238029/container_e10_1538038835235_2238029_01_000361/tf.tgz/tf/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#10 0x00007fcd78ae88ec in std::_Function_handler&lt;void (), tensorflow::Master::RunStep(tensorflow::CallOptions*, tensorflow::RunStepRequestWrapper const*, tensorflow::MutableRunStepResponseWrapper*, std::function&lt;void (tensorflow::Status const&amp;)&gt;)::{lambda()#1}&gt;::_M_invoke(std::_Any_data const&amp;) () from /data9/hadoop/yarn/nm-local-dir/usercache/hadoop-user/appcache/application_1538038835235_2238029/container_e10_1538038835235_2238029_01_000361/tf.tgz/tf/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#11 0x00007fcdf8b34070 in ?? () from /lib64/libstdc++.so.6
#12 0x00007fce7ea10e25 in start_thread () from /lib64/libpthread.so.0
#13 0x00007fce7e031bad in clone () from /lib64/libc.so.6
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='benbotto' date='2018-11-16T15:19:28Z'>
		&lt;denchmark-link:https://github.com/formath&gt;@formath&lt;/denchmark-link&gt;
 That sounds like it could be a different problem. However, there isn't enough information in that one stack to diagnose the cause: it just tells us that one call to  is hanging somewhere. Can you please open a new issue with a minimal reproducible example and/or a full set of thread stacks? Also, to remove a potential source of complexity, please try to reproduce it using  instead of  (although if it only reproduces in an RPC-based session, then that would tell us something useful).
		</comment>
		<comment id='9' author='benbotto' date='2019-02-05T19:09:15Z'>
		We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!
		</comment>
	</comments>
</bug>