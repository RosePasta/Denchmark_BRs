<bug id='35878' author='sarthfrey-db' open_date='2020-01-14T21:36:38Z' closed_time='2020-02-19T00:57:28Z'>
	<summary>MultiWorkerMirroredStrategy Keras Example Hangs</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.0.0
Python version: 3.7.3
Bazel version (if compiling from source): n/a
GCC/Compiler version (if compiling from source): n/a
CUDA/cuDNN version: 10.0
GPU model and memory: Tesla K80, 1 GPU per worker, 2 workers

Describe the current behavior
When I run the distributed training example for the MultiWorkerMirroredStrategy with Keras documented &lt;denchmark-link:https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras&gt;here&lt;/denchmark-link&gt;
, approximately half the time the training will be successful and otherwise the workers will hang after the first epoch.
Describe the expected behavior
The training should successfully complete every time with no hanging.
Code to reproduce the issue
import os, json

os.environ['TF_CONFIG'] = json.dumps({'cluster': {'worker': ['X.X.X.X:2000', 'X.X.X.X:2000']}, 'task': {'type': 'worker', 'index': 0}})
import tensorflow_datasets as tfds
import tensorflow as tf
tf.config.optimizer.set_jit(True)
tfds.disable_progress_bar()
strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(communication=tf.distribute.experimental.CollectiveCommunication.NCCL) # NCCL vs RING
print('Number of devices: {}'.format(strategy.num_replicas_in_sync))
BUFFER_SIZE = 10000
BATCH_SIZE = 64
NUM_WORKERS = 2

def make_datasets_unbatched():
  # Scaling MNIST data from (0, 255] to (0., 1.]
  def scale(image, label):
    image = tf.cast(image, tf.float32)
    image /= 255
    return image, label
  datasets, info = tfds.load(name='mnist',
                            with_info=True,
                            as_supervised=True)
  return datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE)

train_datasets = make_datasets_unbatched().batch(BATCH_SIZE)
def build_and_compile_cnn_model():
  model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),
    tf.keras.layers.MaxPooling2D(),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
  ])
  model.compile(
    loss=tf.keras.losses.sparse_categorical_crossentropy,
    optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
    metrics=['accuracy'])
  return model
GLOBAL_BATCH_SIZE = 64 * NUM_WORKERS
with strategy.scope():
  train_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE)
  multi_worker_model = build_and_compile_cnn_model()
multi_worker_model.fit(x=train_datasets, epochs=3, steps_per_epoch=5)
Other info / logs
&lt;denchmark-code&gt;2020-01-14 20:53:41.329726: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-01-14 20:53:41.393549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 20:53:41.394307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:1e.0
2020-01-14 20:53:41.394552: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-01-14 20:53:41.396280: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-01-14 20:53:41.397495: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2020-01-14 20:53:41.397794: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2020-01-14 20:53:41.399430: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2020-01-14 20:53:41.400687: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2020-01-14 20:53:41.404610: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-01-14 20:53:41.404722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 20:53:41.405478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 20:53:41.406161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-01-14 20:53:41.406769: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-14 20:53:41.414319: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300070000 Hz
2020-01-14 20:53:41.414709: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56414dcdbd50 executing computations on platform Host. Devices:
2020-01-14 20:53:41.414747: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2020-01-14 20:53:41.722508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 20:53:41.723344: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56414dcf9870 executing computations on platform CUDA. Devices:
2020-01-14 20:53:41.723406: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7
2020-01-14 20:53:41.723661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 20:53:41.724362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:1e.0
2020-01-14 20:53:41.724432: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-01-14 20:53:41.724476: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-01-14 20:53:41.724515: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2020-01-14 20:53:41.724558: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2020-01-14 20:53:41.724594: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2020-01-14 20:53:41.724633: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2020-01-14 20:53:41.724670: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-01-14 20:53:41.724757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 20:53:41.725490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 20:53:41.726167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-01-14 20:53:41.726224: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-01-14 20:53:41.727650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-14 20:53:41.727681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0
2020-01-14 20:53:41.727699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N
2020-01-14 20:53:41.728328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 20:53:41.729081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 20:53:41.729792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -&gt; physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)
2020-01-14 20:53:41.731085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 20:53:41.731843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:1e.0
2020-01-14 20:53:41.731906: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-01-14 20:53:41.731950: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-01-14 20:53:41.731993: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2020-01-14 20:53:41.732036: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2020-01-14 20:53:41.732078: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2020-01-14 20:53:41.732119: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2020-01-14 20:53:41.732161: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-01-14 20:53:41.732267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 20:53:41.733006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 20:53:41.733681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-01-14 20:53:41.733719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-14 20:53:41.733746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0
2020-01-14 20:53:41.733768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N
2020-01-14 20:53:41.734378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 20:53:41.735121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 20:53:41.735829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:worker/replica:0/task:1/device:GPU:0 with 10805 MB memory) -&gt; physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)
D0114 20:53:41.736038696   13433 log.cc:95]                  Warning: insecure environment read function 'getenv' used
D0114 20:53:41.736065073   13433 env_linux.cc:71]            Warning: insecure environment read function 'getenv' used
D0114 20:53:41.736085632   13433 env_linux.cc:71]            Warning: insecure environment read function 'getenv' used
D0114 20:53:41.736249487   13433 env_linux.cc:71]            Warning: insecure environment read function 'getenv' used
D0114 20:53:41.736271507   13433 is_epollexclusive_available.cc:86] epoll_ctl with EPOLLEXCLUSIVE | EPOLLONESHOT succeeded. This is evidence of no EPOLLEXCLUSIVE support. Not using epollex polling engine.
I0114 20:53:41.736291557   13433 ev_epollex_linux.cc:1633]   Skipping epollex because it is not supported.
I0114 20:53:41.736308984   13433 ev_epoll1_linux.cc:116]     grpc epoll fd: 22
D0114 20:53:41.736329223   13433 ev_posix.cc:170]            Using polling engine: epoll1
D0114 20:53:41.736356694   13433 env_linux.cc:71]            Warning: insecure environment read function 'getenv' used
D0114 20:53:41.736392859   13433 env_linux.cc:71]            Warning: insecure environment read function 'getenv' used
D0114 20:53:41.736408436   13433 dns_resolver.cc:334]        Using native dns resolver
D0114 20:53:41.736428850   13433 env_linux.cc:71]            Warning: insecure environment read function 'getenv' used
E0114 20:53:41.737641232   13433 socket_utils_common_posix.cc:198] check for SO_REUSEPORT: {"created":"@1579035221.737631945","description":"SO_REUSEPORT unavailable on compiling system","file":"external/grpc/src/core/lib/iomgr/socket_utils_common_posix.cc","file_line":166}
2020-01-14 20:53:41.737822: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:258] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; X.X.X.X:2000, 1 -&gt; localhost:2000}
2020-01-14 20:53:41.738932: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:2000
Number of devices: 2
D0114 20:53:43.049644334   13552 env_linux.cc:71]            Warning: insecure environment read function 'getenv' used
D0114 20:53:43.049683824   13552 env_linux.cc:71]            Warning: insecure environment read function 'getenv' used
D0114 20:53:43.049689625   13552 env_linux.cc:71]            Warning: insecure environment read function 'getenv' used
D0114 20:53:43.049770214   13552 dns_resolver.cc:275]        Start resolving.
I0114 20:53:43.050377745   13518 subchannel.cc:1025]         New connected subchannel at 0x7f2a04006d60 for subchannel 0x7f2a18008070
WARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an "evaluator" task exists in the cluster.
WARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an "evaluator" task exists in the cluster.
WARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.
WARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.
WARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.
WARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.
Train for 5 steps
Epoch 1/3
2020-01-14 20:53:46.911991: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-01-14 20:53:48.937723: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
&lt;/denchmark-code&gt;

The worker hangs indefinitely after Successfully opened dynamic library libcudnn.so.7
	</description>
	<comments>
		<comment id='1' author='sarthfrey-db' date='2020-01-15T19:15:51Z'>
		Encounters an error at the same point of execution in TF 2.1 as TF 2.0 (not just a TF2.0 problem)
&lt;denchmark-code&gt;2020-01-15 19:13:45.016826: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:390] Started server with target: grpc://localhost:2000
Number of devices: 2
Downloading and preparing dataset mnist (11.06 MiB) to /root/tensorflow_datasets/mnist/1.0.0...
WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your
local data directory. If you'd instead prefer to read directly from our public
GCS bucket (recommended if you're running on GCP), you can instead set
data_dir=gs://tfds-data/datasets.

Dataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/1.0.0. Subsequent calls will reuse this data.
WARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an "evaluator" task exists in the cluster.
WARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an "evaluator" task exists in the cluster.
WARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.
WARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.
WARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.
WARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.
Train for 5 steps
Epoch 1/3
2020-01-15 19:14:02.726512: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-01-15 19:14:04.253929: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-01-15 19:14:05.262732: I tensorflow/compiler/jit/xla_compilation_cache.cc:242] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2020-01-15 19:14:05.733056: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at collective_ops.cc:253 : Internal: unhandled system error
2020-01-15 19:14:05.733145: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: unhandled system error
	 [[{{node scoped_allocator_1_1_CollectiveReduce}}]]
	 [[GroupCrossDeviceControlEdges_0/Identity_3/_11]]
2020-01-15 19:14:05.733273: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: unhandled system error
	 [[{{node scoped_allocator_1_1_CollectiveReduce}}]]
Segmentation fault (core dumped)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='sarthfrey-db' date='2020-01-16T18:58:20Z'>
		Changed train_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE) to train_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE).repeat() with no success on TF 2.1
		</comment>
		<comment id='3' author='sarthfrey-db' date='2020-01-23T20:58:28Z'>
		I'm not sure but I think this could be originating from 


tensorflow/tensorflow/core/nccl/nccl_manager.cc


         Line 51
      in
      4c9f777






 return errors::Internal(ncclGetErrorString(nccl_status)); \ 




.
Can you rerun with the &lt;denchmark-link:https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/env.html#nccl-debug&gt;NCCL_DEBUG&lt;/denchmark-link&gt;
 environment variable set to ?
		</comment>
		<comment id='4' author='sarthfrey-db' date='2020-02-18T22:47:01Z'>
		&lt;denchmark-link:https://github.com/dubey&gt;@dubey&lt;/denchmark-link&gt;
 with TF 2.1 I now get the following for the following code.
import os, json
os.environ['TF_CONFIG'] = json.dumps({
  'cluster': {
    'worker': ['X.X.X.X:2000', 'X.X.X.X:2000'],
  },
  'task': {
    'type': 'worker',
    'index': 0
  }
})
os.environ['NCCL_DEBUG'] = 'INFO'
import tensorflow_datasets as tfds
import tensorflow as tf
strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(communication=tf.distribute.experimental.CollectiveCommunication.NCCL) # NCCL vs RING
# strategy = tf.distribute.MirroredStrategy() # NCCL vs RING
print('Number of devices: {}'.format(strategy.num_replicas_in_sync))
BUFFER_SIZE = 10000
BATCH_SIZE = 64
def make_datasets_unbatched():
  # Scaling MNIST data from (0, 255] to (0., 1.]
  def scale(image, label):
    image = tf.cast(image, tf.float32)
    image /= 255
    return image, label
  datasets, info = tfds.load(name='mnist',
                            with_info=True,
                            as_supervised=True)
  return datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE)
train_datasets = make_datasets_unbatched().batch(BATCH_SIZE)
def build_and_compile_cnn_model():
  model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),
    tf.keras.layers.MaxPooling2D(),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
  ])
  model.compile(
    loss=tf.keras.losses.sparse_categorical_crossentropy,
    optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
    metrics=['accuracy'])
  return model
GLOBAL_BATCH_SIZE = 64 * 2
with strategy.scope():
  train_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE)
  multi_worker_model = build_and_compile_cnn_model()
multi_worker_model.fit(x=train_datasets, epochs=3, steps_per_epoch=5)
Logs:
&lt;denchmark-code&gt;2020-02-18 22:43:03.345917: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:2000, 1 -&gt; X.X.X.X:2000}
2020-02-18 22:43:03.347371: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:390] Started server with target: grpc://localhost:2000
Number of devices: 8
0117-190446-scram251-10-95-224-133:7890:8187 [0] NCCL INFO NET/Socket : Using [0]eth0:10.95.224.133&lt;0&gt;
0117-190446-scram251-10-95-224-133:7890:8187 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

0117-190446-scram251-10-95-224-133:7890:8187 [0] external/nccl_archive/src/misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
WARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an "evaluator" task exists in the cluster.
WARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an "evaluator" task exists in the cluster.
WARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.
WARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.
WARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.
WARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.
Train for 5 steps
Epoch 1/3
2020-02-18 22:43:10.121251: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-02-18 22:43:10.477178: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_3}}]]
2020-02-18 22:43:10.808782: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_3}}]]
	 [[SGD/Cast/ReadVariableOp/_2]]
WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 15 batches). You may need to use the repeat() function when building your dataset.
WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 15 batches). You may need to use the repeat() function when building your dataset.
0/5 [..............................] - ETA: 0sTraceback (most recent call last):
  File "x.py", line 47, in &lt;module&gt;
    multi_worker_model.fit(x=train_datasets, epochs=3, steps_per_epoch=5)
  File "/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py", line 819, in fit
    use_multiprocessing=use_multiprocessing)
  File "/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py", line 790, in fit
    *args, **kwargs)
  File "/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py", line 777, in wrapper
    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)
  File "/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py", line 853, in run_distribute_coordinator
    task_id, session_config, rpc_layer)
  File "/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py", line 360, in _run_single_worker
    return worker_fn(strategy)
  File "/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py", line 772, in _worker_fn
    return method(model, **kwargs)
  File "/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 342, in fit
    total_epochs=epochs)
  File "/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 187, in run_one_epoch
    aggregator.finalize()
  File "/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py", line 144, in finalize
    raise ValueError('Empty training data.')
ValueError: Empty training data.
2020-02-18 22:43:11.074620: W tensorflow/core/common_runtime/eager/context.cc:349] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='sarthfrey-db' date='2020-02-18T23:49:56Z'>
		cc &lt;denchmark-link:https://github.com/ckkuang&gt;@ckkuang&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='sarthfrey-db' date='2020-02-19T00:12:00Z'>
		I suspect the issue is because of automatic data sharding. The program uses  to create datasets.  is usually backed up by files in GCS. The mnist dataset probably has only one file, and TF tries to &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/data/experimental/DistributeOptions#auto_shard_policy&gt;shard&lt;/denchmark-link&gt;
 the file to two workers, so one worker ends up getting no input.
Can you try disabling the automatic sharding by following instructions &lt;denchmark-link:https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#dataset_sharding_and_batch_size&gt;here&lt;/denchmark-link&gt;
? Alternatively you can also try setting the  to DATA.
		</comment>
		<comment id='7' author='sarthfrey-db' date='2020-02-19T00:41:38Z'>
		Hi &lt;denchmark-link:https://github.com/ckkuang&gt;@ckkuang&lt;/denchmark-link&gt;
, I have tried your suggestion but unfortunately I receive the same error. Here is the change I made:
import os, json
os.environ['TF_CONFIG'] = json.dumps({
  'cluster': {
    'worker': ['X.X.X.X:2000', 'X.X.X.X:2000'],
  },
  'task': {
    'type': 'worker',
    'index': 0
  }
})
os.environ['NCCL_DEBUG'] = 'INFO'
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
import tensorflow_datasets as tfds
import tensorflow as tf
strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(communication=tf.distribute.experimental.CollectiveCommunication.NCCL) # NCCL vs RING
# strategy = tf.distribute.MirroredStrategy() # NCCL vs RING
print('Number of devices: {}'.format(strategy.num_replicas_in_sync))
BUFFER_SIZE = 10000
BATCH_SIZE = 64
def make_datasets_unbatched():
  # Scaling MNIST data from (0, 255] to (0., 1.]
  def scale(image, label):
    image = tf.cast(image, tf.float32)
    image /= 255
    return image, label
  datasets, info = tfds.load(name='mnist',
                            with_info=True,
                            as_supervised=True)
  return datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE)

train_datasets = make_datasets_unbatched().batch(BATCH_SIZE)

options = tf.data.Options()
options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF
train_datasets = train_datasets.with_options(options)

def build_and_compile_cnn_model():
  model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),
    tf.keras.layers.MaxPooling2D(),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
  ])
  model.compile(
    loss=tf.keras.losses.sparse_categorical_crossentropy,
    optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
    metrics=['accuracy'])
  return model
GLOBAL_BATCH_SIZE = 64 * 2
with strategy.scope():
  train_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE)
  multi_worker_model = build_and_compile_cnn_model()
multi_worker_model.fit(x=train_datasets, epochs=3, steps_per_epoch=5)
logs:
&lt;denchmark-code&gt;Train for 5 steps
Epoch 1/3
2020-02-19 00:40:30.369268: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-02-19 00:40:30.720959: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext}}]]
	 [[metrics/accuracy/div_no_nan/allreduce_1/CollectiveReduce/_10]]
2020-02-19 00:40:30.721068: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext}}]]
WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 15 batches). You may need to use the repeat() function when building your dataset.
WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 15 batches). You may need to use the repeat() function when building your dataset.
0/5 [..............................] - ETA: 0sTraceback (most recent call last):
  File "x.py", line 54, in &lt;module&gt;
    multi_worker_model.fit(x=train_datasets, epochs=3, steps_per_epoch=5)
  File "/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py", line 819, in fit
    use_multiprocessing=use_multiprocessing)
  File "/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py", line 790, in fit
    *args, **kwargs)
  File "/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py", line 777, in wrapper
    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)
  File "/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py", line 853, in run_distribute_coordinator
    task_id, session_config, rpc_layer)
  File "/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py", line 360, in _run_single_worker
    return worker_fn(strategy)
  File "/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py", line 772, in _worker_fn
    return method(model, **kwargs)
  File "/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 342, in fit
    total_epochs=epochs)
  File "/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 187, in run_one_epoch
    aggregator.finalize()
  File "/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py", line 144, in finalize
    raise ValueError('Empty training data.')
ValueError: Empty training data.
2020-02-19 00:40:30.966412: W tensorflow/core/common_runtime/eager/context.cc:349] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='sarthfrey-db' date='2020-02-19T00:45:55Z'>
		I also tried turning off autosharding before calling train_datasets.batch(BATCH_SIZE)
		</comment>
		<comment id='9' author='sarthfrey-db' date='2020-02-19T00:52:40Z'>
		Ah it works now - I added the fix in the wrong place, thanks! I am now receiving a different error but the training makes progress through all the epochs.
import os, json
os.environ['TF_CONFIG'] = json.dumps({
  'cluster': {
    'worker': ['X.X.X.X:2000', 'X.X.X.X:2000'],
  },
  'task': {
    'type': 'worker',
    'index': 0
  }
})
os.environ['NCCL_DEBUG'] = 'INFO'
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
import tensorflow_datasets as tfds
import tensorflow as tf
strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(communication=tf.distribute.experimental.CollectiveCommunication.NCCL) # NCCL vs RING
# strategy = tf.distribute.MirroredStrategy() # NCCL vs RING
print('Number of devices: {}'.format(strategy.num_replicas_in_sync))
BUFFER_SIZE = 10000
BATCH_SIZE = 64
def make_datasets_unbatched():
  # Scaling MNIST data from (0, 255] to (0., 1.]
  def scale(image, label):
    image = tf.cast(image, tf.float32)
    image /= 255
    return image, label
  datasets, info = tfds.load(name='mnist',
                            with_info=True,
                            as_supervised=True)
  return datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE)

def build_and_compile_cnn_model():
  model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),
    tf.keras.layers.MaxPooling2D(),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
  ])
  model.compile(
    loss=tf.keras.losses.sparse_categorical_crossentropy,
    optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
    metrics=['accuracy'])
  return model
GLOBAL_BATCH_SIZE = 64 * 2
with strategy.scope():
  train_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE).repeat()
  options = tf.data.Options()
  options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA
  train_datasets = train_datasets.with_options(options)
  multi_worker_model = build_and_compile_cnn_model()
multi_worker_model.fit(x=train_datasets, epochs=3, steps_per_epoch=5)
logs:
&lt;denchmark-code&gt;Train for 5 steps
Epoch 1/3
2020-02-19 00:50:39.229227: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-02-19 00:50:40.274897: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-02-19 00:50:42.146923: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
0117-190446-scram251-10-95-238-76:12426:12816 [0] NCCL INFO NET/Socket : Using [0]eth0:10.95.238.76&lt;0&gt;
0117-190446-scram251-10-95-238-76:12426:12816 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

0117-190446-scram251-10-95-238-76:12426:12816 [0] external/nccl_archive/src/misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
0117-190446-scram251-10-95-238-76:12426:12818 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff
0117-190446-scram251-10-95-238-76:12426:12818 [0] NCCL INFO Could not find real path of /sys/class/net/eth0/device
0117-190446-scram251-10-95-238-76:12426:12818 [0] NCCL INFO bazel-out/k8-py2-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:24 -&gt; 2
0117-190446-scram251-10-95-238-76:12426:12818 [0] NCCL INFO CUDA Dev 0[0], Socket NIC distance :  SYS
0117-190446-scram251-10-95-238-76:12426:12818 [0] NCCL INFO Could not find real path of /sys/class/net/eth0/device
0117-190446-scram251-10-95-238-76:12426:12818 [0] NCCL INFO bazel-out/k8-py2-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:24 -&gt; 2
0117-190446-scram251-10-95-238-76:12426:12818 [0] NCCL INFO Ring 00 : 0 -&gt; 1 [receive] via NET/Socket/0
0117-190446-scram251-10-95-238-76:12426:12818 [0] NCCL INFO Could not find real path of /sys/class/net/eth0/device
0117-190446-scram251-10-95-238-76:12426:12818 [0] NCCL INFO bazel-out/k8-py2-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:24 -&gt; 2
0117-190446-scram251-10-95-238-76:12426:12818 [0] NCCL INFO Ring 00 : 1 -&gt; 0 [send] via NET/Socket/0
0117-190446-scram251-10-95-238-76:12426:12818 [0] NCCL INFO Could not find real path of /sys/class/net/eth0/device
0117-190446-scram251-10-95-238-76:12426:12818 [0] NCCL INFO bazel-out/k8-py2-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:24 -&gt; 2
0117-190446-scram251-10-95-238-76:12426:12818 [0] NCCL INFO Ring 01 : 0 -&gt; 1 [receive] via NET/Socket/0
0117-190446-scram251-10-95-238-76:12426:12818 [0] NCCL INFO Could not find real path of /sys/class/net/eth0/device
0117-190446-scram251-10-95-238-76:12426:12818 [0] NCCL INFO bazel-out/k8-py2-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:24 -&gt; 2
0117-190446-scram251-10-95-238-76:12426:12818 [0] NCCL INFO Ring 01 : 1 -&gt; 0 [send] via NET/Socket/0
0117-190446-scram251-10-95-238-76:12426:12818 [0] NCCL INFO comm 0x7f9c9800d180 rank 1 nranks 2 cudaDev 0 nvmlDev 0 - Init COMPLETE
5/5 [==============================] - 5s 1s/step - loss: 2.3133 - accuracy: 0.0969
Epoch 2/3
5/5 [==============================] - 0s 5ms/step - loss: 2.3054 - accuracy: 0.0984
Epoch 3/3
5/5 [==============================] - 0s 5ms/step - loss: 2.3137 - accuracy: 0.0984
2020-02-19 00:50:42.330314: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
2020-02-19 00:50:42.341750: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled
2020-02-19 00:50:42.560553: W tensorflow/core/common_runtime/eager/context.cc:349] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='sarthfrey-db' date='2020-02-19T00:52:51Z'>
		Looks like train_datasets is overwritten at the next line of with strategy.scope():. Can you try setting the option after that? You can also set the option inside make_datasets_unbatched() before returning the dataset.
		</comment>
		<comment id='11' author='sarthfrey-db' date='2020-02-19T00:52:52Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35878&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35878&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='sarthfrey-db' date='2020-02-19T00:57:28Z'>
		Thanks for confirmation! The new logs are only showing warnings about caching more input data than needed. Closing this issue now. Feel free to reopen if you encounter new errors.
		</comment>
		<comment id='13' author='sarthfrey-db' date='2020-02-19T00:57:30Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35878&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35878&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='14' author='sarthfrey-db' date='2020-02-19T00:58:00Z'>
		Gotcha, thanks &lt;denchmark-link:https://github.com/ckkuang&gt;@ckkuang&lt;/denchmark-link&gt;
 !
		</comment>
	</comments>
</bug>