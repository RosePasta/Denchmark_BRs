<bug id='39790' author='jaggernaut007' open_date='2020-05-22T15:24:07Z' closed_time='2020-06-08T19:08:34Z'>
	<summary>RuntimeError: Inputs and outputs not all float|uint8|int16 types.Node number 4 (ADD) failed to invoke.</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
TensorFlow installed from (source or binary): binary
TensorFlow version (or github SHA if from source):TF 2.2.0 from anaconda

Command used to run the converter or code if youâ€™re using the Python API
If possible, please share a link to Colab/Jupyter/any notebook.
&lt;denchmark-code&gt;import tensorflow as tf
# Path to the frozen graph file
graph_def_file = 'savedModel.pb'
# A list of the names of the model's input tensors
input_arrays = ['input.1']
# A list of the names of the model's output tensors
output_arrays = ['4358']
# Load and convert the frozen graph
converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(
  graph_def_file, input_arrays, output_arrays)


converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,
    tf.lite.OpsSet.SELECT_TF_OPS,
]
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_types = [tf.float16]
tflite_fp16_model = converter.convert()
tflite_model_fp16_file = "tf_model.tflite"
open(tflite_model_fp16_file , "wb").write(tflite_fp16_model)

&lt;/denchmark-code&gt;

The output from the converter invocation
&lt;denchmark-code&gt;2020-05-22 20:38:17.870933: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-05-22 20:38:17.872070: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2020-05-22 20:38:17.872351: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-7CPMA5J): /proc/driver/nvidia/version does not exist
2020-05-22 20:38:17.872996: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-05-22 20:38:17.880532: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3192000000 Hz
2020-05-22 20:38:17.882702: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f5b64000b60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-05-22 20:38:17.882997: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
&lt;/denchmark-code&gt;

Also, please include a link to the saved model or GraphDef
&lt;denchmark-code&gt;https://drive.google.com/file/d/17d8lFYY-4Z7iExnLNdYbjOt6EQpVbDdI/view?usp=sharing
&lt;/denchmark-code&gt;

Failure details
If the conversion is successful, but the generated model is wrong,
state what is wrong:

Producing wrong results and/or decrease in accuracy
Producing correct results, but the model is slower than expected (model generated from old converter)

Model not running gives error  as below.
Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

[{'name': 'input.1', 'index': 0, 'shape': array([  1,   3, 256, 256], dtype=int32), 'shape_signature': array([  1,   3, 256, 256], dtype=int32), 'dtype': &lt;class 'numpy.float32'&gt;, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
[{'name': '4358', 'index': 4138, 'shape': array([ 1, 17,  3], dtype=int32), 'shape_signature': array([ 1, 17,  3], dtype=int32), 'dtype': &lt;class 'numpy.float32'&gt;, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
Traceback (most recent call last):
File "test_graph.py", line 20, in 
interpreter.invoke()
File "/home/shreyas/.local/lib/python3.8/site-packages/tensorflow/lite/python/interpreter.py", line 511, in invoke
self._interpreter.Invoke()
File "/home/shreyas/.local/lib/python3.8/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py", line 113, in Invoke
return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_Invoke(self)
RuntimeError: Inputs and outputs not all float|uint8|int16 types.Node number 4 (ADD) failed to invoke.

	</description>
	<comments>
		<comment id='1' author='jaggernaut007' date='2020-05-22T15:26:40Z'>
		&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/37099&gt;#37099&lt;/denchmark-link&gt;
 issue has the same problem, solution isnt working for me.
		</comment>
		<comment id='2' author='jaggernaut007' date='2020-05-22T15:55:15Z'>
		&lt;denchmark-link:https://colab.research.google.com/gist/jaggernaut007/efe78df81a697c2f39652cb0cbe88169/untitled0.ipynb&gt;https://colab.research.google.com/gist/jaggernaut007/efe78df81a697c2f39652cb0cbe88169/untitled0.ipynb&lt;/denchmark-link&gt;

Code implementation
		</comment>
		<comment id='3' author='jaggernaut007' date='2020-05-25T11:41:38Z'>
		Was able to reproduce the issue with &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/7c1f34e9fc39f8e24dda699d59b26c12/39790.ipynb&gt;TF v2.2&lt;/denchmark-link&gt;
.
Whereas, running the code with &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/bfd74703cb85b1b2ce765fb87514acc7/39790-tf-nightly.ipynb#scrollTo=JFA6IZKtnYC5&gt;TF-nightly&lt;/denchmark-link&gt;
, throws an error stating .
Please find the attached gist. Thanks!
		</comment>
		<comment id='4' author='jaggernaut007' date='2020-05-31T13:29:02Z'>
		I am also seeing this issue. Any hope of a near term fix? I am blocked.
		</comment>
		<comment id='5' author='jaggernaut007' date='2020-06-08T19:08:34Z'>
		This is working as intended.
The model has Addv2 which accepts type int64. Tensorflow lite doesn't support Addv2 with int64 input/output.
Since, you used SELECT_TF_OPS that means all non supported ops will be exported as TF ops and run in TF.
To run the generated tflite file, you need to use Interpreter which supports SELECT.
Please follow the guide on creating interpreter
&lt;denchmark-link:https://www.tensorflow.org/lite/guide/ops_select&gt;https://www.tensorflow.org/lite/guide/ops_select&lt;/denchmark-link&gt;

Another way is to update your model to use int32 instead of int64, and convert without SELECT_TF_OPS and use regular interpreter.
Thanks
		</comment>
		<comment id='6' author='jaggernaut007' date='2020-06-08T19:08:36Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39790&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39790&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>