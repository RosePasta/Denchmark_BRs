<bug id='32419' author='Aktsvigun' open_date='2019-09-11T10:08:17Z' closed_time='2019-09-11T11:37:52Z'>
	<summary>Masking in BERT</summary>
	<description>
In the original paper of BERT it is said:

Note that the purpose of the masking strategies is to reduce the mismatch between pre-training and fine-tuning, as the [MASK] symbol never appears during the fine-tuning stage.

Let's consider a sentence "I am a Liverpool fan" which with 40% masking will be transformed into "I [MASK] a [MASK] fan". When predicting the first [MASK], will it be predicted by a phrase "I [MASK] a fan", excluding the second [MASK] or "I [MASK] a [MASK] fan", by a full sentence?
And what is the purpose of replacing 10% of masked tokens with themselves? Does it mean they will not be predicted? Or we will predict them, having themselves in the context (like predicting the first [MASK] by "I am a [MASK] fan"?
Will be very grateful for any help!
	</description>
	<comments>
		<comment id='1' author='Aktsvigun' date='2019-09-11T11:24:41Z'>
		Hi,
I will try to answer your questions. Keep in mind, however that a) my knowledge of BERT only comes from the same paper you read (plus the associated source code and my personal experience re-implementing it for TF 2.0) and b) other design choices could be achieved through an alternative implementation.
That being said:

When predicting the first [MASK], will it be predicted by a phrase "I [MASK] a fan", excluding the second [MASK] or "I [MASK] a [MASK] fan", by a full sentence?

Second option :-) Since the training is conducted on a single forward-pass, the prediction of the first masked token will be based on the encoding of the sentence with both masked tokens. In other words, the sentence "I [MASK] a [MASK] fan" will be fed to the network, embedded, processed by the stack of Transformer encoder blocks, and in the end produce an encoding matrix of shape (n_tokens, model_dim). Then, the masked-tokens-prediction pre-training output layer will further process this matrix and produce predicted tokens probability distributions for each and every tokens marked as masked (both the actual [MASK] ones, and those randomly shifted or not shifted at all yet set to be predicted).

And what is the purpose of replacing 10% of masked tokens with themselves? Does it mean they will not be predicted?

To start with the practical point : the masked tokens replaced with themselves are indeed predicted (and made part of the MLM loss). In other words, what happens is that the MLM loss is based both on the ability of the model to a) predict masked tokens based on context, b) rebuild input non-masked tokens based on their encoding (the case you are asking about), and c) put such tokens in doubt based on their context (the case of tokens replaced by random ones).
This point is arguable, but the idea is that we want to ensure, at the same time, that the encoding retains information on the tokens (and does not just focus on [MASK] ones) - this is the "I can predict a token identical to the input" part - AND incorporates contextual elements - this is the "I can predict the right token instead of the random input one" part. In a sense, you would hope that the model is both able to trust and doubt the input information, and produce encodings of non-masked tokens that retain and embed the valuable information needed for downward tasks (including tasks that would not imply any masked tokens).
I hope this is somehow clearer than the few lines in the paper and can help you! To be honest I struggled a lot with understanding the rationale behind the design (independently from the fact that it is relatively easy to implement).
		</comment>
		<comment id='2' author='Aktsvigun' date='2019-09-11T11:37:51Z'>
		&lt;denchmark-link:https://github.com/pandrey-fr&gt;@pandrey-fr&lt;/denchmark-link&gt;
 geniously! Thanks a lot, the whole picture has become apparent to me :)
		</comment>
		<comment id='3' author='Aktsvigun' date='2019-09-11T11:37:53Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=32419&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=32419&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='Aktsvigun' date='2019-09-11T15:27:18Z'>
		&lt;denchmark-link:https://github.com/Aktsvigun&gt;@Aktsvigun&lt;/denchmark-link&gt;
 That is nice to read! You are welcome :)
		</comment>
	</comments>
</bug>