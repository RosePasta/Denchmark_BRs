<bug id='34786' author='johnpjust' open_date='2019-12-03T07:58:24Z' closed_time='2020-01-14T21:58:08Z'>
	<summary>memory leak on tf2.0 with tf.signal.frame</summary>
	<description>
The memory doubles every time I run the tape.gradients part.  I think it's connected to the timedistributedlayers...?
&lt;denchmark-code&gt;###### create model 

    inputs = tf.keras.Input(shape=(6, *data_loader_train[0][0][0].shape), name='img') ## (108, 192, 3)
    x = layers.TimeDistributed(layers.Conv2D(16, 3, activation='relu'))(inputs)
    x = layers.TimeDistributed(layers.Conv2D(16, 3, activation='relu'))(x)
    block_1_output = layers.TimeDistributed(layers.MaxPooling2D(2))(x)

    x = layers.TimeDistributed(layers.Conv2D(16, 3, activation='relu', padding='same'))(block_1_output)
    block_3_output = layers.add([x, block_1_output])
    block_3_output = layers.TimeDistributed(layers.MaxPooling2D(2))(block_3_output)

    x = layers.TimeDistributed(layers.Conv2D(16, 3, activation='relu'))(block_3_output)
    x = layers.TimeDistributed(layers.GlobalAveragePooling2D())(x)

    x = layers.Flatten()(x)
    x = layers.Dense(16, activation='relu')(x)
    x = layers.Dense(1)(x)
    counts = tf.keras.activations.softplus(x)

    model = tf.keras.Model(inputs, counts, name='toy_resnet')
    model.summary()

    ### run model
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;####### running this part doubles memory every two times ##########
for x_ in batch(np.random.uniform(size=(100,6,108,192,3)).astype(np.float32), 10):
     with tf.GradientTape() as tape:
             count_ = tf.reduce_sum(model(x_))
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='johnpjust' date='2019-12-04T04:46:45Z'>
		&lt;denchmark-link:https://github.com/johnpjust&gt;@johnpjust&lt;/denchmark-link&gt;
 ,
When tried running the given code  error was faced, can you provide the standalone code to replicate the issue reported above ?Thanks!
		</comment>
		<comment id='2' author='johnpjust' date='2019-12-04T05:47:10Z'>
		&lt;denchmark-link:https://github.com/oanush&gt;@oanush&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;from tensorflow.keras import layers
import numpy as np
 
def batch(iterable, n=1):
    l = len(iterable)
    for ndx in range(0, l, n):
        yield iterable[ndx:min(ndx + n, l)]
 
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')
        # Currently, memory growth needs to be the same across GPUs
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        logical_gpus = tf.config.experimental.list_logical_devices('GPU')
        print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")
    except RuntimeError as e:
        # Memory growth must be set before GPUs have been initialized
        print(e)
                        
 
with tf.device('/gpu:0'):
    inputs = tf.keras.Input(shape=(6, 108, 192, 3), name='img') ## (108, 192, 3)
    x = layers.TimeDistributed(layers.Conv2D(16, 3, activation='relu'))(inputs)
    x = layers.TimeDistributed(layers.Conv2D(16, 3, activation='relu'))(x)
    block_1_output = layers.TimeDistributed(layers.MaxPooling2D(2))(x)
 
    x = layers.TimeDistributed(layers.Conv2D(16, 3, activation='relu', padding='same'))(block_1_output)
    # x = layers.Conv2D(32, 3, activation='relu', padding='same')(x)
    block_3_output = layers.add([x, block_1_output])
    block_3_output = layers.TimeDistributed(layers.MaxPooling2D(2))(block_3_output)
 
    x = layers.TimeDistributed(layers.Conv2D(16, 3, activation='relu'))(block_3_output)
    x = layers.TimeDistributed(layers.GlobalAveragePooling2D())(x)
 
    x = layers.Flatten()(x)
    x = layers.Dense(16, activation='relu')(x)
    x = layers.Dense(1)(x)
    counts = tf.keras.activations.softplus(x)
    # x = layers.Dropout(0.5)(x)
    # outputs = layers.Dense(10, activation='softmax')(x)
 
    model = tf.keras.Model(inputs, counts, name='toy_resnet')
    model.summary()
 
            ### everytime this is run, gpu memory grows
for x_ in batch(np.random.uniform(size=(100,6,108,192,3)).astype(np.float32), 10):
    temp = model(x_) 

&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='johnpjust' date='2019-12-04T08:44:39Z'>
		&lt;denchmark-link:https://github.com/johnpjust&gt;@johnpjust&lt;/denchmark-link&gt;
 ,
I tried running the code in colab for TF-2.0 and i didn't face any issue,kindly find the &lt;denchmark-link:https://colab.sandbox.google.com/gist/oanush/dc54aed45ba16f16cb96f1e3aa27d1e2/34786.ipynb&gt;gist&lt;/denchmark-link&gt;
 for your reference. Please provide us gist if the issue is replicating from your end.
		</comment>
		<comment id='4' author='johnpjust' date='2019-12-06T17:56:12Z'>
		&lt;denchmark-link:https://github.com/oanush&gt;@oanush&lt;/denchmark-link&gt;
 I'm not familiar with this gist your are referring to, but I have checked this on two different machines, using both Linux and Windows, and with TF2.0 and TF1.13, so i know the issue persists.  If you have a way to watch your GPU memory growth, then please use that and you will see what I'm talking about.  I use "nvidia-smi  -l 2".
		</comment>
		<comment id='5' author='johnpjust' date='2019-12-06T18:38:02Z'>
		&lt;denchmark-link:https://github.com/oanush&gt;@oanush&lt;/denchmark-link&gt;
  Note that I've found a way around this issue by only using the timedistributedlayer once.  For this i just create the CNN model separately and then do a sequential model with the first layer being a timedistributedlayer using the CNN as input.
		</comment>
		<comment id='6' author='johnpjust' date='2019-12-08T06:55:49Z'>
		&lt;denchmark-link:https://github.com/oanush&gt;@oanush&lt;/denchmark-link&gt;
  OK so I've been messing with this some more, and there is definitely a VRAM (GPU) memory leak associated with tf.signal.frame that interacts with the model.  Simply exchanging it with an itertools function eliminates the issue...
&lt;denchmark-code&gt;def batch(iterable, n=1):
    l = len(iterable)
    for ndx in range(0, l, n):
        yield iterable[ndx:min(ndx + n, l)]

####### here's an example model #############
with tf.device('/gpu:0'):
    inputs = tf.keras.Input(shape=(108, 192, 3), name='img') ## (108, 192, 3)
    x = layers.Conv2D(16, 3, activation='relu')(inputs)
    x = layers.Conv2D(16, 3, activation='relu')(x)
    block_1_output = layers.MaxPooling2D(2)(x)

    x = layers.Conv2D(16, 3, activation='relu', padding='same')(block_1_output)
    # x = layers.Conv2D(32, 3, activation='relu', padding='same')(x)
    x = layers.add([x, block_1_output])
    block_2_output = layers.MaxPooling2D(2)(x)

    x = layers.Conv2D(16, 3, activation='relu', padding='same')(block_2_output)
    x = layers.add([x, block_2_output])
    x = layers.MaxPooling2D(2)(x)
    block_3_output = layers.GlobalAveragePooling2D()(x)

    # x = layers.Flatten()(x)
    # x = layers.Dense(16, activation='relu')(x)
    # x = layers.Dense(1)(x)
    # counts = tf.keras.activations.softplus(x)

    cnn = tf.keras.Model(inputs, block_3_output, name='toy_resnet')
    # model = tf.keras.Sequential()
    # model.add(layers.TimeDistributed(cnn, input_shape=(6, 108, 192, 3)))
    # model.add(layers.Dense(16, activation='relu'))
    # model.add(layers.Dense(1))

    input_sequences = tf.keras.Input(shape=(6, 108, 192, 3)) ## (108, 192, 3)
    x = layers.TimeDistributed(cnn)(input_sequences)
    x = layers.Flatten()(x)
    x = layers.Dense(16, activation='relu')(x)
    x = layers.Dense(1)(x)
    counts = tf.keras.activations.softplus(x)
    model = tf.keras.Model(input_sequences, counts, name='toy_resnet')
    model.summary()

###### run code below using tf.signal.frame and watch the VRAM memory on the GPU.  You will see it grow before throwing an error.  Then do the same with "more_itertools" and you'll see it's fine.

for n in range(50):
    ##### exchange more_itertools with tf.signal.frame to get memory leak
    x_mb = tf.signal.frame(np.random.uniform(size=(200,108,192,3)).astype(np.float32), args.num_frames, 1, axis=0)
    for x_ in batch(x_mb, 10):
    ################# no memory leak with more_itertools for sliding window framing ###############
    # for x_ in batch(np.array(list(more_itertools.windowed(np.random.uniform(size=(100, 108, 192, 3)).astype(np.float32), n=6, step=1))),10):
    #########################################
        temp = model(x_)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='johnpjust' date='2019-12-08T20:15:25Z'>
		update:  further testing -- in addition to the above repeatable memory leak.   I've found using the above model and batch loader, and then a toy training loop below, that I'm still seeing the memory leak but it's much more delayed.  As it stands the model will train using about 9gb of VRAM up to incount =~8000 or so, after which it will jump up to to the max value of available VRAM (for me that's ~11gb).  Then if I keep training it will eventually throw an OOM as it leaks more memory.
All in all there are definitely some glitchy things with some of these less-used features in TF like tf.signal.frame and timedistributedlayer, and these are the types of things that we are trying to make use of on the cutting edge front of applications.  Thus it is well worth the time to fix some of these issues.
Thanks.
&lt;denchmark-code&gt;data_loader_train = np.random.uniform(size=(100, 120, 108, 192, 3)).astype(np.float32)

indcount = 0
for epoch in range(args.epochs):
    train_loss = 0
    for ind in range(len(data_loader_train)):
    # for ind in np.random.permutation(len(data_loader_train)):
    #     print(r'epoch:  %i,   index:  %i' % (epoch, ind), end="\r")
        print(r'indcount:  %i' % indcount, end="\r")
        # x_mb = np.array(list(more_itertools.windowed(data_loader_train[ind][0], n=args.num_frames, step=1)))
        # y_mb = data_loader_train[ind][1]
        x_mb = np.array(list(more_itertools.windowed(data_loader_train[ind], n=args.num_frames, step=1)))
        y_mb = np.random.randint(20,40)
        count = 0
        grads = [np.zeros_like(x) for x in model.trainable_variables]
        # print("index:  " + str(ind))
        for x_ in batch(x_mb, args.batch_size):
            indcount += 1
            with tf.GradientTape() as tape:
                count_ = tf.reduce_sum(model(x_))
            count += count_
            grads_ = tape.gradient(count_, model.trainable_variables)
            grads = [x1 + x2 for x1, x2 in zip(grads, grads_)]

        # grads = [None if grad is None else tf.clip_by_norm(grad, clip_norm=args.clip_norm) for grad in grads]
        loss = count-y_mb
        globalstep = optimizer.apply_gradients(zip([2*loss*x for x in grads], model.trainable_variables))

        tf.summary.scalar('loss/train', loss**2, globalstep)
## after 5 epochs
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='johnpjust' date='2019-12-17T22:57:13Z'>
		Apologies for the delay in response. The second last code snippet where you are using tf.signal.frame looks incomplete. It will be great if you can complete it. Also feel free to close this issue if necessary. Thanks!
		</comment>
		<comment id='9' author='johnpjust' date='2020-01-14T21:58:09Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34786&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34786&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>