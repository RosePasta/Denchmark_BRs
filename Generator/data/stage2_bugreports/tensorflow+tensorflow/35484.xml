<bug id='35484' author='ZoltanRado' open_date='2019-12-29T22:46:35Z' closed_time='2020-01-09T16:17:43Z'>
	<summary>variational autoencoder code sample error in TF2.0</summary>
	<description>
Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.
The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: &lt;denchmark-link:https://www.tensorflow.org/community/contribute/docs&gt;https://www.tensorflow.org/community/contribute/docs&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;URL(s) with the issue:&lt;/denchmark-h&gt;

&lt;denchmark-link:https://www.tensorflow.org/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example&gt;https://www.tensorflow.org/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;Description of issue (what needs changing):&lt;/denchmark-h&gt;

The code given in the guide does not run if the latent dimension is set 1, it runs fine for every latent dimension &gt;1!
&lt;denchmark-h:h3&gt;Clear description&lt;/denchmark-h&gt;

when the model is built and trained with code:
&lt;denchmark-code&gt;vae = VariationalAutoEncoder(784, 64, 1)

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)

vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())
vae.fit(x_train, x_train, epochs=3, batch_size=64)
&lt;/denchmark-code&gt;

it throws the error:
ValueError: The last dimension of the inputs to Dense should be defined. Found None
&lt;denchmark-h:h3&gt;Correct links&lt;/denchmark-h&gt;

n/a
&lt;denchmark-h:h3&gt;Parameters defined&lt;/denchmark-h&gt;

latent_dim = 1
(vae = VariationalAutoEncoder(784, 64, 1) )
&lt;denchmark-h:h3&gt;Returns defined&lt;/denchmark-h&gt;

n/a
&lt;denchmark-h:h3&gt;Raises listed and defined&lt;/denchmark-h&gt;

ValueError: The last dimension of the inputs to Dense should be defined. Found None
&lt;denchmark-h:h3&gt;Usage example&lt;/denchmark-h&gt;

code in the guide:
&lt;denchmark-code&gt;class Sampling(layers.Layer):
  """Uses (z_mean, z_log_var) to sample z, the vector encoding a digit."""

  def call(self, inputs):
    z_mean, z_log_var = inputs
    batch = tf.shape(z_mean)[0]
    dim = tf.shape(z_mean)[1]
    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
    return z_mean + tf.exp(0.5 * z_log_var) * epsilon


class Encoder(layers.Layer):
  """Maps MNIST digits to a triplet (z_mean, z_log_var, z)."""

  def __init__(self,
               latent_dim=32,
               intermediate_dim=64,
               name='encoder',
               **kwargs):
    super(Encoder, self).__init__(name=name, **kwargs)
    self.dense_proj = layers.Dense(intermediate_dim, activation='relu')
    self.dense_mean = layers.Dense(latent_dim)
    self.dense_log_var = layers.Dense(latent_dim)
    self.sampling = Sampling()

  def call(self, inputs):
    x = self.dense_proj(inputs)
    z_mean = self.dense_mean(x)
    z_log_var = self.dense_log_var(x)
    z = self.sampling((z_mean, z_log_var))
    return z_mean, z_log_var, z


class Decoder(layers.Layer):
  """Converts z, the encoded digit vector, back into a readable digit."""

  def __init__(self,
               original_dim,
               intermediate_dim=64,
               name='decoder',
               **kwargs):
    super(Decoder, self).__init__(name=name, **kwargs)
    self.dense_proj = layers.Dense(intermediate_dim, activation='relu')
    self.dense_output = layers.Dense(original_dim, activation='sigmoid')

  def call(self, inputs):
    x = self.dense_proj(inputs)
    return self.dense_output(x)


class VariationalAutoEncoder(tf.keras.Model):
  """Combines the encoder and decoder into an end-to-end model for training."""

  def __init__(self,
               original_dim,
               intermediate_dim=64,
               latent_dim=32,
               name='autoencoder',
               **kwargs):
    super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)
    self.original_dim = original_dim
    self.encoder = Encoder(latent_dim=latent_dim,
                           intermediate_dim=intermediate_dim)
    self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)

  def call(self, inputs):
    z_mean, z_log_var, z = self.encoder(inputs)
    reconstructed = self.decoder(z)
    # Add KL divergence regularization loss.
    kl_loss = - 0.5 * tf.reduce_mean(
        z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)
    self.add_loss(kl_loss)
    return reconstructed



vae = VariationalAutoEncoder(784, 64, 1)

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)

vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())
vae.fit(x_train, x_train, epochs=3, batch_size=64)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Request visuals, if applicable&lt;/denchmark-h&gt;

n/a
&lt;denchmark-h:h3&gt;Submit a pull request?&lt;/denchmark-h&gt;

n/a
	</description>
	<comments>
		<comment id='1' author='ZoltanRado' date='2019-12-30T06:34:10Z'>
		I have tried on colab with TF version 2.0 and was able to reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/421ce6dadb234bbe06a11cbec1f6f94a/untitled518.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='ZoltanRado' date='2020-01-09T16:17:43Z'>
		Ok Issue solved please see &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/35464#issuecomment-572177889&gt;post&lt;/denchmark-link&gt;
 for details.
		</comment>
	</comments>
</bug>