<bug id='34062' author='jnd77' open_date='2019-11-07T06:43:44Z' closed_time='2020-10-13T05:29:28Z'>
	<summary>BatchNorm generates NaN moving_variance on GPU with fused set to True for some inputs</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.15.0
Python version: 3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory: the one on Colab

Describe the current behavior
I have an input of shape (1, 1, 1, num_channels), and I run it through a tf.keras.layers.BatchNormalization in training mode.
When running on CPU (fused or not) or GPU (not fused), the batch norm has the expected moving_variance of [0.99, 0.99 ...]
When running on GPU with fused=True, the moving_variance is [nan, nan, ...]
Describe the expected behavior
The moving variance should not be nan

Check this &lt;denchmark-link:https://colab.research.google.com/gist/jnd77/38ae530b17ee84e7b3907e3010c8529a/untitled0.ipynb&gt;gist&lt;/denchmark-link&gt;

Other info / logs
It works fine on CPU.
	</description>
	<comments>
		<comment id='1' author='jnd77' date='2019-11-07T06:59:39Z'>
		Hi !!
The common reason for NaN can be the following :

Gradient Blow up
Bad learning rate policy and params
Faulty Loss function
stride larger than kernel size in "Pooling" layer

If possible check for the above problems.
		</comment>
		<comment id='2' author='jnd77' date='2019-11-07T07:22:46Z'>
		Hi. Thanks for the advice.
As you can see in the gist, I'm not running any model or training. It happens only for certain shapes of inputs (batch size, width and height equal to 1, but maybe for some others ?!), so I guess there is a bug in the fused op on GPU for this corner case ...
		</comment>
		<comment id='3' author='jnd77' date='2019-11-08T08:36:54Z'>
		I have tried on colab with TF version 1.15 on GPU with  ,  and was able to reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/40cbe108f064e2d38bbda7b215f4e69a/untitled0.ipynb&gt;here&lt;/denchmark-link&gt;
.However i am not seeing the issue with CPU. Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/ae83c64e7650839fbfb1c8e86c7eda2c/untitled343.ipynb&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='4' author='jnd77' date='2020-02-27T18:59:30Z'>
		&lt;denchmark-link:https://github.com/jnd77&gt;@jnd77&lt;/denchmark-link&gt;
 -- a lot has changed since this issue was first posted. Is this still an issue in TF v2? Can you try with tf-nightly or the most recent release of tensorflow?
		</comment>
		<comment id='5' author='jnd77' date='2020-02-28T03:18:58Z'>
		&lt;denchmark-link:https://github.com/karmel&gt;@karmel&lt;/denchmark-link&gt;
 --  still an issue. I updated the &lt;denchmark-link:https://colab.research.google.com/gist/jnd77/9a9a60621f9382b99e2dcb63eeae8af5/untitled0.ipynb&gt;gist&lt;/denchmark-link&gt;
 with TF 2.1.
		</comment>
		<comment id='6' author='jnd77' date='2020-03-30T23:13:44Z'>
		It appears  uses the biased variance (meaning, &lt;denchmark-link:https://en.wikipedia.org/wiki/Bessel%27s_correction&gt;Bessel's correction&lt;/denchmark-link&gt;
 is not used) while  uses the unbiased variance. The fact there is a difference is bad, but that is a separate issue. If you take the unbiased variance of a single element, you compute 0/0, so arguably the correct result is getting NaNs. This is why passing  gets NaNs.
The CPU has a special check if there is only one batch element &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/9cc8c98a6017f7337d6b61f53118bcaa222bcd37/tensorflow/core/kernels/fused_batch_norm_op.cc#L176&gt;here&lt;/denchmark-link&gt;
 which is why this doesn't occur on the CPU.
		</comment>
		<comment id='7' author='jnd77' date='2020-05-29T11:30:06Z'>
		This is also still an issue in tf 2.2.
import numpy as np
import tensorflow as tf

import os

x = np.random.randn(1, 1, 1, 1).astype(np.float32)
c = tf.constant(x)

bn = tf.keras.layers.BatchNormalization()
bn(x, training=True)
print(bn.moving_variance) # nan

bn = tf.keras.layers.BatchNormalization(epsilon=100)
bn(x, training=True)
print(bn.moving_variance) # nan

bn = tf.keras.layers.BatchNormalization(fused=False)
bn(x, training=True)
print(bn.moving_variance) # good

with tf.device('/cpu:0'):
    bn = tf.keras.layers.BatchNormalization()
    bn(x, training=True)
    print(bn.moving_variance)  # good
		</comment>
		<comment id='8' author='jnd77' date='2020-10-12T19:14:21Z'>
		&lt;denchmark-link:https://github.com/ben-davidson-6&gt;@ben-davidson-6&lt;/denchmark-link&gt;

I ran the code shared on tf-nightly, please refer to this &lt;denchmark-link:https://colab.research.google.com/gist/Saduf2019/6f22e28af9bdf9b3a3044322dfcd7e05/untitled431.ipynb&gt;gist here&lt;/denchmark-link&gt;
 and let us know if the issue persist.
		</comment>
		<comment id='9' author='jnd77' date='2020-10-13T01:57:18Z'>
		&lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;
 Thanks. It works for me.
		</comment>
		<comment id='10' author='jnd77' date='2020-10-13T04:38:40Z'>
		&lt;denchmark-link:https://github.com/jnd77&gt;@jnd77&lt;/denchmark-link&gt;

Thank you for the update, glad its resolved. Please move the issue to closed status.
		</comment>
		<comment id='11' author='jnd77' date='2020-10-13T05:29:30Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34062&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34062&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='jnd77' date='2020-10-30T16:06:01Z'>
		&lt;denchmark-link:https://github.com/jnd77&gt;@jnd77&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;

The issue has not been resolved.
The reason it appears to be resolved in tf-nightly is because it is running on the CPU only.
I installed tf-nightly-gpu(2.5.0-dev20201029) on a local machine and the issue still persists.
		</comment>
		<comment id='13' author='jnd77' date='2020-10-30T16:14:35Z'>
		
@jnd77 @Saduf2019
The issue has not been resolved.
The reason it appears to be resolved in tf-nightly is because it is running on the CPU only.
I installed tf-nightly-gpu(2.5.0-dev20201029) on a local machine and the issue still persists.

Please refer to the gist, the run time used is GPU. Please create a new issue if you face any using a new template.
		</comment>
		<comment id='14' author='jnd77' date='2020-10-30T17:51:59Z'>
		&lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;

I tried running this &lt;denchmark-link:https://colab.research.google.com/gist/Saduf2019/6f22e28af9bdf9b3a3044322dfcd7e05/untitled431.ipynb&gt;gist&lt;/denchmark-link&gt;
 but as discussed &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/42957&gt;here&lt;/denchmark-link&gt;
 GPUs don't work on nightly builds on colab at the moment even though it is selected as the runtime. If you run  after switching to tf-nightly, there won't be any GPUs available.
		</comment>
	</comments>
</bug>