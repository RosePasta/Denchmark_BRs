<bug id='36793' author='oeyvindds' open_date='2020-02-16T10:53:45Z' closed_time='2020-02-24T15:36:48Z'>
	<summary>Same input - different output</summary>
	<description>
I have written a small code, and notice that I get a different result dependent on if I use Numpy or not.
This code should give the same result for both df_dy and df_dy_p, but it does not. It does not matter if I change the order of the calculations.
`from math import e
import numpy as np
def calc_sigma(y):
return 1 / (1 + e**-y)
def get_derivative(x, y):
with tf.GradientTape(persistent=True) as tape:
tape.watch(x)
sigma = cal_sigma(-y)
f = (x + sigma) / np.power((x - y), 2)
p = (x + sigma) / ((x -y)**2)
print(x, y, f, p)
df_dy = tape.gradient(f, x)
df_dy_p = tape.gradient(p, x)
print(df_dy)
print(df_dy_p)
x = tf.constant (1.)
y = tf.constant (-3.)
get_derivative(x, y)`
The output is:
tf.Tensor(1.0, shape=(), dtype=float32) tf.Tensor(-3.0, shape=(), dtype=float32) tf.Tensor(0.12203588, shape=(), dtype=float32) tf.Tensor(0.12203588, shape=(), dtype=float32)
tf.Tensor(0.0625, shape=(), dtype=float32)
tf.Tensor(0.0014820583, shape=(), dtype=float32)
As one can see, the input is the same, but the output differs.
	</description>
	<comments>
		<comment id='1' author='oeyvindds' date='2020-02-17T08:42:07Z'>
		&lt;denchmark-link:https://github.com/oeyvindds&gt;@oeyvindds&lt;/denchmark-link&gt;
 please provide with simple stand alone code with all dependencies from the code for us to replicate in our environment, i was unable to replicate the code shared by you due to dependencies in the code.
Also please highlight the tensorflow version you are facing th eissue in.
		</comment>
		<comment id='2' author='oeyvindds' date='2020-02-17T09:34:04Z'>
		Hello.
Here is a complete code that runs fine on my machine (Anaconda Python) in an empty notebook:
`import tensorflow as tf
import numpy as np
from math import e
def calc_sigma(y):
return 1 / (1 + e**-y)
def get_derivative(x, y):
with tf.GradientTape(persistent=True) as tape:
tape.watch(x)
sigma = calc_sigma(-y)
f = (x + sigma) / np.power((x - y), 2)
p = (x + sigma) / ((x -y)**2)
print('Variable x: {}'.format(x))
print('Variable y: {}'.format(y))
print('Input from f: {}'.format(f))
print('Input from p: {}'.format(p))
&lt;denchmark-code&gt;df_dy_p = tape.gradient(p, x)
df_dy = tape.gradient(f, x) 

print('Output from f: {}'.format(df_dy))
print('Output from p: {}'.format(df_dy_p))
&lt;/denchmark-code&gt;

x = tf.constant (1.)
y = tf.constant (-3.)
get_derivative(x, y)`
tf.version gives '2.1.0'
The code give the following output:
Variable x: 1.0
Variable y: -3.0
Input from f: 0.12203588336706161
Input from p: 0.12203588336706161
Output from f: 0.0625
Output from p: 0.0014820583164691925
		</comment>
		<comment id='3' author='oeyvindds' date='2020-02-18T06:09:10Z'>
		&lt;denchmark-link:https://github.com/oeyvindds&gt;@oeyvindds&lt;/denchmark-link&gt;
 please provide us with indented code that is executable and has all dependencies, &lt;denchmark-link:https://colab.sandbox.google.com/gist/Saduf2019/026fb6e01f24facc0844a2f731e8c276/untitled44.ipynb&gt;code&lt;/denchmark-link&gt;
 shared is not indented , dependencies are not shared.
you could share a gist of your code using colab.
		</comment>
		<comment id='4' author='oeyvindds' date='2020-02-20T10:33:51Z'>
		Here is the code indented:
&lt;denchmark-code&gt;from math import e
import numpy as np
import tensorflow as tf
print(tf.__version__)

def calc_sigma(y):
    return 1 / (1 + e**-y)

def get_derivative(x, y):
    with tf.GradientTape(persistent=True) as tape:
        tape.watch(x)
        sigma = calc_sigma(-y)
        f = (x + sigma) / np.power((x - y), 2)
        p = (x + sigma) / ((x -y)**2)
        
        print(x, y, f, p)
    df_dy = tape.gradient(f, x)
    df_dy_p = tape.gradient(p, x)
    print(df_dy)
    print(df_dy_p)

x = tf.constant (1.)
y = tf.constant (-3.)
get_derivative(x, y)
&lt;/denchmark-code&gt;

It gives this output:
&lt;denchmark-code&gt;2.1.0
tf.Tensor(1.0, shape=(), dtype=float32) tf.Tensor(-3.0, shape=(), dtype=float32) tf.Tensor(0.12203588, shape=(), dtype=float32) tf.Tensor(0.12203588, shape=(), dtype=float32)
tf.Tensor(0.0625, shape=(), dtype=float32)
tf.Tensor(0.0014820583, shape=(), dtype=float32)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='oeyvindds' date='2020-02-24T06:24:14Z'>
		I am able to replicate this issue, please find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/Saduf2019/e550000c42d48279ab15c62c12253e54/untitled53.ipynb&gt;here&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='oeyvindds' date='2020-02-24T14:01:05Z'>
		&lt;denchmark-link:https://github.com/oeyvindds&gt;@oeyvindds&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;
, The problem is because the f is evaluated by converting (x-y)^2 in numpy before computing gradient. So, in f, denominator  is not a variable at the time of differentiation. Instead, it is a constant with value 16.
Whereas, in p,  is a tensor and it will be variable at the time of differentiation.
Look at below two codes which are tested and their outputs:
Code 1 :
from math import e
import numpy as np
import tensorflow as tf

def calc_sigma(y):
    return 1 / (1 + e**-y)

def get_derivative(x, y):
    with tf.GradientTape(persistent=True) as tape:
        tape.watch(x)
        sigma = calc_sigma(-y)
        f = (x + sigma) / np.power((x - y), 2)
        p = (x + sigma) / ((x-y)**2).numpy()
        
    df_dy = tape.gradient(f, x)
    df_dy_p = tape.gradient(p, x)
    print(df_dy)
    print(df_dy_p)

x = tf.constant (1.)
y = tf.constant (-3.)
get_derivative(x, y)
Output :
tf.Tensor(0.0625, shape=(), dtype=float32)
tf.Tensor(0.0625, shape=(), dtype=float32)
Code 2:
from math import e
import numpy as np
import tensorflow as tf

def calc_sigma(y):
    return 1 / (1 + e**-y)

def get_derivative(x, y):
    with tf.GradientTape(persistent=True) as tape:
        tape.watch(x)
        sigma = calc_sigma(-y)
        f = (x + sigma) / tf.math.square(x - y)
        p = (x + sigma) / ((x-y)**2)
        
    df_dy = tape.gradient(f, x)
    df_dy_p = tape.gradient(p, x)
    print(df_dy)
    print(df_dy_p)

x = tf.constant (1.)
y = tf.constant (-3.)
get_derivative(x, y)
Output :
tf.Tensor(0.0014820583, shape=(), dtype=float32)
tf.Tensor(0.0014820583, shape=(), dtype=float32)
		</comment>
		<comment id='7' author='oeyvindds' date='2020-02-24T15:36:44Z'>
		Thanks for the explanation :-)
		</comment>
		<comment id='8' author='oeyvindds' date='2020-02-24T15:36:49Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36793&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36793&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>