<bug id='33558' author='adish333' open_date='2019-10-21T06:50:42Z' closed_time='2019-11-05T04:23:01Z'>
	<summary>Dev-board Interpretere Runtime Error</summary>
	<description>
I am working on the Coral dev board. I'm trying to deploy a segmentation model on it. When I'm running my deep lab segmentation model it is giving me the following error-
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "infer.py", line 17, in &lt;module&gt;
    interpreter.allocate_tensors()
  File "/home/mendel/.local/lib/python3.5/site-packages/tflite_runtime/interpreter.py", line 244, in allocate_tensors
    return self._interpreter.AllocateTensors()
  File "/home/mendel/.local/lib/python3.5/site-packages/tflite_runtime/interpreter_wrapper.py", line 114, in AllocateTensors
    return _interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)
RuntimeError: Internal: :71 tf_lite_type != kTfLiteUInt8 (9 != 3)Node number 79 (EdgeTpuDelegateForCustomOp) failed to prepare.
&lt;/denchmark-code&gt;

The model and script are working fine if I don't make it TPU compatible using edgetpu_compiler.
Code to reproduce the issue
&lt;denchmark-code&gt;from tqdm import tqdm
import numpy as np
from tflite_runtime.interpreter import Interpreter
from tflite_runtime.interpreter import load_delegate

test_data = np.random.rand(480,480,3)
img = np.array([test_data], dtype=np.float32)

interpreter = Interpreter(
      model_path="deep_lab_quant_edgetpu.tflite",
      experimental_delegates=[load_delegate('libedgetpu.so.1.0')])

interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
interpreter.set_tensor(input_details[0]['index'], img)
interpreter.invoke()
output_data = interpreter.get_tensor(output_details[0]['index'])
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='adish333' date='2019-10-22T16:18:16Z'>
		I get the same error for my custom image processing model. I don't truly know the source of the error, but have found that when reducing the size of the model (number of channels in each layer) and/or the input tensor (height and width dimensions), the error goes away.
However, the output of the edgetpu compiler suggests that even my "full-size" model easily fits on the TPU:
&lt;denchmark-code&gt;Input model: model.tflite
Input size: 925.08KiB
Output model: model_edgetpu.tflite
Output size: 1.04MiB
On-chip memory available for caching model parameters: 5.78MiB
On-chip memory used for caching model parameters: 1019.75KiB
Off-chip memory used for streaming uncached model parameters: 0.00B
Number of Edge TPU subgraphs: 1
Total number of operations: 54
Operation log: model_edgetpu.log
&lt;/denchmark-code&gt;

Here are my system specs:
&lt;denchmark-code&gt;Linux kernel version:
$ uname -r
4.4.0-96-generic

Linux release:
$ lsb_release -a
Distributor ID:	Ubuntu
Description:	Ubuntu 16.04.6 LTS
Release:	16.04
Codename:	xenial

Tensorflow python module version
2.0.0-rc0

Edge TPU python module version:
2.12.1

Edge TPU compiler version:
Edge TPU Compiler version 2.0.267685300

Edge TPU runtime version:
BuildLabel(COMPILER=5.4.0 20160609,DATE=redacted,TIME=redacted,CL_NUMBER=267685300), RuntimeVersion(12)

Paths of available Edge TPU devices, if any:
('/sys/bus/usb/devices/4-3',)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='adish333' date='2019-10-23T07:03:01Z'>
		&lt;denchmark-link:https://github.com/mattroos&gt;@mattroos&lt;/denchmark-link&gt;
 thanks for your suggestion. This did not solve my error though!
Below is the output log of edgetpu compilation of .tflite model. I'm using deeplab for segmentation. let me know if you are able to capture any error here.
Also, can you suggest me any other model which can be run on coral dev board?
Thanks.
&lt;denchmark-code&gt;Edge TPU Compiler version 2.0.267685300
Input: deep_lab_quant.tflite
Output: deep_lab_quant_edgetpu.tflite

Operator                       Count      Status

QUANTIZE                       1          More than one subgraph is not supported
QUANTIZE                       1          Operation is otherwise supported, but not mapped due to some unspecified limitation
MUL                            10         More than one subgraph is not supported
RESHAPE                        1          More than one subgraph is not supported
RESIZE_BILINEAR                2          Image-interpolation layer won't run precisely enough on Edge TPU
ADD                            3          Mapped to Edge TPU
ADD                            7          More than one subgraph is not supported
CONCATENATION                  1          More than one subgraph is not supported
CONV_2D                        15         Mapped to Edge TPU
CONV_2D                        23         More than one subgraph is not supported
DEPTHWISE_CONV_2D              7          Mapped to Edge TPU
DEPTHWISE_CONV_2D              10         Tensor has unsupported rank (up to 3 innermost dimensions mapped)
BATCH_TO_SPACE_ND              10         Operation not supported
DEQUANTIZE                     1          Operation is working on an unsupported data type
SPACE_TO_BATCH_ND              10         Tensor has unsupported rank (up to 3 innermost dimensions mapped)
MEAN                           1          More than one subgraph is not supported
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='adish333' date='2019-10-23T15:56:40Z'>
		See one of my EdgeTPU testing scripts, here: &lt;denchmark-link:https://github.com/mattroos/EdgeTpuTesting/blob/master/test_edgetpu.ipynb&gt;https://github.com/mattroos/EdgeTpuTesting/blob/master/test_edgetpu.ipynb&lt;/denchmark-link&gt;

It builds, converts (to quantized INT8 model), compiles, and runs a very simple model (nothing but a max pooling layer). It works for me, and I've also successfully tried conv2d, relu, add, and separable conv2d layers. [Note that the output cells in the notebook don't really match the code in the cells, because I edited some of the code and saved the notebook before re-running it. But it does work for me, with layers "Mapped to Edge TPU" and giving meaningful output.]
Obviously this model doesn't do anything useful. It's just confirms that small, simple models can be run on the EdgeTPU.
		</comment>
		<comment id='4' author='adish333' date='2019-10-24T21:06:48Z'>
		&lt;denchmark-link:https://github.com/adish333&gt;@adish333&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/mattroos&gt;@mattroos&lt;/denchmark-link&gt;

Nam from Coral Team here, apologize for the issue. We currently have not supported models with tf2.0 yet. Could you guys run into the same issue if downgrading to tf1.5?
Edit: tf1.15
		</comment>
		<comment id='5' author='adish333' date='2019-10-29T21:53:01Z'>
		I'll give it a try. &lt;denchmark-link:https://github.com/Namburger&gt;@Namburger&lt;/denchmark-link&gt;
, did you really mean tf1.5? Did you possibly mean tf1.15?
		</comment>
		<comment id='6' author='adish333' date='2019-10-29T22:01:50Z'>
		&lt;denchmark-link:https://github.com/mattroos&gt;@mattroos&lt;/denchmark-link&gt;
 sorry, tf1.15
		</comment>
		<comment id='7' author='adish333' date='2019-10-29T23:30:51Z'>
		&lt;denchmark-link:https://github.com/Namburger&gt;@Namburger&lt;/denchmark-link&gt;
, I gave it a try with TF1.15, but can't get past the conversion of the model from the saved keras format to the tflite format. When I instantiate the converter I get warnings, but no errors:
&lt;denchmark-code&gt;&gt;&gt;&gt; converter = tf.lite.TFLiteConverter.from_keras_model_file('model_keras') # TF1.15
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;WARNING:tensorflow:From /home/mroos/python_envs/env_tensorflow1.15/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /home/mroos/python_envs/env_tensorflow1.15/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /home/mroos/python_envs/env_tensorflow1.15/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.
WARNING:tensorflow:From /home/mroos/python_envs/env_tensorflow1.15/lib/python3.6/site-packages/tensorflow_core/lite/python/util.py:249: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
WARNING:tensorflow:From /home/mroos/python_envs/env_tensorflow1.15/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
INFO:tensorflow:Froze 60 variables.
INFO:tensorflow:Converted 60 variables to const ops.
&lt;/denchmark-code&gt;

Then when I call convert(), I get an error that I can't interpret:
&lt;denchmark-code&gt;&gt;&gt;&gt; converter.optimizations = [tf.lite.Optimize.DEFAULT]
&gt;&gt;&gt; converter.representative_dataset = tf.lite.RepresentativeDataset(representative_dataset_gen)
&gt;&gt;&gt; converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8] # For EdgeTPU, no float ops allowed
&gt;&gt;&gt; tflite_model = converter.convert()
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-8-7698de81c241&gt; in &lt;module&gt;
----&gt; 1 tflite_model = converter.convert()
      2 open('model.tflite', 'wb').write(tflite_model)

~/python_envs/env_tensorflow1.15/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py in convert(self)
    991     if self._is_calibration_quantize():
    992       result = self._calibrate_quantize_model(result, inference_input_type,
--&gt; 993                                               inference_output_type)
    994 
    995     return result

~/python_envs/env_tensorflow1.15/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py in _calibrate_quantize_model(self, result, inference_input_type, inference_output_type)
    237     return calibrate_quantize.calibrate_and_quantize(
    238         self.representative_dataset.input_gen, inference_input_type,
--&gt; 239         inference_output_type, allow_float)
    240 
    241   def _get_base_converter_args(self):

~/python_envs/env_tensorflow1.15/lib/python3.6/site-packages/tensorflow_core/lite/python/optimize/calibrator.py in calibrate_and_quantize(self, dataset_gen, input_type, output_type, allow_float)
     73     self._calibrator.Prepare()
     74     for calibration_sample in dataset_gen():
---&gt; 75       self._calibrator.FeedTensor(calibration_sample)
     76     return self._calibrator.QuantizeModel(
     77         np.dtype(input_type.as_numpy_dtype()).num,

~/python_envs/env_tensorflow1.15/lib/python3.6/site-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py in FeedTensor(self, input_value)
    110 
    111     def FeedTensor(self, input_value):
--&gt; 112         return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_FeedTensor(self, input_value)
    113 
    114     def QuantizeModel(self, input_py_type, output_py_type, allow_float):

ValueError: Failed to convert value into readable tensor.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='adish333' date='2019-10-30T15:04:22Z'>
		&lt;denchmark-link:https://github.com/mattroos&gt;@mattroos&lt;/denchmark-link&gt;
 Thanks for trying, any chance you are converting the saved keras model that was created with tf2.0?
If it was created with tf1.15 and you are still having issue converting, then we might need to wait for a tensorflower to help out on this one :/
		</comment>
		<comment id='9' author='adish333' date='2019-10-30T17:30:10Z'>
		&lt;denchmark-link:https://github.com/Namburger&gt;@Namburger&lt;/denchmark-link&gt;
, I used a single script to create, save, and (attempt to) convert the model under TF1.15.
		</comment>
		<comment id='10' author='adish333' date='2019-11-05T04:22:57Z'>
		&lt;denchmark-link:https://github.com/Namburger&gt;@Namburger&lt;/denchmark-link&gt;
 Thanks for the suggestion. Our code worked after downgrading to tf 1.15 and retraining the Keras model.
		</comment>
		<comment id='11' author='adish333' date='2019-11-05T04:23:02Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33558&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33558&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='adish333' date='2019-11-05T05:17:10Z'>
		&lt;denchmark-link:https://github.com/adish333&gt;@adish333&lt;/denchmark-link&gt;
, do you mind sharing the lines of code you used for your working, TF1.15 conversion? E.g., did you set values for these attributes before calling convert()? Others?
&lt;denchmark-code&gt;converter.inference_input_type = tf.float32
converter.inference_output_type = tf.float32
&lt;/denchmark-code&gt;

		</comment>
		<comment id='13' author='adish333' date='2019-11-05T23:10:38Z'>
		&lt;denchmark-link:https://github.com/mattroos&gt;@mattroos&lt;/denchmark-link&gt;
 I met the same issue when I tried to do post-training quantization:
Traceback (most recent call last): File "convert_tf.py", line 183, in &lt;module&gt; convert_pb_lite_quant() File "convert_tf.py", line 80, in convert_pb_lite_quant tflite_quant_model = converter.convert() File "/home/jiatian/.local/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py", line 994, in convert inference_output_type) File "/home/jiatian/.local/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py", line 240, in _calibrate_quantize_model inference_output_type, allow_float) File "/home/jiatian/.local/lib/python3.6/site-packages/tensorflow_core/lite/python/optimize/calibrator.py", line 76, in calibrate_and_quantize self._calibrator.FeedTensor(calibration_sample) File "/home/jiatian/.local/lib/python3.6/site-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py", line 113, in FeedTensor return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_FeedTensor(self, input_value) ValueError: Failed to convert value into readable tensor. 
Did you solve this?
		</comment>
		<comment id='14' author='adish333' date='2019-11-05T23:39:15Z'>
		&lt;denchmark-link:https://github.com/JiatianWu&gt;@JiatianWu&lt;/denchmark-link&gt;
, I think I had been using one of the nightly versions of tf1.15. After switching to tf1.15.0, that error went away.
&lt;denchmark-code&gt;pip install tensorflow==1.15.0
&lt;/denchmark-code&gt;

I still get an error similar that of the original poster, however,
&lt;denchmark-code&gt;RuntimeError: Internal: :71 tf_lite_type != kTfLiteUInt8 (9 != 3)Node number 79 (EdgeTpuDelegateForCustomOp) failed to prepare.
&lt;/denchmark-code&gt;

when I build and run a model in which the input tensor is "too large", even with small models and an input tensor that isn't particularly large (e.g., (256, 256, 3) input into a model with one conv2d layer that has three input channels and 256 output channels).
		</comment>
		<comment id='15' author='adish333' date='2020-01-08T20:35:37Z'>
		Same problem here with input tensors that are too large. &lt;denchmark-link:https://github.com/mattroos&gt;@mattroos&lt;/denchmark-link&gt;
 Did you solve your problem ?
		</comment>
		<comment id='16' author='adish333' date='2020-01-08T20:43:57Z'>
		No, &lt;denchmark-link:https://github.com/alexanderfrey&gt;@alexanderfrey&lt;/denchmark-link&gt;
. I've been told by the coral support team that this a problem with the EdgeTPU runtime code, which is provided by their team of engineers (initial release was version 10, current release is version 12). They claimed it would be fixed in a future release but would not give a release date estimate. I'm not even sure it will be resolved in the next release, versus one further down the road. Hard to plan ahead on my projects with this kind of timeline-less problem.
		</comment>
		<comment id='17' author='adish333' date='2020-02-06T11:50:03Z'>
		There is a new update from the coral team. Please have a look at it, you will find your solution. &lt;denchmark-link:https://coral.ai/news/updates-01-2020/&gt;January 2020 Updates &lt;/denchmark-link&gt;

		</comment>
		<comment id='18' author='adish333' date='2020-02-07T04:41:57Z'>
		I haven't fully tested it, but at first pass, yes, the January updates resolved this. Thanks to the Google team!
		</comment>
	</comments>
</bug>