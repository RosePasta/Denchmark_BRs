<bug id='31291' author='xuevin' open_date='2019-08-02T20:09:41Z' closed_time='2019-08-22T18:52:53Z'>
	<summary>tf.keras.optimizers.SGD with momentum does not fit when model metrics are provided</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04.4 LTS
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.14
Python version: 3.6
CUDA/cuDNN version: CUDA Version: 10.1
GPU model and memory: 1080TI

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
momentum in  tensorflow.keras.optimizers.SGD and metrics in model compilation cannot be used together when using fit_generator. However, each works independently. The problem does not exist when using fit
Describe the expected behavior
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
&lt;denchmark-code&gt;from tensorflow.keras.layers import Embedding, Input, Dense, Lambda
from tensorflow.keras import backend as K
from tensorflow.keras import Model
import numpy as np
from tensorflow import keras
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0"


def get_more():
    while(True):
        yield ({'a_input': np.random.randint(0, 10, (32, 1200))},
               np.random.rand(32, 1))


def build():

    input = Input(shape=(1200,), name='a_input', dtype='int32')
    x = Embedding(input_dim=10,
                  output_dim=4,
                  input_length=1200,
                  trainable=True, name='embedding')(input)
    x = Dense(1, activation='linear')(x)
    x = Lambda(lambda x: K.squeeze(x, axis=-1))(x)
    x = Dense(1, name='output')(x)
    this_model = Model(input, x)

    return this_model


####### FAIL
# Situation 1 (+METRICS +MOMENTUM)
#######
K.clear_session()
this_model = build()
optimizer = keras.optimizers.SGD(lr=0.05, momentum=0.9)
this_model.compile(loss='mse', optimizer=optimizer, metrics=['mse'])
this_model.fit_generator(iter(get_more()), steps_per_epoch=10)

####### PASS
# Situation 2 (+METRICS -MOMENTUM)
#######
K.clear_session()
this_model = build()
optimizer = keras.optimizers.SGD(lr=0.05)
this_model.compile(loss='mse', optimizer=optimizer, metrics=['mse'])
this_model.fit_generator(iter(get_more()), steps_per_epoch=10)

####### PASS
# Situation 3 (-METRICS +MOMENTUM)
#######
K.clear_session()
this_model = build()
optimizer = keras.optimizers.SGD(lr=0.05, momentum=0.9)
this_model.compile(loss='mse', optimizer=optimizer)
this_model.fit_generator(iter(get_more()), steps_per_epoch=10)

####### PASS
# Situation 4 (+METRICS +MOMENTUM)  (fit instead of fit_generator)
#######
K.clear_session()
this_model = build()
optimizer = keras.optimizers.SGD(lr=0.05, momentum=0.9)
this_model.compile(loss='mse', optimizer=optimizer, metrics=['mse'])
x_in = {'a_input': np.random.randint(0,10,(500,1200))}
y_out = np.random.rand(500,1)
this_model.fit(x_in, y_out)
&lt;/denchmark-code&gt;

Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
&lt;denchmark-code&gt;---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1355     try:
-&gt; 1356       return fn(*args)
   1357     except errors.OpError as e:

/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1338       # Ensure any changes to the graph are reflected in the runtime.
-&gt; 1339       self._extend_graph()
   1340       return self._call_tf_sessionrun(

/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in _extend_graph(self)
   1373     with self._graph._session_run_lock():  # pylint: disable=protected-access
-&gt; 1374       tf_session.ExtendSession(self._session)
   1375 

InvalidArgumentError: Cannot assign a device for operation embedding/embeddings/Initializer/random_uniform/sub: Could not satisfy explicit device specification '' because the node {{colocation_node embedding/embeddings/Initializer/random_uniform/sub}} was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:GPU:0'. All available devices [/job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:0, /job:localhost/replica:0/task:0/device:GPU:0]. 
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
ResourceSparseApplyKerasMomentum: CPU 
Identity: GPU CPU XLA_CPU XLA_GPU 
ResourceGather: GPU CPU XLA_CPU XLA_GPU 
AssignVariableOp: GPU CPU XLA_CPU XLA_GPU 
RandomUniform: GPU CPU XLA_CPU XLA_GPU 
VarIsInitializedOp: GPU CPU XLA_CPU XLA_GPU 
Const: GPU CPU XLA_CPU XLA_GPU 
Mul: GPU CPU XLA_CPU XLA_GPU 
ReadVariableOp: GPU CPU XLA_CPU XLA_GPU 
Sub: GPU CPU XLA_CPU XLA_GPU 
VarHandleOp: GPU CPU XLA_CPU XLA_GPU 
Add: GPU CPU XLA_CPU XLA_GPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  embedding/embeddings/Initializer/random_uniform/shape (Const) 
  embedding/embeddings/Initializer/random_uniform/min (Const) 
  embedding/embeddings/Initializer/random_uniform/max (Const) 
  embedding/embeddings/Initializer/random_uniform/RandomUniform (RandomUniform)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0
  embedding/embeddings/Initializer/random_uniform/sub (Sub) 
  embedding/embeddings/Initializer/random_uniform/mul (Mul) 
  embedding/embeddings/Initializer/random_uniform (Add) 
  embedding/embeddings (VarHandleOp)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0
  embedding/embeddings/IsInitialized/VarIsInitializedOp (VarIsInitializedOp)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0
  embedding/embeddings/Assign (AssignVariableOp)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0
  embedding/embeddings/Read/ReadVariableOp (ReadVariableOp)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0
  embedding/embedding_lookup (ResourceGather)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0
  embedding/embedding_lookup/Identity (Identity) 
  VarIsInitializedOp_4 (VarIsInitializedOp)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0
  SGD/embedding/embeddings/momentum/Initializer/zeros (Const) 
  SGD/embedding/embeddings/momentum (VarHandleOp) 
  SGD/embedding/embeddings/momentum/IsInitialized/VarIsInitializedOp (VarIsInitializedOp) 
  SGD/embedding/embeddings/momentum/Assign (AssignVariableOp) 
  SGD/embedding/embeddings/momentum/Read/ReadVariableOp (ReadVariableOp) 
  SGD/SGD/update_embedding/embeddings/ResourceSparseApplyKerasMomentum (ResourceSparseApplyKerasMomentum) 
  VarIsInitializedOp_7 (VarIsInitializedOp) 

	 [[{{node embedding/embeddings/Initializer/random_uniform/sub}}]]

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='xuevin' date='2019-08-05T06:15:57Z'>
		Was able to reproduce the issue with Tensorflow 1.14.0 on Colab. Please take a look at gist &lt;denchmark-link:https://colab.research.google.com/drive/1NQk0_HDfUislaKeFCT77xC7KJ4KHia6h&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='xuevin' date='2019-08-05T06:21:56Z'>
		&lt;denchmark-link:https://github.com/xuevin&gt;@xuevin&lt;/denchmark-link&gt;
 This is not issue with Tenosrflow 2.0.0.beta1 and tf-nightly-2.0-preview. You might want to give it a try instead. Thanks!
		</comment>
		<comment id='3' author='xuevin' date='2019-08-06T17:04:26Z'>
		Thanks! The problem must be limited to 1.14 then. 1.13.* seems to be unaffected.
		</comment>
		<comment id='4' author='xuevin' date='2019-08-22T18:42:53Z'>
		@ I couldn't reproduce the issue with . Please check the &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/b350996a893a58e8a54cad0f3cf1a253/tf_31291_keras_opt.ipynb&gt; gist here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='5' author='xuevin' date='2019-08-22T18:52:53Z'>
		Thanks &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 ! &lt;denchmark-link:https://github.com/xuevin&gt;@xuevin&lt;/denchmark-link&gt;
 it seems to be fixed after 1.14
		</comment>
		<comment id='6' author='xuevin' date='2019-08-22T18:52:54Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=31291&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=31291&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='xuevin' date='2019-08-23T13:19:34Z'>
		&lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 This issue happens with gpu-enabled tensorflow.
		</comment>
		<comment id='8' author='xuevin' date='2019-08-23T16:35:14Z'>
		&lt;denchmark-link:https://github.com/llan-ml&gt;@llan-ml&lt;/denchmark-link&gt;
 Based on error trace &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/31291#issue-476337948&gt;mentioned here&lt;/denchmark-link&gt;
, the  related ops looking for a specific device. So when I assign a device like below, everything worked as expected.
&lt;denchmark-code&gt;with tf.device('/cpu:0'):
  this_model.fit_generator(iter(get_more()), steps_per_epoch=10) 
&lt;/denchmark-code&gt;

When you select gpu as , this will throw same error as follows. Error clearly mentions that the . Please check the &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/b350996a893a58e8a54cad0f3cf1a253/tf_31291_keras_opt.ipynb#scrollTo=_Aw5V87sllwL&gt;gist here&lt;/denchmark-link&gt;
.
&lt;denchmark-code&gt;InvalidArgumentError: Cannot assign a device for operation embedding/embeddings/Initializer/random_uniform/sub: Could not satisfy explicit device specification '' because the node {{colocation_node embedding/embeddings/Initializer/random_uniform/sub}} was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:GPU:0'. All available devices [/job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:0, /job:localhost/replica:0/task:0/device:GPU:0]. 
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
ResourceSparseApplyKerasMomentum: CPU 
&lt;/denchmark-code&gt;

Thanks!
		</comment>
		<comment id='9' author='xuevin' date='2019-08-23T17:33:09Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 Thanks for your explanation. I know there are some ops used by Momentum that do not have gpu implementations. However, it is really a limitation that we cannot try the Momentum optimizer for optimizing a model with GPU.
I do not know the internal implementations of different optimizers, but other optimizers such as Adam and RMSprop work normally on GPUs.
Sorry that I just realized that this may not be related to the topic of this issue.
		</comment>
		<comment id='10' author='xuevin' date='2019-08-23T17:49:18Z'>
		&lt;denchmark-link:https://github.com/llan-ml&gt;@llan-ml&lt;/denchmark-link&gt;
 I think none of the optimizers would work with GPU given sparse. And this is something that we're planning to fix and should probably be done by Nov 2019 -- But contribution welcome if this is urgent to you.
		</comment>
		<comment id='11' author='xuevin' date='2019-08-23T18:11:03Z'>
		&lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;
 Just in my cases, my model can be trained with Adam, but it raises the error when I declare the optimizer with  and without any other code changed. I'm using tf-nightly-gpu-preview and follow the subclassed model style with custom training loops. If needed, I can provide some tiny scripts to reproduce.
		</comment>
		<comment id='12' author='xuevin' date='2019-08-23T18:25:33Z'>
		&lt;denchmark-link:https://github.com/llan-ml&gt;@llan-ml&lt;/denchmark-link&gt;
 Oh sorry I missed that, adam is the only one that would probably work in your case because we haven't made a sparse op yet so every op it uses can be run on GPU.
		</comment>
	</comments>
</bug>