<bug id='43698' author='Harrypotterrrr' open_date='2020-10-01T08:32:47Z' closed_time='2020-10-01T19:54:04Z'>
	<summary>TF2 TensorArray with multi-dimensional tensor wrote in it will be stacked into None shape tensor in Autograph mode</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux lz 5.4.0-48-generic #52~18.04.1-Ubuntu
TensorFlow installed from (source or binary):  (in conda env) pip install tensorflow-gpu -i https://pypi.tuna.tsinghua.edu.cn/simple
TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0
Python version: 3.8
CUDA/cuDNN version: conda install cudatoolkit=10.1 cudnn=7.6.5
GPU model and memory:   4 x Titan xp 12GB

Describe the current behavior
I use tf.TensorArray to dynamically store the specified number of multi-dimensional Tensor, and try to convert the TensorArray into Tensor which will be fed into the next network layer. I use @tf.function decorator to convert pythonic code into TF's AutoGraph mode to accelerate the training process. Specifically, take the following code as an example:
import tensorflow as tf

class TestModel(tf.keras.Model):

    def __init__(self, N):
        super(TestModel, self).__init__()
        self.conv_first = tf.keras.layers.Conv2D(4, (3, 3))
        self.nframe = N

    def __call__(self, x):

        aligned_fea = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)

        def cond(i, N, fea_col):
            return i &lt; N

        def body(i, N, fea_col):
            fea_col = fea_col.write(i, x)
            i = tf.add(i, 1)
            return i, N, fea_col

        _, _, aligned_fea = tf.while_loop(cond, body, [0, self.nframe, aligned_fea])

        tf.print("aliged_fea shape:", aligned_fea.size())

        t = aligned_fea.stack()
        tf.print("t shape:", t.shape)

        # without these two lines of reshaping the stacked tensor coercively, the t will have no first dimension
        tt = tf.reshape(t,[self.nframe, 8, 4, 6,3])
        tf.print("tt shape:", tt.shape)

        return t

@tf.function
def foo(tm):
    x = tf.ones([8,4,6,3], dtype=tf.float32)
    output = tm(x)

nframe = 10
tm = TestModel(nframe)
foo(tm)
The output of the above code is:
&lt;denchmark-code&gt;aliged_fea shape: 10
t shape: TensorShape([None, 8, 4, 6, 3])
tt shape: TensorShape([10, 8, 4, 6, 3])
&lt;/denchmark-code&gt;

Describe the expected behavior
I have tested the code in Eager mode, that is to remove the decorator @tf.function. Everything goes well. But error will be thrown when the Autograph mode starts, since the output tensor doesn't have got the first dimension of itself shape, and can not be feed into the next layer of the network.
Moreover, if I write scalar into TensorArray, whether I use Eager or Autograph mode, the output of the shape of Tensor will be fine with true numbers.
My problem is the TensorArray, whatever it contains scalar or multi-dimensional tensor, should automatically detect the size of itself and stack into the tensor with full shape in both Eager and Graph mode,  and the output should be as follows so to speak:
&lt;denchmark-code&gt;aliged_fea shape: 10
t shape: TensorShape([10, 8, 4, 6, 3])
tt shape: TensorShape([10, 8, 4, 6, 3])
&lt;/denchmark-code&gt;

One workaround to deal with this problem is just to reshape the stacked tensor after TensorArray.stack() but it is not robust for my codes. Thus, I wonder if this is a bug or just the feature to fit something unknown.
I am struggling with this for a long time. Appreciate your reply in advance.
	</description>
	<comments>
		<comment id='1' author='Harrypotterrrr' date='2020-10-01T15:05:37Z'>
		I am able to replicate this issue, please find the &lt;denchmark-link:https://colab.research.google.com/gist/Saduf2019/62222f48460bdd79381394fd274bfb33/untitled418.ipynb&gt;gist here&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='Harrypotterrrr' date='2020-10-01T18:05:03Z'>
		Could you try using a fixed size TensorArray tf.TensorArray(dtype=tf.float32, size=self.nframe)
		</comment>
		<comment id='3' author='Harrypotterrrr' date='2020-10-01T19:50:19Z'>
		&lt;denchmark-link:https://github.com/saxenasaurabh&gt;@saxenasaurabh&lt;/denchmark-link&gt;
 Oh yes! TensorArray with fixed size does work! Thanks for your suggestion. But it is still confusing the reason why TensorArray could not be stacked into the Tensor with explicit dimensions.
		</comment>
		<comment id='4' author='Harrypotterrrr' date='2020-10-01T19:54:04Z'>
		Because the size of the TensorArray depends on the algorithm in the while loop and it is hard to statically infer the size of the TA after the loop.
		</comment>
		<comment id='5' author='Harrypotterrrr' date='2020-10-01T19:54:06Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43698&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43698&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='Harrypotterrrr' date='2020-10-21T03:53:58Z'>
		&lt;denchmark-link:https://github.com/saxenasaurabh&gt;@saxenasaurabh&lt;/denchmark-link&gt;
 Hi what do you think of this problem? &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/44073#issuecomment-713113752&gt;#4403&lt;/denchmark-link&gt;

The problem of TensorArray still exists.
		</comment>
		<comment id='7' author='Harrypotterrrr' date='2020-10-26T19:55:49Z'>
		Possibly a shape inference issue. Once we isolate a minimal repro I can look into a fix.
		</comment>
	</comments>
</bug>