<bug id='19333' author='lawsonfulton' open_date='2018-05-16T20:04:24Z' closed_time='2018-06-15T01:16:14Z'>
	<summary>Bug in EluGradGrad</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, see below
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): binary - latest pip tf_nightly
TensorFlow version (use command below): v1.8.0-1674-gd8fac4cb80 1.9.0-dev20180515
Python version: Python 3.6.3
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: CUDA Version 9.1.85
GPU model and memory: GeForce GTX 970
Exact command to reproduce: See script attached

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

There seems to be an issue with EluGradGrad that I have uncovered. Please see the post here: &lt;denchmark-link:https://github.com/renmengye/tensorflow-forward-ad/issues/2#issuecomment-389321546&gt;renmengye/tensorflow-forward-ad#2 (comment)&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

Running this script will demonstrate the incorrect values:
import tensorflow as tf
import numpy as np

def fwd_gradients(ys, xs, d_xs):
    dummy = tf.zeros_like(ys)
    g = tf.gradients(ys, xs, grad_ys=dummy, name="gradients")
    return tf.gradients(g, dummy, grad_ys=d_xs, name="jvp")

def my_elu(x):
    return tf.where(x &gt;= 0.0, x, tf.exp(x) - 1.0)

def main():
    print(tf.__version__)

    sess = tf.InteractiveSession()
    init = tf.global_variables_initializer()
    
    # activation = my_elu # Works correctly tf.nn.relu (or any other non-elu activation)
    activation = tf.nn.elu

    x_size = 3
    y_size = x_size

    # Single ELU or RELU op
    X = tf.placeholder(tf.float64, shape=[x_size]) # Input
    Y = activation(X) # Output

    # Define vjp and jvp
    Vx = tf.placeholder(tf.float64, shape=[x_size]) # Jac-vector product input V
    Vy = tf.placeholder(tf.float64, shape=[y_size]) # vector-jac product input V
    jvp = fwd_gradients(Y, X, d_xs=Vx)
    vjp = tf.gradients(Y, X, grad_ys=Vy)

    # Compute jacobians
    x = np.ones(x_size) - 1.5 # Bug only occurs in x &lt; 0 region
    # x = np.random.normal(-1, 1, x_size)
    tf_jac, numeric_jac = tf.test.compute_gradient(X, [x_size], Y, [y_size], x_init_value=x)
    vjp_jac = np.array([sess.run(vjp, feed_dict={X: x, Vy: v})[0] for v in np.identity(y_size)])
    jvp_jac = np.array([sess.run(jvp, feed_dict={X: x, Vx: v})[0] for v in np.identity(x_size)])

    # Print results as maximum absolute error
    print("Numeric jac:", numeric_jac)
    print("jvp jac:", jvp_jac)
    print("tf error:", np.max(np.abs(numeric_jac - tf_jac)))   # ~0.0
    print("vjp error:", np.max(np.abs(numeric_jac - vjp_jac))) # ~0.0
    print("jvp error:", np.max(np.abs(numeric_jac - jvp_jac))) # LARGE! for ELU

    sess.close()

if __name__ == '__main__':
    main()
The solution is to edit the implementation of  &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/f318765ad5a50b2fbd7cc08dd4ebc249b3924270/tensorflow/python/ops/nn_grad.py#L364&gt;here&lt;/denchmark-link&gt;
.
I've created a pull request that references this issue.
	</description>
	<comments>
		<comment id='1' author='lawsonfulton' date='2018-06-02T07:13:27Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/tatianashp&gt;@tatianashp&lt;/denchmark-link&gt;
: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='2' author='lawsonfulton' date='2018-06-15T01:16:14Z'>
		&lt;denchmark-link:https://github.com/zero-impact&gt;@zero-impact&lt;/denchmark-link&gt;
 Thank you for submitting PR &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/19334&gt;#19334&lt;/denchmark-link&gt;
!
		</comment>
	</comments>
</bug>