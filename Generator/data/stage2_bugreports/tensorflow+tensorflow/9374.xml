<bug id='9374' author='RuofanKong' open_date='2017-04-21T19:11:28Z' closed_time='2017-04-22T18:10:47Z'>
	<summary>`tensorflow.python.client.device_lib.list_local_devices()` Bug</summary>
	<description>
I am trying to set up GPU configuration for Tensorflow. The step is very simple - Call tensorflow.python.client.device_lib.list_local_devices() to detect the number of gpu devices on the machine, and then set config for Tensorflow.  The following is the code for reproducing:
&lt;denchmark-code&gt;from logging import getLogger

import tensorflow as tf
from tensorflow.python.client import device_lib


log = getLogger(__name__)


def get_available_gpus():
    """ Get available GPU devices info. """
    local_device_protos = device_lib.list_local_devices()
    return [x.name for x in local_device_protos if x.device_type == 'GPU']


def test_gpu_memory_usage():
    # Detect available GPU devices info.
    log.info("On this machine, GPU devices: ", get_available_gpus())

    # Set Tensorflow GPU configuration.
    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.1)
    tf_config=tf.ConfigProto(
        allow_soft_placement=True,
        device_count={'GPU': len(get_available_gpus())},
        gpu_options=gpu_options,
        log_device_placement=True)
    session = tf.Session(config=tf_config)

    # Mimick training process.
    while True:
        pass
        

test_gpu_memory_usage()
&lt;/denchmark-code&gt;

If you run the above code, you could notice that even though you set GPU memory fraction per process to 0.1, it still allocates the whole GPU memory by looking at command nvidia-smi. However, if you don't call get_available_gpus(), the memory allocation works fine. That means, there might be a bug in device_lib.list_local_devices() to prevent setting up Tensorflow GPU memory usage.
PS. My code runs on machine with GPU GeForce GTX 1080, CUDA 8.0, OS Ubuntu 16.04 and Python 3.5, and the above issue could be reproduced using either Tensorflow v.0.12, v.1.0 or v.1.1.
	</description>
	<comments>
		<comment id='1' author='RuofanKong' date='2017-04-22T17:04:37Z'>
		Interesting. Thanks for reporting! If memory serves, getting the list does have side effects (registering the devices).
&lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;
 might know on the top of his head. I'll take a look later.
		</comment>
		<comment id='2' author='RuofanKong' date='2017-04-22T18:03:47Z'>
		Yes, I believe calling  &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/48d9915ebca770b40c4497a13c3edb87b6b042d0/tensorflow/python/client/device_lib.i#L32&gt;invokes the (morally static) DeviceFactory::AddDevices() code&lt;/denchmark-link&gt;
 for the GPU devices, but doesn't take a   and so passes &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/48d9915ebca770b40c4497a13c3edb87b6b042d0/tensorflow/python/client/device_lib.i#L30&gt;an empty SessionOptions&lt;/denchmark-link&gt;
 to the factory.
There's no good reason for that... IIRC we added  in order to be able to print diagnostic information about the available devices when running benchmarks, and none of the benchmarks depended on configuring the device initialization. I don't know if there's a technical limitation in the GPU device code that prevents multiple instantiations, or separating the enumeration from the instantiation. &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/8136&gt;#8136&lt;/denchmark-link&gt;
 contains a feature request for rationalizing the runtime initialization code, so it might be worth chiming in there.
		</comment>
		<comment id='3' author='RuofanKong' date='2017-04-22T18:10:47Z'>
		Thanks &lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;
! Closing in favor of &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/8136&gt;#8136&lt;/denchmark-link&gt;
. I believe that &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/8021&gt;#8021&lt;/denchmark-link&gt;
 has the temporary workaround you need.
		</comment>
	</comments>
</bug>