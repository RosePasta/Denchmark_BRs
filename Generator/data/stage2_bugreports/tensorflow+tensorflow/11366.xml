<bug id='11366' author='andrew-anki' open_date='2017-07-08T03:29:11Z' closed_time='2018-03-31T21:19:49Z'>
	<summary>Problems with AOT-Compiled Inception V3 model (runtime crash / nonsense output)</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS X 10.12.5
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): ('v1.2.0-rc2-21-g12f033d', '1.2.0')
Python version: 2.7.13
Bazel version (if compiling from source): bazel release 0.5.1-homebrew
CUDA/cuDNN version: n/a (CPU only build)
GPU model and memory: n/a
Exact command to reproduce: See description below.

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

I've successfully built the AOT compiler and all its tests pass fine. I'm trying to do the analogous thing as in the matmul example to build an AOT-compiled library for a frozen Inception V3 graph: specifically, inception_v3_2016_08_28_frozen.pb, which I hope to incorporate into my larger C++ project.
The bazel build goes fine, and I get a (large) library and header file. I can successfully compile and link that into my larger project. When I Run() it, however, I get a EXC_BAD_ACCESS on this line of disassembly:
&lt;denchmark-code&gt;0x10143f55f &lt;+143&gt;: movq   (%rax), %rax
&lt;/denchmark-code&gt;

with this stack trace from a call to Run():
&lt;denchmark-code&gt;#0	0x000000010143f55f in Eigen::TensorEvaluator&lt;Eigen::TensorContractionOp&lt;Eigen::array&lt;Eigen::IndexPair&lt;long long&gt;, 1ul&gt; const, Eigen::TensorReshapingOp&lt;Eigen::DSizes&lt;long long, 2&gt; const, Eigen::TensorImagePatchOp&lt;-1l, -1l, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 4, 1, long&gt;, 16, Eigen::MakePointer&gt; const&gt; const&gt; const, Eigen::TensorReshapingOp&lt;Eigen::DSizes&lt;long long, 2&gt; const, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 4, 1, long&gt;, 16, Eigen::MakePointer&gt; const&gt; const&gt; const, Eigen::ThreadPoolDevice&gt;::Context&lt;Eigen::internal::gemm_pack_lhs&lt;float, long, Eigen::internal::TensorContractionSubMapper&lt;float, long, 1, Eigen::TensorEvaluator&lt;Eigen::TensorReshapingOp&lt;Eigen::DSizes&lt;long long, 2&gt; const, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 4, 1, long&gt;, 16, Eigen::MakePointer&gt; const&gt; const, Eigen::ThreadPoolDevice&gt;, Eigen::array&lt;long, 1ul&gt;, Eigen::array&lt;long, 1ul&gt;, 4, true, false, 0, Eigen::MakePointer&gt;, 8, 4, 0, false, false&gt;, Eigen::internal::gemm_pack_rhs&lt;float, long, Eigen::internal::TensorContractionSubMapper&lt;float, long, 0, Eigen::TensorEvaluator&lt;Eigen::TensorReshapingOp&lt;Eigen::DSizes&lt;long long, 2&gt; const, Eigen::TensorImagePatchOp&lt;-1l, -1l, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 4, 1, long&gt;, 16, Eigen::MakePointer&gt; const&gt; const&gt; const, Eigen::ThreadPoolDevice&gt;, Eigen::array&lt;long, 1ul&gt;, Eigen::array&lt;long, 1ul&gt;, 4, true, false, 0, Eigen::MakePointer&gt;, 4, 0, false, false&gt;, Eigen::internal::gebp_kernel&lt;float, float, long, Eigen::internal::blas_data_mapper&lt;float, long, 0, 0&gt;, 8, 4, false, false&gt;, Eigen::internal::TensorContractionInputMapper&lt;float, long, 1, Eigen::TensorEvaluator&lt;Eigen::TensorReshapingOp&lt;Eigen::DSizes&lt;long long, 2&gt; const, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 4, 1, long&gt;, 16, Eigen::MakePointer&gt; const&gt; const, Eigen::ThreadPoolDevice&gt;, Eigen::array&lt;long, 1ul&gt;, Eigen::array&lt;long, 1ul&gt;, 4, true, false, 0, Eigen::MakePointer&gt;, Eigen::internal::TensorContractionInputMapper&lt;float, long, 0, Eigen::TensorEvaluator&lt;Eigen::TensorReshapingOp&lt;Eigen::DSizes&lt;long long, 2&gt; const, Eigen::TensorImagePatchOp&lt;-1l, -1l, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 4, 1, long&gt;, 16, Eigen::MakePointer&gt; const&gt; const&gt; const, Eigen::ThreadPoolDevice&gt;, Eigen::array&lt;long, 1ul&gt;, Eigen::array&lt;long, 1ul&gt;, 4, true, false, 0, Eigen::MakePointer&gt;, Eigen::internal::blas_data_mapper&lt;float, long, 0, 0&gt; &gt;::enqueue_packing_helper(long, long, long, bool) ()
#7	0x00000001012ff70b in __tensorflow_aot_test__aot_test ()
&lt;/denchmark-code&gt;

Furthermore, when I write my own simple binary, separate from my larger project, along the lines of the matmul test example for AOT, I can build and run it without the above crash, but no matter what input I feed in, I get the same output: the 1001-element results vector is all zeros except entry 429, which is exactly 1.0. My guess was that the image data I'm feeding in was somehow garbage, but I've verified (?) that I can read in the same binary blob of pre-processed image data in Matlab and it looks reasonable. (Pre-processing here includes resizing the image to the required size (299x299), dividing each element by 255, and storing as floats.)
Is something going wrong in AOT-compiling the graph here, or am I doing something wrong or missing something totally stupid? Is there something used in the Inception architecture that's not supported? Should I be trying with another frozen graph? Note that I'm on the r1.2 branch, but may try switching to master next to see if it's something that's been changed/fixed since r1.2. See below for source.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

BUILD file for my aot_test library and a simple binary to run it:
&lt;denchmark-code&gt;load("//tensorflow/compiler/aot:tfcompile.bzl", "tf_library")

tf_library(
    name = "aot_test",
    cpp_class = "TF_TestAOT",
    graph = "inception_v3_2016_08_28_frozen.pb",
    config = "aot_test.config.pbtxt",
)

cc_binary(
    name = "my_binary",
    srcs = [
        "my_binary.cc", 
    ],
    deps = [
        ":aot_test",  
        "//third_party/eigen3",
    ],
    # I've tried with or without this
    #linkopts = [
    #      "-lpthread",
    #]
)
&lt;/denchmark-code&gt;

Contents of aot_test.config.pbtxt referenced above:
&lt;denchmark-code&gt;feed {
  id { node_name: "input" }
  shape {
    dim { size: 1   }
    dim { size: 299 }
    dim { size: 299 }
    dim { size: 3   }
  }
}

fetch {
  id { node_name: "InceptionV3/Predictions/Reshape_1" }
}
&lt;/denchmark-code&gt;

Contents of my_binary.cc referenced above:
&lt;denchmark-code&gt;#define EIGEN_USE_THREADS
#define EIGEN_USE_CUSTOM_THREAD_POOL

#include &lt;cstdlib&gt;
#include &lt;iostream&gt;
#include "third_party/eigen3/unsupported/Eigen/CXX11/Tensor"
#include "tensorflow/aot_test/aot_test.h" // generated

int main(int argc, char** argv) {
  Eigen::ThreadPool tp(1);  // Size the thread pool as appropriate. (I've tried various options here)
  Eigen::ThreadPoolDevice device(&amp;tp, tp.NumThreads());

  TF_TestAOT test;
  test.set_thread_pool(&amp;device);

  // Set up args and run the computation. 
  // Printing out the data shows it is valid. I've also tried random input data and multiple images.
  FILE* file = fopen("/tmp/img_norm.bin", "rb");
  const size_t n = fread(test.arg0_data(), sizeof(float), 299*299*3, file); 
  fclose(file);
  std::cout &lt;&lt; "Read " &lt;&lt; n &lt;&lt; " floats" &lt;&lt; std::endl;
  
  test.Run();

  std::cout &lt;&lt; "Status: " &lt;&lt; test.error_msg() &lt;&lt; std::endl;
  
  // Check result
  const float* output_data = test.result0_data();
  float maxScore = -1.f;
  int maxIndex = -1;
  for(int i=0; i&lt;1001; ++i)
  {
     // If I print this, i'll see all zeros except entry 429, which is one
    //std::cout &lt;&lt; "Score[" &lt;&lt; i &lt;&lt; "]=" &lt;&lt; output_data[i] &lt;&lt; std::endl;
    if(output_data[i] &gt; maxScore)
    {
      maxScore = output_data[i];
      maxIndex = i;
    }
  }

  std::cout &lt;&lt; "Max score = " &lt;&lt; maxScore &lt;&lt; " at index " &lt;&lt; maxIndex &lt;&lt; std::endl;

  return 0;
}
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='andrew-anki' date='2017-07-08T03:32:06Z'>
		One other note (since I mentioned I'm on r1.2): I did add this target_llvm_triple() definition to tfcompile.bzl as well, having found that in another issue about using AOT on OSX (which appears to have since been merged to master): x86_64-none-darwin
		</comment>
		<comment id='2' author='andrew-anki' date='2017-07-09T01:37:37Z'>
		&lt;denchmark-link:https://github.com/tatatodd&gt;@tatatodd&lt;/denchmark-link&gt;
, do you have any thoughts?
		</comment>
		<comment id='3' author='andrew-anki' date='2017-07-10T23:28:25Z'>
		Update: I rebuilt from master (pulled this morning) and sent in the same two images I was testing with. It seems to be reporting reasonable results now using my standalone binary. So either I was doing something wrong on my r1.2 branch or the root cause has been fixed.
However, I'm still getting the BAD ACCESS crash when linking into my larger project, so any suggestions there would be much appreciated. I assume you'd need more information about the project to help diagnose what the issue is (threading related perhaps?), but I'm not sure what's most helpful to provide. In the mean time, I'm still investigating...
		</comment>
		<comment id='4' author='andrew-anki' date='2017-07-11T02:19:25Z'>
		Also note that my "larger project" is being built with Xcode. I'm adding all the flags and force_loads I see when running bazel with -s to my build settings in Xcode, but still no luck.
&lt;denchmark-code&gt;bazel-out/darwin_x86_64-opt/bin/tensorflow/aot_test/libaot_test.a 
-Wl,-force_load,bazel-out/darwin_x86_64-opt/bin/tensorflow/compiler/tf2xla/kernels/libgather_op_kernel_float_int32.lo 
-Wl,-force_load,bazel-out/darwin_x86_64-opt/bin/tensorflow/compiler/tf2xla/kernels/libgather_op_kernel_float_int64.lo 
-Wl,-force_load,bazel-out/darwin_x86_64-opt/bin/tensorflow/core/kernels/libgather_functor.lo 
-Wl,-force_load,bazel-out/darwin_x86_64-opt/bin/tensorflow/compiler/tf2xla/kernels/libindex_ops_kernel_argmax_float_1d.lo 
-Wl,-force_load,bazel-out/darwin_x86_64-opt/bin/tensorflow/compiler/tf2xla/kernels/libindex_ops_kernel_argmax_float_2d.lo 
bazel-out/darwin_x86_64-opt/bin/tensorflow/compiler/aot/libruntime.a 
bazel-out/darwin_x86_64-opt/bin/tensorflow/compiler/xla/service/cpu/libruntime_conv2d.a 
bazel-out/darwin_x86_64-opt/bin/tensorflow/compiler/xla/service/cpu/libruntime_matmul.a 
bazel-out/darwin_x86_64-opt/bin/tensorflow/compiler/xla/service/cpu/libruntime_single_threaded_conv2d.a 
bazel-out/darwin_x86_64-opt/bin/tensorflow/compiler/xla/service/cpu/libruntime_single_threaded_matmul.a 
bazel-out/darwin_x86_64-opt/bin/tensorflow/compiler/xla/libexecutable_run_options.a 
-undefined dynamic_lookup 
-headerpad_max_install_names 
-no-canonical-prefixes
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='andrew-anki' date='2017-10-26T09:59:55Z'>
		Hi  &lt;denchmark-link:https://github.com/andrew-anki&gt;@andrew-anki&lt;/denchmark-link&gt;
 , I'm making the practice of compiling inception model into binary file with XLA aot, could you sends me your testing image (img_norm.bin)?
Thanks
		</comment>
		<comment id='6' author='andrew-anki' date='2017-10-27T02:20:14Z'>
		Hi, &lt;denchmark-link:https://github.com/andrew-anki&gt;@andrew-anki&lt;/denchmark-link&gt;
, I try it following your instruction and code,  use it to recognize a tabby cat picture, however the result of classification is incorrect. So do you have solve this problem?
		</comment>
		<comment id='7' author='andrew-anki' date='2017-12-20T19:10:33Z'>
		It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
		</comment>
		<comment id='8' author='andrew-anki' date='2018-01-04T19:21:03Z'>
		It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
		</comment>
		<comment id='9' author='andrew-anki' date='2018-01-24T13:24:47Z'>
		Nagging Awaiting TensorFlower: It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
		</comment>
		<comment id='10' author='andrew-anki' date='2018-02-08T19:32:30Z'>
		Nagging Awaiting TensorFlower: It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
		</comment>
		<comment id='11' author='andrew-anki' date='2018-02-23T14:12:02Z'>
		Nagging Awaiting TensorFlower: It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
		</comment>
		<comment id='12' author='andrew-anki' date='2018-03-10T13:19:50Z'>
		Nagging Awaiting TensorFlower: It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
		</comment>
		<comment id='13' author='andrew-anki' date='2018-03-25T12:41:11Z'>
		Nagging TensorFlower: It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
		</comment>
		<comment id='14' author='andrew-anki' date='2018-03-31T21:19:49Z'>
		Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
	</comments>
</bug>