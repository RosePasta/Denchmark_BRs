<bug id='34987' author='yeyupiaoling' open_date='2019-12-10T08:05:21Z' closed_time='2020-02-25T03:56:35Z'>
	<summary>The quantized model has low accuracy on Android</summary>
	<description>
I use these code https://github.com/tensorflow/models/tree/master/research/slim train mobilenet V2 model. Then I freeze graph mobilenet_v2.pb. The last run this command to get  tensorflow lite model.
bazel build tensorflow/lite/toco:toco

bazel-bin/tensorflow/lite/toco/toco \
  --input_file=/mnt/d/tmp/mobilenet_v2.pb \
  --output_file=/mnt/d/tmp/mobilenet_v2.tflite \
  --inference_type=QUANTIZED_UINT8 \                                                                    
  --input_arrays=input \
  --output_arrays=MobilenetV2/Predictions/Reshape_1 \
  --input_shapes=1,224,224,3 \
  --mean_values=128 \
  --std_values=128 \
  --change_concat_input_ranges=false \
  --allow_custom_ops
imgData = ByteBuffer.allocateDirect(ddims[2] * ddims[3] * 3);

Interpreter.Options options = new Interpreter.Options();
options.setNumThreads(NUM_THREADS);
tflite = new Interpreter(file, options);

    private ByteBuffer getScaledMatrix(Bitmap bitmap) {
        imgData.rewind();
        bitmap.getPixels(intValues, 0, ddims[2], 0, 0, ddims[2], ddims[3]);
        for (int i = 0; i &lt; ddims[2]; ++i) {
            for (int j = 0; j &lt; ddims[3]; ++j) {
                int pixelValue = intValues[i * ddims[2] + j];
                imgData.put((byte) ((pixelValue &gt;&gt; 16) &amp; 0xFF));
                imgData.put((byte) ((pixelValue &gt;&gt; 8) &amp; 0xFF));
                imgData.put((byte) (pixelValue &amp; 0xFF));
            }
        }
        if (bitmap.isRecycled()) {
            bitmap.recycle();
        }
        return imgData;
    }


ByteBuffer inputData = getScaledMatrix(bmp);
byte[][] labelProbArray = new byte[1][NUM_CLASS];
tflite.run(inputData, labelProbArray);
But the accuracy of the results is very low.
The accuracy of the non-quantized model did not decrease.
	</description>
	<comments>
		<comment id='1' author='yeyupiaoling' date='2019-12-20T20:07:54Z'>
		
But the accuracy of the results is very low.
The accuracy of the non-quantized model did not decrease.

Can you be more specific, or quantify this? Have you run the model against ImageNet to get a TopK result? Note that there will generally be some accuracy degradation, but it shouldn't be significant.
		</comment>
		<comment id='2' author='yeyupiaoling' date='2020-01-08T01:13:47Z'>
		&lt;denchmark-link:https://github.com/jdduke&gt;@jdduke&lt;/denchmark-link&gt;


Can you be more specific, or quantify this? Have you run the model against ImageNet to get a TopK result? Note that there will generally be some accuracy degradation, but it shouldn't be significant.


I used tensorflow 1.14
Windows 10

I used my dataset for the training model. On the PC, the accuracy rate is 98 percent. But using the quantization model on Android is only 10 percent accurate.
I use these code https://github.com/tensorflow/models/tree/master/research/slim train mobilenet V2 model.

step 1: python3 train_image_classifier.py --quantize=True
step 2: python3 export_inference_graph.py
step 3: freeze graph

&lt;denchmark-code&gt;bazel build tensorflow/python/tools:freeze_graph

bazel-bin/tensorflow/python/tools/freeze_graph \
  --input_graph=/mnt/d/tmp/mobilenet_v2/mobilenet_v2_inf_graph.pb \
  --input_checkpoint=/mnt/d/tmp/mobilenet_v2/model.ckpt-90000 \
  --input_binary=true \
  --output_graph=/mnt/d/tmp/mobilenet_v2/mobilenet_v2.pb \
  --output_node_names=MobilenetV2/Predictions/Reshape_1
&lt;/denchmark-code&gt;


step 4: transformation quantization model

&lt;denchmark-code&gt;bazel build tensorflow/lite/toco:toco

bazel-bin/tensorflow/lite/toco/toco \
  --input_file=/mnt/d/tmp/mobilenet_v2.pb \
  --output_file=/mnt/d/tmp/mobilenet_v2.tflite \
  --inference_type=QUANTIZED_UINT8 \                                                                    
  --input_arrays=input \
  --output_arrays=MobilenetV2/Predictions/Reshape_1 \
  --input_shapes=1,224,224,3 \
  --mean_values=128 \
  --std_values=128 \
  --change_concat_input_ranges=false \
  --allow_custom_ops
&lt;/denchmark-code&gt;


step 5: use in Android

ByteBuffer imgData = ByteBuffer.allocateDirect(ddims[2] * ddims[3] * 3 * 1);

private ByteBuffer getScaledMatrix(Bitmap bitmap) {
    imgData.rewind();
    bitmap.getPixels(intValues, 0, ddims[2], 0, 0, ddims[2], ddims[3]);
    for (int i = 0; i &lt; ddims[2]; ++i) {
        for (int j = 0; j &lt; ddims[3]; ++j) {
            int pixelValue = intValues[i * ddims[2] + j];
            imgData.put((byte) ((pixelValue &gt;&gt; 16) &amp; 0xFF));
            imgData.put((byte) ((pixelValue &gt;&gt; 8) &amp; 0xFF));
            imgData.put((byte) (pixelValue &amp; 0xFF));
        }
    }
    if (bitmap.isRecycled()) {
        bitmap.recycle();
    }
    return imgData;
}

ByteBuffer inputData = getScaledMatrix(b);
byte[][] labelProbArray = new byte[1][NUM_CLASS];
tflite.run(inputData, labelProbArray);
		</comment>
		<comment id='3' author='yeyupiaoling' date='2020-02-25T03:56:35Z'>
		Use Tensorflow2.x train model.
		</comment>
		<comment id='4' author='yeyupiaoling' date='2020-02-25T03:56:37Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34987&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34987&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>