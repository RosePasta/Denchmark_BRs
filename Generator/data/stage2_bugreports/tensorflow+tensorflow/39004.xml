<bug id='39004' author='nicolas-gervais' open_date='2020-04-29T02:49:44Z' closed_time='2020-05-05T18:16:55Z'>
	<summary>Huber Loss crashes training loop due to data type mismatch</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): somewhat custom
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: laptop
TensorFlow installed from (source or binary):
TensorFlow version (use command below):
Python version: 3.6.10
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source): 7.3.0
CUDA/cuDNN version: None
GPU model and memory: None

Describe the current behavior
Huber Loss crashes the script with the following error message:

TypeError: Input 'y' of 'Mul' Op has type float64 that does not match type float32 of argument 'x'.

That happens even though I cast everything to either tf.float32 or tf.float64 manually. It does work if I put this line
&lt;denchmark-code&gt;tf.keras.backend.set_floatx('float32')
&lt;/denchmark-code&gt;

Or if I remove the original line with float64. Seems to me like setting the global data type fails somewhere. And, I get the following warning that tensors are being re-casted automatically:

WARNING:tensorflow:Layer dense_8 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.

Standalone code to reproduce the issue
&lt;denchmark-code&gt;os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
from sklearn.datasets import load_linnerud
import tensorflow as tf
tf.keras.backend.set_floatx('float64')
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, LSTM, Concatenate

X, y = load_linnerud(return_X_y=True)

data = tf.data.Dataset.from_tensor_slices((X, y)).\
    map(lambda a, b: (tf.divide(a, tf.reduce_max(X, axis=0, keepdims=True)), b))

train_data = data.take(16).shuffle(16).batch(4)
test_data = data.skip(16).shuffle(4).batch(4)


class FullyConnectedNetwork(Model):
    def __init__(self):
        super(FullyConnectedNetwork, self).__init__()
        self.layer1 = Dense(9, input_shape=(3,))
        self.layer2 = LSTM(8, return_sequences=True)
        self.layer3 = Dense(27)
        self.layer4 = Dropout(5e-1)
        self.layer5 = Dense(27)
        self.layer6 = Concatenate()
        self.layer7 = Dense(3)

    def __call__(self, x, *args, **kwargs):
        x = tf.nn.tanh(self.layer1(x))
        y = self.layer2(x)
        x = tf.nn.selu(self.layer3(x))
        x = self.layer4(x)
        x = tf.nn.relu(self.layer5(x))
        x = self.layer6([x, y])
        x = self.layer7(x)
        return x


model = FullyConnectedNetwork()

loss_object = tf.keras.losses.Huber()

train_loss = tf.keras.metrics.Mean()
test_loss = tf.keras.metrics.Mean()

optimizer = tf.keras.optimizers.Adamax()


@tf.function
def train_step(inputs, targets):
    with tf.GradientTape() as tape:
        outputs = model(inputs)
        loss = loss_object(outputs, targets)
        train_loss(loss)

    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))


@tf.function
def test_step(inputs, targets):
    outputs = model(inputs)
    print(outputs.dtype, targets.dtype)
    loss = loss_object(outputs, targets)
    test_loss(loss)


def main():
    train_loss.reset_states()
    test_loss.reset_states()

    for epoch in range(1, 10_000 + 1):
        for x, y in train_data:
            train_step(x, y)

        for x, y in test_data:
            test_step(x, y)

        if epoch % 25 == 0:
            print(f'Epoch: {epoch:&gt;4} Train Loss: {train_loss.result().numpy():.2f} '
                  f'Test Loss: {test_loss.result().numpy():.2f}')


if __name__ == '__main__':
    main()
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/home/nicolas/anaconda3/envs/condaenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py", line 3331, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "&lt;ipython-input-30-b08781047662&gt;", line 86, in &lt;module&gt;
    main()
  File "&lt;ipython-input-30-b08781047662&gt;", line 75, in main
    train_step(x, y)
  File "/home/nicolas/anaconda3/envs/condaenv/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", line 568, in __call__
    result = self._call(*args, **kwds)
  File "/home/nicolas/anaconda3/envs/condaenv/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", line 615, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File "/home/nicolas/anaconda3/envs/condaenv/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", line 497, in _initialize
    *args, **kwds))
  File "/home/nicolas/anaconda3/envs/condaenv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 2389, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File "/home/nicolas/anaconda3/envs/condaenv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 2703, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File "/home/nicolas/anaconda3/envs/condaenv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 2593, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File "/home/nicolas/anaconda3/envs/condaenv/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py", line 978, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "/home/nicolas/anaconda3/envs/condaenv/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", line 439, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File "/home/nicolas/anaconda3/envs/condaenv/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py", line 968, in wrapper
    raise e.ag_error_metadata.to_exception(e)
TypeError: in converted code:
    &lt;ipython-input-20-f2c31267a363&gt;:54 train_step  *
        loss = loss_object(outputs, targets)
    /home/nicolas/anaconda3/envs/condaenv/lib/python3.6/site-packages/tensorflow_core/python/keras/losses.py:126 __call__
        losses = self.call(y_true, y_pred)
    /home/nicolas/anaconda3/envs/condaenv/lib/python3.6/site-packages/tensorflow_core/python/keras/losses.py:221 call
        return self.fn(y_true, y_pred, **self._fn_kwargs)
    /home/nicolas/anaconda3/envs/condaenv/lib/python3.6/site-packages/tensorflow_core/python/keras/losses.py:915 huber_loss
        math_ops.multiply(delta, linear))
    /home/nicolas/anaconda3/envs/condaenv/lib/python3.6/site-packages/tensorflow_core/python/util/dispatch.py:180 wrapper
        return target(*args, **kwargs)
    /home/nicolas/anaconda3/envs/condaenv/lib/python3.6/site-packages/tensorflow_core/python/ops/math_ops.py:334 multiply
        return gen_math_ops.mul(x, y, name)
    /home/nicolas/anaconda3/envs/condaenv/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_math_ops.py:6125 mul
        "Mul", x=x, y=y, name=name)
    /home/nicolas/anaconda3/envs/condaenv/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py:504 _apply_op_helper
        inferred_from[input_arg.type_attr]))
    TypeError: Input 'y' of 'Mul' Op has type float64 that does not match type float32 of argument 'x'.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='nicolas-gervais' date='2020-04-29T13:56:43Z'>
		Was able to reproduce the issue with TF v2.1, &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/6baa93476d84d2fef692b159e39eaaaa/39004.ipynb&gt;TF v2.2.0-rc3&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/869d37d6c2c3909353ce483f01fa5df0/39004-tf-nightly.ipynb&gt;TF-nightly&lt;/denchmark-link&gt;
. Please find the attached gist. Thanks!
		</comment>
		<comment id='2' author='nicolas-gervais' date='2020-05-02T22:19:07Z'>
		Potentially related to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/36790&gt;#36790&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='nicolas-gervais' date='2020-05-03T16:36:19Z'>
		Added a PR &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/39123&gt;#39123&lt;/denchmark-link&gt;
 for the fix of this issue.
		</comment>
		<comment id='4' author='nicolas-gervais' date='2020-05-05T18:16:57Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39004&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39004&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>