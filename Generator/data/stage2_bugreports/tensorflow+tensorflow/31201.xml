<bug id='31201' author='DocDriven' open_date='2019-07-31T15:44:48Z' closed_time='2019-08-06T14:04:28Z'>
	<summary>ERROR: Failed to prepare for TPU. generic::failed_precondition: Custom op already assigned to a different TPU.</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
TensorFlow installed from (source or binary): Docker image latest-gpu-py3
TensorFlow version (use command below): 1.14.0
Python version: 3.6
Bazel version (if compiling from source): -
GCC/Compiler version (if compiling from source): -
CUDA/cuDNN version: 10.1
GPU model and memory: RTX 2080 Ti

I'm trying to compile tflite models with the edgetpu_compiler for usage with the Coral TPU stick. I have two versions of the tflite file. For the first one, I do not want to use the TPU acceleration (to compare the performances). I invoked the TFLiteConverter with the following configuration:
&lt;denchmark-code&gt;converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(keras_file)
converter.inference_input_type = tf.float32
converter.inference_output_type = tf.float32
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()
open(&lt;tflite_file_name&gt;, 'wb').write(tflite_model)
&lt;/denchmark-code&gt;

For the TPU version, I chose the following options:
&lt;denchmark-code&gt;converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(keras_file)
converter.representative_dataset = representative_dataset_gen
converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()
open(&lt;tflite_file_name&gt;, 'wb').write(tflite_model)
&lt;/denchmark-code&gt;

To check the output of these, I used the visualize.py script. The second graph has quantize nodes in it. To my understanding, I have to convert the second tflite file with the edgetpu_compiler (the first one is expectedly NOT convertible, as it is not quantized). This seems to work, as the log file outputs:
&lt;denchmark-code&gt;Edge TPU Compiler version 2.0.258810407
Input: linearmodel_1.14.0_uint8.tflite
Output: linearmodel_1.14.0_uint8_edgetpu.tflite

Operator                       Count      Status

FULLY_CONNECTED                1          Mapped to Edge TPU
QUANTIZE                       2          Mapped to Edge TPU
&lt;/denchmark-code&gt;

However, if I use it with the C++ API, I run into an error when allocating tensors:
&lt;denchmark-code&gt;ERROR: Failed to prepare for TPU. generic::failed_precondition: Custom op already assigned to a different TPU.
ERROR: Node number 0 (edgetpu-custom-op) failed to prepare.
&lt;/denchmark-code&gt;

I have not found a solution to this problem. I can however, load the non-compiled tflite model, although I suspect this does not provide any time savings.
I have uploaded all tflite models, their html visualizations and the edgetpu log with verbosity 10 to a github repo of mine: &lt;denchmark-link:https://github.com/DocDriven/experimental&gt;https://github.com/DocDriven/experimental&lt;/denchmark-link&gt;

Is this issue already known to you?
	</description>
	<comments>
		<comment id='1' author='DocDriven' date='2019-08-02T05:23:41Z'>
		The conversion from tflite to edge_tpu seems right. Can you please give an example of how do you invoke the C++ API?
		</comment>
		<comment id='2' author='DocDriven' date='2019-08-02T07:15:13Z'>
		&lt;denchmark-link:https://github.com/lu-wang-g&gt;@lu-wang-g&lt;/denchmark-link&gt;

Sure, just note:
When assigning a value to the input/output variable with
 interpreter-&gt;typed_tensor&lt;uint8_t&gt;(4)[0] = quant_input;
uint8_t quant_output = interpreter-&gt;typed_tensor&lt;uint8_t&gt;(5)[0];
you have to change the input and output IDs to 0 and 1 respectively, if you want to use the tflite model generated by the compiler. Also, using typed_input_tensor and/or typed_output_tensor results in a Seg Fault.
&lt;denchmark-code&gt;#include &lt;iostream&gt;
#include "tensorflow/lite/interpreter.h"
#include "tensorflow/lite/kernels/register.h"
#include "tensorflow/lite/model.h"
#include "tensorflow/lite/tools/gen_op_registration.h"
#include "edgetpu.h"

int main(int argc, char** argv){

    // check for correct CLI input
    if (argc != 3) {
            std::cout &lt;&lt; "Invalid number of arguments.\nUsage: " &lt;&lt; argv[0] &lt;&lt; " &lt;tflite model&gt; &lt;input value&gt;\n";
            exit(0);
    }

    auto tpu_context = edgetpu::EdgeTpuManager::GetSingleton()-&gt;NewEdgeTpuContext();
    std::unique_ptr&lt;tflite::Interpreter&gt; interpreter;
    tflite::ops::builtin::BuiltinOpResolver resolver;

    // load model
    std::unique_ptr&lt;tflite::FlatBufferModel&gt; model = tflite::FlatBufferModel::BuildFromFile(argv[1]);

    if(!model){
        std::cout &lt;&lt; "Failed to mmap model\n";
        exit(0);
    }

    resolver.AddCustom(edgetpu::kCustomOp, edgetpu::RegisterCustomOp());

    // Build the interpreter
    if (tflite::InterpreterBuilder(*model, resolver)(&amp;interpreter) != kTfLiteOk){
            std::cout &lt;&lt; "Failed to build interpreter\n";
            exit(0);
    }
    interpreter-&gt;SetExternalContext(kTfLiteEdgeTpuContext, tpu_context.get());  

    // Allocate memory
    if(interpreter-&gt;AllocateTensors() != kTfLiteOk){
            std::cout &lt;&lt; "Failed to allocate tensors\n";
            exit(0);
    }

    // Get the input from the CL
    float real_input = strtof(argv[2], NULL);

    // Quantize the inputs
    const std::vector&lt;int&gt;&amp; inputs = interpreter-&gt;inputs();
    TfLiteTensor* input_tensor = interpreter-&gt;tensor(inputs[0]);
    const TfLiteQuantizationParams&amp; input_params = input_tensor-&gt;params;
    uint8_t quant_input = (real_input / input_params.scale) + input_params.zero_point;
    interpreter-&gt;typed_tensor&lt;uint8_t&gt;(4)[0] = quant_input;

    // Invoke the interpreter
    interpreter-&gt;Invoke();

    // Get the output from the model
    uint8_t quant_output = interpreter-&gt;typed_tensor&lt;uint8_t&gt;(5)[0];

    // Dequantize the outputs
    const std::vector&lt;int&gt;&amp; outputs = interpreter-&gt;outputs();
    TfLiteTensor* output_tensor = interpreter-&gt;tensor(outputs[0]);
    const TfLiteQuantizationParams&amp; output_params = output_tensor-&gt;params;
    float real_output = output_params.scale * (quant_output - output_params.zero_point);

    // Free resources
    interpreter.reset();
    tpu_context.reset();

    // Do something with the output
    std::cout &lt;&lt; "Result: " &lt;&lt; real_output &lt;&lt; "\n";

    return 0;
}

&lt;/denchmark-code&gt;

The Makefile for compiling this looks like this (paths are specific to my system):
&lt;denchmark-code&gt;PROG = ./output/tpu_demo
CC = g++
CPPFLAGS = -Wall -I/tensorflow -I./src
LDFLAGS = -L./src -ltensorflow-lite -ledgetpu_arm64 -ldl -lrt -pthread
OBJS = ./src/tpu_demo.o

$(PROG) : $(OBJS)
	$(CC) $(OBJS) $(LDFLAGS) -o $(PROG)
tpu_demo.o :
	$(CC) $(CPPFLAGS) -c ./src/tpu_demo.cpp
clean:
	rm -f core $(PROG) $(OBJS)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='DocDriven' date='2019-08-02T10:17:00Z'>
		I realized that I get sporadically different errors when executing the binary. Most of the time it throws the error I reported in the original post, but sometimes I get the following:
&lt;denchmark-code&gt;ERROR: Failed to prepare for TPU. generic::failed_precondition: Missing raw model data.
ERROR: Node number 0 (edgetpu-custom-op) failed to prepare. 
&lt;/denchmark-code&gt;

I am literally executing the exact same binary over and over again, and get two different error messages. Maybe this helps to track down the bug?!
EDIT: I also realized that the documentation on the &lt;denchmark-link:https://coral.withgoogle.com/docs/edgetpu/api-cpp/&gt;Coral site&lt;/denchmark-link&gt;
 and the example provided by &lt;denchmark-link:https://coral.googlesource.com/edgetpu-native/+/refs/heads/release-chef/libedgetpu/edgetpu.h&gt;edgetpu.h&lt;/denchmark-link&gt;
 are not consistent. I went with the header example, as the other one does not work (OpenDevice does not seem to exist).
		</comment>
		<comment id='4' author='DocDriven' date='2019-08-05T09:09:01Z'>
		&lt;denchmark-link:https://github.com/lu-wang-g&gt;@lu-wang-g&lt;/denchmark-link&gt;

Can you confirm to me if the invocation of the C++ API is correct or not, please?
		</comment>
		<comment id='5' author='DocDriven' date='2019-08-05T18:16:37Z'>
		Hi Sebastian, I don't have quick answer about this issue. I've created an internal bug and assigned it to the Coral team. I'll keep you posted on any updates it may have.
		</comment>
		<comment id='6' author='DocDriven' date='2019-08-05T19:00:27Z'>
		&lt;denchmark-link:https://github.com/lu-wang-g&gt;@lu-wang-g&lt;/denchmark-link&gt;

Thanks for letting me know! I appreciate it!
If I can provide something to help tracking this bug down, let me know as well.
		</comment>
		<comment id='7' author='DocDriven' date='2019-08-06T08:21:07Z'>
		&lt;denchmark-link:https://github.com/DocDriven&gt;@DocDriven&lt;/denchmark-link&gt;
 Can you try to download the latest C/C++ libraries from &lt;denchmark-link:https://coral.googlesource.com/edgetpu/+/refs/heads/release-diploria/libedgetpu/&gt;https://coral.googlesource.com/edgetpu/+/refs/heads/release-diploria/libedgetpu/&lt;/denchmark-link&gt;
 and try with these libraries ?
Additionally, try to use/modify one of our examples from &lt;denchmark-link:https://coral.googlesource.com/edgetpu-native/+/refs/heads/release-diploria/edgetpu/cpp/examples/&gt;https://coral.googlesource.com/edgetpu-native/+/refs/heads/release-diploria/edgetpu/cpp/examples/&lt;/denchmark-link&gt;
 and see if your model runs with it.
		</comment>
		<comment id='8' author='DocDriven' date='2019-08-06T14:04:18Z'>
		&lt;denchmark-link:https://github.com/manoj7410&gt;@manoj7410&lt;/denchmark-link&gt;
 Thanks for the help, I was able to run the compiled model with the new libraries. For anyone interested, I replaced the libs on my system with the provided
&lt;denchmark-code&gt;edgetpu.h
libedgetpu_arm64.so
direct/aarch64-linux_gnu/libedgetpu.so.1
&lt;/denchmark-code&gt;

(Last is just a symbolic link).
		</comment>
		<comment id='9' author='DocDriven' date='2019-08-06T14:04:30Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=31201&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=31201&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>