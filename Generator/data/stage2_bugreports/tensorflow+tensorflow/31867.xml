<bug id='31867' author='amapic' open_date='2019-08-21T21:29:33Z' closed_time='2019-08-23T15:38:32Z'>
	<summary>Bug on last batch of first epoch using TPU with pretrained-model on google colab</summary>
	<description>
Hi,
I am trying to train a mobileNetV2 coming from tf.keras but the training stops on the last batch of the first epoch:
Epoch 1/15
1/2 [==============&gt;...............] - ETA: 10s - loss: 2.7432 - acc: 0.1406
Don't mind about the number of batch, adding some more  changes nothing
Here's the error :
Compilation failure: Asked to propagate a dynamic dimension from hlo %convert.283 = f32[8,80,80,32]{3,2,1,0} convert(f32[8,80,80,32]{3,2,1,0} %add.1), metadata={op_type="FusedBatchNorm" op_name="bn_Conv1_2/FusedBatchNorm"}@{}&lt;denchmark-link:https://github.com/0&gt;@0&lt;/denchmark-link&gt;
 to hlo %clamp.288 = f32[8,80,80,32]{3,2,1,0} clamp(f32[8,80,80,32]{3,2,1,0} %broadcast.286, f32[8,80,80,32]{3,2,1,0} %convert.283, f32[8,80,80,32]{3,2,1,0} %broadcast.287), metadata={op_type="Relu6" op_name="Conv1_relu_2/Relu6"}, which is not implemented.
TPU compilation failed
[[{{node TPUReplicateMetadata_1}}]]
image of the error for more clarity:
&lt;denchmark-link:https://drive.google.com/open?id=1Gkzow3mrUJPACYfjz6E7V3Efj8eO261F&gt;https://drive.google.com/open?id=1Gkzow3mrUJPACYfjz6E7V3Efj8eO261F&lt;/denchmark-link&gt;

Here's my configuration below:
Python 3.6.8 (default, Jan 14 2019, 11:02:34)
[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]] on linux
tensorflow=1.14.0
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2018 NVIDIA Corporation
Built on Sat_Aug_25_21:08:01_CDT_2018
Cuda compilation tools, release 10.0, V10.0.130
Here's the link to my ipynb : &lt;denchmark-link:https://drive.google.com/open?id=1w2VJAH1gIv6-tJms3qrQg8IxSoKS1aw0&gt;https://drive.google.com/open?id=1w2VJAH1gIv6-tJms3qrQg8IxSoKS1aw0&lt;/denchmark-link&gt;

Any idea of what's going on ?
	</description>
	<comments>
		<comment id='1' author='amapic' date='2019-08-22T13:29:17Z'>
		Tell me if you have question about my code.
		</comment>
		<comment id='2' author='amapic' date='2019-08-22T16:44:09Z'>
		&lt;denchmark-link:https://github.com/amapic&gt;@amapic&lt;/denchmark-link&gt;
 Please provide a standalone code to reproduce the issue. Please use any toy dataset if the data you are using is private. Thanks!
		</comment>
		<comment id='3' author='amapic' date='2019-08-23T14:05:13Z'>
		Thanks, I made it works by using cifar10. But the problem is not really solved for me, do you have any idea of what was going on with my own data ? It's regular image.
Furthermore, with cifar10, I now have this result :
Epoch 1/15
2/2 [==============================] - 15s 8s/step
2/2 [==============================] - 15s 8s/step
2/2 [==============================] - 35s 18s/step - loss: 2.7288 - acc: 0.0781 - val_loss: 2.3025 - val_acc: 0.0781
Why is there several line of the same steps ?
Here's my standalone code:
&lt;denchmark-code&gt;from google.colab import drive
drive.mount('/content/drive')
import sys
from PIL import Image

import numpy as np
import tensorflow as tf
import os

(x_train, y_train), (x_test, y_test)=tf.keras.datasets.cifar10.load_data()
img_size = 160
EPOCHS = 15
nb_imag=160

x_train, y_train=x_train[0:128], y_train[0:128]
x_test, y_test=x_test[0:128], y_test[0:128]
from skimage.transform import resize

from sklearn import preprocessing
imgs=[]
imgs_test=[]
for img in x_train:
    im = Image.fromarray(img)
    imgs.append(resize(np.array(im), (img_size, img_size,3), anti_aliasing=False))
    
for img in x_test:
    im = Image.fromarray(img)
    imgs_test.append(resize(np.array(im), (img_size, img_size,3), anti_aliasing=False))
    
IMG_ROWS, IMG_COLS = img_size, img_size
imgs=np.array(imgs)
imgs_test=np.array(imgs_test)
labels=y_train
labels_test=y_test
input_shape = (IMG_ROWS, IMG_COLS, 3)

x_train = imgs.astype('float32')
x_train /= 255
x_test = imgs_test.astype('float32')
x_test /= 255

print('x_train shape:', imgs.shape)

NUM_CLASSES=np.unique(labels).size
print("NUM_CLASSES",NUM_CLASSES)


y_train = tf.keras.utils.to_categorical(labels, NUM_CLASSES)
y_test = tf.keras.utils.to_categorical(labels_test, NUM_CLASSES)

BATCH_SIZE = 64
resolver = tf.distribute.cluster_resolver.TPUClusterResolver('grpc://' + os.environ['COLAB_TPU_ADDR'])
tf.config.experimental_connect_to_host(resolver.master())
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.experimental.TPUStrategy(resolver)

with strategy.scope():
  model = build_model(IMG_ROWS, NUM_CLASSES, 1, 0.5, 3)
  model.compile(
  loss=tf.keras.losses.categorical_crossentropy,
  optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.05),
  metrics=['accuracy'])
  
model.fit(
  x_train,
  y_train,
  batch_size=BATCH_SIZE,
  epochs=EPOCHS,
  verbose=1,
  validation_data=(x_test, y_test))
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='amapic' date='2019-08-23T15:38:31Z'>
		&lt;denchmark-link:https://github.com/amapic&gt;@amapic&lt;/denchmark-link&gt;
 As it is working with cifar data, i am sure it is not a bug related to Tensorflow.
This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at &lt;denchmark-link:https://stackoverflow.com/questions/tagged/tensorflow&gt;Stackoverflow&lt;/denchmark-link&gt;
. There is a big community (including us) to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance.
When you post it in Stackoverflow, please provide atleast partial data of your to find the root-cause.
Thanks!
		</comment>
		<comment id='5' author='amapic' date='2019-08-23T15:38:33Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=31867&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=31867&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>