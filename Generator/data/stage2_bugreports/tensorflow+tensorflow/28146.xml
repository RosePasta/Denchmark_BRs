<bug id='28146' author='holyhao' open_date='2019-04-25T09:21:34Z' closed_time='2020-08-06T23:31:31Z'>
	<summary>Float 16 for training ,storage,and running on tflite CPU and GPU</summary>
	<description>
Since the tensorflow lite supports the GPU on mobile phone. But, it only supports float32 and float 16, the model size with float32 is too large. So, will the float16 be available for training and storage, and for converting to .tflite.
	</description>
	<comments>
		<comment id='1' author='holyhao' date='2019-04-26T16:26:35Z'>
		Someone is actively working on this.  I don't have an exact ETA for this, but should land pretty soon.
		</comment>
		<comment id='2' author='holyhao' date='2020-08-06T23:31:31Z'>
		&lt;denchmark-link:https://github.com/holyhao&gt;@holyhao&lt;/denchmark-link&gt;
 This is a stale issue. If someone is looking for an example to implement  quantization, &lt;denchmark-link:https://www.tensorflow.org/lite/performance/post_training_float16_quant&gt;Here&lt;/denchmark-link&gt;
 is an example described in detail in TF website.
I am closing this issue. Please feel free to reopen if there is any related issue. Thanks!
		</comment>
	</comments>
</bug>