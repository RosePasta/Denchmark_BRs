<bug id='41307' author='VoVAllen' open_date='2020-07-11T14:45:55Z' closed_time='2020-09-02T16:38:18Z'>
	<summary>[DLPack] DLPack doesn't work with int32 on gpu</summary>
	<description>
Reproduce code: &lt;denchmark-link:https://colab.research.google.com/drive/1kwR_ZVutSot7yA7FqPUvCsKgXcirmNLh?pli=1#scrollTo=UI-LuV7EUick&gt;https://colab.research.google.com/drive/1kwR_ZVutSot7yA7FqPUvCsKgXcirmNLh?pli=1#scrollTo=UI-LuV7EUick&lt;/denchmark-link&gt;

Error message on my machine:
2020-07-11 14:42:25.598880: E tensorflow/stream_executor/cuda/cuda_driver.cc:1037] failed to enqueue async memcpy from device to host: CUDA_ERROR_INVALID_VALUE: invalid argument; host dst: 0x1abfd2c0; GPU src: 0x1a027e80; size: 12=0xc
2020-07-11 14:42:25.598954: F tensorflow/core/common_runtime/gpu/gpu_util.cc:291] GPU-&gt;CPU Memcpy failed
It seems the int32 tensor's data pointer is on cpu instead of being on gpu, althought its device is "gpu". Previously as I know there's special case handling for int32 tensor(&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/34071&gt;#34071&lt;/denchmark-link&gt;
), not sure whether this is related.
And I just found I used constant_op.constant to test dlpacks, which means now it only test the case on cpu since tf.constant always place data on host memory
Do you have any idea on this issue? &lt;denchmark-link:https://github.com/sanjoy&gt;@sanjoy&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='VoVAllen' date='2020-07-11T18:45:22Z'>
		Looks like  returns the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/0d7e40f0dba759a54a8ee4e4c2a96e05937e14c2/tensorflow/python/framework/ops.py#L420&gt;device of the operation&lt;/denchmark-link&gt;
 which is not correct for operations with  outputs, like &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/0d7e40f0dba759a54a8ee4e4c2a96e05937e14c2/tensorflow/core/kernels/identity_op.cc#L162&gt;int32 identity&lt;/denchmark-link&gt;
.
We can probably add a method that returns the true devices for a given TFE_TensorHandle (Tensors know how to deallocate themselves so this information is in principle already present), but this is a high impact API change and we need to think this through.
Also, &lt;denchmark-link:https://github.com/VoVAllen&gt;@VoVAllen&lt;/denchmark-link&gt;
 , what should be the behavior of  if it is passed a non-GPU tensor?
		</comment>
		<comment id='2' author='VoVAllen' date='2020-07-11T18:45:34Z'>
		CC &lt;denchmark-link:https://github.com/jaingaurav&gt;@jaingaurav&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='VoVAllen' date='2020-07-13T06:32:14Z'>
		For DLPack, there's a ctx field indicating which device this pointer belongs to.
Two things I think needed:

Be able to get the real device of the pointer

I believe there should be such API in tensorflow now, because operator needs to know the pointer's location for further computation.


Be able to explicitly copy int32 tensor to gpu

Currently our workaround is cast it to uint32 -&gt; copy to gpu -&gt;cast it back to int32



I understand why int32 tensor are on cpu if using tf.constant, but why it needs exception in tf.identity also that still not copying it to gpu?
		</comment>
		<comment id='4' author='VoVAllen' date='2020-07-14T05:10:18Z'>
		

I believe there should be such API in tensorflow now, because operator needs to know the pointer's location for further computation.


I agree, I have filed a Google-internal bug about this.

but why it needs exception in tf.identity also that still not copying it to gpu?

The   kernel &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/ac3456f3ad9ab8af38d933f823aa665358f7c4fe/tensorflow/core/kernels/identity_op.cc#L134&gt;explicitly places&lt;/denchmark-link&gt;
 its input and output on the host, via a  annotation.
		</comment>
		<comment id='5' author='VoVAllen' date='2020-07-25T05:26:44Z'>
		Any updates on this issue?
		</comment>
		<comment id='6' author='VoVAllen' date='2020-07-25T05:45:42Z'>
		Internally we decided that this is a documentation bug.  The physical device of a Tensor can be accessed via the backing_device property, not the device property.
		</comment>
		<comment id='7' author='VoVAllen' date='2020-07-25T09:16:45Z'>
		Is there any API to get the backing_device I can use to fix the bug in dlpack for now? Currently I use 


tensorflow/tensorflow/c/eager/dlpack.cc


         Line 112
      in
      3ace299






 const char* device_name = tensorflow::unwrap(h)-&gt;DeviceName(&amp;status-&gt;status); 




, which seems the op device instead of backing_devicce
		</comment>
		<comment id='8' author='VoVAllen' date='2020-07-27T15:47:41Z'>
		Just call BackingDeviceName instead of DeviceName
		</comment>
		<comment id='9' author='VoVAllen' date='2020-09-02T16:38:20Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41307&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41307&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>