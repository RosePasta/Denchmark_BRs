<bug id='42835' author='andrescodas' open_date='2020-08-31T18:49:03Z' closed_time='2020-09-16T14:30:59Z'>
	<summary>tf.GradientTape().jacobian() triggers retracing</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  dockerhub container 'latest' Digest: 7bc36fe0ca1a051a808122e87f5438614b371263515df4794abef9a78440af8b
TensorFlow version (use command below):
tf.version.GIT_VERSION=v2.3.0-rc2-23-gb36436b087
tf.version.VERSION=2.3.0
Python version: 3.6

Describe the current behavior
The use of tf.GradientTape().jacobian() triggers retracing
Describe the expected behavior
No retracing
Standalone code to reproduce the issue
simple example:
import tensorflow as tf
print('tf.version.GIT_VERSION={}'.format(tf.version.GIT_VERSION))
print('tf.version.VERSION={}'.format(tf.version.VERSION))

x = tf.ones((1,1))
for k in range(0, 10):
    with tf.GradientTape() as tape:
        tape.watch(x)
        y = 2*x
    g = tape.jacobian(y, x)
Other info / logs
&lt;denchmark-code&gt;tf.version.GIT_VERSION=v2.3.0-rc2-23-gb36436b087
tf.version.VERSION=2.3.0
WARNING:tensorflow:5 out of the last 5 calls to &lt;function pfor.&lt;locals&gt;.f at 0x7f89839c8e18&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='andrescodas' date='2020-09-01T10:40:43Z'>
		Was able to reproduce the issue with &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/c9eff0a2cb2a538cfb1fefc01a74d991/42835.ipynb#scrollTo=UtnyeZ10BjsV&gt;TF v2.3&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/24cb215026b64262e2734cf450f17e0f/42835-tf-nightly.ipynb&gt;TF-nightly&lt;/denchmark-link&gt;
. Please find the attached gist. Thanks!
		</comment>
		<comment id='2' author='andrescodas' date='2020-09-03T14:16:42Z'>
		I am encountering the same issue, very curious to understand what is causing this. Using tf.GradientTape().gradient() on the same (x,y) variables works fine.
		</comment>
		<comment id='3' author='andrescodas' date='2020-09-16T14:30:59Z'>
		This is because the jacobian implementation uses parallel for that creates a tf.function under the hood for each jacobian call.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/parallel_for/control_flow_ops.py#L204&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/parallel_for/control_flow_ops.py#L204&lt;/denchmark-link&gt;

You're running the tape.jacobian 10 times and so we'll end up creating 10 tf.functions.
tape.gradient doesn't create a tf.function underneath.
		</comment>
		<comment id='4' author='andrescodas' date='2020-09-16T14:31:01Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42835&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42835&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='andrescodas' date='2020-09-16T17:56:56Z'>
		Hi &lt;denchmark-link:https://github.com/rohan100jain&gt;@rohan100jain&lt;/denchmark-link&gt;
 . Thanks for the explanation.  I guess this is a known issue then, and that is why you are closing this one?  How can I know when is this resolved?  Or should I never use tf.GradientTape().jacobian() within a loop?
		</comment>
		<comment id='6' author='andrescodas' date='2020-10-09T16:25:55Z'>
		&lt;denchmark-link:https://github.com/andrescodas&gt;@andrescodas&lt;/denchmark-link&gt;
, sorry for the late response here but this issue was closed because it is intended behavior. You can wrap the whole code into another tf.function if you intend to avoid recompilation and vectorization rewrites on each call.
		</comment>
		<comment id='7' author='andrescodas' date='2020-10-09T17:10:39Z'>
		&lt;denchmark-link:https://github.com/nikitamaia&gt;@nikitamaia&lt;/denchmark-link&gt;
 , thank you!.

You can wrap the whole code into another tf.function if you intend to avoid recompilation

This was not obvious, and that is why I came up the question.  I got the answer from issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/43710&gt;#43710&lt;/denchmark-link&gt;

This is something that could be documented better, i.e., indicating when  is required to prevent retracing or for performance.  I tend to avoid decorating methods with .  &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/43252#issuecomment-696991585&gt;Here&lt;/denchmark-link&gt;
 I found and instance when wrapping a method with  degrades the performance.
		</comment>
		<comment id='8' author='andrescodas' date='2020-10-14T07:06:51Z'>
		Note that tf.function inside the jacobian is an implementation detail of the jacobian function, as is the retracing that happens internally. The logging message could in fact be suppressed in this case. User supplied code has no tf.function so they should not care about retracing here.
GradietTape code is eagerly building and destroying the tape data structure. This is already inefficient and users should generally try tf.function around such code to get better performance. Jacobian has much bigger overheads than GradientTape, as the function also performs code vectorization eagerly. So the performance delta would be much larger. This could be better reflected in the documentation of jacobian as a performance optimization. This could not be enforced since the code in GradientTape scope may in fact be dynamic and may need re-evaluation of jacobian as well as re-vectorization.
		</comment>
	</comments>
</bug>