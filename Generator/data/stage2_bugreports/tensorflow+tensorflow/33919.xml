<bug id='33919' author='sipposip' open_date='2019-11-01T16:28:39Z' closed_time='2020-03-14T06:47:31Z'>
	<summary>parallel_for: No converter defined for SpaceToBatchND</summary>
	<description>
Computation of jacobian with pfor does not work for  tf.keras.layers.Convolution2D with dilation_rate&gt;1 because there is no converter implemented for SpaceToBatchND
tf-version: 2.1.0-dev20191030
import tensorflow as tf

Nlat=10
Nlon=20
n_channels_in = 1
x = tf.ones((1,Nlat,Nlon,n_channels_in))
layer1 = tf.keras.layers.Convolution2D(32, kernel_size=3, dilation_rate=1)
layer2 = tf.keras.layers.Convolution2D(32, kernel_size=3, dilation_rate=2)


with tf.GradientTape(persistent=True) as gt:
    gt.watch(x)
    y1 = layer1(x)
    y2 = layer2(x)

J = gt.jacobian(y1, x)  # works
J2 = gt.jacobian(y2, x) # fails with following error:
ValueError                                Traceback (most recent call last)
/pfs/nobackup/home/s/sebsc/miniconda3/envs/tf2-env/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py in jacobian(self, target, sources, unconnected_gradients, parallel_iterations, experimental_use_pfor)
   1112         output = pfor_ops.pfor(loop_fn, target_size,
-&gt; 1113                                parallel_iterations=parallel_iterations)
   1114       except ValueError as err:

/pfs/nobackup/home/s/sebsc/miniconda3/envs/tf2-env/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py in pfor(loop_fn, iters, parallel_iterations)
    188     f = function.defun(f)
--&gt; 189   return f()
    190 

/pfs/nobackup/home/s/sebsc/miniconda3/envs/tf2-env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)
   2340     with self._lock:
-&gt; 2341       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
   2342     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access

/pfs/nobackup/home/s/sebsc/miniconda3/envs/tf2-env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   2675       self._function_cache.missed.add(call_context_key)
-&gt; 2676       graph_function = self._create_graph_function(args, kwargs)
   2677       self._function_cache.primary[cache_key] = graph_function

/pfs/nobackup/home/s/sebsc/miniconda3/envs/tf2-env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   2565             override_flat_arg_shapes=override_flat_arg_shapes,
-&gt; 2566             capture_by_value=self._capture_by_value),
   2567         self._function_attributes,

/pfs/nobackup/home/s/sebsc/miniconda3/envs/tf2-env/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    957 
--&gt; 958       func_outputs = python_func(*func_args, **func_kwargs)
    959 

/pfs/nobackup/home/s/sebsc/miniconda3/envs/tf2-env/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in wrapper(*args, **kwargs)
    947             if hasattr(e, "ag_error_metadata"):
--&gt; 948               raise e.ag_error_metadata.to_exception(e)
    949             else:

ValueError: in converted code:
    relative to /pfs/nobackup/home/s/sebsc/miniconda3/envs/tf2-env/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for:

    control_flow_ops.py:183 f  *
        return _pfor_impl(loop_fn, iters, parallel_iterations=parallel_iterations)
    control_flow_ops.py:256 _pfor_impl
        outputs.append(converter.convert(loop_fn_output))
    pfor.py:1280 convert
        output = self._convert_helper(y)
    pfor.py:1460 _convert_helper
        (y_op.type, y_op, converted_inputs))

    ValueError: No converter defined for SpaceToBatchND
    name: "loop_body/SpaceToBatchND"
    op: "SpaceToBatchND"
    input: "loop_body/Reshape_2"
    input: "loop_body/SpaceToBatchND/block_shape"
    input: "loop_body/SpaceToBatchND/paddings"
    attr {
      key: "T"
      value {
        type: DT_FLOAT
      }
    }
    attr {
      key: "Tblock_shape"
      value {
        type: DT_INT32
      }
    }
    attr {
      key: "Tpaddings"
      value {
        type: DT_INT32
      }
    }
    
    inputs: [WrappedTensor(t=&lt;tf.Tensor 'loop_body/Reshape_2/pfor/Reshape:0' shape=(3072, 1, 6, 16, 32) dtype=float32&gt;, is_stacked=True, is_sparse_stacked=False), WrappedTensor(t=&lt;tf.Tensor 'loop_body/SpaceToBatchND/block_shape:0' shape=(2,) dtype=int32&gt;, is_stacked=False, is_sparse_stacked=False), WrappedTensor(t=&lt;tf.Tensor 'loop_body/SpaceToBatchND/paddings:0' shape=(2, 2) dtype=int32&gt;, is_stacked=False, is_sparse_stacked=False)]. 
    Either add a converter or set --op_conversion_fallback_to_while_loop=True, which may run slower


During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
&lt;ipython-input-29-dd023aa8e2b5&gt; in &lt;module&gt;
----&gt; 1 J2 = gt.jacobian(y2, x) # fails

/pfs/nobackup/home/s/sebsc/miniconda3/envs/tf2-env/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py in jacobian(self, target, sources, unconnected_gradients, parallel_iterations, experimental_use_pfor)
   1119                 "jacobian computation. Vectorization can be disabled by setting"
   1120                 " experimental_use_pfor to False."),
-&gt; 1121             sys.exc_info()[2])
   1122     else:
   1123       if context.executing_eagerly() and not self._persistent:

/pfs/nobackup/home/s/sebsc/miniconda3/envs/tf2-env/lib/python3.7/site-packages/six.py in reraise(tp, value, tb)
    690                 value = tp()
    691             if value.__traceback__ is not tb:
--&gt; 692                 raise value.with_traceback(tb)
    693             raise value
    694         finally:

/pfs/nobackup/home/s/sebsc/miniconda3/envs/tf2-env/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py in jacobian(self, target, sources, unconnected_gradients, parallel_iterations, experimental_use_pfor)
   1111       try:
   1112         output = pfor_ops.pfor(loop_fn, target_size,
-&gt; 1113                                parallel_iterations=parallel_iterations)
   1114       except ValueError as err:
   1115         six.reraise(

/pfs/nobackup/home/s/sebsc/miniconda3/envs/tf2-env/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py in pfor(loop_fn, iters, parallel_iterations)
    187   if context.executing_eagerly() or _is_under_xla_context():
    188     f = function.defun(f)
--&gt; 189   return f()
    190 
    191 

/pfs/nobackup/home/s/sebsc/miniconda3/envs/tf2-env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)
   2339     """Calls a graph function specialized to the inputs."""
   2340     with self._lock:
-&gt; 2341       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
   2342     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   2343 

/pfs/nobackup/home/s/sebsc/miniconda3/envs/tf2-env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   2674 
   2675       self._function_cache.missed.add(call_context_key)
-&gt; 2676       graph_function = self._create_graph_function(args, kwargs)
   2677       self._function_cache.primary[cache_key] = graph_function
   2678       return graph_function, args, kwargs

/pfs/nobackup/home/s/sebsc/miniconda3/envs/tf2-env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   2564             arg_names=arg_names,
   2565             override_flat_arg_shapes=override_flat_arg_shapes,
-&gt; 2566             capture_by_value=self._capture_by_value),
   2567         self._function_attributes,
   2568         # Tell the ConcreteFunction to clean up its graph once it goes out of

/pfs/nobackup/home/s/sebsc/miniconda3/envs/tf2-env/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    956                                           converted_func)
    957 
--&gt; 958       func_outputs = python_func(*func_args, **func_kwargs)
    959 
    960       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/pfs/nobackup/home/s/sebsc/miniconda3/envs/tf2-env/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in wrapper(*args, **kwargs)
    946           except Exception as e:  # pylint:disable=broad-except
    947             if hasattr(e, "ag_error_metadata"):
--&gt; 948               raise e.ag_error_metadata.to_exception(e)
    949             else:
    950               raise

ValueError: in converted code:
    relative to /pfs/nobackup/home/s/sebsc/miniconda3/envs/tf2-env/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for:

    control_flow_ops.py:183 f  *
        return _pfor_impl(loop_fn, iters, parallel_iterations=parallel_iterations)
    control_flow_ops.py:256 _pfor_impl
        outputs.append(converter.convert(loop_fn_output))
    pfor.py:1280 convert
        output = self._convert_helper(y)
    pfor.py:1460 _convert_helper
        (y_op.type, y_op, converted_inputs))

    ValueError: No converter defined for SpaceToBatchND
    name: "loop_body/SpaceToBatchND"
    op: "SpaceToBatchND"
    input: "loop_body/Reshape_2"
    input: "loop_body/SpaceToBatchND/block_shape"
    input: "loop_body/SpaceToBatchND/paddings"
    attr {
      key: "T"
      value {
        type: DT_FLOAT
      }
    }
    attr {
      key: "Tblock_shape"
      value {
        type: DT_INT32
      }
    }
    attr {
      key: "Tpaddings"
      value {
        type: DT_INT32
      }
    }
    
    inputs: [WrappedTensor(t=&lt;tf.Tensor 'loop_body/Reshape_2/pfor/Reshape:0' shape=(3072, 1, 6, 16, 32) dtype=float32&gt;, is_stacked=True, is_sparse_stacked=False), WrappedTensor(t=&lt;tf.Tensor 'loop_body/SpaceToBatchND/block_shape:0' shape=(2,) dtype=int32&gt;, is_stacked=False, is_sparse_stacked=False), WrappedTensor(t=&lt;tf.Tensor 'loop_body/SpaceToBatchND/paddings:0' shape=(2, 2) dtype=int32&gt;, is_stacked=False, is_sparse_stacked=False)]. 
    Either add a converter or set --op_conversion_fallback_to_while_loop=True, which may run slower

Encountered an exception while vectorizing the jacobian computation. Vectorization can be disabled by setting experimental_use_pfor to False.
	</description>
	<comments>
		<comment id='1' author='sipposip' date='2019-11-04T08:28:59Z'>
		&lt;denchmark-link:https://github.com/sipposip&gt;@sipposip&lt;/denchmark-link&gt;

I tried reproducing the issue using colab, however i am seeing the below error..Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/66529b5afdb793dc273f35c1a47d4ae4/untitled325.ipynb&gt;here&lt;/denchmark-link&gt;
. Is this the expected behavior?.Thanks!
		</comment>
		<comment id='2' author='sipposip' date='2019-11-05T09:14:41Z'>
		The expected behaviour is to get the same result as when not using pfor:
J2 = gt.jacobian(y2, x, experimental_use_pfor=False) # expected behaviour
which gives a tensor of shape [1, 6, 16, 32, 1, 10, 20, 1] (filled with random numbers since the network in this minimal example has random weights)
		</comment>
		<comment id='3' author='sipposip' date='2019-11-06T11:11:40Z'>
		I have tried on colab with TF version 2.1.0-dev20191103 and was able to reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/1cf3040cd25084f0414ff1385820fe58/untitled325.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='4' author='sipposip' date='2020-03-14T06:47:31Z'>
		There is now a converter for SpaceToBatchND.
		</comment>
		<comment id='5' author='sipposip' date='2020-03-14T06:47:33Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33919&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33919&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>