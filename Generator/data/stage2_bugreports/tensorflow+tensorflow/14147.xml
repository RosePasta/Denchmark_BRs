<bug id='14147' author='mmuneebs' open_date='2017-11-01T06:22:53Z' closed_time='2018-01-25T00:14:13Z'>
	<summary>tf.cast zeroes out input tensor when GPU does not have any free memory</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution: Linux Ubuntu 16.04
TensorFlow installed from (source or binary): tf-nightly-gpu binary from 10/31/2017
TensorFlow version (use command below): ('v1.3.0-rc1-4007-gc44f67a', '1.5.0-dev20171031')
Python version: 2.7.12
Bazel version (if compiling from source): N/A
CUDA/cuDNN version: CUDA v8, cuDNN v6
GPU model and memory: 1080 Ti (11 GB)
Exact command to reproduce:

x = tf.cast(tensor, tf.float32)
x = tf.to_float(tensor)
Above commands return a tensor with all values of tensor set to zero WHEN:
The gpu is in full use by another tensorflow process. I'm trying to cast the tensor from dtype=tf.uint16 to tf.float32, using above commands, but whenever this runs while the gpu is in full use (i.e. I get a CUDA out-of-memory error), the program completes execution normally but the casting commands set the tensor to zeros.
I upgraded from an older nightly-gpu binary to the current one, but didn't help. User should not try to use an already in-use gpu, but this error should at least be communicated to the user.
If I set the gpu to a different gpu with available memory, or if I hide all gpus to force it to use CPU, I do not observe this issue.
	</description>
	<comments>
		<comment id='1' author='mmuneebs' date='2017-11-06T19:02:00Z'>
		Thanks for the report,
&lt;denchmark-link:https://github.com/yzhwang&gt;@yzhwang&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
 have been investigating some issues with multiple processes and the GPU memory allocator and might have some more detail to share.
In the mean time, if possible, could you package up detailed instructions to reproduce the problem - ideally a .py  file that we could run (perhaps twice, one for each process) that reproduces the issue?
Thanks.
		</comment>
		<comment id='2' author='mmuneebs' date='2017-11-06T19:27:08Z'>
		Hi &lt;denchmark-link:https://github.com/mmuneebs&gt;@mmuneebs&lt;/denchmark-link&gt;
 Thanks for reporting the issue. See my comment for another issue here: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/14169#issuecomment-342256834&gt;#14169 (comment)&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='mmuneebs' date='2017-12-20T19:20:35Z'>
		It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
		</comment>
		<comment id='4' author='mmuneebs' date='2018-01-04T19:14:15Z'>
		It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
		</comment>
		<comment id='5' author='mmuneebs' date='2018-01-23T23:16:45Z'>
		A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.
		</comment>
		<comment id='6' author='mmuneebs' date='2018-01-25T00:14:13Z'>
		Close this. Please refer to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/14169#issuecomment-355498651&gt;#14169 (comment)&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>