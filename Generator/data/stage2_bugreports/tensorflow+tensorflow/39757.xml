<bug id='39757' author='jomjol' open_date='2020-05-21T13:50:40Z' closed_time='2020-06-15T16:26:33Z'>
	<summary>TfLite Micro - Conv2D layer not running on ESP32 board</summary>
	<description>
@tensorflow/micro
System information

Host OS Platform and Distribution:
Compiling for ESP32: PlattformIO w/ ESP-IDF
Creating TfLite: Windows 10 w/ Anaconda and Python 3.7,
TensorFlow installed from (source or binary):
Compiling C++: tflite/micro from hello-world example
TtLite creation: Python pip in a Anaconda environment with Python 3.7
Tensorflow version (commit SHA if source): Tensorflow 2.2
Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): ESP32CAM (ESP-IDF Compiler)

Describe the problem
Using a tflite-Model with a Conv2D layer results in a crash of the c++ code running on an ESP32 system. Just exchanging the layer to a MaxPool2D layer let the model run smoothly. This gave me the idea, that the problem is in using the Conv2D layer rather than the c++ code or model training.
The model structure looks like following:
&lt;denchmark-code&gt;model = Sequential()
model.add(InputLayer(input_shape=(32,20,3)))
model.add(Conv2D(8, (3, 3)))
# model.add(MaxPool2D(pool_size=(2,2)))
model.add(Flatten())
model.add(Dense(11, activation = "softmax"))
&lt;/denchmark-code&gt;

The error on the ESP32 monitoring is the following
&lt;denchmark-code&gt;Input loaded.
Guru Meditation Error: Core  0 panic'ed (LoadProhibited). Exception was unhandled.
Core 0 register dump:
PC      : 0x40089191  PS      : 0x00060033  A0      : 0x80089913  A1      : 0x3ffb2f90
A2      : 0x3ffb3094  A3      : 0x00000000  A4      : 0x00060021  A5      : 0x000000fe
A6      : 0x00000001  A7      : 0x00000000  A8      : 0x00000000  A9      : 0x3ffb34f8
A10     : 0x00000003  A11     : 0x00060023  A12     : 0x00060021  A13     : 0x000000fe
A14     : 0x0000002a  A15     : 0x3ffb5370  SAR     : 0x0000001f  EXCCAUSE: 0x0000001c
EXCVADDR: 0x00000050  LBEG    : 0x4008e610  LEND    : 0x4008e63e  LCOUNT  : 0x00000000
Core 0 was running in ISR context:
EPC1    : 0x40089191  EPC2    : 0x00000000  EPC3    : 0x00000000  EPC4    : 0x00000000
&lt;/denchmark-code&gt;

The model training is done in a Jypiter Notebook on a Windows 10 system and the tflite file is created with the following conversion sequence:
&lt;denchmark-code&gt;name = "conv2d"
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
open(name + ".tfl", "wb").write(tflite_model)
&lt;/denchmark-code&gt;

The coding for the ESP32 is done in C++ in ESP-IDF. The tflite micro library is copied from the hello-world example with the ESP-IDF compiler running on PlattformIO.
The simplified c++ code is:
&lt;denchmark-code&gt;...  INCLUDES ...

extern "C" void app_main() {
    static tflite::ErrorReporter* error_reporter = nullptr;
    const tflite::Model* model = nullptr;
    static tflite::MicroInterpreter* interpreter = nullptr;
    TfLiteTensor* output = nullptr;     
    static tflite::ops::micro::AllOpsResolver *resolver; 
    static tflite::MicroOpResolver&lt;5&gt; micro_op_resolver;
    int kTensorArenaSize = 128 * 1024;
    uint8_t *tensor_arena = new uint8_t[kTensorArenaSize];
    TfLiteStatus allocate_status;
    error_reporter = new tflite::MicroErrorReporter;

    CAccessSDClass accessSD;    
//    model = tflite::GetModel(accessSD.ReadFileToCharArray("/sdcard/maxpool.tfl"));
    model = tflite::GetModel(accessSD.ReadFileToCharArray("/sdcard/conv2d.tfl"));    
    printf("Model loaded.\n");       

    micro_op_resolver.AddBuiltin(tflite::BuiltinOperator_RESHAPE,
                                 tflite::ops::micro::Register_RESHAPE());
    micro_op_resolver.AddBuiltin(tflite::BuiltinOperator_CONV_2D,
                                 tflite::ops::micro::Register_CONV_2D());
    micro_op_resolver.AddBuiltin(tflite::BuiltinOperator_FULLY_CONNECTED,
                                 tflite::ops::micro::Register_FULLY_CONNECTED());
    micro_op_resolver.AddBuiltin(tflite::BuiltinOperator_SOFTMAX,
                                 tflite::ops::micro::Register_SOFTMAX());
    micro_op_resolver.AddBuiltin(tflite::BuiltinOperator_MAX_POOL_2D,
                                 tflite::ops::micro::Register_MAX_POOL_2D());
    interpreter = new tflite::MicroInterpreter(model, micro_op_resolver, tensor_arena, kTensorArenaSize, error_reporter);

    allocate_status = interpreter-&gt;AllocateTensors();

    CImageBasis cib("/sdcard/zif0.jpg");
    float* input_data_ptr = interpreter-&gt;typed_tensor&lt;float&gt;(0);
    for (int x = 0; x &lt; cib.width; ++x)
        for (int y = 0; y &lt; cib.height; ++y)
            for (int ch = 0; ch &lt; cib.channels; ++ch)
            {
                *(input_data_ptr) = (float) cib.GetPixelColor(x, y, ch);
                input_data_ptr++;
            }
    printf("Input loaded.\n");

    interpreter-&gt;Invoke();
    printf("Invoke Done.\n");    

    output = interpreter-&gt;output(0);
    for (int i = 0; i &lt; 11; ++i)
    {
        printf("Result %d: %f\n", i, output-&gt;data.f[i]);  
    }    
}
&lt;/denchmark-code&gt;

If the Conv2D layer is exchanged by a MaxPooling layer the model is running technically smoothl on exactly the same c++ code. The only change in the model is commenting the Conv2D layer and activation the MaxPool2D layer:
&lt;denchmark-code&gt;model.add(InputLayer(input_shape=(32,20,3)))
# model.add(Conv2D(8, (3, 3)))
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Flatten())
model.add(Dense(11, activation = "softmax"))
&lt;/denchmark-code&gt;

Then the result is as expected w/o any error:
&lt;denchmark-code&gt;Input loaded.
Invoke Done.
Result 0: 0.000000
Result 1: 0.000000
Result 2: 0.000000
Result 3: 0.000000
Result 4: 0.000000
Result 5: 0.000000
Result 6: 0.000000
Result 7: 0.000000
Result 8: 0.000000
Result 9: 0.000000
Result 10: 1.000000
&lt;/denchmark-code&gt;

Any idea or hint where the problem is?
Remark
This model is not doing any usefull anymore. I have reduced the model and the c++ code to a minimum for reproducing the problem. Final target is a image classification on a ESP32 with the inbuild camera. For the problem I need a dedicated CNN with several Conv2D layers.
	</description>
	<comments>
		<comment id='1' author='jomjol' date='2020-05-23T11:34:49Z'>
		I could debug the error into the function call: TfLiteStatus MicroInterpreter::Invoke() in the file micro_interpreter.cc.
There it happens at the following step registration-&gt;invoke(...):
&lt;denchmark-code&gt;    if (registration-&gt;invoke) {
      TfLiteStatus invoke_status = registration-&gt;invoke(&amp;context_, node);
&lt;/denchmark-code&gt;

Maybe this helps.
		</comment>
		<comment id='2' author='jomjol' date='2020-06-05T13:36:34Z'>
		I have a strong hint to the root cause!
It looks like, as if the Conv2D implementatin has in internal limit controlled by the paramter kMaxChannels in the file conv.cc. If I increase this from 1024 to 4096, I get a running code.
constexpr int kMaxChannels = 4096;
Anybody who can confirm this? Is there a plan, to check this parameter at the modul loading and give an error message in case of too big layers?
		</comment>
		<comment id='3' author='jomjol' date='2020-06-15T16:26:33Z'>
		I could confirm my idea. The problem is the variable kMaxChannels. This was just too small for my purpouse. I would suggest to put this in to size it on the fly or minimum give an error message in case of "too small"
		</comment>
		<comment id='4' author='jomjol' date='2020-06-15T16:26:34Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39757&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39757&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>