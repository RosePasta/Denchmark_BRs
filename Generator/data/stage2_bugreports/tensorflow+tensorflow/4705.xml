<bug id='4705' author='darrengarvey' open_date='2016-10-01T14:18:48Z' closed_time='2017-06-16T18:07:28Z'>
	<summary>Error building with cuda after bazel server dies</summary>
	<description>
This is effectively reopening &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/4105&gt;#4105&lt;/denchmark-link&gt;
 as this is another way to reproduce the issue that I am seeing repeatedly. The fix for &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/4105&gt;#4105&lt;/denchmark-link&gt;
 was to make sure after running configure clean+fetch is run.
I see this issue almost every day so I'm not sure if bazel flushes some caches or the daemon is dying, but the issue reliably reproduces by killing the bazel daemon. Having to re-run configure and restart the build from scratch is unreasonable, since there is a workaround that avoids a rebuild so this looks very much like a bug.
&lt;denchmark-code&gt;$ bazel version 
.
Build label: 0.3.1
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Jul 29 09:09:52 2016 (1469783392)
Build timestamp: 1469783392
Build timestamp as int: 1469783392
&lt;/denchmark-code&gt;

Steps to reproduce and "fix":
NB: $bazel_build_dir is something like ~/.cache/bazel/_bazel_foo/6e3dd6b174494dc75d93de11da03e7e7
&lt;denchmark-code&gt;# Run ./configure, enabling GPU support.

# The crosstool BUILD file looks good now and building with --config=cuda works
tensorflow$ cat $bazel_build_dir/external/local_config_cuda/crosstool/BUILD
licenses(["restricted"])```

package(default_visibility = ["//visibility:public"])

cc_toolchain_suite(
    name = "toolchain",
    toolchains = {
        "local|compiler": ":cc-compiler-local",
        "darwin|compiler": ":cc-compiler-darwin",
    },
)

cc_toolchain(
    name = "cc-compiler-local",
    all_files = ":empty",
    compiler_files = ":empty",
    cpu = "local",
    dwp_files = ":empty",
    dynamic_runtime_libs = [":empty"],
    linker_files = ":empty",
    objcopy_files = ":empty",
    static_runtime_libs = [":empty"],
    strip_files = ":empty",
    supports_param_files = 0,
)

cc_toolchain(
    name = "cc-compiler-darwin",
    all_files = ":empty",
    compiler_files = ":empty",
    cpu = "darwin",
    dwp_files = ":empty",
    dynamic_runtime_libs = [":empty"],
    linker_files = ":empty",
    objcopy_files = ":empty",
    static_runtime_libs = [":empty"],
    strip_files = ":empty",
    supports_param_files = 0,
)

filegroup(
    name = "empty",
    srcs = [],
)

# Backup the crosstool directory so it can be restored later
mkdir -p /tmp/crosstool
cp -aR $bazel_build_dir/external/local_config_cuda/crosstool/* /tmp/crosstool/

# Kill bazel server.
kill $(ps aux | awk '/baze[l]/ {print $2; exit}')
tensorflow$ bazel build -c opt --config=cuda //tensorflow/tools/...
.
ERROR: $bazel_build_dir/external/local_config_cuda/crosstool/BUILD:4:1: Traceback (most recent call last):
    File "$bazel_build_dir/external/local_config_cuda/crosstool/BUILD", line 4
        error_gpu_disabled()
    File "$bazel_build_dir/external/local_config_cuda/crosstool/error_gpu_disabled.bzl", line 3, in error_gpu_disabled
        fail("ERROR: Building with --config=c...")
ERROR: Building with --config=cuda but TensorFlow is not configured to build with GPU support. Please re-run ./configure and enter 'Y' at the prompt to build with GPU support.
Unhandled exception thrown during build; message: Unrecoverable error while evaluating node 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@808aaea9' (requested by nodes 'CONFIGURATION_COLLECTION:com.google.devtools.build.lib.skyframe.ConfigurationCollectionValue$ConfigurationCollectionKey@9a9c82e5', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@66ba3b8e', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@97095481')
INFO: Elapsed time: 0.543s
java.lang.RuntimeException: Unrecoverable error while evaluating node 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@808aaea9' (requested by nodes 'CONFIGURATION_COLLECTION:com.google.devtools.build.lib.skyframe.ConfigurationCollectionValue$ConfigurationCollectionKey@9a9c82e5', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@66ba3b8e', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@97095481')
    at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:1070)
    at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:474)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: com.google.devtools.build.lib.packages.NoSuchTargetException: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by $bazel_build_dir/external/local_config_cuda/crosstool/BUILD
    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.getCrosstoolProtofromBuildFile(CrosstoolConfigurationLoader.java:179)
    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.findCrosstoolConfiguration(CrosstoolConfigurationLoader.java:239)
    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.readCrosstool(CrosstoolConfigurationLoader.java:281)
    at com.google.devtools.build.lib.rules.cpp.CppConfigurationLoader.createParameters(CppConfigurationLoader.java:128)
    at com.google.devtools.build.lib.rules.cpp.CppConfigurationLoader.create(CppConfigurationLoader.java:73)
    at com.google.devtools.build.lib.rules.cpp.CppConfigurationLoader.create(CppConfigurationLoader.java:48)
    at com.google.devtools.build.lib.skyframe.ConfigurationFragmentFunction.compute(ConfigurationFragmentFunction.java:78)
    at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:1016)
    ... 4 more
Caused by: com.google.devtools.build.lib.packages.NoSuchTargetException: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by $bazel_build_dir/external/local_config_cuda/crosstool/BUILD
    at com.google.devtools.build.lib.packages.Package.makeNoSuchTargetException(Package.java:559)
    at com.google.devtools.build.lib.packages.Package.getTarget(Package.java:543)
    at com.google.devtools.build.lib.skyframe.SkyframePackageLoaderWithValueEnvironment.getTarget(SkyframePackageLoaderWithValueEnvironment.java:71)
    at com.google.devtools.build.lib.skyframe.ConfigurationFragmentFunction$ConfigurationBuilderEnvironment.getTarget(ConfigurationFragmentFunction.java:193)
    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.getCrosstoolProtofromBuildFile(CrosstoolConfigurationLoader.java:177)
    ... 11 more
java.lang.RuntimeException: Unrecoverable error while evaluating node 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@808aaea9' (requested by nodes 'CONFIGURATION_COLLECTION:com.google.devtools.build.lib.skyframe.ConfigurationCollectionValue$ConfigurationCollectionKey@9a9c82e5', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@66ba3b8e', 'CONFIGURATION_FRAGMENT:com.google.devtools.build.lib.skyframe.ConfigurationFragmentValue$ConfigurationFragmentKey@97095481')
    at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:1070)
    at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:474)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: com.google.devtools.build.lib.packages.NoSuchTargetException: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by $bazel_build_dir/external/local_config_cuda/crosstool/BUILD
    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.getCrosstoolProtofromBuildFile(CrosstoolConfigurationLoader.java:179)
    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.findCrosstoolConfiguration(CrosstoolConfigurationLoader.java:239)
    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.readCrosstool(CrosstoolConfigurationLoader.java:281)
    at com.google.devtools.build.lib.rules.cpp.CppConfigurationLoader.createParameters(CppConfigurationLoader.java:128)
    at com.google.devtools.build.lib.rules.cpp.CppConfigurationLoader.create(CppConfigurationLoader.java:73)
    at com.google.devtools.build.lib.rules.cpp.CppConfigurationLoader.create(CppConfigurationLoader.java:48)
    at com.google.devtools.build.lib.skyframe.ConfigurationFragmentFunction.compute(ConfigurationFragmentFunction.java:78)
    at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:1016)
    ... 4 more
Caused by: com.google.devtools.build.lib.packages.NoSuchTargetException: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by $bazel_build_dir/external/local_config_cuda/crosstool/BUILD
    at com.google.devtools.build.lib.packages.Package.makeNoSuchTargetException(Package.java:559)
    at com.google.devtools.build.lib.packages.Package.getTarget(Package.java:543)
    at com.google.devtools.build.lib.skyframe.SkyframePackageLoaderWithValueEnvironment.getTarget(SkyframePackageLoaderWithValueEnvironment.java:71)
    at com.google.devtools.build.lib.skyframe.ConfigurationFragmentFunction$ConfigurationBuilderEnvironment.getTarget(ConfigurationFragmentFunction.java:193)
    at com.google.devtools.build.lib.rules.cpp.CrosstoolConfigurationLoader.getCrosstoolProtofromBuildFile(CrosstoolConfigurationLoader.java:177)
    ... 11 more

# So now the crosstool dir has been trashed
tensorflow$ cat $bazel_build_dir/external/local_config_cuda/crosstool/BUILD

load("//crosstool:error_gpu_disabled.bzl", "error_gpu_disabled")

error_gpu_disabled()

# Eh? I didn't disable it! Bazel just died...
# Let's restore from backup
cp -aR /tmp/crosstool/* $bazel_build_dir/external/local_config_cuda/crosstool/
# And restart the bazel server... Almost... Just doing "bazel version" isn't enough; the server restarts but at the next build, the same issue occurs and we need to restore the crosstool directory again.
# Running this seems to do the trick:
tensorflow$ bazel fetch //tensorflow/...
# But the crosstool directory has been trashed again
tensorflow$ cat $bazel_build_dir/external/local_config_cuda/crosstool/BUILD

load("//crosstool:error_gpu_disabled.bzl", "error_gpu_disabled")

error_gpu_disabled()

# Restore once more again
cp -aR /tmp/crosstool/* $bazel_build_dir/external/local_config_cuda/crosstool/
# Now we can build fine!
&lt;/denchmark-code&gt;

In short, the workaround whenever the bazel server dies is, assuming you've backed up your crosstool directory in /tmp/crosstool/:
&lt;denchmark-code&gt;bazel fetch //tensorflow/...
cp -aR /tmp/crosstool/* $bazel_build_dir/external/local_config_cuda/crosstool/
&lt;/denchmark-code&gt;

This looks like a bug to me and the resolution of &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/4105&gt;#4105&lt;/denchmark-link&gt;
 just doesn't seem related to this, it just coincidentally fixes it.
	</description>
	<comments>
		<comment id='1' author='darrengarvey' date='2016-10-01T14:42:32Z'>
		Unfortunately the simple workaround isn't enough as eventually the build fails with the error:
&lt;denchmark-code&gt;ERROR: /data/software/machine-learning/tensorflow/tensorflow/stream_executor/BUILD:5:1: C++ compilation of rule '//tensorflow/stream_executor:stream_executor' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 115 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
tensorflow/stream_executor/cuda/cuda_blas.cc:22:36: fatal error: cuda/include/cublas_v2.h: No such file or directory
compilation terminated.
&lt;/denchmark-code&gt;

Still, just killing the bazel server shouldn't mean that the build should have to be done from scratch, surely?
		</comment>
		<comment id='2' author='darrengarvey' date='2016-10-04T21:53:41Z'>
		&lt;denchmark-link:https://github.com/davidzchen&gt;@davidzchen&lt;/denchmark-link&gt;
 Do you have any thoughts on this?
		</comment>
		<comment id='3' author='darrengarvey' date='2016-10-05T01:14:02Z'>
		It seems that if I copy not just the crosstool directory but the whole local_config_cuda directory (ie. the parent), then the rebuild works.
So something is trashing local_config_cuda.
I'm happy to try out various things if it'll help. I'm not sure exactly when this started but it was around the time I updated to cuda v8 - it was the RC at the time, installed on Ubuntu 15.04 via the deb installer but I'm now on Ubuntu 16.04 and the official cuda 8 deb taken from the nvidia site. Whether any of that is relevant is debatable - I tend to keep tensorflow up to date so that could have been the trigger.
		</comment>
		<comment id='4' author='darrengarvey' date='2016-10-23T12:55:20Z'>
		I have the same problem here. It hinders me from developing TensorFlow as it is really expensive to rebuild from scratch (1000s for a 24 core machine!).
Bazel Version
Build label: 0.3.2
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Oct 7 17:25:10 2016 (1475861110)
Build timestamp: 1475861110
Build timestamp as int: 1475861110

Error message I encounter when I try to build again several hours after the successful build.
WARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.
ERROR: /home/ybw/.cache/bazel/_bazel_ybw/36e6bc7e62e4ec6066c451267a1c5751/external/local_config_cuda/crosstool/BUILD:4:1: Traceback (most recent call last):
        File "/home/ybw/.cache/bazel/_bazel_ybw/36e6bc7e62e4ec6066c451267a1c5751/external/local_config_cuda/crosstool/BUILD", line 4
                error_gpu_disabled()
        File "/home/ybw/.cache/bazel/_bazel_ybw/36e6bc7e62e4ec6066c451267a1c5751/external/local_config_cuda/crosstool/error_gpu_disabled.bzl", line 3, in error_gpu_disabled
                fail("ERROR: Building with --config=c...")
ERROR: Building with --config=cuda but TensorFlow is not configured to build with GPU support. Please re-run ./configure and enter 'Y' at the prompt to build with GPU support.
ERROR: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by /home/ybw/.cache/bazel/_bazel_ybw/36e6bc7e62e4ec6066c451267a1c5751/external/local_config_cuda/crosstool/BUILD.

		</comment>
		<comment id='5' author='darrengarvey' date='2016-10-23T13:34:09Z'>
		I just came across this issue after doing a reboot. Suddenly, I got the same error as darrengarvey. Looking into tensorflow/third_party/gpus/cuda_configure.bzl points out that a dummy repository is created 8_create_dummy_repository) when TF_NEED_CUDA=1 is not set. Setting and adding it to my .bashrc solves this for me.
		</comment>
		<comment id='6' author='darrengarvey' date='2016-10-25T03:53:33Z'>
		Confirming that &lt;denchmark-link:https://github.com/firolino&gt;@firolino&lt;/denchmark-link&gt;
 's solution alone was sufficient to resolve the  problem without rebuilding Tensorflow.

However after I tried to load the custom_op.so with , I got the  error. I tried to hack through it but nothing worked.Therefore, I ended up reconfigure with ./configure at Tensorflow's root directory expecting to wait for another 20 minutes of rebuilding process, but this time the building process of the custom_op.so only took about 90 s, weird.
		</comment>
		<comment id='7' author='darrengarvey' date='2016-11-06T18:46:14Z'>
		&lt;denchmark-link:https://github.com/firolino&gt;@firolino&lt;/denchmark-link&gt;
 - You're right that it seems like  needs to be exported in the environment for building with cuda to work. I don't quite see where bazel is picking this environment variable up.
Seems it's just in at third_party/gpus/cuda_configure.bzl:118: which is loaded via tf_workspace() in WORKSPACE. I guess while the bazel server is alive it's doing some caching of the workspace which is why this doesn't show up all of the time.
So it looks a bit like a user (like me) actually needs to export this environment variable in order to build reliably, which isn't mentioned in any docs that I can see. I see TF_NEED_CUDA=1 is manually put into the environment in tensorflow/tools/ci_build/Dockerfile.gpu
An alternative could be to store these variables in a local config file and reference them when loading tf_workspace().
This doesn't look like it's needed for the other variables like TF_NEED_GCP or TF_NEED_HDFS because those values are written to tensorflow/core/platform/default/build_config.bzl. Following the pattern there is a one easy way to fix this, but the disadvantage of doing that is you end up with changes to files that are part of the source tree (a minor inconvenience).
		</comment>
		<comment id='8' author='darrengarvey' date='2016-11-06T19:17:35Z'>
		Well it's not quite that simple unfortunately. I fixed the first issue (see the &lt;denchmark-link:https://github.com/darrengarvey/tensorflow/tree/fix-cuda-failure-when-bazel-dies&gt;branch&lt;/denchmark-link&gt;
), but the next issue is down to all of the other environment variables that are expected to exist, such as  and .
All fixable, but really it looks like either I'm missing something or the assumptions being made in configure regarding these environment variables aren't reasonable (namely, that bazel never dies). I haven't tried yet but the same issues possibly exist with other variables like HOST_CXX_COMPILER, TF_NEED_SYCL, etc. based on a brief look at how they are being used.
		</comment>
		<comment id='9' author='darrengarvey' date='2016-11-08T03:05:06Z'>
		+cc &lt;denchmark-link:https://github.com/damienmg&gt;@damienmg&lt;/denchmark-link&gt;

Sorry for the late reply.
&lt;denchmark-link:https://github.com/darrengarvey&gt;@darrengarvey&lt;/denchmark-link&gt;
 Currently, the way the settings get propagated from  to  is that the  script exports a bunch of environment variables and then runs  before it exits. When this  invocation is run, it picks up the environment variables and sets up the  workspace according to those settings. Then, when you run , because all of the external repositories have already been fetched, then TF will be built with  set up based on the settings given to the  script.
When the Bazel server is killed in this case, this is equivalent to running the configure script without it running bazel fetch and then running bazel fetch or bazel build without any of the settings from the configure script, which results in @local_config_cuda set up without GPU support.
The eventual fix for this is to move the remaining CUDA configuration logic currently in the  script into the  rule so that it will attempt to detect GPU support and the CUDA toolchain and runtime by default. &lt;denchmark-link:https://github.com/damienmg&gt;@damienmg&lt;/denchmark-link&gt;
 is working on &lt;denchmark-link:https://bazel-review.googlesource.com/c/6697/3/site/designs/_posts/2016-10-18-repository-invalidation.md&gt;improvements to Skylark remote repositories&lt;/denchmark-link&gt;
 to both improve its caching invalidation strategy and enable passing environment variables to workspace rules via command line options. This way, we will be able to make  work more similar to the way the Autotools configure invocation works, where it will run its detection logic by default, but users will be able to override whether to build with GPU support or provide a custom CUDA installation directory via command line flags.
		</comment>
		<comment id='10' author='darrengarvey' date='2017-01-03T13:08:20Z'>
		I compiled Tensorflow on my Jetson TX1 for around 4 hours. Then the compilation was finished without compile errors, but it shows a strange error, that some artifacts are missing.
I want to start compile process again with higher verbosity and suddenly I get the error:
&lt;denchmark-code&gt;ERROR: Building with --config=cuda but TensorFlow is not configured to build with GPU support. Please re-run ./configure and enter 'Y' at the prompt to build with GPU support..
&lt;/denchmark-code&gt;

Which is complete nonsense, since I really switched on CUDA support during configure. And of course I will not run configure again, since I don't want to wait another 4 hours until the error with missing artifacts happens again.
Is there any chance to restart the compile process where it stopped last time... with CUDA support?
		</comment>
		<comment id='11' author='darrengarvey' date='2017-01-04T00:17:00Z'>
		&lt;denchmark-link:https://github.com/ZahlGraf&gt;@ZahlGraf&lt;/denchmark-link&gt;
 I believe it is the same issue with &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/4848&gt;#4848&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='12' author='darrengarvey' date='2017-01-04T12:52:03Z'>
		Hi, thanks for your answer.
I did not find a method to recover from this situation, thus I had to wait again 4h.
However I found a workaround, which helps me to recover from this issue in past situations:
Before calling ./configure I just set several environment variables in my shell:
&lt;denchmark-code&gt;export TF_NEED_CUDA=1
export TF_CUDA_VERSION=8.0
export CUDA_TOOLKIT_PATH=/usr/local/cuda
export TF_CUDNN_VERSION=5.1.5
export CUDNN_INSTALL_PATH=/usr/lib/aarch64-linux-gnu/
export TF_CUDA_COMPUTE_CAPABILITIES=5.3
&lt;/denchmark-code&gt;

With this setup, I was able to rebuild as many times as I want without running configure again.
Here you can find some more informations about compiling TF-0.11 on Jetson TX1, the settings from above are also from this page:
&lt;denchmark-link:https://github.com/jetsonhacks/installTensorFlowTX1&gt;https://github.com/jetsonhacks/installTensorFlowTX1&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='darrengarvey' date='2017-01-19T15:02:26Z'>
		&lt;denchmark-link:https://www.tensorflow.org/get_started/os_setup&gt;https://www.tensorflow.org/get_started/os_setup&lt;/denchmark-link&gt;
 should be updated to tell people that bazel must not be run with --batch (and the bazel server not be killed or restarted in any other way) between configure and build. Also, this means that one must not switch to another build machine between these 2 steps.
Related: issues &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/4105&gt;#4105&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/4841&gt;#4841&lt;/denchmark-link&gt;

		</comment>
		<comment id='14' author='darrengarvey' date='2017-04-04T18:15:39Z'>
		&lt;denchmark-link:https://github.com/jowagner&gt;@jowagner&lt;/denchmark-link&gt;
 While these are valid points, we hope these will go away with future bazel releases.
I think one of the problems should be already fixed by bazel 0.4.5
&lt;denchmark-link:https://github.com/damienmg&gt;@damienmg&lt;/denchmark-link&gt;
 can confirm
		</comment>
		<comment id='15' author='darrengarvey' date='2017-04-04T20:15:39Z'>
		With 0.4.5 and the --action_env change, there should be no issue about --batch or restarting the server (unless the change got reverted again). --action_env is basically fixing those environment variable in the rc file instead of relying on the environment to stick.
		</comment>
	</comments>
</bug>