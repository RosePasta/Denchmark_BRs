<bug id='38626' author='WenguoLi' open_date='2020-04-17T07:53:41Z' closed_time='2020-10-13T17:45:53Z'>
	<summary>Add Post-training integer quantization converter for depth_to_space</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
TensorFlow installed from (source or binary): binary
TensorFlow version (or github SHA if from source): 1.15.2 / 2.1

Command used to run the converter or code if you’re using the Python API
If possible, please share a link to Colab/Jupyter/any notebook.
&lt;denchmark-code&gt;    model_name = args.model_name + "_integer_quant" +"_%d" %(args.input_width) + ".tflite"
    converter.representative_dataset = representative_data_gen
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.inference_input_type = tf.uint8
    converter.inference_output_type = tf.uint8
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
    
    tflite_model_quant = converter.convert()
    tflite_model_quant_file = tflite_models_dir/model_name
    tflite_model_quant_file.write_bytes(tflite_model_quant)
    print('Convert using full integer quantization DONE !!!')
&lt;/denchmark-code&gt;

The output from the converter invocation
&lt;denchmark-code&gt;Note the output min/max is different from the input min/max for op RESIZE_BILINEAR at index 29 in subgraph 0. This is legal but should happens rarely.
Note the output min/max is different from the input min/max for op RESIZE_BILINEAR at index 68 in subgraph 0. This is legal but should happens rarely.
Traceback (most recent call last):
  File "gen_quan_tflite.py", line 122, in &lt;module&gt;
    main()    
  File "gen_quan_tflite.py", line 116, in main
    tflite_model_quant = converter.convert()
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py", line 993, in convert
    inference_output_type)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py", line 239, in _calibrate_quantize_model
    inference_output_type, allow_float)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/optimize/calibrator.py", line 78, in calibrate_and_quantize
    np.dtype(output_type.as_numpy_dtype()).num, allow_float)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py", line 115, in QuantizeModel
    return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_QuantizeModel(self, input_py_type, output_py_type, allow_float)
RuntimeError: Quantization not yet supported for op: DEPTH_TO_SPACE
&lt;/denchmark-code&gt;

RuntimeError: Quantization not yet supported for op: DEPTH_TO_SPACE
Also, please include a link to the saved model or GraphDef
&lt;denchmark-code&gt;# Put link here or attach to the issue.
&lt;/denchmark-code&gt;

Failure details
If the conversion is successful, but the generated model is wrong,
state what is wrong:

Producing wrong results and/or decrease in accuracy
Producing correct results, but the model is slower than expected (model generated from old converter)

Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='WenguoLi' date='2020-04-20T12:08:49Z'>
		&lt;denchmark-link:https://github.com/WenguoLi&gt;@WenguoLi&lt;/denchmark-link&gt;
  In order to expedite the trouble-shooting process, could you please provide the complete code along with the dataset to reproduce the issue reported here. Thanks!
		</comment>
		<comment id='2' author='WenguoLi' date='2020-04-21T03:09:55Z'>
		Hi saikumarchalla,
I think the problem is  already obvious that  the Quantization of depth_to_space is not supported in TFLiteConverter.  Just add support, do it. OK?  Thanks!
		</comment>
		<comment id='3' author='WenguoLi' date='2020-04-24T08:49:24Z'>
		Hi saikumarchalla,
Do you have any plans to support integer quantization of depth_to_space  in the  TFLiteConverter?
Thanks
wenguo
		</comment>
		<comment id='4' author='WenguoLi' date='2020-04-27T06:06:21Z'>
		It seems that the kernel of depth_to_space has supported int8 quanzation,  here:



tensorflow/tensorflow/lite/kernels/depth_to_space.cc


         Line 108
      in
      4ce6a9b






 case kTfLiteInt8: 





Only  not  supported on the TFLiteConverter.
any updates ?
Thanks
		</comment>
		<comment id='5' author='WenguoLi' date='2020-06-11T08:44:19Z'>
		I got same issue, I am using TensorFlow 2.2.0
When I use python API to convert  tflite model, show below log，as I know, "toco" is the old tflite converter, and not used for now. I tried to set "converter.experimental_new_converter = True", still show below log, seems the new converter is not enbaled, is there any way to solve this ?
2020-06-11 16:35:50.293990: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 59 operators, 89 arrays (0 quantized)
2020-06-11 16:35:50.294485: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 59 operators, 89 arrays (0 quantized)
2020-06-11 16:35:50.295003: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 19 operators, 48 arrays (0 quantized)
2020-06-11 16:35:50.295232: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 19 operators, 48 arrays (0 quantized)
2020-06-11 16:35:50.295468: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:329] Total transient array allocated size: 58982400 bytes, theoretical optimal value: 39321600 bytes.
2020-06-11 16:35:50.295735: F tensorflow/contrib/lite/toco/tflite/export.cc:315] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need custom implementations:  DEPTH_TO_SPACE.
		</comment>
		<comment id='6' author='WenguoLi' date='2020-09-29T17:09:23Z'>
		&lt;denchmark-link:https://github.com/WenguoLi&gt;@WenguoLi&lt;/denchmark-link&gt;

Is this still an issue, could you please try on the latest tf version and update.
		</comment>
		<comment id='7' author='WenguoLi' date='2020-10-06T17:11:35Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='8' author='WenguoLi' date='2020-10-13T17:45:51Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='9' author='WenguoLi' date='2020-10-13T17:45:57Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38626&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38626&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>