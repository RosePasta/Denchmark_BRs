<bug id='37736' author='jiunoh' open_date='2020-03-20T07:16:35Z' closed_time='2020-03-20T22:23:40Z'>
	<summary>questions about transformer.ipynb tutorial</summary>
	<description>
Hi. I am new to transformer and I'm trying to understand this transformer tutorial (&lt;denchmark-link:https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/transformer.ipynb&gt;https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/transformer.ipynb&lt;/denchmark-link&gt;
).
In this tutorial, a transformer has encoders, decoders, and a final linear layer.
But in the paper, a transformer has a softmax layer after the final linear layer.
I think that predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32) line correctly returns the  expected output anyway, but I just want to know why the softmax layer is not implemented in this tutorial. If I add a softmax layer after the final linear layer and train the model, will the prediction result be different?
And why this tutorial used two separate vocabs instead of a shared subwords vocabulary? (maybe to keep the example simple?) If I want to use a shared vocabulary, should I implement the shared_embedding_and_softmax_weights part in tensor2tensor?
Thanks.
	</description>
	<comments>
		<comment id='1' author='jiunoh' date='2020-03-20T22:23:40Z'>
		The TensorFlow team decided to ban softmax layers from tensorflow.org, unless there's a really good reason.
&lt;denchmark-link:https://github.com/tensorflow/docs/commit/0775cca21a59d7f4c8f0a61a9baca1a8f5eb38e1&gt;tensorflow/docs@0775cca&lt;/denchmark-link&gt;

Basically, TensorFlow has to patch your network to prevent overflows if you try to calculate crossentropy from a softmax output. this leads to surprising behavior when the patching cannot be applied. So it's cleaner to just never do that, and use a Crossentropy(from_logits=True).
The argmax still gets the correct result.
One-vocab or two? I don't know.
		</comment>
	</comments>
</bug>