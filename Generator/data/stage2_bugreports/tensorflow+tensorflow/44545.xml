<bug id='44545' author='oXwvdrbbj8S4wo9k8lSN' open_date='2020-11-03T10:41:48Z' closed_time='2020-11-05T17:37:15Z'>
	<summary>Keras: custom data validation callback on training data always returns validation data results</summary>
	<description>
Just for information: I already asked for help regarding this problem on &lt;denchmark-link:https://stackoverflow.com/questions/64645579/keras-custom-data-validation-callback-on-training-data-always-returns-validatio&gt;stackoverflow&lt;/denchmark-link&gt;
. Since I did not get an answer yet, I assume that this is a non-trivial problem and therefore open an issue here.
System information
python -c "import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)" returned
v2.3.0-54-gfcc4b966f1 2.3.1
Describe the current behavior
When calling model.evaluate on training data (or any other data set) within a callback, always the results of the validation data set are returned.
Describe the expected behavior
I expected to get results for the data set passed to model.evaluate - no matter if called from within a callback or not.

I created a notebook that shows the problem:
&lt;denchmark-link:https://colab.research.google.com/drive/1H-3ULqyRZCpaasXpU1foLkEg12fYNkYK?usp=sharing&gt;https://colab.research.google.com/drive/1H-3ULqyRZCpaasXpU1foLkEg12fYNkYK?usp=sharing&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='oXwvdrbbj8S4wo9k8lSN' date='2020-11-04T18:43:52Z'>
		I am able to replicate the issue reported,please find the &lt;denchmark-link:https://colab.research.google.com/gist/Saduf2019/34c1ed837a6ba2a1e704799f4fe146a9/untitled458.ipynb&gt;gist here&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='2' author='oXwvdrbbj8S4wo9k8lSN' date='2020-11-04T23:11:51Z'>
		&lt;denchmark-link:https://github.com/oXwvdrbbj8S4wo9k8lSN&gt;@oXwvdrbbj8S4wo9k8lSN&lt;/denchmark-link&gt;
 May be I am missing something. I checked your colab. Data looks different and the results (losses) are also different as expected. Can you please check the &lt;denchmark-link:https://colab.research.google.com/gist/jvishnuvardhan/cdb65957379a264c3121b2543595a5da/untitled458.ipynb&gt;gist here&lt;/denchmark-link&gt;
 where I printed different datasets and results. Thanks!
		</comment>
		<comment id='3' author='oXwvdrbbj8S4wo9k8lSN' date='2020-11-05T09:00:10Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 I do not totally get the concept of gists. Therefore, please excuse if I state the obvious.
Between the colab notebook and the gist, something in the behavior of Keras/Tensorflow changed. In my notebook (with tensorflow version 2.3.0), the output of the history is
&lt;denchmark-code&gt;{'custom_loss': [0.7624925374984741, 0.5331208109855652],
 'loss': [0.9665887951850891, 0.6637843251228333],
 'val_loss': [0.7624925374984741, 0.5331208109855652]}
&lt;/denchmark-code&gt;

Here, the results for custom_loss and val_loss are the same although they should differ since the inputs differ. In the gist (currently tensorflow version 2.5.0-dev20201104), however, the output is
&lt;denchmark-code&gt;{'custom_loss': [0.7694963216781616, 0.541864812374115],
 'loss': [0.9665887951850891, 0.6637843251228333],
 'val_loss': [0.7624925374984741, 0.5331208109855652]}
&lt;/denchmark-code&gt;

Here, the outputs differ, which is the behavior I expected. I tested different versions of tensorflow and it seems that the change happened between version 2.3.1 and version 2.4.0rc0. Therefore, I guess that it is actually a bug but you already fixed it at least in the development versions.
		</comment>
		<comment id='4' author='oXwvdrbbj8S4wo9k8lSN' date='2020-11-05T09:15:43Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 I tested version 2.2.1 and also with this version, the outputs differ (as expected). It seems that the problem exists only in versions 2.3.0 and 2.3.1.
		</comment>
		<comment id='5' author='oXwvdrbbj8S4wo9k8lSN' date='2020-11-05T17:37:15Z'>
		&lt;denchmark-link:https://github.com/oXwvdrbbj8S4wo9k8lSN&gt;@oXwvdrbbj8S4wo9k8lSN&lt;/denchmark-link&gt;
 May be there was a bug in  but I just checked with most recent  and  and both showed different results for  and .
Please use recent TF versions. In near future there will be stable TF2.4 which works better than TF2.3.
I am closing this issue as this was resolved in recent versions. If you see the same issue with recent version, feel free to reopen the issue. Thanks!
		</comment>
		<comment id='6' author='oXwvdrbbj8S4wo9k8lSN' date='2020-11-05T17:37:19Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44545&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44545&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>