<bug id='35336' author='CNOCycle' open_date='2019-12-22T05:19:30Z' closed_time='2020-08-07T06:00:44Z'>
	<summary>No clear document explains how to use pre-trained model</summary>
	<description>
&lt;denchmark-h:h2&gt;URL(s) with the issue:&lt;/denchmark-h&gt;

Please provide a link to the documentation entry, for example:
&lt;denchmark-link:https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/keras/applications&gt;https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/keras/applications&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;Description of issue (what needs changing):&lt;/denchmark-h&gt;

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 in Docker
TensorFlow installed from (source or binary): pip install
TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38
Python version: 3.5
CUDA/cuDNN version: 10.0 / 7
GPU model and memory: GTX 1080Ti / 11175MiB

Describe the current behavior
Hi authors and developers,
I noticed that tensorflow doesn't provide a clear document explain how to use pre-trained model.
So, I wrote a benchmark which showed the accuracy of pre-trained model with applying imageNet' validation set.
The following is the result:
&lt;denchmark-code&gt;[Testing][pixel vales are from (0,255)][model:ResNet50] - loss: 2.711 - accuracy: 0.457
[Testing][pixel vales are from (0,255)][model:DenseNet121] - loss: 39.000 - accuracy: 0.006
[Testing][pixel vales are from (0,255)][model:MobileNetV2] - loss: 9.979 - accuracy: 0.003

[Testing][pixel vales are from (0,1)][model:ResNet50] - loss: 8.535 - accuracy: 0.001
[Testing][pixel vales are from (0,1)][model:DenseNet121] - loss: 1.895 - acc: 0.599
[Testing][pixel vales are from (0,1)][model:MobileNetV2] - loss: 2.283 - accuracy: 0.523

[Testing][pixel vales are normalized from (-1,1)][model:ResNet50] - loss: 8.313 - acc: 0.001
[Testing][pixel vales are normalized from (-1,1)][model:DenseNet121] - loss: 1.896 - acc: 0.599
[Testing][pixel vales are normalized from (-1,1)][model:MobileNetV2] - loss: 2.287 - acc: 0.524

&lt;/denchmark-code&gt;

First, we can see the accuracy is not comparable with the original result(Top-1 accuracy is 70% up).
I thought that this issue is I'm not sure which crop and pad method is applied in the original result.
Therefore, I defined a custom function CenterCrop to fit the model's input size.
But, we can skip this issues there.
What I want to mention is normalization issue.
If I don't apply any normalization(run_aug=1 in code), pixel's values are defined in (0, 255).
All models' accuracy are near 0.001, except for resNet50 which achieves a meaningful accuracy.
If I do normalization(run_aug=2 in code), pixel's values are defined in (0, 1).
This time, DenseNet121 and MobileNetV2 have a meaningful accuracy.
If I do standard normalization(run_aug=3 in code), pixel's values are defined in (-1, 1).
The results are similar to previous case. But I'm sure why those two cases have same accuracy.
Those behavior let me confused.
Before applying pre-trained model, I have to which normalization method should be applied.
After reading the source code, I found that those applications are import from keras_application in tensorflow.
&lt;denchmark-link:https://github.com/keras-team/keras-applications&gt;keras-applications&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/fchollet/deep-learning-models&gt;weight download&lt;/denchmark-link&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

I didn't test other models, such as ResNet50V2, InceptionV3 and Xception because their input size are 299 instead of 244 and this is a time consuming task.
However, anyone can modify the test case and do the benchmark.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Because of licence issue for ImageNet, I can't provide imagenet in public.
But the following is the minimal test case:
# pip install tensorflow-gpu==1.14.0
# pip pandas
#%%
import time
import numpy as np
import pandas as pd
import tensorflow as tf

from glob import glob

#%%
# input image dimensions
img_h = 224
img_w = 224
channels = 3

# information for dataset
dataset_path = "dataset-imagenet/"
num_classes = 1000
num_testing = 50000

#%%
class DataGenerator:

    def __init__(self, dataframe, batch_size, run_aug = True):

        self.total_len  = len(dataframe.index)
        self.batch_size = batch_size
        self.run_aug = run_aug
        self.dataframe  = dataframe
        self.on_epoch_end()

    def __build_pipeline(self, file_path, labelY):

        # mapping function in tf
        def preprocess_fn(file_path, labelY):

            def fn_x(img_array):

                img_array = img_array.numpy()

                if self.run_aug == 1:
                    # image's range is [0,255]
                    image = img_array

                if self.run_aug &gt;= 2:
                    # image's range is [0,1]
                    image = img_array / 255.0

                if self.run_aug == 3:
                    # std normalization
                    image[0,:,:] -= 0.485
                    image[1,:,:] -= 0.456
                    image[2,:,:] -= 0.406
                    image[0,:,:] /= 0.229
                    image[1,:,:] /= 0.224
                    image[2,:,:] /= 0.225

                return image

            def fn_y(label):
                return tf.keras.utils.to_categorical(label , num_classes)

            # read image from files
            image = tf.io.read_file(file_path)
            image = tf.image.decode_image(image, channels=channels)
            aug_size = 256
            imageX = tf.compat.v1.image.resize_image_with_pad(image, aug_size, aug_size)
            imageX = tf.image.resize_with_crop_or_pad(image, img_h, img_w)

            # do normalizarion
            [imageX] = tf.py_function(fn_x, [imageX], [tf.float32])
            imageX.set_shape([img_h, img_w, channels])
            imageX = tf.image.random_flip_left_right(imageX)

            [labelY] = tf.py_function(fn_y, [labelY], [tf.float32])
            labelY.set_shape([num_classes])

            return imageX, labelY

        dataset = tf.data.Dataset.from_tensor_slices( (file_path, labelY) )
        dataset = dataset.shuffle(batch_size * 8)
        dataset = dataset.repeat()
        dataset = dataset.map(preprocess_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)
        dataset = dataset.batch(self.batch_size)
        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)

        self.dataset   = dataset

    def  __len__(self):

        return self.total_len // self.batch_size

    def on_epoch_end(self):

        cleanX = np.array(self.dataframe["File"])
        totalY = np.array(self.dataframe["One-hot"])

        # run permutation
        rand_idx = np.random.permutation(self.total_len)
        cleanX = cleanX[rand_idx]
        totalY = totalY[rand_idx]

        self.__build_pipeline(cleanX, totalY)

#%%
def build_clf(model_name):

    if model_name == "ResNet50":
        clf_model = tf.keras.applications.ResNet50(include_top=True, pooling='max', weights='imagenet')

    if model_name == "DenseNet121":
        clf_model = tf.keras.applications.DenseNet121(include_top=True, pooling='max', weights='imagenet')

    if model_name == "MobileNetV2":
        clf_model = tf.keras.applications.MobileNetV2(include_top=True, pooling='max', weights='imagenet')

    if model_name == "InceptionV3":
        clf_model = tf.keras.applications.InceptionV3(include_top=True, weights='imagenet')


    clf_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

    return clf_model

#%%
def list_testing_data(classes, file_path, onehot_map):

    try:
        testing_data = pd.read_pickle('imagenet_test_list.pkl')
        print('[Successful] Testing_data loaded from pickle ...')
    except:
        testing_image_info = []
        for iter_class in classes:
            files = glob(os.path.join(file_path, iter_class, '*.JPEG'))
            for iter_img in files:
                data_info = [iter_img, iter_class]
                testing_image_info.append(data_info)

        testing_data = pd.DataFrame(testing_image_info, columns=['File', 'Class'])
        testing_data["One-hot"] = testing_data["Class"].replace(onehot_map, inplace=False)

        testing_data.to_pickle('imagenet_test_list.pkl')

    assert(testing_data.shape[0] == num_testing, "[Fatal] Mismatched total length of testing data")
    return testing_data

#%%
if __name__ == '__main__':

    # set GPU
    import os
    if os.environ.get("CUDA_VISIBLE_DEVICES") is None:
        os.environ["CUDA_VISIBLE_DEVICES"] = "0"

    # Hyperparameters
    batch_size = 100
    epochs = 5

    # load one-hot labels
    file_path = dataset_path + 'val'
    classes = os.listdir(file_path)
    list_class = sorted( list( set(classes) ) )
    onehot_map = dict( zip( list_class, list(range(0, num_classes)) ))

    # load list of validation data, those data should be considered as testing data
    testing_data = list_testing_data(classes, file_path, onehot_map)

    # build data generator
    gen_type1 = DataGenerator(testing_data, batch_size, run_aug=1)
    gen_type2 = DataGenerator(testing_data, batch_size, run_aug=2)
    gen_type3 = DataGenerator(testing_data, batch_size, run_aug=3)
    gen_list = [gen_type1, gen_type2, gen_type3]

    # build model
    model_list = ["ResNet50", "DenseNet121", "MobileNetV2"]
    
    # print result for type1
    test_gen = gen_type1
    for model_name in model_list:
        model = build_clf(model_name)
        meta_string = '[Testing][pixel vales are from (0,255)][model:{:s}] '.format(model_name)
        prefix_string = ''
        output = model.evaluate(test_gen.dataset, steps = test_gen.__len__())
        for ii in range( len( model.metrics_names) ):
            meta_string = meta_string + '- {:s}{:s}: {:.3f} '.format(prefix_string, model.metrics_names[ii], output[ii])

        print(meta_string)

    # print result for type2
    test_gen = gen_type2
    for model_name in model_list:
        model = build_clf(model_name)
        meta_string = '[Testing][pixel vales are from (0,1)][model:{:s}] '.format(model_name)
        prefix_string = ''
        output = model.evaluate(test_gen.dataset, steps = test_gen.__len__())
        for ii in range( len( model.metrics_names) ):
            meta_string = meta_string + '- {:s}{:s}: {:.3f} '.format(prefix_string, model.metrics_names[ii], output[ii])

        print(meta_string)

    # print result for type3
    test_gen = gen_type3
    for model_name in model_list:
        model = build_clf(model_name)
        meta_string = '[Testing][pixel vales are normalized from (-1,1)][model:{:s}] '.format(model_name)
        prefix_string = ''
        output = model.evaluate(test_gen.dataset, steps = test_gen.__len__())
        for ii in range( len( model.metrics_names) ):
            meta_string = meta_string + '- {:s}{:s}: {:.3f} '.format(prefix_string, model.metrics_names[ii], output[ii])

        print(meta_string)
	</description>
	<comments>
		<comment id='1' author='CNOCycle' date='2019-12-23T08:41:15Z'>
		&lt;denchmark-link:https://github.com/CNOCycle&gt;@CNOCycle&lt;/denchmark-link&gt;
,
Could you please take a look at this &lt;denchmark-link:https://www.tensorflow.org/tutorials/images/transfer_learning&gt;link&lt;/denchmark-link&gt;
 and check if it helps. Thanks!
		</comment>
		<comment id='2' author='CNOCycle' date='2019-12-23T08:59:19Z'>
		&lt;denchmark-link:https://github.com/amahendrakar&gt;@amahendrakar&lt;/denchmark-link&gt;

Thank for your reply. I'm not want to do transfer learning. What I want to do is reproduce the result of pre-trained model.
Those pre-trained model's weight are from imagenet dataset, so those models' top-1 accuracy on imagenet should be 70% up.
Also, from result which I posted, the behavior of those models are not unified.
So, my suggestion is that tf should mentioned input image should do normalization manually or not before feeding input image into pre-trained model.
		</comment>
		<comment id='3' author='CNOCycle' date='2019-12-23T09:54:33Z'>
		Oh, there is still no clear document in tensorflow=2.0.0
&lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/applications&gt;https://www.tensorflow.org/api_docs/python/tf/keras/applications&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='CNOCycle' date='2019-12-23T20:21:36Z'>
		All pre-trained modes using TF mode expect the input channels to be in range of [-1,1]



tensorflow/tensorflow/python/keras/applications/imagenet_utils.py


         Line 36
      in
      23c3bda






 def preprocess_input(x, data_format=None, mode='caffe'): 





		</comment>
		<comment id='5' author='CNOCycle' date='2019-12-24T01:47:59Z'>
		&lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;

Thank your reply.  I found that a clear comment have been added in the master branch.
But, I still doubt whether this description is correct or not.
Because in my test case, ResNet50 model's accuracy is near 0.001 if we normalize the value range to [-1, 1].
In the tutorial(&lt;denchmark-link:https://www.tensorflow.org/tutorials/generative/adversarial_fgsm&gt;https://www.tensorflow.org/tutorials/generative/adversarial_fgsm&lt;/denchmark-link&gt;
),
# Helper function to preprocess the image so that it can be inputted in MobileNetV2
def preprocess(image):
  image = tf.cast(image, tf.float32)
  image = image/255
  image = tf.image.resize(image, (224, 224))
  image = image[None, ...]
  return image
It only rescales data to range [0, 1] instead of [-1, 1].
		</comment>
		<comment id='6' author='CNOCycle' date='2020-01-27T05:55:31Z'>
		&lt;denchmark-link:https://github.com/CNOCycle&gt;@CNOCycle&lt;/denchmark-link&gt;
 , Please try to see Nightly it shows the arguments and returned params if it helps.
		</comment>
		<comment id='7' author='CNOCycle' date='2020-01-30T02:32:14Z'>
		&lt;denchmark-link:https://github.com/ashutosh1919&gt;@ashutosh1919&lt;/denchmark-link&gt;
 , Thanks your comment. I saw the parameters' usage in the function on the lastest master branch.
But, I still have two main concerns.


The input range for pre-trained weights: Not all pre-trained weights require the input range to be [0,1]. It should be highlighted.


The accuracy for pre-trained weights: I have shown the experimental results. All tests' accuracy are under 70%. It means that pre-trained weights are not good enough.


		</comment>
		<comment id='8' author='CNOCycle' date='2020-03-23T14:48:40Z'>
		For anyone else finding this: TensorFlow requires images to be in range [0,1), see https://www.tensorflow.org/api_docs/python/tf/image/convert_image_dtype

Images that are represented using floating point values are expected to have values in the range [0,1). Image data stored in integer data types are expected to have values in the range [0,MAX], where MAX is the largest positive representable number for the data type.

		</comment>
		<comment id='9' author='CNOCycle' date='2020-03-27T13:35:36Z'>
		
All pre-trained modes using TF mode expect the input channels to be in range of [-1,1]



tensorflow/tensorflow/python/keras/applications/imagenet_utils.py


         Line 36
      in
      23c3bda






 def preprocess_input(x, data_format=None, mode='caffe'): 






I think this is the right approach, i.e., using keras-application preprocess_input to get meaningful results, regardless of convert_image_dtype. This should have already been tested.
		</comment>
		<comment id='10' author='CNOCycle' date='2020-08-07T06:00:44Z'>
		Closing this issue now. Thanks!
		</comment>
	</comments>
</bug>