<bug id='8696' author='derekhh' open_date='2017-03-24T17:22:03Z' closed_time='2017-04-17T22:53:09Z'>
	<summary>TensorFlow hangs during training while using with tf.device('/device:CPU:0'):</summary>
	<description>
&lt;denchmark-h:h3&gt;Description&lt;/denchmark-h&gt;

I'm trying to fine-tune an Inception-V1 model and my latest implementation is based on &lt;denchmark-link:https://github.com/tensorflow/models/blob/master/slim/train_image_classifier.py&gt;train_image_classifier.py&lt;/denchmark-link&gt;
. I've noticed that sometimes TF hangs (might be similar to Issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/2788&gt;#2788&lt;/denchmark-link&gt;
) and I'm trying to figure out why.
The following snippet is my load_batch function. I've also used this session_config=tf.ConfigProto(operation_timeout_in_ms=60000) to throw a DeadlineExceededError when things timed out.
I've noticed that if I remove the with tf.device('/device:CPU:0') line, I can run the model for a whole night without any issue. If I add this line back I'll get a DeadlineExceededError fairly quickly (at around 1K steps with a batch size of 32) and I have a consistent repro.
&lt;denchmark-code&gt;def load_batch(dataset, batch_size, height, width):
    dataset_basename = os.path.basename(dataset.data_sources)
    with tf.device('/device:CPU:0'):
        with tf.name_scope(name=dataset_basename):
            data_provider = slim.dataset_data_provider.DatasetDataProvider(dataset=dataset)
            raw_image, label = data_provider.get(items=['image', 'label'])
            tf.summary.image('raw_image', tf.expand_dims(input=raw_image, axis=0))
            image = tf.cast(raw_image, tf.float32) / 255.0
            image = tf.image.resize_images(images=image, size=[height, width], align_corners=True)
            tf.summary.image('resized_image', tf.expand_dims(input=image, axis=0))
            # TensorFlow recommendation:
            # min_after_dequeue + (num_threads + a small safety margin) * batch_size
            # https://www.tensorflow.org/programmers_guide/reading_data
            num_threads = 8
            images, labels = tf.train.batch(tensors=[image, label],
                                            batch_size=batch_size,
                                            num_threads=num_threads,
                                            capacity=(num_threads + 2) * batch_size,
                                            allow_smaller_final_batch=True)
    return images, labels
&lt;/denchmark-code&gt;

The error message is:
&lt;denchmark-code&gt;INFO:tensorflow:global step 1040: loss = 0.2107 (0.17 sec/step)
2017-03-24 10:09:57.915250: W tensorflow/core/kernels/queue_base.cc:294] _0_magazines_train.tfrecord/parallel_read/filenames: Skipping cancelled enqueue attempt with queue not closed
INFO:tensorflow:Error reported to Coordinator: &lt;class 'tensorflow.python.framework.errors_impl.DeadlineExceededError'&gt;, Timed out waiting for notification
INFO:tensorflow:global step 1040: validation loss = 0.3864, validation accuracy = 97.66%
INFO:tensorflow:Finished training! Saving model to disk.
Traceback (most recent call last):
  File "fine-tune.py", line 215, in &lt;module&gt;
    run()
  File "fine-tune.py", line 211, in run
    session_config=tf.ConfigProto(operation_timeout_in_ms=60000))
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py", line 752, in train
    sv.saver.save(sess, sv.save_path, global_step=sv.global_step)
  File "/usr/lib/python2.7/contextlib.py", line 24, in __exit__
    self.gen.next()
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py", line 960, in managed_session
    self.stop(close_summary_writer=close_summary_writer)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py", line 788, in stop
    stop_grace_period_secs=self._stop_grace_secs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/queue_runner_impl.py", line 234, in _run
    sess.run(enqueue_op)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 786, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 994, in _run
    feed_dict_string, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1044, in _do_run
    target_list, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1064, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.DeadlineExceededError: Timed out waiting for notification
&lt;/denchmark-code&gt;

Since I have a consistent repro I'll be willing to provide more information to help debug this issue. Right now I'm not exactly sure what I should provide to help understand the root cause.
This is the TensorBoard information from /batch/fraction_of_320_full:
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/1497445/24305854/aa7c4da8-107b-11e7-864d-dfe1296b9503.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='derekhh' date='2017-03-24T17:26:22Z'>
		Oh, it seems I forgot to provide more information about my current TF build and hardware config.
TF: built from commit &lt;denchmark-link:c7b80d51da4fb6d51ea54a0bdf2601afa379d60c&gt;c7b80d51da4fb6d51ea54a0bdf2601afa379d60c&lt;/denchmark-link&gt;
 with CUDA support but it still repros if I use the normal pip version.
GPU:
&lt;denchmark-code&gt;Fri Mar 24 10:24:51 2017
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  TITAN X (Pascal)    Off  | 0000:05:00.0      On |                  N/A |
| 23%   32C    P8    10W / 250W |    173MiB / 12186MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      1194    G   /usr/lib/xorg/Xorg                             170MiB |
+-----------------------------------------------------------------------------+
&lt;/denchmark-code&gt;

CPU:
&lt;denchmark-code&gt;‚ùØ lscpu
Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                16
On-line CPU(s) list:   0-15
Thread(s) per core:    2
Core(s) per socket:    8
Socket(s):             1
NUMA node(s):          1
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 79
Model name:            Intel(R) Core(TM) i7-6900K CPU @ 3.20GHz
Stepping:              1
CPU MHz:               1200.000
CPU max MHz:           4100.0000
CPU min MHz:           1200.0000
BogoMIPS:              6399.59
Virtualization:        VT-x
L1d cache:             32K
L1i cache:             32K
L2 cache:              256K
L3 cache:              20480K
NUMA node0 CPU(s):     0-15
Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='derekhh' date='2017-03-24T18:07:02Z'>
		And BTW, I've just removed the tf.device('/device:CPU:0') line and at least I'm now into 9K steps:
&lt;denchmark-code&gt;...
INFO:tensorflow:global step 9070: validation loss = 0.3390, validation accuracy = 95.31%
INFO:tensorflow:global step 9071: loss = 0.2249 (0.22 sec/step)
INFO:tensorflow:global step 9072: loss = 0.2727 (0.23 sec/step)
INFO:tensorflow:global step 9073: loss = 0.1870 (0.21 sec/step)
INFO:tensorflow:global step 9074: loss = 0.3343 (0.20 sec/step)
INFO:tensorflow:global step 9075: loss = 0.2105 (0.20 sec/step)
INFO:tensorflow:global step 9076: loss = 0.1722 (0.19 sec/step)
...
&lt;/denchmark-code&gt;

Here is the /batch/fraction_of_320_full information for this run:
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/1497445/24307582/ff2d512a-1081-11e7-9c6a-2a5593505fef.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='derekhh' date='2017-03-26T21:30:25Z'>
		A few guesses/checks that might help:

Checking the list of devices matches what you'd expect. The snippet below will print all the devices seen by TF and their names.

from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())

Thread management. Your error message seems to be more related to joining threads/coordinator. Not sure why this only shows with CPU as your device, but perhaps the following might help. I  don't see the use of a coordinator in your snippet. At the very least, it would help with debugging issues arising from coordinator/session. Usually the following is done when using queue runners and parallel threads.

coord = tf.train.Coordinator()
threads = tf.train.start_queue_runners(sess=sess, coord=coord)
# . . .  start your training, etc . . . 
# When done training and/or exception is caught:
coord.request_stop()
coord.join(threads)
&lt;denchmark-link:https://blog.metaflow.fr/tensorflow-how-to-optimise-your-input-pipeline-with-queues-and-multi-threading-e7c3874157e0#.a64trech2&gt;Further reading on coord/threads&lt;/denchmark-link&gt;

From the &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/train/batch&gt;tf.train.batch&lt;/denchmark-link&gt;
 documentation:

A QueueRunner for the queue is added to the current Graph's QUEUE_RUNNER collection.

As far as I know, that means the user should call start_queue_runners for everything to start properly, and then request stop and join when done for everything to end properly.
If none of this was news to you, then don't mind me. Best of luck!
		</comment>
		<comment id='4' author='derekhh' date='2017-03-27T02:36:00Z'>
		Thanks &lt;denchmark-link:https://github.com/mckinziebrandon&gt;@mckinziebrandon&lt;/denchmark-link&gt;
 for your kind reply!
I've checked my return output for  and it indeed says . For the  part. I'm using  for my training. The implementation includes &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/learning.py#L738&gt;sv.start_queue_runners(sess)&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='5' author='derekhh' date='2017-03-31T20:47:36Z'>
		&lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;
, could you comment on this issue please.
		</comment>
		<comment id='6' author='derekhh' date='2017-03-31T21:43:21Z'>
		One possible issue is the tf.summary.image() calls in your load_batch() function. Since (if I understand the code in slim.learning.train correctly) the merged summaries will be collected from a separate thread, this will cause certain images to be dequeued from the input pipeline (in order to make summaries) but never enqueued to the batch queue for training.
Is your input finite or infinite? If it's finite, maybe running CPU-only means that your program completes steps more slowly (relative to the GPU version, and to the periodic summary thread) so it runs out of input data after 1000 steps? (If that's true, I'd expect the same fate to befall the GPU version, but it would complete more steps before that happens.)
		</comment>
		<comment id='7' author='derekhh' date='2017-04-05T07:56:23Z'>
		Thanks &lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;
. I think what I'm observing is more or less the same as &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/2788&gt;#2788&lt;/denchmark-link&gt;
 and I can also resume the training process by first suspending and then bringing the process back to foreground.
What I don't quite understand is, even if running CPU only means it takes more time for the CPU to prepare the input data - why would that make the process stuck in there without doing anything?
PS: When you say "finite or infinite"? Are you asking whether I'm setting something like the maximum number of steps or the number of epochs? I haven't set these parameters and the training process should be infinite.
		</comment>
		<comment id='8' author='derekhh' date='2017-04-05T07:57:11Z'>
		More information that might be helpful. CPU cores have 100% load when it hangs:
&lt;denchmark-code&gt;  1  [||||||||||||||||||||||||||||||||||||||||||||100.0%]   5  [||||||||||||||||||||||||||||||||||||||||||||100.0%]   9  [                                              0.0%]   13 [                                              0.0%]
  2  [||||||||||||||||||||||||||||||||||||||||||||100.0%]   6  [||||||||||||||||||||||||||||||||||||||||||||100.0%]   10 [||                                            1.3%]   14 [                                              0.0%]
  3  [|                                             0.7%]   7  [                                              0.0%]   11 [||||||||||||||||||||||||||||||||||||||||||||100.0%]   15 [||||||||||||||||||||||||||||||||||||||||||||100.0%]
  4  [                                              0.0%]   8  [                                              0.0%]   12 [||||||||||||||||||||||||||||||||||||||||||||100.0%]   16 [||||||||||||||||||||||||||||||||||||||||||||100.0%]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='derekhh' date='2017-04-17T17:39:59Z'>
		&lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;
 is asking whether your queues are empty -- that would be finite.
So &lt;denchmark-link:https://github.com/derekhh&gt;@derekhh&lt;/denchmark-link&gt;
 did you say that you  then  and it got the process to continue?
		</comment>
		<comment id='10' author='derekhh' date='2017-04-17T21:51:17Z'>
		&lt;denchmark-link:https://github.com/drpngx&gt;@drpngx&lt;/denchmark-link&gt;

Thanks for your reply. Later I've found even running train_image_classifier.py from TF-Slim hangs so I feel it should be related to a particular hardware / system config issue.
I've been trying multiple ways to figure out the root cause in the past few weeks. I believe this is an issue unrelated to TensorFlow but is kind of specific to my motherboard. I'm using a similar motherboard as mentioned in Issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/1947&gt;#1947&lt;/denchmark-link&gt;
 (ASUS X99-E WS 3.1).
Later, I've followed this web page: &lt;denchmark-link:https://www.pugetsystems.com/labs/hpc/Install-Ubuntu-16-04-or-14-04-and-CUDA-8-and-7-5-for-NVIDIA-Pascal-GPU-825/&gt;https://www.pugetsystems.com/labs/hpc/Install-Ubuntu-16-04-or-14-04-and-CUDA-8-and-7-5-for-NVIDIA-Pascal-GPU-825/&lt;/denchmark-link&gt;
 and installed Ubuntu 14.04 LTS with some additional steps of configurations I didn't do before:

Setting "pcie_aspm=off" in the boot options. [This seems to be the fix.]
Installed NVIDIA 375.39 driver and CUDA 8.0 separately instead of using the driver bundled with CUDA 8.0. [I don't feel this is the fix since CUDA 8.0 is also bundled with NVIDIA 375.39.]
Fixed a symbolic link issue related to "libEGL.so.1" as described in this post.

I'm no longer seeing the issue right now. I still can't tell which of the aforementioned configuration steps actually fixed the issue. I'll test it more and update this thread so this might help some other people using similar hardware configs.
		</comment>
		<comment id='11' author='derekhh' date='2017-04-17T22:53:09Z'>
		Thanks for the update! Good to know.
		</comment>
		<comment id='12' author='derekhh' date='2017-09-26T16:55:52Z'>
		I had the same issue with my PC which has an ASUS motherboard.
I updated the BIOS to the latest and turned off "turbo mode" and the issue disappeared.
I have a more verbose version posted under issue &lt;denchmark-link:https://github.com/tflearn/tflearn/issues/241&gt;tflearn/tflearn#241&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>