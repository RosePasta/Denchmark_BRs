<bug id='30870' author='covertg' open_date='2019-07-19T05:34:20Z' closed_time='2019-08-03T22:47:13Z'>
	<summary>Post-Training Integer Quantization on Models for 'Edge' / Coral TPUs</summary>
	<description>
System information

Have I written custom code: Yes (below)
OS Platform and Distribution: Arch
TensorFlow installed from (source or binary): Binary (pip)
TensorFlow version: v2.0.0-beta0-17-g8e423e3
Python version: 3.7.3


I'm interested in using TF2.0's  to build graphs that can be run on an Edge TPU. As that device only supports (8-bit) integer quantized models, I'm trying to use the relatively new &lt;denchmark-link:https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-post-training-integer-quantization-b4964a1ea9ba&gt;post-training integer quantization&lt;/denchmark-link&gt;
. That Medium post states,

Our new post-training integer quantization enables users to take an already-trained floating-point model and fully quantize it to only use 8-bit signed integers (i.e. int8). ... Fixed point hardware accelerators, such as Edge TPUs, will also be able to run these models.

However, even a simple proof-of-concept graph doesn't seem to be accepted by the &lt;denchmark-link:https://coral.withgoogle.com/web-compiler&gt;Coral web compiler&lt;/denchmark-link&gt;
 (with no useful error messages). Unless this is an issue with the Coral site, I wanted to write in here—whether this is a bug or a not-yet-implemented feature, in hopes that it may be resolved. Am I doing anything unsupported at this time? Or, has anyone been able to compile a model for Coral/Edge that may be able to serve as an example?
Code to reproduce the issue
import pathlib
import tensorflow as tf

@tf.function
def simple_layer(x):
    return tf.nn.relu(2 * x + 1)

c_fn = simple_layer.get_concrete_function(tf.TensorSpec(shape=[1], dtype=tf.float32))

converter = tf.lite.TFLiteConverter.from_concrete_functions([c_fn])
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
my_dataset = tf.data.Dataset.from_tensor_slices(np.arange(-100, 100, 0.01, dtype=np.float32))
my_dataset = my_dataset.batch(1)

def rep_dataset():
    for val in my_dataset.shuffle(tf.data.experimental.cardinality(my_dataset)).take(1000):
        yield [val]
converter.representative_dataset = rep_dataset

tflite_quant_model = converter.convert()

dir = 'supersimple_coral'
tflite_models_dir = pathlib.Path(dir)
tflite_models_dir.mkdir(exist_ok=True, parents=True)
tflite_model_file = tflite_models_dir/"model_quant.tflite"
tflite_model_file.write_bytes(tflite_quant_model)
	</description>
	<comments>
		<comment id='1' author='covertg' date='2019-07-25T03:38:14Z'>
		FYI, Edge TPU &lt;denchmark-link:https://coral.withgoogle.com/news/updates-07-2019/&gt;July 2019 Updates&lt;/denchmark-link&gt;
 includes post-training quant support. Didn't try web compiler, offline compiler does work.
		</comment>
		<comment id='2' author='covertg' date='2019-08-03T22:47:13Z'>
		&lt;denchmark-link:https://github.com/freedomtan&gt;@freedomtan&lt;/denchmark-link&gt;
, many thanks for the message. I hadn't seen that update — even wrote to coral support, but only got back that post-training quantization wasn't yet supported. Much appreciated!
		</comment>
		<comment id='3' author='covertg' date='2019-08-03T22:47:14Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=30870&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=30870&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>