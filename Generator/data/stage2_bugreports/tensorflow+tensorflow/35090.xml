<bug id='35090' author='johannabar' open_date='2019-12-13T15:12:47Z' closed_time='2019-12-18T14:49:14Z'>
	<summary>Subsequent calls to tf.data.Dataset.map() with seeded random operation give same random sequences.</summary>
	<description>
System information

Have I written custom code: yes
OS Platform and Distribution: CentOS 7
TensorFlow installed from (source or binary): binary
TensorFlow version: 1.14.0
Python version: 2.7.17
CUDA/cuDNN version: 10.0

Describe the current behavior
In eager mode when setting a random seed, two calls to the same mapping function in tf.data.Dataset.map() would result in two times the same generated random values.
Describe the expected behavior
As I haven't explicitly closed or restarted a session, I would expect both calls to output different generated random values. Is tf.data.Dataset.map() creating its own session and running the mapping function subgraph in it? If so, is there a way to force the map() to take the current session as input?
Code to reproduce the issue
&lt;denchmark-code&gt;import numpy as np  
import tensorflow as tf
tf.compat.v1.enable_eager_execution()

SEED = 88
tf.compat.v1.random.set_random_seed(SEED)

ds_train = tf.data.Dataset.range(0, 4)
ds_val = tf.data.Dataset.range(0, 4)
ds_train = ds_train.map(
    lambda x: tf.random.uniform([1], 0.2, 5.0)
)
ds_val = ds_val.map(
    lambda x: tf.random.uniform([1], 0.2, 5.0) 
)
for el in ds_train:
    print(el.numpy())
    # --&gt; [0.44027787], [1.7892183], [2.8793733], [3.3438706]
for el in ds_val:
    print(el.numpy())
    # why the same here? --&gt; [0.44027787], [1.7892183], [2.8793733], [3.3438706]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='johannabar' date='2019-12-16T06:31:56Z'>
		I could replicate the issue on colab with Tf 1.14 and 1.15.
Please find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/7840922356bfca9f85923b8271f3aa8c/untitled305.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='johannabar' date='2019-12-16T22:00:18Z'>
		 has a argument which is same in both calls. Therefore you see same numbers when you print.
If the graph-level seed is set, but the operation seed is not: The system deterministically picks an operation seed in conjunction with the graph-level seed so that it gets a unique random sequence.
See &lt;denchmark-link:https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/random/set_random_seed&gt;https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/random/set_random_seed&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='johannabar' date='2019-12-17T09:58:47Z'>
		Hi &lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;
 , thank you for your answer!
I understand the double-seed system but in the case where you have the same random operation with the same seed, you should still have a sequence of different random numbers within the same session. This sequence should repeat only if you compute it in a new session. For instance
&lt;denchmark-code&gt;import tensorflow as tf
tf.compat.v1.enable_eager_execution()
SEED = 88
tf.compat.v1.random.set_random_seed(SEED)
rand1 = tf.random.uniform([1], 0.2, 5.0, seed=SEED) #--&gt;[2.7339704]
rand2 = tf.random.uniform([1], 0.2, 5.0, seed=SEED) #--&gt;[1.4490409] different
# Same when only graph-level seed is set
rand1 = tf.random.uniform([1], 0.2, 5.0) #--&gt;[2.9647074]
rand2 = tf.random.uniform([1], 0.2, 5.0) #--&gt;[2.7952404] different
&lt;/denchmark-code&gt;

Which is why I would expect both random operation in the map() function to be a continuation of a sequence of different random numbers as we are still in the same session.
		</comment>
		<comment id='4' author='johannabar' date='2019-12-17T23:14:06Z'>
		Hi &lt;denchmark-link:https://github.com/johannabar&gt;@johannabar&lt;/denchmark-link&gt;
, there are no longer sessions when eager mode is enabled. Every time  is called, it creates a  for the mapping function. The  in the mapping function gets its graph-level seed from the 's default graph's seed (
). I expected this seed to be , but apparently it's not (maybe this is a side effect of ), in which case the two s start from the same seed and the same state (because both are in a fresh new ). BTW you can check out the new RNG (&lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/random/experimental/Generator&gt;https://www.tensorflow.org/api_docs/python/tf/random/experimental/Generator&lt;/denchmark-link&gt;
) which has much better seeding control.
		</comment>
		<comment id='5' author='johannabar' date='2019-12-18T14:49:14Z'>
		Hi &lt;denchmark-link:https://github.com/wangpengmit&gt;@wangpengmit&lt;/denchmark-link&gt;
, thank you for the explanation!
I am closing this issue as what I noticed is correct behaviour.
Though I think it's a little tricky that a map call to the same mapping function will produce the same random numbers. For instance if you have a map function doing random cropping on images and you call it on both your training and validation data separately then you will get the SAME cropping positions for both training and validation data, is that right?
		</comment>
		<comment id='6' author='johannabar' date='2019-12-18T14:49:16Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35090&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35090&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='johannabar' date='2019-12-18T20:46:21Z'>
		Yes. And I confirm that set_random_seed will cause an RNG in tf.function to start from the same global seed (because of 


tensorflow/tensorflow/python/eager/context.py


         Line 427
      in
      1cf0898






 self._seed = seed 




 and 


tensorflow/tensorflow/python/framework/func_graph.py


         Line 229
      in
      1cf0898






 self.seed = context.global_seed() 




). Possible solutions for your code are removing set_random_seed call or calling it before each Dataset.map with different seeds.
		</comment>
	</comments>
</bug>