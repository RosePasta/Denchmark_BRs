<bug id='41778' author='Lif3line' open_date='2020-07-27T18:06:26Z' closed_time='2020-08-05T10:13:26Z'>
	<summary>Models with Conv2D layer cause segmentation fault when invoked in C++</summary>
	<description>
System information

Linux Ubuntu 18.04
TensorFlow Binary (via pip)
Tested on 2.2.0 &amp; 2.3.0rc2

Command used to run the converter or code if youâ€™re using the Python API
import tensorflow as tf
from tensorflow.keras.layers import Conv2D, Flatten, Input, Dense
from tensorflow.keras.models import Model

inputs = Input(shape=[10, 5, 1])
x = inputs
x = Conv2D(32, (3, 3))(x) # *** Functions correctly when removed
x = Flatten()(x)
x = Dense(1)(x)
    
model = Model(inputs, x)
model.compile()

# Convert the model.
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()

# Save the TF Lite model.
with tf.io.gfile.GFile("min_model.tflite", "wb") as f:
    f.write(tflite_model)
The output from the converter invocation
None
Models in SavedModel &amp; tflite formats as well as minimal example build for linux_x86_64:
&lt;denchmark-code&gt;https://www.dropbox.com/sh/2w67cix39onn28m/AAD0O3OR4Z_y_zMJJJdGBdtQa?dl=0
&lt;/denchmark-code&gt;

Failure details

Conversion successful
Causes segmentaion fault on invoke() when run via tflite c++ minimal example

Invokes fine without Conv2D layer



#include "tensorflow/lite/interpreter.h"
#include "tensorflow/lite/kernels/register.h"
#include "tensorflow/lite/model.h"
#include "tensorflow/lite/optional_debug_tools.h"
#include &lt;cstdio&gt;

using namespace tflite;

#define TFLITE_MINIMAL_CHECK(x)                                                \
  if (!(x)) {                                                                  \
    fprintf(stderr, "Error at %s:%d\n", __FILE__, __LINE__);                   \
    exit(1);                                                                   \
  }

int main(int argc, char *argv[]) {
  if (argc != 2) {
    fprintf(stderr, "minimal &lt;tflite model&gt;\n");
    return 1;
  }
  const char *filename = argv[1];

  // Load model
  std::unique_ptr&lt;tflite::FlatBufferModel&gt; model =
      tflite::FlatBufferModel::BuildFromFile(filename);
  TFLITE_MINIMAL_CHECK(model != nullptr);

  // Build the interpreter
  tflite::ops::builtin::BuiltinOpResolver resolver;
  InterpreterBuilder builder(*model, resolver);
  std::unique_ptr&lt;Interpreter&gt; interpreter;
  builder(&amp;interpreter);
  TFLITE_MINIMAL_CHECK(interpreter != nullptr);

  // Allocate tensor buffers.
  TFLITE_MINIMAL_CHECK(interpreter-&gt;AllocateTensors() == kTfLiteOk);
  printf("=== Pre-invoke Interpreter State ===\n");
  tflite::PrintInterpreterState(interpreter.get());

  // Run inference
  TFLITE_MINIMAL_CHECK(interpreter-&gt;Invoke() == kTfLiteOk); // *** Fails here before return
  printf("\n\n=== Post-invoke Interpreter State ===\n");
  tflite::PrintInterpreterState(interpreter.get());

  return 0;
}
Called via minimal min_model.tflite.
	</description>
	<comments>
		<comment id='1' author='Lif3line' date='2020-08-05T10:13:14Z'>
		Closing in favour of &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/42057#issue-673420638&gt;42057&lt;/denchmark-link&gt;
 which more accurately represents the problem.
It appears the issue lies with static linking and thread generation rather than with conv2d or model conversion.
		</comment>
		<comment id='2' author='Lif3line' date='2020-08-05T10:13:27Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41778&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41778&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>