<bug id='32592' author='robertah' open_date='2019-09-17T14:33:38Z' closed_time='2019-09-27T09:16:23Z'>
	<summary>In tf.Estimator training metric is not divided by the batch size</summary>
	<description>
System information

TensorFlow version: TF 1.12
Python version: 3.6

Describe the current behavior
&lt;denchmark-link:https://user-images.githubusercontent.com/23496841/65050585-e9ddd900-d967-11e9-82b5-b8d0a04164db.png&gt;&lt;/denchmark-link&gt;

I am training a model with tf.Estimator and it seems that the training metric (root mean squared error) is not divided by the batch size (for both training and validation the batch size is 100). Indeed, for comparison this is the mean squared error loss for training and validation:
&lt;denchmark-link:https://user-images.githubusercontent.com/23496841/65050797-450fcb80-d968-11e9-9a67-565efced4684.png&gt;&lt;/denchmark-link&gt;

Code to reproduce the issue
I am using the standard structure for model_fn in tf.Estimator. Here's the main part of the code:
def model_fn(features, labels, mode, params):

    # model layers 
    # ...

    rmse = tf.metrics.root_mean_squared_error(labels, predictions)

    if mode == tf.estimator.ModeKeys.PREDICT:
        predictions = {
            'new_states': predictions
        }
        return tf.estimator.EstimatorSpec(mode, predictions=predictions)

    loss = tf.losses.mean_squared_error(labels, predictions)

    metrics = {'rmse': rmse}

    if mode == tf.estimator.ModeKeys.EVAL:
        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)

    optimizer = tf.train.AdamOptimizer(learning_rate=params['learning_rate'])
   
    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())

    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)
	</description>
	<comments>
		<comment id='1' author='robertah' date='2019-09-18T06:52:28Z'>
		&lt;denchmark-link:https://github.com/robertah&gt;@robertah&lt;/denchmark-link&gt;
 ,
Can you share a standalone code to reproduce the issue?thanks!
		</comment>
		<comment id='2' author='robertah' date='2019-09-18T07:50:27Z'>
		I can't share the original input data, but the error can be reproduced in the following way:
import tensorflow as tf
import sys

def model_fn(features, labels, mode, params):

    x = tf.layers.dense(features, 100)
    x = tf.layers.dense(x, 100)
    predictions = tf.layers.dense(x, 100)

    rmse = tf.metrics.root_mean_squared_error(labels, predictions)

    if mode == tf.estimator.ModeKeys.PREDICT:
        predictions = {
            'predictions': predictions
        }
        return tf.estimator.EstimatorSpec(mode, predictions=predictions)

    loss = tf.losses.mean_squared_error(labels, predictions)

    metrics = {'rmse': rmse}
    print_op = tf.print((loss, rmse[1]), output_stream=sys.stdout)
    with tf.control_dependencies([print_op]):
        tf.summary.scalar('rmse', rmse[1])

    if mode == tf.estimator.ModeKeys.EVAL:
        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)

    optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)

    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())

    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)


if __name__ == '__main__':

    import numpy as np

    features = np.random.randn(5000, 100)
    labels = features * 10 + np.random.randn(5000, 100)/1000

    x_train = {
        'features': features[:4500],
        'labels': labels[:4500]
    }

    train_input_fn = tf.estimator.inputs.numpy_input_fn(x_train['features'], x_train['labels'], batch_size=100, shuffle=True, num_epochs=300)

    x_eval = {
        'features': features[4500:],
        'labels': labels[4500:]
    }

    eval_input_fn = tf.estimator.inputs.numpy_input_fn(x_eval['features'], x_eval['labels'], batch_size=100, shuffle=False)

    config = tf.estimator.RunConfig(
        keep_checkpoint_max=1,
        model_dir='model',
        save_summary_steps=100,
        save_checkpoints_steps=100,
        log_step_count_steps=100)

    estimator = tf.estimator.Estimator(model_fn=model_fn, config=config)

    train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=500000)
    eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn)

    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
		</comment>
		<comment id='3' author='robertah' date='2019-09-18T08:38:04Z'>
		Please find the &lt;denchmark-link:https://colab.research.google.com/gist/oanush/2fb22e808e53bc37dfd9ecab4cbfa2bf/32592.ipynb&gt;gist&lt;/denchmark-link&gt;
 of colab when tried executing the given code.Thanks!
		</comment>
		<comment id='4' author='robertah' date='2019-09-18T09:02:05Z'>
		I've added a random seed in the notebook. This are the loss and the rmse value plots:
&lt;denchmark-link:https://user-images.githubusercontent.com/23496841/65133893-ba7caa00-da03-11e9-8cc5-a91b370cc7be.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='robertah' date='2019-09-20T08:44:37Z'>
		So in the end I think the problem is related to tf.losses.mean_squared_error and tf.metrics.mean_squared_error that return different values during the training. Please see the notebook for more details.
		</comment>
		<comment id='6' author='robertah' date='2019-09-27T09:16:24Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=32592&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=32592&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>