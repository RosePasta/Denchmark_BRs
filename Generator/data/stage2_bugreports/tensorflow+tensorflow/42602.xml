<bug id='42602' author='Ghost---Shadow' open_date='2020-08-23T14:38:15Z' closed_time='2020-08-26T13:27:16Z'>
	<summary>[Bug] tf.sqrt Inconsistent behaviour</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
TensorFlow installed from (source or binary): Binary
TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0
Python version: 3.5
Bazel version (if compiling from source): No
GCC/Compiler version (if compiling from source): No
CUDA/cuDNN version: V10.1.243
GPU model and memory: RTX2070 MaxQ, 8GB

Describe the current behavior
When I run this code, I get the result and gradients
x = tf.Variable([[1.0, 0.0]])

with tf.GradientTape() as tape:
    x = tf.square(x)
    x = tf.sqrt(x)
    z = tf.reduce_sum(x, axis=-1)
    loss = tf.nn.l2_loss(z)

tf.print(z, loss)
tf.print(tape.gradient(loss,x))
'''
Output:
[1] 0.5
[[1 1]]
'''
However, when I refactor the three lines inside a function, the gradient becomes NaN
@tf.function # Happens in both eager and graph
def fun(v):
    v = tf.square(v)
    v = tf.sqrt(v)
    return tf.reduce_sum(v, axis=-1)

x = tf.Variable([[1.0, 0.0]])
with tf.GradientTape() as tape:
    z = fun(x)
    loss = tf.nn.l2_loss(z)

tf.print(z, loss)
tf.print(tape.gradient(loss, x))

'''
Output:
[1] 0.5
[[1 nan]]
'''
Changing x = tf.sqrt(x) to x = tf.sqrt(x + 1e-7) makes the gradient non NaN.
Describe the expected behavior
Either result is acceptable as long as the behaviour is consistent.
	</description>
	<comments>
		<comment id='1' author='Ghost---Shadow' date='2020-08-24T04:48:58Z'>
		I am able to replicate the issue faced, please find the &lt;denchmark-link:https://colab.research.google.com/gist/Saduf2019/b8f0c0f7a15aec8140a6a8c3925bb309/untitled385.ipynb&gt;gist here&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='Ghost---Shadow' date='2020-08-25T16:00:44Z'>
		The failure in tf.function looks reasonable but it is unclear to me how this works without tf.function. The problem basically is that we are unable to fold the computation x * 1/x = 1 in the gradients path. This fails when x is 0.
		</comment>
		<comment id='3' author='Ghost---Shadow' date='2020-08-25T16:11:27Z'>
		The eager version of the code overwrites the x Python local variable. I assume that's the only reason they're giving different results.
So far we've decided not to do anything smart with backprop regarding inverse functions canceling etc. (it's been discussed several times).
		</comment>
		<comment id='4' author='Ghost---Shadow' date='2020-08-25T16:14:44Z'>
		While using an epsilon is a reasonable workaround here, you may also want to check out tf.custom_gradient to implement numerically stable gradient functions for cases such as this one.
		</comment>
		<comment id='5' author='Ghost---Shadow' date='2020-08-25T17:07:09Z'>
		I think the refactoring inadvertently changes the logic.
If we make sure the codes are equivalent by just removing the tf.function decorator, then the outputs seem to be actually consistent:
&lt;denchmark-code&gt;# Same result with or without @tf.function
def fun(v):
    v = tf.square(v)
    v = tf.sqrt(v)
    return tf.reduce_sum(v, axis=-1)

x = tf.Variable([[1.0, 0.0]])
with tf.GradientTape() as tape:
    z = fun(x)
    loss = tf.nn.l2_loss(z)

tf.print(z, loss)
tf.print(tape.gradient(loss, x))
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;[1] 0.5
[[1 -nan]]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='Ghost---Shadow' date='2020-08-26T13:19:17Z'>
		&lt;denchmark-link:https://github.com/allenlavoie&gt;@allenlavoie&lt;/denchmark-link&gt;
 It happens in both eager and graph. You can comment out the . I dont think this is being caused by XLA optimization.
Also we should not forget that square(sqrt(x)) =/= sqrt(square(x)) for float.
&lt;denchmark-link:https://github.com/saxenasaurabh&gt;@saxenasaurabh&lt;/denchmark-link&gt;
 The problem is that the same code behaves differently when it is inside a function (eager or not) than if it is outside the function (at root level). As long as it is consistent, I think either result is fine.
		</comment>
		<comment id='7' author='Ghost---Shadow' date='2020-08-26T13:27:16Z'>
		&lt;denchmark-link:https://github.com/Ghost---Shadow&gt;@Ghost---Shadow&lt;/denchmark-link&gt;
 the two pieces of code that you originally included don't calculate the same thing. If you correct the eager version, you will also get nan, as expected:
&lt;denchmark-code&gt;x_var = tf.Variable([[1.0, 0.0]])
x = x_var  # avoid calling tape.gradient wrt intermediate outputs

with tf.GradientTape() as tape:
    x = tf.square(x)
    x = tf.sqrt(x)
    z = tf.reduce_sum(x, axis=-1)
    loss = tf.nn.l2_loss(z)

tf.print(z, loss)
tf.print(tape.gradient(loss, x_var))
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='Ghost---Shadow' date='2020-08-26T13:27:18Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42602&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42602&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='Ghost---Shadow' date='2020-08-26T13:34:53Z'>
		It doesn't, and it's not expected to, because it's not taking a gradient with respect to the non-differentiable sqrt function. Let's rename the variables to make that clearer:
&lt;denchmark-code&gt;x = tf.Variable([[1.0, 0.0]])

with tf.GradientTape() as tape:
    x = tf.square(x)
    wrong_x = tf.sqrt(x)
    z = tf.reduce_sum(wrong_x, axis=-1)
    loss = tf.nn.l2_loss(z)

tf.print(z, loss)
tf.print(tape.gradient(loss, wrong_x))
&lt;/denchmark-code&gt;

So you might as well write this with the same effect:
&lt;denchmark-code&gt;with tf.GradientTape() as tape:
    wrong_x = tf.constant([[1.0, 0.0]])
    tape.watch(wrong_x)
    z = tf.reduce_sum(wrong_x, axis=-1)
    loss = tf.nn.l2_loss(z)

tf.print(z, loss)
tf.print(tape.gradient(loss, wrong_x))
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>