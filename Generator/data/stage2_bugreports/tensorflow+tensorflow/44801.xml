<bug id='44801' author='zetez' open_date='2020-11-12T12:39:19Z' closed_time='2020-12-02T23:03:55Z'>
	<summary>Tutorial code freezes indefinitely on TF 2.4 with tf.function</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No


OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Education 1909 64 bit


Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not used


TensorFlow installed from (source or binary): pip install tensorflow==2.4.0rc1


TensorFlow version (use command below): 2.4.0-rc1(v2.4.0-rc0-30-gef82f4c66c)


Python version: 3.7.9


Bazel version (if compiling from source):


GCC/Compiler version (if compiling from source):


CUDA/cuDNN version: CUDA 11.0.3/cuDNN 8.0.2


GPU model and memory: RTX 2080ti 11 GB



When running the &lt;denchmark-link:https://www.tensorflow.org/tutorials/generative/cyclegan&gt;cyclegan tutorial&lt;/denchmark-link&gt;
 on a local machine the program freezes during the training loop. It successfully executes two  before freezing indefinitely during the third . This does not happen with TF 2.3.1 and if @tf.function is removed from the function  it also does not happen.
Describe the expected behavior
The program should not freeze.

Download the notebook from &lt;denchmark-link:https://www.tensorflow.org/tutorials/generative/cyclegan&gt;https://www.tensorflow.org/tutorials/generative/cyclegan&lt;/denchmark-link&gt;
 and run it on jupyter notebook with TF 2.4 on Windows.
Other info / logs
I also tried using python versions 3.6 and 3.8 as well as cuDNN version 8.0.5 and tensorflow versions 2.4.0rc0 and 2.5.0-dev20201029 with the same results.
No errors are printed when the program halts, this is the complete log:
2020-11-12 13:23:14.333154: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll 2020-11-12 13:23:22.692189: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set 2020-11-12 13:23:22.695520: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll 2020-11-12 13:23:22.764684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: pciBusID: 0000:21:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5 coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 573.69GiB/s 2020-11-12 13:23:22.770470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: pciBusID: 0000:4a:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5 coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 573.69GiB/s 2020-11-12 13:23:22.776596: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll 2020-11-12 13:23:23.156041: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll 2020-11-12 13:23:23.159089: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll 2020-11-12 13:23:23.197670: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll 2020-11-12 13:23:23.224469: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll 2020-11-12 13:23:23.423940: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll 2020-11-12 13:23:23.606110: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll 2020-11-12 13:23:24.736751: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll 2020-11-12 13:23:24.739994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1 2020-11-12 13:23:24.742445: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2020-11-12 13:23:25.086293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: pciBusID: 0000:21:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5 coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 573.69GiB/s 2020-11-12 13:23:25.091969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: pciBusID: 0000:4a:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5 coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 573.69GiB/s 2020-11-12 13:23:25.098150: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll 2020-11-12 13:23:25.101109: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll 2020-11-12 13:23:25.104096: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll 2020-11-12 13:23:25.107660: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll 2020-11-12 13:23:25.110613: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll 2020-11-12 13:23:25.114070: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll 2020-11-12 13:23:25.117107: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll 2020-11-12 13:23:25.120321: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll 2020-11-12 13:23:25.124087: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1 2020-11-12 13:23:25.855306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix: 2020-11-12 13:23:25.858740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1 2020-11-12 13:23:25.860826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N N 2020-11-12 13:23:25.862739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   N N 2020-11-12 13:23:25.865414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8581 MB memory) -&gt; physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:21:00.0, compute capability: 7.5) 2020-11-12 13:23:25.871513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 8581 MB memory) -&gt; physical GPU (device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:4a:00.0, compute capability: 7.5) 2020-11-12 13:23:25.877802: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set 2020-11-12 13:23:26.305762: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2) 2020-11-12 13:23:26.553589: W tensorflow/core/kernels/data/cache_dataset_ops.cc:757] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to dataset.cache().take(k).repeat(). You should use dataset.take(k).cache().repeat() instead. 2020-11-12 13:23:26.563266: W tensorflow/core/kernels/data/cache_dataset_ops.cc:757] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to dataset.cache().take(k).repeat(). You should use dataset.take(k).cache().repeat() instead. 2020-11-12 13:23:26.940273: W tensorflow/core/kernels/data/cache_dataset_ops.cc:757] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to dataset.cache().take(k).repeat(). You should use dataset.take(k).cache().repeat() instead. 2020-11-12 13:23:26.951226: W tensorflow/core/kernels/data/cache_dataset_ops.cc:757] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to dataset.cache().take(k).repeat(). You should use dataset.take(k).cache().repeat() instead. 2020-11-12 13:23:30.401325: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll 2020-11-12 13:23:33.790176: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0 2020-11-12 13:23:33.839252: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0 2020-11-12 13:23:33.852673: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll 2020-11-12 13:23:34.345317: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
	</description>
	<comments>
		<comment id='1' author='zetez' date='2020-11-12T14:04:10Z'>
		I was trying with tf-nightly on Colab but it seems to freeze also without @tf.function. Can you try locally with tf-nighlty?
		</comment>
		<comment id='2' author='zetez' date='2020-11-12T14:44:19Z'>
		I ran it localy with  and it froze with  but ran fine without. It is not possible to reproduce this issue in Colab at the moment because tensorflow 2.4 and tf-nightly does not have access to GPUs due to &lt;denchmark-link:https://github.com/googlecolab/colabtools/issues/1574&gt;this issue&lt;/denchmark-link&gt;
. I don't think it froze in your case it just ran really slowly due to it running on the CPU. I also tried  on Colab and it worked with or without .
		</comment>
		<comment id='3' author='zetez' date='2020-11-12T15:24:57Z'>
		Yes I know that colab issue but I supposed It was already solved.
I will try again on Colab moving the debug prints.
		</comment>
		<comment id='4' author='zetez' date='2020-11-12T18:09:13Z'>
		Ok now I am printing every train_step on Colab (CPU) with @tf.function and I've already printed 15 train_step.
Can you run locally on CPU with tf-nighlty so we could check if it is a GPU only issue.
		</comment>
		<comment id='5' author='zetez' date='2020-11-13T08:33:53Z'>
		I ran it locally on tf-nightly for 20 train_step on the CPU with @tf.function without any problems so it has to be related to a GPU issue. I have noticed when running it on the GPU that it sometimes completes only one train_step and other times completes up to 5 train_step before freezing.
		</comment>
		<comment id='6' author='zetez' date='2020-11-14T15:26:17Z'>
		&lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;
 Temp I don't have a local GPU for tf-nightly/2.4. Please route on your side.
		</comment>
		<comment id='7' author='zetez' date='2020-11-15T09:06:17Z'>
		I'd like to also mention that I've been having issues with Tensorflow freezing the same way on Windows (I've also attempted using and building my own nightly (2.5.0-dev20201111) along with building my own 2.4 r1 on Python 3.8). I've also been trying to run cycle gans and gans, however I'm unsure if the architecture of the model is what's causing it.
A few tidbits I've noticed:

A lower batch size helps the program run longer, but it'll still eventually freeze.
When TF freezes, the CUDA usage in task manager fluctuates, and while when it's running normally its solid.
Removing dividing by STD in normalization layers seems to help Tensorflow run longer without freezing (although it still does it eventually) .
Sometimes when it "freezes" and I close the Python console, the memory in the GPU won't be released and task manager still shows usage for the CUDA cores.

I'm also curious about OP's hardware, as I think this might be an issue with Amphere as I'm using a 3080. I've been using CUDA 11.1 and cudnn 8.0.5.39
		</comment>
		<comment id='8' author='zetez' date='2020-11-15T19:28:15Z'>
		&lt;denchmark-link:https://github.com/aovokaitys&gt;@aovokaitys&lt;/denchmark-link&gt;

Nice to see that I'm not alone.
I'm using two 2080 Ti so I don't think this has anything to do with Ampere.
I launched an AWS g4dn.xlarge instance with Ubuntu 18.04 to see if I could reproduce this issue on Linux.
Using CUDA 11.0 and cuDNN 8.0.5 combined with TF 2.4 and tf-nightly there was no problems so it appears that the issue is related to Windows.
		</comment>
		<comment id='9' author='zetez' date='2020-11-16T00:28:22Z'>
		Now I've also tried Windows Server 2019 on AWS and there were no problems running TF 2.4 or tf-nightly.
I have no idea what the problem might be now.
		</comment>
		<comment id='10' author='zetez' date='2020-11-16T03:26:10Z'>
		&lt;denchmark-link:https://github.com/zetez&gt;@zetez&lt;/denchmark-link&gt;
 I've tried setting the 'CUDA_LAUNCH_BLOCKING' environment variable to '1' and that seems to have resolved/helped the issue. The model has so far ran for ~5k batches with it set, and 2 batches without. I'll have to run the model overnight though to confirm that it's stable, as I've also had another GAN model running without instance normalization for a few hours before freezing. Try setting that environment variable on your end to see if it helps you. Here's the code needed to be appended to the top of your Python script, if you don't know how to do it:
&lt;denchmark-code&gt;import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
&lt;/denchmark-code&gt;

Have a good day o/
		</comment>
		<comment id='11' author='zetez' date='2020-11-16T10:21:57Z'>
		&lt;denchmark-link:https://github.com/aovokaitys&gt;@aovokaitys&lt;/denchmark-link&gt;
 Thank you. Enabling  seems to have resolved the freezing issue.
Unfortunately it also doubles the training time compared to TF 2.3.1(~47% GPU utilization vs 95%) which is not ideal.
Since it works on Ubuntu 18.04 and Windows Server 2019 without any problems there has to be a way to make it work on our configurations as well.
		</comment>
		<comment id='12' author='zetez' date='2020-11-23T10:01:49Z'>
		Anyone have any idea what the problem might be or what steps to take next? I have really tried everything I can think of.
I have successfully run it on remote computers with both Windows and Linux so it is not a problem with the operating system.
I have successfully run it on remote computers with 2080 Ti so it is not a problem with 2080 Ti.
I have multiple graphics cards on my local machine and it doesn't work on any of them so it is very unlikely to be a hardware issue.
I have tried multiple graphics drivers with a clean uninstall(&lt;denchmark-link:https://www.guru3d.com/files-details/display-driver-uninstaller-download.html&gt;DDU&lt;/denchmark-link&gt;
) and it doesn't work on any of them.
I have tried &lt;denchmark-link:https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix&gt;cyclegan&lt;/denchmark-link&gt;
 in pytorch 1.7(Also uses CUDA 11.0 and cuDNN 8.0.X) on my local machine and it works without any problems so the issue appears to be related to tensorflow.
		</comment>
		<comment id='13' author='zetez' date='2020-12-02T23:03:55Z'>
		Fixed this issue by switching to pytorch.
Closing the issue since I have no interest in finding the solution anymore.
		</comment>
		<comment id='14' author='zetez' date='2020-12-02T23:03:57Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44801&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44801&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>