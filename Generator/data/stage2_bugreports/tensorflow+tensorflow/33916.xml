<bug id='33916' author='johan-andersson01' open_date='2019-11-01T14:29:06Z' closed_time='2019-11-23T00:22:46Z'>
	<summary>InternalError: Blas SGEMM launch failed : m=10, n=1, k=4 [Op:Conv2D] thrown when training Keras model with train_on_batch()</summary>
	<description>
System information


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):

Yes



OS Platform and Distribution (e.g., Linux Ubuntu 16.04):

Linux Ubuntu 18.04



Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:


TensorFlow installed from (source or binary):

Docker image, tensorflow/tensorflow:2.0.0-gpu-py3



TensorFlow version (use command below):

v2.0.0-rc2-26-g64c3d38 2.0.0



Python version:

3.6.8



Bazel version (if compiling from source):


GCC/Compiler version (if compiling from source):


CUDA/cuDNN version:


&lt;denchmark-code&gt;+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.87.00    Driver Version: 418.87.00    CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
&lt;/denchmark-code&gt;



GPU model and memory:

GeForce GTX 960M, 2004MiB



Describe the current behavior
When training a keras model with train_on_batch(), an InternalError is thrown (see stacktrace). Training the same model with fit() works.
Describe the expected behavior
No InternalError thrown and the attached code successfully executing.
Code to reproduce the issue
&lt;denchmark-code&gt;mkdir tf_issue
cp run.py tf_issue
docker run --gpus all -it  -v $(pwd)/tf_issue:/tf_issue tensorflow/tensorflow:2.0.0-gpu-py
python3 /tf_issue/run.py
&lt;/denchmark-code&gt;

(On my machine the above takes a few minutes when "setting up" Tensorflow, which is a bit ridiculous. But that's another issue)
&lt;denchmark-code&gt;## run.py ##
import tensorflow as tf
import numpy as np
from tensorflow.keras.layers import Input, Conv2D
from tensorflow.keras.optimizers import Adam

#tf.keras.backend.set_image_data_format("channels_first")

model = tf.keras.Sequential()
model.add(tf.keras.layers.Conv2D(filters=1, kernel_size=3, strides=2, input_shape=(224, 224, 3)))
model.add(tf.keras.layers.Conv2D(filters=1, kernel_size=3, strides=2))
model.add(tf.keras.layers.Conv2D(filters=1, kernel_size=3, strides=2))
model.add(tf.keras.layers.Conv2D(filters=1, kernel_size=3, strides=2))
model.add(tf.keras.layers.Conv2D(filters=1, kernel_size=3, strides=2))
model.add(tf.keras.layers.Conv2D(filters=1, kernel_size=3, strides=2))
model.add(tf.keras.layers.Conv2D(filters=1, kernel_size=2, strides=1, activation='sigmoid'))

model.compile( optimizer=Adam(), loss='binary_crossentropy')

model.summary()

gpu = tf.test.is_gpu_available()
print("GPU is available:", gpu)
assert(gpu)

batch_size = 10
x = np.random.random((batch_size, 224, 224, 3))
y = np.random.random((batch_size, 1, 1, 1))

print("x", x.shape)
print("y", y.shape)

y_pred = model.predict(x)
print("Predict successful: ", y_pred.shape)

print("Begin training with fit")
model.fit(x, y, epochs=10)
print("Fit successful")

print("Begin training with train_on_batch")
for i in range(10):
    model.train_on_batch(x, y)
print("On batch successful")
&lt;/denchmark-code&gt;

Other info / logs
Output and stack trace:
&lt;denchmark-code&gt;Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 111, 111, 1)       28        
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 55, 55, 1)         10        
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 27, 27, 1)         10        
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 13, 13, 1)         10        
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 6, 6, 1)           10        
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 2, 2, 1)           10        
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 1, 1, 1)           5         
=================================================================
Total params: 83
Trainable params: 83
Non-trainable params: 0
_________________________________________________________________
GPU is available: True
x (10, 224, 224, 3)
y (10, 1, 1, 1)
Predict successful:  (10, 1, 1, 1)
Begin training with fit
Train on 10 samples
Epoch 1/10

10/10 [==============================] - 1s 82ms/sample - loss: 0.6654
Epoch 2/10

10/10 [==============================] - 0s 870us/sample - loss: 0.6633
Epoch 3/10

10/10 [==============================] - 0s 882us/sample - loss: 0.6613
Epoch 4/10

10/10 [==============================] - 0s 1ms/sample - loss: 0.6594
Epoch 5/10

10/10 [==============================] - 0s 979us/sample - loss: 0.6574
Epoch 6/10

10/10 [==============================] - 0s 902us/sample - loss: 0.6555
Epoch 7/10

10/10 [==============================] - 0s 872us/sample - loss: 0.6536
Epoch 8/10

10/10 [==============================] - 0s 960us/sample - loss: 0.6517
Epoch 9/10

10/10 [==============================] - 0s 905us/sample - loss: 0.6498
Epoch 10/10

10/10 [==============================] - 0s 1ms/sample - loss: 0.6479
Fit successful
Begin training with train_on_batch
 not compiled to use: AVX2 FMA
2019-11-01 13:45:13.958471: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2592000000 Hz
2019-11-01 13:45:13.959294: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4595560 executing computations on platform Host. Devices:
2019-11-01 13:45:13.959336: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2019-11-01 13:45:14.000846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-01 13:45:14.001574: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x45973c0 executing computations on platform CUDA. Devices:
2019-11-01 13:45:14.001596: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 960M, Compute Capability 5.0
2019-11-01 13:45:14.001719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-01 13:45:14.002377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 960M major: 5 minor: 0 memoryClockRate(GHz): 1.0975
pciBusID: 0000:01:00.0
2019-11-01 13:45:14.002406: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-11-01 13:45:14.002417: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-11-01 13:45:14.002427: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-11-01 13:45:14.002444: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-11-01 13:45:14.002454: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-11-01 13:45:14.002463: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-11-01 13:45:14.002473: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-11-01 13:45:14.002522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-01 13:45:14.003154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-01 13:45:14.003750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-11-01 13:45:14.003777: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-11-01 13:45:14.004647: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-11-01 13:45:14.004660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2019-11-01 13:45:14.004667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2019-11-01 13:45:14.004779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-01 13:45:14.005421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-01 13:45:14.006043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1742 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0)
2019-11-01 13:45:15.207106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-01 13:45:15.207755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 960M major: 5 minor: 0 memoryClockRate(GHz): 1.0975
pciBusID: 0000:01:00.0
2019-11-01 13:45:15.207787: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-11-01 13:45:15.207801: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-11-01 13:45:15.207813: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-11-01 13:45:15.207824: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-11-01 13:45:15.207836: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-11-01 13:45:15.207847: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-11-01 13:45:15.207859: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-11-01 13:45:15.207910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-01 13:45:15.208535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-01 13:45:15.209130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-11-01 13:45:15.209154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-11-01 13:45:15.209161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2019-11-01 13:45:15.209168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2019-11-01 13:45:15.209276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-01 13:45:15.209931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-01 13:45:15.210556: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:0 with 1742 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0)
2019-11-01 13:45:15.339203: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-11-01 13:45:17.264840: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-11-01 13:45:17.421437: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-11-01 13:45:17.421474: W tensorflow/stream_executor/stream.cc:1919] attempting to perform BLAS operation using StreamExecutor without BLAS support
Traceback (most recent call last):
  File "test_sample_weights.py", line 41, in &lt;module&gt;
    model.train_on_batch(x, y)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py", line 973, in train_on_batch
    class_weight=class_weight, reset_metrics=reset_metrics)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py", line 264, in train_on_batch
    output_loss_metrics=model._output_loss_metrics)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_eager.py", line 311, in train_on_batch
    output_loss_metrics=output_loss_metrics))
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_eager.py", line 252, in _process_single_batch
    training=training))
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_eager.py", line 127, in _model_loss
    outs = model(inputs, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py", line 891, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/sequential.py", line 256, in call
    return super(Sequential, self).call(inputs, training=training, mask=mask)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py", line 708, in call
    convert_kwargs_to_constants=base_layer_utils.call_context().saving)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py", line 860, in _run_internal_graph
    output_tensors = layer(computed_tensors, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py", line 891, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/convolutional.py", line 197, in call
    outputs = self._convolution_op(inputs, self.kernel)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py", line 1134, in __call__
    return self.conv_op(inp, filter)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py", line 639, in __call__
    return self.call(inp, filter)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py", line 238, in __call__
    name=self.name)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py", line 2010, in conv2d
    name=name)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_nn_ops.py", line 1031, in conv2d
    data_format=data_format, dilations=dilations, name=name, ctx=_ctx)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_nn_ops.py", line 1130, in conv2d_eager_fallback
    ctx=_ctx, name=name)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File "&lt;string&gt;", line 3, in raise_from
tensorflow.python.framework.errors_impl.InternalError: Blas SGEMM launch failed : m=10, n=1, k=4 [Op:Conv2D]
&lt;/denchmark-code&gt;

Docker version:
&lt;denchmark-code&gt;Client: Docker Engine - Community
 Version:           19.03.4
 API version:       1.40
 Go version:        go1.12.10
 Git commit:        9013bf583a
 Built:             Fri Oct 18 15:54:09 2019
 OS/Arch:           linux/amd64
 Experimental:      false

Server: Docker Engine - Community
 Engine:
  Version:          19.03.4
  API version:      1.40 (minimum version 1.12)
  Go version:       go1.12.10
  Git commit:       9013bf583a
  Built:            Fri Oct 18 15:52:40 2019
  OS/Arch:          linux/amd64
  Experimental:     false
 containerd:
  Version:          1.2.10
  GitCommit:        b34a5c8af56e510852c35414db4c1f4fa6172339
 runc:
  Version:          1.0.0-rc8+dev
  GitCommit:        3e425f80a8c931f88e6d94a8c831b9d5aa481657
 docker-init:
  Version:          0.18.0
  GitCommit:        fec3683
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='johan-andersson01' date='2019-11-01T14:36:29Z'>
		After some research into the error message , I tried adding the following to the start of  as specified in the &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth&gt;docs&lt;/denchmark-link&gt;
:
&lt;denchmark-code&gt;physical_devices = tf.config.experimental.list_physical_devices('GPU')
assert len(physical_devices) &gt; 0, "Not enough GPU hardware devices available"
tf.config.experimental.set_memory_growth(physical_devices[0], True)
&lt;/denchmark-code&gt;

The script then behaves as expected. But I suppose this still is an issue due to the different behaviours of fit and train_on_batch?
		</comment>
		<comment id='2' author='johan-andersson01' date='2019-11-05T10:30:40Z'>
		&lt;denchmark-link:https://github.com/johan-andersson01&gt;@johan-andersson01&lt;/denchmark-link&gt;
 ,
Can you please check this &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/27820#issuecomment-491014072&gt;comment&lt;/denchmark-link&gt;
 and let me know if it helps ?Thanks!
		</comment>
		<comment id='3' author='johan-andersson01' date='2019-11-06T16:02:35Z'>
		&lt;denchmark-link:https://github.com/oanush&gt;@oanush&lt;/denchmark-link&gt;
 My laptop only has one GPU, so does that apply?
		</comment>
		<comment id='4' author='johan-andersson01' date='2019-11-12T21:34:08Z'>
		&lt;denchmark-link:https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth&gt;Limiting gpu memory growth&lt;/denchmark-link&gt;
, switching to single tensorflow process (avoid multiple tf process that consume gpu memory) is a good practice in situations like these. I will close this issue since this resolves your issue. Thanks!
		</comment>
		<comment id='5' author='johan-andersson01' date='2019-11-23T00:22:47Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33916&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33916&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>