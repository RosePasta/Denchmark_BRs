<bug id='27517' author='EricHeidal' open_date='2019-04-04T18:45:43Z' closed_time='2019-04-20T01:03:02Z'>
	<summary>ML Kit for Android fails to load valid TFLite model</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: BLU Vivo XI
TensorFlow installed from (source or binary): Source
TensorFlow version: ('v1.13.1-0-g6612da8951', '1.13.1')
Python version: 2.7
Bazel version (if compiling from source): 0.24.0
GCC/Compiler version (if compiling from source): 7.3.0
CUDA/cuDNN version: N/A
GPU model and memory: NVIDIA GeForce GTX 1050, ~5 GB


I am currently attempting to use Firebase ML Kit to run a Tensorflow-Lite model that was converted to the .tflite format from Keras. I have been following &lt;denchmark-link:https://firebase.google.com/docs/ml-kit/android/use-custom-models&gt;this&lt;/denchmark-link&gt;
 tutorial to host a custom model locally.
Every attempt thus far has ended with the same error: "ByteBuffer is not a valid flatbuffer model" (full stack trace shown below).
The .tflite model has been created correctly; running the Interpreter (from the Python Tensorflow module) to allocate tensors and get the input and output details returned the proper information without error.
Furthermore, the iOS version of the app (using the same model, stored locally and loaded with Firebase ML Kit) is working properly. It used a custom build of Tensorflow (following &lt;denchmark-link:https://firebase.google.com/docs/ml-kit/ios/use-custom-tflite&gt;this&lt;/denchmark-link&gt;
 tutorial) that started by forking  Tensorflow 12.0.0 and then cherry-picked future commits to include the additional ops required. I did not write any custom ops myself; all of the required ops had already been added to Tensorflow at one time or another.
First, I attempted to use the same build by following &lt;denchmark-link:https://firebase.google.com/docs/ml-kit/android/use-custom-tflite&gt;this&lt;/denchmark-link&gt;
 tutorial with Bazel version 0.18.0, which failed. I also attempted &lt;denchmark-link:https://heartbeat.fritz.ai/compiling-a-tensorflow-lite-build-with-custom-operations-cf6330ee30e2&gt;this&lt;/denchmark-link&gt;
 tutorial to import TFLite as a module rather than publish it to my local maven repository. This had the same result.
Then, under the assumption that the latest release of Tensorflow would have all the necessary ops anyway, I built Tensorflow 1.13.1 using Bazel version 0.24.0. This failed with the same error.
Here are my questions, as succinctly as possible:

Is ML Kit currently unequipped to handle the most recent versions of Tensorflow-Lite? (If so, why would the iOS version still work?)
Is Tensorflow-Lite for Android currently not working for Tensorflow 1.13.1? I'm no expert in machine learning, so this may be nonsensical, but are there operations/features created for Tensorflow that Tensorflow-Lite is unable to handle? (And again, if this was the case, why would the iOS version still work?)
Are the tutorials for ML Kit missing any essential information?
Has anyone else been able to get a custom model converted to .tflite and have it work with ML Kit?

Other info / logs
E/ModelResourceManager: Error preloading model resource
com.google.firebase.ml.common.FirebaseMLException: Local model load failed:
at com.google.android.gms.internal.firebase_ml.zzpe.zza(Unknown Source:129)
at com.google.android.gms.internal.firebase_ml.zzpe.zzlp(Unknown Source:104)
at com.google.android.gms.internal.firebase_ml.zznx.zzf(Unknown Source:56)
at com.google.android.gms.internal.firebase_ml.zznz.zzls(Unknown Source:7)
at com.google.android.gms.internal.firebase_ml.zznz.call(Unknown Source:24)
at com.google.android.gms.internal.firebase_ml.zznn.zza(Unknown Source:29)
at com.google.android.gms.internal.firebase_ml.zzno.run(Unknown Source:2)
at android.os.Handler.handleCallback(Handler.java:790)
at android.os.Handler.dispatchMessage(Handler.java:99)
at com.google.android.gms.internal.firebase_ml.zzi.dispatchMessage(Unknown Source:6)
at android.os.Looper.loop(Looper.java:164)
at android.os.HandlerThread.run(HandlerThread.java:65)
Caused by: java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model
at org.tensorflow.lite.NativeInterpreterWrapper.createModelWithBuffer(Native Method)
at org.tensorflow.lite.NativeInterpreterWrapper.(NativeInterpreterWrapper.java:69)
at org.tensorflow.lite.Interpreter.(Interpreter.java:175)
at org.tensorflow.lite.Interpreter.(Interpreter.java:163)
at com.google.android.gms.internal.firebase_ml.zzpe.zzc(Unknown Source:224)
at com.google.android.gms.internal.firebase_ml.zzpf.zzd(Unknown Source:0)
at com.google.android.gms.internal.firebase_ml.zzpe.zzb(Unknown Source:150)
at com.google.android.gms.internal.firebase_ml.zzpe.zza(Unknown Source:118)
at com.google.android.gms.internal.firebase_ml.zzpe.zzlp(Unknown Source:104) 
at com.google.android.gms.internal.firebase_ml.zznx.zzf(Unknown Source:56) 
at com.google.android.gms.internal.firebase_ml.zznz.zzls(Unknown Source:7) 
at com.google.android.gms.internal.firebase_ml.zznz.call(Unknown Source:24) 
at com.google.android.gms.internal.firebase_ml.zznn.zza(Unknown Source:29) 
at com.google.android.gms.internal.firebase_ml.zzno.run(Unknown Source:2) 
at android.os.Handler.handleCallback(Handler.java:790) 
at android.os.Handler.dispatchMessage(Handler.java:99) 
at com.google.android.gms.internal.firebase_ml.zzi.dispatchMessage(Unknown Source:6) 
at android.os.Looper.loop(Looper.java:164) 
at android.os.HandlerThread.run(HandlerThread.java:65)
	</description>
	<comments>
		<comment id='1' author='EricHeidal' date='2019-04-20T01:03:01Z'>
		Issue was fixed using these gradle dependencies together:
implementation 'com.google.firebase:firebase-ml-model-interpreter:18.0.0'
implementation 'org.tensorflow:tensorflow-lite:1.13.1@aar'
		</comment>
		<comment id='2' author='EricHeidal' date='2019-04-20T01:03:03Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=27517&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=27517&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>