<bug id='25970' author='bersbersbers' open_date='2019-02-21T14:38:50Z' closed_time='2019-03-12T17:36:02Z'>
	<summary>tf.keras computes incorrect loss values with 3+D data</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):

Yes. For a minimal example, run
&lt;denchmark-code&gt;from tensorflow import keras

layer = keras.layers.Input(shape=(1, 1, 1))
model = keras.models.Model(inputs=layer, outputs=layer)
model.compile(optimizer='adam', loss='poisson', metrics=['poisson'])
data = [[[[[1]]], [[[2]]], [[[3]]]]]
model.fit(x=data, y=data, batch_size=2, verbose=1, epochs=10)
&lt;/denchmark-code&gt;

and observe that loss and poisson values are different, and loss values vary:
&lt;denchmark-code&gt;Epoch 1/10
3/3 [==============================] - 1s 236ms/sample - loss: 0.6740 - poisson: 0.4393
Epoch 2/10
3/3 [==============================] - 0s 2ms/sample - loss: 0.6740 - poisson: 0.4393
Epoch 3/10
3/3 [==============================] - 0s 40ms/sample - loss: 0.5452 - poisson: 0.4393
Epoch 4/10
3/3 [==============================] - 0s 96ms/sample - loss: 0.5452 - poisson: 0.4393
Epoch 5/10
3/3 [==============================] - 0s 1ms/sample - loss: 0.9772 - poisson: 0.4393
Epoch 6/10
3/3 [==============================] - 0s 2ms/sample - loss: 0.6740 - poisson: 0.4393
Epoch 7/10
3/3 [==============================] - 1s 201ms/sample - loss: 0.6740 - poisson: 0.4393
Epoch 8/10
3/3 [==============================] - 0s 2ms/sample - loss: 0.6740 - poisson: 0.4393
Epoch 9/10
3/3 [==============================] - 0s 999us/sample - loss: 0.9772 - poisson: 0.4393
Epoch 10/10
3/3 [==============================] - 1s 327ms/sample - loss: 0.9772 - poisson: 0.4393
&lt;/denchmark-code&gt;



OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10


TensorFlow installed from (source or binary):
pip install tensorflow


TensorFlow version (use command below):
v1.13.0-rc1-19-gc865ec5621, 1.13.0-rc2


Python version:
3.7.2 x64


CUDA/cuDNN version:
n/a


GPU model and memory:
n/a


Describe the current behavior
When fitting a model with loss="poisson", I would expect reported loss and poisson values to be identical.
Describe the expected behavior
loss values are incorrect. They vary from epoch to epoch.
Code to reproduce the issue
See above.
Other info / logs
More code examples and investigations at &lt;denchmark-link:https://stackoverflow.com/q/54802328/880783&gt;https://stackoverflow.com/q/54802328/880783&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='bersbersbers' date='2019-02-28T17:34:25Z'>
		&lt;denchmark-link:https://github.com/bersbersbers&gt;@bersbersbers&lt;/denchmark-link&gt;
 are you still seeing this issue? I was not able to repro this on the latest nightly.
		</comment>
		<comment id='2' author='bersbersbers' date='2019-02-28T18:11:30Z'>
		&lt;denchmark-link:https://github.com/pavithrasv&gt;@pavithrasv&lt;/denchmark-link&gt;
 you are right,  does not have this issue. I can still repro it in  as well as in  which has been released in the meantime. Since the issue reproduces in a stable release, I would be very interested what the underlying cause is and, in particular, how long it has existed.
		</comment>
		<comment id='3' author='bersbersbers' date='2019-02-28T18:17:57Z'>
		Here's my pip freeze in my current installation that does repro the issue:
&lt;denchmark-code&gt;absl-py==0.7.0
altgraph==0.16.1
astor==0.7.1
astroid==2.1.0
astropy==3.1.2
autopep8==1.4.3
awkward==0.8.4
bz2file==0.98
cachetools==3.1.0
certifi==2018.11.29
chardet==3.0.4
colorama==0.4.1
cycler==0.10.0
decorator==4.3.2
future==0.17.1
gast==0.2.2
grpcio==1.18.0
h5py==2.9.0
idna==2.8
imageio==2.5.0
imageio-ffmpeg==0.2.0
isbnlib==3.9.6
isbnlib-dnb==0.0.3
isbntools==4.3.19
isort==4.3.9
Keras==2.2.4
Keras-Applications==1.0.7
Keras-Preprocessing==1.0.9
kiwisolver==1.0.1
lazy-object-proxy==1.3.1
macholib==1.11
Markdown==3.0.1
matplotlib==3.0.2
mccabe==0.6.1
mock==2.0.0
moviepy==1.0.0
nibabel==2.3.3
numpy==1.16.1
packaging==19.0
pandas==0.24.1
pbr==5.1.2
pefile==2018.8.8
Pillow==5.4.1
pip-review==1.0
pipdeptree==0.13.2
proglog==0.1.9
protobuf==3.6.1
psutil==5.5.1
py-essentials==1.4.12
pycodestyle==2.5.0
pydicom==1.2.2
pyhibp==3.0.0
PyInstaller==3.4
PyJWT==1.7.1
pylint==2.2.2
pyparsing==2.3.1
pypng==0.0.19
python-dateutil==2.8.0
pytz==2018.9
pywin32==224
pywin32-ctypes==0.2.0
PyYAML==3.13
requests==2.21.0
rope==0.12.0
scikit-learn==0.20.2
scipy==1.2.1
seaborn==0.9.0
six==1.12.0
sklearn==0.0
tee==0.0.3
tensorboard==1.13.0
tensorflow==1.13.1
tensorflow-estimator==1.13.0
termcolor==1.1.0
tqdm==4.31.1
uproot==3.4.6
uproot-methods==0.4.3
urllib3==1.24.1
Werkzeug==0.14.1
wrapt==1.11.1
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='bersbersbers' date='2019-03-06T17:58:02Z'>
		I tried to pin down when this issue was introduced and fixed:
&lt;denchmark-code&gt;True = has bug

tf-nightly==
1.13.0-dev20190101    True
1.13.0-dev20190124    True
1.13.0-dev20190125    True
1.13.0-dev20190129    False
1.13.0-dev20190206    False
1.13.0.dev20190227    False
1.14.1-dev20190306    False

tensorflow==
1.10.0        False (Aug 8, 2018)
1.11.0-rc0    True (Sep 13, 2018)
1.11.0        True
1.12.0        True
1.13.0-rc1    True
1.13.1        True (Feb 26, 2019)
2.0.0-alpha0  False (Mar 6, 2019)
&lt;/denchmark-code&gt;

So, introduced some time in Aug/Sep 2018 - due to missing  packages on  from that time, I cannot get any closer. Fixed some time between Jan 25 and 29. That makes about 600 commits:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/search?q=committer-date%3A2019-01-24..2019-01-30&amp;unscoped_q=committer-date%3A2019-01-24..2019-01-30&amp;type=Commits&gt;https://github.com/tensorflow/tensorflow/search?q=committer-date%3A2019-01-24..2019-01-30&amp;unscoped_q=committer-date%3A2019-01-24..2019-01-30&amp;type=Commits&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='bersbersbers' date='2019-03-12T17:36:02Z'>
		I am closing this issue as it has been fixed. Thank you for digging into the release details!
		</comment>
		<comment id='6' author='bersbersbers' date='2019-04-29T12:32:30Z'>
		This bug is still in the most current version, 1.13.1. Is there any release scheduled to be released soon to fix this? If not, I would be glad to use a workaround, but so far, I have not found any.
By the way, this is a reduced example where the batch size does divide the number of samples:
&lt;denchmark-code&gt;from tensorflow.keras import layers, metrics, models

layer = layers.Input(shape=(1, 1, 1))
model = models.Model(inputs=layer, outputs=layer)
model.compile(optimizer='adam', loss=metrics.mse, metrics=[metrics.mse])
model.fit(x=[[[[[0]]], [[[0]]]]], y=[[[[[1]]], [[[1]]]]])
&lt;/denchmark-code&gt;

It outputs:
&lt;denchmark-code&gt;2/2 [==============================]
 - 0s 23ms/sample - loss: 2.0000 - mean_squared_error: 1.0000
&lt;/denchmark-code&gt;

Note how loss and mean_squared_error are different. How can I get them to be identical?
		</comment>
		<comment id='7' author='bersbersbers' date='2019-04-30T18:15:40Z'>
		Have you tried &lt;denchmark-link:https://github.com/tensorflow/tensorflow/releases/tag/v2.0.0-alpha0&gt;https://github.com/tensorflow/tensorflow/releases/tag/v2.0.0-alpha0&lt;/denchmark-link&gt;
? We will also have a TF 1.14 release very soon.
		</comment>
		<comment id='8' author='bersbersbers' date='2019-04-30T19:14:19Z'>
		
Have you tried https://github.com/tensorflow/tensorflow/releases/tag/v2.0.0-alpha0?

Yes, I have, see &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/25970#issuecomment-470210090&gt;#25970 (comment)&lt;/denchmark-link&gt;
. TF2 does not have that issue, but I don't want my research to rely on Alpha software currently :)

We will also have a TF 1.14 release very soon.

That is good news, thank you!
		</comment>
		<comment id='9' author='bersbersbers' date='2019-05-25T09:04:10Z'>
		I can confirm that this bug has been fixed in 1.14.0rc0:
&lt;denchmark-code&gt;from tensorflow import keras
layer = keras.layers.Input(shape=(1, 1, 1))
model = keras.models.Model(inputs=layer, outputs=layer)
model.compile(optimizer='adam', loss='poisson')
data = [[[[[1]]], [[[2]]], [[[3]]]]]
model.fit(x=data, y=data, batch_size=2, verbose=2, epochs=10)
&lt;/denchmark-code&gt;

Output:
&lt;denchmark-code&gt;WARNING: Logging before flag parsing goes to stderr.
W0525 11:03:20.369000  6872 __init__.py:308] Limited tf.compat.v2.summary API due to missing TensorBoard installation.
Epoch 1/10
3/3 - 0s - loss: 0.4393
Epoch 2/10
3/3 - 0s - loss: 0.4393
Epoch 3/10
3/3 - 0s - loss: 0.4393
Epoch 4/10
3/3 - 0s - loss: 0.4393
Epoch 5/10
3/3 - 0s - loss: 0.4393
Epoch 6/10
3/3 - 0s - loss: 0.4393
Epoch 7/10
3/3 - 0s - loss: 0.4393
Epoch 8/10
3/3 - 0s - loss: 0.4393
Epoch 9/10
3/3 - 0s - loss: 0.4393
Epoch 10/10
3/3 - 0s - loss: 0.4393
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='bersbersbers' date='2019-05-25T15:28:00Z'>
		Thank you!
		</comment>
		<comment id='11' author='bersbersbers' date='2020-04-10T05:37:01Z'>
		I am using tf 2.1.0 and experience the same problem, can you suggest anything?
Thank you!
		</comment>
		<comment id='12' author='bersbersbers' date='2020-05-10T10:52:32Z'>
		I am also getting a delta between mse loss and mse metric values, but only when applying regularization (l2 or dropout).
		</comment>
		<comment id='13' author='bersbersbers' date='2020-09-21T11:29:19Z'>
		&lt;denchmark-link:https://user-images.githubusercontent.com/10877157/93761631-ae43b200-fc40-11ea-9913-c1636f45d86b.png&gt;&lt;/denchmark-link&gt;

+1
		</comment>
		<comment id='14' author='bersbersbers' date='2020-10-03T11:46:38Z'>
		I have the same problem with tensorflow 2.3.0 when using l1/l2 regularization.
		</comment>
		<comment id='15' author='bersbersbers' date='2020-10-04T09:37:31Z'>
		I'm having the same issue with TensorFlow GPU 2.1.0 and no regularization. However, this happens only on the validation step.
		</comment>
		<comment id='16' author='bersbersbers' date='2020-10-09T09:54:46Z'>
		&lt;denchmark-link:https://github.com/oO0oO0oO0o0o00&gt;@oO0oO0oO0o0o00&lt;/denchmark-link&gt;
  I have the same strange problem.
		</comment>
	</comments>
</bug>