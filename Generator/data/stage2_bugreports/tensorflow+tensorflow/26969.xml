<bug id='26969' author='jacobhilton' open_date='2019-03-21T04:12:38Z' closed_time='2019-12-26T19:11:16Z'>
	<summary>Batched matmul gives incorrect result on GPU with 32-bit precision and batch size &amp;gt;= 2 ** 16</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): not sure
TensorFlow version (use command below): v1.12.0-0-ga6d8ffa
Python version: 3.6.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: CUDA 10.0
GPU model and memory: GeForce GTX 1080 Ti, 11178MiB

Describe the current behavior
When computing a batched matmul with 32-bit precision and batch size &gt;= 2 ** 16, the first 2 ** 16 - 1 batch elements of the result are correct, and the remaining elements are arbitrary - often zero, but not always. When the sess.run call is made multiple times, the result is usually the same or similar. The bug only seems to occur when using a variable or placeholder.
Describe the expected behavior
Every batch element of the result should be correct.
Code to reproduce the issue
&lt;denchmark-code&gt;import numpy as np
import tensorflow as tf

s = (100000, 1, 1)
p = tf.placeholder(shape=s, dtype=tf.float32)
x = tf.matmul(tf.ones(s), p)

with tf.Session() as sess:
    r = sess.run(x, feed_dict={p: np.ones(s, dtype=np.float32)})
    print(r[2**16 - 5:2**16 + 5, 0, 0])
&lt;/denchmark-code&gt;

Typical output:
&lt;denchmark-code&gt;[1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]
&lt;/denchmark-code&gt;

Other info / logs
	</description>
	<comments>
		<comment id='1' author='jacobhilton' date='2019-06-07T22:25:14Z'>
		Reproduced on:

Tensorflow 1.13.1
Cuda 10.0
Titan RTX
Installed from pip
Ubuntu 18.04.2 LTS

I don't have a separate minimal reproducing example, but I have noticed that in my application, it is not the case that the prefix of the result is correct. In my application, I was doing trilinear interpolation manually on pixels of human-interpretable MRI images. Here is are several pictures of the output (viewed as normalized float 32s), which has a striking pattern -- it may be of help in diagnosing what went wrong. The problem happens only when I use  a GPU, and can be avoided if I wrap my matmul inside with tf.device('cpu:0'). Too my untrained eyes, this looks like some kind of issue where memory got deallocated too early, or overwritten, or maybe the dtype / stride size is wrong somewhere.
&lt;denchmark-link:https://user-images.githubusercontent.com/4404828/59136504-4d537600-8938-11e9-935a-a6c691391242.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/4404828/59136505-4d537600-8938-11e9-9e3c-5ac1d23b08ac.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/4404828/59136507-4d537600-8938-11e9-9f41-d22f8f7d1bec.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/4404828/59136508-4d537600-8938-11e9-8253-377d9061374d.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='jacobhilton' date='2019-06-08T02:35:51Z'>
		This is likely caused by cublasGemmBatchedEx function in the cuBLAS library. I will report this issue to NVIDIA after creating a small example outside of TensorFlow.
		</comment>
		<comment id='3' author='jacobhilton' date='2019-08-15T13:37:54Z'>
		I experienced the same bug and maybe this helps:
If matmul is first called with input tensors of a batch size below 2**16 and then again called with input tensors of a batch size &gt;=2**16, all entries in the result above 2**16-1 are random memory.
Here is a short &lt;denchmark-link:https://drive.google.com/open?id=1txY0rHNIeUkp7c38X65Hh4Cz6e_2_ZZF&gt;colab example&lt;/denchmark-link&gt;

This error does not occur if matmul is initially called with input tensors of batch_size above 2**16-1!
So my workaround will be initializing all matmul operations in my graph with large inputs.
Did not work in my main graph, maybe only works in the special case with zeros in the colab example...
		</comment>
		<comment id='4' author='jacobhilton' date='2019-12-26T19:11:16Z'>
		Duplicate of &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/31166&gt;#31166&lt;/denchmark-link&gt;

Doesn't reproduce with 2.1.0-dev20191226
		</comment>
		<comment id='5' author='jacobhilton' date='2019-12-26T19:11:18Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26969&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26969&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>