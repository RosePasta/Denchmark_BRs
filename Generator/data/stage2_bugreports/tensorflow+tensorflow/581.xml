<bug id='581' author='SkidanovAlex' open_date='2015-12-21T18:16:30Z' closed_time='2015-12-21T19:42:18Z'>
	<summary>ConvNet tutorial</summary>
	<description>
&lt;denchmark-link:https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/g3doc/tutorials/deep_cnn/index.md&gt;https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/g3doc/tutorials/deep_cnn/index.md&lt;/denchmark-link&gt;

Quote:

EXERCISE: The model architecture in inference() differs slightly from the CIFAR-10 model specified in cuda-convnet. In particular, the top layers of Alexâ€™s original model are locally connected and not fully connected. Try editing the architecture to exactly reproduce the locally connected architecture in the top layer.

When I look at the Figure2 of AlexNet paper, it seems that the only locally connected layers are conv1 and conv3 (which I would classify as "bottom" layers), both fully connected layers (which are "top" layers in my understanding) are connected across the two GPUs. Or am I not understanding the exercise correctly?
	</description>
	<comments>
		<comment id='1' author='SkidanovAlex' date='2015-12-21T18:25:56Z'>
		I closed it at first because I realized that it refers to the CIFAR-10 paper by Alex, not to AlexNet, but now that I reread the tutorial, it appears to me that it actually implements the AlexNet, not the CIFAR-10 (at least not this one: &lt;denchmark-link:http://www.cs.utoronto.ca/~kriz/conv-cifar10-aug2010.pdf&gt;http://www.cs.utoronto.ca/~kriz/conv-cifar10-aug2010.pdf&lt;/denchmark-link&gt;
), because the CIFAR-10 paper by Alex is only two-layers deep and has RBMs, which I don't see in the tutorial, so I guess my question from the original post here is still valid.
		</comment>
		<comment id='2' author='SkidanovAlex' date='2015-12-21T19:42:18Z'>
		The architecture used by Alex for CIFAR-10 in cuda-convnet was never published. The most up-to-date code for specifying this architecture is probably Alex's branch in cuda-convnet2 on github.
&lt;denchmark-link:https://github.com/akrizhevsky/cuda-convnet2/blob/master/layers/layer-params-cifar10-11pct.cfg&gt;https://github.com/akrizhevsky/cuda-convnet2/blob/master/layers/layer-params-cifar10-11pct.cfg&lt;/denchmark-link&gt;

Note that what differs in his architecture is the usage of local3 and local4 in lieu of fully connected layers.
		</comment>
	</comments>
</bug>