<bug id='27339' author='Mrpatekful' open_date='2019-03-31T08:34:36Z' closed_time='2019-04-12T20:45:51Z'>
	<summary>TPU training with Keras API raises error in Tensorflow 2.0</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
TensorFlow installed from (source or binary): no
TensorFlow version (use command below): 2.0.0.dev20190330
Python version: 3.6
Bazel version (if compiling from source): -
GCC/Compiler version (if compiling from source): -
CUDA/cuDNN version: -
GPU model and memory: Google Colab Colud TPU

The current behavior
By following the &lt;denchmark-link:https://www.tensorflow.org/alpha/guide/distribute_strategy&gt;official&lt;/denchmark-link&gt;
 guide on Tensorflow 2.0 distributed training, the  method raises an error regarding unregistered op type .
The expected behavior
The expected bahaviour is running code without any exception.
Code to reproduce the issue
The code in the &lt;denchmark-link:https://gist.github.com/Mrpatekful/60401e8c7b8965bb3624c2c2e9b8df0f&gt;google colab&lt;/denchmark-link&gt;
.
!pip install tf-nightly-2.0-preview
from __future__ import absolute_import, division, print_function
import tensorflow as tf

tf.compat.v1.disable_eager_execution()

resolver = tf.distribute.cluster_resolver.TPUClusterResolver()
tf.tpu.experimental.initialize_tpu_system(resolver)
tpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)

with tpu_strategy.scope():
  model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])
  model.compile(loss='mse', optimizer='sgd')

BATCH_SIZE_PER_REPLICA = 10
batch_size = BATCH_SIZE_PER_REPLICA * tpu_strategy.num_replicas_in_sync
dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(1000).batch(batch_size, drop_remainder=True)
model.fit(dataset, epochs=2)
model.evaluate(dataset)
Other info / logs
The full exception:
&lt;denchmark-code&gt;WARNING: Logging before flag parsing goes to stderr.
W0331 08:24:17.732600 140554474739584 training_utils.py:1268] Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.
---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1336     try:
-&gt; 1337       return fn(*args)
   1338     except errors.OpError as e:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1319       # Ensure any changes to the graph are reflected in the runtime.
-&gt; 1320       self._extend_graph()
   1321       return self._call_tf_sessionrun(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _extend_graph(self)
   1354     with self._graph._session_run_lock():  # pylint: disable=protected-access
-&gt; 1355       tf_session.ExtendSession(self._session)
   1356 

NotFoundError: Op type not registered 'ExperimentalRebatchDataset' in binary running on n-23c14aca-w-0. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.

During handling of the above exception, another exception occurred:

NotFoundError                             Traceback (most recent call last)
&lt;ipython-input-7-60cafe7e9d96&gt; in &lt;module&gt;()
      2 batch_size = BATCH_SIZE_PER_REPLICA * tpu_strategy.num_replicas_in_sync
      3 dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(1000).batch(batch_size, drop_remainder=True)
----&gt; 4 model.fit(dataset, epochs=2)
      5 model.evaluate(dataset)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    745             steps_per_epoch=steps_per_epoch,
    746             validation_steps=validation_steps,
--&gt; 747             validation_freq=validation_freq)
    748 
    749     batch_size = self._validate_or_infer_batch_size(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_distributed.py in fit_distributed(model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq)
    117         steps_per_epoch=steps_per_epoch,
    118         validation_steps=validation_steps,
--&gt; 119         validation_freq=validation_freq)
    120   else:
    121     return training_arrays.fit_loop(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_distributed.py in experimental_tpu_fit_loop(model, dataset, epochs, verbose, callbacks, initial_epoch, steps_per_epoch, val_dataset, validation_steps, validation_freq)
    310   # TODO(fchollet): add support for `steps_per_epoch=None` in TPU loops.
    311   current_strategy = model._distribution_strategy
--&gt; 312   iterator = distributed_training_utils.get_iterator(dataset, current_strategy)
    313   steps_per_epoch = training_utils.infer_steps_for_dataset(
    314       dataset, steps_per_epoch, epochs, steps_name='steps_per_epoch')

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/distributed_training_utils.py in get_iterator(dataset, distribution_strategy)
    519   with distribution_strategy.scope():
    520     iterator = distribution_strategy.make_dataset_iterator(dataset)
--&gt; 521   initialize_iterator(iterator, distribution_strategy)
    522   return iterator
    523 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/distributed_training_utils.py in initialize_iterator(iterator, distribution_strategy)
    527     init_op = control_flow_ops.group(iterator.initialize())
    528     if not context.executing_eagerly():
--&gt; 529       K.get_session((init_op,)).run(init_op)
    530 
    531 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    930     try:
    931       result = self._run(None, fetches, feed_dict, options_ptr,
--&gt; 932                          run_metadata_ptr)
    933       if run_metadata:
    934         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1153     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1154       results = self._do_run(handle, final_targets, final_fetches,
-&gt; 1155                              feed_dict_tensor, options, run_metadata)
   1156     else:
   1157       results = []

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1329     if handle is None:
   1330       return self._do_call(_run_fn, feeds, fetches, targets, options,
-&gt; 1331                            run_metadata)
   1332     else:
   1333       return self._do_call(_prun_fn, handle, feeds, fetches)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1349           pass
   1350       message = error_interpolation.interpolate(message, self._graph)
-&gt; 1351       raise type(e)(node_def, op, message)
   1352 
   1353   def _extend_graph(self):

NotFoundError: Op type not registered 'ExperimentalRebatchDataset' in binary running on n-23c14aca-w-0. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Mrpatekful' date='2019-04-12T20:45:44Z'>
		Hi &lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
,
Public releases of Cloud TPUs (for TF 1.11, 1.12, and 1.13) does not currently support TensorFlow 2.0-alpha releases, as there are some op differences. That would change when a final release of TensorFlow 2 happens, but for now the use case is not supported.
That said, you may want to try this again with a TensorFlow 2.0 nightly and a Cloud TPU 1.14 release when it is released around May.
Thanks,
Frank
		</comment>
		<comment id='2' author='Mrpatekful' date='2019-04-12T20:45:53Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=27339&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=27339&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>