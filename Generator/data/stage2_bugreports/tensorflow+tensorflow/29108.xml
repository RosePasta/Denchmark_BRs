<bug id='29108' author='chenchc' open_date='2019-05-29T05:53:32Z' closed_time='2019-06-24T20:57:29Z'>
	<summary>Gradient clipping by norm has different semantics in tf.keras.optimizers against keras.optimizers</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.13.1
Python version: 3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: CUDA 10.0 + cuDNN 7.4
GPU model and memory: NVIDIA Tesla V100 32GB

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
The gradient clipping mechanism is implemented in the abstract class, tf.keras.optimizers.Optimizer (keras.optimizers.Optimizer) so that every optimizer inherited from the class supports it.
We find different semantics of the implementations between tf.keras and keras.io.
In tf.keras, the gradient norm is calculated and clipped per gradient tensor.



tensorflow/tensorflow/python/keras/optimizers.py


        Lines 98 to 99
      in
      bb8cf25






 if hasattr(self, 'clipnorm'): 



 grads = [clip_ops.clip_by_norm(g, self.clipnorm) for g in grads] 





But in keras.io, the gradient norm is calculated globally across all gradient tensors.
        if hasattr(self, 'clipnorm') and self.clipnorm &gt; 0:
            norm = K.sqrt(sum([K.sum(K.square(g)) for g in grads]))
            grads = [clip_norm(g, self.clipnorm, norm) for g in grads]
Code link: &lt;denchmark-link:https://github.com/keras-team/keras/blob/eab1b5bcdf105746ede02d2eb8a5cb3ca359b1b5/keras/optimizers.py#L96-L98&gt;https://github.com/keras-team/keras/blob/eab1b5bcdf105746ede02d2eb8a5cb3ca359b1b5/keras/optimizers.py#L96-L98&lt;/denchmark-link&gt;

IMO, tf.keras should adopt the latter method since
(1) The paper introducing gradient clipping (&lt;denchmark-link:https://arxiv.org/abs/1211.5063&gt;https://arxiv.org/abs/1211.5063&lt;/denchmark-link&gt;
) suggests the latter one, and
(2) tf.keras should be compliant with keras.io.
	</description>
	<comments>
		<comment id='1' author='chenchc' date='2019-06-24T20:57:29Z'>
		I already provided comment in the PR. Closing this.
		</comment>
		<comment id='2' author='chenchc' date='2019-06-24T20:57:30Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=29108&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=29108&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='chenchc' date='2019-06-25T05:59:42Z'>
		Hi &lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;
 , thanks for your reply.
But where can I find the comment you left?
		</comment>
		<comment id='4' author='chenchc' date='2019-06-25T15:09:53Z'>
		&lt;denchmark-link:https://github.com/chenchc&gt;@chenchc&lt;/denchmark-link&gt;
 in the PR, go to files changes tab (or the regular page) you should be able to see it
		</comment>
		<comment id='5' author='chenchc' date='2019-06-26T06:21:55Z'>
		&lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;
 I cannot see the comments in both tabs. Maybe they are invisible to my account?
&lt;denchmark-link:https://user-images.githubusercontent.com/6285919/60151408-33a69f80-980f-11e9-8cf6-360f5933b1f0.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='chenchc' date='2019-06-26T16:09:15Z'>
		&lt;denchmark-link:https://github.com/chenchc&gt;@chenchc&lt;/denchmark-link&gt;
 My major comment is that I don't see where in this paper describing it should use global clip instead of local clip.
		</comment>
	</comments>
</bug>