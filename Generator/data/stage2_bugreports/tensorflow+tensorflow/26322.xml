<bug id='26322' author='eldar' open_date='2019-03-04T14:25:44Z' closed_time='2019-03-19T18:44:46Z'>
	<summary>[Bug]: tf.keras update_op for computing running mean and variance for BatchNorm causes an error</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 9
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.12
Python version: 3.6
CUDA/cuDNN version: CUDA 9.0
GPU model and memory: TITAN X (Pascal)

I am using tf.keras API to build a ConvNet and I train it with the Tensorflow custom training loop, using graph API. The ConvNet contains BatchNorm layers and as I don't use model.fit() for training, I have to manage the updates to the moving mean and variance manually. However this causes an error. Here is a minimal reproducible case:
&lt;denchmark-code&gt;import numpy as np
import tensorflow as tf

from tensorflow.python.keras import layers
from tensorflow.python.keras import initializers
from tensorflow.python.keras import models

tfsum = tf.contrib.summary

HEIGHT = 480
WIDTH = 640

def conv(x, num_out_layers, kernel_size, stride, activation_fn="relu"):
    kernel_initializer = initializers.VarianceScaling()
    x = layers.Conv2D(num_out_layers,
                      (kernel_size, kernel_size),
                      strides=stride,
                      padding="SAME",
                      activation=None,
                      kernel_initializer=kernel_initializer)(x)

    x = layers.BatchNormalization()(x)
    x = layers.Activation(activation_fn)(x)
    return x


def build_cnn(img_shape):
    inputs = tf.keras.layers.Input(img_shape)

    conv1 = conv(inputs, 64, 7, 1)
    outputs = conv(conv1, 3, 7, 1)

    model = models.Model(inputs=[inputs], outputs=[outputs])
    return model


def main_graph():
    learning_rate = 0.001
    batch_size = 1
    num_steps = 100
    train_dir = "."
    dataset_size = 20

    images = np.random.random_sample([dataset_size, HEIGHT, WIDTH, 3]).astype(np.float32)
    dataset = tf.data.Dataset.from_tensor_slices(images)
    dataset = dataset.repeat() \
                     .batch(batch_size)
    iterator = dataset.make_one_shot_iterator()
    image = iterator.get_next()
    print(image)

    global_step = tf.train.get_or_create_global_step()
    summary_writer = tfsum.create_file_writer(
        train_dir, flush_millis=10000)
    with summary_writer.as_default(), tfsum.record_summaries_every_n_global_steps(10):
        model = build_cnn([HEIGHT, WIDTH, 3])
        prediction = model(image, training=True)
        update_ops = model.updates
        train_var_list = model.trainable_variables

        prediction = tf.ensure_shape(prediction, [batch_size, HEIGHT, WIDTH, 3])

        loss = tf.reduce_mean(tf.abs(prediction-image))

        tf.contrib.summary.scalar("loss", loss)

        optimizer = tf.train.AdamOptimizer(learning_rate)
        with tf.control_dependencies(update_ops):
            train_op = optimizer.minimize(loss, global_step=global_step, var_list=train_var_list)

    sess = tf.Session()
    with sess, summary_writer.as_default():
        sess.run(tf.global_variables_initializer())
        sess.run(tf.local_variables_initializer())
        tf.contrib.summary.initialize(graph=tf.get_default_graph())

        for i in range(num_steps+1):
            _, global_step_val, loss_val, _, = sess.run([train_op, global_step, loss,
                                                        tfsum.all_summary_ops()])
            print(f"iter:{global_step_val:06d} loss: {loss_val:04.4f}")


if __name__ == "__main__":
    main_graph()
&lt;/denchmark-code&gt;

This results in the following error message:
&lt;denchmark-code&gt;Caused by op 'input_1', defined at:
  File "../../train_bug.py", line 104, in &lt;module&gt;
    main_graph()
  File "../../train_bug.py", line 72, in main_graph
    model = build_cnn([HEIGHT, WIDTH, 3])
  File "../../train_bug.py", line 44, in build_cnn
    inputs = tf.keras.layers.Input(img_shape)
  File "/BS/eldar-3dshape/work/apps/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/input_layer.py", line 229, in Input
    input_tensor=tensor)
  File "/BS/eldar-3dshape/work/apps/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/input_layer.py", line 112, in __init__
    name=self.name)
  File "/BS/eldar-3dshape/work/apps/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py", line 1747, in placeholder
    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)
  File "/BS/eldar-3dshape/work/apps/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py", line 5206, in placeholder
    "Placeholder", dtype=dtype, shape=shape, name=name)
  File "/BS/eldar-3dshape/work/apps/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/BS/eldar-3dshape/work/apps/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py", line 488, in new_func
    return func(*args, **kwargs)
  File "/BS/eldar-3dshape/work/apps/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 3274, in create_op
    op_def=op_def)
  File "/BS/eldar-3dshape/work/apps/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1770, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'input_1' with dtype float and shape [?,480,640,3]
         [[node input_1 (defined at ../../train_bug.py:44)  = Placeholder[dtype=DT_FLOAT, shape=[?,480,640,3], _device="/job:localhost/replica:0/task:0/device:GPU:0"]()]]
&lt;/denchmark-code&gt;

Removing with tf.control_dependencies(update_ops) makes it run, but then the batchnorm statistics are not being updated. How can I use Keras in this setting?
	</description>
	<comments>
		<comment id='1' author='eldar' date='2019-03-07T20:21:24Z'>
		The problem is very much related to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/23873&gt;#23873&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='eldar' date='2019-03-08T23:34:40Z'>
		&lt;denchmark-link:https://github.com/karmel&gt;@karmel&lt;/denchmark-link&gt;
 can you comment or redirect as necessary? Thanks.
		</comment>
		<comment id='3' author='eldar' date='2019-03-19T18:44:46Z'>
		&lt;denchmark-link:https://github.com/eldar&gt;@eldar&lt;/denchmark-link&gt;
 please replace  with .
model.updates includes all updates, including the ones created when constructing your model off of the keras.Inputs
		</comment>
	</comments>
</bug>