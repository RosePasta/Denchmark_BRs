<bug id='41635' author='sedol1339' open_date='2020-07-22T18:35:02Z' closed_time='2020-07-31T14:00:40Z'>
	<summary>Fitting sometimes leads to NaN loss on TPU, while on CPU doesn't</summary>
	<description>
System information

TensorFlow version (use command below): 2.2.0 (v2.2.0-0-g2b96f3662b)
Python version: 3.6.9
GPU model and memory: Google Colab TPU

I've found that sometimes fitting model on TPU leads to NaN loss, while fitting on CPU doesn't.
I fit Xception network with custom top layers, using only 5 batches, one per epoch. Standalone code:
&lt;denchmark-code&gt;import os
import tensorflow as tf
import numpy as np
from tensorflow.keras.layers import *

tf.get_logger().propagate = False
resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu = 'grpc://' + os.environ['COLAB_TPU_ADDR'])
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.experimental.TPUStrategy(resolver)

with strategy.scope():
#with tf.device('/job:localhost/replica:0/task:0/device:CPU:0'):

  xception = tf.keras.applications.Xception(weights = 'imagenet', include_top = False, input_shape = (256, 256, 3))

  x = xception.output
  x = GlobalAveragePooling2D()(x)
  x = Dense(256, activation = 'relu')(x)
  x = Dropout(0.25)(x)
  predictions = Dense(10)(x)

  model = tf.keras.Model(inputs = xception.input, outputs = predictions)

  model.compile(
      loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),
      optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-8),
      metrics = ['accuracy']
  )

  batch_size = 7
  for epoch in range(5):
    X = np.random.rand(batch_size, 256, 256, 3)
    Y = np.zeros(batch_size)
    model.fit(X, Y, batch_size = batch_size, epochs = 1)
&lt;/denchmark-code&gt;

What i've found?

if using TPU, batch_size &lt; 8 fitting gives NaN loss after second epoch (or second batch, which is the same)
if using TPU, batch_size &lt; 8 and X is multiplied by zero, fitting gives NaN loss after first epoch
if using TPU, batch_size &gt;= 8, fitting doesn't give NaN loss
if using TPU, batch_size &gt;= 8 and X is multiplied by zero, fitting gives NaN loss after first epoch
if using CPU, fitting doesn't give NaN loss in the cases described above

So, fitting gives NaN loss if we are using TPU and batch_size &lt; 8 or X is an array of zeros.
I tested batch sizes 1, 7, 8, 64.
To train on CPU i used this command:
&lt;denchmark-code&gt;with tf.device('/job:localhost/replica:0/task:0/device:CPU:0'):
&lt;/denchmark-code&gt;

or just used Colab version with only CPU, which gives the same results.
Same behaviour occured when I was training this model on real data: fitting immediately led to NaN loss when some batch had less than 8 image-label pairs in it, for example when last batch in dataset was smaller.
I guess that it can be caused by less-precise float on TPU or by some error in parallel calculations on 8 TPU cores (/device:TPU:0, ..., /device:TPU:7).
	</description>
	<comments>
		<comment id='1' author='sedol1339' date='2020-07-23T05:59:20Z'>
		I am able to reproduce the issue in colab with TF version 2.2.
But i am seeing different error message with TF version 2.3-rc1 and nightly version() ().
Please, find the gist &lt;denchmark-link:https://colab.research.google.com/gist/ravikyram/e3ea5e72d7e2866d2e162b6b317173f9/untitled174.ipynb&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='2' author='sedol1339' date='2020-07-23T12:41:22Z'>
		With tf-nightly TPU cannot be even initialized
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/41542&gt;#41542&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='sedol1339' date='2020-07-23T21:43:57Z'>
		Thanks for this report &lt;denchmark-link:https://github.com/sedol1339&gt;@sedol1339&lt;/denchmark-link&gt;
. Regarding the NaNs, I don't think this is too surprising given the small batch size. And I suspect they are caused by all the batch norm layers within Xception. At a batch size of 7 if you set:
&lt;denchmark-code&gt;for layer in xception.layers:
  layer.trainable = False
&lt;/denchmark-code&gt;

then I am not seeing the NaNs. With batch norm, you're dividing by local batch statistics on each core, so with a global batch size &lt; 8 on TPUs, the batch is being split for each core and you are more likely to divide by numbers close to 0.
		</comment>
		<comment id='4' author='sedol1339' date='2020-07-30T08:57:10Z'>
		So isn't it a bug? I thought that using TPU should not affect such things. Why system cannot use less than 8 cores if batch size if less that 8?
		</comment>
		<comment id='5' author='sedol1339' date='2020-07-30T21:03:24Z'>
		No, I don't think it's a bug because batch norm would have problems at such a low global batch size. There's no way for the system to utilize fewer cores even if the batch size is smaller than the number of cores.
		</comment>
		<comment id='6' author='sedol1339' date='2020-07-31T14:00:42Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41635&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41635&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='sedol1339' date='2020-07-31T14:50:14Z'>
		&lt;denchmark-link:https://github.com/nikitamaia&gt;@nikitamaia&lt;/denchmark-link&gt;
 I think i was too hasty to close the ticket, because this problem happens not only when you specify small batch size. Yes, batch size should be large enough for TPU.
But when you split dataset into large batches, the last batch may be very small. In this case loss will immediately become NaN. It's may be hard to understand the reason of NaN loss in this situation. So this behaviour should be eigther fixed or clearly written in TPU documentation.
		</comment>
		<comment id='8' author='sedol1339' date='2020-08-13T21:18:04Z'>
		I had this same issue and after reading your comment, I dropped the last remaining batch and it fixed it for me! Thanks!
		</comment>
		<comment id='9' author='sedol1339' date='2020-10-07T18:09:16Z'>
		I ran into this problem too. I thought it must be data specific since i got this after constructing a bigger dataset and everything else was the same as before (it worked before). Reading this thread helps, it seems the solution is to not include the last batch? Is this still a bug or a hard requirement for TPU? I am guessing that my older dataset was lucky that its last batch is the same as the batch_size (i.e. no remainder smaller batch).
Curiously, I got this error using a custom loss function, but not using just tf.keras.losses.BinaryCrossentropy
		</comment>
	</comments>
</bug>