<bug id='44195' author='jackd' open_date='2020-10-21T05:38:08Z' closed_time='2020-10-24T03:43:57Z'>
	<summary>Random ops the same for different iterations over mapped datasets</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes (copy below + colab link)
OS Platform and Distribution Linux Ubuntu 18.04:
TensorFlow installed from (source or binary): binary (pip)
TensorFlow version (use command below): 2.3 / tf-nightly
Python version: 3.7.7

Describe the current behavior
Random ops in tf.data.Dataset.mapped datasets produce the same values for repeated iterations over the same dataset (though not for datasets using Dataset.repeat) when tf.random.set_seed has been called. This nullifies the effect of any data augmentation when the dataset is looped over and the seed is set.
Describe the expected behavior
Looping over a dataset multpile times should have at least qualitatively equivalent behaviour to looping over a repeated dataset (potentially quantitatively too), and should be qualitatively the same with or without using tf.random.set_seed.

Colab &lt;denchmark-link:https://colab.research.google.com/drive/1Y9wajzK5TYer_DYOK5l_ySexq_A-5tLd?usp=sharing&gt;here&lt;/denchmark-link&gt;

import tensorflow as tf

def get_dataset():
  return tf.data.Dataset.range(2).map(
      lambda x: tf.cast(x, tf.float32) + tf.random.uniform(()))

def print_looped_epochs(num_epochs=2):
  print('Looped epochs:')
  dataset = get_dataset()
  for _ in range(num_epochs):
    print([e.numpy() for e in dataset])

def print_repeated_epochs(num_epochs=2):
  print("Repeated epochs:")
  dataset = get_dataset().repeat(num_epochs)
  print([e.numpy() for e in dataset])

print_looped_epochs()
print_repeated_epochs()

tf.random.set_seed(0)
print('Seed set')
print_looped_epochs()
print_repeated_epochs()
Other info / logs
&lt;denchmark-code&gt;Looped epochs:
[0.6892859, 1.171408]
[0.0070821047, 1.629379]
Repeated epochs:
[0.6894361, 1.446699, 0.60574067, 1.9869001]
Seed set
Looped epochs:
[0.019757032, 1.5400312]
[0.019757032, 1.5400312]
Repeated epochs:
[0.019757032, 1.5400312, 0.51667833, 1.4683528]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='jackd' date='2020-10-21T13:57:22Z'>
		Was able to reproduce the issue with &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/ac7c01702681aa361f0a63c75ad41b8d/44195.ipynb&gt;TF v2.3&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/6ecd7161fc2d1bf13edca166cd70e6ea/44195-tf-nightly.ipynb&gt;TF-nightly&lt;/denchmark-link&gt;
. Please find the attached gist. Thanks!
		</comment>
		<comment id='2' author='jackd' date='2020-10-23T00:05:22Z'>
		When your get_dataset method constructs the input pipeline graph, the seed ends up being embedded in the input pipeline graph, which is why executing the same input pipeline graph multiple times will produce identical results across different epochs.
In other words, the dataset graph does not maintain the concept of an "epoch" and in the absence of unseeded operations, its execution will be fully deterministic.
If you would like your input pipeline to execute both 1) deterministically and 2) differently across different epochs, you will have to maintain the notion of an epoch yourself:
&lt;denchmark-code&gt;epoch_counter = tf.Variable(0, dtype=tf.int64)

ds = tf.data.Dataset.range(5)
seeds = tf.data.experimental.RandomDataset(seed=42)
ds = tf.data.Dataset.zip((ds, seeds))

def map_fn(unused, seed):
  return tf.random.stateless_uniform([], (42, seed + epoch_counter))

ds = ds.map(map_fn)

for i in range(2):
  print("Epoch: ", i)
  for elem in ds:
    print(elem.numpy())
  epoch_counter.assign_add(1)
&lt;/denchmark-code&gt;

This example should always produce the following output:
&lt;denchmark-code&gt;Epoch:  0
0.12457669
0.18042064
0.12596941
0.04956436
0.813197
Epoch:  1
0.98221445
0.53253984
0.20035768
0.276451
0.2731657
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='jackd' date='2020-10-23T00:28:39Z'>
		&lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
 I'm sure there are ways around this (using  ops for one) but is this really the desired behaviour? And why does the issue only occur when  has been explicitly called?
		</comment>
		<comment id='4' author='jackd' date='2020-10-23T15:11:17Z'>
		The fact that an input pipeline with no unseeded random operations (and sources of external state such as shuffle or cache) produces the same order every time it is executed is an important invariant and the expected behavior (and changing it is not an option for backwards compatibility reasons). The tf.random.Generator you mentioned is an example of external state that allows users to explicitly control that across different epochs the behavior of the input pipeline should be (deterministically) different.
Without setting the tf.random.set_seed, the random operations in the input pipeline graph will be unseeded (and thus behave non-deterministically).
		</comment>
		<comment id='5' author='jackd' date='2020-10-24T03:43:57Z'>
		I must say it's extremely counter-intuitive that setting the random seed changes the behaviour of this in such a qualitative way and can potentially completely break data augmentation strategies, but I suppose that's just another reason to avoid tf.random ops.
		</comment>
		<comment id='6' author='jackd' date='2020-10-24T03:43:59Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44195&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44195&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>