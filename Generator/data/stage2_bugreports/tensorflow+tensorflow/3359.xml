<bug id='3359' author='jacobvsdanniel' open_date='2016-07-18T09:40:56Z' closed_time='2016-08-31T00:06:46Z'>
	<summary>Gradient computation fails concatenation in while_loop() body</summary>
	<description>
&lt;denchmark-h:h3&gt;Description&lt;/denchmark-h&gt;

When I was trying to implement RNN with while_loop(), I tried to concatenate output to a matrix.
This worked in forward passes but not in applying gradients.
Also, I saw that there are discussions (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/2237&gt;#2237&lt;/denchmark-link&gt;
) about supporting Recursive NN. There's a workaround by transforming tree structures to a matrix. For example, if we have a binary tree with its node values and structure be like
&lt;denchmark-code&gt;10------20------40
 |       |------50
 | 
 |------30------60
         |------70
&lt;/denchmark-code&gt;

Then we could transform it to a value vector V
&lt;denchmark-code&gt;[70 60 50 40 30 20 10]
&lt;/denchmark-code&gt;

and a (strictly bottom-up) structure matrix M
&lt;denchmark-code&gt;[[0 1 4]  # V[0] and V[1] are V[4]'s children
 [2 3 5]
 [4 5 6]]
&lt;/denchmark-code&gt;

Then we could build our graph with while_loop() by iteratively index into previous output. Note that this is without any specific inputs; only knowing they'll be a vector and a matrix instead.
For more details, see
&lt;denchmark-link:https://github.com/jacobvsdanniel/tf_rnn&gt;https://github.com/jacobvsdanniel/tf_rnn&lt;/denchmark-link&gt;

inspired by
&lt;denchmark-link:https://github.com/ofirnachum/tree_rnn&gt;https://github.com/ofirnachum/tree_rnn&lt;/denchmark-link&gt;

I also ran into problems of computing gradients for nested gather inside while_loop(), but managed to worked around before fixes come up to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/418&gt;#418&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/206&gt;#206&lt;/denchmark-link&gt;
 .
&lt;denchmark-h:h3&gt;Tensorflow version&lt;/denchmark-h&gt;

0.9.0
&lt;denchmark-h:h3&gt;Reproduction steps&lt;/denchmark-h&gt;

def test_concat_loop():
    x = tf.constant([[1.,2.]])
    X = tf.get_variable("X", initializer=x)

    i = tf.constant(0)
    H = tf.zeros([0, 2])

    def condition(i, H):
        return i &lt; 2

    def body(i, H):
        return i+1, tf.concat(0, [H, X])

    _, H = tf.while_loop(condition, body, [i, H])
    s = tf.reduce_sum(H)

    sess = tf.Session()
    sess.run(tf.initialize_all_variables())
    print sess.run(X)
    print sess.run(H)
    print sess.run(s)

    optimizer = tf.train.GradientDescentOptimizer(0.01)
    op = optimizer.minimize(s) #Raise
    print sess.run(op)
    print sess.run(X)
    return
&lt;denchmark-h:h3&gt;Workaround&lt;/denchmark-h&gt;

def test_concat_loop_workaround():
    x = tf.constant([[1.,2.]])
    X = tf.get_variable("X", initializer=x)

    i = tf.constant(0)
    H = tf.zeros([5, 2])

    def condition(i, H):
        return i &lt; 5

    def body(i, H):
        past = tf.zeros([i, 2])
        future = tf.zeros([4-i, 2])
        return i+1, H + tf.concat(0, [past, X, future])

    _, H = tf.while_loop(condition, body, [i, H])
    s = tf.reduce_sum(H)

    sess = tf.Session()
    sess.run(tf.initialize_all_variables())
    print sess.run(X)
    print sess.run(H)
    print sess.run(s)

    optimizer = tf.train.GradientDescentOptimizer(0.01)
    op = optimizer.minimize(s)
    print sess.run(op)
    print sess.run(X)
    return
&lt;denchmark-h:h3&gt;Error Logs&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:924] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: GeForce GTX 980
major: 5 minor: 2 memoryClockRate (GHz) 1.329
pciBusID 0000:01:00.0
Total memory: 4.00GiB
Free memory: 3.86GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:806] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0)
[[ 1.  2.]]
[[ 1.  2.]
 [ 1.  2.]]
6.0
Traceback (most recent call last):
  File "issue-418.py", line 244, in &lt;module&gt;
    main()
  File "issue-418.py", line 233, in main
    test_concat_loop()
  File "issue-418.py", line 53, in test_concat_loop
    op = optimizer.minimize(s)
  File "/home/danniel/.local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py", line 193, in minimize
    grad_loss=grad_loss)
  File "/home/danniel/.local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py", line 250, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File "/home/danniel/.local/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py", line 494, in gradients
    in_grad.set_shape(t_in.get_shape())
  File "/home/danniel/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 404, in set_shape
    self._shape = self._shape.merge_with(shape)
  File "/home/danniel/.local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py", line 570, in merge_with
    (self, other))
ValueError: Shapes (0, 2) and (1, 2) are not compatible
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='jacobvsdanniel' date='2016-07-19T00:28:30Z'>
		Assigning to &lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
: can you take a look?  Thanks.
		</comment>
		<comment id='2' author='jacobvsdanniel' date='2016-07-19T05:22:22Z'>
		Have you tried using a TensorArray?  See tf.nn.dynamic_rnn for example usage.
		</comment>
		<comment id='3' author='jacobvsdanniel' date='2016-07-19T05:46:26Z'>
		Line 498 of gradients.py is too strong for while loop:
in_grad.set_shape(t_in.get_shape())
It looks like a bug introduced recently, and could potentially cause other problems.  We probably want to add a test case to guard this when we fix it.
		</comment>
		<comment id='4' author='jacobvsdanniel' date='2016-07-19T05:51:08Z'>
		Can't quite understand neither TensorArray nor any rnn implementations based on RNNCell at this moment :|
		</comment>
		<comment id='5' author='jacobvsdanniel' date='2016-08-17T16:24:30Z'>
		You're experiencing a bug because of the way  gradients handle shapes of inputs and outputs.  If your input and output shapes are different, gradients can get confused.  For this reason, &lt;denchmark-link:https://github.com/yuanbyu&gt;@yuanbyu&lt;/denchmark-link&gt;
 will soon add a parameter called shape_invariants to while_loop, in which you can declare what shape information is static and what can change from iteration to iteration.  gradients will respect these shapes.  the default behavior will now require that shapes do not change from iteration to iteration (so your original code, without passing shape_invariants, would fail early with a useful error message)
		</comment>
		<comment id='6' author='jacobvsdanniel' date='2016-08-24T04:09:37Z'>
		Expect a fix to show up in a week. However the fix requires the user to provide a shape invariant for a loop variable if its shape is changed by the loop body.
		</comment>
		<comment id='7' author='jacobvsdanniel' date='2016-08-31T00:06:46Z'>
		This was fixed with &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/c46abae176717ef6c6649ba0ca1099c35b90194e&gt;c46abae&lt;/denchmark-link&gt;
.  Please reopen if you are still experiencing this bug with the nightlies/master branch.
		</comment>
	</comments>
</bug>