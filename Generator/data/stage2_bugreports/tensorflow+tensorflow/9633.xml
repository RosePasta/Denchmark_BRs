<bug id='9633' author='tpet' open_date='2017-05-03T15:49:22Z' closed_time='2017-05-05T18:20:38Z'>
	<summary>SIGSEGV with sparse_add and broadcasting</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
yes, enclosed below
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 16.04
TensorFlow installed from (source or binary):
binary via pip
TensorFlow version (use command below):
('v1.0.0-65-g4763edf-dirty', '1.0.1')
Bazel version (if compiling from source):
N/A, using pip installation
CUDA/cuDNN version:
N/A, CPU-only
GPU model and memory:
none
Exact command to reproduce:

&lt;denchmark-code&gt;from __future__ import print_function
import numpy as np
import tensorflow as tf

dense_sz = [1, 1000, 1000]
dense = tf.constant(1.0, shape=dense_sz, dtype=tf.float32)

sparse_sz = [10, 1000, 1000]
nnz = 100
nz_ind = np.random.choice(np.prod(sparse_sz), size=nnz, replace=False)
nz_ind = np.unravel_index(nz_ind, dims=sparse_sz)
nz_ind = np.array(nz_ind).T
assert np.all(nz_ind &lt; np.array(sparse_sz)[None, :])
# Ensure canonical ordering.
ind = np.lexsort([nz_ind[:, i].flatten() for i in reversed(range(nz_ind.shape[1]))])
nz_ind = nz_ind[ind, :]
print('nz_ind\n', nz_ind)

sparse_plc = tf.sparse_placeholder(tf.float32)
sparse_sum = tf.sparse_add(dense, sparse_plc)
init = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
    print('after init')
    res = sess.run(sparse_sum, feed_dict={sparse_plc: tf.SparseTensorValue(nz_ind, np.ones((nnz,)), sparse_sz)})
    print('sum\n', res)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

Running the code above results in
&lt;denchmark-code&gt;[...]
after init

Process finished with exit code 139 (interrupted by signal 11: SIGSEGV)
&lt;/denchmark-code&gt;

For lower values of nnz, (nnz = 1) it finishes fine quite often.
&lt;denchmark-code&gt;[...]
after init
sum
 [[[ 1.  1.  1. ...,  1.  1.  1.]
  [ 1.  1.  1. ...,  1.  1.  1.]
  [ 1.  1.  1. ...,  1.  1.  1.]
  ..., 
  [ 1.  1.  1. ...,  1.  1.  1.]
  [ 1.  1.  1. ...,  1.  1.  1.]
  [ 1.  1.  1. ...,  1.  1.  1.]]]

Process finished with exit code 0
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

See above.
	</description>
	<comments>
		<comment id='1' author='tpet' date='2017-05-03T17:29:05Z'>
		&lt;denchmark-link:https://github.com/concretevitamin&gt;@concretevitamin&lt;/denchmark-link&gt;
 can you take a look at this?
Here's the stacktrace:
&lt;denchmark-code&gt;PC: @     0x7f37662d027d  (unknown)  raise
    @         0x243417e6       1120  FailureSignalHandler()
    @     0x7f37662d03d0       1472  (unknown)
    @     0x7f3764db3191        128  faulthandler_fatal_error
    @     0x7f37662d03d0  (unknown)  (unknown)
    @         0x2217d353       1664  tensorflow::SparseTensorDenseAddOp&lt;&gt;::Compute()
    @         0x22f76586         96  tensorflow::ThreadPoolDevice::Compute()
    @         0x22f0764b       2464  tensorflow::(anonymous namespace)::ExecutorState::Process()
    @         0x22f13cb1        160  std::_Mem_fn&lt;&gt;::operator()&lt;&gt;()
    @         0x22f13bc3         96  std::_Bind&lt;&gt;::__call&lt;&gt;()
    @         0x22f13b06         64  std::_Bind&lt;&gt;::operator()&lt;&gt;()
    @         0x22f136fd         32  std::_Function_handler&lt;&gt;::_M_invoke()
    @         0x13172ede         32  std::function&lt;&gt;::operator()()
    @         0x2325fcc8        128  tensorflow::thread::EigenEnvironment::ExecuteTask()
    @         0x2325f019        208  Eigen::ThreadPoolTempl&lt;&gt;::WorkerLoop()
    @         0x2325ec5e         32  Eigen::ThreadPoolTempl&lt;&gt;::ThreadPoolTempl()::{lambda()#1}::operator()()
    @         0x2325eacd         32  std::_Function_handler&lt;&gt;::_M_invoke()
    @         0x13172ede         32  std::function&lt;&gt;::operator()()
    @         0x2325e934         48  tensorflow::thread::EigenEnvironment::CreateThread()::{lambda()#1}::operator()()
    @         0x2325e76d         32  std::_Function_handler&lt;&gt;::_M_invoke()
    @         0x13172ede         32  std::function&lt;&gt;::operator()()
    @         0x23296f0c         32  tensorflow::(anonymous namespace)::GoogleThread::FuncThread::Run()
    @         0x23d41427        448  Thread::ThreadBody()
    @     0x7f37662c6890        176  start_thread
    @     0x7f3765d2237d  (unknown)  clone
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='tpet' date='2017-05-03T23:10:18Z'>
		Did you modify anything else in the code? The version shows dirty.
&lt;denchmark-link:https://github.com/yifeif&gt;@yifeif&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/av8ramit&gt;@av8ramit&lt;/denchmark-link&gt;
 are the pip installs built from a dirty git repo?
		</comment>
		<comment id='3' author='tpet' date='2017-05-04T01:14:38Z'>
		I'm taking a look.
		</comment>
		<comment id='4' author='tpet' date='2017-05-04T10:56:25Z'>
		The results look the same with tf pip-upgraded to ('v1.1.0-rc0-61-g1ec6ed5', '1.1.0').
		</comment>
		<comment id='5' author='tpet' date='2017-05-04T18:59:16Z'>
		Thanks for the report &lt;denchmark-link:https://github.com/tpet&gt;@tpet&lt;/denchmark-link&gt;
.
I am submitting a "fix" that instead of segfaulting, return a proper error status that requires both operands have matching shapes.  This has always been the assumption in the , but was &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/sparse_tensor_dense_add_op.cc#L56&gt;not enforced&lt;/denchmark-link&gt;
.  The patch should show up in master within a day or two.
Do you exactly require the functionality of "sparse + dense -&gt; dense, with dense-to-sparse broadcast"?  If so, I'd like to mark this as contributions welcome (the current kernels do not support this broadcast pattern).
However, if you can get away with "sparse + dense -&gt; sparse, with dense-to-sparse broadcast", we already have &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/sparse_ops.py#L316&gt;sparse_dense_cwise_add()&lt;/denchmark-link&gt;
 that does this.  Let us know, and we can expose this function as a public method.
		</comment>
		<comment id='6' author='tpet' date='2017-05-05T10:40:45Z'>
		&lt;denchmark-link:https://github.com/concretevitamin&gt;@concretevitamin&lt;/denchmark-link&gt;
 I agree raising an exception is much better. The "sparse + dense -&gt; dense, with dense-to-sparse broadcast" really seems not that much useful, compared to "sparse + dense -&gt; sparse, with dense-to-sparse broadcast". Now I actually don't need this particular thing.
My initial use case was a bit different. In the process of trying to get some reasonable behavior I happened to find the segfault and created this example.
My use case is this:
D + reduce_sum(a * S)
where D and the result is dense [1 n2 n3 n4]
S is sparse [n1 n2 n3 n4]
a is dense [n1 1 1 1] and broadcasts to S.
So far I hasn't been able to get to some reasonable performance with this.
		</comment>
		<comment id='7' author='tpet' date='2017-05-05T10:50:52Z'>
		I'm using a pip installation so it will take some time until it propagates down to me so feel free to close the issue if you think it is resolved. Thanks.
		</comment>
		<comment id='8' author='tpet' date='2017-05-05T18:20:38Z'>
		Okay, closing for now.  For the reduce, take a look at tf.sparse_reduce_sum() and/or tf.sparse_reduce_sum_reduce().
		</comment>
	</comments>
</bug>