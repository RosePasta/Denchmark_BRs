<bug id='35826' author='kevinkingo' open_date='2020-01-13T18:33:44Z' closed_time='2020-01-15T01:06:23Z'>
	<summary>Bug in Transfer learning + Distributed Training</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.0
Python version: 3.7
CUDA/cuDNN version: cuda 10.0, cudnn 7
GPU model and memory: Quadro P6000, 24GB

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

Under the distributed environment, if the model is updated, the trainable_weights of the model is not updated in the distributed training loop. Please see following code for reproducing the bug.
I first created 2 Conv2D layers. I created the first model using only 1 Conv2D layer, and it works fine. Then, I update the mode to create a 2-Conv2D model, then there's the bug. Outside the training loop, there are trainable_weights (2 kernel + 2 bias), but inside the training loop, there's only 2 trainable_weights
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import numpy as np
import tensorflow as tf
from tensorflow import keras

tf.config.set_soft_device_placement(True)
gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
  tf.config.experimental.set_memory_growth(gpu, True)

# Begin
x_in = np.random.randn(2, 64, 64, 3).astype(np.float32)
gt = np.random.randn(2, 64, 64, 3).astype(np.float32)

layer1 = keras.layers.Conv2D(
        input_shape=(None, None, None, 3), filters=3,
        kernel_size=3, strides=1, padding='same',
        name='conv1')
layer2 = keras.layers.Conv2D(
        input_shape=(None, None, None, 3), filters=3,
        kernel_size=3, strides=1, padding='same',
        name='conv2')

strategy = tf.distribute.MirroredStrategy()
@tf.function
def train():
  def train_step():
    with tf.GradientTape() as tape:
      loss = tf.reduce_mean((model(x_in) - gt) ** 2)
    grads = tape.gradient(loss, model.trainable_weights)
    tf.print("Length of trainable_weights: ", len(model.trainable_weights), "Length of grads: ", len(grads))
    optimizer.apply_gradients(zip(grads, model.trainable_weights))
  strategy.experimental_run_v2(train_step)

print('------ First model ------')
with strategy.scope():
  x = keras.Input((64, 64, 3))
  model = keras.Model(inputs=x, outputs=layer1(x))
  optimizer = keras.optimizers.Adam(0.1, amsgrad=True)
print("Length of trainable_weights: ", len(model.trainable_weights))
print(model.trainable_weights[-1].values[0].name, model.trainable_weights[-1].values[0].numpy())
# Print: 
# Length of trainable_weights:  2
# conv1/bias:0 [0. 0. 0.]

for i in range(2):
  train()
# Print:
# Length of trainable_weights:  2 Length of grads:  2
# Length of trainable_weights:  2 Length of grads:  2

print("Length of trainable_weights: ", len(model.trainable_weights))
print(model.trainable_weights[-1].values[0].name, model.trainable_weights[-1].values[0].numpy())
# Print:
# Length of trainable_weights:  2
# conv1/bias:0 [0.03003622 0.03190297 0.02856238]


print('------ Change model ------')
with strategy.scope():
  x = keras.Input((64, 64, 3))
  model = keras.Model(inputs=x, outputs=layer2(layer1(x)))
print("Length of trainable_weights: ", len(model.trainable_weights))
print(model.trainable_weights[-1].values[0].name, model.trainable_weights[-1].values[0].numpy())
# Print: 
# Length of trainable_weights:  4
# conv2/bias:0 [0. 0. 0.]

for i in range(2):
  train()
# Print:
# Length of trainable_weights:  2 Length of grads:  2
# Length of trainable_weights:  2 Length of grads:  2

print("Length of trainable_weights: ", len(model.trainable_weights))
print(model.trainable_weights[-1].values[0].name, model.trainable_weights[-1].values[0].numpy())
# Print: 
# Length of trainable_weights:  4
# conv2/bias:0 [0. 0. 0.]



&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='kevinkingo' date='2020-01-15T01:05:19Z'>
		Hi, &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/function&gt;@tf.function&lt;/denchmark-link&gt;
 decorator compiles a function into a callable TensorFlow graph, and in your case the second training loop is still using the graph compiled from the first model. This is because the "model" parameter captured in your train() function  refers to the first model when the graph compilation happens, and TensorFlow never compiles a new graph in your case.
To compile a new graph, you can either

Remove the @tf.function decorator of train() (slower execution https://www.tensorflow.org/guide/function)
Force TensorFlow to recompile a new graph by redefining a train function with @tf.function decorator, e.g:
def train():
  ...

# define the first model
...
@tf.function
def train_first_model():
   train()

train_first_model()

# define the second model
...
@tf.function
def train_second_model():
  train()

train_second_model()


		</comment>
		<comment id='2' author='kevinkingo' date='2020-01-15T01:06:25Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35826&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35826&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>