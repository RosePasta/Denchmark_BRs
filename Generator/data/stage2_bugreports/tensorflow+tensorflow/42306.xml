<bug id='42306' author='nvrsmeele' open_date='2020-08-13T08:37:27Z' closed_time='2020-09-08T00:16:25Z'>
	<summary>InvalidArgumentError: Operation 'while' has no attr named '_XlaCompile'</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Google Colab (Ubuntu 18.04.3 LTS)
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): Pre-installed / source
TensorFlow version (use command below): 2.3.0 (and tried tf-nightly with 2.4.0-dev20200812)
Python version: 3.6.9
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: CUDA Version = 10.1
GPU model and memory: Tesla T4 and 12G

Describe the problem
I am trying to implement a custom multi-input and -output model which uses a learning algorithm as proposed in a research paper. The model itself works fine without the custom learning algorithm which I use as a baseline. The problem I encounter is that the code got stuck in mc_pred = self.main_classifier([xu, xs], training=True) in the train_step function in the DebiasModel class.
It did not return an error. After running for an hour, I interrupted the kernel and it returns the error message saying:
&lt;denchmark-code&gt;InvalidArgumentError: Operation 'while' has no attr named '_XlaCompile'.
&lt;/denchmark-code&gt;

I am not sure what the issue is and I have tried it with tf-nighly and to use persistent=True in tf.GradientTape as well instead of declaring two gradientTapes in single watch. But, exactly the same error occurs.
Does anyone have any idea what this issue is? And how it can be solved?
Source code
&lt;denchmark-code&gt;import string
import nltk
import time

nltk.download('punkt')

import pandas as pd
import numpy as np
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

from keras.layers import Embedding
from tensorflow.keras import Input, Model
from tensorflow.keras.layers import *

# Load dataset
example_df = pd.read_csv('example.csv')

# Sample train/validation/test data
np.random.seed(100)
train, validation, test = np.split(example_df.sample(frac=1), [int(.7*len(example_df)), int(.85*len(example_df))])

# Class Weight Function
def compute_sample_weights(df, target, t_expect, weight_name):
  # Setup mitigator_weight
  df[weight_name] = 0

  # Get frequencies per target level
  targets = df.groupby(target).size()

  # Compute sample weights
  target_weights = t_expect / (targets / targets.sum())

  # Convert to dictionary
  target_dict = target_weights.to_dict()

  # Add sample weights to dataframe
  for i in target_dict:
    df[weight_name] = np.where((df[target] == i), target_dict[i], df[weight_name])
  
  return df

# Compute Main Class Weights
train = compute_sample_weights(df=train, target='target', t_expect=(1/3), weight_name='mainClass_weight')

# Compute Protect Class Weights
train = compute_sample_weights(df=train, target='protect', t_expect=(1/2), weight_name='protectClass_weight')

# Preprocess Text Data
vocab_size = 25000
max_length = 300
padding_type = 'post'
trunc_type = 'post'
oov_tok = '&lt;unk&gt;'

def Text_to_Seq(train, val, test, vocab_size, max_length, padding_type, trunc_type, oov_tok):
    # Text tokenization
    tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)
    tokenizer.fit_on_texts(train)

    # Create word_index
    word_index = tokenizer.word_index

    # Transforms each text doc to a sequence of integers in train, val and test
    x = tokenizer.texts_to_sequences(train)
    y = tokenizer.texts_to_sequences(val)
    z = tokenizer.texts_to_sequences(test)

    # Pad sequences to the same length
    x = pad_sequences(x, maxlen=max_length, padding=padding_type, truncating=trunc_type)
    y = pad_sequences(y, maxlen=max_length, padding=padding_type, truncating=trunc_type)
    z = pad_sequences(z, maxlen=max_length, padding=padding_type, truncating=trunc_type)

    return x, y, z, word_index

train_text = train['reviewText']
val_text = validation['reviewText']
test_text = test['reviewText']

train_text, val_text, test_text, word_index = Text_to_Seq(train_text, val_text, test_text, vocab_size, max_length, padding_type, trunc_type, oov_tok)

# Create input, target, protect, sample_weights for Train
xu_train = np.array(train_text, dtype=np.int32)
xs_train = np.array(train.iloc[:, np.r_[0, 2, 10:30]], dtype=np.int32)
y_train = np.array(train[['target_negative', 'target_neutral', 'target_positive']], dtype=np.float32)
z_train = np.array(train['protect_m'], dtype=np.float32).reshape((-1,1))
mainClass_weight = np.array(train['mainClass_weight'], dtype=np.float32).reshape((-1,1))
protectClass_weight = np.array(train['protectClass_weight'], dtype=np.float32).reshape((-1,1))

# Create input, target, protect for Validation
xu_val = np.array(val_text, dtype=np.int32)
xs_val = np.array(validation.iloc[:, np.r_[0, 2, 8:28]], dtype=np.int32)
y_val = np.array(validation[['target_negative', 'target_neutral', 'target_positive']], dtype=np.float32)
z_val = np.array(validation['protect_m'], dtype=np.float32).reshape((-1,1))

# Create input, target, protect for Test
xu_test = np.array(test_text, dtype=np.int32)
xs_test = np.array(test.iloc[:, np.r_[0, 2, 8:28]], dtype=np.int32)
y_test = np.array(test[['target_negative', 'target_neutral', 'target_positive']], dtype=np.float32)
z_test = np.array(test['protect_m'], dtype=np.float32).reshape((-1,1))
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;# Setup Pretrained GloVe Embedding
def load_embedding(file_path):
    # Initialize embeddings_index
    embeddings_index = {}

    # Store pretrained word vectors in embeddings_index
    with open(file_path) as f:
        for line in f:
            values = line.split(" ")
            word = values[0]
            coefs = np.asarray(values[1:], dtype='float32')
            embeddings_index[word] = coefs

    # Print number of word vectors found
    print("Found %s word vectors." % len(embeddings_index))

    return embeddings_index

embeddings_index = load_embedding('glove.840B.300d.txt') # obtained from https://nlp.stanford.edu/projects/glove/

# Create Embedding matrix
num_tokens = min(vocab_size, len(word_index))+1
embed_dim = 300

def embedding_matrix(word_index, embeddings_index, num_tokens, embed_dim):
    # Initialize embedding_matrix and counters
    hits = 0
    misses = 0
    embedding_matrix = np.zeros((num_tokens, embed_dim))

    # Create embedding_matrix
    for word, i in word_index.items():
      if i &gt; vocab_size:
        continue
      embedding_vector = embeddings_index.get(word)
      if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector
        hits += 1
      else:
        misses += 1

    # Print number of hits and misses
    print("Converted %d words (%d misses)" % (hits, misses))

    return embedding_matrix

embedding_matrix = embedding_matrix(word_index, embeddings_index, num_tokens, embed_dim)
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;class model_components:

  def mitigation_expert():
    inputs = Input(shape=(300,), dtype=tf.int32, name="me_input")
    x = Embedding(num_tokens, 300, weights=[embedding_matrix], input_length=max_length, trainable=False, name="me_embedding")(inputs)
    x = LSTM(300, return_sequences=False, name="me_lstm")(x)

    model = Model(inputs, x)

    return model

  def control_expert():
    inputs = Input(shape=(22,), dtype=tf.int32, name="ce_input")
    y = Dense(19, activation='relu', name="ce_hidden")(inputs)

    model = Model(inputs, y)

    return model

  def main_classifier():
    # Expert components
    me = model_components.mitigation_expert()
    ce = model_components.control_expert()

    # Main classifier
    ensemble = concatenate([me.output, ce.output], name="pred_ensemble")
    pred_output = Dense(319, activation="relu", name="pred_hidden")(ensemble)
    pred_output = Dense(3, activation="softmax", name="pred_output")(pred_output)

    model = Model(inputs=[me.input, ce.input], outputs=pred_output, name="main_classifier")

    return model
  
  def adversary_classifier():
    # Mitigation Expert component
    me = model_components.mitigation_expert()

    # Adversary classifier
    adv_output = Dense(300, activation='relu', name="adv_hidden")(me.output)
    adv_output = Dense(1, activation='sigmoid', name="adv_output")(adv_output)

    model = Model(inputs=me.input, outputs=adv_output, name="adversary_classifier")

    return model

def tf_normalize(x):
  return x / (tf.norm(x) + np.finfo(np.float32).tiny)

class DebiasModel(keras.Model):
    def __init__(self, main_classifier, adversary_classifier):
        super(DebiasModel, self).__init__()
        self.main_classifier = main_classifier
        self.adversary_classifier = adversary_classifier

    def compile(self, mc_optimizer, adv_optimizer, mc_loss, adv_loss, debias_param):
        super(DebiasModel, self).compile()
        self.mc_optimizer = mc_optimizer
        self.adv_optimizer = adv_optimizer
        self.mc_loss = mc_loss
        self.adv_loss = adv_loss
        self.debias_param = debias_param

    def train_step(self, data):
      # Unpack data from model.fit()
      x, y, sample_weight = data

      # Unpack input and output features
      xu, xs = x
      y_mc = y['pred_output']
      z_adv = y['adv_output']

      # Unpack sample_weights
      mainClass_weights = sample_weight["pred_output"]
      protectClass_weights = sample_weight["adv_output"]

      # Generate prediction and compute loss for Main_Classifier
      with tf.GradientTape() as mc_tape, tf.GradientTape() as me_mc_tape:
        mc_pred = self.main_classifier([xu, xs], training=True)
        mc_loss = self.mc_loss(y_mc, mc_pred, sample_weight=mainClass_weights)
      
      # Compute and Apply Gradients for CE &amp; Main Classifier
      mc_trainable_vars = self.main_classifier.trainable_weights[3:]
      mc_grads = mc_tape.gradient(mc_loss, mc_trainable_vars)
      self.mc_optimizer.apply_gradients(zip(mc_grads, mc_trainable_vars))

      # Generate prediction and compute loss for Adversary_Classifier
      with tf.GradientTape() as adv_tape, tf.GradientTape() as me_adv_tape:
        adv_pred = self.adversary_classifier(xu)
        adv_loss = self.adv_loss(z_adv, adv_pred, sample_weight=protectClass_weights)
      
      # Compute and Apply Gradients for CE &amp; Main Classifier
      adv_trainable_vars = self.adversary_classifier.trainable_weights[3:]
      adv_grads = adv_tape.gradient(adv_loss, adv_trainable_vars)
      self.adv_optimizer.apply_gradients(zip(adv_grads, adv_trainable_vars))

      # Compute and Apply Gradients to debias ME
      me_adv_debias_trainable_vars = self.adversary_classifier.trainable_weights[:3]
      adv_debias_grads = me_adv_tape.gradient(adv_loss, me_adv_debias_trainable_vars)
      adv_debias_dict = tf.lookup.StaticHashTable(
          tf.lookup.KeyValueTensorInitializer(me_adv_debias_trainable_vars, adv_debias_grads), 0)
      
      me_mc_debias_trainable_vars = self.main_classifier.trainable_weights[:3]
      mc_debias_grads = me_mc_tape.gradient(mc_loss, me_mc_debias_trainable_vars)

      me_grads = []

      for g, v in zip(mc_debias_grads, me_mc_debias_trainable_vars):
        unit_adv = tf_normalize(adv_debias_dict.lookup(v))
        g -= tf.math.reduce_sum(g * unit_adv) * unit_adv
        g -= self.debias_param * adv_debias_dict.lookup(v)
        me_grads.append(zip(g, v))
      
      self.mc_optimizer.apply_gradients(me_grads)
      
      return {"pred_loss": mc_loss, "adv_loss": adv_loss}

# Build and Fit Model
model = DebiasModel(model_components.main_classifier(),
                    model_components.adversary_classifier())

model.compile(mc_optimizer=tf.keras.optimizers.Adam(),
              adv_optimizer=tf.keras.optimizers.Adam(),
              mc_loss=tf.keras.losses.CategoricalCrossentropy(),
              adv_loss=tf.keras.losses.BinaryCrossentropy(),
              debias_param=1)

epoch = 5
sample_weights = {
    "pred_output": mainClass_weight,
    "adv_output": protectClass_weight,}

model.fit(x=[xu_train, xs_train],
          y={"pred_output": y_train, "adv_output": z_train},
          validation_data=([xu_val, xs_val], {"pred_output": y_val, "adv_output": z_val}),
          sample_weight=sample_weights,	epochs=epoch, batch_size=256, verbose=1)
&lt;/denchmark-code&gt;

Error Traceback Log
&lt;denchmark-code&gt;---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in get_attr(self, name)
   2485       with c_api_util.tf_buffer() as buf:
-&gt; 2486         pywrap_tf_session.TF_OperationGetAttrValueProto(self._c_op, name, buf)
   2487         data = pywrap_tf_session.TF_GetBuffer(buf)

InvalidArgumentError: Operation 'while' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
54 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)
    330     try:
--&gt; 331       xla_compile = op.get_attr("_XlaCompile")
    332       xla_separate_compiled_gradients = op.get_attr(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in get_attr(self, name)
   2489       # Convert to ValueError for backwards compatibility.
-&gt; 2490       raise ValueError(str(e))
   2491     x = attr_value_pb2.AttrValue()

ValueError: Operation 'while' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

KeyboardInterrupt                         Traceback (most recent call last)
&lt;ipython-input-43-e00f829ae927&gt; in &lt;module&gt;()
      8           y={"pred_output": y_train, "adv_output": z_train},
      9           validation_data=([xu_val, xs_val], {"pred_output": y_val, "adv_output": z_val}),
---&gt; 10           sample_weight=sample_weights,	epochs=epoch, batch_size=256, verbose=1)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
    106   def _method_wrapper(self, *args, **kwargs):
    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
--&gt; 108       return method(self, *args, **kwargs)
    109 
    110     # Running inside `run_distribute_coordinator` already.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1096                 batch_size=batch_size):
   1097               callbacks.on_train_batch_begin(step)
-&gt; 1098               tmp_logs = train_function(iterator)
   1099               if data_handler.should_sync:
   1100                 context.async_wait()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    778       else:
    779         compiler = "nonXla"
--&gt; 780         result = self._call(*args, **kwds)
    781 
    782       new_tracing_count = self._get_tracing_count()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    821       # This is the first call of __call__, so we have to initialize.
    822       initializers = []
--&gt; 823       self._initialize(args, kwds, add_initializers_to=initializers)
    824     finally:
    825       # At this point we know that the initialization is complete (or less

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    695     self._concrete_stateful_fn = (
    696         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--&gt; 697             *args, **kwds))
    698 
    699     def invalid_creator_scope(*unused_args, **unused_kwds):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2853       args, kwargs = None, None
   2854     with self._lock:
-&gt; 2855       graph_function, _, _ = self._maybe_define_function(args, kwargs)
   2856     return graph_function
   2857 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3211 
   3212       self._function_cache.missed.add(call_context_key)
-&gt; 3213       graph_function = self._create_graph_function(args, kwargs)
   3214       self._function_cache.primary[cache_key] = graph_function
   3215       return graph_function, args, kwargs

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3073             arg_names=arg_names,
   3074             override_flat_arg_shapes=override_flat_arg_shapes,
-&gt; 3075             capture_by_value=self._capture_by_value),
   3076         self._function_attributes,
   3077         function_spec=self.function_spec,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    984         _, original_func = tf_decorator.unwrap(python_func)
    985 
--&gt; 986       func_outputs = python_func(*func_args, **func_kwargs)
    987 
    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    598         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    599         # the function a weak reference to itself to avoid a reference cycle.
--&gt; 600         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    601     weak_wrapped_fn = weakref.ref(wrapped_fn)
    602 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    967                     recursive=True,
    968                     optional_features=autograph_options,
--&gt; 969                     user_requested=True,
    970                 ))
    971           except Exception as e:  # pylint:disable=broad-except

/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, args, kwargs, caller_fn_scope, options)
    594     try:
    595       if kwargs is not None:
--&gt; 596         result = converted_f(*effective_args, **kwargs)
    597       else:
    598         result = converted_f(*effective_args)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in tf__train_function(iterator)
     14                 try:
     15                     do_return = True
---&gt; 16                     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
     17                 except:
     18                     do_return = False

/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, args, kwargs, caller_fn_scope, options)
    530 
    531   if not options.user_requested and conversion.is_whitelisted(f):
--&gt; 532     return _call_unconverted(f, args, kwargs, options)
    533 
    534   # internal_convert_user_code is for example turned off when issuing a dynamic

/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in _call_unconverted(f, args, kwargs, options, update_cache)
    338   if kwargs is not None:
    339     return f(*args, **kwargs)
--&gt; 340   return f(*args)
    341 
    342 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in step_function(model, iterator)
    794 
    795       data = next(iterator)
--&gt; 796       outputs = model.distribute_strategy.run(run_step, args=(data,))
    797       outputs = reduce_per_replica(
    798           outputs, self.distribute_strategy, reduction='first')

/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py in run(***failed resolving arguments***)
   1209       fn = autograph.tf_convert(
   1210           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)
-&gt; 1211       return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
   1212 
   1213   # TODO(b/151224785): Remove deprecated alias.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py in call_for_each_replica(self, fn, args, kwargs)
   2583       kwargs = {}
   2584     with self._container_strategy().scope():
-&gt; 2585       return self._call_for_each_replica(fn, args, kwargs)
   2586 
   2587   def _call_for_each_replica(self, fn, args, kwargs):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py in _call_for_each_replica(self, fn, args, kwargs)
   2943         self._container_strategy(),
   2944         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):
-&gt; 2945       return fn(*args, **kwargs)
   2946 
   2947   def _reduce_to(self, reduce_op, value, destinations, experimental_hints):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    253       try:
    254         with conversion_ctx:
--&gt; 255           return converted_call(f, args, kwargs, options=options)
    256       except Exception as e:  # pylint:disable=broad-except
    257         if hasattr(e, 'ag_error_metadata'):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, args, kwargs, caller_fn_scope, options)
    530 
    531   if not options.user_requested and conversion.is_whitelisted(f):
--&gt; 532     return _call_unconverted(f, args, kwargs, options)
    533 
    534   # internal_convert_user_code is for example turned off when issuing a dynamic

/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in _call_unconverted(f, args, kwargs, options, update_cache)
    337 
    338   if kwargs is not None:
--&gt; 339     return f(*args, **kwargs)
    340   return f(*args)
    341 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in run_step(data)
    787 
    788       def run_step(data):
--&gt; 789         outputs = model.train_step(data)
    790         # Ensure counter is updated only if `train_step` succeeds.
    791         with ops.control_dependencies(_minimum_control_deps(outputs)):

&lt;ipython-input-41-7d8c3d718a99&gt; in train_step(self, data)
     39       # Generate prediction and compute loss for Main_Classifier
     40       with tf.GradientTape() as mc_tape, tf.GradientTape() as me_mc_tape:
---&gt; 41         mc_pred = self.main_classifier([xu, xs], training=True)
     42         mc_loss = self.mc_loss(y_mc, mc_pred, sample_weight=mainClass_weights)
     43 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
    983 
    984         with ops.enable_auto_cast_variables(self._compute_dtype_object):
--&gt; 985           outputs = call_fn(inputs, *args, **kwargs)
    986 
    987         if self._activity_regularizer:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py in call(self, inputs, training, mask)
    384     """
    385     return self._run_internal_graph(
--&gt; 386         inputs, training=training, mask=mask)
    387 
    388   def compute_output_shape(self, input_shape):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py in _run_internal_graph(self, inputs, training, mask)
    506 
    507         args, kwargs = node.map_arguments(tensor_dict)
--&gt; 508         outputs = node.layer(*args, **kwargs)
    509 
    510         # Update tensor_dict.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/recurrent.py in __call__(self, inputs, initial_state, constants, **kwargs)
    661 
    662     if initial_state is None and constants is None:
--&gt; 663       return super(RNN, self).__call__(inputs, **kwargs)
    664 
    665     # If any of `initial_state` or `constants` are specified and are Keras

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
    983 
    984         with ops.enable_auto_cast_variables(self._compute_dtype_object):
--&gt; 985           outputs = call_fn(inputs, *args, **kwargs)
    986 
    987         if self._activity_regularizer:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/recurrent_v2.py in call(self, inputs, mask, training, initial_state)
   1181       else:
   1182         (last_output, outputs, new_h, new_c,
-&gt; 1183          runtime) = lstm_with_backend_selection(**normal_lstm_kwargs)
   1184 
   1185       states = [new_h, new_c]

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/recurrent_v2.py in lstm_with_backend_selection(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask, time_major, go_backwards, sequence_lengths, zero_output_for_mask)
   1556   # grappler will kick in during session execution to optimize the graph.
   1557   last_output, outputs, new_h, new_c, runtime = defun_standard_lstm(
-&gt; 1558       **params)
   1559   function.register(defun_gpu_lstm, **params)
   1560 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
   2827     with self._lock:
   2828       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
-&gt; 2829     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   2830 
   2831   @property

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs, cancellation_manager)
   1846                            resource_variable_ops.BaseResourceVariable))],
   1847         captured_inputs=self.captured_inputs,
-&gt; 1848         cancellation_manager=cancellation_manager)
   1849 
   1850   def _call_flat(self, args, captured_inputs, cancellation_manager=None):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1927         possible_gradient_type,
   1928         executing_eagerly)
-&gt; 1929     forward_function, args_with_tangents = forward_backward.forward()
   1930     if executing_eagerly:
   1931       flat_outputs = forward_function.call(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in forward(self)
   1431     """Builds or retrieves a forward function for this call."""
   1432     forward_function = self._functions.forward(
-&gt; 1433         self._inference_args, self._input_tangents)
   1434     return forward_function, self._inference_args + self._input_tangents
   1435 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in forward(self, inference_args, input_tangents)
   1187       (self._forward, self._forward_graph, self._backward,
   1188        self._forwardprop_output_indices, self._num_forwardprop_outputs) = (
-&gt; 1189            self._forward_and_backward_functions(inference_args, input_tangents))
   1190     return self._forward
   1191 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _forward_and_backward_functions(self, inference_args, input_tangents)
   1387       outputs = list(self._func_graph.outputs)
   1388       self._build_functions_for_outputs(
-&gt; 1389           outputs, inference_args, input_tangents)
   1390     (forward_function, forward_graph,
   1391      backward_function, output_indices, num_output_tangents) = (

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _build_functions_for_outputs(self, outputs, inference_args, input_tangents)
    897             self._func_graph.inputs,
    898             grad_ys=gradients_wrt_outputs,
--&gt; 899             src_graph=self._func_graph)
    900 
    901       captures_from_forward = [

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)
    667                 # functions.
    668                 in_grads = _MaybeCompile(grad_scope, op, func_call,
--&gt; 669                                          lambda: grad_fn(op, *out_grads))
    670               else:
    671                 # For function call ops, we add a 'SymbolicGradient'

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)
    334       xla_scope = op.get_attr("_XlaScope").decode()
    335     except ValueError:
--&gt; 336       return grad_fn()  # Exit early
    337 
    338   if not xla_compile:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in &lt;lambda&gt;()
    667                 # functions.
    668                 in_grads = _MaybeCompile(grad_scope, op, func_call,
--&gt; 669                                          lambda: grad_fn(op, *out_grads))
    670               else:
    671                 # For function call ops, we add a 'SymbolicGradient'

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/while_v2.py in _WhileGrad(op, *grads)
    395   cond_grad_graph = func_graph_module.func_graph_from_py_func(
    396       grad_cond_name, grad_cond, loop_vars, {},
--&gt; 397       func_graph=util.WhileCondFuncGraph(grad_cond_name))
    398 
    399   _check_num_inputs_outputs(cond_grad_graph, body_grad_graph, len(loop_vars))

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    901       kwarg_shapes = None
    902     func_args = _get_defun_inputs_from_args(
--&gt; 903         args, arg_names, flat_shapes=arg_shapes)
    904     func_kwargs = _get_defun_inputs_from_kwargs(
    905         kwargs, flat_shapes=kwarg_shapes)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in _get_defun_inputs_from_args(args, names, flat_shapes)
   1137   """Maps Python function positional args to graph-construction inputs."""
   1138   return _get_defun_inputs(
-&gt; 1139       args, names, structure=args, flat_shapes=flat_shapes)
   1140 
   1141 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in _get_defun_inputs(args, names, structure, flat_shapes)
   1210           placeholder = graph_placeholder(
   1211               arg.dtype, placeholder_shape,
-&gt; 1212               name=requested_name)
   1213         except ValueError:
   1214           # Sometimes parameter names are not valid op names, so fall back to

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/graph_only_ops.py in graph_placeholder(dtype, shape, name)
     38   op = g._create_op_internal(  # pylint: disable=protected-access
     39       "Placeholder", [], [dtype], input_types=[],
---&gt; 40       attrs=attrs, name=name)
     41   result, = op.outputs
     42   if op_callbacks.should_invoke_op_callbacks():

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in _create_op_internal(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)
    591     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access
    592         op_type, inputs, dtypes, input_types, name, attrs, op_def,
--&gt; 593         compute_device)
    594 
    595   def capture(self, tensor, name=None, shape=None):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _create_op_internal(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)
   3483           input_types=input_types,
   3484           original_op=self._default_original_op,
-&gt; 3485           op_def=op_def)
   3486       self._create_op_helper(ret, compute_device=compute_device)
   3487     return ret

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)
   1985       tf_output = c_api_util.tf_output(self._c_op, i)
   1986       output_type = pywrap_tf_session.TF_OperationOutputType(tf_output)
-&gt; 1987       tensor = Tensor._create_with_tf_output(self, i, output_type, tf_output)  # pylint: disable=protected-access
   1988       self._outputs.append(tensor)
   1989 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _create_with_tf_output(op, value_index, dtype, tf_output)
    386   @staticmethod
    387   def _create_with_tf_output(op, value_index, dtype, tf_output):
--&gt; 388     ret = Tensor(op, value_index, dtype)
    389     ret._tf_output = tf_output
    390     return ret

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in __init__(self, op, value_index, dtype)
    373     self._op = op
    374     self._value_index = value_index
--&gt; 375     self._dtype = dtypes.as_dtype(dtype)
    376     # This will be set by self._as_tf_output().
    377     self._tf_output = None

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py in as_dtype(type_value)
    623     TypeError: If `type_value` cannot be converted to a `DType`.
    624   """
--&gt; 625   if isinstance(type_value, DType):
    626     return _INTERN_TABLE[type_value.as_datatype_enum]
    627 
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='nvrsmeele' date='2020-08-13T09:18:34Z'>
		&lt;denchmark-link:https://github.com/nvrsmeele&gt;@nvrsmeele&lt;/denchmark-link&gt;

Looks like code is incomplete. num_tokens, 'embedding_matrix' is not defined.
Please, share reproducible code .It helps us in localizing the issue faster.Thanks!
		</comment>
		<comment id='2' author='nvrsmeele' date='2020-08-13T09:49:10Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;

I am sorry. Thanks for mentioning how I can improve the reproducible code!
I have updated the code above. Just for your info, I looked at other similar reported bug issue but nothing worked so far.. I hope you can help me with this.
		</comment>
		<comment id='3' author='nvrsmeele' date='2020-08-13T11:13:17Z'>
		&lt;denchmark-link:https://github.com/nvrsmeele&gt;@nvrsmeele&lt;/denchmark-link&gt;

If possible can you please share example.csvfile.Thanks!
		</comment>
		<comment id='4' author='nvrsmeele' date='2020-08-13T11:39:09Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;

Of course, I have created a test csv-file (see below). Also, I have created a Google Colab &lt;denchmark-link:https://colab.research.google.com/drive/1HldF0WOeQvlGYxQOOJEtpa0v7WPW7xYv?usp=sharing&gt;here&lt;/denchmark-link&gt;
 in which you can run it and reproduce the error. If you need anything more, please let me know!
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/5068745/example.csv.zip&gt;example.csv.zip&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='nvrsmeele' date='2020-08-14T01:24:42Z'>
		&lt;denchmark-link:https://github.com/nvrsmeele&gt;@nvrsmeele&lt;/denchmark-link&gt;
 Can you please simple standalone code to reproduce the issue? It will be too much to debug and find the root-cause if the code is too big. Please try to find minimal reproducible code and share it.
GutHub is mainly for bugs and performance issues. If this is not a bug, then please post it in Stackoverflow as there is big community who can support you faster. Thanks!
		</comment>
		<comment id='6' author='nvrsmeele' date='2020-08-14T14:44:11Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 Thank you for response and help! I have simplified the issue reproduction code in &lt;denchmark-link:https://colab.research.google.com/drive/1HldF0WOeQvlGYxQOOJEtpa0v7WPW7xYv?usp=sharing&gt;this&lt;/denchmark-link&gt;
 Colab environment. Also, you can use &lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/5075055/example.zip&gt;example.zip&lt;/denchmark-link&gt;
 to run the code and reproduce the issue.
The error occurs in the class CustomModel(keras.Model): when the learning procedure in fit() is customized by overriding the train_step. I have tried multiple things but the same issue occurs. From the traceback, I read that the issue is triggered in the following code-block in the class CustomModel(keras.Model)::
&lt;denchmark-code&gt;    with tf.GradientTape(persistent=True) as tape:
      # Forward pass
      pred = self([xu, xs], training=True) # Issue triggered here (see traceback)
      # Compute losses
      mc_loss = self.mainClass_loss(y_mc, pred[0], sample_weight=mainClass_weights)
      adv_loss = self.advClass_loss(z_adv, pred[1], sample_weight=protectClass_weights)
&lt;/denchmark-code&gt;

Just for your information, I have created a multi-input/-output model by using Keras Function API which needs to be trained with a custom learning algorithm.
From the error traceback, my guess is that it could be a back-end error. But, I could also be wrong about this.... To override the , I have used &lt;denchmark-link:https://keras.io/guides/customizing_what_happens_in_fit/&gt;this&lt;/denchmark-link&gt;
 tutorial in the Keras Developer Guide.
With regards to your last comment, I have posted it on Stackoverflow but without success to solve the issue.
If you need more information, please let me know. I hope you can help me with this - many thanks!
		</comment>
		<comment id='7' author='nvrsmeele' date='2020-08-15T11:21:18Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 I just did another attempt by using TF 2.2.0 and Keras 2.3.0 but the same issue occurs again.. However, this time an additional message is printed in the traceback:
InvalidArgumentError: Operation 'TensorListPushBack_145' has no attr named '_XlaCompile'.
I did some more searching on this and found that there was a similar bug issue reported a month ago. The report bug issue was &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/41394&gt;#41394&lt;/denchmark-link&gt;
 - but, it does not seem that the issue is solved.. since the issue has been stalled for some reason..
		</comment>
		<comment id='8' author='nvrsmeele' date='2020-08-19T08:50:14Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 I was wondering if you had the time to look at this issue. Is the simplified reproduction code good enough? I am still facing this issue. If you need anything from my side, please let me know
		</comment>
		<comment id='9' author='nvrsmeele' date='2020-08-24T23:00:41Z'>
		Hi &lt;denchmark-link:https://github.com/nvrsmeele&gt;@nvrsmeele&lt;/denchmark-link&gt;
, thanks for reporting this. Looks like &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/41394&gt;#41394&lt;/denchmark-link&gt;
 was closed because the author never provided reproducible code. I was able to reproduce the behavior with the colab and csv you provided. It seems the line in the source code &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients_util.py#L331&gt;causing the problem is here&lt;/denchmark-link&gt;
.
There is still quite a lot of custom code to unpack in your example, and it would be great if you could help to narrow down the issue. Firstly, have you tried rewriting this code so you're writing your training loop from scratch?  I agree the fact that it's hanging is not great, but the stack trace suggests this has something to do with the model compilation, so I'm wondering if you can get this to work without having to override what happens in fit and just write the whole loop from scratch (&lt;denchmark-link:https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch&gt;docs example here&lt;/denchmark-link&gt;
).
		</comment>
		<comment id='10' author='nvrsmeele' date='2020-08-31T23:22:15Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='11' author='nvrsmeele' date='2020-09-08T00:16:24Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='12' author='nvrsmeele' date='2020-09-08T00:16:27Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42306&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42306&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>