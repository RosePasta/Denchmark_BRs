<bug id='45138' author='gioxc88' open_date='2020-11-24T05:23:31Z' closed_time='2020-12-03T18:10:55Z'>
	<summary>keras doesn't pickle the correct layer in model Sequential</summary>
	<description>
System information

OS Platform: Windows, Linux
TensorFlow version ('v2.3.0-54-gfcc4b966f1', '2.3.1')
Python version: 3.8.6

Current behavior
I need to test two versions of BatchNormalization for making sure that moving to from tf1 to tf2 doesn't cause any dramatic change in the model performance.
I have two alternatives in tf2:
&lt;denchmark-code&gt;from tensorflow.python.keras.layers.normalization import BatchNormalization as BatchNormalization1
from tensorflow.python.keras.layers.normalization_v2 import BatchNormalization as BatchNormalization2
&lt;/denchmark-code&gt;

The first option replicates tf1 behavior whereas the second one replicates tf2 behavior (the second import is also equivalent to from tensorflow.keras.layers import BatchNormalization as BatchNormalization2)
Now I create two simple sequential models containing only one layer each
&lt;denchmark-code&gt;from tensorflow.keras.models import Sequential, load_model

batch_norm1 = BatchNormalization1(input_shape=[28, 28])
batch_norm2 = BatchNormalization2(input_shape=[28, 28])

model1 = Sequential([batch_norm1])
model2 = Sequential([batch_norm2])
&lt;/denchmark-code&gt;

If I print the models layers I get:
&lt;denchmark-code&gt;print(model1.layers)
[&lt;tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x0000022446319160&gt;]

print(model2.layers)
[&lt;tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x0000022446319460&gt;]

&lt;/denchmark-code&gt;

So far so good, nothing unexpected.
However, if I now save the two models and then load them in a different session (or also in the same one) the first model mysteriously changes its class meaning that is loaded as tensorflow.python.keras.layers.normalization_v2.BatchNormalization object
instead of tensorflow.python.keras.layers.normalization.BatchNormalization object
&lt;denchmark-code&gt;model1.save('m1.h5')
model2.save('m2.h5')

model1 = load_model('m1.h5')
model2 = load_model('m2.h5')

print(model1.layers)
[&lt;tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x00000224463C5DC0&gt;]

print(model2.layers)
[&lt;tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x0000022446360C40&gt;]
&lt;/denchmark-code&gt;

I believe this is a bug.
Useful info for isolating the problem

The same behavior is present if I test tensorflow.python.keras.layers.recurrent.GRU and tensorflow.python.keras.layers.recurrent_v2.GRU
This problem is only present if I use the layer inside a keras Model, but if try to pickle the layer directly everything works as expected.

Many thanks
Gio
	</description>
	<comments>
		<comment id='1' author='gioxc88' date='2020-11-26T13:01:43Z'>
		any news on this?
Many thanks
		</comment>
		<comment id='2' author='gioxc88' date='2020-11-27T09:27:37Z'>
		&lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;

I am able to replicate this issue,please find the &lt;denchmark-link:https://colab.research.google.com/gist/Saduf2019/f35f56ba80a00b33449ebb8ce0ffbc53/untitled474.ipynb&gt;gist here&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='3' author='gioxc88' date='2020-12-01T02:00:17Z'>
		I see that if you load the model in TF 1.X session (1.15.2) it gives,
[&lt;tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x7fa89a4561d0&gt;]
[&lt;tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x7fa89a3ef8d0&gt;]
		</comment>
		<comment id='4' author='gioxc88' date='2020-12-02T13:01:35Z'>
		
I see that if you load the model in TF 1.X session (1.15.2) it gives,
[&lt;tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x7fa89a4561d0&gt;]
[&lt;tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x7fa89a3ef8d0&gt;]

that's because in TF1 you don't have the _v2 modules
		</comment>
		<comment id='5' author='gioxc88' date='2020-12-03T18:10:55Z'>
		This is actually intended behavior. The difference between v1 batch normalization and v2 batch normalization is runtime-specific: they're not two different objects with different behaviors, as far as the saved model is concerned. Saving a BN layer in v1 and loading it in v2 is supposed to create the v2 class.
		</comment>
		<comment id='6' author='gioxc88' date='2020-12-03T18:10:57Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45138&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45138&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>