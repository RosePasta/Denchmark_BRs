<bug id='35649' author='emailweixu' open_date='2020-01-07T22:00:07Z' closed_time='2020-01-10T21:58:54Z'>
	<summary>Incorrect name_scope with tf.function decoration</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0
Python version: 3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

Describe the current behavior
If a function is decorated with tf.function, the name_scope is lost
Describe the expected behavior
The name_scope should be same with or without tf.function decoration
Code to reproduce the issue
import tensorflow as tf

@tf.function
def f():
    with tf.name_scope("f") as scope:
        tf.print(scope)

def g():
    with tf.name_scope("g") as scope:
        tf.print(scope)

def main():
    with tf.name_scope("main"):
        f()   # expect to print "main/f/", actually get "f/"
        g()

if __name__ == "__main__":
    main()
The output is
&lt;denchmark-code&gt;f/
main/g/
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='emailweixu' date='2020-01-08T03:51:29Z'>
		Issue is replicating for 2.0 and also TF-nightly, please find the &lt;denchmark-link:https://colab.sandbox.google.com/gist/oanush/40d1bf5f1cd626c48a9a55ff5b25527f/35649.ipynb&gt;gist&lt;/denchmark-link&gt;
 of colab.Thanks!
		</comment>
		<comment id='2' author='emailweixu' date='2020-01-10T21:58:54Z'>
		This is working as intended. The name_scope in the function is rooted at the function graph, and not at the call site (otherwise we'd have to retrace the function every time it's called to make sure it's using the proper name_scope). At runtime you can tell the correct scoping by the nesting of the name of the call op and the ops inside the function graph.
		</comment>
		<comment id='3' author='emailweixu' date='2020-01-10T21:58:56Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35649&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35649&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='emailweixu' date='2020-01-10T23:28:01Z'>
		I think that @tf.function should not change the behavior of the decorated function whenever possible. If a function behave differently after the decoration, it makes the code hard to write.
I am sure there are ways to avoid retracing, similar to experimental_relax_shapes.
The reason I notice this is that I have function doing some tf.summary.scalar(), because of the name scope, the summary tag is different after @tf.function decoration.
		</comment>
	</comments>
</bug>