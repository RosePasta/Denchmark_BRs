<bug id='29365' author='Eladsimba' open_date='2019-06-03T15:50:52Z' closed_time='2019-06-20T02:17:48Z'>
	<summary>training consumes a lot of RAM and gets killed</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 18.04.2 LTS
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
TensorFlow installed from (source or binary): n/a
TensorFlow version (use command below): 1.14.0-rc0
Python version: 3.5.2
Bazel version (if compiling from source): n/a
GCC/Compiler version (if compiling from source): n/a
CUDA/cuDNN version: CUDA 10.2
GPU model and memory: NVIDIA GeForce GTX 1080/PCIe/SSE2


I am trying to run the faster rcnn inception v2 model in a docker container.
i follow the instructions from &lt;denchmark-link:https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10#6-run-the-training&gt;https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10#6-run-the-training&lt;/denchmark-link&gt;

and in the past i was able to run training on their dataset (not inside a docker container).
now, when i try to run training, i get a "killed" message after "Recording summary at step 0".
the proccess is very demanding in terms of memory and reaches over 19 GB just before it stops.
i use nvidia/cuda:10.0-devel-ubuntu16.04 as a container.
Code to reproduce the issue
from models-master/research/object_detection:
python3 train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_v2_pets.config
Other info / logs
2019-06-03 13:02:38.264499: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-06-03 13:02:38.300525: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-06-03 13:02:38.320500: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-06-03 13:02:38.326786: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-06-03 13:02:38.372259: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-06-03 13:02:38.401320: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-06-03 13:02:38.401629: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
2019-06-03 13:02:38.401677: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1663] Cannot dlopen some GPU libraries. Skipping registering GPU devices...
2019-06-03 13:02:38.401725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-06-03 13:02:38.401758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0
2019-06-03 13:02:38.401787: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N
2019-06-03 13:02:40.182868: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
W0603 13:02:41.522990 140514728642304 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
I0603 13:02:41.525922 140514728642304 saver.py:1280] Restoring parameters from /home/models-master/research/object_detection/faster_rcnn_inception_v2_coco_2018_01_28/model.ckpt
I0603 13:02:42.337087 140514728642304 session_manager.py:500] Running local_init_op.
I0603 13:02:42.828116 140514728642304 session_manager.py:502] Done running local_init_op.
I0603 13:02:51.479744 140514728642304 learning.py:754] Starting Session.
I0603 13:02:51.717492 140509938947840 supervisor.py:1117] Saving checkpoint to path training/model.ckpt
I0603 13:02:51.721246 140514728642304 learning.py:768] Starting Queues.
I0603 13:03:00.753286 140509720868608 supervisor.py:1099] global_step/sec: 0
I0603 13:03:09.625022 140509712475904 supervisor.py:1050] Recording summary at step 0.
Killed
the config file:
model {
faster_rcnn {
num_classes: 6
image_resizer {
keep_aspect_ratio_resizer {
min_dimension: 600
max_dimension: 1024
}
}
feature_extractor {
type: 'faster_rcnn_inception_v2'
first_stage_features_stride: 16
}
first_stage_anchor_generator {
grid_anchor_generator {
scales: [0.25, 0.5, 1.0, 2.0]
aspect_ratios: [0.5, 1.0, 2.0]
height_stride: 16
width_stride: 16
}
}
first_stage_box_predictor_conv_hyperparams {
op: CONV
regularizer {
l2_regularizer {
weight: 0.0
}
}
initializer {
truncated_normal_initializer {
stddev: 0.01
}
}
}
first_stage_nms_score_threshold: 0.0
first_stage_nms_iou_threshold: 0.7
first_stage_max_proposals: 300
first_stage_localization_loss_weight: 2.0
first_stage_objectness_loss_weight: 1.0
initial_crop_size: 14
maxpool_kernel_size: 2
maxpool_stride: 2
second_stage_box_predictor {
mask_rcnn_box_predictor {
use_dropout: false
dropout_keep_probability: 1.0
fc_hyperparams {
op: FC
regularizer {
l2_regularizer {
weight: 0.0
}
}
initializer {
variance_scaling_initializer {
factor: 1.0
uniform: true
mode: FAN_AVG
}
}
}
}
}
second_stage_post_processing {
batch_non_max_suppression {
score_threshold: 0.0
iou_threshold: 0.6
max_detections_per_class: 100
max_total_detections: 300
}
score_converter: SOFTMAX
}
second_stage_localization_loss_weight: 2.0
second_stage_classification_loss_weight: 1.0
}
}
train_config: {
batch_size: 1
optimizer {
momentum_optimizer: {
learning_rate: {
manual_step_learning_rate {
initial_learning_rate: 0.0002
schedule {
step: 900000
learning_rate: .00002
}
schedule {
step: 1200000
learning_rate: .000002
}
}
}
momentum_optimizer_value: 0.9
}
use_moving_average: false
}
gradient_clipping_by_norm: 10.0
fine_tune_checkpoint: "/home/models-master/research/object_detection/faster_rcnn_inception_v2_coco_2018_01_28/model.ckpt"
from_detection_checkpoint: true
num_steps: 200000
data_augmentation_options {
random_horizontal_flip {
}
}
}
train_input_reader: {
tf_record_input_reader {
input_path: "/home/models-master/research/object_detection/train.record"
}
label_map_path: "/home/models-master/research/object_detection/training/labelmap.pbtxt"
queue_capacity: 10
min_after_dequeue: 5
}
eval_config: {
num_examples: 67
max_evals: 10
}
eval_input_reader: {
tf_record_input_reader {
input_path: "/home/models-master/research/object_detection/test.record"
}
label_map_path: "/home/models-master/research/object_detection/training/labelmap.pbtxt"
shuffle: false
num_readers: 1
}
	</description>
	<comments>
		<comment id='1' author='Eladsimba' date='2019-06-04T10:58:46Z'>
		This is related, since a year on stack. I wonder why session uses so much memory.
In the code I run, the variables should have memory of about 25MB = float16(2bytes) * numberOfMatrixElements * numberOfVariables. But it demands about 17GB or something which is quite insane.
&lt;denchmark-link:https://stackoverflow.com/questions/46114462/tensorflow-allocating-large-amounts-of-main-memory-at-session-startup-time&gt;https://stackoverflow.com/questions/46114462/tensorflow-allocating-large-amounts-of-main-memory-at-session-startup-time&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='Eladsimba' date='2019-06-04T21:08:21Z'>
		&lt;denchmark-link:https://github.com/Eladsimba&gt;@Eladsimba&lt;/denchmark-link&gt;
 As this is more related to TF models, please post this in &lt;denchmark-link:https://github.com/tensorflow/models/issues&gt;TF model repo&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='3' author='Eladsimba' date='2019-06-19T12:43:14Z'>
		It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue?
		</comment>
		<comment id='4' author='Eladsimba' date='2019-06-20T02:17:49Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=29365&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=29365&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>