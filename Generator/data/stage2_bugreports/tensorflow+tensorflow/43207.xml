<bug id='43207' author='ehazrati' open_date='2020-09-14T13:59:42Z' closed_time='2020-09-28T08:09:33Z'>
	<summary>RuntimeError: Quantization not yet supported for op: %</summary>
	<description>
System information

OS Platform and Distribution (Linux Ubuntu 18.04):
TensorFlow installed from (source or binary): binary
TensorFlow version (or github SHA if from source): TF 2.3.0

Command used to run the converter or code if youâ€™re using the Python API
&lt;denchmark-code&gt;def representative_data_gen():
    dataset_list = os.listdir(data_dir)
    num_calibration_images = 100
    norm_factor = 255.0
    for i in range(num_calibration_images):
        image_name = next(iter(dataset_list))
 
        image = cv2.imread(os.path.join(data_dir, image_name), 1)
        image = image.astype(np.float32)
        image = image/norm_factor

        image = tf.expand_dims(image, 0)
        yield [image]


converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.target_spec.supported_types = [tf.int8]

# These set the input and output tensors to uint8 (added in r2.3)
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8

converter.allow_custom_ops = True
tflite_model = converter.convert()
&lt;/denchmark-code&gt;

** Model definition:
&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

def conv2d_block_3layers(input_tensor, n_filters, kernel_size=3, dropout=0.2, 
                         batchnorm=True, activation=True):
    # first layer
    x = layers.Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),
                      padding = 'same')(input_tensor)
    if batchnorm:
        x = layers.BatchNormalization()(x)
    if activation:
        x = layers.Activation('relu')(x)
    x = layers.Dropout(dropout)(x)
    # second layer
    x = layers.Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),
                      padding = 'same')(x)
    if batchnorm:
        x = layers.BatchNormalization()(x)
    if activation:
        x = layers.Activation('relu')(x)
    x = layers.Dropout(dropout)(x)
    # third layer
    x = layers.Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),
                      padding = 'same')(x)
    if batchnorm:
        x = layers.BatchNormalization()(x)
    if activation:
        x = layers.Activation('relu')(x)
    return x

def UNET_v2(nClasses=25, input_height=288, input_width=224, n_filters=64, dropout=0.2, 
            batchnorm=True, activation=True):
  
    img_input = layers.Input(shape=(input_height, input_width, 3))  

    c1 = conv2d_block_3layers(img_input, n_filters * 1, kernel_size=3, batchnorm = batchnorm, activation=activation)
    p1 = layers.MaxPooling2D((2, 2))(c1) 

    c2 = conv2d_block_3layers(p1, n_filters * 2, kernel_size=3, batchnorm = batchnorm,  activation=activation)
    p2 = layers.MaxPooling2D((2, 2))(c2) 

    c3 = conv2d_block_3layers(p2, n_filters * 4, kernel_size=3, batchnorm = batchnorm,  activation=activation)
    p3 = layers.MaxPooling2D((2, 2))(c3) 

    c4 = conv2d_block_3layers(p3, n_filters * 4, kernel_size=3, batchnorm = batchnorm,  activation=activation)
    p4 = layers.MaxPooling2D((2, 2))(c4) 

    c5 = conv2d_block_3layers(p4, n_filters = n_filters * 8, kernel_size=3, batchnorm = batchnorm, activation=activation)
    p5 = layers.Dropout(dropout)(c5) 

    up6  = layers.Conv2DTranspose(n_filters * 4, kernel_size=(3,3), strides=(2,2), padding="same")(p5)
    # up6  = layers.UpSampling2D()(p5) 
    m6 = layers.Concatenate(axis=3)([up6, c4])
    c6 = conv2d_block_3layers(m6, n_filters * 4, kernel_size = 3, batchnorm = batchnorm, activation=activation)

    up7 = layers.Conv2DTranspose(n_filters * 4, kernel_size=(3,3), strides=(2,2), padding="same")(c6)
    # up7  = layers.UpSampling2D()(c6) 
    m7 = layers.Concatenate(axis=3)([up7, c3])
    c7 = conv2d_block_3layers(m7, n_filters * 4, kernel_size = 3, batchnorm = batchnorm, activation=activation)

    up8 = layers.Conv2DTranspose(n_filters * 2, kernel_size=(3,3), strides=(2,2), padding="same")(c7)
    # up8  = layers.UpSampling2D()(c7) 
    m8  = layers.Concatenate(axis=3)([up8, c2])
    c8 = conv2d_block_3layers(m8, n_filters * 2, kernel_size = 3, batchnorm = batchnorm, activation=activation)

    up9 = layers.Conv2DTranspose(n_filters * 1, kernel_size=(3,3), strides=(2,2), padding="same")(c8)
    # up9  = layers.UpSampling2D()(c8) 
    m9 = layers.Concatenate(axis=3)([up9, c1])
    c9 = conv2d_block_3layers(m9, n_filters * 1, kernel_size = 3, batchnorm = batchnorm, activation=activation)

    outputlayer = tf.keras.layers.Conv2D(filters=nClasses, kernel_size=1, activation="softmax")(c9)
    
    model = tf.keras.Model(inputs=img_input, outputs=outputlayer)
    model.summary(line_length=124)
    return model
&lt;/denchmark-code&gt;

The output from the converter invocation
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "keras_to_tflite.py", line 105, in &lt;module&gt;
    tflite_model = converter.convert()
  File "/home/ths/anaconda3/envs/py37_tf23/lib/python3.7/site-packages/tensorflow/lite/python/lite.py", line 831, in convert
    self).convert(graph_def, input_tensors, output_tensors)
  File "/home/ths/anaconda3/envs/py37_tf23/lib/python3.7/site-packages/tensorflow/lite/python/lite.py", line 638, in convert
    result = self._calibrate_quantize_model(result, **flags)
  File "/home/ths/anaconda3/envs/py37_tf23/lib/python3.7/site-packages/tensorflow/lite/python/lite.py", line 452, in _calibrate_quantize_model
    inference_output_type, allow_float, activations_type)
  File "/home/ths/anaconda3/envs/py37_tf23/lib/python3.7/site-packages/tensorflow/lite/python/optimize/calibrator.py", line 98, in calibrate_and_quantize
    np.dtype(activations_type.as_numpy_dtype()).num)
RuntimeError: Quantization not yet supported for op: %

&lt;/denchmark-code&gt;

The model architecture uses Conv2D, batchnormalization, dropout, MaxPooling2D, Conv2DTranspose, Concatenate layers. Is the operation % used in any of the mentioned layers? The % operation is not used at all.
	</description>
	<comments>
		<comment id='1' author='ehazrati' date='2020-09-14T14:18:12Z'>
		Can you please attach your model/ share google colab code to debug further? Thanks!
		</comment>
		<comment id='2' author='ehazrati' date='2020-09-14T14:28:24Z'>
		&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/5219078/test_model.zip&gt;test_model.zip&lt;/denchmark-link&gt;

Here is the Keras *.hdf5 model file.
		</comment>
		<comment id='3' author='ehazrati' date='2020-09-14T14:38:34Z'>
		I think you need to update your example also with the missing source of converter.representative_dataset = representative_data_gen
		</comment>
		<comment id='4' author='ehazrati' date='2020-09-14T15:57:52Z'>
		&lt;denchmark-link:https://github.com/ehazrati&gt;@ehazrati&lt;/denchmark-link&gt;
,
On running the code I am facing an error stating .
In order to expedite the trouble-shooting process, could you please provide the dataset you are using as well. Thanks!
		</comment>
		<comment id='5' author='ehazrati' date='2020-09-15T07:31:26Z'>
		&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/5223152/dataset.tar.gz&gt;dataset.tar.gz&lt;/denchmark-link&gt;

Here is the example data set. Thanks.
		</comment>
		<comment id='6' author='ehazrati' date='2020-09-15T11:16:25Z'>
		You are using what in Doc is described as &lt;denchmark-link:https://www.tensorflow.org/lite/performance/post_training_integer_quant#convert_using_integer-only_quantization&gt;Integer only quantization&lt;/denchmark-link&gt;
:

To quantize the input and output tensors, and make the converter throw an error if it encounters an operation it cannot quantize, convert the model again with some additional parameters.

You have some operations that it can't quantize.
You can check this running your example with pip install tf-nightly:
&lt;denchmark-code&gt;RuntimeError: Quantization not yet supported for op: 'EXP'.
Quantization not yet supported for op: 'DIV'.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='ehazrati' date='2020-09-15T14:13:29Z'>
		The problem is I want to be able to perform the inference on a Coral USB accelerator which has an edge TPU and it is an "integer-only" hardware. Therefore, I have to convert using integer-only quantization. For that to work, all operations in the model should have a quantized implementation, otherwise I won't get an end-to-end integer-only model and I cannot run it on an edge TPU.
As mentioned in the previous comments, the model uses the following layers:

layers.Conv2D
layers.BatchNormalization
layers.Dropout
layers.MaxPooling2D
layers.Concatenate
layers.Activation('relu')
layers.Conv2DTranspose  or layers.UpSampling2D
layers.Activation('softmax')

According to the guides, the above layers all have a quantized implementation, except for the  layers.Conv2DTranspose layer which I am not sure. For that reason I created two models one with layers.Conv2DTranspose and another one with layers.UpSampling2D.
Trying both, I got the sane Error you mentioned in your comment:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "keras_to_tflite.py", line 108, in &lt;module&gt;
    tflite_model = converter.convert()
  File "/home/ths/anaconda3/envs/py37_tf23/lib/python3.7/site-packages/tensorflow/lite/python/lite.py", line 847, in convert
    self).convert(graph_def, input_tensors, output_tensors)
  File "/home/ths/anaconda3/envs/py37_tf23/lib/python3.7/site-packages/tensorflow/lite/python/lite.py", line 648, in convert
    result = self._calibrate_quantize_model(result, **flags)
  File "/home/ths/anaconda3/envs/py37_tf23/lib/python3.7/site-packages/tensorflow/lite/python/lite.py", line 476, in _calibrate_quantize_model
    inference_output_type, allow_float, activations_type)
  File "/home/ths/anaconda3/envs/py37_tf23/lib/python3.7/site-packages/tensorflow/lite/python/optimize/calibrator.py", line 98, in calibrate_and_quantize
    np.dtype(activations_type.as_numpy_dtype()).num)
RuntimeError: Quantization not yet supported for op: 'EXP'.
Quantization not yet supported for op: 'DIV'.
&lt;/denchmark-code&gt;

I suspect that the error could be related to Softmax, as it is the one uses EXP and DIV operations. But, then I am able to perform all the steps mentioned in this tutorial: convert, compile, and run the model on TPU, without any problem (using tensorflow2.3):
&lt;denchmark-link:https://colab.research.google.com/github/google-coral/tutorials/blob/master/retrain_classification_ptq_tf2.ipynb#scrollTo=19IQ2gqneqmS&gt;https://colab.research.google.com/github/google-coral/tutorials/blob/master/retrain_classification_ptq_tf2.ipynb#scrollTo=19IQ2gqneqmS&lt;/denchmark-link&gt;

The model above uses Softmax as well.
Any idea what is happening? Is there a way to check where exactly the operations 'EXP'  and 'DIV' are performed in the model?
Thanks.
		</comment>
		<comment id='8' author='ehazrati' date='2020-09-15T14:26:14Z'>
		Can you edit/complete you example with your dummy model definition and saving instead of the saved file?
So that we have a minimal but complete example to run.
Thanks
		</comment>
		<comment id='9' author='ehazrati' date='2020-09-16T08:19:32Z'>
		** model definition
&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

def conv2d_block_3layers(input_tensor, n_filters, kernel_size=3, dropout=0.2, 
                         batchnorm=True, activation=True):
    # first layer
    x = layers.Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),
                      padding = 'same')(input_tensor)
    if batchnorm:
        x = layers.BatchNormalization()(x)
    if activation:
        x = layers.Activation('relu')(x)
    x = layers.Dropout(dropout)(x)
    # second layer
    x = layers.Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),
                      padding = 'same')(x)
    if batchnorm:
        x = layers.BatchNormalization()(x)
    if activation:
        x = layers.Activation('relu')(x)
    x = layers.Dropout(dropout)(x)
    # third layer
    x = layers.Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),
                      padding = 'same')(x)
    if batchnorm:
        x = layers.BatchNormalization()(x)
    if activation:
        x = layers.Activation('relu')(x)
    return x

def UNET_v2(nClasses=25, input_height=288, input_width=224, n_filters=64, dropout=0.2, 
            batchnorm=True, activation=True):
  
    img_input = layers.Input(shape=(input_height, input_width, 3))  

    c1 = conv2d_block_3layers(img_input, n_filters * 1, kernel_size=3, batchnorm = batchnorm, activation=activation)
    p1 = layers.MaxPooling2D((2, 2))(c1) 

    c2 = conv2d_block_3layers(p1, n_filters * 2, kernel_size=3, batchnorm = batchnorm,  activation=activation)
    p2 = layers.MaxPooling2D((2, 2))(c2) 

    c3 = conv2d_block_3layers(p2, n_filters * 4, kernel_size=3, batchnorm = batchnorm,  activation=activation)
    p3 = layers.MaxPooling2D((2, 2))(c3) 

    c4 = conv2d_block_3layers(p3, n_filters * 4, kernel_size=3, batchnorm = batchnorm,  activation=activation)
    p4 = layers.MaxPooling2D((2, 2))(c4) 

    c5 = conv2d_block_3layers(p4, n_filters = n_filters * 8, kernel_size=3, batchnorm = batchnorm, activation=activation)
    p5 = layers.Dropout(dropout)(c5) 

    up6  = layers.Conv2DTranspose(n_filters * 4, kernel_size=(3,3), strides=(2,2), padding="same")(p5)
    # up6  = layers.UpSampling2D()(p5) 
    m6 = layers.Concatenate(axis=3)([up6, c4])
    c6 = conv2d_block_3layers(m6, n_filters * 4, kernel_size = 3, batchnorm = batchnorm, activation=activation)

    up7 = layers.Conv2DTranspose(n_filters * 4, kernel_size=(3,3), strides=(2,2), padding="same")(c6)
    # up7  = layers.UpSampling2D()(c6) 
    m7 = layers.Concatenate(axis=3)([up7, c3])
    c7 = conv2d_block_3layers(m7, n_filters * 4, kernel_size = 3, batchnorm = batchnorm, activation=activation)

    up8 = layers.Conv2DTranspose(n_filters * 2, kernel_size=(3,3), strides=(2,2), padding="same")(c7)
    # up8  = layers.UpSampling2D()(c7) 
    m8  = layers.Concatenate(axis=3)([up8, c2])
    c8 = conv2d_block_3layers(m8, n_filters * 2, kernel_size = 3, batchnorm = batchnorm, activation=activation)

    up9 = layers.Conv2DTranspose(n_filters * 1, kernel_size=(3,3), strides=(2,2), padding="same")(c8)
    # up9  = layers.UpSampling2D()(c8) 
    m9 = layers.Concatenate(axis=3)([up9, c1])
    c9 = conv2d_block_3layers(m9, n_filters * 1, kernel_size = 3, batchnorm = batchnorm, activation=activation)

    outputlayer = tf.keras.layers.Conv2D(filters=nClasses, kernel_size=1, activation="softmax")(c9)
    
    model = tf.keras.Model(inputs=img_input, outputs=outputlayer)
    model.summary(line_length=124)
    return model
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='ehazrati' date='2020-09-21T16:10:44Z'>
		Was able to reproduce the issue with &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/3043b00079794220298cb59fb238ba75/43207.ipynb&gt;TF v2.3&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/806ca7fbab4575784d6459cab4ee5fff/43207-tf-nightly.ipynb&gt;TF-nightly&lt;/denchmark-link&gt;
. Please find the attached gist. Thanks!
		</comment>
		<comment id='11' author='ehazrati' date='2020-09-21T17:42:25Z'>
		&lt;denchmark-link:https://github.com/ehazrati&gt;@ehazrati&lt;/denchmark-link&gt;
 I saw your model and I think that what is missing is that you are using the  activation in your conv. This is different from the official colab that you have mentioned in this ticket cause there the  activation is in a Dense layer:
 outputlayer = tf.keras.layers.Conv2D(filters=nClasses, kernel_size=1, activation="softmax")(c9)
		</comment>
		<comment id='12' author='ehazrati' date='2020-09-22T09:13:38Z'>
		Clear, thanks for the support and clarification.
		</comment>
		<comment id='13' author='ehazrati' date='2020-09-25T05:43:34Z'>
		Hi &lt;denchmark-link:https://github.com/njeffrie&gt;@njeffrie&lt;/denchmark-link&gt;
,
Seems the problem is almost resolved, but the initial error message seems weird. Can you take a look?
RuntimeError: Quantization not yet supported for op: %
		</comment>
		<comment id='14' author='ehazrati' date='2020-09-25T10:39:08Z'>
		&lt;denchmark-link:https://github.com/teijeong&gt;@teijeong&lt;/denchmark-link&gt;
 Just a note, the original message was on TF 2.3 only.
I don't know If you want to improve the nightly error message.
		</comment>
		<comment id='15' author='ehazrati' date='2020-09-28T01:48:13Z'>
		Ah I see, thanks!
Hi &lt;denchmark-link:https://github.com/ehazrati&gt;@ehazrati&lt;/denchmark-link&gt;
, could you close this issue if it's resolved?
		</comment>
		<comment id='16' author='ehazrati' date='2020-09-28T08:09:35Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43207&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43207&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>