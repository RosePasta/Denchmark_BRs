<bug id='32769' author='BoysFight' open_date='2019-09-24T09:10:33Z' closed_time='2019-10-21T23:12:30Z'>
	<summary>model.fit() raise FailedPreconditionError after applying "tf.distribute.MirroredStrategy()"</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):NO
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A
TensorFlow installed from (source or binary):binary
TensorFlow version (use command below):tf1.14/tf2.0.0rc1
Python version:3.7.4
Bazel version (if compiling from source):N/A
GCC/Compiler version (if compiling from source):N/A
CUDA/cuDNN version: 9.0/7.6.0(tf1.14.0) , 10.0.130/7.6.0(tf2.0.0rc1)
GPU model and memory: 8G*2

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
I create and complie the keras model under with mirrored_strategy.scope(): ï¼Œthen i got "FailedPreconditionError " when i excute model.fit();
if i move the code out from with mirrored_strategy.scope():, everything is ok.
Describe the expected behavior
when I create and complie the keras model under with mirrored_strategy.scope(): , do not raise the exception and  multi GPU  can come into use.
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
&lt;denchmark-code&gt;mirrored_strategy = tf.distribute.MirroredStrategy()
with mirrored_strategy.scope():
    # Create the base model from the pre-trained model MobileNet V2
    base_model = tf.keras.applications.InceptionV3(input_shape=IMG_SHAPE,
                                                   include_top=False,
                                                   weights='imagenet')
    base_model.trainable = False
    model = tf.keras.Sequential([
      base_model,
      keras.layers.GlobalAveragePooling2D(),
      keras.layers.Dense(len(label_names), activation='sigmoid')
    ])

    model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),
                  loss='binary_crossentropy',
                  metrics=['categorical_accuracy'])

callbacks = [
    tf.keras.callbacks.TensorBoard(log_dir=log_dir),
    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,
                                       save_weight_only=True)
]

epochs = 10
history = model.fit(train_dataset, 
                    epochs=epochs,
                    callbacks=callbacks)
&lt;/denchmark-code&gt;

Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

FailedPreconditionError                   Traceback (most recent call last)
 in 
9                               validation_data=validation_ds,
10                               validation_steps=validation_steps,
---&gt; 11                               callbacks=callbacks)
/usr/local/miniconda3/envs/tf_2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
726         max_queue_size=max_queue_size,
727         workers=workers,
--&gt; 728         use_multiprocessing=use_multiprocessing)
729
730   def evaluate(self,
/usr/local/miniconda3/envs/tf_2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
683         validation_steps=validation_steps,
684         validation_freq=validation_freq,
--&gt; 685         steps_name='steps_per_epoch')
686
687   def evaluate(self,
/usr/local/miniconda3/envs/tf_2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)
297           else:
298             actual_inputs = ins()
--&gt; 299           batch_outs = f(actual_inputs)
300         except errors.OutOfRangeError:
301           if is_dataset:
/usr/local/miniconda3/envs/tf_2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py in call(self, inputs)
3465
3466     fetched = self._callable_fn(*array_vals,
-&gt; 3467                                 run_metadata=self.run_metadata)
3468     self._call_fetch_callbacks(fetched[-len(self._fetches):])
3469     output_structure = nest.pack_sequence_as(
/usr/local/miniconda3/envs/tf_2.0/lib/python3.7/site-packages/tensorflow_core/python/client/session.py in call(self, *args, **kwargs)
1470         ret = tf_session.TF_SessionRunCallable(self._session._session,
1471                                                self._handle, args,
-&gt; 1472                                                run_metadata_ptr)
1473         if run_metadata:
1474           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)
FailedPreconditionError: 2 root error(s) found.
(0) Failed precondition: Error while reading resource variable conv2d_64/kernel from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/conv2d_64/kernel/N10tensorflow3VarE does not exist.
[[{{node inception_v3_1/conv2d_64/Conv2D/ReadVariableOp}}]]
[[batch_normalization_42/Const/_189]]
(1) Failed precondition: Error while reading resource variable conv2d_64/kernel from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/conv2d_64/kernel/N10tensorflow3VarE does not exist.
[[{{node inception_v3_1/conv2d_64/Conv2D/ReadVariableOp}}]]
0 successful operations.
1 derived errors ignored.
	</description>
	<comments>
		<comment id='1' author='BoysFight' date='2019-09-26T23:09:29Z'>
		&lt;denchmark-link:https://github.com/BoysFight&gt;@BoysFight&lt;/denchmark-link&gt;
 Can you provide a standalone code to reproduce the issue? Thanks!
		</comment>
		<comment id='2' author='BoysFight' date='2019-09-27T09:09:09Z'>
		+1. Complete code to repro would be helpful in debugging this further.
The stack trace above is confusing because it seems like it is from TF 2.0, but also seems to be using TF session which is only a 1.x thing. Can you confirm if the stack trace if from 2.0 rc1?
		</comment>
		<comment id='3' author='BoysFight' date='2019-10-12T12:25:44Z'>
		It has been 15 days with no activity and the awaiting response label was assigned. Is this still an issue?
		</comment>
		<comment id='4' author='BoysFight' date='2019-10-14T16:31:59Z'>
		&lt;denchmark-link:https://github.com/BoysFight&gt;@BoysFight&lt;/denchmark-link&gt;
 Just a reminder for a standalone code to reproduce the issue. Thanks!
		</comment>
		<comment id='5' author='BoysFight' date='2019-10-21T23:12:29Z'>
		Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
		<comment id='6' author='BoysFight' date='2019-10-21T23:12:31Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32769&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32769&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>