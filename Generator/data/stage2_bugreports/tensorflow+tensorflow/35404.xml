<bug id='35404' author='x7night' open_date='2019-12-25T08:29:13Z' closed_time='2020-04-12T20:02:56Z'>
	<summary>ValueError: Flattening a PerReplica to components is not supported in replica context.</summary>
	<description>
System information

windows
TensorFlow installed from conda
TensorFlow version:2.0
Python version:3.7.4
CUDA/cuDNN version: 10.2/7.6
GPU model and memory: 2 nvidia rtx 2070s 8GB

Describe the current behavior
i follow the distributed training tutorials to change my code(custom model) for cumtom training loop. but when i run the script, it shows mistake "ValueError: Flattening a PerReplica to components is not supported in replica context.". i don't understande why it is happens.  it can run in the train step,  but it can't run in the test step
Describe the expected behavior
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
&lt;denchmark-code&gt;# i use this strategy before,but it make the same mistake
# strategy = tf.distribute.MirroredStrategy(
#     cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())

strategy = tf.distribute.MirroredStrategy(
        cross_device_ops=tf.distribute.ReductionToOneDevice("/device:CPU:0"))

def train_step_fn(rgb, spec):
            with tf.GradientTape() as tape:
                fake_spec = model(rgb, training=True)
                loss = compute_loss(spec, fake_spec)

            gradients = tape.gradient(loss, model.trainable_variables)
            opt.apply_gradients(zip(gradients, model.trainable_variables))
            # update metrics
            rmse1.update_state(spec, fake_spec)
            rmse2.update_state(spec, fake_spec)
            rrmse1.update_state(spec, fake_spec)
            rrmse2.update_state(spec, fake_spec)
            sam.update_state(spec, fake_spec)
            return loss

def test_step_fn(rgb, sepc):
            fake_spec = model(rgb, training=False)
            loss = loss_object(spec, fake_spec)
            # update metrics
            rmse1.update_state(spec, fake_spec)
            rmse2.update_state(spec, fake_spec)
            rrmse1.update_state(spec, fake_spec)
            rrmse2.update_state(spec, fake_spec)
            sam.update_state(spec, fake_spec)
            return loss

@tf.function
def distributed_train_step(rgb, spec):
            per_replica_losses = strategy.experimental_run_v2(train_step_fn,
                                                              args=(rgb, spec))
            return strategy.reduce(tf.distribute.ReduceOp.SUM,
                                   per_replica_losses,
                                   axis=None)

@tf.function
def distributed_test_step(rgb, spec):
            return strategy.experimental_run_v2(test_step_fn, args=(rgb, spec))

for epoch in range(parser.epochs):
            # train
            for step, (rgb, spec) in enumerate(datas[0]):
                train_mean_loss = distributed_train_step(rgb, spec)
                steps += 1
                ckpt.steps.assign(steps)
                if step == 50:
                    break
            # val
             rmse1.reset_state()
             rmse2.reset_state()
             rrmse1.reset_state()
             rrmse2.reset_state()
             sam.reset_state()
            for step, (rgb, spec) in enumerate(datas[1]):

                test_mean_loss = distributed_test_step(rgb, spec)
&lt;/denchmark-code&gt;

Other info / logs
&lt;denchmark-code&gt;ValueError: in converted code:
    mytrainT.py:163 distributed_test_step  *
        return strategy.experimental_run_v2(test_step_fn, args=(rgb, spec))
    C:\Users\zhangstation\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\distribute\distribute_lib.py:760 experimental_run_v2
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    mytrainT.py:144 test_step_fn  *
        loss = loss_object(spec, fake_spec)
    C:\Users\zhangstation\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\keras\losses.py:125 __call__
        with K.name_scope(scope_name or self.__class__.__name__), graph_ctx:
    C:\Users\zhangstation\Anaconda3\envs\tf\lib\contextlib.py:112 __enter__
        return next(self.gen)
    C:\Users\zhangstation\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\keras\utils\tf_utils.py:435 graph_context_for_symbolic_tensors
        if any(is_symbolic_tensor(v) for v in list(args) + list(kwargs.values())):
    C:\Users\zhangstation\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\keras\utils\tf_utils.py:435 &lt;genexpr&gt;
        if any(is_symbolic_tensor(v) for v in list(args) + list(kwargs.values())):
    C:\Users\zhangstation\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\keras\utils\tf_utils.py:345 is_symbolic_tensor
        return tensor._is_graph_tensor  # pylint: disable=protected-access
    C:\Users\zhangstation\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\framework\composite_tensor.py:119 _is_graph_tensor
        components = self._type_spec._to_components(self)  # pylint: disable=protected-access
    C:\Users\zhangstation\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\distribute\values.py:500 _to_components
        "Flattening a PerReplica to components is not supported in replica "

    ValueError: Flattening a PerReplica to components is not supported in replica context.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='x7night' date='2019-12-26T20:21:10Z'>
		x7night@ The reason the train step works is because it uses the compute_loss function and the test step uses a loss_object. Can you provide details about how you create your loss object? I was not able to reproduce this issue with a simple loss_object that i created.
Also, can you augment your above snippet with where you open the strategy scope, build your model, create losses etc?
		</comment>
		<comment id='2' author='x7night' date='2020-01-16T03:24:23Z'>
		&lt;denchmark-link:https://github.com/anj-s&gt;@anj-s&lt;/denchmark-link&gt;

here is my code. i follow the tutorials(&lt;denchmark-link:https://tensorflow.google.cn/tutorials/distribute/custom_training&gt;https://tensorflow.google.cn/tutorials/distribute/custom_training&lt;/denchmark-link&gt;
) to change my code
&lt;denchmark-code&gt;    strategy = tf.distribute.MirroredStrategy(
        cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())
    print('replicas nums:', strategy.num_replicas_in_sync)
    # Global batch size
    global_batch_size = parser.batchSize
    batch_size_per_replica = int(global_batch_size /
                                 strategy.num_replicas_in_sync)
    print('per repilca size:', batch_size_per_replica)
    # move to GPU
    with strategy.scope():
        # prepare
        def getData(data, data_path):
            data[0], data[1] = utils.getDataset_(data_path,
                                                 split=0.9,
                                                 shuffle=128,
                                                 batch=global_batch_size)
            data[0] = strategy.experimental_distribute_dataset(data[0])
            data[1] = strategy.experimental_distribute_dataset(data[1])
            return data

        datas = [0, 1]
        datas = getData(datas, data_path)

        # build model
        print("\nbuilding models ...\n")
        model = Model()

        # optimizers
        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
            initial_learning_rate=parser.lr,
            decay_steps=20000,
            decay_rate=0.9,
            staircase=True)
        opt = tf.optimizers.Adam(learning_rate=lr_schedule, decay=1e-6)

        # writer = SummaryWriter(opt.outf)
        file_writer = tf.summary.create_file_writer(parser.out)
        # tf.summary.flush()
        file_writer.set_as_default()

        ckpt = tf.train.Checkpoint(steps=tf.Variable(1),
                                   net=model,
                                   optimizer=opt)
        manager = tf.train.CheckpointManager(ckpt,
                                             './tf_ckpts',
                                             max_to_keep=15)

        rmse1 = modules.RMSE1()
        rmse2 = modules.RMSE2()
        rrmse1 = modules.rRMSE1()
        rrmse2 = modules.rRMSE2()
        sam = modules.SAM()

        print('start train\n')
        steps = 0
        start = datetime.datetime.now()
        old_loss = 100.
        cur_loss = 100.

        loss_object = tf.keras.losses.MeanSquaredError(
            reduction=tf.keras.losses.Reduction.NONE)

        def compute_loss(labels, predictions):
            per_example_loss = loss_object(labels, predictions)
            return tf.nn.compute_average_loss(
                per_example_loss, global_batch_size=global_batch_size)

        def train_step_fn(rgb, spec):
            with tf.GradientTape() as tape:
                fake_spec = model(rgb, training=True)
                loss = compute_loss(spec, fake_spec)

            gradients = tape.gradient(loss, model.trainable_variables)
            opt.apply_gradients(zip(gradients, model.trainable_variables))
            # update metrics
            rmse1.update_state(spec, fake_spec)
            rmse2.update_state(spec, fake_spec)
            rrmse1.update_state(spec, fake_spec)
            rrmse2.update_state(spec, fake_spec)
            sam.update_state(spec, fake_spec)
            return loss

        def test_step_fn(rgb, sepc):
            fake_spec = model(rgb)
            loss = loss_object(spec, fake_spec)
            # update metrics
            rmse1.update_state(spec, fake_spec)
            rmse2.update_state(spec, fake_spec)
            rrmse1.update_state(spec, fake_spec)
            rrmse2.update_state(spec, fake_spec)
            sam.update_state(spec, fake_spec)
            return loss

        @tf.function
        def distributed_train_step(rgb, spec):
            per_replica_losses = strategy.experimental_run_v2(train_step_fn,
                                                              args=(rgb, spec))
            return strategy.reduce(tf.distribute.ReduceOp.SUM,
                                   per_replica_losses,
                                   axis=None)

        @tf.function
        def distributed_test_step(rgb, spec):
            return strategy.experimental_run_v2(test_step_fn, args=(rgb, spec))

        for epoch in range(parser.epochs):
            # pre
            status = 'train'
            # progress bar
            widgets = [
                progressbar.Variable('status'),
                ' ',
                progressbar.Variable('epoch', width=2),
                ' ',
                progressbar.Variable('step', width=4),
                ' ',
                progressbar.Timer(),
                ' ',
                progressbar.Variable('rmse1', precision=4),
                ' ',
                progressbar.Variable('rmse2', precision=4),
                ' ',
                progressbar.Variable('rrmse1', precision=4),
                ' ',
                progressbar.Variable('rrmse2', precision=4),
                ' ',
                progressbar.Variable('sam', precision=4),
            ]
            bar = progressbar.ProgressBar(widgets=widgets).start()
            # train
            for step, (rgb, spec) in enumerate(datas[0]):

                train_mean_loss = distributed_train_step(rgb, spec)

                bar.update(status=status,
                           epoch=epoch,
                           step=step,
                           rmse1=float(rmse1.result()),
                           rmse2=float(rmse2.result()),
                           rrmse1=float(rrmse1.result()),
                           rrmse2=float(rrmse2.result()),
                           sam=float(sam.result()))
                if steps % 10 == 0:
                    with file_writer.as_default():
                        tf.summary.scalar('loss',
                                          float(train_mean_loss),
                                          step=steps)
                        tf.summary.scalar('rmse1',
                                          float(rmse1.result()),
                                          step=steps)
                        tf.summary.scalar('rmse2',
                                          float(rmse2.result()),
                                          step=steps)
                        tf.summary.scalar('rrmse1',
                                          float(rrmse1.result()),
                                          step=steps)
                        tf.summary.scalar('rrmse2',
                                          float(rrmse2.result()),
                                          step=steps)
                        tf.summary.scalar('sam',
                                          float(sam.result()),
                                          step=steps)
                steps += 1
                ckpt.steps.assign(steps)
                if step == 50:
                    break
            # val
            status = 'val'
            rmse1.reset_state()
            rmse2.reset_state()
            rrmse1.reset_state()
            rrmse2.reset_state()
            sam.reset_state()
            for step, (rgb, spec) in enumerate(datas[1]):

                test_mean_loss = distributed_test_step(rgb, spec)

                bar.update(status=status,
                           epoch=epoch,
                           step=step,
                           rmse1=float(rmse1.result()),
                           rmse2=float(rmse2.result()),
                           rrmse1=float(rrmse1.result()),
                           rrmse2=float(rrmse2.result()),
                           sam=float(sam.result()))

            with file_writer.as_default():
                tf.summary.scalar('val_rmse1',
                                  float(rmse1.result()),
                                  step=epoch)
                tf.summary.scalar('val_rmse2',
                                  float(rmse2.result()),
                                  step=epoch)
                tf.summary.scalar('val_rrmse1',
                                  float(rrmse1.result()),
                                  step=epoch)
                tf.summary.scalar('val_rrmse2',
                                  float(rrmse2.result()),
                                  step=epoch)
                tf.summary.scalar('val_sam', float(sam.result()), step=epoch)

            # post
            bar.finish()
            rmse1.reset_state()
            rmse2.reset_state()
            rrmse1.reset_state()
            rrmse2.reset_state()
            sam.reset_state()
            datas = getData(datas, data_path)
            # save model
            save_path = manager.save()
&lt;/denchmark-code&gt;

the error logs:
&lt;denchmark-code&gt;raceback (most recent call last):
  File "mytrainT.py", line 310, in &lt;module&gt;
    main(des)
  File "mytrainT.py", line 235, in main
    test_mean_loss = distributed_test_step(rgb, spec)
  File "C:\Users\zhangstation\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\eager\def_function.py", line 457, in __call__
    result = self._call(*args, **kwds)
  File "C:\Users\zhangstation\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\eager\def_function.py", line 503, in _call
    self._initialize(args, kwds, add_initializers_to=initializer_map)
  File "C:\Users\zhangstation\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\eager\def_function.py", line 408, in _initialize
    *args, **kwds))
  File "C:\Users\zhangstation\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\eager\function.py", line 1848, in _get_concrete_function_internal_garba
ge_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File "C:\Users\zhangstation\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\eager\function.py", line 2150, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File "C:\Users\zhangstation\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\eager\function.py", line 2041, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File "C:\Users\zhangstation\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\framework\func_graph.py", line 915, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "C:\Users\zhangstation\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\eager\def_function.py", line 358, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File "C:\Users\zhangstation\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\framework\func_graph.py", line 905, in wrapper
    raise e.ag_error_metadata.to_exception(e)
ValueError: in converted code:

    mytrainT.py:163 distributed_test_step  *
        return strategy.experimental_run_v2(test_step_fn, args=(rgb, spec))
    C:\Users\zhangstation\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\distribute\distribute_lib.py:760 experimental_run_v2
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    mytrainT.py:144 test_step_fn  *
        loss = loss_object(spec, fake_spec)
    C:\Users\zhangstation\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\keras\losses.py:125 __call__
        with K.name_scope(scope_name or self.__class__.__name__), graph_ctx:
    C:\Users\zhangstation\Anaconda3\envs\tf\lib\contextlib.py:112 __enter__
        return next(self.gen)
    C:\Users\zhangstation\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\keras\utils\tf_utils.py:435 graph_context_for_symbolic_tensors
        if any(is_symbolic_tensor(v) for v in list(args) + list(kwargs.values())):
    C:\Users\zhangstation\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\keras\utils\tf_utils.py:435 &lt;genexpr&gt;
        if any(is_symbolic_tensor(v) for v in list(args) + list(kwargs.values())):
    C:\Users\zhangstation\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\keras\utils\tf_utils.py:345 is_symbolic_tensor
        return tensor._is_graph_tensor  # pylint: disable=protected-access
    C:\Users\zhangstation\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\framework\composite_tensor.py:119 _is_graph_tensor
        components = self._type_spec._to_components(self)  # pylint: disable=protected-access
    C:\Users\zhangstation\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\distribute\values.py:500 _to_components
        "Flattening a PerReplica to components is not supported in replica "

    ValueError: Flattening a PerReplica to components is not supported in replica context.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='x7night' date='2020-02-03T10:00:30Z'>
		Hey, did you figure something about this error? I've got the same with a very similar code.
		</comment>
		<comment id='4' author='x7night' date='2020-04-12T20:02:56Z'>
		&lt;denchmark-link:https://github.com/x7night&gt;@x7night&lt;/denchmark-link&gt;
 there is a small type in the . So i think it is using  from the  loop later which has  values (because the loop variables are in the global scope unfortunately). I think this should get fixed if you fix the typo in .
Please re-open if this is not the case.
		</comment>
		<comment id='5' author='x7night' date='2020-04-12T20:02:58Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35404&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35404&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>