<bug id='34944' author='UdiBhaskar' open_date='2019-12-08T17:25:31Z' closed_time='2020-02-08T03:12:36Z'>
	<summary>SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [&amp;lt;tf.Tensor 'keras_learning_phase:0' shape=() dtype=bool&amp;gt;]</summary>
	<description>
Hi,
I am writing Encoder-Decoder architecture with Bahdanau Attention using tf.keras with TensorFlow 2.0. Below is my code This is working with TensorFlow 1.15 but getting the error in 2.0. you can check the code in colab notebook &lt;denchmark-link:https://colab.research.google.com/drive/12Vq1t9xOtrPmXV2Sj_GSoE9bxH7FgaKs&gt;here&lt;/denchmark-link&gt;
.
&lt;denchmark-code&gt;import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow.keras.layers import Input, Dense, Conv2D, BatchNormalization, Activation, Dropout, GRU, Embedding
from tensorflow.keras.models import Model
from tensorflow.keras import activations
from tensorflow.keras.layers import Layer
from tensorflow.keras import layers
import tensorflow as tf
from tensorflow.keras.layers import GRU, concatenate, Lambda

ENCODER_SEQ_LEN = 30
DECODER_SEQ_LEN = 20
VOCAB_SIZE = 500
units = 16

tf.keras.backend.clear_session()
class Encoder(Model):
    def __init__(self, vocab_size, embedding_dim, input_length, units):
        super(Encoder, self).__init__()
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.input_length = input_length
        self.units = units
        self.embedding = Embedding(input_dim=VOCAB_SIZE, output_dim=50, input_length=self.input_length,
                           mask_zero=False, name="embedding_layer_encoder")
        self.gru = GRU(self.units, return_state=True, return_sequences=True, name="Encoder_GRU")
    @tf.function
    def call(self, inputs, training=True):
        x_embedd = self.embedding(inputs)
        gru_output, gru_state = self.gru(x_embedd)
        return gru_output, gru_state
    
class BahdanauAttention(tf.keras.layers.Layer):
    def __init__(self, units):
        super(BahdanauAttention, self).__init__()
        self.W1 = tf.keras.layers.Dense(units)
        self.W2 = tf.keras.layers.Dense(units)
        self.V = tf.keras.layers.Dense(1)

    def call(self, query, values):
        # hidden shape == (batch_size, hidden size)
        # # hidden_with_time_axis shape == (batch_size, 1, hidden size)
        # # we are doing this to perform addition to calculate the score
        hidden_with_time_axis = tf.expand_dims(query, 1)
        # score shape == (batch_size, max_length, 1)
        # # we get 1 at the last axis because we are applying score to self.V
        # # the shape of the tensor before applying self.V is (batch_size, max_length, units)
        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))
        # attention_weights shape == (batch_size, max_length, 1)
        attention_weights = tf.nn.softmax(score, axis=1)
        # context_vector shape after sum == (batch_size, hidden_size)
        context_vector = attention_weights * values
        context_vector = tf.reduce_sum(context_vector, axis=1)
        return context_vector

class onestepDecoder(Model):
    def __init__(self, vocab_size, embedding_dim, dec_units, att_units):
        super(onestepDecoder, self).__init__()
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.dec_units = dec_units
        self.att_units = att_units
        self.embedd = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim,
                      input_length=1, mask_zero=False, name="Decoder_Embedding_layer")
        self.att_layer = BahdanauAttention(units=self.att_units) #name='Attention')
        self.dense = Dense(self.vocab_size, activation="softmax", name="DenseOut")
        self.gru = GRU(units=self.dec_units, return_state=True, name="DecGRU")
    @tf.function
    def call(self, input_decoder, input_state, encoder_outputs, training=True):
        x_embedd = self.embedd(input_decoder)
        context_vector = self.att_layer(input_state, encoder_outputs )
        concat = tf.concat([tf.expand_dims(context_vector, 1), x_embedd], axis=-1)
        decoder_output, Decoder_state = self.gru(concat, initial_state=input_state)
        output = self.dense(decoder_output)
        return (output, Decoder_state)
class Decoder(Model):
    def __init__(self, vocab_size, embedding_dim, dec_units, att_units):
        super(Decoder, self).__init__()
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.dec_units = dec_units
        self.att_units = att_units
        self.stepdec = onestepDecoder(self.vocab_size, self.embedding_dim, self.dec_units, self.att_units)
    @tf.function
    def call(self, input_decoder, input_state, encoder_outputs):
        all_outputs= tf.TensorArray(tf.float32, size=input_decoder.shape[1], name="output_arrays")
        for timestep in range(input_decoder.shape[1]):
            output, input_state = self.stepdec(input_decoder[:,timestep:timestep+1], input_state, encoder_outputs)
            all_outputs = all_outputs.write(timestep, output)
        all_outputs = tf.transpose(all_outputs.stack(), [1, 0, 2])
        return all_outputs

encoder_input = Input(shape=(ENCODER_SEQ_LEN,), name='encoder_input_final')
decoder_input = Input(shape=(DECODER_SEQ_LEN,), name="Decoder_inout_final")
encoder = Encoder(vocab_size=VOCAB_SIZE, embedding_dim=50, input_length=ENCODER_SEQ_LEN, units=16)
x_gru_out, x_gru_state = encoder(encoder_input)
decoder = Decoder(vocab_size=VOCAB_SIZE, embedding_dim=50, dec_units=16, att_units=20)
all_outputs = decoder(decoder_input, x_gru_state, x_gru_out)
encoder_decoder = Model([encoder_input, decoder_input], outputs=all_outputs)
encoder_decoder.compile(optimizer='adam',loss='sparse_categorical_crossentropy')

x = np.random.randint(0, 499, size=(2000, ENCODER_SEQ_LEN))
y = np.random.randint(0, 499, size=(2000, DECODER_SEQ_LEN))

encoder_decoder.fit(x=[x,y], y=y, epochs=1,verbose=1,batch_size=32)
&lt;/denchmark-code&gt;

Error:
TypeError                                 Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
60                                                op_name, inputs, attrs,
---&gt; 61                                                num_outputs)
62   except core._NotOkStatusException as e:
TypeError: An op outside of the function building code is being passed
a "Graph" tensor. It is possible to have Graph tensors
leak out of the function building context by including a
tf.init_scope in your function building code.
For example, the following function will fail:
@tf.function
def has_init_scope():
my_constant = tf.constant(1.)
with tf.init_scope():
added = my_constant * 2
The graph tensor has name: keras_learning_phase:0
During handling of the above exception, another exception occurred:
_SymbolicException                        Traceback (most recent call last)
11 frames
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
73       raise core._SymbolicException(
74           "Inputs to eager execution function cannot be Keras symbolic "
---&gt; 75           "tensors, but found {}".format(keras_symbolic_tensors))
76     raise e
77   # pylint: enable=protected-access
_SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [&lt;tf.Tensor 'keras_learning_phase:0' shape=() dtype=bool&gt;]
	</description>
	<comments>
		<comment id='1' author='UdiBhaskar' date='2019-12-10T12:00:10Z'>
		Error is self-explainatory. If running under eager mode ,tensorflow op will check if the inputs are of type "tensorflow.python.framework.ops.EagerTensor" and keras ops are implemented as DAGs. So the inputs to the eagermode will be of "tensorflow.python.framework.ops.Tensor" and this throws the error
You can change the input type to EagerTensor by explicity telling tensorflow to run in eager mode for keras.
"tf.config.experimental_run_functions_eagerly(True)"
Adding this statement should solve your issue. Although note that there will be significant performance hits since you are running now in eager mode and recommended only for debugging,profiling etc.
		</comment>
		<comment id='2' author='UdiBhaskar' date='2019-12-10T14:13:55Z'>
		can we change any code to work without "tf.config.experimental_run_functions_eagerly(True)" ? &lt;denchmark-link:https://github.com/Athul8raj&gt;@Athul8raj&lt;/denchmark-link&gt;

and also If i am removing tf.function, i am getting this error.
TypeError                                 Traceback (most recent call last)
D:\Softwares\Anaconda3\envs\dlt2\lib\site-packages\tensorflow_core\python\eager\backprop.py in _num_elements(grad)
615   if isinstance(grad, ops.IndexedSlices):
--&gt; 616     return functools.reduce(operator.mul, grad.values._shape_tuple(), 1)  # pylint: disable=protected-access
617   raise ValueError("grad not a Tensor or IndexedSlices.")
TypeError: unsupported operand type(s) for *: 'int' and 'NoneType'
The above exception was the direct cause of the following exception:
SystemError                               Traceback (most recent call last)
 in 
----&gt; 1 encoder_decoder.fit(x=[x,y], y=y, epochs=1,verbose=1,batch_size=32)
D:\Softwares\Anaconda3\envs\dlt2\lib\site-packages\tensorflow_core\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
726         max_queue_size=max_queue_size,
727         workers=workers,
--&gt; 728         use_multiprocessing=use_multiprocessing)
729
730   def evaluate(self,
D:\Softwares\Anaconda3\envs\dlt2\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
322                 mode=ModeKeys.TRAIN,
323                 training_context=training_context,
--&gt; 324                 total_epochs=epochs)
325             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)
326
D:\Softwares\Anaconda3\envs\dlt2\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)
121         step=step, mode=mode, size=current_batch_size) as batch_logs:
122       try:
--&gt; 123         batch_outs = execution_function(iterator)
124       except (StopIteration, errors.OutOfRangeError):
125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?
D:\Softwares\Anaconda3\envs\dlt2\lib\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py in execution_function(input_fn)
84     # numpy translates Tensors to values in Eager mode.
85     return nest.map_structure(_non_none_constant_value,
---&gt; 86                               distributed_function(input_fn))
87
88   return execution_function
D:\Softwares\Anaconda3\envs\dlt2\lib\site-packages\tensorflow_core\python\eager\def_function.py in call(self, *args, **kwds)
455
456     tracing_count = self._get_tracing_count()
--&gt; 457     result = self._call(*args, **kwds)
458     if tracing_count == self._get_tracing_count():
459       self._call_counter.called_without_tracing()
D:\Softwares\Anaconda3\envs\dlt2\lib\site-packages\tensorflow_core\python\eager\def_function.py in _call(self, *args, **kwds)
501       # This is the first call of call, so we have to initialize.
502       initializer_map = object_identity.ObjectIdentityDictionary()
--&gt; 503       self._initialize(args, kwds, add_initializers_to=initializer_map)
504     finally:
505       # At this point we know that the initialization is complete (or less
D:\Softwares\Anaconda3\envs\dlt2\lib\site-packages\tensorflow_core\python\eager\def_function.py in _initialize(self, args, kwds, add_initializers_to)
406     self._concrete_stateful_fn = (
407         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--&gt; 408             *args, **kwds))
409
410     def invalid_creator_scope(*unused_args, **unused_kwds):
D:\Softwares\Anaconda3\envs\dlt2\lib\site-packages\tensorflow_core\python\eager\function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
1846     if self.input_signature:
1847       args, kwargs = None, None
-&gt; 1848     graph_function, _, _ = self._maybe_define_function(args, kwargs)
1849     return graph_function
1850
D:\Softwares\Anaconda3\envs\dlt2\lib\site-packages\tensorflow_core\python\eager\function.py in _maybe_define_function(self, args, kwargs)
2148         graph_function = self._function_cache.primary.get(cache_key, None)
2149         if graph_function is None:
-&gt; 2150           graph_function = self._create_graph_function(args, kwargs)
2151           self._function_cache.primary[cache_key] = graph_function
2152         return graph_function, args, kwargs
D:\Softwares\Anaconda3\envs\dlt2\lib\site-packages\tensorflow_core\python\eager\function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
2039             arg_names=arg_names,
2040             override_flat_arg_shapes=override_flat_arg_shapes,
-&gt; 2041             capture_by_value=self._capture_by_value),
2042         self._function_attributes,
2043         # Tell the ConcreteFunction to clean up its graph once it goes out of
D:\Softwares\Anaconda3\envs\dlt2\lib\site-packages\tensorflow_core\python\framework\func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
913                                           converted_func)
914
--&gt; 915       func_outputs = python_func(*func_args, **func_kwargs)
916
917       # invariant: func_outputs contains only Tensors, CompositeTensors,
D:\Softwares\Anaconda3\envs\dlt2\lib\site-packages\tensorflow_core\python\eager\def_function.py in wrapped_fn(*args, **kwds)
356         # wrapped allows AutoGraph to swap in a converted function. We give
357         # the function a weak reference to itself to avoid a reference cycle.
--&gt; 358         return weak_wrapped_fn().wrapped(*args, **kwds)
359     weak_wrapped_fn = weakref.ref(wrapped_fn)
360
D:\Softwares\Anaconda3\envs\dlt2\lib\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py in distributed_function(input_iterator)
71     strategy = distribution_strategy_context.get_strategy()
72     outputs = strategy.experimental_run_v2(
---&gt; 73         per_replica_function, args=(model, x, y, sample_weights))
74     # Out of PerReplica outputs reduce or pick values to return.
75     all_outputs = dist_utils.unwrap_output_dict(
D:\Softwares\Anaconda3\envs\dlt2\lib\site-packages\tensorflow_core\python\distribute\distribute_lib.py in experimental_run_v2(self, fn, args, kwargs)
758       fn = autograph.tf_convert(fn, ag_ctx.control_status_ctx(),
759                                 convert_by_default=False)
--&gt; 760       return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
761
762   def reduce(self, reduce_op, value, axis):
D:\Softwares\Anaconda3\envs\dlt2\lib\site-packages\tensorflow_core\python\distribute\distribute_lib.py in call_for_each_replica(self, fn, args, kwargs)
1785       kwargs = {}
1786     with self._container_strategy().scope():
-&gt; 1787       return self._call_for_each_replica(fn, args, kwargs)
1788
1789   def _call_for_each_replica(self, fn, args, kwargs):
D:\Softwares\Anaconda3\envs\dlt2\lib\site-packages\tensorflow_core\python\distribute\distribute_lib.py in _call_for_each_replica(self, fn, args, kwargs)
2130         self._container_strategy(),
2131         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):
-&gt; 2132       return fn(*args, **kwargs)
2133
2134   def _reduce_to(self, reduce_op, value, destinations):
D:\Softwares\Anaconda3\envs\dlt2\lib\site-packages\tensorflow_core\python\autograph\impl\api.py in wrapper(*args, **kwargs)
290   def wrapper(*args, **kwargs):
291     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
--&gt; 292       return func(*args, **kwargs)
293
294   if inspect.isfunction(func) or inspect.ismethod(func):
D:\Softwares\Anaconda3\envs\dlt2\lib\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py in train_on_batch(model, x, y, sample_weight, class_weight, reset_metrics)
262       y,
263       sample_weights=sample_weights,
--&gt; 264       output_loss_metrics=model._output_loss_metrics)
265
266   if reset_metrics:
D:\Softwares\Anaconda3\envs\dlt2\lib\site-packages\tensorflow_core\python\keras\engine\training_eager.py in train_on_batch(model, inputs, targets, sample_weights, output_loss_metrics)
309           sample_weights=sample_weights,
310           training=True,
--&gt; 311           output_loss_metrics=output_loss_metrics))
312   if not isinstance(outs, list):
313     outs = [outs]
D:\Softwares\Anaconda3\envs\dlt2\lib\site-packages\tensorflow_core\python\keras\engine\training_eager.py in _process_single_batch(model, inputs, targets, output_loss_metrics, sample_weights, training)
266           model._backwards(tape, scaled_total_loss)
267         else:
--&gt; 268           grads = tape.gradient(scaled_total_loss, trainable_weights)
269           if isinstance(model.optimizer,
270                         loss_scale_optimizer.LossScaleOptimizer):
D:\Softwares\Anaconda3\envs\dlt2\lib\site-packages\tensorflow_core\python\eager\backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)
1012         output_gradients=output_gradients,
1013         sources_raw=flat_sources_raw,
-&gt; 1014         unconnected_gradients=unconnected_gradients)
1015
1016     if not self._persistent:
D:\Softwares\Anaconda3\envs\dlt2\lib\site-packages\tensorflow_core\python\eager\imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)
74       output_gradients,
75       sources_raw,
---&gt; 76       compat.as_str(unconnected_gradients.value))
D:\Softwares\Anaconda3\envs\dlt2\lib\site-packages\tensorflow_core\python\eager\backprop.py in _aggregate_grads(gradients)
596   assert gradients, "No gradients to aggregate"
597
--&gt; 598   if len(gradients) == 1:
599     return gradients[0]
600   if all(isinstance(g, ops.Tensor) for g in gradients):
SystemError:  returned a result with an error set
		</comment>
		<comment id='3' author='UdiBhaskar' date='2019-12-10T16:09:17Z'>
		For the first part of your question, yes it is possible to run in Eagermode without that statement, but instead of the fit function from keras you have to calculate gradients with the help of a gradient tape which will take care of loss gradient and track all the trainable variables
For the second part, I am not sure why you are getting the error, because I am not facing that .
Can you specify the TF version you are running and also the OS Specs.
Actually you should include these details in the subject of this issue. That should be the way tensorflow issues should be raised
The template shown below:
"System information
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): v1.12.0-9492-g2c319fb415 2.0.0-alpha0
Python version: 3.6.8 |Anaconda, Inc.| (default, Feb 21 2019, 18:30:04) [MSC v.1916 64 bit (AMD64)]
Bazel version (if compiling from source): NA
GCC/Compiler version (if compiling from source): NA
CUDA/cuDNN version: NA
GPU model and memory: NA
You can collect some of this information using our environment capture
script
You can also obtain the TensorFlow version with: 1. TF 1.0: python -c "import tensorflow as tf print(tf.GIT_VERSION, tf.VERSION)" 2. TF 2.0: python -c "import tensorflow as tf print(tf.version.GIT_VERSION, tf.version.VERSION)"
Describe the current behavior
The code snippets works fine on Colab but gives the following error on Windows:"
		</comment>
		<comment id='4' author='UdiBhaskar' date='2019-12-10T16:40:36Z'>
		&lt;denchmark-link:https://github.com/Athul8raj&gt;@Athul8raj&lt;/denchmark-link&gt;
 Below is my system information.
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
TensorFlow installed from (source or binary): Binary
TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca 2.0.0
Python version: Python 3.7.3
Bazel version (if compiling from source): NA
GCC/Compiler version (if compiling from source): NA
CUDA/cuDNN version: CUDA toolkit 10.0.130
GPU model and memory: GTX 1050 Ti

Describe the current behavior
The above code snippet with/without @tf.function is not working in colab as well. I am getting the same error.
		</comment>
		<comment id='5' author='UdiBhaskar' date='2019-12-11T05:48:11Z'>
		Can you provide a collab gist?.
		</comment>
		<comment id='6' author='UdiBhaskar' date='2019-12-11T06:57:16Z'>
		below are the two links, please check
&lt;denchmark-link:https://gist.github.com/UdiBhaskar/d7492b7be034db1480db717ceb6e767e&gt;error code without tf.function&lt;/denchmark-link&gt;
.
&lt;denchmark-link:https://gist.github.com/UdiBhaskar/613e671d8785538b1380a7f3909899ed&gt;Error code with tf.function&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='UdiBhaskar' date='2019-12-11T10:49:23Z'>
		If you are not providing "tf.config.experimental_run_functions_eagerly(True)". then you have to give the model to gradient tape as shown in here
&lt;denchmark-link:https://www.tensorflow.org/tutorials/text/nmt_with_attention&gt;https://www.tensorflow.org/tutorials/text/nmt_with_attention&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='UdiBhaskar' date='2019-12-11T11:00:40Z'>
		I saw documentation for tf.config.experimental_run_functions_eagerly(True) and it was saying useful while debugging and read the functional API specs and in that it was saying that, if a model built by a functional API is compiled without any error, you can use that model to train and this won't give any error.
i understand if i am writing the code with tf.functions, i have to give eager tensor as input not keras but if am removing the tf.function, it has to work right? it is working fine with 1.15 version and in above comment you told, it is working on your system with 2.0 alpha. is there any bug in auto graph with keras .fit?
can you provide any referance for "If you are not providing "tf.config.experimental_run_functions_eagerly(True)". then you have to give the model to gradient tape as shown in here" ? i am not able to find any documentation which says this.
		</comment>
		<comment id='9' author='UdiBhaskar' date='2019-12-11T11:32:16Z'>
		As per TF 2.0 docs,there are two ways to create models, one is through object oriented way and one is using the keras functional api. While there are examples where they have called layer subclass in a keras functional model, there are no instances of models subclasses being called in a keras functional api. This may be the reason you are encountering the issue.
One way to go, if you want to stick with the functional api would be to change the model subclasses to layer subclasses and write a single model function.
For more info please refer : &lt;denchmark-link:https://www.tensorflow.org/guide/keras/custom_layers_and_models#the_model_class&gt;https://www.tensorflow.org/guide/keras/custom_layers_and_models#the_model_class&lt;/denchmark-link&gt;

PS: please forgive my assertion in the previous reply. I didnt mean you have to, its just another way of doing it
		</comment>
		<comment id='10' author='UdiBhaskar' date='2019-12-11T12:23:27Z'>
		&lt;denchmark-link:https://github.com/Athul8raj&gt;@Athul8raj&lt;/denchmark-link&gt;
 I checked with the above cases as well and getting the same error. I am not getting any error if i am using a 1.15 version or below.
case-1 : Total model subclassing API  - Got same error. you can check &lt;denchmark-link:https://gist.github.com/UdiBhaskar/8f96a48dbe3afe9b969b4d46e835a93b&gt;here&lt;/denchmark-link&gt;

case-2 : Custom layers(not model subclassing) and functional API - Got same error. You can check &lt;denchmark-link:https://gist.github.com/UdiBhaskar/e795757f6b07f70275ab37739d07e411&gt;here&lt;/denchmark-link&gt;

case-3 : Custom layers and model subclassing API - Got same error. You can check &lt;denchmark-link:https://gist.github.com/UdiBhaskar/000af58031c6b95c046a8365df466ed9&gt;here&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='UdiBhaskar' date='2019-12-12T06:13:31Z'>
		You are not getting error in TF 1.15 and below because in tf 2.0 eager execution is enabled by default.
So I thought of disabling the eager execution and your code worked perfectly.
this is the statement to disable eager execution ""
Here are the docs : &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/compat/v1/disable_eager_execution&gt;https://www.tensorflow.org/api_docs/python/tf/compat/v1/disable_eager_execution&lt;/denchmark-link&gt;

Here is the example code with gradient tape and eager execution : &lt;denchmark-link:https://www.tensorflow.org/tutorials/text/nmt_with_attention&gt;https://www.tensorflow.org/tutorials/text/nmt_with_attention&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='UdiBhaskar' date='2019-12-12T07:19:58Z'>
		I got that if I disable my eager execution, I can run my model without error or if I use gradient tape then also I can run the model without error.  but I will get the error if I use .fit method with my Keras model but it has to work even in TensorFlow 2.0 right. do you think of any bug in .fit method with the autograph in 2.0 with eager execution?
		</comment>
		<comment id='13' author='UdiBhaskar' date='2019-12-12T07:47:03Z'>
		I don't believe it is a bug rather TF gives us freedom in choosing each method. While we can mix match the layer subclass with keras functional api, I guess we can't make the model subclass work with the Model api of keras. This is where, in my opinion the distinction between eager execution and keras graph mode comes into conflict giving rise to this "SymbolicException".
Making TF aware beforehand what mode it should execute solves it.
		</comment>
		<comment id='14' author='UdiBhaskar' date='2019-12-26T08:52:59Z'>
		
I got that if I disable my eager execution, I can run my model without error or if I use gradient tape then also I can run the model without error. but I will get the error if I use .fit method with my Keras model but it has to work even in TensorFlow 2.0 right. do you think of any bug in .fit method with the autograph in 2.0 with eager execution?

hey~ I encountered the same problem. Inspired by &lt;denchmark-link:https://gist.github.com/UdiBhaskar/e795757f6b07f70275ab37739d07e411&gt;your code&lt;/denchmark-link&gt;
 I run the  method with the autograph in tf2.0 with eager execution successfully. You can check the &lt;denchmark-link:https://github.com/Douboo/tf_env_debug/blob/master/custom_layers_and_model_subclassing_API.ipynb&gt;code&lt;/denchmark-link&gt;

		</comment>
		<comment id='15' author='UdiBhaskar' date='2020-02-08T03:12:36Z'>
		Closing this issue since its resolved. Thanks!
		</comment>
		<comment id='16' author='UdiBhaskar' date='2020-02-08T03:12:38Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34944&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34944&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='17' author='UdiBhaskar' date='2020-03-12T15:07:16Z'>
		how?
		</comment>
		<comment id='18' author='UdiBhaskar' date='2020-07-30T13:23:01Z'>
		
Error is self-explainatory. If running under eager mode ,tensorflow op will check if the inputs are of type "tensorflow.python.framework.ops.EagerTensor" and keras ops are implemented as DAGs. So the inputs to the eagermode will be of "tensorflow.python.framework.ops.Tensor" and this throws the error
You can change the input type to EagerTensor by explicity telling tensorflow to run in eager mode for keras.
"tf.config.experimental_run_functions_eagerly(True)"
Adding this statement should solve your issue. Although note that there will be significant performance hits since you are running now in eager mode and recommended only for debugging,profiling etc.

solved my problem, thanks
		</comment>
		<comment id='19' author='UdiBhaskar' date='2020-12-13T19:24:57Z'>
		In order to evade the performance degradation when setting eager execution to True, you could run the code on colab with
%tensorflow_version 1.x
		</comment>
	</comments>
</bug>