<bug id='40297' author='jtrammell-dla' open_date='2020-06-09T01:51:14Z' closed_time='2020-06-09T03:36:04Z'>
	<summary>Mask-RCNN conversion succeeds, but requires select-tf-ops despite selecting only TFLITE_BUILTINS</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 18.04
TensorFlow installed from (source or binary):
tensorflow:2.2.0-gpu-jupyter docker image, so... binary?
TensorFlow version (or github SHA if from source):
2.2.0

Command used to run the converter or code if youâ€™re using the Python API
If possible, please share a link to Colab/Jupyter/any notebook.
&lt;denchmark-code&gt;converter = lite.TFLiteConverter.from_keras_model(model.keras_model)
converter.target_spec.supported_ops = [lite.OpsSet.TFLITE_BUILTINS]
converter.experimental_new_converter = True
converter.allow_custom_ops = False
converter.representative_dataset = [(np.random.random((256,256,3))*255).astype(np.uint8)]
tflite_model = converter.convert()
open("converted_model.tflite", "wb").write(tflite_model)
&lt;/denchmark-code&gt;

The output from the converter invocation
&lt;denchmark-code&gt;256614676
&lt;/denchmark-code&gt;

Also, please include a link to the saved model or GraphDef
&lt;denchmark-code&gt;What is a good way to provide this?
Original model parameters from matterport repo here: https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5
&lt;/denchmark-code&gt;

Failure details
If the conversion is successful, but the generated model is wrong,
state what is wrong:
The produced tflite model works on my android device, but it requires the tf-select-ops library, and fails to accept the provided GpuDelegate.
RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.
Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='jtrammell-dla' date='2020-06-09T03:12:09Z'>
		&lt;denchmark-link:https://github.com/jtrammell-dla&gt;@jtrammell-dla&lt;/denchmark-link&gt;
  Could you share which ops make it require tf-select-ops?
		</comment>
		<comment id='2' author='jtrammell-dla' date='2020-06-09T03:13:32Z'>
		Thanks for the fast response. Gladly! How do I figure that out?
		</comment>
		<comment id='3' author='jtrammell-dla' date='2020-06-09T03:16:39Z'>
		FYI, the GPU delegate supported op set is a subset of TFLite builtin ops and if the given model requires tf ops, which are not supported via TFLite Builtin op, those unsupported ops won't be supported via GPU delegate as well.
You can find the unsupported tf ops in TFLite builtin ops in the failed log when you convert without allowing custom ops and with converter.target_spec.supported_ops = [lite.OpsSet.TFLITE_BUILTINS].
		</comment>
		<comment id='4' author='jtrammell-dla' date='2020-06-09T03:36:04Z'>
		Ugh, my apologies... After a little more digging, it turns out that I had forgotten to actually install the new tflite file on the device. I'm still having problems, but they aren't tf-selec-ops related so I'll post them on a new ticket if I can't work them out. Thanks again for the fast response!
		</comment>
		<comment id='5' author='jtrammell-dla' date='2020-06-21T07:49:51Z'>
		&lt;denchmark-link:https://github.com/jtrammell-dla&gt;@jtrammell-dla&lt;/denchmark-link&gt;
 were you able to convert your model to tflite and run inference on it? I was able to convert it [&lt;denchmark-link:https://gist.github.com/bmabir17/754a6e0450ec4fd5e25e462af949cde6&gt;code&lt;/denchmark-link&gt;
] with tensorflow 1.13.1 but the converted model seems to have different node input as described &lt;denchmark-link:https://github.com/matterport/Mask_RCNN/issues/563#issuecomment-646546709&gt;here&lt;/denchmark-link&gt;
. Would you be kind enough to share how you did this using tensorflow 2.2?
		</comment>
		<comment id='6' author='jtrammell-dla' date='2020-06-22T19:26:42Z'>
		I started with the matterport repo. I then switched all the keras imports to tf.keras, and then I updated the handful of function calls which no longer exist in TF2.2.0. In every case (and there were only a handful, maybe ten at most) it was a trivial issue of the function call being moved to a new path in TF2.2.0. It did not require generating any new code with one very minor exception, which was that the region proposals needed to have their shape specified so downstream ops would work.
Getting it to work from there in TFLite was a little more complex, and will vary depending on your needs. I'm still in the process of adapting it, but yes, it can be done, with some caveats. It will not run with the TFLite GpuDelegate. It will not run without select-tf-ops. I've re-written the only function (tf.image.crop_and_resize) which actually requires the additional AAR, using TFLite supported code, and it compiles, but it fails with inexplicable "dimension mismatch" errors during inference.
		</comment>
		<comment id='7' author='jtrammell-dla' date='2020-06-24T17:24:58Z'>
		
using TFLite supported code, and it compiles, but it fails with inexplicable "dimension mismatch" errors during inference.

&lt;denchmark-link:https://github.com/jtrammell-dla&gt;@jtrammell-dla&lt;/denchmark-link&gt;
 yeah, i am also stuck in there. it seems the converted model was not converted properly.
Please refer to this &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/39179#issuecomment-646544647&gt;comment&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='jtrammell-dla' date='2020-06-24T18:24:37Z'>
		I suggest giving up on a full-on native TFLite implementation, and just stick with tf.image.crop_and_resize, and add tensorflow-lite-select-tf-ops to your build.
		</comment>
		<comment id='9' author='jtrammell-dla' date='2020-07-31T21:07:54Z'>
		
I suggest giving up on a full-on native TFLite implementation, and just stick with tf.image.crop_and_resize, and add tensorflow-lite-select-tf-ops to your build.

&lt;denchmark-link:https://github.com/jtrammell-dla&gt;@jtrammell-dla&lt;/denchmark-link&gt;
  how can you do that ?
		</comment>
		<comment id='10' author='jtrammell-dla' date='2020-07-31T21:15:55Z'>
		

I suggest giving up on a full-on native TFLite implementation, and just stick with tf.image.crop_and_resize, and add tensorflow-lite-select-tf-ops to your build.

@jtrammell-dla how can you do that ?

This guide here discusses including select-tf-ops:
&lt;denchmark-link:https://www.tensorflow.org/lite/guide/ops_select&gt;https://www.tensorflow.org/lite/guide/ops_select&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='jtrammell-dla' date='2020-07-31T21:31:34Z'>
		thanks &lt;denchmark-link:https://github.com/jtrammell-dla&gt;@jtrammell-dla&lt;/denchmark-link&gt;
, will try it...
		</comment>
		<comment id='12' author='jtrammell-dla' date='2020-07-31T22:01:42Z'>
		&lt;denchmark-link:https://github.com/jtrammell-dla&gt;@jtrammell-dla&lt;/denchmark-link&gt;
 thanks a lot, it worked !! I converted it and called it:
interpreter = tf.lite.Interpreter(model_path="../../logs/mask-rcnn-model2.tflite")
INFO: Initialized TensorFlow Lite runtime.
where before got the error
ValueError: Didn't find custom op for name 'CropAndResize' with version 1
now I'm trying to continue with the inference which I'm trying to do in python to check if everything is fine... now I call:
interpreter.allocate_tensors()
and get:
RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you invoke the Flex delegate before inference.Node number 276 (Flex) failed to prepare.
I'm following these instructions: &lt;denchmark-link:https://www.tensorflow.org/lite/guide/inference&gt;https://www.tensorflow.org/lite/guide/inference&lt;/denchmark-link&gt;
, can you tell me if I have to do something different or maybe other link with other instructions, I'm sorry but this is my first time trying to use tflite. If you could I would greatly appreciate it.
		</comment>
		<comment id='13' author='jtrammell-dla' date='2020-07-31T22:03:36Z'>
		Yeah, unfortunately you have to deploy it to a mobile device to test it out. Only native TFLITE ops are supported by the Python implementation of Interpreter.
		</comment>
		<comment id='14' author='jtrammell-dla' date='2020-07-31T22:06:30Z'>
		Or, more specifically, it seems that the SELECT ops have support in the nightly build of the Python implementation, so if you are willing to go down that road, it might work for you...
&lt;denchmark-link:https://www.tensorflow.org/lite/guide/ops_select&gt;https://www.tensorflow.org/lite/guide/ops_select&lt;/denchmark-link&gt;

		</comment>
		<comment id='15' author='jtrammell-dla' date='2020-07-31T22:11:27Z'>
		ok, thanks a lot &lt;denchmark-link:https://github.com/jtrammell-dla&gt;@jtrammell-dla&lt;/denchmark-link&gt;
, will check it out.
		</comment>
	</comments>
</bug>