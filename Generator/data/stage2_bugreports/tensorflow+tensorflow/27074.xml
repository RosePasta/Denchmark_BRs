<bug id='27074' author='Narzhan' open_date='2019-03-24T09:53:49Z' closed_time='2019-05-14T08:52:30Z'>
	<summary>Celery hangs with Tensorflow shared model</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 on Docker (18.03)
TensorFlow installed from (source or binary): pip
TensorFlow version (use command below): 1.13.1
Python version: 3.6
CUDA/cuDNN version: No gpu
GPU model and memory: No gpu
Celery 4.2.2, Keras 2.2.4

Scenario
I'm running a Celery worker which handless ml requests for prediction using LSTM NN. I'm loading the model in a main worker which passes it to the worker process as reference so that the model is not loaded all the time in a concurrent set-up.
Describe the current behavior
When the request comes the execution hangs indefinitely when it hits the access to the passed model reference.  I've searched for a solution and found that the sessions cannot be shared across process which is the cause of the hang. I've tried several solutions which are mentioned lower. However only changing back end to Theano solved the issue. Is there any other solution for this issue or I have to keep loading model in worker who uses it for prediction? Or I am doing something wrong?
Describe the expected behavior
I would like the set-up to work with Tensorflow as is with Theano that is to share a reference with workers to save memory.
Code to reproduce the issue
&lt;denchmark-code&gt;main.py

from keras.models import load_model
from celery import Celery, states
from celery import Task

class DomainAnalyzer(Task):
    ignore_result = True
    def __init__(self):
        self.model = load_model("lstm_model.h5")

    def run(self, data):
        try:
            preprocessor = Preprocessor(data, self.model)
            data = preprocessor.predict_data()
            return data 
        except Exception as e:
            print(e)

app = Celery('worker', broker="broker")
app.register_task(DomainAnalyzer())

if __name__ == '__main__':
    app.start()
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;worker.py

class Preprocessor:

    def __init__(self, data: str, model):
        self.data= data
        self.model = model

    def data_analysis(self):
        return self.model.predict(self.data)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Tried solution 1.&lt;/denchmark-h&gt;

I've specified the multiprocessing start method.
multiprocessing.set_start_method('spawn', force=True)
&lt;denchmark-h:h3&gt;Tried solution 2.&lt;/denchmark-h&gt;

Passing graph object in worker
&lt;denchmark-code&gt;import tensorflow as tf
self.model = load_model("lstm_model.h5")
self.model._make_predict_function()
self.graph = tf.get_default_graph()
preprocessor = Preprocessor(data, self.model, self.graph)
&lt;/denchmark-code&gt;

In worker:
&lt;denchmark-code&gt;def data_analysis(self):
     with self.graph.as_default():
         return self.model.predict(self.data)

&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Tried solution 3. (currently only one which works)&lt;/denchmark-h&gt;

Use Theano :(
	</description>
	<comments>
		<comment id='1' author='Narzhan' date='2019-04-25T20:20:14Z'>
		Can you clarify what your high-level goal is? To run a server for inference? If so, we would recommend you check out TensorFlow Serving: &lt;denchmark-link:https://www.tensorflow.org/tfx/guide/serving&gt;https://www.tensorflow.org/tfx/guide/serving&lt;/denchmark-link&gt;
 . We do not currently support Celery or have existing examples, so it's hard to help you debug here.
		</comment>
		<comment id='2' author='Narzhan' date='2019-04-30T12:18:22Z'>
		Yes, I'm building something of that sort, that is an extensible rest API service which provides some analysis. I've checked TensorFlow Serving during analysis but I'm not sure about the possibility of tweaking and further extensibility. Are you planning on making Tensorflow Celery friendly or rather multiprocess friendlier? Do you know any other workarounds for the general scenario (parallel model use) which could solve the issue apart from those mentioned?
		</comment>
		<comment id='3' author='Narzhan' date='2019-05-06T22:12:30Z'>
		There are no plans for integrating with Celery currently AFAIK. You could load multiple copies of the model into multiple processes, but other than that, I would encourage the use of TF Serving, which is what we use internally for this.
		</comment>
		<comment id='4' author='Narzhan' date='2019-05-14T08:52:30Z'>
		Ok, I'll try that
		</comment>
		<comment id='5' author='Narzhan' date='2019-05-14T08:52:32Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=27074&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=27074&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='Narzhan' date='2019-06-20T09:34:06Z'>
		celery workers serving with billiard, not multiprocessing
		</comment>
	</comments>
</bug>