<bug id='13895' author='hanfeisun' open_date='2017-10-22T03:27:24Z' closed_time='2017-10-23T16:55:59Z'>
	<summary>In the estimator of Tensorflow, how does it work when model_fn is called multiple times?</summary>
	<description>
&lt;denchmark-code&gt;def model_fn(features, labels, mode, params):
  """Model function for Estimator."""

  # Connect the first hidden layer to input layer
  # (features["x"]) with relu activation
  first_hidden_layer = tf.layers.dense(features["x"], 10, activation=tf.nn.relu)

  # Connect the second hidden layer to first hidden layer with relu
  second_hidden_layer = tf.layers.dense(
      first_hidden_layer, 10, activation=tf.nn.relu)

  # Connect the output layer to second hidden layer (no activation fn)
  output_layer = tf.layers.dense(second_hidden_layer, 1)

  # Reshape output layer to 1-dim Tensor to return predictions
  predictions = tf.reshape(output_layer, [-1])

  # Provide an estimator spec for `ModeKeys.PREDICT`.
  if mode == tf.estimator.ModeKeys.PREDICT:
    return tf.estimator.EstimatorSpec(
        mode=mode,
        predictions={"ages": predictions})

  # Calculate loss using mean squared error
  loss = tf.losses.mean_squared_error(labels, predictions)

  # Calculate root mean squared error as additional eval metric
  eval_metric_ops = {
      "rmse": tf.metrics.root_mean_squared_error(
          tf.cast(labels, tf.float64), predictions)
  }

  optimizer = tf.train.GradientDescentOptimizer(
      learning_rate=params["learning_rate"])
  train_op = optimizer.minimize(
      loss=loss, global_step=tf.train.get_global_step())

  # Provide an estimator spec for `ModeKeys.EVAL` and `ModeKeys.TRAIN` modes.
  return tf.estimator.EstimatorSpec(
      mode=mode,
      loss=loss,
      train_op=train_op,
      eval_metric_ops=eval_metric_ops)
&lt;/denchmark-code&gt;

Above is an example of the model_fn used by Tensorflow's &lt;denchmark-link:https://www.tensorflow.org/extend/estimators&gt;Estimator&lt;/denchmark-link&gt;
.
As mentioned in the tutorial, this model_fn could be called in different context (train, predict, evaluate). However, I'm a bit confused, because each time the model_fn is called, instead of reusing existing graph, it seems to create a new graph.(or create new node in the graph)
For example, firstly I called model_fn under TRAIN mode, then I called model_fn with PREDICT mode. How can I make sure the PREDICT one is reusing the weight of the trained values?
	</description>
	<comments>
		<comment id='1' author='hanfeisun' date='2017-10-22T04:56:17Z'>
		Is it true that the code in the tutorial forgets to add "reuse=True" as the parameter of layer functions?
		</comment>
		<comment id='2' author='hanfeisun' date='2017-10-22T23:55:51Z'>
		Right, that might be a problem.
&lt;denchmark-link:https://github.com/MarkDaoust&gt;@MarkDaoust&lt;/denchmark-link&gt;
 WDYT?
		</comment>
		<comment id='3' author='hanfeisun' date='2017-10-23T13:53:04Z'>
		The code and doc is correct, it just doesn't emphasize enough that it rebuilds the graph from scratch for each method, and for evaluate/predict is loads variables from the most recent checkpoint.
Generally we're moving away from the whole name_scope system for reuse, the future of variable sharing is layer objects, if you want to reuse a variable reuse the layer object.
We're rewriting this doc right now, I'll make sure it's more clear about this.
		</comment>
		<comment id='4' author='hanfeisun' date='2017-10-23T16:55:59Z'>
		Closing since the doc will be obsolete soon. Please check the new doc once it's out. Thank you both!
		</comment>
		<comment id='5' author='hanfeisun' date='2018-01-03T10:04:45Z'>
		Any idea when the new doc will be out? I still have this problem, the estimator is virtually useless if it creates the same graphs every time I need to evaluate something.
Cheers,
Francesco Saverio
		</comment>
		<comment id='6' author='hanfeisun' date='2018-01-03T10:46:39Z'>
		&lt;denchmark-link:https://github.com/FrancescoSaverioZuppichini&gt;@FrancescoSaverioZuppichini&lt;/denchmark-link&gt;
: The updated docs will be available in 1.5, but drafts are already in the repo, &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/get_started/checkpoints.md&gt;this doc&lt;/denchmark-link&gt;
 describes how checkpointing works with estimators.
In the mean time, let me summarize:
&lt;denchmark-link:https://github.com/hanfeisun&gt;@hanfeisun&lt;/denchmark-link&gt;
: Yes, each (, , ) method rebuilds the . But the estimator  loads the checkpoints from the  into the graph before doing anything. In fact  and  can't be used until after you've called  at least once, so that there is a checkpoint to load.
The reuse layer argument does something different. It's an outdated way to use the same layer on two different inputs. A much better way to have this effect is to use the layer classes, and just reuse the layer object. I believe the following two code blocks are equivalent:
&lt;denchmark-code&gt;out0 = tf.layers.dense(x, 10)
out1= tf.layers.dense(y,10, reuse=True)
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;my_layer = tf.layers.Dense(10)
out0 = my_layer(x)
out1 = my_layer(y)
&lt;/denchmark-code&gt;

I hope this helps, but in the future this sort of question is probably better suited for stack overflow.
		</comment>
		<comment id='7' author='hanfeisun' date='2018-01-03T13:03:56Z'>
		Thank you &lt;denchmark-link:https://github.com/MarkDaoust&gt;@MarkDaoust&lt;/denchmark-link&gt;
 for the answer. So I am forced to use the layer library? What if I want to use my own implementation? Is there any way to set a flag inside the model_fn? Or to have access to the estimator graph and directly used a scope do build in the same graph? Also, I am wondering how to run validation, assuming I want to train for one epoch and then run the model on the validation set I have to first call  and then  that will re-build the graph slowing everything. Any clever way to do so? I think is better to discuss here since It is issue lots of people have and the first link that pop up on google is this one.
Cheers,
Francesco Saverio
		</comment>
		<comment id='8' author='hanfeisun' date='2018-01-03T13:43:41Z'>
		
So I am forced to use the layer library?

No, we encourage you to use layers to wrap trainable state, but whether or not you use layers has no effect on the checkpoint reloading logic.

I have to first call .train and then .evaluate that will re-build the graph slowing everything.

If you want to run repeated evaluations while training, consider the &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate&gt;train_and_evaluate&lt;/denchmark-link&gt;
 function.
If you're working outside of estimators, &lt;denchmark-link:https://www.tensorflow.org/programmers_guide/datasets&gt;Datasets&lt;/denchmark-link&gt;
 do have advanced iterator types that allow you to swap input pipelines without rebuilding the graph. But often the changes you want to make to the graph between , , and  are more extensive than just the input stream.
		</comment>
		<comment id='9' author='hanfeisun' date='2018-01-03T15:37:27Z'>
		Thank you, really. Definitely, the API could have been better implemented. I am used to the Dataset, I could wrap the Dataset inside the input function but I am not sure how to swap between two datasets using the make_initializable_iterator since train_and_evaluate have not any hooks.
One last question, it is not clear to me how to avoid the re-creation of the graph using a generic implementation (assuming no layers library). Should I always set reuse=True, is there a faster way? Maybe with a scope?
I am sorry to bother you again, but really, the doc is so confusing that I could not find any answers to my questions.
Thank you again :)
		</comment>
		<comment id='10' author='hanfeisun' date='2018-01-16T12:27:35Z'>
		Hello again,
Is there any news? I saw there is a pre-release of the 1.5 version but I was not able to find how to avoid the model re-creation at every call of train/evaluate.
Cheers
		</comment>
		<comment id='11' author='hanfeisun' date='2018-01-16T14:18:54Z'>
		Well that's kind of my point. If you're using estimators you have to rebuild it on each call to train, evaluate or predict.
The only way around that is to use a different model abstraction, or roll your own.
For example &lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples&gt;the Eager examples&lt;/denchmark-link&gt;
 may provide the flexibility you're looking for.
		</comment>
		<comment id='12' author='hanfeisun' date='2018-01-16T15:19:35Z'>
		As far as I understood they said they will fix it in 1.5. Do you have any idea why they do that? I mean, adding a flag something like rebuild=True was impossible?
		</comment>
		<comment id='13' author='hanfeisun' date='2018-01-28T19:41:54Z'>
		Dear &lt;denchmark-link:https://github.com/MarkDaoust&gt;@MarkDaoust&lt;/denchmark-link&gt;
 the eager execution cannot be used with the Estimator. Also  will run evaluation AFTER training. I want to have a validation test that must be run at every train step but this seems impossible to do with the Estimator, maybe a better design could have solved the problem. Any ideas? I want to run one train step and one evaluation step WITHOUT loading the graph in order to also plot the validation score
		</comment>
		<comment id='14' author='hanfeisun' date='2018-02-08T08:32:56Z'>
		I have spent a week on my project based on Estimator until I stopped here. I am an early developer and had full faith in it. After having encountered this countless times, my heart is broken. If we can't evaluate during training, then why is Estimator useful?
		</comment>
		<comment id='15' author='hanfeisun' date='2018-02-08T08:40:34Z'>
		I also stop to use it. It is a total crap. I will write my own
		</comment>
		<comment id='16' author='hanfeisun' date='2018-02-08T22:39:08Z'>
		
If we can't evaluate during training, then why is Estimator useful?

Other possible approaches:

Alternate train and eval calls
In a separate process, monitor the model_dir for checkpoints and run eval on each time a new one appears.

		</comment>
		<comment id='17' author='hanfeisun' date='2018-02-08T22:43:21Z'>
		That is more a hack than a solution
		</comment>
		<comment id='18' author='hanfeisun' date='2018-02-12T03:22:20Z'>
		CC &lt;denchmark-link:https://github.com/martinwicke&gt;@martinwicke&lt;/denchmark-link&gt;

		</comment>
		<comment id='19' author='hanfeisun' date='2018-02-12T17:27:02Z'>
		&lt;denchmark-link:https://github.com/ispirmustafa&gt;@ispirmustafa&lt;/denchmark-link&gt;
 FYI also.
Sorry you all are frustrated. It also appears the documentation is adding to your confusion. Let me clarify a few points:

Estimator will evaluate during training. See the train_and_evaluate documentation. It sounds like you're doing single machine execution.
You can simply pass a function returning a dataset to Estimator. There shouldn't be a problem swapping out inputs.
Estimator will run the model_fn every time you call train, predict, or evaluate.
It will automatically reload the last checkpoint (or you can specify one)
You don't have to worry about reuse between train and eval. As long as your variables are called the same (most likely, simply by using the same or similar enough code to create the layers or raw variables), they will be automatically shared between training and evaluation.
Personally, I would stay away from reuse, and I would never use variable_scope. If you must (or have an existing model using this), that's fine. You can use either of these features to reuse variables within your model. Note that has absolutely no effect on whether variables are shared between training and evaluation. Unless you take steps to prevent it, variables are always shared between training and evaluation.

		</comment>
		<comment id='20' author='hanfeisun' date='2018-02-12T19:38:14Z'>
		an addition to &lt;denchmark-link:https://github.com/martinwicke&gt;@martinwicke&lt;/denchmark-link&gt;
,  creates a new graph for every call of . That makes variable names kept same for each .
		</comment>
		<comment id='21' author='hanfeisun' date='2018-02-14T07:26:50Z'>
		I am kind of confuse again. If  creates a new graph for every call of , then each of  and  has a graph. A loop iterates  and  will create two new graphs every time? &lt;denchmark-link:https://github.com/ispirmustafa&gt;@ispirmustafa&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/martinwicke&gt;@martinwicke&lt;/denchmark-link&gt;

		</comment>
		<comment id='22' author='hanfeisun' date='2018-02-14T07:31:24Z'>
		&lt;denchmark-link:https://github.com/tengerye&gt;@tengerye&lt;/denchmark-link&gt;
  yes! That's the problem
		</comment>
		<comment id='23' author='hanfeisun' date='2018-02-14T17:11:01Z'>
		Yes, if you call train and evaluate in a loop, you will create two new graphs every time. You will also read and write a checkpoint in each iteration of your loop. If you epoch is long, you probably want a checkpoint every time anyway so that's not a big deal, but if you train on less than an epoch in each iteration, such loops are a bad idea.
&lt;denchmark-link:https://github.com/FrancescoSaverioZuppichini&gt;@FrancescoSaverioZuppichini&lt;/denchmark-link&gt;
 I'm afraid I lost track of what the actual problem is.
		</comment>
		<comment id='24' author='hanfeisun' date='2018-02-21T07:31:24Z'>
		Thank you sincerely for your kind answer &lt;denchmark-link:https://github.com/martinwicke&gt;@martinwicke&lt;/denchmark-link&gt;
. According to your reply,  may suit my requirement. I believe &lt;denchmark-link:https://github.com/FrancescoSaverioZuppichini&gt;@FrancescoSaverioZuppichini&lt;/denchmark-link&gt;
 and me are expecting more flexible implementation that we could write some snippets of codes to replace a component instead of re-writing everything.
Thank you again.
		</comment>
		<comment id='25' author='hanfeisun' date='2018-02-22T16:53:24Z'>
		&lt;denchmark-link:https://github.com/tengerye&gt;@tengerye&lt;/denchmark-link&gt;
 which component would you like to replace?
Many of the restrictions of Estimator are in place so you can scale up without having to change your code. That makes local training less convenient, but it avoids a lot of pitfalls later.
		</comment>
		<comment id='26' author='hanfeisun' date='2018-02-23T03:16:50Z'>
		&lt;denchmark-link:https://github.com/martinwicke&gt;@martinwicke&lt;/denchmark-link&gt;
 I see your point now. I thought it would be better if training and evaluation could progress interactively under single machine execution. I think maybe it it better to provide an independent implementation especially for single machine? Since a lot of people work on single machine at the beginning.
		</comment>
		<comment id='27' author='hanfeisun' date='2018-04-23T16:03:07Z'>
		I'd like to get a bit more information on this problem but from the predict() point of view.
I posted on stackoverflow but really there is not much support there for now. I can see why people would try here.
&lt;denchmark-link:https://stackoverflow.com/questions/49966447/keep-the-tensorflow-estimator-in-memory-while-waiting-for-live-prediction-inputs&gt;https://stackoverflow.com/questions/49966447/keep-the-tensorflow-estimator-in-memory-while-waiting-for-live-prediction-inputs&lt;/denchmark-link&gt;

It look like when running the estimator live and doing one prediction at a time (as data comes in) most of the prediction time is wasted on reloading the model.
In my use case I'd like to do the [prediction, action, prediction, action, ...] as fast as possible.
Is there a way around having the model reloaded in between predictions?
		</comment>
		<comment id='28' author='hanfeisun' date='2018-04-23T19:44:18Z'>
		Hi &lt;denchmark-link:https://github.com/fgervais&gt;@fgervais&lt;/denchmark-link&gt;
 ,
for predict could you please check out &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/contrib/predictor/from_estimator&gt;predictor.from_estimator&lt;/denchmark-link&gt;

		</comment>
		<comment id='29' author='hanfeisun' date='2018-04-23T21:29:41Z'>
		I'll give it a try thank you
		</comment>
		<comment id='30' author='hanfeisun' date='2018-04-29T05:43:11Z'>
		To clarify:

train_and_evaluate will just evaluate after the train is done. Meaning, that it will first train for, following our example, 100 epochs and then run the evaluation one once!

That is incorrect. train_and_evaluate trains for a configurable amount of iterations, then runs an evaluation until the specified input_fn finishes. Then it starts training again by restoring the weights that were used for the eval, and consuming the input stream from where it left off. There might be some minor technicalities in my summarized explanation, but it is conceptually accurate.
I've been training reasonably large models on a single laptop using train_and_evaluate since version 1.2, and I think it works extremely well. The time it takes to rebuild the graph and restore parameters is negligible when you take into account all the time that's spent actually training.
		</comment>
		<comment id='31' author='hanfeisun' date='2018-05-01T05:25:24Z'>
		&lt;denchmark-link:https://github.com/ispirmustafa&gt;@ispirmustafa&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/fgervais&gt;@fgervais&lt;/denchmark-link&gt;
 I'm also giving this a try. It feels as if I'm almost there, but I'm still struggling with a couple of issues. I've just posted a question with a minimalistic example on stackoverflow:
&lt;denchmark-link:https://stackoverflow.com/questions/50111636/unable-to-use-core-estimator-with-contrib-predictor&gt;https://stackoverflow.com/questions/50111636/unable-to-use-core-estimator-with-contrib-predictor&lt;/denchmark-link&gt;

		</comment>
		<comment id='32' author='hanfeisun' date='2018-05-07T14:57:29Z'>
		&lt;denchmark-link:https://github.com/ispirmustafa&gt;@ispirmustafa&lt;/denchmark-link&gt;
 the predictor worked, thank you.
It it helps, I added a bit of information on how I did it &lt;denchmark-link:https://stackoverflow.com/questions/49966447/keep-the-tensorflow-estimator-in-memory-while-waiting-for-live-prediction-inputs/&gt;here&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='33' author='hanfeisun' date='2018-05-09T08:50:35Z'>
		I made a public repo which provides a complete example of using the predictor with a custom core estimator, where it compares prediction results and performance between estimator.predict() and predictor():
&lt;denchmark-link:https://github.com/dage/tensorflow-estimator-predictor-example&gt;https://github.com/dage/tensorflow-estimator-predictor-example&lt;/denchmark-link&gt;

		</comment>
		<comment id='34' author='hanfeisun' date='2018-05-09T19:42:22Z'>
		Hi &lt;denchmark-link:https://github.com/dage&gt;@dage&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/fgervais&gt;@fgervais&lt;/denchmark-link&gt;

Thank you for update.
If you want to update pydoc of estimator.predict to give your examples I'll be happy to review.
		</comment>
		<comment id='35' author='hanfeisun' date='2018-05-21T14:33:02Z'>
		Hi, I need to see the plots for validation convergence as I train using estimators, and cannot see how to pass validation set to the train function, or how to compute validation losses, I also tried to write my own, without estimators, and always gets the error that graph cannot be larger than 2Gig, could you please help. thanks.
		</comment>
		<comment id='36' author='hanfeisun' date='2018-05-21T14:51:16Z'>
		&lt;denchmark-link:https://github.com/rabeehkarimi&gt;@rabeehkarimi&lt;/denchmark-link&gt;
 use tf.estimator.train_and_evaluate.
		</comment>
		<comment id='37' author='hanfeisun' date='2018-05-28T22:17:13Z'>
		&lt;denchmark-link:https://github.com/formigone&gt;@formigone&lt;/denchmark-link&gt;
 I'm afraid that your understanding of how input_fn works is not accurate.
According to the instruction:

It is also recommended to train the model a little longer, say multiple epochs, before
performing evaluation, as the input pipeline starts from scratch for each
training.

So i guess with estimator api, if I want to evaluate within one epoch without resetting the input flow, I have to somehow store the iterator status inside the input_fn?
		</comment>
		<comment id='38' author='hanfeisun' date='2018-05-31T00:12:49Z'>
		If you're using Estimator in a distributed system, evaluation happens in parallel to training, and not strictly in between epochs.
In local mode, to evaluate, say, twice per epoch, you can either use a variable in the input_fn, and store state there, or you can store that state in a hook. Either way, this is somewhat complicated.
Another option (and the one used by most of our production pipelines) is to shuffle the inputs. That way, you will get a different set of inputs every time you run, and you can simply evaluate after a given set of steps and then evaluate, without taking special care of the input state. If you do this, be careful that you don't only shuffle the beginning of the data.
		</comment>
		<comment id='39' author='hanfeisun' date='2018-06-12T07:52:35Z'>
		What about the following solution, where we pass in the same estimator to a SessionRunHook and evaluate it every certain number of steps (which could be calculated to be roughly one epoch).  As far as I can tell, since train is only called once, it doesn't recreate its graph [though the calls to evaluate will].  Does this break things?
class EvalEarlyStopHook(tf.train.SessionRunHook)

    def __init__(self, estimator, eval_input_fn, num_steps):

        self._estimator = estimator
        self._input_fn = eval_input
        self._num_steps = num_steps

    ...

    def after_run(self, run_context, run_values):

        global_step = run_values.results['global_step']
        if (global_step-1) % self._num_steps == 0:
            ev_results = self._estimator.evaluate(input_fn=self._input_fn)

            if do_stuff_with_results:
                 run_context.request_stop()
With the main call being something like:
estimator.train(input_fn=train_input_fn,
                        hooks=[EvalEarlyStopHook(estimator, eval_input_fn, num_steps))
		</comment>
		<comment id='40' author='hanfeisun' date='2018-06-28T14:54:26Z'>
		Just came across &lt;denchmark-link:https://www.tensorflow.org/versions/r1.9/api_docs/python/tf/contrib/estimator/InMemoryEvaluatorHook&gt;InMemoryEvaluatorHook&lt;/denchmark-link&gt;
 API in TF 1.9. Thank you &lt;denchmark-link:https://github.com/ispirmustafa&gt;@ispirmustafa&lt;/denchmark-link&gt;
 and TF team!
		</comment>
		<comment id='41' author='hanfeisun' date='2018-06-28T16:07:16Z'>
		Hi &lt;denchmark-link:https://github.com/jperl&gt;@jperl&lt;/denchmark-link&gt;
 ,
Please let us know your feedback based on your usage.
		</comment>
		<comment id='42' author='hanfeisun' date='2018-07-15T17:21:56Z'>
		&lt;denchmark-link:https://github.com/ispirmustafa&gt;@ispirmustafa&lt;/denchmark-link&gt;
 I just upgraded our project to TF 1.9 and the . It works fantastically . It has spec up our train/eval loop significantly -- and now evaluation happens at a deterministic step interval (unlike before when there was some weirdness with throttle secs).
There is just one minor bug that I made a PR for &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/20822/files&gt;https://github.com/tensorflow/tensorflow/pull/20822/files&lt;/denchmark-link&gt;
 (w/ test).
		</comment>
		<comment id='43' author='hanfeisun' date='2018-07-17T02:08:51Z'>
		Hi guys, any suggestions about different post-process input size with estimator? Here is my case:
I use different batch size（32 and 1） in train and evaluation with estimator and use a subclass of tf.train.SessionRunHook to achieve the purpose for validating during training.
In input_fn i use tf.cond to judge the mode(EVAL or TRAIN) and apply different codes respectively. But it seems that when execute estimator.train ，training data with batch size 32 will flow through the codes for validation set（with batch size 1) (i think it is caused by tf.cond)
So i wonder if there are ways to allow data with different size flow through different codes under different mode(TRAIN and EVAL) in input_fn
Thank you ~
		</comment>
		<comment id='44' author='hanfeisun' date='2018-07-24T10:37:52Z'>
		&lt;denchmark-link:https://github.com/jperl&gt;@jperl&lt;/denchmark-link&gt;
 Correct me if I'm wrong, the  solves the issue of validating the  while training it, but the  still creates multiple graphs (one for train + one for evaluation) ?
		</comment>
		<comment id='45' author='hanfeisun' date='2018-07-24T10:53:47Z'>
		That is correct.
		</comment>
		<comment id='46' author='hanfeisun' date='2018-08-12T08:33:03Z'>
		Hi &lt;denchmark-link:https://github.com/martinwicke&gt;@martinwicke&lt;/denchmark-link&gt;
 ,
 looks like a great way to evaluate while training but I am running into issue where all eval_metrics are evaluated to 0.0 after the very first time the evaluation hook is triggered.
Here is a snippet of my output when :
INFO:tensorflow:Saving dict for global step 0: accuracy = 0.158545, global_step = 0, loss = 3.36605
INFO:tensorflow:Saving dict for global step 50: accuracy = 0.0, global_step = 50, loss = 0.0
INFO:tensorflow:Saving dict for global step 100: accuracy = 0.0, global_step = 100, loss = 0.0
INFO:tensorflow:Saving dict for global step 150: accuracy = 0.0, global_step = 150, loss = 0.0
My suspicion is that it does not work because my  input_fn is defined as tf.estimator.inputs.numpy_input_fn with num_epochs=1.
The exact definition of the input function that I use to create the InMemoryEvaluatorHook is:
&lt;denchmark-code&gt;    eval_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={"x": eval_data},
        y=eval_labels,
        num_epochs=1,
        batch_size=25,
        shuffle=False)
&lt;/denchmark-code&gt;

How can I confirm if my input_fn is really to blame here? Any ideas how to get around this? Thanks
		</comment>
		<comment id='47' author='hanfeisun' date='2018-08-13T18:11:58Z'>
		Yes, I think your hunch is correct. You probably have to use num_epochs=None and set the appropriate amount of eval steps to stop evaluation.
		</comment>
		<comment id='48' author='hanfeisun' date='2018-08-13T22:41:14Z'>
		Hi &lt;denchmark-link:https://github.com/tavramov&gt;@tavramov&lt;/denchmark-link&gt;
 ,
Most probably it's not related to num_epoch.
Could you please create a simple script to reproduce this issue? Also do you mind to open a new issue?
		</comment>
		<comment id='49' author='hanfeisun' date='2018-08-14T02:38:42Z'>
		&lt;denchmark-link:https://github.com/martinwicke&gt;@martinwicke&lt;/denchmark-link&gt;
 - Your recommendation works,.The eval metrics get evaluated properly when I set  in the  and use the  argument in  to stop the evaluation.  But this is a dangerous hack, hopefully there is a better solution to this.
&lt;denchmark-link:https://github.com/ispirmustafa&gt;@ispirmustafa&lt;/denchmark-link&gt;
 - Issue can be easily reproduced. I just inserted  to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/layers/cnn_mnist.py&gt;cnn_mnist.py&lt;/denchmark-link&gt;

Submitted Issue #&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/21590&gt;21590&lt;/denchmark-link&gt;
 to track this defect
Thanks
		</comment>
		<comment id='50' author='hanfeisun' date='2019-05-23T18:54:12Z'>
		&lt;denchmark-link:https://github.com/shanest&gt;@shanest&lt;/denchmark-link&gt;
 does your proposal work? it seemed more readable than InMemoryEvaluatorHook, it would be nice if works. It might will create another "evaluate" session inside "train" session, I am not sure how it will behave, could you please report its behavior?
		</comment>
		<comment id='51' author='hanfeisun' date='2019-05-23T19:35:57Z'>
		Although  is not loading checkpoint from disk, &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/25c197e02393bd44f50079945409009dd4d434f8/tensorflow/contrib/estimator/python/estimator/hooks.py#L170&gt;it fetches all variable from "train" graph and feed it to the "evaluation" graph&lt;/denchmark-link&gt;
. It still recreates an evaluation graph and all the variables with it . So the problem of graph recreation remains and the slow loading ckpt from disk is simply replaced with a memory copy. Are there any follow ups on graph reusing in  ?
		</comment>
		<comment id='52' author='hanfeisun' date='2019-05-23T20:40:57Z'>
		&lt;denchmark-link:https://github.com/JerrikEph&gt;@JerrikEph&lt;/denchmark-link&gt;
 If you can I recommend switching to a Keras model / training loop. Then you can create a callback to evaluate using the same model. After switching to Keras models I have not looked back to estimators.
		</comment>
	</comments>
</bug>