<bug id='33494' author='tulasiram58827' open_date='2019-10-18T04:17:38Z' closed_time='2020-11-30T09:26:25Z'>
	<summary>CTC tensorflow lite conversion problem</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 19.04
TensorFlow installed from (source or binary):binary
TensorFlow version (or github SHA if from source): 2.0

Provide the text output from tflite_convert
&lt;denchmark-code&gt;Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: . Here is a list of operators for which you will need custom implementations: CTC_BEAM_SEARCH_DECODER.```

Also, please include a link to a GraphDef or the model if possible.
import tensorflow as tf
class BasicModel(tf.Module):
  def __init__(self):
    self.const = None
  @tf.function(input_signature=[tf.TensorSpec(shape=[None,500,28], dtype=tf.float32),tf.TensorSpec(shape=[None,], dtype=tf.int32)])
  def decoder(self, logits,seq_len):
    decoded, log_prob = tf.nn.ctc_beam_search_decoder(logits, seq_len)
    return decoded
# Create the tf.Module object.
root = BasicModel()
# Get the concrete function.
concrete_func = root.decoder.get_concrete_function()
converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])
converter.allow_custom_ops = False
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.SELECT_TF_OPS]
tflite_model =converter.convert()

open("ctc_greedy_decoder.tflite",'wb').write(tflite_model)

**Any other info / logs**

But according to this link https://github.com/tensorflow/tensorflow/tree/r2.0/tensorflow/lite/experimental/kernels ctc_beam_search_decoder is registered as tflite op.

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='tulasiram58827' date='2019-10-21T22:19:04Z'>
		Hi,
Could you change this line:
converter.allow_custom_ops = False
to
converter.allow_custom_ops = True
I run your script in TF 2 and can successfully do the conversion.
		</comment>
		<comment id='2' author='tulasiram58827' date='2019-10-22T05:39:22Z'>
		But if I do converter.allow_custom_ops=True I can convert it to tflite but I am not able to use it.
Here is the error :
Traceback (most recent call last):
File "tflite.py", line 34, in 
inference_from_tflite(pbfile_path)
File "tflite.py", line 24, in inference_from_tflite
interpreter.allocate_tensors()
File "/home/mihup/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter.py", line 244, in allocate_tensors
return self._interpreter.AllocateTensors()
File "/home/mihup/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py", line 106, in AllocateTensors
return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)
RuntimeError: Encountered unresolved custom op: CTC_BEAM_SEARCH_DECODER.Node number 0 (CTC_BEAM_SEARCH_DECODER) failed to prepare.
Code used for inference:
interpreter = tf.lite.Interpreter(model_path=tflite_model)
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
interpreter.invoke()
print(input_details,output_details)
		</comment>
		<comment id='3' author='tulasiram58827' date='2019-10-29T03:34:37Z'>
		This seems like the default op resolver couldn't find the op kernel for CTC_BEAM_SEARCH_DECODER.
I guess you need to implement something like this:
tflite::ops::builtin::BuiltinOpResolver builtins;
builtins.AddCustom("CTCBeamSearchDecoder", Register_CTC_BEAM_SEARCH_DECODER());
The corresponding python interpreter to use with custom op is:



tensorflow/tensorflow/lite/python/interpreter.py


         Line 499
      in
      0dec1a6






 class InterpreterWithCustomOps(Interpreter): 





Please See link:&lt;denchmark-link:https://www.tensorflow.org/lite/guide/ops_custom#defining_the_kernel_in_the_tensorflow_lite_runtime&gt;https://www.tensorflow.org/lite/guide/ops_custom#defining_the_kernel_in_the_tensorflow_lite_runtime&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='tulasiram58827' date='2019-10-30T10:27:46Z'>
		The corresponding python interpreter   "class InterpreterWithCustomOps(Interpreter): "  to use with custom op is not present in the stable releases like  1.14,1.15,2.0 .
Python interpreter "class InterpreterWithCustomOps(Interpreter): " is only available in the master branch.
How can I use it as I cannot use master branch with pip install
		</comment>
		<comment id='5' author='tulasiram58827' date='2019-11-22T05:13:51Z'>
		&lt;denchmark-link:https://github.com/haozha111&gt;@haozha111&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
  please look into it.
		</comment>
		<comment id='6' author='tulasiram58827' date='2020-06-05T03:56:14Z'>
		&lt;denchmark-link:https://github.com/haozha111&gt;@haozha111&lt;/denchmark-link&gt;
 any update on this?
		</comment>
		<comment id='7' author='tulasiram58827' date='2020-06-08T22:13:27Z'>
		Hi!
InterpreterWithCustomOps seems not the correct recommendation here, since it's currently not exposed as the public API. Looping in &lt;denchmark-link:https://github.com/jdduke&gt;@jdduke&lt;/denchmark-link&gt;
 to take a look. Hi Jared, do you know the recommended interpreter API to use when the model contains customs ops such as ctc beam search decoder? AFAIK the model can convert but it encounters a kernel registration issue during runtime.
		</comment>
		<comment id='8' author='tulasiram58827' date='2020-06-16T22:15:52Z'>
		&lt;denchmark-link:https://github.com/tulasiram58827&gt;@tulasiram58827&lt;/denchmark-link&gt;
 just to be clear, do you want Python support primarily for testing? Or you plan on deploying your model in a Python environment? You can use  if you build from source. How else did you want to plug in your custom ops into that function? Would you be linking it into the TF runtime?
		</comment>
		<comment id='9' author='tulasiram58827' date='2020-08-06T05:02:27Z'>
		&lt;denchmark-link:https://github.com/jdduke&gt;@jdduke&lt;/denchmark-link&gt;
 I planned for deploying my model in cpp enviornment. I want to converty my ctc model to tflite and use it in tflite runtime. I can easily convert my model to tflite by specifying this while conversion(converter.allow_custom_ops = True) but in runtime i am facing difficulties that this op is not registered. Please check this log.
Here is the error :
Traceback (most recent call last):
File "tflite.py", line 34, in
inference_from_tflite(pbfile_path)
File "tflite.py", line 24, in inference_from_tflite
interpreter.allocate_tensors()
File "/home/mihup/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter.py", line 244, in allocate_tensors
return self._interpreter.AllocateTensors()
File "/home/mihup/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py", line 106, in AllocateTensors
return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)
RuntimeError: Encountered unresolved custom op: CTC_BEAM_SEARCH_DECODER.Node number 0 (CTC_BEAM_SEARCH_DECODER) failed to prepare.
		</comment>
		<comment id='10' author='tulasiram58827' date='2020-11-17T08:02:42Z'>
		&lt;denchmark-link:https://github.com/jdduke&gt;@jdduke&lt;/denchmark-link&gt;
 Any update?
		</comment>
		<comment id='11' author='tulasiram58827' date='2020-11-18T21:11:54Z'>
		I see, you would need to link in the CTC_BEAM_SEARCH_DECODER custom op manually. So you'd have to update the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/BUILD#L625&gt;builtin deps&lt;/denchmark-link&gt;
 to include &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/kernels/BUILD#L39&gt;this dependency&lt;/denchmark-link&gt;
, then manually register that kernel &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/register.cc#L315&gt;here&lt;/denchmark-link&gt;
 via:
&lt;denchmark-code&gt;AddCustom("CTC_BEAM_SEARCH_DECODER",
            tflite::ops::experimental::Register_CTC_BEAM_SEARCH_DECODER());
&lt;/denchmark-code&gt;

		</comment>
		<comment id='12' author='tulasiram58827' date='2020-11-30T06:50:17Z'>
		Thanks &lt;denchmark-link:https://github.com/jdduke&gt;@jdduke&lt;/denchmark-link&gt;
 for the pointers. But with this modifications we need to compile tensorflow from source. Is it possible to add this op in next tensorflow release or atleast in TF Nightly because CTC is something widely used in Speech Recognition engines and also OCR engines. Recently i am working to make on device OCR possible in this &lt;denchmark-link:https://github.com/tulasiram58827/ocr_tflite&gt;Repository&lt;/denchmark-link&gt;
 But at the end we need a CTC Decoder to do post processing. It would be very great if TFLite team can add support to CTC op.
		</comment>
		<comment id='13' author='tulasiram58827' date='2020-11-30T09:26:26Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33494&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33494&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='14' author='tulasiram58827' date='2020-12-12T00:00:11Z'>
		&lt;denchmark-link:https://github.com/tulasiram58827&gt;@tulasiram58827&lt;/denchmark-link&gt;
 can you confirm that, by building from source with that op included as a custom op, your model runs successfully? If so, we can look into adding it as a builtin operator. Thanks. It would also help if you can attach the model that is created if you allow custom ops. Thanks.
		</comment>
		<comment id='15' author='tulasiram58827' date='2020-12-14T10:27:32Z'>
		&lt;denchmark-link:https://github.com/jdduke&gt;@jdduke&lt;/denchmark-link&gt;
 I tried building from source including custom op but it seems like  file is missing. Please find the error here.

ERROR: missing input file 'tensorflow/lite/kernels/ctc_beam_search_decoder.cc', owner: '//tensorflow/lite/kernels:ctc_beam_search_decoder.cc'

		</comment>
		<comment id='16' author='tulasiram58827' date='2020-12-14T17:15:13Z'>
		The kernel is actually defined here: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/kernels/BUILD#L39&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/kernels/BUILD#L39&lt;/denchmark-link&gt;
. So you would need to add
&lt;denchmark-code&gt;//tensorflow/lite/experimental/kernels:ctc_beam_search_decoder_op
&lt;/denchmark-code&gt;

to the builtin deps &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/BUILD#L653&gt;here&lt;/denchmark-link&gt;
, then add a line that registers it with the builtin op registry &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/register.cc#L318&gt;here&lt;/denchmark-link&gt;
.
That should make it available for any target you build from source.
		</comment>
		<comment id='17' author='tulasiram58827' date='2020-12-15T04:28:13Z'>
		Thanks &lt;denchmark-link:https://github.com/jdduke&gt;@jdduke&lt;/denchmark-link&gt;
 .Previous error was resolved. New error popped out.

ERROR: /home/ram/Projects/tensorflow/tensorflow/lite/kernels/BUILD:781:1: C++ compilation of rule '//tensorflow/lite/kernels:builtin_ops' failed (Exit 1)
tensorflow/lite/kernels/register.cc: In constructor 'tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()':
tensorflow/lite/kernels/register.cc:322:26: error: 'tflite::ops::experimental' has not been declared
tflite::ops::experimental::Register_CTC_BEAM_SEARCH_DECODER());

		</comment>
		<comment id='18' author='tulasiram58827' date='2020-12-16T00:47:46Z'>
		You need to forward declare it first, e.g., &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/register.cc#L27&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/register.cc#L27&lt;/denchmark-link&gt;
,  but in the right experimental namespace.
		</comment>
		<comment id='19' author='tulasiram58827' date='2020-12-21T07:24:04Z'>
		Hi &lt;denchmark-link:https://github.com/jdduke&gt;@jdduke&lt;/denchmark-link&gt;
 With the new version of TensorFlow 2.4 my conversion of CTC Decoder is successful by enabling TFLite Ops
&lt;denchmark-code&gt;converter.target_spec.supported_ops = [
          tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
          tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
      ]
&lt;/denchmark-code&gt;

My issue is solved. You can check this &lt;denchmark-link:https://github.com/tulasiram58827/ocr_tflite/blob/main/colabs/KERAS_OCR_TFLITE.ipynb&gt;OCR Notebook&lt;/denchmark-link&gt;
 where I successfully used and converted CTC Decoder to TFLite.
		</comment>
		<comment id='20' author='tulasiram58827' date='2020-12-21T16:19:33Z'>
		Great!
		</comment>
	</comments>
</bug>