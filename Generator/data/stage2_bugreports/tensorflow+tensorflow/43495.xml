<bug id='43495' author='twkx' open_date='2020-09-23T16:11:06Z' closed_time='2020-10-15T14:57:47Z'>
	<summary>Cannot convert CenterNet+KeyPoints Model Zoo tf2 models to tflite</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.4 LTS
TensorFlow installed from (source or binary): binary (tf-nightly)
TensorFlow version (use command below): 2.4.0-dev20200907
Python version: 3.6.9
CUDA/cuDNN version: 11.0/8.0

Describe the current behavior
When I try to convert a model from TensorFlow 2 Detection Model Zoo into a tflite version, I get some error.
[EDIT] This is only the case for models using CenterNet AND KeyPoints. All other models are OK
Describe the expected behavior
Convertion is successfull
Standalone code to reproduce the issue
Download "CenterNet Resnet50 V1 FPN Keypoints 512x512" from &lt;denchmark-link:https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md&gt;https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md&lt;/denchmark-link&gt;

Uncompress the archive
Run "tflite_convert --saved_model_dir=centernet_resnet50_v1_fpn_512x512_kpts_coco17_tpu-8/saved_model --output_file=model.tflite"
I got the following error:
loc(callsite(callsite("map/TensorArrayV2_2@__inference___call___13919" at "StatefulPartitionedCall@__inference_signature_wrapper_15800") at "StatefulPartitionedCall")): error: requires element_shape to be 1D tensor during TF Lite transformation pass
loc(callsite(callsite("map/TensorArrayV2_2@__inference___call___13919" at "StatefulPartitionedCall@__inference_signature_wrapper_15800") at "StatefulPartitionedCall")): error: failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal
Traceback (most recent call last):
File "/home/biroute/.local/lib/python3.6/site-packages/tensorflow/lite/python/convert.py", line 199, in toco_convert_protos
enable_mlir_converter)
File "/home/biroute/.local/lib/python3.6/site-packages/tensorflow/lite/python/wrap_toco.py", line 38, in wrapped_toco_convert
enable_mlir_converter)
Exception: :0: error: loc(callsite(callsite("map/TensorArrayV2_2@__inference___call___13919" at "StatefulPartitionedCall@__inference_signature_wrapper_15800") at "StatefulPartitionedCall")): requires element_shape to be 1D tensor during TF Lite transformation pass
:0: note: loc("StatefulPartitionedCall"): called from
:0: note: loc(callsite(callsite("map/TensorArrayV2_2@__inference___call___13919" at "StatefulPartitionedCall@__inference_signature_wrapper_15800") at "StatefulPartitionedCall")): see current operation: %456 = "tf.TensorListReserve"(%67, %77) {device = ""} : (tensor, tensor) -&gt; tensor&lt;!tf.variant&lt;tensor&lt;*xf32&gt;&gt;&gt;
:0: error: loc(callsite(callsite("map/TensorArrayV2_2@__inference___call___13919" at "StatefulPartitionedCall@__inference_signature_wrapper_15800") at "StatefulPartitionedCall")): failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal
:0: note: loc("StatefulPartitionedCall"): called from
:0: note: loc(callsite(callsite("map/TensorArrayV2_2@__inference___call___13919" at "StatefulPartitionedCall@__inference_signature_wrapper_15800") at "StatefulPartitionedCall")): see current operation: %456 = "tf.TensorListReserve"(%67, %77) {device = ""} : (tensor, tensor) -&gt; tensor&lt;!tf.variant&lt;tensor&lt;*xf32&gt;&gt;&gt;
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
File "/home/biroute/.local/bin/tflite_convert", line 8, in 
sys.exit(main())
File "/home/biroute/.local/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py", line 640, in main
app.run(main=run_main, argv=sys.argv[:1])
File "/home/biroute/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py", line 40, in run
_run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
File "/home/biroute/.local/lib/python3.6/site-packages/absl/app.py", line 300, in run
_run_main(main, args)
File "/home/biroute/.local/lib/python3.6/site-packages/absl/app.py", line 251, in _run_main
sys.exit(main(argv))
File "/home/biroute/.local/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py", line 623, in run_main
_convert_tf2_model(tflite_flags)
File "/home/biroute/.local/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py", line 239, in _convert_tf2_model
tflite_model = converter.convert()
File "/home/biroute/.local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py", line 726, in convert
output_tensors)
File "/home/biroute/.local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py", line 643, in convert
**converter_kwargs)
File "/home/biroute/.local/lib/python3.6/site-packages/tensorflow/lite/python/convert.py", line 573, in toco_convert_impl
enable_mlir_converter=enable_mlir_converter)
File "/home/biroute/.local/lib/python3.6/site-packages/tensorflow/lite/python/convert.py", line 202, in toco_convert_protos
raise ConverterError(str(e))
tensorflow.lite.python.convert.ConverterError: :0: error: loc(callsite(callsite("map/TensorArrayV2_2@__inference___call___13919" at "StatefulPartitionedCall@__inference_signature_wrapper_15800") at "StatefulPartitionedCall")): requires element_shape to be 1D tensor during TF Lite transformation pass
:0: note: loc("StatefulPartitionedCall"): called from
:0: note: loc(callsite(callsite("map/TensorArrayV2_2@__inference___call___13919" at "StatefulPartitionedCall@__inference_signature_wrapper_15800") at "StatefulPartitionedCall")): see current operation: %456 = "tf.TensorListReserve"(%67, %77) {device = ""} : (tensor, tensor) -&gt; tensor&lt;!tf.variant&lt;tensor&lt;*xf32&gt;&gt;&gt;
:0: error: loc(callsite(callsite("map/TensorArrayV2_2@__inference___call___13919" at "StatefulPartitionedCall@__inference_signature_wrapper_15800") at "StatefulPartitionedCall")): failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal
:0: note: loc("StatefulPartitionedCall"): called from
:0: note: loc(callsite(callsite("map/TensorArrayV2_2@__inference___call___13919" at "StatefulPartitionedCall@__inference_signature_wrapper_15800") at "StatefulPartitionedCall")): see current operation: %456 = "tf.TensorListReserve"(%67, %77) {device = ""} : (tensor, tensor) -&gt; tensor&lt;!tf.variant&lt;tensor&lt;*xf32&gt;&gt;&gt;
	</description>
	<comments>
		<comment id='1' author='twkx' date='2020-09-24T00:58:14Z'>
		Sorry for encountering this issue. Current TFLite MLIR converter is able to handle 1-D TensorLists only. After loading saved model, you might need to specify input shapes of the model to make the TensorLists proper shapes in Python.
		</comment>
		<comment id='2' author='twkx' date='2020-09-24T07:37:05Z'>
		&lt;denchmark-link:https://github.com/abattery&gt;@abattery&lt;/denchmark-link&gt;
 Thanks for your quick answer.
I tried the following code, and it gives exactly the same previous error (which is normal):
&lt;denchmark-code&gt;converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
tflite_model = converter.convert()
open("model.tflite", "wb").write(tflite_model)
&lt;/denchmark-code&gt;

then I modified it as you requested, but I now have a different error:
&lt;denchmark-code&gt;saved_model = tf.saved_model.load("centernet_resnet50_v1_fpn_512x512_kpts_coco17_tpu-8/saved_model")
concrete_func = saved_model.signatures["serving_default"]
concrete_func.inputs[0].set_shape([1, 512, 512, 3])
converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])
tflite_model = converter.convert()
open("model.tflite", "wb").write(tflite_model)
&lt;/denchmark-code&gt;

020-09-24 09:36:47.818409: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:315] Ignored output_format.
2020-09-24 09:36:47.818448: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:318] Ignored drop_control_dependency.
loc("Func/StatefulPartitionedCall/input/_0"): error: requires all operands and results to have compatible element types
Traceback (most recent call last):
File "/usr/local/lib/python3.8/site-packages/tensorflow/lite/python/convert.py", line 196, in toco_convert_protos
model_str = wrap_toco.wrapped_toco_convert(model_flags_str,
File "/usr/local/lib/python3.8/site-packages/tensorflow/lite/python/wrap_toco.py", line 32, in wrapped_toco_convert
return _pywrap_toco_api.TocoConvert(
Exception: :0: error: loc("Func/StatefulPartitionedCall/input/_0"): requires all operands and results to have compatible element types
:0: note: loc("Func/StatefulPartitionedCall/input/_0"): see current operation: %1 = "tf.Identity"(%arg0) {device = ""} : (tensor&lt;1x512x512x3x!tf.quint8&gt;) -&gt; tensor&lt;1x512x512x3xui8&gt;
&lt;denchmark-code&gt;During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "export_model.py", line 13, in &lt;module&gt;
    main()
  File "export_model.py", line 9, in main
    tflite_model = converter.convert()
  File "/usr/local/lib/python3.8/site-packages/tensorflow/lite/python/lite.py", line 1135, in convert
    return super(TFLiteConverterV2, self).convert()
  File "/usr/local/lib/python3.8/site-packages/tensorflow/lite/python/lite.py", line 960, in convert
    return super(TFLiteFrozenGraphConverterV2,
  File "/usr/local/lib/python3.8/site-packages/tensorflow/lite/python/lite.py", line 641, in convert
    result = _toco_convert_impl(
  File "/usr/local/lib/python3.8/site-packages/tensorflow/lite/python/convert.py", line 594, in toco_convert_impl
    data = toco_convert_protos(
  File "/usr/local/lib/python3.8/site-packages/tensorflow/lite/python/convert.py", line 202, in toco_convert_protos
    raise ConverterError(str(e))
tensorflow.lite.python.convert.ConverterError: &lt;unknown&gt;:0: error: loc("Func/StatefulPartitionedCall/input/_0"): requires all operands and results to have compatible element types
&lt;unknown&gt;:0: note: loc("Func/StatefulPartitionedCall/input/_0"): see current operation: %1 = "tf.Identity"(%arg0) {device = ""} : (tensor&lt;1x512x512x3x!tf.quint8&gt;) -&gt; tensor&lt;1x512x512x3xui8&gt;
&lt;/denchmark-code&gt;

I do understand that the tensor has not the expected type (quint8 vs ui8) but I didn't find how to modify it...
Thanks for your help,
		</comment>
		<comment id='3' author='twkx' date='2020-10-01T12:44:28Z'>
		still crashing with tf-nightly-2.4.0.dev20200930
		</comment>
		<comment id='4' author='twkx' date='2020-10-04T13:12:35Z'>
		Was able to reproduce the issue with &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/09d4c59e95cf33664294291c9d9e3647/43495.ipynb&gt;TF v2.3&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/4305e4aacd3fef25bef156da96c0617b/43495-tf-nightly.ipynb&gt;TF-nightly&lt;/denchmark-link&gt;
. Please find the attached gist. Thanks!
		</comment>
		<comment id='5' author='twkx' date='2020-10-15T07:11:05Z'>
		Hi,
I have made a simple test: I have downloaded every models in "TensorFlow 2 Detection Model Zoo" and tried to convert them with tflite_convert.
It appears that only 4 models failed to convert:
centernet_hg104_1024x1024_kpts_coco17_tpu-32
centernet_hg104_512x512_kpts_coco17_tpu-32
centernet_resnet50_v1_fpn_512x512_kpts_coco17_tpu-8
centernet_resnet50_v2_512x512_kpts_coco17_tpu-8
The common point in those models is that they are the ones using both centernet AND keypoints.
I hope it will give you some clues to investigate...
		</comment>
		<comment id='6' author='twkx' date='2020-10-15T14:57:42Z'>
		Hi TensorFlow Team,
It seems that the bug was on the side of object_detection library.
It has been fixed in commit baacb20:
&lt;denchmark-link:https://github.com/tensorflow/models/commit/baacb20d15066935e4a23c09b1c1a6843331172f#diff-ee421de3daf8a640c16b7f6f5a5d3399d28ded3a5c073b9ee10e5e2a88b7e94f&gt;tensorflow/models@baacb20#diff-ee421de3daf8a640c16b7f6f5a5d3399d28ded3a5c073b9ee10e5e2a88b7e94f&lt;/denchmark-link&gt;

I have trained and exported successfully a centernet with keypoints model with the new object_detection library and it's now ok.
As long as the models in Model Zoo will be trained again with new object_detection framework, they will be ok too!
Thanks for your support
		</comment>
		<comment id='7' author='twkx' date='2020-10-15T14:57:49Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43495&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43495&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='twkx' date='2020-10-16T09:33:58Z'>
		
Hi TensorFlow Team,
It seems that the bug was on the side of object_detection library.
It has been fixed in commit baacb20:
tensorflow/models@baacb20#diff-ee421de3daf8a640c16b7f6f5a5d3399d28ded3a5c073b9ee10e5e2a88b7e94f
I have trained and exported successfully a centernet with keypoints model with the new object_detection library and it's now ok.
As long as the models in Model Zoo will be trained again with new object_detection framework, they will be ok too!
Thanks for your support

Hey, can you please tell me how to convert this model to tflite because I still can not do it ?
		</comment>
		<comment id='9' author='twkx' date='2020-10-16T13:14:33Z'>
		
Hi TensorFlow Team,
It seems that the bug was on the side of object_detection library.
It has been fixed in commit baacb20:
tensorflow/models@baacb20#diff-ee421de3daf8a640c16b7f6f5a5d3399d28ded3a5c073b9ee10e5e2a88b7e94f
I have trained and exported successfully a centernet with keypoints model with the new object_detection library and it's now ok.
As long as the models in Model Zoo will be trained again with new object_detection framework, they will be ok too!
Thanks for your support

Hello &lt;denchmark-link:https://github.com/twkx&gt;@twkx&lt;/denchmark-link&gt;
,
I have the same issue as you, can you give some more detailed information on how you fixed this issue?
I have checked out to the commit "baacb20" but it has not fixed my issue!
&lt;denchmark-code&gt;During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "export_tflite_tf2.py", line 13, in &lt;module&gt;
    tflite_model = converter.convert()
  File "/home/ardit/repos/solaborate/Solaborate.ML/solaborate_packages/model_training/tf_env/lib/python3.6/site-packages/tensorflow/lite/python/lite.py", line 1135, in convert
    return super(TFLiteConverterV2, self).convert()
  File "/home/ardit/repos/solaborate/Solaborate.ML/solaborate_packages/model_training/tf_env/lib/python3.6/site-packages/tensorflow/lite/python/lite.py", line 961, in convert
    self).convert(graph_def, input_tensors, output_tensors)
  File "/home/ardit/repos/solaborate/Solaborate.ML/solaborate_packages/model_training/tf_env/lib/python3.6/site-packages/tensorflow/lite/python/lite.py", line 644, in convert
    **converter_kwargs)
  File "/home/ardit/repos/solaborate/Solaborate.ML/solaborate_packages/model_training/tf_env/lib/python3.6/site-packages/tensorflow/lite/python/convert.py", line 613, in toco_convert_impl
    enable_mlir_converter=enable_mlir_converter)
  File "/home/ardit/repos/solaborate/Solaborate.ML/solaborate_packages/model_training/tf_env/lib/python3.6/site-packages/tensorflow/lite/python/convert.py", line 216, in toco_convert_protos
    raise ConverterError(str(e))
tensorflow.lite.python.convert.ConverterError: &lt;unknown&gt;:0: error: loc("Func/StatefulPartitionedCall/input/_0"): requires all operands and results to have compatible element types
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='twkx' date='2020-10-16T18:45:00Z'>
		Hi Everyone,
We have a change in progress to fix this from our side too.
To explain the problem,
Your model has input with TF type QUINT8, TFLite doesn't understands this type, since this type lacks all quantization information that TFLite relies on.
So instead, if you can update your model to use UINT8 instead then it should work.
We are working to include this change inside the converter if we saw it, so we can handle these cases without users need to change the model.
Thanks
		</comment>
	</comments>
</bug>