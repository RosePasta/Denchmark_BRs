<bug id='26256' author='benleetownsend' open_date='2019-03-01T13:31:45Z' closed_time='2019-10-14T15:16:23Z'>
	<summary>Contrib AdaMax implementation producing NaNs on GPU</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu
TensorFlow installed from (source or binary): Source
TensorFlow version (use command below): Tested on 1.12 and 1.13
Python version: 3.6
CUDA/cuDNN version: 9
GPU model and memory: verified on 1080ti and titan v

Describe the current behavior
On GPU AdaMax from tf.contrib.opt.AdaMaxOptimizer appears to apply NaNs to variables. Seems fine on CPU and bizarrely if any ops are put as a control_dependency to the apply_grads call then everything seems fine.
Describe the expected behavior
Not to produce NaNs.
Code to reproduce the issue
&lt;denchmark-code&gt;import tensorflow as tf

a = tf.get_variable("a", shape=[10000])
b = tf.get_variable("b", shape=[10000])
loss = tf.minimum(tf.reduce_mean(a), tf.reduce_mean(b))
sess = tf.Session()

print("===== NORMALLY ======")

opt_op= tf.contrib.opt.AdaMaxOptimizer().minimize(loss)
sess.run(tf.global_variables_initializer())
for a in range(10):
    print(sess.run([opt_op, loss])[1])

print("===== WITH NOOP ======")

opt = tf.contrib.opt.AdaMaxOptimizer()
grads_and_vars = opt.compute_gradients(loss)
noop = tf.no_op()
    
with tf.control_dependencies([noop]):
    opt_op_fixed = opt.apply_gradients(grads_and_vars)

sess.run(tf.global_variables_initializer())
for a in range(10):
    print(sess.run([opt_op, loss])[1])
&lt;/denchmark-code&gt;

The above code produces on my machine:
&lt;denchmark-code&gt;===== NORMALLY ======
-3.637023e-05
-0.00037572667
nan
nan
nan
nan
nan
nan
nan
nan
===== WITH NOOP ======
-9.1626454e-05
-0.00018933268
-0.00028703889
-0.00038474516
-0.0004824514
-0.0005801577
-0.0006778639
-0.00077557017
-0.0008732763
-0.00097098254
&lt;/denchmark-code&gt;

Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='benleetownsend' date='2019-03-01T13:46:43Z'>
		I can reproduce on GPU device on tf 1.11.0 as well.
		</comment>
		<comment id='2' author='benleetownsend' date='2019-03-02T10:14:16Z'>
		Thank you for pointing it out. I'll take a look at it.
		</comment>
		<comment id='3' author='benleetownsend' date='2019-03-12T05:20:01Z'>
		Hi, could you take a try &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adamax&gt;https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adamax&lt;/denchmark-link&gt;
 ?  Because tf.contrib has been deprecated, and tf.contrib.Adamax will be deleted (replaced by tf.keras.AdaMax). Thank you :-)
		</comment>
		<comment id='4' author='benleetownsend' date='2019-04-29T14:50:41Z'>
		In TF 2.0, tf.keras.optimizers.Adamax (and I think Adam as well) has the same problem. I had to use a different optimizer (SDG) in order to get mobilenet_v2 to train and not produce a NaN loss
		</comment>
		<comment id='5' author='benleetownsend' date='2019-04-29T16:36:33Z'>
		&lt;denchmark-link:https://github.com/benleetownsend&gt;@benleetownsend&lt;/denchmark-link&gt;
 I could not reproduce this with TF1.12.0 (gist is &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/aee69fe92d830ba63f1f8b9b706d28f8/untitled118.ipynb&gt;here&lt;/denchmark-link&gt;
). But TF1.13.1 still has the issue (gist is &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/a38562b050931db33b09bddc34451ded/untitled117.ipynb&gt;here&lt;/denchmark-link&gt;
). Both the gists were created using Google colab with GPU. Thanks!
		</comment>
		<comment id='6' author='benleetownsend' date='2019-04-29T22:26:55Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 Vishnuvardhan, as &lt;denchmark-link:https://github.com/maziello&gt;@maziello&lt;/denchmark-link&gt;
 reported, keras Adamax has the same problem, could you take a try to reproduce it? I think we should address it in keras optimizer module, rather than contrib module.
		</comment>
		<comment id='7' author='benleetownsend' date='2019-05-05T23:32:56Z'>
		Here, with tensorflow 2 (tf.keras):

Adamax: CPU is ok and GPU NaN
Adam: CPU is ok and GPU is ok

		</comment>
		<comment id='8' author='benleetownsend' date='2019-06-24T14:01:05Z'>
		Also have this problem. The other keras optimizers work just fine, Adamax works on CPU but not on GPU, using tensorflow 2.
		</comment>
		<comment id='9' author='benleetownsend' date='2019-06-25T05:38:24Z'>
		&lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/case540&gt;@case540&lt;/denchmark-link&gt;
 Can you take a look? Thanks.
		</comment>
		<comment id='10' author='benleetownsend' date='2019-06-25T15:04:59Z'>
		Can you try Tensorflow 1.14 with tf.keras.optimizers.Adamax?
		</comment>
		<comment id='11' author='benleetownsend' date='2019-06-25T16:14:39Z'>
		Nan loss there as well. If it helps, I'm using a network with an embedding layer and LSTM (CuDNNLSTM), so I assume it could be a problem with either the dense or the sparse update.
		</comment>
		<comment id='12' author='benleetownsend' date='2019-06-25T19:54:57Z'>
		Hmm...does the same thing happen on CPU as well?
		</comment>
		<comment id='13' author='benleetownsend' date='2019-06-25T20:16:49Z'>
		Nope, Adamax works just fine on the CPU. To elaborate: it happens immediately* and every time with Adamax on the GPU, and never with Adamax on the CPU. Confirmed just now in colab with TF 1.14.
* On the second batch, after updating the weights once, the loss is nan.
It's not a problem for me at the moment, though. I switched to LazyAdam from tensorflow addons, which also handles sparse updates well.
		</comment>
		<comment id='14' author='benleetownsend' date='2019-10-08T23:36:56Z'>
		Hello, I have this issue on GPU AWS 1.14.0 using tf.keras.optimizers.Adamax recently which did not occur prior to the update when they were still using 1.13.1
		</comment>
		<comment id='15' author='benleetownsend' date='2019-10-09T00:29:37Z'>
		1.13.1 probably doesn't have GPU kernel that's why it got relocated to CPU.
The GPU kernel was probably introduced after 1.14, which had a major bug.
This is fixed on a 09/11 code change and should be reflected in either 1.15 rc3, or 2.0.
Please try that and let us know if that fixed your issue.
		</comment>
		<comment id='16' author='benleetownsend' date='2019-10-10T04:01:47Z'>
		I tried it on 2.0 on my local machine and it works on GPU. However, AWS is still stuck on 1.14.0 for now which sucks and the bug is still there.
		</comment>
		<comment id='17' author='benleetownsend' date='2019-10-10T15:31:01Z'>
		&lt;denchmark-link:https://github.com/Edwin-Koh1&gt;@Edwin-Koh1&lt;/denchmark-link&gt;
 by "it works on GPU" you mean the NAN is gone, correct?
		</comment>
		<comment id='18' author='benleetownsend' date='2019-10-14T05:51:08Z'>
		Yes, the NAN's are gone for Adamax optimizer on GPU training on tensorflow 2.0.0 but still persists on 1.14
		</comment>
		<comment id='19' author='benleetownsend' date='2019-10-14T15:16:23Z'>
		Ok. I'm gonna close this for now. AWS could update to 1.15 later since that should get fixed.
		</comment>
		<comment id='20' author='benleetownsend' date='2019-10-14T15:16:24Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26256&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26256&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='21' author='benleetownsend' date='2020-02-15T10:10:39Z'>
		Still having this problem on 1.15 is there any solution besides updating to 2.0.0?
		</comment>
		<comment id='22' author='benleetownsend' date='2020-02-15T11:14:53Z'>
		
Still having this problem on 1.15 is there any solution besides updating to 2.0.0?

Did you try tf.contrib optimizer or tf.keras optimizer?
		</comment>
		<comment id='23' author='benleetownsend' date='2020-02-19T04:47:24Z'>
		Yes, but for my specific use case I need the AdaMax optimizer to inherit tf.train.Optimizer which the tf.keras optimizer isn't doing.
		</comment>
		<comment id='24' author='benleetownsend' date='2020-10-12T22:40:57Z'>
		I faced the same inf validation loss using LSTMs with tf 1.15 and GPUs but not with CPUs. Once I switched from Adamax to Adam, it runs great on GPUs. So perhaps, the Adamax bug is only fixed on tf 2+
		</comment>
	</comments>
</bug>