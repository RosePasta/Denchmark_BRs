<bug id='16147' author='louisquinn' open_date='2018-01-16T06:56:15Z' closed_time='2018-07-11T02:35:12Z'>
	<summary>Inference on V100 with TF1.5 is extremely slow.</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
TensorFlow installed from (source or binary): Source and Virtual Env, problem doesn't change
TensorFlow version (use command below): 1.5.0-rc1 (Makes no difference on 1.4)
Python version: 3.5
Bazel version (if compiling from source): 0.9.0
GCC/Compiler version (if compiling from source): 5.4.0
CUDA/cuDNN version: CUDA 9/7.0.5
GPU model and memory: V100 - 16GB
Exact command to reproduce:

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;


Inference using the V100 is very slow. For example performing object detection with SSD Mobilenet is achieving a max frame rate of ~8, compared to ~45 on a GTX1080
Initialization with a warm up image is extremely slow - up to 2 mins for the first image.
I have tried model quantization using graph_transforms/transform_graph (in an attempt to use the FP16 mode) and various combinations of CUDA, cuDNN and Tensorflow versions with no difference.

Is there some recommended environment setup for the V100?
I am successfully running Darknet (&lt;denchmark-link:https://pjreddie.com/darknet/&gt;https://pjreddie.com/darknet/&lt;/denchmark-link&gt;
) with a massive increase of speed.
	</description>
	<comments>
		<comment id='1' author='louisquinn' date='2018-01-16T07:22:22Z'>
		CC &lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tfboyd&gt;@tfboyd&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/reedwm&gt;@reedwm&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='louisquinn' date='2018-01-16T07:46:37Z'>
		If it is only the first image that is very slow, it is very likely that your TensorFlow was not compiled with the right compute capability. So JIT is taking that much time. If you build TensorFlow from source, please make sure to enable "7.0" for V100 support. Thanks!
		</comment>
		<comment id='3' author='louisquinn' date='2018-01-16T08:16:28Z'>
		Hi Zheng,
Yes, it happens on the first image, but every image after that also experiences a much lower frame rate than my GTX1080.
And I did compile Tensorflow with compute capability 7.0
		</comment>
		<comment id='4' author='louisquinn' date='2018-01-18T04:32:17Z'>
		I have also attempted to use the pre-built wheels from here with no improvement:
&lt;denchmark-link:https://github.com/mind/wheels/releases/tag/tf1.4.1-gpu-cuda9&gt;https://github.com/mind/wheels/releases/tag/tf1.4.1-gpu-cuda9&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='louisquinn' date='2018-01-18T17:09:22Z'>
		&lt;denchmark-link:https://github.com/louisquinn&gt;@louisquinn&lt;/denchmark-link&gt;
   I work with XQ and want to reproduce your issue.  I read your comments but I am not 100% which example you are running.  I can run any inference and possibly reproduce the problem, but I would rather run exactly what you are doing so I can have a direct comparison.  Which TF script are you using, is an existing example or do you have code you can share publicly in github?  Thank you.
		</comment>
		<comment id='6' author='louisquinn' date='2018-01-18T23:36:05Z'>
		&lt;denchmark-link:https://github.com/tfboyd&gt;@tfboyd&lt;/denchmark-link&gt;
 Thanks for your response, and thanks in advance for your help!
Here is a gist I worked up which will show the speeds.
You just need to change the graph_path and test_image_path
&lt;denchmark-link:https://gist.github.com/louisquinn/796736446c39875bbdbf27b605cba33f&gt;https://gist.github.com/louisquinn/796736446c39875bbdbf27b605cba33f&lt;/denchmark-link&gt;

The script runs a dummy image as a warm up then a single image. I am getting similar FPS when using a video. Interestingly the V100 performs better than the 1080 for Faster RCNN w/ Inception Resnet V2, although I expect the speed to be much higher?
Attached is a screenshot of my nvidia-smi when the warm up image is running, it looks to be idle. When the test image is passed to the model, the GPU-Util jumps to &gt; 80%
&lt;denchmark-link:https://user-images.githubusercontent.com/5274490/35126671-93719ef0-fd02-11e7-8347-b27e5ca3a0b5.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='louisquinn' date='2018-01-19T06:24:18Z'>
		&lt;denchmark-link:https://github.com/louisquinn&gt;@louisquinn&lt;/denchmark-link&gt;
  Thank you.  My Friday looks a bit weird but I will try to get to this and have some response (like an update :-) on progress and what I am finding and thinking) before EOD Monday 22-JAN.  A complexity is loading the graph vs. the code and the object detection code is not maintained by the TensorFlow team.  I will see if I can discern something from the data I collect.  I am looking to setup a decent inference test and while this may not be it, the outcome may be useful to you and others.
		</comment>
		<comment id='8' author='louisquinn' date='2018-01-19T06:33:18Z'>
		&lt;denchmark-link:https://github.com/tfboyd&gt;@tfboyd&lt;/denchmark-link&gt;
 no worries thanks for taking the time! Just some more information, I am getting similar speeds and wait times when running the TF label_image example with bazel
		</comment>
		<comment id='9' author='louisquinn' date='2018-01-30T01:55:56Z'>
		Seconded ... we're using an Azure NCv3 instance (V100) with CUDA 9.1 and bazel-compiled (with "7.0") and it's about the same speed as my GTX950m laptop for inference. Stink.
We're running &lt;denchmark-link:https://github.com/davidsandberg/facenet&gt;facenet&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='10' author='louisquinn' date='2018-01-30T03:00:40Z'>
		Oh, &lt;denchmark-link:https://www.xcelerit.com/computing-benchmarks/insights/benchmarks-deep-learning-nvidia-p100-vs-v100-gpu/&gt;good find&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/NathanNZ&gt;@NathanNZ&lt;/denchmark-link&gt;
:

The reason for this disappointing performance is that the powerful Tensor Cores in the V100 are only used for matrix multiplications in half-precision (FP16) or mixed-precision mode.

I think our model is float32 ... I can't figure out an easy way to convert a pretrained model to float16.
		</comment>
		<comment id='11' author='louisquinn' date='2018-01-30T03:29:00Z'>
		It would not be slower than a GTX950, something else is likely wrong.  If you are looking to utilize the TensorCores or just in general get the best inference on GPU you will want to look at TensorRT.  It can take a graph and optimize it.  Right now there is a script from NVIDIA that does it, but also a &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/16253&gt;PR&lt;/denchmark-link&gt;
 in progress to build it in to TensorFlow.  If I was trying to use it right now I would likely use the NVIDIA script that I have not seen or used but understand it exists.  I suspect it is not as easy to use as one would hope but some internal teams have tried it and the results were decent thus the desire to make it native to TF.  Progress will happen quickly as this is a priority and the initial focus is on FP16 in Q1 and then INT8.   But as with all things they take time.
I am looking for a good example to benchmark for inference.  I would like it to use tf.data rather than feed_dict and prefer to have the model in code vs. loading from a saved graph.  I do not doubt your problems but I spent an entire afternoon looking for a cpu regression related to inference and the end result was I proved 1.5 was faster than 1.4 and the user realized they made a mistake.  Not a big deal and I was glad to gather the info but the example was not something I wanted to add to the benchmark so that made the work kind of throw away.
		</comment>
		<comment id='12' author='louisquinn' date='2018-01-30T07:00:15Z'>
		
something else is likely wrong ...

That's what we thought. We're running a standard benchmark suite (as we're actually evaluating hardware, and wanted to see what a V100 could do). That said, we've compiled from source, so it's possible something went wrong there. Anyway, it matches the description by the OP ... so maybe it is genuine (at least for out-of-the-box performance). See who else replies.

I am looking for a good example to benchmark for inference

Aren't there already benchmarks for TF (including ones that are rebuilt as part of CI workflow)? Might as well use them.
		</comment>
		<comment id='13' author='louisquinn' date='2018-01-31T05:20:34Z'>
		&lt;denchmark-link:https://github.com/tfboyd&gt;@tfboyd&lt;/denchmark-link&gt;

Thanks for your update.
Actually I am halfway through implementing TensorRT in Python, so I think I will hold off and wait for the official merge from the PR you mentioned.
Thanks so much for your help!
		</comment>
		<comment id='14' author='louisquinn' date='2018-02-08T10:44:17Z'>
		Any update on this issue ? I have also tried to use the V100 on my own code (tensorflow 1.5.0 cuda 9.0 cudnn 7) and did not see any speed improvement with respect to GTX 1080 Ti. I tried either float32 and float 16 without any difference. (but I am not sure exactly if the underlying operations were done in float16 since it seems that operations might still be running float32 even though the graphs/input are defined as float16)
I would be really interested to see any example of code that manages to use the Volta tensor cores power showing significant speed improvement with respect to previous generation of GPU (Pascal / Maxwell architecture)
		</comment>
		<comment id='15' author='louisquinn' date='2018-02-08T10:51:55Z'>
		&lt;denchmark-link:https://github.com/Rov67777&gt;@Rov67777&lt;/denchmark-link&gt;

I'm just following the progress of this &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/16253&gt;PR&lt;/denchmark-link&gt;

TensorRT optimises Tensorflow frozen graphs for Volta architectures. I haven't altered any code to use TensorRT yet, but I may implement a simple model with the demo as a guide, although I don't have access to a V100 at the moment.
		</comment>
		<comment id='16' author='louisquinn' date='2018-02-08T16:21:17Z'>
		&lt;denchmark-link:https://github.com/Rov67777&gt;@Rov67777&lt;/denchmark-link&gt;
   ResNet50 FP32 on V100 is ~344 image/sec.  FP16 665 images/sec.  A GTX 1080 ti is likely similar to the original Titan X (Pascal) and I would guess ~200-210 images/sec  because a P100 is ~230 images/sec.  I do not believe the GTX 1080 has strong FP16 support even for inference and no mixed-mode so training would not so well at all based on my understanding.
I do not know your use case but if you are looking for an FP16 example in the short term I have a link below.  For forward-pass only the results would be as dramatic but it would depend on how you are need to do inference. The benchmark script assumes a batch of images not data streaming in, which is a scenario I hope to have a better example for in the near future as I think it is common.  Meaning using tf.data where you provide some type of buffer to tf.data or something like that, sorry the idea is not complete, if you have a scenario please share and explain why it is interesting.
&lt;denchmark-link:https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks&gt;https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;# Real data
python tf_cnn_benchmarks.py --data_format=NCHW --batch_size=128 --num_batches=100 --model=resnet50 --data_dir=/data/imagenet --optimizer=sgd --variable_update=parameter_server --all_reduce_spec='' --use_fp16=True --nodistortions --local_parameter_device=cpu --num_gpus=1 --display_every=10

# Synthetic Data
python tf_cnn_benchmarks.py --data_format=NCHW --batch_size=128 --num_batches=100 --model=resnet50 --optimizer=sgd --variable_update=parameter_server --all_reduce_spec='' --use_fp16=True --nodistortions --local_parameter_device=cpu --num_gpus=1 --display_every=10
&lt;/denchmark-code&gt;

		</comment>
		<comment id='17' author='louisquinn' date='2018-02-23T13:59:25Z'>
		Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='18' author='louisquinn' date='2018-03-10T13:13:04Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='19' author='louisquinn' date='2018-03-25T12:35:58Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='20' author='louisquinn' date='2018-04-09T12:35:20Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='21' author='louisquinn' date='2018-04-24T18:40:01Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='22' author='louisquinn' date='2018-04-25T01:30:37Z'>
		Isn't this solved with TensorRT in 1.7?
		</comment>
		<comment id='23' author='louisquinn' date='2018-05-10T01:03:19Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='24' author='louisquinn' date='2018-05-27T02:55:40Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
: It has been 31 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='25' author='louisquinn' date='2018-06-10T18:50:00Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
: It has been 46 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='26' author='louisquinn' date='2018-06-25T19:13:31Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
: It has been 61 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='27' author='louisquinn' date='2018-07-10T19:04:57Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
: It has been 76 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='28' author='louisquinn' date='2018-12-20T00:01:03Z'>
		Has this been resolved? I am facing astronomically slow inference speeds 15s for a model with benchmark at 707ms &lt;denchmark-link:https://github.com/tfboyd&gt;@tfboyd&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tensorflowbutler&gt;@tensorflowbutler&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/louisquinn&gt;@louisquinn&lt;/denchmark-link&gt;

Looking forward to a response. Thanks!
		</comment>
	</comments>
</bug>