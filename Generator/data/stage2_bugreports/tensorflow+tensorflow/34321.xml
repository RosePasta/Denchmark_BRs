<bug id='34321' author='ShiroKL' open_date='2019-11-15T18:11:41Z' closed_time='2020-09-18T18:41:16Z'>
	<summary>Colab, TPU training using keras  InternalError: Failed to serialize message</summary>
	<description>
Hi,
I am trying to test TPU on colab in order to see how that works on keras.
I try to train 3 models. And differents bugs/abnormal behaviors occured :


when I trained a simple model ( see "create_model" function in colab) it works well with an image size of (100,100,3) but it does not work with (200,200,3). The batch size is about 80 so I doubt it is about an OOM error.


same as 1) with vgg19/inceptionresnetv2 + issue if I use the parameters imagenet="weights"


using tensorflow 2.0 as backend does not seem to work even with image size equal to (100, 100, 3)


These issue seems abnormal and the message does not help to understand how to solve them. Anyone know how to solve them or why they occured ?
Link to the code : &lt;denchmark-link:https://colab.research.google.com/drive/1z40RZOqBexmniel8m9KdgSa7tSFaUUcG&gt;https://colab.research.google.com/drive/1z40RZOqBexmniel8m9KdgSa7tSFaUUcG&lt;/denchmark-link&gt;

Message error :
1)
&lt;denchmark-code&gt;---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)
   1364     try:
-&gt; 1365       return fn(*args)
   1366     except errors.OpError as e:

13 frames
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1349       return self._call_tf_sessionrun(options, feed_dict, fetch_list,
-&gt; 1350                                       target_list, run_metadata)
   1351 

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)
   1442                                             fetch_list, target_list,
-&gt; 1443                                             run_metadata)
   1444 

InternalError: Failed to serialize message

During handling of the above exception, another exception occurred:

InternalError                             Traceback (most recent call last)
&lt;ipython-input-16-f1e92e5b4b78&gt; in &lt;module&gt;()
      2   X_train.astype(np.float32), Y_train.astype(np.float32),
      3   batch_size = 10*8,
----&gt; 4   epochs=5)#, validation_data=(x_test, y_test))
      5 
      6   #validation_data=(x_test.astype(np.float32), y_test.astype(np.float32)))

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    725         max_queue_size=max_queue_size,
    726         workers=workers,
--&gt; 727         use_multiprocessing=use_multiprocessing)
    728 
    729   def evaluate(self,

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
    617         validation_split=validation_split,
    618         shuffle=shuffle,
--&gt; 619         epochs=epochs)
    620     if not dist_utils.is_distributing_by_cloning(model):
    621       with model._distribution_strategy.scope():

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in _distribution_standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, validation_split, shuffle, epochs, allow_partial_batch)
   2286 
   2287         ds = strategy.extended.experimental_make_numpy_dataset(in_tuple,
-&gt; 2288                                                                session=session)
   2289         if shuffle:
   2290           # We want a buffer size that is larger than the batch size provided by

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py in experimental_make_numpy_dataset(self, numpy_input, session)
   1684     """
   1685     _require_cross_replica_or_default_context_extended(self)
-&gt; 1686     return self._experimental_make_numpy_dataset(numpy_input, session=session)
   1687 
   1688   def _experimental_make_numpy_dataset(self, numpy_input, session):

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/tpu_strategy.py in _experimental_make_numpy_dataset(self, numpy_input, session)
    247     return numpy_dataset.one_host_numpy_dataset(
    248         numpy_input, numpy_dataset.SingleDevice(self._host_device),
--&gt; 249         session)
    250 
    251   def _experimental_distribute_dataset(self, dataset):

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/numpy_dataset.py in one_host_numpy_dataset(numpy_input, colocate_with, session)
     86                       for i in numpy_flat)
     87   for v, i in zip(vars_flat, numpy_flat):
---&gt; 88     init_var_from_numpy(v, i, session)
     89   vars_nested = nest.pack_sequence_as(numpy_input, vars_flat)
     90   return dataset_ops.Dataset.from_tensor_slices(vars_nested)

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/numpy_dataset.py in init_var_from_numpy(input_var, numpy_input, session)
     70           start_placeholder: start,
     71           end_placeholder: end,
---&gt; 72           slice_placeholder: numpy_input[start:end]})
     73       start = end
     74 

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    954     try:
    955       result = self._run(None, fetches, feed_dict, options_ptr,
--&gt; 956                          run_metadata_ptr)
    957       if run_metadata:
    958         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1178     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1179       results = self._do_run(handle, final_targets, final_fetches,
-&gt; 1180                              feed_dict_tensor, options, run_metadata)
   1181     else:
   1182       results = []

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1357     if handle is None:
   1358       return self._do_call(_run_fn, feeds, fetches, targets, options,
-&gt; 1359                            run_metadata)
   1360     else:
   1361       return self._do_call(_prun_fn, handle, feeds, fetches)

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)
   1382                     '\nsession_config.graph_options.rewrite_options.'
   1383                     'disable_meta_optimizer = True')
-&gt; 1384       raise type(e)(node_def, op, message)
   1385 
   1386   def _extend_graph(self):

InternalError: Failed to serialize message
&lt;/denchmark-code&gt;




&lt;denchmark-code&gt;
---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)
   1364     try:
-&gt; 1365       return fn(*args)
   1366     except errors.OpError as e:

10 frames
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1349       return self._call_tf_sessionrun(options, feed_dict, fetch_list,
-&gt; 1350                                       target_list, run_metadata)
   1351 

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)
   1442                                             fetch_list, target_list,
-&gt; 1443                                             run_metadata)
   1444 

NotFoundError: From /job:worker/replica:0/task:0:
2 root error(s) found.
  (0) Not found: Resource worker/block1_conv1/bias/replica_7/N10tensorflow3VarE does not exist.
	 [[{{node TPUReplicateMetadata}}]]
  (1) Not found: Resource worker/block1_conv1/bias/replica_3/N10tensorflow3VarE does not exist.
	 [[{{node TPUReplicateMetadata}}]]
0 successful operations.
7 derived errors ignored.

During handling of the above exception, another exception occurred:

NotFoundError                             Traceback (most recent call last)
&lt;ipython-input-10-f1e92e5b4b78&gt; in &lt;module&gt;()
      2   X_train.astype(np.float32), Y_train.astype(np.float32),
      3   batch_size = 10*8,
----&gt; 4   epochs=5)#, validation_data=(x_test, y_test))
      5 
      6   #validation_data=(x_test.astype(np.float32), y_test.astype(np.float32)))

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    725         max_queue_size=max_queue_size,
    726         workers=workers,
--&gt; 727         use_multiprocessing=use_multiprocessing)
    728 
    729   def evaluate(self,

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
    668             steps_per_epoch=steps_per_epoch,
    669             validation_steps=validation_steps,
--&gt; 670             validation_freq=validation_freq)
    671 
    672     return training_arrays.fit_loop(

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py in experimental_tpu_fit_loop(model, dataset, epochs, verbose, callbacks, initial_epoch, steps_per_epoch, val_dataset, validation_steps, validation_freq)
    241         prev_step_count = step_count
    242       try:
--&gt; 243         _, outputs = K.batch_get_value([train_op, output_tensors])
    244       except errors.OutOfRangeError:
    245         logging.warning('Your dataset iterator ran out of data; '

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py in batch_get_value(tensors)
   3183     raise RuntimeError('Cannot get value inside Tensorflow graph function.')
   3184   if tensors:
-&gt; 3185     return get_session(tensors).run(tensors)
   3186   else:
   3187     return []

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    954     try:
    955       result = self._run(None, fetches, feed_dict, options_ptr,
--&gt; 956                          run_metadata_ptr)
    957       if run_metadata:
    958         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1178     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1179       results = self._do_run(handle, final_targets, final_fetches,
-&gt; 1180                              feed_dict_tensor, options, run_metadata)
   1181     else:
   1182       results = []

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1357     if handle is None:
   1358       return self._do_call(_run_fn, feeds, fetches, targets, options,
-&gt; 1359                            run_metadata)
   1360     else:
   1361       return self._do_call(_prun_fn, handle, feeds, fetches)

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)
   1382                     '\nsession_config.graph_options.rewrite_options.'
   1383                     'disable_meta_optimizer = True')
-&gt; 1384       raise type(e)(node_def, op, message)
   1385 
   1386   def _extend_graph(self):

NotFoundError: From /job:worker/replica:0/task:0:
2 root error(s) found.
  (0) Not found: Resource worker/block1_conv1/bias/replica_7/N10tensorflow3VarE does not exist.
	 [[node TPUReplicateMetadata (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]
  (1) Not found: Resource worker/block1_conv1/bias/replica_3/N10tensorflow3VarE does not exist.
	 [[node TPUReplicateMetadata (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]
0 successful operations.
7 derived errors ignored.

Original stack trace for 'TPUReplicateMetadata':
  File "/usr/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/usr/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py", line 16, in &lt;module&gt;
    app.launch_new_instance()
  File "/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py", line 664, in launch_instance
    app.start()
  File "/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py", line 477, in start
    ioloop.IOLoop.instance().start()
  File "/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py", line 832, in start
    self._run_callback(self._callbacks.popleft())
  File "/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py", line 605, in _run_callback
    ret = callback()
  File "/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py", line 536, in &lt;lambda&gt;
    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))
  File "/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py", line 450, in _handle_events
    self._handle_recv()
  File "/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py", line 480, in _handle_recv
    self._run_callback(callback, msg)
  File "/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py", line 432, in _run_callback
    callback(*args, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File "/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py", line 235, in dispatch_shell
    handler(stream, idents, msg)
  File "/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py", line 399, in execute_request
    user_expressions, allow_stdin)
  File "/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File "/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py", line 533, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py", line 2718, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py", line 2822, in run_ast_nodes
    if self.run_code(code, result):
  File "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py", line 2882, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "&lt;ipython-input-10-f1e92e5b4b78&gt;", line 4, in &lt;module&gt;
    epochs=5)#, validation_data=(x_test, y_test))
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py", line 727, in fit
    use_multiprocessing=use_multiprocessing)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py", line 670, in fit
    validation_freq=validation_freq)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py", line 196, in experimental_tpu_fit_loop
    initial_loop_values=initial_loop_values)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py", line 1756, in experimental_run_steps_on_iterator
    initial_loop_values)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/tpu_strategy.py", line 334, in _experimental_run_steps_on_iterator
    replicate_outputs = rewrite_fn()
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/tpu_strategy.py", line 315, in rewrite_fn
    run_fn, replicate_inputs, device_assignment=self._device_assignment)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tpu/tpu.py", line 639, in replicate
    maximum_shapes=maximum_shapes)[1]
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tpu/tpu.py", line 927, in split_compile_and_replicate
    num_replicas=num_replicas, use_tpu=use_tpu, **metadata_kwargs)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_tpu_ops.py", line 6105, in tpu_replicate_metadata
    name=name)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py", line 794, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 3357, in create_op
    attrs, op_def, compute_device)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 3426, in _create_op_internal
    op_def=op_def)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()
&lt;/denchmark-code&gt;




&lt;denchmark-code&gt;---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
&lt;ipython-input-10-f1e92e5b4b78&gt; in &lt;module&gt;()
      2   X_train.astype(np.float32), Y_train.astype(np.float32),
      3   batch_size = 10*8,
----&gt; 4   epochs=5)#, validation_data=(x_test, y_test))
      5 
      6   #validation_data=(x_test.astype(np.float32), y_test.astype(np.float32)))

13 frames
/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    726         max_queue_size=max_queue_size,
    727         workers=workers,
--&gt; 728         use_multiprocessing=use_multiprocessing)
    729 
    730   def evaluate(self,

/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_distributed.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
    683         validation_steps=validation_steps,
    684         validation_freq=validation_freq,
--&gt; 685         steps_name='steps_per_epoch')
    686 
    687   def evaluate(self,

/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)
    142       steps_per_epoch = training_utils.infer_steps_for_dataset(
    143           inputs, steps_per_epoch, epochs=epochs, steps_name=steps_name)
--&gt; 144     input_iterator = _get_iterator(inputs, model._distribution_strategy)
    145 
    146   # Enter tf.distribute.Strategy scope.

/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_arrays.py in _get_iterator(inputs, distribution_strategy)
    547   if distribution_strategy:
    548     return distributed_training_utils.get_iterator(
--&gt; 549         inputs, distribution_strategy)
    550   return training_utils.get_iterator(inputs)
    551 

/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/distribute/distributed_training_utils.py in get_iterator(dataset, distribution_strategy)
    585 def get_iterator(dataset, distribution_strategy):
    586   with distribution_strategy.scope():
--&gt; 587     iterator = distribution_strategy.make_dataset_iterator(dataset)
    588   initialize_iterator(iterator, distribution_strategy)
    589   return iterator

/tensorflow-2.0.0/python3.6/tensorflow_core/python/distribute/distribute_lib.py in make_dataset_iterator(self, dataset)
    559   def make_dataset_iterator(self, dataset):
    560     """DEPRECATED TF 1.x ONLY."""
--&gt; 561     return self._extended._make_dataset_iterator(dataset)  # pylint: disable=protected-access
    562 
    563   @doc_controls.do_not_generate_docs  # DEPRECATED: TF 1.x only

/tensorflow-2.0.0/python3.6/tensorflow_core/python/distribute/tpu_strategy.py in _make_dataset_iterator(self, dataset)
    225         self._input_workers,
    226         self._container_strategy(),
--&gt; 227         split_batch_by=self._num_replicas_in_sync)
    228 
    229   def _make_input_fn_iterator(

/tensorflow-2.0.0/python3.6/tensorflow_core/python/distribute/input_lib.py in __init__(self, dataset, input_workers, strategy, split_batch_by, input_context)
    760         strategy,
    761         split_batch_by=split_batch_by,
--&gt; 762         input_context=input_context)
    763     worker_iterators = _create_iterators_per_worker(
    764         dist_dataset._cloned_datasets, input_workers)  # pylint: disable=protected-access

/tensorflow-2.0.0/python3.6/tensorflow_core/python/distribute/input_lib.py in __init__(self, dataset, input_workers, strategy, split_batch_by, input_context)
    556         strategy,
    557         split_batch_by=split_batch_by,
--&gt; 558         input_context=input_context)
    559 
    560   def make_one_shot_iterator(self):

/tensorflow-2.0.0/python3.6/tensorflow_core/python/distribute/input_lib.py in __init__(self, dataset, input_workers, strategy, split_batch_by, input_context)
    518           # TODO(b/129506833): Figure out between graph cases
    519           cloned_dataset = input_ops.auto_shard_dataset(  # pylint: disable=protected-access
--&gt; 520               cloned_dataset, len(input_workers.worker_devices), i)
    521           self._cloned_datasets.append(cloned_dataset)
    522 

/tensorflow-2.0.0/python3.6/tensorflow_core/python/distribute/input_ops.py in auto_shard_dataset(dataset, num_shards, index)
     47       return distribute._AutoShardDatasetV1(dataset, num_shards, index)
     48     else:
---&gt; 49       return distribute._AutoShardDataset(dataset, num_shards, index)
     50   else:
     51     return dataset

/tensorflow-2.0.0/python3.6/tensorflow_core/python/data/experimental/ops/distribute.py in __init__(self, input_dataset, num_workers, index)
     54         num_workers=num_workers,
     55         index=index,
---&gt; 56         **self._flat_structure)
     57     super(_AutoShardDataset, self).__init__(input_dataset, variant_tensor)
     58 

/tensorflow-2.0.0/python3.6/tensorflow_core/python/ops/gen_experimental_dataset_ops.py in auto_shard_dataset(input_dataset, num_workers, index, output_types, output_shapes, name)
    169       else:
    170         message = e.message
--&gt; 171       _six.raise_from(_core._status_to_exception(e.code, message), None)
    172   # Add nodes to the TensorFlow graph.
    173   if not isinstance(output_types, (list, tuple)):

/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:worker/replica:0/task:0/device:CPU:0 in order to run AutoShardDataset: Unable to parse tensor proto
Additional GRPC error information:
{"created":"@1573841168.053370969","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Unable to parse tensor proto","grpc_status":3} [Op:AutoShardDataset]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='ShiroKL' date='2019-11-16T21:02:17Z'>
		HI, JUST CHECK THIS
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/29896&gt;#29896&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='ShiroKL' date='2019-11-16T21:02:32Z'>
		HOPE IT HELPS
		</comment>
		<comment id='3' author='ShiroKL' date='2019-11-18T18:20:46Z'>
		Hi &lt;denchmark-link:https://github.com/developer22-university&gt;@developer22-university&lt;/denchmark-link&gt;
 ,
thank for your reply. In factm that helps to answer my second question about the pretrained weights from imagenet.
		</comment>
		<comment id='4' author='ShiroKL' date='2019-12-17T06:38:42Z'>
		I'm getting the same InternalError, were you able to solve your problem &lt;denchmark-link:https://github.com/ShiroKL&gt;@ShiroKL&lt;/denchmark-link&gt;
 ?
		</comment>
		<comment id='5' author='ShiroKL' date='2020-01-16T00:58:04Z'>
		get same error  'Failed to serialize message'
use small dataset works well, use large dataset get error.
		</comment>
		<comment id='6' author='ShiroKL' date='2020-04-18T20:12:22Z'>
		&lt;denchmark-link:https://github.com/algoromeo&gt;@algoromeo&lt;/denchmark-link&gt;
  Sorry for the late response.
I have still the issue, as &lt;denchmark-link:https://github.com/ofpppppppdbfjs&gt;@ofpppppppdbfjs&lt;/denchmark-link&gt;
  said, we need to reduce the amount of data to feed your model. I have no idea why this issue occurs though.
		</comment>
		<comment id='7' author='ShiroKL' date='2020-05-18T22:54:03Z'>
		The issue exist with TF 2.2 version. &lt;denchmark-link:https://colab.research.google.com/gist/ymodak/4c500019b72b992568a5eb7ff36f111a/copy-of-tensorflow-2-0-handbook-tpu-example.ipynb&gt;GitHub gist&lt;/denchmark-link&gt;

Potential duplicate &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/35785&gt;#35785&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='ShiroKL' date='2020-07-13T23:01:36Z'>
		Reassigning to current TensorFlow TPU on-call &lt;denchmark-link:https://github.com/lzr-google&gt;@lzr-google&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='ShiroKL' date='2020-07-14T01:12:32Z'>
		One thing about TF2 TPU training is TPU initialization has to happen first before any dataset/model creation &lt;denchmark-link:https://www.tensorflow.org/guide/tpu#tpu_initialization&gt;https://www.tensorflow.org/guide/tpu#tpu_initialization&lt;/denchmark-link&gt;
, I noticed in this &lt;denchmark-link:https://colab.sandbox.google.com/gist/ymodak/4c500019b72b992568a5eb7ff36f111a/copy-of-tensorflow-2-0-handbook-tpu-example.ipynb#scrollTo=bjps3-L_55qt&gt;GitHub example&lt;/denchmark-link&gt;
 the TPU initialization happen much later, can you move it to the beginning of your code?
		</comment>
		<comment id='10' author='ShiroKL' date='2020-09-04T09:54:14Z'>
		&lt;denchmark-link:https://github.com/Shiro-LK&gt;@Shiro-LK&lt;/denchmark-link&gt;

Is this still an issue, Can you please update as per above comment.
		</comment>
		<comment id='11' author='ShiroKL' date='2020-09-11T10:16:26Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='12' author='ShiroKL' date='2020-09-11T10:35:08Z'>
		Hi, I have a new issue :
&lt;denchmark-code&gt;InternalError: Assigned device '/job:worker/replica:0/task:0/device:TPU:0' does not have registered OpKernel support for _Arg
	 [[{{node iterator_2}}]] [Op:__inference_train_function_3873]
&lt;/denchmark-code&gt;

I do not know what it means. Is there something wrong with the code ?
		</comment>
		<comment id='13' author='ShiroKL' date='2020-09-11T17:55:46Z'>
		Shiro, can you share your colab? Did you follow the instructions in &lt;denchmark-link:https://www.tensorflow.org/guide/tpu#tpu_initialization&gt;https://www.tensorflow.org/guide/tpu#tpu_initialization&lt;/denchmark-link&gt;
 to run TPU initialization at the beginning?
		</comment>
		<comment id='14' author='ShiroKL' date='2020-09-18T18:41:14Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='15' author='ShiroKL' date='2020-09-18T18:41:18Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34321&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34321&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>