<bug id='29958' author='8bitmp3' open_date='2019-06-19T10:32:45Z' closed_time='2020-01-31T00:04:06Z'>
	<summary>[TF 2.0 API Docs] `tf.keras.callbacks` (sub)classes</summary>
	<description>
TensorFlow version: 2.0 (beta1)
&lt;denchmark-h:h4&gt;(Sub)classes of tf.keras.callbacks:&lt;/denchmark-h&gt;

BaseLogger, History, Callback, CSVLogger, ModelCheckpoint, ProgbarLogger, RemoteMonitor, TensorBoard and TerminateOnNaN:
&lt;denchmark-h:h4&gt;Summary:&lt;/denchmark-h&gt;

Since tf.keras.callbacks are important for monitoring models during training, the API docs require extra attention imo.
Examples - to add - see below
Descriptions - to be defined better - see below (especially the Callback custom class)
Returns/raises - to add for better UX when needed
&lt;denchmark-h:h4&gt;Suggested improvements:&lt;/denchmark-h&gt;


Missing examples: one example per (sub)class would be enough for good UX and a link to a tutorial presents an extra step for a user. E.g. a short example inside the docs similar to the ones in EarlyStopping (link), ReduceLROnPlateau (link) or LambdaCallback (link)

Note: ModelCheckpoint and TensorBoard have links to tutorials which have examples. BaseLogger and History are applied by default but that may not help understand them better.
Example of a  callback from &lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;
 Deep Learning with Python book:
class ActivationLogger(keras.callbacks.Callback):
    def set_model(self, model):
        self.model = model # Called by the parent model before training, to inform the callback of what model will be calling it
        layer_outputs = [layer.output for layer in model.layers]
        self.activations_model = keras.models.Model(model.input, layer_outputs) # Model instance that returns the activations of every layer

    def on_epoch_end(self, epoch, logs=None):
        if self.validation_data is None:
                raise RuntimeError('Requires validation_data.')
        validation_sample = self.validation_data[0][0:1] # Obtains the first input sample of the validation data
        activations = self.activations_model.predict(validation_sample)
        f = open('activations_at_epoch_' + str(epoch) + '.npz', 'w') # Saves arrays to disk
        np.savez(f, activations)
        f.close()
Example of a  callback from &lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;
 Deep Learning with Python book:
callbacks = [
    keras.callbacks.TensorBoard(
        log_dir='my_log_dir', # Location of log files
        histogram_freq=1, # Records activation histogram every 1 epoch
        embeddings_freq=1, # Records embedding data every 1 epoch
    )
]

history = model.fit(x_train, y_train, 
                epochs=20, 
                batch_size=128, 
                validation_split=0.2, 
                callbacks=callbacks)

# Browse to http://localhost:6006 and look at your model training
...

Descriptions: to be defined better if needed. Recommend to use the following Medium post  which has quite decent descriptions of each callback: https://medium.com/singlestone/keras-callbacks-monitor-and-improve-your-deep-learning-205a8a27e91c

Note: Mention in the  and  descriptions that they are/should be both typically used together (see  example from &lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;
 with 2 callbacks passed into  below) to stop training when improvement stops and save the current best model during training ():
# A list of 2 or more callbacks that can be passed into `model.fit`
callbacks_list = [
        keras.callbacks.EarlyStopping(
                monitor='acc',
                patience=1,
        ),
        
        keras.callbacks.ModelCheckpoint( # Saves the current weights after every epoch
                filepath='my_model.h5',
                monitor='val_loss',
                save_best_only=True, # These two arguments mean you wonâ€™t overwrite the model file unless val_loss has improved
        )
]

model.compile(optimizer='rmsprop',
                loss='binary_crossentropy',
                metrics=['acc'])

model.fit(x, y,
                epochs=10,
                batch_size=32,
                callbacks=callbacks_list,
                validation_data=(x_val, y_val)
                )

Returns/Raises: to be defined/defined better if needed

&lt;denchmark-h:h4&gt;Doc links:&lt;/denchmark-h&gt;

&lt;denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/BaseLogger&gt;https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/BaseLogger&lt;/denchmark-link&gt;

&lt;denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/History&gt;https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/History&lt;/denchmark-link&gt;

&lt;denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/Callback&gt;https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/Callback&lt;/denchmark-link&gt;

&lt;denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/CSVLogger&gt;https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/CSVLogger&lt;/denchmark-link&gt;

&lt;denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/ModelCheckpoint&gt;https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/ModelCheckpoint&lt;/denchmark-link&gt;

&lt;denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/ProgbarLogger&gt;https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/ProgbarLogger&lt;/denchmark-link&gt;

&lt;denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/RemoteMonitor&gt;https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/RemoteMonitor&lt;/denchmark-link&gt;

&lt;denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/TensorBoard&gt;https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/TensorBoard&lt;/denchmark-link&gt;

&lt;denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/TerminateOnNaN&gt;https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/TerminateOnNaN&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='8bitmp3' date='2019-07-25T07:04:40Z'>
		&lt;denchmark-link:https://github.com/8bitmp3&gt;@8bitmp3&lt;/denchmark-link&gt;
, is there any possible way to dump model gradients and intermediate layer output through callback?
		</comment>
		<comment id='2' author='8bitmp3' date='2019-08-29T01:01:53Z'>
		how to do classificaiton using Fashion MNIST, a data set containing items of clothing. There's another, similar dataset called MNIST which has items of handwriting -- the digits 0 through 9.
Write an MNIST classifier that trains to 99% accuracy or above, and does it without a fixed number of epochs -- i.e. you should stop training once you reach that level of accuracy.
Some notes:
It should succeed in less than 10 epochs, so it is okay to change epochs= to 10, but nothing larger
When it reaches 99% or greater it should print out the string "Reached 99% accuracy so cancelling training!"
If you add any additional variables, make sure you use the same names as the ones used in the class
can you solve this
		</comment>
		<comment id='3' author='8bitmp3' date='2019-08-29T01:02:27Z'>
		import tensorflow as tf
from os import path, getcwd, chdir
&lt;denchmark-h:h1&gt;DO NOT CHANGE THE LINE BELOW. If you are developing in a local&lt;/denchmark-h&gt;

&lt;denchmark-h:h1&gt;environment, then grab mnist.npz from the Coursera Jupyter Notebook&lt;/denchmark-h&gt;

&lt;denchmark-h:h1&gt;and place it inside a local folder and edit the path to that location&lt;/denchmark-h&gt;

path = f"{getcwd()}/../tmp2/mnist.npz"
&lt;denchmark-h:h1&gt;GRADED FUNCTION: train_mnist&lt;/denchmark-h&gt;

class myCallback(tf.keras.callbacks.Callback):
def train_mnist(self, epoch, logs={}):
# Please write your code only where you are indicated.
# please do not remove # model fitting inline comments.
if(logs.get('acc')&gt;0.99):
print("\nReached 99% accuracy so cancelling training!")
self.model.stop_training = True
mnist = tf.keras.datasets.mnist
(x_train, y_train),(x_test, y_test) = mnist.load_data(path=path)
# YOUR CODE SHOULD START HERE
x_train, x_test = x_train / 255.0, x_test / 255.0
# YOUR CODE SHOULD END HERE
model = tf.keras.models.Sequential([
# YOUR CODE SHOULD START HERE
tf.keras.layers.Flatten(input_shape=(28, 28)),
tf.keras.layers.Dense(512, activation=tf.nn.relu),
tf.keras.layers.Dense(10, activation=tf.nn.softmax)
# YOUR CODE SHOULD END HERE
])
model.compile(optimizer='adam',
loss='sparse_categorical_crossentropy',
metrics=['accuracy'])
&lt;denchmark-code&gt;# model fitting
&lt;/denchmark-code&gt;

history = model.fit(x_train, y_train, epochs=10, callbacks=[callbacks])
# model fitting
return history.epoch, history.history['acc'][-1]
		</comment>
		<comment id='4' author='8bitmp3' date='2020-01-26T05:17:24Z'>
		I am using tensorflow==2.1 and when I ran this algorithm I got error saying this:
&lt;denchmark-link:https://user-images.githubusercontent.com/44919399/73130987-fca89e80-4028-11ea-85d7-b7e25c34e621.jpg&gt;&lt;/denchmark-link&gt;

Briefly it says that logs.get('acc') is Nonetype and 0.99 is float so, '&gt;' this cannot compare.
Can someone help me?
		</comment>
		<comment id='5' author='8bitmp3' date='2020-04-04T08:38:23Z'>
		
I am using tensorflow==2.1 and when I ran this algorithm I got error saying this:

Briefly it says that logs.get('acc') is Nonetype and 0.99 is float so, '&gt;' this cannot compare.
Can someone help me?

Same Issue here, any solution found?
		</comment>
		<comment id='6' author='8bitmp3' date='2020-04-04T11:54:28Z'>
		For tensorflow==2.1 it is not log.get('acc') instead try log.get('accuracy'). It should work.
		</comment>
		<comment id='7' author='8bitmp3' date='2020-04-04T14:09:02Z'>
		
I am using tensorflow==2.1 and when I ran this algorithm I got error saying this:

Briefly it says that logs.get('acc') is Nonetype and 0.99 is float so, '&gt;' this cannot compare.
Can someone help me?

Hi..
Use if(logs.get('acc')&gt;0.90):
This worked for me
		</comment>
		<comment id='8' author='8bitmp3' date='2020-04-05T13:16:57Z'>
		Hello..
For me it didn't, then I used 'accuracy' instead of 'acc', this worked for me.
		</comment>
		<comment id='9' author='8bitmp3' date='2020-06-27T03:29:25Z'>
		For tensorflow version 1.14.0, try log.get('acc')
And as per the comments, if tf version is more than 2, then try log.get('accuracy')
		</comment>
	</comments>
</bug>