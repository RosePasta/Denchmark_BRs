<bug id='28040' author='ajinkya933' open_date='2019-04-22T12:13:15Z' closed_time='2019-06-25T05:11:18Z'>
	<summary>tflite segmentation model throws error on android</summary>
	<description>
System information

What is the top-level directory of the model you are using: models-&gt;research -&gt;Deeplab
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
TensorFlow installed from (source or binary): source
TensorFlow version (use command below):1.13
Bazel version (if compiling from source): bazel-0.24.0-installer-darwin-x86_64
CUDA/cuDNN version: NA
GPU model and memory: No GPUs used
Exact command to reproduce:
tflite_convert 
--output_file=test.lite 
--graph_def_file=frozen_inference_graph.pb 
--input_arrays=ImageTensor 
--output_arrays=SemanticPredictions 
--input_shapes=1,600,450,3 
--inference_input_type=QUANTIZED_UINT8 
--inference_type=FLOAT 
--mean_values=128 
--std_dev_values=128

Describe the current behavior
I am using tflite for semantic segmentation. I have a model trained to segment objects from background, this model is trained on deeplab.
I have converted this model(frozen inference graph) into tflite format using the below code:
&lt;denchmark-code&gt;tflite_convert \
  --output_file=test.lite \
  --graph_def_file=frozen_inference_graph.pb \
  --input_arrays=ImageTensor \
  --output_arrays=SemanticPredictions \
  --input_shapes=1,600,450,3 \
  --inference_input_type=QUANTIZED_UINT8 \
  --inference_type=FLOAT \
  --mean_values=128 \
  --std_dev_values=128 
&lt;/denchmark-code&gt;

The model loads on android, but when I try to run inference it gives me this error:
&lt;denchmark-code&gt;    Caused by: java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: third_party/tensorflow/lite/kernels/unpack.cc:54 NumDimensions(input)

        1 was not true.Node number 4 (UNPACK) failed to prepare.

&lt;/denchmark-code&gt;

Code to reproduce the issue
tflite_convert 
--output_file=test.lite 
--graph_def_file=frozen_inference_graph.pb 
--input_arrays=ImageTensor 
--output_arrays=SemanticPredictions 
--input_shapes=1,600,450,3 
--inference_input_type=QUANTIZED_UINT8 
--inference_type=FLOAT 
--mean_values=128 
--std_dev_values=128
How do I resove this error?
	</description>
	<comments>
		<comment id='1' author='ajinkya933' date='2019-04-24T05:08:47Z'>
		I got to the next step by applying:
&lt;denchmark-code&gt;tflite_convert \
  --output_file=test2.lite \
  --graph_def_file=frozen_inference_graph_3mbvoc.pb \
  --input_arrays=ImageTensor \
  --output_arrays=SemanticPredictions \
  --input_shapes=1,450,600,3 \
  --inference_input_type=QUANTIZED_UINT8 \
  --inference_type=FLOAT \
  --mean_values=128 \
  --std_dev_values=128
&lt;/denchmark-code&gt;

The problem was in input shape mismatch. Even if this removes the error the graph generated by using above code does not produce any output.
&lt;denchmark-link:https://user-images.githubusercontent.com/17012391/56633675-bfb81180-667c-11e9-9cde-908655827e40.png&gt;&lt;/denchmark-link&gt;

The graph on left is &lt;denchmark-link:https://drive.google.com/file/d/1uWcOCAHBcYwwf0yQSIJLlrctFNSr91oq/view&gt;my tflight-graph&lt;/denchmark-link&gt;
. and the graph on right is googles tflight-graph.
How do I convert my tflight-graph to match that of &lt;denchmark-link:https://drive.google.com/file/d/1wJ9RMFuZqs_TOW27ZPK6feToqmnNbepw/view&gt;googles graph&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='ajinkya933' date='2019-04-27T12:48:38Z'>
		Give the input_arrays as sub_7 and output_arrays as ResizeBilinear_2. You will get the desired output.
		</comment>
		<comment id='3' author='ajinkya933' date='2019-04-29T05:55:36Z'>
		Sry.. It must be sub_2.. Just now checked your graph properly
		</comment>
		<comment id='4' author='ajinkya933' date='2019-04-29T06:07:09Z'>
		ya just realized that, it generates the graph now, im checking its results on android
		</comment>
		<comment id='5' author='ajinkya933' date='2019-04-29T06:50:03Z'>
		I converted it here (&lt;denchmark-link:https://drive.google.com/file/d/1A-FYCHGCcafyk66ALm_7_EVt7Zd31697/view?usp=sharing&gt;link&lt;/denchmark-link&gt;
)however when I run it on android (I am using &lt;denchmark-link:https://github.com/dailystudio/ml/tree/master/deeplab&gt;this&lt;/denchmark-link&gt;
 guys code to do that). I get an error :
Caused by: java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 810000 bytes and a ByteBuffer with 792588 bytes.
		</comment>
		<comment id='6' author='ajinkya933' date='2019-04-29T07:00:01Z'>
		This is because of the variation in the input and output sizes from the default GPU model. The value fixed by dailystudio is based on 224 as input and output size. You will have to find out what will be your corresponding values.
		</comment>
		<comment id='7' author='ajinkya933' date='2019-06-25T05:11:19Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=28040&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=28040&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>