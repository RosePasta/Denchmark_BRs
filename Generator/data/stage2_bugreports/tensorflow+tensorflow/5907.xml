<bug id='5907' author='gowithqi' open_date='2016-11-28T16:30:02Z' closed_time='2017-01-16T03:21:12Z'>
	<summary>FIFOQueue speed problem</summary>
	<description>
Hi I am testing the speed of FIFOQueue. I create a queue of Tensor with type of tf.float32 and shape of (576, 3, 220, 220). Then I push a tensor to the queue, followed by pop the tensor from the queue.
The speed of single node version( sess = tf.Session())  and distributed version of tensorflow (even in single machine single process scenario, sess = tf.Session(server.target) ) differs much.
Pushing and poping take about 0.2s in single node version while 4s in  distributed version.
I know that using distributed version of tensorflow will encounter some proto serialize overhead, but the overhead seems too much.
The main code is like this (I also give a link to full source code below, which can be used to reproduce the problem)
&lt;denchmark-code&gt;    raw_shape = [576, 3, 220, 220]
    shape = tf.TensorShape(raw_shape)
    if FLAGS.cluster:    
        server = tf.train.Server.create_local_server()
        sess = tf.Session(server.target)
    else:
        sess = tf.Session()
    with tf.device('/cpu:0'): 
        q = tf.FIFOQueue(10, tf.float32, shape)
        rand_data = tf.zeros(shape)
        init_op = tf.initialize_local_variables()
        sess.run(init_op)
        result = q.dequeue()
        x = tf.placeholder(tf.float32, shape, 'data')
        enqueue_op = q.enqueue(x)
        while True:
            log('pushing')
            sess.run(enqueue_op, feed_dict = {x: np.zeros(raw_shape)})
            log('push done')
            sess.run(result)
            log('pop done')
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?&lt;/denchmark-h&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/3009&gt;#3009&lt;/denchmark-link&gt;

But this issue is more related to threading, not same as mine case.
&lt;denchmark-h:h3&gt;Environment info&lt;/denchmark-h&gt;

Operating System:
Ubuntu 14.04
CPU E5-2643 v3 @ 3.40GHz
Installed version of CUDA and cuDNN:
None (I used CUDA_VISIBLE_DEVICES='')
If installed from source, provide

The commit hash (git rev-parse HEAD)
d6b2598
The output of bazel version
Build label: 0.4.0
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Nov 2 17:54:14 2016 (1478109254)
Build timestamp: 1478109254
Build timestamp as int: 1478109254

&lt;denchmark-h:h3&gt;If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)&lt;/denchmark-h&gt;

&lt;denchmark-link:https://gist.github.com/gowithqi/6bcb1dc50facfd992639f09e9af463a8&gt;https://gist.github.com/gowithqi/6bcb1dc50facfd992639f09e9af463a8&lt;/denchmark-link&gt;

Log
CUDA_VISIBLE_DEVICES='' python test_queue_release.py --cluster=true
&lt;denchmark-code&gt;2016-11-28 16:18:13.767 pushing
2016-11-28 16:18:13.998 push done
2016-11-28 16:18:14.112 pop done
2016-11-28 16:18:14.112 pushing
2016-11-28 16:18:14.337 push done
2016-11-28 16:18:14.501 pop done
2016-11-28 16:18:14.502 pushing
2016-11-28 16:18:14.746 push done
2016-11-28 16:18:14.916 pop done
2016-11-28 16:18:14.916 pushing
2016-11-28 16:18:15.155 push done
2016-11-28 16:18:15.269 pop done
&lt;/denchmark-code&gt;

CUDA_VISIBLE_DEVICES='' python test_queue_release.py --cluster=true
&lt;denchmark-code&gt;2016-11-28 16:17:00.218 pushing
2016-11-28 16:17:03.470 push done
2016-11-28 16:17:07.395 pop done
2016-11-28 16:17:07.395 pushing
2016-11-28 16:17:10.908 push done
2016-11-28 16:17:14.732 pop done
2016-11-28 16:17:14.733 pushing
2016-11-28 16:17:17.878 push done
2016-11-28 16:17:21.788 pop done
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='gowithqi' date='2016-11-29T22:19:11Z'>
		Thanks for the very detailed report.  Using it I was able to reproduce your problem and confirm that the speed difference is in fact due to protocol buffer construction and parsing.  At a detailed level, constructing and then parsing a protobuf involves a number of operations that are done separately on each atomic element of the data structure, in this case float32s.  As an experiment you might try changing float32 to int64 and halving one of the dimensions.  The number of bytes is the same, but now the protobuf handling speed should be decreased.   In this test case, for the purpose of higher performance it would be convenient if one could alias the Tensor as a single binary buffer of a particular length, then reinterpret that buffer as the properly typed Tensor on the other side of the grpc call.  Protobufs would support this kind of (reckless) type aliasing, but TensorFlow does not.
		</comment>
		<comment id='2' author='gowithqi' date='2016-11-30T06:49:38Z'>
		Thank you for the explanation. I still wonder some questions now.
1 If I want to implement the " alias the Tensor as a single binary buffer of a particular length" in the Tensorflow code,  can u tell me which code in the TF codebase should I modify(hack)?
2 While transferring big data via FIFOQueue is slow because of serializing overhead, I also tested the multi machine cluster distributed training speed, it seems that the speedup of multi machine training is almost linear, which indicates the data transferring overhead is small comparably. So my question is why transferring hundreds of MB of data by FIFOQueue is slow while not that slow by PS, both of which need proto serializing.
		</comment>
		<comment id='3' author='gowithqi' date='2016-11-30T23:35:21Z'>
		Good questions :)
Second one first: Why is there not a comparable slowdown due to protobuf serialization when talking to a parameter server shard?  Because we optimized the way in which Tensors are protobuf encoded-decoded at the RPC interface for the RecvTensor method.  A numeric Tensor is backed by a dense memory buffer of same-size binary fields.  See core/framework/tensor.proto for how as a protobuf a Tensor can be coded as a large collection of small items, or as a relatively untyped tensor_content byte array.  See the TensorResponse class and supporting code in core/distributed_runtime/tensor_coding.cc and distributed_runtime/rpc/grpc_tensor_coding.cc.  Whenever possible, when building the protobuf for sending a Tensor via RPC, we use a grpc::ByteBuffer to hold the content, rather than as a protobuf of lots of little items.
Back to your first question, using the tensor_content field of tensor.proto, or the TensorResponse interface defined by tensor_coding.h would be the way to go.  I think the issue you're running into with FIFOQUEUE with the distributed interface is that maybe the RPCs involved are not the optimized RecvTensor method, but some other RPC that doesn't have efficient coding.
		</comment>
		<comment id='4' author='gowithqi' date='2016-12-01T00:06:38Z'>
		&lt;denchmark-link:https://github.com/gowithqi&gt;@gowithqi&lt;/denchmark-link&gt;
 I've noticed same kind of problem, not limited to FIFOQueue -- specifically, when Python client requests data from another TensorFlow worker, the data is transferred at a rate of 50-200MB/s even if all the workers are local. The bottleneck seems to be protobuf decoding which happens on a single core. (more cores = smaller cores = worse performance)
Self-contained benchmark that reproduces it is &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/4498#issuecomment-248483967&gt;here&lt;/denchmark-link&gt;
.
So, in your case, your buffer is 576x3x220x220x4=334MB, so I would expect a single dequeue to take 3 seconds.
regarding on why "dequeue" transfer was faster than "ps" transfer -- there are two kinds of transfers:

Between TensorFlow processes (two C runtimes)
Between C runtime and Python runtime of the same process

So you could think of 4 different kind of scenarios on 2 dimensions:

is data transferred between devices in single process, or between multiple processes
result fetched in Python client vs remains in C runtime

So for single process mode on one machine, the speed I saw was 24GB/second (no Python fetch) vs 4GB/second (Python fetch), and for distributed version I saw 1 GB/second (no Python fetch) vs. 60 MB/second (Python fetch)
The way to run without Python transfer in your example is to do sess.run(dequeue_op.op) instead of sess.run(dequeue_op)
		</comment>
		<comment id='5' author='gowithqi' date='2016-12-01T00:17:49Z'>
		BTW, Marc O'Connoranalyzed this slowness (gRPC transfers bottlenecking at 80-200MB/second when you add large vector of 1's in one worker to variable on another local worker and fetch result into Python on original worker) and found following break-down.
Out of 1 second:

Nothing happens (0.2s +- 0.05): main process waits in grpc_competion_queue_pick, other threads wait in std::this_thread::__sleep_for(!)
Workers active (0.6s +-0.05 plus up to 0.2s overlap with next phase). 90% of time split time between:
a.	DoRecvTensor (7%), all spent in Tensor::AsProtoTensorContent calling std::string::_M_replace_safe.
b.	grpc_completion_queue_next(26%), split between post_parse_locked calling gpr_unref (11%) and memmove_ssse3_back (15%)
c.	DoRunGraph (34%) calling Tensor::AsProtoField (11% all spent in RepeatedField::Reserve), RunGraphResponse::ByteSize (7%) and grpc::Serialize (15%)
d.	grpc::Deserialize(23%)
Master merges data (0.2s +- 0.05s): All main thread time in TensorProto::MergePartialFromCodedStream, split evenly between self time, RepeatedField::Reserve and CodedInputStream::BytesUntilLimit. This partially overlaps with the previous phase.â€¨

		</comment>
		<comment id='6' author='gowithqi' date='2016-12-07T20:48:38Z'>
		&lt;denchmark-link:https://github.com/gowithqi&gt;@gowithqi&lt;/denchmark-link&gt;
 does avoiding the Python transfer as &lt;denchmark-link:https://github.com/yaroslavvb&gt;@yaroslavvb&lt;/denchmark-link&gt;
 suggested address your problem?
		</comment>
		<comment id='7' author='gowithqi' date='2017-01-16T03:21:12Z'>
		Closing because we haven't heard back from the OP in a while. &lt;denchmark-link:https://github.com/yaroslavvb&gt;@yaroslavvb&lt;/denchmark-link&gt;
 has offered good guidance here, and he also shared some helpful knowledge regarding queues in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/6845&gt;#6845&lt;/denchmark-link&gt;
. If you're still encountering problems &lt;denchmark-link:https://github.com/gowithqi&gt;@gowithqi&lt;/denchmark-link&gt;
 and believe there's a performance bug in TensorFlow, let us know, and I'll re-open.
		</comment>
	</comments>
</bug>