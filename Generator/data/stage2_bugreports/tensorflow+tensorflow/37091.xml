<bug id='37091' author='HendrikLaux' open_date='2020-02-26T12:51:14Z' closed_time='2020-03-13T22:24:04Z'>
	<summary>LookupError when calculating gradient of gradient with RNN on GPU</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): yes
OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS
TensorFlow installed from (source or
binary): binary (Anaconda/PIP)
TensorFlow version (use command below):
GIT_VERSION v1.12.1-25080-gca585e7
VERSION  2.2.0-dev20200218
Python version: 3.6.10
CUDA/cuDNN version: CUDA 10.2, cuDNN 7.6.2
GPU model and memory: GeForce RTX 2080 Ti, 12GB VRAM

Describe the current behavior
When trying to implement a gradient penalty for a WGAN-GP, which requires to calculate a gradient of a tensor, which itself depends on a gradient, the program terminates with a LookupError and tells me

LookupError: gradient registry has no entry for: CudnnRNNBackprop

This occurs only on the GPU version of Tensorflow and only if a recurrent layer is used (GRU or LSTM, doesn't matter). On the CPU, the provided minimum working example runs as expected. The error does not occur when training a "normal" model with a recurrent layer on the GPU (no gradient penalty).
I tried with tensorflow-gpu 2.0 (installed via conda), tensorflow-gpu 2.1 (installed via pip) and tf-nightly-gpu 2.2 (more specific: 2.2.0-dev20200218) (installed via pip). The error is the same for all GPU versions.
The operation works with tensorflow-gpu, if the parameter unroll of the GRU-Layer is set to True. Since this disables the usage of the cuDNN implementation of the GRU-Layer (deviation from default parameters), this does not solve the actual problem, but might indicate that there is just a small bug in the interface to cuDNN.
Standalone code to reproduce the issue
&lt;denchmark-code&gt;# this code does not make much sense, but is shorter than providing a full optimization loop for a WGAN_GP and produces the same error
import tensorflow as tf
import tensorflow.keras as k
import tensorflow.keras.layers as kl
import numpy as np

physical_devices = tf.config.experimental.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(physical_devices[0], True)

def gradient_penalty(model, input_data):
    # get gradient
    input_data = tf.convert_to_tensor(input_data)
    with tf.GradientTape() as t:
        t.watch(input_data)
        pred = model(input_data)
    grad = t.gradient(pred, [input_data])[0]
    # define gradient penalty
    slopes = tf.sqrt(tf.reduce_sum(tf.square(grad), axis=[1, 2]))
    gp = tf.reduce_mean((slopes - 1.) ** 2)
    return gp

if __name__ == "__main__":
    # model with recurrent layer
    model = k.Sequential([kl.InputLayer(input_shape=(50, 20)), kl.GRU(100), kl.Dense(1)])
    # Optimizer
    opt = tf.optimizers.Adam()
    # Dummy data
    data = np.random.normal(0, 1, (8, 50, 20)).astype(np.float32)
    # Optimize
    with tf.GradientTape() as tape:
        gp = gradient_penalty(model=model, input_data=data)
    grad = tape.gradient(gp, model.trainable_variables)
    opt.apply_gradients(zip(grad, model.trainable_variables))

&lt;/denchmark-code&gt;

Other info / logs
Full log:
&lt;denchmark-code&gt;/home/hendrik/anaconda3/envs/MLS22/bin/python /home/hendrik/PycharmProjects/GAN/MinimumMinimumWorkingExample.py
2020-02-26 13:30:08.149540: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-02-26 13:30:08.172329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-26 13:30:08.172601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2020-02-26 13:30:08.172716: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-02-26 13:30:08.173554: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-02-26 13:30:08.174413: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-02-26 13:30:08.174551: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-02-26 13:30:08.175425: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-02-26 13:30:08.175952: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-02-26 13:30:08.177916: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-02-26 13:30:08.177978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-26 13:30:08.178270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-26 13:30:08.178512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0
2020-02-26 13:30:08.187368: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-02-26 13:30:08.208283: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3600000000 Hz
2020-02-26 13:30:08.208513: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55cac97d38c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-02-26 13:30:08.208523: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-02-26 13:30:08.267483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-26 13:30:08.267804: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55cac97f6830 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-02-26 13:30:08.267814: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5
2020-02-26 13:30:08.267915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-26 13:30:08.268226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2020-02-26 13:30:08.268246: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-02-26 13:30:08.268253: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-02-26 13:30:08.268259: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-02-26 13:30:08.268265: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-02-26 13:30:08.268271: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-02-26 13:30:08.268276: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-02-26 13:30:08.268282: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-02-26 13:30:08.268308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-26 13:30:08.268554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-26 13:30:08.268778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0
2020-02-26 13:30:08.268795: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-02-26 13:30:08.269304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-02-26 13:30:08.269310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      0 
2020-02-26 13:30:08.269314: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0:   N 
2020-02-26 13:30:08.269362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-26 13:30:08.269613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-26 13:30:08.269856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1244] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9813 MB memory) -&gt; physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)
2020-02-26 13:30:08.667776: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-02-26 13:30:09.284739: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
Traceback (most recent call last):
  File "/home/hendrik/PycharmProjects/GAN/MinimumMinimumWorkingExample.py", line 31, in &lt;module&gt;
    grad = tape.gradient(gp, model.trainable_variables)
  File "/home/hendrik/anaconda3/envs/MLS22/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py", line 1048, in gradient
    unconnected_gradients=unconnected_gradients)
  File "/home/hendrik/anaconda3/envs/MLS22/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py", line 77, in imperative_grad
    compat.as_str(unconnected_gradients.value))
  File "/home/hendrik/anaconda3/envs/MLS22/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py", line 145, in _gradient_function
    grad_fn = ops._gradient_registry.lookup(op_name)  # pylint: disable=protected-access
  File "/home/hendrik/anaconda3/envs/MLS22/lib/python3.6/site-packages/tensorflow/python/framework/registry.py", line 97, in lookup
    "%s registry has no entry for: %s" % (self._name, name))
LookupError: gradient registry has no entry for: CudnnRNNBackprop

Process finished with exit code 1
&lt;/denchmark-code&gt;

Thanks in advance for your time!
	</description>
	<comments>
		<comment id='1' author='HendrikLaux' date='2020-02-27T08:25:46Z'>
		I have tried on colab with TF version 2.2.0-dev20200226 and was able to reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/d4f32851c12e3bb4329030a0dd0d297b/untitled681.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='HendrikLaux' date='2020-03-13T22:19:18Z'>
		Hi &lt;denchmark-link:https://github.com/HendrikLaux&gt;@HendrikLaux&lt;/denchmark-link&gt;
, Thanks for reporting the issue.
The root cause is that cudnnRNN kernel doesn't have higher order gradient support, which is the issue you are hitting. I am not sure if Nvidia is going to add high order gradient support the cudnn kernel, for now we can only work around the issue (eg use unroll=True).
		</comment>
		<comment id='3' author='HendrikLaux' date='2020-03-13T22:24:00Z'>
		Note that unroll requite the input to have a static time sequence. If you want to build your model with dynamic time sequence length, you can disable the cudnn backend by:
&lt;denchmark-code&gt;gru_layer = tf.keras.layers.GRU(10)
gru_layer._could_use_gpu_kernel = False.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='HendrikLaux' date='2020-03-13T22:24:06Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37091&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37091&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='HendrikLaux' date='2020-03-17T10:22:49Z'>
		Hey qlzh727,
thank you for the clarification of this issue! Do you think the issue is worth passing to Nvidia, or is the problem too minor and the use cases too limited for this?
		</comment>
		<comment id='6' author='HendrikLaux' date='2020-03-17T17:11:59Z'>
		@houtoms from Nvidia for this.
		</comment>
		<comment id='7' author='HendrikLaux' date='2020-04-08T08:49:45Z'>
		@houtoms &lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;

Any updates on this issue? 
		</comment>
		<comment id='8' author='HendrikLaux' date='2020-04-08T22:28:03Z'>
		Thanks for the update. Let me sync with our team and put some update here.
		</comment>
		<comment id='9' author='HendrikLaux' date='2020-04-10T20:36:16Z'>
		&lt;denchmark-link:https://github.com/HendrikLaux&gt;@HendrikLaux&lt;/denchmark-link&gt;
 We don't have it on our plan in the near term to support higher order gradient RNN. But I can help issue a RFE if you think this is needed because, for example, the performance of it is the bottleneck.
		</comment>
		<comment id='10' author='HendrikLaux' date='2020-05-22T03:16:28Z'>
		This support would be very nice to have.
		</comment>
	</comments>
</bug>