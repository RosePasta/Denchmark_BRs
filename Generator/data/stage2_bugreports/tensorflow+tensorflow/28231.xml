<bug id='28231' author='Jsevillamol' open_date='2019-04-28T15:07:02Z' closed_time='2019-05-01T15:28:26Z'>
	<summary>Tensor.graph is meaningless when eager execution is enabled. in TF 2.0, when compiling model</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): pip install tensorflow-gpu==2.0.0-alpha0
TensorFlow version (use command below): 2.0.0-alpha0
Python version: 3.7.3
CUDA/cuDNN version: 10.0
GPU model and memory: GeForce GTX 1080 Ti, 11175MiB

Describe the current behavior
After migrating to TF 2.0, when compiling a custom model I wrote I get an error AttributeError: Tensor.graph is meaningless when eager execution is enabled.
The model compiled as expected in TF 1.13
Describe the expected behavior
compile the model without raising any exceptions.

Run &lt;denchmark-link:https://gist.github.com/Jsevillamol/1db0121e4d8b2b95a13ea14f010169d0&gt;this script&lt;/denchmark-link&gt;
 passing as an argument &lt;denchmark-link:https://gist.github.com/Jsevillamol/94bbf97e062fff7d6cc95200a6e6d78a&gt;this file&lt;/denchmark-link&gt;
 to create an h5 keras model.
Then load_model the resulting h5 file and call model.compile('adam', 'categorical_crossentropy').
Other info / logs
&lt;denchmark-code&gt;In [5]: model.compile('adam', 'categorical_crossentropy', ['acc'])                                                                                           
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-5-9c84ced6c9fd&gt; in &lt;module&gt;
----&gt; 1 model.compile('adam', 'categorical_crossentropy', ['acc'])

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    454     self._setattr_tracking = False  # pylint: disable=protected-access
    455     try:
--&gt; 456       result = method(self, *args, **kwargs)
    457     finally:
    458       self._setattr_tracking = previous_value  # pylint: disable=protected-access

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)
    428       #                   loss_weight_2 * output_2_loss_fn(...) +
    429       #                   layer losses.
--&gt; 430       self.total_loss = self._prepare_total_loss(skip_target_indices, masks)
    431 
    432       # Functions for train, test and predict will

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _prepare_total_loss(self, skip_target_indices, masks)
   1729 
   1730       # Add regularization penalties and other layer-specific losses.
-&gt; 1731       if self.losses:
   1732         total_loss += losses_utils.scale_loss_for_distribution(
   1733             math_ops.add_n(self.losses))

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py in losses(self)
    664     with ops.init_scope():
    665       if context.executing_eagerly():
--&gt; 666         return [loss for loss in losses
    667                 if loss.graph == ops.get_default_graph()]
    668 

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py in &lt;listcomp&gt;(.0)
    665       if context.executing_eagerly():
    666         return [loss for loss in losses
--&gt; 667                 if loss.graph == ops.get_default_graph()]
    668 
    669     relevant_inputs = []

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in graph(self)
    937   def graph(self):
    938     raise AttributeError(
--&gt; 939         "Tensor.graph is meaningless when eager execution is enabled.")
    940 
    941   @property

AttributeError: Tensor.graph is meaningless when eager execution is enabled.
&lt;/denchmark-code&gt;

model.summary()
&lt;denchmark-code&gt;Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 4, 108, 108, 0                                            
__________________________________________________________________________________________________
conv2D_0 (TimeDistributed)      (None, 4, 108, 108,  320         input_1[0][0]                    
__________________________________________________________________________________________________
maxpool_0 (TimeDistributed)     (None, 4, 54, 54, 32 0           conv2D_0[0][0]                   
__________________________________________________________________________________________________
batchnorm_0 (TimeDistributed)   (None, 4, 54, 54, 32 128         maxpool_0[0][0]                  
__________________________________________________________________________________________________
conv2D_1 (TimeDistributed)      (None, 4, 54, 54, 32 9248        batchnorm_0[0][0]                
__________________________________________________________________________________________________
maxpool_1 (TimeDistributed)     (None, 4, 27, 27, 32 0           conv2D_1[0][0]                   
__________________________________________________________________________________________________
batchnorm_1 (TimeDistributed)   (None, 4, 27, 27, 32 128         maxpool_1[0][0]                  
__________________________________________________________________________________________________
conv2D_2 (TimeDistributed)      (None, 4, 27, 27, 64 18496       batchnorm_1[0][0]                
__________________________________________________________________________________________________
maxpool_2 (TimeDistributed)     (None, 4, 13, 13, 64 0           conv2D_2[0][0]                   
__________________________________________________________________________________________________
batchnorm_2 (TimeDistributed)   (None, 4, 13, 13, 64 256         maxpool_2[0][0]                  
__________________________________________________________________________________________________
conv2D_3 (TimeDistributed)      (None, 4, 13, 13, 12 73856       batchnorm_2[0][0]                
__________________________________________________________________________________________________
maxpool_3 (TimeDistributed)     (None, 4, 6, 6, 128) 0           conv2D_3[0][0]                   
__________________________________________________________________________________________________
batchnorm_3 (TimeDistributed)   (None, 4, 6, 6, 128) 512         maxpool_3[0][0]                  
__________________________________________________________________________________________________
flatten (TimeDistributed)       (None, 4, 4608)      0           batchnorm_3[0][0]                
__________________________________________________________________________________________________
dropout (TimeDistributed)       (None, 4, 4608)      0           flatten[0][0]                    
__________________________________________________________________________________________________
unified_lstm (UnifiedLSTM)      (None, 4, 2048)      54534144    dropout[0][0]                    
__________________________________________________________________________________________________
time_distributed (TimeDistribut (None, 4, 1)         2049        unified_lstm[0][0]               
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 4)            0           time_distributed[0][0]           
__________________________________________________________________________________________________
softmax (Softmax)               (None, 4)            0           flatten_1[0][0]                  
__________________________________________________________________________________________________
dot (Dot)                       (None, 2048)         0           unified_lstm[0][0]               
                                                                 softmax[0][0]                    
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 1024)         2098176     dot[0][0]                        
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 1024)         0           dense_1[0][0]                    
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 512)          524800      dropout_1[0][0]                  
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 512)          0           dense_2[0][0]                    
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 2)            1026        dropout_2[0][0]                  
==================================================================================================
Total params: 57,263,139
Trainable params: 57,262,627
Non-trainable params: 512
__________________________________________________________________________________________________
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Jsevillamol' date='2019-05-01T15:27:38Z'>
		I believe this bug has been fixed in nightly by &lt;denchmark-link:https://github.com/omalleyt12&gt;@omalleyt12&lt;/denchmark-link&gt;
 so upgrade to tf-nightly-2.0-preview or tf-nightly-gpu-2.0-preview and your code will work.
		</comment>
		<comment id='2' author='Jsevillamol' date='2019-05-01T15:28:26Z'>
		Specifically, this commit fixed it: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/809a0332eb53eedf1c3257f2a2b5556ca9320e56&gt;809a033&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='Jsevillamol' date='2019-05-01T15:28:28Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=28231&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=28231&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>