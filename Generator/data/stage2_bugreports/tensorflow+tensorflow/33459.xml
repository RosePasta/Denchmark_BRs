<bug id='33459' author='babaish' open_date='2019-10-17T11:14:50Z' closed_time='2020-02-28T22:29:59Z'>
	<summary>resnet50 imagenet weights are differents in tf.keras vs keras</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10, pycharm (same issue from colab)
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): pycharm
TensorFlow version (use command below): 2.0 and keras 2.3.0 (same issue with tf 1.15 and keras 2.2.5 in google colab)
Python version: 3.7

Describe the current behavior
The weights of the kernel in layers 13 &amp; 14 are different in tf.keras and keras: the weights of the layer 13 in tf.keras are the ones of the layers 14 in keras and the weights of the layer 14 in tf.keras are the ones of the layer 13 in keras:
with tf.keras:
layer 13: conv2_block1_0_conv
[[[[ 0.00460704  0.0613995   0.04595907 ... -0.12616335 -0.00781816
0.03271283]
...
[-0.00736084  0.00832207 -0.00591875 ... -0.16227128  0.00581011
0.01718325]]]]
layer 14: conv2_block1_3_conv
[[[[ 0.00412396 -0.01779881 -0.01002417 ... -0.0397268  -0.01897338
-0.00012411]
...
[ 0.01601992  0.00197976 -0.01605847 ...  0.06464136  0.0353195
0.02405972]]]]
with keras:
layer 13: res2a_branch2c
[[[[ 0.00412396 -0.01779881 -0.01002417 ... -0.0397268  -0.01897338
-0.00012411]
...
[ 0.01601992  0.00197976 -0.01605847 ...  0.06464136  0.0353195
0.02405972]]]]
layer 14: res2a_branch1
[[[[ 0.00460704  0.0613995   0.04595907 ... -0.12616335 -0.00781816
0.03271283]
...
[-0.00736084  0.00832207 -0.00591875 ... -0.16227128  0.00581011
0.01718325]]]]
this is with include_top=True but include_top=False gives the same thing
Describe the expected behavior
I would expect the same weights, not sure which weights are the right ones
Code to reproduce the issue
from tensorflow.python.keras.applications import ResNet50
#from keras.applications import ResNet50
image_size = 224
model = ResNet50(input_shape=(image_size, image_size, 3), include_top=True, weights='imagenet')
print(model.layers[13].name)
weights = model.layers[13].get_weights()[0]
print(weights)
print(model.layers[14].name)
weights = model.layers[14].get_weights()[0]
print(weights)

The difference is coming from the different path used to load the h5 file:
in tf.keras:
'&lt;denchmark-link:https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5&gt;https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5&lt;/denchmark-link&gt;
'
in keras:
'&lt;denchmark-link:https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels.h5&gt;https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels.h5&lt;/denchmark-link&gt;
'
	</description>
	<comments>
		<comment id='1' author='babaish' date='2019-10-18T06:30:29Z'>
		I could reproduce the issue with Tf 2.0.0 keras 2.3.0 and TF 1.15.0 keras 2.2.5. Please take a look at the gist of tf.keras &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/903f26e451e5b6d40ac5f99991716a25/untitled206.ipynb&gt;here&lt;/denchmark-link&gt;
 and keras with tensorflow backend &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/99de5876f3f6ff5937bb29c6ab1185e7/untitled207.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='babaish' date='2019-10-18T08:59:34Z'>
		&lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
 This does not answer anything. The issue still remains. Or is it that they are downloading two versions of resnet (v1 and v2) ?
		</comment>
		<comment id='3' author='babaish' date='2019-11-04T18:38:16Z'>
		Adding Francois for this. When did we update the weights last time?
		</comment>
		<comment id='4' author='babaish' date='2020-01-31T19:50:40Z'>
		Any updates on this please?
		</comment>
		<comment id='5' author='babaish' date='2020-02-02T13:16:59Z'>
		It seems that several blocks are affected. For example, in the conv5_ block the weights of the following layers are switched:

conv5_block1_3_bn and conv5_block1_0_bn
conv5_block1_3_conv and conv5_block1_0_conv.

This affects quite significantly the accuracy of transfer learning (on at least semantic segmentation in my case).
		</comment>
		<comment id='6' author='babaish' date='2020-02-04T07:37:48Z'>
		I guess &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/35336&gt;#35336&lt;/denchmark-link&gt;
 is a similar issue.
In my testing code which loading weights from tf.keras, the accuracy for pre-trained weights are under 70%.
		</comment>
		<comment id='7' author='babaish' date='2020-02-04T10:14:21Z'>
		&lt;denchmark-link:https://github.com/CNOCycle&gt;@CNOCycle&lt;/denchmark-link&gt;
 Nice. I guess we now have enough evidence that the pre-trained weights are broken.
		</comment>
		<comment id='8' author='babaish' date='2020-02-28T00:14:42Z'>
		I think tf.keras.application is exporting the latest implementation of resnet50, while keras.application is exporting the old version of implementation. They are structured differently (eg the layer may appear in different order), but functionality wise, they are the same.
I think the root cause is for keras.application not exporting the latest version is in &lt;denchmark-link:https://github.com/keras-team/keras-applications/blob/master/keras_applications/__init__.py&gt;https://github.com/keras-team/keras-applications/blob/master/keras_applications/__init__.py&lt;/denchmark-link&gt;
. Basically both resnet50.py and resnet.py will export Resnet50, and the former one is exported by contains the legacy implementation.
		</comment>
		<comment id='9' author='babaish' date='2020-02-28T22:29:53Z'>
		Checked with &lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;
 offline for this issue. I think the keras-team/keras-application was exporting the old model. The PR should fix the issue, but since the keras-application is not going to make any new release, I would suggest you to use the version in tf.keras.application, which should be latest and correct.
Btw, the keras-team/keras-application is still correct, it is just structured differently compare to the latest implementation. You can still use it for transfer learning.
		</comment>
		<comment id='10' author='babaish' date='2020-02-28T22:30:01Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33459&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33459&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='babaish' date='2020-03-02T11:29:22Z'>
		Not sure I fully understood the root cause, but I'm happy there's some progress on this.
The difference between tf.keras and keras does not seem to be an issue as they used two different implementations. But is it sure that the weights of tf.keras' ResNet50 are good? As &lt;denchmark-link:https://github.com/CNOCycle&gt;@CNOCycle&lt;/denchmark-link&gt;
 pointed out in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/35336&gt;#35336&lt;/denchmark-link&gt;
, the best accuracy he/she could get on ImageNet validation set is 45.7% for ResNet50, which is far from the &lt;denchmark-link:https://keras.io/applications/&gt;reported&lt;/denchmark-link&gt;
 74.9%.
&lt;denchmark-link:https://github.com/CNOCycle&gt;@CNOCycle&lt;/denchmark-link&gt;
 Would it be possible for you to try applying the above PR and performing your experiment again?
		</comment>
		<comment id='12' author='babaish' date='2020-03-02T13:08:46Z'>
		I do the test again in docker image: tensorflow/tensorflow:2.1.0-gpu-py3
and comment out the line as &lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;
 suggestion
/usr/local/lib/python3.6/dist-packages/keras_applications/__init__.py" 65L
-from . import resnet50
+#from . import resnet50
The following is the output of testing code which is shown in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/35336&gt;#35336&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5
102973440/102967424 [==============================] - 204s 2us/step
2020-03-02 12:37:29.774845: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-03-02 12:37:30.223375: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
500/500 [==============================] - 143s 287ms/step - loss: 2.6850 - accuracy: 0.4601
[Testing][pixel vales are from (0,255)][model:ResNet50] - loss: 2.685 - accuracy: 0.460
Downloading data from https://github.com/keras-team/keras-applications/releases/download/densenet/densenet121_weights_tf_dim_ordering_tf_kernels.h5
33193984/33188688 [==============================] - 107s 3us/step
500/500 [==============================] - 80s 159ms/step - loss: 39.1680 - accuracy: 0.0056
[Testing][pixel vales are from (0,255)][model:DenseNet121] - loss: 39.168 - accuracy: 0.006
Downloading data from https://github.com/JonathanCMitchell/mobilenet_v2_keras/releases/download/v1.1/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224.h5
14540800/14536120 [==============================] - 39s 3us/step
500/500 [==============================] - 41s 81ms/step - loss: 9.9651 - accuracy: 0.0026
[Testing][pixel vales are from (0,255)][model:MobileNetV2] - loss: 9.965 - accuracy: 0.003
500/500 [==============================] - 76s 151ms/step - loss: 8.5355 - accuracy: 0.0010
[Testing][pixel vales are from (0,1)][model:ResNet50] - loss: 8.535 - accuracy: 0.001
500/500 [==============================] - 71s 142ms/step - loss: 1.7376 - accuracy: 0.6029
[Testing][pixel vales are from (0,1)][model:DenseNet121] - loss: 1.738 - accuracy: 0.603
500/500 [==============================] - 45s 91ms/step - loss: 2.2516 - accuracy: 0.5285
[Testing][pixel vales are from (0,1)][model:MobileNetV2] - loss: 2.252 - accuracy: 0.528
500/500 [==============================] - 76s 152ms/step - loss: 8.4942 - accuracy: 0.0010
[Testing][pixel vales are normalized from (-1,1)][model:ResNet50] - loss: 8.494 - accuracy: 0.001
500/500 [==============================] - 71s 142ms/step - loss: 1.9044 - accuracy: 0.5703
[Testing][pixel vales are normalized from (-1,1)][model:DenseNet121] - loss: 1.904 - accuracy: 0.570
500/500 [==============================] - 46s 93ms/step - loss: 2.4290 - accuracy: 0.4992
[Testing][pixel vales are normalized from (-1,1)][model:MobileNetV2] - loss: 2.429 - accuracy: 0.499
&lt;/denchmark-code&gt;

However, I still get similar results. I hope that someone would check correctness of my code.
		</comment>
		<comment id='13' author='babaish' date='2020-03-02T17:24:43Z'>
		&lt;denchmark-link:https://github.com/CNOCycle&gt;@CNOCycle&lt;/denchmark-link&gt;
, note that the downloading url is from &lt;denchmark-link:https://github.com/keras-team/keras-applications/releases&gt;https://github.com/keras-team/keras-applications/releases&lt;/denchmark-link&gt;
, which should be same as the one in tf.keras.application. This somehow indicating that your transfer learning model might not be constructed correctly.
The easiest way to verify whether the original resnet model is correct is to run some eval with the eval data, which I am quite sure we did before we release the model.
		</comment>
		<comment id='14' author='babaish' date='2020-03-03T11:30:25Z'>
		
@CNOCycle, note that the downloading url is from https://github.com/keras-team/keras-applications/releases, which should be same as the one in tf.keras.application. This somehow indicating that your transfer learning model might not be constructed correctly.

Right. I have just tried again with tf-nightly, and indeed, the URLs have changed:

Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5

One can check this in my &lt;denchmark-link:https://colab.research.google.com/drive/13zpJSsJ0GmXluiVNAF4DLSqW3Ao84G5Z&gt;Colab notebook&lt;/denchmark-link&gt;
.
&lt;denchmark-link:https://github.com/CNOCycle&gt;@CNOCycle&lt;/denchmark-link&gt;
 Could you please try again with ? Thanks a lot.
		</comment>
		<comment id='15' author='babaish' date='2020-03-03T12:19:28Z'>
		I checked the weight from storage.googleapis.com and keras-applications/releases. Both files' hash are same.
&lt;denchmark-code&gt;2cb95161c43110f7111970584f804107  /root/.keras/models/resnet50_weights_tf_dim_ordering_tf_kernels.h5
&lt;/denchmark-code&gt;

What I do is not transfer learning. I'm trying to reproduce to accuracy which publish in many papers.
From my experience, my implement of training ImageNet from scratch by TF and pytorch. For pytorch version, its top-1 accuracy is about 74%. For TF version, its top-1 accuracy is about 68%.  For TF version with pre-trained weights, its top-1 accuracy is under 50%
		</comment>
		<comment id='16' author='babaish' date='2020-03-03T20:01:40Z'>
		Hmm, that's bad news...

The easiest way to verify whether the original resnet model is correct is to run some eval with the eval data, which I am quite sure we did before we release the model.

This is exactly what &lt;denchmark-link:https://github.com/CNOCycle&gt;@CNOCycle&lt;/denchmark-link&gt;
 did. &lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;
 Would you mind sharing your eval code so that we can check the weights again on our side?
		</comment>
		<comment id='17' author='babaish' date='2020-04-10T13:57:09Z'>
		Hmm... I think this issue is rather critical... Imagine getting 2-3% lower accuracy than PyTorch just because of bad conversion of pretrained models? That'd be a pity...
		</comment>
		<comment id='18' author='babaish' date='2020-09-14T02:41:28Z'>
		Hello everyone,
I have just tried to use pre-trained ResNet50 with tf.keras.applications.ResNet50(**args). The thing is happening for me is that after downloading weights, nothing else happens!! even after a couple of hours it does not go any further!! Any idea? what may be the casue of this? Any solution?
Many thanks for your help in advance.
I use as follow:
ENCODER_BASE =  tf.keras.applications.ResNet50(include_top=False,input_shape=(None,None,3), weights='imagenet')
		</comment>
		<comment id='19' author='babaish' date='2020-09-19T15:26:40Z'>
		&lt;denchmark-link:https://github.com/nick-nikzad&gt;@nick-nikzad&lt;/denchmark-link&gt;
 At least show us your code to see what you would expect to see after loading the model.
		</comment>
		<comment id='20' author='babaish' date='2020-09-20T01:58:32Z'>
		&lt;denchmark-link:https://github.com/netw0rkf10w&gt;@netw0rkf10w&lt;/denchmark-link&gt;
  Thanks for the reply. I just expect using the output of some blocks as below:
&lt;denchmark-code&gt;ENCODER_BASE=tf.keras.applications.ResNet50(include_top=False,input_shape=(None,None,3), weights='imagenet')
for layer in ENCODER_BASE.layers:
    layer.traiable = True
    
f0 = ENCODER_BASE.get_layer("conv1_relu").output  #43358  

f1 = ENCODER_BASE.get_layer("conv2_block2_out").output  
f2 = ENCODER_BASE.get_layer("conv3_block4_out").output 
f3 = ENCODER_BASE.get_layer("conv4_block6_out").output  
f4 = ENCODER_BASE.get_layer("conv5_block3_out").output 
&lt;/denchmark-code&gt;

		</comment>
		<comment id='21' author='babaish' date='2020-09-20T11:29:17Z'>
		&lt;denchmark-link:https://github.com/nick-nikzad&gt;@nick-nikzad&lt;/denchmark-link&gt;
 Can you reproduce that using Colab?
		</comment>
	</comments>
</bug>