<bug id='31028' author='constiDisch' open_date='2019-07-25T13:14:46Z' closed_time='2019-08-23T21:14:00Z'>
	<summary>tensorflow.keras.layers.TimeDistributed seems to reset batch-size</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution: Linux Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):pip
TensorFlow version (use command below):1.14.0
Python version:3.6.7
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

Describe the current behavior
TimeDistributed resets the batch_shape given in the Input layer.
Could be that this is intended. If so, could you give me a hint how to run the example below? Would be really appreciated.
Describe the expected behavior
When using keras (not tensorflow.keras) the example below works fine
So, I expect that wen given the batchsize, TimeDistributed keeps the batchsize.
Code to reproduce the issue
&lt;denchmark-code&gt;import tensorflow.keras as keras
keras.backend.set_image_data_format('channels_first')
resnet = keras.applications.resnet50.ResNet50(include_top=False, weights='imagenet')

inputShape = (1,None,3,224,224)
input = keras.layers.Input(batch_shape = inputShape)
x = keras.layers.TimeDistributed(resnet)(input)
x = keras.layers.TimeDistributed(keras.layers.Flatten())(x)
x = keras.layers.LSTM(256, stateful=True)(x)
&lt;/denchmark-code&gt;

Other info / logs
&lt;denchmark-code&gt;ValueError: If a RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: 
- If using a Sequential model, specify the batch size by passing a `batch_input_shape` argument to your first layer.
- If using the functional API, specify the batch size by passing a `batch_shape` argument to your Input layer.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='constiDisch' date='2019-07-26T09:05:27Z'>
		I have tried on colab with TF version 1.14.0 was able to reproduce the issue.Please, find the &lt;denchmark-link:https://colab.research.google.com/drive/1vNktBX2U5T6-KilL3zbBDz67FGA3dTnt&gt;gist&lt;/denchmark-link&gt;
 here.Thanks!
		</comment>
		<comment id='2' author='constiDisch' date='2019-08-12T03:37:02Z'>
		&lt;denchmark-link:https://github.com/constiDisch&gt;@constiDisch&lt;/denchmark-link&gt;
 TimeDistributed internally combines the batch and 2nd dimension into one dimension, then computes the computation of the time-distributed layer, that's why shape info is getting lost here. We can probably do better shape inference. As a workaround in the meantime, you can do a  after the last TimeDistributed layer:
&lt;denchmark-code&gt;inputShape = (1,None,3,224,224)
input = keras.layers.Input(batch_shape = inputShape)
x = keras.layers.TimeDistributed(resnet)(input)
x = keras.layers.TimeDistributed(keras.layers.Flatten())(x)
x.set_shape(&lt;the shape with batch defined here&gt;)
x = keras.layers.LSTM(256, stateful=True)(x)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='constiDisch' date='2019-08-23T21:14:00Z'>
		Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!
		</comment>
		<comment id='4' author='constiDisch' date='2019-08-23T21:14:02Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=31028&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=31028&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>