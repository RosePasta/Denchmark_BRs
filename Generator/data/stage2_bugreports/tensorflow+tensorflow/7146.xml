<bug id='7146' author='c810armyHuan' open_date='2017-01-30T10:16:05Z' closed_time='2017-02-17T05:09:05Z'>
	<summary>Nested `tf.while_loop`s hang when they are placed on CPU</summary>
	<description>
When I place all ops within nested tf.while_loop on CPU, evaluating the output of nested tf.while_loop makes the program hang.
I also &lt;denchmark-link:http://stackoverflow.com/questions/41929472/why-does-nested-tf-while-loop-freeze-in-test-session-of-tf-test-testcase&gt;posted a question on StackOverflow&lt;/denchmark-link&gt;
.
&lt;denchmark-h:h3&gt;Environment info&lt;/denchmark-h&gt;

Operating System: Ubuntu 16.04
Installed version of CUDA and cuDNN:
&lt;denchmark-code&gt;/usr/local/cuda/lib64/libcudadevrt.a
/usr/local/cuda/lib64/libcudart.so -&gt; libcudart.so.8.0
/usr/local/cuda/lib64/libcudart.so.8.0 -&gt; libcudart.so.8.0.44
/usr/local/cuda/lib64/libcudart.so.8.0.44
/usr/local/cuda/lib64/libcudart_static.a
/usr/local/cuda/lib64/libcudnn.so -&gt; libcudnn.so.5
/usr/local/cuda/lib64/libcudnn.so.5 -&gt; libcudnn.so.5.1.5
/usr/local/cuda/lib64/libcudnn.so.5.1.5
/usr/local/cuda/lib64/libcudnn_static.a
&lt;/denchmark-code&gt;

If installed from source, provide


The commit hash (git rev-parse HEAD)
28f5099d532ce59787ee58012b8ef04c498947ae


The output of bazel version


&lt;denchmark-code&gt;Build label: 0.4.3
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Dec 22 12:31:25 2016 (1482409885)
Build timestamp: 1482409885
Build timestamp as int: 1482409885
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)&lt;/denchmark-h&gt;

The following code is run on python 3.5:
import tensorflow as tf

config = tf.ConfigProto(allow_soft_placement=False)
with tf.Session(config=config) as sess, tf.device('/cpu:0'):
    num_outer_iters = tf.constant(3)
    num_inner_iters = tf.constant(5)

    def outer_body(loop_outer_index, loop_outer_ta):

        def inner_body(loop_inner_index, loop_inner_ta):
            loop_inner_ta = loop_inner_ta.write(loop_inner_index,
                                                tf.constant(0))
            return (loop_inner_index + 1, loop_inner_ta)

        inner_index = tf.constant(0)
        inner_ta = tf.TensorArray(tf.int32, num_inner_iters)
        (_, inner_ta) = tf.while_loop(
            lambda index, *_: index &lt; num_inner_iters,
            inner_body,
            (inner_index, inner_ta))

        loop_outer_ta = loop_outer_ta.write(loop_outer_index,
                                            inner_ta.stack())
        return loop_outer_index + 1, loop_outer_ta

    outer_index = tf.constant(0)
    outer_ta = tf.TensorArray(tf.int32, num_outer_iters)
    (_, outer_ta) = tf.while_loop(
        lambda index, *_: index &lt; num_outer_iters,
        outer_body,
        (outer_index, outer_ta),
        parallel_iterations=1, back_prop=False)

    print(sess.run(outer_ta.concat()))
&lt;denchmark-h:h3&gt;What other attempted solutions have you tried?&lt;/denchmark-h&gt;

If I replace num_inner_iters = tf.constant(5) with num_inner_iters = 5, the program will exit normally.
&lt;denchmark-h:h3&gt;Logs or other output that would be helpful&lt;/denchmark-h&gt;

The program hangs without any error message.
	</description>
	<comments>
		<comment id='1' author='c810armyHuan' date='2017-01-31T04:21:20Z'>
		I tried your code with the latest head (current master), (configured with XLA) and it works normally there
		</comment>
		<comment id='2' author='c810armyHuan' date='2017-01-31T04:22:30Z'>
		Can you try it with latest head and see if problem persists?
		</comment>
		<comment id='3' author='c810armyHuan' date='2017-01-31T06:50:12Z'>
		The program still froze with latest master branch. (4cc0d1e7905454de7bd3cb6c20c3f9fb459ed335)
I got the following output before the program froze:
&lt;denchmark-code&gt;2017-01-31 14:40:36: I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
2017-01-31 14:40:36: I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
2017-01-31 14:40:36: I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
2017-01-31 14:40:36: I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
2017-01-31 14:40:36: I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
2017-01-31 14:40:38: I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX 1080
major: 6 minor: 1 memoryClockRate (GHz) 1.7335
pciBusID 0000:06:00.0
Total memory: 7.92GiB
Free memory: 5.32GiB
2017-01-31 14:40:38: I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
2017-01-31 14:40:38: I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
2017-01-31 14:40:38: I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 1080, pci bus id: 0000:06:00.0)
2017-01-31 14:40:42: I tensorflow/core/platform/default/cuda_libdevice_path.cc:35] TEST_SRCDIR environment variable not set: using local_config_cuda/cuda under this executable's runfiles directory as the CUDA root.
2017-01-31 14:40:42: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices
2017-01-31 14:40:42: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 20 visible devices
2017-01-31 14:40:42: I tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:
2017-01-31 14:40:42: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;
2017-01-31 14:40:42: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices
2017-01-31 14:40:42: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 20 visible devices
2017-01-31 14:40:42: I tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform CUDA. Devices:
2017-01-31 14:40:42: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): GeForce GTX 1080, Compute Capability 6.1
&lt;/denchmark-code&gt;

I used gdb to attach the program, and the output of bt was:
&lt;denchmark-code&gt;#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00007f436db3591c in std::condition_variable::wait(std::unique_lock&lt;std::mutex&gt;&amp;) ()
   from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00007f4370601f1b in tensorflow::DirectSession::WaitForNotification(tensorflow::Notification*, long long)
    () from /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow.so
#3  0x00007f4370601fcd in tensorflow::DirectSession::WaitForNotification(tensorflow::DirectSession::RunState*, tensorflow::CancellationManager*, long long) ()
   from /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow.so
#4  0x00007f437060c735 in tensorflow::DirectSession::Run(tensorflow::RunOptions const&amp;, std::vector&lt;std::pair&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, tensorflow::Tensor&gt;, std::allocator&lt;std::pair&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, tensorflow::Tensor&gt; &gt; &gt; const&amp;, std::vector&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, std::allocator&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;, std::vector&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, std::allocator&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;, std::vector&lt;tensorflow::Tensor, std::allocator&lt;tensorflow::Tensor&gt; &gt;*, tensorflow::RunMetadata*) ()
   from /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow.so
#5  0x00007f436f1afd21 in TF_Run_Helper(tensorflow::Session*, char const*, TF_Buffer const*, std::vector&lt;std::pair&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, tensorflow::Tensor&gt;, std::allocator&lt;std::pair&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, tensorflow::Tensor&gt; &gt; &gt; const&amp;, std::vector&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, std::allocator&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;, TF_Tensor**, std::vector&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, std::allocator&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;, TF_Buffer*, TF_Status*) [clone .constprop.533] ()
   from /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow.so
#6  0x00007f436f1b0538 in TF_Run ()
   from /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow.so
#7  0x00007f436f0a167d in tensorflow::TF_Run_wrapper_helper(TF_DeprecatedSession*, char const*, TF_Buffer const*, _object*, tensorflow::gtl::InlinedVector&lt;char const*, 8&gt; const&amp;, tensorflow::gtl::InlinedVector&lt;char const*, 8&gt; const&amp;, TF_Status*, tensorflow::gtl::InlinedVector&lt;_object*, 8&gt;*, TF_Buffer*) ()
   from /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow.so
#8  0x00007f436f0a17c3 in tensorflow::TF_Run_wrapper(TF_DeprecatedSession*, TF_Buffer const*, _object*, tensorflow::gtl::InlinedVector&lt;char const*, 8&gt; const&amp;, tensorflow::gtl::InlinedVector&lt;char const*, 8&gt; const&amp;, TF_Status*, tensorflow::gtl::InlinedVector&lt;_object*, 8&gt;*, TF_Buffer*) ()
   from /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow.so
#9  0x00007f436f089365 in _wrap_TF_Run ()
   from /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow.so
#10 0x00000000004e9b9f in PyCFunction_Call ()
#11 0x000000000052b595 in PyEval_EvalFrameEx ()
#12 0x000000000052e87a in PyEval_EvalCodeEx ()
#13 0x00000000004ebcc3 in ?? ()
#14 0x00000000005b7167 in PyObject_Call ()
#15 0x00000000005262af in PyEval_EvalFrameEx ()
#16 0x000000000052d2e3 in ?? ()
#17 0x0000000000529332 in PyEval_EvalFrameEx ()
#18 0x000000000052d82f in ?? ()
#19 0x0000000000529332 in PyEval_EvalFrameEx ()
#20 0x0000000000528814 in PyEval_EvalFrameEx ()
#21 0x000000000052d2e3 in ?? ()
#22 0x0000000000528eee in PyEval_EvalFrameEx ()
#23 0x000000000052d2e3 in ?? ()
#24 0x000000000052dfdf in PyEval_EvalCode ()
#25 0x00000000005c7b85 in ?? ()
#26 0x00000000004e9b9f in PyCFunction_Call ()
#27 0x000000000052b595 in PyEval_EvalFrameEx ()
#28 0x000000000052d2e3 in ?? ()
#29 0x0000000000528eee in PyEval_EvalFrameEx ()
#30 0x000000000052d2e3 in ?? ()
#31 0x0000000000528eee in PyEval_EvalFrameEx ()
#32 0x000000000052d2e3 in ?? ()
#33 0x000000000052dfdf in PyEval_EvalCode ()
#34 0x00000000005fd2c2 in ?? ()
#35 0x00000000005ff76a in PyRun_FileExFlags ()
#36 0x00000000005ff95c in PyRun_SimpleFileExFlags ()
#37 0x000000000063e7d6 in Py_Main ()
#38 0x00000000004cfe41 in main ()
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='c810armyHuan' date='2017-01-31T17:20:12Z'>
		&lt;denchmark-link:https://github.com/yuanbyu&gt;@yuanbyu&lt;/denchmark-link&gt;
 any idea? Why would replacing  with  make a difference for 
		</comment>
		<comment id='5' author='c810armyHuan' date='2017-01-31T19:21:02Z'>
		OK, I can reproduce the hang (I was mistakenly running with CUDA_VISIBLE_DEVICES= which makes it pass). This suggests that with tf.device("/cpu:0") tries to place some parts of the computation on GPU which causes the hang
		</comment>
		<comment id='6' author='c810armyHuan' date='2017-02-17T05:09:05Z'>
		I think that this has been fixed in the HEAD. Re-open if it is not the case.
		</comment>
	</comments>
</bug>