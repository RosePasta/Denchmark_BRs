<bug id='37053' author='henry-prior' open_date='2020-02-25T17:23:13Z' closed_time='2020-10-05T16:04:59Z'>
	<summary>Cannot wrap GradientTape.batch_jacobian with tf.function on tf.keras model with LSTM cells</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock
example script provided in TensorFlow):  Yes
OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04):  macOS Mojave
TensorFlow installed from (source or
binary): binary
TensorFlow version (use command below): 2.1.0
Python version: 3.7.4

Describe the current behavior
The @tf.function wrapper is not able to convert LSTM cells correctly for use in the batch_jacobian function (or jacobian, tested both). These functions work correctly without the decorator.
&lt;denchmark-code&gt;    ValueError: No converter defined for VariableShape
    name: "loop_body/VariableShape_2"
    op: "VariableShape"
    input: "binary_classification_lstm/lstm/while:10"
    attr {
      key: "out_type"
      value {
        type: DT_INT32
      }
    } 
&lt;/denchmark-code&gt;

Describe the expected behavior
This should run without throwing an error
Standalone code to reproduce the issue
&lt;denchmark-code&gt;import tensorflow as tf

class BinaryClassificationLSTM(tf.keras.Model):
    def __init__(self, units, name=None):
        super(BinaryClassificationLSTM, self).__init__(name=name)
        self.lstm_layer = tf.keras.layers.LSTM(units, activation='softsign')
        self.dense = tf.keras.layers.Dense(1)
        self.sigmoid = tf.keras.layers.Activation('sigmoid')

    def call(self, x):
        sequence = self.lstm_layer(x)
        logit = self.dense(sequence)
        prob = self.sigmoid(logit)

        return prob
    

@tf.function
def get_jacobian(model, tensor_in):
    with tf.GradientTape() as tape:
        tape.watch(tensor_in)
        predictions = model(tensor_in)
    
    return tape.batch_jacobian(predictions, tensor_in)

inp = tf.zeros((1, 10, 5))
lstm = BinaryClassificationLSTM(10)
jacobian = get_jacobian(lstm, inp)
&lt;/denchmark-code&gt;

Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "test.py", line 29, in &lt;module&gt;
    jacobian = get_jacobian(lstm, inp)
  File "/Users/henryprior/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 568, in __call__
    result = self._call(*args, **kwds)
  File "/Users/henryprior/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 615, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File "/Users/henryprior/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 497, in _initialize
    *args, **kwds))
  File "/Users/henryprior/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 2389, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File "/Users/henryprior/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 2703, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File "/Users/henryprior/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 2593, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File "/Users/henryprior/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py", line 978, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "/Users/henryprior/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 439, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File "/Users/henryprior/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py", line 968, in wrapper
    raise e.ag_error_metadata.to_exception(e)
ValueError: in converted code:

    test.py:24 get_jacobian  *
        return tape.batch_jacobian(predictions, tensor_in)
    /Users/henryprior/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py:1246 batch_jacobian
        sys.exc_info()[2])
    /Users/henryprior/anaconda3/envs/test/lib/python3.7/site-packages/six.py:702 reraise
        raise value.with_traceback(tb)
    /Users/henryprior/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py:1238 batch_jacobian
        parallel_iterations=parallel_iterations)
    /Users/henryprior/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py:189 pfor
        return f()
    /Users/henryprior/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py:183 f
        return _pfor_impl(loop_fn, iters, parallel_iterations=parallel_iterations)
    /Users/henryprior/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py:256 _pfor_impl
        outputs.append(converter.convert(loop_fn_output))
    /Users/henryprior/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:1280 convert
        output = self._convert_helper(y)
    /Users/henryprior/anaconda3/envs/test/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:1460 _convert_helper
        (y_op.type, y_op, converted_inputs))

    ValueError: No converter defined for VariableShape
    name: "loop_body/VariableShape_2"
    op: "VariableShape"
    input: "binary_classification_lstm/lstm/while:10"
    attr {
      key: "out_type"
      value {
        type: DT_INT32
      }
    }

    inputs: [WrappedTensor(t=&lt;tf.Tensor 'binary_classification_lstm/lstm/while:10' shape=() dtype=resource&gt;, is_stacked=False, is_sparse_stacked=False)].
    Either add a converter or set --op_conversion_fallback_to_while_loop=True, which may run slower
    Encountered an exception while vectorizing the batch_jacobian computation. Vectorization can be disabled by setting experimental_use_pfor to False.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='henry-prior' date='2020-02-26T09:25:07Z'>
		&lt;denchmark-link:https://github.com/henry-prior&gt;@henry-prior&lt;/denchmark-link&gt;
, I tried replicating the reported issue but getting different error, can help us to reproduce the issue. Please find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/abd23e0998d1b89c6e880e6f94ca78b6/untitled406.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='henry-prior' date='2020-02-26T15:28:52Z'>
		&lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
 I got the same error when running the code in a notebook, but when I ran it as a  script (using a fresh venv) I got the error I posted in the issue. From briefly checking the tf code I believe the error in the gist is caused by the underlying problem from the error in the script. Let me know if you're able to reproduce this.
		</comment>
		<comment id='3' author='henry-prior' date='2020-02-28T06:34:45Z'>
		I could replicate the issue on local system as a .py  with Tf 2.1. Thanks!
		</comment>
		<comment id='4' author='henry-prior' date='2020-03-19T17:19:32Z'>
		Addin Rohan from core team. I think it might be an issue for tape. get_jacobian()
ValueError: No converter defined for VariableShape
		</comment>
		<comment id='5' author='henry-prior' date='2020-03-19T17:20:57Z'>
		Unassign myself since I don't think this is a LSTM issue. Feel free to assign back if you feel there is anything need to be fixed on RNN side. Thanks.
		</comment>
		<comment id='6' author='henry-prior' date='2020-07-21T05:50:51Z'>
		Support for VariableShape was added some time back. Can you check if this is still broken, and if so, does it give a different error.
Please test it against the nightly version, not 2.1.
		</comment>
		<comment id='7' author='henry-prior' date='2020-08-08T19:48:24Z'>
		I tested in nightly and got the following (different) error message:
&lt;denchmark-code&gt;NotImplementedError: Vectorization tried to stack variant tensor
Tensor("gradient_tape/binary_classification_lstm/lstm/while/gradients/grad_ys_3/pfor/Identity:0", shape=(), dtype=variant). 
This is likely because vectorization of that variant is not fully supported yet
&lt;/denchmark-code&gt;

And I see the same message in nightly when I swap batch_jacobian for jacobian
		</comment>
		<comment id='8' author='henry-prior' date='2020-10-05T16:04:59Z'>
		Should be fixed now (fix in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/a20b5857915cd573f595942eab6a928ae3837639&gt;a20b585&lt;/denchmark-link&gt;
 integration test in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/d5b5e1148ab3ef1817fadb864694ec3139746400&gt;d5b5e11&lt;/denchmark-link&gt;
), thank you for the report.
		</comment>
		<comment id='9' author='henry-prior' date='2020-10-05T16:05:01Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37053&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37053&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>