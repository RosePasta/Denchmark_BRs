<bug id='27142' author='TOSUKUi' open_date='2019-03-26T09:47:46Z' closed_time='2019-03-29T17:51:14Z'>
	<summary>AttributeError: 'LeakyReLU' object has no attribute '__name__' when running the train model.</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information
Docker container from tensorflow:2.0.0a0-gpu-py3
Describe the current behavior
I use leaky relu activation from advanced_activations module like this.
from tensorflow.python.keras.layers.advanced_activations import LeakyReLU
x1 = Conv2D(filters=32, kernel_size=(8, 8), strides=(2, 2), activation=learky_relu)(inputs)
And error occurred as follow when run the train my model.
&lt;denchmark-code&gt;File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/activations.py", line 206, in serialize
    if activation.__name__ in _TF_ACTIVATIONS_V2:
AttributeError: 'LeakyReLU' object has no attribute '__name__'
&lt;/denchmark-code&gt;

So the leaky relu instance is sent to activations.serialize and called attribute 'name' at 


tensorflow/tensorflow/python/keras/activations.py


         Line 255
      in
      1e43727






 @keras_export('keras.activations.serialize') 




.
But common python objects aren't have attribute 'name' so this bug is occurred.
** expected behavior **
My model should get 100% accuracy and my self-driving RC move as like human driving.
I don't know why advanced_activation classes are implement the Layer.
Maybe I think it better to make abstract class for advanced activation or make them independent function.
I want it is fixed as soon as possible.
good luck.
More info.
My train code is as follow.
&lt;denchmark-code&gt;    def train(self, X, y, saved_model_path, batch_size=8, epochs=100,  train_split=0.8, verbose=1, min_delta=.0005, patience=5, use_early_stop=True):
        """
        Args:
            train: list of traininig data
            validate: list of validation data
            saved_model_path: saved previous model path
        """
        
        # checkpoint to save model after each epoch
        save_best = ModelCheckpoint(saved_model_path,
                                    monitor='val_loss',
                                    verbose=verbose,
                                    save_best_only=True,
                                    mode='min')

        # stop training if the validation error stops improving.
        early_stop = EarlyStopping(monitor='val_loss',
                                    min_delta=min_delta,
                                    patience=patience,
                                    verbose=verbose,
                                    mode='auto')

        callbacks_list = [save_best]
        if use_early_stop:
            callbacks_list.append(early_stop)

        hist = self.model.fit(
            X,
            y,
            batch_size=batch_size,
            epochs=epochs,
            verbose=1,
            validation_split=0.2,
            callbacks=callbacks_list,
        )
        return hist
&lt;/denchmark-code&gt;

See My model code as follow.
&lt;denchmark-code&gt;from tensorflow.python.keras import Input
from tensorflow.python.keras.layers import Conv2D, Flatten, Dense, MaxPool2D
from tensorflow.python.keras.layers.advanced_activations import LeakyReLU
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.utils import plot_model
from tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStoppin

def linear():
    learky_relu = LeakyReLU()
    inputs = Input(shape=(391, 554, 3), name='inputs')
    x1 = Conv2D(filters=32, kernel_size=(8, 8), strides=(2, 2), activation=learky_relu)(inputs)
    x2 = MaxPool2D(pool_size=(4, 4), strides=2)(x1)
    x3 = Conv2D(filters=32, kernel_size=(4, 4), strides=(3, 3), activation=learky_relu)(x2)
    x4 = MaxPool2D(pool_size=(3, 3), strides=2)(x3)
    x5 = Conv2D(filters=48, kernel_size=(4, 4), strides=(2, 2), activation=learky_relu)(x4)
    x6 = MaxPool2D(pool_size=(2, 2), strides=2)(x5)
    x7 = Flatten(name='flattened')(x6)
    d1 = Dense(units=100, activation='linear')(x7)
    d2 = Dense(units=50, activation='linear')(d1)
    handling = Dense(units=1, activation='linear', name='handling')(d2)
    model = Model(inputs=[inputs], outputs=handling)
    model.compile(optimizer='adam', loss={'handling': 'mean_squared_error'} , loss_weights={'handling': 0.5 })
    return model

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='TOSUKUi' date='2019-03-28T23:08:33Z'>
		AFAIK LeakyRelu (advanced activation) should be used as a layer instead of an activation argument.
So something like this should pass,
from keras import Sequential
from keras.layers import Conv2D, LeakyReLU
model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3), activation='linear',input_shape=(30, 30, 1)))
model.add(LeakyReLU(alpha=0.01))
		</comment>
		<comment id='2' author='TOSUKUi' date='2019-03-29T01:24:43Z'>
		Thanks. I try it.
Maybe I had misunderstanding about how to use advanced activations.
I saw several blogs about general usage of keras and they write that advance activations is able to use for activations argument.
		</comment>
		<comment id='3' author='TOSUKUi' date='2019-03-29T17:51:14Z'>
		The keras documentation is not clear about the usage of advanced activation's. Using LeakyRelu as layer makes sense since its imported as any other keras layer. I will close this issue now that we have a workaround. Thanks!
		</comment>
		<comment id='4' author='TOSUKUi' date='2019-03-29T17:51:15Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=27142&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=27142&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='TOSUKUi' date='2020-09-16T04:32:11Z'>
		leakyrelu_alpha = 0.2
gen5 = Conv2D(filters=256, kernel_size=3, strides=1, padding='same')(gen5)
gen5 = LeakyReLU(alpha=leakyrelu_alpha)(gen5)#Activation('relu')'or #LeakyReLU(alpha=0.3)
use this, it will solve your problem
		</comment>
		<comment id='6' author='TOSUKUi' date='2020-11-02T06:06:44Z'>
		
AFAIK LeakyRelu (advanced activation) should be used as a layer instead of an activation argument.
So something like this should pass,
from keras import Sequential
from keras.layers import Conv2D, LeakyReLU
model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3), activation='linear',input_shape=(30, 30, 1)))
model.add(LeakyReLU(alpha=0.01))

Does this have the same effect as the original activation?
		</comment>
	</comments>
</bug>