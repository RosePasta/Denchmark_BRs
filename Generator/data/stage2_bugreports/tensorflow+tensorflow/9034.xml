<bug id='9034' author='shaform' open_date='2017-04-06T23:38:14Z' closed_time='2017-06-16T21:08:35Z'>
	<summary>Computing 2nd-order tf.gradients of tensors throws Exception when used with batch_norm</summary>
	<description>
&lt;denchmark-h:h3&gt;Describe the problem clearly&lt;/denchmark-h&gt;

If the updates_collections of a batch_norm layer is set other than tf.GraphKeys.UPDATE_OPS, it is no longer possible to compute 2nd-order tf.gradients with respect to the weights of a fully_connected layer.
p.s. It is okay when updates_collections is set as tf.GraphKeys.UPDATE_OPS. I think updates_collections should not affect the ability to compute gradients?
&lt;denchmark-h:h3&gt;Environments&lt;/denchmark-h&gt;


Ubuntu 16.04 64bit
Python 3.6.0 |Anaconda 4.3.1 (64-bit)| (default, Dec 23 2016, 12:22:00)
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux
tensorflow-gpu 1.0.1 installed from pip
libcublas.so.8.0, libcudnn.so.5, libcufft.so.8.0,  libcuda.so.1, libcurand.so.8.0

&lt;denchmark-h:h3&gt;Source Code&lt;/denchmark-h&gt;

import tensorflow as tf

with tf.Session() as sess:
    X = tf.placeholder(tf.float32, [None, 2])
    is_training = tf.placeholder(tf.bool, [], name='is_training')

    outputs = tf.contrib.layers.fully_connected(inputs=X, num_outputs=1)
    outputs = tf.contrib.layers.batch_norm(
        inputs=outputs,
        is_training=is_training,
        updates_collections='bad_collections')
    # get gradients of X with respect to outputs values
    grads = tf.gradients(outputs, [
        X,
    ])[0]
    bad_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)
    # get gradients of weights with respect to gradients of X
    bad_grads = tf.gradients(grads, bad_vars) # this line
&lt;denchmark-h:h2&gt;Logs&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;Traceback (most recent call last):
  File "test.py", line 18, in &lt;module&gt;
    bad_grads = tf.gradients(grads, bad_vars)
  File "$HOME/anaconda2/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 474, in gradients
    out_grads[i] = control_flow_ops.ZerosLikeOutsideLoop(op, i)
  File "$HOME/anaconda2/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 1303, in ZerosLikeOutsideLoop
    pred = op_ctxt.pred
AttributeError: 'NoneType' object has no attribute 'pred'
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='shaform' date='2017-04-13T03:07:41Z'>
		Hi &lt;denchmark-link:https://github.com/shaform&gt;@shaform&lt;/denchmark-link&gt;
, this seems a more suitable question for StackOverflow
		</comment>
		<comment id='2' author='shaform' date='2017-04-13T05:24:43Z'>
		Hi &lt;denchmark-link:https://github.com/Carmezim&gt;@Carmezim&lt;/denchmark-link&gt;
, thanks for the reply. I thought this was a bug since I
believed that changing updates_collections shouldn't affect the ability to
compute gradients. Your comment seems to suggest that the exception is
totally expected and therefore shouldn't be fixed. (i.e. It is not a bug.)
Could you share the rationale behind this, please? Thanks.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Thu, Apr 13, 2017, 11:09 AM Adriano Carmezim ***@***.***&gt; wrote:
 Hi @shaform &lt;https://github.com/shaform&gt;, this seems a more suitable
 question for StackOverflow

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#9034 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AAWaRLYuEWp8Wgp5zFXdDzqYn0IXT_ayks5rvZHNgaJpZM4M2S1S&gt;
 .



		</comment>
		<comment id='3' author='shaform' date='2017-04-13T11:51:48Z'>
		I've updated the description and title to make it more clear.
		</comment>
		<comment id='4' author='shaform' date='2017-04-13T12:59:38Z'>
		Looks like it's because when custom  is set, legacy BN implementation would be used &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/contrib/layers/python/layers/layers.py#L536&gt;https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/contrib/layers/python/layers/layers.py#L536&lt;/denchmark-link&gt;
. It's likely that this implementation would cause problems when one attempts 2-nd order .
		</comment>
		<comment id='5' author='shaform' date='2017-04-17T17:22:25Z'>
		&lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;
 any comment on this? (contrib.layers)
		</comment>
		<comment id='6' author='shaform' date='2017-04-17T17:31:03Z'>
		Can you log the specific op for which you cannot obtain second-order gradients?
		</comment>
		<comment id='7' author='shaform' date='2017-04-17T22:13:49Z'>
		&lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;

When the exception is thrown, the op on  is the following:

		</comment>
		<comment id='8' author='shaform' date='2017-04-17T22:57:07Z'>
		
I think updates_collections should not affect the ability to compute gradients?

When you are using a custom updates_collections you are routed to a different implementation of batch norm (the one in tf.layers does not support custom update collections), so this is not about updates_collections affecting gradient computation.
As far as I can tell you are calling an op (as part of the legacy batch norm layer implementation) that does not implement second-order gradients. Can anyone confirm?
		</comment>
		<comment id='9' author='shaform' date='2017-04-17T23:01:44Z'>
		&lt;denchmark-link:https://github.com/yuanbyu&gt;@yuanbyu&lt;/denchmark-link&gt;
 is that something you've seen before?
		</comment>
		<comment id='10' author='shaform' date='2017-06-16T21:08:35Z'>
		Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you.
		</comment>
	</comments>
</bug>