<bug id='34158' author='seandaug' open_date='2019-11-11T16:27:44Z' closed_time='2020-03-16T20:58:19Z'>
	<summary>Metrics incorrect for RNN with mask</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
TensorFlow installed from (source or binary): binary (conda)
TensorFlow version (use command below): 2.0.0
Python version: 3.6.8
Bazel version (if compiling from source): n/a
GCC/Compiler version (if compiling from source): n/a
CUDA/cuDNN version: 10.0.130 / 7.6.0
GPU model and memory: 1080 Ti

Describe the current behavior
I am training an RNN (GRU) where my varying-length sequences are right-padded with 0s and a mask is applied. Many sequences are more than half 0s (padding). I compile the model with a loss of 'mean_squared_error' and a metric of 'mean_squared_error', but the output is different when the mask is in effect.
&lt;denchmark-code&gt;model.compile(optimizer=keras.optimizers.RMSprop(),
              loss='mean_squared_error',
              metrics=['mean_squared_error'])
&lt;/denchmark-code&gt;

Or equivalently:
&lt;denchmark-code&gt;model.compile(optimizer=keras.optimizers.RMSprop(),
              loss=keras.losses.MeanSquaredError(),
              metrics=[keras.metrics.MeanSquaredError()])
&lt;/denchmark-code&gt;

Example output (note the different values for loss vs. mean_squared_error for both training and validation):
&lt;denchmark-code&gt;Epoch 1/50
210328/210328 [==============================] - 610s 3ms/sample - loss: 4.5338e-06 - mean_squared_error: 1.1923e-05 - val_loss: 2.5456e-06 - val_mean_squared_error: 6.7928e-06
Epoch 2/50
210328/210328 [==============================] - 525s 2ms/sample - loss: 2.1835e-06 - mean_squared_error: 5.7421e-06 - val_loss: 2.2920e-06 - val_mean_squared_error: 6.1160e-06
Epoch 3/50
210328/210328 [==============================] - 513s 2ms/sample - loss: 1.9939e-06 - mean_squared_error: 5.2437e-06 - val_loss: 2.2535e-06 - val_mean_squared_error: 6.0133e-06
...
Epoch 50/50
210328/210328 [==============================] - 527s 3ms/sample - loss: 1.5595e-06 - mean_squared_error: 4.1011e-06 - val_loss: 1.7867e-06 - val_mean_squared_error: 4.7677e-06
&lt;/denchmark-code&gt;

When I disable the masking, I get the following output:
&lt;denchmark-code&gt;Epoch 1/3
210328/210328 [==============================] - 516s 2ms/sample - loss: 7.1682e-06 - mean_squared_error: 7.1682e-06 - val_loss: 6.7091e-06 - val_mean_squared_error: 6.7091e-06
Epoch 2/3
210328/210328 [==============================] - 434s 2ms/sample - loss: 5.9133e-06 - mean_squared_error: 5.9133e-06 - val_loss: 6.7091e-06 - val_mean_squared_error: 6.7091e-06
Epoch 3/3
210328/210328 [==============================] - 442s 2ms/sample - loss: 5.9085e-06 - mean_squared_error: 5.9085e-06 - val_loss: 6.7073e-06 - val_mean_squared_error: 6.7073e-06
&lt;/denchmark-code&gt;

Without the mask, the values for loss and mean_squared_error agree. For the validation set, the values are not really improving and the value of 6.7e-06 seems to be what you get when you evaluate on the 0s that would otherwise be ignored by the masking. Comparing the values between the runs suggests that the mean_squared_error calculations are not using the mask when it is in effect, but the loss calculations do use the mask. (We'd expect lower values when we correctly ignore irrelevant time steps.)
Describe the expected behavior
The values for loss and mean_squared_error should agree and both use the masking.
Code to reproduce the issue
I don't have full code and data to share since my model and data are proprietary.
Other info / logs
I can't think of any relevant logs.
	</description>
	<comments>
		<comment id='1' author='seandaug' date='2019-11-12T07:24:40Z'>
		&lt;denchmark-link:https://github.com/seandaug&gt;@seandaug&lt;/denchmark-link&gt;
, Please provide the complete code to reproduce the reported issue. Thanks!
		</comment>
		<comment id='2' author='seandaug' date='2019-11-12T15:12:23Z'>
		Here's a small example that illustrates the issue.
&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow import keras
import numpy as np

# %% Make some sine wave data
num_time_series = 100
max_length = 200
num_points = 1
training_data = np.zeros((num_time_series, max_length, num_points))
np.random.seed(123)
tf.random.set_seed(123)
for i in range(num_time_series):
    # Make a sequence that doesn't fill the array so there's padding at the end
    length = np.random.randint(0.25 * max_length, .75 * max_length + 1)
    period = np.random.random() * 20 + 5
    shift = np.random.random()
    training_data[i, 0:length, 0] = np.sin(2 * np.pi / period * np.linspace(0, length - 1, length) + shift)


# %% Define the model
def make_model(use_mask):
    input_seq = keras.layers.Input(shape=(None, num_points))
    masked_input_seq = keras.layers.Masking(mask_value=0.0)(input_seq) if use_mask else input_seq
    gru = keras.layers.GRU(units=3, return_sequences=True)(masked_input_seq)
    output = keras.layers.Dense(units=1)(gru)
    model = keras.Model(input_seq, output)
    model.compile(optimizer=keras.optimizers.RMSprop(),
                  loss='mean_squared_error',
                  metrics=['mean_squared_error'])
    return model


# %% Train model with mask. Note that reported 'loss' and 'mean_squared_error' differ
make_model(True).fit(training_data[:, :-1, :],
                     training_data[:, 1:, :],
                     batch_size=10,
                     epochs=3,
                     verbose=1)


# %% Train model without mask. Note that reported 'loss' and 'mean_squared_error' match
make_model(False).fit(training_data[:, :-1, :],
                      training_data[:, 1:, :],
                      batch_size=10,
                      epochs=3,
                      verbose=1)
&lt;/denchmark-code&gt;

Example output when mask is used (note that loss and mean_squared_error differ):
&lt;denchmark-code&gt;Epoch 1/3
100/100 [==============================] - 10s 98ms/sample - loss: 0.2741 - mean_squared_error: 0.5393
Epoch 2/3
100/100 [==============================] - 2s 20ms/sample - loss: 0.2690 - mean_squared_error: 0.5292
Epoch 3/3
100/100 [==============================] - 2s 20ms/sample - loss: 0.2650 - mean_squared_error: 0.5214
&lt;/denchmark-code&gt;

Example output when no mask is used (note that loss and mean_squared_error match):
&lt;denchmark-code&gt;Epoch 1/3
100/100 [==============================] - 4s 37ms/sample - loss: 0.2475 - mean_squared_error: 0.2475
Epoch 2/3
100/100 [==============================] - 1s 13ms/sample - loss: 0.2305 - mean_squared_error: 0.2305
Epoch 3/3
100/100 [==============================] - 1s 13ms/sample - loss: 0.2174 - mean_squared_error: 0.2174
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='seandaug' date='2019-11-14T06:54:59Z'>
		I could replicate the issue with Tf 2.0.
Please take a look at the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/740c0fd036e17fe505996957b79d5cbb/untitled255.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='4' author='seandaug' date='2019-11-18T21:16:52Z'>
		Adding &lt;denchmark-link:https://github.com/pavithrasv&gt;@pavithrasv&lt;/denchmark-link&gt;
 who is the owner of metric and loss. I think the loss will take into account of the masks and exclude the masked value, but I don't think metric will do that, which is why you see the value difference here. I will let &lt;denchmark-link:https://github.com/pavithrasv&gt;@pavithrasv&lt;/denchmark-link&gt;
  to confirm.
		</comment>
		<comment id='5' author='seandaug' date='2020-03-16T20:58:19Z'>
		&lt;denchmark-link:https://github.com/seandaug&gt;@seandaug&lt;/denchmark-link&gt;
 I think this was resolved in the . I am not able to reproduce the error with . Please take a look at the &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/6e844af327510d58c27d59f78593825d/untitled255.ipynb&gt;gist here&lt;/denchmark-link&gt;
. Thanks!
I am closing this issue as this was resolved. Please feel free to reopen if I am mistaken. Thanks!
		</comment>
		<comment id='6' author='seandaug' date='2020-03-16T20:58:21Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34158&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34158&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>