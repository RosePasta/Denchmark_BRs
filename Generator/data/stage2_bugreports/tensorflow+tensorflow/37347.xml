<bug id='37347' author='Gorospe' open_date='2020-03-05T10:29:37Z' closed_time='2020-03-24T02:12:59Z'>
	<summary>Quantization problem while reproducing person detection example</summary>
	<description>
@tensorflow/micro
System information

Host OS Platform and Distribution: Ubuntu 18.04.4 LTS
Tensorflow version: 1.15
Target platform: OpenMV Cam H7 (https://openmv.io/products/openmv-cam-h7)


I am trying to reproduce the person detection example (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/person_detection/training_a_model.md&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/person_detection/training_a_model.md&lt;/denchmark-link&gt;
).
The document is not completely updated but with slight modifications I am able to train, freeze and convert the model into TFLite.
Because of the target, I need full integer quantization so, I set the input and output as uint8.
When I try to load the model in the OpenMV, the quantization layer fails with the following error:

tensorflow/lite/micro/kernels/quantize.cc:51 input-&gt;type == kTfLiteFloat32 || input-&gt;type == kTfLiteInt16 was not true.

I guess that the problem is in the post-training quantization, as the output model has an input of uint8 type and afterwards, it has a quantize layer which tries to convert uint8 to int8.
Post-training quantization
&lt;denchmark-code&gt;def representative_dataset_gen():
  record_iterator = tf.python_io.tf_record_iterator(path='coco_dataset/val.record-00000-of-00010')
  count = 0
  for string_record in record_iterator:
    example = tf.train.Example()
    example.ParseFromString(string_record)
    a=io.BytesIO(example.features.feature['image/encoded'].bytes_list.value[0])
    image = PIL.Image.open(a)
    image = image.resize((96, 96))
    image = image.convert('L')
    array = np.array(image)
    array = np.expand_dims(array, axis=2)
    array = np.expand_dims(array, axis=0)
    array = ((array / 127.5) - 1.0).astype(np.float32)
    yield([array])
    count += 1
    if count &gt; 300:
        break

converter = tf.lite.TFLiteConverter.from_frozen_graph('frozen.pb',['input'], ['MobilenetV1/Predictions/Reshape_1'])
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
converter.representative_dataset = representative_dataset_gen
 
tflite_quant_model = converter.convert()
open("quantized.tflite", "wb").write(tflite_quant_model)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Gorospe' date='2020-03-24T01:39:09Z'>
		Yes, you're correct. It expects a float32/int16 as the input to the model. In order to use a fully int-8 model use our experimental branch - &lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/person_detection_experimental&gt;https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/person_detection_experimental&lt;/denchmark-link&gt;
. This model has some accuracy issues so it's under experimental.  I'll update this post when I find a workaround.
Follow-up question: Are you required to train a model from scratch?
		</comment>
		<comment id='2' author='Gorospe' date='2020-03-24T01:58:43Z'>
		We have not had a use case which uses quantize to requantize from uint8 to int8.  One hack we have used when we have an int8 model with uint8 inputs, is to keep the model as int8 and subtract 128 from each input value to convert uint8 to int8.
		</comment>
		<comment id='3' author='Gorospe' date='2020-03-24T02:12:59Z'>
		If you are required to train a model from scratch, you can directly generate a fully int-8 model by updating the following lines in your code to:
&lt;denchmark-code&gt;converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8
&lt;/denchmark-code&gt;

This model will not have quantize/dequantize nodes. You can use &lt;denchmark-link:https://lutzroeder.github.io/netron/&gt;Netron&lt;/denchmark-link&gt;
 to visualize it and verify.
However, as &lt;denchmark-link:https://github.com/njeffrie&gt;@njeffrie&lt;/denchmark-link&gt;
 mentioned above, it's your responsibility to now ensure that the inputs to the model during inference are int8 (if you have uint8 inputs from your device, then just subtract 128 from each value to convert it to int8).
Closing the issue. Feel free to re-open it if your issue is not resolved.
		</comment>
		<comment id='4' author='Gorospe' date='2020-03-24T02:13:02Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37347&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37347&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='Gorospe' date='2020-06-22T21:19:36Z'>
		&lt;denchmark-link:https://github.com/Gorospe&gt;@Gorospe&lt;/denchmark-link&gt;
 Sure! Feel free to tag/mention my Github name in a new github issue discussing the MobileNetV2 error and I can take a look at it.
		</comment>
		<comment id='6' author='Gorospe' date='2020-06-23T08:38:54Z'>
		I deleted the comment as I updated Tensorflo Lite for Micro library and now it works perfectly.
Thank you!
		</comment>
		<comment id='7' author='Gorospe' date='2020-11-03T05:37:59Z'>
		Hi Everyone,
I have followed all step successfully as defined in this readme file to reproduce the same result for person detection.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/person_detection_experimental/training_a_model.md&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/person_detection_experimental/training_a_model.md&lt;/denchmark-link&gt;

But I can't reproduce the expected result. It gives the same score continuously whether a person on a frame or not.
Here is the output of the serial monitor:
&lt;denchmark-code&gt;Starting capture
Image captured
Reading 3080 bytes from ArduCAM
Finished reading
Decoding JPEG and converting to greyscale
Image decoded and processed
Person score: -1 No person score: 1
&lt;/denchmark-code&gt;

Can someone please help me to reproduce the same output
Thanks a ton,
Bhavika
		</comment>
		<comment id='8' author='Gorospe' date='2020-11-03T07:47:28Z'>
		Hello,
I just can tell that I was able to train the Mobilenet V1 0.25 and quantized it to int8 as MeghnaNatraj commented. The performancewas really bad, as I checked It with 100 images (half of them with person), and it had 50% acc with 0% precision, but (if I remember correctly) the outputs weren't always the same.
Have you check whether the output variables are the indicated type (depending on the quantization, could be different)?
I also tried to train different networks, such as the Mobilenet V2 and V3, and the mobilenet V2 with a depth_multiplier of 0.10 worked well for me (better accuracy with a similar size of network). Take into account that if you want to try this, you will need to change the default depth_multiplier parameter.
Eventhough, I'm not an expert, so I hope a contributor might help you more
Good luck,
Joseba
		</comment>
		<comment id='9' author='Gorospe' date='2020-11-03T10:57:38Z'>
		Thanks &lt;denchmark-link:https://github.com/Gorospe&gt;@Gorospe&lt;/denchmark-link&gt;
 for your quick reply.
I have tried to implement both person_detection and person_detection_experimental model. But didn't get success to run on Arduino.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/person_detection&gt;https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/person_detection&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/person_detection_experimental&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/person_detection_experimental&lt;/denchmark-link&gt;

As the TinyML book suggest I have tried to train models on TensorFlow version 1.14. I Can able to achieve good accuracy at step eval_image_classifier.py.
It simply indicate that my model is trained well.
Now, I have doubt that problem is with quantization method. While trying mutiple quantization int8/uint8, I got the various below error message on serial monitor in Arduino. I have install TensorFlowLite Version 1.14.0-ALPHA on Arduino.

Invoke() called after initialization failed
Didn't find op for builtin opcode 'QUANTIZE' version '1'

I have tried both below method for TFLite conversion:



&lt;denchmark-code&gt;converter =
tf.lite.TFLiteConverter.from_frozen_graph('vww_96_grayscale_frozen.pb',
['input'], ['MobilenetV1/Predictions/Reshape_1'])
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8
&lt;/denchmark-code&gt;




&lt;denchmark-code&gt;converter =
tf.lite.TFLiteConverter.from_frozen_graph('vww_96_grayscale_frozen.pb',
['input'], ['MobilenetV1/Predictions/Reshape_1'])
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset_gen

tflite_quant_model = converter.convert()
open("vww_96_grayscale_quantized.tflite", "wb").write(tflite_quant_model)
&lt;/denchmark-code&gt;

Can you please &lt;denchmark-link:https://github.com/MeghnaNatraj&gt;@MeghnaNatraj&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/Gorospe&gt;@Gorospe&lt;/denchmark-link&gt;
 help me to resolve my issue.
Thanks,
Bhavika
		</comment>
		<comment id='10' author='Gorospe' date='2020-11-09T08:41:20Z'>
		Hello &lt;denchmark-link:https://github.com/bhavikapanara&gt;@bhavikapanara&lt;/denchmark-link&gt;

I guess the problem can be in the training / quantization phase or in the version of the library of Tensorflow Lite used for the Arduino. If I remember correctly, I used the first method you stated for quantization as well, but then I runt it on a STM32 microcontroller, where I included the library by hand, taking the necessary files from the last version of github.
Have you tried to use another pretrained network? If you want, I can send you my mobilenet so that you can confirm that the library works correctly.
		</comment>
	</comments>
</bug>