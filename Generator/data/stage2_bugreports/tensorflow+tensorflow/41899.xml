<bug id='41899' author='linkun-1998' open_date='2020-07-30T14:25:13Z' closed_time='2020-08-17T07:27:38Z'>
	<summary>Compilation Failure when using tf.keras.layers.UpSampling2D() with colab TPU</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):macOS, Mojave 10.14.6 (Running in Colab)
TensorFlow version (use command below):2.2.0
Python version: Python 3.6.9

Describe the current behavior
While using tf.keras.layers.Upsampling2D() in Colab TPU, The following error occurs:
&lt;denchmark-code&gt;&lt;ipython-input-34-7125d086c152&gt; in &lt;module&gt;()
     10   model.compile(optimizer='sgd', loss='mse', metrics=['accuracy'])
     11 model.summary()
---&gt; 12 history = model.fit(training_dataset, epochs=10, validation_data=validation_dataset, steps_per_epoch=steps_per_epoch)

10 frames
/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

UnimplementedError: {{function_node __inference_train_function_66860}} Compilation failure: CustomCall is not supported to have a dynamic dimension
	TPU compilation failed
	 [[{{node tpu_compile_succeeded_assert/_7048177641045036345/_5}}]]
&lt;/denchmark-code&gt;

Describe the expected behavior
If I use tf.keras.UpSampling2D((2,2)), I should expect the tensor to be resized by a factor of 2.
Standalone code to reproduce the issue
&lt;denchmark-code&gt;def create_model():
    
    pretrained_model = tf.keras.applications.Xception(input_shape=[*IMAGE_SIZE, 3], include_top=False)
    pretrained_model.trainable = True

    model = tf.keras.Sequential([
        pretrained_model,
        tf.keras.layers.MaxPooling2D((2,2)),
        tf.keras.layers.UpSampling2D((2,2)),
        tf.keras.layers.GlobalAveragePooling2D(),
        #tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(5, activation='softmax', dtype=tf.float32) # the float32 is needed on softmax layer when using mixed precision
    ])

    model.compile(
        optimizer='adam',
        loss = 'categorical_crossentropy',
        metrics=['accuracy']
    )

    return model
with strategy.scope():
  model = create_model()
model.summary()
start_time = time.time()
history = model.fit(training_dataset, validation_data=validation_dataset,
                    steps_per_epoch=TRAIN_STEPS, epochs=EPOCHS, callbacks=[lr_callback])

final_accuracy = history.history["val_accuracy"][-5:]
print("FINAL ACCURACY MEAN-5: ", np.mean(final_accuracy))
print("TRAINING TIME: ", time.time() - start_time, " sec")
&lt;/denchmark-code&gt;

This is the &lt;denchmark-link:https://colab.research.google.com/drive/1yABsMbLguooJutACqH3splUq3lMkc38N?usp=sharing&gt;link&lt;/denchmark-link&gt;
 to the Colab Notebook
Other info / logs
The training dataset is a publicly available TFRecord Dataset
	</description>
	<comments>
		<comment id='1' author='linkun-1998' date='2020-08-03T06:27:23Z'>
		&lt;denchmark-link:https://github.com/linkun-1998&gt;@linkun-1998&lt;/denchmark-link&gt;

Please refer to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/38234#issuecomment-609482622&gt;this link&lt;/denchmark-link&gt;
 and let us know if it helps resolve your issue.&lt;denchmark-link:https://stackoverflow.com/questions/57658114/how-to-solve-propagation-of-dynamic-dimension-failed-error-in-tf-keras-with-tp&gt;[link]&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='linkun-1998' date='2020-08-10T07:21:04Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='3' author='linkun-1998' date='2020-08-17T07:27:35Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='4' author='linkun-1998' date='2020-08-17T07:27:39Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41899&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41899&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='linkun-1998' date='2020-10-26T13:17:10Z'>
		If &lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;
 's answer doesn't solve it for you, there might be two different solutions to this error.
TPU requires explicitly defined shape of input in your data pipeline. First, make sure you're using tf.data pipeline since it offers best compatibility with TPUs.
As the shape of input image/label looks something like (batch_size, height, weight, channels), all of them have to be stated explicitly and statically for the TPU.

For h, w, c,  you should use tf.reshape when reading tf.Example and extracting image, label, etc. Example code below for reading TFRecords. Note that tf.reshape() has to be used even if it just preserves the original shape.

def read_tfrecord(record, dim):
    LABELED_TFREC_FORMAT = {
        "image": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring
        "label": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element
    }
    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)
    image = tf.io.decode_image(example['image'], dtype=tf.float32, channels=3)
    &gt;&gt;&gt;image = tf.reshape(image, (dim, dim, 3))&lt;&lt;&lt;
  
    label = tf.io.decode_image(example['label'], dtype=tf.float32, channels=1)
    &gt;&gt;&gt;label = tf.reshape(label, (dim, dim, 1))&lt;&lt;&lt;
 
    return image, label # returns a dataset of (image, label) pairs

Now, we also need to make sure batch size is constant. This can be ensured by using tf.data.Dataset.batch(batch_size, drop_remainder=True). Second argument is crucial here since without it, batch size would be dynamic in order to include the remainder.

		</comment>
	</comments>
</bug>