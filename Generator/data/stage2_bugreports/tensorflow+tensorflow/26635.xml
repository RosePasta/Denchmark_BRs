<bug id='26635' author='etragas-fathom' open_date='2019-03-13T02:34:27Z' closed_time='2019-03-26T21:31:02Z'>
	<summary>Transformer step/sec decrease over time to 0</summary>
	<description>
System information
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 18.04
- TensorFlow installed from (source or binary):
Binary / pip install
- TensorFlow version
1.13.1
- Python version:
3.6.7
- CUDA/cuDNN version:
CUDA = 10
CUDNN_VERSION 7.5
- GPU model and memory:
8X V100 16 GB
(We observed same behavior with CUDA 9.2 &amp; TF 1.12 compiled for CUDA 9.2.)
&lt;denchmark-link:https://gitlab.com/nvidia/cuda/blob/ubuntu18.04/9.2/devel/cudnn7/Dockerfile&gt;Docker Image&lt;/denchmark-link&gt;

Problem:
Training steps/s while using a Transformer model repeatedly drops from 20 steps/s to &lt;1 step/s.
This is internally reproducible.
GPU usage plummets to ~0% during the periods at &lt;1 step/s

We train with a &lt;denchmark-link:https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py#L175&gt;Transformer model&lt;/denchmark-link&gt;
.
Training behaves poorly with &lt;denchmark-link:https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py#L175&gt;Transformer&lt;/denchmark-link&gt;
, but works well with &lt;denchmark-link:https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/research/universal_transformer.py#L41&gt;Universal Transformer&lt;/denchmark-link&gt;
.
In both scenarios, we use 4x P100, subword tokens (&lt;denchmark-link:https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/text_encoder.py#L448&gt;https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/text_encoder.py#L448&lt;/denchmark-link&gt;
), and &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/contrib/distribute/MirroredStrategy&gt;MirroredStrategy Enabled&lt;/denchmark-link&gt;
 distribution strategy.
(We observed the same behavior with 8x V100, as well.)
Ablation tests on 1) P100 versus V100, 2) MirroredStrategy versus t2t’s built-in multi-GPU, and 3) Universal Transformer versus Transformer reveal that &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/3&gt;#3&lt;/denchmark-link&gt;
 is the driving variable.
We’re using tensor2tensor’s transformer / universal_transformer implementations.
The hparams used are transformer_tiny and universal_transformer_tiny. One noteworthy deviation from common usage is that our hparams.max_length value is large (2500) and our batch size is often small, since our median sequence length is 750 tokens.
Describe the current behaviour
With Transformer, training run’s step/sec alternates between roughly 20 step/sec and 0.5 step/sec.
For the first 3-4 hours it is biased towards 20 steps/sec. For the next is ~2 it hours it  begins dropping to 0.5 step/sec more frequently, before finally dropping almost exclusively to 0.5 step/sec.
If we restart our training process from a checkpoint that was made when the model ran slowly, the behaviour repeats itself, starting at 20 step/sec before dropping back down to 0.5.
Below are two graphs The first is a graph of our model’s step/sec degradation over a training run. The median value early on is ~20 (with some occasional drops to below 5 step/sec) but as the training continues our performance drops to almost 0 step/sec.
Note the vertical axis is logarithmic
&lt;denchmark-link:https://user-images.githubusercontent.com/43351375/54249496-c64c5880-4516-11e9-86ad-3a6d6f1c940b.png&gt;&lt;/denchmark-link&gt;

The second graph shows 3.25 consecutive runs of our model, where each restart picks up a checkpoint the previous run generated. These restarts were not caused an error, but are due to our system automatically preempting gpu intensive jobs after 24 hours. Note the consistent degradation in performance after every restart.
&lt;denchmark-link:https://user-images.githubusercontent.com/43351375/54249582-0b708a80-4517-11e9-9287-663c54799d00.png&gt;&lt;/denchmark-link&gt;

Describe the expected behavior
Our runs with universal transformer exhibit a completely flat step/sec curve. The figure below shows the expected behaviour in red in terms of step/sec variance. Note that the model’s step/sec exhibit almost no variation except for the sharp drops attributed to evaluation steps.
&lt;denchmark-link:https://user-images.githubusercontent.com/43351375/54287868-dc8cff80-457c-11e9-8c64-52dbfa98c833.png&gt;&lt;/denchmark-link&gt;

Other info / logs
Our GPU utilization (as measured by nvidia-smi) is tightly coupled with the above graph. Where our step/sec is high, our gpu utilization is nearly always at 50%, occasionally dropping to 0 for a second or two before shooting back up. When our step/sec consistently drops to below one, our gpu utilization is mostly at 0%. Every few minutes it will briefly shoot up to 50% and then drop to 0% a second later.
In terms of auc performance, our transformer model continues to improve even as the step/sec decay.
Note this is a sibling issue to: &lt;denchmark-link:https://github.com/tensorflow/tensor2tensor/issues/1484&gt;tensorflow/tensor2tensor#1484&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='etragas-fathom' date='2019-03-19T19:21:22Z'>
		Hey &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 any thoughts on this?
		</comment>
		<comment id='2' author='etragas-fathom' date='2019-03-26T21:31:02Z'>
		It is better to first investigate in &lt;denchmark-link:https://github.com/tensorflow/tensor2tensor/issues/1484&gt;tensorflow/tensor2tensor#1484&lt;/denchmark-link&gt;
 to isolate the issue from Tensor2Tensor specific application logic.
Please reopen this issue when further investigation is done and the issue is reproducible with MCV example as described in &lt;denchmark-link:https://stackoverflow.com/help/mcve&gt;https://stackoverflow.com/help/mcve&lt;/denchmark-link&gt;
. You should also consider using profiler to identify the bottleneck.
		</comment>
		<comment id='3' author='etragas-fathom' date='2019-03-26T21:31:03Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=26635&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=26635&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>