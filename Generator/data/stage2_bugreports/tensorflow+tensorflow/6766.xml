<bug id='6766' author='jrosti' open_date='2017-01-10T12:06:06Z' closed_time='2018-01-23T21:12:41Z'>
	<summary>softmax_cross_entropy_with_logits aborts the process, if a tensor with zero first dimension is passed as an argument</summary>
	<description>
&lt;denchmark-h:h3&gt;Environment info&lt;/denchmark-h&gt;

Operating System: Ubuntu 16.04
Installed version of CUDA and cuDNN: CUDA-8.0, CUDNN 5.1.5
Tensorflow version: 0.12.1 installed from
&lt;denchmark-link:https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp35-cp35m-linux_x86_64.whl&gt;https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp35-cp35m-linux_x86_64.whl&lt;/denchmark-link&gt;

Reproduced also using tf-0.11.0, CUDA-7.5, CUDNN-5.1.3
&lt;denchmark-h:h3&gt;Minimal reproducible example&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import tensorflow as tf
y = tf.placeholder("int64", [None], "y")
one_hot_y=tf.one_hot(y,10)
ce = tf.nn.softmax_cross_entropy_with_logits(one_hot_y, one_hot_y)
sess = tf.Session()
sess.run(ce, {y: []})
&lt;/denchmark-code&gt;

Result on GPU:
&lt;denchmark-code&gt;E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes
W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes
F tensorflow/core/common_runtime/gpu/gpu_device.cc:104] EigenAllocator for GPU ran out of memory when allocating 0. See error logs for more detailed info.
Aborted (core dumped)
&lt;/denchmark-code&gt;

Result on CPU:
&lt;denchmark-code&gt;array([], dtype=float32)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='jrosti' date='2017-01-11T05:10:08Z'>
		&lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
 Here's a GPU memory alloc issue. Reported both here and on Stack Overflow: &lt;denchmark-link:http://stackoverflow.com/questions/41530966/memory-error-with-eigenallocator&gt;http://stackoverflow.com/questions/41530966/memory-error-with-eigenallocator&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='jrosti' date='2017-04-09T09:34:00Z'>
		Hi, &lt;denchmark-link:https://github.com/jrosti&gt;@jrosti&lt;/denchmark-link&gt;

Have this problem been solved?
I met the same problem in 
		</comment>
		<comment id='3' author='jrosti' date='2017-04-19T17:20:12Z'>
		I still have this problem which seems to arise when using tensorflow fold.
		</comment>
		<comment id='4' author='jrosti' date='2017-06-02T04:06:18Z'>
		Still getting this issue, as of 06/02/2017 when using tf.gather_nd(...) and softmax_cross_entropy_with_logits(...) together.
		</comment>
		<comment id='5' author='jrosti' date='2017-06-24T23:02:07Z'>
		Anyone looking into this? I'm also experiencing this issue using TensorFlow 1.2.0 (v1.2.0-rc2-21-g12f033d). In my case, a tf.while_loop is being used (in conjunction with tf.TensorArray); however, the same error occurs if I swap out the tf.while_loop for a while statement.
Backtrace (abbr):
&lt;denchmark-code&gt;2017-06-24 05:42:28.894580: E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes
2017-06-24 05:42:28.894638: W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes
2017-06-24 05:42:28.894647: E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes
2017-06-24 05:42:28.894657: W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes
2017-06-24 05:42:28.895314: E tensorflow/core/common_runtime/bfc_allocator.cc:378] tried to deallocate nullptr
2017-06-24 05:42:28.895406: E tensorflow/core/common_runtime/bfc_allocator.cc:378] tried to deallocate nullptr
2017-06-24 05:42:28.896185: W tensorflow/core/framework/op_kernel.cc:1158] Resource exhausted: Ran out of GPU memory when allocating 0 bytes for
         [[Node: while/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/gpu:0"](while/Reshape, while/Reshape_1)]]
2017-06-24 05:42:28.896209: W tensorflow/core/framework/op_kernel.cc:1158] Resource exhausted: Ran out of GPU memory when allocating 0 bytes for
         [[Node: while/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/gpu:0"](while/Reshape, while/Reshape_1)]]
.
.
.
2017-06-24 05:42:28.905449: W tensorflow/core/framework/op_kernel.cc:1158] Resource exhausted: Ran out of GPU memory when allocating 0 bytes for
         [[Node: while/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/gpu:0"](while/Reshape, while/Reshape_1)]]
2017-06-24 05:42:28.905669: W tensorflow/core/framework/op_kernel.cc:1158] Resource exhausted: Ran out of GPU memory when allocating 0 bytes for
         [[Node: while/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/gpu:0"](while/Reshape, while/Reshape_1)]]
&lt;/denchmark-code&gt;

A bit further down,
&lt;denchmark-code&gt;Caused by op 'while/SoftmaxCrossEntropyWithLogits', defined at:
  ...
  File "scripts/gpu_experiment.py", line 400, in build_backend
    backend['o'] = build_outputs(config, backend.get('o', None))
  File "scripts/gpu_experiment.py", line 363, in build_outputs
    loop_vars = tf.while_loop(loop_cond, _build_outputs, loop_vars)
    ...
    cross_ent = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='jrosti' date='2017-07-25T09:14:01Z'>
		I'm also experiencing this issue and error message is same as  &lt;denchmark-link:https://github.com/j-wilson&gt;@j-wilson&lt;/denchmark-link&gt;
   . I am fine tuning object detction api on faster_rcnn_inception_resnet_v2_atrous_coco_11_06_2017 model.ckpt.
		</comment>
		<comment id='7' author='jrosti' date='2017-08-24T11:27:15Z'>
		Also having the same problem in tf 1.3. Would be great if it threw a more descriptive exception.
		</comment>
		<comment id='8' author='jrosti' date='2017-09-17T17:38:02Z'>
		I hit this issue.  After some investigating I realized I was feeding in an empty tensor.  Not sure if that is the only thing that can cause it, but that was my problem at least.
It was easy to fix, but it certainly would be nice if it threw a more descriptive error.
		</comment>
		<comment id='9' author='jrosti' date='2017-11-02T12:02:43Z'>
		Massive thanks. Empty tensor is the cause of my problem too. &lt;denchmark-link:https://github.com/metachi&gt;@metachi&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='jrosti' date='2017-12-20T19:25:48Z'>
		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
		</comment>
		<comment id='11' author='jrosti' date='2018-01-04T19:18:11Z'>
		It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
		</comment>
		<comment id='12' author='jrosti' date='2018-01-10T18:25:03Z'>
		I'm facing same problem even checking if tensor is empty or not:
&lt;denchmark-code&gt;
 loss = K.switch(tf.size(y_true) &gt; 0,
                    tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y_true, dim=1),
                    tf.constant(0.0))
&lt;/denchmark-code&gt;

		</comment>
		<comment id='13' author='jrosti' date='2018-01-10T18:43:20Z'>
		&lt;denchmark-link:https://github.com/filipetrocadoferreira&gt;@filipetrocadoferreira&lt;/denchmark-link&gt;
 Have you tried to wrap the conditional branches into lambda functions?
&lt;denchmark-code&gt;loss = K.switch(tf.size(y_true) &gt; 0,
                    lambda: tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y_true, dim=1),
                    lambda: tf.constant(0.0))
&lt;/denchmark-code&gt;

This should allow a lazy execution of the branches. (see &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/cond&gt;tf.cond&lt;/denchmark-link&gt;
)
		</comment>
		<comment id='14' author='jrosti' date='2018-01-11T09:19:23Z'>
		wow, this seems to work. Can you link to an explanation?
		</comment>
		<comment id='15' author='jrosti' date='2018-01-11T21:24:41Z'>
		Added PR &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/16051&gt;#16051&lt;/denchmark-link&gt;
 for a fix.
		</comment>
		<comment id='16' author='jrosti' date='2018-01-11T22:42:24Z'>
		&lt;denchmark-link:https://github.com/filipetrocadoferreira&gt;@filipetrocadoferreira&lt;/denchmark-link&gt;
 I can’t find any link with an extensive explanation (I tought it was explained in tf.cond ’s documentation but actually it is not). In short, if you don’t define the branches as functions, they will be both executed regardless of the condition. This explains why you still had the problem despite checking the tensor’s size.
		</comment>
	</comments>
</bug>