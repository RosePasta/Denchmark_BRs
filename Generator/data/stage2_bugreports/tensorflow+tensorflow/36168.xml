<bug id='36168' author='maxiwelian' open_date='2020-01-23T20:08:59Z' closed_time='2020-04-30T16:19:11Z'>
	<summary>@tf.custom_gradient does not behave as expected when used with @tf.function (for 2nd order derivatives)</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;



Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes


OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04


TensorFlow installed from (source or binary): binary


TensorFlow version (use command below): 2.2.0-dev20200123 (observed in other versions)


Python version: 3.7.6


CUDA/cuDNN version: These are examples of versions though this has been replicated in other versions of cuda/cudnn


&lt;denchmark-code&gt;CUDA Version 10.1.243

#define CUDNN_MAJOR 7
#define CUDNN_MINOR 6
#define CUDNN_PATCHLEVEL 4
--
#define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)


- **GPU model and memory**: 
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Quadro P3200        On   | 00000000:01:00.0 Off |                  N/A |
| N/A   53C    P5     9W /  N/A |   5949MiB /  6078MiB |     10%      Default |
+-------------------------------+----------------------+----------------------+
&lt;/denchmark-code&gt;


Exact command to reproduce:

&lt;denchmark-code&gt;import tensorflow as tf

print(tf.__version__)

@tf.function
@tf.custom_gradient
def _fn2(a, logdet_a):

    def sec_order_grad(a_dash): # bug still appears if this fn outside custom grad call
        return a_dash * 5., tf.ones_like(logdet_a) * 2.
    
    return a, sec_order_grad

@tf.function
@tf.custom_gradient
def fwd(a):
    sign_a, logdet_a = tf.linalg.slogdet(a)

#     logdet_a = tf.stop_gradient(logdet_a) # include to function
    def grad(dy):
        da = _fn2(a, logdet_a)
        return dy * da

    return 1., grad

ndim = 2

def call_gradient_1st():
    a = tf.zeros((ndim, ndim))
    with tf.GradientTape() as g:
        g.watch(a)
        y = fwd(a)
    z = g.gradient(y, a)
    return y, z


def call_gradient_2nd():
    a = tf.zeros((ndim, ndim))
    with tf.GradientTape() as g:
        g.watch(a)
        with tf.GradientTape() as gg:
            gg.watch(a)
            y = fwd(a)
        z = gg.gradient(y, a)
    gg = g.gradient(z, a)
    return y, z, gg

y, z = call_gradient_1st()
print(y, z)
y, z, gg = call_gradient_2nd()
print(y, z, gg)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

Autograd attempts to call the gradient of a variable inside a custom gradient function. If the gradient is not computable (i.e. in this case when logdet has inf values) then it crashes.
This only appears in the second order (see trace: first order calls, second order doesn't).
This doesn't appear if we stop the gradient (see note in function 'fwd').
This doesn't appear if we do not use the @tf.function wrapper.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;2.2.0-dev20200123
tf.Tensor(1.0, shape=(), dtype=float32) tf.Tensor(
[[0. 0.]
 [0. 0.]], shape=(2, 2), dtype=float32)

---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
&lt;ipython-input-6-af7d2f010d71&gt; in &lt;module&gt;
     48 y, z = call_gradient_1st()
     49 print(y, z)
---&gt; 50 y, z, gg = call_gradient_2nd()
     51 print(y, z, gg)

&lt;ipython-input-6-af7d2f010d71&gt; in call_gradient_2nd()
     42             gg.watch(a)
     43             y = fwd(a)
---&gt; 44         z = gg.gradient(y, a)
     45     gg = g.gradient(z, a)
     46     return y, z, gg

~/anaconda3/envs/fermi21/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)
   1032         output_gradients=output_gradients,
   1033         sources_raw=flat_sources_raw,
-&gt; 1034         unconnected_gradients=unconnected_gradients)
   1035 
   1036     if not self._persistent:

~/anaconda3/envs/fermi21/lib/python3.7/site-packages/tensorflow_core/python/eager/imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)
     75       output_gradients,
     76       sources_raw,
---&gt; 77       compat.as_str(unconnected_gradients.value))

~/anaconda3/envs/fermi21/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _backward_function_wrapper(*args)
   1303           break
   1304       return backward._call_flat(  # pylint: disable=protected-access
-&gt; 1305           processed_args, remapped_captures)
   1306 
   1307     return _backward_function_wrapper, recorded_outputs

~/anaconda3/envs/fermi21/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1748       flat_outputs = forward_function.call(
   1749           ctx, args_with_tangents,
-&gt; 1750           cancellation_manager=cancellation_manager)
   1751     else:
   1752       with ops.get_default_graph()._override_gradient_function(  # pylint: disable=protected-access

~/anaconda3/envs/fermi21/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in call(self, ctx, args, cancellation_manager)
    596               inputs=args,
    597               attrs=attrs,
--&gt; 598               ctx=ctx)
    599         else:
    600           outputs = execute.execute_with_cancellation(

~/anaconda3/envs/fermi21/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     58     ctx.ensure_initialized()
     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
---&gt; 60                                         inputs, attrs, num_outputs)
     61   except core._NotOkStatusException as e:
     62     if name is not None:

InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  Input is not invertible.
	 [[node gradients/LogMatrixDeterminant_grad/MatrixInverse (defined at &lt;ipython-input-6-af7d2f010d71&gt;:43) ]]
	 [[gradients/grad_ys_0/_4]]
  (1) Invalid argument:  Input is not invertible.
	 [[node gradients/LogMatrixDeterminant_grad/MatrixInverse (defined at &lt;ipython-input-6-af7d2f010d71&gt;:43) ]]
0 successful operations.
0 derived errors ignored. [Op:__forward___backward_fwd_655_746]

Function call stack:
__backward_fwd_655 -&gt; __backward_fwd_655
&lt;/denchmark-code&gt;

Thanks for any insight!
	</description>
	<comments>
		<comment id='1' author='maxiwelian' date='2020-01-24T10:46:51Z'>
		Issue is replicating on colab with Tf  2.2.0-dev20200123.
Please find the &lt;denchmark-link:https://colab.research.google.com/gist/gadagashwini/a80d6d940295d12a296ab434e9791371/untitled23.ipynb&gt;gist here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='maxiwelian' date='2020-01-25T13:12:15Z'>
		&lt;denchmark-link:https://github.com/saxenasaurabh&gt;@saxenasaurabh&lt;/denchmark-link&gt;
 could you triage?
		</comment>
		<comment id='3' author='maxiwelian' date='2020-01-28T16:50:55Z'>
		&lt;denchmark-link:https://github.com/allenlavoie&gt;@allenlavoie&lt;/denchmark-link&gt;
 do you have any ideas?
		</comment>
		<comment id='4' author='maxiwelian' date='2020-01-28T17:32:34Z'>
		I commented out the two @tf.function lines and I see the same "Input not invertible" error (from MatrixInverse via LogMatrixDeterminantGrad). Adding back the stop_gradient does avoid the error in either case. This is with 2.2.0-dev20200128.
It's totally possible that tf.function changes the gradient graphs for higher-order gradients in some cases. They're still correct (assuming the original registrations are correct), but there are cases where we could/should do extra pruning or avoid building some extra graphs in the first place. I don't see evidence of that issue from this repro, though.
Is the argument that the stop_gradient should be implicit here?
		</comment>
		<comment id='5' author='maxiwelian' date='2020-02-04T17:47:02Z'>
		My intuition would be that stop_gradient is implicit when functions are called within tf.custom_gradient
		</comment>
		<comment id='6' author='maxiwelian' date='2020-04-30T08:43:29Z'>
		
Issue is replicating on colab with Tf 2.2.0-dev20200123.
Please find the gist here. Thanks!

Did u find the fix to the problem
		</comment>
		<comment id='7' author='maxiwelian' date='2020-04-30T16:11:24Z'>
		&lt;denchmark-link:https://github.com/rajatkhanna1999&gt;@rajatkhanna1999&lt;/denchmark-link&gt;
 Please provide a standalone code to reproduce the issue. Thanks!
		</comment>
		<comment id='8' author='maxiwelian' date='2020-04-30T16:19:10Z'>
		I don't think we want tf.function to change gradient behavior. Please do insert stop_gradients as needed.
		</comment>
		<comment id='9' author='maxiwelian' date='2020-04-30T16:19:12Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36168&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36168&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>