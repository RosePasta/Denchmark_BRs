<bug id='37141' author='JCL823' open_date='2020-02-27T20:11:03Z' closed_time='2020-02-28T22:10:43Z'>
	<summary>ValueError: No gradients provided for any variable in Tensorflow 2.0, WAE</summary>
	<description>
I am a tensorflow2.0 learner who is trying to reproduce the code of Wasserstein AutoEncoder or Adversarial AutoEncoder and do something interesting based on that on my own. My code below is based on&lt;denchmark-link:https://github.com/timsainb/tensorflow2-generative-models&gt; timsainb/tensorflow2-generative-models&lt;/denchmark-link&gt;
 with some of modifications.
&lt;denchmark-code&gt;%tensorflow_version 2.x
import tensorflow as tf
import numpy as np
import pandas as pd
import tensorflow_probability as tfp
from tqdm import tqdm
'''Python 3.6.6
   pip install tensorflow==2.0.0b1
   pip install tfp-nightly==0.7.0.dev20190510
   pip install tensorflow_probability==0.8.0rc0 --user --upgrade'''

ds = tfp.distributions
tf.keras.backend.set_floatx('float32') # sets dtype as tf.float32

#%%%
'''Define the network as tf.keras.model object'''

class VAE(tf.keras.Model):
    """a VAE class for tensorflow

    Extends:
        tf.keras.Model
    """

    def __init__(self, **kwargs):
        super(VAE, self).__init__()
        self.__dict__.update(kwargs)

        self.pad3 = 'same'    

        self.disc = tf.keras.Sequential([
            tf.keras.layers.InputLayer(input_shape=int(self.intrinsic_size/4) * int(self.intrinsic_size/4)),
            tf.keras.layers.Dense(units=32, activation="relu", name="disc1"),
            tf.keras.layers.Dense(units=64, activation="relu", name="disc2"),
            tf.keras.layers.Dense(units=128, activation="relu", name="disc3"),
            tf.keras.layers.Dense(units=1, activation=None)
            ])

        self.enc = tf.keras.Sequential([
            tf.keras.layers.InputLayer(input_shape=self.ipt_shape),        
            tf.keras.layers.Conv2D(filters=self.hidden_layer_sizes[0], kernel_size=5, strides=(2, 2), padding='same', activation=self.activation, name="conv1"),                 
            tf.keras.layers.BatchNormalization(),            
            tf.keras.layers.Conv2D(filters=self.hidden_layer_sizes[1], kernel_size=5, strides=(2, 2), padding='same', activation=self.activation, name="conv2"),                                       
            tf.keras.layers.BatchNormalization(),                   
            tf.keras.layers.Conv2D(self.hidden_layer_sizes[2], kernel_size=3, strides=(2, 2), padding='same', activation=self.activation, name="conv3"),
            tf.keras.layers.BatchNormalization(),                
            tf.keras.layers.Flatten(),
            tf.keras.layers.Dense(units=self.intrinsic_size + self.intrinsic_size * self.intrinsic_size, use_bias=False, name="lastlayer") #no activation function                
            ]) 

        self.dec = tf.keras.Sequential([
            tf.keras.layers.Dense(units=self.hidden_layer_sizes[2]*int(self.ipt_shape[0]/4)*int(self.ipt_shape[0]/4), activation=self.activation, name="revealed"),                
            tf.keras.layers.Reshape(target_shape=(int(self.ipt_shape[0]/4), int(self.ipt_shape[0]/4), self.hidden_layer_sizes[2])),                                     
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Conv2DTranspose(filters=self.hidden_layer_sizes[1], kernel_size=3, strides=(2, 2), padding='same', activation=self.activation, name="deconv3"),                                       
            tf.keras.layers.BatchNormalization(),                      
            tf.keras.layers.Conv2DTranspose(filters=self.hidden_layer_sizes[0], kernel_size=5, strides=(2, 2), padding='same', activation=self.activation, name="deconv2"),
            tf.keras.layers.BatchNormalization(),      
            tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=5, strides=(1, 1), padding='same', activation="sigmoid", name="deconv1")
            ])

        self.vae_optimizer = tf.keras.optimizers.Adam(self.lr)
        self.disc_optimizer = tf.keras.optimizers.Adam(self.lr)
        self.enc_optimizer = tf.keras.optimizers.Adam(self.lr)


    def encode(self, x):
        mu, sigma = tf.split(self.enc(x), num_or_size_splits=[self.intrinsic_size, self.intrinsic_size * self.intrinsic_size], axis=1)
        return mu, sigma    

    def enc_dist(self, x):
        mu, sigma = self.encode(x)
        M = tf.reshape(sigma, (sigma.shape[0], self.intrinsic_size, self.intrinsic_size))        
        N = ds.MultivariateNormalDiag(loc=np.zeros((sigma.shape[0], self.intrinsic_size)), scale_diag=[1.0]*self.intrinsic_size)              
        eps = tf.expand_dims(tf.cast(N.sample(), tf.float32), -1)
        z = tf.expand_dims(mu, -1) + tf.matmul(M, eps)
        z = tf.squeeze(z)        
        return z

    def discriminate(self, x):
        return self.disc(x) 


    def decode(self, z):
        return self.dec(z) 

    def gradient_penalty(self, x, x_gen):
        epsilon = tf.random.uniform([x.shape[0], 1], 0.0, 1.0)
        x_hat = epsilon * x + (1 - epsilon) * x_gen
        self.x_hat = x_hat
        with tf.GradientTape() as t:
            t.watch(x_hat)
            d_hat = self.discriminate(x_hat)
        gradients = t.gradient(d_hat, x_hat)
        ddx = tf.sqrt(tf.reduce_sum(gradients ** 2, axis=[1]))
        d_regularizer = tf.reduce_mean((ddx - 1.0) ** 2)
        return d_regularizer


    def compute_loss_vae(self, x):   
        vae_loss = []                
        latent_loss = []
        enc_loss = []        
        for _ in range(int(self.num_sampling)):           

            self.z = z = self.enc_dist(x)             

            self.x_recon = x_recon = self.decode(z)
            N = ds.MultivariateNormalDiag(loc=np.zeros((z.shape[0], self.intrinsic_size)), scale_diag=[1.] * self.intrinsic_size)                
            self.z_gen = z_gen = tf.cast(N.sample(), tf.float32)

            self.logits_z = logits_z = self.discriminate(z)
            self.logits_z_gen = logits_z_gen = self.discriminate(z_gen)            


            self.d_regularizer = d_regularizer = self.gradient_penalty(z, z_gen)
            self.wasserstein = wasserstein = tf.reduce_mean(logits_z_gen) - tf.reduce_mean(logits_z) + d_regularizer * self.gradient_penalty_weight

            rec = tf.reduce_mean(tf.reduce_sum(tf.math.square(x - x_recon), axis=0))                      
            enc = -tf.reduce_mean(logits_z)

            vae_loss.append(rec)
            latent_loss.append(wasserstein) 
            enc_loss.append(enc)                     

        self.vae_loss = vae_loss = tf.reduce_mean(vae_loss, axis=0)        
        self.latent_loss = latent_loss = tf.reduce_mean(latent_loss, axis=0)    
        self.enc_loss = enc_loss = tf.reduce_mean(enc_loss, axis=0)
        return (vae_loss, latent_loss, enc_loss)


    def compute_gradients(self, x):               
        with tf.GradientTape() as vae_tape, tf.GradientTape() as disc_tape, tf.GradientTape() as enc_tape:
            vae_loss, latent_loss, enc_loss = self.compute_loss_vae(x)

        self.vae_gradients = vae_gradients = vae_tape.gradient( vae_loss, self.enc.trainable_variables +  self.dec.trainable_variables )            
        self.disc_gradients = disc_gradients = disc_tape.gradient( -latent_loss, self.disc.trainable_variables )
        self.enc_gradients = enc_gradients = enc_tape.gradient(enc_loss, self.enc.trainable_variables )

        return vae_gradients, disc_gradients, enc_gradients



    def apply_gradients_vae(self, vae_gradients, disc_gradients, enc_gradients):        
        self.vae_optimizer.apply_gradients(
            zip(vae_gradients, self.enc.trainable_variables + self.dec.trainable_variables)
        )
        self.disc_optimizer.apply_gradients(
            zip(disc_gradients, self.disc.trainable_variables)
        )       

        self.enc_optimizer.apply_gradients(
            zip(enc_gradients, self.enc.trainable_variables)
        )

    @tf.function
    def train(self, x):
        vae_gradients, disc_gradients, enc_gradients = self.compute_gradients(x)
        self.apply_gradients_vae(vae_gradients, disc_gradients, enc_gradients)


    def fit(self, x):

        losses = pd.DataFrame(columns=['vae_loss', 'disc_loss', 'enc_loss'])
        Vae_loss = Disc_loss = Enc_loss = []

        n_samples, n_height, n_width, n_channel = x.shape


        n_batch = (n_samples - 1) // self.batch_size + 1
        self.n_batch = n_batch

        idx = np.arange(int(n_samples))
        np.random.shuffle(idx)       

        for epoch in range(self.epoch_size):
            print(f" Epoch: {epoch+1}/{self.epoch_size}")

            loss = []
            for batch in tqdm(range(self.n_batch), position=0):
                i_start = int(batch * self.batch_size)
                i_end = int((batch + 1) * self.batch_size)
                self.i_start = i_start
                self.i_end = i_end
                self.x_batch = x_batch = x[ idx[ i_start : i_end ] ]  
                self.train(x_batch)


                loss.append(self.compute_loss_vae(x_batch))
                self.loss = loss
            losses.loc[len(losses.T)] = np.mean(loss, axis=0)



            self.Vae_loss = Vae_loss.append(np.mean(loss, axis=0)[0])
            self.Disc_loss = Disc_loss.append(np.mean(loss, axis=0)[1])
            self.Enc_loss = Enc_loss.append(np.mean(loss, axis=0)[2])
            print(" epoch: {}/{} : &amp;\n {}".format(epoch+1, self.epoch_size, losses))

(X_train_origin, y_train_origin), (X_test_origin, y_test_origin) = tf.keras.datasets.mnist.load_data()


if __name__ == "__main__":

    input_shape = (28,28,1)
    num_train = 5000                  
    X_train = X_train_origin[0:num_train]                 
    X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype("float32") / 255.0

    model = VAE(
            lr = 1e-3,
            latent_loss_div=1, 
            epoch_size = 2, 
            batch_size = 128,
            ipt_shape = input_shape,
            activation = "relu",
            hidden_layer_sizes = (32,64,128),
            intrinsic_size = 16,
            num_sampling = 3,
            gradient_penalty_weight = 10.0,
            prior_c = 0.2,
            )

    model.fit(X_train)
&lt;/denchmark-code&gt;

However, I always get a ``No gradients provided'' error. Please check the description:
&lt;denchmark-code&gt;/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    704           except Exception as e:  # pylint:disable=broad-except
    705             if hasattr(e, "ag_error_metadata"):
--&gt; 706               raise e.ag_error_metadata.to_exception(type(e))
    707             else:
    708               raise

ValueError: in converted code:

    &lt;ipython-input-9-8e8f91e623b2&gt;:158 train  *
        self.apply_gradients_vae(vae_gradients, disc_gradients, enc_gradients)
    &lt;ipython-input-9-8e8f91e623b2&gt;:148 apply_gradients_vae
        zip(disc_gradients, self.disc.trainable_variables)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:427 apply_gradients
        grads_and_vars = _filter_grads(grads_and_vars)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:975 _filter_grads
        ([v.name for _, v in grads_and_vars],))

    ValueError: No gradients provided for any variable: ['disc1_4/kernel:0', 'disc1_4/bias:0', 'disc2_4/kernel:0', 'disc2_4/bias:0', 'disc3_4/kernel:0', 'disc3_4/bias:0', 'dense_4/kernel:0', 'dense_4/bias:0'].
&lt;/denchmark-code&gt;

If I print the gradients of discriminators, it always shows:
ListWrapper([None, None, None, None, None, None, None, None])
I know that the issue must be in the discriminator part (because once I remove the apply gradient to discriminator, the code works). However, I don't know how to fix it with discriminator and I've been thinking about it for several days.
Could you please let me know where I didn't connect the network successfully? Or I stupidly messed up something somewhere?
	</description>
	<comments>
		<comment id='1' author='JCL823' date='2020-02-28T10:36:29Z'>
		I have tried on colab with TF version 2.1.0 and was able to reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/1b66029ea4fa213685d03bbd323878eb/untitled689.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='JCL823' date='2020-02-28T21:28:32Z'>
		&lt;denchmark-link:https://github.com/JCL823&gt;@JCL823&lt;/denchmark-link&gt;
 I think tients are performed outside the tape context and this is the reason why the gradients are not available.
For more information, please take a look at this &lt;denchmark-link:https://stackoverflow.com/questions/58947679/no-gradients-provided-for-any-variable-in-tensorflow2-0&gt;issue&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='JCL823' date='2020-02-28T22:09:08Z'>
		&lt;denchmark-link:https://github.com/gowthamkpr&gt;@gowthamkpr&lt;/denchmark-link&gt;
 Oh! Thank you so much!!! You provided me a huge hint!!!
I just simply remove the negative sign for  to be:

then it works!!
		</comment>
		<comment id='4' author='JCL823' date='2020-02-28T22:10:45Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37141&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37141&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>