<bug id='15034' author='khanrc' open_date='2017-12-01T10:24:45Z' closed_time='2018-01-31T23:02:26Z'>
	<summary>Optimize graph &amp; graph transform tools do not support NCHW</summary>
	<description>
I tried optimizing graph using both &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md&gt;Graph transform tool&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/optimize_for_inference.py&gt;Optimize graph for inference&lt;/denchmark-link&gt;
. Both cases produced the same error because the fused batchnorm used not NCHW, but NHWC. I've got the error like this:
&lt;denchmark-code&gt;InvalidArgumentError (see above for traceback): Must provide as many biases as the channel dimension of the input tensor: [256] vs. 19 in [1,256,19,19]
	 [[Node: prefix/convblock/BatchNorm/FusedBatchNorm = BiasAdd[T=DT_FLOAT, data_format="NHWC", _device="/job:localhost/replica:0/task:0/device:GPU:0"](prefix/convblock/Conv2D, prefix/convblock/Conv2D_bn_offset)]
&lt;/denchmark-code&gt;

Although NCHW is faster than NHWC in GPU environment, why the tools do not support NCHW?
	</description>
	<comments>
		<comment id='1' author='khanrc' date='2017-12-01T17:47:50Z'>
		Could you provide a reproducible test case of what exactly you tried to do? Generally speaking, I think a lot of the tooling after training requires NHWC, as that was the only format when those were written. If you could provide a reproducible test case, we could work to improve it. &lt;denchmark-link:https://github.com/petewarden&gt;@petewarden&lt;/denchmark-link&gt;
, do you have any other comments?
		</comment>
		<comment id='2' author='khanrc' date='2017-12-04T03:04:48Z'>
		I tried:
&lt;denchmark-code&gt;python tensorflow/python/tools/optimize_for_inference.py \
--input ./ckpt/frozen_model.pb \
--output ./ckpt/optimized_model.pb \
--frozen_graph true \
--input_names Placeholder \
--output_names policy_head/softmax,value_head/value/Tanh
&lt;/denchmark-code&gt;

and
&lt;denchmark-code&gt;tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
--in_graph='./ckpt/frozen_model.pb' \
--out_graph='./ckpt/transformed_model.pb' \
--inputs='Placeholder' \
--outputs='policy_head/softmax,value_head/value/Tanh' \
--transforms='
fold_constants(ignore_errors=true)
fold_batch_norms
fold_old_batch_norms
fuse_pad_and_conv
fuse_resize_and_conv
fuse_resize_pad_and_conv
'
&lt;/denchmark-code&gt;

In both cases, the error occurred in fused batchnorm. The frozen model worked well, but the optimized model and transformed model emitted error.
		</comment>
		<comment id='3' author='khanrc' date='2017-12-22T07:34:33Z'>
		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
		</comment>
		<comment id='4' author='khanrc' date='2018-01-05T19:08:07Z'>
		Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='5' author='khanrc' date='2018-01-24T13:17:05Z'>
		Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
	</comments>
</bug>