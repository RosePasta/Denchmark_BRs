<bug id='19277' author='YijinLiu' open_date='2018-05-14T19:29:07Z' closed_time='2020-06-26T20:44:36Z'>
	<summary>SSD mobilenet inference is slower w/ MKL</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04
TensorFlow installed from (source or binary):
Source
TensorFlow version (use command below):
1.8.0 (could repro the same problem from head too)
Python version:
N/A
Bazel version (if compiling from source):
0.13.0
GCC/Compiler version (if compiling from source):
5.4.0
CUDA/cuDNN version:
N/A
GPU model and memory:
N/A
Exact command to reproduce:
OMP_NUM_THREADS=1 bazel run --config=mkl --config=opt --config=monolithic //tensorflow/tools/benchmark:benchmark_model -- --graph=ssd_mobilenet_v2_coco_2018_03_29_frozen.pb --show_flops --input_layer=image_tensor --input_layer_type=uint8 --input_layer_shape=1,1920,1080,3 --output_layer=num_detections,detection_classes,detection_scores,detection_boxes --num_threads=1

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

w/ MKL, benchmark_model got 18.98B FLOPs/second, w/o MKL, it got 25.61B.
From the benchmark_model results, we could see that _MklConv2DWithBias is the culprit.
I am using MKL 2018.2.199 and mkldnn 0.14 on a i7-5557U CPU.
A unrelated question: &lt;denchmark-link:https://github.com/agramesh1&gt;@agramesh1&lt;/denchmark-link&gt;
 I filed the same bug on &lt;denchmark-link:https://github.com/intel/mkl-dnn/issues/234&gt;mkldnn&lt;/denchmark-link&gt;
. They told me you have some plan to implement MKL version of DepthwiseConv2dNative. Do you have a timeline for it? I am eager to try it..
benchmark_model results:
w/ MKL
&lt;denchmark-code&gt;         [Node type]  [count]  [avg ms]    [avg %]    [cdf %]  [mem KB][times called]
   _MklConv2DWithBias       12   143.936    42.710%    42.710% 56555.531       12
           _MklConv2D       43    48.872    14.502%    57.212% 43206.305       43
  DepthwiseConv2dNative       21    27.330     8.110%    65.321% 17180.992       21
              _MklMul       43    12.751     3.784%    69.105% 32070.080       43
              _MklAdd       53    12.137     3.601%    72.706% 33662.238       53
                 Cast      183    11.584     3.437%    76.143% 24883.217      183
    _MklInputConversion       96    10.990     3.261%    79.404% 32261.969       96
                Const     1602    10.360     3.074%    82.479%     0.000     1602
                  Mul      127     7.063     2.096%    84.574%     0.004      127
                  Add      119     6.541     1.941%    86.515%    15.344      119
                Relu6       47     3.715     1.102%    87.618%     0.000       47
              Minimum      451     3.680     1.092%    88.709%     0.000      451
               Gather      546     3.607     1.070%    89.780%     0.000      546
                Slice       93     3.543     1.051%    90.831%  1380.240       93
   TensorArrayScatterV3        5     3.261     0.968%    91.799% 25604.012        5
          _MklReshape      107     3.183     0.944%    92.743%   810.972      107
                Where      180     2.560     0.760%    93.503%     1.440      180
         _MklConcatV2       98     2.260     0.671%    94.173%   782.604       98
              Greater      183     2.183     0.648%    94.821%   172.533      183
&lt;/denchmark-code&gt;

w/o MKL
&lt;denchmark-code&gt;          [Node type]  [count]  [avg ms]    [avg %]    [cdf %]  [mem KB][times called]
               Conv2D       55   108.692    47.515%    47.515% 32798.539       55
DepthwiseConv2dNative       21    28.022    12.250%    59.765% 17180.992       21
                  Mul      170    18.866     8.247%    68.012%     0.008      170
                  Add      172    18.460     8.070%    76.082%    15.344      172
                 Cast      183    12.116     5.297%    81.378% 24883.217      183
               Gather      546     4.815     2.105%    83.483%     0.000      546
 TensorArrayScatterV3        5     3.971     1.736%    85.219% 25604.012        5
                Relu6       47     3.619     1.582%    86.801%     0.000       47
                Slice       93     2.915     1.274%    88.075%  1380.240       93
              Minimum      451     2.877     1.258%    89.333%     0.000      451
                Const      476     2.802     1.225%    90.558%     0.000      476
              Maximum      360     2.761     1.207%    91.765%     0.000      360
              Reshape      287     2.318     1.013%    92.778%     0.000      287
                Where      180     1.793     0.784%    93.562%     1.440      180
                  Sub      190     1.749     0.765%    94.327%     0.008      190
                Split      180     1.749     0.765%    95.091%     0.000      180
              Greater      183     1.220     0.533%    95.625%   172.533      183
             ConcatV2       99     1.057     0.462%    96.087%   730.868       99
                Shape      112     1.001     0.438%    96.524%     0.952      112
              Squeeze       92     0.978     0.428%    96.952%     0.000       92
       ResizeBilinear        1     0.885     0.387%    97.339%  1080.000        1
         StridedSlice      113     0.878     0.384%    97.722%     0.432      113
  NonMaxSuppressionV2       90     0.820     0.358%    98.081%     0.000       90
            Transpose        3     0.706     0.309%    98.390%    92.016        3
                Enter       26     0.434     0.190%    98.579%     0.000       26
                 Pack       19     0.433     0.189%    98.769%    30.908       19
            ZerosLike       92     0.362     0.158%    98.927%     0.004       92
              BiasAdd       12     0.326     0.143%    99.069%     0.000       12
        NextIteration        8     0.308     0.135%    99.204%     0.000        8
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

I run benchmark_model with MKLDDN_VERBOSE=1 and got this log.
&lt;denchmark-link:https://drive.google.com/file/d/12ClzFKiOryge6So-trXrvEyEk6ycOheY/view?usp=sharing&gt;https://drive.google.com/file/d/12ClzFKiOryge6So-trXrvEyEk6ycOheY/view?usp=sharing&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='YijinLiu' date='2018-05-15T16:25:45Z'>
		
@agramesh1

		</comment>
		<comment id='2' author='YijinLiu' date='2018-05-16T05:12:49Z'>
		&lt;denchmark-link:https://github.com/YijinLiu&gt;@YijinLiu&lt;/denchmark-link&gt;
 thanks for letting us know. We are working on the implementation of mkl optimized depthwise convolution, we won't have it merged before 1.9, expecting to have it implemented in the next release. Implementing the depthwise convolution in MKL will improve the performance of Conv2Dwithbias as it will receive input in the correct format. Can you also make sure you have the right environment variables as outlined in &lt;denchmark-link:https://www.tensorflow.org/performance/performance_guide#optimizing_for_cpu&gt;https://www.tensorflow.org/performance/performance_guide#optimizing_for_cpu&lt;/denchmark-link&gt;
, especially setting KMP_BLOCKTIME=0
		</comment>
		<comment id='3' author='YijinLiu' date='2018-05-16T07:42:17Z'>
		&lt;denchmark-link:https://github.com/agramesh1&gt;@agramesh1&lt;/denchmark-link&gt;
 I disabled threading for tensorflow. Anyway, run the tests with KMP_BLOCKTIME=0, no difference noticed.
		</comment>
		<comment id='4' author='YijinLiu' date='2018-05-18T00:42:25Z'>
		&lt;denchmark-link:https://github.com/agramesh1&gt;@agramesh1&lt;/denchmark-link&gt;
 Thank you for working on the issue! Please post here once you submit the PR.
		</comment>
		<comment id='5' author='YijinLiu' date='2018-05-18T04:20:13Z'>
		&lt;denchmark-link:https://github.com/agramesh1&gt;@agramesh1&lt;/denchmark-link&gt;
 Thanks. Given that it'll take some time, could we do some quick fix at the same time?
For example, in tensorflow/core/graph/mkl_layout_pass.cc, only rewrite to _MklConv2D when it's not depth wise conv? I do see the normal _MklConv2D is about 2 times faster compared to Conv2D
		</comment>
		<comment id='6' author='YijinLiu' date='2018-06-02T07:12:47Z'>
		As this issue has invited community support, please remove the assignee. Otherwise, remove the community support label. Thank you.
		</comment>
		<comment id='7' author='YijinLiu' date='2018-08-04T18:39:34Z'>
		As this issue has invited community support, please remove the assignee. Otherwise, remove the community support label. Thank you.
		</comment>
		<comment id='8' author='YijinLiu' date='2018-09-15T23:16:44Z'>
		&lt;denchmark-link:https://github.com/agramesh1&gt;@agramesh1&lt;/denchmark-link&gt;
 It's been a while. Any updates on this?
		</comment>
		<comment id='9' author='YijinLiu' date='2018-10-08T02:29:29Z'>
		&lt;denchmark-link:https://github.com/YijinLiu&gt;@YijinLiu&lt;/denchmark-link&gt;
 We have two PRs submitted for depthwise conv and relu6  once merged you should see improved performance on SSD mobilenet &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/22785&gt;#22785&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/22244&gt;#22244&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='YijinLiu' date='2018-10-08T17:44:21Z'>
		&lt;denchmark-link:https://github.com/agramesh1&gt;@agramesh1&lt;/denchmark-link&gt;
 Thanks for the update. I'll find some time to try the two PRs. Do you have numbers about how much it'll improve SSD inference compared to not using MKL?
		</comment>
		<comment id='11' author='YijinLiu' date='2018-12-24T02:01:35Z'>
		Hmm, I tested the two PRs. Following is what I got:

w/o MKL
FLOPs/second: 22.29B
w/  MKL 2019.1.144 and mkldnn 0.17.2
FLOPs/second: 23.96B
MKL version is about 7.5% faster.

Summary:

_MklDepthwiseConv2dNative and _MklConv2D are about 30% faster their counter parts.
_MklMul is about 10% faster than Mul
_MklAdd is same as Add
_MklRelu6 is about 20% slower than Relu6
_MklFusedConv2D is about 30% slower than FusedConv2D
_MklConcatV2 is significantly slower than ConvatV2: 1.471s for 98 ops compared to 0.580s for 99 ops.
MKL version introduces a lot of Const ops: 5.580s for 1612 ops compared to 0.551 for 466 ops.

I guess if I disable _MklRelu6, _MklFusedConv2D and _MklConcatV2, I could probably get 10% faster.
However, given the huge binary size increase by using MKL, the gain is not that huge.
NOTE: all data are collected from a i3-8300 CPU.
Details:


w/o MKL
TF_DISABLE_MKL=1 benchmark_model --graph=testdata/ssdlite_mobilenet_v2_coco_2018_05_09_frozen.pb --show_flops --input_layer=image_tensor --input_layer_type=uint8 --input_layer_shape=1,1920,1080,3 --output_layer=num_detections,detection_classes,detection_scores,detection_boxes --num_threads=1
          [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
DepthwiseConv2dNative	       33	    21.591	    23.264%	    23.264%	 20215.104	       33
               Conv2D	       43	    19.017	    20.491%	    43.755%	 32070.080	       43
                  Add	      174	    10.142	    10.928%	    54.683%	    15.344	      174
                 Cast	      183	     7.057	     7.604%	    62.287%	 24883.217	      183
                  Mul	      137	     4.404	     4.745%	    67.033%	    15.344	      137
         _FusedConv2D	       12	     4.082	     4.398%	    71.431%	   728.460	       12
               Gather	      546	     3.797	     4.091%	    75.522%	     0.000	      546
 TensorArrayScatterV3	        5	     2.535	     2.731%	    78.254%	 25604.012	        5
              Minimum	      451	     2.533	     2.729%	    80.983%	     0.000	      451
              Maximum	      360	     2.282	     2.459%	    83.442%	     0.000	      360
                Relu6	       59	     1.909	     2.057%	    85.499%	     0.000	       59
                Where	      180	     1.355	     1.460%	    86.959%	     1.440	      180
              Reshape	      287	     1.296	     1.396%	    88.355%	     0.000	      287
                Slice	       93	     1.203	     1.296%	    89.652%	  1380.240	       93
        TensorArrayV3	       11	     1.062	     1.144%	    90.796%	     2.244	       11
              Greater	      183	     1.045	     1.126%	    91.922%	   172.533	      183
                  Sub	      188	     0.874	     0.942%	    92.864%	     0.000	      188
                Split	      180	     0.817	     0.880%	    93.744%	     0.000	      180
         StridedSlice	      113	     0.780	     0.840%	    94.584%	     0.432	      113
              Sigmoid	        1	     0.608	     0.655%	    95.240%	     0.000	        1
       ResizeBilinear	        1	     0.584	     0.629%	    95.869%	  1080.000	        1
             ConcatV2	       99	     0.580	     0.625%	    96.494%	   730.868	       99
                Const	      466	     0.551	     0.594%	    97.088%	     0.000	      466
              Squeeze	       92	     0.543	     0.585%	    97.673%	     0.000	       92
                Shape	      112	     0.457	     0.492%	    98.165%	     0.952	      112
  NonMaxSuppressionV2	       90	     0.410	     0.442%	    98.607%	     0.000	       90
            ZerosLike	       92	     0.354	     0.381%	    98.988%	     0.000	       92
                Enter	       26	     0.148	     0.159%	    99.148%	     0.000	       26
  TensorArrayGatherV3	        6	     0.126	     0.136%	    99.283%	  1082.416	        6
                 AddN	       10	     0.108	     0.116%	    99.400%	     0.000	       10
                 NoOp	        1	     0.096	     0.103%	    99.503%	     0.000	        1
        NextIteration	        8	     0.069	     0.074%	    99.578%	     0.000	        8
                Merge	       11	     0.064	     0.069%	    99.647%	     0.044	       19
                 Pack	       19	     0.056	     0.060%	    99.707%	    30.908	       19
               Unpack	        5	     0.046	     0.050%	    99.757%	    92.044	        5
               Switch	       18	     0.044	     0.047%	    99.804%	     0.000	       26
                 Less	        2	     0.031	     0.033%	    99.837%	     0.002	        4
             Identity	        9	     0.024	     0.026%	    99.863%	     0.000	        9
            Transpose	        3	     0.020	     0.022%	    99.885%	    92.016	        3
                Range	       12	     0.019	     0.020%	    99.905%	     0.044	       12
   TensorArrayWriteV3	        6	     0.017	     0.018%	    99.924%	     0.000	        6
    TensorArrayReadV3	        5	     0.012	     0.013%	    99.936%	     0.000	        5
    TensorArraySizeV3	        6	     0.009	     0.010%	    99.946%	     0.024	        6
           ExpandDims	        6	     0.009	     0.010%	    99.956%	     0.000	        6
                 Fill	        4	     0.007	     0.008%	    99.963%	     2.404	        4
                Equal	        2	     0.006	     0.006%	    99.970%	     0.002	        2
                  Exp	        2	     0.005	     0.005%	    99.975%	     0.000	        2
                 Exit	        6	     0.005	     0.005%	    99.981%	     0.000	        6
               Assert	        3	     0.005	     0.005%	    99.986%	     0.000	        3
             LoopCond	        2	     0.004	     0.004%	    99.990%	     0.000	        4
              _Retval	        4	     0.003	     0.003%	    99.994%	     0.000	        4
               TopKV2	        1	     0.002	     0.002%	    99.996%	     0.000	        1
                 Size	        1	     0.002	     0.002%	    99.998%	     0.004	        1
                 _Arg	        1	     0.001	     0.001%	    99.999%	     0.000	        1
                 Tile	        1	     0.001	     0.001%	   100.000%	     0.000	        1



Timings (microseconds): count=133 first=92952 curr=94742 min=76244 max=114538 avg=94480.7 std=5532
Memory (bytes): count=133 curr=108200211(all same)
4371 nodes observed
FLOPs estimate: 1.50B
FLOPs/second: 22.29B


w/  MKL 2019.1.144 and mkldnn 0.17.2
benchmark_model --graph=testdata/ssdlite_mobilenet_v2_coco_2018_05_09_frozen.pb --show_flops --input_layer=image_tensor --input_layer_type=uint8 --input_layer_shape=1,1920,1080,3 --output_layer=num_detections,detection_classes,detection_scores,detection_boxes --num_threads=1
             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
           _MklConv2D	       43	    16.673	    15.742%	    15.742%	 43206.305	       43
 _MklDepthwiseConv2dNative     33	    13.953	    13.174%	    28.916%	 20249.951	       33
              _MklAdd	       76	    10.332	     9.755%	    38.671%	 51790.977	       76
                 Cast	      183	     7.005	     6.614%	    45.285%	 24883.217	      183
      _MklFusedConv2D	       12	     6.187	     5.842%	    51.127%	  7045.664	       12
  _MklInputConversion	      109	     6.165	     5.821%	    56.948%	 51903.664	      109
                Const	     1612	     5.580	     5.268%	    62.216%	     0.000	     1612
              _MklMul	       33	     3.938	     3.718%	    65.934%	 19720.896	       33
               Gather	      546	     3.645	     3.442%	    69.376%	     0.000	      546
              Minimum	      451	     2.844	     2.685%	    72.061%	     0.000	      451
 TensorArrayScatterV3	        5	     2.569	     2.426%	    74.487%	 25604.012	        5
            _MklRelu6	       59	     2.402	     2.268%	    76.755%	    31.152	       59
              Maximum	      360	     2.237	     2.112%	    78.867%	     0.000	      360
                Where	      180	     2.196	     2.073%	    80.940%	     1.440	      180
                Split	      180	     1.561	     1.474%	    82.414%	     0.000	      180
              Greater	      183	     1.522	     1.437%	    83.851%	   172.533	      183
                Slice	       93	     1.487	     1.404%	    85.255%	  1380.240	       93
         _MklConcatV2	       98	     1.471	     1.389%	    86.644%	   782.604	       98
             _MklToTf	      199	     1.366	     1.290%	    87.934%	     0.512	      199
                  Sub	      188	     1.355	     1.279%	    89.213%	     0.000	      188
          _MklReshape	      107	     1.150	     1.086%	    90.299%	   784.956	      107
              Reshape	      180	     1.003	     0.947%	    91.246%	     0.000	      180
         StridedSlice	      113	     0.982	     0.927%	    92.173%	     0.432	      113
       ResizeBilinear	        1	     0.948	     0.895%	    93.068%	  1080.000	        1
                 NoOp	        1	     0.907	     0.856%	    93.924%	     0.000	        1
                Shape	      112	     0.816	     0.770%	    94.695%	     0.952	      112
             _MklAddN	       10	     0.685	     0.647%	    95.341%	  1597.440	       10
              Sigmoid	        1	     0.605	     0.571%	    95.913%	     0.000	        1
  NonMaxSuppressionV2	       90	     0.603	     0.569%	    96.482%	     0.000	       90
                  Add	       98	     0.594	     0.561%	    97.043%	    15.344	       98
                  Mul	      104	     0.519	     0.490%	    97.533%	    15.344	      104
        TensorArrayV3	       11	     0.469	     0.443%	    97.976%	     2.244	       11
              Squeeze	       92	     0.410	     0.387%	    98.363%	     0.000	       92
                Enter	       26	     0.403	     0.381%	    98.743%	     0.000	       26
            ZerosLike	       92	     0.385	     0.364%	    99.107%	     0.000	       92
                Merge	       11	     0.162	     0.153%	    99.260%	     0.044	       19
  TensorArrayGatherV3	        6	     0.138	     0.130%	    99.390%	  1082.416	        6
        NextIteration	        8	     0.134	     0.127%	    99.517%	     0.000	        8
                 Pack	       19	     0.101	     0.095%	    99.612%	    30.908	       19
               Unpack	        5	     0.063	     0.059%	    99.671%	    92.044	        5
                 Less	        2	     0.057	     0.054%	    99.725%	     0.002	        4
               Switch	       18	     0.053	     0.050%	    99.775%	     0.000	       26
                Range	       12	     0.051	     0.048%	    99.823%	     0.044	       12
             Identity	        9	     0.029	     0.027%	    99.851%	     0.000	        9
            Transpose	        3	     0.023	     0.022%	    99.873%	    92.016	        3
   TensorArrayWriteV3	        6	     0.021	     0.020%	    99.892%	     0.000	        6
    TensorArrayReadV3	        5	     0.021	     0.020%	    99.912%	     0.000	        5
    TensorArraySizeV3	        6	     0.015	     0.014%	    99.926%	     0.024	        6
           ExpandDims	        6	     0.013	     0.012%	    99.939%	     0.000	        6
                 Fill	        4	     0.011	     0.010%	    99.949%	     2.404	        4
              _Retval	        4	     0.009	     0.008%	    99.958%	     0.000	        4
                Equal	        2	     0.008	     0.008%	    99.965%	     0.002	        2
                 Exit	        6	     0.006	     0.006%	    99.971%	     0.000	        6
                 Size	        1	     0.005	     0.005%	    99.975%	     0.004	        1
             LoopCond	        2	     0.005	     0.005%	    99.980%	     0.000	        4
                  Exp	        2	     0.005	     0.005%	    99.985%	     0.000	        2
               Assert	        3	     0.005	     0.005%	    99.990%	     0.000	        3
             ConcatV2	        1	     0.004	     0.004%	    99.993%	     0.008	        1
               TopKV2	        1	     0.003	     0.003%	    99.996%	     0.000	        1
                 _Arg	        1	     0.002	     0.002%	    99.998%	     0.000	        1
                 Tile	        1	     0.002	     0.002%	   100.000%	     0.000	        1



Timings (microseconds): count=124 first=100951 curr=133829 min=80570 max=149901 avg=108760 std=10763
Memory (bytes): count=124 curr=251569827(all same)
5825 nodes observed
FLOPs estimate: 1.50B
FLOPs/second: 23.96
		</comment>
		<comment id='12' author='YijinLiu' date='2018-12-24T05:18:07Z'>
		&lt;denchmark-link:https://github.com/YijinLiu&gt;@YijinLiu&lt;/denchmark-link&gt;
 Thanks for posting the data of your experiment. Could you please specify which version of TensorFlow you used.
		</comment>
		<comment id='13' author='YijinLiu' date='2018-12-24T09:23:59Z'>
		It's almost from the HEAD, at commit &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/0a708f8da4afc5099cae1493dfb60a5680dadf2f&gt;0a708f8&lt;/denchmark-link&gt;

BTW, I built both tensorflow and mkldnn with -march=native
		</comment>
		<comment id='14' author='YijinLiu' date='2020-06-26T20:44:38Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/19277&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/19277&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>