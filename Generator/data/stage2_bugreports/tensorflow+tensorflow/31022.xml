<bug id='31022' author='hstuk' open_date='2019-07-25T09:35:25Z' closed_time='2019-12-26T19:17:22Z'>
	<summary>Invalid result on some GPUs, probably einsum</summary>
	<description>
System information

I have written custom code.
Linux Ubuntu 19.04, Debian 9.9
TensorFlow installed from binary
TensorFlow version 2.0.0-b1, 1.14, 1.15.0
Python version: 3.7.1, 3.7.3
CUDA/cuDNN version: 10.0, 10.1
GPU model and memory: Tesla T4, Tesla P4, P100, does not exist on k80

Describe the current behavior
The result on the GPU is significantly different from CPU. It is correct on CPU, but wrong on GPU. The difference on GPU is much bigger than zero.
Describe the expected behavior
The print out of the difference should be zero.
Code to reproduce the issue
tf 2.0.0-b1
&lt;denchmark-code&gt;
import tensorflow as tf
import numpy as np



@tf.function
def sample_y_nn_tf(w, X_sample, config):

        layers = config['layers']
        current_w_index = 0
        h = X_sample
        w_shape = tf.shape(input=w)

        l = layers[0]
        w_current = tf.slice(w, begin=[0,0,0,0],size=[w_shape[0], w_shape[1], w_shape[2], l[0] * l[1]])
        w_current = tf.reshape(w_current, [config['batch_size'], w_shape[1], w_shape[2]] + list(l))
        b_current = tf.slice(w, begin=[0,0,0,l[0] * l[1]-1],size=[w_shape[0], w_shape[1], w_shape[2], l[1]])
        h = tf.einsum('lm,ikjmn-&gt;likjn', h, w_current) + b_current
        current_w_index = current_w_index + (l[0] + 1) * l[1]

        for l in layers[1:]:
            h = tf.nn.elu(h)
            w_current = tf.slice(w, begin=[0,0,0,current_w_index],size=[w_shape[0], w_shape[1], w_shape[2], l[0] * l[1]])
            w_current = tf.reshape(w_current, [config['batch_size'],w_shape[1], w_shape[2]]+list(l))
            b_current = tf.slice(w, begin=[0, 0, 0,current_w_index + l[0] * l[1] - 1], size=[w_shape[0], w_shape[1], w_shape[2], l[1]])
            h = tf.einsum('likjm,ikjmn-&gt;likjn',h,w_current)+b_current
            current_w_index = current_w_index+(l[0]+1)*l[1]

        h = tf.squeeze(h, axis=4)

        return h

config = {'batch_size': 8, 'layers': [[2,2],[2,1]]}

np.random.seed(111)

w = np.random.normal(size=(config['batch_size'],64,512,13)).astype(dtype=np.float32)
x_sample = np.random.normal(size=(100,2)).astype(dtype=np.float32)

permutation = tf.random.shuffle(tf.range(config['batch_size'], dtype=tf.int32), seed=111)

print('permutation', permutation)


with tf.device("/cpu:0"):
    means = sample_y_nn_tf(w=w, X_sample=x_sample, config=config)
    means_p = sample_y_nn_tf( w = tf.gather(w, permutation, axis=0), X_sample=x_sample, config=config)
    print('difference on cpu', np.sum(np.abs(tf.gather(means,permutation, axis=1)-means_p)))

with tf.device("/gpu:0"):
    means = sample_y_nn_tf(w=w, X_sample=x_sample, config=config)
    means_p = sample_y_nn_tf( w = tf.gather(w, permutation, axis=0), X_sample=x_sample, config=config)
    print('difference on gpu', np.sum(np.abs(tf.gather(means,permutation, axis=1)-means_p)))


&lt;/denchmark-code&gt;

Output:

difference on cpu 0.0
difference on gpu 37953784.0

tf.1.14
&lt;denchmark-code&gt;from tensorflow.python.framework.versions import VERSION
import tensorflow as tf
import numpy as np
import os

print(tf.__version__)

@tf.function
def sample_y_nn_tf(w, X_sample, config):
    
        layers = config['layers']
        current_w_index = 0
        h = X_sample
        w_shape = tf.shape(input=w)
        
        l = layers[0]
        w_current = tf.slice(w, begin=[0,0,0,0],size=[w_shape[0], w_shape[1], w_shape[2], l[0] * l[1]])
        w_current = tf.reshape(w_current, [config['batch_size'], w_shape[1], w_shape[2]] + list(l))
        b_current = tf.slice(w, begin=[0,0,0,l[0] * l[1]-1],size=[w_shape[0], w_shape[1], w_shape[2], l[1]])
        h = tf.einsum('lm,ikjmn-&gt;likjn', h, w_current) + b_current
        current_w_index = current_w_index + (l[0] + 1) * l[1]
        
        for l in layers[1:]:
            h = tf.nn.elu(h)
            w_current = tf.slice(w, begin=[0,0,0,current_w_index],size=[w_shape[0], w_shape[1], w_shape[2], l[0] * l[1]])
            w_current = tf.reshape(w_current, [config['batch_size'],w_shape[1], w_shape[2]]+list(l))
            b_current = tf.slice(w, begin=[0, 0, 0,current_w_index + l[0] * l[1] - 1], size=[w_shape[0], w_shape[1], w_shape[2], l[1]])
            h = tf.einsum('likjm,ikjmn-&gt;likjn',h,w_current)+b_current
            current_w_index = current_w_index+(l[0]+1)*l[1]
    
        h = tf.squeeze(h, axis=4)

        return h

config = {'batch_size': 8, 'layers': [[2,2],[2,1]]}

np.random.seed(111)

w = np.random.normal(size=(config['batch_size'],64,512,13)).astype(dtype=np.float32)
precision = np.exp(np.random.normal(size=(config['batch_size'],64,512))).astype(dtype=np.float32)
x_sample = np.random.normal(size=(100,2)).astype(dtype=np.float32)

permutation = tf.random.shuffle(tf.range(config['batch_size'], dtype=tf.int32), seed=111)

conf = tf.ConfigProto()
conf.gpu_options.allow_growth = True
sess = tf.Session(config=conf)


print('permutation', sess.run(permutation))


with tf.device("/cpu:0"):
    means = sample_y_nn_tf(w=w, X_sample=x_sample, config=config)
    means_p = sample_y_nn_tf( w = tf.gather(w, permutation, axis=0), X_sample=x_sample, config=config)
    net_cpu = tf.gather(means,permutation, axis=1)-means_p

with tf.device("/device:GPU:0"):
    means = sample_y_nn_tf(w=w, X_sample=x_sample, config=config)
    means_p = sample_y_nn_tf( w = tf.gather(w, permutation, axis=0), X_sample=x_sample, config=config)
    net_gpu = tf.gather(means,permutation, axis=1)-means_p

net_cpu_result = sess.run(net_cpu)
net_gpu_result = sess.run(net_gpu)
print("tensorflow version: {}".format(VERSION))
print('difference on cpu', np.sum(np.abs(net_cpu_result)))
print('difference on gpu', np.sum(np.abs(net_gpu_result)))

&lt;/denchmark-code&gt;

Output:

difference on cpu 0.0
difference on gpu 39042744.0

Other info / logs
I did not check the tf2 version on K80, P100.
The bug probably persists when using Pythonic slicing operations.
	</description>
	<comments>
		<comment id='1' author='hstuk' date='2019-07-26T09:20:10Z'>
		I have tried on colab with TF version 2.0 beta1,1.14 and was able to reproduce the issue.Please, find the &lt;denchmark-link:https://colab.research.google.com/drive/117O0l7XLQeFAIf1cr4PbNOzSsjUtkKHY&gt;gist &lt;/denchmark-link&gt;
here.Thanks!
		</comment>
		<comment id='2' author='hstuk' date='2019-07-28T10:56:39Z'>
		Thanks. I investigated a bit more. The bug does not occur when using only one layer 'layers': [[2,1]]. I think the minimal configuration is config = {'batch_size': 2, 'layers': [[1,1],[1,1]]}.
		</comment>
		<comment id='3' author='hstuk' date='2019-07-28T10:59:21Z'>
		i also reproduced the bug on geforce gtx 1050 ti
		</comment>
		<comment id='4' author='hstuk' date='2019-12-18T17:39:11Z'>
		I can reproduce the problem with TF 2.0.
&lt;denchmark-link:https://github.com/sanjoy&gt;@sanjoy&lt;/denchmark-link&gt;
, could you help to take a look?
		</comment>
		<comment id='5' author='hstuk' date='2019-12-26T19:17:22Z'>
		Duplicate of &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/31166&gt;#31166&lt;/denchmark-link&gt;

Does not reproduce with tf-nightly that uses CUDA 10.1
		</comment>
		<comment id='6' author='hstuk' date='2019-12-26T19:17:24Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31022&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31022&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>