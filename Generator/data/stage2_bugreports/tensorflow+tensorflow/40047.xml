<bug id='40047' author='anktplwl91' open_date='2020-06-01T13:21:42Z' closed_time='2020-06-11T17:44:23Z'>
	<summary>InvalidArgumentError:  Incompatible shapes: [200,10] vs. [200,5] [[node LogicalAnd_3 (defined at &amp;lt;ipython-input-11-ea6e7bc7fc28&amp;gt;:1) ]]</summary>
	<description>
Please go to Stack Overflow for help and support:
&lt;denchmark-link:https://stackoverflow.com/questions/tagged/tensorflow&gt;https://stackoverflow.com/questions/tagged/tensorflow&lt;/denchmark-link&gt;

If you open a GitHub issue, here is our policy:

It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
The form below must be filled out.
It shouldn't be a TensorBoard issue. Those go here.

Here's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;



Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes, I have written custom code.


OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
I am running into this problem on Google Colab, which has Tensorflow 2.2.0


Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
NA


TensorFlow installed from (source or binary):
Google Colab setup, Tensorflow version 2.2.0


TensorFlow version (use command below):
TensorFlow 2.2.0


Python version: Python 3.6.9


Bazel version (if compiling from source):


GCC/Compiler version (if compiling from source):


CUDA/cuDNN version:


GPU model and memory:


Exact command to reproduce:


You can collect some of this information using our environment capture script:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&lt;/denchmark-link&gt;

You can obtain the TensorFlow version with:
python -c "import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"
&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
While the model is getting trained, it randomly fails in any of the iteration of epoch with an Incompatible shape Error for LogicalAnd node, with the shapes differing as [200, 10] and [200, 5].
The difference in axis 1 here is actually : 2*Batch_size and Batch_size. So, if I change Batch_Size, the shapes change accordingly. Also, I'm not seeing this error on my local machine if I'm running this same code, Tensorflow version 2.2.0_rc1on a CPU. But, on Google Colab, I tried it both on CPU and GPU, and I face this error.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
Deep Network containing Pretrained Bert and  ResNet50V2 networks. The final layer outputs of both networks are concatenated and connected to a final Softmax layer of 2 neurons. There are 4 Input layers for complete network : 3 for BERT and 1 for ResNet50V2.
Here's the architecture code for network :
`
def complete_model():
input_word_ids = tf.keras.layers.Input(
(MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_word_ids')
input_masks = tf.keras.layers.Input(
(MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_masks')
input_segments = tf.keras.layers.Input(
(MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_segments')
&lt;denchmark-code&gt;img_input = tf.keras.layers.Input((256, 256, 3))

bert_layer = hub.KerasLayer(BERT_PATH, trainable=True)

_, sequence_output = bert_layer([input_word_ids, input_masks, input_segments])
bert_out = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)

resnet_model = tf.keras.applications.ResNet50V2(include_top=False, weights='imagenet', input_tensor=img_input, pooling='avg')
resnet_out = resnet_model.output

x = tf.keras.layers.Concatenate()([bert_out, resnet_out])
x = tf.keras.layers.Dropout(0.4)(x)
out = tf.keras.layers.Dense(2, activation='softmax')(x)

model = tf.keras.models.Model(inputs=[img_input, input_word_ids, input_masks, input_segments], outputs=out)
return model
&lt;/denchmark-code&gt;

`
The model is created and then compiled and trained with the tf.keras.fit API. I created a tf.keras.Sequence object for generating inputs in batches in format :
&lt;batch of images for ResNet50V2 of shape (256, 256, 3)&gt;, &lt;batch for Input IDs for BERT of shape (512)&gt;, &lt;batch for Attention Masks for BERT of shape (512)&gt;, &lt;batch for Segment IDs for BERT of shape (512)&gt; and output &lt;batch of class labels (batch_size, 2)&gt;
Below is the Error Trace which I get :
&lt;denchmark-h:h2&gt;Epoch 1/5
18/1700 [..............................] - ETA: 1:31:14 - loss: 1.6233 - accuracy: 0.5889 - auc: 0.5584&lt;/denchmark-h&gt;

InvalidArgumentError                      Traceback (most recent call last)
 in ()
----&gt; 1 hist = model.fit(train_dataseq, epochs=5, verbose=1, steps_per_epoch=8500//5, validation_data=valid_dataseq, validation_steps=500//5)
8 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
64   def _method_wrapper(self, *args, **kwargs):
65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
---&gt; 66       return method(self, *args, **kwargs)
67
68     # Running inside run_distribute_coordinator already.
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
846                 batch_size=batch_size):
847               callbacks.on_train_batch_begin(step)
--&gt; 848               tmp_logs = train_function(iterator)
849               # Catch OutOfRangeError for Datasets of unknown size.
850               # This blocks until the batch has finished executing.
/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in call(self, *args, **kwds)
578         xla_context.Exit()
579     else:
--&gt; 580       result = self._call(*args, **kwds)
581
582     if tracing_count == self._get_tracing_count():
/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
609       # In this case we have created variables on the first call, so we run the
610       # defunned version which is guaranteed to never create variables.
--&gt; 611       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
612     elif self._stateful_fn is not None:
613       # Release the lock early so that multiple threads can perform the call
/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in (self, *args, **kwargs)
2418     with self._lock:
2419       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
-&gt; 2420     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
2421
2422   &lt;denchmark-link:https://github.com/Property&gt;@Property&lt;/denchmark-link&gt;

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs)
1663          if isinstance(t, (ops.Tensor,
1664                            resource_variable_ops.BaseResourceVariable))),
-&gt; 1665         self.captured_inputs)
1666
1667   def _call_flat(self, args, captured_inputs, cancellation_manager=None):
/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
1744       # No tape is watching; skip to running the function.
1745       return self._build_call_outputs(self._inference_function.call(
-&gt; 1746           ctx, args, cancellation_manager=cancellation_manager))
1747     forward_backward = self._select_forward_and_backward_functions(
1748         args,
/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)
596               inputs=args,
597               attrs=attrs,
--&gt; 598               ctx=ctx)
599         else:
600           outputs = execute.execute_with_cancellation(
/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
58     ctx.ensure_initialized()
59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
---&gt; 60                                         inputs, attrs, num_outputs)
61   except core._NotOkStatusException as e:
62     if name is not None:
InvalidArgumentError: 2 root error(s) found.
(0) Invalid argument:  Incompatible shapes: [200,10] vs. [200,5]
[[node LogicalAnd (defined at :1) ]]
[[assert_less_equal/Assert/AssertGuard/pivot_f/_2738/_167]]
(1) Invalid argument:  Incompatible shapes: [200,10] vs. [200,5]
[[node LogicalAnd (defined at :1) ]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_64050]
Function call stack:
train_function -&gt; train_function
	</description>
	<comments>
		<comment id='1' author='anktplwl91' date='2020-06-01T14:06:43Z'>
		&lt;denchmark-link:https://github.com/anktplwl91&gt;@anktplwl91&lt;/denchmark-link&gt;

Can you please share simple stand alone code to replicate the issue faced, or share a colab gist for us to analyse the error. Also include your TensorFlow version.
		</comment>
		<comment id='2' author='anktplwl91' date='2020-06-01T14:17:43Z'>
		Hi &lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;

Here's my Github &lt;denchmark-link:https://gist.github.com/anktplwl91/a0b6612cb3ed649d4e9036933c228134&gt;Gist&lt;/denchmark-link&gt;

Tensorflow version used in Colab is : 2.2.0 GPU and CPU both (facing issue on both)
		</comment>
		<comment id='3' author='anktplwl91' date='2020-06-01T15:03:16Z'>
		&lt;denchmark-link:https://github.com/anktplwl91&gt;@anktplwl91&lt;/denchmark-link&gt;

I ran the code shared and face &lt;denchmark-link:https://colab.sandbox.google.com/gist/Saduf2019/29d965d7786ca7d8813bb3e896173c76/untitled208.ipynb&gt;this issue&lt;/denchmark-link&gt;
, please share all dependencies
		</comment>
		<comment id='4' author='anktplwl91' date='2020-06-01T15:32:54Z'>
		Hi &lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;

Here's the link to my &lt;denchmark-link:https://colab.research.google.com/drive/1J5oXtKy9I1ynpx1lbM4X9jiVufyRM3bi?usp=sharing&gt;colab&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='anktplwl91' date='2020-06-02T07:06:34Z'>
		An update, I ran into this same issue on my local PC. But for some reason, the issue is seen in 5th epoch. I'm using same code there also, but using smaller version of BERT from Transformers repo
TensorFlow Version : 2.2.0_rc1 CPU
Python : 3.6.5
Error Logtrace :
Epoch 1/5
W0601 21:38:38.055769 17108 optimizer_v2.py:1180] Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.
W0601 21:38:43.361550 17108 optimizer_v2.py:1180] Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.
531/531 [==============================] - ETA: 0s - loss: 0.7343 - auc: 0.6148 - accuracy: 0.5972
Epoch 00001: saving model to model_checkpoint
531/531 [==============================] - 10835s 20s/step - loss: 0.7343 - auc: 0.6148 - accuracy: 0.5972 - val_loss: 148.2352 - val_auc: 0.5020 - val_accuracy: 0.5020
Epoch 2/5
531/531 [==============================] - ETA: 0s - loss: 0.6840 - auc: 0.6270 - accuracy: 0.6176
Epoch 00002: saving model to model_checkpoint
531/531 [==============================] - 9856s 19s/step - loss: 0.6840 - auc: 0.6270 - accuracy: 0.6176 - val_loss: 0.8900 - val_auc: 0.4815 - val_accuracy: 0.4960
Epoch 3/5
531/531 [==============================] - ETA: 0s - loss: 0.6715 - auc: 0.6392 - accuracy: 0.6228
Epoch 00003: saving model to model_checkpoint
531/531 [==============================] - 9444s 18s/step - loss: 0.6715 - auc: 0.6392 - accuracy: 0.6228 - val_loss: 0.7030 - val_auc: 0.4856 - val_accuracy: 0.4859
Epoch 4/5
531/531 [==============================] - ETA: 0s - loss: 0.6715 - auc: 0.6364 - accuracy: 0.6289
Epoch 00004: saving model to model_checkpoint
531/531 [==============================] - 13278s 25s/step - loss: 0.6715 - auc: 0.6364 - accuracy: 0.6289 - val_loss: 45.8197 - val_auc: 0.4939 - val_accuracy: 0.4980
Epoch 5/5
376/531 [====================&gt;.........] - ETA: 48:52 - loss: 0.6696 - auc: 0.6402 - accuracy: 0.6330
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

InvalidArgumentError                      Traceback (most recent call last)
 in ()
----&gt; 1 hist = transformer_model.fit(train_dataseq, epochs=5, verbose=1, steps_per_epoch=8500//16, validation_data=valid_dataseq, validation_steps=500//16, callbacks=[tf.keras.callbacks.ModelCheckpoint(filepath='model_checkpoint', save_weights_only=True, verbose=1, save_freq='epoch')])
C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py in _method_wrapper(self, *args, **kwargs)
63   def _method_wrapper(self, *args, **kwargs):
64     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
---&gt; 65       return method(self, *args, **kwargs)
66
67     # Running inside run_distribute_coordinator already.
C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
781                 batch_size=batch_size):
782               callbacks.on_train_batch_begin(step)
--&gt; 783               tmp_logs = train_function(iterator)
784               # Catch OutOfRangeError for Datasets of unknown size.
785               # This blocks until the batch has finished executing.
C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\eager\def_function.py in call(self, *args, **kwds)
578         xla_context.Exit()
579     else:
--&gt; 580       result = self._call(*args, **kwds)
581
582     if tracing_count == self._get_tracing_count():
C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\eager\def_function.py in _call(self, *args, **kwds)
609       # In this case we have created variables on the first call, so we run the
610       # defunned version which is guaranteed to never create variables.
--&gt; 611       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
612     elif self._stateful_fn is not None:
613       # Release the lock early so that multiple threads can perform the call
C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\eager\function.py in (self, *args, **kwargs)
2418     with self._lock:
2419       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
-&gt; 2420     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
2421
2422   &lt;denchmark-link:https://github.com/Property&gt;@Property&lt;/denchmark-link&gt;

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\eager\function.py in _filtered_call(self, args, kwargs)
1663          if isinstance(t, (ops.Tensor,
1664                            resource_variable_ops.BaseResourceVariable))),
-&gt; 1665         self.captured_inputs)
1666
1667   def _call_flat(self, args, captured_inputs, cancellation_manager=None):
C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\eager\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
1744       # No tape is watching; skip to running the function.
1745       return self._build_call_outputs(self._inference_function.call(
-&gt; 1746           ctx, args, cancellation_manager=cancellation_manager))
1747     forward_backward = self._select_forward_and_backward_functions(
1748         args,
C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\eager\function.py in call(self, ctx, args, cancellation_manager)
596               inputs=args,
597               attrs=attrs,
--&gt; 598               ctx=ctx)
599         else:
600           outputs = execute.execute_with_cancellation(
C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\eager\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
58     ctx.ensure_initialized()
59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
---&gt; 60                                         inputs, attrs, num_outputs)
61   except core._NotOkStatusException as e:
62     if name is not None:
InvalidArgumentError:  Incompatible shapes: [200,32] vs. [200,16]
[[node LogicalAnd (defined at :1) ]] [Op:__inference_train_function_24857]
Function call stack:
train_function
		</comment>
		<comment id='6' author='anktplwl91' date='2020-06-02T15:58:19Z'>
		&lt;denchmark-link:https://github.com/anktplwl91&gt;@anktplwl91&lt;/denchmark-link&gt;

I ran the code shared by you, every cell has error in it, can you please share code that is complete with all dependencies so we could replicate it.
		</comment>
		<comment id='7' author='anktplwl91' date='2020-06-02T18:15:47Z'>
		Hi &lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;

Can you please download following files from &lt;denchmark-link:https://drive.google.com/drive/folders/1igDo4noUI-d1SQFUwOfa7JMtQS6vd1tR?usp=sharing&gt;here&lt;/denchmark-link&gt;



img, bert-base-from-tfhub directories


train.jsonl, dev.jsonl, tfv2_tokenization.py and fb_meme_compeition.py files


Also, make sure you have tqdm and tensorflow_hub installed


This should run the code without any errors. Please let me know if you face anymore issues.
Thanks
		</comment>
		<comment id='8' author='anktplwl91' date='2020-06-03T10:37:34Z'>
		Hi &lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;

I tried to run the model by removing metric tf.keras.metrics.AUC from model.compile, and have run the model for quite sometime now, around 10 epochs. I haven't seen this bug pop until now. Maybe the issue is coming from somehwere in AUC metrics' code
		</comment>
		<comment id='9' author='anktplwl91' date='2020-06-11T17:44:23Z'>
		The code sample is fairly complex, and it would need to be simplified before we could reliably debug. Since it looks like you resolved the issue in any case, I will close this. If you find you can reproduce the problem with a straightforward snippet, please file a new issue or reopen.
		</comment>
		<comment id='10' author='anktplwl91' date='2020-06-11T17:44:24Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40047&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40047&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='anktplwl91' date='2020-11-24T18:52:09Z'>
		I also faced the same issue and found that there was issue with my pooling layer. It was giving out incorrect output. Changing maxpool2d to globalaevaragepool solved the issue. If you want to stick to maxpool, then change it's output shape.
		</comment>
	</comments>
</bug>