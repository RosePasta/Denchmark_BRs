<bug id='13537' author='Mazecreator' open_date='2017-10-06T21:13:57Z' closed_time='2018-03-07T16:33:55Z'>
	<summary>Feature Request: tf.assign() support tuples</summary>
	<description>
I have recently updated to V1.3 of Tensorflow.  I have some code that I use for dynamic_rnn which copies the STATE of the cell so it persists to the next .run(), I can also INIT that value as well.  Since the update, I am getting a "WARNING:tensorflow:&lt;tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f278c196940&gt;: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.".  I have tried to enable state_is_tuple but then the assign() commands fail as they don't support the tuple structures.
I have an open StackOverflow question with the details:
&lt;denchmark-link:https://stackoverflow.com/questions/46576194/how-do-i-assign-a-lstmstatetuple-using-tf-assign&gt;https://stackoverflow.com/questions/46576194/how-do-i-assign-a-lstmstatetuple-using-tf-assign&lt;/denchmark-link&gt;

Since it seems like the RNN core is moving in the direction of the tuple for the state, it would be nice if the .assign() can handle this transparently.
	</description>
	<comments>
		<comment id='1' author='Mazecreator' date='2017-10-09T00:30:16Z'>
		&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 do we expect to add tuple support soon?
		</comment>
		<comment id='2' author='Mazecreator' date='2017-10-09T05:12:31Z'>
		you can perform an assign on a tuple by using
variables = (var1, var2)  # or whatever; you can use map_structure with tf.get_variable here as well
updates = tf.contrib.framework.nest.map_structure(
  lambda state, var: tf.assign(var, state),
  final_lstm_state,
  variable_tuple,
  check_types=False)
		</comment>
		<comment id='3' author='Mazecreator' date='2017-10-10T14:58:16Z'>
		&lt;denchmark-link:https://github.com/Mazecreator&gt;@Mazecreator&lt;/denchmark-link&gt;
 does &lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 's suggestion get you back in business?
		</comment>
		<comment id='4' author='Mazecreator' date='2017-10-10T15:47:54Z'>
		&lt;denchmark-link:https://github.com/cy89&gt;@cy89&lt;/denchmark-link&gt;
 this might do it but it doesn't seem like "tf.contrib.framework.nest.map_structure()" exists.  I am using TF1.3 but checked the HEAD and didn't see that "nest" folder.  I tried to find if it moved to another location but couldn't find it.
I am sure this is a good short term solution within the active LSTM graph but will greatly complicate copying variables from one Net to another as the transparency of the .assign() would be appreciated.
Once we have a short-term solution I will post it on Stack Overflow in response to my open issue.
Here was my interpretation of the implementation for the LSTM State:
&lt;denchmark-code&gt;        output, self.new_state = tf.nn.dynamic_rnn(self.cell_L1,hidden_input,time_major=True,\
                initial_state=self.state,  dtype=tf.float32, swap_memory=True)

        self.state = tf.contrib.framework.nest.map_structure(
                lambda state, var: tf.assign(var, state),
                self.new_state,
                #variable_tuple,
                check_types=False)

        with tf.control_dependencies([self.state]):
            outputs = tf.identity(output)       
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='Mazecreator' date='2017-10-10T17:47:37Z'>
		It might not be the version expected but I found a "map_structure" here:
from tensorflow.python.util.nest import *
Running the above code failed with this error so I must have missed something:
&lt;denchmark-code&gt;File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/nest.py", line 325, in map_structure
  structure[0], [func(*x) for x in entries])
File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/nest.py", line 325, in &lt;listcomp&gt;
  structure[0], [func(*x) for x in entries])
TypeError: &lt;lambda&gt;() missing 1 required positional argument: 'var'
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='Mazecreator' date='2017-10-10T18:50:46Z'>
		I have been working on this for a while now.  I was not certain what needs to replace "#variable_tuple," in the map_structure but have a better guess:
&lt;denchmark-code&gt;    output, self.new_state = tf.nn.dynamic_rnn(self.cell_L1,hidden_input,time_major=True,\
            initial_state=self.state,  dtype=tf.float32, swap_memory=True)

    update = tf.contrib.framework.nest.map_structure(
            lambda state, var: tf.assign(var, state),
            self.new_state,
            self.state,
            check_types=False)

    with tf.control_dependencies([update]):
        outputs = tf.identity(output) 
&lt;/denchmark-code&gt;

The problem now is I get a different error which doesn't really make sense to me as Tensors should allow an assign() should it?
&lt;denchmark-code&gt;File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/nest.py", line 325, in map_structure
structure[0], [func(*x) for x in entries])
File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/nest.py", line 325, in &lt;listcomp&gt;
structure[0], [func(*x) for x in entries])
File "/home/greg/DFP_trading_sequence/test6/Network_sequence_new1.py", line 143, in &lt;lambda&gt;
lambda state, var: tf.assign(var,state) ,
File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/state_ops.py", line 272, in assign
return ref.assign(value)
AttributeError: 'Tensor' object has no attribute 'assign'
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='Mazecreator' date='2017-10-10T20:56:10Z'>
		You can only assign something to a Variable object.  So if you want an
initial state it has to be a tuple of Variables (and cell.zero_state()
doesn't return Variables - only Tensors).  You can then assign to those in
your update.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Tue, Oct 10, 2017 at 11:54 AM, Greg Peatfield ***@***.***&gt; wrote:
 I have been working on this for a while now. I was not certain what needs
 to replace "#variable_tuple," in the map_structure but have a better guess:

     output, self.new_state = tf.nn.dynamic_rnn(self.cell_L1,hidden_input,time_major=True,\
             initial_state=self.state,  dtype=tf.float32, swap_memory=True)

     update = tf.contrib.framework.nest.map_structure(
             lambda state, var: tf.assign(var, state),
             self.new_state,
             self.state,
             check_types=False)

     with tf.control_dependencies([update]):
         outputs = tf.identity(output)

 The problem now is I get a different error which doesn't really make sense
 to me as Tensors should allow an assign() should it?

 File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/nest.py", line 325, in map_structure
 structure[0], [func(*x) for x in entries])
 File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/nest.py", line 325, in &lt;listcomp&gt;
 structure[0], [func(*x) for x in entries])
 File "/home/greg/DFP_trading_sequence/test6/Network_sequence_new1.py", line 143, in &lt;lambda&gt;
 lambda state, var: tf.assign(var,state) ,
 File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/state_ops.py", line 272, in assign
 return ref.assign(value)
 AttributeError: 'Tensor' object has no attribute 'assign'

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#13537 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABtimzk-JUX2tqhVVWOTG-cVWMkblG8Dks5sq71bgaJpZM4PxCmq&gt;
 .



		</comment>
		<comment id='8' author='Mazecreator' date='2017-10-13T00:14:13Z'>
		&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/Mazecreator&gt;@Mazecreator&lt;/denchmark-link&gt;
 is this issue resolved? (I don't know anything about the RNN API).
		</comment>
		<comment id='9' author='Mazecreator' date='2017-10-13T00:31:13Z'>
		I think I understand the approach although I have yet to code the solution.  It seems like the more an more I dig into this approach the more complexity and issue the rise.  I may need to wait for the actual support for assign() with Tuples.  I ran into another issue as I was trying to prototype this and it was initializing a variable with a Tuple only returns a single combined tensor.  Hopefully if Assign() supports tuples the "variable()" will support Tuples as well to initialize.  I find the biggest problem is the complexity of other libraries by manually copying like this.  Also, I need to figure out how to screen for a tuple so when I "assign()" the variables I don't collapse the tuple into a single tensor variable as this was tough to find even in a short segment of code.
So I think I might be able to do a short-term solution if needed based upon the comments from &lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 , but I really think if the RNN training is going to force the state to be a tuple then the rest of TensorFlow should help support this new requirement.  Another option is to remove the warning statement and just document that Tuples are handled faster and support both formats moving forward.  I am sure this is an easier solution for now, not sure about the long term support issues as I am sure the team will need to think through.
Here is what I am talking about when the RNN cells are "state_is_tuple=True"
&lt;denchmark-code&gt;        self.init_state = self.cell_L1.zero_state(batch_size, tf.float32)
        self.state = tf.Variable(self.init_state, trainable=False)
&lt;/denchmark-code&gt;

In the code above, this silently functions as you would expect.  The self.state is initialized with the Tuple value from self.init_state, but what you get for self.state is actually a single Tensor variable with the Tuple collapsed.  This causes the RNN functions to fail later as they are expecting a Tuple for the state.
		</comment>
		<comment id='10' author='Mazecreator' date='2017-10-13T02:56:36Z'>
		I'm rather surprised that `tf.Variable(self.init_state)` even works!  Sounds
like a bug.  That should have failed in python-land right away! &lt;denchmark-link:https://github.com/lukaszkaiser&gt;@lukaszkaiser&lt;/denchmark-link&gt;


You need to do something like:

```
counter = [0]
def create_variable(init):
  r = tf.get_variable(initializer=init, name="rnn_state_%d" % counter[0])
  counter[0] += 1
  return r
self.state = tf.contrib.framework.nest.map_structure(create_variable,
self.init_state)
```
		</comment>
		<comment id='11' author='Mazecreator' date='2017-10-13T03:09:03Z'>
		What you saw looks like a bug in tf.convert_to_tensor.  It treats complex
data structures, like LSTMStateTuple and nested LSTMStateTuples, as nested
tuples and converts them to initializers.  We'll look at fixing that bug
separately.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Thu, Oct 12, 2017 at 7:57 PM, ebrevdo ***@***.***&gt; wrote:
 I'm rather surprised that tf.Variable(self.init_state) even works! Sounds
 like a bug. That should have failed in python-land right away! +lukasz

 You need to do something like:


 counter = [0]
 def create_variable(init):
 r = tf.get_variable(initializer=init, name="rnn_state_%d" % counter[0])
 counter[0] += 1
 return r
 self.state = tf.contrib.framework.nest.map_structure(create_variable,
 self.init_state)


 On Thu, Oct 12, 2017 at 5:32 PM, Greg Peatfield ***@***.***&gt;
 wrote:

 &gt; I think I understand the approach although I have yet to code the
 &gt; solution. It seems like the more an more I dig into this approach the
 more
 &gt; complexity and issue the rise. I may need to wait for the actual support
 &gt; for assign() with Tuples. I ran into another issue as I was trying to
 &gt; prototype this and it was initializing a variable with a Tuple only
 returns
 &gt; a single combined tensor. Hopefully if Assign() supports tuples the
 &gt; "variable()" will support Tuples as well to initialize. I find the
 biggest
 &gt; problem is the complexity of other libraries by manually copying like
 this.
 &gt; Also, I need to figure out how to screen for a tuple so when I "assign()"
 &gt; the variables I don't collapse the tuple into a single tensor variable as
 &gt; this was tough to find even in a short segment of code.
 &gt;
 &gt; So I think I might be able to do a short-term solution if needed based
 &gt; upon the comments from @ebrevdo &lt;https://github.com/ebrevdo&gt; , but I
 &gt; really think if the RNN training is going to force the state to be a
 tuple
 &gt; then the rest of TensorFlow should help support this new requirement.
 &gt; Another option is to remove the warning statement and just document that
 &gt; Tuples are handled faster and support both formats moving forward. I am
 &gt; sure this is an easier solution for now, not sure about the long term
 &gt; support issues as I am sure the team will need to think through.
 &gt;
 &gt; Here is what I am talking about when the RNN cells are
 &gt; "state_is_tuple=True"
 &gt;
 &gt; self.init_state = self.cell_L1.zero_state(batch_size, tf.float32)
 &gt; self.state = tf.Variable(self.init_state, trainable=False)
 &gt;
 &gt; In the code above, this silently functions as you would expect. The
 &gt; self.state is initialized with the Tuple value from self.init_state, but
 &gt; what you get for self.state is actually a single Tensor variable with the
 &gt; Tuple collapsed. This causes the RNN functions to fail later as they are
 &gt; expecting a Tuple for the state.
 &gt;
 &gt; —
 &gt; You are receiving this because you were mentioned.
 &gt; Reply to this email directly, view it on GitHub
 &gt; &lt;https://github.com/tensorflow/tensorflow/issues/
 13537#issuecomment-336317687&gt;,
 &gt; or mute the thread
 &gt; &lt;https://github.com/notifications/unsubscribe-auth/
 ABtim8Y8cn7LRauhYWTefrUrFpZTFfDUks5srq-IgaJpZM4PxCmq&gt;
 &gt; .
 &gt;

 —
 You are receiving this because you are subscribed to this thread.
 Reply to this email directly, view it on GitHub
 &lt;#13537 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABtim39WZJ3MfxbtGMMDHyHUTf-ZkBSyks5srtGUgaJpZM4PxCmq&gt;
 .



		</comment>
		<comment id='12' author='Mazecreator' date='2017-10-18T17:05:39Z'>
		I would like to still request either tf.assign() be updated to support Tuples to drastically reduce the complexity of updating the LSTM Tuple (as well as any future changes) or that the current merged tensor remain as supported.
For cases where the network is copied for parallel Reinforcement Learning threads where the global network is copied, the complexity is going to increase due to the unique nature or handling Tuple Named Tensors.
		</comment>
		<comment id='13' author='Mazecreator' date='2017-10-18T17:17:36Z'>
		The current merged tensor behavior is broken since even though you can
assign a merged tensor to a variable, but then you have to slice it back up
when you want to read the states back out again; and in the meantime you've
lost the structure and shape of the intermediate states.

Having tf.assign support tuples may make sense; we'll look into it.  In the
meantime, you should be able to get pretty far using nest:

updated_variables = tf.contrib.framework.nest.map_structure(lambda v, s:
v.assign(s), variables, states)

where variables are created via map_structure using the rnn_cell's
state_size property or by looping over the zero_state tensors.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Wed, Oct 18, 2017 at 10:06 AM, Greg Peatfield ***@***.***&gt; wrote:
 I would like to still request either tf.assign() be updated to support
 Tuples to drastically reduce the complexity of updating the LSTM Tuple (as
 well as any future changes) or that the current merged tensor remain as
 supported.

 For cases where the network is copied for parallel Reinforcement Learning
 threads where the global network is copied, the complexity is going to
 increase due to the unique nature or handling Tuple Named Tensors.

 —
 You are receiving this because you were assigned.
 Reply to this email directly, view it on GitHub
 &lt;#13537 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABtim7gQ0A7m_4sjIO5tMMTtd9SjExDfks5stjAIgaJpZM4PxCmq&gt;
 .



		</comment>
		<comment id='14' author='Mazecreator' date='2017-10-18T17:30:34Z'>
		Thanks, I understand the concern.
The introduction of the Tuple ultimately creates yet another class of Variable which requires special handling, it would be nice to get this new internal structure to work like the other variables.  Maybe the nest.map_structure() can be include within the Assign() if there is some test to detect the input/output are tuples()?
		</comment>
		<comment id='15' author='Mazecreator' date='2017-10-21T21:17:45Z'>
		Yes this is a possibility.  Are you interested in sending a PR?
		</comment>
		<comment id='16' author='Mazecreator' date='2017-10-21T21:18:26Z'>
		nest.is_sequence can be used to test the variables
		</comment>
		<comment id='17' author='Mazecreator' date='2017-10-22T17:34:53Z'>
		I am very tight on time at the moment and think this might need some more holistic thinking.  My thought is it is going to be more than just an Assign update to better integrate the Tuple into TensorFlow "seamlessly".  There is likely to be more issues as well which I am  not sure I will be able to handle as I am not familiar with Named Tuples so it is more than just moving the Tensor as the Name needs to go with it.
My thought right now is a few things need to change to integrate the Tuple():

Assign() (_add() and _subtract() should be considered as well, but don't think that is urgent)
Variable()       - Would be nice to Init a Tuple variable with another Tuple Tensor (Initial State for example)
get_variable() - Not sure if the Tuple would be be supported here
Not sure if Run() will be impacted and if Placeholder needs a face-lift as well...
How with other Ops handle a Tuple if it is passed in place of a Tensor?  Is this a good way to concatenate different inputs

I am tied up until about January/Feb of next year, so if the basic Assign() needs to wait until then I can hopefully tackle it at that time (I may need to come back for hints).  I think the more high level integration would be best within the development team as they would have better visibility of issues that maybe be caused or ways to handle the Tuple Tensor.
For now I need to use the Concatenated State until I have time to try to convert all the code to Tuples() as described above.
		</comment>
		<comment id='18' author='Mazecreator' date='2017-12-20T01:11:07Z'>
		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
		</comment>
		<comment id='19' author='Mazecreator' date='2018-01-03T19:04:21Z'>
		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
		</comment>
		<comment id='20' author='Mazecreator' date='2018-01-18T19:07:00Z'>
		Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='21' author='Mazecreator' date='2018-02-06T07:49:49Z'>
		Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='22' author='Mazecreator' date='2018-02-20T19:45:05Z'>
		Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='23' author='Mazecreator' date='2018-03-07T13:22:51Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='24' author='Mazecreator' date='2018-03-07T16:33:55Z'>
		Let's move this discussion to &lt;denchmark-link:mailto:discuss@tensorflow.org&gt;discuss@tensorflow.org&lt;/denchmark-link&gt;
, which is more appropriate for design docs and proposals.
		</comment>
	</comments>
</bug>