<bug id='29481' author='galeone' open_date='2019-06-06T07:55:04Z' closed_time='2020-05-04T04:04:04Z'>
	<summary>add_update in cross-replica mode is broken (BatchNormalization layer impossible to use)</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Archlinux
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): v1.12.1-3374-g9eb67b17bf 2.0.0-dev20190605
Python version: 3.6
CUDA/cuDNN version: 10
GPU model and memory: 1080 Ti

Describe the current behavior
I expect to do a forward pass with a model with a BachNormalization layer in training mode, when using the tf.distribuite.MirroredStrategy but I can't, because it reises the following exception:

RuntimeError: add_update was called in a cross-replica context. This is not expected. If you require this feature, please file an issue.

Why it is not expected?
Describe the expected behavior
It should work.
The commit that introduced this behavior is: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/316cd57883166e6a0b4c2d0eaacebddad7675b39#diff-8eb7e20502209f082d0cb15119a50413&gt;316cd57#diff-8eb7e20502209f082d0cb15119a50413&lt;/denchmark-link&gt;

Code to reproduce the issue
import tensorflow as tf

model = tf.keras.Sequential(
    [
        tf.keras.layers.Dense(10),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(1),
    ]
)

strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
    out = model(tf.zeros((1, 10)), training=True)
	</description>
	<comments>
		<comment id='1' author='galeone' date='2019-06-10T10:26:25Z'>
		I was able to reproduce the issue on Colab with TensorFlow version 2.0.0-dev20190605.
		</comment>
		<comment id='2' author='galeone' date='2019-06-20T15:25:32Z'>
		does this have been resolved yet?
		</comment>
		<comment id='3' author='galeone' date='2019-06-29T02:43:25Z'>
		I met same question recently，does this have been resolved yet？
		</comment>
		<comment id='4' author='galeone' date='2019-07-01T23:12:20Z'>
		Has this been resolved?  I can't use tf.distribute.MirroredStrategy with BatchNormalization in a training loop
		</comment>
		<comment id='5' author='galeone' date='2019-07-15T17:57:47Z'>
		Hi &lt;denchmark-link:https://github.com/galeone&gt;@galeone&lt;/denchmark-link&gt;
, when using custom training loops with DistributionStrategy you have to use , please see: &lt;denchmark-link:https://www.tensorflow.org/guide/distribute_strategy#using_tfdistributestrategy_with_custom_training_loops&gt;https://www.tensorflow.org/guide/distribute_strategy#using_tfdistributestrategy_with_custom_training_loops&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='galeone' date='2019-07-15T17:57:48Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=29481&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=29481&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='galeone' date='2019-07-15T18:22:14Z'>
		HI &lt;denchmark-link:https://github.com/omalleyt12&gt;@omalleyt12&lt;/denchmark-link&gt;
 , I'm my tests it still continues to fail, even when using .
If the system has more then one GPU (and thus the distribution strategy can really distribute the computation) the same exception is raised even if I change the code in this way:
import tensorflow as tf

strategy = tf.distribute.MirroredStrategy()
with strategy.scope():


  model = tf.keras.Sequential([
        tf.keras.layers.Dense(10),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(1),
    ])

  def forward():
    return model(tf.zeros((1, 10)), training=True)
    
  print(strategy.experimental_run_v2(forward, args=()))
		</comment>
		<comment id='8' author='galeone' date='2019-07-31T21:12:18Z'>
		I hit the same problem when I use batch normalization, too.
&lt;denchmark-code&gt;File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/normalization.py", line 659, in call
    outputs = self._fused_batch_norm(inputs, training=training)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/normalization.py", line 556, in _fused_batch_norm
    self.add_update(mean_update)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py", line 1113, in add_update
    '`add_update` was called in a cross-replica context. This is not '
RuntimeError: `add_update` was called in a cross-replica context. This is not expected. If you require this feature, please file an issue
&lt;/denchmark-code&gt;

The problem happens while Model.build()-s so there is no way to try experimental_run_v2 for me.
Using tf-nightly-gpu-2.0-preview from today.
		</comment>
		<comment id='9' author='galeone' date='2019-08-20T07:22:23Z'>
		&lt;denchmark-link:https://github.com/galeone&gt;@galeone&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/vmarkovtsev&gt;@vmarkovtsev&lt;/denchmark-link&gt;
 this should be fixed in the latest nightly, could you please check and confirm?
		</comment>
		<comment id='10' author='galeone' date='2019-08-20T07:22:50Z'>
		&lt;denchmark-link:https://github.com/vmarkovtsev&gt;@vmarkovtsev&lt;/denchmark-link&gt;
 if it's not fixed for your use case, could you provide a simple repro?
		</comment>
		<comment id='11' author='galeone' date='2019-08-20T18:55:48Z'>
		I confirm that this issue is fixed now. Thank you &lt;denchmark-link:https://github.com/omalleyt12&gt;@omalleyt12&lt;/denchmark-link&gt;
 !
		</comment>
		<comment id='12' author='galeone' date='2019-08-20T20:11:31Z'>
		Thanks!
		</comment>
		<comment id='13' author='galeone' date='2019-08-20T20:11:34Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=29481&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=29481&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='14' author='galeone' date='2019-08-30T02:34:26Z'>
		&lt;denchmark-link:https://github.com/3fen&gt;@3fen&lt;/denchmark-link&gt;
 can you share a simple reproduction that is failing?
		</comment>
		<comment id='15' author='galeone' date='2019-08-30T02:45:58Z'>
		&lt;denchmark-link:https://github.com/omalleyt12&gt;@omalleyt12&lt;/denchmark-link&gt;
  it is similar to the main thread.
model = tf.keras.Sequential(
    [
        tf.keras.layers.Dense(10),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(1),
    ]
)

strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
    out = model(tf.zeros((1, 10)), training=True)
    print(out)
Outupt:

RuntimeError: add_update was called in a cross-replica context. This is not expected. If you require this feature, please file an issue.

		</comment>
		<comment id='16' author='galeone' date='2019-08-30T16:48:49Z'>
		&lt;denchmark-link:https://github.com/3fen&gt;@3fen&lt;/denchmark-link&gt;
 , when using custom training loops with DistributionStrategy you have to use experimental_run_v2, please see: &lt;denchmark-link:https://www.tensorflow.org/guide/distribute_strategy#using_tfdistributestrategy_with_custom_training_loops&gt;https://www.tensorflow.org/guide/distribute_strategy#using_tfdistributestrategy_with_custom_training_loops&lt;/denchmark-link&gt;

		</comment>
		<comment id='17' author='galeone' date='2019-09-02T03:22:47Z'>
		&lt;denchmark-link:https://github.com/omalleyt12&gt;@omalleyt12&lt;/denchmark-link&gt;
 Got it, thanks for the confirmation.
		</comment>
		<comment id='18' author='galeone' date='2019-09-05T13:58:50Z'>
		I hit the same problem when I use batch normalization, too.
i am using tensorflow 2.0 b1..
&lt;denchmark-link:https://user-images.githubusercontent.com/5635674/64348747-afec0900-d030-11e9-9e45-6b3c4609ed4e.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='19' author='galeone' date='2019-09-09T13:55:28Z'>
		&lt;denchmark-link:https://github.com/DAEHEESHIN&gt;@DAEHEESHIN&lt;/denchmark-link&gt;
 v2.0 b1 is ancient, we are discussing the current nightly here.
		</comment>
		<comment id='20' author='galeone' date='2019-10-28T17:57:12Z'>
		I'm having the same issue as &lt;denchmark-link:https://github.com/vmarkovtsev&gt;@vmarkovtsev&lt;/denchmark-link&gt;
 using tf2 and tf-nightly when I use  and :
tf.function(train_model.__call__, autograph=False).get_concrete_function( tf.TensorSpec([batch_size_per_replica, *flags_obj.image_dimension], tf.float32), ... )
		</comment>
		<comment id='21' author='galeone' date='2019-10-28T18:08:40Z'>
		&lt;denchmark-link:https://github.com/gdj0nes&gt;@gdj0nes&lt;/denchmark-link&gt;
 Could you share a larger code snippet that repros this?
		</comment>
		<comment id='22' author='galeone' date='2019-10-28T18:10:02Z'>
		&lt;denchmark-link:https://github.com/omalleyt12&gt;@omalleyt12&lt;/denchmark-link&gt;
 i'm working on making something I can share, just wanted to make sure there wasn't already a clear fix. It seems like it's an issue with setting the  parameter.
		</comment>
		<comment id='23' author='galeone' date='2019-10-28T18:25:46Z'>
		&lt;denchmark-link:https://github.com/omalleyt12&gt;@omalleyt12&lt;/denchmark-link&gt;
, here is an example snippet using tf-nightly:
import tensorflow as tf
from tensorflow.keras import layers

class Model(tf.Module):

    def __init__(self):
        super().__init__()

        with self.name_scope:
            self.layers = [
                layers.Conv2D(10, (3, 3)),
                layers.BatchNormalization()
            ]

    def no_param_call(self, input):
        x = input
        for layer in self.layers:
            x = layer(x)

        return x


    def param_call(self, input):
        x = input
        for layer in self.layers:
            x = layer(x, training=True)

        return x


def app():
    strategy = tf.distribute.MirroredStrategy()
    image_dimension = []
    with strategy.scope():
        model = Model()
        no_param_forward_fn = tf.function(model.no_param_call, autograph=False).get_concrete_function(
                tf.TensorSpec([1, 64, 64, 3], tf.float32)
        )
        print('no param call succeeded')
        param_forward_fn = tf.function(model.param_call, autograph=False).get_concrete_function(
            tf.TensorSpec([1, 64, 64, 3], tf.float32)
        )
        print('param call succeeded')


if __name__ == '__main__':
    app()
Output:
&lt;denchmark-code&gt;2019-10-28 13:20:34.225201: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fbc6e390880 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2019-10-28 13:20:34.225217: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.
no param call succeeded
Traceback (most recent call last):
  File "/Users/{{omitted}}/scratches/scratch_62.py", line 48, in &lt;module&gt;
    app()
  File "/Users/{{omitted}}/scratches/scratch_62.py", line 42, in app
    tf.TensorSpec([1, 64, 64, 3], tf.float32)
  File "/Users/{{omitted}}lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", line 902, in get_concrete_function
    self._initialize(args, kwargs, add_initializers_to=initializers)
  File "/Users/{{omitted}}lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", line 497, in _initialize
    *args, **kwds))
  File "/Users/{{omitted}}lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 2365, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File "/Users/{{omitted}}lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 2673, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File "/Users/{{omitted}}lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 2563, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File "/Users/{{omitted}}lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py", line 958, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "/Users/{{omitted}}lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", line 439, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File "/Users/gareth/Library/Preferences/PyCharm2019.3/scratches/scratch_62.py", line 27, in param_call
    x = layer(x, training=True)
  File "/Users/{{omitted}}lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 778, in __call__
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File "/Users/{{omitted}}lib/python3.6/site-packages/tensorflow_core/python/keras/layers/normalization.py", line 695, in call
    outputs = self._fused_batch_norm(inputs, training=training)
  File "/Users/{{omitted}}lib/python3.6/site-packages/tensorflow_core/python/keras/layers/normalization.py", line 592, in _fused_batch_norm
    self.add_update(mean_update)
  File "/Users/{{omitted}}lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/Users/{{omitted}}lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 1208, in add_update
    '`add_update` was called in a cross-replica context. This is not '
RuntimeError: `add_update` was called in a cross-replica context. This is not expected. If you require this feature, please file an issue.```
&lt;/denchmark-code&gt;

		</comment>
		<comment id='24' author='galeone' date='2019-11-03T16:34:51Z'>
		I find this issue to be very tricky.
The core of the problem comes from creating the batchnorm layer with training=True under the strategy scope. If you look at the code, it checks whether training is True and indeed calls add_update. This comes out only when you try to subclass a Keras model.
I have found a workaround though: wrap your subclassed model in another one:
my_model = Model(...)
tf.keras.Model(inputs=[tf.keras.layers.Input(shape=...)], outputs=my_model.call(training=True))
That wrapping gives an additional bonus of being able to build() and properly summary() the model.
		</comment>
		<comment id='25' author='galeone' date='2019-11-09T09:13:36Z'>
		I guess this issue should be re-opened, I'm facing again the same issue
		</comment>
		<comment id='26' author='galeone' date='2019-11-09T19:44:48Z'>
		Same issue for me!
It only occurs if I use tf.keras.callbacks.ModelCheckpoint, no problem otherwise...
		</comment>
		<comment id='27' author='galeone' date='2019-11-10T04:33:33Z'>
		&lt;denchmark-link:https://github.com/omalleyt12&gt;@omalleyt12&lt;/denchmark-link&gt;
 - are you looking into this?
&lt;denchmark-link:https://github.com/rchao&gt;@rchao&lt;/denchmark-link&gt;
 could this one of the issues you've fixed recently with checkpointing?
		</comment>
		<comment id='28' author='galeone' date='2019-11-10T09:52:31Z'>
		&lt;denchmark-link:https://github.com/guptapriya&gt;@guptapriya&lt;/denchmark-link&gt;
 Thanks for reopening this.
For your information, this issue does not seems to occur for the  format, only the TF format is is affected, as pointed out in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/34127&gt;my recent issue&lt;/denchmark-link&gt;
 (which is a duplicate of this one, sorry):

Note: no error if we use .h5 instead of TF checkpoint format. However, just a few hours ago, @karmel and @jvishnuvardhan recommended me to use TF format (because .h5 can cause issues, okay fine). So, which one should I use finally? None is working? I am lost...

		</comment>
		<comment id='29' author='galeone' date='2019-11-11T21:27:25Z'>
		&lt;denchmark-link:https://github.com/netw0rkf10w&gt;@netw0rkf10w&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/vmarkovtsev&gt;@vmarkovtsev&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/galeone&gt;@galeone&lt;/denchmark-link&gt;
 What tf version are you using? This should be fixed in the nightly
		</comment>
		<comment id='30' author='galeone' date='2019-11-11T22:48:45Z'>
		Recently we have submitted a few fixes related to using batch norm layer with tf.distribute strategies. If you could try with the nightly and let us know if it's fixed it'd be great.
		</comment>
		<comment id='31' author='galeone' date='2020-01-05T01:20:15Z'>
		I confirm that this issue is fixed on tf-nightly v2.1.0-dev20200104.
		</comment>
		<comment id='32' author='galeone' date='2020-04-20T10:19:09Z'>
		&lt;denchmark-link:https://github.com/galeone&gt;@galeone&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/vmarkovtsev&gt;@vmarkovtsev&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/netw0rkf10w&gt;@netw0rkf10w&lt;/denchmark-link&gt;
  Issue seems to be fixed in latest TF version. Please confirm and close the issue if it is resolved for you. Thanks!
		</comment>
		<comment id='33' author='galeone' date='2020-04-27T04:51:27Z'>
		Any updates regarding this issue? Thanks!
		</comment>
		<comment id='34' author='galeone' date='2020-05-04T04:04:04Z'>
		Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
		<comment id='35' author='galeone' date='2020-05-04T04:04:06Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29481&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29481&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='36' author='galeone' date='2020-06-22T09:53:47Z'>
		In TF 2.1.0 the issue is not fixed... anybody help me ?
Is this issue fixed in TF 2.2 ?
		</comment>
		<comment id='37' author='galeone' date='2020-06-22T10:41:49Z'>
		&lt;denchmark-link:https://github.com/taki0112&gt;@taki0112&lt;/denchmark-link&gt;
 I don't understand. Why didn't you simply upgrade to TF 2.2 and see if it works for you?
		</comment>
	</comments>
</bug>