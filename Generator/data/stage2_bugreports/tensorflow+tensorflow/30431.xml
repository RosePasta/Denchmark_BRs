<bug id='30431' author='daniel-eder' open_date='2019-07-05T10:53:34Z' closed_time='2019-09-19T23:21:17Z'>
	<summary>Significantly reduced validation accuracy when switching from alpha0 to beta0/1</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Code is strongly based on: Stock example, but uses kaggle aerial-cactus-identification dataset.
OS Platform and Distribution: Google Colab
TensorFlow installed from: binary via pip install tensorflow-gpu==2.0.0-beta0
TensorFlow version: 2.0.0-beta0
Python version: 3.6.7
CUDA/cuDNN version: N/A (Google Colab as of 05.07.2019)
GPU model and memory:  N/A (Google Colab as of 05.07.2019)


With beta 0 and 1 the training/validation history is as follows:
&lt;denchmark-link:https://camo.githubusercontent.com/da3002977f52e91995528c28b5764cebade985032419cc727062b12ea186b2f7/68747470733a2f2f7075752e73682f444f5939482f386563316131613766352e706e67&gt;&lt;/denchmark-link&gt;

This is a very low validation accuracy. This can of course happen, even with code based on an example, the issue is that this only appears with beta0 and beta1 builds, not with alpha0 (see below) when using the exactly same code and training data.

With alpha0 and the same code the history looks as follows:
&lt;denchmark-link:https://camo.githubusercontent.com/cdca05a1ebdd45ae5ee660f69de13b7420f16c6117b057bb65e0c621f08eee1b/68747470733a2f2f7075752e73682f444f5973732f623134636337663937392e706e67&gt;&lt;/denchmark-link&gt;

An upgrade of the tensorflow version should not affect the resulting accuracy in such a manner.
Code to reproduce the issue

Open this Google Colab
Run the code with Alpha0

Make note of the training history plot.
Restart the runtime.
Run the code with Beta0 or Beta1

Make note of the training history plot.
Observe that with no change to the code but using the beta0 instead of alpha0 the validation accuracy goes down from &gt; 95% to &lt; 90%, with beta1 even to &lt; 80%

Other info / logs
My best guess is that changes have been made to the pretrained MobileNetV2 or to the Adam optimizer, otherwise the drastic loss in accuracy is hard to explin.
	</description>
	<comments>
		<comment id='1' author='daniel-eder' date='2019-07-08T13:46:58Z'>
		I have tried on colab with TF version 2.0 beta 1 and alpha 0 and was able to reproduce the issue.Thanks!
		</comment>
		<comment id='2' author='daniel-eder' date='2019-07-08T22:59:31Z'>
		Could be related to the changes in trainable flags between alpha and beta; &lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;
 , can you take a look?
		</comment>
		<comment id='3' author='daniel-eder' date='2019-07-23T19:29:38Z'>
		I encountered the same issue with beta1 version. I'm also using tf.keras.Model.fit() (but with distributed Dataset)
Hope you guys can get this bug fixed, since people experimenting with tf 2.0 will get unsatisfactory results from their models.
Cheers!
		</comment>
		<comment id='4' author='daniel-eder' date='2019-07-23T21:47:13Z'>
		I was able to reproduce this in colab. Will look into it.
		</comment>
		<comment id='5' author='daniel-eder' date='2019-07-25T03:25:41Z'>
		Hi,
I also encountered this issue in a colab from a course demonstrating transfer learning with MobileNetV2.
I ran the same exact code in alpha0 and then in beta1, and am seeing a reduction of 13.6% in validation accuracy in beta1.
&lt;denchmark-link:https://colab.research.google.com/drive/1KszPPoBVwBoAGwgoSMtUjuLZMYRXzvWV#scrollTo=6XOGP0tExZvv&gt;Colab for Alpha0&lt;/denchmark-link&gt;

&lt;denchmark-link:https://colab.research.google.com/drive/1-s53y_oAMaa1VCZl7JqpzNCAoVumk1Qj#scrollTo=6XOGP0tExZvv&gt;Colab for Beta1&lt;/denchmark-link&gt;

I hope this aids in your diagnosis of the issue.
		</comment>
		<comment id='6' author='daniel-eder' date='2019-07-29T22:09:49Z'>
		In between these 2 versions, we have changed the semantics of setting batch_norm.trainable = False on a BatchNormalization layer.
Old behavior: freeze weights, freeze updates, use training-time behavior during training (i.e. use the batch statistics are used for training-time normalization).
New behavior: freeze weights, freeze updates, use inference-time behavior during training (i.e. use the frozen batch statistics gamma and beta).
In general we expect the new behavior to work better for fine-tuning use cases. But in any case, it is different, so it's expected that you'd see different results.
Note that you can always manually specify what behavior you want by explicitly passing the training argument in call for your BatchNormalization layers when building the model you use for training (and changing it when you use a different model that will be for evaluation and prediction only).
		</comment>
		<comment id='7' author='daniel-eder' date='2019-07-31T06:53:44Z'>
		Thanks for the clarification &lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;
, indeed I see the documentation of the change in the comments of the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/layers/normalization_v2.py#L26-L65&gt;BatchNormalization&lt;/denchmark-link&gt;
 class.
So, since both @taotsetung and I observed this behavior with the MobileNetV2 model provided by tf.keras.applications.MobileNetV2, I presume this model was trained before the change (the weights throughout the model would have been updated based on the gradients of activations normalized using the training-time batch statistics and now don't "recognize" the activations normalized using the new behavior)? Could this explain the drastic drop in validation accuracy we observed and could the solution simply be to re-train the entire base model provided by tf.keras.applications.MobileNetV2 using the new behavior?
I hope I am understanding this correctly, my apologies if I am way off.
		</comment>
		<comment id='8' author='daniel-eder' date='2019-07-31T07:03:53Z'>
		Never mind, I realized just after posting that my suggestion made no sense since the behavior when trainable = True did not change. Re-training the entire model (all layers are trainable) would not lead to any different results.
		</comment>
		<comment id='9' author='daniel-eder' date='2019-09-06T22:54:38Z'>
		Based on &lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;
 explanation and &lt;denchmark-link:https://github.com/AbrahamSanders&gt;@AbrahamSanders&lt;/denchmark-link&gt;
 description, I am closing this issue. However, feel free to open a new issue if you notice similar performance issue with any other models. Thanks!
		</comment>
		<comment id='10' author='daniel-eder' date='2019-09-19T23:21:18Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=30431&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=30431&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>