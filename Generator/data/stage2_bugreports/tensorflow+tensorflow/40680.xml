<bug id='40680' author='prabhat00155' open_date='2020-06-22T15:17:19Z' closed_time='2020-06-24T22:09:49Z'>
	<summary>Segmentation fault when running TFLite's benchmark_model</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Darwin Kernel Version 19.5.0(for model conversion, Linux 4.9.140-tegra for running the benchmark tool)
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): Installed TFLite benchmark_model tool using the instructions given here: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark.
TensorFlow version (use command below): N/A
Python version: N/A
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with:

TF 1.0: python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
TF 2.0: python -c "import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"


I converted emotion_ferplus (&lt;denchmark-link:https://github.com/onnx/models/tree/master/vision/body_analysis/emotion_ferplus&gt;https://github.com/onnx/models/tree/master/vision/body_analysis/emotion_ferplus&lt;/denchmark-link&gt;
) ONNX model to TF using onnx-tensorflow(&lt;denchmark-link:https://github.com/onnx/onnx-tensorflow&gt;https://github.com/onnx/onnx-tensorflow&lt;/denchmark-link&gt;
) and then converted the TF model to TFLite using tflite_convert.
&lt;denchmark-code&gt;$ onnx-tf convert -i model.onnx -o emotion.pb
$ tflite_convert --enable_v1_converter --graph_def_file=emotion.pb --output_file=emotion.tflite --output_format=TFLITE --input_shape=1,1,64,64 --input_arrays=Input3 --output_arrays=Plus692_Output_0 --inference_type=FLOAT --input_data_type=FLOAT
&lt;/denchmark-code&gt;

Then I ran benchmark_model tool on the tflite model I got from the previous step.
&lt;denchmark-code&gt;onnxruntime@onnxruntime-desktop:~/Documents/tensorflow$ bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model --graph=emotion.tflite --num_runs=10 --num_threads=4
STARTING!
Duplicate flags: num_threads
Min num runs: [10]
Min runs duration (seconds): [1]
Max runs duration (seconds): [150]
Inter-run delay (seconds): [-1]
Num threads: [0]
Benchmark name: []
Output prefix: []
Min warmup runs: [1]
Min warmup runs duration (seconds): [0.5]
Graph: [emotion.tflite]
Input layers: []
Input shapes: []
Input value ranges: []
Input layer values files: []
Allow fp16 : [0]
Require full delegation : [0]
Enable op profiling: [0]
Max profiling buffer entries: [1024]
CSV File to export profiling data to: []
Enable platform-wide tracing: [0]
#threads used for CPU inference: [0]
Max number of delegated partitions : [0]
Min nodes per partition : [0]
External delegate path : []
External delegate options : []
Use gpu : [0]
Use xnnpack : [0]
Loaded model ../models/tflite/emotion.tflite
The input model file size (MB): 35.0461
Initialized session in 1.645ms.
Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
Segmentation fault (core dumped)

&lt;/denchmark-code&gt;

Describe the expected behavior
benchmark tool should run without crashing and give model performance metrics like the following:
&lt;denchmark-code&gt;Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
count=10 first=101249 curr=46906 min=46491 max=101249 avg=52839.8 std=16163

Running benchmark for at least 10 iterations and at least 1 seconds but terminate if exceeding 150 seconds.
count=22 first=46543 curr=46554 min=46473 max=49668 avg=46957.8 std=627

Inference timings in us: Init: 218485, First inference: 101249, Warmup (avg): 52839.8, Inference (avg): 46957.8
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Peak memory footprint (MB): init=1.23047 overall=28.8906
&lt;/denchmark-code&gt;

Standalone code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='prabhat00155' date='2020-06-23T02:55:26Z'>
		Hi Can you attach the generated TF lite model? Thanks!
		</comment>
		<comment id='2' author='prabhat00155' date='2020-06-23T07:44:23Z'>
		
Hi Can you attach the generated TF lite model? Thanks!

How do you suggest I do that? I can't attach a TFLite model here. Also, if you follow the steps mentioned above, you can get the TFLite model.
		</comment>
		<comment id='3' author='prabhat00155' date='2020-06-23T14:49:29Z'>
		The dialog box I am typing in now says "Attach files by dragging and dropping":
&lt;denchmark-link:https://user-images.githubusercontent.com/1891418/85418755-0675f580-b526-11ea-99e9-7551fa77677f.png&gt;&lt;/denchmark-link&gt;

I think this should work for the model.
		</comment>
		<comment id='4' author='prabhat00155' date='2020-06-23T15:09:38Z'>
		You can't attach a TFLite model here(unsupported file type).
		</comment>
		<comment id='5' author='prabhat00155' date='2020-06-23T15:12:52Z'>
		I tried creating a zip and attaching it, but the zipped version is 31 MB and there is a 10 MB limit here.
		</comment>
		<comment id='6' author='prabhat00155' date='2020-06-23T19:52:04Z'>
		Ah I see. I created the emotion.pb file by using onnx-tf released version with TF 1.15. I was unable to convert this tflite_convert. What version of TF did you use for TF-&gt;TFL conversion? Can you execute the instructions in the template so we can get version information?
		</comment>
		<comment id='7' author='prabhat00155' date='2020-06-24T08:20:13Z'>
		&lt;denchmark-code&gt;(onnxrt_env) prroy-Mac:emotion_ferplus prroy$ python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
v1.15.0-92-g5d80e1e8e6 1.15.2
(onnxrt_env) prroy-Mac:emotion_ferplus prroy$ tflite_convert --enable_v1_converter --graph_def_file=emotion.pb --output_file=emotion.tflite --output_format=TFLITE --input_shape=1,1,64,64 --input_arrays=Input3 --output_arrays=Plus692_Output_0 --inference_type=FLOAT --input_data_type=FLOAT
2020-06-24 13:49:23.852168: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-06-24 13:49:23.863606: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fbd4d6ecc90 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-06-24 13:49:23.863622: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-06-24 13:49:24.062042: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count &gt;= 8, compute capability &gt;= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)
2020-06-24 13:49:24.062143: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-06-24 13:49:24.500949: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize
2020-06-24 13:49:24.500974: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 175 nodes (-86), 174 edges (-56), time = 243.756ms.
2020-06-24 13:49:24.500979: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 175 nodes (0), 174 edges (0), time = 86.759ms.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='prabhat00155' date='2020-06-24T22:09:49Z'>
		thanks! I was able to successfully convert the model. When I visualized it with tensorflow/lite/tools:visualize I see that the model has int64 inputs to the ADD op:
&lt;denchmark-link:https://user-images.githubusercontent.com/1891418/85630269-33aacc80-b628-11ea-9e03-f57bfba880cc.png&gt;&lt;/denchmark-link&gt;

This is not supported in TF Lite, so when I runbenchmark_model I get this error:
&lt;denchmark-code&gt;ERROR: Inputs and outputs not all float|uint8|int16 types.
ERROR: Node number 3 (ADD) failed to invoke.
&lt;/denchmark-code&gt;

I converted the model successfully with --target_ops=SELECT_TF_OPS and then ran with tensorflow/lite/tools/benchmark:benchmark_model_plus_flex:
&lt;denchmark-code&gt;INFO: Created TensorFlow Lite delegate for select TF ops.
INFO: TfLiteFlexDelegate delegate: 102 nodes delegated out of 102 nodes with 1 partitions.

The input model file size (MB): 35.0607
Initialized session in 34.002ms.
Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
count=16 first=51470 curr=29156 min=28983 max=51470 avg=31982 std=5605

Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.
count=50 first=29166 curr=29189 min=28529 max=29906 avg=28935.9 std=255

Inference timings in us: Init: 34002, First inference: 51470, Warmup (avg): 31982, Inference (avg): 28935.9
&lt;/denchmark-code&gt;

By the way, I think it is best in this case to use the same version of benchmark_model as you used for the ONNX conversion. Apparently their toolchain requires TF 1.15, so I did a git checkout to release 1.15.2 before building and running benchmark_model. Perhaps there is a way to tell ONNX that the largest int type should be int32? I am not very familiar with the tool.
		</comment>
		<comment id='9' author='prabhat00155' date='2020-06-24T22:09:51Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40680&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40680&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='prabhat00155' date='2020-06-25T13:39:01Z'>
		

&lt;denchmark-link:https://user-images.githubusercontent.com/1891418/85630269-33aacc80-b628-11ea-9e03-f57bfba880cc.png&gt;&lt;/denchmark-link&gt;







I didn't get that error when running benchmark_model. The tool itself crashed with segmentation fault. I can retry everything on latest TF version.
		</comment>
		<comment id='11' author='prabhat00155' date='2020-06-25T18:59:44Z'>
		I upgraded TF version and saw the error:
ERROR: tensorflow/lite/kernels/add.cc:326 Type INT64 is unsupported by op Add.
ERROR: Node number 5 (ADD) failed to invoke.
Shouldn't tflite_convert fail in such cases?
		</comment>
	</comments>
</bug>