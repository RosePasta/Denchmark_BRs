<bug id='36704' author='durandg12' open_date='2020-02-12T18:39:49Z' closed_time='2020-02-14T22:57:04Z'>
	<summary>Why K.mean is used in tf.keras.losses.binary_crossentropy ?</summary>
	<description>
In &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/a641fa1c5c10f0a278c7671fd6f7df550a74935d/tensorflow/python/keras/losses.py#L993&gt;line 993&lt;/denchmark-link&gt;
 of the code of ,  is called on axis  of .
I wonder why there is this K.mean call and why tf.keras.losses.binary_crossentropy doesn't simply return K.binary_crossentropy(y_true, y_pred, from_logits=from_logits).
On the contrary, tf.keras.losses.categorical_crossentropy and tf.keras.losses.sparse_categorical_crossentropy just return the call to their tf.keras.backend equivalent.
I think it may be inconsistent and misleading, especially because tf.keras.losses.categorical_crossentropy and tf.keras.backend.categorical_crossentropy behave similarly, but not tf.keras.losses.binary_crossentropy and tf.keras.backend.binary_crossentropy, so higher level objects like tf.keras.metrics.CategoricalCrossentropy may not work as one would expect.
	</description>
	<comments>
		<comment id='1' author='durandg12' date='2020-02-14T18:49:33Z'>
		Thank you &lt;denchmark-link:https://github.com/durandg12&gt;@durandg12&lt;/denchmark-link&gt;
 . Historically, the loss functions have been expecting that inputs are at least 2D and computing mean on the last axis in order to support sample weighting correctly.
&lt;denchmark-code&gt;y_true: Ground truth values. shape = [batch_size, d0, .. dN].
y_pred: The predicted values. shape = [batch_size, d0, .. dN].
sample_weight: Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size], then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight. (Note on dN-1: all metric functions reduce by 1 dimension, usually the last axis (-1)).
&lt;/denchmark-code&gt;

If your input labels are [batch_size, d0] the result from the functions will be [batch_size] ie. one loss value per sample. This applies to binary, categorical and sparse categorical crossentropy functions.
tf.keras.losses.binary_crossentropy([[0], [1]], [[0.3], [0.8]])
&lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.3566748 , 0.22314338], dtype=float32)&gt;
tf.keras.losses.categorical_crossentropy([[0, 1, 0], [0, 0, 1]], [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])
&lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.05129331, 2.3025851 ], dtype=float32)&gt;
tf.keras.losses.sparse_categorical_crossentropy([1, 2], [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])
&lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.05129344, 2.3025851 ], dtype=float32)&gt;
In all the three cases above, the # samples = 2.
Hope this clears the confusion between the cross entropy functions.
		</comment>
		<comment id='2' author='durandg12' date='2020-02-14T22:57:05Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36704&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36704&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>