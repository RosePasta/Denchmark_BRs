<bug id='713' author='wchan' open_date='2016-01-07T02:48:49Z' closed_time='2016-01-09T01:48:30Z'>
	<summary>failed to query current context: CUDA_ERROR_DEINITIALIZED</summary>
	<description>
This is a regression problem, code was working fine 2 wks ago, did a git sync to the head and now I have this log message:
I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcublas.so.7.0 locally
I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcudnn.so.6.5 locally
I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcufft.so.7.0 locally
I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcuda.so locally
I tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcurand.so.7.0 locally
I tensorflow/core/common_runtime/gpu/gpu_init.cc:103] Found device 0 with properties:
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:02:00.0
Total memory: 12.00GiB
Free memory: 11.87GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:127] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_init.cc:137] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:705] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Allocating 11.27GiB bytes.
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:52] GPU 0 memory begins at 0x2306c80000 extends to 0x25d8436a67
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 16.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 32.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 64.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 128.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 256.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 512.0KiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 16.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 32.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 64.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 128.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 256.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 512.00MiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.00GiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.00GiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.00GiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.00GiB
I tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 16.00GiB
create_encoder graph time 0.569116
create_decoder graph time 2.629546
create_loss graph time 0.960969
create_optimizer graph time 34.311540
F tensorflow/stream_executor/cuda/cuda_driver.cc:299] failed to query current context: CUDA_ERROR_DEINITIALIZED
F tensorflow/stream_executor/cuda/cuda_driver.cc:299] failed to query current context: CUDA_ERROR_DEINITIALIZED
F tensorflow/stream_executor/cuda/cuda_driver.cc:299] failed to query current context: CUDA_ERROR_DEINITIALIZED
F tensorflow/stream_executor/cuda/cuda_driver.cc:408] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(prior_context_) (0 vs. 4)
Aborted (core dumped)
Note: (the create_*) is my code creating the graph.
The python code it crashes on is:
sess.run(tf.initialize_all_variables())
	</description>
	<comments>
		<comment id='1' author='wchan' date='2016-01-07T03:21:25Z'>
		here's the stack trace from gdb:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/2&gt;#2&lt;/denchmark-link&gt;
  0x00007fff52e11ad4 in tensorflow::internal::LogMessageFatal::~LogMessageFatal() () from /data-local/wchan/tensorflow_env/local/lib/python2.7/site-packages/tensorflow/python/) () from /data-local/wchan/tensorflow_env/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/7&gt;#7&lt;/denchmark-link&gt;
  0x00007fff52c4eac8 in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) ()
from /data-local/wchan/tensorflow_env/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/8&gt;#8&lt;/denchmark-link&gt;
  0x00007fff52c43d20 in std::_Function_handler&lt;void (), std::_Bind&lt;std::_Mem_fn&lt;void (tensorflow::(anonymous namespace)::ExecutorState::, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)&gt; &gt;::_M_invoke(std::_Any_data const&amp;) ()
from /data-local/wchan/tensorflow_env/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/9&gt;#9&lt;/denchmark-link&gt;
  0x00007fff52dff357 in tensorflow::ThreadPool::WorkerLoop() () from /data-local/wchan/tensorflow_env/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/10&gt;#10&lt;/denchmark-link&gt;
 0x00007ffff436ea40 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/11&gt;#11&lt;/denchmark-link&gt;
 0x00007ffff7bc4182 in start_thread (arg=0x7ffef87f8700) at pthread_create.c:312
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/12&gt;#12&lt;/denchmark-link&gt;
 0x00007ffff78f147d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111
		</comment>
		<comment id='2' author='wchan' date='2016-01-07T15:34:11Z'>
		+1
		</comment>
		<comment id='3' author='wchan' date='2016-01-07T17:30:08Z'>
		I've got the same issue
		</comment>
		<comment id='4' author='wchan' date='2016-01-07T17:45:20Z'>
		I have a suspicion that our mega-commit &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/1c579361cd1e088dd5e05a394b1561a73e3667ba&gt;1c57936&lt;/denchmark-link&gt;
 might have introduced a change that broke this (there are a bunch of changes that could be responsible, including changes to the stream executor and updating to a new version of Eigen, among other things).
It would be great if someone could confirm if it works at &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/295625035eefc7801cfe831bd1ba851faacb7579&gt;2956250&lt;/denchmark-link&gt;
 but fails at the commit right after.
		</comment>
		<comment id='5' author='wchan' date='2016-01-07T18:03:32Z'>
		&lt;denchmark-link:https://github.com/vrv&gt;@vrv&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/295625035eefc7801cfe831bd1ba851faacb7579&gt;2956250&lt;/denchmark-link&gt;
 works for me; &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/1c579361cd1e088dd5e05a394b1561a73e3667ba&gt;1c57936&lt;/denchmark-link&gt;
 does not.
		</comment>
		<comment id='6' author='wchan' date='2016-01-07T18:10:50Z'>
		Cool, thanks for verifying.  If you have time:
Revert the WORKSPACE file and eigen.BUILD changes (to use an earlier version of Eigen) and see if the problem goes away.
I'll dig into potential other issues in that commit.
		</comment>
		<comment id='7' author='wchan' date='2016-01-07T18:36:51Z'>
		Checking out WORKSPACE and eigen.BUILD from &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/295625035eefc7801cfe831bd1ba851faacb7579&gt;2956250&lt;/denchmark-link&gt;
 does not fix the problem.
		</comment>
		<comment id='8' author='wchan' date='2016-01-07T18:37:38Z'>
		Ok thanks, that helps.
		</comment>
		<comment id='9' author='wchan' date='2016-01-07T18:49:25Z'>
		&lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
, who independently was trying to debug this.
		</comment>
		<comment id='10' author='wchan' date='2016-01-07T19:04:19Z'>
		I reverted to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/295625035eefc7801cfe831bd1ba851faacb7579&gt;2956250&lt;/denchmark-link&gt;
 and it seems to work, but the execution speed is cut in half
		</comment>
		<comment id='11' author='wchan' date='2016-01-07T19:09:19Z'>
		Yup, that commit improved speed for some models, &lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
 is looking into what broke it.
		</comment>
		<comment id='12' author='wchan' date='2016-01-08T01:33:14Z'>
		I can confirm that with eigen-a0661a2bb165, an Eigen kernel fails to
launch. But with the older eigen-ce5a455b34c0, this problem goes away.
My reproducing command line:
bazel run -c opt --config=cuda
//tensorflow/models/image/alexnet:alexnet_benchmark
Passing to &lt;denchmark-link:https://github.com/benoitsteiner&gt;@benoitsteiner&lt;/denchmark-link&gt;

On Thu, Jan 7, 2016 at 11:18 AM, Martin Wicke &lt;denchmark-link:mailto:notifications@github.com&gt;notifications@github.com&lt;/denchmark-link&gt;

wrote:

Assigned #713 #713 to
@zheng-xq https://github.com/zheng-xq.
—
Reply to this email directly or view it on GitHub
#713 (comment).

		</comment>
		<comment id='13' author='wchan' date='2016-01-08T02:52:04Z'>
		&lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
: what eigen kernel failed, so we can figure out which kernel was the problem?
		</comment>
		<comment id='14' author='wchan' date='2016-01-08T05:52:17Z'>
		The kernel that was causing problem is:
EigenMetaKernel_Vectorizable&lt;Eigen::TensorEvaluator&lt;Eigen::TensorAssignOp&lt;Eigen::TensorMap&lt;Eigen::Tensor&lt;float,
1, 1, int&gt;, 16&gt;,
Eigen::TensorCwiseUnaryOp&lt;Eigen::internal::scalar_right&lt;float, float,
Eigen::internal::scalar_product_op&lt;float, float&gt;, true&gt;,
Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 1, 1, int&gt;, 16&gt; const&gt; const&gt;
const, Eigen::GpuDevice&gt;, int&gt;
It came from cwise_mul:
tensorflow::BinaryOp&lt;Eigen::GpuDevice, tensorflow::functor::mul &gt;
On Thu, Jan 7, 2016 at 6:53 PM, Vijay Vasudevan &lt;denchmark-link:mailto:notifications@github.com&gt;notifications@github.com&lt;/denchmark-link&gt;

wrote:

@zheng-xq https://github.com/zheng-xq: what eigen kernel failed, so we
can figure out which kernel was the problem?
—
Reply to this email directly or view it on GitHub
#713 (comment)
.

		</comment>
		<comment id='15' author='wchan' date='2016-01-08T18:57:05Z'>
		When running "bazel-bin/tensorflow/models/image/alexnet/alexnet_benchmark --benchmarks=all" on my machine, I get the following error:
external/re2/re2/regexp.cc:136: Bad reference count 65535
F tensorflow/stream_executor/cuda/cuda_driver.cc:408] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(prior_context_) (0 vs. 4)
		</comment>
		<comment id='16' author='wchan' date='2016-01-09T01:48:30Z'>
		Okay, we rolled back the change to switch to the newer version of Eigen, which apparently had some issues.  &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/22267addd64b00f9457605cdce3d82276a478780&gt;22267ad&lt;/denchmark-link&gt;

So things should be now be working.
However, this uses an older version of Eigen that has some speed problems.
&lt;denchmark-link:https://github.com/benoitsteiner&gt;@benoitsteiner&lt;/denchmark-link&gt;
 has upstreamed a newer fix to Eigen which should bring back the speed improvements  not crash things, so look for that soon.
Closing this for now, let us know if you still see problems.
		</comment>
		<comment id='17' author='wchan' date='2016-01-10T22:24:40Z'>
		FWIW, I spent some time independently debugging this issue as well.   What I found was that the preprocessor magic in Eigen was removing nvcc's ability to know what templated kernels to instantiate for the device code.  The fix for the non-convolutional kernels was to modify the eigen/unsupported/CXX11/src/tensor/TensorDeviceCuda.h file and modify the LAUNCH_CUDA_KERNEL macro for the device code to force instantiation of the kernel.
The full patch and details are in the Bitbucket pull request:
&lt;denchmark-link:https://bitbucket.org/eigen/eigen/pull-requests/152/alternative-way-of-forcing-instantiation/diff&gt;https://bitbucket.org/eigen/eigen/pull-requests/152/alternative-way-of-forcing-instantiation/diff&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;#ifndef __CUDA_ARCH__
#define LAUNCH_CUDA_KERNEL(kernel, gridsize, blocksize, sharedmem, device, ...)            \
    (kernel) &lt;&lt;&lt; (gridsize), (blocksize), (sharedmem), (device).stream() &gt;&gt;&gt; (__VA_ARGS__); \
    assert(cudaGetLastError() == cudaSuccess);
#else
#define LAUNCH_CUDA_KERNEL(kernel, ...)  \
    { static const auto __attribute__((__unused__)) __makeTheKernelInstantiate = &amp;(kernel); } \
   eigen_assert(false &amp;&amp; "Cannot launch a kernel from another kernel");
#endif
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>