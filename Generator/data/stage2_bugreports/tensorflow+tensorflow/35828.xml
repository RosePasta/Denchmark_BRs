<bug id='35828' author='RCTimms' open_date='2020-01-13T19:01:42Z' closed_time='2020-06-18T00:52:52Z'>
	<summary>tf.contrib.cudnn_rnn.CudnnGRU runtime error: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Google Colab
TensorFlow version (use command below):
1.15.0
Python version:
3.6.9
CUDA/cuDNN version:
Cuda compilation tools, release 10.0, V10.0.130
GPU model and memory:


I am trying to convert my CPU GRU code to a CudnnGRU implementation. When I run the code &lt;denchmark-link:https://colab.research.google.com/drive/1c64kUiCs8K17I5YygWaf14DKWhObdKA9&gt;here&lt;/denchmark-link&gt;
, my script runs for a seemingly random of number of training epochs (normally between 0 and 20) before the session crashes, often with a CUDA_ERROR_ILLEGAL_ADDRESS error. I see similar behaviour when I run the script on my institution's hardware (TF version 1.14, CUDA Version: 10.1).

See Google colab sheet &lt;denchmark-link:https://colab.research.google.com/drive/1c64kUiCs8K17I5YygWaf14DKWhObdKA9&gt;here&lt;/denchmark-link&gt;
. I select the GPU as the hardware accelerator in the notebook settings.
Other info / logs
&lt;denchmark-code&gt;Learning Rate: 0.001 
Total number of portions: 10 
batch_length: 500 
n_mini_batches 25 
mini_batch_length 20 
number of MEG channels/PCs: 5 

WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/cudnn_rnn/python/layers/cudnn_rnn.py:342: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/cudnn_rnn/python/layers/cudnn_rnn.py:345: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
Alpha coefficients will be soft-plus transformed
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/slot_creator.py:193: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Device mapping:
/job:localhost/replica:0/task:0/device:XLA_CPU:0 -&gt; device: XLA_CPU device
/job:localhost/replica:0/task:0/device:XLA_GPU:0 -&gt; device: XLA_GPU device
/job:localhost/replica:0/task:0/device:GPU:0 -&gt; device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5

Beginning training...
0
1
2
3
4
&lt;/denchmark-code&gt;

Please see the colab-jupyter.log file attached.
Apologies for my ignorance and thank you in advance for any kind help.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/4055241/colab-jupyter.log&gt;colab-jupyter.log&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='RCTimms' date='2020-01-14T10:40:41Z'>
		Same here!
I have tried tf2.1.
These are my logs
``



Jan 14, 2020, 11:37:46 AM
WARNING
WARNING:root:kernel 6f0febf6-6ed2-4820-a9ca-0deef6ba6b6d restarted




Jan 14, 2020, 11:37:46 AM
INFO
KernelRestarter: restarting kernel (1/5), keep random ports


Jan 14, 2020, 11:37:45 AM
WARNING
2020-01-14 10:37:45.132229: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1


Jan 14, 2020, 11:37:45 AM
WARNING
2020-01-14 10:37:45.132148: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered


Jan 14, 2020, 11:37:34 AM
WARNING
2020-01-14 10:37:34.845228: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.


Jan 14, 2020, 11:37:30 AM
WARNING
2020-01-14 10:37:30.658416: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.


Jan 14, 2020, 11:37:30 AM
WARNING
2020-01-14 10:37:30.658401: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia


Jan 14, 2020, 11:37:30 AM
WARNING
2020-01-14 10:37:30.658283: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia


Jan 14, 2020, 11:35:55 AM
WARNING
WARNING:root:kernel 6f0febf6-6ed2-4820-a9ca-0deef6ba6b6d restarted


Jan 14, 2020, 11:35:55 AM
INFO
KernelRestarter: restarting kernel (1/5), keep random ports


``





		</comment>
		<comment id='2' author='RCTimms' date='2020-06-03T23:24:52Z'>
		&lt;denchmark-link:https://github.com/RCTimms&gt;@RCTimms&lt;/denchmark-link&gt;
 Sorry for late response.
I can reproduce the issue with . &lt;denchmark-link:https://colab.research.google.com/gist/jvishnuvardhan/0379988c1c214a22a1233d3445e8025c/debug_code_nbatches.ipynb&gt;Here&lt;/denchmark-link&gt;
 is the gist.
Have you tried to implement this with TF2.x? I am not sure what is the root-cause but there were lot of performance improvements with TF2.x that might help you. Thanks!
		</comment>
		<comment id='3' author='RCTimms' date='2020-06-11T00:09:06Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='4' author='RCTimms' date='2020-06-18T00:52:50Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='5' author='RCTimms' date='2020-06-18T00:52:54Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35828&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35828&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>