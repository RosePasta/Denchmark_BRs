<bug id='5850' author='xuancong84' open_date='2016-11-25T07:59:34Z' closed_time='2017-06-16T16:50:06Z'>
	<summary>adding zero-sized layer costs huge amount of memory, but without increasing total # of trainable parameters</summary>
	<description>
In order to make code generic, my code does append a layer with a variable-sized layer which might be of length 0, i.e.
 tv_concat = tf.concat(1, [tf.reshape(tf.slice(X,[0,time_step,0],[-1,Ncontext,-1]), [tp_current_batch_size,embed_size*Ncontext]), tv_hh])
In the above code, X is a tensor of shape [batch_size, max_time_step, state_size], tv_hh of shape [batch_size, state_size]. Depending on the value of Ncontext, a differently sized layer will be concatenated with the existing layer tv_hh. So when Ncontext=0, a zero-size layer will be concatenated so that tv_concat will be the same as tv_hh. I have explicitly checked and confirmed that tv_concat is indeed of the same shape as tv_hh. I have also checked that the total number of trainable parameters are the same.
However, when Ncontext=0, the graphics memory consumption is &gt;3G, if I use tv_concat=tv_hh, the graphics memory consumption is only 2G.
So is this behaviour expected?
	</description>
	<comments>
		<comment id='1' author='xuancong84' date='2016-11-25T21:53:09Z'>
		Please can you provide all of the information requested in the issues reporting template.
		</comment>
		<comment id='2' author='xuancong84' date='2016-11-26T02:56:04Z'>
		

Operating System:
Ubuntu 14.04.5 LTS


Installed version of CUDA and cuDNN:
/usr/local/cuda-8.0
cudnn-8.0-linux-x64-v5.1.tgz


It is installed from binary pip package


A link to the pip package you installed:
https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl


The output from python -c "import tensorflow; print(tensorflow.version)"
xuancong@wxc-i2r:~/projects/tf-rnnlm$ python -c "import tensorflow; print(tensorflow.version)"
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally
0.11.0


		</comment>
		<comment id='3' author='xuancong84' date='2016-11-26T19:13:00Z'>
		Backprop through multiple tf.slice is memory expensive . Try to rearrange
your code to do one tf.split upfront before any per-timestep calculation.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Fri, Nov 25, 2016, 6:56 PM xuancong84 ***@***.***&gt; wrote:

    1.

    Operating System:
    Ubuntu 14.04.5 LTS
    2.

    Installed version of CUDA and cuDNN:
    /usr/local/cuda-8.0
    cudnn-8.0-linux-x64-v5.1.tgz
    3.

    It is installed from binary pip package
    4.

    A link to the pip package you installed:

    https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl
    5.

    The output from python -c "import tensorflow; print(tensorflow.
    *version*)"
    ***@***.***:~/projects/tf-rnnlm$ python -c "import tensorflow;
    print(tensorflow.*version*)"
    I tensorflow/stream_executor/dso_loader.cc:111] successfully opened
    CUDA library libcublas.so locally
    I tensorflow/stream_executor/dso_loader.cc:111] successfully opened
    CUDA library libcudnn.so locally
    I tensorflow/stream_executor/dso_loader.cc:111] successfully opened
    CUDA library libcufft.so locally
    I tensorflow/stream_executor/dso_loader.cc:111] successfully opened
    CUDA library libcuda.so.1 locally
    I tensorflow/stream_executor/dso_loader.cc:111] successfully opened
    CUDA library libcurand.so locally
    0.11.0

 —
 You are receiving this because you are subscribed to this thread.
 Reply to this email directly, view it on GitHub
 &lt;#5850 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AHk2brhwRvp1-qbzNpNaYOiLbvDB2_Kkks5rB5_ngaJpZM4K8MF9&gt;
 .



		</comment>
		<comment id='4' author='xuancong84' date='2016-11-27T08:03:15Z'>
		&lt;denchmark-link:https://github.com/zffchen78&gt;@zffchen78&lt;/denchmark-link&gt;
 I am aware of that backprop through multiple tf.slice is memory expensive. However, slicing out a zero-sized layer should not cost that much memory I presume.
		</comment>
		<comment id='5' author='xuancong84' date='2016-12-06T16:51:22Z'>
		Does rearranging the code as suggested avoid the high memory cost?
		</comment>
		<comment id='6' author='xuancong84' date='2016-12-07T01:10:59Z'>
		&lt;denchmark-link:https://github.com/michaelisard&gt;@michaelisard&lt;/denchmark-link&gt;
 It is not possible to rearrange the code as suggested because Ncontext is an integer (&gt;=0) parameter to be tuned. Take a closer look at my code, you will find out why.
		</comment>
		<comment id='7' author='xuancong84' date='2016-12-07T20:45:32Z'>
		&lt;denchmark-link:https://github.com/sherrym&gt;@sherrym&lt;/denchmark-link&gt;
 would you reassign to someone who can take a look?
		</comment>
		<comment id='8' author='xuancong84' date='2016-12-07T22:21:26Z'>
		&lt;denchmark-link:https://github.com/xuancong84&gt;@xuancong84&lt;/denchmark-link&gt;
  btw, it could help troubleshooting if you dumped memory
timeline and checked which op is responsible for this extra 1GB allocation,
using timeline as here:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/6019#issuecomment-264544202&gt;#6019 (comment)&lt;/denchmark-link&gt;


&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Wed, Dec 7, 2016 at 12:45 PM, Michael Isard ***@***.***&gt; wrote:
 @sherrym &lt;https://github.com/sherrym&gt; would you reassign to someone who
 can take a look?

 —
 You are receiving this because you are subscribed to this thread.
 Reply to this email directly, view it on GitHub
 &lt;#5850 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AABaHH_M_niMSTyjFS4z4GnrEgME-OSwks5rFxrvgaJpZM4K8MF9&gt;
 .



		</comment>
		<comment id='9' author='xuancong84' date='2017-06-16T16:50:06Z'>
		Automatically closing due to lack of recent activity. We hope that you were able to resolve it on your own. However, since this is a support issue rather than a bug or feature request, you will probably get more information by posting it on StackOverflow.
		</comment>
	</comments>
</bug>