<bug id='28995' author='lgeiger' open_date='2019-05-24T11:16:15Z' closed_time='2019-08-15T20:27:32Z'>
	<summary>Keras doesn't allow tf.data validation without validation_steps</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac 10.14.4, Ubuntu 18.01
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.14.1.dev20190524 | 1.14.0rc0 | 1.14.0
Python version: 3.6.8
CUDA/cuDNN version: -
GPU model and memory: -

Describe the current behavior
Using tf.data as validation_data without defining validation_steps fails with TypeError: 'DatasetV1Adapter' object does not support indexing.  Using tf.data without steps_per_epoch works as expected when using it as training data instead.
Describe the expected behavior
I think the behaviour of training data and validation data in Keras model.fit should be consistent. This would make Keras a lot easier to use together with tf.data because it gets rid of the need for defining a exact number of steps.
Code to reproduce the issue
import tensorflow as tf
import tensorflow_datasets as tfds

train, test = tfds.load(name="mnist", split=[tfds.Split.TRAIN, tfds.Split.TEST], as_supervised=True)

def scale(image, label):
    return tf.cast(image, tf.float32) / 255, label

model = tf.keras.Sequential(
    [
        tf.keras.layers.Conv2D(32, 3, activation="relu", input_shape=(28, 28, 1)),
        tf.keras.layers.MaxPooling2D(),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(64, activation="relu"),
        tf.keras.layers.Dense(10, activation="softmax"),
    ]
)

model.compile(loss="sparse_categorical_crossentropy", optimizer="adam")

model.fit(
    train.batch(256),
    validation_data=test.batch(256),
)
Other info / logs
  File "test.py", line 24, in &lt;module&gt;
    epochs=10,
  File "/Users/lukasgeiger/miniconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py", line 644, in fit
    use_multiprocessing=use_multiprocessing)
  File "/Users/lukasgeiger/miniconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py", line 615, in fit
    steps_name='steps_per_epoch')
  File "/Users/lukasgeiger/miniconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py", line 145, in model_iteration
    _print_train_info(inputs, val_inputs, steps_per_epoch, verbose)
  File "/Users/lukasgeiger/miniconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py", line 450, in _print_train_info
    hasattr(inputs[0], 'shape') and hasattr(val_inputs[0], 'shape')):
TypeError: 'DatasetV1Adapter' object does not support indexing
	</description>
	<comments>
		<comment id='1' author='lgeiger' date='2019-05-27T09:31:19Z'>
		&lt;denchmark-link:https://github.com/lgeiger&gt;@lgeiger&lt;/denchmark-link&gt;
 Can you please provide tensorflow version?.
With Tensorflow tf-nightly and tf-2.0.0-alpha the above code outputs expected output. Thanks!
		</comment>
		<comment id='2' author='lgeiger' date='2019-05-27T09:37:02Z'>
		
@lgeiger Can you please provide tensorflow version?.
With Tensorflow tf-nightly and tf-2.0.0-alpha the above code outputs expected output. Thanks!

Sorry for the missing version, I am using tf-nightly==1.14.1.dev20190524 and tensorflow==1.14.0rc0.
		</comment>
		<comment id='3' author='lgeiger' date='2019-05-27T09:54:14Z'>
		&lt;denchmark-link:https://github.com/lgeiger&gt;@lgeiger&lt;/denchmark-link&gt;
 I tried with tf-nightly==1.14.1.dev20190524 and tensorflow==1.14.0rc0 but the code executed without any error. Can you try once again and let us know if that still gives error. Thanks!
		</comment>
		<comment id='4' author='lgeiger' date='2019-05-27T10:34:36Z'>
		
Can you try once again and let us know if that still gives error.

Yep, just tried it with tensorflow==1.14.0rc0 and it still gives an error. Which python version are you using?
		</comment>
		<comment id='5' author='lgeiger' date='2019-05-27T10:43:49Z'>
		&lt;denchmark-link:https://github.com/lgeiger&gt;@lgeiger&lt;/denchmark-link&gt;
 Thanks for confirming. I am using Python 3.6.7 version.
		</comment>
		<comment id='6' author='lgeiger' date='2019-05-27T11:12:05Z'>
		I can still reproduce this with Python 3.6.8, TensorFlow 1.14.0-rc0 and TensorFlow Datasets 1.0.2
		</comment>
		<comment id='7' author='lgeiger' date='2019-05-30T18:16:40Z'>
		&lt;denchmark-link:https://github.com/lgeiger&gt;@lgeiger&lt;/denchmark-link&gt;
 I don't see the error you mentioned. I ran your code using (&lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/1f15082053c715c76643058dda5eae58/tf28995_gpu.ipynb&gt;gist&lt;/denchmark-link&gt;
),  (&lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/4736ce8fd58e1bd2657728409a34752e/tf_gpu_28995_gpu.ipynb&gt;gist&lt;/denchmark-link&gt;
), and  (&lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/ba8cfffda7f0bb54ca55d41643f06381/tf-nightly_gpu_28995_gpu.ipynb&gt;gist&lt;/denchmark-link&gt;
).
Please try to run it yourself. If you see any issues, let us know. Thanks!
		</comment>
		<comment id='8' author='lgeiger' date='2019-05-30T18:28:42Z'>
		
I don't see the error you mentioned. I ran your code using tensorflow==2.0.0-alpha0(gist), tensorflow-gpu==2.0.0-alpha0 (gist), and tf-nightly-gpu-2.0-preview==2.0.0-dev20190530 (gist).

Thanks for reproducing. As mentioned above the issue is related to 1.14.1.dev20190524 and 1.14.0rc0. TF 2 works fine.
		</comment>
		<comment id='9' author='lgeiger' date='2019-05-30T19:36:38Z'>
		&lt;denchmark-link:https://github.com/lgeiger&gt;@lgeiger&lt;/denchmark-link&gt;
 I ran it in  and don't see any error. Here is the &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/6f99cbefa5cd7f90169f7098fe67f4f0/tf_nightly_1_14_1_gpu_28995_gpu.ipynb&gt;gist&lt;/denchmark-link&gt;
.
It will be helpful If you can create a gist and share.  Thanks!
		</comment>
		<comment id='10' author='lgeiger' date='2019-05-30T20:04:21Z'>
		
Here is a colab using 1.14.0rc0
Here is a colab using 1.14.1.dev20190524

		</comment>
		<comment id='11' author='lgeiger' date='2019-05-30T21:53:04Z'>
		&lt;denchmark-link:https://github.com/lgeiger&gt;@lgeiger&lt;/denchmark-link&gt;
 I could reproduce the issue when i select "cpu". If I select "gpu" as shown in my gist, there is no error. Thanks!
		</comment>
		<comment id='12' author='lgeiger' date='2019-05-30T22:02:01Z'>
		
@lgeiger I could reproduce the issue when i select "cpu". If I select "gpu" as shown in my gist, there is no error. Thanks!

Strange. Good that you can reproduce it now
		</comment>
		<comment id='13' author='lgeiger' date='2019-05-31T03:19:16Z'>
		&lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
 Is this intended behavior? Can you please take a look at this issue? Thanks!
		</comment>
		<comment id='14' author='lgeiger' date='2019-05-31T15:40:13Z'>
		This is a question for tf.keras folks, not tf.data.
		</comment>
		<comment id='15' author='lgeiger' date='2019-06-16T04:46:55Z'>
		I also encounter this problem, tf.keras + tf.data. My tensorflow version is tf13.1.
		</comment>
		<comment id='16' author='lgeiger' date='2019-07-11T00:06:06Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 Any updates on this issue?
		</comment>
		<comment id='17' author='lgeiger' date='2019-07-13T00:02:51Z'>
		Facing the similar issue.
		</comment>
		<comment id='18' author='lgeiger' date='2019-07-17T09:07:46Z'>
		&lt;denchmark-link:https://github.com/omalleyt12&gt;@omalleyt12&lt;/denchmark-link&gt;
 I can still reproduce this issue with TensorFlow 1.14.0:
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-1-89e74e90b73d&gt; in &lt;module&gt;
     21 model.fit(
     22     train.batch(256),
---&gt; 23     validation_data=test.batch(256),
     24 )

~/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    778           validation_steps=validation_steps,
    779           validation_freq=validation_freq,
--&gt; 780           steps_name='steps_per_epoch')
    781 
    782   def evaluate(self,

~/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)
    143 
    144   if mode == ModeKeys.TRAIN:
--&gt; 145     _print_train_info(inputs, val_inputs, steps_per_epoch, verbose)
    146 
    147   # Enter tf.distribute.Strategy scope.

~/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py in _print_train_info(inputs, val_inputs, steps_per_epoch, verbose)
    448 def _print_train_info(inputs, val_inputs, steps_per_epoch, verbose):
    449   if (val_inputs and steps_per_epoch is None and verbose and inputs and
--&gt; 450       hasattr(inputs[0], 'shape') and hasattr(val_inputs[0], 'shape')):
    451     print('Train on %d samples, validate on %d samples' %
    452           (inputs[0].shape[0], val_inputs[0].shape[0]))

TypeError: 'DatasetV1Adapter' object is not subscriptable
		</comment>
		<comment id='19' author='lgeiger' date='2019-07-24T08:19:32Z'>
		Yup, can reproduce here as well with 1.14.0.
Suboptimal workarounds are:

Set verbose=0
Set steps_per_epoch to something that is not None
Don't use validation_data

		</comment>
		<comment id='20' author='lgeiger' date='2019-07-24T16:13:26Z'>
		Hi &lt;denchmark-link:https://github.com/lgeiger&gt;@lgeiger&lt;/denchmark-link&gt;
, sorry for the very late reply. That indeed seems to be a bug for certain runtime condition. As &lt;denchmark-link:https://github.com/benyeoh&gt;@benyeoh&lt;/denchmark-link&gt;
, mentioned, the quickest workaround is to set verbose = 0, which suppress the print of the message, and still give u the same training/eval behavior.
We are in the middle of a refactoring of the training logic, and will take care of the issue in 2.0 release.
		</comment>
		<comment id='21' author='lgeiger' date='2019-07-24T16:28:31Z'>
		Thanks for taking a look

We are in the middle of a refactoring of the training logic, and will take care of the issue in 2.0 release.

Does that mean TF 1.14 won't receive a fix for this? I understand that you are focusing on 2.0 at the moment, but a fix would be very useful for people currently relying still relying on TF 1.x.
		</comment>
		<comment id='22' author='lgeiger' date='2019-07-24T16:37:56Z'>
		We probably won't update 1.14 release for this bug.
Also just check the docstring of model.fit()

validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split. validation_data could be:
tuple (x_val, y_val) of Numpy arrays or tensors
tuple (x_val, y_val, val_sample_weights) of Numpy arrays
dataset or a dataset iterator.
For the first two cases, batch_size must be provided. For the last case, validation_steps must be provided.

Seems that the validation_steps is needed if the validation_data is a dataset.
		</comment>
		<comment id='23' author='lgeiger' date='2019-08-08T20:59:42Z'>
		This is fixed with latest tf-nightly version '1.15.0-dev20190808'
		</comment>
		<comment id='24' author='lgeiger' date='2019-08-15T20:27:33Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=28995&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=28995&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='25' author='lgeiger' date='2020-06-10T12:35:22Z'>
		Sorry for opening this after almost a year, but from docstring of model.fit():
steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. This argument is not supported with array inputs.
Why not do the same (If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted) for validation data?
The docstring is a bit confusing
validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split. validation_data could be:

tuple (x_val, y_val) of Numpy arrays or tensors
tuple (x_val, y_val, val_sample_weights) of Numpy arrays
dataset For the first two cases, batch_size must be provided. For the last case, validation_steps must be provided.

However
validation_steps: Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If validation_data is a tf.data dataset and 'validation_steps' is None, validation will run until the validation_data dataset is exhausted.
For 1.15 during model.fit() when validation data is tf.data and validation_steps is None  the following error raises:
ValueError: When using data tensors as input to a model, you should specify the `steps_per_epoch` argument.
But this is not raised during model.evaluate() with steps = None
		</comment>
		<comment id='26' author='lgeiger' date='2020-06-19T16:33:36Z'>
		
Sorry for opening this after almost a year, but from docstring of model.fit():
steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. This argument is not supported with array inputs.
Why not do the same (If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted) for validation data?
The docstring is a bit confusing
validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split. validation_data could be:

tuple (x_val, y_val) of Numpy arrays or tensors
tuple (x_val, y_val, val_sample_weights) of Numpy arrays
dataset For the first two cases, batch_size must be provided. For the last case, validation_steps must be provided.

However
validation_steps: Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If validation_data is a tf.data dataset and 'validation_steps' is None, validation will run until the validation_data dataset is exhausted.
For 1.15 during model.fit() when validation data is tf.data and validation_steps is None the following error raises:
ValueError: When using data tensors as input to a model, you should specify the `steps_per_epoch` argument.
But this is not raised during model.evaluate() with steps = None

same issue here
		</comment>
		<comment id='27' author='lgeiger' date='2020-12-23T03:44:17Z'>
		Hello,
I am hitting same error - TensorFlow 1.5, Python 2.7.17 inside docker container.
Minimal logical code -
images, labels = read_list ( data_dir, data_list ) &lt;= here data_dir is full path of dir. containing image &amp; label files. data_list is a text file, 2 column containing names of image file and lable file.
The output is array consisting of full path of images &amp; corresponding labels.
queue = tf.data.Dataset.from_tensor_slices([images, labels])
img_contents = tf.io.read_file(queue[0]) &lt;= Error location
label_contents = tf.io.read_file(queue[1])
File "/wasr/wasr_models/image_reader.py", line 180, in read_images_from_disk
img_contents = tf.io.read_file(input_queue[0])
TypeError: 'DatasetV1Adapter' object does not support indexing
I have tried reading various posts but this post has come closest to what I am looking for. Any help to fix the issue will be greatly appreciated.
Thanks,
-Shailesh
		</comment>
	</comments>
</bug>