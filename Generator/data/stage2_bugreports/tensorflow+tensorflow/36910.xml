<bug id='36910' author='matthewp14' open_date='2020-02-20T00:01:10Z' closed_time='2020-02-24T02:55:12Z'>
	<summary>Cannot Convert Tensor Object to Numpy Array</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information -
Have I written custom code - YES
OS Platform and Distribution - macOS Catalina 10.15.2
TensorFlow installed from - Conda
TensorFlow version (use command below): 2.0.0

Python version: 3.6.7
CPU

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
I am writing a custom layer where I need a kernel to be element-wise multiplied on the input. I am trying to convert the input tensor into a Numpy array using K.eval(input) but I get the following error:
AttributeError: 'Tensor' object has no attribute '_numpy'
Describe the expected behavior
Code to reproduce the issue Provide a reproducible test case that is the
bare minimum necessary to generate the problem.
Here is the custom layer that I am trying to implement:
import numpy as np
import tensorflow
from tensorflow.keras import backend as K
from tensorflow.keras.layers import InputSpec, Layer, Dense, Conv2D, Lambda, Multiply
from tensorflow.keras import constraints
from tensorflow.keras import initializers

from binary_ops import binarize


class BinaryConv2D(Conv2D):
    '''Binarized Convolution2D layer
    References: 
    "BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1" [http://arxiv.org/abs/1602.02830]
    '''

    def __init__(self, filters, kernel_lr_multiplier='Glorot',
                 bias_lr_multiplier=None, H=1., **kwargs):
        super(BinaryConv2D, self).__init__(filters, **kwargs)
        self.H = H
        self.kernel_lr_multiplier = kernel_lr_multiplier
        self.bias_lr_multiplier = bias_lr_multiplier

    def build(self, input_shape):
        if self.data_format == 'channels_first':
            channel_axis = 1
        else:
            channel_axis = -1
        if input_shape[channel_axis] is None:
            raise ValueError('The channel dimension of the inputs '
                             'should be defined. Found `None`.')

        input_dim = input_shape[channel_axis]
        kernel_shape = self.kernel_size + (input_dim, self.filters)

        base = self.kernel_size[0] * self.kernel_size[1]
        if self.H == 'Glorot':
            nb_input = int(input_dim * base)
            nb_output = int(self.filters * base)
            self.H = np.float32(np.sqrt(1.5 / (nb_input + nb_output)))
            # print('Glorot H: {}'.format(self.H))

        if self.kernel_lr_multiplier == 'Glorot':
            nb_input = int(input_dim * base)
            nb_output = int(self.filters * base)
            self.kernel_lr_multiplier = np.float32(1. / np.sqrt(1.5 / (nb_input + nb_output)))
            # print('Glorot learning rate multiplier: {}'.format(self.lr_multiplier))

        self.kernel_constraint = Clip(-self.H, self.H)
        self.kernel_initializer = initializers.RandomUniform(-self.H, self.H)
        self.kernel = self.add_weight(shape=kernel_shape,
                                      initializer=self.kernel_initializer,
                                      name='kernel',
                                      regularizer=self.kernel_regularizer,
                                      constraint=self.kernel_constraint)

        if self.use_bias:
            self.lr_multipliers = [self.kernel_lr_multiplier, self.bias_lr_multiplier]
            self.bias = self.add_weight((self.output_dim,),
                                        initializer=self.bias_initializers,
                                        name='bias',
                                        regularizer=self.bias_regularizer,
                                        constraint=self.bias_constraint)

        else:
            self.lr_multipliers = [self.kernel_lr_multiplier]
            self.bias = None

        # Set input spec.
        self.input_spec = InputSpec(ndim=4, axes={channel_axis: input_dim})
        self.built = True

    def call(self, inputs):
        binary_kernel = binarize(self.kernel, H=self.H)

        print(type(K.eval(binary_kernel)))
        
        bk_temp = np.reshape(K.eval(binary_kernel[:,:,:,0]), (-1,self.kernel_size[0],self.kernel_size[0],1))
        bk_cube = np.zeros((30,30,30,1))
        bk_cube[:] = bk_temp
        outputs = inputs * bk_cube
       

        if self.use_bias:
            outputs = K.bias_add(
                outputs,
                self.bias,
                data_format=self.data_format)

        
        if self.activation is not None:
            return self.activation(outputs) 
        return outputs

    def get_config(self):
        config = {'H': self.H,
                  'kernel_lr_multiplier': self.kernel_lr_multiplier,
                  'bias_lr_multiplier': self.bias_lr_multiplier}
        base_config = super(BinaryConv2D, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))
I should add that this error comes up only when I try to compile this into a model. If I just call build then it works fine
Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='matthewp14' date='2020-02-20T06:53:57Z'>
		&lt;denchmark-link:https://github.com/matthewp14&gt;@matthewp14&lt;/denchmark-link&gt;
 I am unable to replicate your code in my local to help you resolve your issue due to the binary_ops dependency used by you in the code, please refer to the &lt;denchmark-link:https://colab.sandbox.google.com/gist/Saduf2019/4e2c57e65ddebdf66c7ed194f3f6d7ee/36910.ipynb&gt;gist&lt;/denchmark-link&gt;
, and share all dependencies to replicate your issue.
		</comment>
		<comment id='2' author='matthewp14' date='2020-02-20T06:55:47Z'>
		Hey &lt;denchmark-link:https://github.com/matthewp14&gt;@matthewp14&lt;/denchmark-link&gt;
  could you provide a gist or code, if possible, for your binarize method?
		</comment>
		<comment id='3' author='matthewp14' date='2020-02-20T16:45:50Z'>
		&lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/Joey155&gt;@Joey155&lt;/denchmark-link&gt;
 I was able to figure out a workaround for the first problem that I was having but now I am having another issue. I am trying to compile my model using some custom lambda functions that I wrote in the lambda_layers.py file. For one of them I need to iterate through the input tensor but I am getting the following error message:
TypeError: You are attempting to use Python control flow in a layer that was not declared to be dynamic. Pass dynamic=True to the class constructor.
Encountered error:
"""
in converted code:
&lt;denchmark-code&gt;/CUP-Net/src/lambda_layers.py:45 streak  *
    shape = list(x)
opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py:396 converted_call
    return py_builtins.overload_of(f)(*args)
opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:547 __iter__
    self._disallow_iteration()
opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:540 _disallow_iteration
    self._disallow_when_autograph_enabled("iterating over `tf.Tensor`")
opt/miniconda3/envs/engs89/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:518 _disallow_when_autograph_enabled
    " decorating it directly with @tf.function.".format(task))

OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.
&lt;/denchmark-code&gt;

"""
I decorated my function with @tf.function but still no luck. This seems to be related to the same issue that I was having before where tf/keras are stuck in graph mode while they should be in eager mode. Any help with this would be great.
Here is a link to the &lt;denchmark-link:https://gist.github.com/matthewp14/1275eb95c72dddf85483fadbcdcb913c&gt;gist&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='matthewp14' date='2020-02-24T02:55:14Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36910&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36910&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>