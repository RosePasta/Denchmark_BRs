<bug id='45950' author='emadboctorx' open_date='2020-12-23T23:54:48Z' closed_time='2020-12-24T14:54:01Z'>
	<summary>Tensorflow does not use the GPU during training with eager execution, despite manual activation</summary>
	<description>
 This issue is being the same issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/45546&gt;here&lt;/denchmark-link&gt;
 and it's being re-opened because this &lt;denchmark-link:https://github.com/sanjoy&gt;person&lt;/denchmark-link&gt;
 decided to close it without resolution / discussing his decision, so there we go again ...
Please make sure that this is an issue related to performance of TensorFlow.
As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux (Google colab)):
TensorFlow installed using pip install tensorflow-gpu:
TensorFlow version (2.3.1):
Python version: 3.6.9
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 10.1.243
GPU model and memory: Tesla T4 computeCapability: 7.5 coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with:

TF 1.0: python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
TF 2.0: python -c "import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"


I'm experiencing a very slow training time when running a third party &lt;denchmark-link:https://github.com/marload/DeepRL-TensorFlow2/blob/master/DQN/DQN_Discrete.py&gt;script&lt;/denchmark-link&gt;
 which is the same(0% difference between GPU and CPU). I use the following command for activating the GPU on google colab after installing  using :
&lt;denchmark-code&gt;physical_devices = tf.config.experimental.list_physical_devices('GPU')
if len(physical_devices) &gt; 0:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)
&lt;/denchmark-code&gt;

I add the lines above in  in the script I referred to earlier and I use &lt;denchmark-link:https://github.com/wandb/client&gt;wandb&lt;/denchmark-link&gt;
 for monitoring the training. Here are the &lt;denchmark-link:https://drive.google.com/file/d/1Fgn7tlm6HBUyZIcPelVjxNz_RWvY7QU_/view?usp=sharing&gt;graphs&lt;/denchmark-link&gt;
 within a few minutes of training showing 0% GPU utilization.
Describe the expected behavior
A fast performance which results in a remarkable difference in speeds (CPU vs GPU) and GPU utilization above 0% if the metrics are accurate and if they are not, I'm still experiencing the same speed when run on CPU or GPU.
Standalone code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
&lt;denchmark-code&gt;import wandb
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.optimizers import Adam

import gym
import argparse
import numpy as np
from collections import deque
import random

tf.keras.backend.set_floatx('float64')
wandb.init(name='DQN', project="deep-rl-tf2")

parser = argparse.ArgumentParser()
parser.add_argument('--gamma', type=float, default=0.95)
parser.add_argument('--lr', type=float, default=0.005)
parser.add_argument('--batch_size', type=int, default=32)
parser.add_argument('--eps', type=float, default=1.0)
parser.add_argument('--eps_decay', type=float, default=0.995)
parser.add_argument('--eps_min', type=float, default=0.01)

args = parser.parse_args()

class ReplayBuffer:
    def __init__(self, capacity=10000):
        self.buffer = deque(maxlen=capacity)
    
    def put(self, state, action, reward, next_state, done):
        self.buffer.append([state, action, reward, next_state, done])
    
    def sample(self):
        sample = random.sample(self.buffer, args.batch_size)
        states, actions, rewards, next_states, done = map(np.asarray, zip(*sample))
        states = np.array(states).reshape(args.batch_size, -1)
        next_states = np.array(next_states).reshape(args.batch_size, -1)
        return states, actions, rewards, next_states, done
    
    def size(self):
        return len(self.buffer)

class ActionStateModel:
    def __init__(self, state_dim, aciton_dim):
        self.state_dim  = state_dim
        self.action_dim = aciton_dim
        self.epsilon = args.eps
        
        self.model = self.create_model()
    
    def create_model(self):
        model = tf.keras.Sequential([
            Input((self.state_dim,)),
            Dense(32, activation='relu'),
            Dense(16, activation='relu'),
            Dense(self.action_dim)
        ])
        model.compile(loss='mse', optimizer=Adam(args.lr))
        return model
    
    def predict(self, state):
        return self.model.predict(state)
    
    def get_action(self, state):
        state = np.reshape(state, [1, self.state_dim])
        self.epsilon *= args.eps_decay
        self.epsilon = max(self.epsilon, args.eps_min)
        q_value = self.predict(state)[0]
        if np.random.random() &lt; self.epsilon:
            return random.randint(0, self.action_dim-1)
        return np.argmax(q_value)

    def train(self, states, targets):
        self.model.fit(states, targets, epochs=1, verbose=0)
    

class Agent:
    def __init__(self, env):
        self.env = env
        self.state_dim = self.env.observation_space.shape[0]
        self.action_dim = self.env.action_space.n

        self.model = ActionStateModel(self.state_dim, self.action_dim)
        self.target_model = ActionStateModel(self.state_dim, self.action_dim)
        self.target_update()

        self.buffer = ReplayBuffer()

    def target_update(self):
        weights = self.model.model.get_weights()
        self.target_model.model.set_weights(weights)
    
    def replay(self):
        for _ in range(10):
            states, actions, rewards, next_states, done = self.buffer.sample()
            targets = self.target_model.predict(states)
            next_q_values = self.target_model.predict(next_states).max(axis=1)
            targets[range(args.batch_size), actions] = rewards + (1-done) * next_q_values * args.gamma
            self.model.train(states, targets)
    
    def train(self, max_episodes=1000):
        for ep in range(max_episodes):
            done, total_reward = False, 0
            state = self.env.reset()
            while not done:
                action = self.model.get_action(state)
                next_state, reward, done, _ = self.env.step(action)
                self.buffer.put(state, action, reward*0.01, next_state, done)
                total_reward += reward
                state = next_state
            if self.buffer.size() &gt;= args.batch_size:
                self.replay()
            self.target_update()
            print('EP{} EpisodeReward={}'.format(ep, total_reward))
            wandb.log({'Reward': total_reward})


def main():
    physical_devices = tf.config.experimental.list_physical_devices('GPU')
    if len(physical_devices) &gt; 0:
        tf.config.experimental.set_memory_growth(physical_devices[0], True)
    env = gym.make('CartPole-v1')
    agent = Agent(env)
    agent.train(max_episodes=1000)

if __name__ == "__main__":
    main()
&lt;/denchmark-code&gt;

Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='emadboctorx' date='2020-12-24T04:59:22Z'>
		Sorry for the confusion, I've reopened the original issue.
		</comment>
		<comment id='2' author='emadboctorx' date='2020-12-24T11:45:40Z'>
		&lt;denchmark-link:https://github.com/emadboctorx&gt;@emadboctorx&lt;/denchmark-link&gt;
,
The original issue has been re-opened. Can we please close this issue since it is already being tracked there. Thanks!
		</comment>
		<comment id='3' author='emadboctorx' date='2020-12-24T14:54:02Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45950&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45950&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>