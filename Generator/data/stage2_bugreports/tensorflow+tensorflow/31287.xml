<bug id='31287' author='svobora' open_date='2019-08-02T17:51:46Z' closed_time='2020-03-30T10:18:55Z'>
	<summary>Optimizer (Adam) does not propagate hyperparameter update on first update</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 64b
TensorFlow version (use command below): 2.0.0b1
Python version: 3.6

Describe the current behavior
Using tf.keras.optimizers.Adam and updating hyperparameters results in hyperparameters not updating on the first update.
Describe the expected behavior
Hyperparameters DO update on the first run, ie they are initialized to tf.Variables at constructor of the optimizer, not on first call.
Code to reproduce the issue
adam = tf.keras.optimizers.Adam()
adam.learning_rate = 0.00001   # this does not update learning_rate, but changes python-typed  hyperparameters in "_hyper" dictionary to tf.Variables
adam.learning_rate = 0.00001  # this finally updates the tf.Variable 
Calling adam._hyper before update yields:
{'learning_rate': 0.001, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999}
Calling adam._hyper after first update yields:
{'learning_rate': &lt;tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001&gt;, 'decay': &lt;tf.Variable 'decay:0' shape=() dtype=float32, numpy=0.0&gt;, 'beta_1': &lt;tf.Variable 'beta_1:0' shape=() dtype=float32, numpy=0.9&gt;, 'beta_2': &lt;tf.Variable 'beta_2:0' shape=() dtype=float32, numpy=0.999&gt;}
	</description>
	<comments>
		<comment id='1' author='svobora' date='2019-08-02T17:59:47Z'>
		Well actually to get it working, first a getter must be called for adam.learning_rate and only then the python-typed variables are replaced with tf.Variable variables and one can use the setter.
		</comment>
		<comment id='2' author='svobora' date='2019-08-05T11:29:32Z'>
		&lt;denchmark-link:https://github.com/svobora&gt;@svobora&lt;/denchmark-link&gt;
, In order to expedite the trouble-shooting process, please provide a complete code to reproduce the issue reported here. Thanks!
		</comment>
		<comment id='3' author='svobora' date='2019-08-05T12:47:38Z'>
		Hello, I will provide a simple and complete, yet a little different example. I believe the actual problem is with the tf.train.Checkpoint. Probably the variables are overwritten by the restored values when adam._hyper is converted from plaintext format to tf.Variable format.
In the provided example, the optimizer.learning_rate is not properly stored. Even though it is a different problem, I believe they are connected. I am not yet able to easily reproduce the first problem, because it appears after ckpt.restore() call (I was able to reproduce it with minimal example), but it depends on the manager.save() (I was not yet able to reproduct with minimal example).
&lt;denchmark-code&gt;import tensorflow as tf

def save():
    optimizer = tf.keras.optimizers.Adam()
    ckpt = tf.train.Checkpoint(optimizer=optimizer)
    manager = tf.train.CheckpointManager(ckpt, "tmp", max_to_keep=3)
    _ = optimizer.learning_rate  # comment this line and "load()" will print (0.001)
    optimizer.learning_rate = 0.0003
    manager.save()

def load():
    pretrained_model = "tmp/ckpt-1"
    optimizer = tf.keras.optimizers.Adam()
    ckpt = tf.train.Checkpoint(optimizer=optimizer)
    ckpt.restore(str(pretrained_model))
    print("Learning rate for epoch {} set to {}.".format(0, optimizer.learning_rate))

if __name__ == '__main__':
    save()
    load()
&lt;/denchmark-code&gt;

Comment the suggested line to see a bug.
		</comment>
		<comment id='4' author='svobora' date='2019-08-06T04:58:06Z'>
		&lt;denchmark-link:https://github.com/svobora&gt;@svobora&lt;/denchmark-link&gt;
 Thanks for providing the reproducible code.
I am able to reproduce the issue with Tenosrflow 2.0.0.beta1 on Colab. Please see the colab &lt;denchmark-link:https://colab.research.google.com/drive/1LmQbwnFbOcEF9g1XZvWWkqQjcgP-Dck_&gt;gist&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='5' author='svobora' date='2019-08-07T21:48:23Z'>
		I think this issue surfaces a discrepancy in Save and Restore method and is not necessarily a bug on  tf.keras.optimizers.Adam side.
Can you please try setting learning_rate arg for the tf.keras.optimizers.Adam() instead?
		</comment>
		<comment id='6' author='svobora' date='2019-08-08T04:55:43Z'>
		I believe the main issue is that tf.keras.optimizers.Adam() returns not fully initialized object. What is the reason not to construct the object with tf.Variable hyperparameters in the first place? Why is the object constructed with hyperparameters in the form {'learning_rate': 0.001, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999} and only on first getter call converted to tf.Variable?
		</comment>
		<comment id='7' author='svobora' date='2019-08-09T16:17:40Z'>
		Thanks for the report!
Variable creation is deferred so that Estimator use case is supported. (you can't create a variable outside of Estimator graph)
The workaround here is just to call opt._create_hypers() so that things will get initialized correctly before calling Checkpoint, but I'm concerned this is not a good solution.
		</comment>
		<comment id='8' author='svobora' date='2019-08-09T16:18:48Z'>
		&lt;denchmark-link:https://github.com/allenlavoie&gt;@allenlavoie&lt;/denchmark-link&gt;
 from the minimal example here, do you think it's better if we create variables eagerly, since checkpoint needs to capture variables at creation time?
		</comment>
		<comment id='9' author='svobora' date='2019-08-12T15:52:30Z'>
		How about the setter also creates a Variable? So if you pass the constructor argument you don't get a variable (since presumably you'll always pass that same value, so no need to restore it from a checkpoint) but if you assign to the attribute you always get a variable.
		</comment>
		<comment id='10' author='svobora' date='2020-03-30T10:18:55Z'>
		You can simply initialize the optimizer with the right learning_rate. This seems a niche case to support. Closing it for now.
		</comment>
		<comment id='11' author='svobora' date='2020-03-30T10:18:57Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31287&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31287&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='svobora' date='2020-03-30T10:46:35Z'>
		You can simply initialize the optimizer with the right learning_rate.
Really? But maybe I should be aware first that save function called on an object doesn't save anything to know that I have to set it manually. The issue is in unexpected behavior of the Saver, which depends on background creation of some other object no user knows about. Like the &lt;denchmark-link:https://github.com/allenlavoie&gt;@allenlavoie&lt;/denchmark-link&gt;
  said, either create on assignment, or ban assignment. Do not allow to assign to preliminary objects.
		</comment>
		<comment id='13' author='svobora' date='2020-03-30T10:49:36Z'>
		What I ended up doing is calling a getter on the object, which as opposed to setter, actually creates the Variable.
_ = optimizer.learning_rate
		</comment>
		<comment id='14' author='svobora' date='2020-03-30T13:21:06Z'>
		
What I ended up doing is calling a getter on the object, which as opposed to setter, actually creates the Variable.
_ = optimizer.learning_rate

This is only a workaround, b/c there are quite some other hyper parameters. I'm not sure what your use case is, why would you use an optimizer without minimizing anything?
On the other hand, we're moving forward with having Adam(var_list) or something like that, which should be able to fix this problem
		</comment>
		<comment id='15' author='svobora' date='2020-03-30T13:51:26Z'>
		why would you use an optimizer without minimizing anything?
How did you infer that I do not minimize anything? I posted a toy example to reproduce the issue, not my company's code, which was affected and I had to find the issue and fix it.
On the other hand, we're moving forward with having Adam(var_list) or something like that, which should be able to fix this problem
Great! Hopefully things will improve over time. Thx for reply.
		</comment>
		<comment id='16' author='svobora' date='2020-03-31T01:17:05Z'>
		
why would you use an optimizer without minimizing anything?
How did you infer that I do not minimize anything? I posted a toy example to reproduce the issue, not my company's code, which was affected and I had to find the issue and fix it.
If you minimize a list of variables, it will create the hyper parameters as tf.Variables after it, so the saving and loading would work.
On the other hand, we're moving forward with having Adam(var_list) or something like that, which should be able to fix this problem
Great! Hopefully things will improve over time. Thx for reply.

		</comment>
		<comment id='17' author='svobora' date='2020-03-31T06:37:45Z'>
		If you minimize a list of variables, it will create the hyper parameters as tf.Variables after it, so the saving and loading would work.
Could you point me to a documentation that specifies you cannot call Saver before minimizing a set of variables?
		</comment>
		<comment id='18' author='svobora' date='2020-03-31T08:21:12Z'>
		
If you minimize a list of variables, it will create the hyper parameters as tf.Variables after it, so the saving and loading would work.
Could you point me to a documentation that specifies you cannot call Saver before minimizing a set of variables?

What I mean is it will serve as a workaround for you before this is fixed in nightly or 2.3
		</comment>
		<comment id='19' author='svobora' date='2020-03-31T08:50:40Z'>
		OK, thanks!
		</comment>
	</comments>
</bug>