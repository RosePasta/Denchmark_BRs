<bug id='14894' author='uptodiff' open_date='2017-11-27T01:06:56Z' closed_time='2017-11-29T18:49:20Z'>
	<summary>'No gradients provided for any variable, check your graph for ops that do not support gradients'</summary>
	<description>
I write the code like following
&lt;denchmark-code&gt;import tensorflow as tf
input1=tf.Variable([1.0,2.0,3.0,4.0,5.0,6.0],name='input1')
input2=tf.Variable([2.0,3.0,4.0,6.0,8.0,9.0],name='input2')
values_range = tf.constant([0., 10.], dtype = tf.float32)
source_hist = tf.histogram_fixed_width(tf.to_float(input1), values_range, 11)
template_hist = tf.histogram_fixed_width(tf.to_float(input2), values_range, 11)
source_hist=tf.cast(source_hist,tf.float32)
template_hist=tf.cast(template_hist,tf.float32)
loss=2*tf.nn.l2_loss(source_hist-template_hist)
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    writer=tf.summary.FileWriter('./',sess.graph)
    train_step=tf.train.AdamOptimizer(0.001).minimize(loss)
    for i in range(0,10000,1):
        sess.run(train_step)
        print('input1_value',input1.eval())
        print('input2_value',input2.eval())
    writer.close()
&lt;/denchmark-code&gt;

Tensorflow throws an error and shows
''ValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables ["&lt;tf.Variable 'input1:0' shape=(6,) dtype=float32_ref&gt;", "&lt;tf.Variable 'input2:0' shape=(6,) dtype=float32_ref&gt;"] and loss Tensor("mul:0", shape=(), dtype=float32).''
I know the op  tf.histogram_fixed_width doesn't support backpropagation and is not differentiable. While
the op tf.floor has the same attribute as  tf.histogram_fixed_width. And the code below can run  without any error which surprises me a lot.
&lt;denchmark-code&gt;import tensorflow as tf
cst=tf.constant([1.2,1.4,2.8,4.6,6.8], dtype=tf.float32)
input=tf.Variable(cst)
new=tf.floor(input)
loss=2*tf.nn.l2_loss(input-new)
train_step=tf.train.AdamOptimizer(0.001).minimize(loss)
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(0,10000,1):
        sess.run(train_step)
        print('input_value',input.eval())
        print('new_value',new.eval())
&lt;/denchmark-code&gt;

I could not figure it out for days, please help
	</description>
	<comments>
		<comment id='1' author='uptodiff' date='2017-11-28T17:51:37Z'>
		I have no idea what you are trying to accomplish.
Your loss is defined in terms of histogram_fixed_width, so your loss is not differentiable in terms of the variables, so you can't optimize this.  You seem to understand that based on "I know the tf.histogram_Fixed_width"...
Perhaps you want to try defining a gradient for histogram_fixed_width that is [None] like Floor has
See e.g. &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/897&gt;#897&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='uptodiff' date='2017-11-29T01:47:46Z'>
		Thanks so much for your reply. In fact my task is to implement the histogram loss mentioned in the paper &lt;denchmark-link:https://arxiv.org/abs/1701.08893&gt;Stable and Controllable Neural Texture Synthesis and Style Transfer Using Histogram Losses&lt;/denchmark-link&gt;
, which involves histogram match technique in the process of computing the loss function. According to the paper we can regard the data flows through the ops that are not differentiable as constants.
While I do not find the op Histogram_fixed_width in  'python/ops/math_grad.py' which mentioned in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/897&gt;#897&lt;/denchmark-link&gt;
. So I do not konw where I can  add the function below.
&lt;denchmark-code&gt;@ops.RegisterGradient("Histogram_fixed_width")
def _Histogram_fixed_widthGrad(_, grad):
    return [None] 
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='uptodiff' date='2017-11-29T18:49:20Z'>
		You can edit the source code and recompile tensorflow yourself, or else you can use the gradient override map
&lt;denchmark-link:https://uoguelph-mlrg.github.io/tensorflow_gradients/&gt;https://uoguelph-mlrg.github.io/tensorflow_gradients/&lt;/denchmark-link&gt;

I would recommend asking this question in StackOverflow as others might have implemented this in tensorflow already.
		</comment>
		<comment id='4' author='uptodiff' date='2017-11-30T08:03:27Z'>
		Thanks for your adviceÔºÅ
		</comment>
		<comment id='5' author='uptodiff' date='2018-03-28T23:36:03Z'>
		&lt;denchmark-link:https://github.com/uptodiff&gt;@uptodiff&lt;/denchmark-link&gt;
 were you able to find a solution? Currently facing this same issue.
		</comment>
		<comment id='6' author='uptodiff' date='2019-07-12T12:44:22Z'>
		&lt;denchmark-link:https://github.com/axelsly&gt;@axelsly&lt;/denchmark-link&gt;
 Did you find an answer? I'm also having the same issue.
		</comment>
	</comments>
</bug>