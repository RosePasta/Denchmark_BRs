<bug id='31998' author='David-Mao' open_date='2019-08-27T07:39:01Z' closed_time='2020-04-03T13:01:36Z'>
	<summary>[TF 2.0] GRU layer doesn't work when called from tf.function</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Darwin Kernel Version 18.6.0
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
TensorFlow installed from (source or binary):
binary
TensorFlow version (use command below):
2.0.0-dev20190826
Python version:
Python 3.6.8 :: Anaconda, Inc.
Bazel version (if compiling from source):
N/A
GCC/Compiler version (if compiling from source):
N/A
CUDA/cuDNN version:
N/A
GPU model and memory:
N/A

Describe the current behavior
The code below raises error:

Tried to convert 'tensor' to a tensor and failed. Error: None values not supported.

As noted in comments, the bug disappears if we don't use tf.function, or set persistent=False in gradient tape.
Code to reproduce the issue
&lt;denchmark-code&gt;import numpy as np
import tensorflow as tf

x = tf.cast(np.random.randn(1, 100), tf.float32)
y = tf.cast(np.random.randn(1, 100, 100), tf.float32)
z = tf.cast(np.random.randn(1, 100), tf.float32)

class Model(tf.keras.Model):
    def __init__(self):
        super(Model, self).__init__()
        self.layer = tf.keras.layers.GRU(100)

    @tf.function # remove this and it works fine
    def call(self, x, y):
        z = self.layer(y, initial_state=x)
        return z

model = Model()

with tf.GradientTape(persistent=True) as tape: # if persistent=False it works fine
    loss = tf.norm(model(x, y) - z)
grads = tape.gradient(loss, model.trainable_variables)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='David-Mao' date='2019-08-28T06:33:21Z'>
		Issue replicating for TF version 2.0, please find the &lt;denchmark-link:https://colab.research.google.com/drive/1wvCV4_fGL9DHqyyVrLreTStmk0tqGcqb&gt;Gist&lt;/denchmark-link&gt;
 of Colab.Thanks!
		</comment>
		<comment id='2' author='David-Mao' date='2019-09-12T17:14:09Z'>
		This looks like a bug in GradientTape, though it's unclear whether it's specific to GRU layer.
		</comment>
		<comment id='3' author='David-Mao' date='2019-09-26T16:43:30Z'>
		Saurabh, looks WhileGrad / TensorList related?
		</comment>
		<comment id='4' author='David-Mao' date='2019-09-26T17:17:14Z'>
		Yeah this seems to be failing when building the gradient of TensorListPopBack. It seems like with persistent=True this takes the path that outputs all necessary intermediates to support higher-order derivatives which triggers taking the derivative of TensorListPopBack. That path is not taken with persistent=False iiuc. I will look into a fix.
		</comment>
		<comment id='5' author='David-Mao' date='2020-03-27T20:03:21Z'>
		&lt;denchmark-link:https://github.com/David-Mao&gt;@David-Mao&lt;/denchmark-link&gt;
,
I was able to reproduce the error with TF v2.0. However, the issue seems to be fixed with &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/7033da4af6575764dcda2cf9b10ac6ed/2-1-template.ipynb&gt;TF v2.1&lt;/denchmark-link&gt;
. Please find the attached gist. Thanks!
		</comment>
		<comment id='6' author='David-Mao' date='2020-04-03T12:41:01Z'>
		
@David-Mao,
I was able to reproduce the error with TF v2.0. However, the issue seems to be fixed with TF v2.1. Please find the attached gist. Thanks!

Any updates regarding this issue? Thanks!
		</comment>
		<comment id='7' author='David-Mao' date='2020-04-03T13:01:36Z'>
		Sorry I don't understand. Should we close this since this seems to be fixed in 2.1? Are you asking if this will be fixed in 2.0? I don't think we would want to do that now since the fix is already available in a public release.
		</comment>
		<comment id='8' author='David-Mao' date='2020-04-03T13:01:38Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31998&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31998&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='David-Mao' date='2020-06-03T13:49:25Z'>
		This problem still exists in TF2.1/2.2, the strangest thing is that the GRU is OK now but the LSTM is not. Does anybody have a solution?
import numpy as np
import tensorflow as tf
from tensorflow.keras import Model as M
from tensorflow.keras import Input as I

x = tf.cast(np.random.randn(1, 100, 100), tf.float32)
y = tf.cast(np.random.randn(1, 100), tf.float32)
z = tf.cast(np.random.randn(1, 100), tf.float32)
u = tf.cast(np.random.randn(1, 100), tf.float32)

class Model(M):
    def __init__(self):
        super().__init__()
        self.layer = tf.keras.layers.GRU(100)
        self(I(shape=(None, 100)), I(shape=(100,)))

    @tf.function # remove this and it works fine
    def call(self, x, y):
        z = self.layer(x, initial_state=y)
        return z


model = Model()

with tf.GradientTape(persistent=True) as tape: # if persistent=False it works fine
    loss = tf.norm(model(x, y) - z)
grads = tape.gradient(loss, model.trainable_variables)

print("###############SUCCESS################")

class Model2(M):
    def __init__(self):
        super().__init__()
        self.layer = tf.keras.layers.LSTM(100)
        self(I(shape=(None, 100)), I(shape=(100,)), I(shape=(100,)))

    @tf.function     # remove this and it works fine
    def call(self, s, h, c):
        z = self.layer(s, initial_state=(h, c))
        return z


model2 = Model2()

with tf.GradientTape(persistent=True) as tape: # if persistent=False it works fine
    loss = tf.norm(model2(x, y, u) - z)
grads = tape.gradient(loss, model2.trainable_variables)


print("###############SUCCESS################")
Still get this error:
ValueError: Tried to convert 'tensor' to a tensor and failed. Error: None values not supported.
&lt;denchmark-link:https://github.com/BlueFisher&gt;@BlueFisher&lt;/denchmark-link&gt;

Finally, construct LSTM in this way works fine:
self.layer = tf.keras.layers.RNN(tf.keras.layers.LSTMCell(100))
But, still unclear why this error happens, is there anything wrong with CuDNN when using persistent=True?
		</comment>
		<comment id='10' author='David-Mao' date='2020-06-04T05:26:29Z'>
		&lt;denchmark-link:https://github.com/StepNeverStop&gt;@StepNeverStop&lt;/denchmark-link&gt;
,
Could you please submit a new issue from &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/new/choose&gt;this link&lt;/denchmark-link&gt;
, so that we can track the issue there. Thanks!
		</comment>
		<comment id='11' author='David-Mao' date='2020-06-04T10:57:39Z'>
		BTW, I checked the code from &lt;denchmark-link:https://github.com/StepNeverStop&gt;@StepNeverStop&lt;/denchmark-link&gt;
 with tf-nightly and the issue doesn't repro, so it might be that the fix landed in 2.3, instead of 2.2. It would be worth double checking.
		</comment>
		<comment id='12' author='David-Mao' date='2020-06-04T14:50:10Z'>
		&lt;denchmark-link:https://github.com/amahendrakar&gt;@amahendrakar&lt;/denchmark-link&gt;
 ,
Since &lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
 has verified that the code works with tf-nightly, I don't think it's necessary to open a new issue to track this problem.
If this error still exists in tf2.3, I'll back to the community for help.
thx.
		</comment>
	</comments>
</bug>