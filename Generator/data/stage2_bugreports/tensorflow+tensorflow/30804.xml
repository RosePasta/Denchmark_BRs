<bug id='30804' author='olesalscheider' open_date='2019-07-17T13:49:49Z' closed_time='2019-08-08T21:22:59Z'>
	<summary>tf.keras.layers.Conv2D fails because it captures tensor from inner function</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): v1.12.1-6461-gc6352706d6 1.14.0
Python version: 3.6.7
Bazel version (if compiling from source): 0.26.1
GCC/Compiler version (if compiling from source): 7.4.0
CUDA/cuDNN version: 10.0 / 7.5
GPU model and memory: Nvidia Geforce GTX 1080 Ti, 11 GB

Describe the current behavior
Since commit 546308e322a6b95542ba9f3cbb14136128aaad1e a tf.keras.layers.Conv2D in my training code fails with the following error:
&lt;denchmark-code&gt;Traceback (most recent call last):                                                                                                                                        
  File "./train.py", line 351, in &lt;module&gt;                                                                                                   
    total_loss = train_step()                                                                                                                    
  File "/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", line 417, in __call__                   
    self._initialize(args, kwds, add_initializers_to=initializer_map)                                                                                
  File "/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", line 360, in _initialize                  
    *args, **kwds))                                                                                                                                  
  File "/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 1709, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)                                                                            
  File "/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 2013, in _maybe_define_function             
    graph_function = self._create_graph_function(args, kwargs)                                                                                     
  File "/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 1899, in _create_graph_function            
    capture_by_value=self._capture_by_value),                                                                                                    
  File "/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py", line 795, in func_graph_from_py_func      
    func_outputs = python_func(*func_args, **func_kwargs)                                                                                             
  File "/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", line 310, in wrapped_fn                      
    return weak_wrapped_fn().__wrapped__(*args, **kwds)                                                                                               
  File "/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py", line 785, in wrapper                       
    raise e.ag_error_metadata.to_exception(type(e))                                                                                                   
ValueError: in converted code:                                                                                                                        
                                                                                                                                                      
    ./train.py:328 train_step  *                                                                                                                      
        per_replica_losses = distribution_strategy.extended.call_for_each_replica(joint_train_step_fn, args=())                                  
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py:1776 call_for_each_replica                                                                                                                                                        
        return self._call_for_each_replica(fn, args, kwargs)                                                                                                        
    /home/salscheider/deeplearning/nnad_playground/model/Resnet.py:173 call  *                                                                   
        x = self.module_3a(x, train_batch_norm=train_batch_norm)                                                                                   
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:713 __call__                               
        outputs = call_fn(inputs, *args, **kwargs)                                                                                                 
    /home/salscheider/deeplearning/nnad_playground/model/Resnet.py:97 call  *                                                                      
        x = self.conv2(x)                                                                                                                        
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:713 __call__                                
        outputs = call_fn(inputs, *args, **kwargs)                                                                                                  
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/convolutional.py:198 call                          
        outputs = self._convolution_op(inputs, self.kernel)                                                                                                                                                                                                                                 
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_ops.py:1078 __call__                                            
        return self.conv_op(inp, filter)                                                                         
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_ops.py:634 __call__                 
        return self.call(inp, filter)                                                                                                                                                                                                                        
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_ops.py:610 _with_space_to_batch_call                         
        block_shape=self.dilation_rate)                                                                                                         
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py:3084 required_space_to_batch_paddings              
        pad_start = base_paddings[:, 0]                                                                                                                                                                                                                      
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py:694 _slice_helper                                                                                                                                              
        name=name)                                                                                                                                                                                                                                           
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py:860 strided_slice                                                                                                                                              
        shrink_axis_mask=shrink_axis_mask)                                                                                                                                                                                                                   
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_array_ops.py:10397 strided_slice                                                                                                                                        
        shrink_axis_mask=shrink_axis_mask, name=name)
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py:793 _apply_op_helper
        op_def=op_def)
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py:528 create_op
        inp = self.capture(inp)
    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py:589 capture
        % (tensor, tensor.graph, self))

    ValueError: Trying to capture a tensor from an inner function. This can be caused by accessing a tensor defined inside a loop or conditional body, or a subfunction, from a calling function, without going through the proper return value mechanism. Consider using TensorFlow mechanisms such as TensorArrays to retur
n tensors from inner functions or loop / conditional bodies. Tensor: Tensor("conv2/stack:0", shape=(2, 2), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0); tensor graph: FuncGraph(name=build_graph, id=140610256740760); this graph: FuncGraph(name=train_step, id=140614894336264)
&lt;/denchmark-code&gt;

This error only occurs with dilation_rate set to 2. With dilation_rate == 1 the training script runs.
It also works if I revert 546308e322a6b95542ba9f3cbb14136128aaad1e.
Describe the expected behavior
The training runs with dilation_rate == 2.
Code to reproduce the issue
So far I could not come up with a small testcase to reproduce the issue. I will continue to try, but until then I only have the backtrace and the problematic commit.
	</description>
	<comments>
		<comment id='1' author='olesalscheider' date='2019-07-18T10:47:18Z'>
		&lt;denchmark-link:https://github.com/olesalscheider&gt;@olesalscheider&lt;/denchmark-link&gt;
 ,
In order to expedite the trouble-shooting process, please provide complete code snippet to reproduce the issue reported here. Thanks!
		</comment>
		<comment id='2' author='olesalscheider' date='2019-07-18T13:03:30Z'>
		You can use this as a testcase:
&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np

def learning_rate_fn():
    return tf.constant(1e-4)

opt = tf.keras.optimizers.Adam(learning_rate_fn, epsilon=1e-3, amsgrad=True)

def get_data():
    return np.random.random([1, 512, 1024, 3]).astype(np.float32)

backbone = tf.keras.layers.Conv2D(16, (3, 3),
            dilation_rate=(2, 2),
            kernel_initializer=tf.keras.initializers.he_normal())

@tf.function
def train_step():
    with tf.GradientTape(persistent=True) as tape:
        img = tf.py_function(get_data, [], tf.float32)
        img.set_shape([1, None, None, 3])
        feature_map = backbone(img)
        img2 = tf.random.uniform([1, 1024, 1024, 3])
        feature_map2 = backbone(img)
        losses = tf.reduce_sum(feature_map) + tf.reduce_sum(feature_map2)
    vs = backbone.trainable_variables
    gs = tape.gradient(losses, vs)
    opt.apply_gradients(zip(gs, vs))

train_step()
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='olesalscheider' date='2019-07-19T09:04:18Z'>
		&lt;denchmark-link:https://github.com/olesalscheider&gt;@olesalscheider&lt;/denchmark-link&gt;
 ,
When i executed the given code with dilation_rate == 2, i got the error . Can you confirm if the same error is faced. Thanks!
		</comment>
		<comment id='4' author='olesalscheider' date='2019-07-19T10:19:37Z'>
		No, I get this error with the testcase:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "./train.py", line 31, in &lt;module&gt;
    train_step()
  File "/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", line 429, in __call__
    return self._stateless_fn(*args, **kwds)
  File "/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 1684, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File "/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 645, in _filtered_call
    self.captured_inputs)
  File "/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 755, in _call_flat
    outputs = self._inference_function.call(ctx, args)
  File "/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 469, in call
    ctx=ctx)
  File "/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py", line 76, in quick_execute
    raise e
  File "/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py", line 61, in quick_execute
    num_outputs)
TypeError: An op outside of the function building code is being passed
a "Graph" tensor. It is possible to have Graph tensors
leak out of the function building context by including a
tf.init_scope in your function building code.
For example, the following function will fail:
  @tf.function
  def has_init_scope():
    my_constant = tf.constant(1.)
    with tf.init_scope():
      added = my_constant * 2
The graph tensor has name: conv2d/dilation_rate:0
&lt;/denchmark-code&gt;

Which version of tensorflow do you use? I used the current master branch.
		</comment>
		<comment id='5' author='olesalscheider' date='2019-07-25T17:31:20Z'>
		I was able to reproduce the issue with . Here is the &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/9fbcb2e8c3b7bb622aeb61455dfff6cb/untitled324.ipynb&gt;gist&lt;/denchmark-link&gt;
. Thanks!
Here is the &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/a2d5481c9d0f4393de6ad872824aa974/tf_30804.ipynb&gt;gist&lt;/denchmark-link&gt;
 with  and the error is different . Thanks!
		</comment>
		<comment id='6' author='olesalscheider' date='2019-08-08T21:22:59Z'>
		&lt;denchmark-link:https://github.com/olesalscheider&gt;@olesalscheider&lt;/denchmark-link&gt;
 this issue should be fixed in tf-nightly. Will close it for now. Feel free to re-open it if the issue appears again.
		</comment>
		<comment id='7' author='olesalscheider' date='2019-08-08T21:23:00Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=30804&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=30804&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>