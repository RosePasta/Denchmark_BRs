<bug id='26763' author='tlkh' open_date='2019-03-15T17:54:13Z' closed_time='2019-05-03T00:50:24Z'>
	<summary>[TF2.0] Embedding batch_input_shape not aware of distribute.MirroredStrategy()</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes, modified an example from Seedbank to use with TF2.0
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): pip binary
TensorFlow version (use command below):  2.0.0-alpha0; git v1.12.0-9492-g2c319fb415
Python version: 3.6
Bazel version (if compiling from source): NIL
GCC/Compiler version (if compiling from source): NIL
CUDA/cuDNN version: 10.0
GPU model and memory: V100 16GB

Describe the current behavior

when batch_input_shape is not specified in tf.keras.layers.Embedding
With or without distribute.MirroredStrategy(), model works perfectly fine
when batch_input_shape is specified in tf.keras.layers.Embedding
Without distribute.MirroredStrategy(), model works perfectly fine, But with distribute.MirroredStrategy(), tf.data.Dataset splits the inputs to the model correctly, but the model's expected input is not correct


The model replicas each expect batchsize_per_replica * replica (the un-split output from Dataset) instead of  batchsize_per_replica (split output from Dataset).
If the model's batch_input_shape or Dataset output is adjusted to match the above expectation, the keras Model immediately errors out as it expects batchsize_per_replica * replica, split to batchsize_per_replica as an input to each model replica.

"Illustrated Example":
&lt;denchmark-code&gt;&gt; batchsize = 4*128
&gt; batchsize_per_replica = 128
&gt; model batch_input_shape = 4*128
&gt; model replica expects 4*128 causing error

&gt; batchsize = 4*128
&gt; batchsize_per_replica = 128
&gt; model batch_input_shape = 128
&gt; model expects 4*128 causing error
&lt;/denchmark-code&gt;

It seems like the problem is everywhere else, the batch size etc. is calculated correctly, except when batch_input_shape is specified in tf.keras.layers.Embedding. If anyone is wondering why we need to specify this, this is for stateful RNNs to work.
Describe the expected behavior
When the scope is distribute.MirroredStrategy(), tf.keras.layers.Embedding specified batch_input_shape should also be divided by the number of replicas.
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
The notebook below contains code that will reproduce the error on a multi-GPU system. On Colab, there is only one GPU hence it runs fine, since distribute.MirroredStrategy() only creates one replica. However, with two or more replicas on a multi-GPU system, the error is observed.
&lt;denchmark-link:https://colab.research.google.com/drive/1R3h2Jf9rKCsi952KLcg7b8PtPyyXgx6b&gt;Notebook presented on Google Colab&lt;/denchmark-link&gt;

There is a header/section that shows the model training with and without distribute.MirroredStrategy().
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Following the Colab notebook above, when training is run on a 4 GPU system (overall batch_size=128, batchsize_per_replica=32), the error is:
&lt;denchmark-code&gt;Invalid input_h shape: [1,128,256] [1,32,256]
	 [[{{node replica_3/unified_lstm_2/CudnnRNN}}]]
	 [[replica_1/loss/dense_1_loss/loss/weighted_loss/broadcast_weights/assert_broadcastable/is_valid_shape/else/_48/has_valid_nonscalar_shape/then/_207/has_invalid_dims/concat/_374]] [Op:__inference_keras_scratch_graph_10991]
&lt;/denchmark-code&gt;

Notebook that shows the entire run resulting in the above error can be seen &lt;denchmark-link:https://nbviewer.jupyter.org/github/tlkh/arxiv-lm/blob/master/tf_distributed_embedding_bugreport.ipynb&gt;here&lt;/denchmark-link&gt;

If you modify batch_input_shape to match for batchsize_per_replica (32):
&lt;denchmark-code&gt;ValueError: The batch output shape of your `Dataset` is 128, which is incompatible with the specified batch size of your Input Layer: 32
&lt;/denchmark-code&gt;

In all cases, model.summary() gives the same result:
&lt;denchmark-code&gt;Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (128, None, 128)          8320      
_________________________________________________________________
unified_lstm_2 (UnifiedLSTM) (128, None, 256)          394240    
_________________________________________________________________
unified_lstm_3 (UnifiedLSTM) (128, None, 256)          525312    
_________________________________________________________________
dense_1 (Dense)              (128, None, 65)           16705     
=================================================================
Total params: 944,577
Trainable params: 944,577
Non-trainable params: 0
&lt;/denchmark-code&gt;

, you can &lt;denchmark-link:https://colab.research.google.com/drive/1R3h2Jf9rKCsi952KLcg7b8PtPyyXgx6b&gt;view Colab demo&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='tlkh' date='2019-03-15T18:06:38Z'>
		&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/26245&gt;#26245&lt;/denchmark-link&gt;
 shows a similar error, but somewhat different in nature

That one is with Keras multi_gpu_model (I believe this is not recommended any more with TF2.0?)
That one gives an error even when batch_input_shape is not specified, whereas in my case it works fine

		</comment>
		<comment id='2' author='tlkh' date='2019-03-16T02:49:26Z'>
		&lt;denchmark-link:https://github.com/tlkh&gt;@tlkh&lt;/denchmark-link&gt;
 why is  not recommended with TF 2.0 any longer?
		</comment>
		<comment id='3' author='tlkh' date='2019-03-16T03:41:23Z'>
		&lt;denchmark-link:https://github.com/ghego&gt;@ghego&lt;/denchmark-link&gt;
 With TF 2.0, tf.Keras works out of the box with . My own experience is that it is much nicer to use. Check out: &lt;denchmark-link:https://www.tensorflow.org/alpha/tutorials/distribute/keras&gt;https://www.tensorflow.org/alpha/tutorials/distribute/keras&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='tlkh' date='2019-03-16T04:19:50Z'>
		yep, I've seen that. Thanks
		</comment>
		<comment id='5' author='tlkh' date='2019-03-19T22:34:15Z'>
		Assigning to Priya since this seems to be related with distribution strategy. Please reassign to someone who owns the embedding if the fix should be on that side.
		</comment>
		<comment id='6' author='tlkh' date='2019-05-03T00:50:24Z'>
		It should be fixed after &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/a3f9c59153203020dcd008527db5247deefad95a&gt;a3f9c59&lt;/denchmark-link&gt;
.  Could you please try and re-open if that doesn't help?
		</comment>
		<comment id='7' author='tlkh' date='2019-05-03T00:50:25Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=26763&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=26763&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>