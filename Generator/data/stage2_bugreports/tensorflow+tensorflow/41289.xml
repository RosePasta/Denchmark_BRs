<bug id='41289' author='linkzero177' open_date='2020-07-10T18:39:29Z' closed_time='2020-07-13T22:30:54Z'>
	<summary>Expecting float value for attr dropout, got int  in gen_cudnn_rnn_ops.py</summary>
	<description>
I was trying to build a chatbot on Google Colab. The training model worked fine. However, when I tried to run the inference model, I got the following error.
&lt;denchmark-code&gt;_FallbackException                        Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_cudnn_rnn_ops.py in cudnn_rnnv3(input, input_h, input_c, params, sequence_lengths, rnn_mode, input_mode, direction, dropout, seed, seed2, num_proj, is_training, time_major, name)
   1895         "num_proj", num_proj, "is_training", is_training, "time_major",
-&gt; 1896         time_major)
   1897       _result = _CudnnRNNV3Output._make(_result)

_FallbackException: Expecting float value for attr dropout, got int

During handling of the above exception, another exception occurred:

UnknownError                              Traceback (most recent call last)

&lt;/denchmark-code&gt;

Here's the google colab link
&lt;denchmark-link:https://drive.google.com/file/d/17qBe-pN68_HeGNAEyobDW-Ph34EEjsO1/view?usp=sharing&gt;https://drive.google.com/file/d/17qBe-pN68_HeGNAEyobDW-Ph34EEjsO1/view?usp=sharing&lt;/denchmark-link&gt;

It follow the script that raised the Exception and it seems to me that the error was from inside the script. I tried to fix it by setting my LSTM layer dropout parameter but it still didn't work.
	</description>
	<comments>
		<comment id='1' author='linkzero177' date='2020-07-13T09:34:59Z'>
		Was able to reproduce the issue with &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/6dab2daa22ec3dd90ed0696516d8b78b/41289.ipynb#scrollTo=Tlfnzc02e2Qg&gt;TF v2.2&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/0d04b97f7e49484112b71abcd33609f3/41289-2-3.ipynb&gt;TF v2.3.0rc1&lt;/denchmark-link&gt;
. Session crashes on running with TF-nightly. Please find the attached gist. Thanks!
		</comment>
		<comment id='2' author='linkzero177' date='2020-07-13T21:06:17Z'>
		I think part of this issue is because of existing &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/33148&gt;#33148&lt;/denchmark-link&gt;
. In short, the cudnn kernel will fail if the inputs contains fully masked inputs. Note that in your colab, the decoder input is initialized to zeros, and embedding layer also created with mask_zeros = True.
You could walk around the issue by turning off the mask_zeros in the decoder embedding layer, since you are doing one word pre step inference anyway.
On the other hand, I think there is some bug in eager runtime, that incorrectly create python var with wrong types. Note that the dropout passed to cudnn kernel is different from the one in LSTM layer. The code fallback to the eager runtime backoff path when it hit the error of fully masked input in cudnn kernel, and then raise the error with incompatible type.
		</comment>
		<comment id='3' author='linkzero177' date='2020-07-13T22:30:49Z'>
		Thank you, that solved the error!
		</comment>
		<comment id='4' author='linkzero177' date='2020-07-13T22:30:56Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41289&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41289&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>