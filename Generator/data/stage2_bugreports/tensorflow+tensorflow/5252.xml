<bug id='5252' author='MInner' open_date='2016-10-28T03:55:02Z' closed_time='2017-06-16T18:07:10Z'>
	<summary>multigpu gradient averaging if grad_op is IndexedSlices; race condition in apply_gradients?</summary>
	<description>
If one follows the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py&gt;cifar10_multi_gpu_train.py&lt;/denchmark-link&gt;
 example (build two towers that share weights, feed data independently, collect gradients, average them, apply then) and passes gradients to the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py#L110&gt;average_gradients()&lt;/denchmark-link&gt;
 function, but graph uses  consequently, the resulting gradient op will be  that has many  values in  field (from my experience, not sure about why this happens), so when tensorflow will attempt to apply  to convert it to Tensor (to apply  on it), it will fail on &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients.py#L92&gt;unsorted_segment_sum&lt;/denchmark-link&gt;
 , because s are indeed not in the indexes range. I have explicitly printed  and they looked like:
&lt;denchmark-code&gt;[0 1 2 3 6 4 5 6 7 8 9 13 12 11 15 16 ... 230 231 -1 -1 ... -1 280 -1 -1 265 -1 -1 ... -1]
&lt;/denchmark-code&gt;

- so the  indeed fails on these examples (people have been reporting similar problems &lt;denchmark-link:http://stackoverflow.com/questions/37074077/fail-to-average-indexedslices-on-porting-ptb-word-lm-py-to-multi-gpu-towers&gt;[1]&lt;/denchmark-link&gt;
, &lt;denchmark-link:http://stackoverflow.com/questions/39017896/tensorflow-how-to-average-several-indexedslicesvalue&gt;[2]&lt;/denchmark-link&gt;
 .. 2-3 more around the web).
One way of dealing with (solution 1) it is just applying gradients consequentially (with lower learning rate) instead of averaging them first:
&lt;denchmark-code&gt; train_op = tf.group(*[gd.apply_gradients(grad) for grad in tower_grads])
&lt;/denchmark-code&gt;

-  this works great and scales nicely, but (in my case) led to some weird convergence issues (eventually leading to nans popping here and there), possibly because of some sort of race conditions (?), while applying gradients (gd.apply_gradients() is probably not very atomic). One possible workaround (solution 2) might be to use tf.control_dependencies:
&lt;denchmark-code&gt;def rigid_op_sequence(op_lambdas):
    ## equivalent to:
    ## op = op1()
    ## with.control_dependencies([op]):
    ##    op = op2()
    ##    with.control_dependencies([op]):
    ##        op = op3()
    ##        ...
    ## return op

    with ExitStack() as stack:
        for op_func in op_lambdas:
            op = op_func()
            context = tf.control_dependencies([op])
            stack.enter_context(context)
    return op

grad_update_lambdas = [(lambda: gd.apply_gradients(grad)) for grad in tower_grads]
train_op = rigid_op_sequence(grad_update_lambdas)
&lt;/denchmark-code&gt;

however this solution seems to somewhat significantly degenerate performance, because of that "blocking" (GPU load dropped from ~90% to ~60%).
Several fundamental questions:

What is officially recommended way of dealing with this "multigpu IndexedSlices" case? (could not find any; people keep asking)
Does (solution 1) indeed might experience race condition or tensorflow handles it somehow and my convergences issues were not related to that? (and we all can just use solution 1)

	</description>
	<comments>
		<comment id='1' author='MInner' date='2016-10-28T17:52:20Z'>
		&lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;
 who might have better insight
		</comment>
		<comment id='2' author='MInner' date='2016-10-31T20:38:15Z'>
		&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
, could you comment on this?
		</comment>
		<comment id='3' author='MInner' date='2016-11-01T18:50:04Z'>
		UPD: after little more testing on relatively large language model, it turned out that even though nvidia-smi shows lower gpu utilization when I'm using "blocking" solution (2), it scales great (on 2 gpus the speedup is 1.83x) and performs actually even better (10% faster) then non-blocking one (1).
		</comment>
		<comment id='4' author='MInner' date='2016-11-01T19:10:18Z'>
		&lt;denchmark-link:https://github.com/MInner&gt;@MInner&lt;/denchmark-link&gt;
 having -1s in the IndexedSlices object output of gather sounds like a bug.  do you know if gather() is being called with -1s for the indices input?
		</comment>
		<comment id='5' author='MInner' date='2017-06-16T18:07:10Z'>
		Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
		<comment id='6' author='MInner' date='2017-09-01T11:03:14Z'>
		I have the same problem when trying to implement a multigpu version model with embedding layer.
What is officially recommended way of dealing with this "multigpu IndexedSlices" case? (could not find any; people keep asking)
		</comment>
		<comment id='7' author='MInner' date='2018-08-13T09:00:59Z'>
		&lt;denchmark-link:https://github.com/TomorrowIsAnOtherDay&gt;@TomorrowIsAnOtherDay&lt;/denchmark-link&gt;
 ,hi do you have any idea of average multigpu indexedslices?
		</comment>
		<comment id='8' author='MInner' date='2019-02-28T20:47:21Z'>
		I have the same problem, I am wondering if this is solved in new versions of tensorflow? or is there any solution for averaging IndexedSlices.
		</comment>
		<comment id='9' author='MInner' date='2019-02-28T23:29:11Z'>
		&lt;denchmark-link:https://github.com/yuefengz&gt;@yuefengz&lt;/denchmark-link&gt;
 can maybe shed some light on this.
		</comment>
		<comment id='10' author='MInner' date='2019-10-14T10:55:22Z'>
		Any ideas? want to know the solutions
		</comment>
		<comment id='11' author='MInner' date='2019-12-30T13:33:50Z'>
		tf.distribute.MirroredStrategy handles aggregating IndexedSlices already afaik. How are you doing multi GPU training?
		</comment>
	</comments>
</bug>