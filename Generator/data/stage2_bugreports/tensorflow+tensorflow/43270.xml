<bug id='43270' author='le8888e' open_date='2020-09-16T17:03:01Z' closed_time='2020-09-25T17:15:04Z'>
	<summary>[RNN]Cannot get saved .tflite model</summary>
	<description>
System information

OS Platform and Distribution: Ubuntu 16.04
TensorFlow installed from: binary
TensorFlow version: tf-nightly 2.2.0

Command used to run the converter or code if you’re using the Python API
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/5233664/convert_pb2_tflite.txt&gt;convert_pb2_tflite.txt&lt;/denchmark-link&gt;


&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/5233693/log.txt&gt;log.txt&lt;/denchmark-link&gt;

Failure details
My .pb model is trained and saved in tensorflow 1.15.0 and I want to convert it to .tflite to deploy on CPU for better performance. When I run the uploaded file convert_pb2_tflite.txt(.py actually), the log attached above seems to include no warning and errors. But there is no tflite file in my save path. What's probably going wrong?
Thank you.
	</description>
	<comments>
		<comment id='1' author='le8888e' date='2020-09-16T22:34:44Z'>
		tf.Sign op is currently supported by the TFLite builtin operator set. You can use the Select TF op set for covering tf.Sign op. Please refer to the following guide pages.
&lt;denchmark-link:https://www.tensorflow.org/lite/guide/ops_select&gt;https://www.tensorflow.org/lite/guide/ops_select&lt;/denchmark-link&gt;

&lt;denchmark-link:https://www.tensorflow.org/lite/guide/reduce_binary_size&gt;https://www.tensorflow.org/lite/guide/reduce_binary_size&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='le8888e' date='2020-09-17T01:44:14Z'>
		Hi &lt;denchmark-link:https://github.com/abattery&gt;@abattery&lt;/denchmark-link&gt;
,
Thank you, I successfully convert following your instruction. But the .pb file before converting is 131MB, and converted .tflite file is 73MB, is this normal?
Thank you.
		</comment>
		<comment id='3' author='le8888e' date='2020-09-17T01:50:20Z'>
		TFLite converter can trim the given graph based on input/output tensors for inference use cases. Often it can reduce overall binary size and the reduction rate can be varied depending on the given graph internal structure.
		</comment>
		<comment id='4' author='le8888e' date='2020-09-17T02:20:49Z'>
		Hi &lt;denchmark-link:https://github.com/abattery&gt;@abattery&lt;/denchmark-link&gt;
,
I tried to do inference with .tflite model.
interpreter.allocate_tensors()   gives this error:
'RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. Node number 3 (FlexSign) failed to prepare.'
According to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/40157#issuecomment-680798660&gt;#40157 (comment)&lt;/denchmark-link&gt;
, seems like SELECT_TF_OPS is not supported by tflite_runtime. There is LSTM layer in my model, do I have to reimplement it with keras?&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/40157#issuecomment-691650953&gt;#40157 (comment)&lt;/denchmark-link&gt;

Thank you
		</comment>
		<comment id='5' author='le8888e' date='2020-09-17T02:24:44Z'>
		To run Select TF ops in android, the android application project requires additional dependency on org.tensorflow:tensorflow-lite-select-tf-ops.
&lt;denchmark-link:https://www.tensorflow.org/lite/guide/ops_select#android_aar&gt;https://www.tensorflow.org/lite/guide/ops_select#android_aar&lt;/denchmark-link&gt;

If you want a trimmed aar, you can follow the below page:
&lt;denchmark-link:https://www.tensorflow.org/lite/guide/reduce_binary_size&gt;https://www.tensorflow.org/lite/guide/reduce_binary_size&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='le8888e' date='2020-09-17T02:26:14Z'>
		Hi &lt;denchmark-link:https://github.com/abattery&gt;@abattery&lt;/denchmark-link&gt;
 ，
I'm running on Linux PC.
		</comment>
		<comment id='7' author='le8888e' date='2020-09-17T02:29:24Z'>
		Great. TensorFlow Lite with select TensorFlow ops will be installed automatically with the &lt;denchmark-link:https://www.tensorflow.org/install/pip&gt;TensorFlow pip package&lt;/denchmark-link&gt;
. You can also choose to only install the &lt;denchmark-link:https://www.tensorflow.org/lite/guide/python#install_just_the_tensorflow_lite_interpreter&gt;TensorFlow Lite Interpreter pip package&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='8' author='le8888e' date='2020-09-17T14:09:43Z'>
		Hi &lt;denchmark-link:https://github.com/abattery&gt;@abattery&lt;/denchmark-link&gt;
 ,
Error
'RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. Node number 3 (FlexSign) failed to prepare.'
seems to caused by tf.sign op in my model since I remove that op, the error is gone.
So do I have to implement this op myself or are there other solutions?
Thank you
		</comment>
		<comment id='9' author='le8888e' date='2020-09-17T14:13:17Z'>
		How did you build your TFLite inference code? Are you using the PIP installer or building natively?
There are two ways to enable tf.Sign op by 1) implementing your own custom op 2) linking Flex delegate.
		</comment>
		<comment id='10' author='le8888e' date='2020-09-17T14:15:45Z'>
		Hi &lt;denchmark-link:https://github.com/abattery&gt;@abattery&lt;/denchmark-link&gt;
 ,
I build with pip installer. Could you please offer an instruction on how to Link Flex delegate?
Thank you
		</comment>
		<comment id='11' author='le8888e' date='2020-09-17T14:18:27Z'>
		Flex delegate will be automatically enabled on Linux when you use tf 2.3.0 or later versions only.
		</comment>
		<comment id='12' author='le8888e' date='2020-09-17T14:33:27Z'>
		OK, I will try tf-nightly 2.4.0. Thank you for your help and patience &lt;denchmark-link:https://github.com/abattery&gt;@abattery&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='le8888e' date='2020-09-18T02:47:10Z'>
		Hi, &lt;denchmark-link:https://github.com/abattery&gt;@abattery&lt;/denchmark-link&gt;
,
I update to tf-nightly2.4 try to run inference on .tflitemodel, but  error occurs. Here is the log:
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

INFO: TfLiteFlexDelegate delegate: 11 nodes delegated out of 129 nodes with 3 partitions.
INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.
INFO: TfLiteFlexDelegate delegate: 3 nodes delegated out of 24 nodes with 3 partitions.
INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.
INFO: TfLiteFlexDelegate delegate: 3 nodes delegated out of 24 nodes with 3 partitions.
[{'name': 'input_x', 'index': 0, 'shape': array([ 1, 50], dtype=int32), 'shape_signature': array([-1, 50], dtype=int32), 'dtype': &lt;class 'numpy.int32'&gt;, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
[{'name': 'output/y_hat', 'index': 225, 'shape': array([   1, 2363], dtype=int32), 'shape_signature': array([  -1, 2363], dtype=int32), 'dtype': &lt;class 'numpy.float32'&gt;, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
2020-09-18 18:37:52.699048: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at tensor_array_ops.cc:1035 : Not found: Container __per_step_0 does not exist. (Could not find resource: __per_step_0/_tensor_arraysrnn/bidirectional_rnn/bw/bw/dynamic_rnn/input_0_1)
Traceback (most recent call last):
File "run_tflite.py", line 60, in 
interpreter.invoke()
File "/home/ai/anaconda5/envs/tf-nightly2.4/lib/python3.6/site-packages/tensorflow/lite/python/interpreter.py", line 539, in invoke
self._interpreter.Invoke()
RuntimeError: Container __per_step_0 does not exist. (Could not find resource: __per_step_0/_tensor_arraysrnn/bidirectional_rnn/bw/bw/dynamic_rnn/input_0_1)
(while executing 'TensorArrayScatterV3' via Eager)Node number 129 (TfLiteFlexDelegate) failed to invoke.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Seems like it's the problem with BiLSTM Layer.
The code I define BiLSTM layer is:
from tensorflow.contrib import rnn
with tf.variable_scope("rnn", reuse=reuse):
lstm_fw_cell = rnn.BasicLSTMCell(hidden_size)
lstm_bw_cell = rnn.BasicLSTMCell(hidden_size)
&lt;denchmark-code&gt;if self.dropout_keep_prob is not None:
    lstm_fw_cell = rnn.DropoutWrapper(lstm_fw_cell, output_keep_prob=self.dropout_keep_prob)
    lstm_bw_cell = rnn.DropoutWrapper(lstm_bw_cell, output_keep_prob=self.dropout_keep_prob)

outputs, _ = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, embedded_chars, dtype=tf.float32)
&lt;/denchmark-code&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

But I refer to info in &lt;denchmark-link:https://www.tensorflow.org/lite/convert/rnn&gt;https://www.tensorflow.org/lite/convert/rnn&lt;/denchmark-link&gt;
, bidirectional_dynamic_rnn() is available.
Refer to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/40157#issuecomment-691650953&gt;#40157 (comment)&lt;/denchmark-link&gt;
, if lstm layer was created by keras, this error was solved. So do I have to implement BiLSTM layer in eras?
Thank you
		</comment>
		<comment id='14' author='le8888e' date='2020-09-18T02:59:01Z'>
		Hi &lt;denchmark-link:https://github.com/le8888e&gt;@le8888e&lt;/denchmark-link&gt;

Sorry for encountering the issue.
TensorArrayScatterV3 op is currently not supported by either TensorFlow Lite builtin op set or Flex delegate. Unfortunately, the above TF graph should be rewritten by replacing the TensorArray with a 1-D TensorList through V2 control flow ops or concatenation operators if possible instead.
When you are creating a TF graph, could you invoke the following call before the TF graph creation part?
&lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/compat/v1/enable_control_flow_v2&gt;https://www.tensorflow.org/api_docs/python/tf/compat/v1/enable_control_flow_v2&lt;/denchmark-link&gt;

Best regards,
		</comment>
		<comment id='15' author='le8888e' date='2020-09-18T03:19:40Z'>
		Hi &lt;denchmark-link:https://github.com/abattery&gt;@abattery&lt;/denchmark-link&gt;
 ,
I have tried to invoke 'tf.enable_control_flow_v2()'  before I create my TF graph. But I cannot converted trained .pb to .tflite. Here is the log:
&lt;denchmark-link:https://user-images.githubusercontent.com/34771253/93551389-4bcc8680-f9a0-11ea-8057-9e521f031c30.jpeg&gt;&lt;/denchmark-link&gt;

Without 'tf.enable_control_flow_v2()', I can finish converting but collapse in inference like I mentioned before.
I will try to replace tf.nn.bidirectional_dynamic_rnn() by tf.lite.experimental.nn.bidirectional_dynamic_rnn() and let you know the result.
Thank you
		</comment>
		<comment id='16' author='le8888e' date='2020-09-18T03:55:32Z'>
		Hi &lt;denchmark-link:https://github.com/abattery&gt;@abattery&lt;/denchmark-link&gt;
,
I try to replace tf.contirb.cnn.BasicLSTMCell() with tf.lite.experimental.nn.TfLiteLSTMCell()
and
tf.nn.bidirectional_dynamic_rnn() with tf.lite.experimental.nn.bidirectional_dynamic_rnn()
Then retrain my model. My tf version to train is tf 1.15.0
Error occurs:
AttributeError: module 'tensorflow._api.v1.lite.experimental.nn' has no attribute 'TfLiteLSTMCell'
Seems like tf.lite.experimental  is not supported by tf 1.*
Maybe I have to transfer my code to tf 2.* or keras : )
		</comment>
		<comment id='17' author='le8888e' date='2020-09-21T07:23:22Z'>
		hi &lt;denchmark-link:https://github.com/abattery&gt;@abattery&lt;/denchmark-link&gt;
 ,
After implementing BiLSTM by Keras, I successfully convert and inference .tflite model.
I want to serve .tflite model by tf-serving  on PC  with python api. Is  this feasible and is there any guidance for this?
THANK YOU
		</comment>
		<comment id='18' author='le8888e' date='2020-09-21T22:10:52Z'>
		As you already asked the question to the tf-serving project, let's see their answers at &lt;denchmark-link:https://github.com/tensorflow/serving/issues/1736&gt;tensorflow/serving#1736&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='19' author='le8888e' date='2020-09-22T08:29:46Z'>
		Hi &lt;denchmark-link:https://github.com/abattery&gt;@abattery&lt;/denchmark-link&gt;
 ,
I was building tf-nightly viac pip. Would XNNPACK built by default? And how can I check it?
Thank you
		</comment>
		<comment id='20' author='le8888e' date='2020-09-22T10:08:05Z'>
		&lt;denchmark-link:https://github.com/le8888e&gt;@le8888e&lt;/denchmark-link&gt;

serving requires model asset to be a saved model, please create a new issue for any new questions. If this resolves your issue please move this to closed status.
		</comment>
		<comment id='21' author='le8888e' date='2020-09-22T21:54:43Z'>
		&lt;denchmark-link:https://github.com/multiverse-tf&gt;@multiverse-tf&lt;/denchmark-link&gt;
 Chao, could you answer &lt;denchmark-link:https://github.com/le8888e&gt;@le8888e&lt;/denchmark-link&gt;
 's comment at &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/43270#issuecomment-696582237&gt;#43270 (comment)&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='22' author='le8888e' date='2020-09-24T07:44:57Z'>
		
Hi @abattery ,
I was building tf-nightly viac pip. Would XNNPACK built by default? And how can I check it?

As for "XNNPACK built by default", do you mean by "applying XNNPACK by default" in the interpreter itself? If yes, this hasn't been enabled yet in tf-nightly build. So, to utilize XNNPACK delegate currently, pls continue to follow this guide: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md#using-xnnpack-engine-with-tensorflow-lite-interpreter&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md#using-xnnpack-engine-with-tensorflow-lite-interpreter&lt;/denchmark-link&gt;

		</comment>
		<comment id='23' author='le8888e' date='2020-09-25T07:52:41Z'>
		&lt;denchmark-link:https://github.com/le8888e&gt;@le8888e&lt;/denchmark-link&gt;

Please update, if this issue can be moved to closed status.
		</comment>
		<comment id='24' author='le8888e' date='2020-09-25T15:26:16Z'>
		&lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;
 ,
This issue can be closed, thank you.
		</comment>
		<comment id='25' author='le8888e' date='2020-09-25T17:15:04Z'>
		Moving the issue to closed status with confirmation
		</comment>
	</comments>
</bug>