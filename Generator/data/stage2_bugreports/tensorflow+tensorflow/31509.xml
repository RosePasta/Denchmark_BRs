<bug id='31509' author='oracle3001' open_date='2019-08-10T15:38:53Z' closed_time='2019-09-07T04:20:58Z'>
	<summary>BaseCollectiveExecutor::StartAbort Out of range: warnings when fit model in graph mode (TF 2.0 Nightly)</summary>
	<description>
System information

OS Platform and Distribution : Windows 10
TensorFlow installed from : Binary
TensorFlow version : TF 2.0 Nightly GPU Preview
Python version:3.6
CUDA/cuDNN version: 10.1
GPU model and memory: 960M

I have a very simple model that I have made by inheriting from tf.Keras.model, which I feed with a dataset i,e
model = MyModel(...)
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01, amsgrad=True),
loss=loss_fn,
run_eagerly=False)
dataset = tf.data.Dataset.from_tensor_slices((x, y))
dataset = dataset.shuffle(buffer_size=10000)
dataset = dataset.batch(batch_size=1000)
model.fit(dataset,
epochs=100,
verbose=0,
callbacks=[LossAndErrorPrintingCallback()])
If I run this using TF 2.0 (beta), works perfectly fine i.e with run_eagerly=False. If I run it using TF nightly preview with run_eagerly=True, again fine. However if I try with run_eagerly=False using nightly preview I get a stream of the following warnings,
2019-08-10 16:35:40.168418: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
[[{{node IteratorGetNext}}]]
	</description>
	<comments>
		<comment id='1' author='oracle3001' date='2019-08-13T07:18:32Z'>
		&lt;denchmark-link:https://github.com/oracle3001&gt;@oracle3001&lt;/denchmark-link&gt;
 ,
In order to expedite the trouble-shooting process, can you please provide complete code snippet to reproduce the issue reported here.Thanks!
		</comment>
		<comment id='2' author='oracle3001' date='2019-08-13T13:08:53Z'>
		Here is a minimal model that produces the same issue when using TF 2.0 nightly (but not TF 2.0 beta)...
&lt;denchmark-code&gt;import tensorflow as tf
import tensorflow_graphics as tfg
from tensorflow.python.framework import ops
from tensorflow.python.ops import math_ops
import numpy as np


class MinimalModelTest(tf.keras.Model):

    def __init__(self,
                 control_vert_pos,
                 dtype=tf.float64):
        super(MinimalModelTest, self).__init__(dtype=dtype)
        self.t_y_spherical = tfg.math.math_helpers.cartesian_to_spherical_coordinates(control_vert_pos)
        self.n_control_verts = control_vert_pos.shape[0]
        self.t_y_radius = self.t_y_spherical[:, 0]
        self.av_radius = np.mean(self.t_y_radius.numpy())

        self.t_av_radius = tf.Variable(self.av_radius,
                                       name='t_av_radius',
                                       dtype=self.dtype,
                                       trainable=False)

        self.t_y_polar = tf.Variable(self.t_y_spherical[:, 1:3],
                                     name='t_y_polar',
                                     dtype=self.dtype,
                                     trainable=True)

    def build(self, input_shape):
        super(MinimalModelTest, self).build(input_shape)

    def spherical_to_cartesian_coordinates(self, point_spherical, name=None):
        r, theta, phi = tf.unstack(point_spherical, axis=-1)
        # r = asserts.assert_all_above(r, 0)
        tmp = r * tf.sin(theta)
        x = tmp * tf.cos(phi)
        y = tmp * tf.sin(phi)
        z = r * tf.cos(theta)
        return tf.stack((x, y, z), axis=-1)

    def loss_fn(self, y_true, y_pred):
        y_pred = ops.convert_to_tensor(y_pred)
        y_true = math_ops.cast(y_true, y_pred.dtype)
        return tf.reduce_mean(tf.square(y_true - y_pred))

    def call(self, inputs):
        t_radius = self.t_av_radius * tf.ones([self.n_control_verts, 1], dtype=self.dtype)
        t_cv_sphere_spherical = tf.concat([t_radius, self.t_y_polar], axis=-1)
        return self.spherical_to_cartesian_coordinates(t_cv_sphere_spherical)



n_points = 10
# generate_prototypes produce n 3d points that lie on a unit sphere
rand_sphere_points_start = np.asarray(generate_prototypes(n_points))
rand_sphere_points_target = np.asarray(generate_prototypes(n_points))

joint_sample_model = MinimalModelTest(
        control_vert_pos=rand_sphere_points_start)

optimizer = tf.optimizers.Adam(learning_rate=0.005, amsgrad=True)
joint_sample_model.compile(optimizer=optimizer,
              loss=[joint_sample_model.loss_fn],
              run_eagerly=False)

batch_size = n_points
dataset = tf.data.Dataset.from_tensor_slices((rand_sphere_points_start, rand_sphere_points_target))
dataset = dataset.batch(batch_size=batch_size)

joint_sample_model.fit(dataset,
          epochs=100,
          verbose=1)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='oracle3001' date='2019-08-14T09:06:58Z'>
		&lt;denchmark-link:https://github.com/oracle3001&gt;@oracle3001&lt;/denchmark-link&gt;
 ,
When tried executing the given code, error  was faced.Thanks!
		</comment>
		<comment id='4' author='oracle3001' date='2019-08-14T12:35:59Z'>
		Sorry, yes it is just a function to create random points. Here is the missing functions.
&lt;denchmark-code&gt;def polar2cartesian(theta, phi):
    return (
        np.cos(theta) * np.sin(phi),
        np.sin(theta) * np.sin(phi),
        np.cos(phi)
    )

def generate_prototypes(n):
    """
        ref: "How to generate equidistributed points on the surface of a sphere" by Markus Deserno
        note that in the paper, {phi, theta} correspond to {theta, phi} in the rest of this code
        n is an approximate number. the number of returned points are smaller
    """
    a = 2.0*3.1415926/n
    d = np.sqrt(a)
    mtheta = int(round(0.5*3.1415926/d))
    dtheta = 0.5*3.1415926/mtheta
    dphi = a/dtheta

    lst = []
    for m in range(mtheta - 1):
        theta = 0.5*3.1415926 * (m + 0.5)/mtheta
        mphi = int(round(2.0*3.1415926*np.sin(theta)/dphi))
        for n in range(mphi):
            phi = 2.0*3.1415926*n/mphi
            lst.append(polar2cartesian(phi, theta))
    return lst
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='oracle3001' date='2019-09-07T04:20:58Z'>
		This is fixed with latest version of tf nightly 2.0 build '2.0.0-dev20190906' . Thanks!
Replace  tf.optmizer with tf.keras.optimizer on line 64
optimizer = tf.keras.optimizers.Adam(learning_rate=0.005, amsgrad=True)
		</comment>
		<comment id='6' author='oracle3001' date='2019-09-07T04:20:59Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=31509&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=31509&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='oracle3001' date='2019-10-01T17:49:58Z'>
		This bug seems still present as of 2.0.0 official release (as well as RC1)
I tried just now with the code snippet above. I have seen a similar problem in our code.
Any hint?
Thanks
Graffa
		</comment>
		<comment id='8' author='oracle3001' date='2019-10-15T22:28:43Z'>
		&lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;
  ^^
		</comment>
		<comment id='9' author='oracle3001' date='2019-10-16T14:17:40Z'>
		bump
		</comment>
		<comment id='10' author='oracle3001' date='2019-11-18T06:48:38Z'>
		any update?
		</comment>
		<comment id='11' author='oracle3001' date='2019-11-18T15:12:50Z'>
		Please look into this issue. The console output becomes a complete mess
		</comment>
		<comment id='12' author='oracle3001' date='2019-11-23T11:37:19Z'>
		Hi, I am facing this issue while training tiny yolo on VOC2012 dataset using model.fit() method. Note that I am using repo &lt;denchmark-link:https://github.com/zzh8829/yolov3-tf2&gt;https://github.com/zzh8829/yolov3-tf2&lt;/denchmark-link&gt;
 and using TF2.0 official release. Any suggestion to fix the error?
		</comment>
		<comment id='13' author='oracle3001' date='2019-11-27T13:47:57Z'>
		Hi, I also have got this problem. Model is learning normally, however console output is unreadable. After every epoch there is an error with message "BaseCollectiveExecutor::StartAbort Out of range: End of sequence".
		</comment>
		<comment id='14' author='oracle3001' date='2019-11-28T00:25:52Z'>
		Same issue on fresh install, using &lt;denchmark-link:https://www.tensorflow.org/tutorials/text/text_generation&gt;code found on the official documentation&lt;/denchmark-link&gt;

Operating System: Windows 10
Python: 3.7-64
Tensorflow Package: tensorflow-gpu@2.0.0
GPU Hardware: 1080Ti
CUDA Lib: 10.0
cuDNN: 7.6.5.32
&lt;denchmark-h:h1&gt;Example Code:&lt;/denchmark-h&gt;

Click to expand
from __future__ import absolute_import, division, print_function, unicode_literals

import tensorflow as tf

import numpy as np
import os
import time

path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')


# Read, then decode for py2 compat.
text = open(path_to_file, 'rb').read().decode(encoding='utf-8')
# length of text is the number of characters in it
print ('Length of text: {} characters'.format(len(text)))

# Take a look at the first 250 characters in text
print(text[:250])

# The unique characters in the file
vocab = sorted(set(text))
print ('{} unique characters'.format(len(vocab)))

# Creating a mapping from unique characters to indices
char2idx = {u:i for i, u in enumerate(vocab)}
idx2char = np.array(vocab)

text_as_int = np.array([char2idx[c] for c in text])

print('{')
for char,_ in zip(char2idx, range(20)):
    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))
print('  ...\n}')

# Show how the first 13 characters from the text are mapped to integers
print ('{} ---- characters mapped to int ---- &gt; {}'.format(repr(text[:13]), text_as_int[:13]))

# The maximum length sentence we want for a single input in characters
seq_length = 100
examples_per_epoch = len(text)//(seq_length+1)

# Create training examples / targets
char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)

for i in char_dataset.take(5):
  print(idx2char[i.numpy()])

sequences = char_dataset.batch(seq_length+1, drop_remainder=True)

for item in sequences.take(5):
  print(repr(''.join(idx2char[item.numpy()])))

def split_input_target(chunk):
    input_text = chunk[:-1]
    target_text = chunk[1:]
    return input_text, target_text

dataset = sequences.map(split_input_target)

for input_example, target_example in  dataset.take(1):
  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))
  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))

for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):
    print("Step {:4d}".format(i))
    print("  input: {} ({:s})".format(input_idx, repr(idx2char[input_idx])))
    print("  expected output: {} ({:s})".format(target_idx, repr(idx2char[target_idx])))

# Batch size
BATCH_SIZE = 64

# Buffer size to shuffle the dataset
# (TF data is designed to work with possibly infinite sequences,
# so it doesn't attempt to shuffle the entire sequence in memory. Instead,
# it maintains a buffer in which it shuffles elements).
BUFFER_SIZE = 10000

dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)

# Length of the vocabulary in chars
vocab_size = len(vocab)

# The embedding dimension
embedding_dim = 256

# Number of RNN units
rnn_units = 1024

def build_model(vocab_size, embedding_dim, rnn_units, batch_size):
  model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim,
                              batch_input_shape=[batch_size, None]),
    tf.keras.layers.GRU(rnn_units,
                        return_sequences=True,
                        stateful=True,
                        recurrent_initializer='glorot_uniform'),
    tf.keras.layers.Dense(vocab_size)
  ])
  return model

model = build_model(
  vocab_size = len(vocab),
  embedding_dim=embedding_dim,
  rnn_units=rnn_units,
  batch_size=BATCH_SIZE)

for input_example_batch, target_example_batch in dataset.take(1):
  example_batch_predictions = model(input_example_batch)
  print(example_batch_predictions.shape, "# (batch_size, sequence_length, vocab_size)")

sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)
sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()

print("Input: \n", repr("".join(idx2char[input_example_batch[0]])))
print()
print("Next Char Predictions: \n", repr("".join(idx2char[sampled_indices ])))

def loss(labels, logits):
  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)

example_batch_loss  = loss(target_example_batch, example_batch_predictions)
print("Prediction shape: ", example_batch_predictions.shape, " # (batch_size, sequence_length, vocab_size)")
print("scalar_loss:      ", example_batch_loss.numpy().mean())

model.compile(optimizer='adam', loss=loss)

# Directory where the checkpoints will be saved
checkpoint_dir = './resources/training_checkpoints'
# Name of the checkpoint files
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt_{epoch}")

checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_prefix,
    save_weights_only=True)

EPOCHS=10

history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])

tf.train.latest_checkpoint(checkpoint_dir)

model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)

model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))

model.build(tf.TensorShape([1, None]))

model.summary()

def generate_text(model, start_string):
  # Evaluation step (generating text using the learned model)

  # Number of characters to generate
  num_generate = 1000

  # Converting our start string to numbers (vectorizing)
  input_eval = [char2idx[s] for s in start_string]
  input_eval = tf.expand_dims(input_eval, 0)

  # Empty string to store our results
  text_generated = []

  # Low temperatures results in more predictable text.
  # Higher temperatures results in more surprising text.
  # Experiment to find the best setting.
  temperature = 1.0

  # Here batch size == 1
  model.reset_states()
  for i in range(num_generate):
      predictions = model(input_eval)
      # remove the batch dimension
      predictions = tf.squeeze(predictions, 0)

      # using a categorical distribution to predict the word returned by the model
      predictions = predictions / temperature
      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()

      # We pass the predicted word as the next input to the model
      # along with the previous hidden state
      input_eval = tf.expand_dims([predicted_id], 0)

      text_generated.append(idx2char[predicted_id])

  return (start_string + ''.join(text_generated))

print(generate_text(model, start_string=u"ROMEO: "))

&lt;denchmark-h:h1&gt;Output&lt;/denchmark-h&gt;

Click to expand
2019-11-27 19:22:25.907687: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
Length of text: 1115394 characters
First Citizen:
Before we proceed any further, hear me speak.

All:
Speak, speak.

First Citizen:
You are all resolved rather to die than to famish?

All:
Resolved. resolved.

First Citizen:
First, you know Caius Marcius is chief enemy to the people.

65 unique characters
{
  '\n':   0,
  ' ' :   1,
  '!' :   2,
  '$' :   3,
  '&amp;' :   4,
  "'" :   5,
  ',' :   6,
  '-' :   7,
  '.' :   8,
  '3' :   9,
  ':' :  10,
  ';' :  11,
  '?' :  12,
  'A' :  13,
  'B' :  14,
  'C' :  15,
  'D' :  16,
  'E' :  17,
  'F' :  18,
  'G' :  19,
  ...
}
'First Citizen' ---- characters mapped to int ---- &gt; [18 47 56 57 58  1 15 47 58 47 64 43 52]
2019-11-27 19:22:27.947945: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2019-11-27 19:22:27.981240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6705
pciBusID: 0000:01:00.0
2019-11-27 19:22:27.987413: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-11-27 19:22:27.992523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-11-27 19:22:27.995950: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-11-27 19:22:28.007916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6705
pciBusID: 0000:01:00.0
2019-11-27 19:22:28.014056: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-11-27 19:22:28.023461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-11-27 19:22:28.605781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-11-27 19:22:28.610061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0
2019-11-27 19:22:28.612627: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2019-11-27 19:22:28.616313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8784 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
F
i
r
s
t
'First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou '
'are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you k'
"now Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us ki"
"ll him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be d"
'one: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citi'
Input data:  'First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou'
Target data: 'irst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou '
Step    0
  input: 18 ('F')
  expected output: 47 ('i')
Step    1
  input: 47 ('i')
  expected output: 56 ('r')
Step    2
  input: 56 ('r')
  expected output: 57 ('s')
Step    3
  input: 57 ('s')
  expected output: 58 ('t')
Step    4
  input: 58 ('t')
  expected output: 1 (' ')
2019-11-27 19:22:30.659472: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2019-11-27 19:22:31.438471: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
(64, 100, 65) # (batch_size, sequence_length, vocab_size)
Input: 
 'ct,\nSuch as move men; beside, she hath prosperous art\nWhen she will play with reason and discourse,\n'

Next Char Predictions:
 "dBBEE'E;Qc\nlUTzu,nVzYVyaQZt?kU,q 3;PE:EuGignL,$oV;LM!NS:&amp;er$RvG'g,Kkzt-knZOrg\n-vmGpSpuiE hzz&amp;ZSHIre\n"
Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)
scalar_loss:       4.1739583
Epoch 1/10
2019-11-27 19:22:32.762911: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_gru_with_fallback_2273_2411_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_3042' and '__inference___backward_standard_gru_2524_2900' both implement 'gru_ea298c2e-507f-4d8b-9d0f-d620fe9617aa' but their signatures do not match.
    172/Unknown - 8s 45ms/step - loss: 2.67862019-11-27 19:22:39.347075: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
         [[{{node IteratorGetNext}}]]
         [[Adam/Adam/update/AssignSubVariableOp/_29]]
2019-11-27 19:22:39.354004: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
         [[{{node IteratorGetNext}}]]
172/172 [==============================] - 8s 46ms/step - loss: 2.6786
Epoch 2/10
171/172 [============================&gt;.] - ETA: 0s - loss: 1.97222019-11-27 19:22:45.913512: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
         [[{{node IteratorGetNext}}]]
         [[Adam/Adam/update/AssignSubVariableOp/_29]]
2019-11-27 19:22:45.921313: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
         [[{{node IteratorGetNext}}]]
172/172 [==============================] - 7s 38ms/step - loss: 1.9722
Epoch 3/10
171/172 [============================&gt;.] - ETA: 0s - loss: 1.70472019-11-27 19:22:52.567125: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
         [[{{node IteratorGetNext}}]]
         [[Adam/Adam/update/AssignSubVariableOp/_29]]
2019-11-27 19:22:52.573933: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
         [[{{node IteratorGetNext}}]]
172/172 [==============================] - 7s 39ms/step - loss: 1.7047
Epoch 4/10
171/172 [============================&gt;.] - ETA: 0s - loss: 1.55052019-11-27 19:22:59.280955: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
         [[{{node IteratorGetNext}}]]
         [[Adam/Adam/update/AssignSubVariableOp/_29]]
2019-11-27 19:22:59.288517: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
         [[{{node IteratorGetNext}}]]
172/172 [==============================] - 7s 39ms/step - loss: 1.5505
Epoch 5/10
171/172 [============================&gt;.] - ETA: 0s - loss: 1.46012019-11-27 19:23:05.895398: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
         [[{{node IteratorGetNext}}]]
         [[Adam/Adam/update/AssignSubVariableOp/_29]]
2019-11-27 19:23:05.911084: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
         [[{{node IteratorGetNext}}]]
172/172 [==============================] - 7s 39ms/step - loss: 1.4601
Epoch 6/10
171/172 [============================&gt;.] - ETA: 0s - loss: 1.39762019-11-27 19:23:12.458498: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
         [[{{node IteratorGetNext}}]]
         [[Adam/Adam/update/AssignSubVariableOp/_29]]
2019-11-27 19:23:12.465456: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
         [[{{node IteratorGetNext}}]]
172/172 [==============================] - 7s 38ms/step - loss: 1.3976
Epoch 7/10
171/172 [============================&gt;.] - ETA: 0s - loss: 1.35182019-11-27 19:23:19.054523: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
         [[{{node IteratorGetNext}}]]
         [[Adam/Adam/update/AssignSubVariableOp/_29]]
2019-11-27 19:23:19.061413: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
         [[{{node IteratorGetNext}}]]
172/172 [==============================] - 7s 38ms/step - loss: 1.3518
Epoch 8/10
171/172 [============================&gt;.] - ETA: 0s - loss: 1.31302019-11-27 19:23:25.635973: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
         [[{{node IteratorGetNext}}]]
         [[Adam/Adam/update/AssignSubVariableOp/_29]]
2019-11-27 19:23:25.642910: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
         [[{{node IteratorGetNext}}]]
172/172 [==============================] - 7s 38ms/step - loss: 1.3130
Epoch 9/10
171/172 [============================&gt;.] - ETA: 0s - loss: 1.27652019-11-27 19:23:32.265397: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
         [[{{node IteratorGetNext}}]]
         [[Adam/Adam/update/AssignSubVariableOp/_29]]
2019-11-27 19:23:32.273219: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
         [[{{node IteratorGetNext}}]]
172/172 [==============================] - 7s 38ms/step - loss: 1.2765
Epoch 10/10
171/172 [============================&gt;.] - ETA: 0s - loss: 1.24442019-11-27 19:23:38.871165: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
         [[{{node IteratorGetNext}}]]
         [[Adam/Adam/update/AssignSubVariableOp/_29]]
2019-11-27 19:23:38.878105: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
         [[{{node IteratorGetNext}}]]
172/172 [==============================] - 7s 38ms/step - loss: 1.2444
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
embedding_1 (Embedding)      (1, None, 256)            16640
_________________________________________________________________
gru_1 (GRU)                  (1, None, 1024)           3938304
_________________________________________________________________
dense_1 (Dense)              (1, None, 65)             66625     
=================================================================
Total params: 4,021,569
Trainable params: 4,021,569
Non-trainable params: 0
_________________________________________________________________
ROMEO: forget, you seems together:
And not of an artiff; stop, and make these wopes
in there.

YORREL:
No. Which if thou forgot thee in the presence;
For by the senate company, for truth her brother:
My gracious son, the copple of love go
To-morrow, and here must attend my own,
Which, laduces and the rockle stands;
And not proof an adult coal.
Now, fie, fines with his grace, got her.

MERCUTIO:
Was ever anothir tidings: if I flatter me
Thurt can cry him to answer to the people.

CLEOREO:
These earthly face lunes;
Rows, away by lime there, to bring forth;
To rich shall not find a supposh-back crume
Unless the grace as wretching desting.
There is thy face; it is not to?

POLIXENES:
It for a divided care
Will break against his fiery drops!
What! would not know him? I'll sit him achore in the
subjects grow, that loves my ear.

COMINIUS:
You scat of a year of remedy; I would put on this blood.
Be gole and love in blows, there is not intend
And princely fisk and done from sweeted knight.

ROMEO:
Ay


		</comment>
		<comment id='15' author='oracle3001' date='2019-11-30T18:32:28Z'>
		I'm having the same issue, I though I was doing something wrong at first loading numpy data to tf.data, but then it gave me same errors following this example
&lt;denchmark-link:https://www.tensorflow.org/tutorials/load_data/numpy&gt;https://www.tensorflow.org/tutorials/load_data/numpy&lt;/denchmark-link&gt;

		</comment>
		<comment id='16' author='oracle3001' date='2019-12-03T19:53:58Z'>
		This bug still seems to be in the Tensorflow 2.0.0 release. I'm getting exactly that error messages multiple times when running the following tutorial project: &lt;denchmark-link:https://www.tensorflow.org/tutorials/text/text_classification_rnn&gt;https://www.tensorflow.org/tutorials/text/text_classification_rnn&lt;/denchmark-link&gt;

The model seems to train and accuracy is increasing, so the warning doesn't seem to cause any harm besides messing up the shell.
Operating System: Ubuntu 18.04.3 LTS
Python: 3.7.5
Tensorflow Package: tensorflow-gpu@2.0.0
CUDA Lib: 10.0.130
GPU Model and Memory: GTX 1050 Ti, 4GB
		</comment>
		<comment id='17' author='oracle3001' date='2019-12-12T15:25:41Z'>
		Getting same issue, would be nice to know the reason for it and how to fix it if possible.
		</comment>
		<comment id='18' author='oracle3001' date='2019-12-13T16:12:03Z'>
		It seems fixed in tensorflow-gpu@2.1.0rc1 (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/releases/tag/v2.1.0-rc1&gt;https://github.com/tensorflow/tensorflow/releases/tag/v2.1.0-rc1&lt;/denchmark-link&gt;
)
Operating System: Ubuntu 18.04
Python: 3.6.9
Tensorflow: tensorflow-gpu@2.1.0rc1
CUDA: 10.1.243
CuDNN: 7.6
GPU: Quadro RTX 5000
NVidia driver: 430.26
		</comment>
		<comment id='19' author='oracle3001' date='2019-12-29T16:51:58Z'>
		Also facing this problem:

Hi, I am facing this issue while training tiny yolo on VOC2012 dataset using model.fit() method. Note that I am using repo https://github.com/zzh8829/yolov3-tf2 and using TF2.0 official release. Any suggestion to fix the error?

Unfortunately

pip3 install --user tensorflow-gpu==2.1.0rc1

did not fix the error
		</comment>
		<comment id='20' author='oracle3001' date='2020-02-25T19:51:49Z'>
		The bug still seems to be in Tensorflow 2.1.0.
(I am now working on a different machine than the one I mentioned above. So it doesn't even seem to be specific to some setup. Different GPU, CPU, OS-Version, etc.)
Operating System: Ubuntu 19.10
Python: 3.7.5
Tensorflow Package: tensorflow@2.1.0
CUDA: 10.1.243
CuDNN: 7.6.5
GPU Model and Memory: RTX 2070 Super, 8GB
		</comment>
		<comment id='21' author='oracle3001' date='2020-03-18T14:41:53Z'>
		I'm facing the same issue in Tensorflow 2.1.0.
After following all the steps in official tutorial &lt;denchmark-link:https://tensorflow.google.cn/tutorials/load_data/csv&gt;https://tensorflow.google.cn/tutorials/load_data/csv&lt;/denchmark-link&gt;
,  I get the trained model. But the issue comes out each time when I tried to evaluate the model on test_data:

OS:  Windows 10
Python :  3.7.6
Tensorflow:  2.1.0
		</comment>
		<comment id='22' author='oracle3001' date='2020-04-03T16:06:46Z'>
		Based on &lt;denchmark-link:https://github.com/keras-team/autokeras/issues/839#issuecomment-590097076&gt;keras-team/autokeras#839 (comment)&lt;/denchmark-link&gt;
:
Concerning a model.fit, setting validation_steps in case you are using validation_data or steps_per_epoch if your input x is a tf.data dataset solves this issue.
I have not tested a model.evaluate but following the same logic, I think you should set the steps parameter.
		</comment>
		<comment id='23' author='oracle3001' date='2020-04-04T11:31:44Z'>
		
Based on keras-team/autokeras#839 (comment):
Concerning a model.fit, setting validation_steps in case you are using validation_data or steps_per_epoch if your input x is a tf.data dataset solves this issue.
I have not tested a model.evaluate but following the same logic, I think you should set the steps parameter.

Thanks for your reply, that's the solution.
		</comment>
		<comment id='24' author='oracle3001' date='2020-04-30T19:46:56Z'>
		This bug still occurs when calling model.predict() on a tf.data.Dataset.from_generator(gen) dataset in TF 2.1.0.
		</comment>
		<comment id='25' author='oracle3001' date='2020-05-06T01:45:51Z'>
		I'm getting it during the validation portion of model.fit()
		</comment>
		<comment id='26' author='oracle3001' date='2020-05-20T08:55:01Z'>
		Still present during validation indeed (or just before).
W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence [[{{node IteratorGetNext}}]] 2020-05-20 10:49:02.923506: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence [[{{node IteratorGetNext}}]] [[IteratorGetNext/_6]] 2020-05-20 10:49:17.997026: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence [[{{node IteratorGetNext}}]] 2020-05-20 10:49:17.997362: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence [[{{node IteratorGetNext}}]] [[IteratorGetNext/_4]]
		</comment>
		<comment id='27' author='oracle3001' date='2020-05-21T00:57:19Z'>
		Same here!
&lt;denchmark-code&gt;W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext}}]]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='28' author='oracle3001' date='2020-05-28T15:58:44Z'>
		In my case with TF-2.1, I load both train and validation dataset from a TFRecord file. E.g. tf.data.TFRecordDataset(filePath). This means, the tf.fit function has no chance to know the number of samples in the file. As &lt;denchmark-link:https://github.com/rmarru&gt;@rmarru&lt;/denchmark-link&gt;
 pointed out, if the params in the tf.fit function (, ) are set, then the function knows the number of elements and no warning should be seen.
If I dont set steps_per_epoch and validation_steps, after the first epoch tf.fit will remember steps_per_epoch, but not  validation_steps. That means, from the second epoch I will receive less warnings, only for the validation.
		</comment>
	</comments>
</bug>