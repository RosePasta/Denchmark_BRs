<bug id='37933' author='ellaJin' open_date='2020-03-26T08:19:30Z' closed_time='2020-04-09T06:08:12Z'>
	<summary>TypeError: 'tensorflow.python.framework.ops.EagerTensor' object is not callable</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock
example script provided in TensorFlow):  Yes
OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04):  Windows 8.1
TensorFlow installed from (source or
binary): - TensorFlow version (use command below):  Tensorflow 2.0
Python version: - Bazel
version (if compiling from source): Python 3.5

Describe the current behavior
I am trying to migrate TF1.14 code to TF 2.0. However, some functions are disappeared in TF2.0.
I changed the codes like this:
loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.output, labels=[
self.winner_loc[0] + self.winner_loc[1]])
optimizer = tf.keras.optimizers.SGD(learning_rate=alpha_op)
optimizer.minimize(loss, self.weightage_vects)
but when I run the code, the error happen:
TypeError: 'tensorflow.python.framework.ops.EagerTensor' object is not callable in the line:
optimizer.minimize(loss, self.weightage_vects)
I tried to change my code like:
with tf.GradientTape() as tape:
loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.output, labels=[
self.winner_loc[0] + self.winner_loc[1]])
gradients = tape.gradient(target=loss, sources=self.weightage_vects)
optimizer.apply_gradients(zip(gradients, self.weightage_vects))
but other errors happened: TypeError: zip argument &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/1&gt;#1&lt;/denchmark-link&gt;
 must support iteration
I don't know how to correct these codes. T.T Please Help!
	</description>
	<comments>
		<comment id='1' author='ellaJin' date='2020-03-26T10:20:19Z'>
		&lt;denchmark-link:https://github.com/ellaJin&gt;@ellaJin&lt;/denchmark-link&gt;
, Did you follow the instructions mentioned in the &lt;denchmark-link:https://www.tensorflow.org/guide/migrate&gt;Tensorflow doc&lt;/denchmark-link&gt;
. Provide the standalone code to reproduce the issue. Thanks!
		</comment>
		<comment id='2' author='ellaJin' date='2020-03-26T11:04:00Z'>
		
@ellaJin, Did you follow the instructions mentioned in the Tensorflow doc. Provide the standalone code to reproduce the issue. Thanks!

import tensorflow as tf
import numpy as np
import re
inputshape = 512
class SOM2(object):
trianed = False
&lt;denchmark-code&gt;def __init__(self, m, n, dim, n_iterations=100, alpha=None, sigma=None, features=None):

    self.m = m
    self.n = n
    self.dim = dim

    if alpha is None:
        self.alpha = 0.3
    else:
        self.alpha = float(alpha)
    if sigma is None:
        self.sigma = max(m, n) / 2.0
    else:
        self.sigma = float(sigma)
    self.vect_input = features
    print(features.shape)
    self.n_iterations = abs(int(n_iterations))
    self.centroid_grid = np.zeros(shape=(m, n))
    self.loaded_weights = None

    self.weightage_vects = tf.Variable(tf.random.normal(
        [m * n, dim], mean=0.5))

    self.location_vects = tf.constant(np.array(
        list(self._neuron_locations(m, n))))

    self.dense_input, self.output, self.dense_weights = self.dense(self.vect_input)

def _neuron_locations(self, m, n):

    for i in range(m):
        for j in range(n):
            yield np.array([i, j])

def dense(self, inputs_vect):
  
    w1 = tf.Variable(tf.random.normal([tf.shape(inputs_vect)[1], self.dim]))    
    b1 = tf.constant(0.1, shape=[self.dim])
    mid1 = tf.constant(1.0, shape=[1, 175104])
    z1 = tf.matmul(tf.matmul(mid1, inputs_vect), w1) + b1
    dense_input = tf.nn.relu(z1)

    w2 = tf.Variable(tf.random.normal([self.dim, self.m * self.n]))
    b2 = tf.constant(0.1, shape=[self.m * self.n])
    output = tf.matmul(dense_input, w2) + b2

    return dense_input, output, w1

def train_operation(self):
    for iter in range(self.n_iterations):
        winner_index = tf.argmin(tf.sqrt(tf.reduce_sum(
            tf.square(tf.subtract(self.weightage_vects, tf.stack(
                [self.dense_input[0] for i in range(self.m * self.n)]))), 1)), 0)
        print('winner_index', winner_index)

        slice_input = tf.pad(tf.reshape(winner_index, [1]),
                             np.array([[0, 1]]))
        print('slice_Input:', slice_input)
        self.winner_loc = tf.reshape(tf.slice(self.location_vects, slice_input,
                                              tf.constant(np.array([1, 2]), dtype=tf.int64)),
                                     [2])  
        print('winner_loc', self.winner_loc)

        learning_rate_op = tf.subtract(1.0, tf.divide(iter,
                                                      self.n_iterations))
        alpha_op = learning_rate_op * self.alpha
        sigma_op = self.sigma * learning_rate_op

        bmu_distance_squares = tf.reduce_sum(tf.square(tf.subtract(
            self.location_vects, tf.stack(
                [self.winner_loc for i in range(self.m * self.n)]))), 1) 

        neighbourhood_func = tf.exp(tf.negative(tf.divide(tf.cast(
            bmu_distance_squares, "float32"), tf.square(sigma_op))))

        learning_rate_op = tf.multiply(alpha_op, neighbourhood_func)  

        learning_rate_multiplier = tf.stack([tf.tile(tf.slice(
            learning_rate_op, np.array([i]), np.array([1])), [self.dim])
            for i in range(self.m * self.n)])

        weightage_delta = tf.multiply(
            learning_rate_multiplier,
            tf.subtract(tf.stack([self.dense_input[0] for i in range(self.m * self.n)]),
                        self.weightage_vects)) 

        new_weightages_op = tf.add(self.weightage_vects,
                                   weightage_delta)


        self.weightage_vects.assign(new_weightages_op)


       tf.train.ProximalGradientDescentOptimizer(learning_rate=alpha_op).minimize(cross_entropy)
        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.output, labels=[
            self.winner_loc[0] + self.winner_loc[1]])
        optimizer = tf.keras.optimizers.SGD(learning_rate=alpha_op)

        optimizer.minimize(loss, self.weightage_vects)
        # with tf.GradientTape() as tape:
        #     loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.output, labels=[
        #     self.winner_loc[0] + self.winner_loc[1]])
        # gradients = tape.gradient(target=loss, sources=self.weightage_vects)
        # optimizer.apply_gradients(zip(gradients, self.weightage_vects))

    self.trianed = True
&lt;/denchmark-code&gt;

Here is the code. Thanks!
		</comment>
		<comment id='3' author='ellaJin' date='2020-04-01T11:34:52Z'>
		&lt;denchmark-link:https://github.com/ellaJin&gt;@ellaJin&lt;/denchmark-link&gt;
, Please follow the instructions mentioned &lt;denchmark-link:https://www.tensorflow.org/guide/migrate&gt;here&lt;/denchmark-link&gt;
  and use upgrade script to convert from Tf1 to Tf2. Find more details &lt;denchmark-link:https://www.tensorflow.org/guide/upgrade&gt;here&lt;/denchmark-link&gt;
. Thanks
		</comment>
		<comment id='4' author='ellaJin' date='2020-04-08T07:15:15Z'>
		&lt;denchmark-link:https://github.com/ellaJin&gt;@ellaJin&lt;/denchmark-link&gt;
, Please update for the above comment.
		</comment>
		<comment id='5' author='ellaJin' date='2020-04-09T06:08:13Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37933&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37933&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='ellaJin' date='2020-10-05T15:40:09Z'>
		Hello,
I am having the same issue but I do not understand clearly the solutions proposed in the previous posts...
I am new to TF.
I get this error:
TypeError: 'tensorflow.python.framework.ops.EagerTensor' object is not callable
When I run this model in Tensorflow V2. The error comes from the last line.
latent_features = 8
learning_rate = 0.001
num_users = len( X )
num_items = len( X[0] )
 
W = tf.Variable(tf.random.normal([num_users, latent_features], stddev=0.05, mean=0) )
H = tf.Variable(tf.random.normal([latent_features, num_items], stddev=0.05, mean=0) )

result = tf.matmul(W, H)
diff_op = tf.math.subtract(result, X)
cost = tf.reduce_sum( tf.square(diff_op) )
opt = tf.optimizers.Adam(learning_rate = learning_rate)
step = opt.minimize(cost, var_list=[W,H] )
Can you help ? Thank you
		</comment>
		<comment id='7' author='ellaJin' date='2020-10-06T03:13:53Z'>
		
Hello,
I am having the same issue but I do not understand clearly the solutions proposed in the previous posts...
I am new to TF.
I get this error:
TypeError: 'tensorflow.python.framework.ops.EagerTensor' object is not callable
When I run this model in Tensorflow V2. The error comes from the last line.
latent_features = 8
learning_rate = 0.001
num_users = len( X )
num_items = len( X[0] )
 
W = tf.Variable(tf.random.normal([num_users, latent_features], stddev=0.05, mean=0) )
H = tf.Variable(tf.random.normal([latent_features, num_items], stddev=0.05, mean=0) )

result = tf.matmul(W, H)
diff_op = tf.math.subtract(result, X)
cost = tf.reduce_sum( tf.square(diff_op) )
opt = tf.optimizers.Adam(learning_rate = learning_rate)
step = opt.minimize(cost, var_list=[W,H] )
Can you help ? Thank you

Sorry, I had changed my code in TF2.0, because I couldn't find a solution with the minimize function in TF2.0.
		</comment>
	</comments>
</bug>