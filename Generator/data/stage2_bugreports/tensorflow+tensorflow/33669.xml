<bug id='33669' author='zhangyi02' open_date='2019-10-24T06:13:32Z' closed_time='2019-11-13T01:44:06Z'>
	<summary>tf.keras.callbacks.ProgbarLogger not works as expected</summary>
	<description>
I was tried to use tf.keras instead of keras, and I found that tf.keras.callbacks.ProgbarLogger not works as expected, or as the old keras did.
In keras, at the end of one-epoch's training, the ProgbarLogger will just wait until the validation done, and the final info on the screen would be like

[training-progress-bar] training-metrics validation-metrics

But In keras, at the end of one-epoch's training, the training-progress-bar will disappear and the validation-progress-bar will show on the screen. And when validation progress finished, the final info on the screen would be like

[validation-progress-bar] validation-metrics
[training-progress-bar] training-metrics validation-metrics

As I expected, the training-progress-bar should stay on the screen during validation, and the final info should be

[training-progress-bar] training-metrics
[validation-progress-bar] validation-metrics

	</description>
	<comments>
		<comment id='1' author='zhangyi02' date='2019-10-25T06:46:01Z'>
		&lt;denchmark-link:https://github.com/zhangyi02&gt;@zhangyi02&lt;/denchmark-link&gt;
, Please provide the standalone code to reproduce the reported issue. Thanks!
		</comment>
		<comment id='2' author='zhangyi02' date='2019-11-07T11:43:25Z'>
		&lt;denchmark-link:https://github.com/zhangyi02&gt;@zhangyi02&lt;/denchmark-link&gt;
, Any update code snippet.
		</comment>
		<comment id='3' author='zhangyi02' date='2019-11-08T02:02:22Z'>
		import numpy as np
import keras
print(keras)


class FakedataSequence(keras.utils.Sequence):
    def __init__(self, batch_size, num_batches, stage='tr'):
        self.batch_size = batch_size
        self.num_batches = num_batches
        self.stage = stage

    def __len__(self):
        return self.num_batches

    def __getitem__(self, item):
        if self.stage == 'tr':
            np.random.seed(item)
        else:
            np.random.seed(100000 + item)
        data = np.random.random((self.batch_size, 10))
        label = np.random.randint(0, 2, size=self.batch_size)
        return data, label


def toy_model():
    x = keras.layers.Input(shape=(10,))
    y = keras.layers.Dense(1, activation='sigmoid')(x)
    return keras.models.Model(x, y)


if __name__ == '__main__':
    tr_sequence = FakedataSequence(32, 100, 'tr')
    cv_sequence = FakedataSequence(32, 10, 'cv')
    model = toy_model()
    model.compile(optimizer='sgd', loss='mse')
    history = model.fit_generator(generator=tr_sequence,
                                  epochs=10,
                                  verbose=1,
                                  validation_data=cv_sequence,
                                  workers=32)
while using the old keras, as print(keras) returns &lt;module 'keras' from '~/.local/lib/python3.6/site-packages/keras/__init__.py'&gt;, the logs looks like:

Epoch 1/10
100/100 [==============================] - 0s 3ms/step - loss: 0.2649 - val_loss: 0.2828
Epoch 2/10
100/100 [==============================] - 0s 3ms/step - loss: 0.2648 - val_loss: 0.2820
Epoch 3/10
100/100 [==============================] - 0s 2ms/step - loss: 0.2641 - val_loss: 0.2809
Epoch 4/10
100/100 [==============================] - 0s 4ms/step - loss: 0.2659 - val_loss: 0.2802
Epoch 5/10
100/100 [==============================] - 0s 3ms/step - loss: 0.2638 - val_loss: 0.2797
Epoch 6/10
100/100 [==============================] - 0s 2ms/step - loss: 0.2648 - val_loss: 0.2790
Epoch 7/10
100/100 [==============================] - 0s 2ms/step - loss: 0.2627 - val_loss: 0.2787
Epoch 8/10
100/100 [==============================] - 0s 2ms/step - loss: 0.2634 - val_loss: 0.2781
Epoch 9/10
100/100 [==============================] - 0s 1ms/step - loss: 0.2634 - val_loss: 0.2778
Epoch 10/10
100/100 [==============================] - 0s 1ms/step - loss: 0.2632 - val_loss: 0.2776

And if change the import keras to from tensorflow import keras, and rerun this script, while print(keras) returns &lt;module 'tensorflow._api.v1.keras' from '~/.local/lib/python3.6/site-packages/tensorflow/_api/v1/keras/__init__.py'&gt;, the logs looks like:

10/10 [==============================] - 0s 3ms/step - loss: 0.2566
100/100 [==============================] - 1s 11ms/step - loss: 0.2571 - val_loss: 0.2566
Epoch 2/10
10/10 [==============================] - 0s 1ms/step - loss: 0.2566
100/100 [==============================] - 0s 3ms/step - loss: 0.2568 - val_loss: 0.2566
Epoch 3/10
10/10 [==============================] - 0s 2ms/step - loss: 0.2640
100/100 [==============================] - 0s 4ms/step - loss: 0.2562 - val_loss: 0.2640
Epoch 4/10
10/10 [==============================] - 0s 2ms/step - loss: 0.2491
100/100 [==============================] - 0s 3ms/step - loss: 0.2554 - val_loss: 0.2491
Epoch 5/10
10/10 [==============================] - 0s 1ms/step - loss: 0.2500
100/100 [==============================] - 0s 4ms/step - loss: 0.2542 - val_loss: 0.2500
Epoch 6/10
10/10 [==============================] - 0s 1ms/step - loss: 0.2598
100/100 [==============================] - 0s 5ms/step - loss: 0.2545 - val_loss: 0.2598
Epoch 7/10
10/10 [==============================] - 0s 688us/step - loss: 0.2496
100/100 [==============================] - 0s 3ms/step - loss: 0.2557 - val_loss: 0.2496
Epoch 8/10
10/10 [==============================] - 0s 2ms/step - loss: 0.2510
100/100 [==============================] - 0s 4ms/step - loss: 0.2544 - val_loss: 0.2510
Epoch 9/10
10/10 [==============================] - 0s 1ms/step - loss: 0.2514
100/100 [==============================] - 0s 4ms/step - loss: 0.2549 - val_loss: 0.2514
Epoch 10/10
10/10 [==============================] - 0s 2ms/step - loss: 0.2520
100/100 [==============================] - 0s 4ms/step - loss: 0.2550 - val_loss: 0.2520

Well, I notice that even "Epoch 1/10" also disappeared.
I supposed the logs should be like:

Epoch 1/10
100/100 [==============================] - 1s 11ms/step - loss: 0.2571
10/10 [==============================] - 0s 3ms/step - val_loss: 0.2566
Epoch 2/10
....

		</comment>
		<comment id='4' author='zhangyi02' date='2019-11-12T08:09:15Z'>
		Issue is replicating with Tensorflow 1.15.
Please see the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/9b7726a8a9845100d2f8b18a9da47ed4/untitled252.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='5' author='zhangyi02' date='2019-11-12T08:32:13Z'>
		&lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
 I tried TF2.0，it works just like the origin Keras. By the way, I'm using TF1.13.1. Maybe it's time change to TF2.0……
		</comment>
		<comment id='6' author='zhangyi02' date='2019-11-12T19:22:07Z'>
		&lt;denchmark-link:https://github.com/zhangyi02&gt;@zhangyi02&lt;/denchmark-link&gt;
 I agree that the output from  doesn't look as good as the output from . As  is the last version in  series, I don't think there is any more updates to that branch unless there is any security related issues.
If possible, I would suggest you to start using TF2.0. Please close the issue, if you don't have any further questions. Thanks!
		</comment>
		<comment id='7' author='zhangyi02' date='2019-11-13T01:44:07Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33669&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33669&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>