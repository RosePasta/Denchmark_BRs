<bug id='23413' author='Svdvoort' open_date='2018-10-31T21:22:22Z' closed_time='2019-02-08T21:50:24Z'>
	<summary>Models cannot be loaded</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


What is the top-level directory of the model you are using:  ~/
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, written a simple MWE script
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.1
TensorFlow installed from (source or binary): Compiled from source
TensorFlow version (use command below): 1.11.0
Python version: 3.6.6
Bazel version (if compiling from source): 0.16.1
GCC/Compiler version: 7.3.0
CUDA/cuDNN version: Cuda: 9.2.148, cuDNN: 9.2-v7.3.1.20
GPU model and memory: GeForce GTX1080Ti, 11GB
Exact command to reproduce:

Minimal working example code
&lt;denchmark-code&gt;from tensorflow.keras.layers import Conv2D, MaxPooling2D, ReLU, Flatten, Dense, Input
from tensorflow.keras.models import Model
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import load_model


def cnn_model(rows, cols, channels):
    model = Model()
    inputs = Input(shape=(rows, cols, channels), dtype='float32')

    conv1 = Conv2D(64, (3, 3),activation='linear',kernel_initializer='he_uniform')(inputs)
    relu1 = ReLU()(conv1)
    pooling1 = MaxPooling2D(pool_size=(5, 5))(relu1)

    flatten = Flatten()(pooling1)

    dense1 = Dense(512)(flatten)

    relu2 = ReLU()(dense1)
    predictions = Dense(4, activation='softmax')(relu2)

    model = Model(inputs=inputs, outputs=predictions)

    return model


batch_size = 16
rows = 128
cols = 128
channels = 1
model_input = np.random.randint(0, 255, (batch_size, rows, cols, channels))
model_labels = np.random.randint(0, 1, (batch_size, 4))

model = cnn_model(rows, cols,channels)

adam = tf.train.AdamOptimizer(learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8)

model.compile(loss='categorical_crossentropy',
              optimizer=adam,
              metrics=['categorical_accuracy'])
model.fit(x=model_input,
          y=model_labels,
          epochs=2)

model.save("/home/svdvoort/test_model.hdf5")
load_model("/home/svdvoort/test_model.hdf5")
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

Returns the following output (error included):
&lt;denchmark-code&gt;Epoch 1/2
2018-10-31 19:09:12.162738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683
pciBusID: 0000:01:00.0
totalMemory: 10.91GiB freeMemory: 10.76GiB
2018-10-31 19:09:12.290346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 1 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683
pciBusID: 0000:02:00.0
totalMemory: 10.92GiB freeMemory: 10.76GiB
2018-10-31 19:09:12.291317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0, 1
2018-10-31 19:09:12.750668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-10-31 19:09:12.750702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 1 
2018-10-31 19:09:12.750707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N Y 
2018-10-31 19:09:12.750710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 1:   Y N 
2018-10-31 19:09:12.751100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10404 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-10-31 19:09:12.855072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10407 MB memory) -&gt; physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)
16/16 [==============================] - 3s 201ms/step - loss: 0.0000e+00 - categorical_accuracy: 0.0000e+00
Epoch 2/2
16/16 [==============================] - 0s 1ms/step - loss: 0.0000e+00 - categorical_accuracy: 0.0000e+00
WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).
Traceback (most recent call last):
  File "Loading_error_example.py", line 47, in &lt;module&gt;
    load_model("/home/svdvoort/test_model.hdf5")
  File "/packages/tensorflow/1.11.0/Python-3.6.6/tensorflow/python/keras/engine/saving.py", line 230, in load_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File "/packages/tensorflow/1.11.0/Python-3.6.6/tensorflow/python/keras/engine/saving.py", line 310, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File "/packages/tensorflow/1.11.0/Python-3.6.6/tensorflow/python/keras/layers/serialization.py", line 64, in deserialize
    printable_module_name='layer')
  File "/packages/tensorflow/1.11.0/Python-3.6.6/tensorflow/python/keras/utils/generic_utils.py", line 173, in deserialize_keras_object
    list(custom_objects.items())))
  File "/packages/tensorflow/1.11.0/Python-3.6.6/tensorflow/python/keras/engine/network.py", line 1292, in from_config
    process_layer(layer_data)
  File "/packages/tensorflow/1.11.0/Python-3.6.6/tensorflow/python/keras/engine/network.py", line 1278, in process_layer
    layer = deserialize_layer(layer_data, custom_objects=custom_objects)
  File "/packages/tensorflow/1.11.0/Python-3.6.6/tensorflow/python/keras/layers/serialization.py", line 64, in deserialize
    printable_module_name='layer')
  File "/packages/tensorflow/1.11.0/Python-3.6.6/tensorflow/python/keras/utils/generic_utils.py", line 175, in deserialize_keras_object
    return cls.from_config(config['config'])
  File "/packages/tensorflow/1.11.0/Python-3.6.6/tensorflow/python/keras/engine/base_layer.py", line 1617, in from_config
    return cls(**config)
  File "/packages/tensorflow/1.11.0/Python-3.6.6/tensorflow/python/keras/layers/advanced_activations.py", line 310, in __init__
    if max_value is not None and max_value &lt; 0.:
TypeError: '&lt;' not supported between instances of 'dict' and 'float'
&lt;/denchmark-code&gt;

Same is true for all other models that are being trained
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

Output of tf_env attached
&lt;denchmark-link:https://github.com/tensorflow/models/files/2535750/tf_env.txt&gt;tf_env.txt&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='Svdvoort' date='2018-11-01T16:58:48Z'>
		After some testing the problem can be resolved by replacing the model with:
&lt;denchmark-code&gt;from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input, Activation
from tensorflow.keras.models import Model

def cnn_model(rows, cols, channels):
    model = Model()
    inputs = Input(shape=(rows, cols, channels), dtype='float32')

    conv1 = Conv2D(64, (3, 3),activation='linear',kernel_initializer='he_uniform')(inputs)
    relu1 = Activation('relu')(conv1)
    pooling1 = MaxPooling2D(pool_size=(5, 5))(relu1)

    flatten = Flatten()(pooling1)

    dense1 = Dense(512)(flatten)

    relu2 = Activation('relu')(dense1)
    flatten = Flatten()(conv1)
    predictions = Dense(4, activation='softmax')(flatten)

    model = Model(inputs=inputs, outputs=predictions)

    return model
&lt;/denchmark-code&gt;

But it seems strange that replacing that replacing the ReLU solves the issue (and the error is quite unclear).
		</comment>
		<comment id='2' author='Svdvoort' date='2018-11-07T16:53:07Z'>
		&lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;
  -  PTAL
		</comment>
		<comment id='3' author='Svdvoort' date='2019-01-08T14:28:20Z'>
		Hi all,
I am experiencing the same problem.
However, simply replacing layers.RelU() with layers.Activation("relu") does not yield the same training result in my case. Of course, I checked if I randomized the data in a non-deterministic way, but in all usages of randomization I explicitely set the seed to the same value.
Is there a systematic difference between ReLU and Activation("relu") which I fail to see?
Thanks for your help!
		</comment>
		<comment id='4' author='Svdvoort' date='2019-02-08T21:50:24Z'>
		I was able to execute your code snippet successfully using TF 1.13.0-rc0 Thanks!
		</comment>
	</comments>
</bug>