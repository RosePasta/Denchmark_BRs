<bug id='45162' author='juanma9613' open_date='2020-11-25T00:15:50Z' closed_time='2020-12-14T11:27:58Z'>
	<summary>Training a tf.keras.Model, with a tf.data.Dataset mapped with a randomly selected action from a large list of actions (callables provided by the user) takes a lot of time to start, if it ever does.</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):

Yes, I have, there's a colab notebook below with the custom code.

OS Platform and Distribution (e.g., Linux Ubuntu 16.04):

colab notebook

TensorFlow installed from (source or binary):

colab default version 2.3.0

TensorFlow version (use command below):

2.3.0

Python version:

3.6.9

GPU model and memory:

It happens using any of the GPUs available in Colab : Nvidia K80s, T4s, P4s and P100s, or even when I'm not using GPU
Describe the current behavior
Training a tf.keras.Model, with a tf.data.Dataset mapped with a custom augmentator, which randomly selects an action from a large list of actions (callables provided during the instantiation of the augmentator) takes a lot of time to start, if it ever does.
If it ever manages to start training (more than 20 minutes), then it takes even more time to start the epoch 3, epoch 5, epoch 7, etc.
Describe the expected behavior
Training should start fast as usual, and shouldn't have overhead to start in some epochs.
Standalone code to reproduce the issue
I have created a colab with about 40 lines of code in which the code can be reproduced:
&lt;denchmark-link:https://colab.research.google.com/drive/1FyNuTlX6puKIosWPNojcEN9zZi9-60Ve?usp=sharing&gt;https://colab.research.google.com/drive/1FyNuTlX6puKIosWPNojcEN9zZi9-60Ve?usp=sharing&lt;/denchmark-link&gt;

Other info
A little bit more information about the colab contents and the issue here:


Context a RandomAugmentationPolicy object can be instantiated with a list of callables, and is used to map a dataset with tf.data.Dataset.map. For each img in the dataset, it randomly samples one of its actions, and augments the img.


Problem : When the length of the list of actions provided to instantiate a RandomAugmentationPolicy object is about 24 or more, training a tf.keras.Model using model.fit never starts the epoch 3 when running on colab.  When running on an ubuntu machine (aws p3.2xlarge) with a CPU that has 8 cores it never starts epoch 7.


Sample code to reproduce the issue : The code in the colab first defines the RandomAugmentationPolicy class previously mentioned, then it created a mocked dataset, and it then tries to train a model tf.keras.Model.fit.


Note : The only reasonable way I found to apply the 'selected_action' (L22 class definition), is to iterate over all the actions and augment the img when the action idx is equal to the desired action.


Thank you in advance for your help with this issue.
	</description>
	<comments>
		<comment id='1' author='juanma9613' date='2020-11-25T09:08:37Z'>
		When i use tf2.3-cpu to run program, have the problem, Can you tell me the reason?
2020-11-25 16:55:33.614954: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found
2020-11-25 16:55:33.615418: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)
2020-11-25 16:55:33.622449: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: DESKTOP-B511OAC
2020-11-25 16:55:33.623218: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: DESKTOP-B511OAC
2020-11-25 16:55:33.624131: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-11-25 16:55:33.637363: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x16247dbe220 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-11-25 16:55:33.637884: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
		</comment>
		<comment id='2' author='juanma9613' date='2020-11-25T11:55:03Z'>
		
When i use tf2.3-cpu to run program, have the problem, Can you tell me the reason?
2020-11-25 16:55:33.614954: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found
2020-11-25 16:55:33.615418: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)
2020-11-25 16:55:33.622449: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: DESKTOP-B511OAC
2020-11-25 16:55:33.623218: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: DESKTOP-B511OAC
2020-11-25 16:55:33.624131: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-11-25 16:55:33.637363: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x16247dbe220 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-11-25 16:55:33.637884: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version

Sorry, but I couldn't completely understand you.  That shouldn't be a problem, but just to verify, I ran the same code on a macbookPro with OS catalina and a new conda environment that I created with the following 3 lines 1.) conda create -n test_dependencies python==3.6.9 , 2.) conda activate test_dependencies 3.) pip install tensorflow-cpu==2.3.0 . I didn't get that message you showed an I still have the same problem on the mac, training never starts the epoch 7 in this case (maybe related to the number of cores in the CPU, 8 in the macbook vs 2 in colab).
		</comment>
		<comment id='3' author='juanma9613' date='2020-11-25T16:18:22Z'>
		&lt;denchmark-link:https://github.com/juanma9613&gt;@juanma9613&lt;/denchmark-link&gt;

I ran the code on tf nightly and do not face the error, please refer to the &lt;denchmark-link:https://colab.research.google.com/gist/Saduf2019/cf53e5235f36d8b4d916a46d02bc1e92/untitled472.ipynb&gt;gist here&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='juanma9613' date='2020-11-25T16:36:26Z'>
		
@juanma9613
I ran the code on tf nightly and do not face the error, please refer to the gist here

&lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;
 Thank you, I was able to run it in 2.5.0-dev20201125.
I still need to be able to run it in a more stable version, do you have an idea of how to avoid the for loop in the method augment_imgs_with_actions ? My original idea was to do just self.actions[selected_action]  , but I can't index a list with a tensor. Do you have any suggestion in that case?
Thank you for your quick answer, I really appreciate your help!
		</comment>
		<comment id='5' author='juanma9613' date='2020-11-27T14:26:38Z'>
		&lt;denchmark-link:https://github.com/juanma9613&gt;@juanma9613&lt;/denchmark-link&gt;

it is a bug in 2.3 and will be fixed in the next release.
		</comment>
		<comment id='6' author='juanma9613' date='2020-12-07T10:51:05Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='7' author='juanma9613' date='2020-12-14T11:27:56Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='8' author='juanma9613' date='2020-12-14T11:27:59Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45162&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45162&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>