<bug id='29839' author='maxima120' open_date='2019-06-16T12:09:53Z' closed_time='2020-01-10T18:00:56Z'>
	<summary>TPU train_on_batch stride size error</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.13.1
Python version: 3.5
GPU model and memory: TPU v3-8

Describe the current behavior
Code and data which runs fine on CPU, throws error on TPU. This only happens if I use train_on_batch instead of fit.
I have 2 versions of same model. One is with fit with 2 loops and with train_on_batch with 3 loops (epoch, day worth of data, batch within day)
train_on_batch throws error: slice index 0 of dimension 0 out of bounds. for 'strided_slice' (op: 'StridedSlice') with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = &lt;0&gt;, input[2] = &lt;1&gt;, input[3] = &lt;1&gt;.
0111 data is label provided in y2. and the size of 4 is correct. Why is computed input tensor is size 3 I don't understand. It looks like a bug very much.
Model
&lt;denchmark-code&gt;model = tf.keras.Sequential()
model.add(layers.LSTM(neurons, input_shape=(window_size, inputs_n), return_sequences=True)) 
model.add(layers.LSTM(neurons))
model.add(layers.Dense(outputs_n, activation='sigmoid'))

opt = tf.train.AdamOptimizer(0.001)

model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])
 
tpu_model = tf.contrib.tpu.keras_to_tpu_model(model, 
        strategy=tf.contrib.tpu.TPUDistributionStrategy(
            tf.contrib.cluster_resolver.TPUClusterResolver(tpu = [TPU_ADDRESS1])))
&lt;/denchmark-code&gt;

Shapes
&lt;denchmark-code&gt;_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_input (InputLayer)      (None, 1024, 7)           0         
_________________________________________________________________
lstm (LSTM)                  (None, 1024, 128)         69632     
_________________________________________________________________
lstm_1 (LSTM)                (None, 128)               131584    
_________________________________________________________________
dense (Dense)                (None, 4)                 516       
=================================================================
&lt;/denchmark-code&gt;

Training:
&lt;denchmark-code&gt;for epoch in epochs:
    for d in days : 
        # get arrays for the day
        features = np.asarray(d[1])[:,2:9].astype(dtype = 'float32')
        labels = np.asarray(d[1])[:, 9:13].astype(dtype = 'int32')
        
        X,y = split_sequence(features, labels_buy, window_size)

        # train 
        for slide in range(window_size):
            try:
                x1, y1 = X[slide], y[slide]
                x2, y2 = x1.reshape(1,1024,7), y1.reshape(1, 4)
                H = tpu_model.train_on_batch(x2,y2)
            except Exception as e:
                print('** train exception **', e)
                continue
&lt;/denchmark-code&gt;

Describe the expected behavior
train_on_batch trains without exception
	</description>
	<comments>
		<comment id='1' author='maxima120' date='2019-06-17T09:12:24Z'>
		&lt;denchmark-link:https://github.com/maxima120&gt;@maxima120&lt;/denchmark-link&gt;
 Please provide the complete code snippet to reproduce the issue reported here. And also include the error log. Thanks!
		</comment>
		<comment id='2' author='maxima120' date='2019-06-17T12:12:49Z'>
		how do I send it to you? data file is big
		</comment>
		<comment id='3' author='maxima120' date='2019-06-18T10:21:29Z'>
		&lt;denchmark-link:https://github.com/maxima120&gt;@maxima120&lt;/denchmark-link&gt;
 Will it be possible to provide minimal code snippet with all the operations to expedite the trouble-shooting process. Thanks!
		</comment>
		<comment id='4' author='maxima120' date='2019-06-18T21:20:54Z'>
		Ok but it's there already. Run "model" excerpt and then "training"
		</comment>
		<comment id='5' author='maxima120' date='2019-06-21T17:38:04Z'>
		The code snippet attached here is complete. Can you please make a GitHub gist or Google Colab with your example? Thanks!
		</comment>
		<comment id='6' author='maxima120' date='2019-06-22T15:34:32Z'>
		this reproduces the issue:
&lt;denchmark-link:https://gist.github.com/maxima120/d1057a0e4bbf2ae2a1434dad57999a3f&gt;https://gist.github.com/maxima120/d1057a0e4bbf2ae2a1434dad57999a3f&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='maxima120' date='2019-06-28T00:43:12Z'>
		&lt;denchmark-link:https://github.com/maxima120&gt;@maxima120&lt;/denchmark-link&gt;
 I've reassigned it to folks more familiar with the code, but with TensorFlow 1.14, can you try distribution strategy instead? That is a more complete implementation: &lt;denchmark-link:https://www.tensorflow.org/guide/distribute_strategy#using_tfdistributestrategy_with_keras&gt;https://www.tensorflow.org/guide/distribute_strategy#using_tfdistributestrategy_with_keras&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='maxima120' date='2019-07-08T16:18:56Z'>
		Sorry for the delay. It looks like train_on_batch hasn't been overridden properly in the keras_to_tpu_model code. As Frank mentions, it would great if you can try using the DistributionStrategy approach instead: that has better support as of TF 1.14.
		</comment>
		<comment id='9' author='maxima120' date='2019-07-09T11:31:31Z'>
		Got error using tf.distribute instead of contribute: train_on_batch is not supported for models distributed with tf.distribute.Strategy.
Used GCP image: Debian with TF 1.14.0-rc0
Code (not sure if I am doing it right):
&lt;denchmark-code&gt;resolver = tf.distribute.cluster_resolver.TPUClusterResolver([TPU_ADDRESS1])
tf.tpu.experimental.initialize_tpu_system(resolver)
tpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)

# build model
with tpu_strategy.scope():
    model = tf.keras.Sequential()
    model.add(layers.LSTM(neurons, input_shape=(window_size, inputs_n), return_sequences=True)) 
    model.add(layers.LSTM(neurons))
    model.add(layers.Dense(neurons, activation='relu'))
    model.add(layers.Dense(outputs_n, activation=activation))

    opt = tf.train.RMSPropOptimizer(learning_rate)

    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])

    X,y = split_sequence(features, labels, window_size)
    print('X shape:', X.shape, 'Y shape:', y.shape)
    for slide in range(window_size):
        x1, y1 = X[slide], y[slide]
        x2, y2 = x1.reshape(1,window_size,inputs_n), y1.reshape(1,outputs_n)
        model.train_on_batch(x2,y2)
&lt;/denchmark-code&gt;

got output:
&lt;denchmark-code&gt;Using TensorFlow backend.
WARNING: Logging before flag parsing goes to stderr.
W0709 11:29:14.795013 140573481281280 deprecation.py:506] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
X shape: (9000, 1000, 7) Y shape: (9000, 4)
---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
&lt;ipython-input-5-0f3e39e3b63e&gt; in &lt;module&gt;
     33         x1, y1 = X[slide], y[slide]
     34         x2, y2 = x1.reshape(1,window_size,inputs_n), y1.reshape(1,outputs_n)
---&gt; 35         model.train_on_batch(x2,y2)
     36 
     37 print(model.metrics_names)

/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)
   1146     if (self._distribution_strategy and
   1147         distribution_strategy_context.in_cross_replica_context()):
-&gt; 1148       raise NotImplementedError('`train_on_batch` is not supported for models '
   1149                                 'distributed with tf.distribute.Strategy.')
   1150     # Validate and standardize user data.

NotImplementedError: `train_on_batch` is not supported for models distributed with tf.distribute.Strategy.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='maxima120' date='2019-07-27T19:36:26Z'>
		&lt;denchmark-link:https://github.com/rjpower&gt;@rjpower&lt;/denchmark-link&gt;
 - I tried your suggestion but got similar error () - see above.. any other suggestions?
		</comment>
		<comment id='11' author='maxima120' date='2019-07-29T17:09:20Z'>
		&lt;denchmark-link:https://github.com/rjpower&gt;@rjpower&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/frankchn&gt;@frankchn&lt;/denchmark-link&gt;
 train_on_batch is broken using with tpu distribution strategy on tensorflow 1.14.0
please check this link &lt;denchmark-link:https://stackoverflow.com/questions/56741126/using-train-on-batch-on-colab-tpu-new-tensorflow-version-1-14&gt;https://stackoverflow.com/questions/56741126/using-train-on-batch-on-colab-tpu-new-tensorflow-version-1-14&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='maxima120' date='2019-07-29T17:19:06Z'>
		Thank you. But this is about TF 1.13
I am not going to use 1.14 ever. I am just waiting for 2.0 being mature and TPUs are implemented..
They are moving like facebook now - move fast break things (translation to English - no quality just ticking checkboxes in managerial reports)
		</comment>
		<comment id='13' author='maxima120' date='2019-07-30T00:39:03Z'>
		Sadly, the original Keras API you're using was always intended as an experimental stop-gap (hence being marked as @experimental in the API signature). DistributionStrategy is intended as it's replacement. I'm not sure at the moment if the missing support for train_on_batch is "we haven't gotten to it yet", or if it's "we can't get to it".
I can check on that, but I think in the meantime, the easiest thing to do (albeit a bit of a kludge)  is to try  API and give it a single batch. For more custom use cases, I believe the intended idea is a "custom training loop". It looks something like this [&lt;denchmark-link:https://github.com/tensorflow/tpu/blob/4d7f7bf76df200a6dd74b01a6dbb95d8b013ecfe/models/common/distributed_executor.py#L165&gt;code&lt;/denchmark-link&gt;
]:
&lt;denchmark-code&gt;   def train_one_step(inputs):
        inputs, labels = inputs

        with tf.GradientTape() as tape:
          logits = model(inputs, training=True)
          prediction_loss = loss_fn(labels, logits)
          logging.info('debug prediction_loss %s', prediction_loss)
          loss = tf.reduce_mean(prediction_loss)
          loss = loss / strategy.num_replicas_in_sync
          return loss

        grads = tape.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(zip(grads, model.trainable_variables))

      per_replica_losses = strategy.experimental_run_v2(_replicated_step, args=(next(iterator),))
      return strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_losses, axis=None)
&lt;/denchmark-code&gt;

RE timelines: I'm inclined to agree -- TF 2.0 should have a more mature distribution strategy and better support for Keras TPUs. FWIW, Keras TPU support has never been a supported feature prior to TF 2.0 (though I can see good arguments for why it should have been prioritized earlier).
		</comment>
		<comment id='14' author='maxima120' date='2019-07-30T22:07:47Z'>
		Thank you I will try it.
Re: TF 2.0 - I would gladly switch but I read 2.0 beta doesn't have tpu support whatsoever. Is it wrong?
		</comment>
		<comment id='15' author='maxima120' date='2019-08-01T19:00:55Z'>
		I know that 2.0 final is expected to have full TPU support. That said,
development is still under way and many things aren't quite ready yet. I'd
wait until at least the first RC before trying anything TPU+2.0 related in
earnest.

(I don't work closely with the teams so take this with a grain of salt...)
		</comment>
		<comment id='16' author='maxima120' date='2020-01-10T18:00:55Z'>
		Apologies for the long delay: TensorFlow 2.1 is now out, and now supports TPUs via DistributionStrategy. Documentation is somewhat sparse, but here's an example:
&lt;denchmark-link:https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/fashion_mnist.ipynb&gt;https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/fashion_mnist.ipynb&lt;/denchmark-link&gt;

		</comment>
		<comment id='17' author='maxima120' date='2020-01-10T18:00:58Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29839&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29839&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>