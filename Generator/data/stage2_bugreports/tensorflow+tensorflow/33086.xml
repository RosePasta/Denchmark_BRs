<bug id='33086' author='gkvoelkl' open_date='2019-10-06T16:49:42Z' closed_time='2019-10-13T17:30:16Z'>
	<summary>Distributed Keras MultiWorkerMirroredStrategy failed</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): stock example modified
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.6
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.0.0
Python version: 3.7

Describe the current behavior
Error when using MultiWorkerMirroredStrategy
Describe the expected behavior
No Error
Code to reproduce the issue
&lt;denchmark-code&gt;# worker.py
import tensorflow as tf
import os
import json

import tensorflow_datasets as tfds
from tensorflow import keras

"""
Shell 1:

TF_CONFIG='{"cluster": {"worker": ["localhost:12345", "localhost:56789"]}, "task": {"index": 0, "type": "worker"}}' python worker.py

Shell 2:
TF_CONFIG='{"cluster": {"worker": ["localhost:12345", "localhost:56789"]}, "task": {"index": 1, "type": "worker"}}' python worker.py
"""

print('tf {}'.format(tf.__version__))

strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()

tfds.disable_progress_bar()

BUFFER_SIZE = 10000
NUM_WORKERS = 2
GLOBAL_BATCH_SIZE = 64 * NUM_WORKERS

def scale(image, label):
    image = tf.cast(image, tf.float32)
    image /= 255
    return image, label

datasets, info = tfds.load(name='mnist',
                           with_info=True,
                           as_supervised=True)

train_datasets_unbatched = datasets['train'].map(scale).shuffle(BUFFER_SIZE)

train_datasets = train_datasets_unbatched.batch(GLOBAL_BATCH_SIZE)

def build_and_compile_cnn_model():
    model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(32,3,activation='relu',input_shape=(28,28,1)),
        tf.keras.layers.MaxPooling2D(),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(64,activation='relu'),
        tf.keras.layers.Dense(10,activation='softmax')
    ])
    model.compile(
        loss=tf.keras.losses.sparse_categorical_crossentropy,
        optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
        metrics=['accuracy'])
    return model


with strategy.scope():
    model =  build_and_compile_cnn_model()

    model.fit(x=train_datasets, epochs=3)
&lt;/denchmark-code&gt;

Other info / logs
&lt;denchmark-code&gt;2019-10-06 15:34:50.454016: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-10-06 15:34:50.786372: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fdb6d700010 executing computations on platform Host. Devices:
2019-10-06 15:34:50.786394: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2019-10-06 15:34:50.839300: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:258] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:12345, 1 -&gt; localhost:56789}
2019-10-06 15:34:50.840100: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:12345
WARNING: Logging before flag parsing goes to stderr.
W1006 15:34:53.847770 4649092544 distribute_coordinator.py:825] `eval_fn` is not passed in. The `worker_fn` will be used if an "evaluator" task exists in the cluster.
W1006 15:34:53.857074 4649092544 distribute_coordinator.py:829] `eval_strategy` is not passed in. No distribution strategy will be used for evaluation.
W1006 15:34:53.870861 4649092544 distributed_training_utils.py:1163] ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.
Epoch 1/3
    469/Unknown - 20s 42ms/step - loss: 2.1887 - accuracy: 0.28872019-10-06 15:35:13.827666: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext}}]]
469/469 [==============================] - 20s 42ms/step - loss: 2.1887 - accuracy: 0.2887
Epoch 2/3
2019-10-06 15:35:15.788116: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-10-06 15:35:15.788143: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-10-06 15:35:15.788208: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:234 : Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-10-06 15:35:15.788223: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
	 [[allreduce_1/CollectiveReduce]]
2019-10-06 15:35:15.789211: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-10-06 15:35:15.789231: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-10-06 15:35:15.789219: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-10-06 15:35:15.789318: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-10-06 15:35:15.789573: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-10-06 15:35:15.789589: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-10-06 15:35:15.789635: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:234 : Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-10-06 15:35:15.790304: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:234 : Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-10-06 15:35:15.790362: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:234 : Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-10-06 15:35:15.797384: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-10-06 15:35:15.797403: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
2019-10-06 15:35:15.797452: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:234 : Out of range: [_Derived_]End of sequence
	 [[{{node IteratorGetNext}}]]
  1/469 [..............................] - ETA: 14:26Traceback (most recent call last):
  File "/Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 123, in run_one_epoch
    batch_outs = execution_function(iterator)
  File "/Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py", line 86, in execution_function
    distributed_function(input_fn))
  File "/Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 457, in __call__
    result = self._call(*args, **kwds)
  File "/Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 487, in _call
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
  File "/Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1823, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File "/Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1141, in _filtered_call
    self.captured_inputs)
  File "/Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1224, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager)
  File "/Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 511, in call
    ctx=ctx)
  File "/Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File "&lt;string&gt;", line 3, in raise_from
tensorflow.python.framework.errors_impl.OutOfRangeError:  [_Derived_]End of sequence
	 [[node IteratorGetNext (defined at /Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]
	 [[allreduce_1/CollectiveReduce]] [Op:__inference_distributed_function_950]

Function call stack:
distributed_function


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "worker.py", line 59, in &lt;module&gt;
    model.fit(x=train_datasets, epochs=3)
  File "/Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py", line 728, in fit
    use_multiprocessing=use_multiprocessing)
  File "/Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py", line 789, in fit
    *args, **kwargs)
  File "/Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py", line 776, in wrapper
    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)
  File "/Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py", line 853, in run_distribute_coordinator
    task_id, session_config, rpc_layer)
  File "/Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py", line 360, in _run_single_worker
    return worker_fn(strategy)
  File "/Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py", line 771, in _worker_fn
    return method(model, **kwargs)
  File "/Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 324, in fit
    total_epochs=epochs)
  File "/Users/xx/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py", line 146, in run_one_epoch
    total_epochs * steps_per_epoch))
TypeError: unsupported operand type(s) for *: 'int' and 'NoneType'
2019-10-06 15:35:16.209044: W tensorflow/core/common_runtime/eager/context.cc:290] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='gkvoelkl' date='2019-10-10T10:33:25Z'>
		Make these changes, should work
&lt;denchmark-code&gt;train_datasets = train_datasets_unbatched.batch(GLOBAL_BATCH_SIZE).repeat()
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;model.fit(x=train_datasets, epochs=3, steps_per_epoch=...)
&lt;/denchmark-code&gt;

Ideally steps_per_epoch should be equal to total_samples//batch_size
		</comment>
		<comment id='2' author='gkvoelkl' date='2019-10-13T17:30:16Z'>
		Thank you
It works
		</comment>
		<comment id='3' author='gkvoelkl' date='2019-10-13T17:30:19Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33086&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33086&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='gkvoelkl' date='2019-12-05T06:06:14Z'>
		Should you put train_datasets = train_datasets_unbatched.batch(GLOBAL_BATCH_SIZE) inside the strategy's scope?
		</comment>
		<comment id='5' author='gkvoelkl' date='2020-04-25T03:43:21Z'>
		
Should you put train_datasets = train_datasets_unbatched.batch(GLOBAL_BATCH_SIZE) inside the strategy's scope?

outside and after.
&lt;denchmark-code&gt;strategy = tf.strategy()
train_dataset = get _dataset()
with stragey..scope():
  model = get_model()
model.fit()
&lt;/denchmark-code&gt;

Make sure you are using TF2.2.0.rec3
		</comment>
	</comments>
</bug>