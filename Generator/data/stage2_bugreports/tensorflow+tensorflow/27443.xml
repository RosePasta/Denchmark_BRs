<bug id='27443' author='buaadf' open_date='2019-04-03T02:34:59Z' closed_time='2019-04-11T10:26:49Z'>
	<summary>Tensorflow lite model inference time increases when adding JNI on Android</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS10.14.3
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: HW P9, Mi8, Oneplus 5, VIVOX9, HuaWei Honor V9....
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below):  1.13.0.dev20190126
Python version: 2.7/3.7(both tryed)

You can collect some of this information using our environment capture &lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with
python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
Describe the current behavior
I use tflite model (quantization-aware training and fully quantized with toco) and deploy on Android for segmentation task. I got correct output of the model, and the inference time is 30ms(1 thread), 17ms(2 threads), 15ms(3 threads). However, when i add some post-processing with C++(JNI), the inference time (only the function Interpreter.run(input, output)  run time)increases to 47ms(1 thread), 34ms(2 threads) , 30ms(3 threads), respectively, and I got the same inference time even though I didn't use the post-processing (just put the jni code in my project).
When I use post-processing with java code, the inference time is 30ms(1 thread), 17ms(2 threads), 15ms(3 threads) again, so I guess JNI would influence the inference time.
Describe the expected behavior
Get same inference time with JNI on Android.
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
just add some jni code to the official demo would got the same issue.
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='buaadf' date='2019-04-03T23:54:40Z'>
		&lt;denchmark-link:https://github.com/buaadf&gt;@buaadf&lt;/denchmark-link&gt;
 Could you provide a code to reproduce bug? what are the commands used to convert model?
Could you try TF1.13.1 (master) and/or TF2.0.0.alpha, or tf-nightly to check whether the bug is persistent with recent TF versions. Thanks!
		</comment>
		<comment id='2' author='buaadf' date='2019-04-11T08:52:53Z'>
		I have tried with 1.13.1 and nightly, the bug is pesistent..
		</comment>
		<comment id='3' author='buaadf' date='2019-04-11T10:26:40Z'>
		I finally found the reason, the demo with and  without jni use armeabi_v7 and arm64_v8a , respectively.
		</comment>
		<comment id='4' author='buaadf' date='2019-04-11T10:26:50Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=27443&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=27443&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>