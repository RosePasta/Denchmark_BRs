<bug id='25469' author='gsutanto' open_date='2019-02-03T10:21:21Z' closed_time='2020-03-14T06:49:14Z'>
	<summary>No converter defined for Switch</summary>
	<description>
This is a similar issue like &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/23397&gt;#23397&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;from tensorflow.python.framework import function
from tensorflow.python.ops.parallel_for.gradients import jacobian as tf_jacobian
import numpy as np
import numpy.matlib
import tensorflow as tf

print(tf.__version__)

my_graph = tf.Graph()
with my_graph.as_default():
    x = tf.placeholder(tf.float32, shape=(None,9))
    tf_is_training_ph = tf.placeholder(tf.bool, shape=())
    
    def computeFunc1D(x_1D, is_training):
        x = tf.reshape(x_1D, [1, 9])
        with tf.variable_scope('Test', reuse=tf.AUTO_REUSE):
            dense_out = tf.layers.dense(x, 3, name='dense')
            bn_out = tf.layers.batch_normalization(dense_out, training=is_training, name='bn')
        return tf.reshape(bn_out, [3])
    
    def tensor_jacobian(X, is_training):
        fn_compute_jacobian_1datapt = lambda x: tf_jacobian(computeFunc1D(x, is_training), x)
        J = tf.map_fn(fn_compute_jacobian_1datapt, X)
        return J
    
#    my_tf_jacobian = tensor_jacobian(x, True) # this is working
    my_tf_jacobian = tensor_jacobian(x, tf_is_training_ph) # this is NOT working

with tf.Session(graph=my_graph) as sess:
    tf.global_variables_initializer().run()
    
    x_ = 0.5 * np.random.random((2,9))
    
    [my_tf_jacobian_val
     ] = sess.run([my_tf_jacobian], feed_dict={x: x_, tf_is_training_ph: True})
    
    print "x_ = ", x_
    print "my_tf_jacobian_val = ", my_tf_jacobian_val
    print "my_tf_jacobian_val.shape = ", my_tf_jacobian_val.shape
&lt;/denchmark-code&gt;

This will result in:
&lt;denchmark-code&gt;1.12.0
Traceback (most recent call last):

  File "&lt;ipython-input-2-b20d7560d318&gt;", line 1, in &lt;module&gt;
    runfile('/home/amdgsutanto/Desktop/test_tf_compute_jacobian_w_batch_norm.py', wdir='/home/amdgsutanto/Desktop')

  File "/usr/local/lib/python2.7/dist-packages/spyder/utils/site/sitecustomize.py", line 705, in runfile
    execfile(filename, namespace)

  File "/usr/local/lib/python2.7/dist-packages/spyder/utils/site/sitecustomize.py", line 94, in execfile
    builtins.execfile(filename, *where)

  File "/home/amdgsutanto/Desktop/test_tf_compute_jacobian_w_batch_norm.py", line 27, in &lt;module&gt;
    my_tf_jacobian = tensor_jacobian(x, tf_is_training_ph) # this is NOT working

  File "/home/amdgsutanto/Desktop/test_tf_compute_jacobian_w_batch_norm.py", line 23, in tensor_jacobian
    J = tf.map_fn(fn_compute_jacobian_1datapt, X)

  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/functional_ops.py", line 494, in map_fn
    maximum_iterations=n)

  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py", line 3291, in while_loop
    return_same_structure)

  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py", line 3004, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)

  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py", line 2939, in _BuildLoop
    body_result = body(*packed_vars_for_body)

  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py", line 3260, in &lt;lambda&gt;
    body = lambda i, lv: (i + 1, orig_body(*lv))

  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/functional_ops.py", line 483, in compute
    packed_fn_values = fn(packed_values)

  File "/home/amdgsutanto/Desktop/test_tf_compute_jacobian_w_batch_norm.py", line 22, in &lt;lambda&gt;
    fn_compute_jacobian_1datapt = lambda x: tf_jacobian(computeFunc1D(x, is_training), x)

  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/parallel_for/gradients.py", line 59, in jacobian
    pfor_outputs = control_flow_ops.pfor(loop_fn, output_size)

  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py", line 129, in pfor
    outputs.append(converter.convert(loop_fn_output))

  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/parallel_for/pfor.py", line 1077, in convert
    output = self._convert_helper(y)

  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/parallel_for/pfor.py", line 1223, in _convert_helper
    "which may run slower" % (y_op.type, y_op, converted_inputs))

ValueError: No converter defined for Switch
name: "map/while/loop_body/gradients/map/while/Test/bn/cond/Merge_grad/cond_grad"
op: "Switch"
input: "map/while/loop_body/gradients/map/while/Test/bn/batchnorm/mul_2_grad/Mul"
input: "map/while/Test/bn/cond/pred_id"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "_class"
  value {
    list {
      s: "loc:@map/while/loop_body/gradients/map/while/Test/bn/batchnorm/mul_2_grad/Mul"
    }
  }
}

inputs: [WrappedTensor(t=&lt;tf.Tensor 'map/while/loop_body/gradients/map/while/Test/bn/batchnorm/mul_2_grad/Mul/pfor/Mul:0' shape=(3, 3) dtype=float32&gt;, is_stacked=True, is_sparse_stacked=False), WrappedTensor(t=&lt;tf.Tensor 'map/while/Test/bn/cond/pred_id:0' shape=() dtype=bool&gt;, is_stacked=False, is_sparse_stacked=False)]. 
Either add a converter or set --op_conversion_fallback_to_while_loop=True, which may run slower
&lt;/denchmark-code&gt;

Would you mind fixing this, similar like &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/23397&gt;#23397&lt;/denchmark-link&gt;
 ?
P.S.: If the fix made it to nightly build, how to update it on my side? Is it just re-installing (e.g. with pip)? Thanks a lot!
	</description>
	<comments>
		<comment id='1' author='gsutanto' date='2019-02-04T02:05:12Z'>
		So, I came along a simple hack to get around this issue (which sort of works):
&lt;denchmark-code&gt;from tensorflow.python.framework import function
from tensorflow.python.ops.parallel_for.gradients import jacobian as tf_jacobian
import numpy as np
import numpy.matlib
import tensorflow as tf

print(tf.__version__)

my_graph = tf.Graph()
with my_graph.as_default():
    x = tf.placeholder(tf.float32, shape=(None,9))
    tf_is_training_ph = tf.placeholder(tf.bool, shape=())
    
    def computeFunc1D(x_1D, is_training):
        x = tf.reshape(x_1D, [1, 9])
        with tf.variable_scope('Test', reuse=tf.AUTO_REUSE):
            dense_out = tf.layers.dense(x, 3, name='dense')
            bn_out = tf.layers.batch_normalization(dense_out, training=is_training, name='bn')
        return tf.reshape(bn_out, [3])
    
    def computeTensorJacobian(X, is_training):
        fn_compute_jacobian_1datapt = lambda x: tf_jacobian(computeFunc1D(x, is_training), x)
        J = tf.map_fn(fn_compute_jacobian_1datapt, X)
        return J
    
    def computeTensorJacobianOnTraining(): return computeTensorJacobian(x, is_training=True)
    def computeTensorJacobianOffTraining(): return computeTensorJacobian(x, is_training=False)
    my_tf_jacobian = tf.cond(tf_is_training_ph, 
                             computeTensorJacobianOnTraining, 
                             computeTensorJacobianOffTraining)

with tf.Session(graph=my_graph) as sess:
    tf.global_variables_initializer().run()
    
    x_ = 0.5 * np.random.random((2,9))
    print "x_ = ", x_
    
    [my_tf_jacobian_val
     ] = sess.run([my_tf_jacobian], feed_dict={x: x_, tf_is_training_ph: True})
    
    print "my_tf_jacobian_val = ", my_tf_jacobian_val
    
    [my_tf_jacobian_val
     ] = sess.run([my_tf_jacobian], feed_dict={x: x_, tf_is_training_ph: False})
    
    print "my_tf_jacobian_val = ", my_tf_jacobian_val
&lt;/denchmark-code&gt;

But then I got another issue:
&lt;denchmark-code&gt;1.12.0
x_ =  [[0.24 0.49 0.47 0.19 0.33 0.03 0.22 0.03 0.39]
 [0.28 0.24 0.33 0.34 0.34 0.24 0.47 0.09 0.13]]
my_tf_jacobian_val =  [[[0. 0. 0. 0. 0. 0. 0. 0. 0.]
  [0. 0. 0. 0. 0. 0. 0. 0. 0.]
  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]

 [[0. 0. 0. 0. 0. 0. 0. 0. 0.]
  [0. 0. 0. 0. 0. 0. 0. 0. 0.]
  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]]
my_tf_jacobian_val =  [[[-0.59 -0.13  0.57 -0.39 -0.54 -0.22 -0.32  0.05 -0.34]
  [ 0.62  0.16 -0.14 -0.18 -0.19 -0.66  0.51  0.19 -0.09]
  [-0.25 -0.23 -0.39 -0.63 -0.14  0.01 -0.5   0.22  0.61]]

 [[-0.59 -0.13  0.57 -0.39 -0.54 -0.22 -0.32  0.05 -0.34]
  [ 0.62  0.16 -0.14 -0.18 -0.19 -0.66  0.51  0.19 -0.09]
  [-0.25 -0.23 -0.39 -0.63 -0.14  0.01 -0.5   0.22  0.61]]]
&lt;/denchmark-code&gt;

i.e. the computed Jacobian is always 0 during training phase when using batch normalization layer.
Obviously I also want to optimize parameters in the batch norm layer (my real case is I have a loss which is a function of the Jacobian of a neural net with batch norm layer).
Any clue or ideas?
Thanks so much for your time!
		</comment>
		<comment id='2' author='gsutanto' date='2019-02-12T05:40:56Z'>
		Hi, just checking: is there any update on this? How long usually this will get resolved? I am working on a paper deadline around the end of next week that depends on this, and I would deeply appreciate if this issue could be fixed soon...
		</comment>
		<comment id='3' author='gsutanto' date='2019-06-14T02:27:00Z'>
		Has this been resolved?
		</comment>
		<comment id='4' author='gsutanto' date='2019-06-14T03:47:16Z'>
		&lt;denchmark-link:https://github.com/agarwal-ashish&gt;@agarwal-ashish&lt;/denchmark-link&gt;
 ptal.
		</comment>
		<comment id='5' author='gsutanto' date='2019-06-14T06:14:06Z'>
		Looks like you are applying batch normalization on input of size 1 with training=True. So the output of that layer will be normalized to a constant value which means the jacobian should be 0. You can print the output of the batch normalization layer to verify.
Also looks like you are doing a map_fn with jacobian computation. You could have used batch_jacobian instead which will be more efficient. However in this case, since each output depends on each input due to batch normalization, so you may actually may need the jacobian of the full output wrt the full input.
		</comment>
		<comment id='6' author='gsutanto' date='2019-06-14T06:14:08Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=25469&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=25469&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='gsutanto' date='2019-06-14T07:21:17Z'>
		&lt;denchmark-link:https://github.com/agarwal-ashish&gt;@agarwal-ashish&lt;/denchmark-link&gt;
 Thank you for response. But I still fail to understand, what is the issue here. I followed all your fix suggestion, but I am getting similar problem. I believe the issue is with the tf.layers.batch_normalization() function.
Can you please look at my code and tell me what's wrong.
&lt;denchmark-code&gt;from __future__ import print_function
from __future__ import division
import tensorflow as tf
import numpy as np
import os

from tensorflow.python.ops.parallel_for.gradients import batch_jacobian, jacobian

def CNN(input, is_training=True, output_channels=3):
    with tf.variable_scope('block1'):
        output = tf.layers.conv2d(input, 64, 3, padding='same', activation=tf.nn.relu)
    with tf.variable_scope('block2'):
        output = tf.layers.conv2d(output, 64, 3, padding='same', name='conv2', use_bias=False)
        output = tf.nn.relu(tf.layers.batch_normalization(output, training=is_training, name='bn2'))
        #output = tf.nn.relu(output)
    with tf.variable_scope('block3'):
        output = tf.layers.conv2d(output, output_channels, 3, padding='same')
    return input-output

def main(_):
    X = tf.placeholder(tf.float32, [None, None, None, 3]) # shape = [batch_size, W, H, 3]

    is_training = tf.placeholder(tf.bool, name='is_training') #for batchnorm

    with tf.variable_scope('my_CNN'):
        output = CNN(X, is_training=is_training)

    scalar = tf.reduce_sum(output, axis=[1,2,3]) # shape becomes [batch_size]
    loss = tf.reduce_sum(batch_jacobian(scalar, X))

    sess = tf.Session()

    #optimizer
    optimizer = tf.train.AdamOptimizer(0.001, name='AdamOptimizer')

    #for batchnorm
    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
    with tf.control_dependencies(update_ops):
        train_op = optimizer.minimize(loss)

    init = tf.global_variables_initializer()
    sess.run(init)

    sess.run(train_op, feed_dict={X: np.random.normal(size = (100, 28, 28, 3)), is_training: True})

    my_loss = sess.run(loss, feed_dict={X: np.random.normal(size = (100, 28, 28, 3)), is_training: False})

    print("Loss: ", my_loss)


if __name__ == '__main__':
    os.environ['CUDA_VISIBLE_DEVICES']='1'
    tf.app.run()

&lt;/denchmark-code&gt;

This results in the following error:
&lt;denchmark-code&gt;  File "github_example.py", line 53, in &lt;module&gt;
    tf.app.run()
  File "/home/shaka/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py", line 125, in run
    _sys.exit(main(argv))
  File "github_example.py", line 29, in main
    loss = tf.reduce_sum(batch_jacobian(scalar, X))
  File "/home/shaka/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/parallel_for/gradients.py", line 118, in batch_jacobian
    pfor_output = control_flow_ops.pfor(loop_fn, output_row_size)
  File "/home/shaka/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py", line 122, in pfor
    outputs.append(converter.convert(loop_fn_output))
  File "/home/shaka/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/parallel_for/pfor.py", line 1075, in convert
    output = self._convert_helper(y)
  File "/home/shaka/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/parallel_for/pfor.py", line 1221, in _convert_helper
    "which may run slower" % (y_op.type, y_op, converted_inputs))
ValueError: No converter defined for Switch
name: "loop_body/gradients/my_CNN/block2/bn2/cond/Merge_grad/cond_grad"
op: "Switch"
input: "loop_body/gradients/my_CNN/block2/Relu_grad/ReluGrad"
input: "my_CNN/block2/bn2/cond/pred_id"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "_class"
  value {
    list {
      s: "loc:@loop_body/gradients/my_CNN/block2/Relu_grad/ReluGrad"
    }
  }
}

inputs: [WrappedTensor(t=&lt;tf.Tensor 'loop_body/gradients/my_CNN/block2/Relu_grad/ReluGrad/pfor/ReluGrad:0' shape=(?, ?, ?, ?, 64) dtype=float32&gt;, is_stacked=True, is_sparse_stacked=False), WrappedTensor(t=&lt;tf.Tensor 'my_CNN/block2/bn2/cond/pred_id:0' shape=&lt;unknown&gt; dtype=bool&gt;, is_stacked=False, is_sparse_stacked=False)]. 
Either add a converter or set --op_conversion_fallback_to_while_loop=True, which may run slower
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='gsutanto' date='2019-06-14T07:58:49Z'>
		The earlier suggestion was to address jacobian being 0 for the second version of the code. Regarding error about missing converter, this is happening because jacobian does not currently support converting tf.cond inside the vectorization logic. When is_training is set to True instead of a placeholder, that tf.cond will not be present and the code should work. Implementing tf.cond converter is a bit involved and may need some time. I'd suggest avoiding the placeholder for is_training if possible.
		</comment>
		<comment id='9' author='gsutanto' date='2020-03-14T06:49:14Z'>
		Note that tf.cond is now supported in vectorization.
		</comment>
		<comment id='10' author='gsutanto' date='2020-03-14T06:49:15Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25469&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25469&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>