<bug id='11182' author='kwotsin' open_date='2017-06-30T16:09:20Z' closed_time='2018-01-29T23:51:25Z'>
	<summary>Quantize weights causes accuracy to plunge when run in mobile but not in computers?</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
TensorFlow installed from (source or binary): Source
TensorFlow version (use command below): 1.2.1
Python version:
Bazel version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:
Exact command to reproduce:

&lt;denchmark-code&gt;/home/kwotsin/tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
--in_graph=./frozen_inception_resnet_v2_for_mobile.pb \
--out_graph=./quantized_inception_resnet_v2_for_mobile_NEW.pb \
--inputs='Placeholder_only' \
--outputs='InceptionResnetV2/Logits/Predictions' \
--transforms='
  add_default_attributes
  strip_unused_nodes(type=float, shape="1,299,299,3")
  remove_nodes(op=Identity, op=CheckNumerics)
  fold_constants(ignore_errors=true)
  fold_batch_norms
  fold_old_batch_norms
  quantize_weights
  strip_unused_nodes
  sort_by_execution_order'
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

Using the above quantization method, the quantization tool works so well that there is hardly a noticeable difference in accuracy drop (less than 0.5%) for a model like inception v3, with a 1/4 size reduction and slightly faster speed. However, when using the exact same files to be run on mobile, the performance gets so poor that there's more than 70-80% accuracy decrease. I'm unsure whether the issue lies with the quantization not getting optimized on mobile architectures (ARM instead of the usual desktop amd architecture), or whether there is a problem in the operations for the tensorflow mobile library.
Note that quantize_nodes is totally unusable. When used to quantize the model, the model size increases a little and then causes the app to crash instantly. The error log produced when using quantize_nodes is this:
&lt;denchmark-code&gt;07-01 00:07:01.760 28272-28357/com.mindorks.tensorflowexample E/art: No implementation found for long org.tensorflow.contrib.android.RunStats.allocate() (tried Java_org_tensorflow_contrib_android_RunStats_allocate and Java_org_tensorflow_contrib_android_RunStats_allocate__)
07-01 00:07:03.508 28272-28357/com.mindorks.tensorflowexample A/libc: Fatal signal 11 (SIGSEGV), code 1, fault addr 0x0 in tid 28357 (pool-1-thread-1)
&lt;/denchmark-code&gt;

Also, for almost every model I ran, the following error appeared:
No implementation found for long org.tensorflow.contrib.android.RunStats.allocate()
What does this mean and how could I resolve it?
FYI: Not sure if it makes a difference, but when I built my lib_tensorflow_inference.so file and the JAR file for using the TF library on mobile, the tensorflow version was cloned from the master branch and not git checked out. Would this make a difference?
Further weird phenomenon:
Although I built my TF from source and bazel built the graph transform tool, the following warnings still appear:
&lt;denchmark-code&gt;2017-07-01 00:05:19.612228: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-07-01 00:05:19.612262: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-07-01 00:05:19.612278: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-07-01 00:05:19.612282: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-07-01 00:05:19.612290: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
&lt;/denchmark-code&gt;

Thank you.
	</description>
	<comments>
		<comment id='1' author='kwotsin' date='2017-06-30T18:26:09Z'>
		Could you please provide us with all available information about the mobile architecture/platform you're targeting?
		</comment>
		<comment id='2' author='kwotsin' date='2017-06-30T19:21:47Z'>
		Oh yes indeed I'm working on an android LG G3 phone that's running ARM architecture I believe.
		</comment>
		<comment id='3' author='kwotsin' date='2017-07-03T09:28:22Z'>
		I might have just solved this problem by looking at what &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/8897&gt;#8897&lt;/denchmark-link&gt;
 did. Basically, I had to include the argument  when building the  file. This reduced the library size for android and made the performance almost exactly as expected when run on a desktop.
Here is the full command to build libtensorflow_inference.so:
&lt;denchmark-code&gt;bazel build -c opt --copt="-DTENSORFLOW_DISABLE_META"  //tensorflow/contrib/android:libtensorflow_inference.so    --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=armeabi-v7a
&lt;/denchmark-code&gt;

and then the JAR file subsequently:
&lt;denchmark-code&gt;bazel build //tensorflow/contrib/android:android_tensorflow_inference_java
&lt;/denchmark-code&gt;

Further, I think there are two problematic transformations: Remove Nodes and Quantize Nodes. Using either one causes the accuracy to be reduced or inference timing to increase by around 5x.
The remaining 8 transformations are what I found as optimal:
&lt;denchmark-code&gt;/home/kwotsin/tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
--in_graph=./frozen_inception_resnet_v2_for_mobile.pb \
--out_graph=./quantized_inception_resnet_v2_for_mobile.pb \
--inputs='Placeholder_only' \
--outputs='InceptionResnetV2/Logits/Predictions' \
--transforms='
  add_default_attributes
  strip_unused_nodes(type=float, shape="1,299,299,3")
  fold_constants(ignore_errors=true)
  fold_batch_norms
  fold_old_batch_norms
  quantize_weights
  strip_unused_nodes
  sort_by_execution_order'
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='kwotsin' date='2017-07-05T12:48:54Z'>
		&lt;denchmark-link:https://github.com/kwotsin&gt;@kwotsin&lt;/denchmark-link&gt;

It seems that the remove transformation also drops output nodes &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/8181&gt;#8181&lt;/denchmark-link&gt;
.
And sometimes removing them damages certain layers (e.g. batch norms) which happen to use some Identity ops which cannot be removed unconditionally.
		</comment>
		<comment id='5' author='kwotsin' date='2017-07-06T05:03:24Z'>
		Thanks for the explanation! Hopefully there's a fix coming soon
		</comment>
		<comment id='6' author='kwotsin' date='2017-07-06T14:57:57Z'>
		Trying to build the inference library like &lt;denchmark-link:https://github.com/kwotsin&gt;@kwotsin&lt;/denchmark-link&gt;
 fails for me:
&lt;denchmark-code&gt;ERROR: /home/androbin/.cache/bazel/_bazel_androbin/0614ddb178f91f25438b47fee5f001b7/external/protobuf/BUILD:73:1: C++ compilation of rule '@protobuf//:protobuf_lite' failed: false failed: error executing command 
  (cd /home/androbin/.cache/bazel/_bazel_androbin/0614ddb178f91f25438b47fee5f001b7/execroot/org_tensorflow &amp;&amp; \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    CUDNN_INSTALL_PATH=/usr/local/cuda \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3.5 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=5.0 \
    TF_CUDA_VERSION=8.0 \
    TF_CUDNN_VERSION=6 \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL=0 \
  /bin/false -DTENSORFLOW_DISABLE_META -MD -MF bazel-out/stub_armeabi-v7a-py3-opt/bin/external/protobuf/_objs/protobuf_lite/external/protobuf/src/google/protobuf/io/zero_copy_stream.pic.d '-frandom-seed=bazel-out/stub_armeabi-v7a-py3-opt/bin/external/protobuf/_objs/protobuf_lite/external/protobuf/src/google/protobuf/io/zero_copy_stream.pic.o' -fPIC -iquote external/protobuf -iquote bazel-out/stub_armeabi-v7a-py3-opt/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/stub_armeabi-v7a-py3-opt/genfiles/external/bazel_tools -isystem external/protobuf/src -isystem bazel-out/stub_armeabi-v7a-py3-opt/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -DHAVE_PTHREAD -Wall -Wwrite-strings -Woverloaded-virtual -Wno-sign-compare -Wno-unused-function -c external/protobuf/src/google/protobuf/io/zero_copy_stream.cc -o bazel-out/stub_armeabi-v7a-py3-opt/bin/external/protobuf/_objs/protobuf_lite/external/protobuf/src/google/protobuf/io/zero_copy_stream.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
Target //tensorflow/contrib/android:libtensorflow_inference.so failed to build
&lt;/denchmark-code&gt;

see &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/10579&gt;#10579&lt;/denchmark-link&gt;
 for details
		</comment>
		<comment id='7' author='kwotsin' date='2017-07-07T18:42:11Z'>
		&lt;denchmark-link:https://github.com/kwotsin&gt;@kwotsin&lt;/denchmark-link&gt;
 is the bug resolved for you? The other threads that &lt;denchmark-link:https://github.com/Androbin&gt;@Androbin&lt;/denchmark-link&gt;
 references (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/8181&gt;#8181&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/10579&gt;#10579&lt;/denchmark-link&gt;
) suggest that there are still some issues being worked out.
		</comment>
		<comment id='8' author='kwotsin' date='2017-07-10T06:09:30Z'>
		&lt;denchmark-link:https://github.com/Androbin&gt;@Androbin&lt;/denchmark-link&gt;
 not sure if it matters, but I am using gcc and not clang when I configured my TF version, + python 2.7; also, I git checked out the master branch instead of r1.2 for the build.
&lt;denchmark-link:https://github.com/cy89&gt;@cy89&lt;/denchmark-link&gt;
 yes I think the bug is resolved for me so far. I am seeing rather good results coming from the quantization when I remove those two problematic transformation -  and 
		</comment>
		<comment id='9' author='kwotsin' date='2017-07-10T13:56:11Z'>
		&lt;denchmark-link:https://github.com/kwotsin&gt;@kwotsin&lt;/denchmark-link&gt;

I am currently using gcc, but thanks for the hint!
The version of python (3.5.2 in my case) should not matter?
Last pulled git commit was &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/454f704b094dee26a31d073c02f488cc8dce5944&gt;454f704&lt;/denchmark-link&gt;
 from master, will update.
		</comment>
		<comment id='10' author='kwotsin' date='2017-07-10T20:09:33Z'>
		&lt;denchmark-link:https://github.com/Androbin&gt;@Androbin&lt;/denchmark-link&gt;
 please let us know how this goes: I will close the issue if it's solved for you.
		</comment>
		<comment id='11' author='kwotsin' date='2017-07-11T20:20:55Z'>
		Rebuilt everything from latest master, last error section changed slightly:
I found zero_copy_stream to be replaced by coded_stream but nothing else changed.
		</comment>
		<comment id='12' author='kwotsin' date='2017-07-12T14:41:42Z'>
		Pete, who is best to look at graph_transform issues, other than you?
		</comment>
		<comment id='13' author='kwotsin' date='2017-07-12T23:45:20Z'>
		In this case, I believe the problem is related to metagemm, since &lt;denchmark-link:https://github.com/kwotsin&gt;@kwotsin&lt;/denchmark-link&gt;
 references  as a fix for a similar problem, and the symptoms make sense. Passing to Maciek to investigate that side.
		</comment>
		<comment id='14' author='kwotsin' date='2017-07-13T09:12:17Z'>
		&lt;denchmark-link:https://github.com/Androbin&gt;@Androbin&lt;/denchmark-link&gt;
 It is quite strange the commands didn't work for you. Perhaps it is a system issue? Please let me know if you require information from my system build; I'll be glad to help out.
		</comment>
		<comment id='15' author='kwotsin' date='2017-07-13T14:56:53Z'>
		&lt;denchmark-link:https://github.com/petewarden&gt;@petewarden&lt;/denchmark-link&gt;

That sounds plausible, could you explain, what  should do and what it is that is not working as intended? And are there any shortcomings of not having it running?
&lt;denchmark-link:https://github.com/kwotsin&gt;@kwotsin&lt;/denchmark-link&gt;

That is improbable, I have a pretty clean setup, not really any other major complaints. TensorFlow and its other tools worked just well till now. But quantization, selective registration and a few other tools have been flawky (for me and others) for quite some time now.
		</comment>
		<comment id='16' author='kwotsin' date='2017-07-14T02:31:33Z'>
		Hi there!
Metagemm is a library of high performance Arm32/64 kernels for quantized matrix multiplication, convolutions and related primitives. If turned off reference implementations will be used for some ops so you could expect lower performance.
I believe I might have let an old bug slip past. Metagemm right now has an aggressive code path for matrix multiplication and convolution with 'dot' dimension under 2048. And there is no test to actually check if k &lt;= 2048.  If k &gt; 2048, then depending on actual data, that you have, some aggregators might overflow.
I should have a patch tomorrow, so you can see if that is actually the case.
		</comment>
		<comment id='17' author='kwotsin' date='2017-07-17T14:25:33Z'>
		Ok the patch is in the Google repository and it should be on github soon.
		</comment>
		<comment id='18' author='kwotsin' date='2017-08-27T08:37:06Z'>
		&lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/bdb2967a298236e24011405907cd19737386934e&gt;bdb2967&lt;/denchmark-link&gt;

		</comment>
		<comment id='19' author='kwotsin' date='2017-11-29T00:35:18Z'>
		Is this resolved?
		</comment>
		<comment id='20' author='kwotsin' date='2017-11-29T04:00:11Z'>
		&lt;denchmark-link:https://github.com/aselle&gt;@aselle&lt;/denchmark-link&gt;
 No, it is not resolved. Working with the mobilenet quantization, still need to add the argument --copt="-DTENSORFLOW_DISABLE_META" when building the "libtensorflow_inference.so" file. But this made the performance slower. When using the nightly-android builds, the result is different between mobile and PC.
		</comment>
		<comment id='21' author='kwotsin' date='2017-12-20T19:06:11Z'>
		It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue? Please update the label and/or status accordingly.
		</comment>
		<comment id='22' author='kwotsin' date='2018-01-02T06:38:35Z'>
		Anyone having the low accuracy problem on iOS? I use transform_graph on a retrain model (based on Inception v3) and label_image works fine on the quantized model transform_dog_retrained.pb (same accuracy as with the pre-transformed model dog_retrained.pb), but the iOS app gives bad recognition results. The iOS app gives the same results as label_image when using the retrained (but before transformed) model dog_retrained.pb.
Commands to retrain, transform, and call label_image are as follows:
&lt;denchmark-code&gt;python tensorflow/examples/image_retraining/retrain.py   \
--model_dir=/tf_files/inception-v3   \
--output_graph=/tf_files/retrained_models/dog_retrained.pb   \
--output_labels=/tf_files/retrained_models/dog_retrained_labels.txt   \
--image_dir ~/Downloads/dog_images   \
--bottleneck_dir=/tf_files/dogs_bottleneck
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;bazel-bin/tensorflow/tools/graph_transforms/transform_graph  \
 --in_graph=/tf_files/retrained_models/dog_retrained.pb   \
--out_graph=/tf_files/retrained_models/transform_dog_retrained.pb   \
--inputs='Mul'   --outputs='final_result'   \
--transforms='
  add_default_attributes
  strip_unused_nodes(type=float, shape="1,299,299,3")
  fold_constants(ignore_errors=true)
  fold_batch_norms
  fold_old_batch_norms
  quantize_weights
  strip_unused_nodes
  sort_by_execution_order'
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;bazel-bin/tensorflow/examples/label_image/label_image \
--graph=/tf_files/retrained_models/transform_dog_retrained.pb \
--image=/tf_files/lab.jpg --input_layer=Mul --output_layer=final_result \
--labels=/tf_files/retrained_models/dog_retrained_labels.txt
&lt;/denchmark-code&gt;

Anyone has any idea how to fix the iOS issue? Btw, if I use the older bazel build tensorflow/python/tools:strip_unused then python tensorflow/tools/quantization/quantize_graph.py, the quantized model generated this way has good results (same accuracy as label_image) on iOS. But I hope to use the new transform_graph method instead of the old way.
What's the right way to use transform_graph so the generated model works on iOS with about the same accuracy as usinglabel_image?
Thanks!
		</comment>
		<comment id='23' author='kwotsin' date='2018-01-02T19:02:18Z'>
		&lt;denchmark-link:https://github.com/petewarden&gt;@petewarden&lt;/denchmark-link&gt;
, do we have someone who can take a look at this since maciek is no longer working on it.
		</comment>
		<comment id='24' author='kwotsin' date='2018-01-04T07:42:38Z'>
		Can &lt;denchmark-link:https://github.com/petewarden&gt;@petewarden&lt;/denchmark-link&gt;
 or someone please take a look? Also, a related problem: I used one quantized MobileNet model to do the retraining:
&lt;denchmark-code&gt;python tensorflow/examples/image_retraining/retrain.py \
  --output_graph=/tf_files/dog_retrained_mobilenet10_224.pb \
  --output_labels=/tf_files/dog_retrained_labels_mobilenet.txt \
  --image_dir ~/Downloads/Images \
  --bottleneck_dir=/tf_files/dogs_bottleneck_mobilenet \
  --architecture mobilenet_1.0_224_quantized
&lt;/denchmark-code&gt;

and then using the label_image script to verify the model's accuracy, which looks pretty good:
&lt;denchmark-code&gt;bazel-bin/tensorflow/examples/label_image/label_image \
--graph=/tf_files/dog_retrained_mobilenet10_224.pb \
--image=/tmp/lab1.jpg \
--input_layer=input \
--output_layer=final_result \
--labels=/tf_files/dog_retrained_labels_mobilenet.txt \
--input_height=224 \
--input_width=224 \
--input_mean=128 \
--input_std=128
&lt;/denchmark-code&gt;

But when I use the retrained model in both iOS and Android, I have to set the threshold to 0.01 in order to see top recognition results, which look ok except the very low confidence values - for example:
&lt;denchmark-code&gt;Predictions: 41 0.0261  n02099712 labrador retriever
60 0.024  n02106166 border collie
34 0.0227  n02099849 chesapeake bay retriever
31 0.0188  n02088238 basset
42 0.0174  n02091032 italian greyhound
&lt;/denchmark-code&gt;

The non-quantized version of a retrained MobileNet model (using --architecture mobilenet_1.0_224 on retraining) shows good top results with high confidence values for the results in iOS and Android - for example:
&lt;denchmark-code&gt;
Predictions: 5 0.328  n02091134 whippet
41 0.307  n02099712 labrador retriever
20 0.0988  n02088364 beagle
105 0.0815  n02087394 rhodesian ridgeback
32 0.0209  n02090379 redbone
&lt;/denchmark-code&gt;

I'm using TensorFlow 1.4 and the iOS simple example and the Android TF Classify example.
Any ideas on how to fix this and make retrained quantized MobileNet models work in iOS and Android? Thanks!
		</comment>
		<comment id='25' author='kwotsin' date='2018-01-18T19:14:13Z'>
		Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='26' author='kwotsin' date='2018-01-29T23:51:25Z'>
		We're now moving over to TF Lite as the recommended way to run quantized models, partly because it is less complex to integrate eight-bit with than classic TensorFlow. We have a demo of running the MobileNet image classification network on Android, and there should soon be full documentation on training it. I'm going to close this bug as not likely to be fixed, since our efforts will be focused on the TF Lite path.
		</comment>
		<comment id='27' author='kwotsin' date='2018-02-19T11:46:48Z'>
		Yeah, well TF Lite is nowhere near where we need it to be to run our graphs. So what's the recommended solution until we can convert graphs to TF Lite (trying to convert gives errors for unsupported ops)? -DTENSORFLOW_DISABLE_META?
		</comment>
	</comments>
</bug>