<bug id='41485' author='rxiang040' open_date='2020-07-17T05:42:31Z' closed_time='2020-09-14T02:29:41Z'>
	<summary>FL16 model run on GPU</summary>
	<description>
System information

Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
TensorFlow installed from (source or binary): binary
Tensorflow version (commit SHA if source): 1.15
Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):Android 9 api28 ,Mali-T864 GPU


We tried to run a post-quantized (to float16) model on a robot with GPU delegate according to &lt;denchmark-link:https://www.tensorflow.org/lite/performance/gpu&gt;https://www.tensorflow.org/lite/performance/gpu&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-float16-quantization-halves-model-size-cc113c75a2fa&gt;https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-float16-quantization-halves-model-size-cc113c75a2fa&lt;/denchmark-link&gt;

but it fails to run on GPU even after we graph transformed non-GPU supported operators in it. The logs is attached. Interesting thing is if we do not quantize it to fl16, all operators of the model can successfully run on GPU. Netron shows there are lots of 'dequantize' operators added to the graph after we use tflite converter to quantize the model to fl16. So what should we do to let the quantized fl16 model run on GPU entirely?
One more question is we found a parameter SetAllowFp16PrecisionForFp32 in tflite c++. What is the difference between 1).set this to true and use a fl32 model. 2). set this to true and use fl16 model. 3). set this to false and use fl32 model. 4) set this to false and use fl16 model?
Many thanks.
Model is uploaded in:
&lt;denchmark-link:https://drive.google.com/drive/folders/18B4Wx4BEPxfptsTmIEZySwILLZNXbE2v?usp=sharing&gt;https://drive.google.com/drive/folders/18B4Wx4BEPxfptsTmIEZySwILLZNXbE2v?usp=sharing&lt;/denchmark-link&gt;

Inputs are image of size 1933
Please provide the exact sequence of commands/steps when you ran into the problem
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for GPU.
ERROR: Next operations are not supported by GPU delegate:
CONV_2D: Expected 1 input tensor(s), but node has 3 runtime input(s).
DEPTHWISE_CONV_2D: Expected 1 input tensor(s), but node has 3 runtime input(s).
DEQUANTIZE: Operation is not supported.
First 0 operations will run on the GPU, and the remaining 198 on the CPU.
	</description>
	<comments>
		<comment id='1' author='rxiang040' date='2020-07-17T08:03:43Z'>
		&lt;denchmark-link:https://github.com/rxiang040&gt;@rxiang040&lt;/denchmark-link&gt;

I am unable to access the code shared on google drive.
		</comment>
		<comment id='2' author='rxiang040' date='2020-07-17T08:51:02Z'>
		
@rxiang040
I am unable to access the code shared on google drive.

Sorry, please use this link:
&lt;denchmark-link:https://drive.google.com/drive/folders/18B4Wx4BEPxfptsTmIEZySwILLZNXbE2v?usp=sharing&gt;https://drive.google.com/drive/folders/18B4Wx4BEPxfptsTmIEZySwILLZNXbE2v?usp=sharing&lt;/denchmark-link&gt;

It's not the code file, but tflite models
		</comment>
		<comment id='3' author='rxiang040' date='2020-07-20T23:53:11Z'>
		Is it because TFlite V1 does not support dequantize op? And I visualize my quantized model by Netro, basically, every operation is has input 'dequantize', and that's the reason none operation can be performed on GPU. So what is this 'dequantize' for? I learned it aims to convert fl16 to fl32, but if I want to run on GPU then they are all useless?
		</comment>
		<comment id='4' author='rxiang040' date='2020-07-22T21:26:33Z'>
		&lt;denchmark-link:https://github.com/rxiang040&gt;@rxiang040&lt;/denchmark-link&gt;
 Can you please share a standalone code to reproduce the issue? Specifically, model conversion code with a saved model or model development code. Did you try   for conversion? Thanks!
		</comment>
		<comment id='5' author='rxiang040' date='2020-07-22T22:15:42Z'>
		
@rxiang040 Can you please share a standalone code to reproduce the issue? Specifically, model conversion code with a saved model or model development code. Did you try tf-nightly for conversion? Thanks!

Hi thanks for your reply. Here's our conversion code:
import tensorflow as tf
import pathlib
root_path = "../bazel_export/"
graph_name = "bazel_optimized_graph.pb"
tflite_path = "../tflite/"
tflite_name = "test1.tflite"
graph_def_file = root_path + graph_name
input_arrays = ["sub_2"]
output_arrays = ["ResizeBilinear_2"]
converter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays, input_shapes={"sub_2" : [1, 193, 321, 3]})
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_types = [tf.float16]
tflite_model = converter.convert()
open(tflite_path + tflite_name, "wb").write(tflite_model)
tflite_path2 = pathlib.Path(tflite_path)
tflite_path2.mkdir(exist_ok=True, parents=True)
tflite_file = tflite_path2/tflite_name
print(tflite_file.write_bytes(tflite_model))
And for model development, we use the code suggested in : &lt;denchmark-link:https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-float16-quantization-halves-model-size-cc113c75a2fa&gt;https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-float16-quantization-halves-model-size-cc113c75a2fa&lt;/denchmark-link&gt;

As
//Prepare GPU delegate.
const TfLiteGpuDelegateOptions options = {
.metadata = NULL,
.compile_options = {
.precision_loss_allowed = 1,  // FP16
.preferred_gl_object_type = TFLITE_GL_OBJECT_TYPE_FASTEST,
.dynamic_batch_enabled = 0,   // Not fully functional yet
},
};
So we use tf 1.15.0, but not tf-nightly. Should we use tf-nightly instead of pip installed tf?
		</comment>
		<comment id='6' author='rxiang040' date='2020-08-26T03:27:59Z'>
		Hi Sachin, can you help take a look?
		</comment>
		<comment id='7' author='rxiang040' date='2020-08-26T20:31:32Z'>
		&lt;denchmark-link:https://github.com/rxiang040&gt;@rxiang040&lt;/denchmark-link&gt;
 You are using an older version of the GPU delegate, which isn't maintained anymore. Could you try with &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/delegate.h#L103&gt;TfLiteGpuDelegateOptionsV2&lt;/denchmark-link&gt;
? Its in a different header. Look at the &lt;denchmark-link:https://www.tensorflow.org/lite/performance/gpu_advanced?source=post_page---------------------------#android_cc&gt;Delegate page&lt;/denchmark-link&gt;
 for other languages.
Out of curiosity, have you tried using the &lt;denchmark-link:https://www.tensorflow.org/lite/performance/gpu_advanced?source=post_page---------------------------#running_quantized_models_experimental&gt;8-bit quantization support&lt;/denchmark-link&gt;
 w/ GPU? There might be a slight precision dip, but the models turn out 50% smaller compared to fp16.
		</comment>
		<comment id='8' author='rxiang040' date='2020-08-27T02:24:39Z'>
		
@rxiang040 You are using an older version of the GPU delegate, which isn't maintained anymore. Could you try with TfLiteGpuDelegateOptionsV2? Its in a different header. Look at the Delegate page for other languages.
Out of curiosity, have you tried using the 8-bit quantization support w/ GPU? There might be a slight precision dip, but the models turn out 50% smaller compared to fp16.

Thanks so much for your advice. We will try TfLiteGpuDelegateOptionsV2. As for the 8-bit quantization support w/ GPU, can GPU perform 8-bit computations? I thought only 32-bit and 16-bit model can use GPU delegate.
		</comment>
		<comment id='9' author='rxiang040' date='2020-08-27T15:51:48Z'>
		The GPU delegate now supports 8-bit quantized models (see link in the reply above), and the latency is on-par with corresponding floating-point models.
		</comment>
		<comment id='10' author='rxiang040' date='2020-08-31T17:39:54Z'>
		&lt;denchmark-link:https://github.com/rxiang040&gt;@rxiang040&lt;/denchmark-link&gt;
 Over to you, feel free to close this with an update once things work on your side :-)
		</comment>
		<comment id='11' author='rxiang040' date='2020-09-08T11:56:43Z'>
		&lt;denchmark-link:https://github.com/srjoglekar246&gt;@srjoglekar246&lt;/denchmark-link&gt;
 Thanks very much for your advice. Now it can run on GPU with tflite2.3. But still we have one problem. I have attached our model structure. At the last layer of our model, it is a ResizeBilinear layer. We found that this operation is much efficient if we can run it with CPU. So we modified  at line 219 by adding the following code:
  if (node_id == 197) {
std::string msg = "197 Bilinear upsamping is not run on GPU but CPU";
unsupported_details = &amp;msg;
return false;
 }
197 is the node_id of ResizeBilinear layer. However when we run our program on the bot, the log writes:
ERROR: Following operations are not supported by GPU delegate:
DEQUANTIZE: 
RESIZE_BILINEAR: 
197 operations will run on the GPU, and the remaining 1 operations will run on the CPU.
Ideally, RESIZE_BILINEAR should be the only one op that will not run on GPU, but somehow, DEQUANTIZE shows up here. More strangely, even here, it reports two names of ops, but in the next sentence it says "the remaining 1 operations will run on the CPU". So do you know what's happening here?
Also I tested FL32 model and FL16M model, they almost have no inference time difference. So why should we use fl16 quantization here? (any advantage of fl16 comparing to fl32?)
Thanks!
&lt;denchmark-link:https://user-images.githubusercontent.com/67887251/92472643-5a7ea500-f20c-11ea-81e4-428656d4538e.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='rxiang040' date='2020-09-08T15:51:59Z'>
		&lt;denchmark-link:https://github.com/rxiang040&gt;@rxiang040&lt;/denchmark-link&gt;
 The LOG statements are sometimes buggy, especially when the graph structure is a bit complex (as is the case for FP16 graph). Its on our radar to improve this :-).
Does the model provide a speedup on the GPU? If yes, I wouldn't worry too much about a node (or two) not being run on GPU, since the other nodes in your visualization seem heavier.
As for FP16 vs FP32, this shouldn't matter on GPU (except FP16 might be a bit faster). The only reason to gofor FP16 is to get a 50% smaller model. 8-bit quantization will give you a model 75% smaller, but with a bit of an accuracy hit.
		</comment>
		<comment id='13' author='rxiang040' date='2020-09-09T02:11:20Z'>
		The model is faster on GPU. We actually tested the inference time of our model (fl32) with the last layer run on GPU or CPU. I would say the difference is huge. If the whole model is on GPU, the inference time is 195 ms/frame, and if the last layer on CPU and others on GPU, the inference time is 162 ms/frame.
So for int8 model, will it run faster on GPU?
		</comment>
		<comment id='14' author='rxiang040' date='2020-09-09T13:46:27Z'>
		How did you test the whole model being on GPU?
int8 models will provide the same latency benefits as fp32, but will be 4x smaller in size. They will also avoid weird DEQUANTIZE issues that fp16 faces.
		</comment>
		<comment id='15' author='rxiang040' date='2020-09-10T02:10:20Z'>
		We have a RK3399 board with Andoird system and we use c++ for deployment. Basically the time is measured from inputting an image to obtaining the final segmentation result. E.g with total 77 ops, test1: time of (76 GPU operation + 1 CPU operation), 162 ms/frame; test2: time of 77 ops on GPU, 195 ms/frame
		</comment>
		<comment id='16' author='rxiang040' date='2020-09-10T15:59:04Z'>
		I see. Can you try using &lt;denchmark-link:https://www.tensorflow.org/lite/performance/post_training_quant#optimizing_an_existing_model&gt;dynamic range quantization&lt;/denchmark-link&gt;
? It will give you a smaller model, and may not have the DEQUANTIZE issue.
In general, the debug statements can be a little weird, so I wouldn't worry if the speedup is good.
		</comment>
		<comment id='17' author='rxiang040' date='2020-09-11T04:37:13Z'>
		Sure, we will try that. So if it's on an RK3399 with GPU using C++ implementation, what would be the fastest setup to run a tflite in general? Like, what the options and quantiztations should be without considering RAM usage.
		</comment>
		<comment id='18' author='rxiang040' date='2020-09-11T15:32:58Z'>
		If size isn't an issue, go with FP32 model &amp; GPU delegate, as long as OpenCL is supported. You could also try 4-threaded CPU execution.
		</comment>
		<comment id='19' author='rxiang040' date='2020-09-14T02:29:41Z'>
		Cool! Thank:) I will close this issue. Thanks for help:)
		</comment>
		<comment id='20' author='rxiang040' date='2020-09-14T02:29:43Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41485&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41485&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='21' author='rxiang040' date='2020-10-13T19:28:49Z'>
		&lt;denchmark-link:https://github.com/rxiang040&gt;@rxiang040&lt;/denchmark-link&gt;
 Hi, Did you figure out how to run float16 tflite model with GPU delegate? I still have a problem with DEQUANTIZE. It shows:
&lt;denchmark-code&gt;Internal error: Failed to apply delegate: Following operations are not supported by GPU delegate:
    DEQUANTIZE: 
    92 operations will run on the GPU, and the remaining 1 operations will run on the CPU.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='22' author='rxiang040' date='2020-10-13T19:49:32Z'>
		&lt;denchmark-link:https://github.com/pl-ang&gt;@pl-ang&lt;/denchmark-link&gt;
 Thats fine, that operation doesn't work on the GPU, but the model is still accelerated.
		</comment>
		<comment id='23' author='rxiang040' date='2020-10-13T20:36:49Z'>
		
@pl-ang Thats fine, that operation doesn't work on the GPU, but the model is still accelerated.

Thanks!
		</comment>
	</comments>
</bug>