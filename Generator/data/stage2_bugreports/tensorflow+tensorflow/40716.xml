<bug id='40716' author='AlexisLattard' open_date='2020-06-23T13:43:06Z' closed_time='2020-07-02T20:47:20Z'>
	<summary>TFLite generation issue with lrn</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
TensorFlow installed from (source or binary):
TensorFlow version (or github SHA if from source): 2.1.0

Infos
Tflite return a type issue when i use the code below but it doesn't when just switching the lrn and multiply declaration order in the code
Custom code (test_tflite.py)
import os
import numpy
import tensorflow.compat.v1 as tf
from tensorflow.python.tools import freeze_graph

tf.disable_v2_behavior()

if __name__ == '__main__':
    data = tf.placeholder(dtype=tf.float32, shape=[2,2,16,16], name="input")
    #output size: h,w,d,b = 2,16,16,2
    random_front_0 = tf.nn.local_response_normalization(
        data,
        depth_radius=2,
        bias=1.0,
        alpha=1.0,
        beta=0.5,
        name='random_front_0'
    )
    #output size: h,w,d,b = 2,16,16,2
    random_back_0 = tf.multiply(
        data,
        tf.Variable(numpy.random.rand(1).astype(dtype=numpy.float32)),
        name='random_back_0'
    )
    output = tf.add(
        random_front_0,
        random_back_0,
        name='output'
    )

    #output size: h,w,d,b = 2,16,16,2
    global_init = tf.global_variables_initializer()
    if len(tf.global_variables()) &gt; 0:
        saver = tf.train.Saver()
    with tf.Session() as sess:
        sess.run(global_init)
        tf.train.write_graph(sess.graph, "./", 'model.pbtxt', as_text=True)
        save_path = saver.save(sess, './model.ckpt')
        print('Model saved in file: {}'.format(save_path))
        # Look here for more details https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph_test.py
        freeze_graph.freeze_graph(
           os.path.join("./", 'model.pbtxt'), # GraphDef
           '',
           False, # is the GraphDef in binary format
           os.path.join("./", 'model.ckpt'), # checkpoint name
           'output', # output node name
           '', '',
           os.path.join("./", 'model.frozen.pb'), # output frozen path graph
           True, # clear devices info from meta-graph
           '', '', '')
        input_arrays = ["input"]
        output_arrays = ["output"]
        converter = tf.lite.TFLiteConverter.from_frozen_graph('./model.frozen.pb', input_arrays, output_arrays)
        def representative_dataset_gen():
            for _ in range(1000):
                yield [numpy.array(numpy.random.randint(0, 255, size=(2, 2, 16, 16)),dtype=numpy.float32)]
        converter.representative_dataset = representative_dataset_gen
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        tflite_model = converter.convert()
        tflite_model_path = './model.tflite'
        open(tflite_model_path, 'wb').write(tflite_model)
    interpreter = tf.lite.Interpreter(model_path=str(tflite_model_path))
    assert(interpreter is not None)
    interpreter.allocate_tensors()
The output
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "test_tflite.py", line 64, in &lt;module&gt;
    interpreter.allocate_tensors()
  File "lib/python2.7/site-packages/tensorflow_core/lite/python/interpreter.py", line 247, in allocate_tensors
    return self._interpreter.AllocateTensors()
  File "lib/python2.7/site-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py", line 110, in AllocateTensors
    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)
RuntimeError: tensorflow/lite/kernels/local_response_norm.cc:47 input-&gt;type != output-&gt;type (9 != 1)Node number 1 (LOCAL_RESPONSE_NORMALIZATION) failed to prepare.
&lt;/denchmark-code&gt;

Model.tflite graph
&lt;denchmark-link:https://user-images.githubusercontent.com/29231663/85408924-735cb000-b565-11ea-8a92-6a5e6cee8e20.png&gt;&lt;/denchmark-link&gt;

Custom code (with switched layers)
import os
import numpy
import tensorflow.compat.v1 as tf
from tensorflow.python.tools import freeze_graph

tf.disable_v2_behavior()

if __name__ == '__main__':
    data = tf.placeholder(dtype=tf.float32, shape=[2,2,16,16], name="input")
    #output size: h,w,d,b = 2,16,16,2
    random_back_0 = tf.multiply(
        data,
        tf.Variable(numpy.random.rand(1).astype(dtype=numpy.float32)),
        name='random_back_0'
    )
    #output size: h,w,d,b = 2,16,16,2
    random_front_0 = tf.nn.local_response_normalization(
        data,
        depth_radius=2,
        bias=1.0,
        alpha=1.0,
        beta=0.5,
        name='random_front_0'
    )
    output = tf.add(
        random_front_0,
        random_back_0,
        name='output'
    )

    #output size: h,w,d,b = 2,16,16,2
    global_init = tf.global_variables_initializer()
    if len(tf.global_variables()) &gt; 0:
        saver = tf.train.Saver()
    with tf.Session() as sess:
        sess.run(global_init)
        tf.train.write_graph(sess.graph, "./", 'model.pbtxt', as_text=True)
        save_path = saver.save(sess, './model.ckpt')
        print('Model saved in file: {}'.format(save_path))
        # Look here for more details https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph_test.py
        freeze_graph.freeze_graph(
           os.path.join("./", 'model.pbtxt'), # GraphDef
           '',
           False, # is the GraphDef in binary format
           os.path.join("./", 'model.ckpt'), # checkpoint name
           'output', # output node name
           '', '',
           os.path.join("./", 'model.frozen.pb'), # output frozen path graph
           True, # clear devices info from meta-graph
           '', '', '')
        input_arrays = ["input"]
        output_arrays = ["output"]
        converter = tf.lite.TFLiteConverter.from_frozen_graph('./model.frozen.pb', input_arrays, output_arrays)
        def representative_dataset_gen():
            for _ in range(1000):
                yield [numpy.array(numpy.random.randint(0, 255, size=(2, 2, 16, 16)),dtype=numpy.float32)]
        converter.representative_dataset = representative_dataset_gen
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        tflite_model = converter.convert()
        tflite_model_path = './model.tflite'
        open(tflite_model_path, 'wb').write(tflite_model)
    interpreter = tf.lite.Interpreter(model_path=str(tflite_model_path))
    assert(interpreter is not None)
    interpreter.allocate_tensors()
Output
No output, work fine
Model.tflite graph
&lt;denchmark-link:https://user-images.githubusercontent.com/29231663/85409979-d0a53100-b566-11ea-8940-04b6367b01cc.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='AlexisLattard' date='2020-06-24T06:51:38Z'>
		Could you try the conversion with tf-nightly?
		</comment>
		<comment id='2' author='AlexisLattard' date='2020-06-24T07:19:49Z'>
		I could run your above code without any problems on the tf-nightly version.
		</comment>
		<comment id='3' author='AlexisLattard' date='2020-07-01T19:26:11Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='4' author='AlexisLattard' date='2020-07-02T20:47:20Z'>
		You may also try with TF version 2.2
The code executes successfully see &lt;denchmark-link:https://colab.research.google.com/gist/ymodak/7e23d4816c0859fa5b80d6a888a58074/github-issue_40716.ipynb&gt;gist&lt;/denchmark-link&gt;
.
Thanks!
		</comment>
		<comment id='5' author='AlexisLattard' date='2020-07-02T20:47:22Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40716&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40716&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>