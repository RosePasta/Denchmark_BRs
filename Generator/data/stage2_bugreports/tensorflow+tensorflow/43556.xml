<bug id='43556' author='powderluv' open_date='2020-09-25T07:21:40Z' closed_time='2020-09-29T11:55:18Z'>
	<summary>[MLIR / MHLO ]: tf_to_gpu_binary  Internal: Lowering to GPU kernels failed.</summary>
	<description>
I am using tf_to_gpu_binary to convert the following mlir file to a gpu kernel
&lt;denchmark-code&gt;func @abs(%arg0: tensor&lt;*xf32&gt;) -&gt; tensor&lt;*xf32&gt; {
 %0 = "tf.Abs"(%arg0) { }
 : (tensor&lt;*xf32&gt;) -&gt; tensor&lt;*xf32&gt;
 return %0 : tensor&lt;*xf32&gt;
}
&lt;/denchmark-code&gt;

On TF top of master (SHA &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/7f5b8ad3708d0e2cd98e63714352df60b772db36&gt;7f5b8ad&lt;/denchmark-link&gt;
)  it fails with:
(tf_env) foo@bar:$ MLIR_CRASH_REPRODUCER_DIRECTORY=. ./build/install/bin/tf_to_gpu_binary --input=tf_abs.mlir --output=abs_kernel.o
loc("-":2:7): error: failed to legalize operation 'mhlo.abs' that was explicitly marked illegal
loc("-":0:0): error: A failure has been detected while processing the MLIR module, a reproducer has been generated in './mlir_reproducer_ironman-725e3dc1-43003-5b01dc8776cbc.mlir'
2020-09-24 23:54:07.367852: E tensorflow/compiler/mlir/tools/kernel_gen/tf_to_gpu_binary.cc:95] Internal: Lowering to GPU kernels failed.
MLIR CRASH Repro:
&lt;denchmark-code&gt;
// configuration: -pass-pipeline='func(xla-legalize-tf{allow-partial-conversion=false device-type=INVALID_DEVICE_TYPE legalize-chlo=true use-tf2xla-fallback=false}, materialize-broadcast, unfuse-batch-norm), unknown&lt;mlir::mhlo::{anonymous}::HloLegalizeToLhlo&gt;{results-escape-function=true}, func(buffer-placement, unknown&lt;{anonymous}::CopyRemovalPass&gt;), shape-to-descriptors, canonicalize, func(unknown&lt;mlir::{anonymous}::LhloLegalizeToLinalgPass&gt;, unknown&lt;mlir::lmhlo::{anonymous}::LhloFuseLinalgPass&gt;{tile-sizes= use-parallel-loops=true}, convert-linalg-to-parallel-loops, canonicalize, cse, unknown&lt;xla::mlir_gpu::{anonymous}::FuseInnerParallelLoopsPass&gt;, cse, unknown&lt;xla::mlir_gpu::{anonymous}::StoreForwardingPass&gt;, unknown&lt;xla::mlir_gpu::{anonymous}::DeadTempBufferRemovalPass&gt;, canonicalize, cse, unknown&lt;xla::mlir_gpu::{anonymous}::MapParallelLoopsPass&gt;), convert-parallel-loops-to-gpu, func(canonicalize, cse, unknown&lt;mlir::mhlo::{anonymous}::LegalizeTrigonometricToApproximationPass&gt;, unknown&lt;xla::mlir_gpu::{anonymous}::MoveScalarComputationsIntoGpuLaunchPass&gt;), gpu-kernel-outlining, func(unknown&lt;xla::mlir_gpu::{anonymous}::RewriteKernelSignaturePass&gt;), lower-affine, convert-scf-to-std'
// note: verifyPasses=true


module {
  func @abs(%arg0: tensor&lt;*xf32&gt;) -&gt; tensor&lt;*xf32&gt; {
    %0 = "tf.Abs"(%arg0) : (tensor&lt;*xf32&gt;) -&gt; tensor&lt;*xf32&gt;
    return %0 : tensor&lt;*xf32&gt;
  }
}

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='powderluv' date='2020-09-29T11:55:18Z'>
		tf_to_gpu_binary does not support unranked code generation, it requires a ranked input. For the unranked case, you can generate a version for a 1d vector and use that with some host-side glue code to transform the unranked tensor into a 1d vector and the inverse for the result.
If you want to have that glue code also be generated, you need to use tf_to_kernel, which will produce a library file that contains both host and device code. Your input is also one of the examples.
Likely, this had not landed when you tried.
		</comment>
		<comment id='2' author='powderluv' date='2020-09-29T11:55:20Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43556&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43556&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>