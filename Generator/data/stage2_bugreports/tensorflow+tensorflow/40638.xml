<bug id='40638' author='Santosh-Gupta' open_date='2020-06-20T21:55:16Z' closed_time='2020-08-04T18:57:41Z'>
	<summary>Keras layer weights/sublayers getting deleted when creating a model with them. model.summary() / plot_model still shows those weights as part of graph though</summary>
	<description>
System information


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes


OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google colab enviroment


TensorFlow installed from (source or binary): Google colab default


Python version: Python 3, Google colab default


CUDA/cuDNN version:   Google colab default


GPU model and memory:  Tested on both Google colab p-100 GPU and CPU


You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with:
2. TF 2.0: 

2020-06-20 21:44:17.003371: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
v2.2.0-0-g2b96f3662b 2.2.0

Describe the current behavior
I created a new model using two layers from and old model. However, now all of the layers/weights from the old model are Not showing up in the new model.
model.summary() and
&lt;denchmark-code&gt;tf.keras.utils.plot_model(
    model, to_file='model.png', show_shapes=False, show_layer_names=True,
    rankdir='TB', expand_nested=False, dpi=96
)
&lt;/denchmark-code&gt;

still has those weights, so I think they're a part of the graph.  But when I print them out, those weights/layers are missing altogether
Describe the expected behavior
All weights from component layers to should be in the model.
Standalone code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
Here is a Colabnotebook with a minimal example that reproduced the issue.
&lt;denchmark-link:https://colab.research.google.com/drive/1n3_XNhdgH6Qo7GT-M570lIKWAoU3TML5?usp=sharing&gt;https://colab.research.google.com/drive/1n3_XNhdgH6Qo7GT-M570lIKWAoU3TML5?usp=sharing&lt;/denchmark-link&gt;

And here is the code
&lt;denchmark-code&gt;!pip install transformers --q
%tensorflow_version 2.x

from transformers import TFBertModel, AutoModel, TFRobertaModel, AutoTokenizer
import tensorflow as tf
import tensorflow_addons as tfa

tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)

from tensorflow import keras
from tensorflow.keras import layers
from copy import deepcopy

logger = tf.get_logger()
logger.info(tf.__version__)


def get_mini_models():
    tempModel = TFRobertaModel.from_pretrained('bert-base-uncased', from_pt=True)

    layer9 = deepcopy(tempModel.layers[0].encoder.layer[8])
    layer10 = deepcopy(tempModel.layers[0].encoder.layer[9])

    inputHiddenVals = tf.keras.Input(shape=[None, None], dtype=tf.float32, name='input_Q',
                                    batch_size=None) 

    hidden1 = layer9((inputHiddenVals, None, None))
    hidden2 = layer10((hidden1[0], None, None))
    modelNew = tf.keras.Model(inputs=inputHiddenVals, outputs=hidden2)

    del tempModel

    return modelNew

@tf.function
def loss_fn(_, probs):
    bs = tf.shape(probs)[0]
    labels = tf.eye(bs, bs)
    return tf.losses.categorical_crossentropy(labels,
                                              probs,
                                              from_logits=True)

model = get_mini_models()
model.compile(loss=loss_fn,
                optimizer=tfa.optimizers.AdamW(weight_decay=1e-4, learning_rate=1e-5, 
                                                epsilon=1e-06))

# Get model and layers directly to compare
tempModel = TFRobertaModel.from_pretrained('bert-base-uncased', from_pt=True)
layer9 = deepcopy(tempModel.layers[0].encoder.layer[8])
layer10 = deepcopy(tempModel.layers[0].encoder.layer[9])

# Only one layer, and that layer also has missing weights. 
for i, var in enumerate(model.weights):
    print(model.weights[i].name)

# Full weights for one layer 
for i, var in enumerate(layer9.weights):
    print(layer9.weights[i].name)

# Test what correct output should be 

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
inputt = tokenizer.encode('This is a sentence', return_tensors='tf')
outt = tempModel(inputt)[0]

# Test model output. Not the same. 

model(outt)

# Model summary somehow lists the weights 
model.summary()

# Model diagram shows the correct connections between all the layers. 

tf.keras.utils.plot_model(
    model, to_file='model.png', show_shapes=False, show_layer_names=True,
    rankdir='TB', expand_nested=False, dpi=96
)

&lt;/denchmark-code&gt;

Edit: I also tried making the layers from scratch, and setting the weights directly, and got the same result. Here's a colab notebook that does this. &lt;denchmark-link:https://colab.research.google.com/drive/1EC_fObSp9lUsj_PFaYgFtRI93ErPYmU9?usp=sharing&gt;https://colab.research.google.com/drive/1EC_fObSp9lUsj_PFaYgFtRI93ErPYmU9?usp=sharing&lt;/denchmark-link&gt;

And here's the code
&lt;denchmark-code&gt;!pip install transformers --q
%tensorflow_version 2.x

from transformers import TFBertModel, AutoModel, TFRobertaModel, AutoTokenizer

import tensorflow as tf
import tensorflow_addons as tfa

tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import (Dense,
                                     Dropout)
import numpy as np
import os

logger = tf.get_logger()
logger.info(tf.__version__)

class TFBertSelfAttention2(tf.keras.layers.Layer):
    def __init__(self, config, **kwargs):
        super().__init__(**kwargs)
        if config.hidden_size % config.num_attention_heads != 0:
            raise ValueError(
                "The hidden size (%d) is not a multiple of the number of attention "
                "heads (%d)" % (config.hidden_size, config.num_attention_heads)
            )

        self.num_attention_heads = config.num_attention_heads
        assert config.hidden_size % config.num_attention_heads == 0
        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)
        self.all_head_size = self.num_attention_heads * self.attention_head_size

        self.query = tf.keras.layers.Dense(
            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name="query_2"
        )
        self.key = tf.keras.layers.Dense(
            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name="key_2"
        )
        self.value = tf.keras.layers.Dense(
            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name="value_2"
        )

        self.dropout = tf.keras.layers.Dropout(config.attention_probs_dropout_prob)

    def transpose_for_scores(self, x, batch_size):
        x = tf.reshape(x, (batch_size, -1, self.num_attention_heads, self.attention_head_size))
        return tf.transpose(x, perm=[0, 2, 1, 3])

    def call(self, inputs, training=False):
        hidden_states, attention_mask, head_mask, output_attentions = inputs

        batch_size = shape_list(hidden_states)[0]
        mixed_query_layer = self.query(hidden_states)
        mixed_key_layer = self.key(hidden_states)
        mixed_value_layer = self.value(hidden_states)

        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)
        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)
        value_layer = self.transpose_for_scores(mixed_value_layer, batch_size)

        # Take the dot product between "query" and "key" to get the raw attention scores.
        attention_scores = tf.matmul(
            query_layer, key_layer, transpose_b=True
        )  # (batch size, num_heads, seq_len_q, seq_len_k)
        dk = tf.cast(shape_list(key_layer)[-1], tf.float32)  # scale attention_scores
        attention_scores = attention_scores / tf.math.sqrt(dk)

        if attention_mask is not None:
            # Apply the attention mask is (precomputed for all layers in TFBertModel call() function)
            attention_scores = attention_scores + attention_mask

        # Normalize the attention scores to probabilities.
        attention_probs = tf.nn.softmax(attention_scores, axis=-1)

        # This is actually dropping out entire tokens to attend to, which might
        # seem a bit unusual, but is taken from the original Transformer paper.
        attention_probs = self.dropout(attention_probs, training=training)

        # Mask heads if we want to
        if head_mask is not None:
            attention_probs = attention_probs * head_mask

        context_layer = tf.matmul(attention_probs, value_layer)

        context_layer = tf.transpose(context_layer, perm=[0, 2, 1, 3])
        context_layer = tf.reshape(
            context_layer, (batch_size, -1, self.all_head_size)
        )  # (batch_size, seq_len_q, all_head_size)

        outputs = (
            (context_layer, attention_probs) if cast_bool_to_primitive(output_attentions) is True else (context_layer,)
        )

        return outputs


class TFBertSelfOutput2(tf.keras.layers.Layer):
    def __init__(self, config, **kwargs):
        super().__init__(**kwargs)
        self.dense = tf.keras.layers.Dense(
            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name="dense2"
        )
        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name="LayerNorm2")
        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)

    def call(self, inputs, training=False):
        hidden_states, input_tensor = inputs

        hidden_states = self.dense(hidden_states)
        hidden_states = self.dropout(hidden_states, training=training)
        hidden_states = self.LayerNorm(hidden_states + input_tensor)
        return hidden_states


class TFBertAttention2(tf.keras.layers.Layer):
    def __init__(self, config, **kwargs):
        super().__init__(**kwargs)
        self.self_attention = TFBertSelfAttention2(config, name="self2")
        self.dense_output = TFBertSelfOutput2(config, name="output2")

    def prune_heads(self, heads):
        raise NotImplementedError

    def call(self, inputs, training=False):
        input_tensor, attention_mask, head_mask, output_attentions = inputs

        self_outputs = self.self_attention(
            [input_tensor, attention_mask, head_mask, output_attentions], training=training
        )
        attention_output = self.dense_output([self_outputs[0], input_tensor], training=training)
        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them
        return outputs


class TFBertIntermediate2(tf.keras.layers.Layer):
    def __init__(self, config, **kwargs):
        super().__init__(**kwargs)
        self.dense = tf.keras.layers.Dense(
            config.intermediate_size, kernel_initializer=get_initializer(config.initializer_range), name="dense2"
        )
        if isinstance(config.hidden_act, str):
            self.intermediate_act_fn = ACT2FN[config.hidden_act]
        else:
            self.intermediate_act_fn = config.hidden_act

    def call(self, hidden_states):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.intermediate_act_fn(hidden_states)
        return hidden_states


class TFBertOutput2(tf.keras.layers.Layer):
    def __init__(self, config, **kwargs):
        super().__init__(**kwargs)
        self.dense = tf.keras.layers.Dense(
            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name="dense2"
        )
        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name="LayerNorm2")
        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)

    def call(self, inputs, training=False):
        hidden_states, input_tensor = inputs

        hidden_states = self.dense(hidden_states)
        hidden_states = self.dropout(hidden_states, training=training)
        hidden_states = self.LayerNorm(hidden_states + input_tensor)
        return hidden_states


class TFBertLayer2(tf.keras.layers.Layer):
    def __init__(self, config, **kwargs):
        super().__init__(**kwargs)
        self.attention = TFBertAttention2(config, name="attention2")
        self.intermediate = TFBertIntermediate2(config, name="intermediate2")
        self.bert_output = TFBertOutput2(config, name="output2")

    def call(self, inputs, training=False):
        hidden_states, attention_mask, head_mask, output_attentions = inputs

        attention_outputs = self.attention(
            [hidden_states, attention_mask, head_mask, output_attentions], training=training
        )
        attention_output = attention_outputs[0]
        intermediate_output = self.intermediate(attention_output)
        layer_output = self.bert_output([intermediate_output, attention_output], training=training)
        outputs = (layer_output,) + attention_outputs[1:]  # add attentions if we output them
        return outputs


class TFBertSelfAttention(tf.keras.layers.Layer):
    def __init__(self, config, **kwargs):
        super().__init__(**kwargs)
        if config.hidden_size % config.num_attention_heads != 0:
            raise ValueError(
                "The hidden size (%d) is not a multiple of the number of attention "
                "heads (%d)" % (config.hidden_size, config.num_attention_heads)
            )

        self.num_attention_heads = config.num_attention_heads
        assert config.hidden_size % config.num_attention_heads == 0
        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)
        self.all_head_size = self.num_attention_heads * self.attention_head_size

        self.query = tf.keras.layers.Dense(
            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name="query_"
        )
        self.key = tf.keras.layers.Dense(
            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name="key_"
        )
        self.value = tf.keras.layers.Dense(
            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name="value_"
        )

        self.dropout = tf.keras.layers.Dropout(config.attention_probs_dropout_prob)

    def transpose_for_scores(self, x, batch_size):
        x = tf.reshape(x, (batch_size, -1, self.num_attention_heads, self.attention_head_size))
        return tf.transpose(x, perm=[0, 2, 1, 3])

    def call(self, inputs, training=False):
        hidden_states, attention_mask, head_mask, output_attentions = inputs

        batch_size = shape_list(hidden_states)[0]
        mixed_query_layer = self.query(hidden_states)
        mixed_key_layer = self.key(hidden_states)
        mixed_value_layer = self.value(hidden_states)

        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)
        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)
        value_layer = self.transpose_for_scores(mixed_value_layer, batch_size)

        # Take the dot product between "query" and "key" to get the raw attention scores.
        attention_scores = tf.matmul(
            query_layer, key_layer, transpose_b=True
        )  # (batch size, num_heads, seq_len_q, seq_len_k)
        dk = tf.cast(shape_list(key_layer)[-1], tf.float32)  # scale attention_scores
        attention_scores = attention_scores / tf.math.sqrt(dk)

        if attention_mask is not None:
            # Apply the attention mask is (precomputed for all layers in TFBertModel call() function)
            attention_scores = attention_scores + attention_mask

        # Normalize the attention scores to probabilities.
        attention_probs = tf.nn.softmax(attention_scores, axis=-1)

        # This is actually dropping out entire tokens to attend to, which might
        # seem a bit unusual, but is taken from the original Transformer paper.
        attention_probs = self.dropout(attention_probs, training=training)

        # Mask heads if we want to
        if head_mask is not None:
            attention_probs = attention_probs * head_mask

        context_layer = tf.matmul(attention_probs, value_layer)

        context_layer = tf.transpose(context_layer, perm=[0, 2, 1, 3])
        context_layer = tf.reshape(
            context_layer, (batch_size, -1, self.all_head_size)
        )  # (batch_size, seq_len_q, all_head_size)

        outputs = (
            (context_layer, attention_probs) if cast_bool_to_primitive(output_attentions) is True else (context_layer,)
        )

        return outputs


class TFBertSelfOutput(tf.keras.layers.Layer):
    def __init__(self, config, **kwargs):
        super().__init__(**kwargs)
        self.dense = tf.keras.layers.Dense(
            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name="dense"
        )
        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name="LayerNorm")
        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)

    def call(self, inputs, training=False):
        hidden_states, input_tensor = inputs

        hidden_states = self.dense(hidden_states)
        hidden_states = self.dropout(hidden_states, training=training)
        hidden_states = self.LayerNorm(hidden_states + input_tensor)
        return hidden_states


class TFBertAttention(tf.keras.layers.Layer):
    def __init__(self, config, **kwargs):
        super().__init__(**kwargs)
        self.self_attention = TFBertSelfAttention(config, name="self")
        self.dense_output = TFBertSelfOutput(config, name="output")

    def prune_heads(self, heads):
        raise NotImplementedError

    def call(self, inputs, training=False):
        input_tensor, attention_mask, head_mask, output_attentions = inputs

        self_outputs = self.self_attention(
            [input_tensor, attention_mask, head_mask, output_attentions], training=training
        )
        attention_output = self.dense_output([self_outputs[0], input_tensor], training=training)
        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them
        return outputs


class TFBertIntermediate(tf.keras.layers.Layer):
    def __init__(self, config, **kwargs):
        super().__init__(**kwargs)
        self.dense = tf.keras.layers.Dense(
            config.intermediate_size, kernel_initializer=get_initializer(config.initializer_range), name="dense"
        )
        if isinstance(config.hidden_act, str):
            self.intermediate_act_fn = ACT2FN[config.hidden_act]
        else:
            self.intermediate_act_fn = config.hidden_act

    def call(self, hidden_states):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.intermediate_act_fn(hidden_states)
        return hidden_states


class TFBertOutput(tf.keras.layers.Layer):
    def __init__(self, config, **kwargs):
        super().__init__(**kwargs)
        self.dense = tf.keras.layers.Dense(
            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name="dense"
        )
        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name="LayerNorm")
        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)

    def call(self, inputs, training=False):
        hidden_states, input_tensor = inputs

        hidden_states = self.dense(hidden_states)
        hidden_states = self.dropout(hidden_states, training=training)
        hidden_states = self.LayerNorm(hidden_states + input_tensor)
        return hidden_states


class TFBertLayer(tf.keras.layers.Layer):
    def __init__(self, config, **kwargs):
        super().__init__(**kwargs)
        self.attention = TFBertAttention(config, name="attention")
        self.intermediate = TFBertIntermediate(config, name="intermediate")
        self.bert_output = TFBertOutput(config, name="output")

    def call(self, inputs, training=False):
        hidden_states, attention_mask, head_mask, output_attentions = inputs

        attention_outputs = self.attention(
            [hidden_states, attention_mask, head_mask, output_attentions], training=training
        )
        attention_output = attention_outputs[0]
        intermediate_output = self.intermediate(attention_output)
        layer_output = self.bert_output([intermediate_output, attention_output], training=training)
        outputs = (layer_output,) + attention_outputs[1:]  # add attentions if we output them
        return outputs

configBase = {
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

class AttrDict(dict):
    def __init__(self, *args, **kwargs):
        super(AttrDict, self).__init__(*args, **kwargs)
        self.__dict__ = self

config = AttrDict(configBase)

def get_initializer(initializer_range=0.02):
    """Creates a `tf.initializers.truncated_normal` with the given range.
    Args:
        initializer_range: float, initializer range for stddev.
    Returns:
        TruncatedNormal initializer with stddev = `initializer_range`.
    """
    return tf.keras.initializers.TruncatedNormal(stddev=initializer_range)


def gelu(x):
    """ Gaussian Error Linear Unit.
    Original Implementation of the gelu activation function in Google Bert repo when initially created.
        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):
        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))
        Also see https://arxiv.org/abs/1606.08415
    """
    cdf = 0.5 * (1.0 + tf.math.erf(x / tf.math.sqrt(2.0)))
    return x * cdf

ACT2FN = {
    "gelu": tf.keras.layers.Activation(gelu),
}

def shape_list(x):
    """Deal with dynamic shape in tensorflow cleanly."""
    static = x.shape.as_list()
    dynamic = tf.shape(x)
    return [dynamic[i] if s is None else s for i, s in enumerate(static)]

def cast_bool_to_primitive(bool_variable, default_tensor_to_true=False):
    """Function arguments can be inserted as boolean tensor
        and bool variables to cope with keras serialization
        we need to cast `output_attentions` to correct bool
        if it is a tensor
    Args:
        default_tensor_to_true: bool, if tensor should default to True
        in case tensor has no numpy attribute
    """
    # if bool variable is tensor and has numpy value
    if tf.is_tensor(bool_variable):
        if hasattr(bool_variable, "numpy"):
            return bool(bool_variable.numpy())
        elif default_tensor_to_true:
            return True

    # else variable is bool
    return bool_variable

def get_2_transformerLayerP(numb):
    tokenizer = AutoTokenizer.from_pretrained('allenai/biomed_roberta_base')
    inputt = tokenizer.encode('This is a sentence', return_tensors='tf')
    tempModel = TFRobertaModel.from_pretrained('allenai/biomed_roberta_base', from_pt=True)
    outt = tempModel(inputt)[0]

    t_layer11 = TFBertLayer(config, name="layer_._{}".format(11+numb))
    t_layer12 = TFBertLayer2(config, name="layer_._{}".format(12+numb))

    t_layer11((outt, None, None, None))
    t_layer12((outt, None, None, None))

    t_layer11.set_weights( tempModel.layers[0].encoder.layer[10].get_weights() )
    t_layer12.set_weights( tempModel.layers[0].encoder.layer[11].get_weights() )

    t_layer12.intermediate.intermediate_act_fn = tf.keras.activations.tanh

    del tokenizer
    del tempModel

    return t_layer11, t_layer12

def get_mini_models():
    P_trans11, P_trans12 = get_2_transformerLayerP(6)

    inputHiddenVals = tf.keras.Input(shape=[None, None], dtype=tf.float32, name='input_Q',
                                    batch_size=None) 

    P_outputs = P_trans11((inputHiddenVals, None, None, None))[0]
    P_outputsFinal = P_trans12((P_outputs, None, None, None))[0]
    modelNew = tf.keras.Model(inputs=inputHiddenVals, outputs=P_outputsFinal)

    return modelNew

@tf.function
def loss_fn(_, probs):

    bs = tf.shape(probs)[0]
    labels = tf.eye(bs, bs)
    return tf.losses.categorical_crossentropy(labels,
                                              probs,
                                              from_logits=True)

model = get_mini_models()
model.compile(loss=loss_fn,
                optimizer=tfa.optimizers.AdamW(weight_decay=1e-4, learning_rate=1e-5, 
                                                epsilon=1e-06))

for i, var in enumerate(model.trainable_weights):
    print(model.trainable_weights[i].name)

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Santosh-Gupta' date='2020-06-22T06:34:09Z'>
		&lt;denchmark-link:https://github.com/Santosh-Gupta&gt;@Santosh-Gupta&lt;/denchmark-link&gt;

I have tried in colab with TF version 2.2 .Please, find the gist &lt;denchmark-link:https://colab.research.google.com/gist/ravikyram/0191e12b7c6d9afeb80ccc009870b255/untitled52.ipynb&gt;here&lt;/denchmark-link&gt;
.Is this the expected behavior?.Thanks!
		</comment>
		<comment id='2' author='Santosh-Gupta' date='2020-06-22T12:40:09Z'>
		Hey

@Santosh-Gupta
I have tried in colab with TF version 2.2 .Please, find the gist here.Is this the expected behavior?.Thanks!

This was a little tricky to checkout, so I split up the cells so some of the outputs are viewable
&lt;denchmark-link:https://colab.research.google.com/gist/ravikyram/0191e12b7c6d9afeb80ccc009870b255/untitled52.ipynb&gt;https://colab.research.google.com/gist/ravikyram/0191e12b7c6d9afeb80ccc009870b255/untitled52.ipynb&lt;/denchmark-link&gt;

The output of cell 6 is
&lt;denchmark-code&gt;tf_roberta_model_1/roberta/encoder/layer_._8/attention/self/query/kernel:0
tf_roberta_model_1/roberta/encoder/layer_._8/attention/self/query/bias:0
tf_roberta_model_1/roberta/encoder/layer_._8/attention/self/key/kernel:0
tf_roberta_model_1/roberta/encoder/layer_._8/attention/self/key/bias:0
tf_roberta_model_1/roberta/encoder/layer_._8/attention/self/value/kernel:0
tf_roberta_model_1/roberta/encoder/layer_._8/attention/self/value/bias:0
&lt;/denchmark-code&gt;

which contains all the model weights. There are weights defined in the model that are missing.
It should look something like the output of cell 7, which prints out the weights of both of the layers which comprise the model.
&lt;denchmark-code&gt;tf_roberta_model_2/roberta/encoder/layer_._8/attention/self/query/kernel:0
tf_roberta_model_2/roberta/encoder/layer_._8/attention/self/query/bias:0
tf_roberta_model_2/roberta/encoder/layer_._8/attention/self/key/kernel:0
tf_roberta_model_2/roberta/encoder/layer_._8/attention/self/key/bias:0
tf_roberta_model_2/roberta/encoder/layer_._8/attention/self/value/kernel:0
tf_roberta_model_2/roberta/encoder/layer_._8/attention/self/value/bias:0
tf_roberta_model_2/roberta/encoder/layer_._8/attention/output/dense/kernel:0
tf_roberta_model_2/roberta/encoder/layer_._8/attention/output/dense/bias:0
tf_roberta_model_2/roberta/encoder/layer_._8/attention/output/LayerNorm/gamma:0
tf_roberta_model_2/roberta/encoder/layer_._8/attention/output/LayerNorm/beta:0
tf_roberta_model_2/roberta/encoder/layer_._8/intermediate/dense/kernel:0
tf_roberta_model_2/roberta/encoder/layer_._8/intermediate/dense/bias:0
tf_roberta_model_2/roberta/encoder/layer_._8/output/dense/kernel:0
tf_roberta_model_2/roberta/encoder/layer_._8/output/dense/bias:0
tf_roberta_model_2/roberta/encoder/layer_._8/output/LayerNorm/gamma:0
tf_roberta_model_2/roberta/encoder/layer_._8/output/LayerNorm/beta:0
tf_roberta_model_2/roberta/encoder/layer_._9/attention/self/query/kernel:0
tf_roberta_model_2/roberta/encoder/layer_._9/attention/self/query/bias:0
tf_roberta_model_2/roberta/encoder/layer_._9/attention/self/key/kernel:0
tf_roberta_model_2/roberta/encoder/layer_._9/attention/self/key/bias:0
tf_roberta_model_2/roberta/encoder/layer_._9/attention/self/value/kernel:0
tf_roberta_model_2/roberta/encoder/layer_._9/attention/self/value/bias:0
tf_roberta_model_2/roberta/encoder/layer_._9/attention/output/dense/kernel:0
tf_roberta_model_2/roberta/encoder/layer_._9/attention/output/dense/bias:0
tf_roberta_model_2/roberta/encoder/layer_._9/attention/output/LayerNorm/gamma:0
tf_roberta_model_2/roberta/encoder/layer_._9/attention/output/LayerNorm/beta:0
tf_roberta_model_2/roberta/encoder/layer_._9/intermediate/dense/kernel:0
tf_roberta_model_2/roberta/encoder/layer_._9/intermediate/dense/bias:0
tf_roberta_model_2/roberta/encoder/layer_._9/output/dense/kernel:0
tf_roberta_model_2/roberta/encoder/layer_._9/output/dense/bias:0
tf_roberta_model_2/roberta/encoder/layer_._9/output/LayerNorm/gamma:0
tf_roberta_model_2/roberta/encoder/layer_._9/output/LayerNorm/beta:0
&lt;/denchmark-code&gt;

So one layer is missing altogether. And in the other layer, weights are missing as well.
		</comment>
		<comment id='3' author='Santosh-Gupta' date='2020-07-07T10:39:44Z'>
		The missing weights issue doesn't seem to happen if the architecture is made using keras model subclassing.
I made a colab gist showing the architecture made using model subclassing, and the same architecture made using the functional API, with the latter resulting in missing trainable weights.
&lt;denchmark-link:https://colab.research.google.com/gist/Santosh-Gupta/273361f873e4daf572fddea691b1f325/missingtrainablevars.ipynb&gt;https://colab.research.google.com/gist/Santosh-Gupta/273361f873e4daf572fddea691b1f325/missingtrainablevars.ipynb&lt;/denchmark-link&gt;

The gist uses the huggingface transformers library to create the layers, so the code is clearer to look through, though it doesn't show the layer code like in my previous colab notebook.
Another issue; using model subclassing may not be a viable workaround for TPU training my architecture; It looks like TPU training does not support subclassed model, as described in this stackoverflow post &lt;denchmark-link:https://stackoverflow.com/questions/60444486/use-tf-distribute-strategies-with-tf-keras-model-subclassing&gt;https://stackoverflow.com/questions/60444486/use-tf-distribute-strategies-with-tf-keras-model-subclassing&lt;/denchmark-link&gt;


ValueError: We currently do not support distribution strategy with a Sequential model that is created without input_shape/input_dim set in its first layer or a subclassed model.

I am also running into a different issue with the subclassed model, which is described in a different github issue (not sure if its related to model subclassing)
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/41074&gt;#41074&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='Santosh-Gupta' date='2020-07-12T06:38:04Z'>
		I tried both the nightly version of tf 2.2.0 and the tf 2.3.0-rc1 in case there was some sort of fix, but both resulted in missing trainable variables for using the functional API.
For convenience, here is a github gist of the code I ran.
&lt;denchmark-link:https://colab.research.google.com/gist/Santosh-Gupta/b02668b47744655ce32e6d427357cb35/missingtrainablevars_nightly_2-3-0-rc1.ipynb&gt;https://colab.research.google.com/gist/Santosh-Gupta/b02668b47744655ce32e6d427357cb35/missingtrainablevars_nightly_2-3-0-rc1.ipynb&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='Santosh-Gupta' date='2020-07-14T17:41:55Z'>
		I wanted to check if the inference was the same for both models, and they are. I tested it using both the Transformers library and creating each layer from scratch (Transformers library only used to copy/set consistent weights)
Here are the colab gists
&lt;denchmark-link:https://colab.research.google.com/gist/Santosh-Gupta/ec9f7c8a189a8d99e73d96fbe728aef8/model_weight_debug_scratch_public_inference.ipynb&gt;https://colab.research.google.com/gist/Santosh-Gupta/ec9f7c8a189a8d99e73d96fbe728aef8/model_weight_debug_scratch_public_inference.ipynb&lt;/denchmark-link&gt;

&lt;denchmark-link:https://colab.research.google.com/gist/Santosh-Gupta/766a27c1500a330cba6f479805dad27d/missingtrainablevarsinference.ipynb&gt;https://colab.research.google.com/gist/Santosh-Gupta/766a27c1500a330cba6f479805dad27d/missingtrainablevarsinference.ipynb&lt;/denchmark-link&gt;

I sort of went through the from scratch version in details, and I am unable to figure out why this is happening for the functional api. From my understanding if the Keras Model fit function, if the weights aren't in the list of the trainable_variables, then they won't receive gradient updates. And this issue seems prone to any Keras model that uses custom layers with the functional API.
Is this something anyone using custom Keras layers with the Functional API should be worried about?
		</comment>
		<comment id='6' author='Santosh-Gupta' date='2020-07-15T01:24:18Z'>
		Hi &lt;denchmark-link:https://github.com/Santosh-Gupta&gt;@Santosh-Gupta&lt;/denchmark-link&gt;
:
It looks like what's going on is:
The layers currently enter a 'functional api construction' mode only if all of the inputs in the first argument come from other Keras layers. However, you have None included in the inputs in the first positional arg, so it's not triggering functional api construction.
That causes the layer to get 'inlined' in the outer functional model rather than correctly included. You should be able to work around this by changing the layer api so Nones should not get passed in.
We have a major cleanup/refactoring of the Functional API mostly done that makes the functional api triggering much clearer (if any symbolic values appear in the inputs) &amp; sorts out a number of other issues as well. But, that will only land in 2.4. It's not immediately obvious if we can squeeze a fix into tf 2.3 as the RC is already out.
		</comment>
		<comment id='7' author='Santosh-Gupta' date='2020-07-15T03:05:15Z'>
		Thanks for the reply! I am wondering what you mean by 'outer functional model', I tried googling 'keras outer functional model' and 'keras outer model' but I wasn't able to find anything.
For clarity, should sending tuples/lists be avoided when sending input to layers, or just Nones should be avoided?
For example, is this ok?
 P_trans11((inputHiddenVals, someOtherInputNotFromKersaLayer))
Or should it only be this
 P_trans11(inputHiddenVals)
		</comment>
		<comment id='8' author='Santosh-Gupta' date='2020-07-15T04:48:47Z'>
		Ah sorry by outer functional model I just meant the functional model you are building (as opposed to the layers inside of the model).
Essentially rather than constructing the functional model as:
inputs -&gt; layer 1 -&gt; layer 2 -&gt; outputs
(which is what should be expected),
it ends up inlining all the contents of layer 1 &amp; layer 2:
inputs -&gt; first op in layer 1 -&gt; nested sublayer in layer 1 -&gt; etc. -&gt; first op in layer 2 ... -&gt; outputs
And so the model lacks a reference to layer 1 &amp; layer 2 themselves and misses any weights that weren't contained in their subweights.

For clarity, should sending tuples/lists be avoided when sending input to layers, or just Nones should be avoided?

Any data structure passed to the first positional arg should contain only symbolic values produced from tf.keras.inputs/other keras layers. For following positional args &amp; any keyword args, you can use arbitrary data structures that may or may not contain symbolic values.
What you're experiencing is specifically an issue when a data structure passed to the first positional arg contains an item that is not a symbolic functional input/output.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

This specific behavior is a historical edge case dating back to when Keras layers only ever accepted a single positional argument that could not be an arbitrary data structure, and all of the inputs had to be symbolic keras inputs/outputs. Unfortunately it's caused this surprising behavior when combined w/ other functionality that has been added since (automatically turning tf op layers into keras layers).
So, historically trying to pass in Nones like you're doing would have triggered a (hard to interpret) error message, because TF/Keras wouldn't be able to inline the tf ops inside the functional model when it calls the layer. Now it silently behaves in a way you didn't expect because tf ops can be used during functional API construction.
		</comment>
		<comment id='9' author='Santosh-Gupta' date='2020-07-15T12:24:42Z'>
		I am wondering if a possible workaround is either a layer or model that can append Nones to the inputs/outputs.
So something like
&lt;denchmark-code&gt;class transformer_IO(tf.keras.layers.Layer):
    def call(self, input):
        return (input, None, None, None)
&lt;/denchmark-code&gt;

or
&lt;denchmark-code&gt;class transformer_IO_model(tf.keras.Model):
    def call(self, input):
        return (input, None, None, None)
&lt;/denchmark-code&gt;

Using either as-is results in "AttributeError: 'NoneType' object has no attribute 'shape'". I am wondering if this sort of workaround is worth further exploring, or if I should just work on redesigning the layers.
For convenience, here is a colab gist of my attempts
&lt;denchmark-link:https://colab.research.google.com/gist/Santosh-Gupta/3b5a2c6bc288c58eda992e08da1986ee/workaroundattempt.ipynb&gt;https://colab.research.google.com/gist/Santosh-Gupta/3b5a2c6bc288c58eda992e08da1986ee/workaroundattempt.ipynb&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='Santosh-Gupta' date='2020-07-16T04:51:07Z'>
		Layers in a functional API can only output tensors/data structures of tensors.
As a workaround in the same vein though if you don't want to redesign the underlying layers:
You can have a layer contain a nested transformer layer, and pass the input args + Nones to the nested layer. You can then use this layer in a functional model w/o issues.
Alternatively if you're okay trying the tf nightlies, you can experiment with the refactoring I mentioned earlier by directly flipping our internal experimental flag:
&lt;denchmark-code&gt;from tensorflow.python.keras.engine import keras_tensor
keras_tensor.enable_keras_tensors()
&lt;/denchmark-code&gt;

I believe it should fix your issue and it should make the functional api generally much more reliable, but like I said it will only be landing in 2.4.
		</comment>
		<comment id='11' author='Santosh-Gupta' date='2020-07-16T06:32:53Z'>
		
from tensorflow.python.keras.engine import keras_tensor
keras_tensor.enable_keras_tensors()

This solved the issue, at least all the weight show up now. For people checking in on this, I used tf-nightly 2.4.0.dev20200715 (just in case it breaks in a future version of nightly)
I'll be testing out training soon.
		</comment>
		<comment id='12' author='Santosh-Gupta' date='2020-07-21T00:02:36Z'>
		Update on this issue: We weren't able to get a fix into 2.3 because it can't be done safely separately from the functional api internals refactoring, but we were able to add raising a meaningful error message for 2.3 in this setting rather than silently missing some of the weights.
(The proper fix is still in the nightlies guarded by the Functional API refactoring, as mentioned above.)
		</comment>
		<comment id='13' author='Santosh-Gupta' date='2020-08-04T18:57:41Z'>
		The functional API refactoring has landed in the nightlies, so this should now be fixed in the nightlies.
		</comment>
		<comment id='14' author='Santosh-Gupta' date='2020-08-04T18:57:43Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40638&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40638&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>