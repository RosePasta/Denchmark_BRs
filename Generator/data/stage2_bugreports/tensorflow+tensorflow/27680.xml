<bug id='27680' author='fabio12345' open_date='2019-04-09T11:48:24Z' closed_time='2019-07-30T20:05:24Z'>
	<summary>tf.data.Dataset.shuffle produces the same results at each dataset iteration in tensorflow 2 alpha</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes. Create a simple dataset, shuffle it and iterate through it.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 18.04
TensorFlow installed from (source or binary): pip
TensorFlow version (use command below):
2.0.0-alpha0
Python version:
Python 3.7.3
CUDA/cuDNN version:
CUDA10.0
Not relevant.
GPU model and memory:
Not relevant.

Describe the current behavior
when using tf.data.Dataset.shuffle and iterating through the dataset multiple times the shuffled order is always the same.
Describe the expected behavior
The order should change for every iteration.
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
&lt;denchmark-code&gt;import tensorflow as tf
input_array = tf.range(10)
print(input_array)
dataset = tf.data.Dataset.from_tensor_slices(input_array).shuffle(5)

def print_values():
    for val in dataset:
        print(val.numpy(), end=" ")
    print()

print_values()
print_values()
&lt;/denchmark-code&gt;

output:
&lt;denchmark-code&gt;tf.Tensor([0 1 2 3 4 5 6 7 8 9], shape=(10,), dtype=int32)
2 1 4 6 0 8 9 5 7 3 
2 1 4 6 0 8 9 5 7 3 

&lt;/denchmark-code&gt;

Other info / logs
Behaviour is correct in tensorflow 1.13.
	</description>
	<comments>
		<comment id='1' author='fabio12345' date='2019-04-11T20:53:23Z'>
		Thanks for trying TF 2.0 alpha. I was able to reproduce your behavior in TF 2.0 alpha however it executed successfully using TF 1.13.1 (after enabling eager mode) .
		</comment>
		<comment id='2' author='fabio12345' date='2019-04-12T00:27:38Z'>
		&lt;denchmark-link:https://github.com/fabio12345&gt;@fabio12345&lt;/denchmark-link&gt;
 when you say the behavior is "correct" in TF 1.13, are you talking about graph mode or eager mode? Could you provide an example of the program that you ran with TF 1.13?
When I try the following eager mode program against TF nightly, I see the same behavior as you do with TF 2.0:
&lt;denchmark-code&gt;from __future__ import print_function
import tensorflow as tf

tf.enable_eager_execution()

input_array = tf.range(10)
print(input_array)
dataset = tf.data.Dataset.from_tensor_slices(input_array).shuffle(5)

def print_values():
  for val in iter(dataset):
    print(val.numpy(), end=" ")
  print()

print_values()
print_values()
&lt;/denchmark-code&gt;

output:
&lt;denchmark-code&gt;tf.Tensor([0 1 2 3 4 5 6 7 8 9], shape=(10,), dtype=int32)
0 5 3 2 4 8 9 1 7 6 
0 5 3 2 4 8 9 1 7 6 
&lt;/denchmark-code&gt;

The behavior you are seeing is expected (both for TF 2.0 and TF 1.13 eager mode) because your program creates multiple separate iterators and the order of iteration of each is deterministic.
To see different order of elements for each epoch, you can make use the .repeat() transformation:
&lt;denchmark-code&gt;from __future__ import print_function
import tensorflow as tf

tf.enable_v2_behavior()

input_array = tf.range(10)
print(input_array)
dataset = tf.data.Dataset.from_tensor_slices(input_array).shuffle(5).repeat(2)

def print_values():
  for val in iter(dataset):
    print(val.numpy(), end=" ")
  print()

print_values()
&lt;/denchmark-code&gt;

output:
&lt;denchmark-code&gt;tf.Tensor([0 1 2 3 4 5 6 7 8 9], shape=(10,), dtype=int32)
0 2 6 5 7 8 9 1 3 4 2 5 1 3 6 8 9 0 7 4 
0 2 6 5 7 8 9 1 3 4 2 5 1 3 6 8 9 0 7 4 
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='fabio12345' date='2019-04-21T00:48:43Z'>
		&lt;denchmark-link:https://github.com/fabio12345&gt;@fabio12345&lt;/denchmark-link&gt;
 Could you try this
&lt;denchmark-code&gt;!pip install tensorflow==2.0.0-alpha0
import tensorflow as tf
input_array = tf.range(10)
print(input_array)
dataset = tf.data.Dataset.from_tensor_slices(input_array)
print(dataset)


def print_values():
  for val in dataset:
    print(val.numpy(), end=" ")

for i in range(3):
  dataset=dataset.shuffle(5,reshuffle_each_iteration=True)
  print_values()
  print()
&lt;/denchmark-code&gt;

Thanks!
		</comment>
		<comment id='4' author='fabio12345' date='2019-05-27T12:05:02Z'>
		I'm also experiencing this problem. &lt;denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#shuffle&gt;API Docs&lt;/denchmark-link&gt;
 state clearly that the intended behavior is that the dataset should be re-shuffled at each 

"the dataset should be pseudorandomly reshuffled each time it is iterated over".

I noticed the reshuffle_each_iteration parameter has no effect (tried either True, False and None), subsequent iterations of the dataset always lead the same result.
The only workaround that came to my mind is to create a lambda which calls the dataset creation function before every iteration, but I worry this would be inefficient (and not elegant):
&lt;denchmark-code&gt;data = np.arange(100)
data = data.reshape([10, 10])
def read_dataset():
    return tf.data.Dataset.from_tensor_slices(data).shuffle(100, reshuffle_each_iteration=True).batch(2)
dataset = read_dataset()
dataset_shuf = lambda: read_dataset()
# This doesn't work
for d in dataset:
    tf.print(d)

# This Works
for d in dataset_shuf():
    tf.print(d)

&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
 If the behavior would be expected as you say, then there's little use in having a "reshuffle_each_iteration" parameter in a framework that is executing eagerly by default. Moreover, repeat() is not a viable option if you need to alternate different type of computations after each epoch, because every time you iterate again it would lead the same sequence of samples.
		</comment>
		<comment id='5' author='fabio12345' date='2019-05-27T17:27:36Z'>
		&lt;denchmark-link:https://github.com/edoardogiacomello&gt;@edoardogiacomello&lt;/denchmark-link&gt;

I agree with you that reshuffle_each_iteration in eager mode does not have the expected effect. The documentation is misleading because "iteration" in the documentation is meant to imply repetition based on repeat and not based on Python iteration.
Your example of "This Works" works because a new dataset graph is created for each iteration. The value of reshuffle_each_iteration is still not respected -- setting reshuffle_each_iteration to False in your input pipeline will not result in identical order of batches between iterations.
I will create a PR that updates the documentation and renames the argument to reshuffle_on_repeat to better communicate what the argument does.
Finally, to achieve the desired effect of reshuffling across different Python iterations of a dataset, your approach based on a dataset factory is idiomatic:
&lt;denchmark-code&gt;def make_dataset():
  return tf.data.Dataset.from_tensor_slices(data).shuffle(100).batch(2)

# illustrate that batches are shuffled in a different order
for _ in range(2):
  for d in make_dataset():
    tf.print(d)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='fabio12345' date='2019-06-19T13:59:42Z'>
		It would be really great to have a way to iterate multiple times over a dataset with different orders.
In the tutorials for TensorFlow 2.0, the recommended way to train a model is:
dataset = ...
dataset = dataset.shuffle(buffer_size)
dataset = dataset.batch(batch_size)

for epoch in range(num_epochs):
    for x, y in dataset:
        train_step(model, x, y, loss_fn)
The issue with this code is that at each epoch, the model sees the batches in the same order.
It would be very useful to have a parameter like reshuffle_each_iteration, but that would work for each time we iterate over the dataset in eager mode.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

The current solution described above involves creating a new dataset each time, which doesn't seem ideal:
dataset = ...


for epoch in range(num_epochs):
    dataset_epoch = dataset.shuffle(buffer_size)
    dataset_epoch = dataset_epoch.batch(batch_size)
    for x, y in dataset_epoch:
        train_step(model, x, y, loss_fn)
		</comment>
		<comment id='7' author='fabio12345' date='2019-06-19T16:45:01Z'>
		We are actively working on this and plan to have this ready by TF 2.0 RC0. Most likely, the mechanism for controlling this will be a  option (as opposed to  option). cc &lt;denchmark-link:https://github.com/rohan100jain&gt;@rohan100jain&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='fabio12345' date='2019-07-30T20:05:26Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=27680&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=27680&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='fabio12345' date='2019-08-02T15:37:52Z'>
		For future reference, this behavior was fixed in this commit: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/e7206a7d8efceda25bce5a530fc4e79a5582be13&gt;e7206a7&lt;/denchmark-link&gt;

Now when reshuffle_each_iteration=True, iterating over the dataset multiple times in eager mode will work as expected.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

The reshuffle_each_iteration argument is True by default.
And the fix from the commit mentioned above is included in the TF 2.0 release candidate ! üçæ
# pip install tensorflow==2.0.0-rc0

dataset = tf.data.Dataset.range(10)
dataset = dataset.shuffle(buffer_size=5, reshuffle_each_iteration=False)

print([x.numpy() for x in dataset])
print([x.numpy() for x in dataset])

dataset = tf.data.Dataset.range(10)
dataset = dataset.shuffle(buffer_size=5, reshuffle_each_iteration=True)

print([x.numpy() for x in dataset])
print([x.numpy() for x in dataset])
This will output something like:

[3, 1, 2, 6, 8, 7, 5, 0, 9, 4]
[3, 1, 2, 6, 8, 7, 5, 0, 9, 4]
[4, 3, 5, 1, 8, 7, 0, 2, 6, 9]
[3, 0, 2, 5, 8, 4, 9, 6, 1, 7]

		</comment>
		<comment id='10' author='fabio12345' date='2019-08-28T08:27:02Z'>
		&lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
 : could it be possible to include an example explaining the behavior of  in eager mode with tf 2.0 in the documentation?
The current documentation only mention:

reshuffle_each_iteration: (Optional.) A boolean, which if true indicates that the dataset should be pseudorandomly reshuffled each time it is iterated over. (Defaults to True.)

which doesn't really explain in full length the new behavior
		</comment>
		<comment id='11' author='fabio12345' date='2019-08-28T17:44:26Z'>
		Sounds good. &lt;denchmark-link:https://github.com/aaudiber&gt;@aaudiber&lt;/denchmark-link&gt;
 could you please extend the  documentation to provide additional information about the semantics of  for both repeat-based epochs and iteration-based epochs? Thank you.
		</comment>
		<comment id='12' author='fabio12345' date='2020-12-03T18:11:09Z'>
		
We are actively working on this and plan to have this ready by TF 2.0 RC0. Most likely, the mechanism for controlling this will be a tf.data.Option option (as opposed to shuffle option). cc @rohan100jain

Dear &lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
 , I am working on a project that requires me to balance the classes in each batch after the dataset is being shuffled. Ideally, I don't want to create a new dataset at the beginning of each epoch. At first I considered the following approach:
def batch_balance(input: tf.data.Dataset):
    #create a function that can re-organize the elements from the shuffled dataset
    #and return a new one that is class-balanced within each batch but still random (to some extent)
    return a_new_dataset
tfds = tf.data.Dataset.from_tensor_slices([[0,0,1],[0,0,1],[0,1,0],[0,1,0],[1,0,0],[1,0,0]])
tfds = tfds.shuffle(buffer_size=6, reshuffle_each_iteration=True)
tfds = tfds.apply(batch_balance)
tfds = tfds.batch(3)
list(tfds.as_numpy_iterator())
list(tfds.as_numpy_iterator())
and I get the following output:

[0,1,0], [0,0,1], [1,0,0]
[1,0,0], [0,0,1], [0,1,0] (first iteration)


[0,1,0], [0,0,1], [1,0,0]
[1,0,0], [0,0,1], [0,1,0] (second iteration)

This does the trick for the first pass, but in fact, after applying the batch_balance function, the returned dataset no longer has the reshuffle_each_iteration feature, which leads to two identical sequences.
To preserve this reshuffle feature, I've considered writing a modified shuffle / batch, and then I notice these two functions are both implemented in the gen_dataset_ops file, which is machine generated (and very difficult to read...). Therefore, I guess the question is, are there any built-in functions that can allow me to modify the dataset after it is being shuffled w/o losing the reshuffle_each_iteration feature? You hinted earlier that tf.data.Option can be used, would you mind elaborating a bit more on this?
Thank you!
		</comment>
		<comment id='13' author='fabio12345' date='2020-12-03T18:17:24Z'>
		&lt;denchmark-link:https://github.com/ff4456&gt;@ff4456&lt;/denchmark-link&gt;
  should continue to work even after applying additional transformations. Can you share a colab to reproduce the issue?
		</comment>
		<comment id='14' author='fabio12345' date='2020-12-03T21:45:20Z'>
		Hi &lt;denchmark-link:https://github.com/aaudiber&gt;@aaudiber&lt;/denchmark-link&gt;
 , sure! Here is the link: &lt;denchmark-link:https://colab.research.google.com/drive/1gWxyYHhh5dFTmFbZiEyXwlU4SuUyPY-B?usp=sharing&gt;https://colab.research.google.com/drive/1gWxyYHhh5dFTmFbZiEyXwlU4SuUyPY-B?usp=sharing&lt;/denchmark-link&gt;

		</comment>
		<comment id='15' author='fabio12345' date='2020-12-03T22:05:45Z'>
		The function passed to apply is only called once, so you're only ever iterating over the shuffled dataset once. That first iteration is stored into balanced_list, which gets reused each time you create an iterator over the tf.data.Dataset.from_generator.
To fix this, you can update the generator function to re-iterate over the input to  each time. Here is an updated example of your code that works as expected: &lt;denchmark-link:https://colab.research.google.com/drive/1Xy8rYbrn2wyJVmvn2uFrSOo2AUHdtxuk&gt;https://colab.research.google.com/drive/1Xy8rYbrn2wyJVmvn2uFrSOo2AUHdtxuk&lt;/denchmark-link&gt;

		</comment>
		<comment id='16' author='fabio12345' date='2020-12-04T06:27:08Z'>
		
The function passed to apply is only called once, so you're only ever iterating over the shuffled dataset once. That first iteration is stored into balanced_list, which gets reused each time you create an iterator over the tf.data.Dataset.from_generator.
To fix this, you can update the generator function to re-iterate over the input to apply each time. Here is an updated example of your code that works as expected: https://colab.research.google.com/drive/1Xy8rYbrn2wyJVmvn2uFrSOo2AUHdtxuk

ah I see... Thank you so much!
		</comment>
		<comment id='18' author='fabio12345' date='2020-12-08T22:07:27Z'>
		&lt;denchmark-link:https://github.com/engrmz&gt;@engrmz&lt;/denchmark-link&gt;
 To get different orders you can use , to repeat the dataset  times, with each repetition doing a reshuffle.
		</comment>
		<comment id='19' author='fabio12345' date='2020-12-08T23:49:36Z'>
		
@engrmz To get different orders you can use data = data.repeat(num_epochs), to repeat the dataset num_epochs times, with each repetition doing a reshuffle.

Hi &lt;denchmark-link:https://github.com/aaudiber&gt;@aaudiber&lt;/denchmark-link&gt;
,
Thanks for your prompt response.
 When I use repeat(5), it repeats the dataset 5 times and shuffles it too. Doing this, I can get shuffled data in every batch. My code (as given above) considers this step as epoch 1 (as I am trying to repeat dataset using epoch loop).
Point 2: However, after 5 repetitions of whole dataset, in epoch 2, I still get the same sequence (5 repatriations) as in Point1: - same as epoch 1.
Is there any way to re-shuffle dataset after every epoch iteration (after sess.run(iterator.initializer)) without using repeat function?
		</comment>
		<comment id='20' author='fabio12345' date='2020-12-09T00:02:39Z'>
		The TF1 way of doing things is to handle multiple iterations of the dataset by using repeat(). Creating a new iterator in each epoch loop is the TF2 way. If you'd like to discuss further please open a separate issue so we can avoid hijacking this one.
		</comment>
	</comments>
</bug>