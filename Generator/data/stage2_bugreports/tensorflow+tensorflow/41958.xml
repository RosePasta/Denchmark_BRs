<bug id='41958' author='HKatoo' open_date='2020-08-01T12:11:03Z' closed_time='2020-08-20T03:35:10Z'>
	<summary>Keras evaluate method returns wrong value after loading a model</summary>
	<description>
System information

OS Platform : Windows10
TensorFlow installed from : source(pip in anaconda)
TensorFlow version : 2.3.0
Python version : 3.8.3

Describe the current behavior
Keras Sequential method:evaluate() returns wrong value after loading a model. value of loss is equal but metrics differs before and after the model is saved and loaded.
I calculated the accuracy manually using sklearn.metrics and got the same value. Therefore, the problem seems to be in this method.
In the below code, the acc1, acc2 are the same. However acc1=0.863 and acc2=0.086 in my environment.
When I ran the code in colab, this issue was not occurred.
Describe the expected behavior
Keras Sequential method:evaluate() returns same value before and after the model is saved and loaded.

This code is a modified version of this &lt;denchmark-link:https://www.tensorflow.org/tutorials/keras/save_and_load&gt;tutorial&lt;/denchmark-link&gt;
 code to reproduce the issue.
If you run the code in the above tutorial on Windows10, I think you are able to reproduce the issue.
For your information, I comment on the results in my environment.
&lt;denchmark-code&gt;import numpy as np
import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import accuracy_score

(train_images, train_labels),(test_images, test_labels) = tf.keras.datasets.mnist.load_data()
train_labels = train_labels[:1000]
test_labels = test_labels[:1000]
train_images = train_images[:1000].reshape(-1, 28 * 28) / 255.0
test_images = test_images[:1000].reshape(-1, 28 * 28) / 255.0

def create_model():
    model = tf.keras.models.Sequential([
        keras.layers.Dense(512, activation='relu', input_shape=(784,)),
        keras.layers.Dropout(0.2),
        keras.layers.Dense(10)
    ])
    model.compile(optimizer='adam',
                  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),
                  metrics=['accuracy'])
    return model

# Create first model
model = create_model()
model.fit(train_images, train_labels, epochs=5)

model.save('saved_model/my_model')

# Create second model (load the saved model)
new_model = tf.keras.models.load_model('saved_model/my_model')

# First model's results
loss1, acc1 = model.evaluate(test_images, test_labels)
accuracy_score1 = accuracy_score(test_labels, np.argmax(model.predict(test_images), axis=1))
# loss1, acc1 = [0.4392167329788208, 0.8629999756813049]
# accuracy_score1 = 0.863

# Second model's results
loss2, acc2 = new_model.evaluate(test_images, test_labels)                           
accuracy_score2 = accuracy_score(test_labels, np.argmax(new_model.predict(test_images), axis=1))
# loss2, acc2 = [0.4392167329788208, 0.0860000029206276] &lt;- THIS!!!
# accuracy_score2 = 0.863
&lt;/denchmark-code&gt;

Why is this happening?
Can you help me?
	</description>
	<comments>
		<comment id='1' author='HKatoo' date='2020-08-03T07:55:06Z'>
		Was able to reproduce the issue with TF v2.2 and TF v2.3. Please find the gist of it &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/bcbb82c4dc189486a4bd7594bfbe92f2/41958.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='HKatoo' date='2020-08-19T16:38:59Z'>
		&lt;denchmark-link:https://github.com/HKatoo&gt;@HKatoo&lt;/denchmark-link&gt;
 This is a known issue that team is working on. Please check the the comment by &lt;denchmark-link:https://github.com/k-w-w&gt;@k-w-w&lt;/denchmark-link&gt;


This is a bug with using the sparse categorical accuracy. For now, please compile the model with metrics='sparse_categorical_accuracy' instead of just 'accuracy'.

With the above modification,  and  are the same. Please check the &lt;denchmark-link:https://colab.research.google.com/gist/jvishnuvardhan/c50083ed3bdeea12ef14ef83724e6cbb/41958.ipynb&gt;gist here&lt;/denchmark-link&gt;
.
Let's monitor the progress &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/42045#issuecomment-674232499&gt;there&lt;/denchmark-link&gt;
 in that issue. Thanks!
		</comment>
		<comment id='3' author='HKatoo' date='2020-08-20T03:35:06Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 Thank you!
		</comment>
		<comment id='4' author='HKatoo' date='2020-08-20T03:35:11Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41958&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41958&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='HKatoo' date='2020-09-03T21:42:11Z'>
		This is resolved in recent tf-nightly. Please feel free to reopen if you notice the issue.
This is available in stable TF2.4 in the near future. Thanks!
		</comment>
	</comments>
</bug>