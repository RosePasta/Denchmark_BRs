<bug id='10641' author='JerrikEph' open_date='2017-06-12T02:16:00Z' closed_time='2018-02-08T17:44:56Z'>
	<summary>bug: BeamSearchDecoder should not assume that  when time &amp;gt; 0 beam will be full</summary>
	<description>
&lt;denchmark-code&gt;  scores_flat = control_flow_ops.cond(
      time &gt; 0,
      lambda: array_ops.reshape(scores, [batch_size, -1]),
      lambda: scores[:, 0])
  num_available_beam = control_flow_ops.cond(
      time &gt; 0,
      lambda: math_ops.reduce_prod(scores_shape[1:]),
      lambda: math_ops.reduce_prod(scores_shape[2:]))

  # Pick the next beams according to the specified successors function
  next_beam_size = math_ops.minimum(
      ops.convert_to_tensor(
          beam_width, dtype=dtypes.int32, name="beam_width"),
      num_available_beam)
  next_beam_scores, word_indices = nn_ops.top_k(scores_flat, k=next_beam_size)
  next_beam_scores.set_shape([static_batch_size, beam_width])
  word_indices.set_shape([static_batch_size, beam_width])
&lt;/denchmark-code&gt;

code start from
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py#L510&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py#L510&lt;/denchmark-link&gt;

Correct me if I am wrong, but I think this code is assuming that, when time &gt; 0 the beam will be full. It is true when the vocabulary is big such as is the case in machine translation. but if the vocabulary is small, the beam might won't be full when time &gt; 0 and might pose a problem.  the value of next_beam_size  in the code seems must be beam_width or it will raise an error since next_beam_scores.set_shape([static_batch_size, beam_width]), which make  next_beam_size = math_ops.minimum useless.
I am trying to write a Pointer Network BeamSearch Decoder by modifying this source file. And the vocabulary is usually small, so there is a possibility that when time == 1 the beam won't be fully filled.
I appreciate finally some one wrote a general BeamSeach decoder, that will make my life easier.
	</description>
	<comments>
		<comment id='1' author='JerrikEph' date='2017-06-13T17:27:39Z'>
		&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
, could you please take a look, it's beyond my expertise.
		</comment>
		<comment id='2' author='JerrikEph' date='2017-06-13T18:07:49Z'>
		This is indeed the assumption.  Unfortunately it's unlikely to be fixed
until after the 1.2 release and I'm away this week.  If you would like to
send a PR to fix this by using the tf.shape() on the input Tensor instead
of assuming beam_width, and adding a unit test, I can review when I return.

On Jun 13, 2017 10:28 AM, "Andrew Selle" &lt;notifications@github.com&gt; wrote:

&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 &lt;&lt;denchmark-link:https://github.com/ebrevdo&gt;https://github.com/ebrevdo&lt;/denchmark-link&gt;
&gt;, could you please take a look, it's
beyond my expertise.

â€”
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub
&lt;&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/10641#issuecomment-308189704&gt;#10641 (comment)&lt;/denchmark-link&gt;
&gt;,
or mute the thread
&lt;&lt;denchmark-link:https://github.com/notifications/unsubscribe-auth/ABtim6kdt62nhk_I7Ie5cDr45ccAy5MRks5sDsbEgaJpZM4N2l0D&gt;https://github.com/notifications/unsubscribe-auth/ABtim6kdt62nhk_I7Ie5cDr45ccAy5MRks5sDsbEgaJpZM4N2l0D&lt;/denchmark-link&gt;
&gt;
.
		</comment>
		<comment id='3' author='JerrikEph' date='2017-06-14T15:01:17Z'>
		&lt;denchmark-code&gt;def initialize(self, name=None):
    """Initialize the decoder.

    Args:
      name: Name scope for any created operations.

    Returns:
      `(finished, start_inputs, initial_state)`.
    """
    finished, start_inputs = self._finished, self._start_inputs

    log_prob_mask = array_ops.one_hot(                          # shape(batch_sz, beam_sz)
        array_ops.ones([self._batch_size], dtype=dtypes.int32),
        depth=self._beam_width, dtype=dtypes.bool)

    log_prob_zeros = array_ops.zeros([self._batch_size, self._beam_width],  # shape(batch_sz, beam_sz)
                    dtype=nest.flatten(self._initial_cell_state)[0].dtype)
    log_prob_neg_inf = array_ops.ones([self._batch_size, self._beam_width],  #shape(batch_sz, beam_sz)
                    dtype=nest.flatten(self._initial_cell_state)[0].dtype) * -float('inf')

    log_probs = array_ops.where(log_prob_mask, log_prob_zeros, log_prob_neg_inf)

    initial_state = BeamSearchDecoderState(
        cell_state=self._initial_cell_state,
        log_probs=log_probs,
        finished=finished,
        lengths=array_ops.zeros(
            [self._batch_size, self._beam_width], dtype=dtypes.int32))

    return (finished, start_inputs, initial_state)
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;

It probably is not  a good idea to push tensors with variant shape to TensorArray.
Actually I think it's a good idea to just set  to negative infinity in initialize function.
and of course set
&lt;denchmark-code&gt;  scores_flat = control_flow_ops.cond(
      time &gt; 0,
      lambda: array_ops.reshape(scores, [batch_size, -1]),
      lambda: scores[:, 0])
  num_available_beam = control_flow_ops.cond(
      time &gt; 0,
      lambda: math_ops.reduce_prod(scores_shape[1:]),
      lambda: math_ops.reduce_prod(scores_shape[2:]))

  # Pick the next beams according to the specified successors function
  next_beam_size = math_ops.minimum(
      ops.convert_to_tensor(
          beam_width, dtype=dtypes.int32, name="beam_width"),
      num_available_beam)
&lt;/denchmark-code&gt;

to a  simple reshape
&lt;denchmark-code&gt;  scores_flat = array_ops.reshape(scores, [batch_size, -1])
&lt;/denchmark-code&gt;

I will add a test unit and test it if you think it's ok.
		</comment>
		<comment id='4' author='JerrikEph' date='2017-12-20T19:15:33Z'>
		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
		</comment>
		<comment id='5' author='JerrikEph' date='2018-01-04T19:21:30Z'>
		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
		</comment>
		<comment id='6' author='JerrikEph' date='2018-01-23T23:07:33Z'>
		A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.
		</comment>
		<comment id='7' author='JerrikEph' date='2018-02-07T13:47:33Z'>
		Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
	</comments>
</bug>