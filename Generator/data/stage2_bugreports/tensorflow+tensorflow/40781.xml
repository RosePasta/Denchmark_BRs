<bug id='40781' author='armkevincheng' open_date='2020-06-24T23:16:35Z' closed_time='2020-11-30T18:47:26Z'>
	<summary>TFLite quantized hard_swish operator precision issue</summary>
	<description>
Hi TFlite team,
Git hash: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/b8a267a9fe95dea518cb04c726031e96874d26a0&gt;b8a267a&lt;/denchmark-link&gt;
, dated at Jun 9 2020
Description:
When I run quantized mobilenet v3, I found an infrequent precision issue in quantized hard_swish operator. Let me directly walk through the example to start.
Note I already stripped off every other operators in mobilenet v3 models and tensor resized to [1] to make it clear and easier to run. The text below is the tflite .mlir.
&lt;denchmark-code&gt;module attributes {tfl.description = "TOCO Converted.", tfl.schema_version = 3 : i32} {
  func @main(%arg0: tensor&lt;1x!quant.uniform&lt;u8:f32, 0.13308307528495789:105&gt;&gt;) -&gt; tensor&lt;1x!quant.uniform&lt;u8:f32, 0.075572937726974487:5&gt;&gt; attributes {tf.entry_function = {inputs = "input", outputs = "MobilenetV3/expanded_conv_6/expand/hard_swish/mul_1"}} {
    %0 = "tfl.hard_swish"(%arg0) : (tensor&lt;1x!quant.uniform&lt;u8:f32, 0.13308307528495789:105&gt;&gt;) -&gt; tensor&lt;1x240x!quant.uniform&lt;u8:f32, 0.075572937726974487:5&gt;&gt;
    return %0 : tensor&lt;1x!quant.uniform&lt;u8:f32, 0.075572937726974487:5&gt;&gt;
  }
}
&lt;/denchmark-code&gt;

A mismatch happens here when input element is 128, and here's how I calculate the expected number:
real_input_value = (quantized_input_value - input_zp) * input_scale = (128 - 105) * 0.13308307528495789 = 3.0609113
real_output_value = x * relu6(x+3) / 6 = x (if x &gt; 3) = 3.0609113
quantized_output_value = round_to_nearest(real_output_value / output_scale) + output_zp = 3.0609113 / 0.075572937726974487 + 5 = round(40.502742) + 5 = 41 + 5 = 46
while tflite gives me answer of 45 in this case, which I don't think is correct. And that's because when rescaling output back to quantized space, the number is too close to 40.5, and tflite reference implementation might lose precision somewhere so it gets &lt; 40.5 in this case, and leads to output=45 eventually. Btw, all other quantized input numbers are running fine.
When I looked at the reference implementation under tflite/kernel/reference/reference_ops.h, it seems like it's using fixed point s0.15 to represent the relu-ish value, even when it's 1.0, which leads to a error factor of 32767 / 32768 for each fixed16_t multiplier. If I have to guess, that could be a source of error. Another possibility could be float vs double. I noticed in HardSwishPrepare it stores input and output scales as float type instead of double.
Thanks in advance!
	</description>
	<comments>
		<comment id='1' author='armkevincheng' date='2020-11-24T16:37:43Z'>
		Thanks for the report! Just a bit of context here from a historical/past maintainer of this code:
Please note that there is no expectation of bit-exactness across implementations of tflite ops; some off-by-one differences are always going to exist due to necessary compromises with efficient implementation considerations on each target hardware.
From your debugging, this looks like a case where a value close to a half-integer gets pushed into the wrong direction by an intermediate rounding; the only way to be bit-exact in a cast like this would be to avoid intermediate roundings at all, which in the case of this function, would mean avoiding using NEON fixed-point multiplications, in NEON optimized code. So even if we modified this reference code to be bit-exact, we still probably wouldn't fix the NEON optimized code. This example illustrates why we don't even aim for our reference code to be bit-exact-normative.
Going beyond ARM CPU, if we consider other hardware backends where this will want to run in IEEE float16 for optimal performance (e.g. GPUs), that gives more examples why bit-exactness can't be the goal.
That's to say that an off-by-one difference isn't necessarily in itself a bug here. But of course, it's always interesting to improve this code if you can think of a better compromise of accuracy and usefulness of this reference code as a porting starting point for optimized implementations.
		</comment>
		<comment id='2' author='armkevincheng' date='2020-11-30T18:47:25Z'>
		Yes thanks for the feedback. After a second thought I generally agree with you and I think there's no point to pursue bit-exact or mathematically true result.
		</comment>
		<comment id='3' author='armkevincheng' date='2020-11-30T18:47:27Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40781&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40781&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>