<bug id='30666' author='Uiuran' open_date='2019-07-12T22:00:02Z' closed_time='2019-08-12T00:45:57Z'>
	<summary>tf.keras.layers.Conv2D does not initialize kernel and bias when called inside name_scope</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.14.0
Python version: Python 2
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: -
GPU model and memory: No GPU

Describe the current behavior
It throws this
FailedPreconditionError: Error while reading resource variable name9/conv_linear/bias from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/gcnn2d_d1/conv_linear/bias)
when trying to run the graph with a feed dict to get variable value
Describe the expected behavior
Expect to see a normal run since the variables are initialized inside the Conv2D function
Code to reproduce the issue
graph = tf.Graph()
with graph.name_scope('name9'):
with graph.as_default():
sign_in = tf.placeholder(tf.float32,(data_shape[0],data_shape[1],data_shape[2],data_shape[3]), name='signal_in')
&lt;denchmark-code&gt;conv = tf.keras.layers.Conv2D(  10, (10,2), padding='valid', name='conv_linear', use_bias=True,  kernel_initializer=tf.initializers.lecun_normal(seed=137), bias_initializer=tf.initializers.lecun_normal(seed=137)  )(sign_in)
&lt;/denchmark-code&gt;

data_tensor = np.random.rand(10,40,2,1)
feed_dict = {
graph.get_tensor_by_name('signal_in:0'):data_tensor
}
op_value = session.run('/name9/conv_linear:0', feed_dict=feed_dict)
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='Uiuran' date='2019-07-15T01:58:19Z'>
		Update, just corrected it by using
with graph.as_default(): with tf.variable_scope('name9'):
In relation to this one has to say " never use name_scope to define ops, unless you wanna recover the name_scope of the graph".
and then before running the Op in question
 tf.global_variables_initializer() sess.run(tf.get_operations()[-1])
but have to do and silly if condition to see if any init Op was not added.
My question relies, why add many Initializers on Variables such Convolutions Variables Scopes and then only initialize them in the run ? Why not immediatly initialize it ?
What do you think ?
		</comment>
		<comment id='2' author='Uiuran' date='2019-07-15T12:53:19Z'>
		&lt;denchmark-link:https://github.com/Uiuran&gt;@Uiuran&lt;/denchmark-link&gt;
 ,
We tried executing the code given by you and we are encountering the error mentioned below
KeyError: "The name 'signal_in:0' refers to a Tensor which does not exist. The operation, 'signal_in', does not exist in the graph."
		</comment>
		<comment id='3' author='Uiuran' date='2019-07-16T20:20:34Z'>
		graph = tf.Graph()
data_shape=(10,40,2,1)

tf.reset_default_graph()
with graph.as_default():
  with tf.variable_scope('name9'):  
    sign_in = tf.placeholder(tf.float32,(data_shape[0],data_shape[1],data_shape[2],data_shape[3]), name='signal_in')
    conv = tf.keras.layers.Conv2D(  10, (10,2), padding='valid', name='conv_linear', use_bias=True,  kernel_initializer=tf.initializers.lecun_normal(seed=137), bias_initializer=tf.initializers.lecun_normal(seed=137)  )(sign_in)

data_tensor = np.random.rand(10,40,2,1)
    
feed_dict = {
  graph.get_tensor_by_name('name9/signal_in:0'):data_tensor
}

session = tf.Session(graph=graph)
op_value = session.run('name9/conv_linear/BiasAdd:0', feed_dict=feed_dict)
alternatively you could do graph.name_scope('name9')
However are still required to tf.global_variables_initializer() on the run function (the custom one i made)
What does not make sense, what makes sense is the initializator to be added programmatically as you add the variables op in the layers, then just get them and run in a shot.
		</comment>
		<comment id='4' author='Uiuran' date='2019-07-16T20:28:28Z'>
		maybe this is just a more aesthetical question, either to add the op programmatically or add all in once, like the changes in the 2.0 version. However it seems to me that it does change the inner workings of tensorflow in a case of serving models to learn in clusters.
		</comment>
		<comment id='5' author='Uiuran' date='2019-07-17T14:51:34Z'>
		I was able to replicate the issue with TF version 1.14.Thanks!
		</comment>
		<comment id='6' author='Uiuran' date='2019-07-18T17:53:58Z'>
		It looks like you are mixing low-level TF APIs with Keras, which can yield strange behavior. When using Keras, tf.keras.Input is preferable to tf.placeholder, and model.predict is preferable to trying to run individual outputs. You can see some examples &lt;denchmark-link:https://www.tensorflow.org/guide/keras&gt;here&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='7' author='Uiuran' date='2019-07-18T18:26:33Z'>
		however the conv implementations force us to use keras, what should be only
a short to a name-space with all ops to do convolution. Also, why input is
supposed to have a different behavior than tf.placeholder ?

Keras, supposing to make the things easier, changes the behavior that one
is waiting from the low level API and make things lot harder to debug.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Thu, Jul 18, 2019 at 3:01 PM Karmel Allison ***@***.***&gt; wrote:
 It looks like you are mixing low-level TF APIs with Keras, which can yield
 strange behavior. When using Keras, tf.keras.Input is preferable to
 tf.placeholder, and model.predict is preferable to trying to run individual
 outputs. You can see some examples here
 &lt;https://www.tensorflow.org/guide/keras&gt;.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#30666?email_source=notifications&amp;email_token=AAOOCXZHNTWSSHP7SNMHE4DQACVXVA5CNFSM4ICSCNQ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2JI7GQ#issuecomment-512921498&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AAOOCX7QNAH4SZV2BLHNCVDQACVXVANCNFSM4ICSCNQQ&gt;
 .



		</comment>
		<comment id='8' author='Uiuran' date='2019-07-18T22:16:24Z'>
		I think that gives me a better sense for what you are trying to do, and I would highly recommend trying out the &lt;denchmark-link:https://www.tensorflow.org/beta&gt;TF 2.0 beta&lt;/denchmark-link&gt;
, in which you can just pass the np array directly through the conv layer:
&lt;denchmark-code&gt;data = np.random.rand(10,40,2,1)
conv = tf.keras.layers.Conv2D(  10, (10,2), padding='valid', name='conv_linear', use_bias=True,  kernel_initializer=tf.initializers.lecun_normal(seed=137), bias_initializer=tf.initializers.lecun_normal(seed=137))
conv(data)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='Uiuran' date='2019-07-21T21:35:23Z'>
		Agree with you, TF 2.0 beta seems to be beautifull and intuitive to use. But i would like to know how much they changed the API and the Backend.
However to me seems that " , either to add the op programmatically or add all in once " affects the behaviour in the case of distributed computing, what make of it a bug or an interface feature request, am i missing anything ?
		</comment>
		<comment id='10' author='Uiuran' date='2019-08-09T00:33:28Z'>
		&lt;denchmark-link:https://github.com/Uiuran&gt;@Uiuran&lt;/denchmark-link&gt;
 Please check &lt;denchmark-link:https://www.tensorflow.org/beta/guide/effective_tf2&gt;effective_tf2&lt;/denchmark-link&gt;
, and &lt;denchmark-link:https://github.com/tensorflow/tensorflow/releases&gt;release_notes&lt;/denchmark-link&gt;
 to know how much was changed in 2.0. There are lots of other resources too including &lt;denchmark-link:https://www.tensorflow.org/tutorials&gt;tutorials&lt;/denchmark-link&gt;
.
Please let us know whether we can close the issue. Thanks!
		</comment>
		<comment id='11' author='Uiuran' date='2019-08-12T00:45:56Z'>
		That's fine for now, i will have look on 2.0beta, the fact is i have a lot of code and in moment cant stop sprinting to migrate or to learn the changes, but will do it asap.
Thank you.
		</comment>
		<comment id='12' author='Uiuran' date='2019-08-12T00:45:58Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=30666&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=30666&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>