<bug id='34376' author='igorhoogerwoord' open_date='2019-11-18T11:44:29Z' closed_time='2019-12-05T00:42:47Z'>
	<summary>RunTimeError: DST Tensor Not initialised - while saving checkpoint, not while training.</summary>
	<description>
System information


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No


OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18 LTS


TensorFlow installed from (source or binary): binary


TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0


Python version: 3.6


CUDA/cuDNN version:
nvidia-driver-418
cuda-10-0
libcudnn7=7.6.2.24-1+cuda10.0
libcudnn7-dev=7.6.2.24-1+cuda10.0
libnvinfer5=5.1.5-1+cuda10.0
libnvinfer-dev=5.1.5-1+cuda10.0


GPU model and memory: T4 16GB


Describe the current behavior
The GPU memory is only used for 58%, detected by using limit_growth option:
GPU Total memory:  15843721216
GPU Free memory:  6628966400
GPU Used memory:  9214754816
GPU Memory Percentage Used:  58
&lt;denchmark-code&gt;    if gpus:
      try:
        # Currently, memory growth needs to be the same across GPUs
        for gpu in gpus:
          tf.config.experimental.set_memory_growth(gpu, True)
        logical_gpus = tf.config.experimental.list_logical_devices('GPU')
        print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")
      except RuntimeError as e:
        # Memory growth must be set before GPUs have been initialized
        print(e)
&lt;/denchmark-code&gt;

Training runs fine, but only when saving the model as a checkpoint, it crashes:
&lt;denchmark-code&gt;
        def createCheckpointManager(self):

            if self.model_weight_path:
                checkpoint_dir = self.model_weight_path + '/' + self.training_session_id + '_checkpoint/ckpt'

                print("Store Checkpoint: ",checkpoint_dir)

                self.checkpoint = tf.train.Checkpoint(optimizer=self.optimizer,
                                                 encode_network=self.encode_network,
                                                 decode_network=self.decode_network)

                self.manager = tf.train.CheckpointManager(self.checkpoint,checkpoint_dir, max_to_keep=3)
            else:
                print("Model checkpoint path not found, not saving checkpoint.")

CheckpointManager.save()
&lt;/denchmark-code&gt;

Log:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/pool.py", line 119, in worker
    result = (True, func(*args, **kwds))
  File "/home/[]/[]/[]/v1/[]/[].py", line 107, in runTrainingEpoch
    [].storeCheckpoint()
  File "/home/[]/[]/[]/v1/[]/[].py", line 179, in storeCheckpoint
    self.manager.save()
  File "/home/[]/env/lib/python3.6/site-packages/tensorflow_core/python/training/checkpoint_management.py", line 720, in save
    save_path = self._checkpoint.write(prefix)
  File "/home/[]/env/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/util.py", line 1819, in write
    output = self._saver.save(file_prefix=file_prefix)
  File "/home/[]/env/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/util.py", line 1155, in save
    file_prefix=file_prefix_tensor, object_graph_tensor=object_graph_tensor)
  File "/home/[]/env/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/util.py", line 1103, in _save_cached_when_graph_building
    save_op = saver.save(file_prefix)  
  File "/home/[]/env/lib/python3.6/site-packages/tensorflow_core/python/training/saving/functional_saver.py", line 230, in save
    sharded_saves.append(saver.save(shard_prefix))
  File "/home/[]/env/lib/python3.6/site-packages/tensorflow_core/python/training/saving/functional_saver.py", line 69, in save
    tensors.append(spec.tensor)
  File "/home/[]/env/lib/python3.6/site-packages/tensorflow_core/python/training/saving/saveable_object.py", line 52, in tensor
    return self._tensor() if callable(self._tensor) else self._tensor
  File "/home/[]/env/lib/python3.6/site-packages/tensorflow_core/python/training/saving/saveable_object_util.py", line 94, in f
    return array_ops.identity(x)
  File "/home/[]/env/lib/python3.6/site-packages/tensorflow_core/python/util/dispatch.py", line 180, in wrapper
    return target(*args, **kwargs)
  File "/home/[]/env/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py", line 209, in identity
    copied = input._copy()  # pylint: disable=protected-access
  File "/home/[]/env/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 1015, in _copy
    new_tensor = self._copy_nograd(ctx, device_name)
  File "/home/[]/env/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 1008, in _copy_nograd
    new_tensor = self._copy_to_device(device_name)
RuntimeError: Dst tensor is not initialized.
&lt;/denchmark-code&gt;

Describe the expected behavior
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
The model is the exact implementation of the CVAE tutorial from the Tensorflow website:
&lt;denchmark-code&gt;self.optimizer = tf.keras.optimizers.Adam(1e-4)

       self.encode_network = tf.keras.Sequential(
              [
                  tf.keras.layers.InputLayer(input_shape=(self.width, self.height, 3)),
                  tf.keras.layers.Conv2D(filters=32, kernel_size=4, strides=(2, 2), activation='relu'),
                  tf.keras.layers.Conv2D(filters=64, kernel_size=2, strides=(2, 2), activation='relu'),
                  tf.keras.layers.Flatten(),
                  tf.keras.layers.Dense(self.latent_dim + self.latent_dim),
              ]
            )

            # Deconvolution 2D Transpose Formula:
            # Output Shape = Input Shape * stride

            self.decode_network = tf.keras.Sequential(
                [
                  tf.keras.layers.InputLayer(input_shape=(self.latent_dim,)),
                  tf.keras.layers.Dense(units=64 * 64 * 128, activation=tf.nn.relu),
                  tf.keras.layers.Reshape(target_shape=(64, 64, 128)),
                  tf.keras.layers.Conv2DTranspose(
                      filters=64,
                      kernel_size=3,
                      strides=(2, 2),
                      padding="SAME",
                      activation='relu'),
                  tf.keras.layers.Conv2DTranspose(
                      filters=32,
                      kernel_size=3,
                      strides=(2, 2),
                      padding="SAME",
                      activation='relu'),
                  tf.keras.layers.Conv2DTranspose(filters=3, kernel_size=2, strides=(1, 1), padding="SAME")
                ]
            )
&lt;/denchmark-code&gt;

Because the layer size is increased, I suspect it has something to do with memory, because earlier issues refer the 'DST tensor' issue to a memory problem with the GPU.
However this is not possible because only 58% is utilised and training runs fine, therefore it seems there is an issue with saving the checkpoint.
	</description>
	<comments>
		<comment id='1' author='igorhoogerwoord' date='2019-11-19T18:56:10Z'>
		&lt;denchmark-link:https://github.com/igorhoogerwoord&gt;@igorhoogerwoord&lt;/denchmark-link&gt;
 Can you share a github gist of the issue preferably on colab. Thanks!
		</comment>
		<comment id='2' author='igorhoogerwoord' date='2019-11-20T01:16:10Z'>
		&lt;denchmark-link:https://github.com/gowthamkpr&gt;@gowthamkpr&lt;/denchmark-link&gt;
 thanks for getting back on this, what would you need me to share specifically in the gist or colab?
		</comment>
		<comment id='3' author='igorhoogerwoord' date='2019-11-20T19:52:01Z'>
		&lt;denchmark-link:https://github.com/igorhoogerwoord&gt;@igorhoogerwoord&lt;/denchmark-link&gt;
 Please try Reproducing your issue &lt;denchmark-link:https://colab.sandbox.google.com/notebooks/welcome.ipynb&gt;here&lt;/denchmark-link&gt;
. May be its related to RAM issue. Thanks!
		</comment>
		<comment id='4' author='igorhoogerwoord' date='2019-11-27T08:35:56Z'>
		Haven't had time to reproduce it in colab, but one pointer to what made this issue disappear may have been that I had too little RAM on the workstation. After testing the same code on a server with more RAM the issue wasn't there.
		</comment>
		<comment id='5' author='igorhoogerwoord' date='2019-12-05T00:42:47Z'>
		Great good to know that. I am gonna close this issue as it has been resolved. Please add additional comments and we can open this issue again. Thanks!
		</comment>
		<comment id='6' author='igorhoogerwoord' date='2019-12-05T00:42:49Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34376&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34376&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='igorhoogerwoord' date='2019-12-05T02:09:16Z'>
		Is it related to RAM however?
Is the runtime error the right way to indicate a memory issue?
Why does it appear during saving the checkpoint and not during training?
		</comment>
	</comments>
</bug>