<bug id='2740' author='ppwwyyxx' open_date='2016-06-08T21:58:55Z' closed_time='2016-06-30T18:31:52Z'>
	<summary>ExponentialMovingAverage.average duplicates the current scope name</summary>
	<description>
Using tensorflow nightly.
import tensorflow as tf
with tf.name_scope('scope'):
    x = tf.Variable(42, dtype=tf.float32)
    ema = tf.train.ExponentialMovingAverage(decay=0.9)
    apply_op = ema.apply([x])
    average = ema.average(x)
    print average.name   # 'scope/scope/Variable/ExponentialMovingAverage:0'
    print ema.average_name(x)  # 'scope/Variable/ExponentialMovingAverage'
	</description>
	<comments>
		<comment id='1' author='ppwwyyxx' date='2016-06-13T23:49:16Z'>
		The problem seems to come from &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/slot_creator.py#L83&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/slot_creator.py#L83&lt;/denchmark-link&gt;
, where the name of an op is used to create a variable. But the name of the op already contain the current scope, therefore the variable to create will contain the scope twice.
		</comment>
		<comment id='2' author='ppwwyyxx' date='2016-06-20T18:41:59Z'>
		It's been a while. I suppose this is a bug right?..
		</comment>
		<comment id='3' author='ppwwyyxx' date='2016-06-27T18:26:10Z'>
		Yes that looks like a bug. I'm asking the author of the code to do the quick fix.
		</comment>
		<comment id='4' author='ppwwyyxx' date='2016-06-28T13:33:00Z'>
		Unfortunately, after checking internally, fixing this bug would result in losing backward compatibility, which we would like to avoid both for internal and external users. So, right now, I am closing the issue.
&lt;denchmark-link:https://github.com/girving&gt;@girving&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='ppwwyyxx' date='2016-06-28T14:48:40Z'>
		Isn't it true that ExponentialMovingAverage is used currently almost always outside any scopes? It shouldn't break (many) existing use cases. If I remember correctly, the class simply doesn't work well if the scope name is duplicated and users have to do manual hacking to fix the variable names.
The workaround that I'm using currently is to wrap the class with
with tf.name_scope(None):
   ema = tf.train.ExponentialMovingAverage(...)
   # [rest of the code]
		</comment>
		<comment id='6' author='ppwwyyxx' date='2016-06-28T21:57:27Z'>
		It's not just ExponentialMovingAverage, but also the accumulators used by optimizers that have this behavior. Anyone who's using either of these inside a scope of any kind would suffer backwards-incompatible checkpoints due to the naming difference. Especially given there is an easy workaround (i.e. do not create / use ExponentialMovingAverage or Optimizers inside a name scope), the pain that fixing this would cause outweighs the benefits.
		</comment>
		<comment id='7' author='ppwwyyxx' date='2016-06-28T22:03:38Z'>
		We don't need to share the create_slot function between EMA and optimizers. It's essentially just a variable creation.
		</comment>
		<comment id='8' author='ppwwyyxx' date='2016-06-28T22:28:19Z'>
		Two notes:

create_slots is more than just a variable creation -- there's some subtle logic in there to deal with properly supporting slots for partitioned variables. So I'd think reusing the logic makes sense.
Even if we were to only fix this for ExponentialMovingAverage, it would still be a backwards-incompatible graph change for anyone who was using EMA inside a name scope.

		</comment>
		<comment id='9' author='ppwwyyxx' date='2016-06-28T22:36:09Z'>
		Is it possible to keep the compatibility for now but print a warning about
a future change that will break compatibility (similar to numpy) ?
		</comment>
		<comment id='10' author='ppwwyyxx' date='2016-06-29T17:31:12Z'>
		&lt;denchmark-link:https://github.com/ppwwyyxx&gt;@ppwwyyxx&lt;/denchmark-link&gt;
 I'm not sure I follow.  How would we be able to safely break backwards compatibility in the future?  An advance warning that models will break is not a sufficient fix.
		</comment>
		<comment id='11' author='ppwwyyxx' date='2016-06-29T20:50:58Z'>
		I mean it will provide a time window for people to make necessary changes to prepare for a future break. But right it won't safely break compatibility.
		</comment>
		<comment id='12' author='ppwwyyxx' date='2016-06-29T21:50:29Z'>
		&lt;denchmark-link:https://github.com/ppwwyyxx&gt;@ppwwyyxx&lt;/denchmark-link&gt;
 I don't think it is worth making this change.  The name fix would be nice, and I agree that the current code is a mistake, but there is a high bar for breaking existing models.
		</comment>
		<comment id='13' author='ppwwyyxx' date='2016-11-27T06:16:25Z'>
		On latest tensorflow, the with tf.name_scope(None) hack still introduces variables with duplicated scope name:
def f():
    v = tf.get_variable('W', [1])
    v = v + 1
    with tf.name_scope(None):
        ema = tf.train.ExponentialMovingAverage(decay=0.9, name='EMA')
        emaop = ema.apply([v])
        average_v = ema.average(v)

with tf.variable_scope('scope'):
    f()
print([k.name for k in tf.global_variables()])
will print:
&lt;denchmark-code&gt;[u'scope/W:0', u'scope/add/EMA:0', u'scope/scope/add/EMA/biased:0', 
u'scope/scope/add/EMA/local_step:0']
&lt;/denchmark-code&gt;

		</comment>
		<comment id='14' author='ppwwyyxx' date='2016-11-30T07:00:23Z'>
		The recently-introduced new variables in EMA also brings error when using with reuse=True.
The example below seems like a common pattern in batch normalization. It works before, but now:
def f(v):
    ema = tf.train.ExponentialMovingAverage(0.9)
    vema = ema.apply([v])
    return vema

with tf.variable_scope('s'):
    v1 = tf.get_variable('W', shape=[])
    v1 = v1 + 1
    f(v1)
with tf.variable_scope('s', reuse=True):
    v2 = tf.get_variable('W', shape=[])
    v2 = v2 + 1
    f(v2)
&lt;denchmark-code&gt;ValueError: Variable s/s_1/s_1/add/ExponentialMovingAverage/biased does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?
&lt;/denchmark-code&gt;

		</comment>
		<comment id='15' author='ppwwyyxx' date='2016-12-01T23:30:41Z'>
		There are a number of TF paradigms that previously worked because ExponentialMovingAverage didn't respect variable scopes (see &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/5652&gt;#5652&lt;/denchmark-link&gt;
, for example). In the case you have here, I'm not sure that I understand the semantics of what you are trying to do: regardless of debiasing, it is an error to call apply on the same variable multiple times: "ValueError: If the moving average of one of the variables is already being computed." (from  description of ). Are you arguing that the wrong error message is thrown in this case?
		</comment>
		<comment id='16' author='ppwwyyxx' date='2016-12-01T23:37:56Z'>
		&lt;denchmark-link:https://github.com/joel-shor&gt;@joel-shor&lt;/denchmark-link&gt;
 Oh. You can change 'W' to 'W2' and move it outside of the scope, it still throws error. I'm only trying to point out that EMA cannot work inside reuse=True.
By the way I don't think I'm calling EMA on the same tensors, because calling  twice creates two .
		</comment>
		<comment id='17' author='ppwwyyxx' date='2016-12-05T16:37:15Z'>
		
The issue should be fixed currently; zero_debias is no longer the default in ExponentialMovingAverage.
There is a strong possibility that is will be the default in the future.
ExponentialMovingAverage create variables, so for future safety you might consider using it as if it respected variable scopes (ie don't put it in resuse=True variable scopes that it doesn't need to be in)

		</comment>
		<comment id='18' author='ppwwyyxx' date='2016-12-05T17:19:06Z'>
		&lt;denchmark-link:https://github.com/joel-shor&gt;@joel-shor&lt;/denchmark-link&gt;
 Thanks. Since there are dedicated issues for the bug we can use them to track. This issue is more about the naming -- and I see there are still very long variable names with duplicated scopes for the newly-introduced biased/local_step variables. Should there be a fix for this?
def f(v):
    ema = tf.train.ExponentialMovingAverage(0.9)
    vema = ema.apply([v])
    return vema
with tf.variable_scope('s'):
    v1 = tf.get_variable('W', shape=[10,10,10,10])
    v1 = v1 + 1
    f(v1)
print [k.name for k in tf.all_variables()]
# [u's/W:0', u's/s/add/ExponentialMovingAverage:0', u's/s/s/add/ExponentialMovingAverage/biased:0', u's/s/s/add/ExponentialMovingAverage/local_step:0']
		</comment>
		<comment id='19' author='ppwwyyxx' date='2017-08-21T23:53:45Z'>
		We are not going to fix this behavior. The reason is that there are two relevant scopes: the scope that the variable was created in, and the scope that the EMA was created in. There will be potential ambiguity if one is removed. Since removing the duplication isn't clearly better, we're going to keep this behavior.
		</comment>
		<comment id='20' author='ppwwyyxx' date='2018-10-16T16:21:07Z'>
		Hi &lt;denchmark-link:https://github.com/joel-shor&gt;@joel-shor&lt;/denchmark-link&gt;
,  I am facing the same problem while trying to reuse batch-normalization in a network which is used twice. I get error "/convnet_branch/conv1_1_bn/moments/Squeeze/ExponentialMovingAverage/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=tf.AUTO_REUSE in VarScope?"
Can you suggest how to deal with this problem, I read all the above comments but couldn't figure out how it could be solved.
Thanks a lot!
		</comment>
		<comment id='21' author='ppwwyyxx' date='2018-11-25T08:34:31Z'>
		Hi &lt;denchmark-link:https://github.com/nainadhingra2012&gt;@nainadhingra2012&lt;/denchmark-link&gt;
. On tensorflow 1.12.0, I had the same problem and fixed it by adding the line:
&lt;denchmark-code&gt;        with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):
&lt;/denchmark-code&gt;

before ema.apply
		</comment>
	</comments>
</bug>