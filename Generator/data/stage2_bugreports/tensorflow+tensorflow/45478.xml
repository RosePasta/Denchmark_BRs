<bug id='45478' author='shlomi-amitai' open_date='2020-12-08T15:33:17Z' closed_time='2020-12-21T11:39:47Z'>
	<summary>Fail to run tflite on DSP - BATCH_TO_SPACE_ND failed to prepare</summary>
	<description>
@tensorflow/micro
System information


Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Android


TensorFlow installed from (source or binary):
source (dde0fe7)


Tensorflow version (commit SHA if source):
tf-nightly 2.4


Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):
dsp on mobile


Describe the problem
INFO: Created TensorFlow Lite delegate for Hexagon.
INFO: Initialized TensorFlow Lite runtime.
INFO: Interpreter::UseNNAPI() is deprecated. Use tflite::NnApiDelegate() directly instead.
INFO: Hexagon delegate: 52 nodes delegated out of 88 nodes with 10 partitions.
ERROR: tensorflow/lite/kernels/batch_to_space_nd.cc:81 output_batch_size % block_shape[dim] != 0 (1 != 0)
ERROR: Node number 6 (BATCH_TO_SPACE_ND) failed to prepare.

tflite model attached
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/5660174/myModel4DSP.zip&gt;myModel4DSP.zip&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='shlomi-amitai' date='2020-12-09T23:02:12Z'>
		Seems like a Micro issue.
		</comment>
		<comment id='2' author='shlomi-amitai' date='2020-12-10T05:28:18Z'>
		From the error message, it looks like the Lite (not Micro) kernels are being use.
&lt;denchmark-link:https://github.com/shlomi-amitai&gt;@shlomi-amitai&lt;/denchmark-link&gt;
 can you please confirm if you are looking to run on the Android CPU + delegates (in which case this would be a Tensorflow Lite Mobile issue) or on an embedded DSP (in which case this would be a TensorflowLite Micro issue).
		</comment>
		<comment id='3' author='shlomi-amitai' date='2020-12-10T06:10:01Z'>
		Hi, I'm trying to run a quantized tflite model on an android mobile DSP.
below listed conversion's params (I use IOQ):
converter.optimizations = [tf.lite.Optimize.DEFAULT] # dynamic range quantization
if quantType == 'ffq' or quantType == 'ioq':
converter.representative_dataset = representative_data_gen  # + float fallback quantization
if quantType == 'ioq':
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]  # +integer only quantization
converter.inference_input_type = tf.int8 # +integer only quantization
converter.inference_output_type  = tf.int8
		</comment>
		<comment id='4' author='shlomi-amitai' date='2020-12-21T11:39:46Z'>
		Solved.
For those who might meet this issue.
Input batch size should be changed to the exact number (e.g. 1)  instead of None (as when training).
		</comment>
		<comment id='5' author='shlomi-amitai' date='2020-12-21T11:39:48Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45478&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45478&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>