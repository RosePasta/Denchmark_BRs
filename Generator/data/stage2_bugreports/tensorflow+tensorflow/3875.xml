<bug id='3875' author='leondz' open_date='2016-08-17T10:54:26Z' closed_time='2017-02-06T23:22:34Z'>
	<summary>Core dump running examples w/gpu</summary>
	<description>
Running the post-build test throws a Floating point exception:
&lt;denchmark-code&gt;$ bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.7.5 locally
I tensorflow/core/common_runtime/gpu/gpu_init.cc:118] Found device 0 with properties: 
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:81:00.0
Total memory: 12.00GiB
Free memory: 11.87GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:138] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:148] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:868] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:81:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:868] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:81:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:868] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:81:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:868] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:81:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:868] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:81:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:868] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:81:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:868] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:81:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:868] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:81:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:868] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:81:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:868] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:81:00.0)
Floating point exception (core dumped)

&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Environment info&lt;/denchmark-h&gt;

Operating System: Ubuntu 14.04 x86_64
Installed version of CUDA and cuDNN:
&lt;denchmark-code&gt;-rw-r--r-- 1 root root   322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so -&gt; libcudart.so.7.5
lrwxrwxrwx 1 root root       19 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -&gt; libcudart.so.7.5.18
-rwxr-xr-x 1 root root   383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18
-rw-r--r-- 1 root root   720192 Aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a
-rwxr-xr-x 1 root root 59909104 Aug 17 11:12 /usr/local/cuda/lib64/libcudnn.so
-rwxr-xr-x 1 root root 59909104 Aug 17 11:12 /usr/local/cuda/lib64/libcudnn.so.5
-rwxr-xr-x 1 root root 59909104 Aug 17 11:12 /usr/local/cuda/lib64/libcudnn.so.5.0.5
-rw-r--r-- 1 root root 58775484 Aug 17 11:12 /usr/local/cuda/lib64/libcudnn_static.a

&lt;/denchmark-code&gt;

If installed from source, provide:

TF commit hash 33b336a
Bazel 0.3.1

&lt;denchmark-code&gt;$ bazel version
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Jan 01 00:00:00 1970 (0)
Build timestamp: Thu Jan 01 00:00:00 1970 (0)
Build timestamp as int: 0
$ apt show bazel
Package: bazel
Version: 0.3.1
Depends: google-jdk | java8-jdk | java8-sdk, pkg-config, zip, g++,
 zlib1g-dev, unzip, bash-completion
...
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Steps to reproduce&lt;/denchmark-h&gt;

(steps taken from &lt;denchmark-link:https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#installing-from-sources&gt;https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#installing-from-sources&lt;/denchmark-link&gt;
)

Install CUDA 7.5 under x86_64 for Ubuntu 14.04 using deb (network)
Install cuDNN 5.0 for Linux (Nvidia no longer supplies cudnn4.0 for CUDA 7.5, only for 7.0)
Install Bazel
Build tensorflow using Bazel
Run $ bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu

&lt;denchmark-h:h3&gt;What have you tried?&lt;/denchmark-h&gt;


Taking off the --use_gpu flag leads to successful execution
The nvidia CUDA examples for deviceQuery and bandwidthTest work OK
The GPU is not in exclusive mode (#1534); logs of nvidia-smi -a below
gdb output is included below, too

&lt;denchmark-h:h3&gt;Logs or other output that would be helpful&lt;/denchmark-h&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/422383/tf-gdb.txt&gt;tf-gdb.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/422382/tf-nvsmi.txt&gt;tf-nvsmi.txt&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='leondz' date='2016-08-19T05:08:27Z'>
		I also meet this problem, but not everytime. according to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/tutorials/example_trainer.cc#L115&gt;source code&lt;/denchmark-link&gt;
, variable is randomly initialized, maybe some numerical operations cause this problem.
		</comment>
		<comment id='2' author='leondz' date='2016-08-22T21:26:40Z'>
		This is not related to the numerics themselves and the bug disappears for me when --num_concurrent_sessions=1 is passed in. The different sessions are apparently writing to the same memory locations. Stay tuned for a fix.
		</comment>
		<comment id='3' author='leondz' date='2016-09-13T07:44:48Z'>
		Hi
I have the exact same problem, running a TITAN X (Pascal) on Ubuntu 16.04, CUDA 8 + patch, cuDNN 5.1 and Bazel 0.3.1
Passing --num_concurrent_sessions=1 does not make the bug disappear for me.
Running without --use_gpu, it works
Running gdb bazel-bin/tensorflow/cc/tutorials_example_trainer and then r --use_gpu, I get
Thread 39 "tutorials_examp" received signal SIGFPE, Arithmetic exception. [Switching to Thread 0x7fffb1b72700 (LWP 20294)] 0x00005555560f9a40 in tensorflow::functor::CastFunctor&lt;Eigen::GpuDevice, float, int&gt;::operator()(Eigen::GpuDevice const&amp;, Eigen::TensorMap&lt;Eigen::Tensor&lt;float, 1, 1, long&gt;, 16&gt;, Eigen::TensorMap&lt;Eigen::Tensor&lt;int const, 1, 1, long&gt;, 16&gt;) () 
(Number of thread is changing each time I execute)
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/3917&gt;#3917&lt;/denchmark-link&gt;
 seems to be the same problem
		</comment>
		<comment id='4' author='leondz' date='2016-09-13T14:35:26Z'>
		Hello,
Same problem here as well.  GTX 1080, Ubuntu 14.04, CUDA 8.0, CuDNN 5.1, nvidia driver 367.27.  --num_concurrent_sessions=1 does not seem to change anything for me, either.  With --use_gpu, roughly 1 in 10-15 runs finishes without error.  Without --use_gpu, it works as expected.
		</comment>
		<comment id='5' author='leondz' date='2016-09-15T11:24:21Z'>
		Same here with &lt;denchmark-link:https://github.com/iraadit&gt;@iraadit&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/jrecursive&gt;@jrecursive&lt;/denchmark-link&gt;
 running Ubuntu 16.04, CUDA 8.0, CuDNN 5.1 and nvidia driver 370.
Backtrace:
#0  0x000055555611e040 in tensorflow::functor::CastFunctor&lt;Eigen::GpuDevice, float, int&gt;::operator()(Eigen::GpuDevice const&amp;, Eigen::TensorMap&lt;Eigen::Tensor&lt;float, 1, 1, long&gt;, 16&gt;, Eigen::TensorMap&lt;Eigen::Tensor&lt;int const, 1, 1, long&gt;, 16&gt;) ()
#1  0x00005555560f364d in std::_Function_handler&lt;void (tensorflow::OpKernelContext*, tensorflow::Tensor const&amp;, tensorflow::Tensor*), tensorflow::GetGpuCastFromInt32(tensorflow::DataType)::{lambda(tensorflow::OpKernelContext*, tensorflow::Tensor const&amp;, tensorflow::Tensor*)#9}&gt;::_M_invoke(std::_Any_data const&amp;, tensorflow::OpKernelContext*&amp;&amp;, tensorflow::Tensor const&amp;, tensorflow::Tensor*&amp;&amp;) ()
#2  0x00005555560bd75d in tensorflow::CastOpBase::Compute(tensorflow::OpKernelContext*) ()
#3  0x0000555556b46da5 in tensorflow::BaseGPUDevice::Compute(tensorflow::OpKernel*,  tensorflow::OpKernelContext*)()
#4  0x0000555556b6bdb5 in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) ()
#5  0x0000555556b5fb15 in std::_Function_handler&lt;void (), std::_Bind&lt;std::_Mem_fn&lt;void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)&gt; (tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)&gt; &gt;::_M_invoke(std::_Any_data const&amp;) ()
#6  0x0000555556cca70c in Eigen::NonBlockingThreadPoolTempl&lt;tensorflow::thread::EigenEnvironment&gt;::WorkerLoop(int)()
#7  0x0000555556cc9337 in std::_Function_handler&lt;void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function&lt;void ()&gt;)::{lambda()#1}&gt;::_M_invoke(std::_Any_data const&amp;) ()
#8  0x00007ffff7182c80 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#9  0x00007ffff79606fa in start_thread (arg=0x7fffcf7fe700) at pthread_create.c:333
#10 0x00007ffff6bf1b5d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109
		</comment>
		<comment id='6' author='leondz' date='2017-01-24T22:11:24Z'>
		Is that still happening with the latest version?
		</comment>
		<comment id='7' author='leondz' date='2017-01-27T18:08:48Z'>
		Closing due to inactivity. Feel free to re-open if you would like us to look again.
		</comment>
		<comment id='8' author='leondz' date='2017-02-06T22:58:15Z'>
		&lt;denchmark-link:https://github.com/aselle&gt;@aselle&lt;/denchmark-link&gt;
 Could you comment on the reopen?  &lt;denchmark-link:https://github.com/poulson-google&gt;@poulson-google&lt;/denchmark-link&gt;
 Did you ever figure out a fix?
		</comment>
		<comment id='9' author='leondz' date='2017-02-06T23:04:23Z'>
		Just wanted to get a final followup since it was a crashing issue.
		</comment>
		<comment id='10' author='leondz' date='2017-02-06T23:17:07Z'>
		This is fixed in HEAD:



tensorflow/tensorflow/cc/tutorials/example_trainer.cc


         Line 149
      in
      6bdc4cd






 // Spawn N threads for N concurrent sessions. 





		</comment>
		<comment id='11' author='leondz' date='2017-02-06T23:22:34Z'>
		Great!
		</comment>
	</comments>
</bug>