<bug id='12396' author='jundengdeng' open_date='2017-08-18T15:42:02Z' closed_time='2017-08-21T20:16:26Z'>
	<summary>tf Dataset with tf.py_func doesn't work as the tutorial says</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Ubuntu 16.04
TF 1.3 (released version)
CUDA 8.0
CUDNN 6.0
Python 3.5

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

Dataset with tf.py_func() doesn't work as the dataset tutorial says in (&lt;denchmark-link:https://www.tensorflow.org/programmers_guide/datasets&gt;https://www.tensorflow.org/programmers_guide/datasets&lt;/denchmark-link&gt;
)
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

minimum code to reproduce the problem.
import tensorflow as tf
from tensorflow.contrib.data.python.ops.dataset_ops import Dataset
import numpy as np
import glob
import sys
import os

def _read_py_function(filename):
    filename = filename.decode(sys.getdefaultencoding())
    data = np.load(filename)
    # print(data.shape)
    label = np.random.randint(5)
    # print(label)
    return data.astype(np.float32), np.cast[np.float32](label)


sample_size = 5
seq_len = 3
num_samples = 7


data_dir = 'test_toy_data/'

if not os.path.exists(data_dir):
    os.makedirs(data_dir)

for i in range(num_samples):
    np.save('%s/toy_%d.npy' % (data_dir, i),
            np.random.rand(seq_len, sample_size))


tr_filenames = glob.glob('%s/toy*.npy' % (data_dir))

tr_dataset = Dataset.from_tensor_slices((tr_filenames))
tr_dataset = tr_dataset.map(
    lambda filename: tf.py_func(_read_py_function,
                                [filename],
                                [tf.float32, tf.float32]))
tr_dataset = tr_dataset.shuffle(buffer_size=20)
tr_dataset = tr_dataset.batch(2)


iterator = tr_dataset.make_initializable_iterator()
next_element = iterator.get_next()

sess = tf.Session()

for _ in range(1):
    # training
    sess.run(iterator.initializer)

    while True:
        try:
            feats, labs = sess.run(next_element)
            print(feats.shape)
            print(labs)
        except tf.errors.OutOfRangeError:
            break
	</description>
	<comments>
		<comment id='1' author='jundengdeng' date='2017-08-18T16:58:40Z'>
		Thanks for reporting this problem. It's definitely a bug, and we're working on a fix.
		</comment>
		<comment id='2' author='jundengdeng' date='2017-08-18T17:01:05Z'>
		In the meantime, you can work around it with the following tweak:
tr_dataset = tr_dataset.map(
    lambda filename: tuple(tf.py_func(_read_py_function,
                                [filename],
                                [tf.float32, tf.float32])))
The problem stems from treating the list returned by tf.py_func() as an invitation to auto-stack the returned tensors into a single tensor. Converting it to tuple prevents that conversion from being attempted. The root cause is that the interpretation of lists in a Dataset nested structure changed between TF 1.2 and 1.3....
		</comment>
		<comment id='3' author='jundengdeng' date='2017-08-18T21:29:52Z'>
		Thanks for pointing out this trick. It works now when converting it to tuple. üëç
		</comment>
	</comments>
</bug>