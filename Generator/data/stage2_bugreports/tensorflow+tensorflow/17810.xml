<bug id='17810' author='darrengarvey' open_date='2018-03-18T14:38:31Z' closed_time='2018-11-17T14:06:52Z'>
	<summary>Dataset.list_files is impractical for large number of files</summary>
	<description>
I would like to use &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/data/Dataset#list_files&gt;Dataset.list_files&lt;/denchmark-link&gt;
 on a large dataset. The dataset is similar to ImageNet in that it's a big list of images nested in subdirectories.
I originally guessed that under the hood the api would use queues to start training early, but that isn't the case - the implementation walks the entire directory tree and loads the filenames into memory before the next operation starts.
This means you can wait tens of minutes before training starts.
Two things would help here:

Use queues under the hood so downstream ops can start immediately
Allow specifying a limit, eg. Dataset.list_files("**/*.jpg", limit=1000)

The latter is mostly useful for quicker iteration. The former needs more work due to the different backends for GetMatchingPaths.
Is this something being worked on already?
I am using &lt;denchmark-link:https://www.python.org/dev/peps/pep-0471/&gt;os.scandir()&lt;/denchmark-link&gt;
 as a workaround but the  is a more natural api for this task and should be faster due to not needing to pass the file names via .
	</description>
	<comments>
		<comment id='1' author='darrengarvey' date='2018-03-19T01:36:24Z'>
		Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.
Have I written custom code
OS Platform and Distribution
TensorFlow installed from
TensorFlow version
Bazel version
CUDA/cuDNN version
GPU model and memory
Exact command to reproduce
		</comment>
		<comment id='2' author='darrengarvey' date='2018-04-03T11:51:57Z'>
		I'm running TF commit &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/90df8ab4ab127fac239597ce0708577a335613bf&gt;90df8ab&lt;/denchmark-link&gt;
 (master  from a few days ago). On Ubuntu 17.10. Built with bazel 0.11.1, gcc 6.4, nvcc 9.1 and cuda 9.1. Relevant bazel build options are  (ie. -march=native).
However the behaviour I'm talking about is just part of the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/matching_files_op.cc#L46&gt;Dataset.list_files() kernel&lt;/denchmark-link&gt;
.
I hacked together a &lt;denchmark-link:https://gist.github.com/darrengarvey/ff05fbe28ab2061c101fe64353b467ff&gt;little script&lt;/denchmark-link&gt;
 to demonstrate the behaviour. Some results on different disks I have here. All run with :

tmpfs:

&lt;denchmark-code&gt;load data: time: 4566.47 ms
prep data: time: 14.50 ms
read first filename: time: 14691.77 ms
read second filename: time: 0.36 ms
read 99998 more filenames: time: 7742.66 ms (0.08 ms per iteration)
&lt;/denchmark-code&gt;


SSD (Samsung SM951 256GB SSD):

&lt;denchmark-code&gt;load data: time: 9190.51 ms
prep data: time: 19.00 ms
read first filename: time: 20781.54 ms
read second filename: time: 0.28 ms
read 99998 more filenames: time: 7919.55 ms (0.08 ms per iteration)
&lt;/denchmark-code&gt;


External HDD (Samsung D3 Station, 4TB):

&lt;denchmark-code&gt;load data: time: 64532.58 ms
prep data: time: 18.13 ms
read first filename: time: 86195.52 ms
read second filename: time: 0.36 ms
read 99998 more filenames: time: 7742.11 ms (0.08 ms per iteration)
&lt;/denchmark-code&gt;


External HDD (Seagate Expansion Desk 5TB):

&lt;denchmark-code&gt;load data: time: 119427.41 ms
prep data: time: 14.98 ms
read first filename: time: 77205.09 ms
read second filename: time: 0.33 ms
read 99998 more filenames: time: 7723.63 ms (0.08 ms per iteration)
&lt;/denchmark-code&gt;

Note I'm not flushing caches at any point. Ok so the external drives are terrible, but as a realistic example, consider 1m files sitting on tmpfs:
&lt;denchmark-code&gt;load data: time: 48649.91 ms
prep data: time: 14.79 ms
read first filename: time: 315815.76 ms
read second filename: time: 0.43 ms
read 999998 more filenames: time: 86427.00 ms (0.09 ms per iteration)
&lt;/denchmark-code&gt;

That's still 5 minutes waiting before training starts.
I suppose I've got a feature request rather than a bug. It might be something I'll fix if no-one else is currently looking at it and the options suggested in the OP seem reasonable.
		</comment>
		<comment id='3' author='darrengarvey' date='2018-04-05T21:15:14Z'>
		This would be a great feature to have! You're right - currently we wait till the MatchingFilesOp succeeds and we get all the files. One potential idea would be to create an iterator over the output of GetMatchingFiles and stream that into the downstream ops. &lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;
 for more ideas.
		</comment>
		<comment id='4' author='darrengarvey' date='2018-04-05T21:20:25Z'>
		+1. A MatchingFilesDataset would be very useful in this case, and it could replace the current implementation of Dataset.list_files(). For bonus points, it would be great if it had the option to yield results in sorted order, since the underlying OS might not guarantee this.
		</comment>
		<comment id='5' author='darrengarvey' date='2018-09-05T04:58:36Z'>
		&lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/rohan100jain&gt;@rohan100jain&lt;/denchmark-link&gt;
  Do you mean to add a new op , which yields results in sorted order?
		</comment>
		<comment id='6' author='darrengarvey' date='2018-09-05T14:26:58Z'>
		Yes, that's right.
		</comment>
		<comment id='7' author='darrengarvey' date='2018-09-05T18:12:08Z'>
		&lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;
 If nobody else is working on it, may I submit a PR for this new op?
		</comment>
		<comment id='8' author='darrengarvey' date='2018-09-10T00:13:50Z'>
		Sure, that'd be welcome!
		</comment>
		<comment id='9' author='darrengarvey' date='2018-09-20T17:41:43Z'>
		&lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/rohan100jain&gt;@rohan100jain&lt;/denchmark-link&gt;
 I implemented a , which could yield the matching files in a sorted order.
The performance experiments run using &lt;denchmark-link:https://gist.github.com/darrengarvey/ff05fbe28ab2061c101fe64353b467ff&gt;the script&lt;/denchmark-link&gt;
 posted by &lt;denchmark-link:https://github.com/darrengarvey&gt;@darrengarvey&lt;/denchmark-link&gt;
 , where  is the number of dirs at each level, and  is the number of nested dirs.
Here are the initial experiment results. What do you think of them?
when width = 1000 and depth = 2:
-- MatchingFilesDataset:
&lt;denchmark-code&gt;read first filename: time: 16.06 ms
read second filename: time: 0.52 ms
read 998 more filenames: time: 336.51 ms (0.34 ms per iteration)
&lt;/denchmark-code&gt;

-- Dataset.list_files():
&lt;denchmark-code&gt;read first filename: time: 253.91 ms
read second filename: time: 0.27 ms
read 998 more filenames: time: 135.77 ms (0.14 ms per iteration)
&lt;/denchmark-code&gt;

when width = 1000 and depth = 20:
-- MatchingFilesDataset:
&lt;denchmark-code&gt;read first filename: time: 22.16 ms
read second filename: time: 3.09 ms
read 998 more filenames: time: 2269.14 ms (2.27 ms per iteration)
&lt;/denchmark-code&gt;

-- Dataset.list_files():
&lt;denchmark-code&gt;read first filename: time: 1860.96 ms
read second filename: time: 0.38 ms
read 998 more filenames: time: 127.23 ms (0.13 ms per iteration)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='darrengarvey' date='2018-09-20T18:19:17Z'>
		Ah, isn't open source fantastic?
&lt;denchmark-link:https://github.com/feihugis&gt;@feihugis&lt;/denchmark-link&gt;
 I hadn't gotten around to implementing this yet so thanks! The results look very promising. So I'm guessing that at each level, you're finding all {files,dirs} in a directory, sorting, then moving to the next level? This sounds great and the results are a big improvement.
Is the sorting optional? In many cases the order isn't interesting so if it could be turned off for extra speed that would be worth a small fortune in virtual bonus points. Sorting by default does seem sensible though.
Kudos!
		</comment>
		<comment id='11' author='darrengarvey' date='2018-09-20T19:53:48Z'>
		&lt;denchmark-link:https://github.com/darrengarvey&gt;@darrengarvey&lt;/denchmark-link&gt;
 Yeah, I implemented it in a similar way. Basically, I use  to search the files instead of  used in . The files and dirs at each level are stored in a , then the output for each  could be sorted.
Sorting is not optional yet. I could make some change to make it optional. Will do some tests to see the performance difference.
		</comment>
		<comment id='12' author='darrengarvey' date='2018-11-05T18:54:29Z'>
		Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the contributions welcome label. Thank you.
		</comment>
		<comment id='13' author='darrengarvey' date='2018-11-17T14:06:52Z'>
		This is fixed now &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/22429&gt;#22429&lt;/denchmark-link&gt;
 is merged. Thanks &lt;denchmark-link:https://github.com/feihugis&gt;@feihugis&lt;/denchmark-link&gt;
.
		</comment>
	</comments>
</bug>