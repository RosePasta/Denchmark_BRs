<bug id='41151' author='chenpengf0223' open_date='2020-07-07T09:59:22Z' closed_time='2020-07-09T21:47:50Z'>
	<summary>uint8 model runtime input(s) num is 2.</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): binary
TensorFlow version (or github SHA if from source): 2-1

Command used to run the converter or code if youâ€™re using the Python API
&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow.keras.models import load_model

keras_file = "./keras-0127127.h5"

model = load_model(keras_file)
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.inference_input_type = tf.uint8
converter.inference_type = tf.uint8    #tf.lite.constants.QUANTIZED_UINT8
converter.inference_output_type = tf.uint8
converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
tflite_uint8_model = converter.convert()
open("uint8.tflite", "wb").write(tflite_uint8_model)
&lt;/denchmark-code&gt;


I use netron software to visualiz the "uint8.tflite" model, it shows that:
&lt;denchmark-link:https://user-images.githubusercontent.com/20535427/86765182-68cd0b00-c07b-11ea-83c2-026fbc5b5c50.png&gt;&lt;/denchmark-link&gt;

We can see some  DEPTHWISE_CONV_2D operations have 2 inputs.
Also, please include a link to the keras model I used above
&lt;denchmark-code&gt;https://github.com/chenpengf0223/semantic_segmentation_distillation/blob/master/keras-0127127.h5
&lt;/denchmark-code&gt;

Failure details
If the conversion is successful, but the generated model is wrong,
state what is wrong:

Producing model that c++ tflite gpu delegate library can not process.

When I use c++ tflite gpu delegate library to run the model, I get such log:
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for GPU.
ERROR: Following operations are not supported by GPU delegate:
DEPTHWISE_CONV_2D: Expected 1 runtime input tensor(s), but node has 2 runtime input(s).
DEQUANTIZE: Expected 1 runtime input tensor(s), but node has 0 runtime input(s).
20 operations will run on the GPU, and the remaining 41 operations will run on the CPU.
INFO: Initialized OpenCL-based API.
INFO: Created 1 GPU delegate kernels.
	</description>
	<comments>
		<comment id='1' author='chenpengf0223' date='2020-07-07T11:06:46Z'>
		&lt;denchmark-link:https://github.com/srjoglekar246&gt;@srjoglekar246&lt;/denchmark-link&gt;
, could you take a look at this issue? It's related to quant model support in gpu delegate.
		</comment>
		<comment id='2' author='chenpengf0223' date='2020-07-07T15:17:53Z'>
		&lt;denchmark-link:https://github.com/chenpengf0223&gt;@chenpengf0223&lt;/denchmark-link&gt;
 From the output, it looks like the model is partially running on GPU.
For some of the nodes, looks like the quantization isn't happening correctly. Could you try the following params for conversion:
&lt;denchmark-code&gt;converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8  # or tf.uint8
converter.inference_output_type = tf.int8  # or tf.uint8
converter.optimizations = [tf.lite.Optimize.DEFAULT]
&lt;/denchmark-code&gt;

(note: no converter.inference_type)
If that also leads to a similar output, there might be some issues with the quantization tooling - DEQUANTIZE nodes with no input tensors should never happen, so there might be some errors. Feel free to attach the tflite model as well, if things don't work after the above changes.
		</comment>
		<comment id='3' author='chenpengf0223' date='2020-07-08T06:34:57Z'>
		&lt;denchmark-link:https://github.com/srjoglekar246&gt;@srjoglekar246&lt;/denchmark-link&gt;
 Thanks for your reply, I try your params as following:
import tensorflow as tf
from tensorflow.keras.models import load_model
import numpy as np
import random
def representative_dataset_gen():
for _ in range(100):
# Get sample input data as a numpy array in a method of your choosing.
data = np.random.random((1,512,512,3))
#print(data.dtype)
data=data.astype('float32')
#print(data)
yield [data]
keras_file = "./keras-0127127.h5"
model = load_model(keras_file)
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.representative_dataset = representative_dataset_gen
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_uint8_model = converter.convert()
open("int8.tflite", "wb").write(tflite_uint8_model)
I get a correct tflite  model, and I attach the model :
&lt;denchmark-link:https://github.com/chenpengf0223/semantic_segmentation_distillation/blob/master/int8.tflite&gt;https://github.com/chenpengf0223/semantic_segmentation_distillation/blob/master/int8.tflite&lt;/denchmark-link&gt;

Then I use tflite gpu delegate to run the model, the function "interpreter_-&gt;Invoke()" costs about 52ms, (I calcaute the time from the 11th frame in a repeated style).
If I run the float model for the same network, the function "interpreter_-&gt;Invoke()" costs about 40ms.
I do not know if it is a normal phenomenon.
Thanks!
		</comment>
		<comment id='4' author='chenpengf0223' date='2020-07-08T18:15:08Z'>
		Do you mean the float model on GPU is faster than int8 model on GPU? Or that float model on CPU is faster than int8 on GPU?
If float model on GPU is faster than int8 model on GPU, could you share the float TFLite model also to debug?
There should not be such a big difference between float and int8.
		</comment>
		<comment id='5' author='chenpengf0223' date='2020-07-09T03:39:13Z'>
		&lt;denchmark-link:https://github.com/srjoglekar246&gt;@srjoglekar246&lt;/denchmark-link&gt;
  I run the float model and the int8 model on GPU using tflite gpu delegate, I attach the float32 model here:
&lt;denchmark-link:https://github.com/chenpengf0223/semantic_segmentation_distillation/blob/master/float32.tflite&gt;https://github.com/chenpengf0223/semantic_segmentation_distillation/blob/master/float32.tflite&lt;/denchmark-link&gt;

The float32 model conversion is as following:
import tensorflow as tf
from tensorflow.keras.models import load_model
import numpy as np
keras_file = "./keras-0127127.h5"
model = load_model(keras_file)
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_float_model = converter.convert()
open("float32.tflite", "wb").write(tflite_float_model)
print('tflite model conversion is done.')
		</comment>
		<comment id='6' author='chenpengf0223' date='2020-07-09T21:47:50Z'>
		Looks like it is working as expected.
For quantized models, we need to perform some extra operations to 'simulate' quantized inference on the GPU - this is done to ensure accuracy (compared to floating point).
This leads to increased latency, depending on characteristics such as number of input/output tensors, number of ops, etc.
Reducing the number of output tensors (6 in this case) will probably help.
However, it all boils down to what you want to trade-off:
For very small model size but higher latency, choose int8. For 4x larger model but great speed, use float32. For something in between (slight accuracy tradeoff, 2x larger model) use &lt;denchmark-link:https://www.tensorflow.org/lite/performance/post_training_float16_quant&gt;float16&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='chenpengf0223' date='2020-07-09T21:47:53Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41151&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41151&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>