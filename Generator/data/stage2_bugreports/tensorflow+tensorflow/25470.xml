<bug id='25470' author='flodorner' open_date='2019-02-03T10:53:25Z' closed_time='2019-03-12T22:54:10Z'>
	<summary>Scoping bug with custom layer</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes, see below
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No
TensorFlow installed from (source or binary):
binary
TensorFlow version (use command below):
1.12.0
Python version:
3.6.8
Bazel version (if compiling from source):

GCC/Compiler version (if compiling from source):

CUDA/cuDNN version:
Using Cpu
GPU model and memory:

Describe the current behavior
I am trying to implement a noisy linear layer in tensorflow, inheriting from tf.keras.layers.Layer . Everything works fine except for reusing variables. This seems to stem from some issue with the scoping: Whenever i use the add_weight function from the superclass and a weight with the same name already existing, it seems to ignore the given reuse-flag in the scope and creates a new variable instead. Interestingly, it does not add a 1 to the variable name in the end as usual in similar cases, but rather adds the 1 to the scope name.
The Code below prints the following variables:
scope/noisy_dense/noisy_kernel:0
scope_1/noisy_dense/noisy_kernel:0
scope/my_variable:0
Describe the expected behavior
Instead, i'd expect the weights to be reused. The print would then only be
scope/noisy_dense/noisy_kernel:0
scope/my_variable:0
Code to reproduce the issue
&lt;denchmark-code&gt;import tensorflow as tf

class NoisyDense(tf.keras.layers.Layer):
    def __init__(self,output_dim):
        self.output_dim=output_dim

        super(NoisyDense, self).__init__()

    def build(self, input_shape):
        self.input_dim = input_shape.as_list()[1]
        self.noisy_kernel = self.add_weight(name='noisy_kernel',shape=  (self.input_dim,self.output_dim))

def noisydense(inputs, units):

    layer = NoisyDense(units)

    return layer.apply(inputs)

inputs = tf.placeholder(tf.float32, shape=(1, 10),name="inputs")



scope="scope"
with tf.variable_scope(scope):
    inputs3 = noisydense(inputs,
           1)
    my_variable = tf.get_variable("my_variable", [1, 2, 3],trainable=True)


with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):
    inputs2 = noisydense(inputs,
           1)
    my_variable = tf.get_variable("my_variable", [1, 2, 3],trainable=True)

tvars = tf.trainable_variables()



init=tf.global_variables_initializer()    
with tf.Session() as sess:
    sess.run(init)
    tvars_vals = sess.run(tvars)

for var, val in zip(tvars, tvars_vals):
    print(var.name, val)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='flodorner' date='2019-03-12T22:54:10Z'>
		Hi &lt;denchmark-link:https://github.com/flodorner&gt;@flodorner&lt;/denchmark-link&gt;
 , tf.keras layers do not respect variable_scopes when creating their variables, as variable_scopes are going away.
Instead, share variables by calling your layer multiple times:
layer = NoisyDense(units)
y = layer(x)
y2 = layer(x2) # will share variables
		</comment>
	</comments>
</bug>