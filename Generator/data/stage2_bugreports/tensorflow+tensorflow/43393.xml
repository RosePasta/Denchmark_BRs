<bug id='43393' author='Kipsora' open_date='2020-09-20T22:40:23Z' closed_time='2020-09-21T23:12:31Z'>
	<summary>GPUDelegate Produces Incorrect Result for reduce_sum</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux (Kernel version 5.8.10)
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung s10e (Android 10)
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): v1.12.1-41975-g46b6537110 2.4.0
Python version: 3.8.5
Bazel version (if compiling from source): 3.5.0
GCC/Compiler version (if compiling from source): 10.2.0
CUDA/cuDNN version: CUDA 11.0.3/cuDNN 8.0.2.39
GPU model and memory: 2080 Ti/11G

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with:

TF 1.0: python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
TF 2.0: python -c "import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"

Describe the current behavior
When using GPU delegate on mobile with reduce_sum, the result is incorrect (the absolute difference is larger than 1e-1). By contrast, If GPU is not used the result is correct.
Describe the expected behavior
The result difference should be relatively very small.
Standalone code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
The codes to generate the graph:
import tensorflow as tf


def generate_buggy_graph(batch_size):
    with tf.Graph().as_default() as graph, tf.compat.v1.Session(graph=graph) as session:
        source = tf.compat.v1.placeholder(tf.float32, shape=[batch_size, 100])
        target = tf.transpose(tf.reduce_sum(tf.transpose(source), axis=1))
        # target = tf.reduce_sum(source, axis=0)

    converter = tf.compat.v1.lite.TFLiteConverter.from_session(session, [source], [target])
    with open("buggy_graph.tflite", "wb") as writer:
        writer.write(converter.convert())


generate_buggy_graph(256)
To push the generated model to the mobile I used the following:
adb push buggy_graph.tflite /storage/emulated/0/Android/data/com.example.mobilenn/files/models/BuggyGraph/buggy_graph.tflite
The codes to run the graph on mobile:
BuggyModel.java
package com.example.mobilenn.lite.models;

import android.util.Log;

import org.tensorflow.lite.Interpreter;
import org.tensorflow.lite.gpu.GpuDelegate;
import org.tensorflow.lite.nnapi.NnApiDelegate;

import java.io.File;
import java.nio.ByteBuffer;
import java.nio.ByteOrder;
import java.util.Locale;

public class BuggyModel {
    public static void run(File basePath) {
        Interpreter.Options interpreterOptions = new Interpreter.Options();
//        interpreterOptions.addDelegate(new NnApiDelegate());
        interpreterOptions.addDelegate(new GpuDelegate());
        Interpreter interpreter = new Interpreter(
                new File(basePath, "buggy_graph.tflite"),
                interpreterOptions
        );

        ByteBuffer inputs = ByteBuffer
                .allocateDirect(interpreter.getInputTensor(0).numBytes())
                .order(ByteOrder.nativeOrder());
        ByteBuffer outputs = ByteBuffer
                .allocateDirect(interpreter.getOutputTensor(0).numBytes())
                .order(ByteOrder.nativeOrder());

        float[] stdAnswer = new float[100];
        for (int batchId = 0; batchId &lt; 256; batchId++) {
            for (int channelId = 0; channelId &lt; 100; channelId++) {
                float value = (float) Math.random();
                stdAnswer[channelId] += value;
                inputs.putFloat(value);
            }
        }
        interpreter.run(inputs, outputs);

        for (int channelId = 0; channelId &lt; 100; channelId++) {
            Log.d("BuggyModel", String.format(
                    Locale.getDefault(),
                    "Channel %d: real: %.3f correct: %.3f diff: %.3f",
                    channelId,
                    outputs.getFloat(channelId * Float.BYTES),
                    stdAnswer[channelId],
                    outputs.getFloat(channelId * Float.BYTES) - stdAnswer[channelId]
            ));
        }
    }
}
MainActivity.java
package com.example.mobilenn;

import android.os.Bundle;

import androidx.appcompat.app.AppCompatActivity;

import com.example.mobilenn.lite.models.BuggyModel;

import java.io.File;

public class MainActivity extends AppCompatActivity {

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main);

        BuggyModel.run(new File(getExternalFilesDir("models"), "BuggyGraph"));
    }

}
Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
When used with GPU delegate:
&lt;denchmark-code&gt;2020-09-20 18:32:39.361 24389-24389/com.example.mobilenn D/BuggyModel: Channel 0: real: 135.750 correct: 135.681 diff: 0.069
2020-09-20 18:32:39.361 24389-24389/com.example.mobilenn D/BuggyModel: Channel 1: real: 131.625 correct: 131.705 diff: -0.080
2020-09-20 18:32:39.361 24389-24389/com.example.mobilenn D/BuggyModel: Channel 2: real: 128.500 correct: 128.509 diff: -0.009
2020-09-20 18:32:39.361 24389-24389/com.example.mobilenn D/BuggyModel: Channel 3: real: 127.500 correct: 127.556 diff: -0.056
2020-09-20 18:32:39.362 24389-24389/com.example.mobilenn D/BuggyModel: Channel 4: real: 126.438 correct: 126.508 diff: -0.071
2020-09-20 18:32:39.362 24389-24389/com.example.mobilenn D/BuggyModel: Channel 5: real: 130.125 correct: 130.037 diff: 0.088
2020-09-20 18:32:39.362 24389-24389/com.example.mobilenn D/BuggyModel: Channel 6: real: 123.938 correct: 123.990 diff: -0.052
2020-09-20 18:32:39.362 24389-24389/com.example.mobilenn D/BuggyModel: Channel 7: real: 128.625 correct: 128.717 diff: -0.092
2020-09-20 18:32:39.363 24389-24389/com.example.mobilenn D/BuggyModel: Channel 8: real: 127.063 correct: 127.131 diff: -0.069
2020-09-20 18:32:39.363 24389-24389/com.example.mobilenn D/BuggyModel: Channel 9: real: 122.500 correct: 122.633 diff: -0.133
...
&lt;/denchmark-code&gt;

When used without any delegate or with NNAPI delegate:
&lt;denchmark-code&gt;2020-09-20 18:39:55.368 24973-24973/com.example.mobilenn D/BuggyModel: Channel 0: real: 135.449 correct: 135.449 diff: 0.000
2020-09-20 18:39:55.368 24973-24973/com.example.mobilenn D/BuggyModel: Channel 1: real: 130.922 correct: 130.922 diff: 0.000
2020-09-20 18:39:55.369 24973-24973/com.example.mobilenn D/BuggyModel: Channel 2: real: 128.056 correct: 128.056 diff: 0.000
2020-09-20 18:39:55.369 24973-24973/com.example.mobilenn D/BuggyModel: Channel 3: real: 133.586 correct: 133.586 diff: 0.000
2020-09-20 18:39:55.369 24973-24973/com.example.mobilenn D/BuggyModel: Channel 4: real: 139.664 correct: 139.664 diff: 0.000
2020-09-20 18:39:55.370 24973-24973/com.example.mobilenn D/BuggyModel: Channel 5: real: 130.863 correct: 130.863 diff: 0.000
2020-09-20 18:39:55.370 24973-24973/com.example.mobilenn D/BuggyModel: Channel 6: real: 127.332 correct: 127.332 diff: 0.000
2020-09-20 18:39:55.370 24973-24973/com.example.mobilenn D/BuggyModel: Channel 7: real: 130.422 correct: 130.422 diff: 0.000
2020-09-20 18:39:55.371 24973-24973/com.example.mobilenn D/BuggyModel: Channel 8: real: 130.867 correct: 130.867 diff: 0.000
2020-09-20 18:39:55.371 24973-24973/com.example.mobilenn D/BuggyModel: Channel 9: real: 122.770 correct: 122.770 diff: 0.000
...
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Kipsora' date='2020-09-21T20:59:11Z'>
		Seems like you're running things in FP16 and precision errors get accumulated.  If the said precision is not good enough for you, then consider running it at FP32.  There'll be always error if you go lower precision, and whether it works for your particular case or not depends on the model / application.  Feeding and adding up random numbers in a system with precision loss will of course bubble up the error.
		</comment>
		<comment id='2' author='Kipsora' date='2020-09-21T22:47:47Z'>
		Thanks for the comments. But as you can see in the code, I used tf.float32 instead of tf.float16 and I believe Java float number is 4 bytes, which should be align with tf.float32. I know by doing some calculations the error will get accumulated when using floating numbers, but I still consider the relative error in magnitude of 1e-3 would be way too large to be practical for any model.
		</comment>
		<comment id='3' author='Kipsora' date='2020-09-21T23:05:30Z'>
		How you create the model (and save the weights) is different from how you carry out the math with the GPU.  That option is set via tensorflow/lite/delegates/gpu/java/src/main/java/org/tensorflow/lite/gpu/GpuDelegate.java.  You can see that precisionLossAllowed is true by default, which is the thing that controls whether math should be carried out in FP32 or FP16.
Also, do not underestimate the error accumulation.  It can easily add up in reducers, and the range of FP16 is quite limiting  &lt;denchmark-link:https://en.wikipedia.org/wiki/Half-precision_floating-point_format&gt;https://en.wikipedia.org/wiki/Half-precision_floating-point_format&lt;/denchmark-link&gt;
  and break your model especially if it's numerically unstable.  Again, rather than having a sum of random numbers with precision loss and claim that the entire runtime is unstable, I would suggest creating a full network, do a full training, and do an eval of whether the FP16 runtime still works for you or not with an eval dat set.
		</comment>
		<comment id='4' author='Kipsora' date='2020-09-21T23:12:31Z'>
		I see your points but I has never never imagined the FP16 calculation will incur such large error. Anyways, the problem is dissolved when setting precisionLossAllowed to true. Thanks!
		</comment>
		<comment id='5' author='Kipsora' date='2020-09-21T23:12:34Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43393&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43393&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>