<bug id='32869' author='mpdn' open_date='2019-09-27T09:21:10Z' closed_time='2020-01-22T20:01:26Z'>
	<summary>sparse_categorical_crossentropy does not use underlying logits of tf.keras.layers.Softmax</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS
TensorFlow installed from (source or binary): pip
TensorFlow version (use command below): 2.0.0-rc0
Python version: 3.6

Describe the current behavior
As per the comment in sparse_categorical_crossentropy, the function will use the underlying logits for better numerical stability if the last op is a softmax: 


tensorflow/tensorflow/python/keras/backend.py


        Lines 4522 to 4533
      in
      1ae01a0






 if (isinstance(output, (ops.EagerTensor, variables_module.Variable)) or 



 output.op.type != 'Softmax'): 



 epsilon_ = _constant_to_tensor(epsilon(), output.dtype.base_dtype) 



 output = clip_ops.clip_by_value(output, epsilon_, 1 - epsilon_) 



 output = math_ops.log(output) 



 else: 



 # When softmax activation function is used for output operation, we 



 # use logits from the softmax function directly to compute loss in order 



 # to prevent collapsing zero when training. 



 # See b/117284466 



 assert len(output.op.inputs) == 1 



 output = output.op.inputs[0] 





But since keras adds an identity op to the output of every layer, the last node of a layer will always be Identity so the condition in the above if statement will never fail for keras models and the underlying logits are never used: 


tensorflow/tensorflow/python/keras/engine/base_layer.py


        Lines 851 to 853
      in
      b0aa37c






 # Wrap Tensors in `outputs` in `tf.identity` to avoid 



 # circular dependencies. 



 outputs = base_layer_utils.mark_as_return(outputs, acd) 





Using non-keras-layers like tf.nn.softmax will work, however.
Describe the expected behavior
Instead, sparse_categorical_crossentropy should check the first non-Identity node when determining whether the last node is a softmax node. I.e. it should first walk the chain of identity nodes.
Code to reproduce the issue
&lt;denchmark-code&gt;import tensorflow as tf

x = tf.keras.layers.Input(shape=[10])

softmax_keras = tf.keras.layers.Softmax()(x)
softmax_tf = tf.nn.softmax(x)

y_true = tf.keras.layers.Input(shape=[])
loss_keras = tf.keras.losses.sparse_categorical_crossentropy(y_true, softmax_keras)
loss_tf = tf.keras.losses.sparse_categorical_crossentropy(y_true, softmax_tf)

def transitive_inputs(tensor):
    return set([tensor.op.type] + [inp for tensor in tensor.op.inputs for inp in transitive_inputs(tensor)])

print('Softmax' in transitive_inputs(loss_keras)) # prints "True"
print('Softmax' in transitive_inputs(loss_tf)) # prints "False"
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='mpdn' date='2019-09-27T09:33:30Z'>
		This seems to have an absolutely HUGE impact on my models. I really recommend anyone using tf.keras.layers.Softmax to use tf.nn.softmax instead until this issue is fixed.
Edit: do be warned that using tf.nn.softmax directly will not propagate any masks you might have. You can hack masks back like this:
&lt;denchmark-code&gt;# Don't do this
output = tf.nn.softmax(x)
output._keras_mask = x._keras_mask
&lt;/denchmark-code&gt;

Edit: Do not do the above. It seems that this puts tensorflow with a weird state wrt. propagating masks where the mask is propagated during graph construction, but not during model fitting. This means that you might see a improvement in your models because it tries to predict masked outputs instead of real outputs!
		</comment>
		<comment id='2' author='mpdn' date='2019-09-30T07:08:11Z'>
		Tried replicating with given code for TF-2.0rc2, the same error was received. Kindly find the &lt;denchmark-link:https://colab.sandbox.google.com/gist/oanush/78bd4585819aa58fbc13155a1ab3871d/32869.ipynb&gt;gist&lt;/denchmark-link&gt;
 of colab.Thanks!
		</comment>
		<comment id='3' author='mpdn' date='2020-01-22T20:01:26Z'>
		&lt;denchmark-link:https://github.com/mpdn&gt;@mpdn&lt;/denchmark-link&gt;
 I think this was resolved in . Please check the &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/d30cfe2f0b6a5c3f6b72227378fbdad9/32869.ipynb&gt;gist here&lt;/denchmark-link&gt;
.
I am closing this issue as it was resolved. Please feel free to reopen if the issue persists again. Thanks!
		</comment>
		<comment id='4' author='mpdn' date='2020-01-22T20:01:28Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32869&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32869&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>