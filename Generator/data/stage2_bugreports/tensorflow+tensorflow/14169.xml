<bug id='14169' author='vincentfung13' open_date='2017-11-02T02:49:38Z' closed_time='2018-01-05T08:25:55Z'>
	<summary>Tensorflow predicts odd results with insufficient GPU memory</summary>
	<description>
Hi all,
I am currently having a problem that, when my code tries to initialize two or more predict instances in a GPU with insufficient memory, instead of throwing an OOM exception, the instances are initialized normally. However, when I try to predict an image with these instances, they produce weird results like [1.0, 0.0, 0.0, â€¦].
Is this a bug when two or more sessions try to race for limited amount of GPU memory?
Please see below for my system information and the exact steps to reproduce the problem. I would very much appreciate it if you could take a look.
Cheers,
Vincent
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution: CentOS Linux release 7.2.1511
TensorFlow installed from (source or binary): official binaries
TensorFlow version (use command below): 1.3.0/ 1.2.0/ 1.1.0
Python version: 2.7.5
CUDA/cuDNN version: CUDA 8.0.61, CUDNN 6.0
GPU model and memory: 1080 Ti (11172MB)
Exact steps to reproduce:


Download and uncompress the file below with two scripts;
Download the official inception_v1 imagenet pretrained model  from http://download.tensorflow.org/models/inception_v1_2016_08_28.tar.gz
Try to occupy most of the memory in the GPU to run on - the exact amount of memory to occupy needs to be carefully tuned to reproduce the problem, in my case, I use pycuda to allocate 10720 out of the 11172MB of my 1080 Ti.
Run test_monitor.py in two separate command prompts, we should see weird results within a few minutes (the scripts stop when encounter such results).

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/1436321/reproduce_code.zip&gt;reproduce_code.zip&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='vincentfung13' date='2017-11-06T19:00:29Z'>
		Thanks for the instructions to reproduce.
&lt;denchmark-link:https://github.com/yzhwang&gt;@yzhwang&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
 have been investigating some issues with memory allocations and multiple processes and might have some more details here.
		</comment>
		<comment id='2' author='vincentfung13' date='2017-11-06T19:22:49Z'>
		Hi &lt;denchmark-link:https://github.com/vincentfung13&gt;@vincentfung13&lt;/denchmark-link&gt;
 . Thanks for reporting the issue. We have encountered similar issues internally when two or more processes are running TensorFlow with limited GPU memory. In our case, a simple const tensor or random tensor creation will cause garbage values instead of any OOM error when one process has already taken most GPU memory.
The short answer to the issue is that: it is due to CUDA runtime silently failing to initialize with insufficient GPU memory. It will cause all the CUDA calls afterwards to produce incorrect results, but not fail with errors. We currently have a workaround which is to initialize CUDA runtime before any of our large GPU memory allocation. It should go in within a week. I'm also investigating the reason for the silent failure. I will provide update when I have something to share with you.
		</comment>
		<comment id='3' author='vincentfung13' date='2017-11-15T03:30:21Z'>
		Hi &lt;denchmark-link:https://github.com/yzhwang&gt;@yzhwang&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
, any updates on this issue? Thanks. :)
		</comment>
		<comment id='4' author='vincentfung13' date='2017-11-16T01:05:10Z'>
		Hi &lt;denchmark-link:https://github.com/vincentfung13&gt;@vincentfung13&lt;/denchmark-link&gt;
 I've made a commit that should solve the problem: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/4535bd5df4d077072a8f207146bf4cd051971237&gt;4535bd5&lt;/denchmark-link&gt;
 Now each process that runs a TF program will try to initialize CUDA runtime before the giant device memory allocation, and if the remaining memory is insufficient, there should be an error for session creation. Please verify if this solves the problem. Thanks!
		</comment>
		<comment id='5' author='vincentfung13' date='2017-12-20T19:20:25Z'>
		It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
		</comment>
		<comment id='6' author='vincentfung13' date='2018-01-04T19:14:24Z'>
		It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
		</comment>
		<comment id='7' author='vincentfung13' date='2018-01-05T08:26:58Z'>
		I believe the problem has been resolved after the commit &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/4535bd5df4d077072a8f207146bf4cd051971237&gt;4535bd5&lt;/denchmark-link&gt;
, closing this for now.
		</comment>
	</comments>
</bug>