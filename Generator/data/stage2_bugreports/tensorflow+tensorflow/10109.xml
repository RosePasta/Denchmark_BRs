<bug id='10109' author='alsrgv' open_date='2017-05-22T17:32:33Z' closed_time='2017-05-22T18:35:55Z'>
	<summary>TensorFlow 1.2.0rc0 strange behavior with Benchmark scripts</summary>
	<description>
I'm running &lt;denchmark-link:https://github.com/tensorflow/benchmarks/&gt;https://github.com/tensorflow/benchmarks/&lt;/denchmark-link&gt;
 scripts by &lt;denchmark-link:https://github.com/tfboyd&gt;@tfboyd&lt;/denchmark-link&gt;
 and I'm observing strange behavior and crashes.

There's a LOT of messages like these (below).  These messages happen in both standalone and distributed runs.

&lt;denchmark-code&gt;2017-05-22 17:21:00.589871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:3) -&gt; (device: 3, name: Tesla P40, pci bus id: 0000:85:00.0)
2017-05-22 17:21:00.600723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla P40, pci bus id: 0000:04:00.0)
2017-05-22 17:21:00.600741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: Tesla P40, pci bus id: 0000:05:00.0)
2017-05-22 17:21:00.600746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:2) -&gt; (device: 2, name: Tesla P40, pci bus id: 0000:84:00.0)
2017-05-22 17:21:00.600750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:3) -&gt; (device: 3, name: Tesla P40, pci bus id: 0000:85:00.0)
2017-05-22 17:21:00.619525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla P40, pci bus id: 0000:04:00.0)
2017-05-22 17:21:00.619543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: Tesla P40, pci bus id: 0000:05:00.0)
2017-05-22 17:21:00.619548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:2) -&gt; (device: 2, name: Tesla P40, pci bus id: 0000:84:00.0)
2017-05-22 17:21:00.619552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:3) -&gt; (device: 3, name: Tesla P40, pci bus id: 0000:85:00.0)
2017-05-22 17:21:00.629770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla P40, pci bus id: 0000:04:00.0)
2017-05-22 17:21:00.629786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: Tesla P40, pci bus id: 0000:05:00.0)
2017-05-22 17:21:00.629807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:2) -&gt; (device: 2, name: Tesla P40, pci bus id: 0000:84:00.0)
2017-05-22 17:21:00.629811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:3) -&gt; (device: 3, name: Tesla P40, pci bus id: 0000:85:00.0)
2017-05-22 17:21:00.649854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla P40, pci bus id: 0000:04:00.0)
2017-05-22 17:21:00.649871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: Tesla P40, pci bus id: 0000:05:00.0)
2017-05-22 17:21:00.649892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:2) -&gt; (device: 2, name: Tesla P40, pci bus id: 0000:84:00.0)
&lt;/denchmark-code&gt;


In distributed mode, TF 1.2.0 rc0 is not able to handle batch size 64 for Inception V3: - This was figured out.

Slave worker crashes with:
&lt;denchmark-code&gt;2017-05-22 17:22:25.411569: W tensorflow/core/framework/op_kernel.cc:1158] Resource exhausted: OOM when allocating tensor with shape[64,32,149,149]
&lt;/denchmark-code&gt;

Chief worker crashes with:
&lt;denchmark-code&gt;Exception in thread Thread-1:
Traceback (most recent call last):
  File "/usr/lib/python2.7/threading.py", line 810, in __bootstrap_inner
    self.run()
  File "tf_cnn_benchmarks.py", line 226, in run
    global_step_val, = self.sess.run([self.global_step_op])
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 789, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 925, in _run
    raise RuntimeError('Attempted to use a closed Session.')
RuntimeError: Attempted to use a closed Session.
&lt;/denchmark-code&gt;

My command:
&lt;denchmark-code&gt;python -u tf_cnn_benchmarks.py --model inception3 --batch_size 64 --num_gpus 4 --worker_hosts {worker_hosts} --ps_hosts {ps_hosts} --task_index {task_index} --job_name {job_name} --local_parameter_device cpu
&lt;/denchmark-code&gt;

This worked perfectly fine on hash &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/cae8ed1ca54a9fd4f9cc64d08cadebce31fd4607&gt;cae8ed1&lt;/denchmark-link&gt;
 (just a little bit past TF 1.1 release).
	</description>
	<comments>
		<comment id='1' author='alsrgv' date='2017-05-22T17:54:38Z'>
		Thanks Alex.  I will take a look and thank you.  I can definitely help and will take a look but I cannot take credit for the scripts.  I was just the person that did the commit to github  :-) .
One quick note before I dig in.  My guess is it is creating multiple copies on each GPU.  Check nvidia-smi to see how many python processes are running on each GPU.  it should be 1 on each GPU so 4 total in your situation.  Sometimes if you kill the script it will not kill the processes on the GPUs and you end up doubling up.  Worth a quick check.  I also made a mistake in the documents and when you start the PS server you want to do this, which I assume you likely already know.  Even if your ps_server is set to CPU TensorFlow ill still try to take memory from the GPUs when starting the stand alone ps_server:
CUDA_VISIBLE_DEVICES='' python tf_cnn_benchmarks.py
I do not know a lot about the P40 other than it seems to be a card usually used for inference and has 24GB of memory which should be more than enough for batch 64.  I have used Batch-Size 128 on the P100 and it only has 16GB.  Not with TF 1.2, so I will give that a try to see if I can repo.
Thank you for the heads up.
		</comment>
		<comment id='2' author='alsrgv' date='2017-05-22T18:24:06Z'>
		&lt;denchmark-link:https://github.com/tfboyd&gt;@tfboyd&lt;/denchmark-link&gt;
 sorry, I found the reason for crash - rogue process was running on the host that locked GPU memory.  Apologies for ruckus.
Any idea about all those Creating TensorFlow device lines?
&lt;denchmark-code&gt;2017-05-22 17:21:00.600746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] Creating TensorFlow device (/gpu:2) -&gt; (device: 2, name: Tesla P40, pci bus id: 0000:84:00.0)
&lt;/denchmark-code&gt;

I originally suspected that's what was causing OOM.
		</comment>
		<comment id='3' author='alsrgv' date='2017-05-22T18:35:55Z'>
		I do not know.  No bother on the ruckus.  It is unlikely I will find time to figure out who makes that message but if I do I will update this post.  Not lack of desire, just a lack of time.  I am closing this issue but again will update if I find out that message purpose / reason.
		</comment>
	</comments>
</bug>