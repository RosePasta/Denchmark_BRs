<bug id='30453' author='IanQS' open_date='2019-07-07T03:45:23Z' closed_time='2020-04-06T19:48:53Z'>
	<summary>[TF 2.0] categorical_column_with_vocabulary_list not usable in custom training loop</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): Source
TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1
Python version: 3.6.8
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

Describe the current behavior
Outside of , e.g in a custom training loop,  results in an error. I have provided a modified version of &lt;denchmark-link:https://www.tensorflow.org/beta/tutorials/keras/feature_columns&gt;Classifying Structured data&lt;/denchmark-link&gt;
 which demonstrates this.
The error is ValueError: Column dtype and SparseTensors dtype must be compatible. key: thal, column dtype: &lt;dtype: 'string'&gt;, tensor dtype: &lt;dtype: 'int32'&gt;
Describe the expected behavior
Code runs without causing an error
Code to reproduce the issue
It should be directly copy-paste-able
&lt;denchmark-code&gt;import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split


def df_to_dataset(df, shuffle=True, batch_size=32):
    df = df.copy()
    labels = df.pop('target')
    ds = tf.data.Dataset.from_tensor_slices(
        (dict(df), labels)
    )
    if shuffle:
        ds = ds.shuffle(buffer_size=len(df))
    ds = ds.batch(batch_size)
    return ds


def generate_features():
    feature_columns = []
    feature_layer_inputs = {}


    thal = tf.feature_column.categorical_column_with_vocabulary_list(
          'thal', ['fixed', 'normal', 'reversible'])
    thal_one_hot = tf.feature_column.indicator_column(thal)
    feature_columns.append(thal_one_hot)
    feature_layer_inputs['thal'] = tf.keras.Input(shape=(1,), name='thal', dtype=tf.string)

    return feature_columns, feature_layer_inputs


def create_model(feature_columns, feature_layer_inputs):
    input_layer = tf.keras.layers.DenseFeatures(feature_columns)
    inputs = input_layer(feature_layer_inputs)

    l1 = tf.keras.layers.Dense(128, activation='relu')(inputs)
    l2 = tf.keras.layers.Dense(128, activation='relu')(l1)

    output = tf.keras.layers.Dense(1, activation='sigmoid')(l2)

    model = tf.keras.Model(
        inputs=[v for v in feature_layer_inputs.values()],
        outputs=[output]
    )
    return model


def make_loss(loss_object):
    def loss(model, x, y):
        y_pred = model(x)
        return loss_object(y_true=y, y_pred=y_pred)
    return loss


def grad(model, inputs, targets, loss):
    with tf.GradientTape() as tape:
        loss_value = loss(model, inputs, targets)
    return loss_value, tape.gradient(loss_value, model.trainable_variables)


def fit(epochs, train_ds, model, optimizer, loss_obj):
    loss = make_loss(loss_obj)
    for epoch in range(epochs):
        for i, (x, y) in enumerate(train_ds):
            loss_values, grad_values = grad(model, x, y, loss)
            optimizer.apply_gradients(zip(grad_values, model.trainable_variables))


if __name__ == '__main__':
    URL = 'https://storage.googleapis.com/applied-dl/heart.csv'
    df = pd.read_csv(URL)
    CUSTOM_TRAINING = True

    train, test = train_test_split(df, test_size=0.2)
    train, val = train_test_split(train, test_size=0.2)

    # hardcoded stuff
    batch_size = 32
    train_ds = df_to_dataset(train, batch_size=batch_size)

    # Create model and features
    feature_columns, feature_layer_inputs = generate_features()
    model = create_model(feature_columns, feature_layer_inputs)

    if CUSTOM_TRAINING:
        print('Trying custom training')
        bce = tf.keras.losses.BinaryCrossentropy()
        adam = tf.keras.optimizers.Adam()
        fit(epochs=5, train_ds=train_ds,
            model=model, optimizer=adam, loss_obj=bce)
    else:
        print('Using pre-defined fit')
        model.compile(optimizer='adam',
                      loss='binary_crossentropy',
                      metrics=['accuracy'])
        model.fit(train_ds, epochs=5)
&lt;/denchmark-code&gt;

If you flip the CUSTOM_TRAINING variable between True and False (line 72) you'll see what I mean.
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

The complete (relevant) stacktrace is

&lt;denchmark-code&gt;  File "/Applications/PyCharm CE.app/Contents/helpers/pydev/pydevd.py", line 1758, in &lt;module&gt;
    main()
  File "/Applications/PyCharm CE.app/Contents/helpers/pydev/pydevd.py", line 1752, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File "/Applications/PyCharm CE.app/Contents/helpers/pydev/pydevd.py", line 1147, in run
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File "/Applications/PyCharm CE.app/Contents/helpers/pydev/_pydev_imps/_pydev_execfile.py", line 18, in execfile
    exec(compile(contents+"\n", file, 'exec'), glob, loc)
  File "/Users/ian.quah/PycharmProjects/tf2/datasets/issues.py", line 90, in &lt;module&gt;
    model=model, optimizer=adam, loss_obj=bce)
  File "/Users/ian.quah/PycharmProjects/tf2/datasets/issues.py", line 65, in fit
    loss_values, grad_values = grad(model, x, y, loss)
  File "/Users/ian.quah/PycharmProjects/tf2/datasets/issues.py", line 57, in grad
    loss_value = loss(model, inputs, targets)
  File "/Users/ian.quah/PycharmProjects/tf2/datasets/issues.py", line 50, in loss
    y_pred = model(x)
  File "/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py", line 712, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File "/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py", line 753, in call
    return self._run_internal_graph(inputs, training=training, mask=mask)
  File "/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py", line 895, in _run_internal_graph
    output_tensors = layer(computed_tensors, **kwargs)
  File "/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py", line 712, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File "/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py", line 474, in call
    self._state_manager)
  File "/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py", line 4299, in get_dense_tensor
    return transformation_cache.get(self, state_manager)
  File "/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py", line 2562, in get
    transformed = column.transform_feature(self, state_manager)
  File "/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py", line 4238, in transform_feature
    transformation_cache, state_manager)
  File "/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py", line 3714, in get_sparse_tensors
    transformation_cache.get(self, state_manager), None)
  File "/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py", line 2562, in get
    transformed = column.transform_feature(self, state_manager)
  File "/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py", line 3692, in transform_feature
    return self._transform_input_tensor(input_tensor)
  File "/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py", line 3668, in _transform_input_tensor
    self.key, self.dtype, input_tensor.dtype))
ValueError: Column dtype and SparseTensors dtype must be compatible. key: thal, column dtype: &lt;dtype: 'string'&gt;, tensor dtype: &lt;dtype: 'int32'&gt;
&lt;/denchmark-code&gt;


Placing a debugger on line 50 leads me to feature_column_v2.py specifically _transform_input_tensor

The input_tensor arg to _transform_input_tensor is
&lt;denchmark-code&gt;SparseTensor(indices=tf.Tensor(
[[ 0  0]
 [ 1  0]
 [ 2  0]
 [ 3  0]
 [ 4  0]
 [ 5  0]
 [ 6  0]
 [ 7  0]
 [ 8  0]
 [ 9  0]
 [10  0]
 [11  0]
 [12  0]
 [13  0]
 [14  0]
 [15  0]
 [16  0]
 [17  0]
 [18  0]
 [19  0]
 [20  0]
 [21  0]
 [22  0]
 [23  0]
 [24  0]
 [25  0]
 [26  0]
 [27  0]
 [28  0]
 [29  0]
 [30  0]
 [31  0]], shape=(32, 2), dtype=int64), values=tf.Tensor(
[49 34 58 46 59 47 55 58 41 68 62 51 61 46 39 48 37 41 51 59 51 70 60 57
 54 60 52 44 65 49 44 59], shape=(32,), dtype=int32), dense_shape=tf.Tensor([32  1], shape=(2,), dtype=int64))
&lt;/denchmark-code&gt;

which seems strange? It's like it forgot that it had transformed those variables?
	</description>
	<comments>
		<comment id='1' author='IanQS' date='2019-07-08T13:03:04Z'>
		Reproduced the error with TF Version 2.0.beta1
		</comment>
		<comment id='2' author='IanQS' date='2019-07-08T17:22:39Z'>
		Thanks!


Does anyone have an intuition as to why this happens in a custom loop but not in fit?


Does anyone have a quick fix?


		</comment>
		<comment id='3' author='IanQS' date='2019-07-15T17:37:08Z'>
		Adding &lt;denchmark-link:https://github.com/rohan100jain&gt;@rohan100jain&lt;/denchmark-link&gt;
 who is the expert on feature columns.
		</comment>
		<comment id='4' author='IanQS' date='2019-10-01T22:35:45Z'>
		Is this still an open issue?
		</comment>
		<comment id='5' author='IanQS' date='2019-11-03T15:35:28Z'>
		facing same issue.
ValueError: Column dtype and SparseTensors dtype must be compatible. key: opt_categories, column dtype: &lt;dtype: 'int64'&gt;, tensor dtype: &lt;dtype: 'float32'&gt;
		</comment>
		<comment id='6' author='IanQS' date='2019-12-05T07:12:38Z'>
		I am also facing the same issue. Any help...
		</comment>
		<comment id='7' author='IanQS' date='2019-12-18T14:03:32Z'>
		This got resolved for me. The datatype of the vocabulary list passed to the method, the data in the column of the record and the data type that we specify in the model have to be the same. Was able to find the issue, it got resolved.
		</comment>
		<comment id='8' author='IanQS' date='2019-12-27T06:39:47Z'>
		same issue with tensorflow 1.14
ValueError: Column dtype and SparseTensors dtype must be compatible. key: adt_sev_flag, column dtype: &lt;dtype: 'int64'&gt;, tensor dtype: &lt;dtype: 'float64'&gt;
		</comment>
		<comment id='9' author='IanQS' date='2019-12-27T06:40:31Z'>
		
This got resolved for me. The datatype of the vocabulary list passed to the method, the data in the column of the record and the data type that we specify in the model have to be the same. Was able to find the issue, it got resolved.

how did you solved it?
		</comment>
		<comment id='10' author='IanQS' date='2020-01-02T10:09:06Z'>
		The have you checked if the datatype of the data in the vocab list, the datatype of the input data and the data type of the feature_column declaration are same. In my case, the vocab list that I passed had infered to a different datatype and I passed it into the categorical_column_with_vocabulary_list layer incorrectly.
		</comment>
		<comment id='11' author='IanQS' date='2020-04-03T17:09:25Z'>
		I have tried this example in tf-nightly, which seems to have been resolved. It's probably a Keras issue instead of FeatureColumn. Can anyone help to verify?
		</comment>
		<comment id='12' author='IanQS' date='2020-04-06T19:48:54Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30453&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30453&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>