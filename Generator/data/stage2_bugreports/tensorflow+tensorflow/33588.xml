<bug id='33588' author='paanguin' open_date='2019-10-22T02:07:07Z' closed_time='2019-11-15T23:41:18Z'>
	<summary>tflite interpreter makes huge difference in output for tf.reduce_mean() when it is quantized</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution: Linux Ubuntu 16.04
TensorFlow installed from source
TensorFlow version (use command below): ('v2.0.0-0-g64c3d38', '2.0.0') &amp; ('v2.0.0-rc2-26-g64c3d38', '2.0.0')
Python version: 2.7.12
Bazel version (if compiling from source): 0.24.1
GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
CUDA/cuDNN version: 10.0 / 7.0
GPU model and memory: GeForce GTX TITAN

Describe the current behavior
When I quantize the graph with tf.reduce_mean, it makes huge difference in outputs, compared original tf function and tflite model (not quantized).
Describe the expected behavior
the quantized model gives similar output values with the original tf graph.
With quantization=True,
&lt;denchmark-code&gt;INFO: Initialized TensorFlow Lite runtime.
INFO: Initialized TensorFlow Lite runtime.
input_x:
[[[[7. 7. 3.]
   [0. 2. 3.]
   [6. 3. 8.]]

  [[5. 2. 9.]
   [7. 3. 7.]
   [9. 9. 8.]]

  [[3. 2. 0.]
   [6. 8. 7.]
   [2. 0. 7.]]]]
output (tf.function):
tf.Tensor(
[[[[5.6666665]
   [1.6666666]
   [5.6666665]]

  [[5.3333335]
   [5.6666665]
   [8.666667 ]]

  [[1.6666666]
   [7.       ]
   [3.       ]]]], shape=(1, 3, 3, 1), dtype=float32)
output (tflite):
[[[[5.011765 ]
   [4.5176473]
   [4.5176473]]

  [[4.5176473]
   [4.5176473]
   [4.5176473]]

  [[4.5176473]
   [4.5176473]
   [0.6      ]]]]
&lt;/denchmark-code&gt;

The output of the tflite model seems too different with that of the original tf function.
When quantization=False,
&lt;denchmark-code&gt;INFO: Initialized TensorFlow Lite runtime.
input_x:
[[[[3. 6. 5.]
   [4. 8. 9.]
   [1. 7. 9.]]

  [[6. 8. 0.]
   [5. 0. 9.]
   [6. 2. 0.]]

  [[5. 2. 6.]
   [3. 7. 0.]
   [9. 0. 3.]]]]
output (tf.function):
tf.Tensor(
[[[[4.6666665]
   [7.       ]
   [5.6666665]]

  [[4.6666665]
   [4.6666665]
   [2.6666667]]

  [[4.3333335]
   [3.3333333]
   [4.       ]]]], shape=(1, 3, 3, 1), dtype=float32)
output (tflite):
[[[[4.6666665]
   [7.       ]
   [5.6666665]]

  [[4.6666665]
   [4.6666665]
   [2.6666667]]

  [[4.3333335]
   [3.3333333]
   [4.       ]]]]
&lt;/denchmark-code&gt;

the their outputs looks the same.
It does not seem normal that the quantization gives too much differences in output values.
Code to reproduce the issue
&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np

@tf.function(input_signature=[
    tf.TensorSpec(shape=[None,3,3,3], dtype=tf.float32, name='x')])
def model(x, epsilon=1e-6):
  mean = tf.reduce_mean(x, axis=[-1], keepdims=True)
  #variance = tf.reduce_mean(tf.square(x - mean), axis=[-1], keepdims=True)
  #norm_x = (x - mean) * tf.math.rsqrt(variance + epsilon)
  #print(norm_x.shape)
  #return norm_x
  return mean

def run():
  def _representative_dataset_gen():
    for i in range(100):
      yield [np.random.random_integers(0, 9, size=[1,3,3,3]).astype('float32'),]

  np.random.seed(1234)
  quantization=True
  cfunc = model.get_concrete_function()
  converter = tf.lite.TFLiteConverter.from_concrete_functions([cfunc])
  if quantization:
         converter.representative_dataset = _representative_dataset_gen
         converter.optimizations = [tf.lite.Optimize.DEFAULT]



  tflite_model = converter.convert()
  model_file = "tflite_mean_4D_v2.tflite"
  open(model_file, "wb").write(tflite_model)

  interpreter = tf.lite.Interpreter(model_path=model_file)
  interpreter.allocate_tensors()
  input_details = interpreter.get_input_details()
  output_details = interpreter.get_output_details()

  # test values
  input_x = np.random.random_integers(0, 9, size=[1,3,3,3]).astype('float32')

  print("input_x:")
  print(input_x)

  output_fun = cfunc(tf.constant(input_x))
  print("output (tf.function):")
  print(output_fun)

  # tflite test
  interpreter.set_tensor(input_details[0]['index'], input_x)
  output_tflite = interpreter.get_tensor(output_details[0]['index'])
  print("output (tflite):")
  print(output_tflite)

if __name__ == '__main__':
  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)
  run()
&lt;/denchmark-code&gt;

Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='paanguin' date='2019-11-04T05:10:35Z'>
		I modified 'lite/kernels/reduce.cc' and obtained following result that seems correct.
&lt;denchmark-code&gt;input_x:
[[[[7. 7. 3.]
   [0. 2. 3.]
   [6. 3. 8.]]

  [[5. 2. 9.]
   [7. 3. 7.]
   [9. 9. 8.]]

  [[3. 2. 0.]
   [6. 8. 7.]
   [2. 0. 7.]]]]
(1, 3, 3, 3)
output (tf.function):
tf.Tensor(
[[[[5.6666665]
   [1.6666666]
   [5.6666665]]

  [[5.3333335]
   [5.6666665]
   [8.666667 ]]

  [[1.6666666]
   [7.       ]
   [3.       ]]]], shape=(1, 3, 3, 1), dtype=float32)
output (tflite):
[[[[5.647059 ]
   [1.6941178]
   [5.647059 ]]

  [[5.329412 ]
   [5.647059 ]
   [8.647059 ]]

  [[1.6941178]
   [6.9882355]
   [3.0000002]]]]
&lt;/denchmark-code&gt;

tflite interpreter runs following function, that is not correct interpretation of tf.reduce_mean().



tensorflow/tensorflow/lite/kernels/reduce.cc


         Line 323
      in
      64c3d38






 reference_integer_ops::Mean( 





&lt;denchmark-code&gt;  if (op_context.input-&gt;type == kTfLiteInt8) {
    tflite::MeanParams op_params;
    op_params.axis_count = num_axis;
    ResolveAxis(GetTensorData&lt;int&gt;(op_context.axis), num_axis, &amp;op_params);
    const TfLiteTensor* input = op_context.input;
    reference_integer_ops::Mean(
        op_params, data-&gt;multiplier, data-&gt;shift, GetTensorShape(input),
        GetTensorData&lt;int8_t&gt;(input), op_context.input-&gt;params.zero_point,
        GetTensorShape(op_context.output),
        GetTensorData&lt;int8_t&gt;(op_context.output),
        op_context.output-&gt;params.zero_point);
    return kTfLiteOk;
  }
&lt;/denchmark-code&gt;

This code block should be changed as
&lt;denchmark-code&gt;  if (op_context.input-&gt;type == kTfLiteInt8) {
    tflite::MeanParams op_params;
    op_params.axis_count = num_axis;
    ResolveAxis(GetTensorData&lt;int&gt;(op_context.axis), num_axis, &amp;op_params);
    const TfLiteTensor* input = op_context.input;
    if (op_context.params-&gt;keep_dims &amp;&amp; NumDimensions(input) == 4 &amp;&amp;
        op_params.axis_count == 2 &amp;&amp;
        ((op_params.axis[0] == 1 &amp;&amp; op_params.axis[1] == 2) ||
         (op_params.axis[0] == 2 &amp;&amp; op_params.axis[1] == 1))){
       reference_integer_ops::Mean(
           op_params, data-&gt;multiplier, data-&gt;shift, GetTensorShape(input),
           GetTensorData&lt;int8_t&gt;(input), op_context.input-&gt;params.zero_point,
           GetTensorShape(op_context.output),
           GetTensorData&lt;int8_t&gt;(op_context.output),
           op_context.output-&gt;params.zero_point);
       return kTfLiteOk;
    }
  }
&lt;/denchmark-code&gt;

to prevent wrong operation.
Secondly, it should be dealt with elsewhere. It seems that the code contains suitable macro 'TF_LITE_MEAN'.



tensorflow/tensorflow/lite/kernels/reduce.cc


         Line 332
      in
      64c3d38






 #define TF_LITE_MEAN(kernel_type, data_type, temp_data_type)        \ 





I don't know why the authors did not call this already, but I can use this to cover the case. I added following code into the switch statement, borrowing the kTfLiteUInt8 case.
&lt;denchmark-code&gt;      case kTfLiteInt8:
        if (op_context.input-&gt;params.zero_point ==
                op_context.output-&gt;params.zero_point &amp;&amp;
            op_context.input-&gt;params.scale == op_context.output-&gt;params.scale) {
          TF_LITE_ENSURE(context, TF_LITE_MEAN(reference_ops, int8_t, int));
        } else {
          TF_LITE_ENSURE(
              context,
              reference_ops::QuantizedMeanOrSum&lt;&gt;(
                  GetTensorData&lt;int8_t&gt;(op_context.input),
                  op_context.input-&gt;params.zero_point,
                  op_context.input-&gt;params.scale, op_context.input-&gt;dims-&gt;data,
                  op_context.input-&gt;dims-&gt;size,
                  GetTensorData&lt;int8_t&gt;(op_context.output),
                  op_context.output-&gt;params.zero_point,
                  op_context.output-&gt;params.scale,
                  op_context.output-&gt;dims-&gt;data, op_context.output-&gt;dims-&gt;size,
                  GetTensorData&lt;int&gt;(op_context.axis), num_axis,
                  op_context.params-&gt;keep_dims, GetTensorData&lt;int&gt;(temp_index),
                  GetTensorData&lt;int&gt;(resolved_axis),
                  GetTensorData&lt;int&gt;(temp_sum),
                  /*compute_sum=*/false));
        }
        break;
&lt;/denchmark-code&gt;

Then the interpreter seems work.
It seems that similar correction is already implemented in the 'master' branch. Maybe it works fine in the tf-nightly version, however, the v2.0.0 version needs this modification to run the test code correctly.
		</comment>
		<comment id='2' author='paanguin' date='2019-11-15T23:41:18Z'>
		Fix is in tf-nightly. It'll appear in the next tf.2.x release soon.
		</comment>
		<comment id='3' author='paanguin' date='2019-11-15T23:41:20Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33588&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33588&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>