<bug id='43262' author='adriang133' open_date='2020-09-16T12:08:49Z' closed_time='2020-09-18T12:20:05Z'>
	<summary>Unexpected behaviour for model.evaluate inside keras Callback</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
TensorFlow installed from (source or binary): Colab
TensorFlow version (use command below): 2.3.0
Python version: 3.6.9

Describe the current behavior
Inside a keras.callbacks.Callback, self.model.evaluate returns results for the validation_data regardless of what is passed in. The issue seems to not be present if validation_data is None.
Describe the expected behavior
self.model.evaluate should evaluate what is passed in.

&lt;denchmark-link:https://colab.research.google.com/drive/1RHrGba3sQ3iw9Ju0_m3WRFC5dog4WG6k?usp=sharing&gt;Colab notebook
&lt;/denchmark-link&gt;
 which illustrates the issue.
	</description>
	<comments>
		<comment id='1' author='adriang133' date='2020-09-16T14:56:07Z'>
		I think that problem is that with def on_train_end you are still in training mode.
		</comment>
		<comment id='2' author='adriang133' date='2020-09-18T01:09:00Z'>
		&lt;denchmark-link:https://github.com/adriang133&gt;@adriang133&lt;/denchmark-link&gt;

Please update as per above comment
		</comment>
		<comment id='3' author='adriang133' date='2020-09-18T06:47:47Z'>
		I'm using on_epoch_end in my example. I'm not too familiar with the innards of Keras, but why would on_epoch_end have the model in training mode still ?
If this is intended, is there a way I can evaluate the model on a separate validation dataset at the end of each epoch ?
		</comment>
		<comment id='4' author='adriang133' date='2020-09-18T09:15:50Z'>
		You can see what happens when you are calling : &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/engine/training.py#L1106&gt;https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/engine/training.py#L1106&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='adriang133' date='2020-09-18T09:58:47Z'>
		I see. The model state during training changes if you specify validation_data vs if you don't which is unintuitive if you ask me, but if this is intended then feel free to close this issue.
TLDR for anyone else running into this, a solution is to move the evaluation of both validation datasets to the callback and call fit(..., validation_data=None).
		</comment>
		<comment id='6' author='adriang133' date='2020-09-18T11:00:20Z'>
		I don't know what you want to do but generally when you have something in validation_data you have also on_train_begin on_train_end callback available that you could customize.
		</comment>
		<comment id='7' author='adriang133' date='2020-09-18T11:40:27Z'>
		I want to evaluate the model on 2 (or more) validation sets at the end of each epoch.  is only called at the end of the training, not per epoch, so it wouldn't do. See &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/38803&gt;#38803&lt;/denchmark-link&gt;
 for example. A google search yields a couple of similar solutions. They no longer work when  is passed, maybe behaviour has changed ?
As mentioned, this solution seems to work:
class EvalCallback(keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        for x, y in validation_datasets:
            print(self.model.evaluate(x, y, return_dict=True, verbose=0))

model.fit(..., validation_data=None, callbacks=[EvalCallback()])
Maybe Keras could provide a callback which evaluates one or more validation sets instead of the validation_data parameter?
		</comment>
		<comment id='8' author='adriang133' date='2020-09-18T12:17:17Z'>
		If you want direct support for multiple validations we are going in the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/38803&gt;#38803&lt;/denchmark-link&gt;
 perimeter.
I think that you can close this and upvote that issue.
		</comment>
		<comment id='9' author='adriang133' date='2020-09-18T12:20:06Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43262&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43262&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>