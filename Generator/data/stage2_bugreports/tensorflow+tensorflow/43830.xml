<bug id='43830' author='golden0080' open_date='2020-10-06T21:41:13Z' closed_time='2020-10-07T18:58:33Z'>
	<summary>Tensorflow cannot run inference together with TensorRT on the same GPU</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No, for TF model there is no custom code.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):
source
TensorFlow version (use command below):
1.15 / Command line: unknown 1.15.2
Python version: N/A
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 10.2/7.6
GPU model and memory:  2080Ti, 12GB

Describe the current behavior
I'm running C++ inference of TF and TensorRT on the same GPU. However, what I found is TF inference would be broken if I initialized TRT engine in any way.
Both TF and TRT are doing inference in their separated CPU threads, and creating separated CUcontext for these 2 inference engine returns the same errors from TF side.
The error is like this:
&lt;denchmark-code&gt;[Switching to Thread 0x7ffc1de50700 (LWP 26137)]
Cuda API error detected: cudaPointerGetAttributes returned (0x1)
(cuda-gdb) bt
#0  0x00007ffca7b41130 in cudbgReportDriverApiError () from /usr/lib/x86_64-linux-gnu/libcuda.so.1
#1  0x00007ffca7b440ca in cudbgReportDriverInternalError () from /usr/lib/x86_64-linux-gnu/libcuda.so.1
#2  0x00007ffca7b475a3 in cudbgApiDetach () from /usr/lib/x86_64-linux-gnu/libcuda.so.1
#3  0x00007ffca7d8abe3 in cudbgMain () from /usr/lib/x86_64-linux-gnu/libcuda.so.1
#4  0x00007ffca7d9d5a3 in cudbgMain () from /usr/lib/x86_64-linux-gnu/libcuda.so.1
#5  0x00007fffcbc60d99 in cudaPointerGetAttributes () from /home/.cache/_solib_local/_U@tensorflow_S_S_Ctensorflow___Uexternal_Stensorflow_Slib/libtensorflow_cc.so.1
#6  0x00007fffccbe2c24 in void stream_executor::gpu::(anonymous namespace)::CheckPointerIsValid&lt;void const*&gt;(void const*, absl::string_view) [clone .constprop.161] ()
   from /home/.cache/_solib_local/_U@tensorflow_S_S_Ctensorflow___Uexternal_Stensorflow_Slib/libtensorflow_cc.so.1
#7 stream_executor::gpu::GpuDriver::AsynchronousMemcpyH2D(stream_executor::gpu::GpuContext*, unsigned long long, void const*, unsigned long long, CUstream_st*) ()
...
&lt;/denchmark-code&gt;

Describe the expected behavior
I would expect both inference engine could be ran without broken by the other.
	</description>
	<comments>
		<comment id='1' author='golden0080' date='2020-10-06T21:45:16Z'>
		Creating CUcontext or not doesn't seem to affect TF inference, but initialization of TRT inference engine does break TF inference.
Does TF inference assumes certain CUcontext to be available? Or other stateful variables from Nvidia device API?
		</comment>
		<comment id='2' author='golden0080' date='2020-10-07T04:21:28Z'>
		&lt;denchmark-link:https://github.com/golden0080&gt;@golden0080&lt;/denchmark-link&gt;

Could you please upgrade to tf 2.x and let us know fi the issue exist, as there is no official support for 1.x.
		</comment>
		<comment id='3' author='golden0080' date='2020-10-07T18:58:33Z'>
		Thanks &lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;
 , guess I'm closing this ticket and update when I upgraded to 2.0.
		</comment>
		<comment id='4' author='golden0080' date='2020-10-07T18:58:35Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43830&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43830&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>