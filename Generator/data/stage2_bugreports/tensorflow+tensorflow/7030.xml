<bug id='7030' author='yaroslavvb' open_date='2017-01-24T01:25:10Z' closed_time='2018-09-13T18:07:21Z'>
	<summary>Feature Request: an op that returns bytes_in_use for its device</summary>
	<description>
We are trying to optimize some models to fit into TitanX's 12GB of RAM, and it's hard because of lack of transparency in TF available memory.
What would help this situation is an op that returns amount of bytes on its device when executed. Something similar to what's done in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/64edd34ce69b4a8033af5d217cb8894105297d8a/tensorflow/core/kernels/stack_ops.cc#L222&gt;stack_ops&lt;/denchmark-link&gt;
 for memory-aware heuristics
&lt;denchmark-code&gt;DeviceContext* device_ctxt = ctx-&gt;op_device_context();
auto device = static_cast&lt;tensorflow::Device*&gt;(ctx-&gt;device());
Allocator* allocator = device-&gt;GetAllocator(alloc_attrs);
AllocatorStats stats;
allocator-&gt;GetStats(&amp;stats)
//  output stats.bytes_in_use

&lt;/denchmark-code&gt;

This op can be wedged between other ops using control dependencies and used for memory debugging. This is complementary to request in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/6716&gt;#6716&lt;/denchmark-link&gt;
 because it would account for memory from parallel run calls, variables, persistent tensors.
I can take this issue if this op fits into TF framework
	</description>
	<comments>
		<comment id='1' author='yaroslavvb' date='2017-01-24T02:12:47Z'>
		&lt;denchmark-link:https://github.com/prb12&gt;@prb12&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/vrv&gt;@vrv&lt;/denchmark-link&gt;

That sounds like a pretty useful idea.
		</comment>
		<comment id='2' author='yaroslavvb' date='2017-01-24T02:40:16Z'>
		&lt;denchmark-link:https://github.com/yaroslavvb&gt;@yaroslavvb&lt;/denchmark-link&gt;
 maybe try writing it as a custom op using load_op_library for now so you can try the idea out and see how useful it is for you first -- can always add it to the core later if it's widely useful.
		</comment>
		<comment id='3' author='yaroslavvb' date='2017-01-24T17:54:17Z'>
		load_library seems like a great idea. Can custom ops depend on //tensorflow/core:core_cpu? I need this for Allocator header tensorflow/core/common_runtime/device.h but adding the dependency fails
&lt;denchmark-code&gt;tensorflow/core:core_cpu cannot depend on tensorflow/core:framework.
ERROR: Analysis of target '//tensorflow/core/user_ops:bytes_in_use.so' failed; build aborted.
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/yaroslavvb/tensorflow/blob/custom_op/tensorflow/core/user_ops/BUILD&gt;https://github.com/yaroslavvb/tensorflow/blob/custom_op/tensorflow/core/user_ops/BUILD&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='yaroslavvb' date='2017-01-24T18:06:28Z'>
		core_cpu is an internal target, but core in that directory has includes everything and has //visibility:public.
		</comment>
		<comment id='5' author='yaroslavvb' date='2017-01-24T18:28:23Z'>
		Same error when depending on core:core, this seems like a circular dependency somewhere
&lt;denchmark-code&gt;tensorflow/core:core cannot depend on tensorflow/core:framework.
ERROR: Analysis of target '//tensorflow/core/user_ops:bytes_in_use.so' failed; build aborted.

&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='yaroslavvb' date='2017-01-24T18:49:52Z'>
		From the error message it looks like "core" depends on ":framework" somehow. That should fail everywhere. Do you need that build file in core/user_ops? It looks like it's globbed by user_ops_libs already.
		</comment>
		<comment id='7' author='yaroslavvb' date='2017-01-24T19:03:18Z'>
		Yes, the error message doesn't make sense. I followed instructions in &lt;denchmark-link:https://www.tensorflow.org/how_tos/adding_an_op/&gt;https://www.tensorflow.org/how_tos/adding_an_op/&lt;/denchmark-link&gt;
 which says to put  file in . It passes analysis if I don't add dependency to  or , but then bazel complains about missing 
cc &lt;denchmark-link:https://github.com/keveman&gt;@keveman&lt;/denchmark-link&gt;
 in case he knows how to properly have user ops depend on  (part of  target)
		</comment>
		<comment id='8' author='yaroslavvb' date='2017-01-26T05:48:30Z'>
		OK, custom op turned out to be relatively straightforward, I put it here: &lt;denchmark-link:https://github.com/yaroslavvb/memory_probe_ops&gt;https://github.com/yaroslavvb/memory_probe_ops&lt;/denchmark-link&gt;

One snag is that when the op is placed on CPU, the allocator returns 0 for bytes_in_use, so this only works or GPUs. I still see LOGMEMORY messages coming from cpu allocations, so it seems this info gets lost somewhere.
Also, when graph optimization are turned on, it sometimes returns 0, and I suspect that ctx-&gt;device()-&gt;GetAllocator() gives me a CPU allocator. This is despite run_metadata and log_device_placement showing the op as existing on GPU. Things go back to normal when setting up session with tf.OptimizerOptions.L0
		</comment>
		<comment id='9' author='yaroslavvb' date='2017-01-27T17:38:07Z'>
		Maybe you have to run it with control_dependencies?
		</comment>
		<comment id='10' author='yaroslavvb' date='2017-01-27T19:07:26Z'>
		I got around the optimization issue by adding .SetIsStateful() to my op. Regarding CPU allocations, looks like by default byte tracking it is turned off. There's a function EnableCPUAllocatorStats(true); which I can call from the kernel, but it doesn't do anything. I suspect the problem is that the underlying variable is defined as static bool cpu_allocator_collect_stats = false;, so it can't be modified outside of pywraptensorflow.so
		</comment>
		<comment id='11' author='yaroslavvb' date='2017-01-27T19:54:36Z'>
		Right. If you're linked against it, it should.
For some reason we chose to use a Mutex, but I think std::atomic would have worked fine.
		</comment>
		<comment id='12' author='yaroslavvb' date='2017-01-27T20:28:05Z'>
		So I think for this feature to be complete using  mechanism, what's missing is the ability to enable CPU allocator stats either from TF client, or from dynamically loaded op kernel. &lt;denchmark-link:https://github.com/michaelisard&gt;@michaelisard&lt;/denchmark-link&gt;
  any suggestions?
		</comment>
		<comment id='13' author='yaroslavvb' date='2017-02-01T14:44:47Z'>
		BTW, the issue with using Bazel to build such op is that the such internal dependencies are disallowed. IE there's this &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/a0d784bdd31b27e013a7eac58a86ba62e86db299/tensorflow/tensorflow.bzl#L701&gt;line&lt;/denchmark-link&gt;
 in tf_custom_op
            disallowed_deps=["//tensorflow/core:framework",
Since //tensorflow/core:core depends on //tensorflow/core:framework and //tensorflow/core:framework is disallowed, I can't have a custom op depending on //tensorflow/core:core
		</comment>
		<comment id='14' author='yaroslavvb' date='2017-02-01T16:58:44Z'>
		&lt;denchmark-link:https://github.com/keveman&gt;@keveman&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/josh11b&gt;@josh11b&lt;/denchmark-link&gt;
  I feel like we should allow the following public headers from libraries as we do in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/BUILD#L22&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/BUILD#L22&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/yaroslavvb&gt;@yaroslavvb&lt;/denchmark-link&gt;
 have you tried not using bazel and trying to build your code using &lt;denchmark-link:https://www.tensorflow.org/how_tos/adding_an_op/#building_the_op_library&gt;https://www.tensorflow.org/how_tos/adding_an_op/#building_the_op_library&lt;/denchmark-link&gt;
 and then just loading the .so?  I suspect that will work because we package more headers there.
		</comment>
		<comment id='15' author='yaroslavvb' date='2017-02-01T17:09:45Z'>
		&lt;denchmark-link:https://github.com/vrv&gt;@vrv&lt;/denchmark-link&gt;
 indeed, I was able to compile it successfully using g++ on Mac and Linux, and released final product here -- &lt;denchmark-link:https://github.com/yaroslavvb/memory_probe_ops&gt;https://github.com/yaroslavvb/memory_probe_ops&lt;/denchmark-link&gt;

Curiously, I can build it using Bazel on MacOS without adding that dependency, using same version of bazel (0.4.3), it seems in MacOS the build is not fully hermetic
The only remaining issue is that cpu allocator always reports 0 bytes allocated. There's this line in allocator.cc
static bool cpu_allocator_collect_stats = false; 
But it feels like it should be superceded by this line:
 if (cpu_allocator_collect_full_stats || LogMemory::IsEnabled()) {
so I'm still tracking that one down
		</comment>
		<comment id='16' author='yaroslavvb' date='2017-02-18T21:12:06Z'>
		&lt;denchmark-link:https://github.com/josh11b&gt;@josh11b&lt;/denchmark-link&gt;
 suggested this op is not a good target for user-op in long-term because it would require opening up Allocator API. Made a tracking issue here &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/7581&gt;#7581&lt;/denchmark-link&gt;

		</comment>
		<comment id='17' author='yaroslavvb' date='2018-09-13T17:39:06Z'>
		&lt;denchmark-link:https://github.com/yaroslavvb&gt;@yaroslavvb&lt;/denchmark-link&gt;
 Hi, any update on this ? I can close this if there is a solution provided for the op requested or any workaround is done.
		</comment>
		<comment id='18' author='yaroslavvb' date='2018-09-13T18:07:20Z'>
		I think it can be closed, there are now ops which return memory + memory is available in timeline, I have examples of extracting memory from timeline in &lt;denchmark-link:https://github.com/yaroslavvb/memory_util&gt;https://github.com/yaroslavvb/memory_util&lt;/denchmark-link&gt;

		</comment>
		<comment id='19' author='yaroslavvb' date='2018-09-13T18:32:26Z'>
		Sounds good. Thank you !
		</comment>
	</comments>
</bug>