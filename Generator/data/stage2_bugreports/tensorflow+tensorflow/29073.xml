<bug id='29073' author='EmanueleGhelfi' open_date='2019-05-28T09:05:22Z' closed_time='2019-05-30T21:35:31Z'>
	<summary>Empty trainable variables in keras model (tf 2.0)</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): v1.12.1-2504-g2be59a5191 2.0.0-dev20190522
Python version: 3.6.8
CUDA/cuDNN version: CUDA Version 10.0.130
GPU model and memory: Nvidia GeForce GTX 1080 Ti

Describe the current behavior
I want to create UNet using the Keras Model subclassing API:
The current code is:
class UNet(keras.Model):
    """
    UNet Architecture concatenating encoder and decoder

    Examples:
        * Direct Usage:

            .. testcode::

                x = tf.ones((1, 512, 512, 3))
                u_net = UNet(input_res = 512,
                             min_res=4,
                             kernel_size=4,
                             initial_filters=64,
                             filters_cap=512,
                             channels=3)
                y = u_net(x)
                print(y.shape)

            .. testoutput::
                (1, 512, 512, 3)

    """

    def __init__(
        self,
        input_res,
        min_res,
        kernel_size,
        initial_filters,
        filters_cap,
        channels,
        use_dropout_encoder=True,
        use_dropout_decoder=True,
        dropout_prob=0.3,
        encoder_non_linearity=keras.layers.LeakyReLU,
        decoder_non_linearity=keras.layers.ReLU,
    ):
        super().__init__()

        # layer specification
        self.use_dropout_encoder = use_dropout_encoder
        self.use_dropout_decoder = use_dropout_decoder
        self.dropout_probability = dropout_prob
        self.encoder_non_linearity = encoder_non_linearity
        self.decoder_non_linearity = decoder_non_linearity
        self.kernel_size = kernel_size

        # encoder layers is a list of list, each list is a "block",
        # this makes easy the creation of decoder
        self.encoder_layers = []
        self.decoder_layers = []
        self.concat_layers = []

        # ########### Encoder creation
        encoder_layers_spec = [128, 256, 512, 512, 512, 512, 512, 512]

        decoder_layer_spec = []
        for i, filters in enumerate(encoder_layers_spec):
            self.encoder_layers.append(self.get_encoder_block(filters, use_bn=(i != 0)))

        # ############## Decoder creation
        decoder_layer_spec =[512, 512, 512, 512, 512, 256, 128]

        for i, filters in enumerate(decoder_layer_spec):
            self.concat_layers.append(keras.layers.Concatenate())
            self.decoder_layers.append(
                self.get_decoder_block(filters, use_dropout=(i &lt; 3))
            )

        # final layer
        initializer = tf.random_normal_initializer(0.0, 0.02)
        self.final_layer = keras.layers.Conv2DTranspose(
            channels,
            self.kernel_size,
            strides=(2, 2),
            padding="same",
            activation=keras.activations.tanh,
            kernel_initializer=initializer,
        )

    def get_block(
        self,
        filters,
        conv_layer=None,
        use_bn=True,
        use_dropout=False,
        non_linearity=keras.layers.LeakyReLU,
    ):
        initializer = tf.random_normal_initializer(0.0, 0.02)
        # Conv2D
        block = [
            conv_layer(
                filters,
                self.kernel_size,
                strides=(2, 2),
                padding="same",
                use_bias=False,
                kernel_initializer=initializer,
            )
        ]

        # Batch normalization
        if use_bn:
            block.append(keras.layers.BatchNormalization())

        # dropout
        if use_dropout:
            block.append(keras.layers.Dropout(self.dropout_probability))

        # Non linearity
        block.append(non_linearity())

        return block

    def get_encoder_block(self, filters, use_bn=True):
        return self.get_block(
            filters,
            conv_layer=keras.layers.Conv2D,
            use_bn=use_bn,
            use_dropout=self.use_dropout_encoder,
            non_linearity=self.encoder_non_linearity,
        )

    def get_decoder_block(self, filters, use_bn=True, use_dropout=False):
        return self.get_block(
            filters,
            conv_layer=keras.layers.Conv2DTranspose,
            use_bn=use_bn,
            use_dropout=self.use_dropout_decoder and use_dropout,
            non_linearity=self.decoder_non_linearity,
        )

    def __call__(self, inputs, training=True):
        # encoders evaluated
        encoder_layer_eval = []
        x = inputs

        for block in self.encoder_layers:
            for layer in block:
                if isinstance(layer, keras.layers.BatchNormalization) or isinstance(
                    layer, keras.layers.Dropout
                ):
                    x = layer(x, training=training)
                else:
                    x = layer(x)
            encoder_layer_eval.append(x)

        encoder_layer_eval = encoder_layer_eval[:-1]

        for i, block in enumerate(self.decoder_layers):
            for layer in block:
                if isinstance(layer, keras.layers.BatchNormalization) or isinstance(
                    layer, keras.layers.Dropout
                ):
                    x = layer(x, training=training)
                else:
                    x = layer(x)
            x = self.concat_layers[i]([x, encoder_layer_eval[-1 - i]])

        x = self.final_layer(x)

        return x
When I evaluate the model using an input and check the trainable variables they are empty:
x = tf.ones((1, 512, 512, 3))
u_net = UNet(input_res = 512,
                       min_res=4, 
                       kernel_size=4,
                       initial_filters=64,
                       filters_cap=512,
                       channels=3)
                y = u_net(x)               
                print(u_net.trainable_variables) # it prints: []
Describe the expected behavior
The code should print the list of trainable variables of the Net. The output is correct so the __call__ method is correctly called.
	</description>
	<comments>
		<comment id='1' author='EmanueleGhelfi' date='2019-05-28T12:33:39Z'>
		Update: After some debugging I found the cause of this issue.
The problem is that keras does its magic under the hood.
In the UNet init:
self.encoder_layers = []
self.decoder_layers = []
self.concat_layers = []
Keras intercepts the creation of the lists and initializes its attributes. Keras does not manage the append of layers to a list.
Changing the above code in:
    def __init__(
        self,
        input_res,
        min_res,
        kernel_size,
        initial_filters,
        filters_cap,
        channels,
        use_dropout_encoder=True,
        use_dropout_decoder=True,
        dropout_prob=0.3,
        encoder_non_linearity=keras.layers.LeakyReLU,
        decoder_non_linearity=keras.layers.ReLU,
    ):
        super().__init__()

        # layer specification
        self.use_dropout_encoder = use_dropout_encoder
        self.use_dropout_decoder = use_dropout_decoder
        self.dropout_probability = dropout_prob
        self.encoder_non_linearity = encoder_non_linearity
        self.decoder_non_linearity = decoder_non_linearity
        self.kernel_size = kernel_size

        # encoder layers is a list of list, each list is a "block",
        # this makes easy the creation of decoder
        # IMPORTANT! Do not initialize here instance attributes
        encoder_layers = []
        decoder_layers = []
        concat_layers = []

        # ########### Encoder creation
        encoder_layers_spec =  [128, 256, 512, 512, 512, 512, 512, 512]

        for i, filters in enumerate(encoder_layers_spec):
            block = self.get_encoder_block(filters, use_bn=(i != 0))
            encoder_layers.append(block)

        # ############## Decoder creation
        decoder_layer_spec = [512, 512, 512, 512, 512, 256, 128]

        for i, filters in enumerate(decoder_layer_spec):
            concat_layers.append(keras.layers.Concatenate())
            block = self.get_decoder_block(filters, use_dropout=(i &lt; 3))
            decoder_layers.append(block)

        # final layer
        initializer = tf.random_normal_initializer(0.0, 0.02)
        self.final_layer = keras.layers.Conv2DTranspose(
            channels,
            self.kernel_size,
            strides=(2, 2),
            padding="same",
            activation=keras.activations.tanh,
            kernel_initializer=initializer,
        )
       # intialize here object attributes!!!!!!!
        self.encoder_layers = encoder_layers
        self.decoder_layers = decoder_layers
        self.concat_layers = concat_layers
Solves the issue. In this way the variables are correctly added to the trainable variables. I think this is a serious bug. It should be fixed or at least documented.
		</comment>
		<comment id='2' author='EmanueleGhelfi' date='2019-05-30T17:49:16Z'>
		We agree that this should be better documented. Would you be interested in adding examples and references in the base Layer docstring?
		</comment>
		<comment id='3' author='EmanueleGhelfi' date='2019-05-30T18:08:39Z'>
		Just ran this by &lt;denchmark-link:https://github.com/allenlavoie&gt;@allenlavoie&lt;/denchmark-link&gt;
, this  have been fixed in newer versions of TensorFlow and is worth checking with the nightly. (It was initially broken, then fixed, then there was a regression with some of &lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;
 layer refactoring changes, then may have been fixed again).
		</comment>
		<comment id='4' author='EmanueleGhelfi' date='2019-05-30T21:35:12Z'>
		I wasn't able to reproduce the error with nightly version of TF. Basically adding the layer to the list attribute of the subclass model should work. Nested list should work too, since we recursively find all the layer like objects.
class M(tf.keras.models.Model):

    def __init__(self):
        super(M, self).__init__()
        self._list = []
        self._list.append([tf.keras.layers.Dense(5), tf.keras.layers.Dense(5)])
        self._list.append([tf.keras.layers.Dense(5), tf.keras.layers.Dense(5)])

    def call(self, inputs):
        output = inputs
        for l_list in self._list:
            for l in l_list:
                output = l(output)
        return output

m = M()
m(tf.ones((10, 10)))
print(len(m.trainable_variables)) # Got 8
		</comment>
		<comment id='5' author='EmanueleGhelfi' date='2019-05-30T21:35:31Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=29073&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=29073&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='EmanueleGhelfi' date='2019-05-31T07:12:05Z'>
		Update: this is fixed in version v1.12.1-2994-g5fa098e3b9 2.0.0-dev20190530
		</comment>
	</comments>
</bug>