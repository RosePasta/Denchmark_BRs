<bug id='31819' author='pavanky' open_date='2019-08-21T01:25:26Z' closed_time='2019-09-03T17:40:10Z'>
	<summary>keras.model.fit does not work with SparseTensor inputs with functional API</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes


OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 18.04


Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No


TensorFlow installed from (source or binary):
Binary


TensorFlow version (use command below):
1.14.0


Describe the current behavior
The program exits with the following error:
&lt;denchmark-code&gt;tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'sparse_tensor/indices' with dtype int64 and shape [?,2]
	 [[{{node sparse_tensor/indices}}]]

&lt;/denchmark-code&gt;

Describe the expected behavior
No error occurs and training continues.
Code to reproduce the issue
&lt;denchmark-code&gt;import numpy as np
import tensorflow.compat.v2 as tf

def dummy_parse_fn(iterable):
  features = {}
  # the input is always constant
  features['sparse_tensor'] = tf.SparseTensor(
    indices=tf.constant([[0,0],[1,1]], dtype=tf.int64),
    values=tf.constant([1.0, 1.0], dtype=tf.float32),
    dense_shape=tf.constant([2, 2], dtype=tf.int64))
  features['sparse_tensor/indices'] = features['sparse_tensor'].indices
  features['sparse_tensor/values'] = features['sparse_tensor'].values
  features['sparse_tensor/dense_shape'] = features['sparse_tensor'].dense_shape
  labels = tf.constant([1.0, 1.0], dtype=tf.float32)
  return features, labels


def get_dummy_dataset():
  iterable =  np.random.random((128, 1)).astype(np.float32)
  return (
    tf.data.Dataset
    .from_tensor_slices(iterable)
    .map(dummy_parse_fn)
    .take(1024)
  )


if __name__ == "__main__":

  print(tf.__version__)

  inputs = tf.keras.layers.Input(shape=(2, ), sparse=True, name="sparse_tensor")
  weights = tf.Variable(name='weights', shape=(2, 1), initial_value=[[1.0], [1.0]])
  output = tf.sparse.sparse_dense_matmul(inputs, weights)
  model = tf.keras.Model([inputs], output)

  optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9, nesterov=True)
  loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)

  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])
  model.fit(get_dummy_dataset(), epochs=2)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='pavanky' date='2019-08-21T17:54:12Z'>
		This is failing in tf-nightly as well:
&lt;denchmark-code&gt;1.15.0-dev20190821
W0821 10:51:36.244404 139992655496832 deprecation.py:506] From /home/pyalamanchili/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2019-08-21 10:51:36.246914: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-08-21 10:51:36.266789: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2112500000 Hz
2019-08-21 10:51:36.267215: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55948d8b0c80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2019-08-21 10:51:36.267235: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
Traceback (most recent call last):
  File "/tmp/sparse_model.py", line 36, in &lt;module&gt;
    model = tf.keras.Model([inputs], output)
  File "/home/pyalamanchili/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py", line 147, in __init__
    super(Model, self).__init__(*args, **kwargs)
  File "/home/pyalamanchili/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py", line 164, in __init__
    self._init_graph_network(*args, **kwargs)
  File "/home/pyalamanchili/.local/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py", line 457, in _method_wrapper
    result = method(self, *args, **kwargs)
  File "/home/pyalamanchili/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py", line 267, in _init_graph_network
    base_layer_utils.create_keras_history(self._nested_outputs)
  File "/home/pyalamanchili/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py", line 184, in create_keras_history
    _, created_layers = _create_keras_history_helper(tensors, set(), [])
  File "/home/pyalamanchili/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py", line 229, in _create_keras_history_helper
    constants[i] = backend.function([], op_input)([])
  File "/home/pyalamanchili/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py", line 3476, in __call__
    run_metadata=self.run_metadata)
  File "/home/pyalamanchili/.local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py", line 1472, in __call__
    run_metadata_ptr)
tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'sparse_tensor/indices' with dtype int64 and shape [?,2]
	 [[{{node sparse_tensor/indices}}]]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='pavanky' date='2019-08-21T18:22:20Z'>
		Alternative way to do this  in tf-nightly can be seen here: &lt;denchmark-link:https://gist.github.com/pavanky/3e1635edb322c81436d66ac9d9bdf69f&gt;https://gist.github.com/pavanky/3e1635edb322c81436d66ac9d9bdf69f&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='pavanky' date='2019-08-22T09:19:03Z'>
		I could able to reproduce the issue with Tensorflow 1.14.0. Please see the gist &lt;denchmark-link:https://colab.research.google.com/drive/1RvCNPW8pDR0vUD23d2Q8rzqjYWMQb5sj&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='4' author='pavanky' date='2019-08-28T01:29:49Z'>
		Took a quick look at the issue and seems like we may need special case handling for SparseTensor/SparseOps in the InputLayer/TensorFlowOpLayer implementation as the inputs of the sparse ops are (indices, values, dense_shape, other inputs) and not the sparse tensor itself.
When creating input layer for sparse tensor we set _keras_history on the sparse tensor. When we try to convert the sparse matmul op into a TensorFlow Op layer, we check if the inputs to the layer have _keras_history property set. The property was set on the sparse tensor, however op.inputs on sparse matmul op will return (indices, values, dense_shape, variable) tensors and not the sparse tensor.
		</comment>
		<comment id='5' author='pavanky' date='2019-08-28T01:53:43Z'>
		&lt;denchmark-link:https://github.com/pavithrasv&gt;@pavithrasv&lt;/denchmark-link&gt;
 would a potential workaround be that I create a custom  with the right inputs?
		</comment>
		<comment id='6' author='pavanky' date='2019-08-28T01:59:44Z'>
		Nvm, I guess no matter what we do in the functional API, this will be a problem with sparse operations.
		</comment>
		<comment id='7' author='pavanky' date='2019-08-28T03:16:32Z'>
		&lt;denchmark-link:https://github.com/pavanky&gt;@pavanky&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/pavithrasv&gt;@pavithrasv&lt;/denchmark-link&gt;

This is what worked for me (only tested in 2.0), note that any stateful operations in the Functional API have to be contained inside a Layer:
!pip install tf-nightly-2.0-preview

import numpy as np
import tensorflow as tf

def dummy_parse_fn(iterable):
  # the input is always constant
  features = tf.SparseTensor(
    indices=tf.constant([[0,0],[1,1]], dtype=tf.int64),
    values=tf.constant([1.0, 1.0], dtype=tf.float32),
    dense_shape=tf.constant([2, 2], dtype=tf.int64))
  labels = tf.constant([1.0, 1.0], dtype=tf.float32)
  return features, labels


def get_dummy_dataset():
  iterable =  np.random.random((128, 1)).astype(np.float32)
  return (
    tf.data.Dataset
    .from_tensor_slices(iterable)
    .map(dummy_parse_fn)
    .take(1024)
  )

print(tf.__version__)

inputs = tf.keras.layers.Input(shape=(2, ), sparse=True, name="sparse_tensor")
# Variables and any stateful ops in the Functional API have to be defined inside Layers,
# even if just a lambda layer.
outputs = tf.keras.layers.Lambda(
    lambda x: tf.sparse.sparse_dense_matmul(x, tf.Variable(name='weights', shape=(2, 1), initial_value=[[1.0], [1.0]])))(inputs)
model = tf.keras.Model([inputs], outputs)

optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9, nesterov=True)
loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)

model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])
model.fit(get_dummy_dataset(), epochs=2)
		</comment>
		<comment id='8' author='pavanky' date='2019-08-28T03:20:28Z'>
		
When we try to convert the sparse matmul op into a TensorFlow Op layer, we check if the inputs to the layer have _keras_history property set. The property was set on the sparse tensor, however op.inputs on sparse matmul op will return (indices, values, dense_shape, variable) tensors and not the sparse tensor.

&lt;denchmark-link:https://github.com/pavithrasv&gt;@pavithrasv&lt;/denchmark-link&gt;
 Yeah I think this is still a problem for the TensorFlowOpLayers. We should probably  when setting the _keras_history in the InputLayer, and also in  / wherever else we set it
		</comment>
		<comment id='9' author='pavanky' date='2019-08-29T18:32:40Z'>
		I have a workaround by patching  from 1.15 into 1.14 and using the &lt;denchmark-link:https://gist.github.com/pavanky/3e1635edb322c81436d66ac9d9bdf69f&gt;layer API&lt;/denchmark-link&gt;
 so this isnt a blocker.
		</comment>
		<comment id='10' author='pavanky' date='2019-08-30T21:59:35Z'>
		&lt;denchmark-link:https://github.com/pavanky&gt;@pavanky&lt;/denchmark-link&gt;
 For now we are raising an exception disallowing usage of sparse ops without wrapping it in Lambda layer and providing the workaround information in the error message.
		</comment>
		<comment id='11' author='pavanky' date='2019-09-03T17:40:10Z'>
		Closing this issue as we are raising a clear exception disallowing the use case instead of failing with a random error. We will look into whether we can support the usage of sparse ops with TensorFlow op layer in the long run. Thank you!
		</comment>
		<comment id='12' author='pavanky' date='2019-09-03T17:40:11Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=31819&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=31819&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='pavanky' date='2019-09-03T21:32:50Z'>
		&lt;denchmark-link:https://github.com/pavithrasv&gt;@pavithrasv&lt;/denchmark-link&gt;
 I am not 100% sure the suggested alternative works in tensorflow 1.14:
I originally created this to simplify the repro code, but this fails in 1.14 too
&lt;denchmark-code&gt;import numpy as np
import tensorflow as tf
import tensorflow.keras as keras

class Sparse(keras.layers.Layer):
  def __init__(self, units, activation=None, use_bias=True,
               kernel_initializer='glorot_uniform',
               kernel_regularizer=None,
               **kwargs):
    super(Sparse, self).__init__(**kwargs)
    self.units = units
    self.kernel_initializer = keras.initializers.get(kernel_initializer)
    self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)

  def build(self, input_shape):

    if self.built:
      return

    input_shape = tf.compat.v2.TensorShape(input_shape)
    input_shape = input_shape.with_rank(2)
    input_size = input_shape[-1].value

    if input_size is None:
      raise ValueError("Sparse layer expects the last dimension of Input to be known")

    self.kernel = self.add_weight(
      name="kernel",
      shape=[input_size, self.units],
      initializer=self.kernel_initializer,
      regularizer=self.kernel_regularizer,
      trainable=True)
    self.built = True

  def compute_output_shape(self, input_shape):
    input_shape = tf.compat.v2.TensorShape(input_shape)
    input_shape = input_shape.with_rank(2)
    return input_shape[:-1].concatenate(self.units)

  def call(self, inputs):
    outputs = tf.sparse.sparse_dense_matmul(inputs, self.kernel)
    return outputs


def dummy_parse_fn(iterable):
  # the input is always constant
  features = tf.SparseTensor(
    indices=tf.constant([[0,0],[1,1]], dtype=tf.int64),
    values=tf.constant([1.0, 1.0], dtype=tf.float32),
    dense_shape=tf.constant([2, 2], dtype=tf.int64))
  labels = tf.constant([1.0, 1.0], dtype=tf.float32)
  return features, labels


def get_dummy_dataset():
  iterable =  np.random.random((128, 1)).astype(np.float32)
  return (
    tf.data.Dataset
    .from_tensor_slices(iterable)
    .map(dummy_parse_fn)
    .take(1024)
  )

print(tf.__version__)

# not setting the batch size doesnt work??
inputs = tf.keras.layers.Input(shape=(2, ), batch_size=2, sparse=True, name="sparse_tensor")
# Variables and any stateful ops in the Functional API have to be defined inside Layers,
# even if just a lambda layer.
outputs = Sparse(1)(inputs)
model = tf.keras.Model([inputs], outputs)

optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9, nesterov=True)
loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)

model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])
model.fit(get_dummy_dataset(), epochs=2)
&lt;/denchmark-code&gt;

But this fails with a different error that seems to be fixed in 1.15?
Are there any plans to patch 1.14 or is a fix only expected in the next release?
		</comment>
		<comment id='14' author='pavanky' date='2019-09-03T21:41:55Z'>
		The exception is in the commit &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/e3805e70c4634f89af772524b4135953fa27dc2b#diff-8bfbe586f2ba61d9aa9e39676c7f69ee&gt;e3805e7#diff-8bfbe586f2ba61d9aa9e39676c7f69ee&lt;/denchmark-link&gt;
 . This is not included in 1.14. Since this is just raising an exception with a workaround suggestion and the workaround is provided here already we decided not to cherrypick this into a release. Please let me know if that is ok.
		</comment>
		<comment id='15' author='pavanky' date='2019-09-03T22:01:34Z'>
		&lt;denchmark-link:https://github.com/pavithrasv&gt;@pavithrasv&lt;/denchmark-link&gt;
 thanks, will wait for the next release. We have work arounds for now.
		</comment>
		<comment id='16' author='pavanky' date='2019-09-23T20:01:52Z'>
		BTW a quicker workaround for this is to use Keras 2.2.5 instead of tf.keras.
For some reason, the implementation of Keras that is shipped with Tensorflow does not have all the features of original Keras (which supported sparse inputs, even when using functional API and Model.fit).
Also worth noticing that TF 1.15.0rc1 does not fix the issue.
		</comment>
	</comments>
</bug>