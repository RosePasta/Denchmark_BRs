<bug id='5914' author='vasantivmahajan' open_date='2016-11-28T22:07:48Z' closed_time='2016-12-07T21:34:38Z'>
	<summary>No documentation for sending input parameters of request to TensorFlow Serving</summary>
	<description>
I am running the sample iris program in TensorFlow Serving. Since it is a TF.Learn model, I am exporting the model using the following classifier.export(export_dir=model_dir,signature_fn=my_classification_signature_fn)
and the signature_fn is defined as shown below:
&lt;denchmark-code&gt;def my_classification_signature_fn(examples, unused_features, predictions):
  """Creates classification signature from given examples and predictions.
  Args:
    examples: `Tensor`.
    unused_features: `dict` of `Tensor`s.
    predictions: `Tensor` or dict of tensors that contains the classes tensor
      as in {'classes': `Tensor`}.
  Returns:
    Tuple of default classification signature and empty named signatures.
  Raises:
    ValueError: If examples is `None`.
  """
  if examples is None:
    raise ValueError('examples cannot be None when using this signature fn.')

  if isinstance(predictions, dict):
    default_signature = exporter.classification_signature(
        examples, classes_tensor=predictions['classes'])

  else:

    default_signature = exporter.classification_signature(
        examples, classes_tensor=predictions)
  named_graph_signatures={
        'inputs': exporter.generic_signature({'x_values': examples}),
        'outputs': exporter.generic_signature({'preds': predictions})}    
  return default_signature, named_graph_signatures
&lt;/denchmark-code&gt;

The model gets successfully exported using the following piece of code.
I have created a client which makes real-time predictions using TensorFlow Serving.
The following is the code for the client:
&lt;denchmark-code&gt;flags.DEFINE_string("model_dir", "/tmp/iris_model_dir", "Base directory for output models.")
tf.app.flags.DEFINE_integer('concurrency', 1,
                            'maximum number of concurrent inference requests')
tf.app.flags.DEFINE_string('server', '', 'PredictionService host:port')

#connection
host, port = FLAGS.server.split(':')
channel = implementations.insecure_channel(host, int(port))
stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)


# Classify two new flower samples.
new_samples = np.array([5.8, 3.1, 5.0, 1.7], dtype=float)

request = predict_pb2.PredictRequest()
request.model_spec.name = 'iris'

request.inputs["x_values"].CopyFrom(
        tf.contrib.util.make_tensor_proto(new_samples))

result = stub.Predict(request, 10.0)  # 10 secs timeout
&lt;/denchmark-code&gt;

However, on making the predictions, the following error is displayed:
grpc.framework.interfaces.face.face.AbortionError: AbortionError(code=StatusCode.INTERNAL, details="Output 0 of type double does not match declared output type string for node _recv_input_example_tensor_0 = _Recv[client_terminated=true, recv_device="/job:localhost/replica:0/task:0/cpu:0", send_device="/job:localhost/replica:0/task:0/cpu:0", send_device_incarnation=2016246895612781641, tensor_name="input_example_tensor:0", tensor_type=DT_STRING, _device="/job:localhost/replica:0/task:0/cpu:0"]()")
Here is the entire stack trace.
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/17990840/20688020/fac6285a-b58c-11e6-8a93-1fed6d78b8de.png&gt;&lt;/denchmark-link&gt;

The iris model is defined in the following manner:
&lt;denchmark-code&gt;# Specify that all features have real-value data
feature_columns = [tf.contrib.layers.real_valued_column("", dimension=4)]

# Build 3 layer DNN with 10, 20, 10 units respectively.
classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,
                                            hidden_units=[10, 20, 10],
                                            n_classes=3, model_dir=model_dir)

# Fit model.
classifier.fit(x=training_set.data, 
               y=training_set.target, 
               steps=2000)
&lt;/denchmark-code&gt;

Can someone provide some documentation for creating the client program which sends the request to TensorFlow Serving.
	</description>
	<comments>
		<comment id='1' author='vasantivmahajan' date='2016-11-29T17:49:02Z'>
		Can someone help me with this?
		</comment>
		<comment id='2' author='vasantivmahajan' date='2016-12-01T02:27:17Z'>
		Generally, this kind of question is best asked on stackoverflow.  This forum is for bug reports.
I'm not an expert on this topic.  Have you had success using TensorFlow Serving with any other models?  That is, have you tried (and succeeded) with these tutorials?
&lt;denchmark-link:https://tensorflow.github.io/serving/serving_basic.html&gt;https://tensorflow.github.io/serving/serving_basic.html&lt;/denchmark-link&gt;

&lt;denchmark-link:https://tensorflow.github.io/serving/serving_advanced.html&gt;https://tensorflow.github.io/serving/serving_advanced.html&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='vasantivmahajan' date='2016-12-01T14:57:53Z'>
		&lt;denchmark-link:https://github.com/poxvoculi&gt;@poxvoculi&lt;/denchmark-link&gt;
: I have already asked the question on stack overflow, however, I haven't received any help. And I have also tried running the basic models (mnist and inception) and they seem to work fine. However, there is no documentation for actually creating a client program for any other algorithms, for instance, the models build using Tf.Learn API. Which is why I raised a concern here so that there could be some documentation for the same.
		</comment>
		<comment id='4' author='vasantivmahajan' date='2016-12-07T20:50:01Z'>
		&lt;denchmark-link:https://github.com/martinwicke&gt;@martinwicke&lt;/denchmark-link&gt;
 could you reassign to someone who might provide documentation?
		</comment>
		<comment id='5' author='vasantivmahajan' date='2016-12-07T21:34:38Z'>
		Can you please ask this question on tensorflow/serving?
		</comment>
	</comments>
</bug>