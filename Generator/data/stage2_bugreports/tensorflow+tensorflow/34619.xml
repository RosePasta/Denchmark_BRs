<bug id='34619' author='andmax' open_date='2019-11-26T16:01:20Z' closed_time='2020-02-28T19:38:22Z'>
	<summary>Wrong output printing when using fit_generator</summary>
	<description>
On TensorFlow 1.15 (OS Ubuntu 16.04), when I'm using generators to train a model via the tf.keras.model fit_generator() function call prints a wrong and duplicated output of "Epoch 1" when starting the validation steps on every epoch.  The function fit_generator() prints:
&lt;denchmark-code&gt;Epoch 1/2
Epoch 1/2
32/32 - 5s - loss: 2.3132 - val_loss: 2.3116
Epoch 2/2
Epoch 1/2
32/32 - 1s - loss: 2.3070 - val_loss: 2.3075
&lt;/denchmark-code&gt;

The minimal code to reproduce is:
&lt;denchmark-code&gt;import math
import tensorflow as tf
from tensorflow.keras.utils import to_categorical


def data_generator(X, Y, batch_size, start=0, end=None):
    end = len(X) if end is None else end
    num_batches = int(math.ceil((end-start)/batch_size))
    while True:
        lob = list(range(num_batches))
        for bi in lob:
            sb = start + bi*batch_size
            eb = sb + batch_size
            eb = end if eb &gt; end else eb
            Xb = X[sb:eb]
            Yb = Y[sb:eb]
            yield Xb, Yb


(Xtr, Ytr), (Xva, Yva) = tf.keras.datasets.cifar10.load_data()
Xtr, Ytr, Xva, Yva, nc = Xtr[:1000], Ytr[:1000], Xva[:100], Yva[:100], 10
Xtr, Xva = Xtr.astype('float32') / 255, Xva.astype('float32') / 255
Ytr, Yva, ins = to_categorical(Ytr, nc), to_categorical(Yva, nc), Xtr.shape[1:]

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(ins))
model.add(tf.keras.layers.Conv2D(8, (3, 3)))
model.add(tf.keras.layers.Activation('relu'))
model.add(tf.keras.layers.Conv2D(8, (3, 3), activation='relu'))
model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))
model.add(tf.keras.layers.Dropout(0.25))
model.add(tf.keras.layers.Conv2D(16, (3, 3), activation='relu'))
model.add(tf.keras.layers.Conv2D(16, (3, 3), activation='relu'))
model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))
model.add(tf.keras.layers.Dropout(0.25))
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(32, activation='relu'))
model.add(tf.keras.layers.Dropout(0.5))
model.add(tf.keras.layers.Dense(nc, activation='softmax'))
opt = tf.keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)
model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[])

batch_size = 32
tr_gen = data_generator(Xtr, Ytr, batch_size)
va_gen = data_generator(Xva, Yva, batch_size)
tr_steps = int(math.ceil(len(Xtr)/batch_size))
va_steps = int(math.ceil(len(Xva)/batch_size))

model.fit_generator(tr_gen, steps_per_epoch=tr_steps, epochs=2, verbose=2,
                    validation_data=va_gen, validation_steps=va_steps)
&lt;/denchmark-code&gt;

If I run it on TensorFlow 2.0, the duplicated and wrong prints disappear.
	</description>
	<comments>
		<comment id='1' author='andmax' date='2019-11-28T05:43:24Z'>
		Could replicate the issue with Tf 1.15 and its working fine with Tf 2.0.
Please see the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/aca26f3a33c9dabd1819dcce2d4af363/untitled275.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='andmax' date='2020-02-28T19:38:22Z'>
		&lt;denchmark-link:https://github.com/andmax&gt;@andmax&lt;/denchmark-link&gt;
 Thanks for the issue! This is fixed in the latest nightly, unfortunately we can't backport changes like this to older version.
Please try it out with pip install -U tf-nightly
		</comment>
		<comment id='3' author='andmax' date='2020-02-28T19:38:24Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34619&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34619&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>