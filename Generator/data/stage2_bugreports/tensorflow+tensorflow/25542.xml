<bug id='25542' author='chandanchinta' open_date='2019-02-06T11:19:05Z' closed_time='2019-03-01T00:50:23Z'>
	<summary>Issues when training with keras.callbacks.tensorboard</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows 10
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:Np
TensorFlow installed from (source or binary):Through anaconda
TensorFlow version (use command below):latest
Python version:3.6
Bazel version (if compiling from source):No
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:No
GPU model and memory:Nvidia 920m

You can collect some of this information using our environment capture &lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with
python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
Code-Begins
from nltk.tokenize import word_tokenize
from tensorflow import keras
import tensorflow as tf
import pandas as pd
import re
all_words = []
#importing the data
dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\t', quoting = 3)
#converting words present in each review to arrays
for i in range(0, 1000):
review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])
review = review.lower()
review = review.split()
review = ' '.join(review)
all_words.append(word_tokenize(review))
#reading each word from all the reviews
words = []
for i in range(len(all_words)):
for word in all_words[i]:
words.append(word)
#removing repeated words
clean_words = []
for word in words:
if word not in clean_words:
clean_words.append(word)
#assigning a number to each word and creating sequence of words
count = []
for i in range(0,len(clean_words)):
count.append(i)
sequence_of_words = dict(zip(clean_words,count))
with open('metadata.tsv', 'w') as metadata_file:
metadata_file.write('ID\tWord\n')
for Key, Value in sequence_of_words.items():
metadata_file.write('{}\t{}\n'.format(Value, Key))
#saving the sequence of words
import pickle
with open('sequenceofwords' , 'wb') as fid:
pickle.dump(sequence_of_words , fid)
#reading the labels into an array
labels =[]
for i in range(0,1000):
labels.append(dataset.Liked[i])
#converting each review to sequence of words
reverse_mapping=[]
for i in range(0, 1000):
lop = []
review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])
review = review.lower()
for text in review.split():
if text not in sequence_of_words.keys():
lop.append('0')
if text in sequence_of_words.keys():
lop.append(sequence_of_words[text])
reverse_mapping.append(lop)
#deviding the data for training and testing
trainer_data = list(reverse_mapping[:700])
train_labels = [labels[:700]]
tester_data = list(reverse_mapping[700:])
test_labels = [labels[700:]]
train_data = keras.preprocessing.sequence.pad_sequences(trainer_data,
value=0,
padding='post',
maxlen=256)
test_data = keras.preprocessing.sequence.pad_sequences(tester_data, value = 0 , padding = 'post' , maxlen = 256)
#creating the neural network
vocab_size = 2021
model = keras.Sequential()
model.add(keras.layers.Embedding(vocab_size, 16, input_length = 256))
model.add(keras.layers.GlobalAveragePooling1D())
model.add(keras.layers.Dense(16, activation=tf.nn.relu))
model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))
#configuring the model
model.compile(optimizer='adam',
loss='binary_crossentropy',
metrics=['accuracy'])
#creating a meta file for tensorboard.
meta_file = open(r'E:\ENTERTAINMENT\OneDrive\Desktop\NLTK - Workspace\NLP\Natural_Language_Processing\review classifier\Tensorflow classifier\ML-\Training\metadata.tsv')
cb = keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=5, batch_size=256, write_graph=True, write_grads=True, write_images=False, embeddings_freq=5, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=train_data)
#training the model
model.fit(train_data, train_labels ,epochs = 40, batch_size=6,validation_data=(test_data,test_labels) ,verbose= 1,callbacks = [cb])
model.summary()
results = model.evaluate(test_data, test_labels)
print(results)
#saving the model
keras_file = "training.h5"
keras.models.save_model(model, keras_file)
Code-Ends
Describe the expected behavior
Create log files to run tensorboard. But gives error.
Error-Begins
InvalidArgumentError: Tensor embedding_6_input:0, specified in either feed_devices or fetch_devices was not found in the Graph
Error-Ends
	</description>
	<comments>
		<comment id='1' author='chandanchinta' date='2019-02-08T14:36:45Z'>
		I have had the same problem and I have solved it setting write_graph=False inside the callback creation, i.e.:
cb = keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=5, batch_size=256, write_graph=False, write_grads=True, write_images=False, embeddings_freq=5, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=train_data).
This should be because the different use of graphs in eager_mode (used by TF2).
Anyway, I'm interested in this behavior because now I don't have the problem anymore. Maybe I missed something in a recent update?
I would anyway ask to know how Keras works inside TF2, in particular how it manages the graphs.
Thank you.
		</comment>
		<comment id='2' author='chandanchinta' date='2019-02-08T21:27:29Z'>
		&lt;denchmark-link:https://github.com/chandanchinta&gt;@chandanchinta&lt;/denchmark-link&gt;
 Please try solution provided by &lt;denchmark-link:https://github.com/iLeW&gt;@iLeW&lt;/denchmark-link&gt;
, if the solution works then close this issue and open another issue for TF2.0. Thanks!
		</comment>
		<comment id='3' author='chandanchinta' date='2019-03-01T00:50:23Z'>
		Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
	</comments>
</bug>