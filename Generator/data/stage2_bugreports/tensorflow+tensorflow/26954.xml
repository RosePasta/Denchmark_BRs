<bug id='26954' author='ankitvgupta' open_date='2019-03-20T18:22:48Z' closed_time='2019-04-17T20:40:43Z'>
	<summary>Documentation Request: Transfer Learning in TF 2 with same and/or different final layer</summary>
	<description>
Please make sure that this is a documentation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template
System information

TensorFlow version: 2 alpha
Doc Link: N/A

Describe the documentation issue
Hello, I'm in the process of porting some of our existing TF 1.x based code to be ready for TF 2.0. This means we are trying to be focused around tf.keras, so I was hoping for an example of the best way to do transfer learning in tf.keras. Specifically, we have the following (I assume common) use case.

Train a model (say it's classification) with on dataset a, with 100 classes in the outputs.
Finetune the model on dataset b, using the same architecture, also with 100 classes.
Finetune the model on dataset c, which is the same except it has 50 classes, and thus the final layer has a different shape. So, we want to load all of the weights except the final layer,  and then train.

In TF 1.x, we used tf.estimator. For task 3, we would label the final layer with the name "final_layer", and then when warm-starting the models for finetuning, we would use the WarmstartSettings object to exclude weights that have final_layer in them. What is the equivalent for tf.keras in TF 2.x? Probably the cleanest way we could have the old functionality is if tf.keras.Model.load_weights had a regex field in the same form as WarmStartSettings, which only loaded a subset of variables.
Thanks!
We welcome contributions by users. Will you be able to update submit a PR (use the doc style guide) to fix the doc Issue?
I'm happy to write the docs if someone could give a high-level summary of how this is done. All the ways I've come up with are pretty jank, and I assume there is a better way to do this given how elegant it was in TF 1.X.
	</description>
	<comments>
		<comment id='1' author='ankitvgupta' date='2019-03-20T22:48:28Z'>
		The easiest would be to collect the common layers in a single Model and save and load weights on that. While training each configuration (50 / 100 classes) you can save_weights and load_weights on the slightly larger Model which includes the common Model, but to make the transfer I'd use the common Model.
Does that make sense? So three Models, one common and one which includes it for each task. It should just work with current saving APIs.
		</comment>
		<comment id='2' author='ankitvgupta' date='2019-03-20T23:03:28Z'>
		Thanks very much for the help!
To clarify, are you suggesting something like this:
common_model = tf.keras.Model(..)
model_50 = tf.Sequential([common_model, tf.keras.Dense(50)])
model_100 = tf.Sequential([common_model, tf.keras.Dense(100)])
then, for example, I could train model_100, and then save_weights on that.
When I want to train model_50, would first running model_50.load_weights on the exported weights from model_100 work? I would have guessed that would yell at me since the architectures are different. I'd like to use the SavedModel format for backwards compat with other stuff if possible.
		</comment>
		<comment id='3' author='ankitvgupta' date='2019-03-20T23:14:54Z'>
		Yep, those are the models I had in mind. Then if you use common_model.save_weights after training model_100, you can load it into a compatible common_model with common_model.load_weights when you want to train model_50. (Or if you can keep them both in the same program they'll naturally share weights.)
It's also possible to extract common_model's weights from model_100's checkpoint, by making a tf.Sequential([common_model]) and using load_weights on that. It'll ignore the weights from the missing final Dense, and then you can use common_model in model_50.
Not sure what you mean by the SavedModel comment. You can save in the TensorFlow checkpoint format from save_weights; that's the default as long as you don't use a .h5 suffix. You could also save a SavedModel for common_model and re-use that if you wanted.
		</comment>
		<comment id='4' author='ankitvgupta' date='2019-03-21T14:51:19Z'>
		Got it, that makes sense!
Is there any equivalent (or planned equivalent) to the regex-based selection system for loading weights as was the case in WarmstartSettings? That was very convenient since it let us train a model in 1 way, but then flexibly experiment with warmstarting different subsets of it.
		</comment>
		<comment id='5' author='ankitvgupta' date='2019-03-21T16:10:48Z'>
		No, I don't know of any plans for regex matching of variables. We don't use variable names to match checkpoints with variable objects in 2.x (except in Estimator), so it'd be a pretty weird API.
It's certainly possible to programatically filter weights like you describe, though. Just mirror the objects in the real model, attaching just the variable objects you want to load. There's some mention of this &lt;denchmark-link:https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/checkpoints.ipynb&gt;in the 2.x checkpoints guide&lt;/denchmark-link&gt;
. Without too much work you could wrap that in an API that takes regular expressions.
		</comment>
		<comment id='6' author='ankitvgupta' date='2019-03-21T16:57:18Z'>
		Got it, thanks.
OK, I like that idea. Could you just clarify what you mean by "mirror the objects in the real model"? My thinking is I have a model that's something like,
def build_model():
    return tf.keras.Sequential([
        Dense(10, input_shape=(50, ), activation="relu"), 
        Dense(5, name="final_layer")
    ])

model = build_model()
model.fit(...)
model.save_weights(..)
Now, when I want to load just the weights associated with the first dense layer in the model via some regex, here is my thinking of how to do it:
&lt;denchmark-code&gt;model = build_model()
model_mirror = build_model()
model_mirror.load_weights(..)

for variable, mirrored_variable in zip(model.variables, model_mirror.variables):
    if regex_matches(variable.name):
        variable.assign(mirrored_variable.value())
&lt;/denchmark-code&gt;

Does that match what you were thinking?
		</comment>
		<comment id='7' author='ankitvgupta' date='2019-03-21T17:22:39Z'>
		That's better than what I was thinking. Eventually you should be able to do a traversal over the object graph and re-construct it with modifications. The problem is we don't yet have a public API for iterating over checkpoint dependencies with their names.
If we did, you could do something like this:
import tensorflow as tf

model = tf.keras.Sequential(
    [tf.keras.layers.Dense(5),
     tf.keras.layers.Dense(6),
     tf.keras.layers.Dense(7)])
model(tf.constant([[1.]]))  # create variables

def visit_and_filter(obj, substitute, variable_predicate):
  for k, v in obj._checkpoint_dependencies:
    if isinstance(v, tf.Variable):
      if variable_predicate(v):
        setattr(substitute, k, v)
    elif isinstance(v, tf.keras.layers.Layer):
      v_substitute = tf.keras.Model()
      setattr(substitute, k, v_substitute)
      visit_and_filter(v, v_substitute, variable_predicate)

mirrored = tf.keras.Model()
visit_and_filter(model, mirrored, lambda v: "bias" in v.name)

print([w.name for w in model.weights])
print([w.name for w in mirrored.weights])
Which works, but uses a non-stable/private API (_checkpoint_dependencies). I've considered adding a stable API for this in the past, but haven't gotten something everyone was happy with. dict or vars() doesn't really work because Sequential tracks things without assigning them to attributes; changing that would be another fix.
This would also let you filter whole Layer or Model objects, which might be neat. And if you filtered Layer objects instead of Variables directly, you wouldn't even need the model() call to create variables and could instead use restore-on-create.
		</comment>
		<comment id='8' author='ankitvgupta' date='2019-03-21T17:58:33Z'>
		Got it, this makes a lot of sense. OK thanks so much for the help!
		</comment>
		<comment id='9' author='ankitvgupta' date='2019-04-10T17:53:00Z'>
		&lt;denchmark-link:https://github.com/ankitvgupta&gt;@ankitvgupta&lt;/denchmark-link&gt;
 Is this resolved? If yes, please close the issue. Thanks!
		</comment>
		<comment id='10' author='ankitvgupta' date='2019-04-17T20:40:43Z'>
		I think it was resolved. I am closing the issue. But, please let me know if I'm mistaken.
		</comment>
	</comments>
</bug>