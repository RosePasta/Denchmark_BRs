<bug id='5422' author='QuantumLiu' open_date='2016-11-06T03:09:43Z' closed_time='2016-11-27T21:09:20Z'>
	<summary>Why my TF is much slower than Theano?</summary>
	<description>
Using keras,TF cost 10s for each epoch,and Theano only cost 2s!
I'm using Ubuntu16.04 sever,GTX1060,CUDA8.0,cudnn5.1
~$ python
Python 2.7.12 (default, Jul  1 2016, 15:12:24)
[GCC 5.4.0 20160609] on linux2
Type "help", "copyright", "credits" or "license" for more information.



import tensorflow as tf
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally
tf
&lt;module 'tensorflow' from '/usr/local/lib/python2.7/dist-packages/tensorflow/init.pyc'&gt;



	</description>
	<comments>
		<comment id='1' author='QuantumLiu' date='2016-11-06T04:45:41Z'>
		Maybe the issue in Keras in that case?
		</comment>
		<comment id='2' author='QuantumLiu' date='2016-11-07T18:33:17Z'>
		&lt;denchmark-link:https://github.com/QuantumLiu&gt;@QuantumLiu&lt;/denchmark-link&gt;
, could you provide a reproducible full example. There are many possible performance pitfalls in using TensorFlow. A layer like Keras can only make those pitfalls even more plentiful. We definitely are interested in always improving our performance and helping users understand how to get great performance, but without any details, there is almost nothing we can do to help. Thanks.
		</comment>
		<comment id='3' author='QuantumLiu' date='2016-11-07T18:34:44Z'>
		&lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;
, ccing you for future posts.
		</comment>
		<comment id='4' author='QuantumLiu' date='2016-11-07T18:57:08Z'>
		I've opened an issue in the Keras repo where I experienced the same with a MNIST example.
&lt;denchmark-link:https://github.com/keras-team/keras/issues/4287&gt;keras-team/keras#4287&lt;/denchmark-link&gt;

It seems that using Keras + Theano without cnmem makes the performance just as "slow" as Keras + Tensorflow.
Additionally, the GPU Utilization is at 10-25% without cnmem in theano, and 20% in tensorflow all the time. With cnmem on, the utilization is around 80%, and becomes around 5 times faster per epoch.
I will try to reproduce this tomorrow without Keras and just Tensorflow.
		</comment>
		<comment id='5' author='QuantumLiu' date='2016-11-07T23:32:39Z'>
		Alright, please post when you have more information. Thanks!
		</comment>
		<comment id='6' author='QuantumLiu' date='2016-11-08T13:14:45Z'>
		MLP MNIST with native TF:
&lt;denchmark-code&gt;Mean epoch time: 1.82s
~20-45% gpu util
&lt;/denchmark-code&gt;

Keras with TF backend and native TF code:
&lt;denchmark-code&gt;Mean epoch time: 1.94s
~20-45% gpu util
&lt;/denchmark-code&gt;

Keras with TF backend and Keras sequential model:
&lt;denchmark-code&gt;Mean epoch time: 3.22s
~30% gpu util
&lt;/denchmark-code&gt;

Keras with Theano backend and Keras sequential model without cnmem:
&lt;denchmark-code&gt;Mean epoch time: 4.86s
~20% gpu util
&lt;/denchmark-code&gt;

Keras with Theano backend and Keras sequential model with cnmem 90%:
&lt;denchmark-code&gt;Mean epoch time: 1.45s
~80% gpu util
&lt;/denchmark-code&gt;

On a Tesla K20, using the same dataset (55000, 784)
Looks like Keras' sequential model is just slowing things down, and if you use Tensorflow's native code within Keras, the performance decrease is negligible. Also cnmem results in a huge performance boost.
		</comment>
		<comment id='7' author='QuantumLiu' date='2016-11-08T14:38:15Z'>
		&lt;denchmark-link:https://github.com/aselle&gt;@aselle&lt;/denchmark-link&gt;
  thank you!
`from keras.models import Model
from keras.layers import Dense, Dropout, Activation, Flatten, Input, Merge
from keras.layers import Convolution2D, MaxPooling2D,AveragePooling2D
from keras.optimizers import SGD
input=Input(shape=(1, 50, 125))
conv1=Convolution2D(32, 5, 5, border_mode='valid')(input)
pool1=MaxPooling2D(pool_size=(2, 2))(conv1)
relu1=Activation('relu')(pool1)
conv2=Convolution2D(32, 5, 5, border_mode='valid')(relu1)
pool2=AveragePooling2D(pool_size=(2, 2))(conv2)
relu2=Activation('relu')(pool2)
conv3=Convolution2D(32, 3, 3, border_mode='valid')(relu2)
pool3=AveragePooling2D(pool_size=(2, 2))(conv3)
relu3=Activation('relu')(pool3)
flatten=Flatten()(relu3)
fc1=Dense(512)(flatten)
fc21=Dense(23,activation='softmax')(fc1)
fc22=Dense(23,activation='softmax')(fc1)
fc23=Dense(23,activation='softmax')(fc1)
fc24=Dense(23,activation='softmax')(fc1)
#fc2=Merge(layers=[fc21,fc22,fc23,fc24],mode='concat')
fc2=Merge(mode='concat')([fc21,fc22,fc23,fc24])
OCRnet=Model(input=input,output=fc2)
sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)
OCRnet.compile(loss='mean_squared_error',optimizer=sgd)
import numpy as np
X_train=np.ones((10000,1,50,125),'float32')
Y_train=np.ones((10000,92),'float32')
OCRnet.fit(X_train, Y_train, batch_size=32, nb_epoch=100,validation_split=0.1,verbose=2)
`
		</comment>
		<comment id='8' author='QuantumLiu' date='2016-11-08T14:58:41Z'>
		&lt;denchmark-link:https://github.com/QuantumLiu&gt;@QuantumLiu&lt;/denchmark-link&gt;
 try defining your model using Tensorflow tensors as described in the first section here: &lt;denchmark-link:https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html&gt;https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html&lt;/denchmark-link&gt;

This is likely an issue with Keras, not Tensorflow
		</comment>
		<comment id='9' author='QuantumLiu' date='2016-11-10T02:18:42Z'>
		Please try this &lt;denchmark-link:https://github.com/QuantumLiu&gt;@QuantumLiu&lt;/denchmark-link&gt;
 , and let us know if this solves the issue.
		</comment>
		<comment id='10' author='QuantumLiu' date='2016-11-10T02:42:07Z'>
		&lt;denchmark-link:https://github.com/QuantumLiu&gt;@QuantumLiu&lt;/denchmark-link&gt;
 generally TensorFlow has been optimized for large models/large data, and it takes some work to make it efficient for tiny models/tiny data. In the case of MNIST, the issue the common source of bottlenecks is the data transfer between Python runtime and TensorFlow runtime. There are at least 2 copies incurred -- copying from Python to TensorFlow CPU, and then internally, copying data from CPU to GPU. Since these copies are blocking, things are not efficient, and you can improve things by pre-loading data in parallelÂ using queues/input pipelines. So the thing to check is how  is implemented in Keras and how efficient it is with data transfers. One caveat for pre-loading, it is inefficient for tiny datasets/models like MNIST because Python can not switch threads &lt;denchmark-link:http://stackoverflow.com/questions/39840323/benchmark-of-howto-reading-data/39842628#39842628&gt;fast enough&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='QuantumLiu' date='2016-11-27T21:09:19Z'>
		Closing due to lack of activity.
		</comment>
		<comment id='12' author='QuantumLiu' date='2017-04-19T13:06:33Z'>
		Same thing here. On my particular model with 35M parameters, keras with tf backend and sequential model takes around 33 s. per epoch. With theano backend and cumem at 80%, each epoch takes 16 s.
My images are also bigger than MNIST they're 80*80.
		</comment>
	</comments>
</bug>