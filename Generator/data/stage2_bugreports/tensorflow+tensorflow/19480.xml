<bug id='19480' author='Noahyt' open_date='2018-05-22T21:10:42Z' closed_time='2018-06-05T23:13:42Z'>
	<summary>Manually placing operations in eager execution raises FailedPreconditionError.</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
TensorFlow installed from (source or binary): Binary
TensorFlow version (use command below): tensorflow 1.6
Python version: python3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: V9.0.176
GPU model and memory:  NVIDIA Corporation GV100GL [Tesla V100 SXM2 16GB] (rev a1)
Exact command to reproduce:

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

I have a model that I believe was not automatically being placed onto an available GPU.
I then placed this part of the computation inside a with_device() block.  This schematically looks like:
&lt;denchmark-code&gt;# define some variables
with tf.device("/device:GPU:0"):
    with tfe.GradientTape(persistent=True) as tape:
       # calculate loss based on variables

&lt;/denchmark-code&gt;

The error is thrown during the loss calculation step.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

2018-05-22 13:57:11.963021: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-05-22 13:57:12.324280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties:
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:04:00.0
totalMemory: 15.78GiB freeMemory: 15.36GiB
2018-05-22 13:57:12.324584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0
2018-05-22 13:57:12.623548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14878 MB memory) -&gt; physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:04:00.0, compute capability: 7.0)
Traceback (most recent call last):
File "tf_registration_continuous.py", line 619, in 
cProfile.run('main()', file)
File "/share/software/user/open/python/3.6.1/lib/python3.6/cProfile.py", line 16, in run
return _pyprofile._Utils(Profile).run(statement, filename, sort)
File "/share/software/user/open/python/3.6.1/lib/python3.6/profile.py", line 55, in run
prof.run(statement)
File "/share/software/user/open/python/3.6.1/lib/python3.6/cProfile.py", line 95, in run
return self.runctx(cmd, dict, dict)
File "/share/software/user/open/python/3.6.1/lib/python3.6/cProfile.py", line 100, in runctx
exec(cmd, globals, locals)
File "", line 1, in 
File "tf_registration_continuous.py", line 615, in main
accuracy ,runtime, final_loss = run_registration(directory='output/' + hparams_run.name, hparams=hparams_run, dataset_path = dataset_path, save_figs=False)
File "tf_registration_continuous.py", line 525, in run_registration
rm.register(save_summaries = save_figs, make_animation=save_figs)
File "tf_registration_continuous.py", line 190, in register
num_images_to_optimize_params= self.hparams.num_images_to_optimize_params
File "tf_registration_continuous.py", line 105, in single_registration_step
_ = self.eif.warp(scale, num_images_for_loss_calculation)
File "/home/groups/bmacint/Ultrasound/timing/elastic_image_field.py", line 118, in warp
field_image.initialize_translation()
File "/home/groups/bmacint/Ultrasound/timing/field_image.py", line 87, in initialize_translation
self.translation_warp_points = tf.tile(self.translation[tf.newaxis, tf.newaxis, :],
File "/share/software/user/open/py-tensorflow/1.6.0_py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py", line 828, in _SliceHelperVar
return _slice_helper(var._AsTensor(), slice_spec, var)
File "/share/software/user/open/py-tensorflow/1.6.0_py36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py", line 741, in _AsTensor
return self.value()
File "/share/software/user/open/py-tensorflow/1.6.0_py36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py", line 572, in value
return self._read_variable_op()
File "/share/software/user/open/py-tensorflow/1.6.0_py36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py", line 655, in _read_variable_op
self._dtype)
File "/share/software/user/open/py-tensorflow/1.6.0_py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py", line 304, in read_variable_op
attrs=_attrs, ctx=_ctx, name=name)
File "/share/software/user/open/py-tensorflow/1.6.0_py36/lib/python3.6/site-packages/tensorflow/python/eager/execute.py", line 66, in quick_execute
six.raise_from(core._status_to_exception(e.code, message), None)
File "", line 2, in raise_from
tensorflow.python.framework.errors_impl.FailedPreconditionError: Error while reading resource variable 152 from Container: eager-execution-0/. This could mean that the variable was uninitialized. Invalid argument: Trying to access resource located in device /job:localhost/replica:0/task:0/device:CPU:0 from device /job:localhost/replica:0/task:0/device:GPU:0 [Op:ReadVariableOp]
	</description>
	<comments>
		<comment id='1' author='Noahyt' date='2018-05-22T22:32:21Z'>
		I have been able to reproduce this issue with a simple minimal example:
&lt;denchmark-code&gt;import tensorflow as tf

tfe = tf.contrib.eager
tfe.enable_eager_execution(config=tf.ConfigProto(allow_soft_placement=True,
                                        log_device_placement=True), device_policy=tfe.DEVICE_PLACEMENT_WARN)

a = tfe.Variable([2.,3.])

with tf.device("/device:CPU:0"):
    print(a)
    print(a.device)
with tf.device("/device:GPU:0"):
    print(a)
    print(a.device)
&lt;/denchmark-code&gt;

The error is similar to the one quoted above:
2018-05-22 15:31:14.700544: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-05-22 15:31:14.995858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties:
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:84:00.0
totalMemory: 11.92GiB freeMemory: 11.85GiB
2018-05-22 15:31:14.996414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0
2018-05-22 15:31:15.471576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11492 MB memory) -&gt; physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:84:00.0, compute capability: 3.7)
Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -&gt; device: 0, name: Tesla K80, pci bus id: 0000:84:00.0, compute capability: 3.7
2018-05-22 15:31:15.717080: I tensorflow/core/common_runtime/direct_session.cc:297] Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -&gt; device: 0, name: Tesla K80, pci bus id: 0000:84:00.0, compute capability: 3.7
&lt;tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([ 2.,  3.], dtype=float32)&gt;
Traceback (most recent call last):
File "minimal_failing_case.py", line 16, in 
print(a)
File "/share/software/user/open/py-tensorflow/1.6.0_py36/lib/python3.6/site-packages/tensorflow/python/ops/variables.py", line 239, in repr
ops.numpy_text(self.read_value(), is_repr=True))
File "/share/software/user/open/py-tensorflow/1.6.0_py36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py", line 677, in read_value
value = self._read_variable_op()
File "/share/software/user/open/py-tensorflow/1.6.0_py36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py", line 655, in _read_variable_op
self._dtype)
File "/share/software/user/open/py-tensorflow/1.6.0_py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py", line 304, in read_variable_op
attrs=_attrs, ctx=_ctx, name=name)
File "/share/software/user/open/py-tensorflow/1.6.0_py36/lib/python3.6/site-packages/tensorflow/python/eager/execute.py", line 66, in quick_execute
six.raise_from(core._status_to_exception(e.code, message), None)
File "", line 2, in raise_from
tensorflow.python.framework.errors_impl.FailedPreconditionError: Error while reading resource variable 2 from Container: eager-execution-0/. This could mean that the variable was uninitialized. Invalid argument: Trying to access resource located in device /job:localhost/replica:0/task:0/device:CPU:0 from device /job:localhost/replica:0/task:0/device:GPU:0 [Op:ReadVariableOp]
		</comment>
		<comment id='2' author='Noahyt' date='2018-06-04T17:52:15Z'>
		&lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
, PTAL.
Thanks!
		</comment>
		<comment id='3' author='Noahyt' date='2018-06-04T18:21:04Z'>
		&lt;denchmark-link:https://github.com/akshaym&gt;@akshaym&lt;/denchmark-link&gt;
 can you take a look at this? Something which is supposed to force-colocate the read variable op with the handle is not force-colocating it. The colocation code is in 
 but some condition must be missed.
		</comment>
		<comment id='4' author='Noahyt' date='2018-06-05T23:13:42Z'>
		Hi &lt;denchmark-link:https://github.com/Noahyt&gt;@Noahyt&lt;/denchmark-link&gt;
,
Thanks for the report. I'm able to reproduce this on 1.6.0 but unable to reproduce on the latest version of tensorflow (1.8), so its been fixed already. It'd be great if you could upgrade your installation of tensorflow!
Thanks!
		</comment>
		<comment id='5' author='Noahyt' date='2019-03-01T05:40:03Z'>
		I'm noticing a simiilar problem with TF 2.0 nightly (2.0.0-dev20190228) which enforces Eager mode.
The following minimal code:
import tensorflow as tf
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense
from tensorflow.keras.layers import Conv2D


(X_train, y_train), _ = cifar10.load_data()
X_train = X_train.astype('float32') / 255.0


with tf.device('cpu:0'):
    model = Sequential([
        Conv2D(32, (3, 3), input_shape=(32, 32, 3)),
        Flatten(),
        Dense(10, activation='softmax')
    ])
    
    model.compile(loss='sparse_categorical_crossentropy',
                  optimizer='rmsprop',
                  metrics=['accuracy'])

    
model.fit(X_train, y_train,
          batch_size=1024,
          epochs=2)
produces error:
---------------------------------------------------------------------------
FailedPreconditionError                   Traceback (most recent call last)
&lt;ipython-input-1-d00bb2a707a4&gt; in &lt;module&gt;
     24 model.fit(X_train, y_train,
     25           batch_size=1024,
---&gt; 26           epochs=2)

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    871           validation_steps=validation_steps,
    872           validation_freq=validation_freq,
--&gt; 873           steps_name='steps_per_epoch')
    874 
    875   def evaluate(self,

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)
    349 
    350         # Get outputs.
--&gt; 351         batch_outs = f(ins_batch)
    352         if not isinstance(batch_outs, list):
    353           batch_outs = [batch_outs]

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in __call__(self, inputs)
   3215         value = math_ops.cast(value, tensor.dtype)
   3216       converted_inputs.append(value)
-&gt; 3217     outputs = self._graph_fn(*converted_inputs)
   3218     return nest.pack_sequence_as(self._outputs_structure,
   3219                                  [x.numpy() for x in outputs])

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
    523       raise TypeError("Keyword arguments {} unknown. Expected {}.".format(
    524           list(kwargs.keys()), list(self._arg_keywords)))
--&gt; 525     return self._call_flat(args)
    526 
    527   def _filtered_call(self, args, kwargs):

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args)
    592     # Only need to override the gradient in graph mode and when we have outputs.
    593     if context.executing_eagerly() or not self.outputs:
--&gt; 594       outputs = self._inference_function.call(ctx, args)
    595     else:
    596       self._register_gradient()

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args)
    380             attrs=("executor_type", executor_type,
    381                    "config_proto", config),
--&gt; 382             ctx=ctx)
    383       # Replace empty list with None
    384       outputs = outputs or None

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     64     else:
     65       message = e.message
---&gt; 66     six.raise_from(core._status_to_exception(e.code, message), None)
     67   except TypeError as e:
     68     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):

~/miniconda3/envs/tf2/lib/python3.7/site-packages/six.py in raise_from(value, from_value)

FailedPreconditionError: Error while reading resource variable _AnonymousVar15 from Container: localhost. This could mean that the variable was uninitialized. Invalid argument: Trying to access resource _AnonymousVar15 located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0
	 [[{{node training/RMSprop/RMSprop/update_3/mul/ReadVariableOp}}]] [Op:__inference_keras_scratch_graph_646]
I've been able to fix this in 3 ways:

use tensorflow.compat.v1.disable_eager_execution()
remove the Conv2D layer
remove the batch_size and epochs arguments from the .fit call

the behavior seems weird, can anyone explain what's going on?
		</comment>
		<comment id='6' author='Noahyt' date='2019-03-01T06:07:10Z'>
		I've also opened a separate issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/26244&gt;#26244&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>