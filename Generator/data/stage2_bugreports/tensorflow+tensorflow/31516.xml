<bug id='31516' author='zwenju' open_date='2019-08-11T04:52:00Z' closed_time='2019-08-16T17:44:05Z'>
	<summary>tensorflow 2.0 tensorflow.python.eager.function.TfMethodTarget object at could not be transformed and will be executed as-is.</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
WARNING: Entity &lt;bound method TensorFlowOpLayer._defun_call of &lt;tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5FA485A58&gt;&gt; could not be transformed and will be executed as-is.
Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, export AUTOGRAPH_VERBOSITY=10) and attach the full output. Cause: converting &lt;bound method TensorFlowOpLayer._defun_call of &lt;tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5FA485A58&gt;&gt;: AssertionError: W0811 12:49:24.207197  2284 ag_logging.py:145] Entity &lt;bound method TensorFlowOpLayer._defun_call of &lt;tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5FA4A82B0&gt;&gt; could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, export AUTOGRAPH_VERBOSITY=10) and attach the full output. Cause: converting &lt;bound method TensorFlowOpLayer._defun_call of &lt;tensorflow.python.eager.function.TfMethodTarget object at 0x000001D5FA4A82B0&gt;&gt;: AssertionError:
System information
Windows 10 , Anaconda, Python 3.7  Tensorflow 2.0b1
You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
Describe the expected behavior
Code to reproduce the issue
&lt;denchmark-code&gt;import time
import math
import tensorflow as tf
import numpy as np
import tensorflow as keras
from tensorflow.keras import layers
from tensorflow import keras
from tensorflow.keras.utils import plot_model
from scipy . stats import multivariate_normal as normal

d = 10
T = 0.1
n_time = 5
n_sample = 100
batch_size = 100
n_maxstep = 400
h = (T + 0.0) / n_time
t_stamp = np.arange (0, n_time) * h

def f_tf (t, X, Y, Z):
    V =  Y - tf.math.sin (Y)
    return V

def g_tf (t, X):
    V =  tf.math.reduce_sum (X**3, 1,keepdims=True)
    return V

def k_tf ( n_sample ):
    W = np.zeros ([ n_sample, d, n_time  ], dtype = np.float64)
    X_sample  = np.zeros ([ n_sample, d, n_time+1], dtype = np.float64)
    for i in range (n_time):
        W [:, :, i  ] = np.reshape ( normal.rvs ( mean = np.zeros(d,dtype = np.float64),\
                                      cov =1, size = n_sample ), ( n_sample, d))
        X_sample  [:, :, i+1] =  W [:, :, i]
    return W, X_sample

def nn_tf(x):
    x = keras.layers.BatchNormalization(batch_size = n_sample)(x)
    x = keras.layers.Dense(d, batch_size = n_sample)(x)
    x = keras.layers.BatchNormalization(batch_size = n_sample)(x)
    return x

dW = keras.Input(shape = (d, n_time  ), batch_size=n_sample, dtype = tf.float64, name = 'dW')
XX = keras.Input(shape = (d, n_time+1), batch_size=n_sample, dtype = tf.float64, name = 'X' )
X = XX
Y = tf.zeros([n_sample, 1], dtype = tf.float64)
Z = tf.zeros([n_sample, d], dtype = tf.float64)

for it in range(n_time-1):
    with tf.name_scope(str(it+1)):
        Y = Y +  tf.math.reduce_sum( Z * dW[:,:,it],  1, keepdims=True)
        subX = tf.reshape(X[:,:,it], shape = [n_sample, d])
        Z = nn_tf(subX) / d

Y = Y + tf.math.reduce_sum (Z * dW [:, :, n_time-1], 1, keepdims=True)
model = keras.Model(inputs=[XX,dW], outputs=[Y])

optimizer = keras.optimizers.Adam(learning_rate=1e-3)
train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')
dW_train, X_train = k_tf ( n_sample )

for epoch in range (10):
    with tf.GradientTape() as tape:
        predictions = model( [X_train, dW_train] )
        label = g_tf (T, X_train[:, :, n_time])
        loss_value = tf.reduce_sum( tf.keras.losses.MSE (label, predictions ) )
    grads = tape.gradient(loss_value,  model.trainable_variables)
    optimizer.apply_gradients( zip(grads, model.trainable_variables) )
    accuracy = train_accuracy(label, predictions)
    print("step ", epoch, "loss = ", loss_value.numpy(), "accuracy = ", accuracy.numpy())

&lt;/denchmark-code&gt;

Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='zwenju' date='2019-08-13T09:11:37Z'>
		&lt;denchmark-link:https://github.com/zwenju&gt;@zwenju&lt;/denchmark-link&gt;
 ,
I was unable execute the give code, can you please provide the code with proper indentation?Thanks
		</comment>
		<comment id='2' author='zwenju' date='2019-08-13T09:40:33Z'>
		Thank you,  the new code is provided
		</comment>
		<comment id='3' author='zwenju' date='2019-08-14T09:41:38Z'>
		&lt;denchmark-link:https://github.com/zwenju&gt;@zwenju&lt;/denchmark-link&gt;
 ,
Thank you for the updated code,when tried executing the same I got the output as per the screenshot
&lt;denchmark-link:https://user-images.githubusercontent.com/52397990/63011334-bcec6100-bea5-11e9-967e-2f2f8ef832dd.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='zwenju' date='2019-08-14T11:38:30Z'>
		Please run this code twice continuously,   I find the first run no errors,    the second run will show the errors.
		</comment>
		<comment id='5' author='zwenju' date='2019-08-16T10:59:52Z'>
		I tried executing the given code in both colab and Jupyter notebook,couldn't replicate the issue.
		</comment>
		<comment id='6' author='zwenju' date='2019-08-16T12:38:46Z'>
		Thank youï¼Œmay be it a bug of Spyder?
I am not sure why I got this error.
thanks again.
		</comment>
		<comment id='7' author='zwenju' date='2019-08-16T17:44:05Z'>
		&lt;denchmark-link:https://github.com/zwenju&gt;@zwenju&lt;/denchmark-link&gt;
 I think there are issues with  in your code specifically related to . In , if you don't set , it will use default dtype which is . Most of your code user  except in the  layer.
You can remove the errors in two ways

use dtype=tf.float64 in the keras.layers.BatchNormalization layer.
If you think you want set default dtype as tf.float64 then add tf.keras.backend.set_floatx('float64') at the top of the program as shown in the code below.

&lt;denchmark-code&gt;import time
import math
import tensorflow as tf
import numpy as np
import tensorflow as keras
from tensorflow.keras import layers
from tensorflow import keras
from tensorflow.keras.utils import plot_model
from scipy . stats import multivariate_normal as normal
# tf.keras.backend.set_floatx('float64') # sets dtype as tf.float64

d = 10
T = 0.1
n_time = 5
n_sample = 100
batch_size = 100
n_maxstep = 400
h = (T + 0.0) / n_time
t_stamp = np.arange (0, n_time) * h

def f_tf (t, X, Y, Z):
    V =  Y - tf.math.sin (Y)
    return V

def g_tf (t, X):
    V =  tf.math.reduce_sum (X**3, 1,keepdims=True)
    return V

def k_tf ( n_sample ):
    W = np.zeros ([ n_sample, d, n_time  ], dtype = np.float64)
    X_sample  = np.zeros ([ n_sample, d, n_time+1], dtype = np.float64)
    for i in range (n_time):
        W [:, :, i  ] = np.reshape ( normal.rvs ( mean = np.zeros(d,dtype = np.float64),\
                                      cov =1, size = n_sample ), ( n_sample, d))
        X_sample  [:, :, i+1] =  W [:, :, i]
    return W, X_sample

def nn_tf(x):
    x = keras.layers.BatchNormalization(batch_size = n_sample,dtype=tf.float64)(x)
    x = keras.layers.Dense(d, batch_size = n_sample)(x)
    x = keras.layers.BatchNormalization(batch_size = n_sample,dtype=tf.float64)(x)
    return x

dW = keras.Input(shape = (d, n_time  ), batch_size=n_sample, dtype = tf.float64, name = 'dW')
XX = keras.Input(shape = (d, n_time+1), batch_size=n_sample, dtype = tf.float64, name = 'X' )
X = XX
Y = tf.zeros([n_sample, 1], dtype = tf.float64)
Z = tf.zeros([n_sample, d], dtype = tf.float64)

for it in range(n_time-1):
    with tf.name_scope(str(it+1)):
        Y = Y +  tf.math.reduce_sum( Z * dW[:,:,it],  1, keepdims=True)
        subX = tf.reshape(X[:,:,it], shape = [n_sample, d])
        Z = nn_tf(subX) / d

Y = Y + tf.math.reduce_sum (Z * dW [:, :, n_time-1], 1, keepdims=True)
model = keras.Model(inputs=[XX,dW], outputs=[Y])

optimizer = keras.optimizers.Adam(learning_rate=1e-3)
train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')
dW_train, X_train = k_tf ( n_sample )

for epoch in range (10):
    with tf.GradientTape() as tape:
        predictions = model( [X_train, dW_train] )
        label = g_tf (T, X_train[:, :, n_time])
        loss_value = tf.reduce_sum( tf.keras.losses.MSE (label, predictions ) )
    grads = tape.gradient(loss_value,  model.trainable_variables)
    optimizer.apply_gradients( zip(grads, model.trainable_variables) )
    accuracy = train_accuracy(label, predictions)
    print("step ", epoch, "loss = ", loss_value.numpy(), "accuracy = ", accuracy.numpy())
&lt;/denchmark-code&gt;

I am closing this issue as it was resolved. Feel free to reopen if the issue persists again. &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/31720e5d1d44dcb9ffeaa84c98301307/tf_31516_batchnormalization.ipynb&gt;Here&lt;/denchmark-link&gt;
 is the gist for your reference. Thanks!
		</comment>
		<comment id='8' author='zwenju' date='2019-08-16T17:44:07Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=31516&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=31516&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='zwenju' date='2019-08-17T08:14:26Z'>
		Thanks.
		</comment>
	</comments>
</bug>