<bug id='35524' author='farotem' open_date='2020-01-01T17:14:31Z' closed_time='2020-06-19T03:52:54Z'>
	<summary>Suspected memory leak - when loading multiple models with tf.keras.models.load_model()</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
NA
TensorFlow installed from (source or binary): binary wheel via PyPI
TensorFlow version (use command below):
2.1.0-dev20191231 (v1.12.1-21412-g3a094e6 2.1.0-dev20191231)
Python version:
Bazel version (if compiling from source):
NA
GCC/Compiler version (if compiling from source):
NA
CUDA/cuDNN version:
CUDA 10.0
GPU model and memory:
V100 32 GB
You can collect some of this information using our environment capture
script
You can also obtain the TensorFlow version with: 1. TF 1.0: python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)" 2. TF 2.0: python -c "import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"

Describe the current behavior
I'm suspecting a CPU memory leak when loading multiple models.
When im running infinite loop that keeps loading the same model while using the same variable the memory (private bytes and working set) of the process keep increasing. At some points the working set seems to free some memory, but the trend is that the memory keeps on rising.
I used a simple model (attached).
This trend happens even though I call gc.collect() on every iteration and tf.keras.backend.clear_session().
the issue also happens in TF 2.0 (v2.0.0-rc2-26-g64c3d38 2.0.0).
for a specific model:
running in TF 2.0 each iteration adds 16 MiB
running in TF 2.1 each iteration adds 2 MiB
Describe the expected behavior
The memory shouldnt increase on each interation
Code to reproduce the issue
`
&lt;denchmark-code&gt;import os
import tensorflow as tf
import gc # garbage collector
import objgraph
from memory_profiler import profile

def mem_stat():
  objs = gc.get_objects()
  print("total objects count", len(objs))

@profile
def profile_own_model():
    model = tf.keras.models.Sequential([
        tf.keras.layers.Flatten(input_shape=(28, 28)),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    # model.save('my_model')
    tf.keras.backend.clear_session()
    del model
    gc.collect()

@profile
def profile_load_model(path):
    model = tf.keras.models.load_model(model_path, compile=False)
    tf.keras.backend.clear_session()
    del model
    gc.collect()



model_path = f'/my_model.hd5'
print("load model in loops:")

c = 1
while True:
    print("----------- iter", c)
    profile_load_model(model_path)

    print("mem stat after model creation:")
    mem_stat()
    objgraph.show_growth(limit=30)
    c += 1
&lt;/denchmark-code&gt;

`

&lt;denchmark-link:https://user-images.githubusercontent.com/27951762/71644038-d9f5c500-2cca-11ea-96e3-b8aedd2e4efb.png&gt;&lt;/denchmark-link&gt;

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='farotem' date='2020-01-03T10:27:27Z'>
		&lt;denchmark-link:https://github.com/farotem&gt;@farotem&lt;/denchmark-link&gt;
, I tried reproducing the issue with Tf 2.1.but got different error message. Please find the &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/71946c07f0ba1906fc249029847f4259/untitled335.ipynb&gt;gist&lt;/denchmark-link&gt;
 and confirm. Thanks!
		</comment>
		<comment id='2' author='farotem' date='2020-01-03T11:03:20Z'>
		&lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
 hi I looked at the gist,
it looks like the profiler can't work under Ipython environment, you can look at the memory with your own tools.
to run the code just comment the memory_profiler part:
&lt;denchmark-code&gt;import os
import tensorflow as tf
import gc # garbage collector
import objgraph
#from memory_profiler import profile

&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;def mem_stat():
    objs = gc.get_objects()
    print("total objects count", len(objs))

#@profile
def profile_own_model():
    model = tf.keras.models.Sequential([
        tf.keras.layers.Flatten(input_shape=(28, 28)),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    # model.save('my_model')
    tf.keras.backend.clear_session()
    del model
    gc.collect()

#@profile
def profile_load_model(path):
    model = tf.keras.models.load_model(model_path, compile=False)
    tf.keras.backend.clear_session()
    del model
    gc.collect()

model_path = f'/my_model.hd5'
print("load model in loops:")

c = 1
while True:
    print("----------- iter", c)
    profile_load_model(model_path)

    print("mem stat after model creation:")
    mem_stat()
    objgraph.show_growth(limit=30)
    c += 1

&lt;/denchmark-code&gt;

`
		</comment>
		<comment id='3' author='farotem' date='2020-01-07T10:50:00Z'>
		Installing tf-nightly-gpu with "pip install tf-nightly-gpu" fixed the issue for me
		</comment>
		<comment id='4' author='farotem' date='2020-01-09T05:38:58Z'>
		&lt;denchmark-link:https://github.com/farotem&gt;@farotem&lt;/denchmark-link&gt;
, I tried replicating the issue with commenting profile, looks like issue is related my_model file. Please find the &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/4fcd1d5ae63b0f72dcc65076e10aad15/untitled338.ipynb?authuser=1&gt;gist&lt;/denchmark-link&gt;
. Can you try &lt;denchmark-link:https://github.com/retviews&gt;@retviews&lt;/denchmark-link&gt;
's suggestion and let us know how it progresses. Thanks!
		</comment>
		<comment id='5' author='farotem' date='2020-01-09T14:45:50Z'>
		Hi I tried installing tf-nightly-gpu, version: 2.1.0.dev20200107
still receives memory leaks in the CPU.
note 1:
the number of python objects doesn't seem to be the cause for the leak, after few iterations the number of python objects do not increase but the memory usage of the CPU is still increasing.
note 2:
I tried loading the same model from json and there are no memory leaks.
I think it means the memory leaks is somewhere in deserializing the model from folder.
I looked at the gist, below is the fixed code: (changed the path of file)
&lt;denchmark-code&gt;import os
import tensorflow as tf
import gc # garbage collector

def build_and_save_own_model():
    model = tf.keras.models.Sequential([
        tf.keras.layers.Flatten(input_shape=(28, 28)),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    model.save('my_model')
    tf.keras.backend.clear_session()
    del model
    gc.collect()

def profile_load_model(path):
    model = tf.keras.models.load_model(model_path, compile=False)
    tf.keras.backend.clear_session()
    del model
    gc.collect()

model_path = 'my_model'
build_and_save_own_model()
print("load model in loops:")
c = 1
while True:
    print("----------- iter", c)
    profile_load_model(model_path)
    c += 1
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='farotem' date='2020-01-21T11:12:35Z'>
		Issue is replicating on colab.
Please take a look at &lt;denchmark-link:https://colab.research.google.com/drive/1TLk3pOxde4KERIBOEHE96hlfgNZOZA67?usp=sharing&gt;gist&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='7' author='farotem' date='2020-06-05T02:26:39Z'>
		Is this still an issue with latest version of tensorflow (2.2)?
		</comment>
		<comment id='8' author='farotem' date='2020-06-12T03:09:08Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='9' author='farotem' date='2020-06-19T03:52:53Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='10' author='farotem' date='2020-06-19T03:52:55Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35524&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35524&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='farotem' date='2020-08-22T11:55:36Z'>
		I'm having this same issue - any fix in the roadmap?
		</comment>
		<comment id='12' author='farotem' date='2020-12-01T05:05:30Z'>
		I seem to be having this same issue on TF 2.3.
		</comment>
		<comment id='13' author='farotem' date='2020-12-02T09:23:07Z'>
		tested on TF 2.3 same issue..
		</comment>
		<comment id='14' author='farotem' date='2020-12-14T23:06:58Z'>
		This seems to be reduced with the latest tf-nightly, tested with &lt;denchmark-link:https://colab.research.google.com/gist/idfah/dff83de8d2a6406c9b92221e6282a8d6/tf2_load_mem_leak.ipynb#scrollTo=5IhHO-wynZKC&gt;this colab notebook&lt;/denchmark-link&gt;
.
See also &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/40171&gt;#40171&lt;/denchmark-link&gt;
 for more discussion.
		</comment>
		<comment id='15' author='farotem' date='2020-12-14T23:17:57Z'>
		It is reduced but not fixed sadly its still an issue for our team
I opened a new bug for nightly version (tf2. 4)
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/45453#issuecomment-740755898&gt;#45453 (comment)&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>