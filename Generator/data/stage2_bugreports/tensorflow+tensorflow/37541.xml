<bug id='37541' author='gthb' open_date='2020-03-12T14:53:46Z' closed_time='2020-03-18T06:09:54Z'>
	<summary>load_model(filename) fails on weight ordering if sublayer .trainable is modified after init</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS, Darwin-19.3.0-x86_64-i386-64bit, mac version: ('10.15.3', ('', '', ''), 'x86_64')
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): v1.12.1-26991-g5a6ae102f0 2.2.0-dev20200311
Python version: 3.7.6

Describe the current behavior
Creating a layer and then setting layer.trainable = False (after creation) and then creating a model using that layer and saving that model with model.save(filename), and then loading that model with tf.keras.models.load_model(filename, custom_objects=...), fails because weights ordering for the layer is inconsistent.
Describe the expected behavior
I expect to be able to load a model I saved, despite having changed the trainable attribute on some layer before compiling and saving. (The trainable attribute is documented, with no warning that it must not be mutated, nor is there any runtime warning about this.)
Standalone code to reproduce the issue
import tensorflow as tf
from tensorflow import keras


class LayerWithSublayers(keras.layers.Layer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.embedding = keras.layers.Embedding(3, 2)
        self.dense = keras.layers.Dense(4)

    def call(self, inputs, **kwargs):
        return self.dense(self.embedding(inputs))


input_ids = keras.Input(shape=(4,), dtype=tf.int32, name='input_ids')
output_layer = LayerWithSublayers()
output_layer.embedding.trainable = False
output = output_layer(input_ids)

model = keras.Model(inputs=[input_ids], outputs=[output])
model.compile(loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])

model_file_name = 'foo.h5'
model.save(filepath=model_file_name)

loaded_model = keras.models.load_model(model_file_name, custom_objects={'LayerWithSublayers': LayerWithSublayers})
If I change this to pass trainable=False when creating the keras.layers.Embedding, then the model loads just fine. So the problem is that the change of trainable is not reflected when the model is serialized to HDF5 format.
Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
Running the above script outputs this traceback:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "keras_save_load_h5_nontrainable.py", line 26, in &lt;module&gt;
    loaded_model = keras.models.load_model(model_file_name, custom_objects={'LayerWithSublayers': LayerWithSublayers})
  File "/Users/gbr/.pyenv/versions/NLNIGHTLY/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py", line 184, in load_model
    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)
  File "/Users/gbr/.pyenv/versions/NLNIGHTLY/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py", line 173, in load_model_from_hdf5
    load_weights_from_hdf5_group(f['model_weights'], model.layers)
  File "/Users/gbr/.pyenv/versions/NLNIGHTLY/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py", line 704, in load_weights_from_hdf5_group
    K.batch_set_value(weight_value_tuples)
  File "/Users/gbr/.pyenv/versions/NLNIGHTLY/lib/python3.7/site-packages/tensorflow/python/keras/backend.py", line 3402, in batch_set_value
    x.assign(np.asarray(value, dtype=dtype(x)))
  File "/Users/gbr/.pyenv/versions/NLNIGHTLY/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py", line 842, in assign
    self._shape.assert_is_compatible_with(value_tensor.shape)
  File "/Users/gbr/.pyenv/versions/NLNIGHTLY/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py", line 1117, in assert_is_compatible_with
    raise ValueError("Shapes %s and %s are incompatible" % (self, other))
ValueError: Shapes (3, 2) and (2, 4) are incompatible
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='gthb' date='2020-03-12T15:54:24Z'>
		when you are passing trainable = True  its works fine with .h5 and when I am changing .h5 with .tf it also load model with trainable = False but model loaded is with trainable = True
 import tensorflow as tf
from tensorflow import keras


class LayerWithSublayers(keras.layers.Layer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.embedding = keras.layers.Embedding(3, 2)
        self.dense = keras.layers.Dense(4)

    def call(self, inputs, **kwargs):
        return self.dense(self.embedding(inputs))


input_ids = keras.Input(shape=(4,), dtype=tf.int32, name='input_ids')
output_layer = LayerWithSublayers()
output_layer.embedding.trainable = False
output = output_layer(input_ids)

model = keras.Model(inputs=[input_ids], outputs=[output])
model.compile(loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])

model_file_name = 'foo.tf'

model.save(filepath=model_file_name)

loaded_model = keras.models.load_model(model_file_name, custom_objects={'LayerWithSublayers': LayerWithSublayers})
Output :

Total params: 18
Trainable params: 18
Non-trainable params: 0

but in actual with trainable = False model summary be like :

Total params: 18
Trainable params: 12
Non-trainable params: 6

		</comment>
		<comment id='2' author='gthb' date='2020-03-13T10:15:32Z'>
		&lt;denchmark-link:https://github.com/gthb&gt;@gthb&lt;/denchmark-link&gt;

I tried to reproduce the issue in colab with TF nightly version.Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/dcc1aa41b6575cee41691418d02d3d90/untitled728.ipynb&gt;here&lt;/denchmark-link&gt;
.Is this the expected behavior? Thanks!
		</comment>
		<comment id='3' author='gthb' date='2020-03-13T10:26:50Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
 Thanks for checking it, but when I save mode as instead of  I am able to load mode with  but loaded model has params same as with `trainable = True. So is model always loaded with trainable = True ?
		</comment>
		<comment id='4' author='gthb' date='2020-03-13T11:14:19Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;


I tried to reproduce the issue in colab with TF nightly version.Please, find the gist here.Is this the expected behavior? Thanks!

This is the behavior I'm reporting as a bug — specifically the result in &lt;denchmark-link:https://colab.research.google.com/gist/ravikyram/dcc1aa41b6575cee41691418d02d3d90/untitled728.ipynb#scrollTo=3hk5lvUWNydJ&amp;line=1&amp;uniqifier=1&gt;this cell&lt;/denchmark-link&gt;
, . This is caused by trying to assign a value to the wrong variable. That in turn is because the variables are assigned in a single batch operation with the assumption that their order matches the order of variable values in the serialized file, and that assumption is undoubtedly failing because variables are ordered trainable first, then untrainable, and the  information is not correctly recorded in the serialized file if the  attribute is assigned after layer creation.
		</comment>
		<comment id='5' author='gthb' date='2020-03-13T11:17:03Z'>
		The expected behavior is for the serialized file to get deserialized correctly on load_model, yielding a model identical to the one I saved.
(Or alternatively, for the .trainable attribute to be a read-only property, or output a warning when assigned to, or at least be documented as not safe to assign to.)
		</comment>
		<comment id='6' author='gthb' date='2020-03-18T06:09:54Z'>
		Restoring the trainable value can be tricky.
Say that LayerWithSublayers defines the trainable status of the embedding layer in the class definition, and is saved with the embedding.trainable set to True:
&lt;denchmark-code&gt;class LayerWithSublayers(keras.layers.Layer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.embedding = keras.layers.Embedding(3, 2)
        self.embedding.trainable = True
&lt;/denchmark-code&gt;

Then the definition of the class is changed so that embedding.trainable is False:
&lt;denchmark-code&gt;class LayerWithSublayers(keras.layers.Layer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.embedding = keras.layers.Embedding(3, 2)
        self.embedding.trainable = False
&lt;/denchmark-code&gt;

When loading the model with the new definition, should the embedding layer retain it's original trainable status, or should it use the trainable status set in the LayerWithSublayers class?
The current behavior is that we will load whatever is returned by  the layer's get_config/from_config (i.e. the latter of the two options). If you want LayerWithSublayers to retain the trainable status of one of its internal layers, then consider adding that to the layer's get_config method.
e.g.
&lt;denchmark-code&gt;class LayerWithSublayers(keras.layers.Layer):
    def __init__(self, train_embeddings=True, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.embedding = keras.layers.Embedding(3, 2)
        self.embedding.trainable = train_embeddings
    def get_config(self):
        config = super().get_config()
        config['train_embeddings'] = self.embedding.trainable
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='gthb' date='2020-03-18T06:09:56Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37541&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37541&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='gthb' date='2020-03-27T12:41:12Z'>
		
Restoring the trainable value can be tricky.

Sure, but that's missing the point. Deserialization doesn't just fail to restore the trainable attribute, it fails completely, if the trainable attribute has been changed. This is because the ordering of weights depends on the trainable attribute, and the serialization/deserialization assumes that the ordering is unchanged.

If you want LayerWithSublayers to retain the trainable status of one of its internal layers, then consider adding that to the layer's get_config method.

Sure, I can do that for my own use case, but the point remains that setting  to  after initialization quietly causes serialization to write output which then will fail to deserialize. This violates &lt;denchmark-link:https://en.wikipedia.org/wiki/Principle_of_least_astonishment&gt;POLA&lt;/denchmark-link&gt;
, and causes wasted time as people run long training runs and then can't use the result.
To avoid that, trainable should be a read-only property rather than an attribute — or at the very least should be clearly documented as not safe to mutate after initialization.
		</comment>
		<comment id='9' author='gthb' date='2020-03-27T15:10:40Z'>
		A workaround (for someone who knows about this gotcha) is to set the .trainable attribute of all layers back to True before serializing. That's tested and working in my use case.
		</comment>
	</comments>
</bug>