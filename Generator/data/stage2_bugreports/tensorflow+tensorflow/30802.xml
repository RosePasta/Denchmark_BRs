<bug id='30802' author='BeWe11' open_date='2019-07-17T13:08:05Z' closed_time='2019-08-20T15:14:28Z'>
	<summary>Using tf.function while enumerating a dataset causes an infinite loop</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.5
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.0.0-beta1
Python version: 3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

Describe the current behavior
Using tf.function when enumerating a dataset will cause an infinite loop.
Describe the expected behavior
Using tf.function when enumerating a dataset should not change the looping behavior.
Code to reproduce the issue
The code snippet below will hang after the last function call. I'm not printing anything because calling tf.print results in a syntax error on colab and I know that these snippets are being run on colab by you. When printing the variable i, it's clear that the loop just never stops, i.e. i increase indefinitely.
import tensorflow as tf

ds = tf.data.Dataset.from_tensor_slices([1,2,3,4])

def test_loop_without_enumerate_without_decorator(ds):
    for val in ds:
        pass

@tf.function
def test_loop_without_enumerate_with_decorator(ds):
    for val in ds:
        pass

def test_loop_with_enumerate_without_decorator(ds):
    for i, val in enumerate(ds):
        pass

@tf.function
def test_loop_with_enumerate_with_decorator(ds):
    for i, val in enumerate(ds):
        pass

print("Without tf.function and without enumerate")
test_loop_without_enumerate_without_decorator(ds)

print("Without tf.function and with enumerate")
test_loop_with_enumerate_without_decorator(ds)

print("With tf.function and without enumerate")
test_loop_without_enumerate_with_decorator(ds)

print("With tf.function and with enumerate")
test_loop_with_enumerate_with_decorator(ds)
Other info / logs
Output of the above snippet:
&lt;denchmark-code&gt;Without tf.function and without enumerate
Without tf.function and with enumerate
With tf.function and without enumerate
With tf.function and with enumerate
**HANGS HERE**
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='BeWe11' date='2019-07-18T09:45:09Z'>
		I was able to reproduce the issue on Colab with Tensorflow 2.0.0.beta1. Please have a look at gist of &lt;denchmark-link:https://colab.research.google.com/drive/1UXGUopt4bVq-NUslCJVxw_BHeOlq0Rkz&gt;Colab&lt;/denchmark-link&gt;
 link. Thanks
		</comment>
		<comment id='2' author='BeWe11' date='2019-07-22T14:26:49Z'>
		AutoGraph does not currently override enumerate, so it has the wrong behavior in graph mode. It would be a nice feature to add, and it shouldn't be too hard to do it.
In the mean time, to enumerate over datasets, please use ds.enumerate(). It works both inside and outside tf.function:
&lt;denchmark-code&gt;def test_loop_with_ds_enumerate_without_decorator(ds):
    for i, val in ds.enumerate():
        tf.print(i, val)

@tf.function
def test_loop_with_ds_enumerate_with_decorator(ds):
    for i, val in ds.enumerate():
        tf.print(i, val)

print("Without tf.function and ds.enumerate")
test_loop_with_ds_enumerate_without_decorator(ds)

print("With tf.function and ds.enumerate")
test_loop_with_ds_enumerate_with_decorator(ds)
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;Without tf.function and ds.enumerate
0 1
1 2
2 3
3 4
With tf.function and ds.enumerate
0 1
1 2
2 3
3 4
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='BeWe11' date='2019-07-22T14:56:29Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
 thank you, that worked for me. I'm gonna let this issue stay open, as the underlying problem hasn't been fixed yet, but feel free to close if you need to.
		</comment>
		<comment id='4' author='BeWe11' date='2019-07-25T13:12:35Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
 I'm interested in working on this! I'm confused on how to get started. I've read the documentation on AutoGraph &amp; tf.function decorator as well as &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/eager/def_function.py#L725-L1027&gt;the code&lt;/denchmark-link&gt;
 and I can't spot which code is responsible for overriding such action. Pardon my inexperience.
		</comment>
		<comment id='5' author='BeWe11' date='2019-07-25T13:38:46Z'>
		&lt;denchmark-link:https://github.com/ilhamfp&gt;@ilhamfp&lt;/denchmark-link&gt;
 that's great, happy to help you get started!
The dynamic dispatch can indeed make things a bit confusing.
The place to add this is in the Python builtin overloads file: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/operators/py_builtins.py&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/operators/py_builtins.py&lt;/denchmark-link&gt;

The file contains a number of overloads, check out  for instance, that you can use as a model. Then, once you have the overload of , you just need to add it to the lists at the bottom of the file,  and  (there's a bit of duplication there). That should be it - the overload for function calls (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/impl/api.py#L390&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/impl/api.py#L390&lt;/denchmark-link&gt;
, placed in the  module for various reasons) should pick it up at Python runtime.
		</comment>
		<comment id='6' author='BeWe11' date='2019-08-20T14:51:42Z'>
		
AutoGraph does not currently override enumerate, so it has the wrong behavior in graph mode.

Hi &lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
 . Just out of curiosity, what do you mean by "wrong behavior in graph mode"? With , I often use  to iterate over a list of layers within a call function as follows:
&lt;denchmark-code&gt;In [2]: import tensorflow as tf

In [3]: tf.__version__
Out[3]: '2.0.0-alpha0'

In [4]: from tensorflow.keras.layers import Dense

In [5]: layers = [Dense(10), Dense(20), Dense(30)]

In [6]: @tf.function
   ...: def call(x):
   ...:     for i, layer in enumerate(layers):
   ...:         x = layer(x)
   ...:     return x

In [7]: y = call(tf.random.uniform((10, 20)))
&lt;/denchmark-code&gt;

It would be grateful if you could explain a bit more.
		</comment>
		<comment id='7' author='BeWe11' date='2019-08-20T15:14:28Z'>
		&lt;denchmark-link:https://github.com/llan-ml&gt;@llan-ml&lt;/denchmark-link&gt;
 I was referring to calling  with a  argument. Called with Python lists it was working fine. See the original post - before &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/31038&gt;#31038&lt;/denchmark-link&gt;
, calling  in the original example caused an infinite loop, which was incorrect. At any rate,  should now be fully supported for datasets.
		</comment>
		<comment id='8' author='BeWe11' date='2019-08-20T15:14:29Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=30802&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=30802&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='BeWe11' date='2020-02-17T13:31:14Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;


AutoGraph does not currently override enumerate, so it has the wrong behavior in graph mode. It would be a nice feature to add, and it shouldn't be too hard to do it.

Could you guys add this to &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/data/Dataset&gt;the documentation&lt;/denchmark-link&gt;
 for tf.Dataset? Seems like a quick addition that would prove super valuable. Thanks!
		</comment>
		<comment id='10' author='BeWe11' date='2020-02-17T14:10:39Z'>
		&lt;denchmark-link:https://github.com/tgsmith61591&gt;@tgsmith61591&lt;/denchmark-link&gt;
 This issue should be resolved - enumerate should now work correctly with datasets in tf.function, with TF &gt;= 1.5. Have you been still experiencing issues?
		</comment>
	</comments>
</bug>