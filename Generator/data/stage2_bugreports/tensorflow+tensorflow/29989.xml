<bug id='29989' author='whhu' open_date='2019-06-20T02:46:50Z' closed_time='2019-07-17T03:38:32Z'>
	<summary>Segmentation fault when saving checkpoints with saveable Dataset Iterator</summary>
	<description>
System information


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes


OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7.6.1810


Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No


TensorFlow installed from (source or binary): Binary


TensorFlow version (use command below): 1.13.1


Python version: 3.6.8


Bazel version (if compiling from source): None


GCC/Compiler version (if compiling from source): None


CUDA/cuDNN version: None


GPU model and memory: None


tf.version: 'v1.13.1-0-g6612da8951' 1.13.1


Describe the current behavior
Segmentation fault in saving an initializable dataset iterator when entering the tf.train.MonitoredSession context manager.
Describe the expected behavior
The initializable iterator is saved and restored properly, behaving the same with the one shot iterator.
Code to reproduce the issue
"""Illustrate saveable dataset iterator
"""
import tensorflow as tf

DATASET_SIZE = 4
SAVE_STEPS = 2
TRAIN_STEP = 3
CHECKPOINT_DIR = '/tmp/tf_dataset_saveable'

def test_saveable():
    """test saveable"""
    graph = tf.Graph()
    with graph.as_default():
        dataset = tf.data.Dataset.range(DATASET_SIZE).repeat()
#        dataset_iterator = dataset.make_one_shot_iterator()
        dataset_iterator = dataset.make_initializable_iterator()
        dataset_init = dataset_iterator.initializer
        data = dataset_iterator.get_next()

        saveable = tf.contrib.data.make_saveable_from_iterator(dataset_iterator)
        tf.add_to_collection(tf.GraphKeys.SAVEABLE_OBJECTS, saveable)

        global_step = tf.train.get_or_create_global_step()
        inc_global_step = tf.assign_add(global_step, 1)  # critical

        saver = tf.train.Saver()
        checkpoint_dir = CHECKPOINT_DIR
        scaffold = tf.train.Scaffold(saver=saver)
        checkpoint_hook = tf.train.CheckpointSaverHook(
            checkpoint_dir=checkpoint_dir,
            save_steps=SAVE_STEPS, scaffold=scaffold)

        hooks = [checkpoint_hook]
        session_creator = tf.train.ChiefSessionCreator(
            scaffold=scaffold, checkpoint_dir=checkpoint_dir)
        with tf.train.MonitoredSession(
                session_creator=session_creator, hooks=hooks) as mon_sess:
            gstep = mon_sess.run(global_step)
            if not gstep:
                mon_sess.run(dataset_init)
            for _ in range(TRAIN_STEP):
                print(mon_sess.run([global_step, data]))
                mon_sess.run(inc_global_step)

if __name__ == '__main__':
    test_saveable()
Other info / logs
(tf-1.13-py3) [huwh1@huwh1-centos worksync]$ python tf_dataset_saveable.py 
WARNING:tensorflow:From /home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py:1419: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.

WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From tf_dataset_saveable.py:20: make_saveable_from_iterator (from tensorflow.contrib.data.python.ops.iterator_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.experimental.make_saveable_from_iterator(...)`.
2019-06-20 10:43:20.947675: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-06-20 10:43:20.951984: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3408000000 Hz
2019-06-20 10:43:20.952497: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x3f10ca0 executing computations on platform Host. Devices:
2019-06-20 10:43:20.952539: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;
Segmentation fault (core dumped)
(tf-1.13-py3) [huwh1@huwh1-centos worksync]$ 
	</description>
	<comments>
		<comment id='1' author='whhu' date='2019-06-21T09:45:11Z'>
		I tried on colab with Tensorflow 1.13.1. I am able to reproduce the issue.
		</comment>
		<comment id='2' author='whhu' date='2019-07-03T17:15:49Z'>
		allenl@ -- not sure if these APIs are expected to work all together. Can you advise as to whether there's a better way?
		</comment>
		<comment id='3' author='whhu' date='2019-07-03T17:18:33Z'>
		Oops. Actually tagging &lt;denchmark-link:https://github.com/allenlavoie&gt;@allenlavoie&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='whhu' date='2019-07-03T17:36:23Z'>
		Is the issue that the iterator isn't initialized when the first checkpoint is written? You may need to get MonitoredTrainingSession to do the initialization so it happens before saving. Otherwise the APIs should go fine together AFAIK.
Either way, it probably shouldn't segfault. &lt;denchmark-link:https://github.com/saxenasaurabh&gt;@saxenasaurabh&lt;/denchmark-link&gt;
 may be more familiar with the serialization op itself.
		</comment>
		<comment id='5' author='whhu' date='2019-07-15T21:52:19Z'>
		&lt;denchmark-link:https://github.com/allenlavoie&gt;@allenlavoie&lt;/denchmark-link&gt;
 is correct, you need to run the  before you can save the iterator.
I am going to create a change that will produce an informative error message for this case (as opposed to segfaulting).
		</comment>
		<comment id='6' author='whhu' date='2019-07-16T00:30:26Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=29989&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=29989&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='whhu' date='2019-07-16T07:25:51Z'>
		Many thanks to the fruitful discussions. Adding dataset initializer to the collection TABLE_INITIALIZERS fixes the segfaulting. Nevertheless, the iterator starts from the very beginning of the dataset every time after recovering from the checkpoint.
Is there any solution to the problem at present? Maybe something like init_fn in tf.train.Scaffold ...
		</comment>
		<comment id='8' author='whhu' date='2019-07-16T17:44:35Z'>
		That should not be the case. The whole point of saving the iterator is that you can checkpoint its state (and we have tests that verify that this functionality works).
Please create a new issue with instructions on how to reproduce your issue so that someone can investigate why is your program resetting the iterator state.
		</comment>
		<comment id='9' author='whhu' date='2019-07-16T18:10:07Z'>
		I think this is a missing feature. We added &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/contrib/data/CheckpointInputPipelineHook&gt;CheckpointInputPipelineHook&lt;/denchmark-link&gt;
 to fix exactly this &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/5e2a91f65cfb23f996136b9201d9312c9c36b941/tensorflow/python/data/experimental/ops/iterator_ops.py#L194&gt;problem&lt;/denchmark-link&gt;
. However, it requires an estimator as an arg. It doesn't necessarily have to. We just did it this way for simplicity.
It should be fairly straightforward to extend  CheckpointInputPipelineHook to support non-estimator use-cases e.g. by explicitly passing the required args i.e. num_worker_replicas, task_type, task_id, model_dir , save_checkpoints_secs, save_checkpoints_steps. I would be happy to review if you want to send in a PR.
As a really hacky workaround you could just build a mock Estimator object that implements the expected fields. That may be prone to breakages though.
		</comment>
		<comment id='10' author='whhu' date='2019-07-17T03:38:32Z'>
		Thank you very much. It helps greatly! :-)
		</comment>
		<comment id='11' author='whhu' date='2019-07-17T03:38:33Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=29989&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=29989&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>