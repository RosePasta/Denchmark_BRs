<bug id='38598' author='Dobiasd' open_date='2020-04-16T08:55:25Z' closed_time='2020-05-14T06:57:24Z'>
	<summary>Memory-leak-like behavior on subsequent predictions (since TF 2.1.0)</summary>
	<description>
The following minimal example produces (and logs) continuous memory growth:
# tf_memleak.py

import os

import numpy as np
import psutil

# Disable GPU, use CPU only
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
import tensorflow as tf

print(tf.version.GIT_VERSION, tf.version.VERSION)

inputs = tf.keras.layers.Input((None, None, 3))
x = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), padding='same')(inputs)
model = tf.keras.models.Model(inputs=inputs, outputs=x)

for count in range(10000):
    memory_usage_in_MiB = psutil.Process().memory_info().rss / (1024 * 1024)
    print(f'Memory usage after {count} prediction(s):',
          f'{memory_usage_in_MiB:.3f} MiB',
          flush=True)
    image_shape = [*np.random.randint(128, 1024, (2,)).tolist(), 3]
    image = np.random.uniform(0, 256, image_shape)
    model.predict(image[np.newaxis])
&lt;denchmark-link:https://gist.githubusercontent.com/Dobiasd/09c63fbbf6ec11e2aa631f6f4b8adf2f/raw/ce410998c82bcc514a516617a69975208a9f26c0/gistfile1.txt&gt;Output&lt;/denchmark-link&gt;
:
&lt;denchmark-code&gt;[...]
v2.1.0-rc2-17-ge5bf8de 2.1.0
[...]
Memory usage after 0 prediction(s): 306.641 MiB
Memory usage after 1 prediction(s): 328.238 MiB
Memory usage after 2 prediction(s): 344.785 MiB
Memory usage after 3 prediction(s): 349.023 MiB
Memory usage after 4 prediction(s): 417.152 MiB
WARNING: Logging before flag parsing goes to stderr.
W0416 10:25:29.859581 140207157561088 def_function.py:586] 5 out of the last 5 calls to &lt;function _make_execution_function.&lt;locals&gt;.distributed_function at 0x7f83e8146440&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.
Memory usage after 5 prediction(s): 424.629 MiB
W0416 10:25:29.972562 140207157561088 def_function.py:586] 6 out of the last 6 calls to &lt;function _make_execution_function.&lt;locals&gt;.distributed_function at 0x7f83e8146440&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.
Memory usage after 6 prediction(s): 426.527 MiB
[...]
W0416 10:25:31.405993 140207157561088 def_function.py:586] 11 out of the last 11 calls to &lt;function _make_execution_function.&lt;locals&gt;.distributed_function at 0x7f83e8146440&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.
Memory usage after 20 prediction(s): 534.414 MiB
[...]
Memory usage after 100 prediction(s): 660.227 MiB
[...]
Memory usage after 200 prediction(s): 825.668 MiB
[...]
Memory usage after 1000 prediction(s): 1152.734 MiB
[...]
Memory usage after 2000 prediction(s): 1608.641 MiB
[...]
Memory usage after 3000 prediction(s): 1973.738 MiB
[...]
Memory usage after 4000 prediction(s): 2450.891 MiB
[...]
Memory usage after 9000 prediction(s): 4416.121 MiB
[...]
&lt;/denchmark-code&gt;

The situation can also be created using Docker by running docker build -t deleteme . with the following Dockerfile:
&lt;denchmark-code&gt;FROM python:3.7

RUN pip install tensorflow==2.1.0 psutil==5.7.0

# Disable the Docker cache from this stage on, see https://stackoverflow.com/a/58801213/1866775
ADD "https://www.random.org/cgi-bin/randbyte?nbytes=10&amp;format=h" skipcache

ADD ./tf_memleak.py /
RUN python /tf_memleak.py
&lt;/denchmark-code&gt;

The problem persists even when using the following:
tf.config.threading.set_inter_op_parallelism_threads(1)
tf.config.threading.set_intra_op_parallelism_threads(1)
And tf.config.experimental.set_memory_growth(tf.config.list_physical_devices()[0], True) fails with ValueError: Cannot set memory growth on non-GPU devices.
I've tested with different versions of TensorFlow:

1.15.2: ok
2.0.0: ok
2.0.1: ok
2.1.0rc0: leaky
2.1.0: leaky
2.2.0: leaky

So it seems the problem was introduced between versions 2.0.1 and 2.1.0rc0. Also, the "ok" versions don't show the tf.function-retracing warnings, and all the "leaky" versions do.
	</description>
	<comments>
		<comment id='1' author='Dobiasd' date='2020-04-23T11:12:50Z'>
		&lt;denchmark-link:https://github.com/Dobiasd&gt;@Dobiasd&lt;/denchmark-link&gt;

is this still an issue
		</comment>
		<comment id='2' author='Dobiasd' date='2020-04-23T11:38:54Z'>
		Yes, it is.
I just tested with the latest TF release (2.2.0rc3), and it's still leaking memory. (And I'm still doing weird workarounds on our production servers because of this.)
		</comment>
		<comment id='3' author='Dobiasd' date='2020-05-06T22:59:10Z'>
		I am also running into this issue with TF 2.2.0-rc4.
I haven't been able to find a way to work around the issue.  I've tried these calls, but it doesn't resolve the issue:
K.clear_session() gc.collect()
		</comment>
		<comment id='4' author='Dobiasd' date='2020-05-08T06:36:27Z'>
		The &lt;denchmark-link:https://github.com/tensorflow/tensorflow/releases/tag/v2.2.0&gt;release notes of version 2.2.0&lt;/denchmark-link&gt;
 list the following:

Add CompositeTensor support for DistributedIterators. This should help prevent unnecessary function retracing and memory leaks.

So I tested again with version 2.2.0.
However, the problem persists. Memory usage is still growing continuously.
		</comment>
		<comment id='5' author='Dobiasd' date='2020-05-14T00:07:19Z'>
		&lt;denchmark-link:https://github.com/Dobiasd&gt;@Dobiasd&lt;/denchmark-link&gt;
 with latest tf-nightly I do not see this issue, here is a &lt;denchmark-link:https://colab.research.google.com/gist/goldiegadde/77f7474252ca3d2c089cb72d339e0985/github-issue-38598.ipynb&gt;github gist&lt;/denchmark-link&gt;
 of the code you provided.
I see the mem growth stabilize after 500 predictions.
Could you please try with the latest tf-nightly and let us know ?
		</comment>
		<comment id='6' author='Dobiasd' date='2020-05-14T06:56:37Z'>
		Just tried with the latest  (), and can happily confirm, the memleak &lt;denchmark-link:https://gist.github.com/Dobiasd/da5e045d69fac73fc53af462c5b6abf9&gt;seems to be fixed&lt;/denchmark-link&gt;
! 
		</comment>
		<comment id='7' author='Dobiasd' date='2020-05-14T06:57:25Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38598&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38598&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='Dobiasd' date='2020-05-27T09:06:51Z'>
		Trying the latest nightly (tf_nightly-2.3.0.dev20200526-cp37-cp37m-manylinux2010_x86_64.whl), the problem is unfortunately only partly solved for me. After around 3 million model predicts, memory consumption still increased by 200 MB. The memory consumption of the code is completely stable with tensorflow-gpu 2.0.1.
		</comment>
	</comments>
</bug>