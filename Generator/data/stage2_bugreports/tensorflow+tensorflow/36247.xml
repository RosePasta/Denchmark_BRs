<bug id='36247' author='himanshisyadav' open_date='2020-01-27T14:21:53Z' closed_time='2020-02-25T03:22:38Z'>
	<summary>XLA Warning and could not interpret set environment</summary>
	<description>
System information

OS Platform and Distribution:18.04
TensorFlow version: 1.14.0
Python version: 2.7.17

Couldn't interpret value =/home/hyadav/deephyp/lib/python2.7/site-packages/tensorflow/compiler/xla:--tf_xla_cpu_global_jit=/home/hyadav/deephyp/lib/python2.7/site-packages/tensorflow/compiler/xla:=--tf_xla_cpu_global_jit=--tf_xla_cpu_global_jit for flag tf_xla_cpu_global_jit.
I tried using the following export TF_XLA_FLAGS=--tf_xla_cpu_global_jit=/mytensorflowpath/tensorflow/compiler/xla:$TF_XLA_FLAGS=--tf_xla_cpu_global_jit from issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/30308&gt;#30308&lt;/denchmark-link&gt;
 and get the above.
	</description>
	<comments>
		<comment id='1' author='himanshisyadav' date='2020-01-28T11:50:45Z'>
		&lt;denchmark-link:https://github.com/himanshisyadav&gt;@himanshisyadav&lt;/denchmark-link&gt;
, Could you provide the standalone code to reproduce the reported issue. Thanks!
		</comment>
		<comment id='2' author='himanshisyadav' date='2020-01-28T17:45:41Z'>
		I am using the autoencoder example codes from &lt;denchmark-link:https://github.com/lloydwindrim/hyperspectral-autoencoders&gt;this&lt;/denchmark-link&gt;
 github repo. Specifically running autoencoder_test_MLP_basic.py.
I actually unset tf_xla_cpu_jit variable and the above error was solved. However, I was trying so many things simultaneously that I do not remember what did the trick.
		</comment>
		<comment id='3' author='himanshisyadav' date='2020-02-03T13:50:16Z'>
		Hi &lt;denchmark-link:https://github.com/himanshisyadav&gt;@himanshisyadav&lt;/denchmark-link&gt;
,
The person in the previous issue was using this flag wrong. It should be only TF_XLA_FLAGS=--tf_xla_cpu_global_jit This is not a key-value pair. It is just a flag.
Regarding the XLA warning you mention, if you were seeing the same warning mentioned in the previous issue, it is fixed in newer versions of Tensorflow and will not be backported. If you don't want to use XLA in your code, just ignore the warning.
		</comment>
		<comment id='4' author='himanshisyadav' date='2020-02-14T06:52:48Z'>
		&lt;denchmark-link:https://github.com/himanshisyadav&gt;@himanshisyadav&lt;/denchmark-link&gt;
 Please let us know if the above comment helps resolve the issue.
		</comment>
		<comment id='5' author='himanshisyadav' date='2020-02-25T03:22:40Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36247&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36247&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>