<bug id='25021' author='Vooblin' open_date='2019-01-18T09:37:09Z' closed_time='2019-01-21T08:14:20Z'>
	<summary>[TFLite, Quantization, Performance] float32 nodes faster than uint8.</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.12.0, b'v1.12.0-6341-g8a5d48a'
Python version: 3.5
Bazel version (if compiling from source): 0.19.2
GCC/Compiler version (if compiling from source): c++ (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0
CUDA/cuDNN version: No
GPU model and memory: No

Describe the current behavior
I converted my TF model to two TFLite models: float32 and uint8. Then I compared them and nodes DEPTHWISE_CONV2D and SOFTMAX with float32 were faster than with uint8.
Describe the expected behavior
I thought, that at least conv2d with uint8 will faster than with float32.
Code to reproduce the issue
My code:
&lt;denchmark-code&gt;import tensorflow as tf

for fl_quant in (0, 1):
    X = tf.placeholder(tf.float32, [100, 10, 10, 1], 'X')

    W1 = tf.Variable(tf.random_normal([3, 3, 1, 32]), name='W1')
    B1 = tf.Variable(tf.random_normal([32]), name='B1')

    W2 = tf.Variable(tf.random_normal([3200, 10]), name='W2')
    B2 = tf.Variable(tf.random_normal([10]), name='B2')

    XW1 = tf.nn.conv2d(X, W1, [1, 1, 1, 1], 'SAME')
    XWB1 = tf.nn.relu6(tf.nn.bias_add(XW1, B1))
    XWB1 = tf.reshape(XWB1, [100, 3200])
    XWB2 = tf.nn.bias_add(tf.matmul(XWB1, W2), B2)
    Result = tf.nn.softmax(XWB2)

    if fl_quant:
        tf.contrib.quantize.create_eval_graph(tf.get_default_graph())

    init = tf.global_variables_initializer()

    with tf.Session() as sess:
        sess.run(init)
        converter_dot = tf.lite.TFLiteConverter.from_session(sess, [X], [Result])
        converter_tflite = tf.lite.TFLiteConverter.from_session(sess, [X], [Result])
    tf.reset_default_graph()
    if fl_quant:
        converter_dot.inference_type = tf.lite.constants.QUANTIZED_UINT8
        converter_dot.inference_input_type = tf.lite.constants.QUANTIZED_UINT8
        converter_dot.quantized_input_stats = {'X': (127, 127)}
        converter_dot.default_ranges_stats = (0, 255)

        converter_tflite.inference_type = tf.lite.constants.QUANTIZED_UINT8
        converter_tflite.inference_input_type = tf.lite.constants.QUANTIZED_UINT8
        converter_tflite.quantized_input_stats = {'X': (127, 127)}
        converter_tflite.default_ranges_stats = (0, 255)
    converter_dot.output_format = tf.lite.constants.GRAPHVIZ_DOT
    model_dot = converter_dot.convert()
    model_tflite = converter_tflite.convert()
    if fl_quant:
        open("quant_model.dot", "wb").write(model_dot)
        open("quant_model.tflite", "wb").write(model_tflite)
    else:
        open("model.dot", "wb").write(model_dot)
        open("model.tflite", "wb").write(model_tflite)
&lt;/denchmark-code&gt;

Other info / logs
Benchmark for float32:
&lt;denchmark-code&gt;Min num runs: [50]
Min runs duration (seconds): [1]
Inter-run delay (seconds): [-1]
Num threads: [1]
Benchmark name: []
Output prefix: []
Min warmup runs: [1]
Min warmup runs duration (seconds): [0.5]
Graph: [model.tflite]
Input layers: []
Input shapes: []
Use nnapi : [0]
Loaded model model.tflite
resolved reporter
Initialized session in 0.378ms
Running benchmark for at least 1 iterations and at least 0.5 seconds
count=302 first=5983 curr=1592 min=1586 max=5983 avg=1652.99 std=360

Running benchmark for at least 50 iterations and at least 1 seconds
count=626 first=1675 curr=1587 min=1586 max=1684 avg=1592.71 std=14

============================== Run Order ==============================
                     [node type]                  [start]         [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name]
               DEPTHWISE_CONV_2D                    0.000           0.802           0.719        45.131%         45.131%             0.000              1       [Relu6]
                 FULLY_CONNECTED                    0.719           0.848           0.849        53.325%         98.456%             0.000              1       [BiasAdd_1]
                         SOFTMAX                    1.568           0.025           0.025         1.544%        100.000%             0.000              1       [Softmax]

============================== Top by Computation Time ==============================
                     [node type]                  [start]         [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name]
                 FULLY_CONNECTED                    0.719           0.848           0.849        53.325%         53.325%             0.000              1       [BiasAdd_1]
               DEPTHWISE_CONV_2D                    0.000           0.802           0.719        45.131%         98.456%             0.000              1       [Relu6]
                         SOFTMAX                    1.568           0.025           0.025         1.544%        100.000%             0.000              1       [Softmax]

Number of nodes executed: 3
============================== Summary by node type ==============================
                     [Node type]          [count]         [avg ms]          [avg %]         [cdf %]       [mem KB]      [times called]
                 FULLY_CONNECTED                1            0.849          53.363%         53.363%          0.000              1
               DEPTHWISE_CONV_2D                1            0.718          45.129%         98.492%          0.000              1
                         SOFTMAX                1            0.024           1.508%        100.000%          0.000              1

Timings (microseconds): count=626 first=1675 curr=1587 min=1585 max=1684 avg=1592.5 std=14
Memory (bytes): count=0
3 nodes observed


Average inference timings in us: Warmup: 1652.99, Init: 378, no stats: 1592.71
&lt;/denchmark-code&gt;

Benchmark for uint8:
&lt;denchmark-code&gt;Min num runs: [50]
Min runs duration (seconds): [1]
Inter-run delay (seconds): [-1]
Num threads: [1]
Benchmark name: []
Output prefix: []
Min warmup runs: [1]
Min warmup runs duration (seconds): [0.5]
Graph: [quant_model.tflite]
Input layers: []
Input shapes: []
Use nnapi : [0]
Loaded model quant_model.tflite
resolved reporter
Initialized session in 0.311ms
Running benchmark for at least 1 iterations and at least 0.5 seconds
count=159 first=9037 curr=3022 min=3020 max=9037 avg=3152.67 std=598

Running benchmark for at least 50 iterations and at least 1 seconds
count=316 first=3222 curr=3194 min=3020 max=3254 avg=3162.37 std=67

============================== Run Order ==============================
                     [node type]                  [start]         [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name]
               DEPTHWISE_CONV_2D                    0.000           2.884           2.829        89.470%         89.470%             0.000              1       [Relu6]
                 FULLY_CONNECTED                    2.829           0.288           0.283         8.960%         98.430%             0.000              1       [BiasAdd_1]
                         SOFTMAX                    3.113           0.050           0.050         1.570%        100.000%             0.000              1       [Softmax]

============================== Top by Computation Time ==============================
                     [node type]                  [start]         [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name]
               DEPTHWISE_CONV_2D                    0.000           2.884           2.829        89.470%         89.470%             0.000              1       [Relu6]
                 FULLY_CONNECTED                    2.829           0.288           0.283         8.960%         98.430%             0.000              1       [BiasAdd_1]
                         SOFTMAX                    3.113           0.050           0.050         1.570%        100.000%             0.000              1       [Softmax]

Number of nodes executed: 3
============================== Summary by node type ==============================
                     [Node type]          [count]         [avg ms]          [avg %]         [cdf %]       [mem KB]      [times called]
               DEPTHWISE_CONV_2D                1            2.829          89.497%         89.497%          0.000              1
                 FULLY_CONNECTED                1            0.283           8.953%         98.450%          0.000              1
                         SOFTMAX                1            0.049           1.550%        100.000%          0.000              1

Timings (microseconds): count=316 first=3222 curr=3194 min=3019 max=3254 avg=3162.23 std=67
Memory (bytes): count=0
3 nodes observed


Average inference timings in us: Warmup: 3152.67, Init: 311, no stats: 3162.37
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Vooblin' date='2019-01-18T19:34:40Z'>
		If you're measuring latency on an Intel cpu, then I don't think uint8 ops would be optimized, which would explain why your floating model is running faster. Try measuring on an ARM core.
		</comment>
		<comment id='2' author='Vooblin' date='2019-01-18T19:47:08Z'>
		&lt;denchmark-link:https://github.com/Vooblin&gt;@Vooblin&lt;/denchmark-link&gt;
 This is not due to TF Lite but due to hardware CPU/GPU configuration as mentioned by &lt;denchmark-link:https://github.com/ruffles1&gt;@ruffles1&lt;/denchmark-link&gt;
. This is well know issue which is similar to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/1300&gt;1&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/15585&gt;2&lt;/denchmark-link&gt;
, and &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/5592&gt;3&lt;/denchmark-link&gt;
. All these links have many more groups mentioning similar issues. In future, please post these kind of support questions in Stackoverflow as the community will get benefited more. Thanks!
		</comment>
		<comment id='3' author='Vooblin' date='2019-01-21T08:13:49Z'>
		&lt;denchmark-link:https://github.com/ruffles1&gt;@ruffles1&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 Thanks a lot for your responses! It was very helpful!
		</comment>
	</comments>
</bug>