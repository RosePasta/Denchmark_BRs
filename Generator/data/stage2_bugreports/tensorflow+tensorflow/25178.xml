<bug id='25178' author='0x0L' open_date='2019-01-24T20:59:24Z' closed_time='2019-02-01T19:18:01Z'>
	<summary>Inconsistent behavior of sample weight for Keras</summary>
	<description>
TF: home compiled master from a few days ago
Keras: 2.2.4 (pypi)
Hi
Consider the following snippet
import numpy as np
import tensorflow as tf
import keras
from tensorflow import keras as tf_keras

x = np.array([[1.], [2], [3]])
y = np.array([[1], [4], [5]])
w = np.array([0.1, 0., 0.1])

i_x = keras.layers.Input(shape=(1,))
model = keras.Model(inputs=i_x, outputs=i_x)

model.compile('sgd', 'mse')
print(model.evaluate(x, y, sample_weight=w, verbose=False))
# ---&gt; 0.2 = sum(w * squared_diff) / count(w not null)

model.compile('sgd', keras.losses.mean_squared_error)
print(model.evaluate(x, y, sample_weight=w, verbose=False))
# ---&gt; 0.2

i_x = tf_keras.layers.Input(shape=(1,))
model = tf_keras.Model(inputs=i_x, outputs=i_x)

model.compile('sgd', 'mse')
print(model.evaluate(x, y, sample_weight=w, verbose=False))
# ---&gt; 0.13333 = mean(w * squared_diff)

model.compile('sgd', keras.losses.mean_squared_error)
print(model.evaluate(x, y, sample_weight=w, verbose=False))
# ---&gt; 2.0 = mean(w * squared_diff) / mean(w)
This seems a bit all over the place. Keras behavior is cringeworthy as the loss is discontinuous in w . For w[1] = 1e-6, one gets 0.13333... but at least its consistent.
I don't know what should be done but I'm sure others have wasted half a day of work because of this ;)
	</description>
	<comments>
		<comment id='1' author='0x0L' date='2019-01-29T09:37:54Z'>
		Possible &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/23767&gt;#23767&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='2' author='0x0L' date='2019-01-30T01:57:29Z'>
		Hi &lt;denchmark-link:https://github.com/0x0L&gt;@0x0L&lt;/denchmark-link&gt;
, thanks for the issue!
It looks like the inconsistency b/t 'mse' and tf.keras.losses.mean_squared_error is fixed in the latest nightly, I'm seeing 0.13 for both.
I think the external Keras logic should be updated to reflect the tf.keras logic, as 0 values are being considered as null in external Keras and thus increasing the effective sample weight for other samples
Made a tracking bug in external Keras: &lt;denchmark-link:https://github.com/keras-team/keras/issues/12176&gt;keras-team/keras#12176&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='0x0L' date='2019-02-01T19:10:51Z'>
		&lt;denchmark-link:https://github.com/omalleyt12&gt;@omalleyt12&lt;/denchmark-link&gt;

Hi, thanks for testing this. I wasn't able to try tf-nightly since I'm using python 3.7.
I guess we should close this issue ?
EDIT: I just recompiled master and can confirm tf gives 0.13 for both losses
		</comment>
	</comments>
</bug>