<bug id='26557' author='Fordacre' open_date='2019-03-11T02:29:47Z' closed_time='2019-10-07T17:04:30Z'>
	<summary>tf.keras.optimizers.Adam with tf.stop_gradient leads to ValueError: An operation has `None` for gradient.</summary>
	<description>
This template is for miscellaneous issues not covered by the other issue categories.
For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to &lt;denchmark-link:https://stackoverflow.com/questions/tagged/tensorflow&gt;StackOverflow&lt;/denchmark-link&gt;
.
If you are reporting a vulnerability, please use the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md&gt;dedicated reporting process&lt;/denchmark-link&gt;
.
For high-level discussions about TensorFlow, please post to &lt;denchmark-link:mailto:discuss@tensorflow.org&gt;discuss@tensorflow.org&lt;/denchmark-link&gt;
, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to &lt;denchmark-link:mailto:developers@tensorflow.org&gt;developers@tensorflow.org&lt;/denchmark-link&gt;
.
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;

OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 16.04.5 LTS (GNU/Linux 4.15.0-45-generic x86_64)
TensorFlow installed from (source or binary):
pip
TensorFlow version (use command below):
'1.12.0'
Python version:
Python 3.5.2
Bazel version (if compiling from source):
N/A
GCC/Compiler version (if compiling from source):
N/A
CUDA/cuDNN version:
9.0.176
GPU model and memory:
Tesla V100
Exact command to reproduce:
&lt;denchmark-code&gt;import os
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (LSTM, Activation, BatchNormalization, Bidirectional,
                                     Conv1D, Dense, Dropout, Input, Lambda, Masking,
                                     TimeDistributed)

import numpy as np
os.environ["CUDA_VISIBLE_DEVICES"]="1"


Input_layer = Input(shape=(2, ))
Dense1 = Dense(8, input_shape=(2, ))(Input_layer)
Dense1_stop = Lambda(lambda x: tf.stop_gradient(x))(Dense1)
Dense2 = Dense(4)(Dense1_stop)
Dense3 = Dense(1, activation='softmax')(Dense2)
model = Model(inputs=Input_layer, outputs=Dense3)

# model.compile(optimizer=tf.train.AdamOptimizer(), loss='mse')
model.compile(optimizer=tf.keras.optimizers.Adam(), loss='mse')


x = np.random.uniform(0, 1, (100, 2))
y = np.random.uniform(0, 1, (100, 1))
model.fit(x=x, y=y, validation_split=0.2)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Describe the problem：&lt;/denchmark-h&gt;

This problem occurs when I change tf.train.AdamOptimizer to tf.keras.optimizers.Adam. I'm not sure whether it is a bug or I wrongly written the codes. Therefore, I put it here. Thanks for your attention.
&lt;denchmark-h:h3&gt;Source code / logs：&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;Traceback (most recent call last):
  File "optimizer_test.py", line 25, in &lt;module&gt;
    model.fit(x=x, y=y, validation_split=0.2)
  File "/home/jcc/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py", line 1639, in fit
    validation_steps=validation_steps)
  File "/home/jcc/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_arrays.py", line 86, in fit_loop
    model._make_train_function()
  File "/home/jcc/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py", line 700, in _make_train_function
    params=self._collected_trainable_weights, loss=self.total_loss)
  File "/home/jcc/.local/lib/python3.5/site-packages/tensorflow/python/keras/optimizers.py", line 457, in get_updates
    grads = self.get_gradients(loss, params)
  File "/home/jcc/.local/lib/python3.5/site-packages/tensorflow/python/keras/optimizers.py", line 85, in get_gradients
    raise ValueError('An operation has `None` for gradient. '
ValueError: An operation has `None` for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Fordacre' date='2019-03-11T10:22:03Z'>
		&lt;denchmark-link:https://github.com/Fordacre&gt;@Fordacre&lt;/denchmark-link&gt;
 have u faced any problem after using 
The reason for the failure is that there is no gradient calculation for the Dense1 tensor because of  , could you please help me to understand what you are trying out with the line 
		</comment>
		<comment id='2' author='Fordacre' date='2019-03-11T13:24:33Z'>
		&lt;denchmark-link:https://github.com/joyalbin&gt;@joyalbin&lt;/denchmark-link&gt;
 I'm sorry for the ambiguity of my description. I just wrote a demo about when will this problem occurs. I want the first dense layer doesn't influence the loss in this demo.
However, in my practical application, it is a Multi outputs model with multi losses. What I want to do is to prevent some part of my model from influencing one of those losses.
In addition, if I use tf.train.AdamOptimizer which has been commented instead this error won't occur.
Thanks.
		</comment>
		<comment id='3' author='Fordacre' date='2019-03-11T14:30:34Z'>
		&lt;denchmark-link:https://github.com/Fordacre&gt;@Fordacre&lt;/denchmark-link&gt;

Do I understand correctly that you want to train Dense2-Dense3 and Dense3-output while keeping Dense1-Dense2 and Input-Dense1 fixed?
		</comment>
		<comment id='4' author='Fordacre' date='2019-03-12T01:12:35Z'>
		&lt;denchmark-link:https://github.com/timudk&gt;@timudk&lt;/denchmark-link&gt;
 Sorry, but I'm not quite understand why you put two layers together. Actually, I just want to keep variables of Dense1 fixed.
		</comment>
		<comment id='5' author='Fordacre' date='2019-03-12T22:07:12Z'>
		&lt;denchmark-link:https://github.com/Fordacre&gt;@Fordacre&lt;/denchmark-link&gt;
 Could you try TF2.0 and check whether same bug persists? thanks!
		</comment>
		<comment id='6' author='Fordacre' date='2019-03-13T03:20:52Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 I test it on my PC with TF2.0, unfortunately, tf.keras.optimizers.Adam() doesn't work and even tf.optimizers.Adam() doesn't work.
&lt;denchmark-code&gt;raise ValueError("An operation has `None` for gradient. "
ValueError: An operation has `None` for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval.

&lt;/denchmark-code&gt;

In addition, the same problem occurs when I use tensorboard with histogram_freq in 1.12 version.
		</comment>
		<comment id='7' author='Fordacre' date='2019-03-21T00:31:25Z'>
		Can you update tf version to 1.13.1 and see if it goes away?
		</comment>
		<comment id='8' author='Fordacre' date='2019-03-21T06:46:48Z'>
		&lt;denchmark-code&gt;Traceback (most recent call last):
  File "&lt;input&gt;", line 1, in &lt;module&gt;
  File "D:\Program Files\JetBrains\PyCharm 2018.3.1\helpers\pydev\_pydev_bundle\pydev_umd.py", line 197, in runfile
    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script
  File "D:\Program Files\JetBrains\PyCharm 2018.3.1\helpers\pydev\_pydev_imps\_pydev_execfile.py", line 18, in execfile
    exec(compile(contents+"\n", file, 'exec'), glob, loc)
  File "D:/workplace/apg_net_win/test/optimizer_test.py", line 25, in &lt;module&gt;
    model.fit(x=x, y=y, validation_split=0.2)
  File "C:\Users\John\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\engine\training.py", line 880, in fit
    validation_steps=validation_steps)
  File "C:\Users\John\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\engine\training_arrays.py", line 195, in model_iteration
    f = _make_execution_function(model, mode)
  File "C:\Users\John\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\engine\training_arrays.py", line 122, in _make_execution_function
    return model._make_execution_function(mode)
  File "C:\Users\John\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\engine\training.py", line 1983, in _make_execution_function
    self._make_fit_function()
  File "C:\Users\John\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\engine\training.py", line 1926, in _make_fit_function
    '_fit_function', [self.total_loss] + metrics_tensors)
  File "C:\Users\John\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\engine\training.py", line 1895, in _make_train_function_helper
    params=self._collected_trainable_weights, loss=self.total_loss)
  File "C:\Users\John\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\optimizers.py", line 485, in get_updates
    grads = self.get_gradients(loss, params)
  File "C:\Users\John\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\optimizers.py", line 95, in get_gradients
    raise ValueError('An operation has `None` for gradient. '
ValueError: An operation has `None` for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval.
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;
 I test it on my PC with tf version 1.13.1 again. Thanks for your advice, however, it doesn't help.
		</comment>
		<comment id='9' author='Fordacre' date='2019-04-26T17:17:00Z'>
		I think as mentioned above, you have variables in your model that is not in the backprop path because of stop_gradient. while tf.train.AdamOptimizer does not explicitly check for None gradient, tf.keras.optimizers.Adam does. The entire Keras model.fit has been relying on the fact that all variables needs to be present for training. If this is not your case, I'd suggest instead of using stop_gradient, you should try Keras + customized training loop, see:
&lt;denchmark-link:https://www.tensorflow.org/tutorials/eager/custom_training_walkthrough&gt;https://www.tensorflow.org/tutorials/eager/custom_training_walkthrough&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='Fordacre' date='2019-10-06T18:51:09Z'>
		I have the same issue. The code runs perfectly in tensorflow 1.14. But not in tensorflow 2.0.
Oddly, tf.keras.optimizers.Adam works in tensorflow 1.14.
Exact same situation.
		</comment>
		<comment id='11' author='Fordacre' date='2019-10-07T17:04:30Z'>
		&lt;denchmark-link:https://github.com/Nephalen&gt;@Nephalen&lt;/denchmark-link&gt;
 I think original issue was resolved by &lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;
 suggestion. I am closing this issue.
Please open a new issue with details about your issue, platform details and a standalone code to reproduce the issue. Thanks!
		</comment>
		<comment id='12' author='Fordacre' date='2019-10-07T17:04:32Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26557&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26557&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>