<bug id='23924' author='w4-sjcho' open_date='2018-11-22T14:45:32Z' closed_time='2018-11-26T19:11:44Z'>
	<summary>Memory leak when using tf.contrib.data.unbatch()</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0
Python version: Python 3.6.7 :: Anaconda, Inc.
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 9.0
GPU model and memory: 1080ti

Describe the current behavior
Memory usage continuously increase when using tf.contrib.data.unbatch().
Describe the expected behavior
Memory usage should not increase.
Code to reproduce the issue
&lt;denchmark-code&gt;from absl import app
from absl import flags
from absl import logging
import tensorflow as tf


FLAGS = flags.FLAGS
flags.DEFINE_integer('epochs', 1000, '')
flags.DEFINE_boolean('use_unbatch', False, '')


def create_dataset(input_holder):
    dataset = tf.data.Dataset.from_tensor_slices((input_holder,))

    def generate_random_tensor(size):
        return tf.random_uniform([5, size, size], dtype=tf.float32)

    dataset = dataset.map(generate_random_tensor)
    if FLAGS.use_unbatch:
        dataset = dataset.apply(tf.contrib.data.unbatch())
    else:
        dataset = dataset.flat_map(
            lambda x: tf.data.Dataset.from_tensor_slices((x,)))
        # The output of the dataset becomes a single-element tuple w/o this.
        dataset = dataset.map(lambda x: x)
    return dataset


def main(_):
    with tf.Session() as sess:
        size_holder = tf.placeholder(tf.int32, shape=[None])
        dataset = create_dataset(size_holder)

        iterator = dataset.make_initializable_iterator()
        get_next = iterator.get_next()

        for i in range(FLAGS.epochs):
            logging.info('Epoch #%d', i)
            sess.run(iterator.initializer, feed_dict={
                size_holder: [1000 + (i % 100)],
            })
            try:
                while True:
                    array = sess.run(get_next)
                    logging.info('  Generated: %s', array.shape)
            except tf.errors.OutOfRangeError:
                pass


if __name__ == '__main__':
    app.run(main)
&lt;/denchmark-code&gt;

Memory usage will increase with --use_unbatch, while with --nouse_unbatch, memory usage does not increase.
Other info / logs
It seems like  call is missing in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/unbatch_dataset_op.cc#L42&gt;UnbatchDatasetOp&lt;/denchmark-link&gt;
.
	</description>
	<comments>
		<comment id='1' author='w4-sjcho' date='2018-11-22T16:13:45Z'>
		Possibly related to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/23904&gt;#23904&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='w4-sjcho' date='2018-11-26T18:43:15Z'>
		Thanks for tracking down the issue: indeed, the missing input_-&gt;Unref() seems to be the culprit.
&lt;denchmark-link:https://github.com/artsobolev&gt;@artsobolev&lt;/denchmark-link&gt;
 I think the root cause for this bug is different. I'll comment on the other thread.
		</comment>
	</comments>
</bug>