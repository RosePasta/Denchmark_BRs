<bug id='12358' author='nolanliou' open_date='2017-08-17T11:20:09Z' closed_time='2017-08-28T03:50:54Z'>
	<summary>[Feature Request]  Add an extra argument `is_duplicated` to `scatter_sub/*` Ops and make the kernel to multi-thread for speedup.</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
ubuntu 16.04
TenorFlow installed from (source or binary):
source
TensorFlow version (use command below):
v1.3.0-rc2
Python version:
2.7
Bazel version (if compiling from source):
0.5.2
CUDA/cuDNN version:
8.0/5.1.10
GPU model and memory:
nvidia M40
CPU
32-cores

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

I profile the  on single machine with tfprof, in which i put the  on cpu. So the  and  Op are also put on cpu.
Profiling result shows  and  Op takes so much time, the former takes about 33%, the latter takes about 43%. Making the  op kernel to multi-thread gain about 10x speedup, relate to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/12246&gt;PR&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/11709&gt;11709&lt;/denchmark-link&gt;
.
Follow the same thought, I want to make the  to be multi-thread.

If there is no duplicate index, we can realize lock-free code which could gain about 10x speedup too.
If there are duplicate indices, use lock which can gain little speedup.

So, I think an extra argument is_duplicated is a good choice to divide the two situation.
Here is some profiling result:
&lt;denchmark-h:h3&gt;tfprof&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;# orignial
ScatterSub                   8589.93MB (65.56%, 32.67%),       45.63ms (62.09%, 43.75%),            0us (45.25%, 0.00%),       45.63ms (62.90%, 45.87%)
# with lock
ScatterSub                   8589.93MB (65.56%, 32.67%),       38.10ms (86.95%, 54.44%),            0us (48.97%, 0.00%),       38.10ms (89.98%, 58.78%)
# without lock
ScatterSub                   8589.93MB (65.56%, 32.67%),        4.22ms (73.05%, 12.58%),            0us (48.86%, 0.00%),        4.22ms (76.99%, 14.63%)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Code&lt;/denchmark-h&gt;

&lt;denchmark-link:https://gist.github.com/nolanliou/c00af5938b2aecfdc5ea1189426b8624&gt;embedding_lookup_sparse&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='nolanliou' date='2017-08-18T19:41:17Z'>
		/CC &lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='nolanliou' date='2017-08-18T20:46:10Z'>
		Since the indices Tensor is much smaller than the values Tensor I think it's possible to check for duplicates cheaply before doing the multi-threaded code, to make it appropriately lock-free. I'd prefer that over an argument we'd need to plumb across the entire interface.
		</comment>
		<comment id='3' author='nolanliou' date='2017-08-20T08:01:29Z'>
		&lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
 Yes, this way just need little change, I got about 2x speedup. If neccessary, I'll send a PR. by the way thanks for your advice.
		</comment>
		<comment id='4' author='nolanliou' date='2017-08-21T08:29:21Z'>
		After some tests, we found another way whose performance is close to the lock-free code. Declare a mutex array with the size equals to the indices size, then use hash to get the mutex and lock every element's update. lock is cheap and contention is expensive.
		</comment>
	</comments>
</bug>