<bug id='35442' author='elzino' open_date='2019-12-27T08:33:15Z' closed_time='2020-01-16T23:06:38Z'>
	<summary>[TF 2.0]ParameterServerStrategy and CentralStorageStrategy doesn't work with Keras when using GPU, even though it works well with CPU.</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I used code in MultiWorkerMirroredStrategy tutorial. And i only changed MultiWorkerMirroredStrategy to ParameterServerStrategy and turned off the eager mode.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2 LTS
TensorFlow installed from (source or binary): pip3
TensorFlow version (use command below): tensorflow 2.0.0 / tensorflow-gpu 2.0.0
Python version: python 3.6.8
CUDA/cuDNN version: CUDA 10.0
GPU model and memory: TITAN Xp

Code to reproduce the issue
&lt;denchmark-code&gt;from __future__ import absolute_import, division, print_function, unicode_literals
import tensorflow_datasets as tfds
import tensorflow as tf

tf.compat.v1.disable_eager_execution()
strategy = tf.distribute.experimental.ParameterServerStrategy()

BUFFER_SIZE = 10000
BATCH_SIZE = 64
NUM_WORKERS = 2

GLOBAL_BATCH_SIZE = BATCH_SIZE * NUM_WORKERS

def scale(image, label):
  image = tf.cast(image, tf.float32)
  image /= 255
  return image, label

datasets, info = tfds.load(name='mnist',
                           with_info=True,
                           as_supervised=True)

train_datasets_unbatched = datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE)
train_datasets = train_datasets_unbatched.batch(GLOBAL_BATCH_SIZE).repeat()

def build_and_compile_cnn_model():
  model = tf.keras.Sequential([
      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),
      tf.keras.layers.MaxPooling2D(),
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(64, activation='relu'),
      tf.keras.layers.Dense(10, activation='softmax')
  ])
  model.compile(
      loss=tf.keras.losses.sparse_categorical_crossentropy,
      optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
      metrics=['accuracy'])
  return model

with strategy.scope():
  multi_worker_model = build_and_compile_cnn_model()
multi_worker_model.fit(x=train_datasets, epochs=3, steps_per_epoch=938)
&lt;/denchmark-code&gt;

Describe the current behavior
Above code works well with CPU. But when using GPU it produces errors like below.
My TF_CONFIG variable was like this ( TF_CONFIG='{"cluster": {"worker": ["localhost:7779"], "ps": ["localhost:7777"]}, "task": {"index": 0, "type": "ps"}}' ).
And it also produces same errors when I tried to apply CentralStorageStrategy.
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "temp.py", line 48, in &lt;module&gt;
    multi_worker_model = build_and_compile_cnn_model()
  File "temp.py", line 35, in build_and_compile_cnn_model
    tf.keras.layers.Dense(10, activation='softmax')
  File "/home/elzino/tf2/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py", line 457, in _method_wrapper
    result = method(self, *args, **kwargs)
  File "/home/elzino/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/sequential.py", line 114, in __init__
    self.add(layer)
  File "/home/elzino/tf2/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py", line 457, in _method_wrapper
    result = method(self, *args, **kwargs)
  File "/home/elzino/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/sequential.py", line 178, in add
    layer(x)
  File "/home/elzino/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 817, in __call__
    self._maybe_build(inputs)
  File "/home/elzino/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 2141, in _maybe_build
    self.build(input_shapes)
  File "/home/elzino/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/convolutional.py", line 165, in build
    dtype=self.dtype)
  File "/home/elzino/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 2311, in __setattr__
    if val.trainable:
  File "/home/elzino/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py", line 477, in trainable
    raise NotImplementedError
NotImplementedError
&lt;/denchmark-code&gt;

Describe the expected behavior
I should work well like when using CPU.
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='elzino' date='2020-01-15T00:33:14Z'>
		Could you provide more details on how you run with CPU and GPU? ParameterServerStrategy is only supported with Estimator API, not Keras.
		</comment>
		<comment id='2' author='elzino' date='2020-01-15T01:34:19Z'>
		I used environment variable CUDA_VISIBLE_DEVICES to switch between CPU and GPU. Like CUDA_VISIBLE_DEVICES= python above_code.py or CUDA_VISIBLE_DEVICES=0,1,2,3,4 python above_code.py.
Is there any plan about when to support Keras? I think it will be easily done because it already works well with CPU. Recently i'm digging into tf distribute codes. Can i contribute to this part if there is any chance?
		</comment>
		<comment id='3' author='elzino' date='2020-01-15T04:35:39Z'>
		I found out the reason. When using ParameterServerStrategy, it wraps the value with  AggregatingVariable but AggregatingVariable doesn't have the trainable property. I fixed this issue in above PR. Please check that.
&lt;denchmark-h:h3&gt;More detail:&lt;/denchmark-h&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/distribute/parameter_server_strategy.py#L392&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/distribute/parameter_server_strategy.py#L392&lt;/denchmark-link&gt;

When using CPU, self._num_replicas_in_sync was not more than one. So it just created value without the AggregatingVariable wrapper. But when using GPU, It wrapped the value with AggregatingVariable class, and the errors came up.
		</comment>
		<comment id='4' author='elzino' date='2020-01-15T04:54:38Z'>
		also related to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/35017&gt;#35017&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='elzino' date='2020-01-16T23:06:40Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35442&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35442&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='elzino' date='2020-01-17T00:20:19Z'>
		Thanks for triage the issue. We're actively working on PS strategy and there're still some design discussion happening. Before that, you can use CentralStorageStrategy if you only have one worker.
		</comment>
		<comment id='7' author='elzino' date='2020-01-17T01:31:47Z'>
		Thank you for letting me know. Actually, I'm working to make &lt;denchmark-link:https://github.com/snuspl/parallax&gt;parallax&lt;/denchmark-link&gt;
 compatible with tf 2.0. So i need to use more than one worker. But still thanks for your comment and hope your work goes well!
		</comment>
	</comments>
</bug>