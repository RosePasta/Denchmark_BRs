<bug id='37745' author='PINTO0309' open_date='2020-03-20T10:45:22Z' closed_time='2020-04-12T04:55:20Z'>
	<summary>[EfficientDet] ValueError: Cannot find the Placeholder op that is an input to the ReadVariableOp</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu18.04 x86_64, CUDA10
TensorFlow installed from (source or binary): binary
TensorFlow version (or github SHA if from source): tf_nightly-2.2.0.dev20200319
Python version: 3.6

Command used to run the converter or code if youâ€™re using the Python API
I am trying to perform Weight Quantization using converter from saved_model, but I am suffering from the problem that the INPUT OP of saved_model is not recognized correctly by converter. You can confirm that the INPUT OP is included in the saved_model in the following steps, but an error will occur when the conversion is performed.

Download the EfficientDet google/automl trained checkpoint from the link here. This is the Google official release EfficientDet model. Then unzip the downloaded tar.gz file to any location.
Clone the google / automl repository.

&lt;denchmark-code&gt;$ sudo pip3 install tf-nightly
$ git clone https://github.com/google/automl.git
$ cd automl/efficientdet
&lt;/denchmark-code&gt;


The following script freezes the downloaded checkpoint.

$ python3 model_inspect.py \
  --model_name=efficientdet-d0 \
  --delete_logdir=False \
  --freeze=True \
  --runmode=freeze \
  --input_image_size=512 \
  --ckpt_path=${HOME}/Downloads/efficientdet-d0 \
  --logdir=${HOME}/Downloads/efficientdet-d0/log \
  --export_ckpt=${HOME}/Downloads/efficientdet-d0/export \
  --threads=4

Use the following Python script to extract the layer name of the INPUT layer. This indicates that the INPUT layer name of the model is "input".

### tf-nightly 2.2.0-dev20200319

import tensorflow as tf
import os
from tensorflow.python import ops

def get_graph_def_from_file(graph_filepath):
  tf.compat.v1.reset_default_graph()
  with ops.Graph().as_default():
    with tf.compat.v1.gfile.GFile(graph_filepath, 'rb') as f:
      graph_def = tf.compat.v1.GraphDef()
      graph_def.ParseFromString(f.read())
      return graph_def

# Look up the name of the placeholder for the input node
graph_def=get_graph_def_from_file('./efficientdet-d0_train.pb')
input_name=""
for node in graph_def.node:
    if node.op=='Placeholder':
        print("##### efficientdet-d0_train - Input Node Name #####", node.name) # this will be the input node
        input_name=node.name

The following command converts efficientdet-d0_train.pb and save_model is created in  saved_model folder.

### tf-nightly 2.2.0-dev20200319

import tensorflow as tf
import os
import shutil
from tensorflow.python.saved_model import tag_constants
from tensorflow.python import ops

def get_graph_def_from_file(graph_filepath):
  tf.compat.v1.reset_default_graph()
  with ops.Graph().as_default():
    with tf.compat.v1.gfile.GFile(graph_filepath, 'rb') as f:
      graph_def = tf.compat.v1.GraphDef()
      graph_def.ParseFromString(f.read())
      return graph_def

def convert_graph_def_to_saved_model(export_dir, graph_filepath, input_name, outputs):
  graph_def = get_graph_def_from_file(graph_filepath)
  with tf.compat.v1.Session(graph=tf.Graph()) as session:
    tf.import_graph_def(graph_def, name='')
    tf.compat.v1.saved_model.simple_save(
        session,
        export_dir,# change input_image to node.name if you know the name
        inputs={input_name: session.graph.get_tensor_by_name('{}:0'.format(node.name))
            for node in graph_def.node if node.op=='Placeholder'},
        outputs={t.rstrip(":0"):session.graph.get_tensor_by_name(t) for t in outputs}
    )
    print('Optimized graph converted to SavedModel!')

# Look up the name of the placeholder for the input node
graph_def=get_graph_def_from_file('./efficientdet-d0_train.pb')
input_name="input"
outputs = ['class_net/class-predict/BiasAdd:0',
           'class_net/class-predict_1/BiasAdd:0',
           'class_net/class-predict_2/BiasAdd:0',
           'class_net/class-predict_3/BiasAdd:0',
           'class_net/class-predict_4/BiasAdd:0',
           'box_net/box-predict/BiasAdd:0',
           'box_net/box-predict_1/BiasAdd:0',
           'box_net/box-predict_2/BiasAdd:0',
           'box_net/box-predict_3/BiasAdd:0',
           'box_net/box-predict_4/BiasAdd:0']
# convert this to a TF Serving compatible mode
shutil.rmtree('./saved_model', ignore_errors=True)
convert_graph_def_to_saved_model('./saved_model', './efficientdet-d0_train.pb', input_name, outputs)

Confirm that saved_model is generated correctly by the following command.

&lt;denchmark-code&gt;$ saved_model_cli show --dir ./saved_model --all

MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:

signature_def['serving_default']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['input'] tensor_info:
        dtype: DT_FLOAT
        shape: (1, 512, 512, 3)
        name: input:0
  The given SavedModel SignatureDef contains the following output(s):
    outputs['box_net/box-predict/BiasAdd'] tensor_info:
        dtype: DT_FLOAT
        shape: (1, 64, 64, 36)
        name: box_net/box-predict/BiasAdd:0
    outputs['box_net/box-predict_1/BiasAdd'] tensor_info:
        dtype: DT_FLOAT
        shape: (1, 32, 32, 36)
        name: box_net/box-predict_1/BiasAdd:0
    outputs['box_net/box-predict_2/BiasAdd'] tensor_info:
        dtype: DT_FLOAT
        shape: (1, 16, 16, 36)
        name: box_net/box-predict_2/BiasAdd:0
    outputs['box_net/box-predict_3/BiasAdd'] tensor_info:
        dtype: DT_FLOAT
        shape: (1, 8, 8, 36)
        name: box_net/box-predict_3/BiasAdd:0
    outputs['box_net/box-predict_4/BiasAdd'] tensor_info:
        dtype: DT_FLOAT
        shape: (1, 4, 4, 36)
        name: box_net/box-predict_4/BiasAdd:0
    outputs['class_net/class-predict/BiasAdd'] tensor_info:
        dtype: DT_FLOAT
        shape: (1, 64, 64, 810)
        name: class_net/class-predict/BiasAdd:0
    outputs['class_net/class-predict_1/BiasAdd'] tensor_info:
        dtype: DT_FLOAT
        shape: (1, 32, 32, 810)
        name: class_net/class-predict_1/BiasAdd:0
    outputs['class_net/class-predict_2/BiasAdd'] tensor_info:
        dtype: DT_FLOAT
        shape: (1, 16, 16, 810)
        name: class_net/class-predict_2/BiasAdd:0
    outputs['class_net/class-predict_3/BiasAdd'] tensor_info:
        dtype: DT_FLOAT
        shape: (1, 8, 8, 810)
        name: class_net/class-predict_3/BiasAdd:0
    outputs['class_net/class-predict_4/BiasAdd'] tensor_info:
        dtype: DT_FLOAT
        shape: (1, 4, 4, 810)
        name: class_net/class-predict_4/BiasAdd:0
  Method name is: tensorflow/serving/predict
&lt;/denchmark-code&gt;


Finally, an error occurs when performing Weight Quantization with the following Python script.

### tf_nightly-2.2.0.dev20200319
import tensorflow as tf

# Weight Quantization - Input/Output=float32
converter = tf.lite.TFLiteConverter.from_saved_model('./saved_model')
#converter.experimental_new_converter = True
converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
converter.allow_custom_ops = True
tflite_quant_model = converter.convert()
with open('./efficientdet-d0_train.tflite', 'wb') as w:
    w.write(tflite_quant_model)
print("Weight Quantization complete! - efficientdet-d0_train.tflite")
The following error occurs even though the "input" Placeholder does exist in the last check procedure.
&lt;denchmark-code&gt;ValueError: Cannot find the Placeholder op that is an input to the ReadVariableOp.
&lt;/denchmark-code&gt;

The output from the converter invocation
2020-03-20 17:36:18.835931: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:
2020-03-20 17:36:18.836031: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:
2020-03-20 17:36:18.836056: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.
2020-03-20 17:36:20.375954: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-03-20 17:36:20.388477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-20 17:36:20.388790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1070 with Max-Q Design computeCapability: 6.1
coreClock: 1.2655GHz coreCount: 16 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 238.66GiB/s
2020-03-20 17:36:20.388916: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:
2020-03-20 17:36:20.389000: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:
2020-03-20 17:36:20.389063: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:
2020-03-20 17:36:20.389126: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:
2020-03-20 17:36:20.389185: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:
2020-03-20 17:36:20.389246: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:
2020-03-20 17:36:20.391780: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-03-20 17:36:20.391813: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1592] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-03-20 17:36:20.392010: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-03-20 17:36:20.414641: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
2020-03-20 17:36:20.415705: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xc1805f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-03-20 17:36:20.415744: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-03-20 17:36:20.460041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-20 17:36:20.460435: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xc216420 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-03-20 17:36:20.460451: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1070 with Max-Q Design, Compute Capability 6.1
2020-03-20 17:36:20.460530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-03-20 17:36:20.460538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      
2020-03-20 17:36:21.550114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-20 17:36:21.550409: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count &gt;= 8, compute capability &gt;= 0.0): 1
2020-03-20 17:36:21.550490: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-03-20 17:36:21.551093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-20 17:36:21.551353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1070 with Max-Q Design computeCapability: 6.1
coreClock: 1.2655GHz coreCount: 16 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 238.66GiB/s
2020-03-20 17:36:21.551456: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:
2020-03-20 17:36:21.551525: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:
2020-03-20 17:36:21.551587: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:
2020-03-20 17:36:21.551648: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:
2020-03-20 17:36:21.551710: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:
2020-03-20 17:36:21.551772: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:
2020-03-20 17:36:21.551790: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-03-20 17:36:21.551796: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1592] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-03-20 17:36:21.551808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-03-20 17:36:21.551812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-03-20 17:36:21.551818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-03-20 17:36:21.582079: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize
2020-03-20 17:36:21.582108: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.002ms.
2020-03-20 17:36:21.582113: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.
Traceback (most recent call last):
  File "03_weight_quantization.py", line 12, in &lt;module&gt;
    tflite_quant_model = converter.convert()
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py", line 423, in convert
    self._funcs[0], lower_control_flow=False)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/convert_to_constants.py", line 506, in convert_variables_to_constants_v2
    raise ValueError("Cannot find the Placeholder op that is an input "
ValueError: Cannot find the Placeholder op that is an input to the ReadVariableOp.
Also, please include a link to the saved model or GraphDef
&lt;denchmark-code&gt;https://storage.googleapis.com/cloud-tpu-checkpoints/efficientdet/coco/efficientdet-d0.tar.gz
&lt;/denchmark-code&gt;

Failure details
The error occurs even though the "input" Placeholder does exist in the last check procedure.
Any other info / logs
The .pb, checkpoint, and script I used to check are committed below.
https://github.com/PINTO0309/PINTO_model_zoo/tree/master/18_EfficientDet/01_float32
	</description>
	<comments>
		<comment id='1' author='PINTO0309' date='2020-03-21T03:22:09Z'>
		Hi  &lt;denchmark-link:https://github.com/PINTO0309&gt;@PINTO0309&lt;/denchmark-link&gt;
 , I follow the steps to create the frozen graph .pb, also I have the same problem for create the .tflite.
On the other hand, do you have any way to make inference with this frozen .pb ? , i was following the steps from the classic &lt;denchmark-link:https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb&gt;https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb&lt;/denchmark-link&gt;
 but I have this problem in the end trying to make some inference.
FailedPreconditionError:  Error while reading resource variable fpn_cells/cell_2/fnode3/WSM_1_load_168070 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/fpn_cells/cell_2/fnode3/WSM_1_load_168070/N10tensorflow3VarE does not exist.
	 [[node fpn_cells/cell_2/fnode3/Relu_1/ReadVariableOp (defined at :3) ]] [Op:__inference_pruned_179631]

Function call stack:
pruned
Thanks &lt;denchmark-link:https://github.com/PINTO0309&gt;@PINTO0309&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='PINTO0309' date='2020-03-21T04:18:03Z'>
		&lt;denchmark-link:https://github.com/stanlee321&gt;@stanlee321&lt;/denchmark-link&gt;

Although it is not directly related to this issue, it has succeeded until I created a .pb of EfficientDet and confirmed the operation below. However, it is not an official Google model. If you stick with the official model, ignore it. Unfortunately, I haven't tested with the official model .pb so far. My goal is to infer fast on low-spec devices such as RaspberryPi. I hope that my information is useful.
&lt;denchmark-h:h2&gt;Sample repository&lt;/denchmark-h&gt;

https://github.com/xuannianz/EfficientDet.git
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;


Ubuntu18.04 x86_64
Pascal-VOC 2012 Dataset Retraining
EfficientDet D0
Threshould 0.70

&lt;denchmark-h:h2&gt;Demo result&lt;/denchmark-h&gt;


&lt;denchmark-link:https://user-images.githubusercontent.com/33194443/77219075-7f576100-6b75-11ea-910a-60599dec4efd.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='PINTO0309' date='2020-03-21T22:33:54Z'>
		Thank you &lt;denchmark-link:https://github.com/PINTO0309&gt;@PINTO0309&lt;/denchmark-link&gt;
 !!
		</comment>
		<comment id='4' author='PINTO0309' date='2020-04-12T04:55:20Z'>
		The problem that Placeholder is not detected has been solved by tf-nightly==2.2.0-dev20200411. So this issue will be closed.
The following is an error log that is displayed when issue is resolved and weight quantification is performed, but it has nothing to do with this issue.
Traceback (most recent call last):
  File "05_weight_quantization.py", line 13, in &lt;module&gt;
    tflite_quant_model = converter.convert()
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py", line 621, in convert
    self._funcs[0], lower_control_flow=False))
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/convert_to_constants.py", line 706, in convert_variables_to_constants_v2_as_graph
    func, lower_control_flow, aggressive_inlining)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/convert_to_constants.py", line 512, in _convert_variables_to_constants_v2_impl
    func.prune([], [identity_node.name])()[0])
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py", line 1605, in __call__
    return self._call_impl(args, kwargs)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py", line 1645, in _call_impl
    return self._call_flat(args, self.captured_inputs, cancellation_manager)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py", line 1746, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py", line 598, in call
    ctx=ctx)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.FailedPreconditionError:  Attempting to use uninitialized value fpn_cells/cell_0/fnode3/resample_0_0_8/conv2d/kernel
	 [[node Identity (defined at /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py:621) ]] [Op:__inference_pruned_10971]

Function call stack:
pruned
		</comment>
		<comment id='5' author='PINTO0309' date='2020-04-12T04:55:22Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37745&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37745&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>