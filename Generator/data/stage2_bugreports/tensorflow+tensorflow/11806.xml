<bug id='11806' author='mavenlin' open_date='2017-07-27T08:30:47Z' closed_time='2019-08-06T22:03:32Z'>
	<summary>sparse ClusterSpec fails when using tf.cond</summary>
	<description>
Here's the minimal code to reproduce.
on machine 1 and machine 2
import sys
import tensorflow as tf

cluster_spec = tf.train.ClusterSpec({
  "a": { 0: "machine1:8000" },
  "b": { 0: "machine2:8001" },
})
jobname = sys.argv[1]
taskid = int(sys.argv[2])
server = tf.train.Server(cluster_spec, jobname, taskid)

with tf.device("/job:a/task:0/cpu:0"):
  queue = tf.FIFOQueue(
    capacity=100, dtypes=[tf.int64],
    shapes=[[]], shared_name="a_queue", name="a_queue")

if jobname == "a" and taskid == 0:
  enqueue_op = queue.enqueue(10)
  sess = tf.Session(server.target)
  while True:
    sess.run(enqueue_op)
else:
  dequeue_op = queue.dequeue()
  sess = tf.Session(server.target)
  while True:
    print(sess.run(dequeue_op))
on machine 3:
import sys
import tensorflow as tf

cluster_spec = tf.train.ClusterSpec({
  "a": { 0: "machine1:8000" },
  "b": { 1: "machine3:8001" },
})
jobname = sys.argv[1]
taskid = int(sys.argv[2])
server = tf.train.Server(cluster_spec, jobname, taskid)

with tf.device("/job:a/task:0/cpu:0"):
  queue = tf.FIFOQueue(
    capacity=100, dtypes=[tf.int64],
    shapes=[[]], shared_name="a_queue", name="a_queue")

if jobname == "a" and taskid == 0:
  enqueue_op = queue.enqueue(10)
  sess = tf.Session(server.target)
  while True:
    sess.run(enqueue_op)
else:
  with tf.device("/job:b/task:1"):
    out = queue.dequeue()
    queue_b = tf.FIFOQueue(capacity=100, dtypes=[tf.int64], shapes=[[]], name="b_queue")
    # 1.
    # enq = queue_b.enqueue(out)
    # no_op = tf.no_op()
    # out = tf.cond(tf.equal(out, 10), lambda: enq, lambda: no_op)
    # 2.
    out = tf.cond(tf.equal(out, 10), lambda: queue_b.enqueue(out), lambda: tf.no_op())

  sess = tf.Session(server.target)
  while True:
    print(sess.run(out))
On machine3, it crashes complaining
tensorflow.python.framework.errors_impl.InternalError: No worker known as /job:b/replica:0/task:1
	 [[Node: cond/pred_id_S5 = _HostRecv[client_terminated=false, recv_device="/job:a/replica:0/task:0/cpu:0", send_device="/job:b/replica:0/task:1/gpu:0", send_device_incarnation=720279685140440577, tensor_name="edge_8_cond/pred_id", tensor_type=DT_BOOL, _device="/job:a/replica:0/task:0/cpu:0"]()]]
There is no problem if the true_fn and false_fn just returns a already constructed op, like in the commented code.
	</description>
	<comments>
		<comment id='1' author='mavenlin' date='2017-07-28T18:17:50Z'>
		Can &lt;denchmark-link:https://github.com/yaroslavvb/stuff/tree/master/mavelin&gt;reproduce&lt;/denchmark-link&gt;
 on my laptop, maybe &lt;denchmark-link:https://github.com/saeta&gt;@saeta&lt;/denchmark-link&gt;
 has an idea? Essentially a worker named  is complaining that it doesn't know about worker named 
(ps: generated graph also has unsatisfiable colocation constraints, but that issue is unrelated to the error here)
		</comment>
		<comment id='2' author='mavenlin' date='2017-07-28T20:54:13Z'>
		Brennan &lt;denchmark-link:https://github.com/saeta&gt;@saeta&lt;/denchmark-link&gt;
 , could you please take a look?
		</comment>
		<comment id='3' author='mavenlin' date='2019-08-06T22:03:32Z'>
		This is a stale issue. Please check the issue with latest TensorFlow. If the issue still persists in the newer version of TF, please feel free to reopen it by providing details about the issue and a standalone code to reproduce the issue. Thanks!
		</comment>
		<comment id='4' author='mavenlin' date='2019-08-06T22:03:34Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=11806&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=11806&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>