<bug id='7187' author='Randl' open_date='2017-02-01T14:29:46Z' closed_time='2017-05-10T02:21:47Z'>
	<summary>hclhkbu dlbench shows Tensorflow is slower than other frameworks</summary>
	<description>
Based on &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/7065#issuecomment-276648478&gt;#7065 (comment)&lt;/denchmark-link&gt;

Recent update of &lt;denchmark-link:https://arxiv.org/abs/1608.07249&gt;Benchmarking State-of-the-Art Deep Learning Software Tools&lt;/denchmark-link&gt;
 (by &lt;denchmark-link:https://github.com/shyhuai&gt;@shyhuai&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/FreemanX&gt;@FreemanX&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/xiaowec&gt;@xiaowec&lt;/denchmark-link&gt;
 , if I got it right) shows some performance issues. For example, (see table 7)  is significantly (~ 10 times) slower in TF than in other frameworks, an it's even slower at GTX 980 than at GTX 1080. Also, ResNet-50 is ~5.5 times faster in MXNet. Those are most significant differences.
In addition, LSTM is around 3 times faster in CNTK, and ResNet-56 is twice faster in MXNet.
Version used was TensorFlow 0.11 (commit &lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/47dd089db3cd16d76595791b2e8483e2fd0b0a25&gt;47dd089&lt;/denchmark-link&gt;
) with CUDA 8.0 and cuDNN 5.1
cc &lt;denchmark-link:https://github.com/yaroslavvb&gt;@yaroslavvb&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/annarev&gt;@annarev&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='Randl' date='2017-02-01T14:36:14Z'>
		PS, the code is at &lt;denchmark-link:https://github.com/hclhkbu/dlbench/&gt;https://github.com/hclhkbu/dlbench/&lt;/denchmark-link&gt;
, and the authors are quite prompt at updating it. I found an issue with CIFAR multi-GPU benchmark and it was merged within a day -- &lt;denchmark-link:https://github.com/hclhkbu/dlbench/pull/4&gt;hclhkbu/dlbench#4&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='Randl' date='2017-02-01T15:16:52Z'>
		assigning to &lt;denchmark-link:https://github.com/annarev&gt;@annarev&lt;/denchmark-link&gt;
 for further triage
		</comment>
		<comment id='3' author='Randl' date='2017-02-09T02:42:18Z'>
		Hi Randl,
Thank you for the information.  We are working through the benchmark and days away from publishing a performance guide.  There are a couple things that I noticed at a glance with the code they are using.  Before I mention them I want to stress that this is code that was published in the TensorFlow repo and I am not shifting blame.  Now some things to look for:

Loading data with feed_dict as such:  sess.run([train_op, average_op], feed_dict=feed_dict).
This is almost the slowest possible approach and is often used in examples.
Allowing the preprocessing (of say images) to end up on the GPU.  This happens if it is not placed on the CPU.  This can result in 6x+ increased performance.

There are other little tweaks but with just the two "tricks" above I suspect a few of the benchmarks you listed would improve dramatically.  There are other tweaks but using TF 1.0+ and the above techniques would help a lot.
I will leave this open until I can post some numbers and possibly get someone to post a PR to the benchmark project.
Thank you again for following up and opening a new issue.
		</comment>
		<comment id='4' author='Randl' date='2017-02-09T03:23:26Z'>
		&lt;denchmark-link:https://github.com/tfboyd&gt;@tfboyd&lt;/denchmark-link&gt;
 great to know you guys are looking at it! I think their multi-GPU resnet example is especially intriguing. It's basically fork of official cifar multi-GPU example which I think was originally made by &lt;denchmark-link:https://github.com/shlens&gt;@shlens&lt;/denchmark-link&gt;
 and doesn't have obvious problems like feed_dict, yet there's a large performance gap with competitors.
		</comment>
		<comment id='5' author='Randl' date='2017-03-08T22:31:07Z'>
		How much performance hit is there in that tf stores its  filter weights differently from what gets passed to Cudnn, requiring a dimension reordering on calls to fprop and bprop?
		</comment>
		<comment id='6' author='Randl' date='2017-03-09T00:12:46Z'>
		It should be a "pretty big" hit, GPU things should should use NCHW because of CuDNN -- &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/b10f50ff15944badb7262a207f6628dfa52d6a9d/tensorflow/docs_src/performance/performance_guide.md&gt;https://github.com/tensorflow/tensorflow/blob/b10f50ff15944badb7262a207f6628dfa52d6a9d/tensorflow/docs_src/performance/performance_guide.md&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='Randl' date='2017-03-09T00:16:38Z'>
		&lt;denchmark-link:https://github.com/yaroslavvb&gt;@yaroslavvb&lt;/denchmark-link&gt;
 To be clear I am referring to how the filter weights are stored, not how the input or output data is represented. I believe that for weights cuDNN supports only one format and that TF must do a live conversion.
		</comment>
		<comment id='8' author='Randl' date='2017-03-09T00:23:09Z'>
		&lt;denchmark-link:https://github.com/cancan101&gt;@cancan101&lt;/denchmark-link&gt;
 I'm not aware of any transposes done on filter weights between TF and CuDNN. Because filters are learned, it's fine to have a layout mismatch between TF and CuDNN, the training will adapt to the wrong layout.
		</comment>
		<comment id='9' author='Randl' date='2017-03-09T00:26:54Z'>
		What about: 


tensorflow/tensorflow/core/kernels/conv_ops.cc


        Lines 610 to 612
      in
      0be8143






 functor::TransformFilter&lt;GPUDevice, T, int, 4&gt;()( 



     ctx-&gt;eigen_device&lt;GPUDevice&gt;(), To32Bit(filter.tensor&lt;T, 4&gt;()), 



 To32Bit(transformed_filter.tensor&lt;T, 4&gt;())); 





See also 


tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc


        Lines 335 to 338
      in
      33e88f6






 // A GPU helper function that converts TensorFlow filter format to Cudnn filter 



 // format. 



 template &lt;typename T, int NDIMS&gt; 



 struct TransformFilter&lt;GPUDevice, T, int, NDIMS&gt; { 




:

// A GPU helper function that converts TensorFlow filter format to Cudnn filter
// format.

		</comment>
		<comment id='10' author='Randl' date='2017-03-09T00:30:39Z'>
		Nice find .... cc &lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
 our GPU perf expert
		</comment>
		<comment id='11' author='Randl' date='2017-03-09T00:39:40Z'>
		&lt;denchmark-link:https://github.com/yaroslavvb&gt;@yaroslavvb&lt;/denchmark-link&gt;
 Also even if you can just treat the dimensions of the filter weight matrix as opaque during training, inference may happen on another device, etc that isn't GPU based so the weights must be converted somewhere.
The &lt;denchmark-link:https://www.tensorflow.org/extend/tool_developers/#weight_formats&gt;docs for TF&lt;/denchmark-link&gt;
 state:

The ordering of convolution weight values is often tricky to deal with when converting between different frameworks. In TensorFlow, the filter weights for the Conv2D operation are stored on the second input, and are expected to be in the order [filter_height, filter_width, input_depth, output_depth], where filter_count increasing by one means moving to an adjacent value in memory.

that being said, pending answers to the perf question, I am thinking of opening a feature request to add a weight_format along the lines of data_format to Conv2D. This will allow various kernel implementers to store weights more efficiently (and mark them as such for down the road conversions).
In addition to cuDNN, BNNS (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/3001&gt;#3001&lt;/denchmark-link&gt;
) also uses that same format.
		</comment>
		<comment id='12' author='Randl' date='2017-03-29T20:04:56Z'>
		Jumping onto this thread because &lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
 and I tuned the perf in the past... As long as you use NCHW order, cudnn speed should basically be good. Another thing you might want to check is the workspace size: usually if you pass in a workspace size that is above 128MB, cudnn can choose a faster path. If you pass in something under 16, speed may be affected because winograd (usually the fastest one) usually need 64 or so.
The benchmark is a bit unfair to TensorFlow (and Caffe, fwiw), mainly because one will need to choose exhaustive search with a big enough workspace in cudnn, and that is sometimes overlooked by benchmarkers. For example, TensorFlow uses cudnnGet* instead of cudnnFind*:



tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc


         Line 1844
      in
      904edee






 status = wrap::cudnnGetConvolutionForwardAlgorithm( 





which is a fine approach in most cases with maybe 5-10% perf difference in corner cases.
		</comment>
		<comment id='13' author='Randl' date='2017-03-29T20:09:47Z'>
		&lt;denchmark-link:https://github.com/Yangqing&gt;@Yangqing&lt;/denchmark-link&gt;
 what are your thoughts on &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/8287&gt;#8287&lt;/denchmark-link&gt;
, namely that regardless of the input dimension ordering, TF much convert the  on each forward / backward pass.
		</comment>
		<comment id='14' author='Randl' date='2017-03-29T21:47:56Z'>
		Nowadays TensorFlow does autotuning by itself. For example:

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/core/kernels/conv_ops.cc#L657&gt;https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/core/kernels/conv_ops.cc#L657&lt;/denchmark-link&gt;


For most conv operations, filters transformation is a small overhead
compared to the actual computation. However, this might not be true in
extreme cases. So yes, eventually filter_format support might be needed
somewhere down the road.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Wed, Mar 29, 2017 at 1:10 PM, Alex Rothberg ***@***.***&gt; wrote:
 @Yangqing &lt;https://github.com/Yangqing&gt; what are your thoughts on #8287
 &lt;#8287&gt;, namely that
 regardless of the input dimension ordering, TF much convert the *weights*
 on each forward / backward pass.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#7187 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/APAgThpyAewgYIZy60TcGL2Brs6qLRUcks5rqrrTgaJpZM4Lz7Z1&gt;
 .



		</comment>
		<comment id='15' author='Randl' date='2017-03-30T01:26:08Z'>
		&lt;denchmark-link:https://github.com/cancan101&gt;@cancan101&lt;/denchmark-link&gt;
 I haven't benchmarked but I believe that the transform overhead is minimal. cudnnTransformTensor should be having a very high throughput. I like &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/8287&gt;#8287&lt;/denchmark-link&gt;
 though, especially in inference time when you have a small batch (and computation becomes much quicker). For training the overhead is negligible like &lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
 said.
		</comment>
		<comment id='16' author='Randl' date='2017-03-30T02:12:58Z'>
		Obvious to me that they choose to force all through tensors and still didn't understand that each "class" of will have a different assert needed. The example given was only one of many possibilities. If training tensor example, or if input buffer there should be a different method more appropriate for the "class" being used
		</comment>
		<comment id='17' author='Randl' date='2017-03-30T02:29:11Z'>
		&lt;denchmark-link:https://github.com/Yangqing&gt;@Yangqing&lt;/denchmark-link&gt;
 What exactly is the "workspace size" that you are referring to, and what is it defaulted to? I see some stuff &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/904edee4456a61d50d5b1ffe9858a7772acc423e/tensorflow/stream_executor/cuda/cuda_dnn.cc#L1832-L1833&gt;here&lt;/denchmark-link&gt;
, but don't really see it documented. Further how often / when is the optimal algorithm chosen?  Is  what you are talking about? The default is 4GB which seems &gt;&gt; size you listed.
Might be cool to offer some sort of verbose logging about the tuning options along the lines of &lt;denchmark-link:https://github.com/szagoruyko/cudnn.torch/blob/master/README.md#modes&gt;torch7&lt;/denchmark-link&gt;
.
Also why does TensorFlow use cudnnGet* instead of cudnnFind* (I would think specifically the cudnnFind...Ex variety)?
		</comment>
		<comment id='18' author='Randl' date='2017-03-30T02:46:57Z'>
		I have not had a chance to test the latest nightly yet. So previous benchmark performance can't be relative.
		</comment>
		<comment id='19' author='Randl' date='2017-03-30T02:47:32Z'>
		Relavent
		</comment>
		<comment id='20' author='Randl' date='2017-04-19T22:50:29Z'>
		So back to the initial issue, any updates on the performance differences observed on (single) core in TF vs (py)Torch?
		</comment>
		<comment id='21' author='Randl' date='2017-04-19T23:54:12Z'>
		Benchmark results (TensorFlow not comparison to other platforms) and code should be posted late next week, which will help get us all on the same set of scripts.  I have done some PyTorch testing with real data on image-classification models and did not see a significant difference in my very informal testing.  I found TensorFlow to be slightly faster with real data, but am not a PyTorch expert.  I did notice that the VGG16 model loss goes to NaN after a few steps, which then results in a speedup but I was not sure if I did something wrong or if it was the model.  Beyond that PyTorch had some issues with their input pipeline with models that process images very quickly, e.g. something like Alexnet with batch size of 512, but that did not seem like a huge deal.  If there is something specific let me know, I did not see pytorch referenced in this thread and only recall the VGG16 thread.  If it it looks like it would lead to a TensorFlow speedup then it is a worth while investigation for the community.
		</comment>
		<comment id='22' author='Randl' date='2017-04-20T00:03:56Z'>
		I was playing around earlier today with my implementation of Wide ResNets in TF (using NCHW and fused batch norm, with  but verifying that  showed near 100% GPU usage throughout) and comparing to a PyTorch implementation (&lt;denchmark-link:https://github.com/xternalz/WideResNet-pytorch&gt;https://github.com/xternalz/WideResNet-pytorch&lt;/denchmark-link&gt;
).
I saw batches taking something like 55% as long in the PyTorch version as in my TF version – ~155 ms for the PyTorch impl v ~285 ms in my TF impl on a p2.xlarge on AWS, using WRN-16-4. I'll post my code.
		</comment>
		<comment id='23' author='Randl' date='2017-04-20T00:58:21Z'>
		&lt;denchmark-link:https://github.com/taion&gt;@taion&lt;/denchmark-link&gt;
 That would be great.  If all goes well I would like to start focusing on other types of models in about a week.
		</comment>
		<comment id='24' author='Randl' date='2017-04-20T02:14:18Z'>
		I've described the situation in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/9322&gt;#9322&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='25' author='Randl' date='2017-05-10T02:21:47Z'>
		I am marking this closed.  We may add CIFAR-10 to the performance script but for training ImageNet the numbers between TensorFlow and other platforms are very similar.  The benchmarks cover K80s on Amazon and Google as well as P100s via the DGX-1.
		</comment>
		<comment id='26' author='Randl' date='2017-05-10T05:12:04Z'>
		&lt;denchmark-link:https://github.com/tfboyd&gt;@tfboyd&lt;/denchmark-link&gt;
 I'm still a bit worried about this strange AlexNet-R (AlexNet on CIFAR-10) behavior. The fact that GTX 980 is faster GTX 1080 (and overall low speed) supposes some bug in either TF or implementation of network. Will appreciate if you take a look one more time.
		</comment>
		<comment id='27' author='Randl' date='2017-05-10T14:49:35Z'>
		&lt;denchmark-link:https://github.com/Randl&gt;@Randl&lt;/denchmark-link&gt;
   I have not tested it but if they wrapped this in a with device CPU I suspect it would make a huge difference based on helping a few people recently.
images, labels = cifar10_input.inputs(False, FLAGS.data_dir, FLAGS.batch_size)
It would make a big difference in all of the tests that used CIFAR that did not place the input pipeline on the CPU.  The AlexNet numbers on the benchmarks we published looked good.  Wrapping the input pipeline in the with device CPU has resulted in huge (as in 4x+) performance increases for people.  And on the positive side if you use Estimators it does this for you by default.  You give it the input function and it makes sure it is placed on the CPU.  Since they just modified the CIFAR example, which did not wrap the input pipeline in the CPU (our bad for sure), I am pretty confident of the performance boost.  On another issue someone doubted the boost and was shocked they got I think 4x or more increase in images per second.
		</comment>
		<comment id='28' author='Randl' date='2017-05-10T20:56:01Z'>
		Still sounds unlikely to have lower speed on 1080 compared to 980
		</comment>
		<comment id='29' author='Randl' date='2017-05-10T21:12:17Z'>
		&lt;denchmark-link:https://github.com/Randl&gt;@Randl&lt;/denchmark-link&gt;
 where are the AlexNet 980/1080 numbers? Note that AlexNet has become "the ugly stepchild" of model benchmarking since it has low compute to data-transfer ratio so it's hard to get high utilization on newer hardware
		</comment>
		<comment id='30' author='Randl' date='2017-05-10T21:24:25Z'>
		It is a one line change.  It just takes time to figure out how to run their test so I can verify it works as expected.
Until that is fixed, guessing why one GPU is slower than another is guessing.  Hopefully I sort it out in the next 48 hours or so.  There may be other minor issues but I am not concerned if their model is correct or everything is optimal.  Just like everyone else in the community, I want to make sure they have the basics so they can maximize whatever they are doing.
		</comment>
		<comment id='31' author='Randl' date='2017-05-10T21:25:56Z'>
		Tensorflow:
980 - 0.227
1080 - 0.317
K80 - 0.385
At the same time for Caffe:
980 - 0.041
1080 - 0.027
K80 - 0.137
		</comment>
		<comment id='32' author='Randl' date='2017-05-11T14:06:07Z'>
		&lt;denchmark-link:https://github.com/Randl&gt;@Randl&lt;/denchmark-link&gt;

one line change to move input processing to CPU.  This would also impact the resnet test if I was reading it correctly.  I have no idea if the AlexNet implementations are the same between the platforms or if anything else is different.  My point is make sure to put your input processing on CPU it makes a difference.  Using your number I reduced the time by 83%.  On my testing I was getting 0.144 per step (I love the GTX 1080 but there are a lot of variations and the last dlbench was on TF .11 which was a long time ago), which is a reduction of 63%.  Put another way I went from 7K images/sec to 19K images per second in my apples-to-apples test (my numbers before and after) for a speedup of 2.7x.
It is possible your numbers are from a different batch size than the default 1024.
      with tf.device('/cpu:0'):
        images, labels = cifar10_input.inputs(False, FLAGS.data_dir, FLAGS.batch_size)
python alexnet_cifar10.py --data_dir=/home/toby/CIFAR-10/cifar-10-batches-bin
2017-05-11 06:51:40.045511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-05-11 06:51:40.045815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:907] Found device 0 with properties: 
name: GeForce GTX 1080
major: 6 minor: 1 memoryClockRate (GHz) 1.835
pciBusID 0000:01:00.0
Total memory: 7.91GiB
Free memory: 7.34GiB
2017-05-11 06:51:40.045828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:928] DMA: 0 
2017-05-11 06:51:40.045832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:938] 0:   Y 
2017-05-11 06:51:40.045843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:997] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)
min_queue_examples:  20000
('Images: ', &lt;tf.Tensor 'shuffle_batch:0' shape=(1024, 32, 32, 3) dtype=float32&gt;)
('padded_input: ', &lt;tf.Tensor 'Pad_1:0' shape=(1024, 36, 36, 3) dtype=float32&gt;)
('padded_input: ', &lt;tf.Tensor 'Pad_2:0' shape=(1024, 19, 19, 32) dtype=float32&gt;)
('padded_input: ', &lt;tf.Tensor 'Pad_3:0' shape=(1024, 11, 11, 32) dtype=float32&gt;)
('pool3: ', &lt;tf.Tensor 'pool3:0' shape=(1024, 3, 3, 64) dtype=float32&gt;)
2017-05-11 06:51:41.810110: step 0, loss = 2.30 (736.7 examples/sec; 1.390 sec/batch)
epoch: 1, loss: 2.35
2017-05-11 06:51:44.403091: step 50, loss = 2.30 (19262.2 examples/sec; 0.053 sec/batch)
epoch: 2, loss: 2.29
2017-05-11 06:51:47.085227: step 100, loss = 2.28 (19319.7 examples/sec; 0.053 sec/batch)
epoch: 3, loss: 2.27
2017-05-11 06:51:49.763903: step 150, loss = 2.25 (19135.9 examples/sec; 0.054 sec/batch)
epoch: 4, loss: 2.27
2017-05-11 06:51:52.439839: step 200, loss = 2.30 (19603.0 examples/sec; 0.052 sec/batch)
epoch: 5, loss: 2.30
2017-05-11 06:51:55.103668: step 250, loss = 2.30 (19260.7 examples/sec; 0.053 sec/batch)

		</comment>
		<comment id='33' author='Randl' date='2017-05-11T14:20:56Z'>
		I am also CPU bound.  I am using an old skool cool 2500K clocked to 4.2Ghz or so and it was maxed.  No idea if that will make a real world difference but just FYI.
		</comment>
		<comment id='34' author='Randl' date='2017-05-11T14:34:44Z'>
		ResNet (I assume 50) was not a dramatic as I saw ~170ms and then after adding the with CPU and the WINOGRAD NONFUSED flag I was getting 150ms or so.  Again I have no idea if everyone is using the same ResNet.  When NVIDIA does these tests they check the FLOPS used to make sure they are similar between the platforms.  Making sure the models are exact seems painful.  Again my goal is that people doing research and work are following best practices and getting decent performance.  I bet there are a couple (not much more than that) more tweaks I can make to the code and I am pretty novice.
toby@tfbelwar:~/hongkong_bench/dlbench/tools/tensorflow/cnn/resnet_tf_10$ TF_ENABLE_WINOGRAD_NONFUSED=1 python resnet_cifar10.py --data_dir=/home/toby/CIFAR-10/cifar-10-batches-bin
2017-05-11 07:26:26.859794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-05-11 07:26:26.860125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:907] Found device 0 with properties: 
name: GeForce GTX 1080
major: 6 minor: 1 memoryClockRate (GHz) 1.835
pciBusID 0000:01:00.0
Total memory: 7.91GiB
Free memory: 7.26GiB
2017-05-11 07:26:26.860140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:928] DMA: 0 
2017-05-11 07:26:26.860144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:938] 0:   Y 
2017-05-11 07:26:26.860155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:997] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)
min_queue_examples:  20000
('Images: ', &lt;tf.Tensor 'shuffle_batch:0' shape=(128, 32, 32, 3) dtype=float32&gt;)
('shortcut: ', &lt;tf.Tensor 'scale1/block9/Relu:0' shape=(128, 32, 32, 16) dtype=float32&gt;)
('x: ', &lt;tf.Tensor 'scale2/block1/B/batchnorm/add_1:0' shape=(128, 16, 16, 32) dtype=float32&gt;)
('stride: ', 2)
('shortcut: ', &lt;tf.Tensor 'scale2/block1/shortcut/avg_pool:0' shape=(128, 16, 16, 16) dtype=float32&gt;)
('shortcut: ', &lt;tf.Tensor 'scale2/block1/shortcut/Pad:0' shape=(128, 16, 16, 32) dtype=float32&gt;)
('shortcut: ', &lt;tf.Tensor 'scale2/block9/Relu:0' shape=(128, 16, 16, 32) dtype=float32&gt;)
('x: ', &lt;tf.Tensor 'scale3/block1/B/batchnorm/add_1:0' shape=(128, 8, 8, 64) dtype=float32&gt;)
('stride: ', 2)
('shortcut: ', &lt;tf.Tensor 'scale3/block1/shortcut/avg_pool:0' shape=(128, 8, 8, 32) dtype=float32&gt;)
('shortcut: ', &lt;tf.Tensor 'scale3/block1/shortcut/Pad:0' shape=(128, 8, 8, 64) dtype=float32&gt;)
2017-05-11 07:26:42.052644: step 0, loss = 4.66 (51.4 examples/sec; 2.490 sec/batch)
2017-05-11 07:26:56.862701: step 100, loss = 1.65 (855.0 examples/sec; 0.150 sec/batch)
2017-05-11 07:27:11.718538: step 200, loss = 1.40 (853.8 examples/sec; 0.150 sec/batch)
2017-05-11 07:27:26.637125: step 300, loss = 1.51 (883.9 examples/sec; 0.145 sec/batch)
2017-05-11 07:27:41.486478: step 400, loss = 1.34 (879.2 examples/sec; 0.146 sec/batch)

I was not CPU maxed but there might be a small gain with a better CPU.  My GTX 1080 might also be clocked higher, I did not read their settings.  I also use my GTX as my main display but I doubt that matters in this case.
		</comment>
		<comment id='35' author='Randl' date='2017-05-11T14:54:27Z'>
		My GTX 1080 is clocked slightly higher.  I happened to check their logs, which is nice that they include.  The gains are still solid for AlexNet.  My ResNet number is similar to their number for v7 testing.  To be honest I am kind of surprised because I get a much lower number on my faster clocked GTX before I fix the input pipeline problem.  Oh well, interesting but not interesting enough to install TF 0.11.0.  :-)
		</comment>
		<comment id='36' author='Randl' date='2017-05-15T17:25:52Z'>
		I changed their ResNet to use Fused and it is now down to ~105ms which is aligned with the other platforms.  I also checked the accuracy at 42 epochs (that seems to be what they are doing) and it is ~79% which seems to line up with the previous results.  I am pretty sure they are using NHWC so there might be a small gain moving to NCHW but given the step times are close to the other platforms I am not sure I would mess with it for CIFAR-10.  I think this is extra closed at this point.  I am trying to find time to send a PR to the HK team.
		</comment>
	</comments>
</bug>