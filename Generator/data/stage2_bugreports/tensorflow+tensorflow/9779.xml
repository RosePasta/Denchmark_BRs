<bug id='9779' author='liusiye' open_date='2017-05-09T04:42:12Z' closed_time='2019-08-06T22:16:11Z'>
	<summary>memory leak when implement rnn attention decoder</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;

== cat /etc/issue ===============================================
Linux quad 4.4.0-72-generic &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/93&gt;#93&lt;/denchmark-link&gt;
-Ubuntu SMP Fri Mar 31 14:07:41 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION="16.04 LTS (Xenial Xerus)"
VERSION_ID="16.04"
== are we in docker =============================================
No
== compiler =====================================================
c++ (Ubuntu 4.9.4-2ubuntu1~16.04) 4.9.4
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
== uname -a =====================================================
Linux quad 4.4.0-72-generic &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/93&gt;#93&lt;/denchmark-link&gt;
-Ubuntu SMP Fri Mar 31 14:07:41 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
== check pips ===================================================
numpy (1.12.1)
protobuf (3.3.0)
tensorflow-gpu (1.1.0)
== check for virtualenv =========================================
True
== tensorflow import ============================================
tf.VERSION = 1.1.0
tf.GIT_VERSION = v1.1.0-rc0-61-g1ec6ed5
tf.COMPILER_VERSION = v1.1.0-rc0-61-g1ec6ed5
Sanity check: array([1], dtype=int32)
== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset
== nvidia-smi ===================================================
Tue May  9 12:16:54 2017
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX TIT...  Off  | 0000:05:00.0     Off |                  N/A |
| 22%   47C    P0    76W / 250W |      0MiB / 12205MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX TIT...  Off  | 0000:06:00.0     Off |                  N/A |
| 22%   60C    P2   129W / 250W |  11713MiB / 12207MiB |     80%      Default |
+-------------------------------+----------------------+----------------------+
|   2  GeForce GTX TIT...  Off  | 0000:09:00.0     Off |                  N/A |
| 22%   49C    P0    83W / 250W |      0MiB / 12207MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  GeForce GTX TIT...  Off  | 0000:0A:00.0     Off |                  N/A |
| 24%   63C    P2   117W / 250W |  11713MiB / 12207MiB |     70%      Default |
+-------------------------------+----------------------+----------------------+
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    1     21558    C   python3                                      11709MiB |
|    3     21346    C   python3                                      11709MiB |
+-----------------------------------------------------------------------------+
== cuda libs  ===================================================
/usr/local/cuda-7.5/doc/man/man7/libcudart.7
/usr/local/cuda-7.5/doc/man/man7/libcudart.so.7
/usr/local/cuda-7.5/lib64/libcudart_static.a
/usr/local/cuda-7.5/lib64/libcudart.so.7.5.18
/usr/local/cuda-7.5/lib/libcudart_static.a
/usr/local/cuda-7.5/lib/libcudart.so.7.5.18
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/lib64/libcudart_static.a
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.27
&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

When we try to implement a complex network which contain a rnn attention decoder, It will consume all the memory after several days. I extract the decoder in a test file, the memory still grow in a slower speed. also found that if change softmax to sigmoid, memory doesn't leak.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

test code:
&lt;denchmark-code&gt;# __author__ = "liusiye"
# -*- coding: utf-8 -*-
import tensorflow as tf
from tensorflow.contrib.rnn import GRUCell, MultiRNNCell
from os import getpid
import psutil
import gc
import tensorflow as tf
import numpy as np


process = psutil.Process(getpid())
B, T, H = 20, 60, 256
layer_num = 4

def apply_attention(encoding, rnn_output):
    ''' encoding: [t, b, h1]
        rnn_output: [b, h2]
    '''
    T, B, H1 = encoding.get_shape().as_list()
    _, H2 = rnn_output.get_shape().as_list()
    with tf.variable_scope('attention'):
        w_encoder = tf.get_variable(
            name='W_encoder',
            shape=[H1, H1],
            initializer=tf.random_uniform_initializer(-0.01, 0.01))
        w_decoder = tf.get_variable(
            name='W_decoder',
            shape=[H2, H1],
            initializer=tf.random_uniform_initializer(-0.01, 0.01))
        w_attention = tf.get_variable(
            name='W_attention',
            shape=[H1, 1],
            initializer=tf.random_uniform_initializer(-0.01, 0.01))
    r_decoder = tf.matmul(rnn_output, w_decoder)  # [b, h1]

    r_encoder = tf.matmul(tf.reshape(encoding, [-1, H1]), w_encoder)
    r_encoder = tf.reshape(r_encoder, [T, B, H1])
    # [t, b, h] -&gt; [t * b, h] -&gt; [t, b, h]

    r_attention = tf.tanh(r_encoder + r_decoder)  # [t, b, h1]
    attention = tf.matmul(tf.reshape(r_attention, [-1, H1]), w_attention)
    attention = tf.nn.softmax(tf.reshape(attention, [T, B]), dim=0)
    #attention = tf.nn.sigmoid(tf.reshape(attention, [T, B]))
    encoding = tf.transpose(encoding, perm=[2, 0, 1])  # [t, b, h1]-&gt;[h1, t, b]

    context = tf.reduce_sum(encoding * attention, axis=1)  # [h1, b]
    return tf.transpose(context)  # [b, h1]

def rnn_attention_decoder_test():
    encoding = tf.get_variable(name='encoding', shape=[T, B, H], dtype=tf.float32)
    rnn_outputs = []  # t * [b, h]
    scope = tf.get_variable_scope()

    zero_input = tf.constant(0, shape=[B, H], dtype=tf.float32)
    cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.GRUCell(H) for i in range(layer_num)], state_is_tuple=True)
    state = cell.zero_state(B, tf.float32)
    with tf.variable_scope(scope) as outer_scope:
        for t in range(T):#T):
            attention_input = zero_input
            rnn_input = apply_attention(encoding, attention_input)
            rnn_output, state = cell(rnn_input, state)
            rnn_outputs.append(rnn_output)
            outer_scope.reuse_variables()
    return tf.stack(rnn_outputs, axis=1), state  # t * [b, h] -&gt; [b, t, h]

with tf.Session() as sess, tf.variable_scope('model'):
    with tf.variable_scope('model', reuse=None):
        tensor_lists_test = rnn_attention_decoder_test()
    init_op = tf.group(
        tf.global_variables_initializer(),
        tf.local_variables_initializer()
    )
    sess.run(init_op)
    sess.graph.finalize()
    for step in range(100000):
        after = process.memory_percent()
        if step &gt; 0:
            print("MEMORY CHANGE %.7f -&gt; %.7f" % (before, after))
        before = process.memory_percent()
        sess.run(tensor_lists_test)
        gc.collect()
&lt;/denchmark-code&gt;

log:
&lt;denchmark-code&gt;2017-05-09 09:27:55.435870: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-09 09:27:55.435903: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-09 09:27:55.435909: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-05-09 09:27:55.435913: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-05-09 09:27:55.435916: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-05-09 09:27:55.732204: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.2405
pciBusID 0000:09:00.0
Total memory: 11.92GiB
Free memory: 11.81GiB
2017-05-09 09:27:55.732232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-05-09 09:27:55.732238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-05-09 09:27:55.732247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:09:00.0)
MEMORY CHANGE 1.1297021 -&gt; 1.3666840
MEMORY CHANGE 1.3666840 -&gt; 1.3666840
MEMORY CHANGE 1.3666840 -&gt; 1.3697929
MEMORY CHANGE 1.3697929 -&gt; 1.3697929
MEMORY CHANGE 1.3697929 -&gt; 1.3729200
MEMORY CHANGE 1.3729200 -&gt; 1.3733208
MEMORY CHANGE 1.3733208 -&gt; 1.3733208
MEMORY CHANGE 1.3733208 -&gt; 1.3737215
MEMORY CHANGE 1.3737215 -&gt; 1.3768487
MEMORY CHANGE 1.3768487 -&gt; 1.3799758
MEMORY CHANGE 1.3799758 -&gt; 1.3803644
MEMORY CHANGE 1.3803644 -&gt; 1.3834976
MEMORY CHANGE 1.3834976 -&gt; 1.3834976
MEMORY CHANGE 1.3834976 -&gt; 1.3838862
MEMORY CHANGE 1.3838862 -&gt; 1.3838862
MEMORY CHANGE 1.3838862 -&gt; 1.3842870
MEMORY CHANGE 1.3842870 -&gt; 1.3846878
MEMORY CHANGE 1.3846878 -&gt; 1.3850885
MEMORY CHANGE 1.3850885 -&gt; 1.3850885
MEMORY CHANGE 1.3850885 -&gt; 1.3850885
MEMORY CHANGE 1.3850885 -&gt; 1.3850885
MEMORY CHANGE 1.3850885 -&gt; 1.3850885
MEMORY CHANGE 1.3850885 -&gt; 1.3854771
MEMORY CHANGE 1.3854771 -&gt; 1.3854771
MEMORY CHANGE 1.3854771 -&gt; 1.3854771
MEMORY CHANGE 1.3854771 -&gt; 1.3858779
MEMORY CHANGE 1.3858779 -&gt; 1.3858779
MEMORY CHANGE 1.3858779 -&gt; 1.3858779
MEMORY CHANGE 1.3858779 -&gt; 1.3858779
MEMORY CHANGE 1.3858779 -&gt; 1.3858779
MEMORY CHANGE 1.3858779 -&gt; 1.3858779
MEMORY CHANGE 1.3858779 -&gt; 1.3862665
MEMORY CHANGE 1.3862665 -&gt; 1.3866673
MEMORY CHANGE 1.3866673 -&gt; 1.3866673
MEMORY CHANGE 1.3866673 -&gt; 1.3870680
MEMORY CHANGE 1.3870680 -&gt; 1.3870680
MEMORY CHANGE 1.3870680 -&gt; 1.3870680
MEMORY CHANGE 1.3870680 -&gt; 1.3870680
MEMORY CHANGE 1.3870680 -&gt; 1.3870680
MEMORY CHANGE 1.3870680 -&gt; 1.3870680
MEMORY CHANGE 1.3870680 -&gt; 1.3870680
MEMORY CHANGE 1.3870680 -&gt; 1.3874688
MEMORY CHANGE 1.3874688 -&gt; 1.3874688
MEMORY CHANGE 1.3874688 -&gt; 1.3878695
MEMORY CHANGE 1.3878695 -&gt; 1.3878695
MEMORY CHANGE 1.3878695 -&gt; 1.3882581
MEMORY CHANGE 1.3882581 -&gt; 1.3882581
MEMORY CHANGE 1.3882581 -&gt; 1.3886589
MEMORY CHANGE 1.3886589 -&gt; 1.3886589
MEMORY CHANGE 1.3886589 -&gt; 1.3890597
MEMORY CHANGE 1.3890597 -&gt; 1.3894604
MEMORY CHANGE 1.3894604 -&gt; 1.3894604
MEMORY CHANGE 1.3894604 -&gt; 1.3898612
MEMORY CHANGE 1.3898612 -&gt; 1.3898612
MEMORY CHANGE 1.3898612 -&gt; 1.3902619
MEMORY CHANGE 1.3902619 -&gt; 1.3906627
MEMORY CHANGE 1.3906627 -&gt; 1.3906627
MEMORY CHANGE 1.3906627 -&gt; 1.3906627
MEMORY CHANGE 1.3906627 -&gt; 1.3906627
MEMORY CHANGE 1.3906627 -&gt; 1.3906627
MEMORY CHANGE 1.3906627 -&gt; 1.3906627
MEMORY CHANGE 1.3906627 -&gt; 1.3906627
MEMORY CHANGE 1.3906627 -&gt; 1.3906627
MEMORY CHANGE 1.3906627 -&gt; 1.3937898
MEMORY CHANGE 1.3937898 -&gt; 1.3937898
MEMORY CHANGE 1.3937898 -&gt; 1.3937898
MEMORY CHANGE 1.3937898 -&gt; 1.3937898
MEMORY CHANGE 1.3937898 -&gt; 1.3937898
MEMORY CHANGE 1.3937898 -&gt; 1.3937898
MEMORY CHANGE 1.3937898 -&gt; 1.3937898
MEMORY CHANGE 1.3937898 -&gt; 1.3937898
MEMORY CHANGE 1.3937898 -&gt; 1.3941906
MEMORY CHANGE 1.3941906 -&gt; 1.3945913
MEMORY CHANGE 1.3945913 -&gt; 1.3945913
MEMORY CHANGE 1.3945913 -&gt; 1.3945913
MEMORY CHANGE 1.3945913 -&gt; 1.3945913
MEMORY CHANGE 1.3945913 -&gt; 1.3945913
MEMORY CHANGE 1.3945913 -&gt; 1.3945913
MEMORY CHANGE 1.3945913 -&gt; 1.3945913
MEMORY CHANGE 1.3945913 -&gt; 1.3945913
MEMORY CHANGE 1.3945913 -&gt; 1.3945913
MEMORY CHANGE 1.3945913 -&gt; 1.3945913
MEMORY CHANGE 1.3945913 -&gt; 1.3945913
MEMORY CHANGE 1.3945913 -&gt; 1.3945913
MEMORY CHANGE 1.3945913 -&gt; 1.3945913
MEMORY CHANGE 1.3945913 -&gt; 1.3945913
MEMORY CHANGE 1.3945913 -&gt; 1.3945913
MEMORY CHANGE 1.3945913 -&gt; 1.3945913
MEMORY CHANGE 1.3945913 -&gt; 1.3945913
MEMORY CHANGE 1.3945913 -&gt; 1.3945913
MEMORY CHANGE 1.3945913 -&gt; 1.3945913
MEMORY CHANGE 1.3945913 -&gt; 1.3945913
MEMORY CHANGE 1.3945913 -&gt; 1.3949921
MEMORY CHANGE 1.3949921 -&gt; 1.3949921
MEMORY CHANGE 1.3949921 -&gt; 1.3949921
MEMORY CHANGE 1.3949921 -&gt; 1.3949921
MEMORY CHANGE 1.3949921 -&gt; 1.3949921
MEMORY CHANGE 1.3949921 -&gt; 1.3949921
MEMORY CHANGE 1.3949921 -&gt; 1.3949921
MEMORY CHANGE 1.3949921 -&gt; 1.3949921
MEMORY CHANGE 1.3949921 -&gt; 1.3949921
MEMORY CHANGE 1.3949921 -&gt; 1.3949921
MEMORY CHANGE 1.3949921 -&gt; 1.3949921
MEMORY CHANGE 1.3949921 -&gt; 1.3949921
MEMORY CHANGE 1.3949921 -&gt; 1.3949921
MEMORY CHANGE 1.3949921 -&gt; 1.3949921
MEMORY CHANGE 1.3949921 -&gt; 1.3949921
MEMORY CHANGE 1.3949921 -&gt; 1.3949921
MEMORY CHANGE 1.3949921 -&gt; 1.3949921
MEMORY CHANGE 1.3949921 -&gt; 1.3949921
MEMORY CHANGE 1.3949921 -&gt; 1.3953929
MEMORY CHANGE 1.3953929 -&gt; 1.3953929
MEMORY CHANGE 1.3953929 -&gt; 1.3953929
MEMORY CHANGE 1.3953929 -&gt; 1.3953929
MEMORY CHANGE 1.3953929 -&gt; 1.3953929
MEMORY CHANGE 1.3953929 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3957936
MEMORY CHANGE 1.3957936 -&gt; 1.3961944
MEMORY CHANGE 1.3961944 -&gt; 1.3961944
MEMORY CHANGE 1.3961944 -&gt; 1.3961944
MEMORY CHANGE 1.3961944 -&gt; 1.3961944
MEMORY CHANGE 1.3961944 -&gt; 1.3961944
MEMORY CHANGE 1.3961944 -&gt; 1.3961944
MEMORY CHANGE 1.3961944 -&gt; 1.3961944
MEMORY CHANGE 1.3961944 -&gt; 1.3961944
MEMORY CHANGE 1.3961944 -&gt; 1.3961944
MEMORY CHANGE 1.3961944 -&gt; 1.3961944
MEMORY CHANGE 1.3961944 -&gt; 1.3961944
MEMORY CHANGE 1.3961944 -&gt; 1.3961944
MEMORY CHANGE 1.3961944 -&gt; 1.3961944
MEMORY CHANGE 1.3961944 -&gt; 1.3961944
MEMORY CHANGE 1.3961944 -&gt; 1.3961944
MEMORY CHANGE 1.3961944 -&gt; 1.3961944
MEMORY CHANGE 1.3961944 -&gt; 1.3961944
MEMORY CHANGE 1.3961944 -&gt; 1.3961944
MEMORY CHANGE 1.3961944 -&gt; 1.3961944
MEMORY CHANGE 1.3961944 -&gt; 1.3961944
MEMORY CHANGE 1.3961944 -&gt; 1.3961944
MEMORY CHANGE 1.3961944 -&gt; 1.3961944
MEMORY CHANGE 1.3961944 -&gt; 1.3961944
MEMORY CHANGE 1.3961944 -&gt; 1.3961944
MEMORY CHANGE 1.3961944 -&gt; 1.3961944
MEMORY CHANGE 1.3961944 -&gt; 1.3965951
MEMORY CHANGE 1.3965951 -&gt; 1.3965951
MEMORY CHANGE 1.3965951 -&gt; 1.3965951
MEMORY CHANGE 1.3965951 -&gt; 1.3969959
MEMORY CHANGE 1.3969959 -&gt; 1.3969959
MEMORY CHANGE 1.3969959 -&gt; 1.3969959
MEMORY CHANGE 1.3969959 -&gt; 1.3969959
MEMORY CHANGE 1.3969959 -&gt; 1.3969959
MEMORY CHANGE 1.3969959 -&gt; 1.3969959
MEMORY CHANGE 1.3969959 -&gt; 1.3969959
MEMORY CHANGE 1.3969959 -&gt; 1.3969959
MEMORY CHANGE 1.3969959 -&gt; 1.3973967
MEMORY CHANGE 1.3973967 -&gt; 1.3973967
MEMORY CHANGE 1.3973967 -&gt; 1.3973967
MEMORY CHANGE 1.3973967 -&gt; 1.3973967
MEMORY CHANGE 1.3973967 -&gt; 1.3977974
MEMORY CHANGE 1.3977974 -&gt; 1.3977974
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='liusiye' date='2017-05-09T05:08:47Z'>
		by the way , if we implement softmax like this, the problem doesn't exists.
&lt;denchmark-h:h1&gt;original softmax in TF&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;attention = tf.nn.softmax(tf.reshape(attention, [T, B]), dim=0)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h1&gt;mannul softmax&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;attention = tf.matmul(tf.reshape(r_attention, [-1, H1]), w_attention)
attention = tf.reshape(attention, [T, B])
attention_exp = tf.exp(attention)  # [T ,B]
attention_sum = tf.reduce_sum(tf.exp(attention), axis=0)  # [B]
attention = attention_exp / attention_sum  # [T, B]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='liusiye' date='2017-05-09T16:02:42Z'>
		&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 Can you take a look?  This seems like a reasonably minimized memory leak bug.
		</comment>
		<comment id='3' author='liusiye' date='2018-01-05T19:09:55Z'>
		Is this reproducible in the nightlies?
		</comment>
		<comment id='4' author='liusiye' date='2019-08-06T22:16:11Z'>
		This is a stale issue. Please check the issue with latest TensorFlow. If the issue still persists in the newer version of TF, please feel free to reopen it by providing details about the issue and a standalone code to reproduce the issue. Thanks!
		</comment>
		<comment id='5' author='liusiye' date='2019-08-06T22:16:12Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=9779&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=9779&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>