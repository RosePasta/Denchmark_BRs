<bug id='19420' author='jcRisch' open_date='2018-05-20T14:42:20Z' closed_time='2020-01-24T21:38:56Z'>
	<summary>tf.keras.layers.Dropout has no 'is_training' argument</summary>
	<description>
Dropout layer is (usually) only used while training. Keras automatically enable dropout layers while training and disable them while predicting.
Thus, tf.keras.layers.Dropout has no 'is_training' argument (but it exists in tf.layers.dropout). That can mislead Keras and tf users when trying to mix Keras with TF layers. Indeed, predicting new data with dropout layers doesn't produce any error but reduces the accuracy ! Maybe a warning note in 'tf.keras.layers.Dropout' documentation can be useful.
With tf.layers
&lt;denchmark-code&gt;dropout = tf.layers.dropout(inputs=dense, rate=0.2, training=is_training, name="dropout-1")
&lt;/denchmark-code&gt;

With tf.Keras.layers
&lt;denchmark-code&gt;dense = ...
if(is_training):
      dropout = tf.keras.layers.Dropout(0.2, noise_shape=None, seed=None)(dense)
  else :
      dropout = dense
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='jcRisch' date='2018-05-23T01:13:41Z'>
		Hi &lt;denchmark-link:https://github.com/jcRisch&gt;@jcRisch&lt;/denchmark-link&gt;
,
I also struggled with this a bit before and I found out that tf.keras.layers.Dropout supports a training argument in its call function. Therefore, you could write:
dropout = tf.keras.layers.Dropout(0.2, noise_shape=None, seed=None)(dense, training=is_training)
That also applies to tf.keras.layers.BatchNormalization (and possibly other layers).
I hope that helps.
		</comment>
		<comment id='2' author='jcRisch' date='2018-05-23T23:08:02Z'>
		The problems seems that the related doc is missing here  and upstream check &lt;denchmark-link:https://github.com/keras-team/keras/issues/9412#issuecomment-366487734&gt;keras-team/keras#9412 (comment)&lt;/denchmark-link&gt;
 /cc &lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='jcRisch' date='2018-09-16T08:29:10Z'>
		So we'd better to update Dropout's document, right? This seems like a good beginner issue.
		</comment>
		<comment id='4' author='jcRisch' date='2018-10-01T18:55:35Z'>
		Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the contributions welcome label. Thank you.
		</comment>
		<comment id='5' author='jcRisch' date='2018-12-11T05:43:11Z'>
		Hi, I would like to contribute to this issue. May I work on it?
		</comment>
		<comment id='6' author='jcRisch' date='2019-01-31T01:06:50Z'>
		Is anyone working on this? If no, I'd like to add description about it.
		</comment>
		<comment id='7' author='jcRisch' date='2019-08-12T19:25:35Z'>
		This still seems relevant because, as far as I know, the training argument does not exist in the Sequential API.
		</comment>
		<comment id='8' author='jcRisch' date='2020-01-24T21:38:55Z'>
		Sequential accepts atraining argument now.
		</comment>
		<comment id='9' author='jcRisch' date='2020-01-25T02:19:40Z'>
		&lt;denchmark-link:https://www.tensorflow.org/guide/keras/custom_layers_and_models#privileged_training_argument_in_the_call_method&gt;The doc&lt;/denchmark-link&gt;
 about  argument dropout is outdated.
Now  argument is a boolean tensor, not a python boolean.
So can't use python  only, instead actually use , or &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/layers/core.py#L170&gt;smart_cond&lt;/denchmark-link&gt;
 to choose one of branches at the running time.
		</comment>
		<comment id='10' author='jcRisch' date='2020-05-22T00:53:07Z'>
		&lt;denchmark-link:https://github.com/carsonDB&gt;@carsonDB&lt;/denchmark-link&gt;
 Hi, could you elaborate more, I am a beginner, and I am trying to create a custom layer by subclassing the 
Could you give an example of using  in the call() function of the custom layers class. I currently treat It as a python boolean and this is what I have written, please suggest the necessary changes to make.


		</comment>
		<comment id='11' author='jcRisch' date='2020-05-22T07:41:13Z'>
		&lt;denchmark-link:https://github.com/Ashwin-Ramesh2607&gt;@Ashwin-Ramesh2607&lt;/denchmark-link&gt;
 Hi, if you use standard  layer, no need to judge if , it handles already. If write custom dropout like I mentioned, your code can like this:
&lt;denchmark-code&gt;from tensorflow.python.keras import backend as K

def call(self, inputs, training=None):
    if training is None: # actually I test it in my tensorflow-1.15, `training` is always `None`.
          training = K.learning_phase()

    def dropped_inputs():
      return tf.nn.dropout(
          inputs,
          rate=self.dropout_rate)

    output = K.in_train_phase(dropped_inputs, inputs, training=training)
&lt;/denchmark-code&gt;

Since I don't test it, you'd better test it before using it.
		</comment>
	</comments>
</bug>