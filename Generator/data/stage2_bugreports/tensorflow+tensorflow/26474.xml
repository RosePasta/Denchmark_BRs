<bug id='26474' author='anilsathyan7' open_date='2019-03-08T07:14:54Z' closed_time='2020-04-08T14:00:13Z'>
	<summary>Android TFLite benchmark performance issue with deeplab segmentation model (DepthwiseConv2d)</summary>
	<description>
System information
The benchmark tests were carried out using the following tools and devices:-
Bazel version:Build label: 0.23.1
Tensorflow Version: 1.13.0
Android Device: OnePlus 3
Describe the current behavior
We trained a segmentation model using deeplab with mobilenet with TF 1.13.0 to replicate the
segmentation model by tensorflow i.e 'deeplabv3_257_mv_gpu.tflite'(Using sample code  in repo for pascal voc).
However there was significant time difference for average running time for 'DepthwiseConv2d', when we benchmarked the two tflite models with 'tflite android benchmark tool'.
In the official model avg time was around 28 ms; whereas in our own model it was around 103 ms.
The only difference between the two float models being the quantization levels i.e Ours was 0-255
and official model has -1 to 0.99.Also, there is a difference in model size(Official:2.7Mb vs. Ours:3.3Mb).
It seems the official model is using a newer kernel for depthwiseconv2d and hence runs significantly faster compared to our trained model.Was the official model trained or optimised using TF2.0 tools?What could be done to achieve similar speed (DepthwiseConv2d) using TF 1.13.0 or do we have to migrate and retrain our model using TF 2.0?
Describe the expected behavior
The depthwise conv2d performance should be same in the official model and the replicated model.
Code to reproduce the issue
Benchmark commands:-
&lt;denchmark-code&gt;# bazel build -c opt   --config=android_arm64   --cxxopt='--std=c++11' --copt=-DTFLITE_PROFILING_ENABLED tensorflow/lite/tools/benchmark:benchmark_model

# adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/deeplabv3_257_mv_gpu.tflite --num_threads=1
&lt;/denchmark-code&gt;

Benchmark results:-
Official model:-
&lt;denchmark-code&gt;Number of nodes executed: 70
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	                 CONV_2D	       38	   137.792	    76.447%	    76.447%	     0.000	       38
	       DEPTHWISE_CONV_2D	       17	    27.808	    15.428%	    91.875%	     0.000	       17
	         RESIZE_BILINEAR	        3	    13.577	     7.533%	    99.408%	     0.000	        3
	           CONCATENATION	        1	     0.528	     0.293%	    99.701%	     0.000	        1
	                     ADD	       10	     0.410	     0.227%	    99.928%	     0.000	       10
	         AVERAGE_POOL_2D	        1	     0.129	     0.072%	   100.000%	     0.000	        1

Timings (microseconds): count=50 first=180683 curr=179967 min=177601 max=186179 avg=180284 std=1486
Memory (bytes): count=0
70 nodes observe
&lt;/denchmark-code&gt;

Replicated model:-
&lt;denchmark-code&gt;Number of nodes executed: 70
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	                 CONV_2D	       38	   141.189	    54.356%	    54.356%	     0.000	       38
	       DEPTHWISE_CONV_2D	       17	   103.263	    39.755%	    94.110%	     0.000	       17
	         RESIZE_BILINEAR	        3	    14.178	     5.458%	    99.568%	     0.000	        3
	           CONCATENATION	        1	     0.554	     0.213%	    99.782%	     0.000	        1
	                     ADD	       10	     0.437	     0.168%	    99.950%	     0.000	       10
	         AVERAGE_POOL_2D	        1	     0.130	     0.050%	   100.000%	     0.000	        1

Timings (microseconds): count=50 first=258703 curr=259361 min=255581 max=269627 avg=259789 std=3844
Memory (bytes): count=0
70 nodes observed
&lt;/denchmark-code&gt;

Other info / logs
However using TF-12.0 with Bazel 0.16.0, the 'benchmark_model' built failed for 'official model'; but worked with replicated model.
Bazel build:-
&lt;denchmark-code&gt;# bazel build -c opt   --config=android_arm64   --cxxopt='--std=c++11' --linkopt='-llog' --copt=-DTFLITE_PROFILING_ENABLED tensorflow/contrib/lite/tools/benchmark:benchmark_model

# adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/deeplabv3_257_mv_gpu.tflite --num_threads=1
&lt;/denchmark-code&gt;

TF-Lite benchmark:-
adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/deeplabv3_257_mv_gpu.tflite --num_threads=1
adb: /opt/intel/intelpython27/lib/libcrypto.so.1.0.0: no version information available (required by adb)
STARTING!
Num runs: [50]
Inter-run delay (seconds): [-1]
Num threads: [1]
Benchmark name: []
Output prefix: []
Warmup runs: [1]
Graph: [/data/local/tmp/deeplabv3_257_mv_gpu.tflite]
Input layers: []
Input shapes: []
Use nnapi : [0]
nnapi error: unable to open library libneuralnetworks.so
Loaded model /data/local/tmp/deeplabv3_257_mv_gpu.tflite
resolved reporter
Didn't find op for builtin opcode 'DEPTHWISE_CONV_2D' version '2'
Registration failed.
Failed to construct interpreter
Aborted
Can you provide the corresponding optimised pb file before tflite conversion, for the same model ?
	</description>
	<comments>
		<comment id='1' author='anilsathyan7' date='2019-03-15T16:11:31Z'>
		&lt;denchmark-link:https://github.com/anilsathyan7&gt;@anilsathyan7&lt;/denchmark-link&gt;
 :
Version 2 of &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/toco/tflite/operator.cc#L177&gt;Depthwiseconv2d&lt;/denchmark-link&gt;
 :
is used when dilation width and height factors are equal to 1.
This doesn't have to do anything with TF 2.0. Adding &lt;denchmark-link:https://github.com/jianlijianli&gt;@jianlijianli&lt;/denchmark-link&gt;
  who can give a definitive answer.
		</comment>
		<comment id='2' author='anilsathyan7' date='2019-03-16T15:43:59Z'>
		It looks like the dilation_w_factor and dilation_h_factor in the official model for depthwiseconv2d  are not equal to 1 everywhere.Initially it is 1 for couple of depthwiseconv2d layers, then it becomes 2 and finally it is 4 for last few depthwiseconv2d layers. This seems to be the difference between both models (replicated model has dilation width and height factors equal to one in all the nodes in network).This also seems to make the avg execution time for deothwiseonv2d in official model smaller.How can we configure the dilation width and height factors separately for each nodes  during deeplabv3+ training?Or is it a problem with tflite conversion or tf-version ?
		</comment>
		<comment id='3' author='anilsathyan7' date='2019-03-21T05:58:35Z'>
		We tried benchmarking the tflite obtained from code in official deeplab repository, with depthwise conv2d Version 2. The dilation_w_factor and dilation_h_factor for deeplab is set as 1, by default. The tflite from the output of the graph transform model, seemed to provide no speed difference as we believe graph transformation (Flatten_atrous_conv), has affected the speed improvement by DepthwiseConv2d Version 2. The direct output from the optimize for inference, was giving speedup in the Depthwise Conv2d V2, but when trying to run it on the mobile GPU, it increased the time of execution, since it had operators like SpacetoBatchND and BatchtoSpaceND, which are not optimized for GPU processing. Is there any way, we can remove the spacetobatch and batchtospace nodes from the optimized graph, so as to obtain the speed gain by Depthwise Conv2d Version2 on android GPU... Tried doing the remove_nodes graph transform on the optimized graph, but it did not remove the nodes..
Attached with here, are the commands, benchmarks and tflite files..
Benchmark Results: (Oneplus 3)
1.Without Batch2Space
Command &amp; Result:-
adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/withoutbatchtospace.tflite --num_threads=1
adb: /opt/intel/intelpython27/lib/libcrypto.so.1.0.0: no version information available (required by adb)
STARTING!
Num runs: [50]
Inter-run delay (seconds): [-1]
Num threads: [1]
Benchmark name: []
Output prefix: []
Warmup runs: [1]
Graph: [/data/local/tmp/withoutbatchtospace.tflite]
Input layers: []
Input shapes: []
Use nnapi : [0]
nnapi error: unable to open library libneuralnetworks.so
Loaded model /data/local/tmp/withoutbatchtospace.tflite
resolved reporter
Initialized session in 7.374ms
Running benchmark for 1 iterations
count=1 curr=248072
Running benchmark for 50 iterations
count=50 first=243147 curr=234508 min=234107 max=245867 avg=237050 std=3526
Number of nodes executed: 70
============================== Summary by node type ==============================
[Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
CONV_2D	       38	   128.807	    54.361%	    54.361%	     0.000	       38
DEPTHWISE_CONV_2D	       17	    90.016	    37.990%	    92.351%	     0.000	       17
RESIZE_BILINEAR	        3	    17.095	     7.215%	    99.566%	     0.000	        3
CONCATENATION	        1	     0.505	     0.213%	    99.779%	     0.000	        1
ADD	       10	     0.401	     0.169%	    99.949%	     0.000	       10
AVERAGE_POOL_2D	        1	     0.122	     0.051%	   100.000%	     0.000	        1
Timings (microseconds): count=50 first=243080 curr=234449 min=234036 max=245808 avg=236985 std=3526
Memory (bytes): count=0
70 nodes observed
Average inference timings in us: Warmup: 248072, Init: 7374, no stats: 237050
2.With Batch2Space
Command &amp; Result:-
adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/withbatchtospace.tflite --num_threads=1
adb: /opt/intel/intelpython27/lib/libcrypto.so.1.0.0: no version information available (required by adb)
STARTING!
Num runs: [50]
Inter-run delay (seconds): [-1]
Num threads: [1]
Benchmark name: []
Output prefix: []
Warmup runs: [1]
Graph: [/data/local/tmp/withbatchtospace.tflite]
Input layers: []
Input shapes: []
Use nnapi : [0]
nnapi error: unable to open library libneuralnetworks.so
Loaded model /data/local/tmp/withbatchtospace.tflite
resolved reporter
Initialized session in 6.757ms
Running benchmark for 1 iterations
count=1 curr=207830
Running benchmark for 50 iterations
count=50 first=197780 curr=196627 min=195049 max=200776 avg=196418 std=1165
Number of nodes executed: 110
============================== Summary by node type ==============================
[Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
CONV_2D	       38	   129.772	    66.131%	    66.131%	     0.000	       38
DEPTHWISE_CONV_2D	       17	    30.027	    15.302%	    81.432%	     0.000	       17
RESIZE_BILINEAR	        3	    17.773	     9.057%	    90.489%	     0.000	        3
MUL	       10	     8.193	     4.175%	    94.665%	     0.000	       10
SPACE_TO_BATCH_ND	       10	     3.707	     1.889%	    96.554%	     0.000	       10
ADD	       20	     3.707	     1.889%	    98.443%	     0.000	       20
BATCH_TO_SPACE_ND	       10	     2.446	     1.246%	    99.689%	     0.000	       10
CONCATENATION	        1	     0.492	     0.251%	    99.940%	     0.000	        1
AVERAGE_POOL_2D	        1	     0.118	     0.060%	   100.000%	     0.000	        1
Timings (microseconds): count=50 first=197577 curr=196500 min=194929 max=200638 avg=196286 std=1161
Memory (bytes): count=0
110 nodes observed
Average inference timings in us: Warmup: 207830, Init: 6757, no stats: 196418
Optimize for inference
bazel-bin/tensorflow/python/tools/optimize_for_inference --input=frozen.pb --output=stripped.pb --frozen_graph=True --input_names="sub_2" --output_names="ResizeBilinear_2"
Graph Transforms
bazel-bin/tensorflow/tools/graph_transforms/transform_graph --in_graph="stripped.pb" --out_graph="flatten.pb"  --inputs='sub_2' --outputs='ResizeBilinear_2' --transforms='flatten_atrous_conv'
TFLite Conversion
CUDA_VISIBLE_DEVICES="0" tflite_convert --graph_def_file="flatten.pb" --output_file="withoutbatchtospace.tflite" --output_format=TFLITE --input_format=TENSORFLOW_GRAPHDEF --input_arrays="sub_2" --output_arrays="ResizeBilinear_2" --input_shapes=1,257,257,3 

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/2990941/tf_depthwise_conv2d_v2.zip&gt;tf_depthwise_conv2d_v2.zip&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='anilsathyan7' date='2019-03-21T12:13:46Z'>
		Hi &lt;denchmark-link:https://github.com/ilous12&gt;@ilous12&lt;/denchmark-link&gt;
, those frozen graph and tflite files will have depth multiplier 0.5 in this case...
		</comment>
		<comment id='5' author='anilsathyan7' date='2019-03-21T16:05:34Z'>
		
@anilsathyan7 :
Version 2 of Depthwiseconv2d :
is used when dilation width and height factors are equal to 1.
This doesn't have to do anything with TF 2.0. Adding @jianlijianli who can give a definitive answer.

&lt;denchmark-link:https://github.com/jianlijianli&gt;@jianlijianli&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/shashishekhar&gt;@shashishekhar&lt;/denchmark-link&gt;
  The code snippet referred, states that version2 of depthwiseconv2d will be used, when either of dilation_width_factor or dilation_height_factor is not equal to 1. This is exactly opposite what was written above.
&lt;denchmark-link:https://github.com/jianlijianli&gt;@jianlijianli&lt;/denchmark-link&gt;
 How to set dilation height and width when using Mobilenet-v2 architecture to take advantage of Depthwiseconv2d version 2?
		</comment>
		<comment id='6' author='anilsathyan7' date='2019-03-22T11:24:21Z'>
		&lt;denchmark-link:https://github.com/anilsathyan7&gt;@anilsathyan7&lt;/denchmark-link&gt;
 thanks your help. I succeeded by following you.
One more question.
I don't know why my NHWC is "1x17x17x96" and "deeplabv3_257_mv_gpu.tflite"'s NHWC is "1x17x17x96" on MobilenetV2/expanded_conv_6. You will see an image below.
&lt;denchmark-link:https://user-images.githubusercontent.com/3174693/54818973-e1b91f80-4cdd-11e9-9992-cf4f277e3d9d.png&gt;&lt;/denchmark-link&gt;

Do you know how to fix?
thanks.
		</comment>
		<comment id='7' author='anilsathyan7' date='2019-03-28T07:35:22Z'>
		The following seems to be the major differences between the official model and our replicated model:-
Deptwiseconv2d dilation factors, its corresponding weight dimensions and the input bias id (i.e no batc2spaceND_bias):-
&lt;denchmark-link:https://user-images.githubusercontent.com/1130185/55138564-c9bc2280-5159-11e9-9d3a-2c21571caf05.png&gt;&lt;/denchmark-link&gt;

How can we fix it? Should we use some different training settings or can we fix it while tflite conversion?
&lt;denchmark-link:https://github.com/ilous12&gt;@ilous12&lt;/denchmark-link&gt;
  are you having similar issues?If so, did you manage to resolve the same?What is dilation_w_factor and dilation_h_factor values for node 25?
		</comment>
		<comment id='8' author='anilsathyan7' date='2019-06-13T00:11:44Z'>
		Similar issue here. Does anyone replicated model without BATCH_TO_SPACE?
		</comment>
		<comment id='9' author='anilsathyan7' date='2019-06-17T17:21:31Z'>
		Any updates on this? Getting the same error on arm64_v8 with tensorflow lite 1.13.1
		</comment>
		<comment id='10' author='anilsathyan7' date='2019-07-17T16:55:44Z'>
		Any luck getting ride of those pesty BATCH_TO_SPACE?
		</comment>
		<comment id='11' author='anilsathyan7' date='2019-11-15T02:09:09Z'>
		I have the same issue,  can anyone help us and give a definite answer?thanks.
		</comment>
		<comment id='12' author='anilsathyan7' date='2019-11-15T04:53:18Z'>
		&lt;denchmark-link:https://github.com/zhuyan288&gt;@zhuyan288&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jsolves&gt;@jsolves&lt;/denchmark-link&gt;

I think tflite_convert in 1.15 can do the conversion directly.
		</comment>
		<comment id='13' author='anilsathyan7' date='2020-04-08T14:00:13Z'>
		Latest tf-nightly and tf1.15 seems to solve this problem ; but tf-nightly sometimes creates new nodes that may not be supported by gpu-delegate.Other option is to use transform graph tool (with corresponding tf version &amp; bazel) with flatten atrous conv option. This seems to work for spacetobatch/batchtospace issue for depthwise or diated convolutions in general.
		</comment>
		<comment id='14' author='anilsathyan7' date='2020-04-08T14:00:14Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26474&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26474&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>