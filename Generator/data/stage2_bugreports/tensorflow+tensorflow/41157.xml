<bug id='41157' author='jmontalt' open_date='2020-07-07T15:08:35Z' closed_time='2020-07-13T16:22:20Z'>
	<summary>TimeDistributed does not infer output batch size when timesteps=None</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): N/A
TensorFlow version (use command below): v2.2.0-0-g2b96f3662b 2.2.0
Python version: 3.6.9
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: N/A
GPU model and memory: N/A

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with:

TF 1.0: python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
TF 2.0: python -c "import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"

Describe the current behavior
I am trying to create a convolutional LSTM model which makes use of ConvLSTM2D as well as other layers such as Conv2D and MaxPool2D, wrapped in TimeDistributed layers. I'm using the Functional API.
This model should be stateful. Therefore, I pass the batch size to the Input layer, but I would like to keep the timesteps dimension flexible, so I set that to None.
The problem arises when using TimeDistributed layers, because their output batch size is always set to None, even if their input batch size is fixed (see following summary).
&lt;denchmark-code&gt;_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_18 (InputLayer)        [(4, None, 64, 64, 1)]    0         
_________________________________________________________________
time_distributed_27 (TimeDis (None, None, 64, 64, 1)   257       # no LSTMs after this point because batch_size=None
=================================================================
&lt;/denchmark-code&gt;

As a result, stateful LSTM layers cannot be used after TimeDistributed, as they will fail with "ValueError: If a RNN is stateful, it needs to know its batch size".
Describe the expected behavior
I certainly could be missing something, but I don't see any reason why the TimeDistributed layer shouldn't be able to infer its output batch size correctly (is it not always the same as the input batch size?). Even if that's not true, shouldn't it be possible to have a mechanism to help the layer figure it out?
Standalone code to reproduce the issue
Link to a simple example in Colab:
&lt;denchmark-link:https://colab.research.google.com/drive/11efUrQpkahgk-jgb0-4Flwii7Fd1y5Ej?usp=sharing&gt;https://colab.research.google.com/drive/11efUrQpkahgk-jgb0-4Flwii7Fd1y5Ej?usp=sharing&lt;/denchmark-link&gt;

Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='jmontalt' date='2020-07-08T05:38:36Z'>
		&lt;denchmark-link:https://github.com/jmontalt&gt;@jmontalt&lt;/denchmark-link&gt;

Please refer to below issues with similar error and let us know if it helps.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/36363&gt;#36363&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/31028&gt;#31028&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/keras-team/keras/issues/7770&gt;link&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/keras-team/keras/issues/7770#issuecomment-326762841&gt;link1&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/31028#issuecomment-520290743&gt;link2&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='jmontalt' date='2020-07-13T10:51:14Z'>
		Hi, thank you very much for your reply. It looks like the problem is the same as that described in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/36363&gt;#36363&lt;/denchmark-link&gt;
 for the  layer, and indeed the same solution (i.e. setting the static shape using  at the end of the layer's  method) works for me. Should I raise a PR with the equivalent fix for ?
		</comment>
		<comment id='3' author='jmontalt' date='2020-07-13T16:22:20Z'>
		&lt;denchmark-link:https://github.com/jmontalt&gt;@jmontalt&lt;/denchmark-link&gt;

Please go ahead with the PR creation, proceeding to move this issue to closed status as its resolved.
		</comment>
		<comment id='4' author='jmontalt' date='2020-07-13T16:22:22Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41157&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41157&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='jmontalt' date='2020-09-06T21:39:23Z'>
		&lt;denchmark-link:https://github.com/jmontalt&gt;@jmontalt&lt;/denchmark-link&gt;
 Did you edit the PR ? (I was not able to find it and I still have the issue with tf-nightly 2.4.0.dev20200906).
If not, can you publish here the solution please ?
		</comment>
		<comment id='6' author='jmontalt' date='2020-09-06T23:19:46Z'>
		This slipped my mind, but I have created the PR now. In the meantime, you can fix the issue using set_shape on the output of your TimeDistributed layer.
&lt;denchmark-code&gt;layer = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(16, (3, 3)))
inputs = tf.keras.Input(batch_shape=(1, None, 32, 32, 1))
outputs = layer(inputs)
outputs.set_shape(layer.compute_output_shape(inputs.shape))
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='jmontalt' date='2020-09-07T07:38:12Z'>
		Thanks a lot, I applied your PR locally and it is working well !
(BTW a review is required in order to merge, one test has failed)
		</comment>
	</comments>
</bug>