<bug id='18260' author='Pyrestone' open_date='2018-04-05T12:58:56Z' closed_time='2018-04-06T21:25:38Z'>
	<summary>[BUG] [1.8] prefetch_to_device() doesn't work with eager Iterators</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES, Minimal (non-) working example can be seen below.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
TensorFlow installed from (source or binary): pip install tf-nightly-gpu (05.04.2018)
TensorFlow version (use command below): 1.8.0.dev20180329
Python version: Python 3.6.3
Bazel version (if compiling from source): --
GCC/Compiler version (if compiling from source): --
CUDA/cuDNN version: CUDA 9, CuDNN 7.1
GPU model and memory: 2x GTX 1080 8GB
Exact command to reproduce:

import tensorflow as tf
contrib_data=tf.contrib.data
data=tf.data
import tensorflow.contrib.eager as tfe
tf.enable_eager_execution()
ds=data.Dataset.range(50)
ds=ds.apply(contrib_data.prefetch_to_device("/gpu:1"))
it=tfe.Iterator(ds)
&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

Even when applying prefetch_to_device as the last operation, it throws the following error:
&lt;denchmark-h:h3&gt;logs&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;WARNING:tensorflow:From C:\Users\PHANTOM\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Use the retry module or similar alternatives.
2018-04-05 14:40:24.616506: I C:\tf_jenkins\workspace\tf-nightly-windows\M\windows-gpu\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2018-04-05 14:40:24.831563: I C:\tf_jenkins\workspace\tf-nightly-windows\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1355] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.835
pciBusID: 0000:01:00.0
totalMemory: 8.00GiB freeMemory: 6.52GiB
2018-04-05 14:40:24.883091: I C:\tf_jenkins\workspace\tf-nightly-windows\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1355] Found device 1 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.835
pciBusID: 0000:02:00.0
totalMemory: 8.00GiB freeMemory: 6.52GiB
2018-04-05 14:40:24.883562: I C:\tf_jenkins\workspace\tf-nightly-windows\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1434] Adding visible gpu devices: 0, 1
2018-04-05 14:40:25.804203: I C:\tf_jenkins\workspace\tf-nightly-windows\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:922] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-04-05 14:40:25.804487: I C:\tf_jenkins\workspace\tf-nightly-windows\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:928]      0 1 
2018-04-05 14:40:25.804689: I C:\tf_jenkins\workspace\tf-nightly-windows\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:941] 0:   N N 
2018-04-05 14:40:25.804889: I C:\tf_jenkins\workspace\tf-nightly-windows\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:941] 1:   N N 
2018-04-05 14:40:25.805180: I C:\tf_jenkins\workspace\tf-nightly-windows\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1052] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6303 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-04-05 14:40:26.469466: I C:\tf_jenkins\workspace\tf-nightly-windows\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1052] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 6303 MB memory) -&gt; physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1)
Traceback (most recent call last):
  File "MWE.py", line 17, in &lt;module&gt;
    it=tfe.Iterator(ds)
  File "C:\Users\PHANTOM\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\contrib\eager\python\datasets.py", line 76, in __init__
    super(Iterator, self).__init__(dataset)
  File "C:\Users\PHANTOM\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\data\ops\iterator_ops.py", line 463, in __init__
    ds_variant = dataset._as_variant_tensor()  # pylint: disable=protected-access
  File "C:\Users\PHANTOM\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\contrib\data\python\ops\prefetching_ops.py", line 154, in _as_variant_tensor
    raise NotImplementedError("`prefetch_to_device()` must be the last "
NotImplementedError: `prefetch_to_device()` must be the last transformation in a dataset pipeline.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Pyrestone' date='2018-04-05T13:04:12Z'>
		Is this related to the migration of eager mode switch in 1.7? I believe they are available outside contrib now.
		</comment>
		<comment id='2' author='Pyrestone' date='2018-04-05T13:58:12Z'>
		&lt;denchmark-link:https://github.com/byronyi&gt;@byronyi&lt;/denchmark-link&gt;
 which specific migration do you mean?
tfe.Iterator?
Because I used the migrated switch...
i also tried:
import tensorflow as tf
contrib_data=tf.contrib.data

tf.enable_eager_execution()

ds=tf.data.Dataset.range(50)
ds=ds.apply(contrib_data.prefetch_to_device("/gpu:1"))

for c in ds:
    print(c)
same error.
		</comment>
		<comment id='3' author='Pyrestone' date='2018-04-05T23:31:04Z'>
		This is currently broken and I have a fix in review, but here's a temporary workaround that should work:
import tensorflow as tf
contrib_data=tf.contrib.data
data=tf.data
import tensorflow.contrib.eager as tfe
tf.enable_eager_execution()

ds=data.Dataset.range(50)
with tf.device("/gpu:1"):
  it=tfe.Iterator(ds)
		</comment>
		<comment id='4' author='Pyrestone' date='2018-04-06T13:07:31Z'>
		yeah, that works. Doesn't have the performance benefits of prefetch_to_device() though...
		</comment>
		<comment id='5' author='Pyrestone' date='2018-04-06T13:57:33Z'>
		It is supposed to have the same benefits... the tfe.Iterator has an on-device buffer internally when itâ€™s placed on a non-CPU device. Are you observing different performance?
		</comment>
		<comment id='6' author='Pyrestone' date='2018-04-06T14:47:46Z'>
		In my current Implementation I see no difference between putting the Iterator on GPU or not.
In both cases my GPU spends a decent amount of time copying data, though that might be for different batches, GPU Compute load stays the same.
Doesn't mean there is no difference though. Might just be bound somewhere else.
I'll have to try disabling eager mode, using prefetch_to_device and see if it's faster then...
		</comment>
		<comment id='7' author='Pyrestone' date='2018-04-06T15:12:56Z'>
		Update: I checked without my model code (just putting some dummy GPU op in place) and that is significantly faster with the Iterator on GPU.
		</comment>
		<comment id='8' author='Pyrestone' date='2018-04-06T15:35:11Z'>
		Thanks for confirming!
		</comment>
		<comment id='9' author='Pyrestone' date='2018-04-06T16:20:18Z'>
		Here's how to check if anyone's interested:

Click to expand
import tensorflow as tf
contrib_data=tf.contrib.data
import tensorflow.contrib.eager as tfe
cp=tf.ConfigProto()
cp.allow_soft_placement=True
cp.gpu_options.allow_growth=True
cp.log_device_placement=False
tf.enable_eager_execution(cp)
import time

##GPU Version

#generate some large dummy values
ds=tf.data.Dataset.range(100000*64*20)
ds=ds.batch(100000)
ds=ds.map(lambda x: tf.reshape(x,(100000,)))
ds=ds.batch(64)
#cache them to not be input-bound
ds=ds.cache('')
ds=ds.repeat(1000)
#prefetch on CPU
ds=ds.prefetch(5)

#create Iterator on GPU
with tf.device("/gpu:1"):
    it=tfe.Iterator(ds)

#make sure all batches are cached 
#so it's not bound anywhere else    
for i in range(20):
    next(it)

mt=0
nt=0
t=time.time()
with tf.device("/gpu:1"):
    for c in it:
        o=c*c
        nt+=1
mt=time.time()-t
print(mt/nt)#&gt;&gt;&gt;0.01107s/batch

##CPU Version

#generate some large dummy values
ds=tf.data.Dataset.range(100000*64*20)
ds=ds.batch(100000)
ds=ds.map(lambda x: tf.reshape(x,(100000,)))
ds=ds.batch(64)
#cache them to not be input-bound
ds=ds.cache('')
ds=ds.repeat(1000)
#prefetch on CPU
ds=ds.prefetch(5)

#create Iterator on CPU
with tf.device("/cpu:0"):
    it=tfe.Iterator(ds)

#make sure all batches are cached 
#so it's not bound anywhere else    
for i in range(20):
    next(it)

mt=0
nt=0
t=time.time()
with tf.device("/gpu:1"):
    for c in it:
        o=c*c
        nt+=1
mt=time.time()-t
print(mt/nt)#&gt;&gt;0.02176s/batch


		</comment>
		<comment id='10' author='Pyrestone' date='2018-05-22T10:39:52Z'>
		Hi guys, I facing that error.How can I do that?
WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Use the retry module or similar alternatives.
		</comment>
	</comments>
</bug>