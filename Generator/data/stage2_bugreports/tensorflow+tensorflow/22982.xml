<bug id='22982' author='suzirui123' open_date='2018-10-15T09:10:38Z' closed_time='2018-10-30T22:39:42Z'>
	<summary>lose some early epoch checkpoint file during training</summary>
	<description>
Have I written custom code: Y
OS Platform and Distribution: centos release 6.5
TensorFlow installed from: pip install
TensorFlow version: 1.40
Bazel version: N/A
CUDA/cuDNN version: Cuda 8.0
GPU model and memory: N/A
Exact command to reproduce: N/A
Mobile device: N/A
I found myself losing some checkpoint in the target dir after training. For example, I lose 1-13 epoch checkpoint files after 60-epoch training.
I used tf.train.Saver and tf.train.Supervisor in my code:
tf.train.Supervisor( is_chief=is_chief, logdir=FLAGS.log_dir, global_step=global_step, saver=tf.train.Saver(var_list=var_list,max_to_keep=80,sharded=True), save_model_secs=FLAGS.save_model_secs, save_summaries_secs=FLAGS.save_summary_secs, init_op=tf.global_variables_initializer(), local_init_op=tf.group(tf.local_variables_initializer(),tf.tables_initializer())
where save_model_secs=1800, save_summary_secs=1800
FYI, it costs me about 30 minutes for training 1 epoch.WHY?
	</description>
	<comments>
		<comment id='1' author='suzirui123' date='2018-10-15T22:15:48Z'>
		&lt;denchmark-link:https://github.com/suzirui123&gt;@suzirui123&lt;/denchmark-link&gt;
 Hi, just to confirm, have you looked into protocol buffer to check the list of checkpoints ?
		</comment>
		<comment id='2' author='suzirui123' date='2018-10-16T03:34:56Z'>
		&lt;denchmark-link:https://github.com/Harshini-Gadige&gt;@Harshini-Gadige&lt;/denchmark-link&gt;
 I'm using distributed_training, could you please tell me where to find the protocol buffer you mentioned?
		</comment>
		<comment id='3' author='suzirui123' date='2018-10-16T03:45:40Z'>
		&lt;denchmark-link:https://github.com/Harshini-Gadige&gt;@Harshini-Gadige&lt;/denchmark-link&gt;
 Do you mean freeze the graph to check the pb of checkpoints? I haven't tried that.  But I succeeded restoring the checkpoint to predict auc of test data. I wonder whether this is caused by some arguments which cause the deleting of checkpoints?
		</comment>
		<comment id='4' author='suzirui123' date='2018-10-18T22:55:49Z'>
		&lt;denchmark-link:https://github.com/suzirui123&gt;@suzirui123&lt;/denchmark-link&gt;
  - Hi, can you please try with the tensorflow latest versions(1.10 or 1.11) with CUDA /cuDNN
9/7 and see if you are running into the same problem.
Request you to provide the following information before we look into the issue. Thank you !
-&gt;Small Code Snippet to reproduce the issue from our end.
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):
TensorFlow version (use command below):
Python version:
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

		</comment>
		<comment id='5' author='suzirui123' date='2018-10-30T22:39:42Z'>
		Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
	</comments>
</bug>