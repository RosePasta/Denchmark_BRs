<bug id='14405' author='tranhungnghiep' open_date='2017-11-09T11:05:43Z' closed_time='2017-11-10T21:38:46Z'>
	<summary>`keras` model with `tf.keras.layers.Conv2D` layers compiled with `tf.losses.softmax_cross_entropy` loss does not converge</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): simple MNIST model in keras
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 8
TensorFlow installed from (source or binary): binary from pip
TensorFlow version (use command below): 1.4
Python version: 3.5.3

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

When using tf.losses.softmax_cross_entropy in a tf.keras model with tf.keras.layers.Conv2D layer, model.fit() does not converge, although no exception raised loss does not change.
When change loss function to tf.keras.losses.categorical_crossentropy, or layer to tf.layers.Conv2D, it converges quickly.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

model.fit() log:
Train on 50000 samples, validate on 10000 samples
Epoch 1/5
50000/50000 [==============================] - 84s - loss: 2.3622 - acc: 0.0986 - val_loss: 2.3662 - val_acc: 0.0950
Epoch 2/5
50000/50000 [==============================] - 83s - loss: 2.3633 - acc: 0.0978 - val_loss: 2.3662 - val_acc: 0.0950
Epoch 3/5
50000/50000 [==============================] - 83s - loss: 2.3633 - acc: 0.0978 - val_loss: 2.3662 - val_acc: 0.0950
Epoch 4/5
50000/50000 [==============================] - 83s - loss: 2.3633 - acc: 0.0978 - val_loss: 2.3662 - val_acc: 0.0950
Epoch 5/5
50000/50000 [==============================] - 84s - loss: 2.3633 - acc: 0.0978 - val_loss: 2.3662 - val_acc: 0.0950
9984/10000 [============================&gt;.] - ETA: 0s
Test loss: 2.362950. Test accuracy: 0.098200
	</description>
	<comments>
		<comment id='1' author='tranhungnghiep' date='2017-11-09T19:59:05Z'>
		Can you provide a minimum viable sample of code that replicates this issue?
		</comment>
		<comment id='2' author='tranhungnghiep' date='2017-11-10T04:01:03Z'>
		Please see the &lt;denchmark-link:https://gist.github.com/tranhungnghiep/f2465272561aaaf68a01ad567d6c4292&gt;notebook&lt;/denchmark-link&gt;
 for replicate, note the two different outputs when changing  in the model.
Now when I check again, it is quite subtle. When changing the configuration a little the behavior will change, not sure why.
		</comment>
		<comment id='3' author='tranhungnghiep' date='2017-11-10T21:38:46Z'>
		It looks like this is a configuration problem -- if your recent changes haven't resolved your issue, please ask a question on &lt;denchmark-link:http://stackoverflow.com/questions/tagged/tensorflow&gt;StackOverflow&lt;/denchmark-link&gt;
 since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!
		</comment>
		<comment id='4' author='tranhungnghiep' date='2017-11-11T06:29:51Z'>
		@angersson imho, I think this is a bug. Notice that for the same model, tf.keras.losses.categorical_crossentropy lets it converge, but tf.losses.softmax_cross_entropy does not. So this shows a case when the compatibility in tf fails. Thanks for looking into it.
		</comment>
		<comment id='5' author='tranhungnghiep' date='2017-11-11T07:49:09Z'>
		The two functions don't even take the same arguments. One takes logits and one takes prediction. They are never compatible.
		</comment>
		<comment id='6' author='tranhungnghiep' date='2017-11-11T12:40:16Z'>
		OK, I see. I was confused by its name. Putting the logits in and it works fine. Thanks.
		</comment>
	</comments>
</bug>