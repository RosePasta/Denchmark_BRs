<bug id='30813' author='yanghoonkim' open_date='2019-07-17T17:09:30Z' closed_time='2019-08-26T17:44:03Z'>
	<summary>variable scope is changed by force when reopened (with regard to use Adam)</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):
TensorFlow version (use command below): 1.12
Python version: 2.7
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory: 1080 ti

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
Describe the expected behavior
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
&lt;denchmark-code&gt;import tensorflow as tf

with tf.variable_scope('scope'):
    a = tf.get_variable('a', [4,3,2], tf.float32)
    b = tf.get_variable('b', [4,3,2], tf.float32)
    c = tf.reduce_sum(a + b)
   
    d = tf.train.AdamOptimizer()
    #g = d.compute_gradients(c, tf.trainable_variables())
    #h = d.apply_gradients(g)

with tf.variable_scope('scope'):
    g = d.compute_gradients(c, tf.trainable_variables())
    h = d.apply_gradients(g)

print tf.global_variable()
&lt;/denchmark-code&gt;

the scope of beta1_power and beta2_power is not 'scope' but 'scope_1'.
Instead of reopening the variable_scope, if I run those two commented lines, the scope of beta1_power and beta2_power will be 'scope' not 'scope_1'
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='yanghoonkim' date='2019-07-18T10:37:40Z'>
		&lt;denchmark-link:https://github.com/yanghoonkim&gt;@yanghoonkim&lt;/denchmark-link&gt;
 ,
I tried executing the same code and faced the following error .let us know if the same issue is faced by you.Thanks!
		</comment>
		<comment id='2' author='yanghoonkim' date='2019-07-18T11:26:26Z'>
		Oh, It was my fault.The last line of the code should be print tf.global_variables()
		</comment>
		<comment id='3' author='yanghoonkim' date='2019-07-23T09:13:37Z'>
		&lt;denchmark-link:https://github.com/yanghoonkim&gt;@yanghoonkim&lt;/denchmark-link&gt;
 ,
Can you please elaborate  beta1_power ,beta2_power and scope_1.Thanks!
		</comment>
		<comment id='4' author='yanghoonkim' date='2019-07-25T01:53:36Z'>
		Please compare the two code blocks below and their printed result.
The only difference is that I reopened the tf.variable_scope('myscope') and defined 'compute_gradient', and 'apply_gradient' in the reopened scope.
The first code
&lt;denchmark-code&gt;import tensorflow as tf

with tf.variable_scope('myscope'):
    a = tf.get_variable('a', [4,3,2], tf.float32)
    b = tf.get_variable('b', [4,3,2], tf.float32)
    c = tf.reduce_sum(a + b)
   
    d = tf.train.AdamOptimizer()
    g = d.compute_gradients(c, tf.trainable_variables())
    h = d.apply_gradients(g)

print tf.global_variables()
&lt;/denchmark-code&gt;

Result:
[&lt;tf.Variable 'myscope/a:0' shape=(4, 3, 2) dtype=float32_ref&gt;, &lt;tf.Variable 'myscope/b:0' shape=(4, 3, 2) dtype=float32_ref&gt;, &lt;tf.Variable 'myscope/beta1_power:0' shape=() dtype=float32_ref&gt;, &lt;tf.Variable 'myscope/beta2_power:0' shape=() dtype=float32_ref&gt;, &lt;tf.Variable 'myscope/myscope/a/Adam:0' shape=(4, 3, 2) dtype=float32_ref&gt;, &lt;tf.Variable 'myscope/myscope/a/Adam_1:0' shape=(4, 3, 2) dtype=float32_ref&gt;, &lt;tf.Variable 'myscope/myscope/b/Adam:0' shape=(4, 3, 2) dtype=float32_ref&gt;, &lt;tf.Variable 'myscope/myscope/b/Adam_1:0' shape=(4, 3, 2) dtype=float32_ref&gt;]
The second code
&lt;denchmark-code&gt;import tensorflow as tf

with tf.variable_scope('myscope'):
    a = tf.get_variable('a', [4,3,2], tf.float32)
    b = tf.get_variable('b', [4,3,2], tf.float32)
    c = tf.reduce_sum(a + b)
   
    d = tf.train.AdamOptimizer()

with tf.variable_scope('myscope'):
    g = d.compute_gradients(c, tf.trainable_variables())
    h = d.apply_gradients(g)

print tf.global_variables()
&lt;/denchmark-code&gt;

Result:
[&lt;tf.Variable 'myscope/a:0' shape=(4, 3, 2) dtype=float32_ref&gt;, &lt;tf.Variable 'myscope/b:0' shape=(4, 3, 2) dtype=float32_ref&gt;, &lt;tf.Variable 'myscope_1/beta1_power:0' shape=() dtype=float32_ref&gt;, &lt;tf.Variable 'myscope_1/beta2_power:0' shape=() dtype=float32_ref&gt;, &lt;tf.Variable 'myscope/myscope/a/Adam:0' shape=(4, 3, 2) dtype=float32_ref&gt;, &lt;tf.Variable 'myscope/myscope/a/Adam_1:0' shape=(4, 3, 2) dtype=float32_ref&gt;, &lt;tf.Variable 'myscope/myscope/b/Adam:0' shape=(4, 3, 2) dtype=float32_ref&gt;, &lt;tf.Variable 'myscope/myscope/b/Adam_1:0' shape=(4, 3, 2) dtype=float32_ref&gt;]
And I don't know why the basic scope of beta1_power and beta2_power is changed from 'myscope' to 'myscope_1'
		</comment>
		<comment id='5' author='yanghoonkim' date='2019-07-29T12:30:14Z'>
		I was able to replicate the issue with TF version-1.12.Thanks!
		</comment>
		<comment id='6' author='yanghoonkim' date='2019-08-20T23:12:09Z'>
		Apologies for the delay in response.
You may try creating optimizer outside of the variable scope;
import tensorflow as tf

with tf.variable_scope('myscope'):#, reuse=False):
    a = tf.get_variable('a', [4,3,2], tf.float32)
    b = tf.get_variable('b', [4,3,2], tf.float32)
    c = tf.reduce_sum(a + b)
    
#Create optimizer outside of variable scope
d = tf.train.AdamOptimizer()
g = d.compute_gradients(c, tf.trainable_variables())
h = d.apply_gradients(g)

print(tf.global_variables())
Output:
[&lt;tf.Variable 'myscope/a:0' shape=(4, 3, 2) dtype=float32_ref&gt;, &lt;tf.Variable 'myscope/b:0' shape=(4, 3, 2) dtype=float32_ref&gt;, &lt;tf.Variable 'beta1_power:0' shape=() dtype=float32_ref&gt;, &lt;tf.Variable 'beta2_power:0' shape=() dtype=float32_ref&gt;, &lt;tf.Variable 'myscope/a/Adam:0' shape=(4, 3, 2) dtype=float32_ref&gt;, &lt;tf.Variable 'myscope/a/Adam_1:0' shape=(4, 3, 2) dtype=float32_ref&gt;, &lt;tf.Variable 'myscope/b/Adam:0' shape=(4, 3, 2) dtype=float32_ref&gt;, &lt;tf.Variable 'myscope/b/Adam_1:0' shape=(4, 3, 2) dtype=float32_ref&gt;]
		</comment>
		<comment id='7' author='yanghoonkim' date='2019-08-26T17:44:04Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=30813&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=30813&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='yanghoonkim' date='2019-12-02T05:30:03Z'>
		I just want to know why this kind of situation happens and is it solved in the updated version?
		</comment>
	</comments>
</bug>