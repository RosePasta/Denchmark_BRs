<bug id='38285' author='MeghnaNatraj' open_date='2020-04-06T21:36:58Z' closed_time='2020-08-24T02:28:52Z'>
	<summary>Unsupported Full-Integer TensorFlow Lite models in TF 2</summary>
	<description>
Describe the issue
In TF2, the full-integer quantized models produced by the TFLite Converter can only have float input and output type. This is a blocker for users who require int8 or uint8 input and/or output type.
UPDATE: We now support this workflow.
: &lt;denchmark-link:https://colab.sandbox.google.com/github/google-coral/tutorials/blob/master/retrain_classification_ptq_tf2.ipynb&gt;https://colab.sandbox.google.com/github/google-coral/tutorials/blob/master/retrain_classification_ptq_tf2.ipynb&lt;/denchmark-link&gt;


You can refer to the code &lt;denchmark-link:https://www.tensorflow.org/lite/performance/post_training_quantization#integer_only&gt;here&lt;/denchmark-link&gt;
, also given below:
&lt;denchmark-code&gt;import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
def representative_dataset_gen():
  for _ in range(num_calibration_steps):
    # Get sample input data as a numpy array in a method of your choosing.
    yield [input]
converter.representative_dataset = representative_dataset_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8  # or tf.uint8
converter.inference_output_type = tf.int8  # or tf.uint8
tflite_model = converter.convert()
&lt;/denchmark-code&gt;


 that the one caveat with integer-only models is this -- you need to manually map (aka quantize) the float inputs to integer inputs during inference. To understand how this can be done -- refer to the equation provided in &lt;denchmark-link:https://www.tensorflow.org/lite/performance/quantization_spec&gt;TensorFlow Lite 8-bit quantization specification&lt;/denchmark-link&gt;
 document and it's equivalent code in python below:
&lt;denchmark-code&gt;import numpy as np
import tensorflow as tf

# Input to the TF model are float values in the range [0, 10] and of size (1, 100)
np.random.seed(0)
tf_input = np.random.uniform(low=0, high=10, size=(1, 100)).astype(np.float32)

# Output of the TF model.
tf_output = keras_model.predict(input)

# Output of the TFLite model.
interpreter = tf.lite.Interpreter(model_content=tflite_model) 
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()[0]
# Manually quantize the input from float to integer
scale, zero_point = input_details['quantization']
tflite_integer_input = tf_input / scale + zero_point
tflite_integer_input = tflite_integer_input.astype(input_details['dtype'])
interpreter.set_tensor(input_details['index'], tflite_integer_input)
interpreter.invoke()
output_details = interpreter.get_output_details()[0]
tflite_integer_output = interpreter.get_tensor(output_details['index'])
# Manually dequantize the output from integer to float
scale, zero_point = output_details['quantization']
tflite_output = tflite_integer_output.astype(np.float32)
tflite_output = (tflite_output - zero_point) * scale
 
# Verify that the TFLite model's output is approximately (expect some loss in 
# accuracy due to quantization) the same as the TF model's output
assert np.allclose(tflite_output, tf_output, atol=1e-04) == True
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='MeghnaNatraj' date='2020-04-08T05:39:22Z'>
		I am interested in this issue. When fix is available, please let me know.
(I suppose I will get the notify when write message here, right?)
Ruey-An
		</comment>
		<comment id='2' author='MeghnaNatraj' date='2020-04-08T17:05:31Z'>
		&lt;denchmark-link:https://github.com/rayeh5&gt;@rayeh5&lt;/denchmark-link&gt;
 Yes. Whenever we post updates, you would receive emails.
		</comment>
		<comment id='3' author='MeghnaNatraj' date='2020-04-08T21:21:28Z'>
		&lt;denchmark-link:https://github.com/MeghnaNatraj&gt;@MeghnaNatraj&lt;/denchmark-link&gt;
: in the meanwhile, could you update &lt;denchmark-link:https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization_of_weights_and_activations&gt;https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization_of_weights_and_activations&lt;/denchmark-link&gt;
 and other documentation to reflect that the full-integer path (including inputs and outputs) is only available in TF 1.X and not TF 2.X?
		</comment>
		<comment id='4' author='MeghnaNatraj' date='2020-04-17T15:25:07Z'>
		expecting
		</comment>
		<comment id='5' author='MeghnaNatraj' date='2020-04-24T03:22:20Z'>
		Hello ,
Any updates about this issue ? Will v2 converter support input output Flags?
		</comment>
		<comment id='6' author='MeghnaNatraj' date='2020-04-24T03:52:44Z'>
		Yes, it is currently a work in progress. We will update this github issue
once it's completed.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Thu, Apr 23, 2020 at 8:22 PM mahdichtourou24051994 &lt; ***@***.***&gt; wrote:
 Hello ,
 Any updates about this issue ? Will v2 converter support input output
 Flags?

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#38285 (comment)&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/ACDCGJ2GOLCDOWIFHCDJ2VDROEAX3ANCNFSM4MCTYA6A&gt;
 .



		</comment>
		<comment id='7' author='MeghnaNatraj' date='2020-04-29T19:00:44Z'>
		Just checking in, im also waiting for v2.
		</comment>
		<comment id='8' author='MeghnaNatraj' date='2020-05-03T06:22:20Z'>
		Also waiting for this issue to be resolved, thanks.
		</comment>
		<comment id='9' author='MeghnaNatraj' date='2020-05-12T04:54:14Z'>
		waiting for this issue to be resolved, thanks.
		</comment>
		<comment id='10' author='MeghnaNatraj' date='2020-05-25T07:12:21Z'>
		waiting too
		</comment>
		<comment id='11' author='MeghnaNatraj' date='2020-05-25T08:39:57Z'>
		I am facing this issue in TF 1.15 and TF 2.
		</comment>
		<comment id='12' author='MeghnaNatraj' date='2020-05-28T07:07:34Z'>
		By the way, can we use converter to create an uint8 inference tflite model rather than an int8 inference model?
		</comment>
		<comment id='13' author='MeghnaNatraj' date='2020-05-28T18:54:41Z'>
		Note: The following discussion is not related to the current issue of supporting full integer tensorflow lite models, including input and output, in TF 2.0
&lt;denchmark-link:https://github.com/dreamPoet&gt;@dreamPoet&lt;/denchmark-link&gt;
 No, this is not possible in TensorFlow 2. We cannot create a uint8 inference tflite model and only support int8 inference model. We've moved away from the uint8 quantization because with int8 we're able to use asymmetrical quantization ranges without paying the same penalty. Refer to &lt;denchmark-link:https://www.tensorflow.org/lite/performance/quantization_spec&gt;https://www.tensorflow.org/lite/performance/quantization_spec&lt;/denchmark-link&gt;
 for more information
For more details:
Youtube: &lt;denchmark-link:https://www.youtube.com/watch?v=-jBmqY_aFwE&gt;https://www.youtube.com/watch?v=-jBmqY_aFwE&lt;/denchmark-link&gt;

Slides used in the Youtube Video: &lt;denchmark-link:https://docs.google.com/presentation/d/1zGm5bqGrkAepwJZ5PABiYjrIKq1pDnzafa8ZYeaFhXY/edit?usp=sharing&gt;https://docs.google.com/presentation/d/1zGm5bqGrkAepwJZ5PABiYjrIKq1pDnzafa8ZYeaFhXY/edit?usp=sharing&lt;/denchmark-link&gt;

		</comment>
		<comment id='14' author='MeghnaNatraj' date='2020-05-29T08:13:25Z'>
		&lt;denchmark-link:https://github.com/MeghnaNatraj&gt;@MeghnaNatraj&lt;/denchmark-link&gt;
  How about in tf1.x? I can only use toco and lite.converter without representative_dataset attributes to produce uint8 inference tflite model, while for int8 inference model, I can set representative_dataset to help quantize, is that right?
		</comment>
		<comment id='15' author='MeghnaNatraj' date='2020-06-04T23:31:40Z'>
		&lt;denchmark-link:https://github.com/dreamPoet&gt;@dreamPoet&lt;/denchmark-link&gt;
 Could you create a separate github issue describing your problem and assign it to me? I'd be glad to provide more information on the new issue.
		</comment>
		<comment id='16' author='MeghnaNatraj' date='2020-06-05T10:38:56Z'>
		&lt;denchmark-link:https://github.com/MeghnaNatraj&gt;@MeghnaNatraj&lt;/denchmark-link&gt;
 Hi, I am working on a benchmark for the Edge TPU and this is blocking me for using it, which is really frustrating. Are you aware of any workaround? Could you provide an estimation of when the converter would be ready? Since this issue wasn't mentioned anywhere I embarked on this project but now I've reached a point where there is nothing else to do but waiting.
		</comment>
		<comment id='17' author='MeghnaNatraj' date='2020-06-05T16:54:29Z'>
		&lt;denchmark-link:https://github.com/MeghnaNatraj&gt;@MeghnaNatraj&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/stefano555&gt;@stefano555&lt;/denchmark-link&gt;
  Excuse me, do your test the speed of the quantified model? I found there was only a slight increase in speed.
		</comment>
		<comment id='18' author='MeghnaNatraj' date='2020-06-06T14:33:08Z'>
		
@MeghnaNatraj @stefano555 Excuse me, do your test the speed of the quantified model? I found there was only a slight increase in speed.

I would test it, if tf lite allowed me to do that :-) since I cannot get the fully quantized model, I cannot do the benchmark
		</comment>
		<comment id='19' author='MeghnaNatraj' date='2020-06-08T01:31:11Z'>
		Update: We now support TensorFlow Lite Full-Integer models in TF 2.0, i.e, with integer (tf.int8 and tf.uint8 types) input and output.
Exception: Support for quantize-aware trained models is still in progress
: &lt;denchmark-link:https://colab.sandbox.google.com/github/google-coral/tutorials/blob/master/retrain_classification_ptq_tf2.ipynb&gt;https://colab.sandbox.google.com/github/google-coral/tutorials/blob/master/retrain_classification_ptq_tf2.ipynb&lt;/denchmark-link&gt;


You can refer to the code &lt;denchmark-link:https://www.tensorflow.org/lite/performance/post_training_quantization#integer_only&gt;here&lt;/denchmark-link&gt;
, also given below:
&lt;denchmark-code&gt;import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
def representative_dataset_gen():
  for _ in range(num_calibration_steps):
    # Get sample input data as a numpy array in a method of your choosing.
    yield [input]
converter.representative_dataset = representative_dataset_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8  # or tf.uint8
converter.inference_output_type = tf.int8  # or tf.uint8
tflite_model = converter.convert()
&lt;/denchmark-code&gt;


 that the one caveat with integer-only models is this -- you need to manually map (aka quantize) the float inputs to integer inputs during inference. To understand how this can be done -- refer to the equation provided in &lt;denchmark-link:https://www.tensorflow.org/lite/performance/quantization_spec&gt;TensorFlow Lite 8-bit quantization specification&lt;/denchmark-link&gt;
 document and it's equivalent code in python below:
&lt;denchmark-code&gt;import numpy as np
import tensorflow as tf

# Input to the TF model are float values in the range [0, 10] and of size (1, 100)
np.random.seed(0)
tf_input = np.random.uniform(low=0, high=10, size=(1, 100)).astype(np.float32)

# Output of the TF model.
tf_output = keras_model.predict(input)

# Output of the TFLite model.
interpreter = tf.lite.Interpreter(model_content=tflite_model) 
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()[0]
# Manually quantize the input from float to integer
scale, zero_point = input_details['quantization']
tflite_integer_input = tf_input / scale + zero_point
tflite_integer_input = tflite_integer_input.astype(input_details['dtype'])
interpreter.set_tensor(input_details['index'], tflite_integer_input)
interpreter.invoke()
output_details = interpreter.get_output_details()[0]
tflite_integer_output = interpreter.get_tensor(output_details['index'])
# Manually dequantize the output from integer to float
scale, zero_point = output_details['quantization']
tflite_output = tflite_integer_output.astype(np.float32)
tflite_output = (tflite_output - zero_point) * scale
 
# Verify that the TFLite model's output is approximately (expect some loss in 
# accuracy due to quantization) the same as the TF model's output
assert np.allclose(tflite_output, tf_output, atol=1e-04) == True
&lt;/denchmark-code&gt;

		</comment>
		<comment id='20' author='MeghnaNatraj' date='2020-07-01T13:31:17Z'>
		
@hahadashi could you confirm if you are using delegate when using the models from your Android phone?
thxs your reply.

i just follow the tensorflow/lite/g3doc/guide/inference.md
(```c++
// Load the model
std::unique_ptrtflite::FlatBufferModel model =
tflite::FlatBufferModel::BuildFromFile(filename);
// Build the interpreter
tflite::ops::builtin::BuiltinOpResolver resolver;
std::unique_ptrtflite::Interpreter interpreter;
tflite::InterpreterBuilder(*model, resolver)(&amp;interpreter);
// Resize input tensors, if desired.
interpreter-&gt;AllocateTensors();
float* input = interpreter-&gt;typed_input_tensor(0);
// Fill input.
interpreter-&gt;Invoke();
float* output = interpreter-&gt;typed_output_tensor(0);
&lt;denchmark-code&gt;
develop a inference code to run the tflite on the phone.  i need do more thing?
&lt;/denchmark-code&gt;

		</comment>
		<comment id='21' author='MeghnaNatraj' date='2020-07-01T14:30:32Z'>
		You might want to follow this tutorial once: &lt;denchmark-link:https://www.tensorflow.org/lite/performance/gpu&gt;https://www.tensorflow.org/lite/performance/gpu&lt;/denchmark-link&gt;
. Here's the demo code snippet:
&lt;denchmark-link:https://user-images.githubusercontent.com/22957388/86256183-a17d6800-bbd5-11ea-9af4-924c5119796e.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='22' author='MeghnaNatraj' date='2020-07-01T15:51:35Z'>
		
You might want to follow this tutorial once: https://www.tensorflow.org/lite/performance/gpu. Here's the demo code snippet:


thxs, i have a question. why we need use GPU delegate to accelerate speed by gpu. the arm cpu not support quant inference?
i see the tensorflow.org,  it say quant model can speed up 1.5X ~ 4X
		</comment>
		<comment id='23' author='MeghnaNatraj' date='2020-07-07T18:53:06Z'>
		
https://www.youtube.com/watch?v=-jBmqY_aFwE

&lt;denchmark-link:https://github.com/MeghnaNatraj&gt;@MeghnaNatraj&lt;/denchmark-link&gt;
 thanks for the great link to Pete Wardens screencast!
		</comment>
		<comment id='24' author='MeghnaNatraj' date='2020-07-13T01:59:33Z'>
		&lt;denchmark-link:https://github.com/stefano555&gt;@stefano555&lt;/denchmark-link&gt;
 I am working on debugging your issue right now. Looks like &lt;denchmark-link:https://drive.google.com/corp/drive/folders/11XruNeJzdIm9DTn7FnuIWYaSalqg2F0B&gt;https://drive.google.com/corp/drive/folders/11XruNeJzdIm9DTn7FnuIWYaSalqg2F0B&lt;/denchmark-link&gt;
 has a ton of jupyter notebooks, and i'm not sure how to debug this. Please create a new github issue for "TFLite Converter", add me as the assignee and post detailed instructions on how I can reproduce your issue.
		</comment>
		<comment id='25' author='MeghnaNatraj' date='2020-07-21T00:19:57Z'>
		It looks like the issue has been solved in TF 2.4? Could we get an official confirmation?
		</comment>
		<comment id='26' author='MeghnaNatraj' date='2020-07-28T12:47:57Z'>
		&lt;denchmark-link:https://github.com/MeghnaNatraj&gt;@MeghnaNatraj&lt;/denchmark-link&gt;
 is this solved in TF 2.4 (nightly)?
		</comment>
		<comment id='27' author='MeghnaNatraj' date='2020-07-28T17:26:13Z'>
		Hello, the issue is resolved now with TF 2.4 (nightly). Let us know if you face any issue. (This issue will remain open until we also fix all documentation, and resolve any issues that may arise in the next few days)
		</comment>
		<comment id='28' author='MeghnaNatraj' date='2020-07-28T21:42:15Z'>
		Dear &lt;denchmark-link:https://github.com/MeghnaNatraj&gt;@MeghnaNatraj&lt;/denchmark-link&gt;
, sorry for answering you only now. The issue has not been resolved. I will create a new GitHub issue and add you as an assignee.
		</comment>
		<comment id='29' author='MeghnaNatraj' date='2020-07-29T05:42:14Z'>
		&lt;denchmark-link:https://github.com/MeghnaNatraj&gt;@MeghnaNatraj&lt;/denchmark-link&gt;
 ran into:  TensorFlow Version: . Let me know if you'd want me to create a separate issue.
Here's the &lt;denchmark-link:https://colab.research.google.com/gist/sayakpaul/8c8a1d7c94beca26d93b67d92a90d3f0/qat-bad-accuracy.ipynb&gt;Colab Gist&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='30' author='MeghnaNatraj' date='2020-07-29T16:01:09Z'>
		&lt;denchmark-link:https://github.com/MeghnaNatraj&gt;@MeghnaNatraj&lt;/denchmark-link&gt;
 I wasn't able to assign you to the new issue, you can find it here &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/41840#issue-667409532&gt;#41840 (comment)&lt;/denchmark-link&gt;

		</comment>
		<comment id='31' author='MeghnaNatraj' date='2020-07-30T08:58:28Z'>
		sorry, I don't know if the question is appropriate to ask here:
it seems that the int8 quantization is a symmetric quantization one, will TF-Lite (or Micro vesion) has plan to support "symmetric quantization"?
thanks for all of your time
		</comment>
		<comment id='32' author='MeghnaNatraj' date='2020-08-05T16:32:14Z'>
		&lt;denchmark-link:https://github.com/sayakpaul&gt;@sayakpaul&lt;/denchmark-link&gt;
 Could you create a separate issue and tag me on it? I am looking into it.
&lt;denchmark-link:https://github.com/stefano555&gt;@stefano555&lt;/denchmark-link&gt;
 I am looking into &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/41840&gt;#41840&lt;/denchmark-link&gt;
 right now.
&lt;denchmark-link:https://github.com/rayeh5&gt;@rayeh5&lt;/denchmark-link&gt;
  could you explain your question in detail? Currently we support int8 quantization by default.
		</comment>
		<comment id='33' author='MeghnaNatraj' date='2020-08-05T16:48:16Z'>
		&lt;denchmark-link:https://github.com/MeghnaNatraj&gt;@MeghnaNatraj&lt;/denchmark-link&gt;
 I don't think community members can assign someone to an issue (correct me if I am wrong). I referred to the following statement of yours and hence I decided to inform you about the issue here.

(This issue will remain open until we also fix all documentation, and resolve any issues that may arise in the next few days)

Please let me know if you'd still like me to open up a new issue.
		</comment>
		<comment id='34' author='MeghnaNatraj' date='2020-08-05T17:28:09Z'>
		&lt;denchmark-link:https://github.com/sayakpaul&gt;@sayakpaul&lt;/denchmark-link&gt;
 Yes, community members cannot assign an issue to a specific person. You can create a new issue, post the details about your issue (as in this &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/38285#issuecomment-665444152&gt;comment&lt;/denchmark-link&gt;
 and post a link to the issue here. I will continue the discussion on that thread.
		</comment>
		<comment id='35' author='MeghnaNatraj' date='2020-08-06T01:53:56Z'>
		&lt;denchmark-link:https://github.com/sayakpaul&gt;@sayakpaul&lt;/denchmark-link&gt;
  I have created a separate issue here: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/42082&gt;#42082&lt;/denchmark-link&gt;

		</comment>
		<comment id='36' author='MeghnaNatraj' date='2020-08-06T02:21:53Z'>
		
@sayakpaul Could you create a separate issue and tag me on it? I am looking into it.
@stefano555 I am looking into #41840 right now.
@rayeh5 could you explain your question in detail? Currently we support int8 quantization by default.

As my understanding of the "TensorFlow Lite 8-bit quantization specification", Weights are symmetric, but the Activations are asymmetric. Is it possible to support both as symmetric quantization?
Thanks for your time.
ruey-an
		</comment>
		<comment id='37' author='MeghnaNatraj' date='2020-08-06T03:37:01Z'>
		

@sayakpaul Could you create a separate issue and tag me on it? I am looking into it.
@stefano555 I am looking into #41840 right now.
@rayeh5 could you explain your question in detail? Currently we support int8 quantization by default.

As my understanding of the "TensorFlow Lite 8-bit quantization specification", Weights are symmetric, but the Activations are asymmetric. Is it possible to support both as symmetric quantization?
Thanks for your time.
ruey-an

&lt;denchmark-link:https://github.com/suharshs&gt;@suharshs&lt;/denchmark-link&gt;
 could you respond to this question?
		</comment>
		<comment id='38' author='MeghnaNatraj' date='2020-08-13T08:16:58Z'>
		&lt;denchmark-link:https://github.com/MeghnaNatraj&gt;@MeghnaNatraj&lt;/denchmark-link&gt;
  hi, i use post quantization as follows:
&lt;denchmark-code&gt;.........
converter.representative_dataset = representative_dataset_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
&lt;/denchmark-code&gt;

only input and output  of the quantization model  have the  datatype uint8,  the convolution of the middle layer is still int8.
how do I get  the model that  is  all uint8?  thanks
		</comment>
		<comment id='39' author='MeghnaNatraj' date='2020-08-13T18:57:50Z'>
		&lt;denchmark-link:https://github.com/aa12356jm&gt;@aa12356jm&lt;/denchmark-link&gt;
 This is currently unsupported, you can refer to this &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/38285#issuecomment-635533037&gt;comment&lt;/denchmark-link&gt;
 for more information.
		</comment>
		<comment id='40' author='MeghnaNatraj' date='2020-08-24T02:28:52Z'>
		Closing this issue as it has been resolved in the latest tensorflow version (, version &gt;= 2.4.0-dev20200823). The documentation for TensorFlow Lite has been updated. If you face an issue due to this change, either re-open this issue or create a new one and mention this issue (by adding &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/38285&gt;#38285&lt;/denchmark-link&gt;
 in a comment)
		</comment>
		<comment id='41' author='MeghnaNatraj' date='2020-08-24T02:28:54Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38285&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38285&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>