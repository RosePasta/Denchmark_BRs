<bug id='25754' author='ageron' open_date='2019-02-14T12:34:33Z' closed_time='2019-07-15T18:47:27Z'>
	<summary>Model.fit() leaves training arg and learning_phase unset (TF2-preview)</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mac OS X 10.13.6
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
TensorFlow installed from (source or binary):
binary
TensorFlow version (use command below):
VERSION='2.0.0-dev20190213'
GIT_VERSION="v1.12.0-7959-gabeb4d0acd"
Python version:
3.6.8
Bazel version (if compiling from source):
N/A
GCC/Compiler version (if compiling from source):
N/A
CUDA/cuDNN version:
N/A
GPU model and memory:
N/A

Describe the current behavior
When training a Keras model containing a custom layer whose behavior depends on the training phase, training is not set by the fit() method, and K.learning_phase() is 0. I can work around this issue by calling K.set_learning_phase(1) before creating the model, but oddly this does not work if I call K.set_learning_phase(1) after creating and compiling the model, and just before calling the fit() method. It looks like the learning_phase is "hard-coded" into the call() method's graph when the model is built?
Describe the expected behavior
I expected the training argument to be set to True (or 1) automatically when I called the fit() method. Alternatively, I expected that at least K.learning_phase() would return True (or 1), but it's always 0.
Code to reproduce the issue
If you're in a hurry, just look at the failing tests: C, H and I.
import numpy as np
import tensorflow as tf
from tensorflow import keras
K = keras.backend

class MyLayer(keras.layers.Layer):
    @tf.function
    def call(self, inputs, training=None):
        if training is None:
            training = K.learning_phase()
        tf.print("training: ", training)
        return keras.backend.in_test_phase(inputs + 1., inputs + 2., training)

X = np.zeros((1, 2))

print("Test A: MyLayer()(X)")
y_pred = MyLayer()(X)
print("Pred:", y_pred)
print("training should be 0\n")

print("Test B: MyLayer()(X, training=True)")
y_pred = MyLayer()(X, training=True)
print("Pred:", y_pred)
print("training should be True\n")

print("Test C: model = ...; model.compile(...); model.fit(...)")
model = keras.models.Sequential([MyLayer(input_shape=[2])])
model.compile(loss="mse", optimizer="sgd")
history = model.fit(X, X, epochs=2)
print("training should be 1 (used to fail, now passes)\n")

print("Test D: K.set_learning_phase(1); MyLayer()(X)")
K.set_learning_phase(1)
y_pred = MyLayer()(X)
print("Pred:", y_pred)
print("training should be 1\n")
K.clear_session()

print("Test E: layer = MyLayer(); K.set_learning_phase(1); layer(X)")
layer = MyLayer()
K.set_learning_phase(1)
y_pred = layer(X)
print("Pred:", y_pred)
print("training should be 1\n")
K.clear_session()

print("Test F: K.set_learning_phase(1); layer = MyLayer(); K.set_learning_phase(0); layer(X)")
K.set_learning_phase(1)
layer = MyLayer()
K.set_learning_phase(0)
y_pred = layer(X)
print("Pred:", y_pred)
print("training should be 0\n")
K.clear_session()

print("Test G: K.set_learning_phase(1); model = ...; model.compile(...); model.fit(...)")
K.set_learning_phase(1)
model = keras.models.Sequential([MyLayer(input_shape=[2])])
model.compile(loss="mse", optimizer="sgd")
history = model.fit(X, X, epochs=2)
print("training should be 1\n")
K.clear_session()

print("Test H: model = ...; model.compile(...); K.set_learning_phase(1); model.fit(...)")
model = keras.models.Sequential([MyLayer(input_shape=[2])])
model.compile(loss="mse", optimizer="sgd")
K.set_learning_phase(1)
history = model.fit(X, X, epochs=2)
print("training should be 1 (ERROR?)\n")
K.clear_session()

print("Test I: K.set_learning_phase(1); model = ...; K.set_learning_phase(0); model.compile(...); model.fit(...)")
K.set_learning_phase(1)
model = keras.models.Sequential([MyLayer(input_shape=[2])])
K.set_learning_phase(0)
model.compile(loss="mse", optimizer="sgd")
history = model.fit(X, X, epochs=2)
print("This test does not make much sense, why would you call fit with learning phase 0?\n")
K.clear_session()
	</description>
	<comments>
		<comment id='1' author='ageron' date='2019-02-14T14:38:30Z'>
		Ah, I think I got it: I'm using @tf.function, so the call() method gets traced, and at that point K.learning_phase() gets called and it turns out that it returns 0 as an integer, instead of a symbolic tensor, so this value gets burnt into the graph (edit: actually it does return a symbolic tensor, but it always evaluates to 0). This would explain all 3 failing tests above.
Shouldn't K.learning_phase() return a symbolic tensor when running in graph mode? (edit: it does, but apparently, it is always 0)
		</comment>
		<comment id='2' author='ageron' date='2019-02-14T21:28:40Z'>
		&lt;denchmark-link:https://github.com/ageron&gt;@ageron&lt;/denchmark-link&gt;
 Is this resolved? If yes, please close it. Thanks!
		</comment>
		<comment id='3' author='ageron' date='2019-02-15T00:22:22Z'>
		Hi &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 ,
No, it's not resolved, "I think I got it" just meant I thought I understood the issue, not that it was fixed, I'll be clearer next time. I think what's happening is that when  is run in the context of a , it returns a symbolic tensor that is not affected by the  method.
		</comment>
		<comment id='4' author='ageron' date='2019-02-25T09:57:18Z'>
		FYI, &lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;
 wrote :"I think in this case tf.function is extracting the layer's call in a form where it is no longer dependent on the learning phase placeholder. The  method sets the value of the placeholder, but by that time the tf.function is completely independent from it (and relies on the default value of 0)."
And: "I have a fix for this issue, that should be reflected in the nightly build within a few days."
		</comment>
		<comment id='5' author='ageron' date='2019-02-27T04:57:04Z'>
		Hi &lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;
, test C (which is the most important), now seems to work, but unfortunately not test H and test I. I'm using version 2.0.0-dev20190226 (git version v1.12.0-8969-g0ae3728b74).
Side note: it would be nice if keras.set_learning_phase(None) could reset the learning phase to the default mode, so we wouldn't have to clear the whole session for that. Wdyt?
		</comment>
		<comment id='6' author='ageron' date='2019-02-27T06:25:37Z'>
		Oops, I think the fix created a new issue (I can create a new issue if you prefer, but I think it's all about the same thing):
from tensorflow import keras
import numpy as np

model = keras.models.Sequential([keras.layers.Dense(1)])
model.compile(loss="mse", optimizer="sgd")
model.fit(np.random.rand(1000, 1), np.random.rand(1000, 1))

with keras.backend.learning_phase_scope(1):
    model.fit(np.random.rand(1000, 1), np.random.rand(1000, 1))
Last line raise this exception:
&lt;denchmark-code&gt;---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-3-b6b901d6926c&gt; in &lt;module&gt;
      7
      8 with keras.backend.learning_phase_scope(1):
----&gt; 9     model.fit(np.random.rand(1000, 1), np.random.rand(1000, 1))
     10

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    871           validation_steps=validation_steps,
    872           validation_freq=validation_freq,
--&gt; 873           steps_name='steps_per_epoch')
    874
    875   def evaluate(self,

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)
    349
    350         # Get outputs.
--&gt; 351         batch_outs = f(ins_batch)
    352         if not isinstance(batch_outs, list):
    353           batch_outs = [batch_outs]

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/backend.py in __call__(self, inputs)
   3215         value = math_ops.cast(value, tensor.dtype)
   3216       converted_inputs.append(value)
-&gt; 3217     outputs = self._graph_fn(*converted_inputs)
   3218     return nest.pack_sequence_as(self._outputs_structure,
   3219                                  [x.numpy() for x in outputs])

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
    524       raise TypeError("Keyword arguments {} unknown. Expected {}.".format(
    525           list(kwargs.keys()), list(self._arg_keywords)))
--&gt; 526     return self._call_flat(args)
    527
    528   def _filtered_call(self, args, kwargs):

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args)
    593     # Only need to override the gradient in graph mode and when we have outputs.
    594     if context.executing_eagerly() or not self.outputs:
--&gt; 595       outputs = self._inference_function.call(ctx, args)
    596     else:
    597       self._register_gradient()

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args)
    363       raise ValueError(
    364           "Arguments and signature arguments do not match: %s %s " %
--&gt; 365           (len(args), len(list(self.signature.input_arg))))
    366
    367     function_call_options = ctx.get_function_call_options()

ValueError: Arguments and signature arguments do not match: 7 8
&lt;/denchmark-code&gt;

I suppose this is because the first call to fit() creates a FuncGraph with 7 arguments, but the learning_phase_scope() adds an extra argument, so it fails.
		</comment>
		<comment id='7' author='ageron' date='2019-04-06T03:24:22Z'>
		Hi &lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;
, unfortunately, right now the  argument seems broken (always ), using:
&lt;denchmark-code&gt;tf.version.VERSION=2.0.0-dev20190405
tf.version.GIT_VERSION=v1.12.0-11808-ga1e3d4490d
&lt;/denchmark-code&gt;

Because of the learning_phase issues, I am trying to use the training argument instead, but it is not set by the fit() method, as shown in the following code:
import numpy as np
import tensorflow as tf
from tensorflow import keras

class MyLayer(keras.layers.Layer):
    def call(self, inputs, training=None):
        print("(tracing) training =", training)
        tf.print("(running) training =", training)
        return inputs + 1.

model = keras.models.Sequential([MyLayer()])
model.compile(loss="mse", optimizer="sgd")
X_train, Y_train, X_valid, Y_valid = np.random.rand(4, 100, 2)
model.fit(X_train, Y_train, validation_data=(X_valid, Y_valid))
Y_pred = model.predict(X_valid)
This code displays:
&lt;denchmark-code&gt;(tracing) training = None
...
(running) training = None
...
(running) training = None
(running) training = None
(running) training = None
...
&lt;/denchmark-code&gt;

This is true even if I decorate the call() method with @tf.function.
		</comment>
		<comment id='8' author='ageron' date='2019-05-08T09:30:30Z'>
		Any updates on this?
		</comment>
		<comment id='9' author='ageron' date='2019-05-08T16:47:20Z'>
		&lt;denchmark-link:https://github.com/ageron&gt;@ageron&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/gijswobben&gt;@gijswobben&lt;/denchmark-link&gt;
 , are you still seeing this issue? There was a commit &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/cf31e9001c5ab89fc2e92eb98443d48cd37a726b#diff-8bfbe586f2ba61d9aa9e39676c7f69ee&gt;cf31e90#diff-8bfbe586f2ba61d9aa9e39676c7f69ee&lt;/denchmark-link&gt;
 which may have fixed this issue. Can you try on the latest nightly release?
		</comment>
		<comment id='10' author='ageron' date='2019-05-09T01:35:01Z'>
		Hi &lt;denchmark-link:https://github.com/pavithrasv&gt;@pavithrasv&lt;/denchmark-link&gt;
 ,
I just tested again, unfortunately there are still some issues:

The good news is that the training argument now works as expected.
But the learning_phase seems to be always equal to 0.  I think TF2 should get rid of global scopes anyway, and this includes K.learning_phase, so I would recommend just deprecating learning_phase and learning_phase_scope altogether. However, it's part of the Keras API.

Here's a little test that highlights what works and what doesn't:
import numpy as np
import tensorflow as tf
from tensorflow import keras
K = keras.backend

X = np.zeros((1, 2))

class MyLayer(keras.layers.Layer):
    @tf.function
    def call(self, inputs, training=None):
        tf.print("training: ", training)
        tf.print("K.learning_phase(): ", K.learning_phase())
        return keras.backend.in_test_phase(inputs + 1., inputs + 2., training)

model = keras.models.Sequential([MyLayer(input_shape=[2])])
model.compile(loss="mse", optimizer="sgd")
print("_" * 80)
print("&gt;&gt;&gt; model.fit(...)")
history = model.fit(X, X, epochs=2)
print("_" * 80)
print("&gt;&gt;&gt; model(X)")
print("=&gt;", model(X))
print("_" * 80)
print("&gt;&gt;&gt; model(X, training=True)")
print("=&gt;", model(X, training=True))
print("_" * 80)
print("&gt;&gt;&gt; model(X, training=False)")
print("=&gt;", model(X, training=False))
print("_" * 80)
print("&gt;&gt;&gt; with K.learning_phase_scope(1): model(X)")
with K.learning_phase_scope(1):
    print("=&gt;", model(X))
It outputs this:
&lt;denchmark-code&gt;________________________________________________________________________________
&gt;&gt;&gt; model.fit(...)
Epoch 1/2
training:  1
K.learning_phase():  0
1/1 [==============================] - 0s 25ms/sample - loss: 4.0000
Epoch 2/2
training:  1
K.learning_phase():  0
1/1 [==============================] - 0s 959us/sample - loss: 4.0000
________________________________________________________________________________
&gt;&gt;&gt; model(X)
training:  None
K.learning_phase():  0
=&gt; tf.Tensor([[1. 1.]], shape=(1, 2), dtype=float64)
________________________________________________________________________________
&gt;&gt;&gt; model(X, training=True)
training:  True
K.learning_phase():  0
=&gt; tf.Tensor([[2. 2.]], shape=(1, 2), dtype=float64)
________________________________________________________________________________
&gt;&gt;&gt; model(X, training=False)
training:  False
K.learning_phase():  0
=&gt; tf.Tensor([[1. 1.]], shape=(1, 2), dtype=float64)
________________________________________________________________________________
&gt;&gt;&gt; with K.learning_phase_scope(1): model(X)
training:  None
K.learning_phase():  0
=&gt; tf.Tensor([[1. 1.]], shape=(1, 2), dtype=float64)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='11' author='ageron' date='2019-06-06T15:57:58Z'>
		When is this issue going to be fixed?
		</comment>
		<comment id='12' author='ageron' date='2019-06-06T21:11:13Z'>
		Sorry about the delay. The issue here is because of the @tf.function decoration on call. It causes learning phase value to be cached.
Decorating small code snippets using tf.function will lead to lot of performance overhead and is not recommended. Also, if you are training using fit this is not required. If you are training using custom training loops, we recommend decorating the training step function with tf.function.
Thank you!
		</comment>
		<comment id='13' author='ageron' date='2019-07-15T18:47:27Z'>
		Closing this as the main questions have been addressed I think.Thank you!
		</comment>
		<comment id='14' author='ageron' date='2019-07-15T18:47:29Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=25754&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=25754&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>