<bug id='36984' author='fuhailin' open_date='2020-02-23T02:19:51Z' closed_time='2020-03-11T19:45:18Z'>
	<summary>TensorFlow 2.x prevents me using two tf.data.Dataset in multiprocess</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock
example script provided in TensorFlow):
ðŸ‘‰
OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04):
ðŸ‘‰Macos Catalina version 10.15.3
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device:
ðŸ‘‰No
TensorFlow installed from (source or
binary): - TensorFlow version (use command below):
ðŸ‘‰From binary, 2.0.0 and 2.1.0
Python version: - Bazel
version (if compiling from source):
ðŸ‘‰Python 3.7.4
GCC/Compiler version (if compiling from
source):
ðŸ‘‰No
CUDA/cuDNN version: - GPU model and memory:
ðŸ‘‰No
You can collect some of this information using our environment capture
script
You can also obtain the TensorFlow version with: 1. TF 1.0: python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)" 2. TF 2.0: python -c "import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"

Describe the current behavior
ðŸ‘‰ In my multiprocess program, I create a Queue for process_communication, but when I use two different tf.data.Dataset in different Process my program gets stuck.
Describe the expected behavior
ðŸ‘‰ When I replace one of these tf.data.Dataset to a list, everything work ideally.  And this piece of code transfer from TF1.x which also works well.
Standalone code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
import os
from multiprocessing import Process, Queue

import tensorflow as tf


class Trainable(object):
    def __init__(self):
        self.queue = Queue()
        self.valid_handle = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5])  # [1, 2, 3, 4, 5]
        self.train_handle = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6, 7, 8, 9])

    def eval(self, q):
        print('Process to write: %s' % os.getpid())
        tmp = -1
        for parsed_record in self.valid_handle:
            print(parsed_record)
            tmp = parsed_record
        self.queue.put((q + 1, tmp))

    def train(self):
        process = None
        print('Process to read: %s' % os.getpid())
        for context in self.train_handle:
            if process:
                valid_detail = self.queue.get()
                process = None
                print(valid_detail)
                if 8 in valid_detail:
                    print('Early stopping')
                    break

            process = Process(target=self.eval, args=(context.numpy(),))
            process.start()
            if not self.queue.empty():
                valid_detail = self.queue.get()
                process = None
                print(valid_detail)


if __name__ == '__main__':
    model = Trainable()
    model.train()

 Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
Here is my question posted on StackOverflow : &lt;denchmark-link:https://stackoverflow.com/questions/60354870/tensorflow-2-x-prevent-me-using-two-tf-data-dataset-in-multiprocess-why&gt;https://stackoverflow.com/questions/60354870/tensorflow-2-x-prevent-me-using-two-tf-data-dataset-in-multiprocess-why&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='fuhailin' date='2020-02-24T10:56:41Z'>
		&lt;denchmark-link:https://github.com/fuhailin&gt;@fuhailin&lt;/denchmark-link&gt;

I have tried on colab with TF version 2.1.0 beta and was able to reproduce the issue.However I think this was resolved recently in tf-nightly(). I ran it with tf-nightly without any issue. Please check the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/880272580be3b640c8cc599cbc771def/untitled666.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='fuhailin' date='2020-02-24T11:34:25Z'>
		Thanks, it seems this bug will be fixed in next stable version.
		</comment>
		<comment id='3' author='fuhailin' date='2020-02-24T15:01:10Z'>
		I have tried to replace one of the data handle to a TFRecordDataset, the problem seems still existent in tf-nightly(!pip install tf-nightly) with multiprocess. Here is my another standalone code to reproduce the issue:
import os
from multiprocessing import Process, Queue

import tensorflow as tf

print(tf.version.GIT_VERSION, tf.version.VERSION)
# v1.12.1-25210-gcafd3318ed 2.2.0-dev20200219

fsns_test_file = tf.keras.utils.get_file("fsns.tfrec",
                                         "https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001")

class Trainable(object):

    def __init__(self):
        self.queue = Queue()
        self.valid_handle = tf.data.TFRecordDataset(filenames=[fsns_test_file])
        self.train_handle = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6, 7, 8, 9])

    def eval(self, q):
        print('Process to write: %s' % os.getpid())
        tmp = -1
        for parsed_record in self.valid_handle:
            print(parsed_record)
            tmp = parsed_record
        self.queue.put((q + 1, tmp))

    def train(self):
        process = None
        print('Process to read: %s' % os.getpid())
        for context in self.train_handle:
            if process:
                valid_detail = self.queue.get()
                process = None
                print(valid_detail)
                if 1 in valid_detail:
                    print('Early stopping')
                    break

            process = Process(target=self.eval, args=(context.numpy(),))
            process.start()
            if not self.queue.empty():
                valid_detail = self.queue.get()
                process = None
                print(valid_detail)


if __name__ == '__main__':
    model = Trainable()
    model.train()
		</comment>
		<comment id='4' author='fuhailin' date='2020-02-25T05:19:37Z'>
		I have tried on colab with TF nightly version with the recent code and was able to reproduce the issue.Please,find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/d42bbf325dad48866e49ae0ddc4feb03/untitled669.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='5' author='fuhailin' date='2020-03-11T19:45:18Z'>
		TLDR: Using multiprocessing with TF is not guaranteed to work.
TensorFlow runtime internally uses multi-threading and forking a multi-threaded program is generally not safe.  As per POSIX standard:

A process shall be created with a single thread. If a multi-threaded process calls fork(), the new process shall contain a replica of the calling thread and its entire address space, possibly including the states of mutexes and other resources. Consequently, to avoid errors, the child process may only execute async-signal-safe operations until such time as one of the exec functions is called. [THR] [Option Start]  Fork handlers may be established by means of the pthread_atfork() function in order to maintain application invariants across fork() calls.

The reason your program is hanging is because the single thread replicated in the child process attempts to (inside of TensorFlow runtime) coordinate with others threads.
You could "fix" your program as follows:
&lt;denchmark-code&gt;    def __init__(self):
        self.queue = Queue()
        options = tf.data.Options()
        options.experimental_optimization.autotune = False
        self.valid_handle = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5]).with_options(options)
        self.train_handle = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6, 7, 8, 9])
&lt;/denchmark-code&gt;

but there is no guarantee that as you start relying on other parts of TensorFlow runtime, this issue won't resurface.
		</comment>
		<comment id='6' author='fuhailin' date='2020-03-11T19:45:20Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36984&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36984&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='fuhailin' date='2020-11-06T22:57:49Z'>
		
TLDR: Using multiprocessing with TF is not guaranteed to work.
TensorFlow runtime internally uses multi-threading and forking a multi-threaded program is generally not safe. As per POSIX standard:

A process shall be created with a single thread. If a multi-threaded process calls fork(), the new process shall contain a replica of the calling thread and its entire address space, possibly including the states of mutexes and other resources. Consequently, to avoid errors, the child process may only execute async-signal-safe operations until such time as one of the exec functions is called. [THR] [Option Start]  Fork handlers may be established by means of the pthread_atfork() function in order to maintain application invariants across fork() calls.

The reason your program is hanging is because the single thread replicated in the child process attempts to (inside of TensorFlow runtime) coordinate with others threads.
You could "fix" your program as follows:
    def __init__(self):
        self.queue = Queue()
        options = tf.data.Options()
        options.experimental_optimization.autotune = False
        self.valid_handle = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5]).with_options(options)
        self.train_handle = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6, 7, 8, 9])

but there is no guarantee that as you start relying on other parts of TensorFlow runtime, this issue won't resurface.

Forking may not work, but importing from multiprocessing import set_start_method, get_context and starting with
&lt;denchmark-code&gt;if __name__ == '__main__':
    set_start_method("spawn", force=True)
    with get_context("spawn").Pool(1) as pool: 
        pool.starmap 
&lt;/denchmark-code&gt;

worked for me. Sure, this may limit the design of the software a bit, but at least I'm not facing any memory issues after three cross-validation folds and can happily train a six-headed DenseNet beast until eternity.
		</comment>
	</comments>
</bug>