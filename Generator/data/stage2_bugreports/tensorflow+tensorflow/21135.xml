<bug id='21135' author='ybsave' open_date='2018-07-25T22:09:23Z' closed_time='2019-09-13T21:09:39Z'>
	<summary>[Bug] Conflict between Tensorboard and Tensorflow Training</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.9
Python version: 3.6
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: 9.0/7
GPU model and memory: Quadro M4000
Exact command to reproduce: N/A

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
Using the new Tensorflow v1.9 framework's keras interfaces, I just add Tensorboard callback in the example Keras classification codes. However, when tensorboard is activated, the training crashed. The error is
2018-07-24 16:08:28.615130: W T:\src\github\tensorflow\tensorflow\core\framework\op_kernel.cc:1318] OP_REQUIRES failed at save_restore_v2_ops.cc:137 : Unknown: Failed to rename: E:\projects\keras1\sc_weight.ckpt.index.tempstate893681564054566973 to: E:\projects\keras1\sc_weight.ckpt.index : Access is denied.
When tensorboard is not used, training has no problem.
It seems that the Tensorflow needs to rename the files every time when saving occurs. The tensorboard people thinks they could do nothing on their end, and the solution should be based the Tensorflow end. (See the last response in the link below)
&lt;denchmark-link:https://github.com/tensorflow/tensorboard/issues/892#issuecomment-407901434&gt;tensorflow/tensorboard#892 (comment)&lt;/denchmark-link&gt;

Could people on Tensorflow's end fix this "renaming" issue? Thank you.
	</description>
	<comments>
		<comment id='1' author='ybsave' date='2018-08-05T18:33:24Z'>
		&lt;denchmark-link:https://github.com/asimshankar&gt;@asimshankar&lt;/denchmark-link&gt;
 could you please take a look or assign it to Object Detection API folks if its on their end.
		</comment>
		<comment id='2' author='ybsave' date='2018-08-23T05:23:38Z'>
		If I understand correctly, the checkpoint is being written to the same file repeatedly. Can this be avoided, i.e., each checkpoint be written with a different file prefix so that the checkpoint file isn't being rewritten while Tensorboard is reading it?
		</comment>
		<comment id='3' author='ybsave' date='2018-09-06T20:03:10Z'>
		The issue has not been solved. I think that if Tensorflow side changes the saving scheme as &lt;denchmark-link:https://github.com/asimshankar&gt;@asimshankar&lt;/denchmark-link&gt;
 mentioned, the problem would be solved.
		</comment>
		<comment id='4' author='ybsave' date='2018-09-07T05:58:17Z'>
		&lt;denchmark-link:https://github.com/ybsave&gt;@ybsave&lt;/denchmark-link&gt;
 : My question was intended for you - could you avoid writing the checkpoint to the same file repeatedly?
Or, can you provide a &lt;denchmark-link:https://stackoverflow.com/help/mcve&gt;minival, verifiable, complete example&lt;/denchmark-link&gt;
 to reproduce the problem?
		</comment>
		<comment id='5' author='ybsave' date='2018-09-07T15:23:35Z'>
		&lt;denchmark-link:https://github.com/asimshankar&gt;@asimshankar&lt;/denchmark-link&gt;
 Thank you for your response. Manually setting the checkpoint and writing into different files does work; however, I believe it should be processed by Tensorflow internally but not a user's extra effort. In previous Tensorflow versions, there is no such problem, but just since version 1.9, I started to encounter this conflict.
One sample code could just be the Tensorflow's tutorial codes with a few extra lines as below:
&lt;denchmark-code&gt;mnist = tf.keras.datasets.mnist
def test_tesnorboard():
  (x_train, y_train),(x_test, y_test) = mnist.load_data()
  x_train, x_test = x_train / 255.0, x_test / 255.0

  model = tf.keras.models.Sequential([
	tf.keras.layers.Flatten(),
	tf.keras.layers.Dense(512, activation=tf.nn.relu),
	tf.keras.layers.Dropout(0.2),
	tf.keras.layers.Dense(10, activation=tf.nn.softmax)
  ])
  model.compile(optimizer='adam',
				loss='sparse_categorical_crossentropy',
				metrics=['accuracy'])

  saver_callback = keras.callbacks.ModelCheckpoint(
	'test/test_save.ckpt', verbose=1, save_weights_only=True, period=1)

  model.fit(x_train, y_train, epochs=1000000, callbacks=[saver_callback])
  model.evaluate(x_test, y_test)
&lt;/denchmark-code&gt;

If we run the tensorboard at the same time during training, the training program will crash.
		</comment>
		<comment id='6' author='ybsave' date='2018-10-23T07:58:13Z'>
		I have the same problem (windows 10, tensorflow 1.11.0, tensorbard 1.11.0, tensorflow-gpu 1.9.0): saving the model weights with Saver.save() multiple times to the same file within the tensorboard logdir folder fails. As a workaround, I write them outside the tensorboard logdir folder and it works.
		</comment>
		<comment id='7' author='ybsave' date='2018-11-27T14:56:45Z'>
		I have the same problem (windows 10, tensorflow 1.11.0, tensorbard 1.11.0), although I am not using keras. I also tried with other version of tensorflow 1.8, but the problem goes on. Is it possible that someone tell me other versi√≥n of tensorflow that I could use while this bug is solved?
		</comment>
		<comment id='8' author='ybsave' date='2018-12-17T02:56:11Z'>
		I have found that I get this error if I have an Explorer window also watching the folder. I close the Explorer window and the error stopped appearing. That leads me to think that it's Explorer that is locking the file and TensorBoard gets locked out.
		</comment>
		<comment id='9' author='ybsave' date='2019-05-26T08:16:07Z'>
		Also get that problem.
Don't have an explorer window open.
Also deactivated antivirus software.
Need to watch progress of a running tensorflow training progress.
Still allways get this problem when i'm using the tensorboard.
I'm using:
tensorflow Version: 1.13.1
tensorboard Version: 1.12.2
python version: 3.6.4
Error still happens after i upgrade:
tensorboard to version: 1.13.1
python to version: 3.6.8
		</comment>
		<comment id='10' author='ybsave' date='2019-07-25T12:28:00Z'>
		&lt;denchmark-link:https://github.com/ybsave&gt;@ybsave&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/TheCrazyT&gt;@TheCrazyT&lt;/denchmark-link&gt;
 ,
Sorry for the delayed response. I tried reproducing the issue with Tensorflow Version 1.14 and 1.13.1 but I didn't get the error you mentioned. Instead, I got the below error. Can you please provide the working reproducible code. Thanks!
&lt;denchmark-code&gt;---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-4-7080519d0cae&gt; in &lt;module&gt;
     15     model.evaluate(x_test, y_test)
     16 
---&gt; 17 test_tesnorboard()

&lt;ipython-input-4-7080519d0cae&gt; in test_tesnorboard()
     12     saver_callback = keras.callbacks.ModelCheckpoint('test/test_save.ckpt', verbose=1, save_weights_only=True, period=1)
     13 
---&gt; 14     model.fit(x_train, y_train, epochs=1000000, callbacks=[saver_callback])
     15     model.evaluate(x_test, y_test)
     16 

~/anaconda3/envs/TF_PY_36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)
    878           initial_epoch=initial_epoch,
    879           steps_per_epoch=steps_per_epoch,
--&gt; 880           validation_steps=validation_steps)
    881 
    882   def evaluate(self,

~/anaconda3/envs/TF_PY_36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)
    323         # Callbacks batch_begin.
    324         batch_logs = {'batch': batch_index, 'size': len(batch_ids)}
--&gt; 325         callbacks._call_batch_hook(mode, 'begin', batch_index, batch_logs)
    326         progbar.on_batch_begin(batch_index, batch_logs)
    327 

~/anaconda3/envs/TF_PY_36/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py in _call_batch_hook(self, mode, hook, batch, logs)
    194     t_before_callbacks = time.time()
    195     for callback in self.callbacks:
--&gt; 196       batch_hook = getattr(callback, hook_name)
    197       batch_hook(batch, logs)
    198     self._delta_ts[hook_name].append(time.time() - t_before_callbacks)

AttributeError: 'ModelCheckpoint' object has no attribute 'on_train_batch_begin'
&lt;/denchmark-code&gt;

		</comment>
		<comment id='11' author='ybsave' date='2019-08-09T12:59:45Z'>
		Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
		<comment id='12' author='ybsave' date='2019-08-09T12:59:47Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=21135&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=21135&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='ybsave' date='2019-08-10T09:52:13Z'>
		Error still happens.
I made a proof of concept on appveyor:
&lt;denchmark-link:https://ci.appveyor.com/project/TheCrazyT/tensorflow-poe/builds/26605745#L378&gt;https://ci.appveyor.com/project/TheCrazyT/tensorflow-poe/builds/26605745#L378&lt;/denchmark-link&gt;

Important error is the following (you can ignore the others about pip upgrade):
"tensorflow.python.framework.errors_impl.UnknownError: Failed to rename"
Source code used (appveyor runs the run_test.ps1 script):
&lt;denchmark-link:https://github.com/TheCrazyT/tensorflow_poe&gt;https://github.com/TheCrazyT/tensorflow_poe&lt;/denchmark-link&gt;

To reproduce you need some random script that uses tensorflow and stores checkpoint-data.
Then run tensorboard.
To finally produce the error you need to access the tensorboard with a webbrowser.
Thats it.
		</comment>
		<comment id='14' author='ybsave' date='2019-08-17T21:50:39Z'>
		&lt;denchmark-link:https://github.com/rmothukuru&gt;@rmothukuru&lt;/denchmark-link&gt;
 Doesn't seem to be resolved, and it's easily reproducible since a TensorFlow tutorial script crashes with the same error message when watching progress with TensorBoard. This is using tensorflow-gpu 1.14 on Windows 10.
The tutorial script is at &lt;denchmark-link:https://github.com/tensorflow/hub/raw/master/examples/image_retraining/retrain.py&gt;https://github.com/tensorflow/hub/raw/master/examples/image_retraining/retrain.py&lt;/denchmark-link&gt;

It dies after training is complete, so run it with for example --how_many_training_steps=40.
Full stacktrace:
&lt;denchmark-code&gt;2019-08-17 23:36:20.321895: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at save_restore_v2_ops.cc:137 : Unknown: Failed to rename: /tmp/_retrain_checkpoint.index.tempstate14236087840509299070 to: /tmp/_retrain_checkpoint.index : Access is denied.
; Input/output error
Traceback (most recent call last):
  File "C:\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\client\session.py", line 1356, in _do_call
    return fn(*args)
  File "C:\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\client\session.py", line 1341, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "C:\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\client\session.py", line 1429, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.
  (0) Unknown: Failed to rename: /tmp/_retrain_checkpoint.index.tempstate14236087840509299070 to: /tmp/_retrain_checkpoint.index : Access is denied.
; Input/output error
         [[{{node save_2/SaveV2}}]]
  (1) Unknown: Failed to rename: /tmp/_retrain_checkpoint.index.tempstate14236087840509299070 to: /tmp/_retrain_checkpoint.index : Access is denied.
; Input/output error
         [[{{node save_2/SaveV2}}]]
         [[save_2/SaveV2/_1602]]
0 successful operations.
0 derived errors ignored.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "retrain.py", line 1349, in &lt;module&gt;
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File "C:\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\platform\app.py", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File "C:\envs\tensorflow-gpu\lib\site-packages\absl\app.py", line 300, in run
    _run_main(main, args)
  File "C:\envs\tensorflow-gpu\lib\site-packages\absl\app.py", line 251, in _run_main
    sys.exit(main(argv))
  File "retrain.py", line 1143, in main
    train_saver.save(sess, FLAGS.checkpoint_path)
  File "C:\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\training\saver.py", line 1173, in save
    {self.saver_def.filename_tensor_name: checkpoint_file})
  File "C:\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\client\session.py", line 950, in run
    run_metadata_ptr)
  File "C:\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\client\session.py", line 1173, in _run
    feed_dict_tensor, options, run_metadata)
  File "C:\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\client\session.py", line 1350, in _do_run
    run_metadata)
  File "C:\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\client\session.py", line 1370, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.
  (0) Unknown: Failed to rename: /tmp/_retrain_checkpoint.index.tempstate14236087840509299070 to: /tmp/_retrain_checkpoint.index : Access is denied.
; Input/output error
         [[node save_2/SaveV2 (defined at retrain.py:1069) ]]
  (1) Unknown: Failed to rename: /tmp/_retrain_checkpoint.index.tempstate14236087840509299070 to: /tmp/_retrain_checkpoint.index : Access is denied.
; Input/output error
         [[node save_2/SaveV2 (defined at retrain.py:1069) ]]
         [[save_2/SaveV2/_1602]]
0 successful operations.
0 derived errors ignored.

Errors may have originated from an input operation.
Input Source operations connected to node save_2/SaveV2:
 module/InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_mean/Read/ReadVariableOp (defined at C:\envs\tensorflow-gpu\lib\site-packages\tensorflow_hub\native_module.py:459)
 final_retrain_ops/biases/final_biases (defined at retrain.py:765)
 final_retrain_ops/weights/final_weights (defined at retrain.py:761)

Input Source operations connected to node save_2/SaveV2:
 module/InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_mean/Read/ReadVariableOp (defined at C:\envs\tensorflow-gpu\lib\site-packages\tensorflow_hub\native_module.py:459)
 final_retrain_ops/biases/final_biases (defined at retrain.py:765)
 final_retrain_ops/weights/final_weights (defined at retrain.py:761)

Original stack trace for 'save_2/SaveV2':
  File "retrain.py", line 1349, in &lt;module&gt;
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File "C:\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\platform\app.py", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File "C:\envs\tensorflow-gpu\lib\site-packages\absl\app.py", line 300, in run
    _run_main(main, args)
  File "C:\envs\tensorflow-gpu\lib\site-packages\absl\app.py", line 251, in _run_main
    sys.exit(main(argv))
  File "retrain.py", line 1069, in main
    train_saver = tf.train.Saver()
  File "C:\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\training\saver.py", line 825, in __init__
    self.build()
  File "C:\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\training\saver.py", line 837, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File "C:\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\training\saver.py", line 875, in _build
    build_restore=build_restore)
  File "C:\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\training\saver.py", line 505, in _build_internal
    save_tensor = self._AddSaveOps(filename_tensor, saveables)
  File "C:\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\training\saver.py", line 206, in _AddSaveOps
    save = self.save_op(filename_tensor, saveables)
  File "C:\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\training\saver.py", line 122, in save_op
    tensors)
  File "C:\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\ops\gen_io_ops.py", line 2060, in save_v2
    name=name)
  File "C:\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\framework\op_def_library.py", line 788, in _apply_op_helper
    op_def=op_def)
  File "C:\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\util\deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "C:\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\framework\ops.py", line 3616, in create_op
    op_def=op_def)
  File "C:\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\framework\ops.py", line 2005, in __init__
    self._traceback = tf_stack.extract_stack()
&lt;/denchmark-code&gt;

		</comment>
		<comment id='15' author='ybsave' date='2019-08-19T11:14:10Z'>
		I'm also still observing the error. It is possible (though I don't know for sure) that this is related to: &lt;denchmark-link:https://github.com/tensorflow/models/issues/4177&gt;tensorflow/models#4177&lt;/denchmark-link&gt;

		</comment>
		<comment id='16' author='ybsave' date='2019-09-03T12:38:23Z'>
		It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue?
		</comment>
		<comment id='17' author='ybsave' date='2019-09-03T15:33:10Z'>
		Guess someone should remove that label ...
		</comment>
		<comment id='18' author='ybsave' date='2019-09-05T16:36:17Z'>
		&lt;denchmark-link:https://github.com/ybsave&gt;@ybsave&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/TheCrazyT&gt;@TheCrazyT&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/Efaq&gt;@Efaq&lt;/denchmark-link&gt;
 Can you check the following &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/bd452d8652f0fe7c9059a67d07fb0d53/tf_21135_callbacks.ipynb&gt;gitst&lt;/denchmark-link&gt;
 and let me know your comments. I updated &lt;denchmark-link:https://github.com/ybsave&gt;@ybsave&lt;/denchmark-link&gt;
 code little bit and ran it in google colab. I don't see any issue but want to know whether you have any concerns. Thanks!
		</comment>
		<comment id='19' author='ybsave' date='2019-09-06T03:59:45Z'>
		Correct me if I'm wrong, but google collab is linux (atleast thats what "!uname -a" shows in jupyter).
But the problem is related to windows version of tensorflow!
(thats why I used appveyor)
Edit:
Oh maybe i just misunderstood, will check the nightly build of tensorflow at my windows pc this weekend.
		</comment>
		<comment id='20' author='ybsave' date='2019-09-06T04:40:31Z'>
		First test with my testcode and tf-nightly failed(&lt;denchmark-link:https://ci.appveyor.com/project/TheCrazyT/tensorflow-poe/builds/27221666&gt;https://ci.appveyor.com/project/TheCrazyT/tensorflow-poe/builds/27221666&lt;/denchmark-link&gt;
), will check your code once I have the time.
		</comment>
		<comment id='21' author='ybsave' date='2019-09-08T08:20:39Z'>
		Ok ... second test seem to work, so it looks like "tf.train.Saver()" is the problem?
(Thats what i use at: &lt;denchmark-link:https://github.com/TheCrazyT/tensorflow_poe/blob/master/doit.py&gt;https://github.com/TheCrazyT/tensorflow_poe/blob/master/doit.py&lt;/denchmark-link&gt;
)
Atleast I'm not 100% shure why your version with callbacks seem to work.
&lt;denchmark-link:https://ci.appveyor.com/project/TheCrazyT/tensorflow-poe/builds/27259776&gt;https://ci.appveyor.com/project/TheCrazyT/tensorflow-poe/builds/27259776&lt;/denchmark-link&gt;

(atleast it runned 25 epochs with 25 checkpoints without problems)
Edit:
Oh well I'm now totally confused, tried to rerun my old code but it also didn't produce the problem anymore (although version of tf-nightly was the same).
Edit#2:
Currently can't reliably test on appveyor because of:
"tb-nightly 1.15.0a20190907 has requirement setuptools&gt;=41.0.0, but you'll have setuptools 40.6.2 which is incompatible."
Guess thats why I have no problems at the moment, because it uses an older version.
(sadly can't even update  setuptools to the required version)
		</comment>
		<comment id='22' author='ybsave' date='2019-09-10T02:34:26Z'>
		Hello
I am also facing the same problem and getting following error at step 93526 while training ssd_inception_v2_coco.config  and visualizing its performance with TensorBoard on windows 10 (tensorflow-estimator 1.14.0, tensorflow-gpu  1.14.0, Python 3.7.4) .
I do not know why I am getting this error.  (I have not it synchronized it with google or one drive for saving)
Would you please guide me that why am I getting this error and how can I fix it?
coordinator.py:219] Error reported to Coordinator: Failed to rename: training\checkpoint.tmp5e2c65c350c24211a132a1aabfa7d446 to: training\checkpoint : Access is denied.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

I0910 00:01:09.369776 16632 supervisor.py:1117] Saving checkpoint to path training/model.ckpt
I0910 00:01:10.502128 21188 learning.py:507] global step 93525: loss = 3.5969 (1.415 sec/step)
I0910 00:01:10.557096 17904 supervisor.py:1050] Recording summary at step 93525.
I0910 00:01:11.644474 21188 learning.py:507] global step 93526: loss = 3.4009 (1.140 sec/step)
I0910 00:01:11.886335 16632 coordinator.py:219] Error reported to Coordinator: Failed to rename: training\checkpoint.tmp5e2c65c350c24211a132a1aabfa7d446 to: training\checkpoint : Access is denied.
; Input/output error
Traceback (most recent call last):
File "C:\Users\sbas\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\training\coordinator.py", line 297, in stop_on_exception
yield
File "C:\Users\sbas\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\training\coordinator.py", line 495, in run
self.run_loop()
File "C:\Users\sbas\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\training\supervisor.py", line 1119, in run_loop
self._sess, self._sv.save_path, global_step=self._sv.global_step)
File "C:\Users\sbas\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\training\saver.py", line 1183, in save
save_relative_paths=self._save_relative_paths)
File "C:\Users\sbas\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\training\checkpoint_management.py", line 242, in update_checkpoint_state_internal
text_format.MessageToString(ckpt))
File "C:\Users\sbas\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\lib\io\file_io.py", line 540, in atomic_write_string_to_file
rename(temp_pathname, filename, overwrite)
File "C:\Users\sbas\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\lib\io\file_io.py", line 502, in rename
rename_v2(oldname, newname, overwrite)
File "C:\Users\sbas\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\lib\io\file_io.py", line 519, in rename_v2
compat.as_bytes(src), compat.as_bytes(dst), overwrite)
tensorflow.python.framework.errors_impl.UnknownError: Failed to rename: training\checkpoint.tmp5e2c65c350c24211a132a1aabfa7d446 to: training\checkpoint : Access is denied.
; Input/output error
I0910 00:01:13.594354 21188 learning.py:507] global step 93527: loss = 2.3798 (1.322 sec/step)
I0910 00:01:13.596354 21188 learning.py:785] Finished training! Saving model to disk.
C:\Users\sbas\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\summary\writer\writer.py:386: UserWarning: Attempting to use a closed FileWriter. The operation will be a noop unless the FileWriter is explicitly reopened.
warnings.warn("Attempting to use a closed FileWriter. "
Traceback (most recent call last):
File "legacy/train.py", line 184, in 
tf.app.run()
File "C:\Users\sbas\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\platform\app.py", line 40, in run
_run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
File "C:\Users\sbas\Anaconda3\envs\tensorflow1\lib\site-packages\absl\app.py", line 300, in run
_run_main(main, args)
File "C:\Users\sbas\Anaconda3\envs\tensorflow1\lib\site-packages\absl\app.py", line 251, in _run_main
sys.exit(main(argv))
File "C:\Users\sbas\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\util\deprecation.py", line 324, in new_func
return func(*args, **kwargs)
File "legacy/train.py", line 180, in main
graph_hook_fn=graph_rewriter_fn)
File "C:\tensorflow1\models\research\object_detection\legacy\trainer.py", line 416, in train
saver=saver)
File "C:\Users\sbas\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\contrib\slim\python\slim\learning.py", line 790, in train
ignore_live_threads=ignore_live_threads)
File "C:\Users\sbas\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\training\supervisor.py", line 839, in stop
ignore_live_threads=ignore_live_threads)
File "C:\Users\sbas\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\training\coordinator.py", line 389, in join
six.reraise(*self._exc_info_to_raise)
File "C:\Users\sbas\Anaconda3\envs\tensorflow1\lib\site-packages\six.py", line 693, in reraise
raise value
File "C:\Users\sbas\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\training\coordinator.py", line 297, in stop_on_exception
yield
File "C:\Users\sbas\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\training\coordinator.py", line 495, in run
self.run_loop()
File "C:\Users\sbas\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\training\supervisor.py", line 1119, in run_loop
self._sess, self._sv.save_path, global_step=self._sv.global_step)
File "C:\Users\sbas\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\training\saver.py", line 1183, in save
save_relative_paths=self._save_relative_paths)
File "C:\Users\sbas\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\training\checkpoint_management.py", line 242, in update_checkpoint_state_internal
text_format.MessageToString(ckpt))
File "C:\Users\sbas\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\lib\io\file_io.py", line 540, in atomic_write_string_to_file
rename(temp_pathname, filename, overwrite)
File "C:\Users\sbas\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\lib\io\file_io.py", line 502, in rename
rename_v2(oldname, newname, overwrite)
File "C:\Users\sbas\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\lib\io\file_io.py", line 519, in rename_v2
compat.as_bytes(src), compat.as_bytes(dst), overwrite)
tensorflow.python.framework.errors_impl.UnknownError: Failed to rename: training\checkpoint.tmp5e2c65c350c24211a132a1aabfa7d446 to: training\checkpoint : Access is denied.
; Input/output error
(tensorflow1) C:\tensorflow1\models\research\object_detection&gt;I0910 00:01:09.084940 21188 learning.py:507] global step 93524: loss = 5.1544 (1.134 sec/step)
'I0910' is not recognized as an internal or external command,
operable program or batch file.
(tensorflow1) C:\tensorflow1\models\research\object_detection&gt;I0910 00:01:09.369776 16632 supervisor.py:1117] Saving checkpoint to path training/model.ckpt
'I0910' is not recognized as an internal or external command,
operable program or batch file.
(tensorflow1) C:\tensorflow1\models\research\object_detection&gt;I0910 00:01:10.502128 21188 learning.py:507] global step 93525: loss = 3.5969 (1.415 sec/step)
'I0910' is not recognized as an internal or external command,
operable program or batch file.
(tensorflow1) C:\tensorflow1\models\research\object_detection&gt;I0910 00:01:10.557096 17904 supervisor.py:1050] Recording summary at step 93525.
'I0910' is not recognized as an internal or external command,
operable program or batch file.
(tensorflow1) C:\tensorflow1\models\research\object_detection&gt;I0910 00:01:11.644474 21188 learning.py:507] global step 93526: loss = 3.4009 (1.140 sec/step)
'I0910' is not recognized as an internal or external command,
operable program or batch file.
(tensorflow1) C:\tensorflow1\models\research\object_detection&gt;I0910 00:01:11.886335 16632 coordinator.py:219] Error reported to Coordinator: Failed to rename: training\checkpoint.tmp5e2c65c350c24211a132a1aabfa7d446 to: training\checkpoint : Access is denied.
'I0910' is not recognized as an internal or external command,
operable program or batch file.
(tensorflow1) C:\tensorflow1\models\research\object_detection&gt;; Input/output error
'Input' is not recognized as an internal or external command,
operable program or batch file.
(tensorflow1) C:\tensorflow1\models\research\object_detection&gt;Traceback (most recent call last):
'Traceback' is not recognized as an internal or external command,
operable program or batch file.
(tensorflow1) C:\tensorflow1\models\research\object_detection&gt;  File "C:\Users\sbas\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\training\coordinator.py", line 297, in stop_on_exception
C:\Users\sbas\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\training\coordinator.py,: cannot open C:\Users\sbas\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\training\coordinator.py,' (No such file or directory) line:                                                                                                     cannot open line' (No such file or directory)
297,:                                                                                                     cannot open 297,' (No such file or directory) in:                                                                                                       cannot open in' (No such file or directory)
stop_on_exception:                                                                                        cannot open `stop_on_exception' (No such file or directory)
		</comment>
		<comment id='23' author='ybsave' date='2019-09-13T21:09:39Z'>
		&lt;denchmark-link:https://github.com/Docker300&gt;@Docker300&lt;/denchmark-link&gt;
 Can you post your issue in Tensorboard repo where the team actively responds and resolves issues faster. When you post there, Please provide a simple standalone code. Thanks!
I am closing issue here as the issue is more related to the tensorboard repo. Thanks!
		</comment>
		<comment id='24' author='ybsave' date='2019-09-13T21:09:41Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=21135&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=21135&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='25' author='ybsave' date='2020-02-08T10:17:19Z'>
		I stubled upon this unable to rename error doing just basic tensorflow-gpu stuff on win10, without actually knowing what tensorboard is and not actively using it.
It happened to me while using checkpoints between epochs in training a LSTM NN.
someone at &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/892&gt;#892&lt;/denchmark-link&gt;
 mentioned, that the Windows Explorer (or other programs) locking the directory for writing (for some reason), so I closed VS Code and any explorer windows and the script runs as intended.
(edit: i had the folder opened in VS Code using the Windows Subsystem for Linux (WSL), opening the folder in regular VS Code without WSL seems to have resolved it.)
		</comment>
		<comment id='26' author='ybsave' date='2020-02-19T11:17:49Z'>
		The solution: Look on the rename file names and delete the target file ( e.g.  Failed to rename: training\checkpoint.tmp5e2c65c350c24211a132a1aabfa7d446 to: training\checkpoint -&gt; delete training\checkpoint)
In my case i think the error cause was that a backup program(syncplicity) locked the target file when it was supposed to be deleted.
		</comment>
		<comment id='27' author='ybsave' date='2020-02-25T13:52:57Z'>
		&lt;denchmark-link:https://github.com/dshirron&gt;@dshirron&lt;/denchmark-link&gt;
 thank you, i have the same issue with syncplicty.
		</comment>
		<comment id='28' author='ybsave' date='2020-04-27T17:13:22Z'>
		
I stubled upon this unable to rename error doing just basic tensorflow-gpu stuff on win10, without actually knowing what tensorboard is and not actively using it.
It happened to me while using checkpoints between epochs in training a LSTM NN.
someone at #892 mentioned, that the Windows Explorer (or other programs) locking the directory for writing (for some reason), so I closed VS Code and any explorer windows and the script runs as intended.
(edit: i had the folder opened in VS Code using the Windows Subsystem for Linux (WSL), opening the folder in regular VS Code without WSL seems to have resolved it.)

This seems to be working in my case. I closed my Spyder IDE and the programs runs normally now.
		</comment>
		<comment id='29' author='ybsave' date='2020-10-15T14:33:31Z'>
		I was facing the same problem. Just change the data_dir attribute like data_dir=r"D:\data" and make sure it should not be in C: drive. You are good to go.
		</comment>
	</comments>
</bug>