<bug id='46145' author='TimOgden' open_date='2021-01-04T17:07:11Z' closed_time='2021-01-05T01:14:57Z'>
	<summary>Pandas pct_change() function results in nan loss when training</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below):  v2.4.0-rc4-71-g582c8d236cb 2.4.0
Python version: Python 3.7.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: Running on CPU
GPU model and memory:

Describe the current behavior
When training a model with data from a pandas DataFrame, the model trains fine unless I use the df.pct_change() pandas function on it. If I do so, the loss is always nan. I specifically made sure to remove any NaN or inf values from the dataset but the problem persists. I don't know if I'm just missing something obvious and the issue is on my end.
I created a simplified jupyter notebook to demonstrate the issue.
Describe the expected behavior
The loss should be a real number in the last cell of the notebook (after I remove all NaN and inf values) but it is still nan.

Here is the notebook:
&lt;denchmark-link:https://drive.google.com/file/d/1k3dHHtFI4tGTKswF0d0TJdj9TfaqfBnX/view?usp=sharing&gt;https://drive.google.com/file/d/1k3dHHtFI4tGTKswF0d0TJdj9TfaqfBnX/view?usp=sharing&lt;/denchmark-link&gt;

 Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='TimOgden' date='2021-01-05T01:14:59Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46145&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46145&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='TimOgden' date='2021-01-05T01:15:15Z'>
		Figured out the answer on my own. Stupidly, the example code I wrote out above is exactly the issue, haha.
Indeed, the model will train fine on the actual data, and indeed the model will not train fine on any data that has NaN or inf values in it.
But if you try to train on data with NaN or inf values, then the model will never stop having a nan loss, despite if you train it with new, clean data.
Start training the model only when you know there are no NaN or inf values in your data.
		</comment>
	</comments>
</bug>