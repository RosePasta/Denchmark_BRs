<bug id='26244' author='ghego' open_date='2019-03-01T06:05:13Z' closed_time='2020-03-20T06:06:25Z'>
	<summary>FailedPreconditionError when running Convolutional Keras model on CPU in TF 2.0</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.0.0-dev20190228
Python version: 3.7
CUDA/cuDNN version: 10/7.4
GPU model and memory:  Tesla M60 on AWS g3.8xlarge

Describe the current behavior
Training a model defined on the CPU raises a FailedPreconditionError when using a machine with a GPU in TF 2.0 nightly.
Describe the expected behavior
No error is raised if I use one of the following fixes:

use tensorflow.compat.v1.disable_eager_execution()
remove the Conv2D layer
remove the batch_size and epochs arguments from the .fit call

However, the context setter seems to have no effect and training is happening on the GPU anyways (I can tell by how fast it's training)
the behavior seems weird, can anyone explain what's going on?
Code to reproduce the issue
import tensorflow as tf
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense
from tensorflow.keras.layers import Conv2D

(X_train, y_train), _ = cifar10.load_data()
X_train = X_train.astype('float32') / 255.0

with tf.device('cpu:0'):
    model = Sequential([
        Conv2D(32, (3, 3), input_shape=(32, 32, 3)),
        Flatten(),
        Dense(10, activation='softmax')
    ])
    
    model.compile(loss='sparse_categorical_crossentropy',
                  optimizer='rmsprop',
                  metrics=['accuracy'])

    
model.fit(X_train, y_train,
          batch_size=1024,
          epochs=2)
Other info / logs
traceback:
---------------------------------------------------------------------------
FailedPreconditionError                   Traceback (most recent call last)
&lt;ipython-input-1-d00bb2a707a4&gt; in &lt;module&gt;
     24 model.fit(X_train, y_train,
     25           batch_size=1024,
---&gt; 26           epochs=2)

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    871           validation_steps=validation_steps,
    872           validation_freq=validation_freq,
--&gt; 873           steps_name='steps_per_epoch')
    874 
    875   def evaluate(self,

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)
    349 
    350         # Get outputs.
--&gt; 351         batch_outs = f(ins_batch)
    352         if not isinstance(batch_outs, list):
    353           batch_outs = [batch_outs]

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in __call__(self, inputs)
   3215         value = math_ops.cast(value, tensor.dtype)
   3216       converted_inputs.append(value)
-&gt; 3217     outputs = self._graph_fn(*converted_inputs)
   3218     return nest.pack_sequence_as(self._outputs_structure,
   3219                                  [x.numpy() for x in outputs])

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
    523       raise TypeError("Keyword arguments {} unknown. Expected {}.".format(
    524           list(kwargs.keys()), list(self._arg_keywords)))
--&gt; 525     return self._call_flat(args)
    526 
    527   def _filtered_call(self, args, kwargs):

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args)
    592     # Only need to override the gradient in graph mode and when we have outputs.
    593     if context.executing_eagerly() or not self.outputs:
--&gt; 594       outputs = self._inference_function.call(ctx, args)
    595     else:
    596       self._register_gradient()

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args)
    380             attrs=("executor_type", executor_type,
    381                    "config_proto", config),
--&gt; 382             ctx=ctx)
    383       # Replace empty list with None
    384       outputs = outputs or None

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     64     else:
     65       message = e.message
---&gt; 66     six.raise_from(core._status_to_exception(e.code, message), None)
     67   except TypeError as e:
     68     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):

~/miniconda3/envs/tf2/lib/python3.7/site-packages/six.py in raise_from(value, from_value)

FailedPreconditionError: Error while reading resource variable _AnonymousVar15 from Container: localhost. This could mean that the variable was uninitialized. Invalid argument: Trying to access resource _AnonymousVar15 located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0
	 [[{{node training/RMSprop/RMSprop/update_3/mul/ReadVariableOp}}]] [Op:__inference_keras_scratch_graph_646]
	</description>
	<comments>
		<comment id='1' author='ghego' date='2019-03-04T16:53:43Z'>
		&lt;denchmark-link:https://github.com/ezhulenev&gt;@ezhulenev&lt;/denchmark-link&gt;
 The presence of AnonymousVar here makes me think this is related to the arithmetic optimizer.
		</comment>
		<comment id='2' author='ghego' date='2019-03-04T18:16:58Z'>
		It's just the name auto-generated in 


tensorflow/tensorflow/core/framework/resource_mgr.cc


         Line 48
      in
      abdbe6e






 result.set_name(strings::StrCat("_AnonymousVar", current_id_.fetch_add(1))); 




, I don't see any artifact of arithmetic optimizer in the variable name. I think it's just the problem with Keras graph.
		</comment>
		<comment id='3' author='ghego' date='2019-03-04T18:36:49Z'>
		&lt;denchmark-link:https://github.com/robieta&gt;@robieta&lt;/denchmark-link&gt;
 can you take a look at this?
		</comment>
		<comment id='4' author='ghego' date='2019-03-04T18:47:06Z'>
		Sure, let me take a look.
		</comment>
		<comment id='5' author='ghego' date='2019-03-04T19:45:02Z'>
		It looks like what's happening is that the model weights are initialized inside of the tf.device context, but the RMSProp variables aren't. So when we try to call fit, the resultant variables are then (lazily) created and assigned to the default device: gpu:0. I still need to dig into why this only happens in 2.0 though.
		</comment>
		<comment id='6' author='ghego' date='2019-03-08T03:36:38Z'>
		Thanks &lt;denchmark-link:https://github.com/robieta&gt;@robieta&lt;/denchmark-link&gt;
 for looking into this. Keep in mind that:

this only happens in eager mode, if I use tensorflow.compat.v1.disable_eager_execution() no error is shown
in this case, the code executes but I suspect all variables are created on the GPU regardless of the context setter because training runs really fast

		</comment>
		<comment id='7' author='ghego' date='2019-03-15T17:33:45Z'>
		&lt;denchmark-link:https://github.com/robieta&gt;@robieta&lt;/denchmark-link&gt;
 I think optimizer colocate its variable with model.trainable_weights, so it's likely that the model variables are lazily constructed, which does not respect tf.device("cpu:0") anymore, ergo optimizer follows the lead and does the same thing.
		</comment>
		<comment id='8' author='ghego' date='2019-03-27T17:56:54Z'>
		I'm too facing this. I use tensorflow low level apis. Inside my while loop, the message of '_AnonymousVar60 variable was uninitialized' pops once the counter goes to [1]
@tf.function
def func():
with tf.device('/device:GPU:0'):
for i in tf.range(10):
{runs smoothly for i=0}
I use cuda 10.0 and built tensorflow 2.0.0 from source around 20th March.
Ubuntu 18.04, NVIDIA GTX 1060
		</comment>
		<comment id='9' author='ghego' date='2019-05-31T06:40:14Z'>
		import tensorflow as tf
from tensorflow.keras.applications import Xception
from tensorflow.keras.utils import multi_gpu_model
import numpy as np
num_samples = 32
height = 224
width = 224
num_classes = 32
"Instantiate the base model (or "template" model).
" We recommend doing this with under a CPU device scope,
" so that the model's weights are hosted on CPU memory.
" Otherwise they may end up hosted on a GPU, which would
" complicate weight sharing.
with tf.device('/cpu:0'):
model = Xception(weights=None,
input_shape=(height, width, 3),
classes=num_classes)
"Replicates the model on 2 GPUs.
" This assumes that your machine has 2 available GPUs.
parallel_model = multi_gpu_model(model, gpus=2)
parallel_model.compile(loss='categorical_crossentropy',
optimizer='rmsprop')
" Generate dummy data.
x = np.random.random((num_samples, height, width, 3))
y = np.random.random((num_samples, num_classes))
" This fit call will be distributed on 8 GPUs.
" Since the batch size is 256, each GPU will process 32 samples.
parallel_model.fit(x, y, epochs=20, batch_size=256)
" Save model via the template model (which shares the same weights):
" Save model via the template model (which shares the same weights):
model.save('my_model.h5')
&lt;denchmark-h:h2&gt;and the error is
Epoch 1/20&lt;/denchmark-h&gt;

FailedPreconditionError                   Traceback (most recent call last)
 in 
31 # This fit call will be distributed on 8 GPUs.
32 # Since the batch size is 256, each GPU will process 32 samples.
---&gt; 33 parallel_model.fit(x, y, epochs=20, batch_size=256)
34
35 # Save model via the template model (which shares the same weights):
~\Anaconda3\envs\py36\lib\site-packages\tensorflow\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
871           validation_steps=validation_steps,
872           validation_freq=validation_freq,
--&gt; 873           steps_name='steps_per_epoch')
874
875   def evaluate(self,
~\Anaconda3\envs\py36\lib\site-packages\tensorflow\python\keras\engine\training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)
350
351         # Get outputs.
--&gt; 352         batch_outs = f(ins_batch)
353         if not isinstance(batch_outs, list):
354           batch_outs = [batch_outs]
~\Anaconda3\envs\py36\lib\site-packages\tensorflow\python\keras\backend.py in call(self, inputs)
3215         value = math_ops.cast(value, tensor.dtype)
3216       converted_inputs.append(value)
-&gt; 3217     outputs = self._graph_fn(*converted_inputs)
3218     return nest.pack_sequence_as(self._outputs_structure,
3219                                  [x.numpy() for x in outputs])
~\Anaconda3\envs\py36\lib\site-packages\tensorflow\python\eager\function.py in call(self, *args, **kwargs)
556       raise TypeError("Keyword arguments {} unknown. Expected {}.".format(
557           list(kwargs.keys()), list(self._arg_keywords)))
--&gt; 558     return self._call_flat(args)
559
560   def _filtered_call(self, args, kwargs):
~\Anaconda3\envs\py36\lib\site-packages\tensorflow\python\eager\function.py in _call_flat(self, args)
625     # Only need to override the gradient in graph mode and when we have outputs.
626     if context.executing_eagerly() or not self.outputs:
--&gt; 627       outputs = self._inference_function.call(ctx, args)
628     else:
629       self._register_gradient()
~\Anaconda3\envs\py36\lib\site-packages\tensorflow\python\eager\function.py in call(self, ctx, args)
413             attrs=("executor_type", executor_type,
414                    "config_proto", config),
--&gt; 415             ctx=ctx)
416       # Replace empty list with None
417       outputs = outputs or None
~\Anaconda3\envs\py36\lib\site-packages\tensorflow\python\eager\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
64     else:
65       message = e.message
---&gt; 66     six.raise_from(core._status_to_exception(e.code, message), None)
67   except TypeError as e:
68     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):
~\Anaconda3\envs\py36\lib\site-packages\six.py in raise_from(value, from_value)
FailedPreconditionError: Error while reading resource variable _AnonymousVar341 from Container: localhost. This could mean that the variable was uninitialized. Invalid argument: Trying to access resource _AnonymousVar341 located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0
[[{{node training/RMSprop/RMSprop/update_99/mul/ReadVariableOp}}]]
[[training_rmsprop_rmsprop_update_98_mul_readvariableop_resource/_3]] [Op:__inference_keras_scratch_graph_37943]
		</comment>
		<comment id='10' author='ghego' date='2019-10-28T13:41:34Z'>
		
I'm too facing this. I use tensorflow low level apis. Inside my while loop, the message of '_AnonymousVar60 variable was uninitialized' pops once the counter goes to [1]
@tf.function
def func():
with tf.device('/device:GPU:0'):
for i in tf.range(10):
{runs smoothly for i=0}
I use cuda 10.0 and built tensorflow 2.0.0 from source around 20th March.
Ubuntu 18.04, NVIDIA GTX 1060

This is basically the exact error I am facing, the first batch executes fine, then one the second batch, this error appears.
How is this even possible? Did you ever find a solution?
		</comment>
		<comment id='11' author='ghego' date='2020-03-12T07:14:52Z'>
		&lt;denchmark-link:https://github.com/ghego&gt;@ghego&lt;/denchmark-link&gt;
, This issue is fixed in Tf-nightly version.
Please find the &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/ee6d90cb4fb5c3c9ea7c377afa9e327c/untitled442.ipynb&gt;gist&lt;/denchmark-link&gt;
 and let us know if you are happy with fix and close this issue. Thanks!
		</comment>
		<comment id='12' author='ghego' date='2020-03-19T09:06:34Z'>
		&lt;denchmark-link:https://github.com/ghego&gt;@ghego&lt;/denchmark-link&gt;
, Did you review the above attached gist.
Please close the issue if it was already resolved for you. Thanks!
		</comment>
		<comment id='13' author='ghego' date='2020-03-20T06:06:25Z'>
		It works. Thanks
		</comment>
		<comment id='14' author='ghego' date='2020-03-20T06:06:27Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26244&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26244&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>