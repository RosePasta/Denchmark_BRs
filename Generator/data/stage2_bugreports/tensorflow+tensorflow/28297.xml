<bug id='28297' author='lcf87' open_date='2019-05-01T01:03:14Z' closed_time='2019-07-30T21:32:07Z'>
	<summary>Unexpected bahaviour of per_process_gpu_memory_fraction</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): Source
TensorFlow version (use command below): 1.13
Python version: 3.6.7
Bazel version (if compiling from source): 0.21.0
GCC/Compiler version (if compiling from source): 7.30
CUDA/cuDNN version: 10.0/7.5
GPU model and memory: RTX2080Ti,/11GB

Describe the current behavior
I experimented with the per_process_gpu_memory_fraction option (mainly to test unified memory) and found out that the option works only when the model is fairly small.
To compare, I have a very simple model (one Matmul and one add) and an AlexNet.
The experiment code:
gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1.7)
config = tf.ConfigProto(gpu_options=gpu_options)
s = tf.Session(config=config)
s.run(tf.global_variables_initializer())
s.run(train_op)
In which train_op is the target op of the simple model and alexnet.

With the simple model, I can verify that the allocation always follows the set ratio, for instance in the case above (1.7x more memory), I can see from the verbose log

&lt;denchmark-code&gt;tensorflow/stream_executor/stream_executor_pimpl.cc:524] Called    StreamExecutor::UnifiedMemoryAllocate(size=19587203072) returns 0x7f03ac000000
tensorflow/core/common_runtime/bfc_allocator.cc:135] Extending allocation by 18.24GiB bytes.
&lt;/denchmark-code&gt;

And it works when the ratio is less than 1.0, it just allocates less memory.


With AlexNet, no matter what the number I set, it will always allocate what's available. I ran gdb on the C++ library and stopped at core/common_runtime/gpu/gpu_devices.cc in function CreateDevices() and printed out the SessionOption protobuf. per_process_gpu_memory_fraction is always 0.


If I define both models, the behavior is the same as that in the AlexNet case. I guess adding many layers changed something in tensorflow python library?


Also, I saw somewhere that if per_process_gpu_memory_fraction is too small and will not fit the model, tensorflow will ignore the option. I tested with 0.5 and I think 5GB is more than enough to train AlexNet with batch size of 8.


Describe the expected behavior
As long as the memory specified (total_mem * per_process_gpu_memory_fraction) fits the model, the option should be preserved. Especially when unified (really the managed memory) memory is available on GPUs.
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
The simple model:
&lt;denchmark-code&gt;a = tf.get_variable('a', [1024, 1024], tf.float32, xavier_initializer())
with tf.device("/device:CPU:0"):
    b = tf.ones([1024, 1], name='b')

with tf.device("/device:GPU:0"):
    axb = tf.matmul(a, b, name='axb')  # 1024x1
    relu_b = tf.nn.relu(b, name='relu_b')  # 1024x1

train_op = axb + relu_b
&lt;/denchmark-code&gt;

AlexNet is a standard implementation with tf.nn.conv2d and tf.nn.relu, etc
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Thanks a lot for your help!
	</description>
	<comments>
		<comment id='1' author='lcf87' date='2019-05-01T16:10:51Z'>
		After some more digging, found that,
if the session is created before the model is defined, the option works fine.
		</comment>
		<comment id='2' author='lcf87' date='2019-05-06T20:34:03Z'>
		@LuyuanChen Is this resolved? If it was resolved, Please close the issue. Thanks!
		</comment>
		<comment id='3' author='lcf87' date='2019-05-07T02:43:08Z'>
		I don't understand. It's some undefined behaviour in tf. Although I can get past it, it doesn't mean the bahaviour is correct. Please take a look.
		</comment>
		<comment id='4' author='lcf87' date='2019-07-16T22:57:42Z'>
		@LuyuanChen Sorry for the delay in my response. Could you check whether the issue still persists with TF1.14 or tf-nightly (TF1.15). Thanks!
		</comment>
		<comment id='5' author='lcf87' date='2019-07-30T21:32:07Z'>
		Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
		<comment id='6' author='lcf87' date='2019-07-30T21:32:08Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=28297&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=28297&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>