<bug id='6285' author='aseuteurideu' open_date='2016-12-13T13:34:33Z' closed_time='2017-03-03T03:15:28Z'>
	<summary>in debug mode, got Assertion `cudaGetLastError() == cudaSuccess' failed</summary>
	<description>
Environment: Ubuntu 14.04, CUDA 8, CuDNN 5.1, Tensorflow r0.12
Build command
bazel build -c opt --config cuda -c dbg --strip=never  //tensorflow/tools/pip_package:build_pip_package
previously, following the installation of tensorflow (not in debug mode), my code works well. But, after I rebuild with above command, it shows this error in the middle of running the session.

I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcupti.so.8.0 locally
python: external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:262: static void Eigen::internal::TensorExecutor&lt;Expression, Eigen::GpuDevice, Vectorizable&gt;::run(const Expression&amp;, const Eigen::GpuDevice&amp;) [with Expression = const Eigen::TensorAssignOp&lt;Eigen::TensorMap&lt;Eigen::Tensor&lt;float, 1, 1, long int&gt;, 16, Eigen::MakePointer&gt;, const Eigen::TensorCwiseBinaryOp&lt;Eigen::internal::scalar_max_op&lt;const float, const float&gt;, const Eigen::TensorMap&lt;Eigen::Tensor&lt;const float, 1, 1, long int&gt;, 16, Eigen::MakePointer&gt;, const Eigen::TensorCwiseNullaryOp&lt;Eigen::internal::scalar_constant_op, const Eigen::TensorMap&lt;Eigen::Tensor&lt;const float, 1, 1, long int&gt;, 16, Eigen::MakePointer&gt; &gt; &gt; &gt;; bool Vectorizable = true]: Assertion `cudaGetLastError() == cudaSuccess' failed.

the backtrace result from gdb when the error comes (seems from ReLU operator):

(gdb) bt
#0  0x00007ff450df6c37 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
#1  0x00007ff450dfa028 in __GI_abort () at abort.c:89
#2  0x00007ff450defbf6 in __assert_fail_base (
fmt=0x7ff450f403b8 "%s%s%s:%u: %s%sAssertion `%s' failed.\n%n",
assertion=assertion@entry=0x7ff42dc57260 "cudaGetLastError() == cudaSuccess",
file=file@entry=0x7ff42dc57210 "external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h", line=line@entry=262,
function=function@entry=0x7ff42dc58f80 &lt;Eigen::internal::TensorExecutor&lt;Eigen::TensorAssignOp&lt;Eigen::TensorMap&lt;Eigen::Tensor&lt;float, 1, 1, long&gt;, 16, Eigen::MakePointer&gt;, Eigen::TensorCwiseBinaryOp&lt;Eigen::internal::scalar_max_op&lt;float const, float const&gt;, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 1, 1, long&gt;, 16, Eigen::MakePointer&gt; const, Eigen::TensorCwiseNullaryOp&lt;Eigen::internal::scalar_constant_op, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 1, 1, long&gt;, 16, Eigen::MakePointer&gt; const&gt; const&gt; const&gt; const, Eigen::GpuDevice, true&gt;::run(Eigen::TensorAssignOp&lt;Eigen::TensorMap&lt;Eigen::Tensor&lt;float, 1, 1, long&gt;, 16, Eigen::MakePointer&gt;, Eigen::TensorCwiseBinaryOp&lt;Eigen::internal::scalar_max_op&lt;float const, float const&gt;, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 1, 1, long&gt;, 16, Eigen::MakePointer&gt; const, Eigen::TensorCwiseNullaryOp&lt;Eigen::internal::scalar_constant_op, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 1, 1, long&gt;, 16, Eigen::MakePointer&gt; const&gt; const&gt; const&gt; const&amp;, Eigen::GpuDevice const&amp;)::PRETTY_FUNCTION&gt; "static void Eigen::internal::TensorExecutor&lt;Expression, Eigen::GpuDevice, Vectorizable&gt;::run(const Expression&amp;, const Eigen::GpuDevice&amp;) [with Expression = const Eigen::TensorAssignOp&lt;Eigen::TensorMap"...) at assert.c:92
#3  0x00007ff450defca2 in __GI___assert_fail (
assertion=0x7ff42dc57260 "cudaGetLastError() == cudaSuccess",
file=0x7ff42dc57210 "external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h", line=262,
function=0x7ff42dc58f80 &lt;Eigen::internal::TensorExecutor&lt;Eigen::TensorAssignOp&lt;Eigen::TensorMap&lt;Eigen::Tensor&lt;float, 1, 1, long&gt;, 16, Eigen::MakePointer&gt;, Eigen::TensorCwiseBinaryOp&lt;Eigen::internal::scalar_max_op&lt;float const, float const&gt;, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 1, 1, long&gt;, 16, Eigen::MakePointer&gt; const, Eigen::TensorCwiseNullaryOp&lt;Eigen::internal::scalar_constant_op, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 1, 1, long&gt;, 16, Eigen::MakePointer&gt; const&gt; const&gt; const&gt; const, Eigen::GpuDevice, true&gt;::run(Eigen::TensorAssignOp&lt;Eigen::TensorMap&lt;Eigen::Tensor&lt;float, 1, 1, long&gt;, 16, Eigen::MakePointer&gt;, Eigen::TensorCwiseBinaryOp&lt;Eigen::internal::scalar_max_op&lt;float const, float const&gt;, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 1, 1, long&gt;, 16, Eigen::MakePointer&gt; const, Eigen::TensorCwiseNullaryOp&lt;Eigen::internal::scalar_constant_op, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 1, 1, long&gt;, 16, Eigen::MakePointer&gt; const&gt; const&gt; const&gt; const&amp;, Eigen::GpuDevice const&amp;)::PRETTY_FUNCTION&gt; "static void Eigen::internal::TensorExecutor&lt;Expression, Eigen::GpuDevice, Vectorizable&gt;::run(const Expression&amp;, const Eigen::GpuDevice&amp;) [with Expression = const Eigen::TensorAssignOp&lt;Eigen::TensorMap"...) at assert.c:101
#4  0x00007ff42ad39672 in Eigen::internal::TensorExecutor&lt;Eigen::TensorAssignOp&lt;Eigen::TensorMap&lt;Eigen::Tensor&lt;float, 1, 1, long&gt;, 16, Eigen::MakePointer&gt;, Eigen::TensorCwiseBinaryOp&lt;Eigen::internal::scalar_max_op&lt;float const, float const&gt;, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 1, 1, long&gt;, 16, Eigen::MakePointer&gt; const, Eigen::TensorCwiseNullaryOp&lt;Eigen::internal::scalar_constant_op, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 1, 1, long&gt;, 16, Eigen::MakePointer&gt; const&gt; const&gt; const&gt; const, Eigen::GpuDevice, true&gt;::run (expr=..., device=...)
at external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:260
#5  0x00007ff42ad37a6b in Eigen::TensorDevice&lt;Eigen::TensorMap&lt;Eigen::Tensor&lt;float, 1, 1, long&gt;, 16, Eigen::MakePointer&gt;, Eigen::GpuDevice&gt;::operator=&lt;Eigen::TensorCwiseBinaryOp&lt;Eigen::internal::scalar_max_op&lt;float const, float const&gt;, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 1, 1, long&gt;, 16, Eigen::MakePointer&gt; const, Eigen::TensorCwiseNullaryOp&lt;Eigen::internal::scalar_constant_op, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 1, 1, long&gt;, 16, Eigen::MakePointer&gt; const&gt; const&gt; &gt; (
this=0x7ff35b7fcfe0, other=...)
at external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDevice.h:35
#6  0x00007ff42ad36cb0 in tensorflow::functor::Relu&lt;Eigen::GpuDevice, float&gt;::operator() (
this=0x7ff35b7fd04f, d=..., features=..., activations=...)
at ./tensorflow/core/kernels/relu_op_functor.h:35
#7  0x00007ff42acf48c1 in tensorflow::ReluOp&lt;Eigen::GpuDevice, float&gt;::Operate (this=0x59fde00,
context=0x7ff35b7fd8f0, input=..., output=0x7ff23ce68390)
at ./tensorflow/core/kernels/relu_op.h:40
#8  0x00007ff42acee479 in tensorflow::UnaryElementWiseOp&lt;float, tensorflow::ReluOp&lt;Eigen::GpuDevice, float&gt; &gt;::Compute (this=0x59fde00, context=0x7ff35b7fd8f0)
at ./tensorflow/core/framework/numeric_op.h:62
#9  0x00007ff42c18523c in tensorflow::BaseGPUDevice::Compute (this=0x5964560, op_kernel=0x59fde00,
context=0x7ff35b7fd8f0) at tensorflow/core/common_runtime/gpu/gpu_device.cc:385
#10 0x00007ff42c402c39 in tensorflow::(anonymous namespace)::ExecutorState::Process (
this=0x3d553c80, tagged_node=..., scheduled_usec=1481634999542636)
at tensorflow/core/common_runtime/executor.cc:1364
#11 0x00007ff42c404881 in tensorflow::(anonymous namespace)::ExecutorState::__lambda4::operator() (
__closure=0x3d557a70) at tensorflow/core/common_runtime/executor.cc:1746
#12 0x00007ff42c40ada4 in std::_Function_handler&lt;void(), tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady(const TaggedNodeSeq&amp;, tensorflow::(anonymous namespace)::ExecutorState::TaggedNodeReadyQueue*)::__lambda4&gt;::_M_invoke(const std::_Any_data &amp;) (__functor=...)
at /usr/include/c++/4.8/functional:2071
#13 0x00007ff428bcc7a8 in std::function&lt;void ()&gt;::operator()() const (this=0x3d557ab0)
at /usr/include/c++/4.8/functional:2471
#14 0x00007ff42c6f0b80 in tensorflow:ðŸ§µ:EigenEnvironment::ExecuteTask (this=0x59921a8, t=...)
at tensorflow/core/lib/core/threadpool.cc:82
#15 0x00007ff42c6f29fb in Eigen::NonBlockingThreadPoolTempltensorflow:ðŸ§µ:EigenEnvironment::WorkerLoop (this=0x59921a0, thread_id=7)
at external/eigen_archive/unsupported/Eigen/CXX11/src/ThreadPool/NonBlockingThreadPool.h:167
#16 0x00007ff42c6f1508 in Eigen::NonBlockingThreadPoolTempltensorflow:ðŸ§µ:EigenEnvironment::NonBlockingThreadPoolTempl(int, tensorflow:ðŸ§µ:EigenEnvironment)::{lambda()#1}::operator()() const
() at external/eigen_archive/unsupported/Eigen/CXX11/src/ThreadPool/NonBlockingThreadPool.h:58
#17 0x00007ff42c6f3927 in std::_Function_handler&lt;void (), Eigen::NonBlockingThreadPoolTempltensorflow:ðŸ§µ:EigenEnvironment::NonBlockingThreadPoolTempl(int, tensorflow:ðŸ§µ:EigenEnvironment)::{lambda()#1}&gt;::_M_invoke(std::_Any_data const&amp;) (__functor=...) at /usr/include/c++/4.8/functional:2071
#18 0x00007ff428bcc7a8 in std::function&lt;void ()&gt;::operator()() const (this=0x59c3bf0)

I haven't tried anything as I don't have any idea about this error. I tried with other architecture, it was working. The difference between this and other architecture is this architecture (the one with error) has depthwise layer and the other (the one without error) doesn't have depthwise layer. Seems not connected with the error from relu layer, so I don't have any idea. Both architectures were working in the non-debugging mode
	</description>
	<comments>
		<comment id='1' author='aseuteurideu' date='2016-12-15T18:54:40Z'>
		Just adding some information (we don't currently have a solution or cycles to look at this). We have seen issues like this in the past where it is either an ODR violation or a bug in nvcc.
		</comment>
		<comment id='2' author='aseuteurideu' date='2016-12-29T20:15:23Z'>
		&lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
, do you have any other thoughts. If we can't reproduce this, I will need to close it as unreproducible. Sorry :(.
		</comment>
		<comment id='3' author='aseuteurideu' date='2017-02-24T03:20:41Z'>
		@kalkaneus Did you resolve the problem? I have a similar issue here.
		</comment>
		<comment id='4' author='aseuteurideu' date='2017-02-27T02:53:33Z'>
		&lt;denchmark-link:https://github.com/endernewton&gt;@endernewton&lt;/denchmark-link&gt;
 unfortunately not :(
		</comment>
		<comment id='5' author='aseuteurideu' date='2017-02-27T03:13:33Z'>
		Oh I actually got it working here. The reason is I am compiling on my centos server without sudo access. I need to use the customized gcc to compile blaze. I didn't set up blaze correctly. Once it was compiled with right gcc, the problem is gone.
		</comment>
		<comment id='6' author='aseuteurideu' date='2017-03-01T06:42:52Z'>
		@kalkaneus A few suggestions:
Perhaps you can try TensorFlow 1.0?  Or take &lt;denchmark-link:https://github.com/endernewton&gt;@endernewton&lt;/denchmark-link&gt;
 's advice and try to ensure you have your build environment set up to use the right gcc?
Note that the build command you listed has both -c opt and -c dbg.  That is an uncommon usage:
bazel build -c opt --config cuda -c dbg --strip=never //tensorflow/tools/pip_package:build_pip_package
If you just want symbols in your binary, you should drop the -c dbg:
bazel build -c opt --config cuda --strip=never //tensorflow/tools/pip_package:build_pip_package
Let us know whether that helps!
		</comment>
		<comment id='7' author='aseuteurideu' date='2017-03-02T00:45:21Z'>
		&lt;denchmark-link:https://github.com/tatatodd&gt;@tatatodd&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/endernewton&gt;@endernewton&lt;/denchmark-link&gt;
 thank you for your suggestions. But as for now, I am doing something else, so later when I have extra time, I'll try to do that in my machine. I'll let you know the result afterwards
And I want to have dbg option as I want thoroughly debug it. Without the dbg flag, everything works well so far.
		</comment>
		<comment id='8' author='aseuteurideu' date='2017-03-03T03:15:28Z'>
		In that case I'll close this issue out.  @kalkaneus feel free to file a new issue if you encounter new problems.  Thanks!
		</comment>
	</comments>
</bug>