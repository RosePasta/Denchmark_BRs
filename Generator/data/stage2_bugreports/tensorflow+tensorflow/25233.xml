<bug id='25233' author='mk123qwe' open_date='2019-01-27T16:52:22Z' closed_time='2019-02-22T19:39:16Z'>
	<summary>import tensorflow.keras,code can not work.import keras,the same code run perfectly.</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):custom code
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):win10 x64
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):binary
TensorFlow version (use command below):1.12
Python version:3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:4g

You can collect some of this information using our environment capture &lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with
python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
Describe the current behavior
import tensorflow.keras,code can not work.
import keras,the same code run perfectly.
Describe the expected behavior
import tensorflow.keras,code can work.
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
import numpy as np
train_x = np.random.random((1000, 32))
train_y = np.random.random((1000, 10))

import tensorflow as tf
from tensorflow.keras import layers
inputs = tf.keras.Input(shape=(32,)) 
x = layers.Dense(60, activation='relu')(inputs)
x = layers.Dense(30, activation='relu')(x)
predictions = layers.Dense(10)(x)
model = tf.keras.Model(inputs=inputs, outputs=predictions)

model.compile(optimizer='adam',
              loss='mse',
              metrics=['accuracy'])

import keras.backend as K
def scheduler(epoch):
    lr = K.get_value(model.optimizer.lr)
    print("lr:{}".format(lr * 1))
    return K.get_value(model.optimizer.lr)
 
reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)
history = model.fit(train_x, train_y, batch_size=16, epochs=10,callbacks=[reduce_lr])
b = history.history['lr']
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
FailedPreconditionError (see above for traceback): Error while reading resource variable Adam/lr from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/Adam/lr)
	 [[node Adam/lr/Read/ReadVariableOp (defined at &lt;ipython-input-2-48494f67c56a&gt;:15)  = ReadVariableOp[dtype=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:GPU:0"](Adam/lr)]]
	 [[{{node Adam/lr/Read/ReadVariableOp/_1}} = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_5_Adam/lr/Read/ReadVariableOp", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]
Code to reproduce the Success
import numpy as np
train_x = np.random.random((1000, 32))
train_y = np.random.random((1000, 10))

from keras.models import Model
from keras.layers import Input, Dense
inputs = Input(shape=(32,)) 
x = Dense(60, activation='relu')(inputs)
x = Dense(30, activation='relu')(x)
predictions = Dense(10)(x)

model = Model(inputs=inputs, outputs=predictions)
model.compile(optimizer='adam',
              loss='mse',
              metrics=['acc'])

import keras.backend as K
from keras.callbacks import LearningRateScheduler
def scheduler(epoch):
    lr = K.get_value(model.optimizer.lr)
    print("lr:{}".format(lr * 1))
    return K.get_value(model.optimizer.lr)
 
reduce_lr = LearningRateScheduler(scheduler)
history = model.fit(train_x, train_y, batch_size=16, epochs=10,callbacks=[reduce_lr])
b = history.history['lr']
	</description>
	<comments>
		<comment id='1' author='mk123qwe' date='2019-02-22T17:25:58Z'>
		Hi &lt;denchmark-link:https://github.com/mk123qwe&gt;@mk123qwe&lt;/denchmark-link&gt;
 , can you please try this with the latest nightly? This should be fixed
		</comment>
		<comment id='2' author='mk123qwe' date='2019-02-22T17:54:23Z'>
		&lt;denchmark-link:https://github.com/omalleyt12&gt;@omalleyt12&lt;/denchmark-link&gt;
 The issue persists with latest nightly build. 1.13.0-dev20190221
		</comment>
		<comment id='3' author='mk123qwe' date='2019-02-22T19:12:09Z'>
		&lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;
 would you be able to take a look at this? looks like the Adam lr is not getting initialized in some case
		</comment>
		<comment id='4' author='mk123qwe' date='2019-02-22T19:18:09Z'>
		&lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;
 try having everything in tf.keras, in this particular case replace "import keras.backend as K" to "import tensorflow.keras.backend as K" and see if it works fine for you?
		</comment>
		<comment id='5' author='mk123qwe' date='2019-02-22T19:20:26Z'>
		Oh great catch, I missed that. Yeah, that's very likely to be it
		</comment>
		<comment id='6' author='mk123qwe' date='2019-02-22T19:30:39Z'>
		&lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;
 That worked. Tested against TF '1.13.0-rc2' Thanks!
		</comment>
		<comment id='7' author='mk123qwe' date='2019-02-22T19:39:15Z'>
		Glad it works for you!
		</comment>
	</comments>
</bug>