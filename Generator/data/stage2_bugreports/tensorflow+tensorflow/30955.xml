<bug id='30955' author='DeepBlender' open_date='2019-07-23T13:07:42Z' closed_time='2019-08-12T03:46:14Z'>
	<summary>Subclassing Model prevents the computation of intermediate values in graph mode</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Google Colab
TensorFlow version (use command below):
Tensorflow 2.0 beta 1

Describe the current behavior
Computing intermediate values within a subclassed model doesn't work in graph mode.
Describe the expected behavior
It should most likely work, as it was possible in TensorFlow 1.x.

Try it directly in Google Colab. Setting 'use_eager_mode' to True/False switches between the two cases.
&lt;denchmark-link:https://gist.github.com/DeepBlender/6ab324ab3b14552109979a97bf4acb8f&gt;https://gist.github.com/DeepBlender/6ab324ab3b14552109979a97bf4acb8f&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='DeepBlender' date='2019-07-24T08:52:11Z'>
		I tried executing the given code on colab and I am able to reproduce the issue with use_eager_mode=False. Thanks!
		</comment>
		<comment id='2' author='DeepBlender' date='2019-07-26T17:49:23Z'>
		The code provided snippet while printing intermediate results fails with same message in TF 2.X (eager mode disabled) as well as TF 1.X (eager disabled by default).
AttributeError: Layer conv2d_1 has no inbound nodes.
Can you please confirm if you are able to execute this script successfully in  TF 1.X?
		</comment>
		<comment id='3' author='DeepBlender' date='2019-07-27T12:12:52Z'>
		&lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;
 sorry for the confusion, I meant to say similar functionality, but that does not seem to be accurate either.
It has been my understanding that in TensorFlow 2.0 it should be possible to switch between graph and eager mode flawlessly. This is a relatively simple example where it doesn't work.
		</comment>
		<comment id='4' author='DeepBlender' date='2019-07-29T17:10:23Z'>
		Even though, the example I presented has a workaround, a minor change will eliminate this workaround. If you add "x = x - 0.5" in "call", the code simply doesn't work anymore and there is no way to compute the intermediate results.
As this way of using the Model class is presented as pretty much the "TensorFlow 2.0 way", this issue is reducing the flexibility quite significantly.
Edit: I just realized that a workaround is to use Lambda. However, that's not suggested anywhere and nothing in the error message would help me to figure that out.
		</comment>
		<comment id='5' author='DeepBlender' date='2019-07-30T15:29:38Z'>
		&lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;
 I'm also having trouble creating subgraph for model using the subclassing API (on tensorflow-gpu==2.0.0-beta1).
Google Colab: &lt;denchmark-link:https://colab.research.google.com/drive/1kRbgCtF2CIfiK3F75FjDZiI241cfPC-f&gt;https://colab.research.google.com/drive/1kRbgCtF2CIfiK3F75FjDZiI241cfPC-f&lt;/denchmark-link&gt;

Code below:
import numpy as np
import tensorflow as tf

NUM_CLASSES = 10
TARGET_LAYER_NAME = 'target_layer_name'

# Create the Subclassing API Class
class SubclassedModel(tf.keras.Model):
  def __init__(self, name='subclassed'):
    super(SubclassedModel, self).__init__(name=name)
    self.conv_1 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')
    self.conv_2 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', name=TARGET_LAYER_NAME)
    self.maxpool_1 = tf.keras.layers.MaxPool2D(pool_size=(2, 2))

    self.flatten = tf.keras.layers.Flatten()

    self.dense_1 = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')

  def call(self, inputs, **kwargs):
    x = inputs
    for layer in [self.conv_1, self.conv_2, self.maxpool_1, self.flatten, self.dense_1]:
        x = layer(x)

    return x

  def compute_output_shape(self, input_shape):
    shape = tf.TensorShape(input_shape).as_list()
    return tf.TensorShape([shape[0], NUM_CLASSES])

# Initialize a model using the subclassing API
model = SubclassedModel()
model(np.random.random((4, 28, 28, 1)).astype('float32'))  # Sample call to build the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Trying to fit some random data, all goes well
training_size = 256
sample_x = np.random.random((training_size, 28, 28, 1)).astype('float32')
sample_y = np.eye(NUM_CLASSES)[np.random.choice(NUM_CLASSES, training_size)]
history = model.fit(sample_x, sample_y, epochs=3, verbose=0)

# Trying to extract a subgraph
submodel = tf.keras.Model([model.inputs], [model.get_layer(TARGET_LAYER_NAME).output])
submodel.summary()

# Trying the same thing with sequential API -- it works

model_seq = tf.keras.Sequential([
  tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),
  tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', name=TARGET_LAYER_NAME),
  tf.keras.layers.MaxPool2D(pool_size=(2, 2)),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(NUM_CLASSES, activation='softmax'),
])
model_seq.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model_seq.fit(sample_x, sample_y, epochs=3, verbose=0)

submodel_seq = tf.keras.Model([model_seq.inputs], [model_seq.get_layer(TARGET_LAYER_NAME).output])
submodel_seq.summary()
Stacktrace is:
&lt;denchmark-link:https://user-images.githubusercontent.com/12402673/62143005-921fdb80-b2ef-11e9-9c10-ac7285bf4a2d.png&gt;&lt;/denchmark-link&gt;

Is there a workaround for this?
Edit: Error also appears on tf 1.14
		</comment>
		<comment id='6' author='DeepBlender' date='2019-08-12T03:46:13Z'>
		&lt;denchmark-link:https://github.com/DeepBlender&gt;@DeepBlender&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/RaphaelMeudec&gt;@RaphaelMeudec&lt;/denchmark-link&gt;

In general it's not possible to extract subgraphs of subclassed Models. For those use cases, please use Functional API models
Functional API models are described by their DAG, but for subclassed Models, the code is the definition and in general it's not possible to extract a DAG from them
Please note that even in 2.0, model.fit executes inside its own tf.function (similar to 1.x graph style). To run model.fit eagerly, compile your Model with model.compile(..., run_eagerly=True)
		</comment>
		<comment id='7' author='DeepBlender' date='2019-08-12T03:46:15Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=30955&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=30955&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='DeepBlender' date='2019-08-13T11:33:51Z'>
		Thanks for the clarification!
At this point, the documentation feels very misleading in the sense that "Everything is Possible in TensorFlow 2.0!" kind of sense. For me, it reads like: Do whatever you want. (Intentionally exaggerated of course!)
However, those details are missing (as far as I can see), but they are certainly very important. Should I create a new issue to point to this deficiency in the documentation out or can we repurpose this issue?
		</comment>
		<comment id='9' author='DeepBlender' date='2019-12-05T16:12:14Z'>
		Has there been any update on this limitation?
		</comment>
		<comment id='10' author='DeepBlender' date='2019-12-28T12:53:48Z'>
		
Thanks for the clarification!
At this point, the documentation feels very misleading in the sense that "Everything is Possible in TensorFlow 2.0!" kind of sense. For me, it reads like: Do whatever you want. (Intentionally exaggerated of course!)
However, those details are missing (as far as I can see), but they are certainly very important. Should I create a new issue to point to this deficiency in the documentation out or can we repurpose this issue?

As it allows DAG definition, it will be confusing to have output bound to layers in the functional and subclassing model api.
For example, consider a simple param sharing case:
&lt;denchmark-code&gt;dense = Dense(256, activation='relu')
x1 = dense(x)
x2 = dense(x1)
&lt;/denchmark-code&gt;

What should be bound to the output of the 'dense' layer? A list seems work, but that conflict with layer having multiple outputs.
In sequential api, the model is linear, and things are simple. In the DAG case, it'd be better to operate manually.
		</comment>
	</comments>
</bug>