<bug id='28151' author='Sean0123456789' open_date='2019-04-25T12:38:52Z' closed_time='2019-06-12T19:44:29Z'>
	<summary>after quantization aware training, when I convert to tflite quant int8 model, still need to  provide min/max value for  add operation? why add operation need min/max value?</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):binary
TensorFlow version (use command below):1.13
Python version:3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:9.2/ cudnn 7
GPU model and memory:

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
after quantization aware training, why i'm still asked to provide min/max value for add operation when convert to tflite int8 model?
tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.
2019-04-25 20:18:08.606359: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 2360 operators, 3530 arrays (0 quantized)
2019-04-25 20:18:08.663749: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 2360 operators, 3530 arrays (0 quantized)
2019-04-25 20:18:08.993938: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 273 operators, 505 arrays (1 quantized)
2019-04-25 20:18:08.998229: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 273 operators, 505 arrays (1 quantized)
2019-04-25 20:18:09.000372: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 160 operators, 392 arrays (1 quantized)
2019-04-25 20:18:09.002236: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 160 operators, 392 arrays (1 quantized)
2019-04-25 20:18:09.004023: F tensorflow/lite/toco/tooling_util.cc:1708] Array slim_mobilenetv2/Add, which is an input to the Conv operator producing the output array slim_mobilenetv2/Conv_6/Relu6, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.
Fatal Python error: Aborted
Current thread 0x00007fb011618740 (most recent call first):
File "/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py", line 33 in execute
File "/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/absl/app.py", line 251 in _run_main
File "/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/absl/app.py", line 300 in run
File "/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/tensorflow/python/platform/app.py", line 40 in run
File "/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py", line 59 in main
File "/home/syshang/anaconda3/envs/tfnightly0410/bin/toco_from_protos", line 10 in 
Aborted (core dumped)
Describe the expected behavior
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='Sean0123456789' date='2019-05-02T02:36:10Z'>
		any ideas?
		</comment>
		<comment id='2' author='Sean0123456789' date='2019-05-15T01:57:25Z'>
		any updates?
		</comment>
		<comment id='3' author='Sean0123456789' date='2019-05-18T02:47:20Z'>
		a similar problem, do you solve it? bro
		</comment>
		<comment id='4' author='Sean0123456789' date='2019-05-20T09:52:06Z'>
		I meet the same prblem
		</comment>
		<comment id='5' author='Sean0123456789' date='2019-06-12T19:44:29Z'>
		Based on feedback that the contrib/quantize quantization-aware training tool is a bit brittle and hard to use on some model architectures, we have released a &lt;denchmark-link:https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-post-training-integer-quantization-b4964a1ea9ba&gt;post-training integer quantization tool&lt;/denchmark-link&gt;
, that requires a small calibration dataset. Please take a look at the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tutorials/post_training_integer_quant.ipynb&gt;tutorial&lt;/denchmark-link&gt;
 and give it a try, it should work much better! And let us know if you run into any issues.
Closing this issue, since we are rethinking and working on an api to replace contrib/quantize quantization-aware-training (although post-training quantization above should be sufficient for the majority of use cases).
Thanks!
-Suharsh
		</comment>
		<comment id='6' author='Sean0123456789' date='2019-06-12T19:44:31Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=28151&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=28151&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>