<bug id='31117' author='makercob' open_date='2019-07-29T02:39:22Z' closed_time='2019-08-04T14:38:02Z'>
	<summary>Model does not converge during training with distribute strategy and tf.py_function in dataset.map</summary>
	<description>
System information

Have I written custom code: Yes
OS Platform and Distribution:Windows 10 Pro
TensorFlow installed from:binary(pip)
TensorFlow version:2.0.0-beta1
Python version:3.6.8
CUDA/cuDNN version:10.0
GPU model and memory:Tesla K80

Describe the current behavior
Model does not converge and training is extremely slow with distribute strategy, if tf.py_function called in dataset.map.
Works fine without distribute strategy or implementing map function without tf.py_function.
Describe the expected behavior
Succesful training with distribute strategy when tf.py_function called in dataset.map.

Reproducible in Colaboratory with TF 1.14.0
&lt;denchmark-link:https://colab.research.google.com/drive/1J98yNqa4gslsTe1SiCrhomhNB6UREkLC&gt;https://colab.research.google.com/drive/1J98yNqa4gslsTe1SiCrhomhNB6UREkLC&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow import keras


def get_keys():
    return list(range(-1000, 1000))

# generate positive feature and label
def generate_positive():
    x = tf.random.uniform(shape=(), minval=0.0, maxval=20.0)
    y = tf.random.uniform(shape=(), minval=0.0, maxval=20.0)
    label = tf.constant(1)
    return x, y, label


# generate negative feature and label
def generate_negative():
    x = tf.random.uniform(shape=(), minval=40.0, maxval=100.0)
    y = tf.random.uniform(shape=(), minval=40.0, maxval=100.0)
    label = tf.constant(0)
    return x, y, label


# generate pos/neg feature and label
def generate_pf(key):
    if key &gt; -1:
        x = tf.random.uniform(shape=(), minval=0.0, maxval=20.0)
        y = tf.random.uniform(shape=(), minval=0.0, maxval=20.0)
        label = tf.constant(1)
    else:
        x = tf.random.uniform(shape=(), minval=40.0, maxval=100.0)
        y = tf.random.uniform(shape=(), minval=40.0, maxval=100.0)
        label = tf.constant(0)

    x = tf.math.sign(tf.random.uniform(shape=(), minval=-1.0, maxval=1.0)) * tf.abs(x)
    y = tf.math.sign(tf.random.uniform(shape=(), minval=-1.0, maxval=1.0)) * tf.abs(y)
    feature = tf.stack([x, y])
    return feature, label


# dataset map function: works with or without distribute strategy
def map_tf(key):
    x, y, label = tf.cond(tf.greater(key, -1), generate_positive, generate_negative)
    x = tf.math.sign(tf.random.uniform(shape=(), minval=-1.0, maxval=1.0)) * tf.abs(x)
    y = tf.math.sign(tf.random.uniform(shape=(), minval=-1.0, maxval=1.0)) * tf.abs(y)
    feature = tf.stack([x, y])
    return feature, label


# dataset map function: works without distribute strategy
def map_py_func(key):
    feature, label = tf.py_function(func=generate_pf,
                                    inp=[key],
                                    Tout=[tf.float32, tf.int32])
    # TODO: skip shape setting if training without distribute strategy
    feature.set_shape([2])
    label.set_shape([1])
    return feature, label


def get_dataset():
    keys = get_keys()
    dataset = tf.data.Dataset.from_tensor_slices(keys)
    dataset = dataset.repeat()
    dataset = dataset.shuffle(buffer_size=len(keys))
    # replace 'map_py_func' with 'map_tf' to have successful training
    dataset = dataset.map(lambda key: map_py_func(key), 
                          num_parallel_calls=tf.data.experimental.AUTOTUNE)
    dataset = dataset.batch(100)
    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
    return dataset


def get_model():
    x = inputs = keras.Input([2])
    x = keras.layers.Dense(units=4, activation='relu')(inputs=x)
    x = keras.layers.Dense(units=4, activation='relu')(inputs=x)
    x = keras.layers.Dense(units=1, activation='sigmoid')(inputs=x)
    return keras.Model(inputs=inputs, outputs=x)


# server = tf.distribute.Server.create_local_server()
train_dataset = get_dataset()
val_dataset = get_dataset()

strategy = tf.distribute.OneDeviceStrategy('/gpu:0')
# strategy = tf.distribute.MirroredStrategy(devices=["/gpu:0", "/gpu:1"],
#                                           cross_device_ops=tf.distribute.ReductionToOneDevice())
with strategy.scope():
    model = get_model()
    optimizer = keras.optimizers.Adam(learning_rate=1e-2)
    model.compile(optimizer=optimizer,
                  loss=keras.losses.BinaryCrossentropy(name='loss'),
                  metrics=[keras.metrics.BinaryAccuracy(name='accuracy')],
                  run_eagerly=False)

model.fit(train_dataset,
          initial_epoch=0,
          epochs=20,
          steps_per_epoch=50,
          validation_data=val_dataset,
          validation_steps=1,
          validation_freq=1)
&lt;/denchmark-code&gt;

Other info / logs
&lt;denchmark-code&gt;  1/100 [..............................] - ETA: 11:27 - loss: 15.6796 - accuracy: 0.5120
  2/100 [..............................] - ETA: 6:00 - loss: 18.0561 - accuracy: 0.4938 
  3/100 [..............................] - ETA: 4:10 - loss: 19.7856 - accuracy: 0.4705
  4/100 [&gt;.............................] - ETA: 3:14 - loss: 18.6577 - accuracy: 0.4707
  5/100 [&gt;.............................] - ETA: 2:42 - loss: 17.9387 - accuracy: 0.4689
  6/100 [&gt;.............................] - ETA: 2:19 - loss: 17.1595 - accuracy: 0.4741
  7/100 [=&gt;............................] - ETA: 2:04 - loss: 16.9366 - accuracy: 0.4744
  8/100 [=&gt;............................] - ETA: 1:53 - loss: 16.6326 - accuracy: 0.4814
  9/100 [=&gt;............................] - ETA: 1:44 - loss: 16.1086 - accuracy: 0.4825
 10/100 [==&gt;...........................] - ETA: 1:36 - loss: 15.7002 - accuracy: 0.4848
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='makercob' date='2019-07-30T05:22:20Z'>
		&lt;denchmark-link:https://github.com/makercob&gt;@makercob&lt;/denchmark-link&gt;
 I tried executing the Colab link provided, Please confirm us is this expected behavior. Please look at the colab &lt;denchmark-link:https://colab.research.google.com/drive/1vmljazkzTyE7Xyo7vruf7oyUYk1Q78y9&gt;gist&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='makercob' date='2019-07-30T05:46:26Z'>
		&lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
 Thanks! Issue is reproduced in the gist: Without discarding distribute strategy or replacing 'map_py_func' with 'map_tf', accuracy will always oscillate around 50%.
		</comment>
		<comment id='3' author='makercob' date='2019-07-31T21:19:07Z'>
		I tried reproducing this but I think I am getting the same behavior with and without strategy - the accuracy hovers around 50% in either case.. can you send a version that does work?
		</comment>
		<comment id='4' author='makercob' date='2019-07-31T21:20:22Z'>
		I tried reproducing but even without distribution strategy, I am seeing the same behavior (accuracy hovers around 50%). Can you share a working example?
&lt;denchmark-link:https://user-images.githubusercontent.com/14104855/62248998-4a0bc200-b39e-11e9-9371-1ae1495a17d4.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='makercob' date='2019-08-01T01:31:31Z'>
		&lt;denchmark-link:https://github.com/guptapriya&gt;@guptapriya&lt;/denchmark-link&gt;
 oops, my bad. In Colab(TF 1.14), eager mode must be enabled to achieve successful training.
&lt;denchmark-link:https://colab.research.google.com/drive/1J98yNqa4gslsTe1SiCrhomhNB6UREkLC&gt;https://colab.research.google.com/drive/1J98yNqa4gslsTe1SiCrhomhNB6UREkLC&lt;/denchmark-link&gt;

here  in TensorFlow2.0.0b, accuracy will grow to 1.00:
&lt;denchmark-link:https://user-images.githubusercontent.com/6904036/62258773-5cd5d380-b43e-11e9-971e-7d24cce1f27e.png&gt;&lt;/denchmark-link&gt;

BTW, according to the doc: &lt;denchmark-link:url&gt;https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/py_function&lt;/denchmark-link&gt;


The operation must run in the same address space as the Python program that calls tf.py_function(). If you are using distributed TensorFlow, you must run a tf.distribute.Server in the same process as the program that calls tf.py_function() and you must pin the created operation to a device in that server (e.g. using with tf.device():).

Could you please elaborate on how to pin operation when distribute strategy is involved? Thanks!
		</comment>
		<comment id='6' author='makercob' date='2019-08-02T00:56:42Z'>
		&lt;denchmark-link:https://github.com/guptapriya&gt;@guptapriya&lt;/denchmark-link&gt;
 feature.set_shape([2]) is the culprit. I think this issue could be closed. Thank you.
		</comment>
		<comment id='7' author='makercob' date='2019-08-04T14:38:04Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=31117&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=31117&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>