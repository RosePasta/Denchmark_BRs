<bug id='28956' author='EmanueleGhelfi' open_date='2019-05-23T07:12:16Z' closed_time='2019-06-19T23:34:12Z'>
	<summary>Distribute Strategy wrong reduction</summary>
	<description>
&lt;denchmark-h:h2&gt;URL(s) with the issue: https://www.tensorflow.org/guide/distribute_strategy#using_tfdistributestrategy_with_custom_training_loops&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;Description of issue (what needs changing):&lt;/denchmark-h&gt;

In the documentation the snippet:
def train_step():
  def step_fn(inputs):
    features, labels = inputs
    logits = model(features)
    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(
        logits=logits, labels=labels)  
    loss = tf.reduce_sum(cross_entropy) * (1.0 / global_batch_size)
    train_op = optimizer.minimize(loss)
    with tf.control_dependencies([train_op]):
      return tf.identity(loss)

  per_replica_losses = mirrored_strategy.experimental_run(
      step_fn, input_iterator)
  mean_loss = mirrored_strategy.reduce(
      tf.distribute.ReduceOp.MEAN, per_replica_losses)
  return mean_loss
calculates the loss using the distribution strategy. However the loss for each replica is reduced using tf.distribute.ReduceOp.MEAN. I think that the correct loss to return is the loss reduced using tf.distribute.ReduceOp.SUM since every loss is already reduced using the partial mean over the global_batch_size ( tf.reduce_sum(cross_entropy) * (1.0 / global_batch_size)) .
I think a better snippet would be:
def train_step():
  def step_fn(inputs):
    features, labels = inputs
    logits = model(features)
    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(
        logits=logits, labels=labels)  
    loss = tf.reduce_sum(cross_entropy) * (1.0 / global_batch_size)
    train_op = optimizer.minimize(loss)
    with tf.control_dependencies([train_op]):
      return tf.identity(loss)

  per_replica_losses = mirrored_strategy.experimental_run(
      step_fn, input_iterator)
  loss = mirrored_strategy.reduce(
      tf.distribute.ReduceOp.SUM, per_replica_losses)
  return loss
Am I wrong?
&lt;denchmark-h:h3&gt;Submit a pull request?&lt;/denchmark-h&gt;

Maybe I can submit a pull request to fix this.
Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: &lt;denchmark-link:https://www.tensorflow.org/community/contribute/docs&gt;https://www.tensorflow.org/community/contribute/docs&lt;/denchmark-link&gt;
 and the
docs style guide: &lt;denchmark-link:https://www.tensorflow.org/community/contribute/docs_style&gt;https://www.tensorflow.org/community/contribute/docs_style&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='EmanueleGhelfi' date='2019-06-19T23:34:12Z'>
		hi &lt;denchmark-link:https://github.com/EmanueleGhelfi&gt;@EmanueleGhelfi&lt;/denchmark-link&gt;
 - you're absolutely right, and looks like we've fixed the documentation too :) thanks for reporting and apologies for the delay!
		</comment>
	</comments>
</bug>