<bug id='27525' author='alexsludds' open_date='2019-04-04T23:58:14Z' closed_time='2019-04-08T23:10:58Z'>
	<summary>tf.function-decorated function</summary>
	<description>
System information

I have written several custom layers for this code. However, they are tested and are not the cause of this bug.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Debian Stable (up to date)
TensorFlow installed from (source or binary): downloaded the latest alpha of tf2 through pip
TensorFlow version (use command below): That command does not work for tf2
Python version: 3.6.8

I have an error in my code (located : &lt;denchmark-link:https://github.com/alexsludds/6888_final_project&gt;here&lt;/denchmark-link&gt;
). If you look at the main.py file you see that I am trying to change a variable and train a model repeatedly, extracting the testing accuracy each time. If you run this file for a minute, you see that on the second step of this first for loop I get an error:
ValueError: tf.function-decorated function tried to create variables on non-first call.
To me this means that train is trying to create a new tf variable, but I don't understand why, the model should be different and the everything is getting called again.
Is there anything I can do to get around this by flushing the graph, or is this a bug in the alpha version of tf2?
Thank you
	</description>
	<comments>
		<comment id='1' author='alexsludds' date='2019-04-05T20:04:40Z'>
		I did some debugging. The error occurs at
optimizer.apply_gradients(zip(grads, model.trainable_variables))
in my train.py file.
I don't know why this is occuring. I don't think this function should try to create any new variables.
		</comment>
		<comment id='2' author='alexsludds' date='2019-04-08T21:44:08Z'>
		Are you using a different optimizer on subsequent calls to the tf.function than the one you used on the first call? TF optimizers can create variables.
		</comment>
		<comment id='3' author='alexsludds' date='2019-04-08T21:57:41Z'>
		Hi alextp,
In the code I posted every time I generate a new model I generate a new instance of the Adam optimizer. I changed my code and moved the generation of the optimizer out of the functions and just have it get called once. However, I still get the same error as before.
Any ideas?
		</comment>
		<comment id='4' author='alexsludds' date='2019-04-08T22:05:42Z'>
		tf.function was designed so you need to make a new tf.function object every
time you generate a new model
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Mon, Apr 8, 2019 at 3:00 PM Alex Sludds ***@***.***&gt; wrote:
 Hi alextp,
 In the code I posted every time I generate a new model I generate a new
 instance of the Adam optimizer. I changed my code and moved the generation
 of the optimizer out of the functions and just have it get called once.
 However, I still get the same error as before.
 Any ideas?

 —
 You are receiving this because you were assigned.
 Reply to this email directly, view it on GitHub
 &lt;#27525 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AAATxcBmtmr1XUccnAoLKBPxB7wBCdeBks5ve7wXgaJpZM4cd7JA&gt;
 .


-- 
 - Alex

		</comment>
		<comment id='5' author='alexsludds' date='2019-04-08T22:17:57Z'>
		Oh I see. If I have a function, such as train in my code, how do I create many a new train object on each iteration of a for-loop?
		</comment>
		<comment id='6' author='alexsludds' date='2019-04-08T22:22:16Z'>
		If f is a python function, tf_f = tf.function(f) will create a new tf
function from f.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Mon, Apr 8, 2019 at 3:21 PM Alex Sludds ***@***.***&gt; wrote:
 Oh I see. If I have a function, such as train in my code, how do I create
 many a new train object on each iteration of a for-loop?

 —
 You are receiving this because you were assigned.
 Reply to this email directly, view it on GitHub
 &lt;#27525 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AAATxYBoX6dUeBsOc0SFZWfvhNVBLZp8ks5ve8DbgaJpZM4cd7JA&gt;
 .


-- 
 - Alex

		</comment>
		<comment id='7' author='alexsludds' date='2019-04-08T22:40:47Z'>
		Oh awesome! That worked!
Thanks for the help.
A problem I will probably have soon I might as well ask, is there a way for me to clear the graph in tf2 to reduce the RAM used?
		</comment>
		<comment id='8' author='alexsludds' date='2019-04-08T23:08:35Z'>
		Currently it's not easy, we're working on doing this right.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Mon, Apr 8, 2019 at 3:45 PM Alex Sludds ***@***.***&gt; wrote:
 Oh awesome! That worked!
 Thanks for the help.
 A problem I will probably have soon I might as well ask, is there a way
 for me to clear the graph in tf2 to reduce the RAM used?

 —
 You are receiving this because you were assigned.
 Reply to this email directly, view it on GitHub
 &lt;#27525 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AAATxT4INQG54ddlhMS7FWCgRSI0NoTuks5ve8Z5gaJpZM4cd7JA&gt;
 .


-- 
 - Alex

		</comment>
		<comment id='9' author='alexsludds' date='2019-04-08T23:10:59Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=27525&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=27525&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='alexsludds' date='2019-04-27T19:04:55Z'>
		Is it the reason that tf 2.0 GPU utilization is lower than tf 1.* ?? since we cannot use apply_gradients with tf.function decorator ?
		</comment>
		<comment id='11' author='alexsludds' date='2019-04-29T15:50:05Z'>
		&lt;denchmark-link:https://github.com/Chunpai&gt;@Chunpai&lt;/denchmark-link&gt;
 GPU utilization should be the same for equivalent code, and apply_gradients should work with tf.function.
Please open a separate issue if you're having problems.
		</comment>
		<comment id='12' author='alexsludds' date='2019-06-24T02:24:36Z'>
		getting this issue in 1.14 using skopt to do hyperparameter optimization, the graph runs great but hangs during later searches
		</comment>
	</comments>
</bug>