<bug id='27687' author='rohith14' open_date='2019-04-09T15:01:56Z' closed_time='2019-11-15T21:34:33Z'>
	<summary>quantized mobilenet_v1_1.0_224 is 4x slower than un-qualtized version (both testing on tflite)</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below):1.13.0-rc0
Python version:Python 3.5.5 :: Anaconda, Inc
Bazel version (if compiling from source):NA
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:10/7.4.1
GPU model and memory:Titan V, 12 GB

You can collect some of this information using our environment capture &lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with
python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
Describe the current behavior
quantized mobilenet_v1_1.0_224 is 4x slower than un-qualtized version (both testing on tflite)
Describe the expected behavior
Code to reproduce the issue
&lt;denchmark-code&gt;# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import numpy as np

from PIL import Image
import tensorflow as tf
import time
import os

def Covert_to_tflite():    
    graph_def_file = os.path.join(os.getcwd(), 'mobilenet_v1_1.0_224/mobilenet_v1_1.0_224_frozen.pb')
    input_arrays = ["input"]
    output_arrays = ["MobilenetV1/Predictions/Softmax"]

    converter = tf.lite.TFLiteConverter.from_frozen_graph(
    graph_def_file, input_arrays, output_arrays)
    tflite_model = converter.convert()
    open("mobilenet_v1_1.0_224/converted_model.tflite", "wb").write(tflite_model)

    converter.post_training_quantize=True
    tflite_quantized_model=converter.convert()
    open("mobilenet_v1_1.0_224/quantized_converted_model.tflite", "wb").write(tflite_quantized_model)
def load_labels(filename):
  my_labels = []
  input_file = open(filename, 'r')
  for l in input_file:
    my_labels.append(l.strip())
  return my_labels
if __name__ == '__main__':
    
    # print('### for coverting to tflite - START ###')
    # Covert_to_tflite()
    # print('### for coverting to tflite - END ###')
    
    print('### for testing performance/accuracy - START###')
    file_name = "./grace_hopper.jpg"
    # model_file = "mobilenet_v1_1.0_224/mobilenet_v1_1.0_224.tflite"
    # model_file = "mobilenet_v1_1.0_224/mobilenet_v1_1.0_224_quant.tflite"
    label_file = "mobilenet_v1_1.0_224/labels.txt"
    input_mean = 127.5
    input_std = 127.5
    floating_model = False

    parser = argparse.ArgumentParser()
    parser.add_argument("--graph", help=".tflite model to be executed")
    args = parser.parse_args()

    if args.graph:
        model_file = args.graph
  
    interpreter = tf.lite.Interpreter(model_path=model_file)
    interpreter.allocate_tensors()

    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    # check the type of the input tensor
    if input_details[0]['dtype'] == type(np.float32(1.0)):
        floating_model = True

    # NxHxWxC, H:1, W:2
    height = input_details[0]['shape'][1]
    width = input_details[0]['shape'][2]
    img = Image.open(file_name)
    img = img.resize((width, height))

    # add N dim
    input_data = np.expand_dims(img, axis=0)

    if floating_model:
        input_data = (np.float32(input_data) - input_mean) / input_std

    start = time.time()
    interpreter.set_tensor(input_details[0]['index'], input_data)

    interpreter.invoke()
    end = time.time()

    output_data = interpreter.get_tensor(output_details[0]['index'])
    results = np.squeeze(output_data)

    top_k = results.argsort()[-5:][::-1]
    labels = load_labels(label_file)
    for i in top_k:
        if floating_model:
            print('{0:08.6f}'.format(float(results[i]))+":", labels[i])
        else:
            print('{0:08.6f}'.format(float(results[i]/255.0))+":", labels[i])
    print("{}, Inference time: {} sec".format(model_file, end-start))
    print('### for testing performance/accuracy - END###')
&lt;/denchmark-code&gt;

Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Other info / logs
&lt;denchmark-code&gt;$ python convert_and_test_tflite.py --graph mobilenet_v1_1.0_224/converted_model.tflite
### for testing performance/accuracy - START###
0.445140: 653:military uniform
0.162699: 458:bow tie, bow-tie, bowtie
0.137379: 907:Windsor tie
0.121642: 466:bulletproof vest
0.047150: 668:mortarboard
mobilenet_v1_1.0_224/converted_model.tflite, Inference time: 0.056842803955078125 sec
### for testing performance/accuracy - END###

$ python convert_and_test_tflite.py --graph mobilenet_v1_1.0_224/quantized_converted_model.tflite
### for testing performance/accuracy - START###
0.259418: 653:military uniform
0.239987: 466:bulletproof vest
0.239753: 907:Windsor tie
0.180003: 458:bow tie, bow-tie, bowtie
0.016782: 668:mortarboard
mobilenet_v1_1.0_224/quantized_converted_model.tflite, Inference time: 0.21327567100524902 sec
### for testing performance/accuracy - END###
&lt;/denchmark-code&gt;

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='rohith14' date='2019-11-15T21:34:33Z'>
		We report performance numbers on mobile devices. Seems like you're running this test on a desktop. We don't have clear baseline for desktop performance for now.
		</comment>
		<comment id='2' author='rohith14' date='2019-11-15T21:34:35Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27687&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27687&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>