<bug id='31192' author='kassemyu' open_date='2019-07-31T10:24:56Z' closed_time='2020-09-17T10:42:23Z'>
	<summary>conv2d_transpose param tensor shapes differ from conv2d shapes</summary>
	<description>
System information

Have I written custom code: yes
OS Platform and Distribution: Ubuntu 16.04
Mobile device: Not tested
TensorFlow installed from: binary
TensorFlow version: 1.13 and 1.12
Python version: 3.7.3 and 3.6.8
Bazel version:
GCC/Compiler version:
CUDA/cuDNN version: 10.0/7.3.1
GPU model and memory: Nvidia, driver 418.56, 11178MiB

Describe the current behavior
The kernel tensor for conv2d and conv2d_transpose layers (in contrib.layers) behave differently. The number of outputs determines the number of featuremaps/filters and is at the last position (X,X,X,here) while for conv2d_transpose layers it is (X,X,here,X) as the following code example will show. Is this behaviour wanted? Because when building in model where the input_dimensions and out_put_dimensions differ I need to manually transpose the last 2 dimensions in Order for the model to be trainable.
Describe the expected behavior
The expected Behaviour would be that the output number for conv2d_layers is at the same position as for usual conv2d_layers.
This problem is independent of the data_format and saving/loading procedures.
Code to reproduce the issue
This is a small code snippet for reproduction. Please set the breakpoint at the last print(...) and evaluate the shapes-dictionary. You will see that conv2d layer 'conv1' has the kernel shape
(3, 3, 1, 32), indicating that 1 is the depth/number of channels for the input.
32 is the number of filters/outputs/feature maps.
If you look at the conv2d_transposed layer 'up1' the number of outputs is set to 16 but the kernel shape is (3,3,16,32) indicating that the input dimensions are 16 even though they are 32 from the conv2d layer 'conv3' preceding it. I believe that this shape should instead be (3,3,32,16) because the number of outputs for the conv2d_transposed layer is set to 16.
&lt;denchmark-code&gt;import tensorflow as tf
import tensorflow.contrib.layers as layers


from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("/tmp/data/", one_hot=True)
train_X = mnist.train.images
train_Y = mnist.train.labels
test_X = mnist.test.images


def shapes_of_built_model(layer_names):
    layer_names_and_shapes = {}
    for name in layer_names:
        layer_names_and_shapes[name] = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=name)[0].shape
    return layer_names_and_shapes


def model(inputs_):
    # Encoder
    conv1 = layers.conv2d(inputs_, num_outputs=32, kernel_size=(3, 3), scope='conv1')
    conv_str2 = layers.conv2d(conv1, num_outputs=32, kernel_size=(3, 3), stride=2, scope='conv_str2')
    conv2 = layers.conv2d(conv_str2, num_outputs=32, kernel_size=(3, 3), scope='conv2')
    encoded = layers.conv2d(conv2, num_outputs=32, kernel_size=(3, 3), stride=2, scope='encoding')

    conv3 = layers.conv2d(encoded, num_outputs=32, kernel_size=(3, 3), scope='conv3')
    upsample1 = layers.conv2d_transpose(conv3, num_outputs=16, kernel_size=3, stride=2, scope='up1')
    upsample2 = layers.conv2d_transpose(upsample1, num_outputs=32, kernel_size=3,  stride=2, scope='up2')
    logits = layers.conv2d(upsample2, num_outputs=1, kernel_size=(3, 3),  scope='logits', padding='SAME')
    decoded = tf.sigmoid(logits, name='reconstruct')
    return decoded


with tf.name_scope('Input'):
    x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1], name='X')
    y = tf.placeholder(tf.float32, shape=[None, 28, 28, 1], name='Y')

output_logits = model(x)

with tf.variable_scope('Train'):
    with tf.variable_scope('Loss'):
        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output_logits), name='loss')
    tf.summary.scalar('loss', loss)
    with tf.variable_scope('Optimizer'):
        optimizer = tf.train.AdamOptimizer(learning_rate=0.05, name='Adam-op')
        optimizer = optimizer.minimize(loss)
    with tf.variable_scope('Accuracy'):
        correct_prediction = tf.equal(tf.argmax(output_logits, 1), tf.argmax(y, 1), name='correct_pred')
        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')
    tf.summary.scalar('accuracy', accuracy)
    with tf.variable_scope('Prediction'):
        cls_prediction = tf.argmax(output_logits, axis=1, name='predictions')


init = tf.global_variables_initializer()
merged = tf.summary.merge_all()
sess = tf.InteractiveSession()
sess.run(init)

names = ['conv1', 'conv_str2', 'conv2', 'encoding', 'conv3', 'up1', 'up2']
shapes = shapes_of_built_model(names)
print('set breakpoint here')
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='kassemyu' date='2019-08-01T07:04:55Z'>
		&lt;denchmark-link:https://github.com/kassemyu&gt;@kassemyu&lt;/denchmark-link&gt;
 I executed the code and got the following error,
 Thanks!
		</comment>
		<comment id='2' author='kassemyu' date='2019-08-01T09:34:29Z'>
		Hey &lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
 thank you for the quick reply. I do not encounter this error and did not set anything additionally. When I run the supplied code-snippet in pycharm's debugging mode I get to evaluate the shapes-dictionary as described.
		</comment>
		<comment id='3' author='kassemyu' date='2019-08-02T08:10:04Z'>
		&lt;denchmark-link:https://github.com/kassemyu&gt;@kassemyu&lt;/denchmark-link&gt;
 Will it be possible to execute the code in Colab and attache it here? Thanks!
		</comment>
		<comment id='4' author='kassemyu' date='2019-08-08T08:57:02Z'>
		Hi &lt;denchmark-link:https://github.com/gadagashwini&gt;@gadagashwini&lt;/denchmark-link&gt;
, I will give it a try and keep you informed.
		</comment>
		<comment id='5' author='kassemyu' date='2019-08-08T09:19:53Z'>
		I just tried it in Colab (new Python 3 notebook) and it worked with the following snippet:
The shapes that are outputted are as described in the original description of the problem.
After the first run Colab seems to cache in the graph because if a second run is attempted I get the same error message you mentioned before 'ValueError: Variable conv1/weights already exists...'
But for the first run in a new python 3 notebook it should all be fine.
Thanks!
&lt;denchmark-code&gt;import tensorflow as tf
import tensorflow.contrib.layers as layers


from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("/tmp/data/", one_hot=True)
train_X = mnist.train.images
train_Y = mnist.train.labels
test_X = mnist.test.images


def shapes_of_built_model(layer_names):
    layer_names_and_shapes = {}
    for name in layer_names:
        layer_names_and_shapes[name] = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=name)[0].shape
    return layer_names_and_shapes


def model(inputs_):
    # Encoder
    conv1 = layers.conv2d(inputs_, num_outputs=32, kernel_size=(3, 3), scope='conv1')
    conv_str2 = layers.conv2d(conv1, num_outputs=32, kernel_size=(3, 3), stride=2, scope='conv_str2')
    conv2 = layers.conv2d(conv_str2, num_outputs=32, kernel_size=(3, 3), scope='conv2')
    encoded = layers.conv2d(conv2, num_outputs=32, kernel_size=(3, 3), stride=2, scope='encoding')

    conv3 = layers.conv2d(encoded, num_outputs=32, kernel_size=(3, 3), scope='conv3')
    upsample1 = layers.conv2d_transpose(conv3, num_outputs=16, kernel_size=3, stride=2, scope='up1')
    upsample2 = layers.conv2d_transpose(upsample1, num_outputs=32, kernel_size=3,  stride=2, scope='up2')
    logits = layers.conv2d(upsample2, num_outputs=1, kernel_size=(3, 3),  scope='logits', padding='SAME')
    decoded = tf.sigmoid(logits, name='reconstruct')
    return decoded


with tf.name_scope('Input'):
    x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1], name='X')
    y = tf.placeholder(tf.float32, shape=[None, 28, 28, 1], name='Y')

output_logits = model(x)

with tf.variable_scope('Train'):
    with tf.variable_scope('Loss'):
        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output_logits), name='loss')
    tf.summary.scalar('loss', loss)
    with tf.variable_scope('Optimizer'):
        optimizer = tf.train.AdamOptimizer(learning_rate=0.05, name='Adam-op')
        optimizer = optimizer.minimize(loss)
    with tf.variable_scope('Accuracy'):
        correct_prediction = tf.equal(tf.argmax(output_logits, 1), tf.argmax(y, 1), name='correct_pred')
        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')
    tf.summary.scalar('accuracy', accuracy)
    with tf.variable_scope('Prediction'):
        cls_prediction = tf.argmax(output_logits, axis=1, name='predictions')


init = tf.global_variables_initializer()
merged = tf.summary.merge_all()
sess = tf.InteractiveSession()
sess.run(init)

names = ['conv1', 'conv_str2', 'conv2', 'encoding', 'conv3', 'up1', 'up2']
shapes = shapes_of_built_model(names)
print('conv1 shape: ' + str(shapes['conv1']))
print('up1 shape: ' + str(shapes['up1']))
print('up2 shape: ' + str(shapes['up2']))

&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='kassemyu' date='2019-08-09T06:28:38Z'>
		&lt;denchmark-link:https://github.com/kassemyu&gt;@kassemyu&lt;/denchmark-link&gt;
 Yes you are right. For first, I could able to reproduce the issue and second run i got the error. Please see the gist &lt;denchmark-link:https://colab.research.google.com/drive/1lNplSpFek8DBS6X_Jzy68aWv5xXZX_jR&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='7' author='kassemyu' date='2019-08-12T13:30:33Z'>
		Hey, thanks for the quick reply.
Glad it is now reproducible. &lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;
, do you have any idea why the shapes of transposed layers behave like this?
Thanks in advance.
		</comment>
		<comment id='8' author='kassemyu' date='2019-08-14T22:20:26Z'>
		Apologies for the delay in response.  module is deprecated. Can you please try the example  using &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/layers&gt;tf.keras.layers&lt;/denchmark-link&gt;
 with latest  build? Thanks!
		</comment>
		<comment id='9' author='kassemyu' date='2019-08-19T13:29:23Z'>
		Hey, thanks for the input!
I will give it a try and tell you about the results.
		</comment>
		<comment id='10' author='kassemyu' date='2019-08-27T11:47:55Z'>
		Hey &lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;
 , I read the documentation of various layers and ops and it seems to be intended for conv_2d layers and conv2d_transpose layers to have different kernels (i.e. the last dimensions being swapped, also for keras layers since they rely on nn.layers at the core). In Use-cases such as structured pruning this can be really confusing because the number of outputs, which corresponds to the number of prunable filters, is on a different position, making it necessary to keep track of which convolutional layer is transposed etc. and swapping when pruning a transposed one.


[filter_height, filter_width, in_channels, out_channels] for conv2d layers


[height, width, output_channels, in_channels] for conv2d_transpose (where in_channels have to be the same as the ones in input's [batch, in_channels, height, width] in_channel value, which I kept track of)


Even when doing so, for transposed layers, errors such as input depth must be evenly divisible by filter depth, occur in cases where the transposed layers kernel looks something like this
(3,3, 136 (outputs/filters), 256(in_channels))
while for conv2d (3,3, 256(input_depth), 136(outputs/filters)) would not yield any problems (despite the two values not being divisible (for keras layers)). Maybe this should not be reported as a bug since the documentation of nn.layers describes the filters in the way they are defined. But the error about the divisibility occurs only for transposed layers.
Is there a particular reason for this behaviour? For now I stopped pruning transposed layers and hope for a solution to my problem.
Thanks in advance!
		</comment>
		<comment id='11' author='kassemyu' date='2019-08-27T19:02:40Z'>
		Adding zhenyu who is an expert on this topic.
		</comment>
		<comment id='12' author='kassemyu' date='2019-09-03T14:42:49Z'>
		Hey &lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;
 ,
could you please give me your opinion on the so far described problem?
Thank you very much in advance and thanks &lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;
 for adding an expert.
I am very grateful for this opportunity.
		</comment>
		<comment id='13' author='kassemyu' date='2020-01-01T21:06:07Z'>
		I also ran into this issue while writing a custom Wrapper object that takes both Conv2D and Conv2DTranspose. I checked the tensorflow source code and was surprised to see it always assumes the output depth is at self.kernel[-1]. For example, the kernel initializers will compute fanout variable to be [-1], when it should be [-2] for Conv2DTranspose. This isn't a problem for Glorot, since it just averages fanout/fanin, but for others, it will be initializing incorrectly.
I don't know any other places in the main source code that this will be incorrect, personally. Though, I did see the Wrapper objects (e.g. WeightNormalization layer), also make this assumption.
		</comment>
		<comment id='14' author='kassemyu' date='2020-03-26T18:00:02Z'>
		Contrib layers have been deprecated and removed from 2.x. Please take a look at the builtin Conv2D layers: &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D&gt;https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D&lt;/denchmark-link&gt;

		</comment>
		<comment id='15' author='kassemyu' date='2020-03-26T18:25:23Z'>
		&lt;denchmark-link:https://github.com/karmel&gt;@karmel&lt;/denchmark-link&gt;
 The main issue is that the code is assuming output depth at self.kernel[-1] when it is actually [-2] for Conv2DTranspose. So fanout kernel initializer is being computed incorrectly for Conv2DTranspose specifically.
I was just mentioning that the Wrapper objects also have the bug, but since those are deprecated, no need to worry about that. But the bug persists in the builtin Conv2DTranspose layer (and maybe other parts of the code, as I said before).
		</comment>
		<comment id='16' author='kassemyu' date='2020-09-17T10:42:23Z'>
		&lt;denchmark-link:https://github.com/kassemyu&gt;@kassemyu&lt;/denchmark-link&gt;
  Closing this issue as of now since it was the intended behavior. Please feel free to re-open the issue if you have any concern.Thanks!
		</comment>
		<comment id='17' author='kassemyu' date='2020-09-17T10:42:25Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31192&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31192&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>