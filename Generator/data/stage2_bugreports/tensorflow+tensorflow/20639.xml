<bug id='20639' author='mohanr' open_date='2018-07-09T07:11:56Z' closed_time='2018-07-17T14:57:17Z'>
	<summary>Assignment inside while loop affected by concurrency</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 8 (CPU)
TensorFlow installed from (source or binary):
Binary
TensorFlow version (use command below):
1.2.1
Python version:
Python 3.6.5 |Anaconda
Bazel version:
N/A
CUDA/cuDNN version:
N/A
GPU model and memory:
N/A
Exact command to reproduce:

No gpu
&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

I wrote this code to answer a SO question but then realized the results are inconsistent probably due to the concurrency involved. The correct answer is returned repeatedly a few times but then very randomly. All other answers are wrong.
I have also read about a switch that turns off concurrent execution but generally the concurrency API and the memory model take care of almost all cases without the involvement of the programmer. In the case of Java it is &lt;denchmark-link:http://download.oracle.com/otndocs/jcp/memory_model-1.0-pfd-spec-oth-JSpec/&gt;JMM&lt;/denchmark-link&gt;

It isn't clear whether this is concurrency or parallelism ?
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import tensorflow as tf

v = [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
n = len(v)
a1 = tf.Variable(v, name = 'a')

def cond(i, _):
    return i &lt; n

s = tf.InteractiveSession()
s.run(tf.global_variables_initializer())

def body( i, _):
    x = a1[i-1]
    y = a1[i-2]
    z = tf.add(x,y)
    op = a1[i].assign( tf.add(x,y) )
    increment = tf.add(i, 1)
    return increment, op

print(s.run(tf.while_loop(cond, body, [2, a1])))
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='mohanr' date='2018-07-09T18:44:08Z'>
		Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.
Bazel version
CUDA/cuDNN version
GPU model and memory
		</comment>
		<comment id='2' author='mohanr' date='2018-07-11T10:43:48Z'>
		That information is updated.
		</comment>
		<comment id='3' author='mohanr' date='2018-07-13T00:15:04Z'>
		&lt;denchmark-link:https://github.com/skye&gt;@skye&lt;/denchmark-link&gt;
 so you have any comment on this. I was not able to reproduce this on Ubuntu, is it Windows specific?
		</comment>
		<comment id='4' author='mohanr' date='2018-07-13T09:11:24Z'>
		&lt;denchmark-code&gt;(11, array([1, 1, 2, 1, 0, 1, 0, 1, 1, 0, 0]))
(11, array([ 1,  1,  2,  3,  5,  8,  5,  5, 10, 15, 10]))
(11, array([ 1,  1,  2,  3,  5,  8, 13, 21, 34, 21,  0]))
(11, array([ 1,  1,  2,  3,  5,  8, 13, 21, 34,  0,  0]))
(11, array([ 1,  1,  2,  3,  5,  8, 13, 21, 13, 13, 13]))
(11, array([ 1,  1,  2,  1,  3,  4,  7, 11, 18, 29, 47]))
(11, array([ 1,  1,  2,  3,  5,  8, 13, 21,  0,  0,  0]))
(11, array([ 1,  1,  2,  3,  5,  8, 13, 21, 34, 55,  0]))
&lt;/denchmark-code&gt;

This set doesn't have the correct result  (11, array([ 1,  1,  2,  3,  5,  8, 13, 21, 34, 55, 89]))
		</comment>
		<comment id='5' author='mohanr' date='2018-07-16T15:38:55Z'>
		Can you try adding a control dependency on op to increment? Something like:
&lt;denchmark-code&gt;with tf.control_dependencies([op]):
  increment = tf.add(i, 1)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='mohanr' date='2018-07-17T07:07:34Z'>
		Yes. That produces the correct result. Is this a thread-safety rule ?
		</comment>
		<comment id='7' author='mohanr' date='2018-07-17T14:57:16Z'>
		No, it's about TF's execution model. You could theoretically see this result with a single thread.
Here's the issue: TF will execute an operation once its inputs are ready, which may not correspond to the order operations are defined in your program. If the inputs to multiple ops are ready, there are no guarantees about which one will run first.
In the case of while_loop, you can imagine the loop is unrolled:
i1 = 2
# iteration 1
x1 = a1[i1-1]
y1 = a1[i1-2]
z1 = tf.add(x1,y1)
op1 = a1[i1].assign( tf.add(x1,y1) )
increment1 = tf.add(i1, 1)
i2 = increment1
# iteration 2
x2 = a1[i2-1]
y2 = a1[i2-2]
z2 = tf.add(x2,y2)
op2 = a1[i2].assign( tf.add(x2,y2) )
increment2 = tf.add(i2, 1)
# ...
Given that the only data input to iteration 2 is i2, which ultimately only depends on i1, TF can actually start executing iteration 2 before iteration 1 completes. This means it might run op2 before op1 is run, which is what causes the wrong results in your original example. Adding the control dependency forces op{i} to complete before increment{i} can run, which forces all the assign ops to run in the correct order.
The root issue is that the assign ops have visible side effects (i.e. writing to a1, which is then read in subsequent operations), but the execution doesn't take this into account; it only looks at the direct inputs and control deps.
This is obviously super confusing and error-prone, so we're working on coming up with better execution semantics. One option is to enable &lt;denchmark-link:https://www.tensorflow.org/guide/eager&gt;eager exeuction&lt;/denchmark-link&gt;
, which does run everything in program order.
Another option is to use &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/contrib/eager/defun&gt;tf.contrib.eager.defun&lt;/denchmark-link&gt;
, although this is still being actively developed. This creates a graph, but automatically makes sure that ops with side effects (like variable assignment) happen in the order they're defined. This way you don't have to worry about manually adding control dependencies.
Sorry this is so long, I hope it helps. I'm gonna close this issue since it's not actually a concurrency bug,  but feel free to comment if something is still confusing or you have more questions.
		</comment>
		<comment id='8' author='mohanr' date='2019-04-04T20:58:30Z'>
		I got this error too. This error is so vague. if while_loop works in this weird way, then how can anybody even know their program is working properly?  this terribly/disastrously   designed API should be deprecated from using at all.
		</comment>
		<comment id='9' author='mohanr' date='2019-04-04T21:15:21Z'>
		Agreed! tf.function (part of TF2) actually fixes this. See &lt;denchmark-link:https://www.tensorflow.org/alpha/tutorials/eager/tf_function#state_in_tffunction&gt;https://www.tensorflow.org/alpha/tutorials/eager/tf_function#state_in_tffunction&lt;/denchmark-link&gt;
:

For example, when writing code which has multiple reads and writes to the same variables, a dataflow graph might not naturally encode the originally intended order of operations. In tf.function, however, because we're converting code which was traced from Python, we know the intended execution order.
This means there's no need to add manual control dependencies; tf.function is smart enough to add the minimal set of necessary and sufficient control dependencies for your code to run correctly.

This holds true if you use a while_loop inside a tf.function. So yes, the original tf.Graph behavior will be deprecated in favor of tf.function :)
		</comment>
		<comment id='10' author='mohanr' date='2019-04-04T21:19:05Z'>
		
Agreed! tf.function (part of TF2) actually fixes this. See https://www.tensorflow.org/alpha/tutorials/eager/tf_function#state_in_tffunction:

For example, when writing code which has multiple reads and writes to the same variables, a dataflow graph might not naturally encode the originally intended order of operations. In tf.function, however, because we're converting code which was traced from Python, we know the intended execution order.
This means there's no need to add manual control dependencies; tf.function is smart enough to add the minimal set of necessary and sufficient control dependencies for your code to run correctly.

This holds true if you use a while_loop inside a tf.function. So yes, the original tf.Graph behavior will be deprecated in favor of tf.function :)

I still need to use tf 1.XX, not every company is ready to move to tf 2.0.
For example, How do I make the output of  not random? thanks!
if I use CPU, it's not random, if I GPU, it's random.
&lt;denchmark-link:https://colab.research.google.com/drive/1WcBo8HLIe8Wq9yakNNAYINwiP6GrmAQI&gt;https://colab.research.google.com/drive/1WcBo8HLIe8Wq9yakNNAYINwiP6GrmAQI&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='mohanr' date='2019-04-04T21:30:25Z'>
		tf.function will also be available in TF 1.14 (it's already in the nightly releases, if you're brave).
I don't have permission to see your colab. Also, I switched teams and am no longer working on TF... I suggest filing a new issue if you're having trouble with something.
		</comment>
		<comment id='12' author='mohanr' date='2019-04-04T21:41:16Z'>
		
tf.function will also be available in TF 1.14 (it's already in the nightly releases, if you're brave).
I don't have permission to see your colab. Also, I switched teams and am no longer working on TF... I suggest filing a new issue if you're having trouble with something.

Ohh thanks!, unfortuantely I am still using tf 1.12. : / . new link is here: &lt;denchmark-link:https://colab.research.google.com/drive/1uz_64qp6rlTQRbDRqOye6s7yddnbCODB&gt;https://colab.research.google.com/drive/1uz_64qp6rlTQRbDRqOye6s7yddnbCODB&lt;/denchmark-link&gt;
 .  thanks! : D
		</comment>
		<comment id='13' author='mohanr' date='2019-11-08T07:46:47Z'>
		

tf.function will also be available in TF 1.14 (it's already in the nightly releases, if you're brave).
I don't have permission to see your colab. Also, I switched teams and am no longer working on TF... I suggest filing a new issue if you're having trouble with something.

Ohh thanks!, unfortuantely I am still using tf 1.12. : / . new link is here: https://colab.research.google.com/drive/1uz_64qp6rlTQRbDRqOye6s7yddnbCODB . thanks! : D

The "tf.assign_add" function has a parameter "use_locking", set it to true and you will get fixed result each time. I think this  can make thread safety.
		</comment>
		<comment id='14' author='mohanr' date='2020-05-24T16:53:02Z'>
		tf.function still have similar problem.
&lt;denchmark-code&gt;import tensorflow as tf

a = tf.Variable(0)


@tf.function
def f():
    dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
    for i in dataset:
        a.assign(i)
        tf.print(a)
    tf.print(a)


f()
&lt;/denchmark-code&gt;

The code above produce 1 2 3 0 in TF 2.1.
Using use_locking=True doesn't help.
However, TF 2.0 will produce 1 2 3 3.
		</comment>
	</comments>
</bug>