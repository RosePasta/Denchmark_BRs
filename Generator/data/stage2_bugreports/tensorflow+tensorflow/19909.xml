<bug id='19909' author='spivakoa' open_date='2018-06-11T14:15:49Z' closed_time='2018-08-24T11:49:09Z'>
	<summary>Tensorflow C++ API Batch Inference</summary>
	<description>
Hello,
My question is considering the inference in C++ API using batches. I'm trying to run SSD net based on Mobilenet. It works on a single image, but when I try to use multiple images inference, I can't understand how to get the multiple outputs. It seems like my program runs properly as a run time increased for a batch inference though.
Here is my sample code: I'm loading a batch of 32 images with OpenCV, transform them into a input tensor and run the Session. I need to understand what should I change in order to get 32 outputs.
Thank you.
&lt;denchmark-code&gt;int main(int argc, char* argv[]) {
	//string image(argv[1]);
	int batchSize = 32;
	string pathFilenameImg1 = "Patch6.jpg";
	string pathFilenameImg2 = "Patch1.jpg";
	string pathFilenameImg3 = "Patch2.jpg";
	string pathFilenameImg4 = "Patch3.jpg";
	string pathFilenameImg5 = "Patch0.jpg";
	string pathFilenameImg6 = "Patch1.jpg";
	string pathFilenameImg7 = "Patch2.jpg";
	string pathFilenameImg8 = "Patch3.jpg";
	string graph = "ssd_mobilenet.pb";
	string labels = "security_labels.txt";
	int32 input_width = 512;
	int32 input_height = 512;
	int32 input_depth = 3;
	string input_layer = "image_tensor";
	vector&lt;string&gt; output_layer = { "detection_boxes:0", "detection_scores:0", "detection_classes:0", "num_detections:0"};

	bool self_test = false;
	string root_dir = "";

	// First we load and initialize the model.
	std::unique_ptr&lt;tensorflow::Session&gt; session;
	string graph_path = tensorflow::io::JoinPath(root_dir, graph);
	LOG(ERROR) &lt;&lt; "graph_path:" &lt;&lt; graph_path;
	Status load_graph_status = LoadGraph(graph_path, &amp;session);
	if (!load_graph_status.ok()) {
		LOG(ERROR) &lt;&lt; "LoadGraph ERROR!!!!" &lt;&lt; load_graph_status;
		return -1;
	}


	// Read and prepare images using OpenCV:
	std::vector&lt;string&gt; imgPathArray= { pathFilenameImg1 ,pathFilenameImg2 ,pathFilenameImg3 ,pathFilenameImg1, pathFilenameImg1 ,pathFilenameImg2 ,pathFilenameImg3 ,pathFilenameImg1, 
										pathFilenameImg1 ,pathFilenameImg2 ,pathFilenameImg3 ,pathFilenameImg1, pathFilenameImg1 ,pathFilenameImg2 ,pathFilenameImg3 ,pathFilenameImg1,
										pathFilenameImg1 ,pathFilenameImg2 ,pathFilenameImg3 ,pathFilenameImg1, pathFilenameImg1 ,pathFilenameImg2 ,pathFilenameImg3 ,pathFilenameImg1,
		pathFilenameImg1 ,pathFilenameImg2 ,pathFilenameImg3 ,pathFilenameImg1, pathFilenameImg1 ,pathFilenameImg2 ,pathFilenameImg3 ,pathFilenameImg1
	};
	std::vector&lt;cv::Mat&gt; imgArray;
	for (int i = 0; i &lt; batchSize; i++)
		imgArray.push_back(cv::imread(imgPathArray.at(i)));


	// creating a Tensor for storing the data
	Tensor input_tensor(tensorflow::DT_UINT8, tensorflow::TensorShape({ batchSize, input_height, input_width, input_depth}));
	auto input_tensor_mapped = input_tensor.tensor&lt;uchar, 4&gt;();

	for (int bz = 0; bz &lt; batchSize; ++bz) {
		uchar *source_data;
		source_data = imgArray.at(bz).data;
		for (int y = 0; y &lt; input_height; ++y) {
			uchar* source_row = source_data + (y * input_width * input_depth);
			for (int x = 0; x &lt; input_width; ++x) {
				uchar* source_pixel = source_row + (x * input_depth);
				uchar* source_B = source_pixel + 0;
				uchar* source_G = source_pixel + 1;
				uchar* source_R = source_pixel + 2;
				input_tensor_mapped(bz, y, x, 0) = *source_R;
				input_tensor_mapped(bz, y, x, 1) = *source_G;
				input_tensor_mapped(bz, y, x, 2) = *source_B;
			}
		}
	}


	/*// Get the image from disk as a float array of numbers, resized and normalized
	// to the specifications the main graph expects.
	std::vector&lt;Tensor&gt; resized_tensors;
	string image_path = tensorflow::io::JoinPath(root_dir, image);
	Status read_tensor_status = ReadTensorFromImageFile(image_path, input_height, input_width, input_mean, input_std, &amp;resized_tensors);
	if (!read_tensor_status.ok()) {
		LOG(ERROR) &lt;&lt; read_tensor_status;
		return -1;
	}
	const Tensor&amp; resized_tensor = resized_tensors[0];

	LOG(ERROR) &lt;&lt; "image shape:" &lt;&lt; resized_tensor.shape().DebugString() &lt;&lt; ",len:" &lt;&lt; resized_tensors.size() &lt;&lt; ",tensor type:" &lt;&lt; resized_tensor.dtype();*/


	// Actually run the image through the model.
	std::vector&lt;Tensor&gt; outputs;

	std::chrono::steady_clock::time_point begin = std::chrono::steady_clock::now();
	Status run_status = session-&gt;Run({ { input_layer, input_tensor} }, output_layer, {}, &amp;outputs);
	std::chrono::steady_clock::time_point end = std::chrono::steady_clock::now();	std::cout &lt;&lt; "Time difference (sec) = " &lt;&lt; (std::chrono::duration_cast&lt;std::chrono::microseconds&gt;(end - begin).count()) / 1000000.0 &lt;&lt; std::endl;

	if (!run_status.ok()) {
		LOG(ERROR) &lt;&lt; "Running model failed: " &lt;&lt; run_status;
		return -1;
	}

	//int image_width = resized_tensor.dims();
	//int image_height = 0;
	//int image_height = resized_tensor.shape()[1];

	//LOG(ERROR) &lt;&lt; "output size:" &lt;&lt; outputs.size() &lt;&lt; ",image_width:" &lt;&lt; image_width &lt;&lt; ",image_height:" &lt;&lt; image_height &lt;&lt; endl;

	auto boxes = outputs[0].flat_outer_dims&lt;float, 3&gt;();
	tensorflow::TTypes&lt;float&gt;::Flat scores = outputs[1].flat&lt;float&gt;();
	tensorflow::TTypes&lt;float&gt;::Flat classes = outputs[2].flat&lt;float&gt;();
	tensorflow::TTypes&lt;float&gt;::Flat num_detections = outputs[3].flat&lt;float&gt;();


	//LOG(ERROR) &lt;&lt; "num_detections:" &lt;&lt; num_detections(0) &lt;&lt; "," &lt;&lt; outputs[0].shape().DebugString();
	int BarcodeCnt = 0;
	int GyoshCnt = 0;
	for (size_t i = 0; i &lt; num_detections(0) &amp;&amp; i &lt; 20; ++i)
	{
		if (scores(i) &gt; 0.9)
		{
			LOG(ERROR) &lt;&lt;"score:" &lt;&lt; scores(i) &lt;&lt; ",class:" &lt;&lt; classes(i) &lt;&lt; ",box:" &lt;&lt; "," &lt;&lt; boxes(0, i, 0) &lt;&lt; "," &lt;&lt; boxes(0, i, 1) &lt;&lt; "," &lt;&lt; boxes(0, i, 2) &lt;&lt; "," &lt;&lt; boxes(0, i, 3);
			if(classes(i) == 1)
				BarcodeCnt++;
			else
				GyoshCnt ++;
		}
	}
	LOG(ERROR) &lt;&lt; "Total number of Security Barcodes is :" &lt;&lt; BarcodeCnt;
	LOG(ERROR) &lt;&lt; "Total number of Security Gyoshes is :" &lt;&lt; GyoshCnt;

	


	return 0;
}
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='spivakoa' date='2018-06-12T01:05:43Z'>
		Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.
Have I written custom code
OS Platform and Distribution
TensorFlow installed from
TensorFlow version
Bazel version
CUDA/cuDNN version
GPU model and memory
Exact command to reproduce
		</comment>
		<comment id='2' author='spivakoa' date='2018-06-12T06:54:19Z'>
		Have I written custom code     - Yes
OS Platform and Distribution - Win10 x64
TensorFlow installed from - Tensorflow site
TensorFlow version - 1.8
Bazel version - N/A
CUDA/cuDNN version    CUDA 9.0/ CuDNN 7
GPU model and memory   Quadro P4000, 8GB
Exact command to reproduce - Code pasted above in my original comment.
		</comment>
		<comment id='3' author='spivakoa' date='2018-06-12T15:26:21Z'>
		This question is better asked on &lt;denchmark-link:http://stackoverflow.com/questions/tagged/tensorflow&gt;StackOverflow&lt;/denchmark-link&gt;
 since it is not a bug or feature request. There is also a larger community that reads questions there.
If you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/new&gt;issue template&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='4' author='spivakoa' date='2018-06-13T15:06:04Z'>
		You are right, it's not a bug in the code, however,  there is no explanation on this issue anywhere in Tensorflow C++ API documentation. I consider it a serious issue, as I am left guessing what should be done in order to get my code working.
Despite StackOverflow being a bigger community, there are no dedicated persons to deal with those specific issues. My question there remains unanswered and a previous same question from 2016 is still open.
I expect you guys to have a knowledge to help developers and not leaving them frustrated.
Thank you,
Alex.
		</comment>
		<comment id='5' author='spivakoa' date='2018-06-13T16:08:33Z'>
		Thanks, &lt;denchmark-link:https://github.com/spivakoa&gt;@spivakoa&lt;/denchmark-link&gt;
 for clarifying-- if this is specifically a request for documentation, we can address that. Can you clarify what function/process you feel lacks documentation? A link would help.
		</comment>
		<comment id='6' author='spivakoa' date='2018-06-13T18:56:28Z'>
		It is in the name of the topic: How can I run inference on batch of images instead of a single one?
In my code example above I showed how I "fill" an input 4-D Tensor with pixel values.
Here it is:
// creating a Tensor for storing the data
Tensor input_tensor(tensorflow::DT_UINT8, tensorflow::TensorShape({ batchSize, input_height, input_width, input_depth}));
auto input_tensor_mapped = input_tensor.tensor&lt;uchar, 4&gt;();

for (int bz = 0; bz &lt; batchSize; ++bz) {
	uchar *source_data;
	source_data = imgArray.at(bz).data;
	for (int y = 0; y &lt; input_height; ++y) {
		uchar* source_row = source_data + (y * input_width * input_depth);
		for (int x = 0; x &lt; input_width; ++x) {
			uchar* source_pixel = source_row + (x * input_depth);
			uchar* source_B = source_pixel + 0;
			uchar* source_G = source_pixel + 1;
			uchar* source_R = source_pixel + 2;
			input_tensor_mapped(bz, y, x, 0) = *source_R;
			input_tensor_mapped(bz, y, x, 1) = *source_G;
			input_tensor_mapped(bz, y, x, 2) = *source_B;
		}
	}
} 
Then I run the Graph:
Status run_status = session-&gt;Run({ { input_layer, input_tensor} }, output_layer, {}, &amp;outputs);
However the outputs is vector of Tensors with size of 4, because the output layer is defined as follows:
output_layer = { "detection_boxes:0", "detection_scores:0", "detection_classes:0", "num_detections:0"};
I need to understand how should I define the output to reveal 4xbatch size values.
This what Tensorflow documentation lacks with. No explanation whatsoever of how to run multiple images using C++ API.
		</comment>
		<comment id='7' author='spivakoa' date='2018-06-13T18:59:32Z'>
		&lt;denchmark-link:https://github.com/MarkDaoust&gt;@MarkDaoust&lt;/denchmark-link&gt;
 - perhaps running inference would be a good notebook to include with the set of examples?
		</comment>
		<comment id='8' author='spivakoa' date='2018-06-16T06:38:25Z'>
		&lt;denchmark-link:https://github.com/MarkDaoust&gt;@MarkDaoust&lt;/denchmark-link&gt;
 Any thoughts? Thx.
		</comment>
		<comment id='9' author='spivakoa' date='2018-06-16T16:24:15Z'>
		Hey &lt;denchmark-link:https://github.com/spivakoa&gt;@spivakoa&lt;/denchmark-link&gt;

Is this based on an existing example? which one (there are lots of example folders)? We could look into adding some better handling for batching there.
I haven't used the c++ interface (but java is similar) so I'm not 100% sure about this, but I think the problem is here:
	auto boxes = outputs[0].flat_outer_dims&lt;float, 3&gt;();
	tensorflow::TTypes&lt;float&gt;::Flat scores = outputs[1].flat&lt;float&gt;();
	tensorflow::TTypes&lt;float&gt;::Flat classes = outputs[2].flat&lt;float&gt;();
	tensorflow::TTypes&lt;float&gt;::Flat num_detections = outputs[3].flat&lt;float&gt;();
The first dimension of the output tensors should be the batch dimension. Can you check the shapes of the tensors to confirm?
But it looks like you're flattening the tensors (to remove the batch dimension for the batch_size=1 base case). if you check the length of those resulting tensors you should see that they're batch times as long.
If you can iterate or index along that first dimension those slices will be the single image results you're looking for.
		</comment>
		<comment id='10' author='spivakoa' date='2018-06-16T16:53:09Z'>
		Hi Mark,
I wrote my code based on the following code:
&lt;denchmark-link:https://github.com/memo/ofxMSATensorFlow/issues/34&gt;memo/ofxMSATensorFlow#34&lt;/denchmark-link&gt;

My output Tensor is defined as follows:
&lt;denchmark-code&gt;output_layer = { "detection_boxes:0", "detection_scores:0", "detection_classes:0", "num_detections:0"};
&lt;/denchmark-code&gt;

Those variables are taken from the net output  layer:
&lt;denchmark-code&gt;vector&lt;string&gt; output_layer = { "detection_boxes", "detection_scores", "detection_classes", "num_detections"};
&lt;/denchmark-code&gt;

Thus the first dimension is the detection_boxes coordinates from the SSD.
I thought that using batch inference should result in an additional dimension in output variable, maybe defining it as a vector of vector of Tensors, but this apparently doesn't work. The output dimension is dependent on a number of outputs in the outer net layer, 4 in my case.
It seems like something "good" happens when increasing the batch size as the run time of the code also increases, I just can't figure out where to find the particular output for each batch image.
Thank you,
Alex.
		</comment>
		<comment id='11' author='spivakoa' date='2018-06-25T10:53:26Z'>
		Hey &lt;denchmark-link:https://github.com/MarkDaoust&gt;@MarkDaoust&lt;/denchmark-link&gt;
,
Do you have any update on this issue?
Thank you,
Alex.
		</comment>
		<comment id='12' author='spivakoa' date='2018-06-25T15:05:38Z'>
		Hi, thanks for the ping.
&lt;denchmark-link:https://github.com/skye&gt;@skye&lt;/denchmark-link&gt;
, we've got a couple of questions about handling batching in the CC api.  Do you know any good examples demonstrating this? Am I pointing &lt;denchmark-link:https://github.com/spivakoa&gt;@spivakoa&lt;/denchmark-link&gt;
 in the right direction here?
I haven't gotten to look at this in more detail, but it looks like we still don't understand each other.

vector&lt;string&gt; output_layer = { "detection_boxes", "detection_scores", "detection_classes", "num_detections"};

Thus the first dimension is the detection_boxes coordinates from the SSD.

This thing returns 4 tensors.
Each tensor is a multidimensional array.
The first dimension of the array is (probably) the batch size.
This line (looks like) it's flattening the num_detections tensor into a 1D array:
&lt;denchmark-code&gt;tensorflow::TTypes&lt;float&gt;::Flat num_detections = outputs[3].flat&lt;float&gt;();
&lt;/denchmark-code&gt;

And this line is only checking the detections in the first item in the batch:
&lt;denchmark-code&gt;for (size_t i = 0; i &lt; num_detections(0) &amp;&amp; i &lt; 20; ++i)
&lt;/denchmark-code&gt;

You need some sort of outer loop over the number of detections in each image. this is only looking at the first image, which made sense with the old batch size of 1.
		</comment>
		<comment id='13' author='spivakoa' date='2018-06-29T18:21:00Z'>
		I don't know of any examples, sorry. I believe the C++ API is only supported as "best effort", so we don't provide canonical examples like we do for Python.
		</comment>
		<comment id='14' author='spivakoa' date='2018-07-16T13:07:51Z'>
		&lt;denchmark-link:https://github.com/spivakoa&gt;@spivakoa&lt;/denchmark-link&gt;
 Try this modified code from tensorflow examples:
bool Run(const std::vector&lt;cv::Mat&gt; &amp;images, std::vector&lt;Result&gt; &amp;results) const {
    std::string inputLayer = "image_tensor:0";
    std::vector&lt;std::string&gt; outputLayer = {"detection_boxes:0", "detection_scores:0", "detection_classes:0",
                                            "num_detections:0"};

    int height = images.front().rows;
    int width = images.front().cols;
    int depth = 3;
    int batch = (int) images.size();
    tensorflow::Tensor input(tensorflow::DT_UINT8, tensorflow::TensorShape({batch, height, width, depth}));
    {
        std::uint8_t *dataPtr = input.flat&lt;std::uint8_t&gt;().data();
        for (const auto &amp;image : images) {
            cv::Mat tmp(height, width, CV_8UC3, dataPtr);
            image.convertTo(tmp, CV_8UC3);
            dataPtr += height * width * depth;
        }
    }

    std::vector&lt;tensorflow::Tensor&gt; outputs;
    tensorflow::Status runStatus = session_-&gt;Run({{inputLayer, input}}, outputLayer, {}, &amp;outputs);
    if (!runStatus.ok())
        return false;

    auto boxes = outputs[0].flat_outer_dims&lt;float, 3&gt;();
    auto scores = outputs[1].matrix&lt;float&gt;();
    auto classes = outputs[2].matrix&lt;float&gt;();
    auto num_detections = outputs[3].flat&lt;float&gt;();

    for (size_t b = 0; b &lt; batch; ++b) {
        for (size_t i = 0; i &lt; num_detections(b); ++i) {
            if (scores(b, i) &gt; kProbabilityThreshold) {
                Result result;
                result.Score = scores(b, i);
                result.ClassId = classes(b, i);
                result.Box = cv::Rect(
                        cv::Point(boxes(b, i, 1) * width, boxes(b, i, 0) * height),
                        cv::Point(boxes(b, i, 3) * width, boxes(b, i, 2) * height));
                results.push_back(result);
            }
        }
    }
    return true;
}
		</comment>
		<comment id='15' author='spivakoa' date='2018-08-15T13:05:42Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/MarkDaoust&gt;@MarkDaoust&lt;/denchmark-link&gt;
: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='16' author='spivakoa' date='2018-08-15T15:37:36Z'>
		&lt;denchmark-link:https://github.com/spivakoa&gt;@spivakoa&lt;/denchmark-link&gt;
 : So does &lt;denchmark-link:https://github.com/ilidar&gt;@ilidar&lt;/denchmark-link&gt;
's answer solve this problem?
&lt;denchmark-link:https://github.com/ilidar&gt;@ilidar&lt;/denchmark-link&gt;
 if this is the solution could you submit it as a PR to the file you based this on?
Thanks.
		</comment>
		<comment id='17' author='spivakoa' date='2018-08-24T09:33:10Z'>
		&lt;denchmark-link:https://github.com/spivakoa&gt;@spivakoa&lt;/denchmark-link&gt;
 I have the similar code about opencv and tensorflow c++ interface compile together in Ubuntu16.04, but I got many opencv error while compiling like this: /usr/include/opencv2/contrib/contrib.hpp:393:9: error: reference to 'int64' is ambiguous
my CMakeLists.txt defined like:
FILE(GLOB HEADER_FILES *.h)
find_package(OpenCV REQUIRED highgui imgproc)

include(CheckCXXCompilerFlag)
 CHECK_CXX_COMPILER_FLAG("-std=c++11" COMPILER_SUPPORTS_CXX11)
 CHECK_CXX_COMPILER_FLAG("-std=c++0x" COMPILER_SUPPORTS_CXX0X)
 if(COMPILER_SUPPORTS_CXX11)
   set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -std=c++11 -fPIC -O3")
 elseif(COMPILER_SUPPORTS_CXX0X)
   set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -std=c++0x")
 else()
   message(STATUS "The compiler ${CMAKE_CXX_COMPILER} has no C++11 support. Please use a different C++ compiler.")
 endif()

set(eigen_root_path ${PROJECT_SOURCE_DIR}/../../../software/eigen3)
set(tf_root_path ${PROJECT_SOURCE_DIR}/../../../tensorflow)
set(glog_root_path ${PROJECT_SOURCE_DIR}/../../../software/glog-0.3.5)
set(gtest_root_path ${PROJECT_SOURCE_DIR}/../../../software/gtest-1.8.0)

include_directories(
  ${gtest_root_path}/include
  ${glog_root_path}/include
  ${tf_root_path}/bazel-genfiles
  ${tf_root_path}/tensorflow/contrib/makefile/gen/protobuf/include
  ${tf_root_path}
  ${eigen_root_path}
  ${OpenCV_INCLUDE_DIRS}
)

link_directories(/usr/local/lib ${glog_root_path}/lib ${OpenCV_LIBRARY_DIRS})

add_library(tf_lib SHARED
  util/inifile.cc
  util/util.cc
  ${HEADER_FILES}
)

target_link_libraries(tf_lib glog tensorflow_cc tensorflow_framework ${OpenCV_LIBS})

I think these error come from tensorflow defined symbols, that opencv redefined, how can I fix it, can you help me?
		</comment>
		<comment id='18' author='spivakoa' date='2018-08-24T11:49:09Z'>
		&lt;denchmark-link:https://github.com/mrlittlepig&gt;@mrlittlepig&lt;/denchmark-link&gt;
 This error has nothing to do tensorflow. Your problem is inside the contrib library in opencv.
If you go to opencv2/contrib/contrib.hpp:393:9 I guess you will find some kind of definition like:
typedef int64_t int64;
It is conflicting with something else in your code. Try to check this.
		</comment>
		<comment id='19' author='spivakoa' date='2019-08-13T01:07:28Z'>
		&lt;denchmark-link:https://github.com/spivakoa&gt;@spivakoa&lt;/denchmark-link&gt;
 Hi,could you tell me the batch inference is slower or not than single inference? I test it &lt;denchmark-link:url&gt;https://stackoverflow.com/questions/57460782/batch-inference-is-as-slow-as-single-image-inference-in-tensorflow-c&lt;/denchmark-link&gt;
 and find the time is too long .
		</comment>
	</comments>
</bug>