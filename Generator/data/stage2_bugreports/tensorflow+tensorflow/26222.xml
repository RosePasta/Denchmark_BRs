<bug id='26222' author='Silb78dg' open_date='2019-02-28T19:29:08Z' closed_time='2020-01-02T23:17:57Z'>
	<summary>Estimator training hangs in multiple gpu if dataset doesn't have enough element to feed both gpus last batches</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
YES
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Distributed training (one node, Multiple GPUs)
TensorFlow installed from (source or binary):
PIP
TensorFlow version (use command below):
TF 1.12
Python version:
3.6.8
CUDA/cuDNN version:
9.0
GPU model and memory:
2 GTX1080 8Go

Describe the current behavior
Basically, if the dataset doesn't have enough elements to feed both gpus last batches the training hangs.

If you doesn't have enough to feed the first gpu last batch and don't want to drop the last batch then the training hangs.
If you doesn't have enough to feed the first gpu last batch and want to drop the last batch then you're fine.
If you have enough to feed the first gpu last batch but not the second gpu last batch and don't want to drop the last batch then the training hangs
If you have enough to feed the first gpu last batch but not the second gpu last batch and want to drop the last batch then the training hangs

Describe the expected behavior

If you doesn't have enough to feed the first gpu last batch and don't want to drop the last batch then run the first gpu partial batch and do nothing with the second gpu
If you doesn't have enough to feed the first gpu last batch and want to drop the last batch then drop the last batch for both gpus.
If you have enough to feed the first gpu last batch but not the second gpu last batch and don't want to drop the last batch then run the first gpu entire batch and run the second gpu partial batch.
If you have enough to feed the first gpu last batch but not the second gpu last batch and want to drop the last batch then run the first gpu entire batch and do nothing with the second gpu

Code to reproduce the issue
&lt;denchmark-code&gt;import tensorflow as tf

# Play with sample count (5, 6, 7) and drop_remainder (True, False) to reproduce the issue
sample_count = 5
drop_remainder = False


def run():
    # Config
    run_config = tf.estimator.RunConfig(
        session_config=tf.ConfigProto(allow_soft_placement=True),
        train_distribute=tf.contrib.distribute.MirroredStrategy(num_gpus=2))

    # Estimator
    estimator = tf.estimator.Estimator(model_fn=model_fn, config=run_config)
    estimator.train(train_input_fn)


# Times two dataset
def train_input_fn():
    return tf.data.Dataset \
        .range(sample_count) \
        .repeat(1) \
        .map(lambda x: (x, x * 2)) \
        .batch(2, drop_remainder)


# Times two model
def model_fn(features, labels, mode):
    input_layer = tf.cast(tf.reshape(features, [-1, 1]), tf.float32)
    expected_output = tf.cast(tf.reshape(labels, [-1, 1]), tf.float32)

    logit = tf.layers.dense(input_layer, 1, None, False)
    loss = tf.losses.mean_squared_error(expected_output, logit)

    logging_hook = tf.train.LoggingTensorHook(tensors={"feature_value": features.name}, every_n_iter=1)

    if mode == tf.estimator.ModeKeys.TRAIN:
        optimizer = tf.train.AdamOptimizer(0.001)
        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())
        return tf.estimator.EstimatorSpec(mode=mode,
                                          loss=loss,
                                          train_op=train_op,
                                          training_hooks=[logging_hook])


if __name__ == '__main__':
    tf.logging.set_verbosity(tf.logging.DEBUG)
    run()

&lt;/denchmark-code&gt;

Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
&lt;denchmark-code&gt;INFO:tensorflow:Initializing RunConfig with distribution strategies.
INFO:tensorflow:Not using Distribute Coordinator.
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp0ofz0qx1
INFO:tensorflow:Using config: {'_save_checkpoints_steps': None, '_device_fn': None, '_experimental_distribute': None, '_task_type': 'worker', '_tf_random_seed': None, '_keep_checkpoint_every_n_hours': 10000, '_distribute_coordinator_mode': None, '_service': None, '_save_summary_steps': 100, '_model_dir': '/tmp/tmp0ofz0qx1', '_master': '', '_keep_checkpoint_max': 5, '_train_distribute': &lt;tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x7feedf9f5dd8&gt;, '_protocol': None, '_task_id': 0, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
, '_is_chief': True, '_num_worker_replicas': 1, '_global_id_in_cluster': 0, '_evaluation_master': '', '_log_step_count_steps': 100, '_cluster_spec': &lt;tensorflow.python.training.server_lib.ClusterSpec object at 0x7feedf9f5e48&gt;, '_eval_distribute': None, '_num_ps_replicas': 0}
2019-02-28 11:18:22.645478: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-02-28 11:18:22.818624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-02-28 11:18:22.820057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.847
pciBusID: 0000:01:00.0
totalMemory: 7.90GiB freeMemory: 7.11GiB
2019-02-28 11:18:22.954140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-02-28 11:18:22.955822: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 1 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.847
pciBusID: 0000:02:00.0
totalMemory: 7.93GiB freeMemory: 7.81GiB
2019-02-28 11:18:22.957142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1
2019-02-28 11:18:23.349095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-02-28 11:18:23.349133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 
2019-02-28 11:18:23.349139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y 
2019-02-28 11:18:23.349143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N 
2019-02-28 11:18:23.349775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:0 with 6853 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
2019-02-28 11:18:23.350097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:1 with 7535 MB memory) -&gt; physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1)
INFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0
INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_GPU:0
INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_CPU:0
INFO:tensorflow:Configured nccl all-reduce.
2019-02-28 11:18:23.372783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1
2019-02-28 11:18:23.373002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-02-28 11:18:23.373032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 
2019-02-28 11:18:23.373038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y 
2019-02-28 11:18:23.373043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N 
2019-02-28 11:18:23.373272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6853 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
2019-02-28 11:18:23.373346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7535 MB memory) -&gt; physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1)
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:batch_all_reduce invoked for batches size = 1 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized.
2019-02-28 11:18:23.707824: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1
2019-02-28 11:18:23.707940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-02-28 11:18:23.707963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 
2019-02-28 11:18:23.707967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y 
2019-02-28 11:18:23.707988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N 
2019-02-28 11:18:23.708250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6853 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
2019-02-28 11:18:23.708475: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7535 MB memory) -&gt; physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1)
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp0ofz0qx1/model.ckpt.
INFO:tensorflow:loss = 47.902126, step = 0
INFO:tensorflow:feature_value = [2 3]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Silb78dg' date='2019-02-28T23:01:35Z'>
		&lt;denchmark-link:https://github.com/Silb78dg&gt;@Silb78dg&lt;/denchmark-link&gt;
 Could you check whether the bug persists with newer TF versions? Thanks!
		</comment>
		<comment id='2' author='Silb78dg' date='2019-03-01T06:45:41Z'>
		I encounter the same problem using tf1.12 and MirroredStratety. This can be solved by use batch after shuffle, not after repeat. However, I think this bug should be solved, because batch should be after repeat for data efficiency.
Successful code:
def input_fn(image_file, labels_file, batch_size, num_epoch):
    image_ds = tf.data.FixedLengthRecordDataset(
        image_file, 28*28, header_bytes=16)#.map(decode_image, num_parallel_calls=40)
    labels_ds = tf.data.FixedLengthRecordDataset(
        labels_file, 1, header_bytes=8)#.map(decode_label, num_parallel_calls=40)
    dataset = tf.data.Dataset.zip((image_ds, labels_ds))
    dataset = dataset.shuffle(buffer_size=10000)
    dataset = dataset.batch(batch_size, drop_remainder=True)
    dataset = dataset.repeat(num_epoch)
    dataset = dataset.prefetch(buffer_size=1)
    # for distributed strategy, it must return a tf.data.Dataset
    return dataset
Failed code
&lt;denchmark-code&gt;def input_fn(image_file, labels_file, batch_size, num_epoch):
    image_ds = tf.data.FixedLengthRecordDataset(
        image_file, 28*28, header_bytes=16)#.map(decode_image, num_parallel_calls=40)
    labels_ds = tf.data.FixedLengthRecordDataset(
        labels_file, 1, header_bytes=8)#.map(decode_label, num_parallel_calls=40)
    dataset = tf.data.Dataset.zip((image_ds, labels_ds))
    dataset = dataset.shuffle(buffer_size=10000)
    dataset = dataset.repeat(num_epoch)
    dataset = dataset.batch(batch_size, drop_remainder=True)
    dataset = dataset.prefetch(buffer_size=1)
    # for distributed strategy, it must return a tf.data.Dataset
    return dataset
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='Silb78dg' date='2019-03-08T00:52:06Z'>
		Hey &lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
, that's the issue we were discussing during tf dev summit.
Thanks for your help.
Denis.
		</comment>
		<comment id='4' author='Silb78dg' date='2019-03-08T04:41:54Z'>
		&lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
 - will you look into this issue?
		</comment>
		<comment id='5' author='Silb78dg' date='2019-03-08T20:47:57Z'>
		I believe that &lt;denchmark-link:https://github.com/rxsang&gt;@rxsang&lt;/denchmark-link&gt;
 is actively working on fixing this issue.
		</comment>
		<comment id='6' author='Silb78dg' date='2020-01-02T23:17:57Z'>
		&lt;denchmark-link:https://github.com/rxsang&gt;@rxsang&lt;/denchmark-link&gt;
 mentioned that this is fixed in TF 2 so closing the issue.
		</comment>
		<comment id='7' author='Silb78dg' date='2020-01-02T23:17:59Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26222&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26222&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>