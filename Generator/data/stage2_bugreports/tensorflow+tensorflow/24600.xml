<bug id='24600' author='COthello' open_date='2018-12-27T14:42:04Z' closed_time='2019-02-11T16:31:36Z'>
	<summary>cudnn gru has bug</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):
TensorFlow version (use command below):
Python version: 2.7.12
Bazel version (if compiling from source):0.19
GCC/Compiler version (if compiling from source):5.4.0
CUDA/cuDNN version:10.0
GPU model and memory:

You can collect some of this information using our environment capture &lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with
python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
Describe the current behavior
I write a unit test to compare tf.CudnnGRU and native gru but they got different result, i think it's cudnn's problem....becase i don't find any problem in my code....
Describe the expected behavior
should be same
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
import numpy as np
import tensorflow as tf
from tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops
S_MIN=-40
S_MAX=13
EXP_MAX=40
def native_gru(input, w):
&lt;denchmark-code&gt;    seq_len, batch_size, hidden_size=input.shape

    offset = 0
    wu = w[offset:offset + hidden_size*hidden_size].reshape(
            (hidden_size,hidden_size)).transpose()
    offset += hidden_size*hidden_size
    wr = w[offset:offset + hidden_size*hidden_size].reshape(
            (hidden_size,hidden_size)).transpose()
    offset += hidden_size*hidden_size
    wc = w[offset:offset + hidden_size*hidden_size].reshape(
            (hidden_size,hidden_size)).transpose()
    offset += hidden_size*hidden_size


    ru = w[offset:offset + hidden_size*hidden_size].reshape(
            (hidden_size,hidden_size)).transpose()
    offset += hidden_size*hidden_size
    rr = w[offset:offset + hidden_size*hidden_size].reshape(
            (hidden_size,hidden_size)).transpose()
    offset += hidden_size*hidden_size
    rc = w[offset:offset + hidden_size*hidden_size].reshape(
            (hidden_size,hidden_size)).transpose()
    offset += hidden_size*hidden_size



    bx_u = w[offset:offset + hidden_size]
    offset += hidden_size
    bx_r = w[offset:offset + hidden_size]
    offset += hidden_size
    bx_c = w[offset:offset + hidden_size]
    offset += hidden_size


    bh_u = w[offset:offset + hidden_size]
    offset += hidden_size
    bh_r = w[offset:offset + hidden_size]
    offset += hidden_size
    bh_c = w[offset:offset + hidden_size]
    offset += hidden_size

    def sigmod(x):
            y = np.copy(x)
            y[x &lt; S_MIN] = S_MIN
            y[x&gt;S_MAX] = S_MAX
            return 1./(1. + np.exp(-y))

    def tanh(x):
            y=-2.*x
            y[y&gt;EXP_MAX] = EXP_MAX
            return (2./(1.+np.exp(y))) - 1

    output = []
    pre_h = np.zeros((batch_size, hidden_size), dtype=input.dtype)

    for i in range(seq_len):
            emb_1 = input[i]
            update_gate = sigmod(np.matmul(emb_1, wu) + np.matmul(pre_h, ru)+bx_u+bh_u)
            reset_gate = sigmod(np.matmul(emb_1, wr)+np.matmul(pre_h, rr)+bx_r+bh_r)
            h_t_temp = tanh(np.matmul(emb_1, wc)+reset_gate*(np.matmul(pre_h,rc)+bh_c)+bx_c)
            new_h = update_gate*pre_h + (1-update_gate)*h_t_temp
            pre_h = new_h
            output.append(new_h)

    output = np.concatenate(output, -1)
    output = output.reshape((batch_size, -1, hidden_size))
    output = output.transpose((1,0,2))
    return output
&lt;/denchmark-code&gt;

def TestGRU():
num_steps = 2
batch_size = 1
hidden_size = 1
input_w_size = 6hidden_sizehidden_size + 6*hidden_size
x = np.random.uniform(low=-0.1, high=0.1, size=(num_steps, batch_size, hidden_size)).astype(np.float32)
#x = np.ones((num_steps, batch_size, hidden_size)).astype(np.float32)
&lt;denchmark-code&gt;    flat_w = np.random.uniform(low=-0.1, high=0.1, size=(input_w_size)).astype(np.float32)
    #flat_w = np.ones((input_w_size)).astype(np.float32)
    out = native_gru(x, flat_w)

    n_layer = 1
    rnn_cudnn = cudnn_rnn_ops.CudnnGRU(n_layer, hidden_size, hidden_size, 'linear_input')
    #param_cudnn = tf.Variable(tf.ones([rnn_cudnn.params_size()]), validate_shape=False)
    param_cudnn = tf.Variable(flat_w)

    y_cudnn, state_cudnn = rnn_cudnn(x,#tf.transpose(x, [1,0,2]),
    tf.zeros([n_layer, batch_size, hidden_size]), param_cudnn)
    print("native gru:"+str(out))
    with tf.Session() as sess:
            sess.run(tf.global_variables_initializer())
            print("cudnn gru:" +str(sess.run([y_cudnn])))
&lt;/denchmark-code&gt;

if name == 'main':
TestGRU()
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='COthello' date='2019-02-08T01:12:25Z'>
		Thank for reporting this, and sorry for the very late reply.
The cudnn gru kernel is provide by Nvidia, and we don't think it has a bug. tf.keras.layers.GRU is an equivalent implementation as the Cudnn GRU. They should generate same math result if the inputs and kernel weights are the same. Due to the limit of resource, we don't have enough time to debug your code. You can refer to tf.keras.layers.GRUCell for the details the math, and compare the intermediate results.
		</comment>
		<comment id='2' author='COthello' date='2019-02-12T14:33:23Z'>
		&lt;denchmark-link:https://github.com/COthello&gt;@COthello&lt;/denchmark-link&gt;
  can you  put all your code into a gist and share with me? I want to try it.
		</comment>
		<comment id='3' author='COthello' date='2019-03-29T15:10:42Z'>
		yes, it's not a bug, I misunderstand cudnn spec, sorry for the problem
		</comment>
	</comments>
</bug>