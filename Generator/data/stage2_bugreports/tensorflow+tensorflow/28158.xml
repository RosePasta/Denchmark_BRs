<bug id='28158' author='ipod825' open_date='2019-04-25T19:18:29Z' closed_time='2019-04-29T19:40:10Z'>
	<summary>Keras ValueError stops autograph building</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
TensorFlow installed from (source or binary):
pip
TensorFlow version (use command below):
2.0.0-dev20190424
Python version:
3.7.1
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
cudatoolkit-10.0.130-0
cudnn-7.3.1-cuda10.0_0
GPU model and memory:
GeForce RTX 2080 Ti

Describe the current behavior
Calling keras layer without calling build() automatically infers the shapes of the trainable variables. This works both in eager mode and graph mode in the current 2.0-alpha version. However, running the provided code in 2.0.0-dev20190424 version, it gives the following error message:
&lt;denchmark-code&gt;W0425 12:08:40.775576 139922429134656 tf_logging.py:161] Entity &lt;function update at 0x7f41dd7038c8&gt; could not be transform
ed and will be executed as-is. Some features (e.g. tensor-dependent conditionals and loops) may not work as expected. Erro
r details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY &gt;= 1. Please report this to the
AutoGraph team. Cause: ValueError during conversion: Weights for model sequential have not yet been created. Weights are c
reated when the Model is first called on inputs or `build()` is called with an `input_shape`.
&lt;/denchmark-code&gt;

Describe the expected behavior
Code to reproduce the issue
import os

import tensorflow as tf
from tensorflow.keras import layers, models, optimizers

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

model = models.Sequential([layers.Dense(1, activation='relu')])
optimizer = optimizers.SGD()

# Is this line needed in graph mode?
# model.build((None, 1))


@tf.function
def update(batch):
    with tf.GradientTape() as tape:
        output = model(batch)
    grads = tape.gradient(output, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))


if __name__ == "__main__":

    batch = tf.zeros((1, 1), dtype=tf.float32)
    update(batch)
	</description>
	<comments>
		<comment id='1' author='ipod825' date='2019-04-26T22:47:29Z'>
		I could reproduce the issue with tf-nightly. However, there is no error with TF2.0.0-alpha0. Thanks!
		</comment>
		<comment id='2' author='ipod825' date='2019-04-29T19:40:11Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=28158&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=28158&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='ipod825' date='2019-04-29T19:49:15Z'>
		Just submitted a fix that should allow your code to work without calling model.build.
		</comment>
	</comments>
</bug>