<bug id='8554' author='Panaetius' open_date='2017-03-20T10:41:44Z' closed_time='2017-06-16T21:17:25Z'>
	<summary>Floating Point Exception/SIGFPE in tf.map_fn over an empty tensor</summary>
	<description>
Working on a custom implementation of Faster-RCNN, it runs fine for ~150 - 300 batches but then I get a Floating Point Error, apparently in ConcatGPUImpl.
Source is here: &lt;denchmark-link:https://github.com/Panaetius/woipv&gt;https://github.com/Panaetius/woipv&lt;/denchmark-link&gt;
 (src/models/train_model.py and src/models/model.py, it's a bit of a mess still since it's a work in progress), reproducible as of commit 6eb1e3c5e818919b64a0a981abb98c2f3bc3dea1
&lt;denchmark-h:h3&gt;Environment info&lt;/denchmark-h&gt;

Operating System: Ubuntu 16.04
Installed version of CUDA and cuDNN: CUDA 8.0, cuDNN 5
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
-rw-r--r-- 1 root root   558720 Okt  4 23:15 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16 Okt  4 23:15 /usr/local/cuda/lib64/libcudart.so -&gt; libcudart.so.8.0
lrwxrwxrwx 1 root root       19 Okt  4 23:15 /usr/local/cuda/lib64/libcudart.so.8.0 -&gt; libcudart.so.8.0.44
-rwxr-xr-x 1 root root   415432 Okt  4 23:15 /usr/local/cuda/lib64/libcudart.so.8.0.44
-rw-r--r-- 1 root root   775162 Okt  4 23:15 /usr/local/cuda/lib64/libcudart_static.a
lrwxrwxrwx 1 root root       13 Okt  4 23:23 /usr/local/cuda/lib64/libcudnn.so -&gt; libcudnn.so.5
lrwxrwxrwx 1 root root       17 Okt  4 23:23 /usr/local/cuda/lib64/libcudnn.so.5 -&gt; libcudnn.so.5.1.5
-rwxr-xr-x 1 root root 79337624 Okt  4 23:23 /usr/local/cuda/lib64/libcudnn.so.5.1.5
-rw-r--r-- 1 root root 69756172 Okt  4 23:23 /usr/local/cuda/lib64/libcudnn_static.a
If installed from binary pip package, provide:

A link to the pip package you installed: official python 3 tensorflow-gpu package
The output from python -c "import tensorflow; print(tensorflow.__version__)":

I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
1.0.0
&lt;denchmark-h:h3&gt;Logs or other output that would be helpful&lt;/denchmark-h&gt;

gdb:
Thread 49 "python" received signal SIGFPE, Arithmetic exception.
[Switching to Thread 0x7fff10bb8700 (LWP 8227)]
0x00007fffcaab91de in void tensorflow::ConcatGPUImpl&lt;float, int&gt;(Eigen::GpuDevice const&amp;, tensorflow::CudaDeviceArrayStruct&lt;float const*, 8&gt; const&amp;, tensorflow::CudaDeviceArrayStruct&lt;int, 8&gt; const&amp;, bool, int, tensorflow::TTypes&lt;float, 2, long&gt;::Matrix*) ()
from /home/zenon/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so
Backtrace:
#0  0x00007fffcaab91de in void tensorflow::ConcatGPUImpl&lt;float, int&gt;(Eigen::GpuDevice const&amp;, tensorflow::CudaDeviceArrayStruct&lt;float const*, 8&gt; const&amp;, tensorflow::CudaDeviceArrayStruct&lt;int, 8&gt; const&amp;, bool, int, tensorflow::TTypes&lt;float, 2, long&gt;::Matrix*) () from /home/zenon/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/1&gt;#1&lt;/denchmark-link&gt;
  0x00007fffcaaaf45c in void tensorflow::(anonymous namespace)::ConcatGPUCall&lt;float, int&gt;(tensorflow::OpKernelContext*, std::vector&lt;std::unique_ptr&lt;tensorflow::TTypes&lt;float, 2, long&gt;::ConstMatrix, std::default_delete&lt;tensorflow::TTypes&lt;float, 2, long&gt;::ConstMatrix&gt; &gt;, std::allocator&lt;std::unique_ptr&lt;tensorflow::TTypes&lt;float, 2, long&gt;::ConstMatrix, std::default_delete&lt;tensorflow::TTypes&lt;float, 2, long&gt;::ConstMatrix&gt; &gt; &gt; &gt; const&amp;, tensorflow::TTypes&lt;float, 2, long&gt;::Tensor*) ()
from /home/zenon/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/2&gt;#2&lt;/denchmark-link&gt;
  0x00007fffc9b14c16 in tensorflow::TensorArrayPackOrGatherOp&lt;Eigen::GpuDevice, float, false&gt;::Compute(tensorflow::OpKernelContext*) () from /home/zenon/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/3&gt;#3&lt;/denchmark-link&gt;
  0x00007fffcad155b2 in tensorflow::BaseGPUDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) ()
from /home/zenon/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/4&gt;#4&lt;/denchmark-link&gt;
  0x00007fffcad56183 in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) ()
from /home/zenon/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/5&gt;#5&lt;/denchmark-link&gt;
  0x00007fffcad569fa in std::_Function_handler&lt;void (), tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady(tensorflow::gtl::InlinedVector&lt;tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, 8&gt; const&amp;, tensorflow::(anonymous namespace)::ExecutorState::TaggedNodeReadyQueue*)::{lambda()&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/1&gt;#1&lt;/denchmark-link&gt;
}&gt;::_M_invoke(std::_Any_data const&amp;) ()
from /home/zenon/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/6&gt;#6&lt;/denchmark-link&gt;
  0x00007fffcb09a960 in std::_Function_handler&lt;void (), Eigen::NonBlockingThreadPoolTempltensorflow::EigenEnvironment::NonBlockingThreadPoolTempl(int, tensorflow::EigenEnvironment)::{lambda()&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/1&gt;#1&lt;/denchmark-link&gt;
}&gt;::_M_invoke(std::_Any_data const&amp;) ()
from /home/zenon/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/7&gt;#7&lt;/denchmark-link&gt;
  0x00007fffcb099c10 in std::_Function_handler&lt;void (), tensorflow::EigenEnvironment::CreateThread(std::function&lt;void ()&gt;)::{lambda()&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/1&gt;#1&lt;/denchmark-link&gt;
}&gt;::_M_invoke(std::_Any_data const&amp;) ()
from /home/zenon/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so
---Type  to continue, or q  to quit---
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/8&gt;#8&lt;/denchmark-link&gt;
  0x00007fffc7f2c260 in ?? () from /home/zenon/anaconda3/envs/tensorflow/bin/../lib/libstdc++.so.6
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/9&gt;#9&lt;/denchmark-link&gt;
  0x00007ffff76d16fa in start_thread (arg=0x7fff10bb8700) at pthread_create.c:333
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/10&gt;#10&lt;/denchmark-link&gt;
 0x00007ffff6aefb5d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109
	</description>
	<comments>
		<comment id='1' author='Panaetius' date='2017-03-21T22:03:49Z'>
		Upon researching this issue some more (with the good old "Sprinkle Prints everywhere" method of debugging), it seems to happen when the input labels/regions are empty. The MSCOCO dataset apparently has images with no labels, so those would cause the issue. I'm reading the labels/regions with a tf.VarLengthFeature, so the dimensions aren't known ahead of time.
It's somewhere inside one of the two tf.map_fn() applications, going by the gdb stacktrace, though have yet to narrow it down precisely to write a simple test case
		</comment>
		<comment id='2' author='Panaetius' date='2017-03-21T22:17:44Z'>
		Ok I managed to reproduce it in a simple test case: &lt;denchmark-link:https://gist.github.com/Panaetius/8da26064491ab4ad1890bca3dfd86eff&gt;https://gist.github.com/Panaetius/8da26064491ab4ad1890bca3dfd86eff&lt;/denchmark-link&gt;

The test case can probably be made smaller still (for instance, the whole save data -&gt; load data with tf.VarLenFeature is probably not needed, you could just construct an empty SparseTensor, though I couldn't get this to work when I gave it a quick try).
Basically, calling tf.map_fn with an empty tensor ( [ ] ) leads to the SIGFPE, most likely when the x[0] is done in the lambda.
I'll change the title of the issue to reflect that this is an issue with tf.map_fn
Of course I'll have to change my code to correctly deal with empty/missing labels, so I doubt this will be an issue for me in the future, though tf.map_fn actually trying to run over an empty tensor and then just straight crashing seems like a bug that should be fixed to me
		</comment>
		<comment id='3' author='Panaetius' date='2017-03-28T17:47:09Z'>
		&lt;denchmark-link:https://github.com/yuanbyu&gt;@yuanbyu&lt;/denchmark-link&gt;
, do you think there is a way we could make this failure case in tf.map_fn more intuitive to detect?
		</comment>
		<comment id='4' author='Panaetius' date='2017-06-16T21:17:25Z'>
		Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
	</comments>
</bug>