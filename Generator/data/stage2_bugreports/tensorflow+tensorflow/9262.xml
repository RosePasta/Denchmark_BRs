<bug id='9262' author='StarvingMarvin' open_date='2017-04-17T08:28:29Z' closed_time='2017-04-18T16:11:00Z'>
	<summary>Estimator numpy_input_fn doesn't work when reading input with pytables</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Relatively straightforward attempt at using estimator flow with input_fn
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Gentoo Linux
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): b'v1.0.0-2171-gbaa85cb' 1.0.1
Bazel version (if compiling from source): 0.4.4-
CUDA/cuDNN version: 8.0.61
GPU model and memory: GTX 970 4GB
Exact command to reproduce:

Other stuff: Python 3.4, pytables 3.3.0, numpy 1.12.1, hdf5-1.8.18
&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

I have an issue when reading multidimensional arrays from hdf5 file using pytables. I can fix the issue for myself locally by changing _OrderedDictNumpyFeedFn class (line 173 in my version) from
 column[integer_indexes]
to
np.take(column, integer_indexes, axis=0)
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

Relevant bit of code:
def make_input_fn(data, batch_size):
    input_features = {node.name: node for node in data.get_node('/x')}
    ys = data.get_node('/y')
    return numpy_input_fn(input_features, ys, batch_size=batch_size, num_epochs=5, shuffle=False, num_threads=1)

def main(unused_argv):
    model_fn = make_train_model(feat)
    config = learn.RunConfig(save_checkpoints_secs=60)
    est = learn.Estimator(
        model_fn=model_fn, model_dir="data/tf/try1", config=config
    )
    training_data = tables.open_file('data/hdf5/training.h5')
    train_in = make_input_fn(training_data, 256)
    est.fit(
        input_fn=train_in,
        steps=100
    )
    training_data.close()
Error I get:
&lt;denchmark-code&gt;INFO:tensorflow:Using config: {'_is_chief': True, '_keep_checkpoint_max': 5, '_save_checkpoints_secs': 60, '_environment': 'local', '_num_ps_replicas': 0, '_master': '', '_task_type': None, '_tf_config': gpu_options {
  per_process_gpu_memory_fraction: 1.0
}
, '_num_worker_replicas': 0, '_cluster_spec': &lt;tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff377eaf550&gt;, '_keep_checkpoint_every_n_hours': 10000, '_save_checkpoints_steps': None, '_evaluation_master': '', '_model_dir': None, '_save_summary_steps': 100, '_tf_random_seed': None, '_task_id': 0}
INFO:tensorflow:Create CheckpointSaverHook.
2017-04-17 10:05:38.231511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-04-17 10:05:38.231863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: GeForce GTX 970
major: 5 minor: 2 memoryClockRate (GHz) 1.4305
pciBusID 0000:01:00.0
Total memory: 3.94GiB
Free memory: 3.45GiB
2017-04-17 10:05:38.232016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-04-17 10:05:38.232048: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-04-17 10:05:38.232081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0)
INFO:tensorflow:Restoring parameters from data/tf/try1/model.ckpt-4
INFO:tensorflow:Error reported to Coordinator: &lt;class 'ValueError'&gt;, operands could not be broadcast together with shapes (256,) (4,) 
INFO:tensorflow:Saving checkpoints for 4 into data/tf/try1/model.ckpt.
Traceback (most recent call last):
  File ".../venv/lib/python3.4/site-packages/tables/array.py", line 651, in __getitem__
    startl, stopl, stepl, shape = self._interpret_indexing(key)
  File ".../venv/lib/python3.4/site-packages/tables/array.py", line 408, in _interpret_indexing
    raise TypeError("Non-valid index or slice: %s" % key)
TypeError: Non-valid index or slice: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib64/python3.4/runpy.py", line 170, in _run_module_as_main
    "__main__", mod_spec)
  File "/usr/lib64/python3.4/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/zfs_data/Sources/betahex/betahex/training/supervised.py", line 106, in &lt;module&gt;
    tf.app.run()
  File ".../venv/lib/python3.4/site-packages/tensorflow/python/platform/app.py", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File "/zfs_data/Sources/betahex/betahex/training/supervised.py", line 99, in main
    steps=50
  File ".../venv/lib/python3.4/site-packages/tensorflow/python/util/deprecation.py", line 281, in new_func
    return func(*args, **kwargs)
  File ".../venvs/default/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py", line 429, in fit
    loss = self._train_model(input_fn=input_fn, hooks=hooks)
  File ".../venv/lib/python3.4/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py", line 977, in _train_model
    _, loss = mon_sess.run([model_fn_ops.train_op, model_fn_ops.loss])
  File .../venv/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py", line 500, in __exit__
    self._close_internal(exception_type)
  File ".../venv/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py", line 535, in _close_internal
    self._sess.close()
  File ".../venv/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py", line 769, in close
    self._sess.close()
  File ".../venv/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py", line 866, in close
    ignore_live_threads=True)
  File ".../venv/lib/python3.4/site-packages/tensorflow/python/training/coordinator.py", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File "/usr/lib/python3.4/site-packages/six.py", line 686, in reraise
    raise value
  File ".../venv/lib/python3.4/site-packages/tensorflow/python/estimator/inputs/queues/feeding_queue_runner.py", line 93, in _run
    feed_dict = None if feed_fn is None else feed_fn()
  File ".../venv/lib/python3.4/site-packages/tensorflow/python/estimator/inputs/queues/feeding_functions.py", line 175, in __call__
    for column in self._ordered_dict_of_arrays.values()
  File ".../venv/lib/python3.4/site-packages/tensorflow/python/estimator/inputs/queues/feeding_functions.py", line 175, in &lt;listcomp&gt;
    for column in self._ordered_dict_of_arrays.values()
  File ".../venv/lib/python3.4/site-packages/tables/array.py", line 656, in __getitem__
    coords = self._point_selection(key)
  File ".../venv/lib/python3.4/site-packages/tables/leaf.py", line 561, in _point_selection
    coords[idx] = (coords + self.shape)[idx]
ValueError: operands could not be broadcast together with shapes (256,) (4,) 
Closing remaining open files:data/hdf5/training.h5...done
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='StarvingMarvin' date='2017-04-17T16:58:29Z'>
		&lt;denchmark-link:https://github.com/sguada&gt;@sguada&lt;/denchmark-link&gt;
 is that a scenario that is supposed to work?
		</comment>
		<comment id='2' author='StarvingMarvin' date='2017-04-17T17:46:19Z'>
		Not sure better ask &lt;denchmark-link:https://github.com/martinwicke&gt;@martinwicke&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='StarvingMarvin' date='2017-04-17T20:26:03Z'>
		Interesting, your np.take expression ought to be equivalent to the fancy indexing, but maybe pytables doesn't fully support the fancy expression. Do you want to send a PR? Looks like yours is strictly more robust.
		</comment>
		<comment id='4' author='StarvingMarvin' date='2017-04-18T01:05:50Z'>
		np.take converts its input to a numpy array, so of course it works. So would np.asarray(column)[integer_indexes].
Instead, it would probably make sense for numpy_input_fn to either coerce each value to a numpy array with np.asarray, or raise an error if any inputs aren't numpy arrays.
		</comment>
		<comment id='5' author='StarvingMarvin' date='2017-04-18T05:33:24Z'>
		Ah, right. coercing each input to be actual numpy arrays makes most sense.
		</comment>
		<comment id='6' author='StarvingMarvin' date='2017-04-18T12:56:19Z'>
		Thank you all for your comments, they made me explore this issue a bit further, and get to the following conclusions:

As @shoyer observed, np.take() works because numpy does np.asaray(), but that means that entire array is loaded into memory.
pytables does support "fancy" indexing, but arity must still match with array's dimensions
but they do support ellipsis, so data[list,...] actually works.

At this point I feel this should be treated as an issue with pytables. Doing np.asarray(column) would actually be a regression in my case, because it would slow things down due to copying (things are slow with the fix I mentioned in the issue, and I now have an idea why). Changing tensorflow code to add ellipsis when reading a batch from column is what I'll do locally, to see if thing are gonna speed up, but I feel it's too specific workaround to be merged upstream.
		</comment>
		<comment id='7' author='StarvingMarvin' date='2017-04-18T16:10:59Z'>
		Thanks &lt;denchmark-link:https://github.com/StarvingMarvin&gt;@StarvingMarvin&lt;/denchmark-link&gt;
. I will close this issue. If you determine that the ellipsis works without negative side effects, feel free to send a PR.
		</comment>
	</comments>
</bug>