<bug id='45038' author='Tessil' open_date='2020-11-20T10:40:01Z' closed_time='2021-01-06T10:57:50Z'>
	<summary>[TFLite] Experimental new quantizer calculates the wrong scaling when the input range doesn't include 0</summary>
	<description>
Hello,
When creating a quantized network with the new experimental quantizer it seems there is a problem with the scaling of the nodes when the input range provided by the representative dataset doesn't include 0 (e.g. [2.3, 5.4] or [-1.2, -0.4] range).
The small example below creates a network with a single multiply operator and the representative dataset provides a [1.3, 1.4] input range. The expected result, and the one provided with the stable quantizer, of the 1.4*1.4 multiplication is 1.96 but the returned result is 0.00952942.
The new experimental quantizer seems to calculate the input scaling of the multiply node as (1.4 - 1.3)/255 with a -128 zero-point instead of (1.4 - 0.0)/255 with the same zero-point as done by the stable quantizer. Changing the input range of the representative dataset to [0.0, 1.4] solves the problem.
I know the new quantizer is experimental so it isn't too much of a problem yet but it's mainly to report the problem if it isn't already known.
Tested version: tf-nightly-2.5.0.dev20201119
import numpy as np
import tensorflow as tf


input = tf.keras.Input(shape=(1))
output = tf.keras.layers.Multiply()([input, input])
model = tf.keras.Model(inputs=input, outputs=output)


def representative_data_gen():
    yield [np.array([[1.3]], dtype=np.float32)]
    yield [np.array([[1.4]], dtype=np.float32)]
    # It works with this line if _experimental_new_quantizer=True
    # yield [np.array([[0.0]], dtype=np.float32)]


converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.representative_dataset = representative_data_gen
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter._experimental_new_quantizer = True

tflite_model = converter.convert()

interpreter = tf.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors()

input = np.array([[1.4]], dtype=np.float32)
interpreter.set_tensor(interpreter.get_input_details()[0]["index"], input)
interpreter.invoke()
output = interpreter.get_tensor(interpreter.get_output_details()[0]["index"])

print("input ", input)
print("output ", output)
	</description>
	<comments>
		<comment id='1' author='Tessil' date='2020-12-04T07:41:34Z'>
		I have tried in colab with TF nightly version() and was able to reproduce the issue. Please, find the gist &lt;denchmark-link:https://colab.research.google.com/gist/ravikyram/508c562adc928bf62d651853fdfe8184/untitled566.ipynb&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='2' author='Tessil' date='2020-12-10T20:56:06Z'>
		&lt;denchmark-link:https://github.com/liufengdb&gt;@liufengdb&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/daverim&gt;@daverim&lt;/denchmark-link&gt;
 can you take a look?
		</comment>
		<comment id='3' author='Tessil' date='2021-01-06T10:57:50Z'>
		It seems the problem was solved by commit &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/05303d0dcee5c415c2e39233b023f660914b5b8c&gt;05303d0&lt;/denchmark-link&gt;
, I will thus close the issue. Thanks &lt;denchmark-link:https://github.com/liufengdb&gt;@liufengdb&lt;/denchmark-link&gt;
!
		</comment>
		<comment id='4' author='Tessil' date='2021-01-06T10:57:52Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45038&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45038&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>