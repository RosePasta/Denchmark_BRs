<bug id='40504' author='manmay-nakhashi' open_date='2020-06-16T09:45:07Z' closed_time='2020-07-03T06:06:50Z'>
	<summary>RuntimeError: tensorflow/lite/kernels/range.cc:39 (start &amp;gt; limit &amp;&amp; delta &amp;lt; 0) || (start &amp;lt; limit &amp;&amp; delta &amp;gt; 0) was not true.Node number 3 (RANGE) failed   to invoke. Node number 393 (WHILE) failed to invoke. current error :RuntimeError: tensorflow/lite/kernels/reshape.cc:55 stretch_dim != -1 (0 != -1)Node number 83 (RESHAPE) failed to prepare.</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
TensorFlow installed from (source or binary):
TensorFlow version (or github SHA if from source):

Command used to run the converter or code if youâ€™re using the Python API
&lt;denchmark-code&gt;import numpy as np
import tensorflow as tf
from tensorflow_tts.processor import LJSpeechProcessor

# Load the TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_path="fastspeech.tflite")
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
print("input_details: ", input_details)
print("output_details", output_details)

print(input_details[0])
print(input_details[1])
print(input_details[2])
# fastspeech inference
attention_mask = interpreter.tensor(interpreter.get_input_details()[0]["index"])()
speaker_id = interpreter.tensor(interpreter.get_input_details()[1]["index"])()
input_id = interpreter.tensor(interpreter.get_input_details()[2]["index"])()

input_id = tf.convert_to_tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]], tf.int32)
attention_mask = tf.convert_to_tensor([[True, True, True, True, True, True, True, True, True, True]], tf.bool)
speaker_id = tf.convert_to_tensor([0], tf.int32)

#out_p = interpreter.tensor(interpreter.get_output_details()[0]["index"])
interpreter.invoke()
interpreter.invoke()
interpreter.invoke()
print("done")
# The function `get_tensor()` returns a copy of the tensor data.
# Use `tensor()` in order to get a pointer to the tensor.
masked_mel_before = interpreter.get_tensor(output_details[2]['index'])
print(masked_mel_before)

&lt;/denchmark-code&gt;

The output from the converter invocation
&lt;denchmark-code&gt;input_details:  [{'name': 'attention_mask', 'index': 0, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([ 1, 10], dtype=int32), 'dtype': &lt;class 'numpy.bool_'&gt;, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'input_ids', 'index': 1, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([ 1, 10], dtype=int32), 'dtype': &lt;class 'numpy.int32'&gt;, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'speaker_ids', 'index': 2, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': &lt;class 'numpy.int32'&gt;, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
output_details [{'name': 'Identity', 'index': 585, 'shape': array([], dtype=int32), 'shape_signature': array([], dtype=int32), 'dtype': &lt;class 'numpy.float32'&gt;, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'Identity_1', 'index': 621, 'shape': array([], dtype=int32), 'shape_signature': array([], dtype=int32), 'dtype': &lt;class 'numpy.float32'&gt;, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'Identity_2', 'index': 537, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([ 1, 10], dtype=int32), 'dtype': &lt;class 'numpy.float32'&gt;, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
{'name': 'attention_mask', 'index': 0, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([ 1, 10], dtype=int32), 'dtype': &lt;class 'numpy.bool_'&gt;, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}
{'name': 'input_ids', 'index': 1, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([ 1, 10], dtype=int32), 'dtype': &lt;class 'numpy.int32'&gt;, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}
{'name': 'speaker_ids', 'index': 2, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': &lt;class 'numpy.int32'&gt;, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}
2020-06-16 06:20:21.460754: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2020-06-16 06:20:21.460788: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)
2020-06-16 06:20:21.460819: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist
2020-06-16 06:20:21.461131: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-06-16 06:20:21.468935: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2593990000 Hz
2020-06-16 06:20:21.469795: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f0cfc000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-06-16 06:20:21.469821: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
Traceback (most recent call last):
  File "test_tflite.py", line 28, in &lt;module&gt;
    interpreter.invoke()
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py", line 511, in invoke
    self._interpreter.Invoke()
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py", line 113, in Invoke
    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_Invoke(self)
RuntimeError: tensorflow/lite/kernels/range.cc:39 (start &gt; limit &amp;&amp; delta &lt; 0) || (start &lt; limit &amp;&amp; delta &gt; 0) was not true.Node number 3 (RANGE) failed to invoke.
Node number 393 (WHILE) failed to invoke.

&lt;/denchmark-code&gt;

Also, please include a link to the saved model or GraphDef
&lt;denchmark-code&gt;saved_model
https://drive.google.com/file/d/136KmfVwBT2htxPDZeYw4-TXmxPYe7Vsa/view?usp=sharing
fastspeech.tflite
https://drive.google.com/file/d/1QYyc5cUZbmbv7SwQMDTdM622ZiCFG2cp/view?usp=sharing
&lt;/denchmark-code&gt;

Failure details
conversion is successful, but there is runtime error,
state what is wrong:
interpreter.invoke() failing
Any other info / logs
pb conversion
&lt;denchmark-code&gt;import yaml
import numpy as np
import matplotlib.pyplot as plt

import yaml
import numpy as np
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow_tts.configs import FastSpeechConfig
from tensorflow_tts.models import TFFastSpeech

with open('examples/fastspeech/conf/fastspeech.v3.yaml') as f:
    config = yaml.load(f, Loader=yaml.Loader)

config = FastSpeechConfig(**config["fastspeech_params"])

fastspeech = TFFastSpeech(config=config, name="fastspeech")
fastspeech._build()

fastspeech.load_weights("examples/fastspeech/pretrained/model-150000.h5",by_name=True, skip_mismatch=True)
tf.saved_model.save(fastspeech, "./test_saved")
&lt;/denchmark-code&gt;

fastspeech model code
&lt;denchmark-code&gt;import numpy as np
import tensorflow as tf


def get_initializer(initializer_range=0.02):
    """Creates a `tf.initializers.truncated_normal` with the given range.

    Args:
        initializer_range: float, initializer range for stddev.

    Returns:
        TruncatedNormal initializer with stddev = `initializer_range`.

    """
    return tf.keras.initializers.TruncatedNormal(stddev=initializer_range)


def gelu(x):
    """Gaussian Error Linear unit."""
    cdf = 0.5 * (1.0 + tf.math.erf(x / tf.math.sqrt(2.0)))
    return x * cdf


def gelu_new(x):
    """Smoother gaussian Error Linear Unit."""
    cdf = 0.5 * (1.0 + tf.tanh((np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))
    return x * cdf


def swish(x):
    """Swish activation function."""
    return x * tf.sigmoid(x)


def mish(x):
    return x * tf.math.tanh(tf.math.softplus(x))


ACT2FN = {
    "identity": tf.keras.layers.Activation('linear'),
    "tanh": tf.keras.layers.Activation('tanh'),
    "gelu": tf.keras.layers.Activation(gelu),
    "relu": tf.keras.activations.relu,
    "swish": tf.keras.layers.Activation(swish),
    "gelu_new": tf.keras.layers.Activation(gelu_new),
    "mish": tf.keras.layers.Activation(mish)
}


class TFFastSpeechEmbeddings(tf.keras.layers.Layer):
    """Construct charactor/phoneme/positional/speaker embeddings."""

    def __init__(self, config, **kwargs):
        """Init variables."""
        super().__init__(**kwargs)
        self.vocab_size = config.vocab_size
        self.hidden_size = config.hidden_size
        self.initializer_range = config.initializer_range
        self.config = config

        self.position_embeddings = tf.keras.layers.Embedding(
            config.max_position_embeddings + 1,
            config.hidden_size,
            weights=[self._sincos_embedding()],
            name="position_embeddings",
            trainable=False,
        )

        if config.n_speakers &gt; 1:
            self.encoder_speaker_embeddings = tf.keras.layers.Embedding(
                config.n_speakers,
                config.hidden_size,
                embeddings_initializer=get_initializer(self.initializer_range),
                name="speaker_embeddings"
            )
            self.speaker_fc = tf.keras.layers.Dense(units=config.hidden_size, name='speaker_fc') 
    def build(self, input_shape):
        """Build shared charactor/phoneme embedding layers."""
        with tf.name_scope("charactor_embeddings"):
            self.charactor_embeddings = self.add_weight(
                "weight",
                shape=[self.vocab_size, self.hidden_size],
                initializer=get_initializer(self.initializer_range),
            )
        super().build(input_shape)
    #@tf.function(experimental_relax_shapes=True)
    def call(self, inputs, training=False):
        """Get charactor embeddings of inputs.

        Args:
            1. charactor, Tensor (int32) shape [batch_size, length].
            2. speaker_id, Tensor (int32) shape [batch_size]
        Returns:
            Tensor (float32) shape [batch_size, length, embedding_size].

        """
        return self._embedding(inputs, training=training)
    def _embedding(self, inputs, training=False):
        """Applies embedding based on inputs tensor."""
        input_ids, speaker_ids = inputs

        input_shape = tf.shape(input_ids)
        seq_length = input_shape[1]

        position_ids = tf.range(1, seq_length + 1, dtype=tf.int32)[tf.newaxis, :]

        # create embeddings
        inputs_embeds = tf.gather(self.charactor_embeddings, input_ids)
        position_embeddings = self.position_embeddings(position_ids)

        # sum embedding
        embeddings = inputs_embeds + position_embeddings
        if self.config.n_speakers &gt; 1:
            speaker_embeddings = self.encoder_speaker_embeddings(speaker_ids)
            speaker_features = tf.math.softplus(self.speaker_fc(speaker_embeddings))
            # extended speaker embeddings
            extended_speaker_features = speaker_features[:, tf.newaxis, :]
            embeddings += extended_speaker_features

        return embeddings
    def _sincos_embedding(self):
        position_enc = np.array([
            [pos / np.power(10000, 2.0 * (i // 2) / self.hidden_size) for i in range(self.hidden_size)]
            for pos in range(self.config.max_position_embeddings + 1)
        ])

        position_enc[:, 0::2] = np.sin(position_enc[:, 0::2])
        position_enc[:, 1::2] = np.cos(position_enc[:, 1::2])

        # pad embedding.
        position_enc[0] = 0.0

        return position_enc


class TFFastSpeechSelfAttention(tf.keras.layers.Layer):
    """Self attention module for fastspeech."""

    def __init__(self, config, **kwargs):
        """Init variables."""
        super().__init__(**kwargs)
        if config.hidden_size % config.num_attention_heads != 0:
            raise ValueError(
                "The hidden size (%d) is not a multiple of the number of attention "
                "heads (%d)" % (config.hidden_size, config.num_attention_heads)
            )
        self.output_attentions = config.output_attentions
        self.num_attention_heads = config.num_attention_heads
        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)
        self.all_head_size = self.num_attention_heads * self.attention_head_size

        self.query = tf.keras.layers.Dense(
            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name="query"
        )
        self.key = tf.keras.layers.Dense(
            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name="key"
        )
        self.value = tf.keras.layers.Dense(
            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name="value"
        )

        self.dropout = tf.keras.layers.Dropout(config.attention_probs_dropout_prob)

    def transpose_for_scores(self, x, batch_size):
        """Transpose to calculate attention scores."""
        x = tf.reshape(x, (batch_size, -1, self.num_attention_heads, self.attention_head_size))
        return tf.transpose(x, perm=[0, 2, 1, 3])

    @tf.function(experimental_relax_shapes=True)
    def __call__(self, inputs, training=False):
        """Call logic."""
        hidden_states, attention_mask = inputs

        batch_size = tf.shape(hidden_states)[0]
        mixed_query_layer = self.query(hidden_states)
        mixed_key_layer = self.key(hidden_states)
        mixed_value_layer = self.value(hidden_states)

        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)
        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)
        value_layer = self.transpose_for_scores(mixed_value_layer, batch_size)

        attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)
        dk = tf.cast(tf.shape(key_layer)[-1], tf.float32)  # scale attention_scores
        attention_scores = attention_scores / tf.math.sqrt(dk)

        if attention_mask is not None:
            # extended_attention_masks for self attention encoder.
            extended_attention_mask = attention_mask[:, tf.newaxis, tf.newaxis, :]
            extended_attention_mask = tf.cast(extended_attention_mask, tf.float32)
            extended_attention_mask = (1.0 - extended_attention_mask) * -1e9
            attention_scores = attention_scores + extended_attention_mask

        # Normalize the attention scores to probabilities.
        attention_probs = tf.nn.softmax(attention_scores, axis=-1)
        attention_probs = self.dropout(attention_probs, training=training)

        context_layer = tf.matmul(attention_probs, value_layer)
        context_layer = tf.transpose(context_layer, perm=[0, 2, 1, 3])
        context_layer = tf.reshape(
            context_layer, (batch_size, -1, self.all_head_size)
        )

        outputs = (context_layer, attention_probs) if self.output_attentions else (context_layer,)
        return outputs


class TFFastSpeechSelfOutput(tf.keras.layers.Layer):
    """Fastspeech output of self attention module."""

    def __init__(self, config, **kwargs):
        """Init variables."""
        super().__init__(**kwargs)
        self.dense = tf.keras.layers.Dense(
            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name="dense"
        )
        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name="LayerNorm")
        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)

    @tf.function(experimental_relax_shapes=True)
    def __call__(self, inputs, training=False):
        """Call logic."""
        hidden_states, input_tensor = inputs

        hidden_states = self.dense(hidden_states)
        hidden_states = self.dropout(hidden_states, training=training)
        hidden_states = self.LayerNorm(hidden_states + input_tensor)
        return hidden_states


class TFFastSpeechAttention(tf.keras.layers.Layer):
    """Fastspeech attention module."""

    def __init__(self, config, **kwargs):
        """Init variables."""
        super().__init__(**kwargs)
        self.self_attention = TFFastSpeechSelfAttention(config, name="self")
        self.dense_output = TFFastSpeechSelfOutput(config, name="output")
    @tf.function(experimental_relax_shapes=True)
    def __call__(self, inputs, training=False):
        input_tensor, attention_mask = inputs

        self_outputs = self.self_attention([input_tensor, attention_mask], training=training)
        attention_output = self.dense_output([self_outputs[0], input_tensor], training=training)
        masked_attention_output = attention_output * tf.cast(tf.expand_dims(attention_mask, 2), dtype=tf.float32)
        outputs = (masked_attention_output,) + self_outputs[1:]  # add attentions if we output them
        return outputs


class TFFastSpeechIntermediate(tf.keras.layers.Layer):
    """Intermediate representation module."""

    def __init__(self, config, **kwargs):
        """Init variables."""
        super().__init__(**kwargs)
        self.conv1d_1 = tf.keras.layers.Conv1D(
            config.intermediate_size,
            kernel_size=config.intermediate_kernel_size,
            kernel_initializer=get_initializer(config.initializer_range),
            padding='same',
            name="conv1d_1"
        )
        self.conv1d_2 = tf.keras.layers.Conv1D(
            config.hidden_size,
            kernel_size=config.intermediate_kernel_size,
            kernel_initializer=get_initializer(config.initializer_range),
            padding='same',
            name="conv1d_2"
        )
        if isinstance(config.hidden_act, str):
            self.intermediate_act_fn = ACT2FN[config.hidden_act]
        else:
            self.intermediate_act_fn = config.hidden_act

    @tf.function(experimental_relax_shapes=True)
    def __call__(self, inputs, training=False):
        """Call logic."""
        hidden_states, attention_mask = inputs

        hidden_states = self.conv1d_1(hidden_states)
        hidden_states = self.intermediate_act_fn(hidden_states)
        hidden_states = self.conv1d_2(hidden_states)

        masked_hidden_states = hidden_states * tf.cast(tf.expand_dims(attention_mask, 2), dtype=tf.float32)
        return masked_hidden_states


class TFFastSpeechOutput(tf.keras.layers.Layer):
    """Output module."""

    def __init__(self, config, **kwargs):
        """Init variables."""
        super().__init__(**kwargs)
        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name="LayerNorm")
        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)
    @tf.function(experimental_relax_shapes=True)
    def __call__(self, inputs, training=False):
        """Call logic."""
        hidden_states, input_tensor = inputs

        hidden_states = self.dropout(hidden_states, training=training)
        hidden_states = self.LayerNorm(hidden_states + input_tensor)
        return hidden_states


class TFFastSpeechLayer(tf.keras.layers.Layer):
    """Fastspeech module (FFT module on the paper)."""

    def __init__(self, config, **kwargs):
        """Init variables."""
        super().__init__(**kwargs)
        self.attention = TFFastSpeechAttention(config, name="attention")
        self.intermediate = TFFastSpeechIntermediate(config, name="intermediate")
        self.bert_output = TFFastSpeechOutput(config, name="output")

    @tf.function(experimental_relax_shapes=True)
    def __call__(self, inputs, training=False):
        """Call logic."""
        hidden_states, attention_mask = inputs

        attention_outputs = self.attention([hidden_states, attention_mask], training=training)
        attention_output = attention_outputs[0]
        intermediate_output = self.intermediate([attention_output, attention_mask], training=training)
        layer_output = self.bert_output([intermediate_output, attention_output], training=training)
        masked_layer_output = layer_output * tf.cast(tf.expand_dims(attention_mask, 2), dtype=tf.float32)
        outputs = (masked_layer_output,) + attention_outputs[1:]  # add attentions if we output them
        return outputs


class TFFastSpeechEncoder(tf.keras.layers.Layer):
    """Fast Speech encoder module."""

    def __init__(self, config, **kwargs):
        """Init variables."""
        super().__init__(**kwargs)
        self.output_attentions = config.output_attentions
        self.output_hidden_states = config.output_hidden_states
        self.layer = [TFFastSpeechLayer(config, name="layer_._{}".format(i)) for i in range(config.num_hidden_layers)]
    
    @tf.function(experimental_relax_shapes=True)
    def __call__(self, inputs, training=False):
        """Call logic."""
        hidden_states, attention_mask = inputs

        all_hidden_states = ()
        all_attentions = ()
        for _, layer_module in enumerate(self.layer):
            if self.output_hidden_states:
                all_hidden_states = all_hidden_states + (hidden_states,)

            layer_outputs = layer_module([hidden_states, attention_mask], training=training)
            hidden_states = layer_outputs[0]

            if self.output_attentions:
                all_attentions = all_attentions + (layer_outputs[1],)

        # Add last layer
        if self.output_hidden_states:
            all_hidden_states = all_hidden_states + (hidden_states,)

        outputs = (hidden_states,)
        if self.output_hidden_states:
            outputs = outputs + (all_hidden_states,)
        if self.output_attentions:
            outputs = outputs + (all_attentions,)
        return outputs  # outputs, (hidden states), (attentions)


class TFFastSpeechDecoder(TFFastSpeechEncoder):
    """Fast Speech decoder module."""

    def __init__(self, config, **kwargs):
        super().__init__(config, **kwargs)
        self.config = config

        # create decoder positional embedding
        self.decoder_positional_embeddings = tf.keras.layers.Embedding(
            config.max_position_embeddings + 1,
            config.hidden_size,
            weights=[self._sincos_embedding()],
            name="position_embeddings",
            trainable=False
        )

        if config.n_speakers &gt; 1:
            self.decoder_speaker_embeddings = tf.keras.layers.Embedding(
                config.n_speakers,
                config.hidden_size,
                embeddings_initializer=get_initializer(config.initializer_range),
                name="speaker_embeddings"
            )
            self.speaker_fc = tf.keras.layers.Dense(units=config.hidden_size, name='speaker_fc')
    @tf.function(experimental_relax_shapes=True)
    def __call__(self, inputs, training=False):
        hidden_states, speaker_ids, encoder_mask, decoder_pos = inputs

        # calculate new hidden states.
        hidden_states = hidden_states + self.decoder_positional_embeddings(decoder_pos)

        if self.config.n_speakers &gt; 1:
            speaker_embeddings = self.decoder_speaker_embeddings(speaker_ids)
            speaker_features = tf.math.softplus(self.speaker_fc(speaker_embeddings))
            # extended speaker embeddings
            extended_speaker_features = speaker_features[:, tf.newaxis, :]
            hidden_states += extended_speaker_features

        return super().call([hidden_states, encoder_mask], training=training)

    def _sincos_embedding(self):
        position_enc = np.array([
            [pos / np.power(10000, 2.0 * (i // 2) / self.config.hidden_size) for i in range(self.config.hidden_size)]
            for pos in range(self.config.max_position_embeddings + 1)
        ])

        position_enc[:, 0::2] = np.sin(position_enc[:, 0::2])
        position_enc[:, 1::2] = np.cos(position_enc[:, 1::2])

        # pad embedding.
        position_enc[0] = 0.0

        return position_enc


class TFTacotronPostnet(tf.keras.layers.Layer):
    """Tacotron-2 postnet."""

    def __init__(self, config, **kwargs):
        """Init variables."""
        super().__init__(**kwargs)
        self.conv_batch_norm = []
        for i in range(config.n_conv_postnet):
            conv = tf.keras.layers.Conv1D(
                filters=config.postnet_conv_filters if i &lt; config.n_conv_postnet - 1 else config.num_mels,
                kernel_size=config.postnet_conv_kernel_sizes,
                padding='same',
                name='conv_._{}'.format(i)
            )
            batch_norm = tf.keras.layers.BatchNormalization(name='batch_norm_._{}'.format(i))
            self.conv_batch_norm.append((conv, batch_norm))
        self.dropout = tf.keras.layers.Dropout(rate=config.postnet_dropout_rate, name='dropout')
        self.activation = [tf.nn.tanh] * (config.n_conv_postnet - 1) + [tf.identity]

    @tf.function(experimental_relax_shapes=True)
    def __call__(self, inputs, training=False):
        """Call logic."""
        outputs, mask = inputs
        extended_mask = tf.cast(tf.expand_dims(mask, axis=2), tf.float32)
        for i, (conv, bn) in enumerate(self.conv_batch_norm):
            outputs = conv(outputs)
            outputs = bn(outputs)
            outputs = self.activation[i](outputs)
            outputs = self.dropout(outputs, training=training)
        return outputs * extended_mask


class TFFastSpeechDurationPredictor(tf.keras.layers.Layer):
    """FastSpeech duration predictor module."""

    def __init__(self, config, **kwargs):
        """Init variables."""
        super().__init__(**kwargs)
        self.conv_layers = []
        for i in range(config.num_duration_conv_layers):
            self.conv_layers.append(
                tf.keras.layers.Conv1D(
                    config.duration_predictor_filters,
                    config.duration_predictor_kernel_sizes,
                    padding='same',
                    name='conv_._{}'.format(i)
                )
            )
            self.conv_layers.append(
                tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name="LayerNorm_._{}".format(i))
            )
            self.conv_layers.append(
                tf.keras.layers.Activation(tf.nn.relu6)
            )
            self.conv_layers.append(
                tf.keras.layers.Dropout(config.duration_predictor_dropout_probs)
            )
        self.conv_layers_sequence = tf.keras.Sequential(self.conv_layers)
        self.output_layer = tf.keras.layers.Dense(1)

    @tf.function(experimental_relax_shapes=True)
    def __call__(self, inputs, training=False):
        """Call logic."""
        encoder_hidden_states, attention_mask = inputs
        attention_mask = tf.cast(tf.expand_dims(attention_mask, 2), tf.float32)

        # mask encoder hidden states
        masked_encoder_hidden_states = encoder_hidden_states * attention_mask

        # pass though first layer
        outputs = self.conv_layers_sequence(masked_encoder_hidden_states)
        outputs = self.output_layer(outputs)
        masked_outputs = outputs * attention_mask
        return tf.squeeze(tf.nn.relu6(masked_outputs), -1)  # make sure positive value.


class TFFastSpeechLengthRegulator(tf.keras.layers.Layer):
    """FastSpeech lengthregulator module."""

    def __init__(self, config, **kwargs):
        """Init variables."""
        super().__init__(**kwargs)
        self.config = config
    @tf.function(experimental_relax_shapes=True)
    def __call__(self, inputs, training=False):
        """Call logic.

        Args:
            1. encoder_hidden_states, Tensor (float32) shape [batch_size, length, hidden_size]
            2. durations_gt, Tensor (float32/int32) shape [batch_size, length]
        """
        encoder_hidden_states, durations_gt = inputs
        outputs, encoder_masks = self._length_regulator(encoder_hidden_states, durations_gt)
        return outputs, encoder_masks

    def _length_regulator(self, encoder_hidden_states, durations_gt):
        """Length regulator logic."""
        sum_durations = tf.reduce_sum(durations_gt, axis=-1)  # [batch_size]
        max_durations = tf.reduce_max(sum_durations)

        input_shape = tf.shape(encoder_hidden_states)
        batch_size = input_shape[0]
        hidden_size = input_shape[-1]

        # initialize output hidden states and encoder masking.
        outputs = tf.zeros(shape=[0, max_durations, hidden_size], dtype=tf.float32)
        encoder_masks = tf.zeros(shape=[0, max_durations], dtype=tf.int32)

        def condition(i,
                      batch_size,
                      outputs,
                      encoder_masks,
                      encoder_hidden_states,
                      durations_gt,
                      max_durations):
            return tf.less(i, batch_size)

        def body(i,
                 batch_size,
                 outputs,
                 encoder_masks,
                 encoder_hidden_states,
                 durations_gt,
                 max_durations):
            repeats = durations_gt[i]
            real_length = tf.reduce_sum(repeats)
            pad_size = max_durations - real_length
            masks = tf.sequence_mask([real_length], max_durations, dtype=tf.int32)
            repeat_encoder_hidden_states = tf.repeat(
                encoder_hidden_states[i],
                repeats=repeats,
                axis=0
            )
            repeat_encoder_hidden_states = tf.expand_dims(
                tf.pad(
                    repeat_encoder_hidden_states, [[0, pad_size], [0, 0]]
                ),
                0)  # [1, max_durations, hidden_size]
            outputs = tf.concat([outputs, repeat_encoder_hidden_states], axis=0)
            encoder_masks = tf.concat([encoder_masks, masks], axis=0)
            return [i + 1, batch_size, outputs, encoder_masks,
                    encoder_hidden_states, durations_gt, max_durations]

        # initialize iteration i.
        i = tf.constant(0, dtype=tf.int32)
        _, _, outputs, encoder_masks, _, _, _, = tf.while_loop(
            condition,
            body,
            [i, batch_size, outputs, encoder_masks, encoder_hidden_states, durations_gt, max_durations],
            shape_invariants=[i.get_shape(),
                              batch_size.get_shape(),
                              tf.TensorShape([None, None, self.config.hidden_size]),
                              tf.TensorShape([None, None]),
                              encoder_hidden_states.get_shape(),
                              durations_gt.get_shape(),
                              max_durations.get_shape()]
        )

        return outputs, encoder_masks


class TFFastSpeech(tf.keras.Model):
    """TF Fastspeech module."""

    def __init__(self, config, **kwargs):
        """Init layers for fastspeech."""
        super().__init__(**kwargs)
        self.embeddings = TFFastSpeechEmbeddings(config, name='embeddings')
        self.encoder = TFFastSpeechEncoder(config, name='encoder')
        self.duration_predictor = TFFastSpeechDurationPredictor(config, name='duration_predictor')
        self.length_regulator = TFFastSpeechLengthRegulator(config, name='length_regulator')
        self.decoder = TFFastSpeechDecoder(config, name='decoder')
        self.mel_dense = tf.keras.layers.Dense(units=config.num_mels, name='mel_before')
        self.postnet = TFTacotronPostnet(config=config, name='postnet')

    def _build(self):
        """Dummy input for building model."""
        # fake inputs
        input_ids = tf.convert_to_tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]], tf.int32)
        attention_mask = tf.convert_to_tensor([[True, True, True, True, True, True, True, True, True, True]], tf.bool)
        speaker_ids = tf.convert_to_tensor([0], tf.int32)
        self(input_ids, attention_mask, speaker_ids)

    @tf.function(experimental_relax_shapes=True,
                 input_signature=[tf.TensorSpec(shape=[None, 10], dtype=tf.int32),
                                  tf.TensorSpec(shape=[None, 10], dtype=tf.bool),
                                  tf.TensorSpec(shape=[None, ], dtype=tf.int32)])
    def __call__(self,
             input_ids,
             attention_mask,
             speaker_ids,
             training=False):
        """Call logic."""
        embedding_output = self.embeddings([input_ids, speaker_ids], training=training)
        encoder_output = self.encoder([embedding_output, attention_mask], training=training)
        last_encoder_hidden_states = encoder_output[0]

        # duration predictor, here use last_encoder_hidden_states, u can use more hidden_states layers
        # rather than just use last_hidden_states of encoder for duration_predictor.
        duration_outputs = self.duration_predictor([last_encoder_hidden_states, attention_mask])  # [batch_size, length]
        speed_ratios = tf.convert_to_tensor(np.array([1.0]), dtype=tf.float32)

        duration_gts = tf.cast(tf.math.round(duration_outputs), tf.int32)

        length_regulator_outputs, encoder_masks = self.length_regulator([
            last_encoder_hidden_states, duration_gts], training=training)

        # create decoder positional embedding
        decoder_pos = tf.range(1, tf.shape(length_regulator_outputs)[1] + 1, dtype=tf.int32)
        masked_decoder_pos = tf.expand_dims(decoder_pos, 0) * encoder_masks

        decoder_output = self.decoder(
            [length_regulator_outputs, speaker_ids, encoder_masks, masked_decoder_pos], training=training)
        last_decoder_hidden_states = decoder_output[0]

        # here u can use sum or concat more than 1 hidden states layers from decoder.
        mel_before = self.mel_dense(last_decoder_hidden_states)
        mel_after = self.postnet([mel_before, encoder_masks], training=training) + mel_before
        outputs = (mel_before, mel_after, duration_outputs)
        #model10 = keras.models.Model(inputs=[input_ids,attention_mask,speaker_ids], output=outputs)
        return outputs

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='manmay-nakhashi' date='2020-06-16T12:56:58Z'>
		&lt;denchmark-link:https://github.com/manmay-nakhashi&gt;@manmay-nakhashi&lt;/denchmark-link&gt;
 could you confirm that when you feed the exact same input data to the TF graph, it generates a correct result?
		</comment>
		<comment id='2' author='manmay-nakhashi' date='2020-06-16T14:47:59Z'>
		&lt;denchmark-link:https://github.com/manmay-nakhashi&gt;@manmay-nakhashi&lt;/denchmark-link&gt;
 I can reproduce your errors. However, the model is kind of complicated one. I think the problematic portion is . Could you help creating a small tflite that reproduces the same error and only contains the above keras layer? That
will make us easier to debug.
		</comment>
		<comment id='3' author='manmay-nakhashi' date='2020-06-16T15:49:53Z'>
		&lt;denchmark-link:https://github.com/abattery&gt;@abattery&lt;/denchmark-link&gt;
 i am able to load and generate correct result in pb file but i have to modify a code little bit, if with same code i am trying to convert it to tflite then i am getting this error
&lt;denchmark-code&gt;traceback (most recent call last):
  File "load_model.py", line 12, in &lt;module&gt;
    tflite_model = converter.convert()
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py", line 483, in convert
    _get_tensor_name(tensor), shape_list))
ValueError: None is only supported in the 1st dimension. Tensor 'attention_mask' has invalid shape '[None, None]'.
&lt;/denchmark-code&gt;

if i am giving fixed length dimensions  while creating model , in signature then i am getting this error
which is from TFFastSpeechEmbeddings layer
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "load_model.py", line 11, in &lt;module&gt;
    tflite_model = converter.convert()
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py", line 518, in convert
    **converter_kwargs)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py", line 496, in toco_convert_impl
    enable_mlir_converter=enable_mlir_converter)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py", line 227, in toco_convert_protos
    raise ConverterError("See console for info.\n%s\n%s\n" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: See console for info.
2020-06-16 15:46:16.544808: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:144] Ignored output_format.
2020-06-16 15:46:16.544861: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:147] Ignored drop_control_dependency.
Traceback (most recent call last):
  File "/usr/local/bin/toco_from_protos", line 8, in &lt;module&gt;
    sys.exit(main())
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py", line 93, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File "/usr/local/lib/python3.6/dist-packages/absl/app.py", line 299, in run
    _run_main(main, args)
  File "/usr/local/lib/python3.6/dist-packages/absl/app.py", line 250, in _run_main
    sys.exit(main(argv))
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py", line 56, in execute
    enable_mlir_converter)
Exception: Failed to find function '__inference___call___372'. The imported TensorFlow GraphDef is ill-formed.
&lt;/denchmark-code&gt;

so i commented @tf.functions in embedding and i am getting above error
		</comment>
		<comment id='4' author='manmay-nakhashi' date='2020-06-16T15:51:11Z'>
		
@manmay-nakhashi I can reproduce your errors. However, the model is kind of complicated one. I think the problematic portion is TFFastSpeechLengthRegulator. Could you help creating a small tflite that reproduces the same error and only contains the above keras layer? That
will make us easier to debug.

i'll do that
		</comment>
		<comment id='5' author='manmay-nakhashi' date='2020-06-16T22:19:24Z'>
		&lt;denchmark-link:https://github.com/manmay-nakhashi&gt;@manmay-nakhashi&lt;/denchmark-link&gt;
 Thanks for your support :)
		</comment>
		<comment id='6' author='manmay-nakhashi' date='2020-06-18T12:53:14Z'>
		&lt;denchmark-link:https://github.com/abattery&gt;@abattery&lt;/denchmark-link&gt;
   getting this error while converting from TFFastSpeechLengthRegulator  as you suspected  it is able to convert in tf.while_cond and and while_body after that it is crashing
&lt;denchmark-code&gt;2020-06-18 12:36:46.320955: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 1031 nodes (-666), 1893 edges (-909), time = 731.028ms.
2020-06-18 12:36:46.320968: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 1031 nodes (0), 1893 edges (0), time = 60.799ms.
2020-06-18 12:36:46.320978: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: __inference_while_body_3091_3082_frozen
2020-06-18 12:36:46.320988: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 117 nodes (5), 129 edges (12), time = 3.444ms.
2020-06-18 12:36:46.320997: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 117 nodes (0), 129 edges (0), time = 1.798ms.
2020-06-18 12:36:46.321006: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: __inference_while_cond_3090_2763_frozen
2020-06-18 12:36:46.321016: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 12 nodes (0), 4 edges (0), time = 0.241ms.
2020-06-18 12:36:46.321025: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 12 nodes (0), 4 edges (0), time = 0.138ms.
Traceback (most recent call last):
  File "load_model.py", line 11, in &lt;module&gt;
    tflite_model = converter.convert()
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py", line 518, in convert
    **converter_kwargs)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py", line 496, in toco_convert_impl
    enable_mlir_converter=enable_mlir_converter)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py", line 227, in toco_convert_protos
    raise ConverterError("See console for info.\n%s\n%s\n" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: See console for info.
2020-06-18 12:36:48.585792: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:144] Ignored output_format.
2020-06-18 12:36:48.585849: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:147] Ignored drop_control_dependency.
Traceback (most recent call last):
  File "/usr/local/bin/toco_from_protos", line 8, in &lt;module&gt;
    sys.exit(main())
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py", line 93, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File "/usr/local/lib/python3.6/dist-packages/absl/app.py", line 299, in run
    _run_main(main, args)
  File "/usr/local/lib/python3.6/dist-packages/absl/app.py", line 250, in _run_main
    sys.exit(main(argv))
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py", line 56, in execute
    enable_mlir_converter)
Exception: Failed to find function '__inference___call___8538'. The imported TensorFlow GraphDef is ill-formed
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='manmay-nakhashi' date='2020-06-18T13:02:27Z'>
		&lt;denchmark-link:https://github.com/manmay-nakhashi&gt;@manmay-nakhashi&lt;/denchmark-link&gt;
 Can you share the converted tflite that contains only TFFastSpeechLengthRegulator?
		</comment>
		<comment id='8' author='manmay-nakhashi' date='2020-06-18T13:04:49Z'>
		If the conversion is halted, can you share a minimal reproducible step?
		</comment>
		<comment id='9' author='manmay-nakhashi' date='2020-06-18T13:20:48Z'>
		
@manmay-nakhashi Can you share the converted tflite that contains only TFFastSpeechLengthRegulator?

ok
		</comment>
		<comment id='10' author='manmay-nakhashi' date='2020-06-18T15:21:46Z'>
		&lt;denchmark-link:https://github.com/abattery&gt;@abattery&lt;/denchmark-link&gt;
  length_regulator.py , i have changes output to return single element just to make it simple, i am not able to build model with input signature because of tf.while_loop
&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np
from tensorflow_tts.configs import FastSpeechConfig
import yaml
class TFFastSpeechLengthRegulator(tf.keras.layers.Layer):
    """FastSpeech lengthregulator module."""

    def __init__(self, config, **kwargs):
        """Init variables."""
        super().__init__(**kwargs)
        self.config = config
    @tf.function(experimental_relax_shapes=True)
    def __call__(self, inputs, training=False):
        """Call logic.
        Args:
            1. encoder_hidden_states, Tensor (float32) shape [batch_size, length, hidden_size]
            2. durations_gt, Tensor (float32/int32) shape [batch_size, length]
        """
        encoder_hidden_states, durations_gt = inputs
        """Length regulator logic."""
        sum_durations = tf.reduce_sum(durations_gt, axis=-1)  # [batch_size]
        max_durations = tf.reduce_max(sum_durations)

        input_shape = tf.shape(encoder_hidden_states)
        batch_size = input_shape[0]
        hidden_size = input_shape[-1]

        # initialize output hidden states and encoder masking.
        outputs = tf.zeros(shape=[0, max_durations, hidden_size], dtype=tf.float32)
        encoder_masks = tf.zeros(shape=[0, max_durations], dtype=tf.int32)

        def condition(i,
                      batch_size,
                      outputs,
                      encoder_masks,
                      encoder_hidden_states,
                      durations_gt,
                      max_durations):
            return tf.less(i, batch_size)

        def body(i,
                 batch_size,
                 outputs,
                 encoder_masks,
                 encoder_hidden_states,
                 durations_gt,
                 max_durations):
            repeats = durations_gt[i]
            real_length = tf.reduce_sum(repeats)
            pad_size = max_durations - real_length
            masks = tf.sequence_mask([real_length], max_durations, dtype=tf.int32)
            repeat_encoder_hidden_states = tf.repeat(
                encoder_hidden_states[i],
                repeats=repeats,
                axis=0
            )
            repeat_encoder_hidden_states = tf.expand_dims(
                tf.pad(
                    repeat_encoder_hidden_states, [[0, pad_size], [0, 0]]
                ),
                0)  # [1, max_durations, hidden_size]
            outputs = tf.concat([outputs, repeat_encoder_hidden_states], axis=0)
            encoder_masks = tf.concat([encoder_masks, masks], axis=0)
            return [i + 1, batch_size, outputs, encoder_masks,
                    encoder_hidden_states, durations_gt, max_durations]

        # initialize iteration i.
        i = tf.constant(0, dtype=tf.int32)
        _, _, outputs, encoder_masks, _, _, _, = tf.while_loop(
            condition,
            body,
            [i, batch_size, outputs, encoder_masks, encoder_hidden_states, durations_gt, max_durations],
            shape_invariants=[i.get_shape(),
                              batch_size.get_shape(),
                              tf.TensorShape([None, None, self.config.hidden_size]),
                              tf.TensorShape([None, None]),
                              encoder_hidden_states.get_shape(),
                              durations_gt.get_shape(),
                              max_durations.get_shape()]
        )

        return encoder_masks

#last_encoder_hidden_states = np.array(np.random.random_sample((16,10,384)), dtype=np.float32) #tf.keras.Input((None,10,64))
#duration_gts = np.array(np.random.random_sample((16,10)), dtype=np.int32) #tf.keras.Input((None,10), dtype=tf.int32)
with open('examples/fastspeech/conf/fastspeech.v3.yaml') as f:
    config = yaml.load(f, Loader=yaml.Loader)
config = FastSpeechConfig(**config["fastspeech_params"])

model = tf.keras.Sequential()
model.add(TFFastSpeechLengthRegulator(config, name='fastspeech_length_regulator'))

tf.saved_model.save(model, "./test_lr")
&lt;/denchmark-code&gt;

		</comment>
		<comment id='11' author='manmay-nakhashi' date='2020-06-18T15:24:44Z'>
		change FastSpeechConfig and fastspeech.v3.yaml accordingly
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/4799490/supported_files.zip&gt;supported_files.zip&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='manmay-nakhashi' date='2020-06-18T23:31:20Z'>
		&lt;denchmark-link:https://github.com/manmay-nakhashi&gt;@manmay-nakhashi&lt;/denchmark-link&gt;
 Sorry, again. Could you share the saved_model in the test_lr directory to me? :)
		</comment>
		<comment id='13' author='manmay-nakhashi' date='2020-06-19T05:07:50Z'>
		&lt;denchmark-link:https://github.com/abattery&gt;@abattery&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/4802539/test_lr.zip&gt;test_lr.zip&lt;/denchmark-link&gt;

		</comment>
		<comment id='14' author='manmay-nakhashi' date='2020-06-19T06:34:05Z'>
		Could you try again with the following code snippet?
&lt;denchmark-code&gt;  interpreter = tf.lite.Interpreter(model_path="fastspeech.tflite")
  interpreter.allocate_tensors()
  input_details = interpreter.get_input_details()
  output_details = interpreter.get_output_details()

  test_attention_mask = np.array([[True, True, True, True, True, True, True, True, True, True]], dtype=np.bool)
  test_input_id = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]], dtype=np.int32)
  test_speaker_id = np.array([0], dtype=np.int32)

  interpreter.set_tensor(input_details[0]['index'], test_attention_mask)
  interpreter.set_tensor(input_details[1]['index'], test_input_id)
  interpreter.set_tensor(input_details[2]['index'], test_speaker_id)

  interpreter.invoke()
&lt;/denchmark-code&gt;

		</comment>
		<comment id='15' author='manmay-nakhashi' date='2020-06-19T07:32:09Z'>
		
Could you try again with the following code snippet?
  interpreter = tf.lite.Interpreter(model_path="fastspeech.tflite")
  interpreter.allocate_tensors()
  input_details = interpreter.get_input_details()
  output_details = interpreter.get_output_details()

  test_attention_mask = np.array([[True, True, True, True, True, True, True, True, True, True]], dtype=np.bool)
  test_input_id = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]], dtype=np.int32)
  test_speaker_id = np.array([0], dtype=np.int32)

  interpreter.set_tensor(input_details[0]['index'], test_attention_mask)
  interpreter.set_tensor(input_details[1]['index'], test_input_id)
  interpreter.set_tensor(input_details[2]['index'], test_speaker_id)

  interpreter.invoke()


&lt;denchmark-link:https://github.com/abattery&gt;@abattery&lt;/denchmark-link&gt;
  Segmentation fault (core dumped) without any error
		</comment>
		<comment id='16' author='manmay-nakhashi' date='2020-07-01T14:39:51Z'>
		any update on this ?
		</comment>
		<comment id='17' author='manmay-nakhashi' date='2020-07-03T05:55:26Z'>
		Hi, I am Jae from TensorFlow Lite team, jaesung's colleague. We investigated your FastSpeech model and conclude that we need some modifications on your model. Jaesung and I could convert the modified version of FastSpeech with tf-nightly version.
The first error like [None, None] is not supported will be gone in the tf-nightly. However, we saw a little subtle problem inside the model. The preferred remedy is to fix the batch_size = 1 during inference. By setting [1, None] the error will be gone.
In addition, the real problem was the input tensor_spec mismatch inside of tf.while_loop(). the 4th and 5th input tensors are growing (outputs, encoder_masks). this kind of dynamic size (not dynamic shape, please be careful of words. they are different) is not well recommended in TFLite converter. So we also fixed the size. The details can be found in the PR that I will open soon.
		</comment>
		<comment id='18' author='manmay-nakhashi' date='2020-07-03T06:06:49Z'>
		Since the model conversion is done, and the changes are pushed into the new PRs by request from the owner of TensorflowTTS, I close this issue and leave the link to the PR.
&lt;denchmark-link:https://github.com/TensorSpeech/TensorFlowTTS/pull/84&gt;TensorSpeech/TensorFlowTTS#84&lt;/denchmark-link&gt;

		</comment>
		<comment id='19' author='manmay-nakhashi' date='2020-07-03T06:46:45Z'>
		&lt;denchmark-link:https://github.com/jaeyoo&gt;@jaeyoo&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/abattery&gt;@abattery&lt;/denchmark-link&gt;
 thank you so much for the support.
		</comment>
		<comment id='20' author='manmay-nakhashi' date='2020-08-10T16:59:35Z'>
		&lt;denchmark-link:https://github.com/jaeyoo&gt;@jaeyoo&lt;/denchmark-link&gt;
  android build is crashing in latest nightly build
&lt;denchmark-code&gt;W/System.err: java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/reshape.cc:55 stretch_dim != -1 (0 != -1)
    Node number 83 (RESHAPE) failed to prepare.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='21' author='manmay-nakhashi' date='2020-08-12T07:00:46Z'>
		&lt;denchmark-link:https://github.com/manmay-nakhashi&gt;@manmay-nakhashi&lt;/denchmark-link&gt;
 Could you print the shapes related to Reshape?
Reshape can only stretch one dimension, Looks like you have two dimensions = -1.
		</comment>
		<comment id='22' author='manmay-nakhashi' date='2020-08-12T08:24:44Z'>
		&lt;denchmark-link:https://github.com/thaink&gt;@thaink&lt;/denchmark-link&gt;
 sure
		</comment>
		<comment id='23' author='manmay-nakhashi' date='2020-09-17T06:38:12Z'>
		
@dathudeptrai .pb or you could save it to a saved_model dir

it's here (&lt;denchmark-link:https://drive.google.com/file/d/1wxxgOQ5u4vL3XZQYRGUlN_Fb5DlJYZHc/view?usp=sharing&gt;https://drive.google.com/file/d/1wxxgOQ5u4vL3XZQYRGUlN_Fb5DlJYZHc/view?usp=sharing&lt;/denchmark-link&gt;
)
		</comment>
		<comment id='24' author='manmay-nakhashi' date='2020-09-21T12:00:56Z'>
		&lt;denchmark-link:https://github.com/dathudeptrai&gt;@dathudeptrai&lt;/denchmark-link&gt;
 I got an error while trying to convert it.
google3.third_party.tensorflow.lite.python.convert.ConverterError: :0: error: loc("length_regulator/while@__inference__inference_6859"): 'tf.While' op body result type tensor&lt;1x?x?xf32&gt; is incompatible with result type tensor&lt;0x?x?xf32&gt; at index 4
Are you able to converting with the saved model?
		</comment>
		<comment id='25' author='manmay-nakhashi' date='2020-09-22T09:34:19Z'>
		&lt;denchmark-link:https://github.com/thaink&gt;@thaink&lt;/denchmark-link&gt;
 this is my minimal code to reproduce the bug (&lt;denchmark-link:https://colab.research.google.com/drive/1IJbr7Nu7cSAmEJivApR5w_ymvnsR5_YH?usp=sharing&gt;https://colab.research.google.com/drive/1IJbr7Nu7cSAmEJivApR5w_ymvnsR5_YH?usp=sharing&lt;/denchmark-link&gt;
). The bug is because conv1d
		</comment>
		<comment id='26' author='manmay-nakhashi' date='2020-09-22T11:50:21Z'>
		
@thaink this is my minimal code to reproduce the bug (https://colab.research.google.com/drive/1IJbr7Nu7cSAmEJivApR5w_ymvnsR5_YH?usp=sharing). The bug is because conv1d

then how to fix that?
any workaround?
		</comment>
		<comment id='27' author='manmay-nakhashi' date='2020-09-23T07:20:46Z'>
		&lt;denchmark-link:https://github.com/dathudeptrai&gt;@dathudeptrai&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/Zak-SA&gt;@Zak-SA&lt;/denchmark-link&gt;

A fix is to make the batch_size specific in:
@tf.function(
input_signature=[tf.TensorSpec(shape=[1, None, 384], dtype=tf.float32)])
Change None to 1.
		</comment>
		<comment id='28' author='manmay-nakhashi' date='2020-09-23T07:42:02Z'>
		&lt;denchmark-link:https://github.com/thaink&gt;@thaink&lt;/denchmark-link&gt;
 what do you mean ? the input shape is [B, T, F] so i explicit it by [1, None, 384].
		</comment>
		<comment id='29' author='manmay-nakhashi' date='2020-09-23T07:49:32Z'>
		&lt;denchmark-link:https://github.com/dathudeptrai&gt;@dathudeptrai&lt;/denchmark-link&gt;
 could you make your keras model have a fixed batch size? Since keras model usually have an unspecified size in batch size and you also have an unspecified size in the above input_signature. TF and TFLite both does not support two unspecified sizes in the one shape array.
		</comment>
		<comment id='30' author='manmay-nakhashi' date='2020-09-23T07:51:55Z'>
		&lt;denchmark-link:https://github.com/abattery&gt;@abattery&lt;/denchmark-link&gt;
 the input signature is [1, None, 384] already in my colab.
&lt;denchmark-link:https://user-images.githubusercontent.com/43868663/93983218-79546e00-fdac-11ea-8b92-3053902155f5.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='31' author='manmay-nakhashi' date='2020-09-23T07:57:04Z'>
		&lt;denchmark-link:https://github.com/dathudeptrai&gt;@dathudeptrai&lt;/denchmark-link&gt;
  Can you make your model with a fixed input signature? The none value will be propagated to a reshape op. and that reshape op has a wrong shape. Could you revisit your logic to avoid this behavior?
		</comment>
		<comment id='32' author='manmay-nakhashi' date='2020-09-23T07:59:10Z'>
		
@dathudeptrai Can you make your model with a fixed input signature? The none value will be propagated to a reshape op. and that reshape op has a wrong shape. Could you revisit your logic to avoid this behavior?

Hi, this is NLP/Speech task so i need dynamic time dimention. The old tf-nightly don't have this bug (tf.2.3 is also ok.)
		</comment>
		<comment id='33' author='manmay-nakhashi' date='2020-09-23T08:01:13Z'>
		&lt;denchmark-link:https://github.com/dathudeptrai&gt;@dathudeptrai&lt;/denchmark-link&gt;
 I see. Do you remember which version of tf-nightly does not have this issue before? That helps us find the root cause.
		</comment>
		<comment id='34' author='manmay-nakhashi' date='2020-09-23T08:03:01Z'>
		&lt;denchmark-link:https://github.com/abattery&gt;@abattery&lt;/denchmark-link&gt;
 tf-nightly==2.4.0-dev20200630 is the previous nightly version i used. Maybe we need iterate install some version to find the version that cause this bug :)
		</comment>
		<comment id='35' author='manmay-nakhashi' date='2020-09-23T08:04:14Z'>
		Thank you for reporting the regression. I will take a look at this issue.
		</comment>
		<comment id='36' author='manmay-nakhashi' date='2020-09-23T08:19:31Z'>
		
Thank you for reporting the regression. I will take a look at this issue.

Hi, tf-nightly==2.4.0-dev20200730 -&gt;&gt; bug, tf-nightly==2.4.0-dev20200729 -&gt;&gt; ok. So the commit cause this problem may in 30/07/2020
		</comment>
		<comment id='37' author='manmay-nakhashi' date='2020-09-23T08:22:08Z'>
		&lt;denchmark-link:https://github.com/dathudeptrai&gt;@dathudeptrai&lt;/denchmark-link&gt;
 Thank you for conducting your binary search! This information is very helpful for debugging. 
		</comment>
		<comment id='38' author='manmay-nakhashi' date='2020-09-27T21:32:38Z'>
		I tried converting a FastSpeech2 model to TensorFlow lite and I seem to be getting the same error, but changing the version tf-nightly does not seem to fix the issue for me.
Using tensorflow nightly version 2.4.0-dev20200925

java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/reshape.cc:55 stretch_dim != -1 (0 != -1)
Node number 81 (RESHAPE) failed to prepare.


Using tensorflow nightly version 2.4.0-dev20200716

java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/range.cc:45 (start &gt; limit &amp;&amp; delta &lt; 0) || (start &lt; limit &amp;&amp; delta &gt; 0) was not true.
Node number 524 (RANGE) failed to invoke.


Using tensorflow version 2.3.0

java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/range.cc:45 (start &gt; limit &amp;&amp; delta &lt; 0) || (start &lt; limit &amp;&amp; delta &gt; 0) was not true.
Node number 524 (RANGE) failed to invoke.


I can't find much information on the issue, and I'm not really sure why I've been getting the bug.  If it makes any difference, I'm using a machine without GPU to convert to TF-lite.
		</comment>
		<comment id='39' author='manmay-nakhashi' date='2020-09-28T13:19:09Z'>
		&lt;denchmark-link:https://github.com/gcervantes8&gt;@gcervantes8&lt;/denchmark-link&gt;
 can you try any build between nightly 2.4.0-dev20200630 - 2.4.0-dev20200729 ?
		</comment>
		<comment id='40' author='manmay-nakhashi' date='2020-09-28T15:23:38Z'>
		I tried version tf-nightly 2.4.0-dev20200713, I got the same error.

java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/range.cc:45 (start &gt; limit &amp;&amp; delta &lt; 0) || (start &lt; limit &amp;&amp; delta &gt; 0) was not true.
Node number 524 (RANGE) failed to invoke.

		</comment>
		<comment id='41' author='manmay-nakhashi' date='2020-09-28T17:13:57Z'>
		&lt;denchmark-link:https://github.com/gcervantes8&gt;@gcervantes8&lt;/denchmark-link&gt;
 are you using enable_tflite_convertible flag?
		</comment>
		<comment id='42' author='manmay-nakhashi' date='2020-09-28T17:16:28Z'>
		Yes! When I reload the model using FastSpeech2Config, I give the parameter:
enable_tflite_convertible=True
		</comment>
		<comment id='43' author='manmay-nakhashi' date='2020-10-16T06:43:44Z'>
		

Thank you for reporting the regression. I will take a look at this issue.

Hi, tf-nightly==2.4.0-dev20200730 -&gt;&gt; bug, tf-nightly==2.4.0-dev20200729 -&gt;&gt; ok. So the commit cause this problem may in 30/07/2020

I also encounter this bug in the conv1d layer, your solution really helps!
		</comment>
		<comment id='44' author='manmay-nakhashi' date='2020-10-27T08:34:19Z'>
		I have a working fix for this. It will be fixed soon.
		</comment>
		<comment id='45' author='manmay-nakhashi' date='2020-10-27T08:35:21Z'>
		
I have a working fix for this. It will be fixed soon.

thank you so much :D.
		</comment>
		<comment id='46' author='manmay-nakhashi' date='2020-11-01T10:28:20Z'>
		the tf.repeat function can also cause errors when I use tf-nightly==2.5.0-dev20201029
tensorflow.lite.python.convert.ConverterError: ...../lib/python3.6/site-packages/tensorflow/python/saved_model/load.py:890:0: error: 'tf.Reshape' op requires 'shape' to have at most one dynamic dimension, but got multiple dynamic dimensions at indices 1 and 2
........
Exception ignored in: &lt;bound method Buckets.del of &lt;tensorflow.python.eager.monitoring.ExponentialBuckets object at 0x7f6e852aa4c8&gt;&gt;
Traceback (most recent call last):
File "......./lib/python3.6/site-packages/tensorflow/python/eager/monitoring.py", line 407, in del
AttributeError: 'NoneType' object has no attribute 'TFE_MonitoringDeleteBuckets'
		</comment>
	</comments>
</bug>