<bug id='30806' author='vibhatha' open_date='2019-07-17T14:04:17Z' closed_time='2020-03-20T06:06:46Z'>
	<summary>Inference on Larger Data Size on Edge</summary>
	<description>
System information

Have I written custom code (Custom Code using resources and my own design):
OS Platform and Distribution (Linux Ubuntu 16.04):
Mobile device (trying inference on google coral dev board)
TensorFlow installed from (pip3 install tensorflow==2.0.0-beta1):
TensorFlow version (2.0.0-beta1):
Python version: 3.6.9
Bazel version (not used):
GCC/Compiler version (not used):
CUDA/cuDNN version: not used
GPU model and memory: not used
Run On CPU

Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                8
On-line CPU(s) list:   0-7
Thread(s) per core:    2
Core(s) per socket:    4
Socket(s):             1
NUMA node(s):          1
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 94
Model name:            Intel(R) Core(TM) i7-6700HQ CPU @ 2.60GHz
Stepping:              3
CPU MHz:               800.210
CPU max MHz:           3500.0000
CPU min MHz:           800.0000
BogoMIPS:              5183.86
Virtualization:        VT-x
L1d cache:             32K
L1i cache:             32K
L2 cache:              256K
L3 cache:              6144K
32 GB RAM
&lt;denchmark-code&gt;# The full neural network code!
###############################
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.layers import UpSampling1D

#Reference:https://colab.research.google.com/gist/ohtaman/c1cf119c463fd94b0da50feea320ba1e/edgetpu-with-keras.ipynb
def UpSamplingCustom2D(scale=(2, 2)):
  if isinstance(scale, int):
    scale = (scale, scale)

  def upsampling(x):
    shape = x.shape
    print("Upsampling : ", shape)
    x = keras.layers.Concatenate(-2)([x] * scale[0])
    x = keras.layers.Reshape([shape[1] * scale[0], shape[2], shape[3]])(x)
    x = keras.layers.Concatenate(-1)([x] * scale[1])
    x = keras.layers.Reshape([shape[1] * scale[0], shape[2] * scale[1], shape[3]])(x)
    return x

  return upsampling


image_size = 28
output_image_size = image_size * 2

train_images = np.random.rand(10,image_size,image_size,3)
train_images_labels = np.random.rand(10,output_image_size,output_image_size,3)


test_images = np.random.rand(10,image_size,image_size,3)
test_images_labels = np.random.rand(10,output_image_size,output_image_size,3)

inputs = keras.layers.Input(shape=(image_size, image_size, 3))
x = keras.layers.Dense(image_size, activation='relu')(inputs)
x = keras.layers.Dense(image_size, activation='relu')(x)
x = UpSamplingCustom2D()(x)
decoded = keras.layers.Dense(3, activation='relu')(x)
model = keras.models.Model(inputs, decoded)

#Compile the model.
model.compile(
  optimizer='adam',
  loss='binary_crossentropy',
)

#Train the model.
model.fit(
  train_images,
  train_images_labels,
  epochs=1,
  batch_size=32,
)

#Evaluate the model.
model.evaluate(
  test_images,
  test_images_labels
)



#Save the model to disk.
model.save_weights('model.h5')

#Load the model from disk later using:
model.load_weights('model.h5')

#Predict on the first 5 test images.
predictions = model.predict(test_images)

print(predictions[0].shape)


def representative_dataset_gen():
  for i in range(5):
    yield [train_images[i: i + 1].astype(np.float32)]

keras_file = "upsampling2d.h5"
tf.keras.models.save_model(model, keras_file)

#Convert to TensorFlow Lite model.
if (tf.__version__ == '1.14.0'):
  converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(keras_file)
if (tf.__version__ == '2.0.0-beta1'):
  converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(keras_file)
converter.representative_dataset = representative_dataset_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()
tflite_fname = "upsampling2d_" + str(tf.__version__) + ".tflite"
open(tflite_fname, "wb").write(tflite_model)

interpreter = tf.lite.Interpreter(model_path=tflite_fname)
interpreter.allocate_tensors()

input_detail = interpreter.get_input_details()[0]
output_detail = interpreter.get_output_details()[0]

print("Input Details : ", input_detail)
print("Output Details : ", output_detail)

def quantize(real_value):
  std, mean = input_detail['quantization']
  return (real_value / std + mean).astype(np.uint8)


sample_input = quantize(test_images[0]).reshape(input_detail['shape'])
print("Sample Input Shape : ", sample_input.shape)
print("Inference Sample Input Shape : ", sample_input.shape)

interpreter.set_tensor(input_detail['index'], sample_input)
interpreter.invoke()

#original_image = test_images[0].reshape((28, 28))
pred_original_model = model.predict(test_images[:1])
pred_quantized_model = interpreter.get_tensor(output_detail['index'])

print("Prediction : ",pred_quantized_model.shape)
&lt;/denchmark-code&gt;

Because of quantization issue for UpSampling2D, I used a snippet from
https://colab.research.google.com/gist/ohtaman/c1cf119c463fd94b0da50feea320ba1e/edgetpu-with-keras.ipynb
My issue is when I use image sizes from 28 till 256 (I just used powers of 2 and used 28 just to check), it worked fine for inferencing. I tested inferencing on the CPU itself using the interpreter package. But 512 or anything above freezes the machine and gives the following error.
2019-07-17 08:33:18.567609: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 5368709120 exceeds 10% of system memory. 2019-07-17 08:33:20.626046: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 5368709120 exceeds 10% of system memory. 2019-07-17 08:33:26.517300: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 10737418240 exceeds 10% of system memory. 2019-07-17 08:33:27.836867: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 21474836480 exceeds 10% of system memory. terminate called after throwing an instance of 'std::bad_alloc' what():  std::bad_alloc Aborted (core dumped) 
Is there a limit for the array size that can be used at inferencing after making the input array flatten. Or am I doing something wrong here?
	</description>
	<comments>
		<comment id='1' author='vibhatha' date='2019-07-31T20:43:43Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/liyunlu0618&gt;@liyunlu0618&lt;/denchmark-link&gt;

Were you able to follow this issue? For now, I use a workaround and it adds unnecessary complication to our system. I just need to clarify this before we design the production model.
Is this a known issue or something else is wrong in my end?
		</comment>
		<comment id='2' author='vibhatha' date='2019-10-08T17:56:43Z'>
		&lt;denchmark-link:https://github.com/vibhatha&gt;@vibhatha&lt;/denchmark-link&gt;
 Is this still an issue? Can you check with  and let us know whether the issue persists with latest TF version. Thanks!
		</comment>
		<comment id='3' author='vibhatha' date='2019-10-08T20:11:57Z'>
		Yes it is. I checked with TF2.0rc versions. Couldnâ€™t check with the stable release. I will also check that. Is it possible for you to test this as well?
		</comment>
		<comment id='4' author='vibhatha' date='2020-03-19T13:48:38Z'>
		&lt;denchmark-link:https://github.com/vibhatha&gt;@vibhatha&lt;/denchmark-link&gt;

Could you please let us know if this issue still persist.
		</comment>
		<comment id='5' author='vibhatha' date='2020-03-19T13:52:57Z'>
		This is a 7-month-old issue. At that time, I solved this by feeding smaller images and stitching this up.
I haven't looked into this after that time. Sorry, I cannot confirm it now. I no longer work on this project.
		</comment>
		<comment id='6' author='vibhatha' date='2020-03-19T14:37:38Z'>
		&lt;denchmark-link:https://github.com/vibhatha&gt;@vibhatha&lt;/denchmark-link&gt;

please confirm if we can move this issue to closed status
		</comment>
		<comment id='7' author='vibhatha' date='2020-03-19T14:43:33Z'>
		Yes, please close the issue.
		</comment>
		<comment id='8' author='vibhatha' date='2020-03-20T06:06:46Z'>
		Closing the issue since its resolved. Thanks!
		</comment>
		<comment id='9' author='vibhatha' date='2020-03-20T06:06:48Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30806&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30806&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>