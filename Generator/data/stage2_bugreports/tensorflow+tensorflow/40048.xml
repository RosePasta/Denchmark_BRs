<bug id='40048' author='Sidong-Wei' open_date='2020-06-01T14:02:04Z' closed_time='2020-10-27T12:15:44Z'>
	<summary>Test case //tensorflow/python/eager:def_function_test_cpu_only fails on TF2.2 due to inconsistent XLA flag.</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 2.2.0
Python version: 3.8.2
Bazel version (if compiling from source): 2.0.0
GCC/Compiler version (if compiling from source): gcc (Ubuntu 9.3.0-10ubuntu2) 9.3.0
CUDA/cuDNN version: N/A
GPU model and memory: N/A

Describe the current behavior
The test case //tensorflow/python/eager:def_function_test_cpu_only fails with the following error log:
&lt;denchmark-code&gt;======================================================================
ERROR: testExperimentalCompileRaisesExceptionWhenXlaIsUnsupported (__main__.DefFunctionCpuOnlyTest)
testExperimentalCompileRaisesExceptionWhenXlaIsUnsupported (__main__.DefFunctionCpuOnlyTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/sidong/.cache/bazel/_bazel_sidong/9ef871a29c692fc6a18121463529e145/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/eager/def_function_test_cpu_only.runfiles/org_tensorflow/tensorflow/python/eager/def_function_test_cpu_only.py", line 46, in testExperimentalCompileRaisesExceptionWhenXlaIsUnsupported
    fn([1, 1, 2, 3])
  File "/home/sidong/.cache/bazel/_bazel_sidong/9ef871a29c692fc6a18121463529e145/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/eager/def_function_test_cpu_only.runfiles/org_tensorflow/tensorflow/python/eager/def_function.py", line 576, in __call__
    result = self._call(*args, **kwds)
  File "/home/sidong/.cache/bazel/_bazel_sidong/9ef871a29c692fc6a18121463529e145/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/eager/def_function_test_cpu_only.runfiles/org_tensorflow/tensorflow/python/eager/def_function.py", line 650, in _call
    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access
  File "/home/sidong/.cache/bazel/_bazel_sidong/9ef871a29c692fc6a18121463529e145/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/eager/def_function_test_cpu_only.runfiles/org_tensorflow/tensorflow/python/eager/function.py", line 1661, in _filtered_call
    return self._call_flat(
  File "/home/sidong/.cache/bazel/_bazel_sidong/9ef871a29c692fc6a18121463529e145/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/eager/def_function_test_cpu_only.runfiles/org_tensorflow/tensorflow/python/eager/function.py", line 1745, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File "/home/sidong/.cache/bazel/_bazel_sidong/9ef871a29c692fc6a18121463529e145/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/eager/def_function_test_cpu_only.runfiles/org_tensorflow/tensorflow/python/eager/function.py", line 593, in call
    outputs = execute.execute(
  File "/home/sidong/.cache/bazel/_bazel_sidong/9ef871a29c692fc6a18121463529e145/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/eager/def_function_test_cpu_only.runfiles/org_tensorflow/tensorflow/python/eager/execute.py", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError: Function invoked by the following node is not compilable: {{node __inference_fn_7}} = __inference_fn_7[_XlaMustCompile=true, config_proto="\n\007\n\003CPU\020\001\n\007\n\003GPU\020\0002\002J\0008\001", executor_type=""]().
Uncompilable nodes:
Unique: unsupported op: No registered 'Unique' OpKernel for XLA_CPU_JIT devices compatible with node {{node Unique}}
        Stacktrace:
                Node: __inference_fn_7, function:
                Node: Unique, function: __inference_fn_7
 [Op:__inference_fn_7]

----------------------------------------------------------------------
Ran 2 tests in 0.318s

FAILED (errors=1, skipped=1)
&lt;/denchmark-code&gt;

Describe the expected behavior
The test case should raise a ValueError "Attempting to use experimental_compile, but XLA support is not linked in. Rebuild with --define=with_xla_support=true.", and captured by self.assertRaisesRegexp()
The test case should pass.

I modified the test case a little bit, and it could be reproduced from this &lt;denchmark-link:https://colab.research.google.com/drive/1qweKRl1ZxEPWtN-qlz6DsNCVR0J8M8fg?usp=sharing&gt;gist&lt;/denchmark-link&gt;

Other info / logs
I looked into this issue, it seems that it is caused by inconsistent internal APIs. When debugging this test file, I get the following results:
&lt;denchmark-code&gt;&gt; /home/sidong/.cache/bazel/_bazel_sidong/338a466d2403fbfe3413e7ca6003e4cf/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/eager/def_function_test_cpu_only.runfiles/org_tensorflow/tensorflow/python/eager/def_function_test_cpu_only.py(47)testExperimentalCompileRaisesExceptionWhenXlaIsUnsupported()
-&gt; fn([1, 1, 2, 3])
(Pdb) p test_util.is_xla_enabled()
False
(Pdb) p pywrap_tfe.TF_IsXlaEnabled()
True
&lt;/denchmark-code&gt;

Basically, the test proceeds if  is false, and it will raise the exception as intended when  is false. The inconsistency caused the exception not correctly raised.
One way to fix it is to discard  and only use , but I think maybe it's better to solve this inconsistency issue. I noticed that the  API and this test case was added from the same &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/96e0b87d1e23ac1dd7a7aa984e3f479647267b32&gt;commit&lt;/denchmark-link&gt;
, and I assume the cause of inconsistency is that the function  was called incorrectly. Could you look into this issue and check why the function was called when xla was not enabled? Thanks.
Sidong
	</description>
	<comments>
		<comment id='1' author='Sidong-Wei' date='2020-06-02T11:34:32Z'>
		Was able to reproduce the issue with &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/8434fed6f34cd6ea642fbba585c8ba4c/40048.ipynb&gt;TF v2.2&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/8ffb1016c0291672d7b9d0508874e083/40048-tf-nightly.ipynb&gt;TF-nightly&lt;/denchmark-link&gt;
. Please find the attached gist. Thanks!
		</comment>
		<comment id='2' author='Sidong-Wei' date='2020-06-09T16:20:58Z'>
		Hi &lt;denchmark-link:https://github.com/cheshire&gt;@cheshire&lt;/denchmark-link&gt;
 , I noticed that you contributed to this part of xla code and specifically the  flag, could you take a look into this issue and provides some insights? Thank you.
		</comment>
		<comment id='3' author='Sidong-Wei' date='2020-06-09T16:42:26Z'>
		&lt;denchmark-link:https://github.com/Sidong-Wei&gt;@Sidong-Wei&lt;/denchmark-link&gt;
 looks like a bug, thanks, I wonder why it passes internally on our CI.
		</comment>
		<comment id='4' author='Sidong-Wei' date='2020-10-27T12:15:44Z'>
		Looks like it has been fixed by this &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/c90cf2e18f808f93d315ba35e6f9103065cd0384&gt;commit&lt;/denchmark-link&gt;
, close for now.
		</comment>
		<comment id='5' author='Sidong-Wei' date='2020-10-27T12:15:46Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40048&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40048&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>