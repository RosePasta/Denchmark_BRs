<bug id='8047' author='agupta74' open_date='2017-03-03T06:36:32Z' closed_time='2017-03-20T20:31:14Z'>
	<summary>Error in tensorflow debugger (tfdbg) while executing session run call in child thread</summary>
	<description>
I ran tensorflow debugger using the command "python -m  --debug" but got the following error (i,e Signal only works in main thread):
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/21690396/23540767/24df69c6-ff98-11e6-96bf-5ee98a439f77.PNG&gt;&lt;/denchmark-link&gt;

The error is thrown when a session run call is executed in a child thread (spawned from main thread). Is tensorflow debugger only supported for single threaded applications?
&lt;denchmark-h:h3&gt;What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?&lt;/denchmark-h&gt;

None
&lt;denchmark-h:h3&gt;Environment info&lt;/denchmark-h&gt;

Operating System: CentOS 7.2.1511
Installed version of CUDA and cuDNN:
(please attach the output of ):
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/21690396/23540624/38732a46-ff97-11e6-9749-a43b8af6c5aa.PNG&gt;&lt;/denchmark-link&gt;

If installed from binary pip package, provide:


A link to the pip package you installed: https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.0-cp35-cp35m-linux_x86_64.whl


The output from python -c "import tensorflow; print(tensorflow.__version__)".



If installed from source, provide

The commit hash (git rev-parse HEAD)
The output of bazel version

&lt;denchmark-h:h3&gt;If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;What other attempted solutions have you tried?&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Logs or other output that would be helpful&lt;/denchmark-h&gt;

(If logs are large, please upload as attachment or provide link).
	</description>
	<comments>
		<comment id='1' author='agupta74' date='2017-03-03T17:29:49Z'>
		&lt;denchmark-link:https://github.com/agupta74&gt;@agupta74&lt;/denchmark-link&gt;
 tfdbg's CLI class attempts to register signal handlers even when not running on the main threads, which is the reason for this error. We will send a bug fix to the master branch.
As a workaround until the bug is fixed, you can pretend that the child process is a "remote process" and use the approach outlined here:
&lt;denchmark-link:https://www.tensorflow.org/programmers_guide/debugger#offline_debugging_of_remotely-running_sessions&gt;https://www.tensorflow.org/programmers_guide/debugger#offline_debugging_of_remotely-running_sessions&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;from tensorflow.python import debug as tf_debug

# ... Code where your session and graph are set up...

run_options = tf.RunOptions()
tf_debug.watch_graph(
      run_options,
      session.graph,
      debug_urls=["file:///shared/storage/location/tfdbg_dumps_1"])
# Be sure to use different directories for different run() calls.
session.run(fetches, feed_dict=feeds, options=run_options)
&lt;/denchmark-code&gt;

Later, in an environment that you have terminal access to, you can load and inspect the data in the dump directory on the shared storage by using the offline_analyzer of tfdbg. For example:
&lt;denchmark-code&gt;python -m tensorflow.python.debug.cli.offline_analyzer \
    --dump_dir=/shared/storage/location/tfdbg_dumps_1
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='agupta74' date='2017-03-03T17:32:20Z'>
		Thank you for reporting this bug, &lt;denchmark-link:https://github.com/agupta74&gt;@agupta74&lt;/denchmark-link&gt;
!
		</comment>
		<comment id='3' author='agupta74' date='2017-03-07T16:41:06Z'>
		&lt;denchmark-link:https://github.com/caisq&gt;@caisq&lt;/denchmark-link&gt;
 Any updates regarding this issue? I've tried using  per your suggestion; but, keep running into  as in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/7051&gt;#7051&lt;/denchmark-link&gt;
. Unlike &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/7051&gt;#7051&lt;/denchmark-link&gt;
, I'm running locally on CPU. The problem seems to stem from repeated calls to , where  is an instance of ; but, I'm not sure how to get around this issue when using  given that a call to an initializer seems necessary. Included below is a code snippet.
&lt;denchmark-code&gt;# Dependencies
import subprocess
import tensorflow as tf
from tempfile import TemporaryDirectory
from tensorflow.python import debug as tf_debug

dump_dir = TemporaryDirectory() #temp. directory for demo
with tf.Session() as _sess:
  # Create 'DumpingDebugWrapperSession' wrapper
  def watch_fn(fetches, feed_dict):
    node_name_regex_whitelist = '(watch_[0-9]+)'
    debug_ops, op_type_regex_whitelist = None, None
    return debug_ops, node_name_regex_whitelist, op_type_regex_whitelist
  sess = tf_debug.DumpingDebugWrapperSession(_sess,
                  dump_dir.name, watch_fn=watch_fn)

  # Declare TensorFlow variables
  tf_vars = [tf.get_variable('watch_0', shape=[32], dtype='float32'),
             tf.get_variable('ignore_0', shape=[32], dtype='float32')]

  # Calls sess.run()
  init_op = tf.global_variables_initializer()
  sess.run(init_op)
  # &lt;stuff goes here&gt;
  fetches = sess.run(tf_vars, feed_dict={})

# Call to offline_analyzer
argv = ['python3.5', '-m', 'tensorflow.python.debug.cli.offline_analyzer',
        '--ui_type', 'readline', '--dump_dir', dump_dir.name]
cmdline = " ".join(argv)
try:
  proc = subprocess.Popen(cmdline, shell=True)
  proc.wait()
finally:
  dump_dir.cleanup()
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='agupta74' date='2017-03-07T16:49:47Z'>
		&lt;denchmark-link:https://github.com/j-wilson&gt;@j-wilson&lt;/denchmark-link&gt;
 With regard to the duplicate node name error you are getting, are you using the same file:// debug URL (i.e., the same directory) for different session.run() calls? If that's the case, can you try using a unique different URL (i.e. ,directory) for each session.run() call?
		</comment>
		<comment id='5' author='agupta74' date='2017-03-08T10:09:30Z'>
		&lt;denchmark-link:https://github.com/caisq&gt;@caisq&lt;/denchmark-link&gt;
 The script runs if modified s.t. a new wrapper/directory is used for each call to ; however, none of the tensors show up inside the debugger CLI, i.e. .
As a separate comment, is there a reason why separate wrappers/directories are needed for each call to sess.run()? The wrapper creates subdirectories run_&lt;run_id&gt;, suggesting that reuse of a single wrapper might be possible --- which would help with usability.
		</comment>
		<comment id='6' author='agupta74' date='2017-03-08T14:58:19Z'>
		It should work with a single wrapper.
As for the reason why you see 0 dumped tensor(s), it might be that

the node_name_regex_whitelist value returned from your watch_fn hits no nodes in the graph? What if you use watch_fn=None, so the default behavior (i.e., watch all nodes) can be used?
ops are constant folded during graph optimization (less likely)?

		</comment>
	</comments>
</bug>