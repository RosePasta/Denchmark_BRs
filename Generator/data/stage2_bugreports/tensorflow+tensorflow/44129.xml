<bug id='44129' author='michaelnguyen11' open_date='2020-10-18T14:50:30Z' closed_time='2020-11-11T07:48:37Z'>
	<summary>Tensorflow Lite with Nvidia GPU on Ubuntu happen coredump when creating delegate.</summary>
	<description>
Hi Supporters,
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): master branch (2.3.1)
Python version: 3.8
Bazel version (if compiling from source): 3.1.0
GCC/Compiler version (if compiling from source): 7.5.0
CUDA/cuDNN version: 10.1
GPU model and memory: GeForce GTX 1050 Ti, 4Gb

Describe the current behavior
I tried to inference TF lite model with GPU, everything is normal with TF using CPU, but when create delagate, coredump happens immediately.
Standalone code to reproduce the issue
&lt;denchmark-code&gt;#include &lt;iostream&gt;
#include &lt;memory&gt;

#include &lt;tensorflow/lite/interpreter.h&gt;
#include &lt;tensorflow/lite/kernels/register.h&gt;
#include &lt;tensorflow/lite/string_util.h&gt;
#include &lt;tensorflow/lite/model.h&gt;
#include &lt;tensorflow/lite/delegates/gpu/delegate.h&gt;
#include &lt;tensorflow/lite/delegates/gpu/gl_delegate.h&gt;

int main()
{
    std::unique_ptr&lt;tflite::FlatBufferModel&gt; model = tflite::FlatBufferModel::BuildFromFile("superpoint_640x480.tflite");
    if (!model)
    {
        std::cerr &lt;&lt; "Failed to mmap tflite model" &lt;&lt; std::endl;
        return -1;
    }
    tflite::ops::builtin::BuiltinOpResolver resolver;
    std::unique_ptr&lt;tflite::Interpreter&gt; interpreter;
    if (tflite::InterpreterBuilder(*model.get(), resolver)(&amp;interpreter) != kTfLiteOk)
    {
        std::cerr &lt;&lt; "Failed to interpreter tflite model" &lt;&lt; std::endl;
        return -1;
    }

    // auto* delegate = TfLiteGpuDelegateCreate(nullptr);
    // if (interpreter-&gt;ModifyGraphWithDelegate(delegate) == kTfLiteOk)
    // {
    //     std::cerr &lt;&lt; "Failed to enable GPU." &lt;&lt; std::endl;
    //     return -1;
    // }

    //  NEW: Prepare custom options with feature enabled.
    TfLiteGpuDelegateOptionsV2 options = TfLiteGpuDelegateOptionsV2Default();
    options.experimental_flags = TFLITE_GPU_EXPERIMENTAL_FLAGS_NONE;

    auto* delegate = TfLiteGpuDelegateV2Create(&amp;options);
    if (interpreter-&gt;ModifyGraphWithDelegate(delegate) != kTfLiteOk){
        std::cerr &lt;&lt; "Failed to register delegate" &lt;&lt; std::endl;
        return -1;
    }

    return 0;
}
&lt;/denchmark-code&gt;

TF Lite model file :
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/5397657/superpoint_640x480.zip&gt;superpoint_640x480.zip&lt;/denchmark-link&gt;

 Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/5397656/backtrace.txt&gt;backtrace.txt&lt;/denchmark-link&gt;

As I know so far, TF Lite on GPU supported PAD ops, but don't know why the coredump happened here.
Have you seen this ? Could you help me overcome this kind of issue ?
Many thanks in advance.
	</description>
	<comments>
		<comment id='1' author='michaelnguyen11' date='2020-10-19T23:00:46Z'>
		Are you trying TF Lite gpu delegate on desktop machine?
TensorFlow Lite delegate APIs are supported on Android and iOS platform as of now. Thanks!
See &lt;denchmark-link:https://www.tensorflow.org/lite/performance/gpu&gt;https://www.tensorflow.org/lite/performance/gpu&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='michaelnguyen11' date='2020-10-20T04:36:03Z'>
		Hi &lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;
,
Yes I'm using TF Lite GPU delegate on Desktop machine. So the TF Lite GPU supports only Android and iOS platforms, does it ?
I'm developing a TF Lite model and expecting to inference it on embedded device which included NPU. So could you please give me some hint that I can use the TF Lite GPU delegate to inference the model on the edge with NPU ?
Many thanks in advance.
		</comment>
		<comment id='3' author='michaelnguyen11' date='2020-10-20T23:45:41Z'>
		NPU has nothing in common with GPU, and I'm kinda confused why you would start with the GPU.
Having said that, I don't see anything in the code that could cause a segfault in the PadOperationParser.  Can you compile with -g to get more info from the debugger?
		</comment>
		<comment id='4' author='michaelnguyen11' date='2020-10-21T04:22:14Z'>
		Hi &lt;denchmark-link:https://github.com/impjdi&gt;@impjdi&lt;/denchmark-link&gt;
,
Thank you for your response.
As so far I know, creating GPU and NNAPI delegate and register to Interpreter are pretty similar, so I would like to start with GPU first for easy to develop and test on laptop.
Added CMAKE_BUILD_TYPE=Debug to cmake. Please see that attach file for back trace
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/5413234/gdb.txt&gt;gdb.txt&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='michaelnguyen11' date='2020-10-21T23:48:52Z'>
		
No symbol table info available.

tells me that the delegate isn't built with -g =/
		</comment>
		<comment id='6' author='michaelnguyen11' date='2020-11-11T07:48:37Z'>
		Thanks for you help. I managed to run the Nvidia GPU with TF Lite version 2.3
Used this command to build TF Lite gpu delegate :
&lt;denchmark-code&gt;bazel build -s -c opt --config=monolithic --config=noaws --config=nonccl --config=v2 --define=tflite_convert_with_select_tf_ops=true --define=with_select_tf_ops=true --copt="-DMESA_EGL_NO_X11_HEADERS" tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so
&lt;/denchmark-code&gt;

As well as the use this code to register gpu delegate:
&lt;denchmark-code&gt;const TfLiteGpuDelegateOptionsV2 options = {
    .is_precision_loss_allowed = 1, // FP16
    .inference_preference = TFLITE_GPU_INFERENCE_PREFERENCE_FAST_SINGLE_ANSWER,
    .inference_priority1 = TFLITE_GPU_INFERENCE_PRIORITY_MIN_LATENCY,
    .inference_priority2 = TFLITE_GPU_INFERENCE_PRIORITY_AUTO,
    .inference_priority3 = TFLITE_GPU_INFERENCE_PRIORITY_AUTO,
};

TfLiteDelegate *delegate = TfLiteGpuDelegateV2Create(&amp;options);
if (interpreter-&gt;ModifyGraphWithDelegate(delegate) != kTfLiteOk)
{
    std::cerr &lt;&lt; "Failed to modify graph with delegate" &lt;&lt; std::endl;
    exit(0);
}
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='michaelnguyen11' date='2020-11-11T07:48:38Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44129&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44129&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>