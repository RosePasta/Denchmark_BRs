<bug id='37067' author='alexdwu13' open_date='2020-02-25T22:45:27Z' closed_time='2020-03-20T18:55:01Z'>
	<summary>Usage Documentation Needed for tf.image.yuv_to_rgb</summary>
	<description>
&lt;denchmark-h:h2&gt;URL(s) with the issue:&lt;/denchmark-h&gt;

&lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/image/yuv_to_rgb&gt;https://www.tensorflow.org/api_docs/python/tf/image/yuv_to_rgb&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;Description of issue (what needs changing):&lt;/denchmark-h&gt;

The tf.image.yuv_to_rgb method specifies a YUV input of shape (H,W,3) and an RGB output of the same shape. It also notes:

The output is only well defined if the Y value in images are in [0,1], U and V value are in [-0.5,0.5].

However YUV is natively encoded in HxWx1.5 Bytes, with values ranging from 0-255. Considering that multiple YUV-RGB conversion standards exist, it is unclear what pre-processing steps need to be done by a user who wants to pass YUV inputs to his network (&lt;denchmark-link:https://en.wikipedia.org/wiki/YCbCr#JPEG_conversion&gt;https://en.wikipedia.org/wiki/YCbCr#JPEG_conversion&lt;/denchmark-link&gt;
).
&lt;denchmark-h:h3&gt;Usage example&lt;/denchmark-h&gt;

More documentation on the proper usage of this method would be highly helpful. Specifically:

An example showing how to pre-process a raw YUV image of size HxWx1.5B to the expected shape of (H,W,3) with normalized values Y: [0,1], UV: [-0.5,0.5].
An example of how one might append this method to an RGB-trained model to enable it to accept YUV inputs during inference. A likely scenario might be exporting a frozen model to an Android device that natively captures in YUV.

	</description>
	<comments>
		<comment id='1' author='alexdwu13' date='2020-02-26T15:16:32Z'>
		&lt;denchmark-link:https://github.com/alexdwu13&gt;@alexdwu13&lt;/denchmark-link&gt;
 Thanks for creating the issue. Would you like to create a PR to update the doc? Thanks!
		</comment>
		<comment id='2' author='alexdwu13' date='2020-02-26T23:32:23Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 I'd be happy to create a PR -- but I myself am not clear about the usage. I've tried munging YUV images to the input format expected by  but have so far not been successful &lt;denchmark-link:https://stackoverflow.com/questions/60067415/what-is-the-correct-usage-of-tf-image-yuv-to-rgb-output-is-distorted&gt;https://stackoverflow.com/questions/60067415/what-is-the-correct-usage-of-tf-image-yuv-to-rgb-output-is-distorted&lt;/denchmark-link&gt;
.
Would you be able to grab ahold of the original authors of this method?
		</comment>
		<comment id='3' author='alexdwu13' date='2020-06-08T04:39:28Z'>
		I left a comment on the PR 37565. I think the documentation has been incorrect on the [-0.5, 0.5] range for the UV channels. And there are bigger issues with the function itself.

Prior to this issue, the documentation specified that the U and V channels need to be in the range [-0.5, 0.5]. That is incorrect. The kernels in the source show that the correct ranges are

u: -0.43601035 to + 0.43601035
v: -0.61497538 to + 0.61497538

The defined ranges for U and V per the ITU spec are in fact

[-0.436, +0.436]
[-0.615, +0.615]



The PR 37565 created a linear interpolation example for the (incorrectly documented) range of [-0.5, 0.5] for the U and V channels.



PR 37565  implies that a TF developer might end up with a YUV range of [0, 256). That is extremely unlikely because the YUV space is not "square", it is "diamond" and corners like YUV= (1,1,1) are in fact impossible colors. YUV=(1,1,1) would mean a "maximum bright white color with a lot of red and blue". This results in a color with more red and blue than an LCD display can render, or the human eye can perceive.



tf.image.yuv_to_rgb(tf.constant([1.0, 0.436, 0.615]))
&lt;tf.Tensor: shape=(3,), dtype=float32, numpy=array([1.7010281, 0.4708535, 1.8859789], dtype=float32)&gt;



The conceivable sources of a YUV image would be directly pulled from an analog CRT display, or, via transforms on a tensor that came from the tf.image.rgb_to_yuv. The former seems very unlikely, the latter is also unlikely because manipulating YUV directly without running into clipping against the "diamond shaped" visible color space is very difficult.



Finally, the rgb_to_yuv and yuv_to_rgb functions are problematic because they assume a linear, non-gamma corrected RGB color values. The vast majority of images on the web or captured by a camera will be rendered in a gamma corrected color-space, namely sRGB.
--
To fix the documentation, I'd suggest a quick update to the correct UV ranges.  I'd also suggest a strong caveat that this function is likely not what people are looking for - a better transform for gamma corrected RGB (namely, sRGB) would be YCbCr. I'd also suggest we remove the example recently added of the linear transform, since it's unlikely a developer would ever get the raw YUV values from something other than the rgb_to_yuv function.
Here is the start of some example code for yuv_to_rgb... if these suggestions make sense, I'll finish this up and make it a PR.
def yuv_to_rgb(images):
"""Converts one or more images from YUV to RGB.
Outputs a tensor of the same shape as the images tensor, containing the RGB
value of the pixels.
The output is only well defined if the Y values in images are in [0,1],
U values are in [-0.43601035, 0.43601035], and V value are in
[-0.61497538,0.61497538].
&lt;denchmark-h:h1&gt;Test approximate range of Y, U, and V.&lt;/denchmark-h&gt;




rgb_images = tf.random.uniform(shape=[60000, 32, 32, 3], minval=0, maxval=256, dtype=tf.int32)
rgb_images = tf.cast(rgb_images, tf.uint8)
rgb_float_images = tf.image.convert_image_dtype(rgb_images, tf.float32)
yuv_images = tf.image.rgb_to_yuv(rgb_float_images)






Approximate min and max for Y channel
tf.reduce_min(yuv_images[...,0])
&lt;tf.Tensor: shape=(), dtype=float32, numpy=0.0&gt;
tf.reduce_max(yuv_images[...,0])
&lt;tf.Tensor: shape=(), dtype=float32, numpy=1.0&gt;



&lt;denchmark-h:h1&gt;Approximate min and max for U channel&lt;/denchmark-h&gt;




tf.reduce_min(yuv_images[...,1])
&lt;tf.Tensor: shape=(), dtype=float32, numpy=-0.43543333&gt;
tf.reduce_max(yuv_images[...,1])
&lt;tf.Tensor: shape=(), dtype=float32, numpy=0.43601036&gt;



&lt;denchmark-h:h1&gt;Approximate min and max for V channel&lt;/denchmark-h&gt;




tf.reduce_min(yuv_images[...,2])
&lt;tf.Tensor: shape=(), dtype=float32, numpy=-0.6149754&gt;
tf.reduce_max(yuv_images[...,2])
&lt;tf.Tensor: shape=(), dtype=float32, numpy=0.6149754&gt;



This function is likely not appropriate for images typically found on the web or generated by
digital cameras. Those typically adjust the RGB values to be gamma corrected, meaning they are rendered in the sRGB color space. The YCbCr colorspace was designed for the
sRGB colorspace and may be more appropriate. The YUV colorspace
was based on analog, non-gamma corrected RGB values.
		</comment>
	</comments>
</bug>