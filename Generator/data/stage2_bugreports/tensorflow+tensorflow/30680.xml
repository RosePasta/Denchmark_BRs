<bug id='30680' author='Adnan-annan' open_date='2019-07-13T11:58:17Z' closed_time='2019-08-01T13:14:37Z'>
	<summary>ReadTensorFromImageFile() C++ function too slow for large size images</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows10
TensorFlow installed from (source or binary):from source
TensorFlow version (use command below):1.8.0
Python version:3.5
-Microsoft Visual Studio 2017

Describe the current behavior
I am using tensoirflow object detection api in C++ , trained model with version 1.8.0, now i am running it on a laptop with no GPU and in Microsoft Visual studio 2017. The ReadTensorFromImageFile function takes alot of time and i would like it if it can work faster. i am loading 8K image and it takes around two seconds running through the ReadTensorFromImageFile() function. Is there a way to make it faster ??
Describe the expected behavior
I would like this function ReadTensorFromImageFile() to work faster for 8K size images.
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='Adnan-annan' date='2019-07-15T10:08:07Z'>
		In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!
		</comment>
		<comment id='2' author='Adnan-annan' date='2019-07-16T03:18:03Z'>
		`Status ReadTensorFromImageFile(const string&amp; file_name, const int input_height,
const int input_width, const float input_mean,
const float input_std,
std::vector* out_tensors) {
auto root = tensorflow::Scope::NewRootScope();
using namespace ::tensorflow::ops;  // NOLINT(build/namespaces)
&lt;denchmark-code&gt;string input_name = "file_reader";
string output_name = "normalized";

// read file_name into a tensor named input
Tensor input(tensorflow::DT_STRING, tensorflow::TensorShape());
TF_RETURN_IF_ERROR(
	ReadEntireFile(tensorflow::Env::Default(), file_name, &amp;input));

// use a placeholder to read input data
auto file_reader =
	Placeholder(root.WithOpName("input"), tensorflow::DataType::DT_STRING);

std::vector&lt;std::pair&lt;string, tensorflow::Tensor&gt;&gt; inputs = {
	{"input", input},
};

// Now try to figure out what kind of file it is and decode it.
const int wanted_channels = 3;
tensorflow::Output image_reader;
if (tensorflow::StringPiece(file_name).ends_with(".png")) {
	image_reader = DecodePng(root.WithOpName("png_reader"), file_reader,
		DecodePng::Channels(wanted_channels));
}
else if (tensorflow::StringPiece(file_name).ends_with(".gif")) {
	// gif decoder returns 4-D tensor, remove the first dim
	image_reader =
		Squeeze(root.WithOpName("squeeze_first_dim"),
			DecodeGif(root.WithOpName("gif_reader"), file_reader));
}
else {
	// Assume if it's neither a PNG nor a GIF then it must be a JPEG.
	image_reader = DecodeJpeg(root.WithOpName("jpeg_reader"), file_reader,
		DecodeJpeg::Channels(wanted_channels));
}
// Now cast the image data to float so we can do normal math on it.
// auto float_caster =
//     Cast(root.WithOpName("float_caster"), image_reader, tensorflow::DT_FLOAT);

auto uint8_caster = Cast(root.WithOpName("uint8_caster"), image_reader, tensorflow::DT_UINT8);

// The convention for image ops in TensorFlow is that all images are expected
// to be in batches, so that they're four-dimensional arrays with indices of
// [batch, height, width, channel]. Because we only have a single image, we
// have to add a batch dimension of 1 to the start with ExpandDims().
auto dims_expander = ExpandDims(root.WithOpName("dim"), uint8_caster, 0);

// Bilinearly resize the image to fit the required dimensions.
// auto resized = ResizeBilinear(
//     root, dims_expander,
//     Const(root.WithOpName("size"), {input_height, input_width}));


// Subtract the mean and divide by the scale.
// auto div =  Div(root.WithOpName(output_name), Sub(root, dims_expander, {input_mean}),
//     {input_std});


//cast to int
//auto uint8_caster =  Cast(root.WithOpName("uint8_caster"), div, tensorflow::DT_UINT8);

// This runs the GraphDef network definition that we've just constructed, and
// returns the results in the output tensor.
tensorflow::GraphDef graph;
TF_RETURN_IF_ERROR(root.ToGraphDef(&amp;graph));

std::unique_ptr&lt;tensorflow::Session&gt; session(
	tensorflow::NewSession(tensorflow::SessionOptions()));
TF_RETURN_IF_ERROR(session-&gt;Create(graph));
TF_RETURN_IF_ERROR(session-&gt;Run({ inputs }, { "dim" }, {}, out_tensors));
return Status::OK();
&lt;/denchmark-code&gt;

}`
&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
 I am using this function code in C++ to read tensor from image file. The problem i am facing is that i have the input image of size 8000x4000 which is too big size, converting this image to tensor takes almost 700 to 800 miliseconds which is too much for my application. Afterwards model takes 0.5 seconds for predictions which is fine, but loading and converting this image into tensor is taking alot of time. How do you suggest i solve this problem ?
		</comment>
		<comment id='3' author='Adnan-annan' date='2019-07-16T08:39:36Z'>
		&lt;denchmark-link:https://github.com/Adnan-annan&gt;@Adnan-annan&lt;/denchmark-link&gt;

Can you please confirm whether ReadTensorFromImageFile you are using is a custom defined function or Tensorflow defined function.Thanks!
		</comment>
		<comment id='4' author='Adnan-annan' date='2019-07-16T12:24:55Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
 I alos copied it from the github but it appears to be a custom code written by someone, the ReadTensorFromImageFile() function in tensorflow/tensorflow/examples/label_image/main.cc ia different. I used both of them and both of them take 600 to 700 miliseconds loading and converting the 8000x4000 pixels image into tensor. And the tensor is later fed to actually run the model.
		</comment>
		<comment id='5' author='Adnan-annan' date='2019-07-17T19:30:38Z'>
		&lt;denchmark-link:https://github.com/Adnan-annan&gt;@Adnan-annan&lt;/denchmark-link&gt;
 Did you try latest TF versions (TF1.14, tf-nightly(TF1.15), TF2.0.0b1). There were lot of performance improvements implemented in latest versions. Thanks!
		</comment>
		<comment id='6' author='Adnan-annan' date='2019-08-01T12:43:31Z'>
		It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue?
		</comment>
		<comment id='7' author='Adnan-annan' date='2019-08-01T13:14:36Z'>
		Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
		<comment id='8' author='Adnan-annan' date='2019-08-01T13:14:38Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=30680&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=30680&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>