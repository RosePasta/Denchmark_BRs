<bug id='28007' author='ipod825' open_date='2019-04-20T21:45:58Z' closed_time='2020-08-07T14:34:44Z'>
	<summary>InvalidArgumentError when running map_fn on strings inside a tf.function</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
TensorFlow installed from (source or binary):
TensorFlow version (use command below):

conda install tensorflow-gpu==2.0-alpha

Python version:
3.7.1
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
cudatoolkit-10.0.130-0
cudnn-7.3.1-cuda10.0_0
GPU model and memory:
GeForce RTX 2080 Ti

Describe the current behavior
Running the provided code on GPUs leads to error message tensorflow.python.framework.errors_impl.InvalidArgumentError: During Variant Host-&gt;Device Copy: non-DMA-copy attempted of tensor type: string
Without feeding the tensor to the convolution layer, summary.image would succeed.
Describe the expected behavior
Should run smoothly.
Code to reproduce the issue
import tensorflow as tf
from tensorflow.keras import layers

H, W, C = 10, 10, 3
imgs = tf.zeros([10, H, W, C])
ds = tf.data.Dataset.from_tensor_slices(imgs)
ds = ds.batch(2)
conv = layers.Conv2D(32, (4, 4), strides=(2, 2), padding='same')


@tf.function
def run(img, i):
    conv(img)
    tf.summary.image('img', img, i)


if __name__ == "__main__":
    train_summary_writer = tf.summary.create_file_writer('/tmp/testsummary')
    with train_summary_writer.as_default():
        for i, img in enumerate(ds):
            run(img, i)
Other info / logs
&lt;denchmark-code&gt;TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-04-20 14:44:30.818841: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1700000000 Hz
2019-04-20 14:44:30.819976: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x55b6fa788f50 executing computa
tions on platform Host. Devices:
2019-04-20 14:44:30.820029: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): &lt;undefined&gt;, &lt;u
ndefined&gt;
2019-04-20 14:44:30.825689: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic li
brary libcuda.so.1
2019-04-20 14:44:31.062487: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x55b6fc634120 executing computa
tions on platform CUDA. Devices:
2019-04-20 14:44:31.062554: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): GeForce RTX 208
0 Ti, Compute Capability 7.5
2019-04-20 14:44:31.063894: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 0 with properties:
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.635
pciBusID: 0000:19:00.0
totalMemory: 10.73GiB freeMemory: 10.57GiB
2019-04-20 14:44:31.063942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Adding visible gpu devices: 0
2019-04-20 14:44:31.064034: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic li
brary libcudart.so.10.0
2019-04-20 14:44:31.067082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor wi
th strength 1 edge matrix:
2019-04-20 14:44:31.067114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021]      0
2019-04-20 14:44:31.067130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0:   N
2019-04-20 14:44:31.068283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:local
host/replica:0/task:0/device:GPU:0 with 10284 MB memory) -&gt; physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id
: 0000:19:00.0, compute capability: 7.5)
2019-04-20 14:44:33.628228: W tensorflow/core/common_runtime/base_collective_executor.cc:214] BaseCollectiveExecutor::Star
tAbort Invalid argument: During Variant Host-&gt;Device Copy: non-DMA-copy attempted of tensor type: string
         [[{{node img_1/encode_each_image/while/body/_1/TensorArrayV2Write/TensorListSetItem/_54}}]]
         [[img_1/encode_each_image/while/loop_body_control/_19/_33]]
2019-04-20 14:44:33.628374: W tensorflow/core/common_runtime/base_collective_executor.cc:214] BaseCollectiveExecutor::Star
tAbort Invalid argument: During Variant Host-&gt;Device Copy: non-DMA-copy attempted of tensor type: string
         [[{{node img_1/encode_each_image/while/body/_1/TensorArrayV2Write/TensorListSetItem/_54}}]]
2019-04-20 14:44:33.628468: E tensorflow/core/common_runtime/process_function_library_runtime.cc:764] Component function e
xecution failed: Invalid argument: During Variant Host-&gt;Device Copy: non-DMA-copy attempted of tensor type: string
         [[{{node img_1/encode_each_image/while/body/_1/TensorArrayV2Write/TensorListSetItem/_54}}]]
         [[img_1/encode_each_image/while/loop_body_control/_19/_33]]
2019-04-20 14:44:33.628456: E tensorflow/core/common_runtime/process_function_library_runtime.cc:764] Component function e
xecution failed: Invalid argument: During Variant Host-&gt;Device Copy: non-DMA-copy attempted of tensor type: string
         [[{{node img_1/encode_each_image/while/body/_1/TensorArrayV2Write/TensorListSetItem/_54}}]]
Traceback (most recent call last):
  File "test.py", line 21, in &lt;module&gt;
    run(img, i)
  File "/home/swang150/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/eager/def_function.
py", line 438, in __call__
    return self._stateless_fn(*args, **kwds)
  File "/home/swang150/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/eager/function.py",
 line 1288, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File "/home/swang150/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/eager/function.py",
 line 574, in _filtered_call
    (t for t in nest.flatten((args, kwargs))
  File "/home/swang150/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/eager/function.py",
 line 627, in _call_flat
    outputs = self._inference_function.call(ctx, args)
  File "/home/swang150/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/eager/function.py",
 line 415, in call
    ctx=ctx)
  File "/home/swang150/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/eager/execute.py",
line 66, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File "&lt;string&gt;", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: During Variant Host-&gt;Device Copy: non-DMA-copy attempted of
tensor type: string
         [[{{node img_1/encode_each_image/while/body/_1/TensorArrayV2Write/TensorListSetItem/_54}}]]
         [[img_1/encode_each_image/while/loop_body_control/_19/_33]] [Op:__inference_run_343]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='ipod825' date='2019-04-22T22:45:41Z'>
		&lt;denchmark-link:https://github.com/ipod825&gt;@ipod825&lt;/denchmark-link&gt;
 I ran the code in Google colab without any error. Could you try with Google colab and see whether issue persists there. If it doesn't persist in Google colab, then try to upgrade your TF2.0 and run the code again. Thanks!
&lt;denchmark-code&gt;!pip install tensorflow==2.0.0-alpha0
import tensorflow as tf
from tensorflow.keras import layers

H, W, C = 10, 10, 3
imgs = tf.zeros([10, H, W, C])
ds = tf.data.Dataset.from_tensor_slices(imgs)
ds = ds.batch(2)
conv = layers.Conv2D(32, (4, 4), strides=(2, 2), padding='same')


@tf.function
def run(img, i):
    conv(img)
    tf.summary.image('img', img, i)


if __name__ == "__main__":
    train_summary_writer = tf.summary.create_file_writer('/tmp/testsummary')
    with train_summary_writer.as_default():
        for i, img in enumerate(ds):
            run(img, i)
            print(i)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='ipod825' date='2019-04-22T23:13:33Z'>
		You need to run it on GPU.
!pip install tensorflow-gpu==2.0.0-alpha0
import tensorflow as tf
from tensorflow.keras import layers

H, W, C = 10, 10, 3
imgs = tf.zeros([10, H, W, C])
ds = tf.data.Dataset.from_tensor_slices(imgs)
ds = ds.batch(2)
conv = layers.Conv2D(32, (4, 4), strides=(2, 2), padding='same')


@tf.function
def run(img, i):
    conv(img)
    tf.summary.image('img', img, i)


if __name__ == "__main__":
    train_summary_writer = tf.summary.create_file_writer('/tmp/testsummary')
    with tf.device('/gpu:0'):
      with train_summary_writer.as_default():
          for i, img in enumerate(ds):
              run(img, i)
              print(i)
		</comment>
		<comment id='3' author='ipod825' date='2019-07-01T01:33:47Z'>
		sorry about my poor English.  I have the same problem.But I found a solution.
I'm using Nvidia 2080Ti , tf-nightly-gpu-2.0-preview. python3.7.3 ,ubuntu 19.04
When I used tf.summary.image("gen", generated_images, max_outputs=25, step=0), I got error :During Variant Host-&gt;Device Copy: non-DMA-copy attempted of tensor type: string.
If I wrote like this:
&lt;denchmark-code&gt;with tf.device("cpu:0"): &lt;&lt;-- add this line
   with log["writer"].as_default():
     tf.summary.image("gen", generated_images, max_outputs=25, step=0)
&lt;/denchmark-code&gt;

everything is fine.
		</comment>
		<comment id='4' author='ipod825' date='2019-08-13T22:51:14Z'>
		Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!
		</comment>
		<comment id='5' author='ipod825' date='2019-08-13T22:51:16Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=28007&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=28007&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='ipod825' date='2019-08-13T23:14:54Z'>
		Hi &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
, the snippet I wrote still failed with the same error message in colab (using gpu accelerator). Even 2.0-beta version failed too. Could you please describe how you make it work?
		</comment>
		<comment id='7' author='ipod825' date='2019-08-15T22:42:44Z'>
		&lt;denchmark-link:https://github.com/ipod825&gt;@ipod825&lt;/denchmark-link&gt;
 As mentioned by &lt;denchmark-link:https://github.com/faruba&gt;@faruba&lt;/denchmark-link&gt;
 I changed a line in your code from  to . It works without any error. Please check the &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/295c237a134755b28f94af9ce040af35/tf_28007_summary.ipynb&gt;gist here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='8' author='ipod825' date='2019-08-15T23:37:17Z'>
		Well, it seems to be just a workaround to me. The main issue here is that the summary operation raises error when running on GPU. Forcing the operation to run on CPU doesn't really solve the problem but just ignores the problem. I don't know how summary operation works, probably even if running under GPU, it would still copy tensor back to CPU memory (which then would be similar to explicitly asking it to run on CPU). Even if this is the case (if not, we lose some efficiency), from an API point of view, I don't think this issue is solved as someone might encounter the same problem and don't know why it happen and how to solve it without bumping into this thread.
		</comment>
		<comment id='9' author='ipod825' date='2019-08-19T11:43:32Z'>
		&lt;denchmark-link:https://github.com/ipod825&gt;@ipod825&lt;/denchmark-link&gt;
 I have the same problem (did try tf2.0 alphas and betas) and agree that assigning the summary op to /cpu:0 is only a workaround. Moreover, the fix is not working for me if I build from the r2.0 branch from source.
It would be nice if this issue would be reopened, so the problem can be solved.
I took a look at &lt;denchmark-link:https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/image/summary_v2.py&gt;github&lt;/denchmark-link&gt;
: the map_fn in line 75 is causing the issues.
		</comment>
		<comment id='10' author='ipod825' date='2019-08-19T16:31:29Z'>
		&lt;denchmark-link:https://github.com/ipod825&gt;@ipod825&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/loffermann&gt;@loffermann&lt;/denchmark-link&gt;
 Sorry for closing. Reopened. Thanks!
		</comment>
		<comment id='11' author='ipod825' date='2019-09-18T17:38:21Z'>
		I ran to a similar issue, when I tried to save images to tf.summary.image using MirroredStrategy. Oddly enough, when debugging using tf.config.experimental_run_functions_eagerly(True) this error does not occur.
(0) Invalid argument:  2 root error(s) found.
(0) Invalid argument: During Variant Host-&gt;Device Copy: non-DMA-copy attempted of tensor type: string
(1) Invalid argument: During Variant Host-&gt;Device Copy: non-DMA-copy attempted of tensor type: string
0 successful operations.
0 derived errors ignored.
[[{{node training-image/encode_each_image/while/body/_1/TensorArrayV2Write/TensorListSetItem/_62}}]]
[[Func/training-image/encode_each_image/while/body/_1/input/_50/_60]]
(1) Invalid argument:  2 root error(s) found.
(0) Invalid argument: During Variant Host-&gt;Device Copy: non-DMA-copy attempted of tensor type: string
(1) Invalid argument: During Variant Host-&gt;Device Copy: non-DMA-copy attempted of tensor type: string
0 successful operations.
0 derived errors ignored.
[[{{node training-image/encode_each_image/while/body/_1/TensorArrayV2Write/TensorListSetItem/_62}}]]
0 successful operations.
0 derived errors ignored.
Encountered when executing an operation using EagerExecutor. This error cancels all future operations and poisons their output tensors. [Op:__inference_train_step_16043]
Function call stack:
train_step -&gt; train_step
		</comment>
		<comment id='12' author='ipod825' date='2019-10-10T12:26:34Z'>
		I also ran into this issue. Here's a fairly minimal piece of code that reproduces it:
import tensorflow.compat.v2 as tf
tf.enable_v2_behavior()

def decode_png(data):
  return tf.image.decode_png(data)

@tf.function  # &lt;= No exception if you comment this line out
def decode_all(images):
  return tf.map_fn(decode_png, images, dtype=tf.uint8)

img = b'\x89PNG\r\n\x1a\n\x00\x00\x00\rIHDR\x00\x00\x00\x01\x00\x00\x00\x01\x08\x06\x00\x00\x00\x1f\x15\xc4\x89\x00\x00\x00\rIDATx\xdac\xfc\xcf\xf0\xbf\x1e\x00\x06\x83\x02\x7f\x94\xad\xd0\xeb\x00\x00\x00\x00IEND\xaeB`\x82'
images = tf.constant([img, img])
decode_all(images)
and here's the full stackstrace:
&lt;denchmark-code&gt;InvalidArgumentError                      Traceback (most recent call last)
&lt;ipython-input-72-a59f4c54298a&gt; in &lt;module&gt;()
     11 img = b'\x89PNG\r\n\x1a\n\x00\x00\x00\rIHDR\x00\x00\x00\x01\x00\x00\x00\x01\x08\x06\x00\x00\x00\x1f\x15\xc4\x89\x00\x00\x00\rIDATx\xdac\xfc\xcf\xf0\xbf\x1e\x00\x06\x83\x02\x7f\x94\xad\xd0\xeb\x00\x00\x00\x00IEND\xaeB`\x82'
     12 images = tf.constant([img, img])
---&gt; 13 decode_all(images)

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)
    465               *args, **kwds)
    466       # If we did not create any variables the trace we have is good enough.
--&gt; 467       return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access
    468 
    469     def fn_with_cond(*inner_args, **inner_kwds):

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in _filtered_call(self, args, kwargs)
   1139          if isinstance(t, (ops.Tensor,
   1140                            resource_variable_ops.BaseResourceVariable))),
-&gt; 1141         self.captured_inputs)
   1142 
   1143   def _call_flat(self, args, captured_inputs, cancellation_manager=None):

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1222     if executing_eagerly:
   1223       flat_outputs = forward_function.call(
-&gt; 1224           ctx, args, cancellation_manager=cancellation_manager)
   1225     else:
   1226       gradient_name = self._delayed_rewrite_functions.register()

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in call(self, ctx, args, cancellation_manager)
    509               inputs=args,
    510               attrs=("executor_type", executor_type, "config_proto", config),
--&gt; 511               ctx=ctx)
    512         else:
    513           outputs = execute.execute_with_cancellation(

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     65     else:
     66       message = e.message
---&gt; 67     six.raise_from(core._status_to_exception(e.code, message), None)
     68   except TypeError as e:
     69     keras_symbolic_tensors = [

/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  2 root error(s) found.
  (0) Invalid argument: During Variant Host-&gt;Device Copy: non-DMA-copy attempted of tensor type: string
  (1) Invalid argument: During Variant Host-&gt;Device Copy: non-DMA-copy attempted of tensor type: string
0 successful operations.
0 derived errors ignored.
	 [[{{node map/TensorArrayUnstack/TensorListFromTensor/_12}}]]
  (1) Invalid argument:  2 root error(s) found.
  (0) Invalid argument: During Variant Host-&gt;Device Copy: non-DMA-copy attempted of tensor type: string
  (1) Invalid argument: During Variant Host-&gt;Device Copy: non-DMA-copy attempted of tensor type: string
0 successful operations.
0 derived errors ignored.
	 [[{{node map/TensorArrayUnstack/TensorListFromTensor/_12}}]]
	 [[Func/map/while/body/_1/input/_43/_24]]
0 successful operations.
0 derived errors ignored. [Op:__inference_decode_all_20554]

Function call stack:
decode_all -&gt; decode_all
&lt;/denchmark-code&gt;

I ran this on Colab with a GPU Runtime, using TF 1.15.0rc3. It will probably bomb as well on TF 2.0.0 but I haven't tried.
		</comment>
		<comment id='13' author='ipod825' date='2019-10-15T03:57:00Z'>
		A modification of the earlier gist posted by &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 causes the error to occur again. &lt;denchmark-link:https://colab.research.google.com/gist/rharish101/23772d541af0c8144348aa1bd831c8b9/tf_28007_summary.ipynb&gt;Here&lt;/denchmark-link&gt;
 it is.
I am using both the GPU and CPU inside the decorated function because there might be a computationally expensive part that I have to run on the GPU. The CPU is used to run the summary ops as a workaround to this bug. However, the error still occurs. The only way to get around this is to run the entire function on the CPU when calling it, as &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 's gist, which is not ideal when I want to train a network.
		</comment>
		<comment id='14' author='ipod825' date='2020-03-18T20:20:19Z'>
		This should be fixed for simple tf.map_fn example, however the underlying problem is still there, and might be triggered in more complex use cases. A fix commit has a repro with explanation.
		</comment>
		<comment id='15' author='ipod825' date='2020-04-16T07:47:24Z'>
		Also having the same issue using TF 2.1. Works fine on a machine with just a CPU, but fails on a machine with a GPU, even when using with tf.device('/cpu:0'). Would appreciate an update this asap.
		</comment>
		<comment id='16' author='ipod825' date='2020-04-17T07:35:29Z'>
		&lt;denchmark-h:h3&gt;I have the same error, but error raised when I use tensorflow serving（GPU VERSION）&lt;/denchmark-h&gt;

my model incude function below:
&lt;denchmark-code&gt;def preprocess_and_decode(img_str, new_shape=target_size):
        img = tf.io.decode_base64(img_str)
        img = tf.image.decode_jpeg(img, channels=3)
        img = tf.image.resize(img, new_shape, method=method)
        return img

input64 = tf.keras.layers.Input(shape=(1,), dtype="string", name=input_name)
ouput_tensor = tf.keras.layers.Lambda(
        lambda img: tf.map_fn(lambda im: preprocess_and_decode(im[0]), img, dtype="float32"))(input64)
&lt;/denchmark-code&gt;

It's ok to delopy with serving cpu, but got error with gpu like below:
'{ "error": "2 root error(s) found.\\n  (0) Invalid argument: 2 root error(s) found.\\n  (0) Invalid argument: During Variant Host-&gt;Device Copy: non-DMA-copy attempted of tensor type: string\\n  (1) Invalid argument: During Variant Host-&gt;Device Copy: non-DMA-copy attempted of tensor type: string\\n0 successful operations.\\n0 derived errors ignored.\\n\\t [[{{node model_11/lambda_16/map/TensorArrayUnstack/TensorListFromTensor}}]]\\n  (1) Invalid argument: 2 root error(s) found.\\n  (0) Invalid argument: During Variant Host-&gt;Device Copy: non-DMA-copy attempted of tensor type: string\\n  (1) Invalid argument: During Variant Host-&gt;Device Copy: non-DMA-copy attempted of tensor type: string\\n0 successful operations.\\n0 derived errors ignored.\\n\\t [[{{node model_11/lambda_16/map/TensorArrayUnstack/TensorListFromTensor}}]]\\n\\t [[Func/StatefulPartitionedCall/StatefulPartitionedCall/model_11/lambda_16/map/while/body/_887/input/_935/_935]]\\n0 successful operations.\\n0 derived errors ignored." }' 
Any solutions for this?
		</comment>
		<comment id='17' author='ipod825' date='2020-04-17T08:03:57Z'>
		I am also facing this issue on GPU (no error on CPU) when using to map_fn for string tensor (with float tensor all right):
&lt;denchmark-code&gt;def process_string(sample):
    # here i want to write something to file, but even with identity
    # function I get "non-DMA-copy attempted..." error
    return sample

@tf.function
def f(self, y_true, y_pred):
    string_tensor = y_true["path"]  # Tensor("data_batch_19:0", shape=(2,), dtype=string)
    tf.map_fn(process_string, string_tensor)
&lt;/denchmark-code&gt;

I do not know why, but for me manually placing map_fn on cpu AND making mock return from tf.function helped:
&lt;denchmark-code&gt;@tf.function
def f(self, y_true, y_pred):
    string_tensor = y_true["path"]  # Tensor("data_batch_19:0", shape=(2,), dtype=string)
    with tf.device("/cpu:0"):
        tf.map_fn(process_string, string_tensor)
    return y_pred  # do not work for me without return something
&lt;/denchmark-code&gt;

I am using tf2.0.1
		</comment>
		<comment id='18' author='ipod825' date='2020-04-17T10:18:33Z'>
		
I am also facing this issue on GPU (no error on CPU) when using to map_fn for string tensor (with float tensor all right):
def process_string(sample):
    # here i want to write something to file, but even with identity
    # function I get "non-DMA-copy attempted..." error
    return sample

@tf.function
def f(self, y_true, y_pred):
    string_tensor = y_true["path"]  # Tensor("data_batch_19:0", shape=(2,), dtype=string)
    tf.map_fn(process_string, string_tensor)

I do not know why, but for me manually placing map_fn on cpu AND making mock return from tf.function helped:
@tf.function
def f(self, y_true, y_pred):
    string_tensor = y_true["path"]  # Tensor("data_batch_19:0", shape=(2,), dtype=string)
    with tf.device("/cpu:0"):
        tf.map_fn(process_string, string_tensor)
    return y_pred  # do not work for me without return something

I am using tf2.0.1

is this valid in tensorflow serving gpu?
		</comment>
		<comment id='19' author='ipod825' date='2020-04-17T10:24:23Z'>
		&lt;denchmark-link:https://github.com/Lannister-Xiaolin&gt;@Lannister-Xiaolin&lt;/denchmark-link&gt;
, I have not tried. I run program locally, without wrapping map_fn in keras.layer (as in your example).
		</comment>
		<comment id='20' author='ipod825' date='2020-04-19T23:33:50Z'>
		Hi, sorry for my poor English. I solved this problem by log the dataset images outside the @tf.function.
		</comment>
		<comment id='21' author='ipod825' date='2020-04-20T02:40:57Z'>
		&lt;denchmark-link:https://github.com/TIGERCHANG123&gt;@TIGERCHANG123&lt;/denchmark-link&gt;
 - interesting! can you please share a code example?
		</comment>
		<comment id='22' author='ipod825' date='2020-04-20T08:47:09Z'>
		I'm having the same issue using map_fn to decode a image list. Even when using my cpu.
&lt;denchmark-code&gt;def decode_image_fn(x):
  t = tf.image.decode_image(x, 3)
  t.set_shape((224, 224, 3))
  return t

frames = tf.map_fn(decode_image_fn, frames_raw, dtype=tf.uint8)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='23' author='ipod825' date='2020-04-22T21:34:17Z'>
		Im having this issue too with TF2.2rc3
		</comment>
		<comment id='24' author='ipod825' date='2020-04-27T13:55:35Z'>
		
I am also facing this issue on GPU (no error on CPU) when using to map_fn for string tensor (with float tensor all right):
def process_string(sample):
    # here i want to write something to file, but even with identity
    # function I get "non-DMA-copy attempted..." error
    return sample

@tf.function
def f(self, y_true, y_pred):
    string_tensor = y_true["path"]  # Tensor("data_batch_19:0", shape=(2,), dtype=string)
    tf.map_fn(process_string, string_tensor)

I do not know why, but for me manually placing map_fn on cpu AND making mock return from tf.function helped:
@tf.function
def f(self, y_true, y_pred):
    string_tensor = y_true["path"]  # Tensor("data_batch_19:0", shape=(2,), dtype=string)
    with tf.device("/cpu:0"):
        tf.map_fn(process_string, string_tensor)
    return y_pred  # do not work for me without return something

I am using tf2.0.1

Hi thanks this trick works for my case also
		</comment>
		<comment id='25' author='ipod825' date='2020-07-02T04:59:25Z'>
		&lt;denchmark-link:https://github.com/ipod825&gt;@ipod825&lt;/denchmark-link&gt;

Is this still an issue?
I was able to run the code without any issues with TF v2.2, please find the gist of it &lt;denchmark-link:https://colab.research.google.com/gist/saikumarchalla/c5aabfe8ba2812abe71c2380c13f73a9/-28007.ipynb&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='26' author='ipod825' date='2020-07-02T07:19:01Z'>
		&lt;denchmark-link:https://github.com/saikumarchalla&gt;@saikumarchalla&lt;/denchmark-link&gt;
 Yes, here's a &lt;denchmark-link:https://colab.research.google.com/drive/1HgUZDuao1MW7pAT9qUt3wGNDBdONfGde?usp=sharing&gt;Colab notebook&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='27' author='ipod825' date='2020-08-06T18:36:52Z'>
		&lt;denchmark-link:https://github.com/rharish101&gt;@rharish101&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/saikumarchalla&gt;@saikumarchalla&lt;/denchmark-link&gt;
 I think this has been fixed. I just ran the colab from &lt;denchmark-link:https://github.com/rharish101&gt;@rharish101&lt;/denchmark-link&gt;
 with tf nightly, and am not seeing an error. Please confirm.
		</comment>
		<comment id='28' author='ipod825' date='2020-08-07T07:01:16Z'>
		&lt;denchmark-link:https://github.com/nikitamaia&gt;@nikitamaia&lt;/denchmark-link&gt;
 Yes, this is fixed on TensorFlow 2.3.0. I tested this on the colab and also on my local machine (running TF 2.3.0 on Arch Linux).
		</comment>
		<comment id='29' author='ipod825' date='2020-08-07T14:34:44Z'>
		Closing this issue now since the bug has been fixed.
		</comment>
		<comment id='30' author='ipod825' date='2020-08-07T14:34:46Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28007&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28007&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>