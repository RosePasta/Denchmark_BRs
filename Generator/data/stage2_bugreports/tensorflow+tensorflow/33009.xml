<bug id='33009' author='MProx' open_date='2019-10-03T09:08:05Z' closed_time='2020-01-11T09:18:28Z'>
	<summary>Memory leak</summary>
	<description>
System information
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, see below
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 (also demonstrated in Windows 7)
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0 (problem disappears on v1.14.0-rc1-22-gaf24dc91b5 1.14.0)
- Python version: 3.6.8
- CUDA/cuDNN version: Cuda v10.0, CuDNN v7.6.2.24
- GPU model and memory: Nvidia GeForce 840M, but problem persists in non-GPU version
Describe the current behavior
When creating a trivially simple model and then entering a loop that calls predict() with dummy input, memory consumption increases indefinitely over time. On my system, a model with a single hidden layer of only 32 nodes will consume all available system RAM (&gt;10gb) after only 10 minutes. The problem happens on v2.0 (GPU or CPU) of tensorflow, but disappears when running identical code on v1.14.
Describe the expected behavior
Expect memory consumption to quickly stabilize but it never does.
Code to reproduce the issue
&lt;denchmark-code&gt;from tensorflow.keras import Input, Model
from tensorflow.keras.layers import Dense
import numpy as np

# Build model
In = Input(shape=(10,))
x = Dense(32)(In)
Out = Dense(2)(x)

# Compile
model = Model(inputs=In, outputs=Out)
model.compile(optimizer='adam', loss='mse')        

# Create dummy input data
fake_data = np.random.uniform(low=0, high=1.0, size=(1, 10, ))

while True:
    # Repeatedly predict:
    model.predict(fake_data) # No memory leak if this line is replaced with "pass"
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='MProx' date='2019-10-03T09:28:08Z'>
		I have also experienced this critical issue.
		</comment>
		<comment id='2' author='MProx' date='2019-10-03T14:59:09Z'>
		I have managed to get around this error by using  instead of . This returns an object of type  - not a numpy array as claimed in &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict_on_batch&gt;the docs&lt;/denchmark-link&gt;
 - but it can be cast by calling  to get the output I want.
Side note: I also noticed a similar memory leak problem with calling model.fit() in a loop, albeit with a slower memory accumulation, but this can be fixed in a similar way using model.train_on_batch().
		</comment>
		<comment id='3' author='MProx' date='2019-10-03T15:27:18Z'>
		I think same error here.
This code (&lt;denchmark-link:https://github.com/ipsec/dqn_tf2/blob/master/nchain-train.py&gt;https://github.com/ipsec/dqn_tf2/blob/master/nchain-train.py&lt;/denchmark-link&gt;
) not finishing.
		</comment>
		<comment id='4' author='MProx' date='2019-10-04T04:52:34Z'>
		&lt;denchmark-link:https://github.com/MProx&gt;@MProx&lt;/denchmark-link&gt;
 ,
Can you confirm if the issue is resolved with the workaround? Thanks!
		</comment>
		<comment id='5' author='MProx' date='2019-10-04T05:07:52Z'>
		Hi. Yes it did. I tested it overnight with no leak. I'm not sure why the original functions do not work as expected, but the workaround has fixed my problem.
		</comment>
		<comment id='6' author='MProx' date='2019-10-04T07:56:44Z'>
		This issue is NOT resolved. I use predict_on_batch and still get an OOM error after some time of processing data.
		</comment>
		<comment id='7' author='MProx' date='2019-10-04T08:33:02Z'>
		Interesting. I think that OOM is telling you your GPU memory is being depleted, and that sounds like a different problem because mine was filling up system RAM, not GPU. Using predict_on_batch() and train_on_batch(), I ran ~800k iterations of my model over 10 hours last night, and it seemed to work fine.
Another thing is that I had to set the environment variable TF_FORCE_GPU_ALLOW_GROWTH=true to avoid tensorflow errors on startup. Maybe try that?
		</comment>
		<comment id='8' author='MProx' date='2019-10-04T08:44:07Z'>
		Yeah, I don't have any critical errors when running on CPU. It's definitely GPU memory that's leaky. The GPU memory doesn't deplete immediately, but only after some number of calls to predict_on_batch. I am able to train using fit with no problems, which runs over that same data set over many epochs.
		</comment>
		<comment id='9' author='MProx' date='2019-10-04T08:50:54Z'>
		I can confirm actually, that when run on CPU, the leak still persists.
		</comment>
		<comment id='10' author='MProx' date='2019-10-04T09:03:47Z'>
		In that case I'm going to re-open this issue so that maybe the TF team can help you.
		</comment>
		<comment id='11' author='MProx' date='2019-10-04T09:39:47Z'>
		Issue replicating for TF-2.0, kindly find the &lt;denchmark-link:https://colab.sandbox.google.com/gist/oanush/07da0eef3fbfbd8ff74e6c840ad5b6d5/33009.ipynb&gt;gist&lt;/denchmark-link&gt;
 of colab.Thanks!
		</comment>
		<comment id='12' author='MProx' date='2019-10-07T22:14:06Z'>
		This is an issue with how certain object lifetimes are being managed in the tf function cache. Behind the scenes Model.predict is creating some functions which aren't being spun down properly, which is why there is a small but consistent leak each iteration. We are currently working on a fix.
		</comment>
		<comment id='13' author='MProx' date='2019-10-17T22:05:11Z'>
		Oh, I see &lt;denchmark-link:https://github.com/robieta&gt;@robieta&lt;/denchmark-link&gt;
 already &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/33009#issuecomment-539226841&gt;pinned the issue&lt;/denchmark-link&gt;
. Apologies for the extra tests, we are expecting this to be fixed on master soon (if not already)
		</comment>
		<comment id='14' author='MProx' date='2019-10-17T22:10:50Z'>
		No worries about the tests. I just hope a new release with the fix comes out soon.
		</comment>
		<comment id='15' author='MProx' date='2019-10-17T22:21:40Z'>
		It's worth noting that the solution that I was referring to will only fix CPU OOM; it wouldn't explain a GPU leak.
		</comment>
		<comment id='16' author='MProx' date='2019-10-24T23:48:32Z'>
		I was experiencing the same issue on the 2.0.0 docker (latest-gpu-py3) image; I changed to the nightly image (tensorflow/tensorflow:nightly-gpu-py3 to be exact) and the problem seems to be fixed, no more memory leak!
So anyone else here looking for a workaround until the changes are reflected in the stable release, I'd try this out
		</comment>
		<comment id='17' author='MProx' date='2019-11-11T10:24:41Z'>
		Avoiding model.predict and using model.predict_on_batch solves the Memory Error for me. Here's an example to create batched predictions on your test set.
&lt;denchmark-code&gt;# custom batched prediction loop to avoid memory leak issues for now in the model.predict call
y_pred_probs = np.empty([len(X_test), VOCAB_SIZE], dtype=np.float32)  # pre-allocate required memory for array for efficiency

BATCH_INDICES = np.arange(start=0, stop=len(X_test), step=BATCH_SIZE)  # row indices of batches
BATCH_INDICES = np.append(BATCH_INDICES, len(X_test))  # add final batch_end row

for index in np.arange(len(BATCH_INDICES) - 1):
    batch_start = BATCH_INDICES[index]  # first row of the batch
    batch_end = BATCH_INDICES[index + 1]  # last row of the batch
    y_pred_probs[batch_start:batch_end] = model.predict_on_batch(X_test[batch_start:batch_end])
&lt;/denchmark-code&gt;

(Note that if only pre-allocating the results array already results in a MemoryError then simply the array does not fit in your available memory regardless of the memory leak issue.)
		</comment>
		<comment id='18' author='MProx' date='2019-11-11T20:56:23Z'>
		model.predict_on_batch solved this problem for me too
		</comment>
		<comment id='19' author='MProx' date='2019-11-12T13:51:17Z'>
		Same issue with TF 2.0 stable
Solved with tf.compat.v1.disable_eager_execution()
		</comment>
		<comment id='20' author='MProx' date='2019-11-22T12:16:34Z'>
		I got the same problem both on rc2 and on official 2.0.0 release.
model.predict_on_batch workaround worked for me
		</comment>
		<comment id='21' author='MProx' date='2019-12-12T23:03:38Z'>
		For anyone visiting this thread with similar issues of memory leakage when invoking  in a loop, the work-around for this case is similar to the one proposed by &lt;denchmark-link:https://github.com/MProx&gt;@MProx&lt;/denchmark-link&gt;
, use  instead.
		</comment>
		<comment id='22' author='MProx' date='2019-12-29T01:18:28Z'>
		Building a model by subclassing the Model class can avoid this problem.
&lt;denchmark-code&gt;class MyModel(tf.keras.Model):
    def __init__(self):
        super(MyModel, self).__init__()
        self.dense1 = tf.keras.layers.Dense(32, activation=tf.nn.relu)
        self.dense2 = tf.keras.layers.Dense(2, activation=tf.nn.softmax)
    def call(self, inputs):
        x = self.dense1(inputs)
        return self.dense2(x)
model = MyModel()
model.compile(optimizer='adam', loss='mse')        

fake_data = np.random.uniform(low=0, high=1.0, size=(1, 10, ))
while True:
    # Repeatedly predict:
    # model.predict(fake_data) # memory leak will happen if execute this line
    model(fake_data) # There is no memory leak
&lt;/denchmark-code&gt;

		</comment>
		<comment id='23' author='MProx' date='2020-01-10T21:10:49Z'>
		&lt;denchmark-link:https://github.com/MProx&gt;@MProx&lt;/denchmark-link&gt;
, about a month ago we added a fix for memory leak and there's a possibility it has fixed this. Can you try  and see if it resolves your issue?
		</comment>
		<comment id='24' author='MProx' date='2020-01-11T09:18:28Z'>
		Hi. I have done what you ask and tested tf-nightly with my trivial code example from the original issue above, and it does indeed seem to fix the problem. I also tried it in a different application that I've been using the predict_on_batch workaround with, and there seem to be no memory leaks there either. So yes, the leak seems to have been dealt with.
Thank you!
		</comment>
		<comment id='25' author='MProx' date='2020-01-11T09:18:29Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33009&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33009&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='26' author='MProx' date='2020-01-15T19:25:09Z'>
		Thanks &lt;denchmark-link:https://github.com/MProx&gt;@MProx&lt;/denchmark-link&gt;
 for the updates!
		</comment>
		<comment id='27' author='MProx' date='2020-01-17T05:27:29Z'>
		Hi,
I am trying to call model.predict() on CPU (model trained on GPU) multiple times and I observe RAM memory leak. clear_session with model reload doesn't solve the issue. model.predict_on_batch() fails to solve the issue as well. Is there a workaround for this issue? I am using TF 1.13 and Python 3.6. Have been struggling to solve this problem since so long. Kinda need help.
		</comment>
		<comment id='28' author='MProx' date='2020-01-17T06:38:15Z'>
		Does the issue persist of you use the minimal code example I supplied in
the original issue post?
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Fri, Jan 17, 2020, 05:27 Yash Ubale ***@***.***&gt; wrote:
 Hi,

 I am trying to call model.predict() on CPU (model trained on GPU)
 multiple times and I observe RAM memory leak. clear_session with model
 reload doesn't solve the issue. model.predict_on_batch() fails to solve
 the issue as well. Is there a workaround for this issue? I am using TF 1.13
 and Python 3.6. Have been struggling to solve this problem since so long.
 Kinda need help.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#33009?email_source=notifications&amp;email_token=AIITZUJCKIFIPYWKAKTL7YLQ6E6URA5CNFSM4I5AMYCKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEJGPZCY#issuecomment-575470731&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/AIITZUNSS6RQ32Q3GPU4HMDQ6E6URANCNFSM4I5AMYCA&gt;
 .



		</comment>
		<comment id='29' author='MProx' date='2020-01-17T08:39:05Z'>
		&lt;denchmark-link:https://github.com/MProx&gt;@MProx&lt;/denchmark-link&gt;

Yes, the original code as shown below causes memory leak.
&lt;denchmark-code&gt;from tensorflow.keras import Input, Model
from tensorflow.keras.layers import Dense
import numpy as np

# Build model
In = Input(shape=(10,))
x = Dense(32)(In)
Out = Dense(2)(x)

# Compile
model = Model(inputs=In, outputs=Out)
model.compile(optimizer='adam', loss='mse')        

# Create dummy input data
fake_data = np.random.uniform(low=0, high=1.0, size=(1, 10, ))

while True:
    # Repeatedly predict:
    model.predict(fake_data) # No memory leak if this line is replaced with "pass"
&lt;/denchmark-code&gt;

Below is the memory consumption over time (it increases initially which is expected, but after certain point when it's supposed to be constant, it rather increases although by a very small amount, I ran it for only 30 minutes)
&lt;denchmark-code&gt;CMDLINE python test.py
CMDLINE python test.py
MEM 0.609375 1579247671.2321
MEM 22.660156 1579247671.3397
MEM 68.218750 1579247671.4467
MEM 104.500000 1579247671.5552
MEM 114.675781 1579247671.6605
MEM 124.125000 1579247671.7657
MEM 131.191406 1579247671.8710
MEM 146.972656 1579247671.9783
MEM 156.164062 1579247672.0838
MEM 168.886719 1579247672.1893
MEM 182.667969 1579247672.2945
MEM 192.363281 1579247672.4000
MEM 202.539062 1579247672.5052
MEM 211.316406 1579247672.6102
MEM 211.968750 1579247672.7151
MEM 215.250000 1579247672.8199
MEM 215.464844 1579247672.9256
MEM 215.738281 1579247673.0316
MEM 215.890625 1579247673.1366
MEM 216.050781 1579247673.2414
MEM 216.468750 1579247673.3461
MEM 221.535156 1579247673.4508
MEM 226.820312 1579247673.5557
MEM 227.011719 1579247673.7651
MEM 227.261719 1579248604.1910
MEM 227.511719 1579249397.2364
MEM 228.515625 1579250955.6305
MEM 228.304688 1579251348.9152
&lt;/denchmark-code&gt;

		</comment>
		<comment id='30' author='MProx' date='2020-04-04T07:55:08Z'>
		I've had the same problem and none of the above solutions worked for me.
Ultimately, setting  in  fixed it.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/22098&gt;#22098&lt;/denchmark-link&gt;

		</comment>
		<comment id='31' author='MProx' date='2020-04-15T12:17:56Z'>
		Still getting this error with tf-nightly (2.2.0-dev20200415)
		</comment>
		<comment id='32' author='MProx' date='2020-04-17T07:37:18Z'>
		&lt;denchmark-link:https://github.com/rchao&gt;@rchao&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/MProx&gt;@MProx&lt;/denchmark-link&gt;
 Still getting this error with TensorFlow version 2.2.0-rc3 or tf-nightly (2.2.0-dev20200416) on Google Colab. See this &lt;denchmark-link:https://colab.research.google.com/gist/bentyeh/5a624a95f8614a7d140592e62b7967fa/tf_keras_issue33009.ipynb&gt;gist&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='33' author='MProx' date='2020-05-04T12:15:18Z'>
		Same getting error even after using train_on_batch i am using custom keras layers
		</comment>
		<comment id='34' author='MProx' date='2020-05-24T05:54:55Z'>
		The issue persists with tensorflow 2.2.0. Would be better if someone from tensorflow team made a statement about it. It severely hinders the ability to train for larger number of epochs
		</comment>
		<comment id='35' author='MProx' date='2020-08-21T07:38:55Z'>
		Hi,
I am trying to call model.predict() on CPU multiple times and I observe RAM memory leak. clear_session() with model reload and gc.collect() doesn't solve the issue. I ran the code on tensorflow 2.1 and 2.3 as well but issue still persists. Is there a workaround for this issue? I am using TF 1.14 and Python 3.6. Have been struggling to solve this problem since so long.
		</comment>
	</comments>
</bug>