<bug id='38924' author='ybNo1' open_date='2020-04-27T01:42:58Z' closed_time='2020-05-06T08:06:25Z'>
	<summary>mirroredstrategy not working? parallel GPUs working like serial</summary>
	<description>
I'm working with TF2.0
I find a situation confusing when using mirroredstrategy just the same as the tutorial:
&lt;denchmark-code&gt;strategy = tf.distribute.MirroredStrategy()

# Create a dataset
dataset = dataset_ops.Dataset.TFRecordDataset([
  "/a/1.tfr", "/a/2.tfr", "/a/3.tfr", "/a/4.tfr"])

# Distribute that dataset
dist_dataset = strategy.experimental_distribute_dataset(dataset)
# Iterate over the distributed dataset
for x in dist_dataset:
  # process dataset elements
  strategy.experimental_run_v2(train_step, args=(x,))
&lt;/denchmark-code&gt;

here's step_fn() in function train_step(), I made 2 time stamp in the code:
&lt;denchmark-code&gt;# @tf.function(experimental_relax_shapes=True)
def train_step(dist_inputs):
    def step_fn(inputs):
        (mel_specs, pred_inp, 
         spec_lengths, label_lengths, labels) = inputs
        with tf.GradientTape() as tape:
            outputs = model([mel_specs, pred_inp], 
                training=True)
            # adding line below
           begin_time = time.tme()
            loss = loss_fn(labels, outputs,
                spec_lengths, label_lengths)
            loss *= (1. / batch_size)

        if train_metrics is not None:
            metric_results = run_metrics(mel_specs, labels,
                metrics=train_metrics)
            metric_results = {name: result * (1. / max(len(gpus), 1)) for name, result in metric_results.items()}

        gradients = tape.gradient(loss, model.trainable_variables)

        # adding line below before update gradients
        end_time = time.time()
        print(begin_time, end_time, end_time-begin_time)
       optimizer.apply_gradients(zip(gradients, model.trainable_variables))

        return loss, metric_results
    losses, metrics_results = strategy.experimental_run_v2(step_fn, args=(dist_inputs,))
    mean_loss = strategy.reduce(
        tf.distribute.ReduceOp.SUM, losses, axis=0)
    mean_metrics = {name: strategy.reduce(
        tf.distribute.ReduceOp.SUM, result, axis=0) for name, result in metrics_results.items()}
    return mean_loss, mean_metrics
&lt;/denchmark-code&gt;

I found that if using 2 GPUs(say gpu0 and gpu1),
the two gpu printing time_stamp: bg1, ed1 and bg2, ed2
the four params relationship is like this:
bg1&lt;ed1&lt;bg2&lt;ed2
that is only when gpu0 finished inference, gpu1 start inferencing
Was the MirroredStrategy not working?
I commented the tf.function() ,was there any relationship?
when I not comment the tf.function(), the code ran into the ERROR: python segment fault (core dumped)...
someone can help me? thanks
	</description>
	<comments>
		<comment id='1' author='ybNo1' date='2020-05-03T23:06:44Z'>
		When not using tf.function, this is expected. We do not recommend using MirroredStrategy with eager op by op mode for now unless for debugging purposes.
When using with tf.function, the time you are printing is for the function tracing / graph construction, and is not representative of the actual execution time.
		</comment>
		<comment id='2' author='ybNo1' date='2020-05-06T08:06:25Z'>
		
When not using tf.function, this is expected. We do not recommend using MirroredStrategy with eager op by op mode for now unless for debugging purposes.
When using with tf.function, the time you are printing is for the function tracing / graph construction, and is not representative of the actual execution time.

thanks for reply!
		</comment>
		<comment id='3' author='ybNo1' date='2020-05-06T08:06:28Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38924&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38924&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>