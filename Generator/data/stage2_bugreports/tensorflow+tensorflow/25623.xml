<bug id='25623' author='cih9088' open_date='2019-02-08T17:13:50Z' closed_time='2019-02-22T18:35:13Z'>
	<summary>behaviour of tf.GPUOptions(allow_growth=False) seems different</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.5 LTS
TensorFlow installed from (source or binary): pip
TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0
Python version: 3.5.2:
CUDA/cuDNN version: cuda-9.0, cudnn-7.3.1
GPU model and memory: Titan Xp, Titan V

Describe the current behavior
In previous version, 1.10.0 wich I used, per_process_gpu_memory_fraction is occupied as soon as session is created.
However with this version (1.12.0), GPU memory occupied after a single sess.run().
&lt;denchmark-code&gt;config = tf.ConfigProto(
        allow_soft_placement=True,
        log_device_placement=False,
        gpu_options=tf.GPUOptions(force_gpu_compatible=True,
                                  per_process_gpu_memory_fraction=0.5,
                                  allow_growth=False))

a = tf.constant(1)
sess = tf.Session(config=config)
# &lt;- GPU memory is not fully occupied (v.1.12.0) 
#       but it was occupied right after in previous version (v.1.10.0)
sess.run(a)
# &lt;- GPU memory is now fully occupied (v.1.12.0)
&lt;/denchmark-code&gt;

Describe the expected behavior
After session is created gpu memory should be fully occupied as specified in a variable per_process_gpu_memory_fraction
Code to reproduce the issue
shown above
	</description>
	<comments>
		<comment id='1' author='cih9088' date='2019-02-21T06:25:57Z'>
		Thanks for filing this issue. Could you also explain your preference for the old behavior?
I haven't tried to reproduce this yet but adding some devs who may know if there was any change in the memory allocator implementation that may have caused this.
&lt;denchmark-link:https://github.com/aaroey&gt;@aaroey&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/poxvoculi&gt;@poxvoculi&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='cih9088' date='2019-02-21T16:52:06Z'>
		Exactly what do you mean by GPU memory being fully occupied?  How are you testing that?  And, most importantly, why would the old behavior be preferable?
		</comment>
		<comment id='3' author='cih9088' date='2019-02-22T03:43:37Z'>
		What I mean by that if I set per_process_gpu_memory_fraction to 0.5, 50% of GPU memory is occupied as soon as session is created in v1.10.0 but in v1.12.0 only small fraction of GPU memory, say 5% of memory, is occupied right after session is created.
This new behaviour is easily addressed by executing arbitrary ops or variables in session right after it is created. That is if I run any ops or variables, 50% of GPU memory is successfully occupied right after a run.
In my case, before training any models, I should do some time-consuming computations before actual training or evaluation. I usually train 3~5 models on single GPU at once due to resource limitation and the machine is shared with teammates. Because of computations before training, I occupy certain amount of memory sufficient for my model before computations by opening a session to use certain portion of GPU exclusively. If not, other colleague unexpectedly came in and occupy GPU memory resulting in out of memory error after time-consuming computation which is annoying and waste of time.
		</comment>
		<comment id='4' author='cih9088' date='2019-02-22T04:58:13Z'>
		Thanks for the explanation.
As a workaround, did you consider running a dummy operation immediately after creating the session to make sure that memory is reserved immediately after your process starts?
		</comment>
		<comment id='5' author='cih9088' date='2019-02-22T05:25:33Z'>
		Yes, that's exactly what I do for the time being.
But the old behaviour makes more sense to me. Shouldn't memory be occupied as soon as session is created if allow_growth option is False?
		</comment>
		<comment id='6' author='cih9088' date='2019-02-22T18:35:13Z'>
		GPU allocators are BFCAllocators with a suballocator that gets large regions of GPU memory.  bfc_allocator.cc is also used for CPU memory.  In that second role it is capable of growing the regions it manages, on demand.  When used for GPU memory we constrain it to grab a single large region in once.  In either case, the first memory region is allocated by the suballocator on demand, the first time an actual AllocateRaw request is presented to the allocator.
What likely caused the change you're seeing is that the allocation of Eigen GPU scratch buffers was moved from GPUDevice creation time to ReinitializeGPUDevice call time.  Hence now you see that the GPU region is not allocated until the first allocation triggered by your TF program.  That was done to break a difficult ordering problem related to properly configuring all the allocators.
		</comment>
	</comments>
</bug>