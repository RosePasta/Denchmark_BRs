<bug id='25175' author='randolf-scholz' open_date='2019-01-24T17:59:36Z' closed_time='2019-01-25T22:30:50Z'>
	<summary>Dropout layer is broken</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.10
TensorFlow installed from (source or binary): conda default channel
TensorFlow version (use command below): 1.12 (conda version mkl_py36h69b6ba0_0)
Python version: 3.6.8

Describe the current behavior
The Dropout layer always acts as if it is in testing mode.
Describe the expected behavior
The Dropout layer switches between training and testing.

The tutorial code available from the &lt;denchmark-link:https://www.tensorflow.org/tutorials/&gt;hompage&lt;/denchmark-link&gt;
:
&lt;denchmark-code&gt;import tensorflow as tf
mnist = tf.keras.datasets.mnist

(x_train, y_train),(x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(512, activation=tf.nn.relu),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5)
model.evaluate(x_test, y_test)
&lt;/denchmark-code&gt;

Setting the droprate to 0.999 yields the same performance as 0.2 (well above 95% acc). This is not possible, it means that the training/training switch inside the Dropout layer is broken, it is always returning the inputs, even during the training phase.
With tensorflow 1.10, the tutorial script gives  the "correct" result which is ~11% accuracy with 99.9% droprate.
Other info / logs
It seems that either tensorflow.keras.backend.learning_phase is at the root of the problem, or model.fit doesn't correctly sets the training flag.
	</description>
	<comments>
		<comment id='1' author='randolf-scholz' date='2019-01-24T19:41:29Z'>
		&lt;denchmark-link:https://github.com/randolf-scholz&gt;@randolf-scholz&lt;/denchmark-link&gt;
 I test it on tf-nightly(1.13.0.dev20190124). The results are in below and look correct.
When drop_rate = 0.99
&lt;denchmark-code&gt;Epoch 1/5
60000/60000==============================] - 5s 76us/sample - loss: 2.2957 - acc: 0.1542
Epoch 2/5
60000/60000==============================] - 4s 74us/sample - loss: 2.1568 - acc: 0.1891
Epoch 3/5
60000/60000==============================] - 4s 73us/sample - loss: 2.1143 - acc: 0.2052
Epoch 4/5
60000/60000==============================] - 4s 73us/sample - loss: 2.0843 - acc: 0.2174
Epoch 5/5
60000/60000==============================] - 4s 73us/sample - loss: 2.0794 - acc: 0.2218
10000/10000==============================] - 0s 24us/sample - loss: 1.1996 - acc: 0.8066
&lt;/denchmark-code&gt;

When drop_rate = 0.2
&lt;denchmark-code&gt;Epoch 1/5
60000/60000==============================] - 5s 84us/sample - loss: 0.2183 - acc: 0.9349
Epoch 2/5
60000/60000==============================] - 5s 82us/sample - loss: 0.0980 - acc: 0.9704
Epoch 3/5
60000/60000==============================] - 5s 79us/sample - loss: 0.0669 - acc: 0.9789
Epoch 4/5
60000/60000==============================] - 5s 80us/sample - loss: 0.0527 - acc: 0.9829
Epoch 5/5
60000/60000==============================] - 5s 79us/sample - loss: 0.0433 - acc: 0.9856
10000/10000==============================] - 0s 27us/sample - loss: 0.0697 - acc: 0.9801
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='randolf-scholz' date='2019-01-25T03:23:31Z'>
		+1. The results are correct with tf-nightly. Please test against tf-nightly. Feel free to reopen if still running into problems. Thanks!
		</comment>
		<comment id='3' author='randolf-scholz' date='2019-01-25T17:35:44Z'>
		&lt;denchmark-link:https://github.com/aselle&gt;@aselle&lt;/denchmark-link&gt;
 I don't know when this was fixed, but this may be important enough to cherry-pick into 1.13 if it wasn't fixed before the branch cut.
		</comment>
		<comment id='4' author='randolf-scholz' date='2019-01-25T17:44:33Z'>
		Also run the tests on  tensorflow==1.13.0rc0. The results look right.
		</comment>
		<comment id='5' author='randolf-scholz' date='2019-01-25T17:49:43Z'>
		Thanks &lt;denchmark-link:https://github.com/feihugis&gt;@feihugis&lt;/denchmark-link&gt;
!
		</comment>
		<comment id='6' author='randolf-scholz' date='2019-01-25T17:58:25Z'>
		&lt;denchmark-link:https://github.com/martinwicke&gt;@martinwicke&lt;/denchmark-link&gt;
 tested dropout with 1.12 (from PyPI), works fine. We have unit tests for this, always had.
		</comment>
		<comment id='7' author='randolf-scholz' date='2019-01-25T18:03:26Z'>
		Hmm... strange thing to be broken by a conda rebuild. But then... who knows.
		</comment>
		<comment id='8' author='randolf-scholz' date='2019-01-25T18:10:39Z'>
		&lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/martinwicke&gt;@martinwicke&lt;/denchmark-link&gt;

The problems seems to appear in tf.keras, but not in the standalone keras package. The issue also seems affect the  keras.backend.in_train_phase  function. I tested it with the following files:


https://pastebin.com/SvTwrSC8


https://pastebin.com/VtBwL5mz


		</comment>
		<comment id='9' author='randolf-scholz' date='2019-01-25T19:31:52Z'>
		&lt;denchmark-link:https://github.com/martinwicke&gt;@martinwicke&lt;/denchmark-link&gt;
 tested dropout with 1.12 (from PyPI and in colab), with the above code. Does not work fine.
		</comment>
		<comment id='10' author='randolf-scholz' date='2019-01-25T22:02:56Z'>
		Lets leave the issue open while we figure out more details and provide updates here.
		</comment>
		<comment id='11' author='randolf-scholz' date='2019-01-25T22:07:16Z'>
		&lt;denchmark-link:https://github.com/randolf-scholz&gt;@randolf-scholz&lt;/denchmark-link&gt;
: thank you for taking the time to report this.
To be fair I have not done a full investigation and I have not looked at what was wrong with the script posted, or at which commit its behavior might have started changing.
Here is the test script I have used. I have only tested it with 1.13 and 1.12 on CPU. I have tested it in eager and graph mode. The script rigorously test that the Dropout layer injects ones in the forward pass activations of the Dense layer below it.
import numpy as np
import tensorflow as tf

print('tf.version:', tf.__version__)  # 1.12.0

# tf.enable_eager_execution()  # try with eager or not

inputs = keras.Input(shape=(100,))
x = keras.layers.Dense(100,
                       kernel_initializer='zeros',
                       bias_initializer='ones',
                       trainable=False)(inputs)
x = keras.layers.Dropout(0.5)(x)
model = keras.Model(inputs, x)
model.compile(tf.train.RMSPropOptimizer(0.1), 'mse')

print(model.train_on_batch(np.ones((1, 100)), np.ones((1, 100))))  # Positive loss
model.fit(np.ones((1, 100)), np.ones((1, 100)))  # Positive loss

print(model.test_on_batch(np.ones((1, 100)), np.ones((1, 100))))  # 0 loss
model.evaluate(np.ones((1, 100)), np.ones((1, 100)))  # 0 loss
In addition to this, we have correctness tests for dropout in our test suite (multiple in fact, because we typically use a dropout layer as a way to test whether the learning phase was correctly set, which is something we have to test for a range of model types and situations).
To be the best I can tell tf.keras.layers.Dropout() is working normally in TF 1.12 in training and inference modes with fit, train_on_batch, test_on_batch, evaluate.
		</comment>
		<comment id='12' author='randolf-scholz' date='2019-01-25T22:30:50Z'>
		I have now done a full investigation of the issue with the script posted. I confirm that incorrect behavior is present in 1.11 and 1.12.
This behavior is entirely unrelated to either the Dropout layer, or to the in_train_phase backend utility. The Dropout layer works completely fine.
The bug is an issue that occurs when using a Sequential model in "deferred mode". Deferred mode is a recently-introduce way to use Sequential without passing an input_shape argument as first layer. It's looking like the learning phase value was incorrectly set in this case.
I think this would not have affected many users since "deferred mode" for Sequential is new, and not widely used, because hardly any Keras example use it (none on keras.io). It is unfortunate that some of our tutorials on tensorflow.org have started using it.
I'm contacting devrel to make sure that the code examples are updated to add input_shape arguments in our Sequential models (which is better practice anyway since it allows for static layer compatibility checks).
You can fix the issue in the script by simply passing input_shape=(28, 28) to the first flatten layer.
This bug has been fixed some time ago on the TF side.
		</comment>
	</comments>
</bug>