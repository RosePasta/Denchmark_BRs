<bug id='16316' author='mholzel' open_date='2018-01-23T08:07:11Z' closed_time='2018-01-27T00:00:02Z'>
	<summary>Lack of clarity in tf.while_loop documentation</summary>
	<description>
I believe that the documentation for tf.while_loop is lacking usage clarity, and actually provides contradictory statements.
Specifically, it seems that many people are using the tf.while_loop as a "for loop" (&lt;denchmark-link:https://stackoverflow.com/questions/35330117/how-can-i-run-a-loop-with-a-tensor-as-its-range-in-tensorflow&gt;see stackoverflow&lt;/denchmark-link&gt;
). However, the &lt;denchmark-link:https://www.tensorflow.org/versions/r0.12/api_docs/python/control_flow_ops/control_flow_operations#while_loop&gt;tf.while_loop&lt;/denchmark-link&gt;
 docs state:

For correct programs, while_loop should return the same result for any parallel_iterations &gt; 0.

A loop counter inside of the "while loop" body, seems to violate this constraint despite the fact that this is given as an example usage in the docs:

python i = tf.constant(0) c = lambda i: tf.less(i, 10) b = lambda i: tf.add(i, 1) r = tf.while_loop(c, b, [i])

So it seems that there are two bad outcomes here:


If this is indeed the canonical way of creating a "for loop", then the example explicitly creates a dependency between iterations, meaning that the "while loop" iterations cannot be run in parallel.


The example is incorrect?


It seems like the while_loop docs should have an example which better illustrates how to use it as a "for loop", if such usage is indeed intended, or a warning on the implications of the provided example.
	</description>
	<comments>
		<comment id='1' author='mholzel' date='2018-01-24T01:26:10Z'>
		Thank you for your report. As far as I can see, the loop that you show above is correct. In Python, parallel execution does not allow more than one thread run at a time. The execution of multiple threads might overlap, however. So, a code with reduction, such as the one shown above works just fine if executed in parallel.
Let me confirm with an expert, though. &lt;denchmark-link:https://github.com/michaelisard&gt;@michaelisard&lt;/denchmark-link&gt;
 could you take a quick look?
		</comment>
		<comment id='2' author='mholzel' date='2018-01-24T08:44:30Z'>
		&lt;denchmark-link:https://github.com/tatianashp&gt;@tatianashp&lt;/denchmark-link&gt;
 I found &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/1984#issuecomment-211199897&gt;this comment in issue 1984&lt;/denchmark-link&gt;
 that was incredibly helpful in understanding what is going on.  But even that comment is not complete &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/1984#issuecomment-359713984&gt;as I note here&lt;/denchmark-link&gt;
. It would be nice for somebody to have a look at these comments and incorporate them into the documentation.
Finally, one more thing that is not explained in the docs is why &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/16333&gt;this phenomenon occurs (reported in issue 16333)&lt;/denchmark-link&gt;
. Specifically, it seems as though the while loop is doing some type of copy semantics instead of referencing the inputs. In my mind, that is not at all the behavior I expect.
		</comment>
		<comment id='3' author='mholzel' date='2018-01-27T00:00:02Z'>
		The discussion of this issue has migrated to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/16333&gt;#16333&lt;/denchmark-link&gt;
.
		</comment>
	</comments>
</bug>