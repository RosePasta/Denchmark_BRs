<bug id='37963' author='hardianlawi' open_date='2020-03-27T05:04:55Z' closed_time='2020-03-31T18:01:19Z'>
	<summary>tf dataset cache behave differently with filter for TF 1.x</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock
example script provided in TensorFlow):  Yes
OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Linux Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: NA
TensorFlow installed from (source or
binary): binary (pip)
TensorFlow version (use command below): v1.15.0-rc3-22-g590d6eef7e 1.15.0
Python version: - Bazel
version (if compiling from source): NA
GCC/Compiler version (if compiling from
source): NA
CUDA/cuDNN version: - GPU model and memory: NA

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
When applying filter before cache on tf.data.Dataset, the outputs when caching to file compared to caching to memory are different.
Describe the expected behavior
They should be the same.
TF 2x is working as expected.
Standalone code to reproduce the issue
Cache to memory
import tensorflow as tf

tf.enable_eager_execution()

data = tf.data.Dataset.from_tensor_slices(list(range(50)))
data = data.filter(lambda x: tf.random.uniform([]) &lt; 0.5)
data = data.cache()

outputs = [x for x in data]
outputs_1 = [x for x in data]

assert len(outputs) == len(outputs_1)

for x, y in zip(outputs, outputs_1):
    assert x.numpy() == y.numpy()
Cache to file
import tensorflow as tf

tf.enable_eager_execution()

data = tf.data.Dataset.from_tensor_slices(list(range(50)))
data = data.filter(lambda x: tf.random.uniform([]) &lt; 0.5)
data = data.cache('/tmp/dummy_data')

outputs = [x for x in data]
outputs_1 = [x for x in data]

assert len(outputs) == len(outputs_1)

for x, y in zip(outputs, outputs_1):
    assert x.numpy() == y.numpy()
Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='hardianlawi' date='2020-03-27T10:40:32Z'>
		Was able to reproduce the issue with TF 1.15.2. Please find the Gist &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/7714f97c7adc37db9f361bdc40fe9f5a/37963.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='hardianlawi' date='2020-03-31T07:09:53Z'>
		Hi, I am facing a similar issue and I've managed to replicate the problem too. Is there a fix for this?
		</comment>
		<comment id='3' author='hardianlawi' date='2020-03-31T18:01:19Z'>
		In TF 1, cache does not work with eager iteration (i.e. from elem in dataset). You should either use TF 2 or append repeat to the end of your dataset consume multiple epochs.
		</comment>
		<comment id='4' author='hardianlawi' date='2020-03-31T18:01:21Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37963&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37963&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='hardianlawi' date='2020-04-01T15:02:53Z'>
		Hi &lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
, thank you for your reply. I tried replicating it in non-eager and still experienced the same issue.
Why are you saying that cache does not work with eager mode in TF 1? By "does not work", do you mean it will not behave properly? Caching with file behaves as expected though.
		</comment>
		<comment id='6' author='hardianlawi' date='2020-04-01T16:25:24Z'>
		In TF 1, memory-based cache only supports reuse through subsequent repeat. This is because the implementation uses an in-memory cache that is owned by the iterator and thus different iterators will have different caches. This is not an issue for file-based caches because different iterators can access the same file-based cache.
This is fixed in TF 2 where a single in-memory cache is shared among all iterators created for a dataset.
		</comment>
	</comments>
</bug>