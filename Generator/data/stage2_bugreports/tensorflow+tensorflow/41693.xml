<bug id='41693' author='ageron' open_date='2020-07-24T05:38:13Z' closed_time='2020-09-10T19:49:48Z'>
	<summary>InaccessibleTensorError in custom Model using add_loss and build</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
MacOSX 10.15.5
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No
TensorFlow installed from (source or binary):
binary
TensorFlow version (use command below):
v2.2.0-rc4-8-g2b96f3662b 2.2.0
Python version:
3.7.6
Bazel version (if compiling from source):
N/A
GCC/Compiler version (if compiling from source):
N/A
CUDA/cuDNN version:
N/A
GPU model and memory:
N/A

Describe the current behavior
When running the following code, which uses add_loss() and creates a layer in the build() method, I get an InaccessibleTensorError:
import tensorflow as tf
from tensorflow import keras

class MyModel(keras.models.Model):
    def build(self, batch_input_shape):
        self.output_layer = keras.layers.Dense(1)
        super().build(batch_input_shape)

    def call(self, inputs, training=None):
        self.add_loss(1.0)
        return self.output_layer(inputs)

model = MyModel()
model.compile(loss="mse", optimizer="nadam")

X = tf.random.uniform((100, 10))
y = tf.random.uniform((100, 1))
history = model.fit(X, y, epochs=2)
Describe the expected behavior
I expect the model to be trained normally, without error.
Standalone code to reproduce the issue
See the code above. You can run it in this colab: &lt;denchmark-link:https://colab.research.google.com/drive/1c_NBNJ2vKt412WSg1HiPUNYjVmf1--YL&gt;https://colab.research.google.com/drive/1c_NBNJ2vKt412WSg1HiPUNYjVmf1--YL&lt;/denchmark-link&gt;

Other info / logs
Here is the full stacktrace:
&lt;denchmark-code&gt;InaccessibleTensorError: in user code:

    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *
        outputs = self.distribute_strategy.run(
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:533 train_step  **
        y, y_pred, sample_weight, regularization_losses=self.losses)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:231 __call__
        reg_loss = math_ops.add_n(regularization_losses)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:180 wrapper
        return target(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3239 add_n
        return gen_math_ops.add_n(inputs, name=name)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:420 add_n
        "AddN", inputs=inputs, name=name)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:744 _apply_op_helper
        attrs=attr_protos, op_def=op_def)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:591 _create_op_internal
        inp = self.capture(inp)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:641 capture
        % (tensor, tensor.graph, self))

    InaccessibleTensorError: The tensor 'Tensor("Const:0", shape=(), dtype=float32)' cannot be accessed here: it is defined in another function or code block. Use return values, explicit Python locals or TensorFlow collections to access it. Defined in: FuncGraph(name=build_graph, id=139933898535488); accessed from: FuncGraph(name=train_function, id=139933898618920).
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='ageron' date='2020-07-24T05:49:16Z'>
		After further testing, it seems that you cannot use add_loss() in a custom model where the build() method is defined:
class MyModel2(keras.models.Model):
    def build(self, batch_input_shape):
        super().build(batch_input_shape)

    def call(self, inputs, training=None):
        self.add_loss(1.0)
        return tf.reduce_mean(inputs)

model2 = MyModel2()
model2.compile(loss="mse", optimizer="nadam")
history = model2.fit(X, y, epochs=2)
If you comment out the build() method, everything works fine, even though the build() method does nothing more than call super().build(). That's really odd...
		</comment>
		<comment id='2' author='ageron' date='2020-07-24T17:02:17Z'>
		I am able to replicate the issue faced, please find the &lt;denchmark-link:https://colab.research.google.com/gist/Saduf2019/60cb24d362d9b04cadd5cc4f631c5cb1/untitled294.ipynb&gt;gist here&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='ageron' date='2020-08-27T14:35:17Z'>
		Is there any progress on this issue?
Or a suggested workaround in the meantime?
Thanks in advance!
		</comment>
		<comment id='4' author='ageron' date='2020-09-10T19:49:48Z'>
		Thanks for the issue!
Constant Tensors can't be used for losses in Functional Models, because the graph used to build the model is different than the graph used to execute it, and the building graph will capture the constant. Instead if you need a constant loss, you can use a callable loss:
&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow import keras

class MyModel(keras.models.Model):

    def build(self, batch_input_shape):
        self.output_layer = keras.layers.Dense(1)
        super().build(batch_input_shape)

    def call(self, inputs, training=None):
        self.add_loss(lambda: 1.0)
        return self.output_layer(inputs)

model = MyModel()
model.compile(loss="mse", optimizer="nadam")

X = tf.random.uniform((100, 10))
y = tf.random.uniform((100, 1))
history = model.fit(X, y, epochs=2)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='ageron' date='2020-09-10T19:49:50Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41693&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41693&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>