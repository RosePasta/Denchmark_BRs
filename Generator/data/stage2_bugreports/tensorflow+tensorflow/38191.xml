<bug id='38191' author='jenishah' open_date='2020-04-03T10:34:19Z' closed_time='2020-04-03T11:09:38Z'>
	<summary>Error while passing initial_state to Bidirectional LSTM</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock
example script provided in TensorFlow):  yes
OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): MacOS Mojave (10.14.6)
TensorFlow installed from (source or
binary): binary (Anaconda)
Python version: 3.6.10
Tensorflow version: 2.1.0

Describe the current behavior
Error while passing initial_state to tf.keras.layers.Bidirectional with tf.keras.layers.LSTM as cell.
Describe the expected behavior
Should be able to pass the initial state
Standalone code to reproduce the issue
&lt;denchmark-code&gt;import tensorflow as tf
import tensorflow.keras.layers as layers


encoder_units = 100
batch_size = 5
embedding_dim = 300

lstm =  layers.Bidirectional(layers.LSTM(encoder_units,
                                                  return_sequences=True,
                                                    return_state=True,
                                                     time_major=False))
initial_state = [tf.zeros((batch_size, encoder_units)), tf.zeros((batch_size, encoder_units))]
embedding_inp =  tf.zeros((batch_size, encoder_units, embedding_dim))
encoder_op, state_h, state_c = lstm(embedding_inp, initial_state= initial_state)
&lt;/denchmark-code&gt;

Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
/anaconda3/envs/tf2/bin/python /Users/jshah02/Library/Preferences/PyCharmCE2019.2/scratches/scratch_8.py
2020-04-03 16:02:31.592278: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-04-03 16:02:31.606727: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fe10d5c24e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-04-03 16:02:31.606745: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
Traceback (most recent call last):
File "/Users/jshah02/Library/Preferences/PyCharmCE2019.2/scratches/scratch_8.py", line 15, in 
encoder_op, state_h, state_c = lstm(embedding_inp, initial_state= initial_state)
File "/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/wrappers.py", line 605, in call
return super(Bidirectional, self).call(inputs, **kwargs)
File "/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 818, in call
self._maybe_build(inputs)
File "/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 2116, in _maybe_build
self.build(input_shapes)
File "/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/wrappers.py", line 697, in build
self.forward_layer.build(input_shape)
File "/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent.py", line 574, in build
self._validate_state_spec(state_size, self.state_spec)
File "/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent.py", line 605, in _validate_state_spec
raise validation_error
ValueError: An initial_state was passed that is not compatible with cell.state_size. Received state_spec=ListWrapper([InputSpec(shape=(5, 100), ndim=2)]); however cell.state_size is [100, 100]
Process finished with exit code 1
	</description>
	<comments>
		<comment id='1' author='jenishah' date='2020-04-03T10:59:03Z'>
		I have tried on colab with TF version 2.1.0 , 2.2.0-rc2 and was able to reproduce the issue. Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/56a61e5c22d9c78fc49eea3b9434590d/untitled764.ipynb#scrollTo=5-mB1Y5GqzbP&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='jenishah' date='2020-04-03T11:09:34Z'>
		Hi,
I found the correct approach.
&lt;denchmark-code&gt;import tensorflow as tf
import tensorflow.keras.layers as layers


encoder_units = 100
batch_size = 5
embedding_dim = 300

lstm =  layers.Bidirectional(layers.LSTM(encoder_units,
                                                  return_sequences=True,
                                                    return_state=True,
                                                     time_major=False),
                             merge_mode='concat')
initial_state = [tf.zeros((batch_size, encoder_units)) for _ in range(4)]
embedding_inp =  tf.zeros((batch_size, encoder_units, embedding_dim))
encoder_op, state_hf, state_cf, state_hb, state_cb = lstm(embedding_inp, initial_state= initial_state)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='jenishah' date='2020-04-03T11:09:40Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38191&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38191&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>