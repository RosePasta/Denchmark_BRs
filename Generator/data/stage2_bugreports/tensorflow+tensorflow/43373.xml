<bug id='43373' author='shangh1' open_date='2020-09-20T06:32:33Z' closed_time='2020-11-19T04:41:55Z'>
	<summary>TF2.0, how to include feature processing code in saved model.</summary>
	<description>
 make sure that this is a bug. As per our
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md&gt;GitHub Policy&lt;/denchmark-link&gt;
,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.
OS Platform and Distribution : macOS Catalina 10.15.3
TensorFlow installed from : binary
TensorFlow version : 2.2.0
Python version: 3.7.3

Hi, I have a working code (shown below): it processed the raw input (in tensor functions); then the processed input is used as the input of the keras model. I'm able to save the model, and also can make inference after loading model. But my problem is: this feature processing is done before inputing to the keras model, so it is not part of the saved model. When I run inference, I need to process the raw input first, then run model.predict on the processed data.
Because we would like to put the model on production, we need to include the feature processing code into the saved model, so the raw data can be fed into the saved model directly. I've tried different ways to make it work, but all failed. Please help, thanks!
Note that in the following code, 'FeatureProcess' is the function processing the raw input, I don't show the details of this 'FeatureProcess' function due to privacy reason. Inside this 'FeatureProcess' function is all tensor functions and manipulations.
def line_to_multiple_features(inputString, mode):
inputString = [inputString]
batch_ids, input_ids_long, labels, input_ids = FeatureProcess(inputString)
d = {'ids':batch_ids[0], 'input_ids_long':input_ids_long[0], 'input_ids':input_ids[0]}
if mode == 'train':
return d, labels[0]
else:
return d
def get_unbatched_feature_label(filename, mode, epochs, feature_delimiter):
dataset = tf.data.TextLineDataset(filename)
if mode == 'train':
dataset = dataset.repeat(epochs).shuffle(10000)
# convert every line to features
unbatch_parse_data = dataset.map(lambda x: line_to_multiple_features(x))
return unbatch_parse_data
&lt;denchmark-h:h1&gt;Dataset for input data&lt;/denchmark-h&gt;

def get_feature_label(_input):
input_data = ctx.absolute_path(_input)
input_files = tf.data.Dataset.list_files(input_data)
datasets_unbatched = get_unbatched_feature_label(input_files, args.mode, args.epochs, '\t')
data_feature_label = datasets_unbatched.batch(GLOBAL_BATCH_SIZE)
return data_feature_label
def build_keras_model():
vocab_size = 100000
embedding_dim = args.hidden_size
num_filters = 512
filter_sizes = [2,3,4]
drop = 0.5
inputs = {'ids':Input(shape=(4, ), dtype='string'),
'input_ids_long':Input(shape=(args.seq_length_long, ), dtype='int32'),
'input_ids':Input(shape=(args.seq_length, ), dtype='int32'))
}
embedding_short = Embedding(input_dim=vocab_size, output_dim=embedding_dim,
input_length=args.seq_length)(inputs['input_ids'])
embedding_long = Embedding(input_dim=vocab_size, output_dim=embedding_dim,
input_length=args.seq_length_long)(inputs['input_ids_long'])
reshape_short = Reshape((args.seq_length, embedding_dim, 1))(embedding_short)
reshape_long = Reshape((args.seq_length_long, embedding_dim, 1))(embedding_long)
conv_0_short = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape_short)
conv_1_short = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape_short)
conv_2_short = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape_short)
conv_0_long = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape_long)
conv_1_long = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape_long)
conv_2_long = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape_long)
maxpool_0_short = MaxPool2D(pool_size=(args.seq_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0_short)
maxpool_1_short = MaxPool2D(pool_size=(args.seq_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1_short)
maxpool_2_short = MaxPool2D(pool_size=(args.seq_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2_short)
maxpool_0_long = MaxPool2D(pool_size=(args.seq_length_long - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0_long)
maxpool_1_long = MaxPool2D(pool_size=(args.seq_length_long - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1_long)
maxpool_2_long = MaxPool2D(pool_size=(args.seq_length_long - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2_long)
concatenated_tensor = Concatenate(axis=1)([maxpool_0_short, maxpool_1_short, maxpool_2_short, maxpool_0_long, maxpool_1_long, maxpool_2_long])
flatten = Flatten()(concatenated_tensor)
dropout = Dropout(drop)(flatten)
logits = Dense(units=args.num_classes)(dropout)
pred_probs = tf.keras.activations.sigmoid(logits)
model = Model(inputs=inputs, outputs=pred_probs)
def get_loss_fn():
def get_loss(labels, outputs):
logits = tf.math.log((outputs+0.0000001) / ( 1 - outputs+0.0000001))
loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels)
cost = tf.reduce_mean(
tf.reduce_sum(loss, axis=1)
)
return cost
return get_loss
adam = Adam(lr=0.001)
model.compile(optimizer=adam,
loss=get_loss_fn(),
metrics=['accuracy'])
return model
tf.io.gfile.makedirs(args.model_dir)
filepath = args.model_dir # + "/weights-{epoch:04d}"
callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=filepath, monitor="val_accuracy", verbose=1, save_best_only=True, mode='auto', save_freq=10)]
steps_per_epoch = 60000 / GLOBAL_BATCH_SIZE * 0.9
if args.mode == 'train':
with strategy.scope():
multi_worker_model = build_keras_model()
train_feature_label = get_feature_label(args.input)
validation_feature_label = get_feature_label(args.val_path)
multi_worker_model.fit(x=train_feature_label, validation_data=validation_feature_label,
epochs=args.epochs, steps_per_epoch=steps_per_epoch, callbacks=callbacks)
results = multi_worker_model.evaluate(validation_feature_label, verbose=2)
multi_worker_model.save(args.model_dir)
else:
print('now is inference mode')
test_feature = get_feature_label(args.test_path)
reconstructed_model = tf.keras.models.load_model(args.model_dir, compile=False)
for elem in test_feature:
y_pred_elem = reconstructed_model.predict(elem)
	</description>
	<comments>
		<comment id='1' author='shangh1' date='2020-09-20T06:34:29Z'>
		sorry I put indented spaces for the code, but the indents were all gone after I submitted it.
		</comment>
		<comment id='2' author='shangh1' date='2020-09-21T09:05:36Z'>
		
sorry I put indented spaces for the code, but the indents were all gone after I submitted it.

I have the same &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/42552&gt;problem&lt;/denchmark-link&gt;
 . They suggest me use .  : )
		</comment>
		<comment id='3' author='shangh1' date='2020-09-21T11:06:50Z'>
		&lt;denchmark-link:https://github.com/shangh1&gt;@shangh1&lt;/denchmark-link&gt;

Please share a colab gist with error reported.
		</comment>
		<comment id='4' author='shangh1' date='2020-09-21T16:31:54Z'>
		&lt;denchmark-link:https://github.com/DachuanZhao&gt;@DachuanZhao&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;
  Thank you both! I'll try tf.keras.layers.experimental.preprocessing, and update you.
		</comment>
		<comment id='5' author='shangh1' date='2020-09-22T20:48:13Z'>
		Hi &lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/DachuanZhao&gt;@DachuanZhao&lt;/denchmark-link&gt;
, my issue is similar as this &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/42552&gt;problem&lt;/denchmark-link&gt;
 brought up by &lt;denchmark-link:https://github.com/DachuanZhao&gt;@DachuanZhao&lt;/denchmark-link&gt;
. But I just looked at functions in tf.keras.layers.experimental.preprocessing, it only has around 20 classes, seems very limited. My data processing code is written all in tf-functions, and I'm not able to convert all tf-functions in my data processing code to tf.keras.layers.experimental.preprocessing. Some of my tf functions in data processing code include: tf.strings.split, tf.lookup.StaticHashTable, tf.strings.to_number, tf.sparse.SparseTensor, tf.compat.v1.sparse_to_dense, tf.fill, tf.concat, tf.not_equal, tf.cast.
		</comment>
		<comment id='6' author='shangh1' date='2020-09-22T20:49:33Z'>
		&lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;
 sorry I'm not able to share the data processing code as it belongs to the company. Did I describe my problem clearly?
		</comment>
		<comment id='7' author='shangh1' date='2020-09-25T07:11:14Z'>
		&lt;denchmark-link:https://github.com/shangh1&gt;@shangh1&lt;/denchmark-link&gt;

In this case, could you please provide us with dummy data with similar shape and size to reproduce the issue reported here.
		</comment>
		<comment id='8' author='shangh1' date='2020-09-29T23:23:03Z'>
		&lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;
 Thanks. I simplify the code, and put it here. &lt;denchmark-link:https://github.com/shangh1/model_tf2/tree/master&gt;https://github.com/shangh1/model_tf2/tree/master&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='shangh1' date='2020-10-14T06:05:05Z'>
		&lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/gowthamkpr&gt;@gowthamkpr&lt;/denchmark-link&gt;
 hello, is there any update on this? Thanks.
		</comment>
		<comment id='10' author='shangh1' date='2020-10-14T06:09:16Z'>
		
@Saduf2019 @gowthamkpr hello, is there any update on this? Thanks.

Have you read the new document &lt;denchmark-link:https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers&gt;https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers&lt;/denchmark-link&gt;
 ? It solves my problem .
		</comment>
		<comment id='11' author='shangh1' date='2020-11-03T18:01:01Z'>
		&lt;denchmark-link:https://github.com/shangh1&gt;@shangh1&lt;/denchmark-link&gt;
 Please take a look at the above comment.Thanks!
		</comment>
		<comment id='12' author='shangh1' date='2020-11-05T04:10:00Z'>
		
@shangh1 Please take a look at the above comment.Thanks!

Hi , I want to know whether it will slow down performance  or not in inference status if a model contains a preprocessing layer s  , especially when the model which contains a preprocessing layer served in tf-serving . Is there any document for that ?
		</comment>
		<comment id='13' author='shangh1' date='2020-11-12T04:19:48Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='14' author='shangh1' date='2020-11-19T04:41:54Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='15' author='shangh1' date='2020-11-19T04:41:56Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43373&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43373&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>