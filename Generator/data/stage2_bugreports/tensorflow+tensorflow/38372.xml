<bug id='38372' author='ShaneSmiskol' open_date='2020-04-08T23:56:26Z' closed_time='2020-06-12T09:33:46Z'>
	<summary>Windows 10 shuts down when using GPU (tf 2.1.0 with tf.keras and tf 1.14 with keras)</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): Yes, custom code
OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Windows 10 64 bit
TensorFlow installed from (source or
binary): Tensorflow was installed using pip install tensorflow
TensorFlow version (use command below): 2.1.0 (but occurs on 1.14, 1.15, 2.0 as well)
Python version: Occurs on Python 3.5, 3.6, 3.7
Bazel
version (if compiling from source):
GCC/Compiler version (if compiling from
source):
CUDA/cuDNN version: I have the following installed: CUDA 9.2, CUDA 10.0, CUDA 10.1, CUDA 10.2 (with appropriate CuDNN versions)
GPU model and memory: Occurs on new RTX 2080 ti (11 GB) and my old GTX 1070 (8GB)

Describe the current behavior: When using either tensorflow or tensorflow-gpu with two of my graphics cards (1070, 2080 ti) my system shuts off completely (no warning, as if I unplugged the power) if I train a model with a high batch size (~32-42). Specifically using tf.keras's Conv2D and CuDNNLSTM/CuDNNGRU. It does not seem to have any problems training a fully connected model with batch sizes up to 512. Everything is fine if I train my Conv2D model with a batch size of 16 or lower. Also, if I use nvidia-smi to limit my GPU's power consumption from stock 250W to 150W it works fine, but is very slow. However when it trains with 16 batch size, it goes above 200W regularly and has no problem.
Describe the expected behavior: System does not shut off and should spit back a memory error if it's running out of memory.
Standalone code to reproduce the issue:
&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np
import random

from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten
from tensorflow.keras import Sequential

num_samples = 500
h = 665
w = 814
c = 3

x = np.random.rand(num_samples, h, w, c)
y = [[0] * 4 for _ in range(num_samples)]
for i in range(len(y)):
    y[i][random.randint(0, 3)] = 1
y = np.array(y)

kernel_size = (3, 3)
model = Sequential()
model.add(Conv2D(12, kernel_size, strides=1, activation='relu', input_shape=x.shape[1:]))

model.add(Conv2D(24, kernel_size, strides=1, activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(48, kernel_size, strides=1, activation='relu'))
model.add(MaxPooling2D(pool_size=(3, 3)))

model.add(Conv2D(64, kernel_size, strides=1, activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(12, kernel_size, strides=1, activation='relu'))

model.add(Flatten())
model.add(Dense(32, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(4, activation='softmax'))

model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

model.fit(x, y, batch_size=64, epochs=500)
&lt;/denchmark-code&gt;

Other info / logs Include any logs or source code that would be helpful to
diagnose the problem: No errors are generated when it shuts down, it just cuts power immediately.
Extra info: I've fully replaced my CPU, power supply, graphics card, motherboard, and RAM and this issue still occurs. PSU was purchased about a month ago brand new. Only thing that is the same in the system since it last occurred is my PC case, storage (SSDs), and the Windows 10 install.
It also never occurs in GPU and CPU stress tests and neither GPU or CPU overheats when it shuts down. It's always after the first epoch starts training, or just before.
	</description>
	<comments>
		<comment id='1' author='ShaneSmiskol' date='2020-04-09T09:12:54Z'>
		I have tried in colab and i am able to reproduce the issue. I am seeing the error message (Your session crashed after using all available RAM).Thanks!
		</comment>
		<comment id='2' author='ShaneSmiskol' date='2020-04-09T20:27:42Z'>
		Hmm, that sounds like it could be related to the amount of RAM Colab gives you, I'm not super experienced with Colab. But that script takes about 16GB to load the data into system memory then of course only 64 images are loaded into GPU memory, which should be fine
		</comment>
		<comment id='3' author='ShaneSmiskol' date='2020-04-09T21:01:26Z'>
		Update: It seems if I use this model here instead of the one posted above, everything is fine up to batch size 64 while the one above shuts down my desktop immediately at batch size 32 (the below model is a snippet from my actual script I'm using to train an image model, I converted the one above into a standalone file however this model in the place of the model above also shuts down my PC):
&lt;denchmark-code&gt;model = Sequential()
model.add(Conv2D(12, kernel_size, activation='relu', input_shape=self.cropped_shape))
model.add(MaxPooling2D(pool_size=(3, 3)))

model.add(Conv2D(12, kernel_size, activation='relu'))
model.add(MaxPooling2D(pool_size=(3, 3)))

model.add(Conv2D(24, kernel_size, activation='relu'))
model.add(MaxPooling2D(pool_size=(3, 3)))

model.add(Conv2D(36, kernel_size, activation='relu'))

model.add(Flatten())
model.add(Dense(32, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(3, activation='softmax'))
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='ShaneSmiskol' date='2020-04-10T17:32:40Z'>
		&lt;denchmark-link:https://github.com/ShaneSmiskol&gt;@ShaneSmiskol&lt;/denchmark-link&gt;
 Similar issue has been discussed &lt;denchmark-link:https://www.nvidia.com/en-us/geforce/forums/discover/260522/pc-auto-shuts-down-when-running-keras-tensorflow-backend-may-be-a-psu-issue-/&gt;here&lt;/denchmark-link&gt;
. Please take a look at it.
		</comment>
		<comment id='5' author='ShaneSmiskol' date='2020-04-10T20:51:59Z'>
		I've seen that and many more posts online about tensorflow or just gaming in general that shuts down their computer. However nearly all of them conclude that the PSU is the issue which is strange because it happened with my old 550W PSU as well as my brand new 700W Thermaltake PSU I just installed, so I don't understand what's going wrong here. I've seen the GPU power usage above 225W yet it trains fine if I set my batch size lower. It seems only specific models end up shutting down my PC and I can't find a pattern. Any tips would be appreciated, thanks
		</comment>
		<comment id='6' author='ShaneSmiskol' date='2020-05-14T21:08:11Z'>
		Seems to be a GPU memory issue at runtime, let me add someone from runtime team with more insights. I can repro the error above on colab, which crash the session with "Your session crashed after using all available RAM."
		</comment>
		<comment id='7' author='ShaneSmiskol' date='2020-05-29T18:14:02Z'>
		Reassigning to Sanjoy to take a look at potential GPU memory issues. Thanks!
		</comment>
		<comment id='8' author='ShaneSmiskol' date='2020-06-01T11:42:38Z'>
		I switched back to a different GTX 1070 and everything seems to be running fine as far as the random shut downs with high batch sizes. Not too sure why it occurred on two different card with a new PSU but not with this 1070. I'll report back if it ever occurs again
		</comment>
		<comment id='9' author='ShaneSmiskol' date='2020-06-12T09:33:48Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38372&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38372&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>