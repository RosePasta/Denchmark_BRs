<bug id='20253' author='jhorowitz' open_date='2018-06-23T22:20:05Z' closed_time='2019-11-11T04:33:14Z'>
	<summary>Contrib Loss Function Bug</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;

I've reproduced this problem on multiple systems including two with CPU only and three with GPU. The specs below represent one such system.
Have I written custom code
Yes
OS Platform and Distribution
Mac OS X 10.13.2
17.3.0 Darwin Kernel 17.3.0: Thu Nov  9 18:09:22 PST 2017; root:xnu-4570.31.3~1/RELEASE_X86_64 x86_64
TensorFlow installed from
Pip (python3)
TensorFlow version
1.8.0
Bazel version
N/A
CUDA/cuDNN version
N/A
GPU model and memory
N/A
Exact command to reproduce
Please see code below
&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

There are several conditions that cause triplet_semihard_loss from tf.contrib to return nan as the loss. This totally messes up the gradient and causes divergence (at least in my build). Below is a minimal reproduction of one way to cause nan which is to have a batch of all different classes which causes there to be no positive for any chosen anchor.
The lack of a negative does not result in nan, just a super high loss which while not as bad, is still not what I'd expect.
When running backprop, I would expect to have no gradient if there are no valid triplets to train off of.
I'm happy to contribute to fixes if you guys agree this is a bug.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

import numpy as np
import tensorflow as tf
from tensorflow.contrib.losses import metric_learning

labels = tf.placeholder(tf.int32, [None], name='labels')
embeddings = tf.placeholder(tf.float32, [None, 128], name='embeddings')

loss_ph = metric_learning.triplet_semihard_loss(labels, embeddings)

sess = tf.InteractiveSession()
sess.run(tf.global_variables_initializer())

loss = sess.run(loss_ph, feed_dict={
    embeddings: np.random.rand(2, 128),
    labels: np.array([1, 2])
})

print("loss", loss)
# loss nan
	</description>
	<comments>
		<comment id='1' author='jhorowitz' date='2018-06-24T06:33:11Z'>
		Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.
Have I written custom code
OS Platform and Distribution
TensorFlow installed from
TensorFlow version
Bazel version
CUDA/cuDNN version
GPU model and memory
Exact command to reproduce
		</comment>
		<comment id='2' author='jhorowitz' date='2018-06-24T17:28:07Z'>
		&lt;denchmark-link:https://github.com/tensorflowbutler&gt;@tensorflowbutler&lt;/denchmark-link&gt;
 Updated!
		</comment>
		<comment id='3' author='jhorowitz' date='2018-06-25T18:25:35Z'>
		&lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
 Could you take a look at this?
		</comment>
		<comment id='4' author='jhorowitz' date='2018-07-07T21:23:07Z'>
		Bump &lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;

I imagine you're pretty busy. I just want to make sure you've seen this.
		</comment>
		<comment id='5' author='jhorowitz' date='2018-07-09T15:52:12Z'>
		I've seen this. However I didn't write the code nor do we promise we support code in contrib to the same level. Glancing over the implementation I'm not surprised you see NaNs.
I'd happy if you submit pull requests clearing this up somehow.
		</comment>
		<comment id='6' author='jhorowitz' date='2018-07-09T18:25:24Z'>
		Hi &lt;denchmark-link:https://github.com/jhorowitz&gt;@jhorowitz&lt;/denchmark-link&gt;
, I'm wondering what would be the most helpful fix here.
I agree you can reproduce the bug by failing to provide at least one anchor-positive pair for a metric learning batch, but that also seems like kind of a rare edge case in metric learning? The safest thing seems to be to do a tf assert runtime check on the validity of the provided label set, but also maybe the right fix is just improving the documentation?
		</comment>
		<comment id='7' author='jhorowitz' date='2018-07-11T14:58:08Z'>
		Hi &lt;denchmark-link:https://github.com/coreylynch&gt;@coreylynch&lt;/denchmark-link&gt;
,
I definitely think usability could be improved via documentation but I don't think that would be enough. The issues exists in other scenarios as well when there are both positive and negative examples for any given anchor, lacking a positive was just the easiest to reproduce. The tf.assert would need to compute the triplets in order to comprehensively determine if a nan will be produced.
I can provide samples if that would be helpful. Please advise.
Thanks,
Josh
		</comment>
		<comment id='8' author='jhorowitz' date='2019-11-11T04:33:14Z'>
		Closing out this issue, as tf.contrib is no longer supported in TensorFlow 2.0.
If you would like to debug the code for  and upgrade the code for the loss function, a good home for it might be &lt;denchmark-link:https://www.github.com/tensorflow/addons&gt;TensorFlow Addons&lt;/denchmark-link&gt;
.
Thanks! ðŸ™‚
		</comment>
		<comment id='9' author='jhorowitz' date='2019-11-11T04:33:16Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20253&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20253&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>