<bug id='43449' author='woodyx218' open_date='2020-09-22T04:59:17Z' closed_time='2020-09-23T01:26:57Z'>
	<summary>tf.autodiff.ForwardAccumulator fails for Embedding layer</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): pip
TensorFlow version (use command below): 2.4.0-dev20200813
Python version: 3.8.3
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 10.1.168/7.6.5
GPU model and memory:  GeForce GTX 1050/4GB

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with:

TF 1.0: python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
TF 2.0: python -c "import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"

Describe the current behavior
Calculating the Jacobian-vector product of an embedding layer produces
AttributeError: 'IndexedSlices' object has no attribute '_as_tf_output'
Describe the expected behavior
No error, just like Dense, LSTM, convolutional layers. See a notebook link below that shows this error is ONLY related to Embedding layer.
Standalone code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
&lt;denchmark-code&gt;import tensorflow as tf

class RNN_Model(tf.keras.Model):
    def __init__(self):
        super(RNN_Model, self).__init__()
        self.embed=tf.keras.layers.Embedding(5,1)
        self.d2 = tf.keras.layers.Dense(2)
        self(tf.constant([4,3,2])) # initialize
    
    @tf.function
    def call(self, x):
        x = self.embed(x)
        return self.d2(x)
     
model=RNN_Model()

v=[tf.ones(w.shape) for w in model.trainable_variables]
with tf.autodiff.ForwardAccumulator(primals = model.trainable_variables, tangents = v) as acc:
    loss = tf.reduce_sum(tf.constant([1,0])-model(tf.constant([[2,2,2], [1,1,1]]), training=True))
acc.jvp(loss)
&lt;/denchmark-code&gt;

See a complete example in &lt;denchmark-link:https://drive.google.com/file/d/10Cb1nxcovmBSNE5zJOvRhyu-lHYhux15/view?usp=sharing&gt;this notebook&lt;/denchmark-link&gt;

Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='woodyx218' date='2020-09-22T11:17:46Z'>
		Have you tried to build the model  (model.build) after model creation model=RNN_Model()?
		</comment>
		<comment id='2' author='woodyx218' date='2020-09-22T15:00:29Z'>
		Yes I did. Still got the same error. It has nothing to do with building but with embedding layer. Please see my newly added example notebook.
		</comment>
		<comment id='3' author='woodyx218' date='2020-09-22T15:20:32Z'>
		I don't think it is only the embedding layer. Can you try to remove  @tf.function from the call in the embedding case?
		</comment>
		<comment id='4' author='woodyx218' date='2020-09-22T18:38:33Z'>
		If I remove @tf.function from the call, there is some other error. Please take a look at the notebook hyper-link. It is only the embedding layer. Keeping @tf.function works fine for other layers.
		</comment>
		<comment id='5' author='woodyx218' date='2020-09-22T18:54:01Z'>
		If I remove @tf.function from the call is working:
&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow.keras.layers import Dense, Embedding,Bidirectional,LSTM

class RNN_Model(tf.keras.Model):
    def __init__(self, dataset):
        super(RNN_Model, self).__init__()
        self.embed=Embedding(maxfeature,64)
        self.blstm = Bidirectional(LSTM(64))
        self.d1 = Dense(64, activation='relu')
        self.d2 = Dense(2)
                
    # Doing this to initialize model.trainable_variables
        for text, labels in dataset:
            self(text)
            break

    def call(self, x):
        x = self.embed(x)
        x = self.blstm(x)
        x = self.d1(x)
        x = self.d2(x)
        return tf.nn.log_softmax(x)
        
## Training Settings
batch_size = 64
maxfeature=100
SEQ_LEN=32

## Load Problems
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=maxfeature)
x_train=tf.keras.preprocessing.sequence.pad_sequences(x_train,maxlen=SEQ_LEN)
train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(5000).batch(batch_size)

model=RNN_Model(train_ds)

loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)

# Compute JVP
card = train_ds.cardinality()
step=0
for images, labels in train_ds:
    step+=1
    print("\r%d/%d" % (step , card), end="")
    v=[tf.ones(w.shape) for w in model.trainable_variables]
    with tf.autodiff.ForwardAccumulator(primals = model.trainable_variables, tangents = v) as acc:
        loss = loss_obj(labels, model(images, training=True))
    acc.jvp(loss)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='woodyx218' date='2020-09-22T18:57:58Z'>
		/cc &lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
 if could be interested in this tracing case.
		</comment>
		<comment id='7' author='woodyx218' date='2020-09-22T19:01:58Z'>
		I am afraid on my end, it shows the following error with your code:


StagingError                              Traceback (most recent call last)
 in 
44     v=[tf.ones(w.shape) for w in model.trainable_variables]
45     with tf.autodiff.ForwardAccumulator(primals = model.trainable_variables, tangents = v) as acc:
---&gt; 46         loss = loss_obj(labels, model(images, training=True))
47     acc.jvp(loss)
~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\keras\engine\base_layer.py in call(self, *args, **kwargs)
988
989         with ops.enable_auto_cast_variables(self._compute_dtype_object):
--&gt; 990           outputs = call_fn(inputs, *args, **kwargs)
991
992         if self._activity_regularizer:
 in call(self, x)
17     def call(self, x):
18         x = self.embed(x)
---&gt; 19         x = self.blstm(x)
20         x = self.d1(x)
21         x = self.d2(x)
~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\keras\layers\wrappers.py in call(self, inputs, initial_state, constants, **kwargs)
528
529     if initial_state is None and constants is None:
--&gt; 530       return super(Bidirectional, self).call(inputs, **kwargs)
531
532     # Applies the same workaround as in RNN.__call__
~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\keras\engine\base_layer.py in call(self, *args, **kwargs)
988
989         with ops.enable_auto_cast_variables(self._compute_dtype_object):
--&gt; 990           outputs = call_fn(inputs, *args, **kwargs)
991
992         if self._activity_regularizer:
~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\keras\layers\wrappers.py in call(self, inputs, training, mask, initial_state, constants)
641         forward_state, backward_state = None, None
642
--&gt; 643       y = self.forward_layer(forward_inputs,
644                              initial_state=forward_state, **kwargs)
645       y_rev = self.backward_layer(backward_inputs,
~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\keras\layers\recurrent.py in call(self, inputs, initial_state, constants, **kwargs)
660
661     if initial_state is None and constants is None:
--&gt; 662       return super(RNN, self).call(inputs, **kwargs)
663
664     # If any of initial_state or constants are specified and are Keras
~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\keras\engine\base_layer.py in call(self, *args, **kwargs)
988
989         with ops.enable_auto_cast_variables(self._compute_dtype_object):
--&gt; 990           outputs = call_fn(inputs, *args, **kwargs)
991
992         if self._activity_regularizer:
~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\keras\layers\recurrent_v2.py in call(self, inputs, mask, training, initial_state)
1269           # GPU implementation when GPU is available.
1270           if can_use_gpu:
-&gt; 1271             last_output, outputs, new_h, new_c, runtime = gpu_lstm(
1272                 **gpu_lstm_kwargs)
1273           else:
~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\keras\layers\recurrent_v2.py in gpu_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask, time_major, go_backwards, sequence_lengths)
1514       # Reverse axis 0 since the input is already convert to time major.
1515       inputs = array_ops.reverse(inputs, axis=[0])
-&gt; 1516     outputs, h, c, _ = gen_cudnn_rnn_ops.cudnn_rnn(
1517         inputs, input_h=init_h, input_c=init_c, params=params, is_training=True,
1518         rnn_mode='lstm')
~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\ops\gen_cudnn_rnn_ops.py in cudnn_rnn(input, input_h, input_c, params, rnn_mode, input_mode, direction, dropout, seed, seed2, is_training, name)
97       pass
98     try:
---&gt; 99       return cudnn_rnn_eager_fallback(
100           input, input_h, input_c, params, rnn_mode=rnn_mode,
101           input_mode=input_mode, direction=direction, dropout=dropout,
~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\ops\gen_cudnn_rnn_ops.py in cudnn_rnn_eager_fallback(input, input_h, input_c, params, rnn_mode, input_mode, direction, dropout, seed, seed2, is_training, name, ctx)
180                              attrs=_attrs, ctx=ctx, name=name)
181   if _execute.must_record_gradient():
--&gt; 182     _execute.record_gradient(
183         "CudnnRNN", _inputs_flat, _attrs, _result)
184   _result = _CudnnRNNOutput._make(_result)
~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\eager\backprop.py in _record_gradient(op_name, inputs, attrs, results)
173
174 def _record_gradient(op_name, inputs, attrs, results):
--&gt; 175   return pywrap_tfe.TFE_Py_RecordGradient(op_name, inputs, attrs, results,
176                                           ops.get_name_scope())
177
~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\eager\forwardprop.py in _jvp_dispatch(op_name, attr_tuple, inputs, outputs, tangents, use_batch)
211   # means we may trace a few more exact shapes before moving on to relaxation.
212   if _TRACE_COUNT.get(op_name, 0) &lt; _TRACE_COUNT_LIMIT:
--&gt; 213     return _jvp_exact_shapes(op_name, attr_tuple, inputs, outputs, tangents,
214                              use_batch)
215   return _jvp_relaxed_shapes(op_name, attr_tuple, inputs, outputs, tangents,
~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\eager\function.py in call(self, *args, **kwargs)
2937     with self._lock:
2938       graph_function, flat_args, flat_kwargs = 
-&gt; 2939           self._maybe_define_function(args, kwargs)
2940     return graph_function._filtered_call(flat_args, flat_kwargs)  # pylint: disable=protected-access
2941
~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\eager\function.py in _maybe_define_function(self, args, kwargs)
3342
3343       self._function_cache.missed.add(call_context_key)
-&gt; 3344       graph_function = self._create_graph_function(args, kwargs)
3345       self._function_cache.primary[cache_key] = graph_function
3346
~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\eager\function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
3187     arg_names = base_arg_names + missing_arg_names
3188     graph_function = ConcreteFunction(
-&gt; 3189         func_graph_module.func_graph_from_py_func(
3190             self._name,
3191             self._python_function,
~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\framework\func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
985         _, original_func = tf_decorator.unwrap(python_func)
986
--&gt; 987       func_outputs = python_func(*func_args, **func_kwargs)
988
989       # invariant: func_outputs contains only Tensors, CompositeTensors,
~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\framework\func_graph.py in wrapper(*args, **kwargs)
972           except Exception as e:  # pylint:disable=broad-except
973             if hasattr(e, "ag_error_metadata"):
--&gt; 974               raise e.ag_error_metadata.to_exception(e)
975             else:
976               raise
StagingError: in user code:
C:\Users\Fight4Life\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\eager\forwardprop.py:179 _jvp_helper_wrapper  *
    return _jvp_helper(op_name, attr_tuple, inputs, outputs, tangents)
C:\Users\Fight4Life\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\eager\forwardprop.py:138 _jvp_helper  **
    nontrivial_output_tangents = transpose_tape.gradient(
C:\Users\Fight4Life\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\eager\backprop.py:1080 gradient
    flat_grad = imperative_grad.imperative_grad(
C:\Users\Fight4Life\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\eager\imperative_grad.py:71 imperative_grad
    return pywrap_tfe.TFE_Py_TapeGradient(
C:\Users\Fight4Life\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\eager\backprop.py:151 _gradient_function
    grad_fn = ops._gradient_registry.lookup(op_name)  # pylint: disable=protected-access
C:\Users\Fight4Life\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\framework\registry.py:98 lookup
    raise LookupError(

LookupError: gradient registry has no entry for: CudnnRNNBackprop


		</comment>
		<comment id='8' author='woodyx218' date='2020-09-22T19:09:02Z'>
		I've just run your notebook on colab and it is running fine on CPU but we have the same error on the GPU runtime.
		</comment>
		<comment id='9' author='woodyx218' date='2020-09-22T19:11:18Z'>
		The GPU one I think that it is another issue like &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/37091#issuecomment-612205749&gt;#37091 (comment)&lt;/denchmark-link&gt;
 /cc &lt;denchmark-link:https://github.com/kaixih&gt;@kaixih&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='woodyx218' date='2020-09-23T01:26:58Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43449&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43449&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='woodyx218' date='2020-09-23T03:08:15Z'>
		Although it works on CPU now, it is still desirable to work on GPU.
		</comment>
		<comment id='12' author='woodyx218' date='2020-09-23T10:11:28Z'>
		&lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/allenlavoie&gt;@allenlavoie&lt;/denchmark-link&gt;
 Are you still interested for the tracing part of the issue?
&lt;denchmark-code&gt;@tf.function
def call(self, x):
&lt;/denchmark-code&gt;

		</comment>
		<comment id='13' author='woodyx218' date='2020-09-23T15:59:06Z'>
		Looks like the GPU issue has another bug (which is closed, but Scott would have more background there). It's just a missing RegisterGradient call for the gradient op.
&lt;denchmark-link:https://github.com/bhack&gt;@bhack&lt;/denchmark-link&gt;
 what tracing issue? The fix has a test with/without tf.function.
		</comment>
		<comment id='14' author='woodyx218' date='2020-09-23T16:13:03Z'>
		&lt;denchmark-link:https://github.com/allenlavoie&gt;@allenlavoie&lt;/denchmark-link&gt;
 Yes I've retested the code example now and it is working also on GPU (at least on Colab).
It was just a problem about timezones with tf-nightly builds to catch that commit.
		</comment>
	</comments>
</bug>