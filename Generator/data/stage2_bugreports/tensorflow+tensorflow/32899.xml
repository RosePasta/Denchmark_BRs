<bug id='32899' author='ChosunOne' open_date='2019-09-29T04:42:48Z' closed_time='2019-09-30T21:25:33Z'>
	<summary>Model training does not improve accuracy</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 19.04
TensorFlow installed from (source or binary): pip
TensorFlow version (use command below): 2.0.0-rc2
Python version: 3.7.3
CUDA/cuDNN version: release 10.0, V10.0.130
GPU model and memory: nVidia GTX 1080 Ti

Describe the current behavior
When attempting to train a sequential model on the MNIST dataset, the model remains at 11% accuracy.  This is only resolved when the inputs are scaled by 255.
Describe the expected behavior
The model should improve in accuracy.

&lt;denchmark-link:https://github.com/PacktPublishing/What-s-New-in-TensorFlow-2.0/blob/master/Chapter02/end_to_end_sequential.py&gt;https://github.com/PacktPublishing/What-s-New-in-TensorFlow-2.0/blob/master/Chapter02/end_to_end_sequential.py&lt;/denchmark-link&gt;


Please reference the issue at &lt;denchmark-link:https://github.com/PacktPublishing/What-s-New-in-TensorFlow-2.0/issues/1&gt;PacktPublishing/What-s-New-in-TensorFlow-2.0#1&lt;/denchmark-link&gt;
 for more information.  This behavior was not observed in 2.0.0-beta0
	</description>
	<comments>
		<comment id='1' author='ChosunOne' date='2019-09-30T06:44:08Z'>
		I have tried on colab with TF version 2.0 beta 1, 2.0.0-rc2 and was able to reproduce the issue. Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/2d329adc843eeca37e3c219e74515b9b/untitled233.ipynb&gt;here.&lt;/denchmark-link&gt;
 Thanks!
		</comment>
		<comment id='2' author='ChosunOne' date='2019-09-30T09:01:52Z'>
		The same Model / with inputs in range [0,255] on Pytorch gives the similar accuracy of around 11%. So I don't suppose its a tf issue.
Also, 2.0.0rc output is consistent with tf 1.14.0.
Anyway, using Adam optimizer or SGD with momentum boosts up the accuracy on both tf and PyTorch.
		</comment>
		<comment id='3' author='ChosunOne' date='2019-09-30T21:25:33Z'>
		&lt;denchmark-link:https://github.com/ChosunOne&gt;@ChosunOne&lt;/denchmark-link&gt;
 I agree with &lt;denchmark-link:https://github.com/sghoshjr&gt;@sghoshjr&lt;/denchmark-link&gt;
. If you change the learning rate from default  to  gives you ~60% training accuracy. So you need to change
&lt;denchmark-code&gt;model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.SGD(learning_rate=0.0005),
              metrics=['accuracy'])
&lt;/denchmark-code&gt;

In general, it is always suggested to normalize the inputs data so that you have faster convergence. Even without normalizing input data, you can still get better performance by tuning some hyper parameters. Please check very similar code on TF &lt;denchmark-link:https://colab.sandbox.google.com/github/tensorflow/docs/blob/r2.0rc/site/en/r2/tutorials/quickstart/beginner.ipynb&gt;website&lt;/denchmark-link&gt;
. If you want better performance, then change learning rate, optimization, normalize data etc.
I am closing the issue as it is resolved. But, please let me know if I'm mistaken.Thanks!
		</comment>
		<comment id='4' author='ChosunOne' date='2019-09-30T21:25:35Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=32899&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=32899&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>