<bug id='44854' author='abdulbasitds' open_date='2020-11-13T18:23:11Z' closed_time='2020-12-01T05:51:06Z'>
	<summary>Keras custom data generator giving dimension errors with multi input and multi output( functional api model)</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 20.04 and Colab):
TensorFlow installed from; Pip3 install ..
Tensorflow version: v2.3.0-54-gfcc4b966f1 2.3.1

Colab Code to reproduce: &lt;denchmark-link:https://colab.research.google.com/drive/1bSJm44MMDCWDU8IrG2GXKBvXNHCuY70G?usp=sharing&gt;https://colab.research.google.com/drive/1bSJm44MMDCWDU8IrG2GXKBvXNHCuY70G?usp=sharing&lt;/denchmark-link&gt;

Note: I have updated the issue at the end as well.
I have written a generator function with Keras, before returning X,y from __getitem__ I have double check the shapes of the X's and Y's and they are alright, but generator is giving dimension mismatch array and warnings.
I suspect that the problem is relating multiple input, which is being cosidered one by the input layer of tf.keras. Each input features is of shape (32,10,1) but yielding [input_array_1,input_array_2,input_array_3] makes it (3,32,10,1)
My training and validation generators are pretty musch same as
&lt;denchmark-code&gt;class ValidGenerator(Sequence):
    def __init__(self, df, batch_size=64):
        self.batch_size = batch_size
        self.df = df
        self.indices = self.df.index.tolist()
        self.num_classes = num_classes
        self.shuffle = shuffle
        self.on_epoch_end()

    def __len__(self):
        return int(len(self.indices) // self.batch_size)

    def __getitem__(self, index):
        index = self.index[index * self.batch_size:(index + 1) * self.batch_size]
        batch = [self.indices[k] for k in index]
        
        X, y = self.__get_data(batch)
        return X, y

    def on_epoch_end(self):
        self.index = np.arange(len(self.indices))
        if self.shuffle == True:
            np.random.shuffle(self.index)

    def __get_data(self, batch):
        #some logic is written here
        #hat prepares 3 X features and 3 Y outputs 
        X = [input_array_1,input_array_2,input_array_3]
        y = [out_1,out_2,out_3]
        #print(len(X))
        
        return X, y
&lt;/denchmark-code&gt;

I am returning tuple of X,y from which has 3 input features and 3 output features each, so shape of X is (3,32,10,1)
I am using functional api to build model(I have things like concatenation, multi input/output, which isnt possible with sequential)  with following structure
&lt;denchmark-link:https://i.stack.imgur.com/EaShF.png&gt;&lt;/denchmark-link&gt;

When I try to fit the model with generator with following code
&lt;denchmark-code&gt;train_datagen = TrainGenerator(df=train_df,  batch_size=32, num_classes=None, shuffle=True)
valid_datagen = ValidGenerator(df=train_df,  batch_size=32, num_classes=None, shuffle=True)
model.fit(train_datagen, epochs=2,verbose=1,callbacks=[checkpoint,es])
&lt;/denchmark-code&gt;

I get these warnings and errors, that dont go away

Epoch 1/2
WARNING:tensorflow:Model was constructed with shape (None, 10) for input &gt;Tensor("input_1:0", shape=(None, 10), dtype=float32), but it was called &gt;on an input with incompatible shape (None, None, None).


WARNING:tensorflow:Model was constructed with shape (None, 10) for input
Tensor("input_2:0", shape=(None, 10), dtype=float32), but it was
called on an input with incompatible shape (None, None, None).
WARNING:tensorflow:Model was constructed with shape (None, 10) for
input Tensor("input_3:0", shape=(None, 10), dtype=float32), but it was
called on an input with incompatible shape (None, None, None).
...
...
call
return super(RNN, self).call(inputs, **kwargs)
/home/eduardo/.virtualenvs/kgpu3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:975
call
input_spec.assert_input_compatibility(self.input_spec, inputs,
/home/eduardo/.virtualenvs/kgpu3/lib/python3.8/site-packages/tensorflow/python/keras/engine/input_spec.py:176
assert_input_compatibility
raise ValueError('Input ' + str(input_index) + ' of layer ' +
ValueError: Input 0 of layer lstm is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: [None, None, None, 88]


I have rechecked whole code and it isnt possible to have input (None,None,None) like in warning or in error, my input dimension is (3,32,10,1)
Update
I have also tried to write a generator function with python and got exactly same error.
My generator function
&lt;denchmark-code&gt;def generate_arrays_from_file(batchsize,df):
    #print(bat)
    inputs = []
    targets = []
    batchcount = 0
    while True:
            
            df3 = df.loc[np.arange(batchcount*batchsize,(batchcount*batchsize)+batchsize)]
            #Some pre processing
            X = [input_array_1,input_array_2,input_array_3]
            y = [out_1,out_2,out_3]
            yield X,y 
            batchcount = batchcount +1
&lt;/denchmark-code&gt;

It seems like it is something wrong internally wit keras (may be due to the fact I am using functional API)
Update 2
I also tried to output tuple
&lt;denchmark-code&gt;          X = (input1_X,input2_X,input3_X)
          y = (output1_y,output2_y,output3_y)
&lt;/denchmark-code&gt;

and also named input/output, but it doesnt work
&lt;denchmark-code&gt;        X =  {"input_1": input1_X, "input_2": input2_X,"input_3": input3_X}
        y = {"output_1": output1_y, "output_2": output2_y,"output_3": output3_y}
&lt;/denchmark-code&gt;

Note about problem formulation:
Changing the individual X features to shape (32,10) instead of (32,10,1) might help to get rid of this error but that is not what I want, it changes my problem(I no longer have 10 time steps with one feature each)
	</description>
	<comments>
		<comment id='1' author='abdulbasitds' date='2020-11-13T21:00:38Z'>
		Can you reorganize your code s that we could just copy, paste and run (or share a Colab)? Thanks
		</comment>
		<comment id='2' author='abdulbasitds' date='2020-11-14T17:49:34Z'>
		&lt;denchmark-link:https://github.com/bhack&gt;@bhack&lt;/denchmark-link&gt;
 thankyou for the response, I have updated the answer with the link and other things that i tried to resolve.
		</comment>
		<comment id='3' author='abdulbasitds' date='2020-11-14T18:17:31Z'>
		Thanks, but this is a support request not a bug on your specific use case so you need to use &lt;denchmark-link:https://stackoverflow.com/questions/tagged/tensorflow&gt;https://stackoverflow.com/questions/tagged/tensorflow&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='abdulbasitds' date='2020-11-14T18:21:01Z'>
		&lt;denchmark-link:https://github.com/bhack&gt;@bhack&lt;/denchmark-link&gt;
  Thankyou for the response.
I have already posted it on stack overflow and also, I am pretty sure it is bug inside fitting function relating to 1) generator as an input 2) functional API
		</comment>
		<comment id='5' author='abdulbasitds' date='2020-11-14T18:22:15Z'>
		
@bhack  Thankyou for the response.
I have already posted it on stack overflow and also, I am pretty sure it is bug inside fitting function relating to 1) generator as an input 2) functional API

Do you mean a Tensorflow bug?
		</comment>
		<comment id='6' author='abdulbasitds' date='2020-11-14T18:24:15Z'>
		&lt;denchmark-link:https://github.com/bhack&gt;@bhack&lt;/denchmark-link&gt;
  sorry if my understanding i write, Keras is part of tensorflow right? I mean how  handles generator is buggy in case of multiple input. So, I am not sure if you consider it tensorflow bug or keras?
		</comment>
		<comment id='7' author='abdulbasitds' date='2020-11-14T18:27:10Z'>
		If you still think that It is a TF/Keras bug I think you can reproduce it with less than 10 lines of code.
Can you minimize your code surface to reproduce the bug?
		</comment>
		<comment id='8' author='abdulbasitds' date='2020-11-14T18:33:03Z'>
		&lt;denchmark-link:https://github.com/bhack&gt;@bhack&lt;/denchmark-link&gt;
 There are two examples for the bug (each with approximately 10-15 lines), which I am afraid are required as the problem relates to multi input/ multi-output generator.
the two examples are of

Keras generator
Custom generator

And model.fit suffers same problem for the both.
I have &lt;denchmark-link:https://stackoverflow.com/questions/64811632/keras-custom-data-generator-giving-dimension-errors-with-multi-input-and-multi-o&gt;posted it as a question on stackoverflow&lt;/denchmark-link&gt;
 and it seems that there isnt any solution or problem isnt relating my code or how i write the generator
		</comment>
		<comment id='9' author='abdulbasitds' date='2020-11-15T00:29:56Z'>
		I still think it is not a bug but a stackoverflow support question.
You need to use reshape(dlen,pad_len) and reshape(dlen,pad_len)
		</comment>
		<comment id='10' author='abdulbasitds' date='2020-11-15T12:45:54Z'>
		&lt;denchmark-link:https://github.com/bhack&gt;@bhack&lt;/denchmark-link&gt;
 I think there is a typo in last part of the comment,you wrote  two time.
And reshape(dlen,pad_len) is not same as reshape(dlen,pad_len,1) and it is totally different modelling situation in case of NLP and LSTMS.
I dont know if you have interest in sequential modelling, but I will try to explain. If you reshape the data of format (batch,timestep,features), which in my case is (32,10,1) , to (batch,timestep), then you loose the purpose of sequential modelling, because you basically are having 10 features and one time step now, that makes it equal to using feed forward network and will lose purpose of using LSTMs.
So, the way generator expects the shape of input, it is not possible to have multiple inputs and input of shape (batch,timestamp,no_features) simultaneously.
I have added the reference to many-to-one sequence prediction problem below, that can help to understand why (dlen,pad_len) is not same as (dlen,pad_len,1)


https://stackoverflow.com/questions/43034960/many-to-one-and-many-to-many-lstm-examples-in-keras



https://stackabuse.com/solving-sequence-problems-with-lstm-in-keras/



		</comment>
		<comment id='11' author='abdulbasitds' date='2020-11-15T13:00:56Z'>
		Yes it was a typo the second was (dlen).
The problem is that for generators in your original code sample you had:

ValueError: Input 0 of layer lstm is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: [None, None, None, 88]

So It Is ndim=4 and not (batch,timestep,features)
		</comment>
		<comment id='12' author='abdulbasitds' date='2020-11-15T13:06:23Z'>
		It is 4 dimensional because I am yielding 3 inputs (X = [input_array_1,input_array_2,input_array_3]), which takes (batch,timestep,features) to dimension (3,batch,timestep,features).
fit() method doesnt recognises the fact that we are returning 3 input features not one. i,e, it doesn't understands that it is [(batch,time-step,features),(batch,time-step,features),(batch,timestep,features)], it takes it as single input of (3,batch,time-step,features)
I am not sure if I have conveyed the point, let me know if it not, I will explain more :)
		</comment>
		<comment id='13' author='abdulbasitds' date='2020-11-16T13:25:34Z'>
		Ok just to investigate more, can you try to reformulate a very small snippet with &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=nightly#from_generator&gt;tf.data.Dataset.from_generator&lt;/denchmark-link&gt;
 with your data (cause internally it is what is used) without the model definition or the other code.
Just to check what kind of dataset we have in your case.
		</comment>
		<comment id='14' author='abdulbasitds' date='2020-11-16T18:16:51Z'>
		&lt;denchmark-link:https://github.com/bhack&gt;@bhack&lt;/denchmark-link&gt;
  I have created a generator with as minimum code as possible with 
Please the colab working link below, let me know if it is what you asked or i get it worng
&lt;denchmark-link:https://colab.research.google.com/drive/1fwYqGGdhISGlmJGlkZAeVW_qP8mS1t0V?usp=sharing&gt;https://colab.research.google.com/drive/1fwYqGGdhISGlmJGlkZAeVW_qP8mS1t0V?usp=sharing&lt;/denchmark-link&gt;

		</comment>
		<comment id='15' author='abdulbasitds' date='2020-11-16T18:44:14Z'>
		I suppose that this Dataset is working with your model/fit right? Did you try?
&lt;denchmark-code&gt;for element in ds_counter:
  print(element[0]["input_1"].shape)
  print(element[0]["input_2"].shape)
  print(element[0]["input_3"].shape)
  print(element[1]["output_1"].shape)
  print(element[1]["output_2"].shape)
  print(element[1]["output_3"].shape)
  break
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;(32, 10, 1)
(32, 10, 1)
(32, 10, 1)
(32, 71)
(32, 487)
(32, 85)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='16' author='abdulbasitds' date='2020-11-16T18:54:07Z'>
		&lt;denchmark-link:https://github.com/bhack&gt;@bhack&lt;/denchmark-link&gt;
, can i pass  directly to the fit method like generator(sorry I am not very much aware of this dt.data.dataset thing) as below?
&lt;denchmark-code&gt;   model.fit(ds_counter,epochs=2,verbose=1)
&lt;/denchmark-code&gt;

it gives error
TypeError: 'NoneType' object is not callable
Or shoud i do something like
&lt;denchmark-code&gt;for element in ds_counter:
  print(element[0]["input_1"].shape)
  print(element[0]["input_2"].shape)
  print(element[0]["input_3"].shape)
  print(element[1]["output_1"].shape)
  print(element[1]["output_2"].shape)
  print(element[1]["output_3"].shape)
  model.fit([element[0]["input_1"],element[0]["input_2"],element[0]["input_3"]],[element[1]["output_1"],element[1]["output_2"],element[1]["output_3"]],epochs=2,verbose=1)
&lt;/denchmark-code&gt;

I get TypeError: 'NoneType' object is not callable with this as well
		</comment>
		<comment id='17' author='abdulbasitds' date='2020-11-16T19:04:01Z'>
		sorry ignore my previous comment, thankyou for refering to this approach.
the nonetype error wasnt related with this scenerio.
Should i run it like this?
&lt;denchmark-code&gt;tr_dataset = ds_counter.batch(batch_size=32)
tr_dataset = tr_dataset.repeat()
model.fit(tr_dataset,epochs=2,verbose=1)
&lt;/denchmark-code&gt;

If it works with this approach, does it means that it cant be done with usual generator?
		</comment>
		<comment id='18' author='abdulbasitds' date='2020-11-16T19:17:41Z'>
		Yes fit can consume dataset as x

A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights).

Let me know if it works so that we could check how internally is called tf.data.Dataset.from_generator in your old Generator/fit case.
		</comment>
		<comment id='19' author='abdulbasitds' date='2020-11-16T19:21:12Z'>
		Thankyou for looking into this
Doing model.fit(ds_counter,epochs=2,verbose=1) gives this error

InvalidArgumentError: 2 root error(s) found.
(0) Invalid argument:  transpose expects a vector of size 4. But input(1) is a vector of size 3
[[node functional_3/lstm_3/transpose_1 (defined at :1) ]]
[[gradient_tape/functional_3/embedding_4/embedding_lookup/Reshape/_202]]


(1) Invalid argument:  transpose expects a vector of size 4. But input(1) is a vector of size 3
[[node functional_3/lstm_3/transpose_1 (defined at :1) ]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_25790]
Function call stack:
train_function -&gt; train_function

		</comment>
		<comment id='20' author='abdulbasitds' date='2020-11-16T19:32:54Z'>
		Can you share this new example?
		</comment>
		<comment id='21' author='abdulbasitds' date='2020-11-16T19:37:54Z'>
		I have updated the same colab
&lt;denchmark-link:https://colab.research.google.com/drive/1fwYqGGdhISGlmJGlkZAeVW_qP8mS1t0V#scrollTo=pUzbIleKYhDW&gt;https://colab.research.google.com/drive/1fwYqGGdhISGlmJGlkZAeVW_qP8mS1t0V#scrollTo=pUzbIleKYhDW&lt;/denchmark-link&gt;

But it seems that the I have this error

ValueError: as_list() is not defined on an unknown TensorShape.


I previously used ds_counter.batch(batch_size=32) and repeat() etc which might have cuased the last error
		</comment>
		<comment id='22' author='abdulbasitds' date='2020-11-16T19:49:56Z'>
		That one is a known bug &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/32912&gt;#32912&lt;/denchmark-link&gt;

You could enforce the shape:
&lt;denchmark-code&gt;output_shapes=({"input_1": [None,10,1], "input_2": [None,10,1],"input_3": [None,10,1]}, {"output_1": [None,71], "output_2": [None,487],"output_3": [None,85]}
&lt;/denchmark-code&gt;

		</comment>
		<comment id='23' author='abdulbasitds' date='2020-11-16T20:04:35Z'>
		As an alternative you could also use &lt;denchmark-link:https://www.tensorflow.org/tutorials/load_data/pandas_dataframe#alternative_to_feature_columns&gt;https://www.tensorflow.org/tutorials/load_data/pandas_dataframe#alternative_to_feature_columns&lt;/denchmark-link&gt;

		</comment>
		<comment id='24' author='abdulbasitds' date='2020-11-16T20:07:36Z'>
		Thankyou for sharing this, it did something as one epoch ran almost midway before crashing.
it produces following error

KeyError: "Passing list-likes to .loc or [] with any missing labels is no longer supported. The following labels were missing: Int64Index([1000, 1001, 1002, 1003, 1004,\n            ...\n            1019, 1020, 1021, 1022, 1023],\n           dtype='int64', length=24). See https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike"

		</comment>
		<comment id='25' author='abdulbasitds' date='2020-11-16T20:10:33Z'>
		Edit: i think this error isnt related to the problem here, let me check again
I added output_shapes=({"input_1": [None,10,1], "input_2": [None,10,1],"input_3": [None,10,1]}, {"output_1": [None,71], "output_2": [None,487],"output_3": [None,85]} to from_generator()
		</comment>
		<comment id='26' author='abdulbasitds' date='2020-11-16T21:09:55Z'>
		This is a different issue check if you have missing labels
		</comment>
		<comment id='27' author='abdulbasitds' date='2020-11-16T21:25:56Z'>
		Thankyou. Yes, I was doing a mistake, I was already hanling batch in the function and was trying to use batch() fro dataset as well.
This example works now with  and .
&lt;denchmark-link:https://colab.research.google.com/drive/1fwYqGGdhISGlmJGlkZAeVW_qP8mS1t0V#scrollTo=WoDme6jVZQtz&gt;https://colab.research.google.com/drive/1fwYqGGdhISGlmJGlkZAeVW_qP8mS1t0V#scrollTo=WoDme6jVZQtz&lt;/denchmark-link&gt;

May be we can figure out why keras generator with get_item wasnt working looking at this dataset implementation
		</comment>
		<comment id='28' author='abdulbasitds' date='2020-11-16T21:27:10Z'>
		Probably you need to limit/reset batchcount
		</comment>
		<comment id='29' author='abdulbasitds' date='2020-11-16T21:28:26Z'>
		in keras generator (extends from Sequence) one? or this one? this one is working fine now
		</comment>
		<comment id='30' author='abdulbasitds' date='2020-11-16T21:31:18Z'>
		I meant this one but if you have the batch under control ok..

May be we can figure out why keras generator with get_item wasnt working looking at this dataset implementation

What I can see is that with your old Sequence/Generator/fit code it was going to internally create the Dataset object with this parameters:
&lt;denchmark-code&gt;dataset = tf.data.Dataset.from_generator(
    gen,
    output_types=((tf.int64, tf.int64, tf.int64), (tf.float32, tf.float32,
                                                   tf.float32)),
    output_shapes=((tf.TensorShape([None, None,
                                    None]), tf.TensorShape([None, None, None]),
                    tf.TensorShape([None, None,
                                    None])), (tf.TensorShape([None, None]),
                                              tf.TensorShape([None, None]),
                                              tf.TensorShape([None, None]))))
&lt;/denchmark-code&gt;

		</comment>
		<comment id='31' author='abdulbasitds' date='2020-11-16T21:35:48Z'>
		As you can see the flow to DatasetV2.from_generator in:



tensorflow/tensorflow/python/keras/engine/data_adapter.py


        Lines 798 to 811
      in
      a52c2d0






 output_shapes = nest.map_structure(_get_dynamic_shape, peek) 



 output_types = nest.map_structure(lambda t: t.dtype, peek) 



 



 # Note that dataset API takes a callable that creates a generator object, 



 # rather than generator itself, which is why we define a function here. 



 generator_fn = self._handle_multiprocessing(x, workers, use_multiprocessing, 



 max_queue_size) 



 



 def wrapped_generator(): 



 for data in generator_fn(): 



 yield self._standardize_batch(data) 



 



 dataset = dataset_ops.DatasetV2.from_generator( 



 wrapped_generator, output_types, output_shapes=output_shapes) 





		</comment>
		<comment id='32' author='abdulbasitds' date='2020-11-16T21:52:21Z'>
		I think i found a similar issue, will look into your pointers and this issue tommorow
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/20698&gt;#20698&lt;/denchmark-link&gt;

		</comment>
		<comment id='33' author='abdulbasitds' date='2020-11-16T22:57:26Z'>
		/cc &lt;denchmark-link:https://github.com/omalleyt12&gt;@omalleyt12&lt;/denchmark-link&gt;
 What do you think?
		</comment>
		<comment id='34' author='abdulbasitds' date='2020-11-24T05:26:03Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='35' author='abdulbasitds' date='2020-12-01T05:51:04Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='36' author='abdulbasitds' date='2020-12-01T05:51:07Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44854&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44854&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>