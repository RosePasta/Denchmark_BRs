<bug id='5277' author='sjperkins' open_date='2016-10-29T17:23:34Z' closed_time='2018-02-02T23:43:25Z'>
	<summary>Eigen implemented CPU op is 10 times slower than OpenMP</summary>
	<description>
I implemented a phase CPU operator consisting of four loop levels. I couldn't find any Eigen tensor docs at that stage so I use OpenMP to trivially parallelise the outer loops. I recently found the Eigen tensor documentation so I thought I'd take advantage of it and get all the multithreading/AVX/SSE goodies for free!
Unfortunately the Eigen version is about 10 times slower!
&lt;denchmark-h:h3&gt;What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Environment info&lt;/denchmark-h&gt;

Operating System: Ubuntu 16.04
Installed version of CUDA and cuDNN: CUDA 8.0 and cuDNN 5.1
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
&lt;denchmark-code&gt;$ ls -l /usr/local/cuda-8.0/lib64/libcud*
-rw-r--r-- 1 root root 558720 Sep 15 01:02 /usr/local/cuda-8.0/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root     16 Sep 15 01:05 /usr/local/cuda-8.0/lib64/libcudart.so -&gt; libcudart.so.8.0
lrwxrwxrwx 1 root root     19 Sep 15 01:05 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -&gt; libcudart.so.8.0.44
-rw-r--r-- 1 root root 415432 Sep 15 01:02 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.44
-rw-r--r-- 1 root root 775162 Sep 15 01:02 /usr/local/cuda-8.0/lib64/libcudart_static.a
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;$ ls -l /usr/local/cudnn-5.1-cuda-8.0/lib64/lib*
lrwxrwxrwx 1 root root       13 Oct 28 10:07 /usr/local/cudnn-5.1-cuda-8.0/lib64/libcudnn.so -&gt; libcudnn.so.5
lrwxrwxrwx 1 root root       17 Oct 28 10:07 /usr/local/cudnn-5.1-cuda-8.0/lib64/libcudnn.so.5 -&gt; libcudnn.so.5.1.5
-rwxr-xr-x 1 root root 79337624 Oct 28 10:07 /usr/local/cudnn-5.1-cuda-8.0/lib64/libcudnn.so.5.1.5
-rw-r--r-- 1 root root 69756172 Oct 28 10:07 /usr/local/cudnn-5.1-cuda-8.0/lib64/libcudnn_static.a
&lt;/denchmark-code&gt;

If installed from binary pip package, provide:

A link to the pip package you installed: python 2.7 linux GPU nightly
The output from python -c "import tensorflow; print(tensorflow.__version__)".

&lt;denchmark-code&gt;$ python -c "import tensorflow; print(tensorflow.__version__)"
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcurand.so locally
0.11.0rc1
&lt;/denchmark-code&gt;

If installed from source, provide

The commit hash (git rev-parse HEAD)
The output of bazel version

&lt;denchmark-h:h3&gt;If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)&lt;/denchmark-h&gt;


Source code.
Makefile. In terms of optimisations I'm using -O2 and -fopenmp
Test Case and timing code.

&lt;denchmark-h:h4&gt;OpenMP&lt;/denchmark-h&gt;


Timings

&lt;denchmark-code&gt;Tensorflow custom GPU time 0.342187
Tensorflow expression GPU time 0.270267
Tensorflow CPU time 0.417076
Numpy CPU time 2.542890
&lt;/denchmark-code&gt;


Code

// Compute the complex phase
#pragma omp parallel for
for(int src=0; src&lt;nsrc; ++src)
{
    FT l = lm(src,0);
    FT m = lm(src,1);
    FT n = std::sqrt(1.0 - l*l - m*m) - 1.0;

    for(int time=0; time&lt;ntime; ++time)
    {
        for(int antenna=0; antenna&lt;na; ++antenna)
        {
            FT u = uvw(time,antenna,0);
            FT v = uvw(time,antenna,1);
            FT w = uvw(time,antenna,2);

            FT real_phase_base = minus_two_pi_over_c*(l*u + m*v + n*w);

            for(int chan=0; chan&lt;nchan; ++chan)
            {
                // Our real phase input to the exponential function is purely imaginary so we can
                // can elide a call to std::exp&lt;complex&lt;FT&gt;&gt; and just compute the cos and sin
                FT real_phase = real_phase_base*frequency(chan);
                complex_phase(src,time,antenna,chan) = { std::cos(real_phase), std::sin(real_phase) };
            }
        }
    }
}
&lt;denchmark-h:h4&gt;Eigen&lt;/denchmark-h&gt;


Timings

&lt;denchmark-code&gt;Tensorflow custom GPU time 0.344653
Tensorflow expression GPU time 0.275525
Tensorflow CPU time 9.616667
Numpy CPU time 2.505482
&lt;/denchmark-code&gt;


Code

// Doing it this way might give us SIMD's and threading automatically...
const CPUDevice &amp; device = context-&gt;eigen_device&lt;CPUDevice&gt;();

// Shapes for reshaping and broadcasting
Eigen::DSizes&lt;int, 4&gt;   lm_shape(nsrc, 1,     1,  1    );
Eigen::DSizes&lt;int, 4&gt;  uvw_shape(1,    ntime, na, 1    );
Eigen::DSizes&lt;int, 4&gt; freq_shape(1,    1,     1,  nchan);

auto l = lm.slice(
        Eigen::DSizes&lt;int, 2&gt;(0,    0),
        Eigen::DSizes&lt;int, 2&gt;(nsrc, 1))
    .reshape(lm_shape);
auto m = lm.slice(
        Eigen::DSizes&lt;int, 2&gt;(0,    1),
        Eigen::DSizes&lt;int, 2&gt;(nsrc, 1))
    .reshape(lm_shape);

auto u = uvw.slice(
        Eigen::DSizes&lt;int, 3&gt;(0,     0,  0),
        Eigen::DSizes&lt;int, 3&gt;(ntime, na, 1))
    .reshape(uvw_shape);

auto v = uvw.slice(
        Eigen::DSizes&lt;int, 3&gt;(0,     0,  1),
        Eigen::DSizes&lt;int, 3&gt;(ntime, na, 1))
    .reshape(uvw_shape);

auto w = uvw.slice(
        Eigen::DSizes&lt;int, 3&gt;(0,     0,  2),
        Eigen::DSizes&lt;int, 3&gt;(ntime, na, 1))
    .reshape(uvw_shape);

// Compute n
auto n = (l.constant(1.0) - l*l - m*m).sqrt() - l.constant(1.0);

// Compute the real phase
auto real_phase = (
    l.broadcast(uvw_shape)*u.broadcast(lm_shape) +
    m.broadcast(uvw_shape)*v.broadcast(lm_shape) +
    n.broadcast(uvw_shape)*w.broadcast(lm_shape))
        .broadcast(freq_shape);

// Reshape and broadcast frequency to match real_phase
auto f = frequency.reshape(freq_shape).broadcast(
    Eigen::DSizes&lt;int, 4&gt;(nsrc, ntime, na, 1));

// Calculate the phase
auto phase = real_phase*f*real_phase.constant(minus_two_pi_over_c);
auto sinp = phase.unaryExpr(Eigen::internal::scalar_sin_op&lt;FT&gt;());
auto cosp = phase.unaryExpr(Eigen::internal::scalar_cos_op&lt;FT&gt;());

// Now evaluate the complex phase on the device
// by combining the cosine and sine of the phase
// to form a complex number
complex_phase.device(device) = cosp.binaryExpr(
    sinp, make_complex_functor&lt;FT&gt;());
&lt;denchmark-h:h3&gt;What other attempted solutions have you tried?&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Logs or other output that would be helpful&lt;/denchmark-h&gt;

(If logs are large, please upload as attachment or provide link).
	</description>
	<comments>
		<comment id='1' author='sjperkins' date='2016-10-29T17:39:09Z'>
		Thanks! That's pretty interesting. That's an Eigen issue technically, but it might prompt us to have both ops available, at least for comparison's sake.
&lt;denchmark-link:https://github.com/annarev&gt;@annarev&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/benoitsteiner&gt;@benoitsteiner&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='sjperkins' date='2016-10-30T08:46:01Z'>
		It's likely that part of this problem is that sinp and cosp depends on a common sub expression, phase. Eigen is probably evaluating it, and most of the computation,  twice, if it doesn't reuse phase. Even so, there's about an order of magnitude difference in the implementations.
I'll try force early evaluation of phase some time to confirm.
		</comment>
		<comment id='3' author='sjperkins' date='2016-10-30T20:25:39Z'>
		Early evaluation of the common sub expression does reduce the overall Eigen compute time by a factor of 2 to 4.71 seconds.
		</comment>
		<comment id='4' author='sjperkins' date='2016-11-02T02:13:59Z'>
		&lt;denchmark-link:https://github.com/sjperkins&gt;@sjperkins&lt;/denchmark-link&gt;
 Can you try to see how much speedup you get from IndexList ? For example, you can replace Eigen::DSizes&lt;int, 3&gt;(0,     0,  1) with Eigen::IndexListEigen::type2index&lt;0, Eigen::type2index&lt;0&gt;, Eigen::type2index&lt;1&gt;&gt;.
You can also mix in runtime values. For example, you could replace Eigen::DSizes&lt;int, 4&gt;   lm_shape(nsrc, 1,     1,  1    ); with Eigen::IndexList&lt;int, Eigen::type2index&lt;1&gt;, Eigen::type2index&lt;1&gt;, Eigen::type2index&lt;1&gt;&gt; list; list.set(0, nsrc);
		</comment>
		<comment id='5' author='sjperkins' date='2016-11-02T11:50:32Z'>
		&lt;denchmark-link:https://github.com/benoitsteiner&gt;@benoitsteiner&lt;/denchmark-link&gt;
 OK, with  and early evaluation of the  sub-expressions Eigen now takes  seconds vs OpenMP's  seconds. Did this in &lt;denchmark-link:https://github.com/ska-sa/montblanc/commit/7b7a43aff8f1ad537d23e44051ecb51d507d8ccf&gt;ska-sa/montblanc@7b7a43a&lt;/denchmark-link&gt;
.
Relevant code is here:
// Doing it this way might give us SIMD's and threading automatically...
const CPUDevice &amp; device = context-&gt;eigen_device&lt;CPUDevice&gt;();

using idx0 = Eigen::type2index&lt;0&gt;;
using idx1 = Eigen::type2index&lt;1&gt;;
using idx2 = Eigen::type2index&lt;2&gt;;

// Shapes for reshaping and broadcasting
Eigen::IndexList&lt;int, idx1, idx1, idx1&gt; lm_shape;
lm_shape.set(0, nsrc);

Eigen::IndexList&lt;idx1, int, int, idx1&gt; uvw_shape;
uvw_shape.set(1, ntime);
uvw_shape.set(2, na);

Eigen::IndexList&lt;idx1, idx1, idx1, int&gt; freq_shape;
freq_shape.set(3, nchan);

Eigen::IndexList&lt;idx0, idx0&gt; l_slice_offset;
Eigen::IndexList&lt;idx0, idx1&gt; m_slice_offset;

Eigen::IndexList&lt;int, idx1&gt; lm_slice_size;
lm_slice_size.set(0, nsrc);

// Slice lm to get l and m arrays
auto l = lm.slice(l_slice_offset, lm_slice_size)
    .reshape(lm_shape);
auto m = lm.slice(m_slice_offset, lm_slice_size)
    .reshape(lm_shape);

Eigen::IndexList&lt;idx0, idx0, idx0&gt; u_slice_offset;
Eigen::IndexList&lt;idx0, idx0, idx1&gt; v_slice_offset;
Eigen::IndexList&lt;idx0, idx0, idx2&gt; w_slice_offset;
Eigen::IndexList&lt;int, int, idx1&gt; uvw_slice_size;
uvw_slice_size.set(0, ntime);
uvw_slice_size.set(1,  na);

// Slice uvw to get u, v and w arrays
auto u = uvw.slice(u_slice_offset, uvw_slice_size)
    .reshape(uvw_shape);

auto v = uvw.slice(v_slice_offset, uvw_slice_size)
    .reshape(uvw_shape);

auto w = uvw.slice(w_slice_offset, uvw_slice_size)
    .reshape(uvw_shape);

// Compute n
auto n = (l.constant(1.0) - l*l - m*m).sqrt() - l.constant(1.0);

// Compute the real phase
auto real_phase = (
    l.broadcast(uvw_shape)*u.broadcast(lm_shape) +
    m.broadcast(uvw_shape)*v.broadcast(lm_shape) +
    n.broadcast(uvw_shape)*w.broadcast(lm_shape))
        .broadcast(freq_shape);

Eigen::IndexList&lt;int, int, int, idx1&gt; freq_broad;
freq_broad.set(0, nsrc);
freq_broad.set(1, ntime);
freq_broad.set(2, na);

// Reshape and broadcast frequency to match real_phase
auto f = frequency.reshape(freq_shape).broadcast(freq_broad);

// Evaluate common sub-expression early so that its
// not recomputed twice for sin and cosine.
Eigen::Tensor&lt;FT, 4, Eigen::RowMajor&gt; phase(nsrc, ntime, na, nchan);
phase.device(device) = real_phase*f*real_phase.constant(minus_two_pi_over_c);
// Calculate the phase
//auto phase = real_phase*f*real_phase.constant(minus_two_pi_over_c);
auto sinp = phase.unaryExpr(Eigen::internal::scalar_sin_op&lt;FT&gt;());
auto cosp = phase.unaryExpr(Eigen::internal::scalar_cos_op&lt;FT&gt;());

// Now compute the complex phase by combining the cosine
// and sine of the phase to from a complex number
complex_phase.device(device) = cosp.binaryExpr(
    sinp, make_complex_functor&lt;FT&gt;());

		</comment>
		<comment id='6' author='sjperkins' date='2016-11-04T02:51:06Z'>
		There are 2 simple things you can do to speed things up:
l_l -&gt; l.square()
m_m -&gt; m.square()
Also, you're using l and m both to compute real_phase and n: you could force their evaluation (like you've done for phase) to avoid their recomputation.
Last but not least, try to add an eval() before the broadcasts. For example, u.broadcast(lm_shape) will probably evaluate faster if you write it as u.eval().broadcast(lm_shape)
		</comment>
		<comment id='7' author='sjperkins' date='2016-11-04T07:41:18Z'>
		Those changes take the total evaluation time to between 2.87 and 2.89 seconds.
Implemented in &lt;denchmark-link:https://github.com/ska-sa/montblanc/commit/2400e551861e633c3bc27bbf0ee1f36ef7ab03fd&gt;ska-sa/montblanc@2400e55&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='8' author='sjperkins' date='2017-06-16T17:52:00Z'>
		&lt;denchmark-link:https://github.com/tfboyd&gt;@tfboyd&lt;/denchmark-link&gt;
 If we decide to worry more about CPU performance, this information may come in handy. Not sure whether this holds up considering AVX or MKL.
		</comment>
		<comment id='9' author='sjperkins' date='2017-12-22T07:38:58Z'>
		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
		</comment>
		<comment id='10' author='sjperkins' date='2018-01-05T19:12:02Z'>
		Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='11' author='sjperkins' date='2018-01-24T13:21:38Z'>
		Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='12' author='sjperkins' date='2018-01-29T07:26:20Z'>
		&lt;denchmark-link:https://github.com/tatianashp&gt;@tatianashp&lt;/denchmark-link&gt;
 FYI for the ongoing OpenMP discussions.
		</comment>
		<comment id='13' author='sjperkins' date='2018-02-02T23:43:07Z'>
		It's a custom op with two implementations - OpenMP and Eigen. OpenMP is default.   The code is at &lt;denchmark-link:https://github.com/ska-sa/montblanc/blob/master/montblanc/impl/rime/tensorflow/rime_ops/phase_op_cpu.h&gt;https://github.com/ska-sa/montblanc/blob/master/montblanc/impl/rime/tensorflow/rime_ops/phase_op_cpu.h&lt;/denchmark-link&gt;
.  Performance gain from using OpenMP is impressive. Thanks for the pointer.
		</comment>
	</comments>
</bug>