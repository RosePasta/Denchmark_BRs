<bug id='25486' author='Vooblin' open_date='2019-02-04T14:25:39Z' closed_time='2019-02-08T09:21:42Z'>
	<summary>[Bug, Quantization, MKL] Rounding error of fake quantization not corresponds to rounding error of real quantization</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.12.0
Python version: 3.5
Bazel version (if compiling from source): 0.19.2
GCC/Compiler version (if compiling from source): C++ (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
CUDA/cuDNN version: No
GPU model and memory: No

Describe the current behavior
There is considering quantized_conv2d() operation. If kernel type is quint8 than simple kernel will be performed. But if kernel type is qint8 than mkl kernel will be performed.
If I use not mkl kernel for quantized_conv2d() then rounding errors correspond both for fake quantization and for real quantization:
&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np

A = tf.random_normal([100, 30, 30, 1])
min_A = tf.reduce_min(A)
max_A = tf.reduce_max(A)

W = tf.constant(np.random.normal(size=[3, 3, 1, 4]).astype(np.float32))
min_W = tf.reduce_min(W)
max_W = tf.reduce_max(W)

fqA = tf.fake_quant_with_min_max_vars(A, min_A, max_A)
fqW = tf.fake_quant_with_min_max_vars(W, min_W, max_W)
fqAW = tf.nn.conv2d(fqA, fqW, [1, 1, 1, 1], 'SAME')

qA = tf.quantize(A, min_A, max_A, tf.quint8, mode='MIN_FIRST')
qW = tf.quantize(W, min_W, max_W, tf.quint8, mode='MIN_FIRST')
qAW = tf.nn.quantized_conv2d(qA[0], qW[0], qA[1], qA[2], qW[1], qW[2], [1, 1, 1, 1], 'SAME')
qAW = tf.dequantize(*qAW, mode='MIN_FIRST')
result = tf.reduce_mean(tf.abs(fqAW - qAW))

print('Result :', tf.Session().run(result))
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;Result : 2.017457e-07
&lt;/denchmark-code&gt;

But if I use mkl kernel for quantized_conv2d() than for all possible quantization modes combinations rounding errors aren't corresponding:
&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np

A = tf.random_normal([100, 30, 30, 1])
min_A = tf.reduce_min(A)
max_A = tf.reduce_max(A)

W = tf.constant(np.random.normal(size=[3, 3, 1, 4]).astype(np.float32))
min_W = tf.reduce_min(W)
max_W = tf.reduce_max(W)

fqA = tf.fake_quant_with_min_max_vars(A, min_A, max_A)
fqW = tf.fake_quant_with_min_max_vars(W, min_W, max_W)
fqAW = tf.nn.conv2d(fqA, fqW, [1, 1, 1, 1], 'SAME')

modes = ['MIN_COMBINED', 'MIN_FIRST', 'SCALED']
modes_variants = [[i, j, k] for i in modes for j in modes for k in modes]
for variant in modes_variants:
    qA = tf.quantize(A, min_A, max_A, tf.quint8, mode=variant[0])
    qW = tf.quantize(W, min_W, max_W, tf.qint8, mode=variant[1])
    qAW = tf.nn.quantized_conv2d(qA[0], qW[0], qA[1], qA[2], qW[1], qW[2], [1, 1, 1, 1], 'SAME')
    qAW = tf.dequantize(*qAW, mode=variant[2])
    result = tf.reduce_mean(tf.abs(fqAW - qAW))

    print('{:&lt;50} - {:.4}'.format(str(variant), tf.Session().run(result)))
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;['MIN_COMBINED', 'MIN_COMBINED', 'MIN_COMBINED']   - 6.242
['MIN_COMBINED', 'MIN_COMBINED', 'MIN_FIRST']      - 6.926
['MIN_COMBINED', 'MIN_COMBINED', 'SCALED']         - 6.526
['MIN_COMBINED', 'MIN_FIRST', 'MIN_COMBINED']      - 6.111
['MIN_COMBINED', 'MIN_FIRST', 'MIN_FIRST']         - 6.051
['MIN_COMBINED', 'MIN_FIRST', 'SCALED']            - 6.575
['MIN_COMBINED', 'SCALED', 'MIN_COMBINED']         - 5.819
['MIN_COMBINED', 'SCALED', 'MIN_FIRST']            - 5.704
['MIN_COMBINED', 'SCALED', 'SCALED']               - 7.089
['MIN_FIRST', 'MIN_COMBINED', 'MIN_COMBINED']      - 6.403
['MIN_FIRST', 'MIN_COMBINED', 'MIN_FIRST']         - 6.296
['MIN_FIRST', 'MIN_COMBINED', 'SCALED']            - 5.376
['MIN_FIRST', 'MIN_FIRST', 'MIN_COMBINED']         - 5.7
['MIN_FIRST', 'MIN_FIRST', 'MIN_FIRST']            - 6.233
['MIN_FIRST', 'MIN_FIRST', 'SCALED']               - 6.769
['MIN_FIRST', 'SCALED', 'MIN_COMBINED']            - 6.188
['MIN_FIRST', 'SCALED', 'MIN_FIRST']               - 6.119
['MIN_FIRST', 'SCALED', 'SCALED']                  - 5.869
['SCALED', 'MIN_COMBINED', 'MIN_COMBINED']         - 1.38
['SCALED', 'MIN_COMBINED', 'MIN_FIRST']            - 1.395
['SCALED', 'MIN_COMBINED', 'SCALED']               - 1.383
['SCALED', 'MIN_FIRST', 'MIN_COMBINED']            - 1.389
['SCALED', 'MIN_FIRST', 'MIN_FIRST']               - 1.395
['SCALED', 'MIN_FIRST', 'SCALED']                  - 1.405
['SCALED', 'SCALED', 'MIN_COMBINED']               - 1.352
['SCALED', 'SCALED', 'MIN_FIRST']                  - 1.355
['SCALED', 'SCALED', 'SCALED']                     - 1.357
&lt;/denchmark-code&gt;

Other info / logs
Tensorflow is build by command
&lt;denchmark-code&gt;bazel build --config=mkl --config=opt --copt=-DINTEL_MKL_QUANTIZED -k //tensorflow/tools/pip_package:build_pip_package
&lt;/denchmark-code&gt;

StackOverflow link:
&lt;denchmark-code&gt;https://stackoverflow.com/questions/54518253/rounding-error-of-fake-quantization-not-corresponds-to-rounding-error-of-real-qu
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Vooblin' date='2019-02-07T00:14:46Z'>
		&lt;denchmark-link:https://github.com/Vooblin&gt;@Vooblin&lt;/denchmark-link&gt;
: MKL quantization related works only support  mode which means  maps to . Current mkl version of , only supports  input and  filter. So you need to make sure that original (fp32) input to the convolution is indeed non-negative, which is a typical scenario in popular CNNs, due to . As a practical workflow, while converting a  to a , we keep a  node in  if its input contains negative values.
When you perform quantization with , by the design of TensorFlow it clips all negative values to zero. So you are doing a  operation internally. A Non-MKL (Eigen) build for the code below has the following result.
&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np

A = tf.random_normal([100, 30, 30, 1])
min_A = tf.reduce_min(A)
max_A = tf.reduce_max(A)

W = tf.constant(np.random.normal(size=[3, 3, 1, 4]).astype(np.float32))
min_W = tf.reduce_min(W)
max_W = tf.reduce_max(W)

fqA = tf.fake_quant_with_min_max_vars(A, min_A, max_A)
fqW = tf.fake_quant_with_min_max_vars(W, min_W, max_W)
fqAW = tf.nn.conv2d(fqA, fqW, [1, 1, 1, 1], 'SAME')

qA = tf.quantize(A, min_A, max_A, tf.quint8, mode='SCALED')
qW = tf.quantize(W, min_W, max_W, tf.quint8, mode='MIN_FIRST')
qAW = tf.nn.quantized_conv2d(qA[0], qW[0], qA[1], qA[2], qW[1], qW[2], [1, 1, 1, 1], 'SAME')
qAW = tf.dequantize(*qAW, mode='MIN_FIRST')
result = tf.reduce_mean(tf.abs(fqAW - qAW))

print('Result :', tf.Session().run(result))
&lt;/denchmark-code&gt;

('Result :', 1.6090124)
A slight modification gives the following result
&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np

A = tf.nn.relu(tf.random_normal([100, 30, 30, 1]))
min_A = tf.reduce_min(A)
max_A = tf.reduce_max(A)

W = tf.constant(np.random.normal(size=[3, 3, 1, 4]).astype(np.float32))
min_W = tf.reduce_min(W)
max_W = tf.reduce_max(W)

fqA = tf.fake_quant_with_min_max_vars(A, min_A, max_A)
fqW = tf.fake_quant_with_min_max_vars(W, min_W, max_W)
fqAW = tf.nn.conv2d(fqA, fqW, [1, 1, 1, 1], 'SAME')

qA = tf.quantize(A, min_A, max_A, tf.quint8, mode='SCALED')
qW = tf.quantize(W, min_W, max_W, tf.quint8, mode='MIN_FIRST')
qAW = tf.nn.quantized_conv2d(qA[0], qW[0], qA[1], qA[2], qW[1], qW[2], [1, 1, 1, 1], 'SAME')
qAW = tf.dequantize(*qAW, mode='MIN_FIRST')
result = tf.reduce_mean(tf.abs(fqAW - qAW))

print('Result :', tf.Session().run(result))
&lt;/denchmark-code&gt;

('Result :', 1.6503674e-07)
Now the above code with MKL build gives the following result
&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np

A = tf.nn.relu(tf.random_normal([100, 30, 30, 1]))
min_A = tf.reduce_min(A)
max_A = tf.reduce_max(A)

W = tf.constant(np.random.normal(size=[3, 3, 1, 4]).astype(np.float32))
min_W = tf.reduce_min(W)
max_W = tf.reduce_max(W)

fqA = tf.fake_quant_with_min_max_vars(A, min_A, max_A)
fqW = tf.fake_quant_with_min_max_vars(W, min_W, max_W)
fqAW = tf.nn.conv2d(fqA, fqW, [1, 1, 1, 1], 'SAME')

qA = tf.quantize(A, min_A, max_A, tf.quint8, mode='SCALED')
qW = tf.quantize(W, min_W, max_W, tf.qint8, mode='SCALED')
qAW = tf.nn.quantized_conv2d(qA[0], qW[0], qA[1], qA[2], qW[1], qW[2], [1, 1, 1, 1], 'SAME')
qAW = tf.dequantize(*qAW, mode='MIN_FIRST')
result = tf.reduce_mean(tf.abs(fqAW - qAW))

print('Result :', tf.Session().run(result))
&lt;/denchmark-code&gt;

('Result :', 0.009543991)
The reason for slightly reduced precision with MKL is due to SCALED mode implementation of quantized_conv2d where you will have less resolution if min-max are not symmetric.
		</comment>
		<comment id='2' author='Vooblin' date='2019-02-08T09:21:31Z'>
		&lt;denchmark-link:https://github.com/mdfaijul&gt;@mdfaijul&lt;/denchmark-link&gt;
 Thanks for you response!
		</comment>
	</comments>
</bug>