<bug id='41959' author='PuchatekwSzortach' open_date='2020-08-01T12:58:07Z' closed_time='2020-08-01T13:13:52Z'>
	<summary>tf.math.l2_normalize - gradients seem to be wrong</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX 10.16.16, Linux Ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): docker container, versions tensorflow/tensorflow:2.2.0-gpu and tensorflow/tensorflow:2.3.0
TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0 and v2.3.0-rc2-23-gb36436b087 2.3.0
Python version: 3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 10.0.130 on Linux, no CUDA on OSX
GPU model and memory: GTX 1080 Ti 12GB on Linux, no GPU on OSX

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with:

TF 1.0: python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
TF 2.0: python -c "import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"

Describe the current behavior
Gradients of tf.math.l2_normalize seem to be wrong. I confirmed both analytically and with numerical computations that values computed by tensorflow are off. There is always a chance my math is wrong, but I'm quite confident in it + numerical computations indicate same results as my workings.
Here's a unit test that compares gradients computed by tensorflow to expected values.
Since they differ, test fails:
def test_l2_gradients_with_tensorflow():

    x = tf.constant([3, 4], tf.float32)

    with tf.GradientTape() as gradient_tape:

        gradient_tape.watch(x)
        y = tf.math.l2_normalize(x)

    expected_y = np.array([0.6, 0.8])

    assert np.all(np.isclose(expected_y, y))

    gradients = gradient_tape.gradient(y, x)

    expected_gradients = np.array([0.128, 0.072])

    # This line fails, tensorflow computes gradients to be [0.032 -0.024]
    assert np.all(np.isclose(expected_gradients, gradients))
Describe the expected behavior
Below are my workings for what the correct dy/dx1 derivative should be for values in unit test above. dy/dx2 follows the same logic.
&lt;denchmark-link:https://user-images.githubusercontent.com/13689310/89102189-b0a83d80-d441-11ea-8b52-0e9a05e7d2e3.jpg&gt;&lt;/denchmark-link&gt;

I also double checked with numerical computations in numpy, and they agree with my numbers:
def test_l2_gradients_with_numpy():

    # Specify x1 only
    x1 = np.arange(2.998, 3.003, 0.001)

    # index for x1 = 3
    test_index = 2

    # x2 is set to constant 4
    y = x1 / np.sqrt(x1*x1 + 16)
    expected_y = 0.6

    assert np.isclose(expected_y, y[test_index])

    gradients = np.gradient(y, x1)
    expected_gradient = 0.128

    assert np.isclose(expected_gradient, gradients[test_index])
Standalone code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
Please refer to unit tests above
Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='PuchatekwSzortach' date='2020-08-01T13:13:52Z'>
		And 10 minutes after posting the issue I understood I was wrong.
What I computed was d(x1') / dx1, what I needed to compute was d(x1')/d(x1) + d(x2')/dx1 + dx1 where x1' and x2' are normalized values of x1 and x2, since x1 contributes to both of them. I forgot about this and only computed contributions from x1'. Adding d(x2')/dx1 + dx1 yielded same results as computed by tensorflow.
My apologies for the mistake.
		</comment>
		<comment id='2' author='PuchatekwSzortach' date='2020-08-01T13:13:54Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41959&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41959&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>