<bug id='40648' author='meet-minimalist' open_date='2020-06-21T14:43:30Z' closed_time='2020-09-26T18:49:15Z'>
	<summary>Inconsistant tf.name_scope behaviour with Custom model in TF2.X.</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab Environment
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): Binary (Colab Pre-Installed)
TensorFlow version (use command below): 2.3.0-nightly
Python version: 3.6.9
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A (Colab CPU Environment used)
GPU model and memory: N/A

Describe the current behavior
I am trying to use tf.name_scope inside a custom model which is inherited from tf.keras.Model and it has custom layers which are inherited from tf.keras.layers.Layer.
Describe the expected behavior
I expect the scope_name that is added with the help of tf.name_scope as a prefix to the weights name. E.g. weight "conv1/conv2d_35/kernel:0" should be named as "Conv_Block/conv1/conv2d_35/kernel:0" when used with tf.name_scope("Conv_Block).
Standalone code to reproduce the issue
&lt;denchmark-code&gt;class Config:
    weight_decay = 0.0005
    weight_init_seed = 42
    bnorm_momentum = 0.9

config = Config()

class ConvLayer(tf.keras.layers.Layer):
    def __init__(self, name, filters, k_size=(3, 3), strides=1, padding='same', use_bnorm=True):
        super(ConvLayer, self).__init__(name=name, trainable=True, dtype=tf.float32)        

        self.use_bnorm = use_bnorm

        self.weight_decay = config.weight_decay
        self.k_reg = tf.keras.regularizers.l2(self.weight_decay)
        self.k_init = tf.keras.initializers.GlorotNormal(seed=config.weight_init_seed)
        self.b_init = tf.zeros_initializer()

        self.conv = tf.keras.layers.Conv2D(filters=filters, kernel_size=k_size, strides=strides, padding=padding, activation=None, use_bias=not self.use_bnorm, kernel_initializer=self.k_init, bias_initializer=self.b_init, kernel_regularizer=self.k_reg)
        if self.use_bnorm:
            self.bn = tf.keras.layers.BatchNormalization(momentum=config.bnorm_momentum)
        self.mx_pool = tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=2, padding=padding)

        self.act = tf.nn.relu

    def call(self, x, training=False):
        if self.use_bnorm:
            return self.act(self.mx_pool(self.bn(self.conv(x), training=training)))
        else:
            return self.act(self.mx_pool(self.conv(x)))


class DenseLayer(tf.keras.layers.Layer):
    def __init__(self, name, units, use_bnorm=True, use_act=True):
        super(DenseLayer, self).__init__(name=name)
        self.use_bnorm = use_bnorm
        self.use_act = use_act
        
        self.weight_decay = config.weight_decay
        self.k_reg = tf.keras.regularizers.l2(self.weight_decay)
        self.k_init = tf.keras.initializers.GlorotNormal(seed=config.weight_init_seed)
        self.b_init = tf.zeros_initializer()

        self.fc = tf.keras.layers.Dense(units=units, activation=None, use_bias=not self.use_bnorm, kernel_initializer=self.k_init, bias_initializer=self.b_init, kernel_regularizer=self.k_reg)
        if self.use_bnorm:
            self.bn = tf.keras.layers.BatchNormalization(momentum=config.bnorm_momentum)
        
        if self.use_act:
            self.act = tf.nn.relu

    def call(self, x, training=False):
        if self.use_bnorm:
            op = self.bn(self.fc(x), training=training)
        else:
            op = self.fc(x)

        if self.use_act:
            return self.act(op)
        else:
            return op


class ClassifierModel(tf.keras.Model):
    def __init__(self, num_classes=2):
        super(ClassifierModel, self).__init__(name='ClassifierModel')
        self.num_classes = num_classes
        
        # input will be of shape : [None, 128, 128, 3]

        with tf.name_scope("Conv_Block"):
            self.conv1 = ConvLayer('conv1', 32, (3, 3), 1, 'same', True)
            # [64 x 64 x 32]
            
            self.conv2 = ConvLayer('conv2', 48, (3, 3), 1, 'same', True)
            # [32 x 32 x 48]
            
            self.conv3 = ConvLayer('conv3', 72, (3, 3), 1, 'same', True)
            # [16 x 16 x 72]

            self.conv4 = ConvLayer('conv4', 96, (3, 3), 1, 'same', True)
            # [8 x 8 x 96]

            self.conv5 = ConvLayer('conv5', 128, (3, 3), 1, 'same', True)
            # [4 x 4 x 128]

            self.gap = tf.keras.layers.GlobalAveragePooling2D()
            self.mx_pool = tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=2, padding='same')

        with tf.name_scope("Dense_Block"):
            self.fc1 = DenseLayer('fc1', 64, True)
            # [64]

            self.fc2 = DenseLayer('fc2', 2, False, False)
            # [2]
    
    def call(self, x, training=False):
        # x         : [B, 128, 128, 3]
        # training  : True / False - used for batch norm.
        
        layer_1 = self.conv1(x, training=training)
        layer_1 = self.mx_pool(layer_1)

        layer_2 = self.conv2(layer_1, training=training)
        layer_2 = self.mx_pool(layer_2)

        layer_3 = self.conv3(layer_2, training=training)
        layer_3 = self.mx_pool(layer_3)

        layer_4 = self.conv4(layer_3, training=training)
        layer_4 = self.mx_pool(layer_4)

        layer_5 = self.conv5(layer_4, training=training)
        layer_5 = self.mx_pool(layer_5)

        gap = self.gap(layer_5)

        fc1 = self.fc1(gap, training=training)

        logits = self.fc2(fc1, training=None)
        outputs = tf.nn.softmax(logits)

        return logits, outputs


clf = ClassifierModel()
clf.build(input_shape=tf.TensorShape([None, 128, 128, 3]))

for v in clf.trainable_variables:
    print(v.name, "\t\t", v.shape)
&lt;/denchmark-code&gt;

Generated output is as below:
&lt;denchmark-code&gt;conv1/conv2d_35/kernel:0 		 (3, 3, 3, 32)
conv1/batch_normalization_41/gamma:0 		 (32,)
conv1/batch_normalization_41/beta:0 		 (32,)
conv2/conv2d_36/kernel:0 		 (3, 3, 32, 48)
conv2/batch_normalization_42/gamma:0 		 (48,)
conv2/batch_normalization_42/beta:0 		 (48,)
conv3/conv2d_37/kernel:0 		 (3, 3, 48, 72)
conv3/batch_normalization_43/gamma:0 		 (72,)
conv3/batch_normalization_43/beta:0 		 (72,)
conv4/conv2d_38/kernel:0 		 (3, 3, 72, 96)
conv4/batch_normalization_44/gamma:0 		 (96,)
conv4/batch_normalization_44/beta:0 		 (96,)
conv5/conv2d_39/kernel:0 		 (3, 3, 96, 128)
conv5/batch_normalization_45/gamma:0 		 (128,)
conv5/batch_normalization_45/beta:0 		 (128,)
fc1/dense_12/kernel:0 		 (128, 64)
fc1/batch_normalization_46/gamma:0 		 (64,)
fc1/batch_normalization_46/beta:0 		 (64,)
fc2/dense_13/kernel:0 		 (64, 2)
fc2/dense_13/bias:0 		 (2,)
&lt;/denchmark-code&gt;

Output should be as below:
&lt;denchmark-code&gt;Conv_Block/conv1/conv2d_35/kernel:0 		 (3, 3, 3, 32)
Conv_Block/conv1/batch_normalization_41/gamma:0 		 (32,)
Conv_Block/conv1/batch_normalization_41/beta:0 		 (32,)
Conv_Block/conv2/conv2d_36/kernel:0 		 (3, 3, 32, 48)
Conv_Block/conv2/batch_normalization_42/gamma:0 		 (48,)
Conv_Block/conv2/batch_normalization_42/beta:0 		 (48,)
Conv_Block/conv3/conv2d_37/kernel:0 		 (3, 3, 48, 72)
Conv_Block/conv3/batch_normalization_43/gamma:0 		 (72,)
Conv_Block/conv3/batch_normalization_43/beta:0 		 (72,)
Conv_Block/conv4/conv2d_38/kernel:0 		 (3, 3, 72, 96)
Conv_Block/conv4/batch_normalization_44/gamma:0 		 (96,)
Conv_Block/conv4/batch_normalization_44/beta:0 		 (96,)
Conv_Block/conv5/conv2d_39/kernel:0 		 (3, 3, 96, 128)
Conv_Block/conv5/batch_normalization_45/gamma:0 		 (128,)
Conv_Block/conv5/batch_normalization_45/beta:0 		 (128,)
Dense_Block/fc1/dense_12/kernel:0 		 (128, 64)
Dense_Block/fc1/batch_normalization_46/gamma:0 		 (64,)
Dense_Block/fc1/batch_normalization_46/beta:0 		 (64,)
Dense_Block/fc2/dense_13/kernel:0 		 (64, 2)
Dense_Block/fc2/dense_13/bias:0 		 (2,)
&lt;/denchmark-code&gt;

 &lt;denchmark-link:https://colab.research.google.com/drive/1MxMsIYVy7vGiY6jctw5OVkHUPGabHu9Z?usp=sharing&gt;https://colab.research.google.com/drive/1MxMsIYVy7vGiY6jctw5OVkHUPGabHu9Z?usp=sharing&lt;/denchmark-link&gt;

Other info / logs
The same issue is reproduced in my local machine where Python 3.6.10 is installed with TensorFlow 2.1.0.
	</description>
	<comments>
		<comment id='1' author='meet-minimalist' date='2020-06-21T14:49:36Z'>
		Same issue
		</comment>
		<comment id='2' author='meet-minimalist' date='2020-06-22T07:02:20Z'>
		I am able to replicate this issue, please find the &lt;denchmark-link:https://colab.research.google.com/gist/Saduf2019/367b460e6f592bd3a677cb312ca6d0cf/untitled233.ipynb&gt;gist here&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='meet-minimalist' date='2020-06-22T23:54:10Z'>
		This is intended behavior. Please refer &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/28558&gt;#28558&lt;/denchmark-link&gt;
 to know more.
You may &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/28558#issuecomment-503598352&gt;explicitly name layers&lt;/denchmark-link&gt;
 by passing name arg.
 tf.keras.layers.Dense(name='your own layer name')
		</comment>
		<comment id='4' author='meet-minimalist' date='2020-06-23T14:32:47Z'>
		
This is intended behavior. Please refer #28558 to know more.
You may explicitly name layers by passing name arg.
 tf.keras.layers.Dense(name='your own layer name')

Thanks for the reply. Actually I am looking for grouping similar layers in model definition. Like in ResNet we can group a single residual block and name it 'ResBlock_1', 'ResBlock_2' and so on. It would be very easy to use name_scope and prefix all the layers inside it with a single name rather than passing that name into all the sub-layers and sub-layers' of those sub-layers' to change their name.
		</comment>
		<comment id='5' author='meet-minimalist' date='2020-09-26T18:49:15Z'>
		This question is better asked on &lt;denchmark-link:http://stackoverflow.com/questions/tagged/tensorflow&gt;StackOverflow&lt;/denchmark-link&gt;
 since it is not a bug or feature request. There is also a larger community that reads questions there.
Thanks!
		</comment>
		<comment id='6' author='meet-minimalist' date='2020-09-26T18:49:17Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40648&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40648&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='meet-minimalist' date='2020-09-27T16:06:23Z'>
		
This question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there.
Thanks!

But it is not as per the expected behavior. I am not asking for possible work around. I am expecting tf.name_scope to add a prefix for all the children variables' name. That's what it was doing in TF1.X. How come this behavior is changed?
		</comment>
	</comments>
</bug>