<bug id='32819' author='IvanUkhov' open_date='2019-09-25T18:38:47Z' closed_time='2019-10-07T18:23:03Z'>
	<summary>Shared and mutated state between training and validation callbacks</summary>
	<description>
System information

Have I written custom code: Yes
OS Platform and Distribution: Datalab
Mobile device: NA
TensorFlow installed from: Binary
TensorFlow version: 1.15.0rc1
Python version: 3.5.6
Bazel version: NA
GCC/Compiler version: NA
CUDA/cuDNN version: NA
GPU model and memory: NA

Describe the current behavior
When invoking keras.Model.fit with two data.Datasets, one is for training, and one is validation, so that the training one is infinite with epochs and steps_per_epoch provided to fit, and the validation one is finite without any additional parameters to fit, the progress bar shows incorrect number of steps after the first validation. In particular, the number of steps per epoch gets set to the number of steps in the validation set.
Describe the expected behavior
The prescribed steps_per_epoch remains the same in the progress bar for all epochs.
Code to reproduce the issue
See below.
Other info / logs
I believe the problem is due to this modification when the input is exhausted:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/python/keras/engine/training_v2.py#L136&gt;https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/python/keras/engine/training_v2.py#L136&lt;/denchmark-link&gt;

The number of steps gets overwritten. However, self.params is shared across both the training and validation callbacks, which essentially misleads the progress bar for training. The state share is potentially due to the following line:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/python/keras/engine/training_v2.py#L347&gt;https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/python/keras/engine/training_v2.py#L347&lt;/denchmark-link&gt;

In other words, validation_callbacks is based on training_callbacks. configure_callbacks should probably take callbacks instead.
	</description>
	<comments>
		<comment id='1' author='IvanUkhov' date='2019-09-26T05:39:55Z'>
		&lt;denchmark-link:https://github.com/IvanUkhov&gt;@IvanUkhov&lt;/denchmark-link&gt;

In order to expedite the trouble-shooting process, please provide a minimal standalone code to reproduce the issue reported here. Thanks!
		</comment>
		<comment id='2' author='IvanUkhov' date='2019-09-26T06:43:24Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
, it is good that you asked for an example. It turned out it was only about those datasets whose size could not be known in advance. I had to use a generator to reproduce.
import tensorflow as tf

tf.enable_eager_execution()

def generator():
    for _ in range(100):
        yield [1, 1], 1

training = tf.data.Dataset \
    .from_generator(
        generator=generator,
        output_types=(tf.float64, tf.float64),
        output_shapes=(tf.TensorShape([2]), tf.TensorShape([])),
    ) \
    .batch(2) \
    .repeat()

validation = tf.data.Dataset \
    .from_generator(
        generator=generator,
        output_types=(tf.float64, tf.float64),
        output_shapes=(tf.TensorShape([2]), tf.TensorShape([])),
    ) \
    .batch(2)

model = tf.keras.Sequential([
    tf.keras.layers.InputLayer(2),
    tf.keras.layers.Dense(1),
])
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(
    x=training,
    validation_data=validation,
    epochs=10,
    steps_per_epoch=20,
)
&lt;denchmark-code&gt;Train for 20 steps
Epoch 1/10
20/20 [==============================] - 0s 24ms/step - loss: 1.1096 - val_loss: 1.0443
Epoch 2/10
20/50 [===========&gt;..................] - ETA: 0s - loss: 0.9881 - val_loss: 0.0000e+00Epoch 3/10
20/50 [===========&gt;..................] - ETA: 0s - loss: 0.8761 - val_loss: 0.0000e+00Epoch 4/10
20/50 [===========&gt;..................] - ETA: 0s - loss: 0.7738 - val_loss: 0.0000e+00Epoch 5/10
20/50 [===========&gt;..................] - ETA: 0s - loss: 0.6808 - val_loss: 0.0000e+00Epoch 6/10
20/50 [===========&gt;..................] - ETA: 0s - loss: 0.5967 - val_loss: 0.0000e+00Epoch 7/10
20/50 [===========&gt;..................] - ETA: 0s - loss: 0.5209 - val_loss: 0.0000e+00Epoch 8/10
20/50 [===========&gt;..................] - ETA: 0s - loss: 0.4528 - val_loss: 0.0000e+00Epoch 9/10
20/50 [===========&gt;..................] - ETA: 0s - loss: 0.3920 - val_loss: 0.0000e+00Epoch 10/10
20/50 [===========&gt;..................] - ETA: 0s - loss: 0.3378 - val_loss: 0.0000e+00
&lt;tensorflow.python.keras.callbacks.History at 0x7f01119bdb38&gt;
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='IvanUkhov' date='2019-09-27T10:13:23Z'>
		I have tried on colab with TF version 1.15.0-rc0,1.15.0-rc1 and was able to reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/608e6fcff91f3b80d8025f7f9c4876f6/untitled231.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='4' author='IvanUkhov' date='2019-09-27T10:37:20Z'>
		I have an idea of how it could be fixed, and I will try to explore it in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/32847&gt;#32847&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='5' author='IvanUkhov' date='2019-10-07T18:23:04Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32819&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32819&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>