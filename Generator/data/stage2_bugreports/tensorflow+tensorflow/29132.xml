<bug id='29132' author='srihari-humbarwadi' open_date='2019-05-29T15:13:50Z' closed_time='2019-09-09T21:48:44Z'>
	<summary>Tensorflow2.0 keras subclassed model, problems with getting correct output shape, shows 'multiple'</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): TensorFlow 2.0.0-alpha0
Python version: 3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 10.0, 7.5
GPU model and memory: 1080ti 11gb

Describe the current behavior
model.summary() prints 'multiple' as layer output shapes
Describe the expected behavior
Code to reproduce the issue
&lt;denchmark-code&gt;import tensorflow as tf

print('TensorFlow', tf.__version__)

class ResidualBlock(tf.keras.Model):
    def __init__(self, block_type=None, n_filters=None):
        super(ResidualBlock, self).__init__()
        self.n_filters = n_filters
        if block_type == 'identity':
            self.strides = 1
        elif block_type == 'conv':
            self.strides = 2
            self.conv_shorcut = tf.keras.layers.Conv2D(filters=self.n_filters, 
                               kernel_size=1, 
                               padding='same',
                               strides=self.strides,
                               kernel_initializer='he_normal')
            self.bn_shortcut = tf.keras.layers.BatchNormalization(momentum=0.9)
            
        self.conv_1 = tf.keras.layers.Conv2D(filters=self.n_filters, 
                               kernel_size=3, 
                               padding='same',
                               strides=self.strides,
                               kernel_initializer='he_normal')
        self.bn_1 = tf.keras.layers.BatchNormalization(momentum=0.9)
        self.relu_1 = tf.keras.layers.ReLU()
        
        self.conv_2 = tf.keras.layers.Conv2D(filters=self.n_filters, 
                               kernel_size=3, 
                               padding='same', 
                               kernel_initializer='he_normal')
        self.bn_2 = tf.keras.layers.BatchNormalization(momentum=0.9)
        self.relu_2 = tf.keras.layers.ReLU()
        
    def call(self, x, training=False):
        shortcut = x
        if self.strides == 2:
            shortcut = self.conv_shorcut(x)
            shortcut = self.bn_shortcut(shortcut)
        y = self.conv_1(x)
        y = self.bn_1(y)
        y = self.relu_1(y)
        y = self.conv_2(y)
        y = self.bn_2(y)
        y = tf.add(shortcut, y)
        y = self.relu_2(y)
        return y
    
class ResNet34(tf.keras.Model):
    def __init__(self, include_top=True, n_classes=1000):
        super(ResNet34, self).__init__()
        
        self.n_classes = n_classes
        self.include_top = include_top
        self.conv_1 = tf.keras.layers.Conv2D(filters=64, 
                                               kernel_size=7, 
                                               padding='same', 
                                               strides=2, 
                                               kernel_initializer='he_normal')
        self.bn_1 = tf.keras.layers.BatchNormalization(momentum=0.9)
        self.relu_1 = tf.keras.layers.ReLU()
        self.maxpool = tf.keras.layers.MaxPool2D(3, 2, padding='same')
        self.residual_blocks = tf.keras.Sequential()
        for n_filters, reps, downscale in zip([64, 128, 256, 512], 
                                              [3, 4, 6, 3], 
                                              [False, True, True, True]):
            for i in range(reps):
                if i == 0 and downscale:
                    self.residual_blocks.add(ResidualBlock(block_type='conv', 
                                                              n_filters=n_filters))
                else:
                    self.residual_blocks.add(ResidualBlock(block_type='identity', 
                                                              n_filters=n_filters))
        self.GAP = tf.keras.layers.GlobalAveragePooling2D()
        self.fc = tf.keras.layers.Dense(units=self.n_classes)
        
    def call(self, x, training=False):
        y = self.conv_1(x)
        y = self.bn_1(y)
        y = self.relu_1(y)
        y = self.maxpool(y)
        y = self.residual_blocks(y)
        if self.include_top:
            y = self.GAP(y)
            y = self.fc(y)
        return y
    
model = ResNet34()
model.build((1, 224, 224, 3))
model.summary()
&lt;/denchmark-code&gt;

output log
&lt;denchmark-code&gt;
TensorFlow 2.0.0-alpha0
Model: "res_net34"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              multiple                  9472      
_________________________________________________________________
batch_normalization_v2 (Batc multiple                  256       
_________________________________________________________________
re_lu (ReLU)                 multiple                  0         
_________________________________________________________________
max_pooling2d (MaxPooling2D) multiple                  0         
_________________________________________________________________
sequential (Sequential)      multiple                  21300480  
_________________________________________________________________
global_average_pooling2d (Gl multiple                  0         
_________________________________________________________________
dense (Dense)                multiple                  513000    
=================================================================
Total params: 21,823,208
Trainable params: 21,806,184
Non-trainable params: 17,024
_________________________________________________________________
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='srihari-humbarwadi' date='2019-05-29T15:16:43Z'>
		&lt;denchmark-link:https://github.com/achandraa&gt;@achandraa&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='srihari-humbarwadi' date='2019-05-30T06:57:48Z'>
		&lt;denchmark-link:https://github.com/srihari-humbarwadi&gt;@srihari-humbarwadi&lt;/denchmark-link&gt;
 Able to reproduce the issue in TF 2.0, TF 1.13.1 , tf nightly (GPU versions) gist is &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/237bfe24fb3a21aca23d2949ea063330/keras_tf29132.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='3' author='srihari-humbarwadi' date='2019-05-30T17:51:51Z'>
		The difficulty here is that with subclassed models, summary() does not have enough information to know what the output shape of the model is. The list of layers is known, and hence printed, but how those layers are used is not known. We could print something other than "multiple" in that case, but it's not clear what would be more informative. What do you think?
		</comment>
		<comment id='4' author='srihari-humbarwadi' date='2019-05-30T18:07:06Z'>
		
Is there some lower level API to set output shapes, which could perhaps be called in build method?
I tried using a subclassed model within another  functional model, and i had no problem in getting the output shapes in the summary. This made me think and come up with the earlier query

		</comment>
		<comment id='5' author='srihari-humbarwadi' date='2019-06-22T16:24:01Z'>
		Example:
import tensorflow as tf
import numpy as np

class MyModel(tf.keras.Model):
    def __init__(self):
        super(MyModel, self).__init__()
        self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
        self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)
    
    def call(self, inputs):
        x = self.dense1(inputs)
        return self.dense2(x)
    
    def build_graph(self, input_shape): 
        input_shape_nobatch = input_shape[1:]
        self.build(input_shape)
        inputs = tf.keras.Input(shape=input_shape_nobatch)
        
        if not hasattr(self, 'call'):
            raise AttributeError("User should define 'call' method in sub-class model!")
        
        _ = self.call(inputs)
        

model = MyModel()
model.build_graph((32,10,))

model.summary()

input = np.random.random((32,10,))
input = input.astype(np.float32)
output = model.call(input)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    result = output.eval(session=sess)

print('Model output shape:')
print(result.shape)
Output:
&lt;denchmark-code&gt;WARNING:tensorflow:From C:\Users\Jairo\Anaconda3\envs\py35\lib\site-packages\ten
sorflow\python\ops\resource_variable_ops.py:435: colocate_with (from tensorflow.
python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
dense (Dense)                (None, 4)                 44
_________________________________________________________________
dense_1 (Dense)              (None, 5)                 25
=================================================================
Total params: 69
Trainable params: 69
Non-trainable params: 0
_________________________________________________________________
Model output shape:
(32, 5)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='srihari-humbarwadi' date='2019-09-09T21:48:44Z'>
		I am closing this issue based on &lt;denchmark-link:https://github.com/karmel&gt;@karmel&lt;/denchmark-link&gt;
 explanation on why it is difficult to track shape in subclassed models. You could also use workaround mentioned by &lt;denchmark-link:https://github.com/Ja1r0&gt;@Ja1r0&lt;/denchmark-link&gt;
 . Thanks!
Closing this out since I understand it to be resolved, but please let me know if I'm mistaken.
		</comment>
		<comment id='7' author='srihari-humbarwadi' date='2019-09-09T21:48:45Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=29132&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=29132&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='srihari-humbarwadi' date='2019-12-27T08:00:01Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/Ja1r0&gt;@Ja1r0&lt;/denchmark-link&gt;
 's solution works, and I also adopt it. But if decorating with @tf.function, running build_model(...) will raise error. So it seems that one may firstly does not decorate it, get those definite shapes in the NN. Once the architecture is fixed, one may decorate it in training and testing etc. Is that an acceptable work-around?
		</comment>
	</comments>
</bug>