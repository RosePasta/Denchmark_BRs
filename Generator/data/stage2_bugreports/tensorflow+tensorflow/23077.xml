<bug id='23077' author='kami93' open_date='2018-10-18T15:42:23Z' closed_time='2019-02-18T18:47:23Z'>
	<summary>Slow training speed and incompatible ops issue in tf.keras.utils.multi_gpu_model</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes. See https://github.com/kami93/PredRNN
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.1
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A
TensorFlow installed from (source or binary):Source
TensorFlow version (use command below):v1.11.0
Python version: Anaconda Python 3.6.6
Bazel version (if compiling from source):0.17.2
GCC/Compiler version (if compiling from source): GCC 7.3.0
CUDA/cuDNN version: CUDA 9.2, cuDNN 7.3
GPU model and memory: Gefroce 1080Ti * 4 (4-way GPU), 11,178 MiB each.


Hi. I have built a TensorFlow Keras Sequential model (See &lt;denchmark-link:https://github.com/kami93/PredRNN/blob/master/predRNN.py&gt;predRNN.py&lt;/denchmark-link&gt;
) which consists of several Keras layers including my custom layer (See &lt;denchmark-link:https://github.com/kami93/PredRNN/blob/master/keras_custom/layers/STLSTM.py&gt;keras_custom/layers/STLSTM.py&lt;/denchmark-link&gt;
).
The model by itself runs very well when tf.keras.utils.multi_gpu_model is not applied. The model can be created, compiled, and perform the training (by Model.fit method) without any warning or error.
However, if tf.keras.utils.multi_gpu_model is applied to replicate the model on several GPUs for multi gpu training, warnings like the followings are raised.
&lt;denchmark-code&gt;No node-device colocations were active during op 'replica_2/sequential/stlst_m2d/while/convolution_14' creation.
Device assignments active during op 'replica_2/sequential/stlst_m2d/while/convolution_14' creation:
  with tf.device(/gpu:2): &lt;/home/simon/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/utils/multi_gpu_utils.py:221&gt;
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;WARNING:tensorflow:Tried to colocate op 'training/Adam/gradients/replica_3/sequential/stlst_m2d/while/convolution_14_grad/ShapeN/Const' (defined at predRNN.py:142) having device '/device:CPU:1' with op 'replica_3/sequential/stlst_m2d/while/convolution_14' (defined at /home/simon/Desktop/git/PredRNN/keras_custom/layers/STLSTM.py:969) which had an incompatible device '/device:GPU:3'.
&lt;/denchmark-code&gt;

I think here "tf.keras.utils.multi_gpu_model" is not being able to properly assign the convolution ops to other devices which they are initially created for the "model_creation_device" (See &lt;denchmark-link:https://github.com/kami93/PredRNN/blob/master/predRNN.py#L97&gt;line 97 in predRNN.py&lt;/denchmark-link&gt;
).
Actually, the moodel behaves different with the different selection of "model_creation_device".
Case 1
If model_creation_device == '/cpu:0', the CPU usage is almost 100% in every core. For the GPU usages, gpu:0, gpu:1, gpu:2 and gpu:3 are all underutilized below 30%. The training is extremely slow compared to other cases. I think some convolution ops are performed by 'CPU:0' here.
&lt;denchmark-link:https://user-images.githubusercontent.com/20102/47162030-33740d00-d32e-11e8-8dc4-b5fd40be6c09.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/20102/47163769-a3d05d80-d331-11e8-8e39-d8fa27ec9542.png&gt;&lt;/denchmark-link&gt;

##################################################################################
Case 2
If model_creation_device == '/cpu:1', the CPU usage is normal below 30% in every core. However memory shortage warnings such as the followings are raised.
&lt;denchmark-code&gt;2018-10-18 23:14:56.079586: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.04GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-10-18 23:14:56.082410: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.06GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
&lt;/denchmark-code&gt;

For the GPUs usage, only the 'gpu:0' is highly utilized around 100% all the time. Others are mostly underutilized around 30%. However, the training speed is fastest amongst the cases.
&lt;denchmark-link:https://user-images.githubusercontent.com/20102/47166363-fc562980-d336-11e8-88d8-1c4b7708a135.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/20102/47166330-e7799600-d336-11e8-862d-de21e67fc211.png&gt;&lt;/denchmark-link&gt;

##################################################################################
Case 3
If model_creation_device == '/gpu:0', '/gpu:1', '/gpu:2', or '/gpu:3', the CPU usage is normal below 30% in every core. However, memory shortage warnings are raised.
The GPUs usage is somewhat strange. Only the "model_creation_device" is highly utilized around 100% all the time, and others are mostly underutilized around 30%.
&lt;denchmark-link:https://user-images.githubusercontent.com/20102/47165523-1989f880-d335-11e8-9728-424b649c967c.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/20102/47165150-64efd700-d334-11e8-83e1-abc2cf41b02f.png&gt;&lt;/denchmark-link&gt;

The training speed is way faster than model_creation_device == '/cpu:0' case. However, slightly slower than model_creation_device == '/cpu:1' case.
Describe the expected behavior
Multi GPU training of "PredRNN" keras model using "tf.keras.utils.multi_gpu_model" works without any warning related ops compatibility with devices. All CPU core's usage is below 30% and GPUs usage are around 100% equally for all GPUs while training.

Run predRNN.py in &lt;denchmark-link:https://github.com/kami93/PredRNN&gt;https://github.com/kami93/PredRNN&lt;/denchmark-link&gt;
.
Modify the "model_creation_device" to reproduce issues.

Model source code: &lt;denchmark-link:https://github.com/kami93/PredRNN&gt;https://github.com/kami93/PredRNN&lt;/denchmark-link&gt;

logs when model_creation_device = '/cpu:0'
&lt;denchmark-link:https://pastebin.com/esjt8ZLa&gt;https://pastebin.com/esjt8ZLa&lt;/denchmark-link&gt;

logs when model_creation_device = '/cpu:1'
&lt;denchmark-link:https://pastebin.com/yRfm9yW8&gt;https://pastebin.com/yRfm9yW8&lt;/denchmark-link&gt;

logs when model_creation_device = '/gpu:0'
&lt;denchmark-link:https://pastebin.com/CB6Wbzn7&gt;https://pastebin.com/CB6Wbzn7&lt;/denchmark-link&gt;

logs when model_creation_device = '/gpu:1'
&lt;denchmark-link:https://pastebin.com/c917nU53&gt;https://pastebin.com/c917nU53&lt;/denchmark-link&gt;

logs when model_creation_device = '/gpu:2'
&lt;denchmark-link:https://pastebin.com/rbMSPaWN&gt;https://pastebin.com/rbMSPaWN&lt;/denchmark-link&gt;

logs when model_creation_device = '/gpu:3'
&lt;denchmark-link:https://pastebin.com/j3xGDvrw&gt;https://pastebin.com/j3xGDvrw&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='kami93' date='2018-10-18T16:32:10Z'>
		I tried tf.contrib.distribute.MirroredStrategy too.
Firstly, the model raised the following ValueError with tf.contrib.distribute.MirroredStrategy being declared with the default arguments.
&lt;denchmark-code&gt;ValueError: Device /replica:0/task:0/device:CPU:0 not found in dict_keys(['/replica:0/task:0/device:GPU:0', '/replica:0/task:0/device:GPU:1', '/replica:0/task:0/device:GPU:2', '/replica:0/task:0/device:GPU:3']) (current device )
&lt;/denchmark-code&gt;

From &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/22550&gt;#22550&lt;/denchmark-link&gt;
, The ValueError was resolved by specifying device list.
&lt;denchmark-code&gt;tf.contrib.distribute.MirroredStrategy(['/device:CPU:0', '/device:CPU:1', '/device:GPU:0', '/device:GPU:1', '/device:GPU:2', '/device:GPU:3'])
&lt;/denchmark-code&gt;

##################################################################################
PredRNN.py gist
&lt;denchmark-link:https://gist.github.com/kami93/4d4dc9a220e342c9a7710de26dfc34c5&gt;https://gist.github.com/kami93/4d4dc9a220e342c9a7710de26dfc34c5&lt;/denchmark-link&gt;

Setting model_creation_device == '/cpu:1'  raises the following errors.
&lt;denchmark-code&gt;InvalidArgumentError (see above for traceback): Could not colocate node with its resource and reference inputs; devices /replica:0/task:0/device:CPU:1 and /job:localhost/replica:0/task:0/device:GPU:0 are not compatible.
	 [[{{node training/TFOptimizer/Adam/update_1/update_stlst_m2d_1/kernel_x/replica_1/ResourceApplyAdam}} = ResourceApplyAdam[T=DT_FLOAT, use_locking=false, use_nesterov=false, _device="/replica:0/task:0/device:CPU:1"](stlst_m2d_1/kernel_x/replica_1, stlst_m2d_1/kernel_x/Adam/replica_1, stlst_m2d_1/kernel_x/Adam_1/replica_1, training/TFOptimizer/Adam/update_1/update_stlst_m2d_1/kernel_x/replica_1/ResourceApplyAdam/ReadVariableOp, training/TFOptimizer/Adam/update_1/update_stlst_m2d_1/kernel_x/replica_1/ResourceApplyAdam/ReadVariableOp_1, training/TFOptimizer/Adam/learning_rate, training/TFOptimizer/Adam/beta1, training/TFOptimizer/Adam/beta2, training/TFOptimizer/Adam/epsilon, training/TFOptimizer/Identity_1)]]

&lt;/denchmark-code&gt;

Traceback: &lt;denchmark-link:https://pastebin.com/RzRdUMk1&gt;https://pastebin.com/RzRdUMk1&lt;/denchmark-link&gt;

##################################################################################
Setting the model_creation_device == '/gpu:1'  raises the following errors.
&lt;denchmark-code&gt;InvalidArgumentError (see above for traceback): Could not colocate node with its resource and reference inputs; devices /replica:0/task:0/device:CPU:1 and /job:localhost/replica:0/task:0/device:GPU:0 are not compatible.
	 [[{{node training/TFOptimizer/Adam/update_1/update_stlst_m2d_1/kernel_x/replica_1/ResourceApplyAdam}} = ResourceApplyAdam[T=DT_FLOAT, use_locking=false, use_nesterov=false, _device="/replica:0/task:0/device:CPU:1"](stlst_m2d_1/kernel_x/replica_1, stlst_m2d_1/kernel_x/Adam/replica_1, stlst_m2d_1/kernel_x/Adam_1/replica_1, training/TFOptimizer/Adam/update_1/update_stlst_m2d_1/kernel_x/replica_1/ResourceApplyAdam/ReadVariableOp, training/TFOptimizer/Adam/update_1/update_stlst_m2d_1/kernel_x/replica_1/ResourceApplyAdam/ReadVariableOp_1, training/TFOptimizer/Adam/learning_rate, training/TFOptimizer/Adam/beta1, training/TFOptimizer/Adam/beta2, training/TFOptimizer/Adam/epsilon, training/TFOptimizer/Identity_1)]]
&lt;/denchmark-code&gt;

Traceback: &lt;denchmark-link:https://pastebin.com/bAkzrt31&gt;https://pastebin.com/bAkzrt31&lt;/denchmark-link&gt;

##################################################################################
Not specifying the device context with model_creation_device raises the following errors.
gist: &lt;denchmark-link:https://gist.github.com/kami93/4283471067c2e36fc48cf0f5d8604e8a&gt;https://gist.github.com/kami93/4283471067c2e36fc48cf0f5d8604e8a&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;InvalidArgumentError (see above for traceback): Could not colocate node with its resource and reference inputs; devices /replica:0/task:0/device:CPU:1 and /job:localhost/replica:0/task:0/device:GPU:0 are not compatible.
	 [[{{node training/TFOptimizer/Adam/update_1/update_stlst_m2d_1/kernel_x/replica_1/ResourceApplyAdam}} = ResourceApplyAdam[T=DT_FLOAT, use_locking=false, use_nesterov=false, _device="/replica:0/task:0/device:CPU:1"](stlst_m2d_1/kernel_x/replica_1, stlst_m2d_1/kernel_x/Adam/replica_1, stlst_m2d_1/kernel_x/Adam_1/replica_1, training/TFOptimizer/Adam/update_1/update_stlst_m2d_1/kernel_x/replica_1/ResourceApplyAdam/ReadVariableOp, training/TFOptimizer/Adam/update_1/update_stlst_m2d_1/kernel_x/replica_1/ResourceApplyAdam/ReadVariableOp_1, training/TFOptimizer/Adam/learning_rate, training/TFOptimizer/Adam/beta1, training/TFOptimizer/Adam/beta2, training/TFOptimizer/Adam/epsilon, training/TFOptimizer/Identity_1)]]
&lt;/denchmark-code&gt;

Traceback: &lt;denchmark-link:https://pastebin.com/bF8DByeG&gt;https://pastebin.com/bF8DByeG&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='kami93' date='2018-11-13T16:01:04Z'>
		Is there a fix to this:
&lt;denchmark-code&gt;No node-device colocations were active during op 'replica_2/sequential/stlst_m2d/while/convolution_14' creation.
Device assignments active during op 'replica_2/sequential/stlst_m2d/while/convolution_14' creation:
  with tf.device(/gpu:2): &lt;/home/simon/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/utils/multi_gpu_utils.py:221&gt;
&lt;/denchmark-code&gt;

I am having the same problem.
Thanks a lot.
		</comment>
		<comment id='3' author='kami93' date='2018-12-03T22:10:25Z'>
		DistributionStrategies will be the new API for distributing Keras models. Can you try using &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/README.md#multi-gpu-training&gt;the new API&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='4' author='kami93' date='2018-12-04T10:18:39Z'>
		I have been trying the new api using distribution = tf.contrib.distribute.MirroredStrategy(). The code ran through with the following warning:
WARNING:tensorflow:Not all devices in DistributionStrategy are visible to TensorFlow session.                                                                                                   WARNING:tensorflow:You are accessing attribute optimizerof the DistributedCallbackModel that may not have been set correctly. WARNING:tensorflow:You are accessing attribute _unconditional_checkpoint_dependenciesof the DistributedCallbackModel that may not have been set correctly.
I also did not see any GPU involved via gpustats.
Is this normal?
		</comment>
		<comment id='5' author='kami93' date='2018-12-04T21:50:29Z'>
		No, but it is difficult to help you debug without further information. Can you provide a minimal code snippet that will allow us to reproduce what you are seeing?
		</comment>
		<comment id='6' author='kami93' date='2018-12-06T09:00:55Z'>
		System information

Have I written custom code
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.12.0
Python version: 3.5
GCC/Compiler version (if compiling from source): 5.4.0
CUDA/cuDNN version: 9.0
GPU model and memory: TITAN Xp (12196MiB)
--

Describe the current behavior

WARNINGS SHOWN ABOVE
While the script is running, I also did not see any GPU involved via gpustats.

The script without distribution = tf.contrib.distribute.MirroredStrategy() ran well. Since I need to populate the network with a huge set, I plan to parallelize the work on various GPUs.
Describe the expected behavior

Please kindly let me know how should I interpret the WARNINGS above and get rid of them.
Multiple GPUs are expected to be in usage while the script is called.

Code to reproduce the issue
My network architecture looks like this (a hierarchical one):
&lt;denchmark-code&gt;config = tf.ConfigProto(allow_soft_placement = True, log_device_placement= False)
os.environ['CUDA_VISIBLE_DEVICES'] = options.GPU  # specify the GPU used by parser.add_option()

# load in data
#######################DNN Level 1########################
if options.L1_model == 0:
	model = BuildModel()
        tf.keras.backend.get_session().run(tf.global_variables_initializer())
	model.fit(X_train, y_train[:, 0],
			validation_data=(X_test, y_test[:, 0]),
			epochs=options.epochs,
			verbose=2,
			batch_size=options.batch_size_L1)
          #######################DNN Level 1########################
if options.L2_model == 0:
 
	model = BuildModel()
        tf.keras.backend.get_session().run(tf.global_variables_initializer())
	model.fit(..., batch_size=options.batch_size_L2)
&lt;/denchmark-code&gt;

I don't think the warnings have something to do with a hierarchical structure. In a flat model, the same warnings showed up.
BuildModel() is defined this way:
&lt;denchmark-code&gt;def buildModel(word_index, embeddings_index, nClasses, 
MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, gpusno, nLayers=3,Number_Node=100, dropout=0.5):
	
	embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))
	for word, i in word_index.items():
		embedding_vector = embeddings_index.get(word)
		if embedding_vector is not None:
			embedding_matrix[i] = embedding_vector
			
	model = tf.keras.models.Sequential()
	
	model.add(tf.keras.layers.Embedding(len(word_index) + 1,
								EMBEDDING_DIM,
								weights=[embedding_matrix],
								input_length=MAX_SEQUENCE_LENGTH,
								trainable=True))
	model.add(tf.keras.layers.Flatten())
	
	for i in range(0,nLayers):
		model.add(tf.keras.layers.Dense(Number_Node, activation='relu'))
		model.add(tf.keras.layers.Dropout(dropout))
	
	model.add(tf.keras.layers.Dense(nClasses, activation='softmax'))

	
	distribution = tf.contrib.distribute.MirroredStrategy()
        # i cannot add argument num_gpus, as tf will say it cannot find the devices.
	
	model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,
				  optimizer=tf.train.AdamOptimizer(),
				  metrics=['accuracy'], 
				  distribute=distribution)
				  
	print('model summary:') 
	model.summary()

	return model
&lt;/denchmark-code&gt;

BTW, I compared the multi_gpu mode and single gpu mode on a smaller sample after 1 epoch. The training and validation accuracies vary largely. In most of the submodels, single gpu model produces much higher accuracy numbers. I will run the scripts for more epochs and see about results. Any suggestions on this?
		</comment>
		<comment id='7' author='kami93' date='2019-01-04T12:12:37Z'>
		
It has been 15 days with no activity and the awaiting response label was assigned. Is this still an issue?

Yes, it is still an issue, please advise accordingly. Thank you.
		</comment>
		<comment id='8' author='kami93' date='2019-02-18T18:47:52Z'>
		We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!
		</comment>
	</comments>
</bug>