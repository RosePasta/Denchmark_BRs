<bug id='23369' author='dsindex' open_date='2018-10-30T08:00:29Z' closed_time='2018-10-30T17:51:43Z'>
	<summary>KeyError: 'BlockLSTM' when using tf.train.import_meta_graph()</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
CentOS 7
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):
pip install tensorflow-gpu
TensorFlow version (use command below):
1.11
Python version:
python 3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
CUDA 9.0, cuDNN 7.31
GPU model and memory:
TITAN X (Pascal), 12G

You can collect some of this information using our environment capture &lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with
python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
v1.11.0-0-gc19e29306c 1.11.0
Describe the current behavior
I'm using two kinds of LSTM operations like below.
    def __bi_lstm(self, inputs, lengths, rnn_size, keep_prob=0.5, scope='bi-lstm'):
        """Apply bi-directional LSTM
        """
        with tf.variable_scope(scope):
            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size)
            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size)
            (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(cell_fw,
                                                                        cell_bw,
                                                                        inputs,
                                                                        sequence_length=lengths,
                                                                        dtype=tf.float32)
            outputs = tf.concat([output_fw, output_bw], axis=-1)
            return tf.nn.dropout(outputs, keep_prob)

    def __bi_lstm_fused(self, inputs, lengths, rnn_size, keep_prob=0.5, scope='bi-lstm-fused'):
        """Apply bi-directional LSTM block fused
        """
        with tf.variable_scope(scope):
            t = tf.transpose(inputs, perm=[1, 0, 2])  # Need time-major
            lstm_cell_fw = tf.contrib.rnn.LSTMBlockFusedCell(rnn_size)
            lstm_cell_bw = tf.contrib.rnn.LSTMBlockFusedCell(rnn_size)
            lstm_cell_bw = tf.contrib.rnn.TimeReversedFusedRNN(lstm_cell_bw)
            output_fw, _ = lstm_cell_fw(t, dtype=tf.float32, sequence_length=lengths)
            output_bw, _ = lstm_cell_bw(t, dtype=tf.float32, sequence_length=lengths)
            outputs = tf.concat([output_fw, output_bw], axis=-1)
            outputs = tf.transpose(outputs, perm=[1, 0, 2])
            return tf.nn.dropout(outputs, keep_prob)
i trained a model and save it via saver.save().
and then, tried to import_meta_data
loader = tf.train.import_meta_graph(meta_file, clear_devices=True)
....
in case tf.contrib.rnn.LSTMCell(), everything goes fine.
but, when it comes to use tf.contrib.rnn.LSTMBlockFusedCell(),
there is an key error.
 Traceback (most recent call last):
  File "export.py", line 55, in &lt;module&gt;
    export(args)
  File "export.py", line 13, in export
    loader = tf.train.import_meta_graph(meta_file, clear_devices=True)
  File "~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/training/saver.py", line 1666, in import_meta_graph
    meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]
  File "~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/training/saver.py", line 1688, in _import_meta_graph_with_return_elements
    **kwargs))
  File "~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/framework/meta_graph.py", line 806, in import_scoped_meta_graph_with_return_elements
    return_elements=return_elements)
  File "~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py", line 488, in new_func
    return func(*args, **kwargs)
  File "~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/framework/importer.py", line 391, in import_graph_def
    _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def)
  File "~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/framework/importer.py", line 158, in _RemoveDefaultAttrs
    op_def = op_dict[node.op]
KeyError: 'BlockLSTM'
Describe the expected behavior
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='dsindex' date='2018-10-30T17:51:43Z'>
		The contrib ops are lazily imported, so you need to add a line just saying "tf.contrib.rnn" before importing the metagraph.
		</comment>
		<comment id='2' author='dsindex' date='2018-12-08T09:45:49Z'>
		
The contrib ops are lazily imported, so you need to add a line just saying "tf.contrib.rnn" before importing the metagraph.

What he said is  right. So you have to do  the following
&lt;denchmark-code&gt;import tensorflow as tf
tf.contrib.rnn
sess = tf.Session()
imported_meta = tf.train.import_meta_graph("model_dir/model.ckpt-0.meta")  
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='dsindex' date='2019-03-13T18:08:19Z'>
		I am having exact the same issue in a C++ program that loads a frozen graph and attempts to do inference. How I could import those contrib ops in a C++ program?
		</comment>
		<comment id='4' author='dsindex' date='2019-03-13T18:13:56Z'>
		On a C++ program you need to dlopen the .so file containing the contrib ops. If you search for .so / dll it in the contrib/ directory of the python tf installation (inside site_packages or wherever else your tf was installed) you can find it.
		</comment>
		<comment id='5' author='dsindex' date='2019-03-13T23:28:25Z'>
		i had the same issue and solved it.
&lt;denchmark-link:https://github.com/dsindex/etagger/blob/master/inference/cc/src/TFUtil.cc&gt;https://github.com/dsindex/etagger/blob/master/inference/cc/src/TFUtil.cc&lt;/denchmark-link&gt;


for LSTMBlockFusedCell()
$ rnn_path=python -c "import tensorflow; print(tensorflow.contrib.rnn.__path__[0])"
$ rnn_ops_lib=${rnn_path}/python/ops/_lstm_ops.so
$ cp -rf ${rnn_ops_lib} ${TENSORFLOW_BUILD_DIR}
$ export LD_LIBRARY_PATH=${TENSORFLOW_BUILD_DIR}:$LD_LIBRARY_PATH

		</comment>
		<comment id='6' author='dsindex' date='2019-06-25T20:42:40Z'>
		
On a C++ program you need to dlopen the .so file containing the contrib ops. If you search for .so / dll it in the contrib/ directory of the python tf installation (inside site_packages or wherever else your tf was installed) you can find it.

Can you (or anyone who has done this) provide more details? I tried that and the error remains.
Error is: Op type not registered 'Resampler' in binary running on &lt;computer-name&gt;
So I added this code:
&lt;denchmark-code&gt;void LoadResamplerOps() {
    std::cout &lt;&lt; "Attempting dlopen(" &lt;&lt; kResamplerOpsLibraryPath &lt;&lt; ")" &lt;&lt; std::endl;
    void *handle =  dlopen(kResamplerOpsLibraryPath.c_str(), RTLD_NOW);
    if (!handle) {
        std::cerr &lt;&lt; "Error opening " &lt;&lt; kResamplerOpsLibraryPath &lt;&lt; ": " &lt;&lt; dlerror() &lt;&lt; std::endl;
        return;
    }
    // Clear any existing error if file loaded successfully
    dlerror();
    std::cout &lt;&lt; kResamplerOpsLibraryPath &lt;&lt; " loaded successfully" &lt;&lt; std::endl;
}
&lt;/denchmark-code&gt;

I successfully load the .so file but still get the same error.  I'm guessing after dlopen I have to do some additional things, but I am not sure what they would be.
		</comment>
		<comment id='7' author='dsindex' date='2019-06-25T22:33:41Z'>
		It sometimes happens when the op is pinned to a device (like GPU), but the
op doesn't on that device.

Did you run the graph successfully before exporting? Are you running the
import on the same machine?
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Tue, Jun 25, 2019 at 1:50 PM Apollys ***@***.***&gt; wrote:
 On a C++ program you need to dlopen the .so file containing the contrib
 ops. If you search for .so / dll it in the contrib/ directory of the python
 tf installation (inside site_packages or wherever else your tf was
 installed) you can find it.

 Can you (or anyone who has done this) provide more details? I tried that
 and the error remains.

 Error is: Op type not registered 'Resampler' in binary running on
 &lt;computer-name&gt;

 So I added this code:

 void LoadResamplerOps() {
     std::cout &lt;&lt; "Attempting dlopen(" &lt;&lt; kResamplerOpsLibraryPath &lt;&lt; ")" &lt;&lt; std::endl;
     void *handle =  dlopen(kResamplerOpsLibraryPath.c_str(), RTLD_NOW);
     if (!handle) {
         std::cerr &lt;&lt; "Error opening " &lt;&lt; kResamplerOpsLibraryPath &lt;&lt; ": " &lt;&lt; dlerror() &lt;&lt; std::endl;
         return;
     }
     // Clear any existing error if file loaded successfully
     dlerror();
     std::cout &lt;&lt; kResamplerOpsLibraryPath &lt;&lt; " loaded successfully" &lt;&lt; std::endl;
 }

 I successfully load the .so file but still get the same error. I'm
 guessing after dlopen I have to do some additional things, but I am not
 sure what they would be.

 —
 You are receiving this because you were assigned.
 Reply to this email directly, view it on GitHub
 &lt;#23369?email_source=notifications&amp;email_token=AE75E3PIBYQDD636VIBEC7LP4KAJBA5CNFSM4GAFPLX2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODYRRCSI#issuecomment-505614665&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AE75E3KHFNI46IFLUK7JVETP4KAJBANCNFSM4GAFPLXQ&gt;
 .



		</comment>
	</comments>
</bug>