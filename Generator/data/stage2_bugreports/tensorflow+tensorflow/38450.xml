<bug id='38450' author='ekuznetsov139' open_date='2020-04-11T09:34:33Z' closed_time='2020-04-21T00:35:59Z'>
	<summary>XLA without autoclustering?</summary>
	<description>
&lt;denchmark-link:https://www.tensorflow.org/xla/#explicit_compilation_with_tffunction&gt;https://www.tensorflow.org/xla/#explicit_compilation_with_tffunction&lt;/denchmark-link&gt;
 seems to imply that it is possible to enable XLA for parts of the graph without involving autoclustering.
I am attempting to do this for the Gelu op in BERT &lt;denchmark-link:https://github.com/google-research/bert/blob/master/modeling.py#L264&gt;https://github.com/google-research/bert/blob/master/modeling.py#L264&lt;/denchmark-link&gt;
 as suggested here &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/37937&gt;#37937&lt;/denchmark-link&gt;
 and the observed behavior does not match my expectation.
I'm adding the tf.function tag to a function 'gelu' that contains tf.tanh and some cwise arithmetics, but not touching global jit options, etc. My expectation is to produce a number of identical clusters implementing this 'gelu', and possibly a number of clusters implementing its gradients.
What I'm actually seeing instead, is a number of clusters that seem to be greedily constructed around 'gelu's, but are also sweeping everything in the neighborhood, up to and including GEMMs:
&lt;denchmark-code&gt;2020-04-11 09:17:46.357008: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:888] Assigning node gradients/AddN_149 to cluster cluster_1
2020-04-11 09:17:46.372574: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1471] *** Clustering info for graph of size 14527
2020-04-11 09:17:46.372603: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1472]  Built 50 clusters, size 4788 / 14527 (32.96%)
2020-04-11 09:17:46.372622: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1481]   cluster_0 114 / 14527 (0.78%)
2020-04-11 09:17:46.372630: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    AddN: 2 instances
2020-04-11 09:17:46.372636: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    AddV2: 4 instances
2020-04-11 09:17:46.372642: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    BiasAdd: 3 instances
2020-04-11 09:17:46.372648: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    BiasAddGrad: 2 instances
2020-04-11 09:17:46.372654: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Const: 24 instances
2020-04-11 09:17:46.372660: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Exp: 1 instances
2020-04-11 09:17:46.372666: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    LogSoftmax: 2 instances
2020-04-11 09:17:46.372671: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    MatMul: 5 instances
2020-04-11 09:17:46.372677: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Mean: 3 instances
2020-04-11 09:17:46.372683: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Mul: 19 instances
2020-04-11 09:17:46.372688: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Neg: 4 instances
2020-04-11 09:17:46.372695: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    OneHot: 2 instances
2020-04-11 09:17:46.372701: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    PartitionedCall: 1 instances
2020-04-11 09:17:46.372706: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    ReadVariableOp: 8 instances
2020-04-11 09:17:46.372712: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    RealDiv: 1 instances
2020-04-11 09:17:46.372717: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Reciprocal: 1 instances
2020-04-11 09:17:46.372723: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Reshape: 10 instances
2020-04-11 09:17:46.372729: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Rsqrt: 1 instances
2020-04-11 09:17:46.372734: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    RsqrtGrad: 1 instances
2020-04-11 09:17:46.372742: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    SquaredDifference: 1 instances
2020-04-11 09:17:46.372750: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Squeeze: 1 instances
2020-04-11 09:17:46.372759: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    StridedSlice: 1 instances
2020-04-11 09:17:46.372767: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Sub: 3 instances
2020-04-11 09:17:46.372775: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Sum: 9 instances
2020-04-11 09:17:46.372783: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Tanh: 1 instances
2020-04-11 09:17:46.372792: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Tile: 4 instances
2020-04-11 09:17:46.372810: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1481]   cluster_1 232 / 14527 (1.60%)
2020-04-11 09:17:46.372840: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    AddN: 13 instances
2020-04-11 09:17:46.372849: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    BatchMatMulV2: 8 instances
2020-04-11 09:17:46.372859: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    BiasAddGrad: 10 instances
2020-04-11 09:17:46.372868: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Const: 50 instances
2020-04-11 09:17:46.372877: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    MatMul: 23 instances
2020-04-11 09:17:46.372885: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Mul: 58 instances
2020-04-11 09:17:46.372895: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Neg: 4 instances
2020-04-11 09:17:46.372904: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    PartitionedCall: 1 instances
2020-04-11 09:17:46.372913: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Reshape: 19 instances
2020-04-11 09:17:46.372927: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    RsqrtGrad: 4 instances
2020-04-11 09:17:46.372936: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Sub: 6 instances
2020-04-11 09:17:46.372945: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Sum: 19 instances
2020-04-11 09:17:46.372958: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Tile: 8 instances
2020-04-11 09:17:46.372967: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Transpose: 8 instances
2020-04-11 09:17:46.372976: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    UnsortedSegmentSum: 1 instances
2020-04-11 09:17:46.372995: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1481]   cluster_10 113 / 14527 (0.78%)
2020-04-11 09:17:46.373006: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    AddN: 6 instances
2020-04-11 09:17:46.373016: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    BatchMatMulV2: 4 instances
2020-04-11 09:17:46.373024: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    BiasAddGrad: 6 instances
2020-04-11 09:17:46.373032: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Const: 23 instances
2020-04-11 09:17:46.373040: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    MatMul: 12 instances
2020-04-11 09:17:46.373049: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Mul: 29 instances
2020-04-11 09:17:46.373059: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Neg: 2 instances
2020-04-11 09:17:46.373068: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    PartitionedCall: 1 instances
2020-04-11 09:17:46.373077: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Reshape: 8 instances
2020-04-11 09:17:46.373087: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    RsqrtGrad: 2 instances
2020-04-11 09:17:46.373111: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Sub: 3 instances
2020-04-11 09:17:46.373119: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Sum: 9 instances
2020-04-11 09:17:46.373128: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Tile: 4 instances
2020-04-11 09:17:46.373136: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Transpose: 4 instances
2020-04-11 09:17:46.373151: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1481]   cluster_11 113 / 14527 (0.78%)

&lt;/denchmark-code&gt;

This presents reliability problems (if XLA is going to drag random operations from the graph into the cluster, I can never be sure that it won't end up with something it can't compile correctly) and also performance problems (instead of compiling 1 or 2 relatively short kernels, it ends up trying to compile 100 large kernels and that takes several minutes.)
Is this the intended behavior, and can this be avoided?
	</description>
	<comments>
		<comment id='1' author='ekuznetsov139' date='2020-04-12T04:11:42Z'>
		I should note that I see this even if I set autoclustering policy in compiler/jit/xla_gpu_device.cc to kIfExplicitlyRequested.
I think this is happening because the function MarkForCompilationPassImpl::RunEdgeContractionLoop() is executed regardless of the global JIT setting. If I comment it out, I get the exact behavior I originally expected:
&lt;denchmark-code&gt;2020-04-12 03:17:07.617912: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1471] *** Clustering info for graph of size 14527
2020-04-12 03:17:07.617948: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1472]  Built 50 clusters, size 50 / 14527 (0.34%)
2020-04-12 03:17:07.617973: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1481]   cluster_0 1 / 14527 (0.01%)
2020-04-12 03:17:07.617984: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    PartitionedCall: 1 instances
2020-04-12 03:17:07.617992: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1481]   cluster_1 1 / 14527 (0.01%)
2020-04-12 03:17:07.618001: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    PartitionedCall: 1 instances
2020-04-12 03:17:07.618009: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1481]   cluster_10 1 / 14527 (0.01%)
2020-04-12 03:17:07.618018: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    PartitionedCall: 1 instances
2020-04-12 03:17:07.618026: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1481]   cluster_11 1 / 14527 (0.01%)
2020-04-12 03:17:07.618035: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    PartitionedCall: 1 instances
2020-04-12 03:17:07.618043: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1481]   cluster_12 1 / 14527 (0.01%)
2020-04-12 03:17:07.618052: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    PartitionedCall: 1 instances
2020-04-12 03:17:07.618060: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1481]   cluster_13 1 / 14527 (0.01%)
&lt;/denchmark-code&gt;

Perhaps that function should be made conditional on the setting of tf_xla_auto_jit?
		</comment>
		<comment id='2' author='ekuznetsov139' date='2020-04-19T20:46:57Z'>
		&lt;denchmark-link:https://github.com/cheshire&gt;@cheshire&lt;/denchmark-link&gt;
 can you comment?
		</comment>
		<comment id='3' author='ekuznetsov139' date='2020-04-20T17:37:39Z'>
		Are you using the nightly build?

https://www.tensorflow.org/xla/#explicit_compilation_with_tffunction seems to imply that it is possible to enable XLA for parts of the graph without involving autoclustering.

Yes!

I'm adding the tf.function tag to a function 'gelu' that contains tf.tanh and some cwise arithmetics, but not touching global jit options, etc

Yes, that makes sense, it should be enough to add the annotation @tf.function(experimental_compile=True)

My expectation is to produce a number of identical clusters implementing this 'gelu', and possibly a number of clusters implementing its gradients

Basically yes, though calling them "clusters" can cause confusion because it's a separate mechanism from autoclustering. It will note produce identical clusters during reuse, but it would have to recompile for each new shape.

What I'm actually seeing instead, is a number of clusters that seem to be greedily constructed around 'gelu's, but are also sweeping everything in the neighborhood, up to and including GEMMs:

mark_for_compilation_pass is for autoclustering, and actually should not be doing anything at all for tf.function. What is the exact command you are using? Is autoclustering definitely off?

Is this the intended behavior, and can this be avoided?

It honestly sounds like the autoclustering mode is on.

I should note that I see this even if I set autoclustering policy in compiler/jit/xla_gpu_device.cc to kIfExplicitlyRequested.

Are you using the XLA_GPU devices? They are deprecated and cause a lot of confusion and we are trying to remove them, but it takes time. A simple answer here is not to use XLA_GPU device and not to change any settings there.

MarkForCompilationPassImpl::RunEdgeContractionLoop() is executed regardless of the global JIT setting

I think it's always executed, but it should not do anything if nothing is marked for compilation?
		</comment>
		<comment id='4' author='ekuznetsov139' date='2020-04-20T20:40:36Z'>
		I am using the rocm fork with all the latest changes from master merged in, I am not using XLA_GPU devices, and autoclustering is definitely off.
Here's my proposed fix &lt;denchmark-link:https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/pull/929/files#diff-73244848675e057e66040c0bcef78b51&gt;https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/pull/929/files#diff-73244848675e057e66040c0bcef78b51&lt;/denchmark-link&gt;
 - on its own, it breaks a few unit tests, and I have to fix them by adding tf_xla_auto_jit=2  to TF_XLA_FLAGS. Any problems with it?
An alternative would be to go deeper into the code and e.g. check whether both ends of an edge have is_xla_compile_attr_true flags before contracting them.
		</comment>
		<comment id='5' author='ekuznetsov139' date='2020-04-20T21:02:22Z'>
		&lt;denchmark-link:https://github.com/ekuznetsov139&gt;@ekuznetsov139&lt;/denchmark-link&gt;
 Is there a simple reproducer I could try to see this problem?
If you are willing to dive deep, could you see why  is doing anything at all? There is a &lt;denchmark-link:https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/compiler/jit/mark_for_compilation_pass.cc;l=1044?q=bool%20is_xla_compile_attr_true%20&gt;filter&lt;/denchmark-link&gt;
 which should only consider nodes marked with  (those are created by ) when the global JIT setting is on.
		</comment>
		<comment id='6' author='ekuznetsov139' date='2020-04-21T00:30:55Z'>
		Just noticed this &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/f7d337b2ba182332a8a465d105af3608535b20df&gt;f7d337b&lt;/denchmark-link&gt;

The change is dated 4/1, and I was working with a fork synchronized up to 3/30 :( Pretty sure it fixes my problem.
		</comment>
		<comment id='7' author='ekuznetsov139' date='2020-04-21T00:36:00Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38450&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38450&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='ekuznetsov139' date='2020-04-21T00:44:50Z'>
		Yup, that's why I have asked whether it is latest TF.
		</comment>
	</comments>
</bug>