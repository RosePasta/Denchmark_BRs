<bug id='38581' author='ludakas' open_date='2020-04-15T21:13:00Z' closed_time='2020-08-26T11:38:08Z'>
	<summary>Memory leak when validation generator is used</summary>
	<description>
System information
Tested in 2 environments:


my computer with Linux Ubuntu 18.04.3 LTS,
tensorflow GIT_VERSION and VERSION: v1.12.1-27410-g0f2c6e4a99 2.2.0-dev20200317,
CUDA release 10.1, V10.1.243, GeForce GTX 1060 6GB


colaboratory
tensorflow GIT_VERSION and VERSION: v2.2.0-rc2-0-ge6e5d6df2a 2.2.0-rc2


Describe the current behavior
When the validation generator is passed to the model fit function the memory is gradually filling until it is completely used up and the process is killed.
The memory is filled faster when input is larger - the input vector in the example has length 500000 (in my real scenario, I use high-resolution images..). The memory is filled only once during the validation iteration, it does not matter how many validation_steps are taken (in the example is set to 2).
Important observation
The memory leak occurs only when keras is imported from tensorflow eg. from tensorflow import keras. It works fine when keras is imported separately eg. import keras with tensorflow backend.
Describe the expected behavior
The memory usage should not increase after validation iteration.

The issue is reproduced in this colab notebook:
&lt;denchmark-link:https://colab.research.google.com/drive/1AyHgzv4JmN5iFBuMD-Cbey3Uj0WpKcka&gt;https://colab.research.google.com/drive/1AyHgzv4JmN5iFBuMD-Cbey3Uj0WpKcka&lt;/denchmark-link&gt;

or it can be reproduced with the following script. It prints the maximal memory usage. The memory does not have to increase in every iteration but with enough epochs it fills the whole memory.
&lt;denchmark-code&gt;import sys
import resource  # used for monitoring memory usage
import logging
import numpy as np

from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras import callbacks

"""In order to prevent the memory leak remove/comment out the tensorflow 
imports above and uncomment keras imports below"""
# import keras
# from keras.models import Sequential
# from keras.layers import Dense
# from keras import callbacks


class MemoryLoggerCallback(callbacks.Callback):
    def __init__(self):
        self.memory_usage = []

    def on_epoch_end(self, epoch, logs=None):
        max_used_memory = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
        logging.info(f"\nmemory usage after end of epoch hook: {max_used_memory}")
        if epoch % 10 == 0:
            self.memory_usage.append(max_used_memory)
            logging.info("############")
            logging.info(self.memory_usage)
            logging.info("############")


def dummy_gen_simple(input_dim: int, batch_size: int = 3):
    """random data and label generator"""
    while True:
        x = np.random.random((batch_size, input_dim))
        y = keras.utils.to_categorical(np.random.randint(10, size=(batch_size, 1)), num_classes=10)
        yield (x, y, None)


def init_logger():
    """initialise logger which redirects to stderr,
    so it prints log messages during training"""
    # set up global logger
    logger = logging.getLogger('')
    logger.setLevel(logging.INFO)
    logger.handlers = []  # remove default handlers
    # set up STDERR handler
    stderr_handler = logging.StreamHandler(sys.stderr)
    logger.addHandler(stderr_handler)


init_logger()

input_dim = 500000

# arbitrary simple model
model = Sequential()
model.add(Dense(64, activation='relu', input_dim=input_dim))
model.add(Dense(64, activation='relu'))
model.add(Dense(10, activation='softmax'))

model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

memory_logger_callback = MemoryLoggerCallback()

model.fit(dummy_gen_simple(input_dim, batch_size=32),
          steps_per_epoch=10,
          epochs=252,
          validation_data=dummy_gen_simple(input_dim, batch_size=16),
          validation_steps=2,
          callbacks=[memory_logger_callback])

&lt;/denchmark-code&gt;

Other info / logs
Always increasing maximum memory used, logged every 10th epoch (tensorflow.keras used):
[2994176, 2994972, 3560776, 4187488, 4813912, 5440976, 6067128, 6694228, 7352372, 7978688, 8605452, 9326704, 10061912, 10428392, 11055996, 11603124, 12187312]
It should look like:
Stabilized memory use, logged every 50th epoch (keras used):
[2677340, 2864996, 2864996, 2864996, 2864996, 2864996, 2927704, 2990080, 2990080, 2990080, 2990080, 2990080, 2990080, 2990080, 2990080, 2990080, 2990344, 2990344, 2990344, 2990344, 2990344]
	</description>
	<comments>
		<comment id='1' author='ludakas' date='2020-04-16T12:23:53Z'>
		&lt;denchmark-link:https://github.com/ludakas&gt;@ludakas&lt;/denchmark-link&gt;
,
I tried to reproduce the issue, but I did not observe much difference between the two. Here are the logs


Using tf.keras
[3365956, 3592952, 3592952, 3592952, 3592952]


When Keras is imported directly
[3597388, 3722468, 3722468, 3722468, 3722468, 3722468, 3722468, 3722468, 3722468, 3722468, 3722468, 3723300, 3723300, 3723300, 3723816, 3723816, 3723816, 3723816, 3723816, 3723816, 3723816]


Please find the gist of it &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/6b1bc34036bcc588d7a19a9244f349f0/38581.ipynb#scrollTo=x-GFPMh94iIg&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='ludakas' date='2020-04-16T17:57:42Z'>
		Thank you &lt;denchmark-link:https://github.com/amahendrakar&gt;@amahendrakar&lt;/denchmark-link&gt;
. It seems to me that you did not run the cells in order. The list with memory usage should be logged every 10 epochs () in the tf.keras (upper) part, but in your logs it is being logged every 50 epochs as it is in the bottom part (functioning when keras is imported directly).
When I run the first 4 cells of your gist without any changes, I get the following list of memory usages:
[&lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/17672848bce6d03c0d25da7eee616b7d8fc2f838&gt;1767284&lt;/denchmark-link&gt;
, 2520072, 3146460, 3772900, 4399492, 5026388, 5652824, 6279420, 6968312, &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/7563824cb49a01087414e03d7771ddfc567f7ef5&gt;7563824&lt;/denchmark-link&gt;
, 8190500, 8816916, 9693812, 10511008, 11327748, 12207008, 13023604, 13839876, 14718724, 15660240, 16413816, 17293184, 18171708, 19113372, 19867548]
It was run on a machine with increased memory to 25GB RAM (increased by Colab automatically or you), but of course it this is not the solution...
		</comment>
		<comment id='3' author='ludakas' date='2020-05-27T19:49:56Z'>
		&lt;denchmark-link:https://colab.research.google.com/gist/jvishnuvardhan/921420847232e4e7b2cc76d6c24867d3/valid_generator_memory_leak.ipynb&gt;Here&lt;/denchmark-link&gt;
 a gist with recent tf-nightly. This is for our reference. Thanks
		</comment>
		<comment id='4' author='ludakas' date='2020-08-20T23:21:26Z'>
		&lt;denchmark-link:https://github.com/ludakas&gt;@ludakas&lt;/denchmark-link&gt;
 I think this was resolved in recent . There was a recent work on improving the performance and i can see the memory leak as I noticed earlier and you reported initially. &lt;denchmark-link:https://colab.research.google.com/gist/jvishnuvardhan/52e855cb464feef3889cdd61d4adc93f/valid_generator_memory_leak.ipynb&gt;Here&lt;/denchmark-link&gt;
 is the gist for your reference.
Here is the memory usage from tensorflow.keras
[1453924, 1706588, 1831892, 1831896, 1831900, 1832168, 1832184, 1832184, 1832184, 1832188, 1832204, 1832208, 1832208, 1832208, 1832208, 1832208, 1832208, 1832208, 1832212, 1832212, 1832212, 1832212, 1832212, 1832216, 1832216]
Can you please verify once and close the issue if this was resolved for you. Thanks!
		</comment>
		<comment id='5' author='ludakas' date='2020-08-26T11:38:08Z'>
		Yes, I can confirm that this was resolved. Thank you very much!
I tested it on both tensorflow 2.3.0 and tf-nightly 2.4.0a20200825 and the memory leak does not occur in either.
Just an observation, when I use the set up (input size, model architecture...) as is in the colab above the memory usage "saturates" at about 2.6 GB on tf 2.3.0 while on tf-nightly 2.4.0a20200825 at about 1.9 GB (as is in the colab). Apparently the recent work on improving the performance is noticeable. Good job!
		</comment>
		<comment id='6' author='ludakas' date='2020-08-26T11:38:09Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38581&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38581&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>