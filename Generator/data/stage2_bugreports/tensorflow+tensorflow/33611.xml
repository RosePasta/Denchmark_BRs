<bug id='33611' author='yeyupiaoling' open_date='2019-10-22T14:22:07Z' closed_time='2019-11-16T02:10:22Z'>
	<summary>An error occurred in build  quantized model to lite</summary>
	<description>
tensorflow 1.14.0
when I build quantized SSD MobileNetV2 model, haapen error.  reference documentation: &lt;denchmark-link:https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md&gt;https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;bazel run -c opt tensorflow/lite/toco:toco -- \
--input_file=/home/zengpr/lite/model/tflite_graph.pb \
--output_file=/home/zengpr/lite/model/ssd_mobilenet_v2.tflite \
--input_shapes=1,300,300,3 \
--input_arrays=normalized_input_image_tensor \
--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \
--inference_type=QUANTIZED_UINT8 \
--mean_values=128 \
--std_values=128 \
--change_concat_input_ranges=false \
--allow_custom_ops
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;Array FeatureExtractor/MobilenetV2/Conv/Relu6, which is an input to the DepthwiseConv operator 
producing the output array FeatureExtractor/MobilenetV2/expanded_conv/depthwise/Relu6, is 
lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-
quantized output format, or run quantized training with your model from a floating point checkpoint
 to change the input graph to contain min/max information. If you don't care about accuracy, you can 
pass --default_ranges_min= and --default_ranges_max= for easy experimentation.
&lt;/denchmark-code&gt;

But I used  --default_ranges_min= and --default_ranges_max= ,   model canit detection in predict.
&lt;denchmark-code&gt;bazel run -c opt tensorflow/lite/toco:toco -- \
--input_file=/home/zengpr/lite/model/tflite_graph.pb \
--output_file=/home/zengpr/lite/model/ssd_mobilenet_v2.tflite \
--input_shapes=1,300,300,3 \
--input_arrays=normalized_input_image_tensor \
--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \
--inference_type=QUANTIZED_UINT8 \
--mean_values=128 \
--std_values=128 \
--change_concat_input_ranges=false \
--default_ranges_min=0 \
--default_ranges_max=6 \
--allow_custom_ops
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='yeyupiaoling' date='2019-11-16T02:10:22Z'>
		Try to quantize this model with the post-training integer quantization tool:
&lt;denchmark-link:https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-post-training-integer-quantization-b4964a1ea9ba&gt;https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-post-training-integer-quantization-b4964a1ea9ba&lt;/denchmark-link&gt;

We've tested it on mobilenet-v2-ssd models internally and it worked pretty well.
Feel free to open another issue when you run into problem with it. Thanks!
		</comment>
		<comment id='2' author='yeyupiaoling' date='2019-11-16T02:10:23Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33611&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33611&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>