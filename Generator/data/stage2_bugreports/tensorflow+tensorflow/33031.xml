<bug id='33031' author='anhmeow' open_date='2019-10-03T21:28:48Z' closed_time='2019-10-08T13:50:27Z'>
	<summary>Model.evaluate diverging from fit on Unet based model</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):
TensorFlow version (use command below): 1.14
Python version: 3.6.8
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory: Colab (K80 ?)

Describe the current behavior
the model.evaluate is providing totally different results from the model.fit method.
To demonstrate this, i've don't this on colab :
1/ Create a notebook with GPU setup
2/ create a dataset with only one image
3/ feed a training loop (tf.keras.model.fit on 100 epochs) with the image embedded as a tf.Data.Dataset.
As there are only one image, i got a loss of 0.0571 and a binary accuracy close to 1
4/ the, i run the model.evaluate method and i got something totally different :
loss of 0.3667 and a binary accuracy of 0.8373
5/ a run the fit method again for 1 epoch and got results close to step 3 :
loss of 0.0558 and a binary accuracy close to 1
Describe the expected behavior
The evaluate method must provide results close to the fit method, especially after 100 epochs
Code to reproduce the issue
&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from google.colab import drive

#get a image as input data for model : tensorflow logo
!curl https://avatars0.githubusercontent.com/u/15658638?s=256 --output tensor_logo.png

def train_input_fn():

  def _process_string_image(img_path):
    raw_image = tf.read_file(img_path)
    image_decoded = tf.image.decode_png(raw_image, channels=3)
    image_decoded = tf.cast(image_decoded, tf.float32)/255.
    dummy_labels = tf.round(image_decoded)
    return image_decoded, dummy_labels

  list_files = tf.data.Dataset.from_tensor_slices(['/content/tensor_logo.png'])
  files = list_files.map(_process_string_image)
  files = files.repeat(100)
  files = files.batch(1, drop_remainder=True)
  return files

#basic check to compare train_input_fn_dummy_data, train_input_fn_from_tf_records
#can't be run after TPU initialisation
with tf.Session() as sess:
  batch = train_input_fn().make_one_shot_iterator().get_next()
  item = sess.run(batch)
  print('shape of first item :', item[0].shape, item[1].shape)
  plt.imshow(item[0][0])
  plt.show()
  plt.imshow(item[1][0])

def Unet_Encoder(inp, layer_nb, activation = 'relu', padding = 'same', batchnorm = True, is_pool = True):
    inp = tf.keras.layers.Conv2D(8 * 2**layer_nb, (3, 3), activation=activation, padding=padding, name = f"conv_1_L{layer_nb}") (inp)
    if batchnorm:
        inp = tf.keras.layers.BatchNormalization(name = f"bn_1_L{layer_nb}")(inp)
    inp = tf.keras.layers.Conv2D(8 * 2**layer_nb, (3, 3), activation=activation, padding=padding, name = f"conv_2_L{layer_nb}") (inp)
    if batchnorm:
        inp = tf.keras.layers.BatchNormalization(name = f"bn_2_L{layer_nb}")(inp)
        
    if is_pool:
        return inp, tf.keras.layers.MaxPooling2D((2, 2), name = f"maxpool_L{layer_nb}") (inp)
    
    return inp

def Unet_Decoder(inp, attention, layer_nb, activation = 'relu', padding = 'same', batchnorm = True):
    inp = tf.keras.layers.Conv2DTranspose(8 * 2**layer_nb, (2, 2), strides=(2, 2), padding=padding, name = f"convT_L{layer_nb}") (inp)
    if batchnorm:
        inp = tf.keras.layers.BatchNormalization()(inp)
    inp = tf.keras.layers.concatenate([inp, attention])
    inp = tf.keras.layers.Conv2D(8 * 2**layer_nb, (3, 3), activation=activation, padding=padding, name = f"uconv_1_L{layer_nb}") (inp)
    if batchnorm:
        inp = tf.keras.layers.BatchNormalization(name = f"ubn_1_L{layer_nb}")(inp)
    inp = tf.keras.layers.Conv2D(8 * 2**layer_nb, (3, 3), activation=activation, padding=padding, name = f"uconv_2_L{layer_nb}") (inp)
    if batchnorm:
        inp = tf.keras.layers.BatchNormalization(name = f"ubn_2_L{layer_nb}")(inp)
    
    return inp

def Build_Unet(input_shape, depth, activation = 'relu', padding = 'same', batchnorm = True):
    inputs = tf.keras.layers.Input(input_shape)
    conv = [None for x in range(depth)]
    
    ###### encoding part ######
    last_pool = inputs
    for i in range(0, depth):
        conv[i], last_pool = Unet_Encoder(last_pool, i, activation, padding, batchnorm)
    
    ###### bottom layer ###### 
    uconv = Unet_Encoder(last_pool, depth, activation, padding, batchnorm, is_pool=False)
    
    ###### encoding part ######
    for i in range(depth-1, -1, -1):
        uconv = Unet_Decoder(uconv, conv[i], i, activation, padding, batchnorm)
    
    outputs = tf.keras.layers.Conv2D(3, (1, 1), activation='sigmoid') (uconv)
    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])

    return model

#build model and compile
model = Build_Unet((None, None, 3), 4)

adam = tf.keras.optimizers.Adam(learning_rate=0.005)
model.compile(adam, loss='binary_crossentropy', metrics=['binary_accuracy'])

model.summary()

# fit the model
model.fit(train_input_fn(), epochs= 100, steps_per_epoch=1)

model.evaluate(train_input_fn(), steps=1)

model.fit(train_input_fn(), epochs=1, steps_per_epoch=1)
&lt;/denchmark-code&gt;

It seems that the more epochs i run, the more the results are different from train and fit methods.
With a really simple 'dummy' models like this one, i don't have the issue :
&lt;denchmark-code&gt;inputs = tf.keras.layers.Input(shape=(256, 256, 3))
outputs = tf.keras.layers.Conv2D(6,(2,2),padding='same')(inputs)
outputs = tf.keras.layers.MaxPool2D()(outputs)
outputs = tf.keras.layers.Conv2D(12,(2,2),padding='same')(outputs)
outputs = tf.keras.layers.Conv2DTranspose(6,(4,4),(2,2),padding='same')(outputs)
outputs = tf.keras.layers.Conv2D(3,(2,2),padding='same', activation='sigmoid')(outputs)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='anhmeow' date='2019-10-04T06:47:09Z'>
		I have tried on colab with TF version 1.14,1.15.0-rc1 and was able to reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/2cba665f3b22aa25bb1c8343793fc660/untitled243.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='anhmeow' date='2019-10-04T11:43:21Z'>
		i've added a check by calling the model.predict : (just add this code after the one provided)
&lt;denchmark-code&gt;#getting the generated truth
with tf.Session() as sess:
  item = train_input_fn().make_one_shot_iterator().get_next()
  record = sess.run(item)
  print(record[1].shape)

#getting the prediction from model
preds = model.predict(train_input_fn(), steps=1)
print(preds.shape)
#calculate the binary accuracy
print(np.mean(record[1].reshape(-1) == np.round(preds).reshape(-1)))
&lt;/denchmark-code&gt;

the last line provide a result of 0.862284
With this, i have the feeling that the issue is within the fit method and the loss calculation (can be potentially link with a specific layer... i can't figure it out)
		</comment>
		<comment id='3' author='anhmeow' date='2019-10-07T16:29:23Z'>
		&lt;denchmark-link:https://github.com/anhmeow&gt;@anhmeow&lt;/denchmark-link&gt;
 I think this is expected as you are using BatchNorm. There are some layers (BatchNorm, Dropout) that are enabled during training and disabled while evaluation. If you disable  and run the training () and Evaluation (), the results are very very close (upto 3rd ot 4th decimal). Please check the &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/735dcb2ba93dbbfd0574214765eabffd/tf33031.ipynb&gt;gist here&lt;/denchmark-link&gt;
. Please check the colab gist and let us know what you think. My gist is with , however you can try with  also.
Please close the issue if this was resolved for you. Thanks!
		</comment>
		<comment id='4' author='anhmeow' date='2019-10-08T13:50:27Z'>
		Thanks for this feedback, indeed, without batchnorm, i have 'almost' the same results even with TF 1.14.
I will look deeper in BatchNorm but the issue is now solved for me !
		</comment>
		<comment id='5' author='anhmeow' date='2019-10-08T13:50:29Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33031&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33031&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>