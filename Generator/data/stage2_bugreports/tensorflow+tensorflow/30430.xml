<bug id='30430' author='ialdencoots' open_date='2019-07-05T09:53:21Z' closed_time='2019-07-17T18:03:02Z'>
	<summary>Batch size affects prediction output in RNN layers (LSTM, GRU)</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): docker
TensorFlow version (use command below): 1.12.3 and 2.0.0-beta1
Python version: 2.7.12 and 3.5.2
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: N/A
GPU model and memory: N/A

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
The output during prediction from RNN layers, i.e. LSTM and GRU is dependent on the batch size when stateful=False.
Describe the expected behavior
The predicted output tensor for a given input tensor should always produce the same result regardless of the size of the batch the sample is found in.
Code to reproduce the issue
&lt;denchmark-code&gt;from __future__ import print_function
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import LSTM, GRU

if __name__ == "__main__":
    with tf.device('/CPU:0'):
        batches = [1] + list(range(1, 10))
        shape = (1000, 512)
        inputs = tf.keras.Input(shape=shape)
        rnn = GRU(shape[-1]//2, return_sequences=True)(inputs)
        model = tf.keras.Model(inputs=inputs, outputs=rnn)
        results = []
        for i, batch in enumerate(batches):
            x = tf.ones((batch,) + shape)
            y = model.predict_on_batch(x)
            results.append(y[0])

        for b, x in list(zip(batches, results))[1:]:
            print(b, np.max(np.abs(results[0] - x)))
        if not all(np.allclose(x, results[0]) for x in results[1:]):
            raise ValueError("Varying batch size produces different results")
&lt;/denchmark-code&gt;

Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='ialdencoots' date='2019-07-09T06:58:04Z'>
		I have tried on colab with TF version 2.0 beta1 and 1.12 and was able to reproduce the issue.Thanks!
		</comment>
		<comment id='2' author='ialdencoots' date='2019-07-17T17:30:36Z'>
		Thanks for reporting the issue, let me take a look.
		</comment>
		<comment id='3' author='ialdencoots' date='2019-07-17T18:03:02Z'>
		I think it is because the numeric underflow when some certain prediction produce a very small number in the result.
if you change your code with self.assertAllClose(x, results[0]), you will notice that most of time, there is only less than 1% of the result are different, and the diff is actually very small (~1e-5). If you add the atol and rtol to the assertAllClose or np.allclose, the error actually goes away.
		</comment>
		<comment id='4' author='ialdencoots' date='2019-07-17T18:03:04Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=30430&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=30430&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='ialdencoots' date='2019-07-17T21:17:35Z'>
		If there is underflow, would that just be the particulars of a given computation? Shouldn't it be the same regardless of the batch size? Also, a diff of 1e-5 is not actually that small when the average values we're talking about are less than 1.
		</comment>
		<comment id='6' author='ialdencoots' date='2019-07-17T21:58:37Z'>
		1e-5 is 0.00001 which is quite small compare to 1.
I think the indeterministic behavior is caused by floating point number, and we actually hit it a lot during unit test, which is why we sometimes have to give the test some atol and rtol.
		</comment>
		<comment id='7' author='ialdencoots' date='2019-07-19T09:25:51Z'>
		Whether or not 1e-5 is significant depends on the application. Setting greater tolerance values only removes errors from comparisons in test cases, it doesn't resolve the issue.
With respect to this behavior, it is deterministic. If you give the same sample in a batch with the same number of samples you always get the same value. The values returned by inference should not depend on the batch size, unless the other samples in the batch are somehow affecting each other.
		</comment>
		<comment id='8' author='ialdencoots' date='2020-11-23T03:45:54Z'>
		still have the same isssue on tf 2.3
		</comment>
	</comments>
</bug>