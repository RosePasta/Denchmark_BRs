<bug id='26994' author='dynamicwebpaige' open_date='2019-03-21T15:35:12Z' closed_time='2019-07-02T12:19:07Z'>
	<summary>2.0 Reference Models: BERT (TPU with dist strat and Keras)</summary>
	<description>
&lt;denchmark-link:https://github.com/google-research/bert&gt;BERT, or Bidirectional Encoder Representations from Transformers&lt;/denchmark-link&gt;
, is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.
The research team's academic paper describes BERT in detail and provides full results on a number of tasks: &lt;denchmark-link:https://arxiv.org/abs/1810.04805&gt;https://arxiv.org/abs/1810.04805&lt;/denchmark-link&gt;
.
An example of using BERT can be found &lt;denchmark-link:https://colab.sandbox.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb&gt;here&lt;/denchmark-link&gt;
.
The purpose of this issue is to migrate models into the TF 2.0 Keras API. Each migrated model must be eager and distribution compatible, with tests, and all associated engineering artifacts.
	</description>
	<comments>
		<comment id='1' author='dynamicwebpaige' date='2019-03-27T02:10:04Z'>
		&lt;denchmark-link:https://github.com/dynamicwebpaige&gt;@dynamicwebpaige&lt;/denchmark-link&gt;
 I'd like to contribute this.
		</comment>
		<comment id='2' author='dynamicwebpaige' date='2019-04-02T19:41:31Z'>
		&lt;denchmark-link:https://github.com/dynamicwebpaige&gt;@dynamicwebpaige&lt;/denchmark-link&gt;

very interested in this.  how can help?
		</comment>
		<comment id='3' author='dynamicwebpaige' date='2019-04-03T18:43:15Z'>
		&lt;denchmark-link:https://github.com/rok&gt;@rok&lt;/denchmark-link&gt;

are you working on this?  i think ill start.  would you like to collaborate?  im just getting into TF2..
		</comment>
		<comment id='4' author='dynamicwebpaige' date='2019-04-03T20:54:13Z'>
		Hey &lt;denchmark-link:https://github.com/icanswim&gt;@icanswim&lt;/denchmark-link&gt;
! Sure, it would be a pleasure.
I didn't properly start yet because TPU support for TF2 seems to be lacking right now.
I gave you access to &lt;denchmark-link:https://github.com/rok/models&gt;my fork&lt;/denchmark-link&gt;
 or we can work via yours. I'll ping when I have some work done.
		</comment>
		<comment id='5' author='dynamicwebpaige' date='2019-05-18T15:57:10Z'>
		Hey &lt;denchmark-link:https://github.com/icanswim&gt;@icanswim&lt;/denchmark-link&gt;
, I've done some more work on top of your commit and I'll do some more (tests are passing but there are still warnings).
I already &lt;denchmark-link:https://github.com/tensorflow/models/pull/6817&gt;opened a PR&lt;/denchmark-link&gt;
, could you please sign the CLA there so it can be reviewed?
		</comment>
		<comment id='6' author='dynamicwebpaige' date='2019-05-20T21:50:05Z'>
		Sorry we missed this.  We are working on this internally and are almost done.  Expect code to be pushed before end of May-2019.
		</comment>
		<comment id='7' author='dynamicwebpaige' date='2019-06-16T07:44:45Z'>
		Are there any updates on this?
		</comment>
		<comment id='8' author='dynamicwebpaige' date='2019-06-17T17:52:37Z'>
		&lt;denchmark-link:https://github.com/tensorflow/models/tree/master/official/bert&gt;https://github.com/tensorflow/models/tree/master/official/bert&lt;/denchmark-link&gt;

The README has not been updated yet, but I think the information below will get you started.
I am only testing the GPU version but it was build for TPU so that would have been tested first.  I do not have the flags to use but I have the test cases we run nightly.  Below is what I know, I have not been going even a little deep into BERT so I don't know how to state the accuracy numbers other than what we are tracking nightly.
BERT (SQUAD):  ~.903
BERT (Classify):  .8603 with mrpc

Test Case SQUAD: official.bert.benchmark.bert_squad_benchmark.BertSquadBenchmarkReal.benchmark_8_gpu
Test Case Classify: official.bert.benchmark.bert_benchmark.BertClassifyAccuracy.benchmark_8_gpu_mrpc

I realize it is a bit painful but you can extract the FLAGS from the tests above. We are still working on performance but we are already exceeding the TF 1.0 for both single and multi-gpu single machine.  FP16 support will be coming soon.  We are working on SQUAD first but the gains would apply to both...in most instances.
Warning: The data in gs://cloud-tpu-checkpoints is public and you can pull that down.  I did not make the actual training data public because I have not taken time to find out if it can be shared directly.  It likely can, but I like to be careful.
I will help if I can; another group owns this but I deal with it daily as well for now.
		</comment>
		<comment id='9' author='dynamicwebpaige' date='2019-06-17T19:49:11Z'>
		We will publish tutorial/colab to help go through the training process.
I will first update README to explain basics like checkpoints and training data generation. (ETA: by the end of this week)
		</comment>
		<comment id='10' author='dynamicwebpaige' date='2019-07-02T12:19:07Z'>
		Thank you, &lt;denchmark-link:https://github.com/saberkun&gt;@saberkun&lt;/denchmark-link&gt;
! Closing out this issue - and excellent work! 
		</comment>
	</comments>
</bug>