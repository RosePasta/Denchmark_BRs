<bug id='29825' author='maxima120' open_date='2019-06-15T14:26:37Z' closed_time='2019-06-24T20:05:02Z'>
	<summary>Shape change during TPU training</summary>
	<description>
Once I start train my model on TPU v3-8 I get this output and model start recompiling and continue train with different shape:
&lt;denchmark-code&gt;INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(128, 1024, 7), dtype=tf.float32, name='lstm_input_10'), TensorSpec(shape=(128, 4), dtype=tf.float32, name='dense_target_30')]
INFO:tensorflow:Overriding default placeholder.
INFO:tensorflow:Remapping placeholder for lstm_input
INFO:tensorflow:Started compiling
INFO:tensorflow:Finished compiling. Time elapsed: 5.74567985534668 secs
INFO:tensorflow:Setting weights on TPU model.
 9216/11126 [=======================&gt;......] - ETA: 4s - loss: nan - categorical_accuracy: 0.0250INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(110,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(110, 1024, 7), dtype=tf.float32, name='lstm_input_10'), TensorSpec(shape=(110, 4), dtype=tf.float32, name='dense_target_30')]
INFO:tensorflow:Overriding default placeholder.
INFO:tensorflow:Remapping placeholder for lstm_input
INFO:tensorflow:Started compiling
INFO:tensorflow:Finished compiling. Time elapsed: 8.460586786270142 secs
&lt;/denchmark-code&gt;

Compare shapes before train kicks in and after:
&lt;denchmark-code&gt;INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(128, 1024, 7), dtype=tf.float32, name='lstm_input_10'), TensorSpec(shape=(128, 4), dtype=tf.float32, name='dense_target_30')]
INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(110,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(110, 1024, 7), dtype=tf.float32, name='lstm_input_10'), TensorSpec(shape=(110, 4), dtype=tf.float32, name='dense_target_30')]
&lt;/denchmark-code&gt;

Model shapes:
&lt;denchmark-code&gt;Layer (type)                 Output Shape              Param #   
=================================================================
lstm_input (InputLayer)      (None, 1024, 7)           0         
_________________________________________________________________
lstm (LSTM)                  (None, 1024, 256)         270336    
_________________________________________________________________
dropout (Dropout)            (None, 1024, 256)         0         
_________________________________________________________________
lstm_1 (LSTM)                (None, 256)               525312    
_________________________________________________________________
dropout_1 (Dropout)          (None, 256)               0         
_________________________________________________________________
dense (Dense)                (None, 4)                 1028      
=================================================================
&lt;/denchmark-code&gt;

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9
TensorFlow installed from (source or binary): 1.13.1
Python version: 3.5
GPU model and memory: TPU v3-8

Describe the current behavior
shapes change from 128 to 110 during first few batches training
Describe the expected behavior
shapes don't change
	</description>
	<comments>
		<comment id='1' author='maxima120' date='2019-06-17T13:31:33Z'>
		&lt;denchmark-link:https://github.com/maxima120&gt;@maxima120&lt;/denchmark-link&gt;
 In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!
		</comment>
		<comment id='2' author='maxima120' date='2019-06-19T14:57:30Z'>
		I will need to send you data and code for this. I don't think you can use just a snippet. Please let me know how to contact you. Thanks
		</comment>
		<comment id='3' author='maxima120' date='2019-06-20T12:07:02Z'>
		&lt;denchmark-link:https://github.com/maxima120&gt;@maxima120&lt;/denchmark-link&gt;
 Will it be possible you to share code and data in zip file. If it is not private. Thanks!
		</comment>
		<comment id='4' author='maxima120' date='2019-06-20T13:47:30Z'>
		code is no secret but data I don't want to put on the open. hope you understand.
		</comment>
		<comment id='5' author='maxima120' date='2019-06-20T13:57:11Z'>
		&lt;denchmark-code&gt;def split_sequence(features, labels, window_size):
    X, y = list(), list()
    rng = len(features) - window_size
    for i in range(rng):
        last_ix = i + window_size
        # gather input and output parts of the pattern
        seq_x, seq_y = features[i:last_ix], labels[last_ix]
        X.append(seq_x)
        y.append(seq_y)
    return np.asarray(X), np.asarray(y)
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;import tensorflow as tf
import keras

window_size = 1024
n_steps = len(data[0]) - window_size
inputs_n = 7
outputs_n = 4
neurons = 128
learning_rate = 0.00001
activation = 'softmax'

from keras.layers import Dense, Activation, Dropout, LSTM, Dropout
from tensorflow.keras import layers

tf.keras.backend.clear_session()
tf.keras.backend.get_session().run(tf.global_variables_initializer())

model = tf.keras.Sequential()
model.add(layers.LSTM(neurons, input_shape=(window_size, inputs_n), return_sequences=True)) 
model.add(layers.LSTM(neurons))
model.add(layers.Dense(neurons, activation='relu'))
model.add(layers.Dropout(0.2))
model.add(layers.Dense(outputs_n, activation=activation))

opt = tf.train.RMSPropOptimizer(learning_rate)

model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])

print(model.summary())
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;import time
# loop by day - separate features and labels

buy_sell = 'sell'
print('Training.. LABEL:', buy_sell, 'neurons:', neurons, 'learning rate:', learning_rate, 'activation:', activation)

epochs_n = 1
epochs =  range(epochs_n)

histories = list()
day_count = 0

start_time = time.time()

# d is tuple from groupby - d[0] = date, d[1] = values
for epoch in epochs:
    for d in data : 
        # get arrays for the day
        features = np.asarray(d)[:,2:9].astype(dtype = 'float32')
        labels = np.asarray(d)[:, 9:13].astype(dtype = 'int8')
        
        X,y = split_sequence(features, labels, window_size)

        try:
            H = model.fit(X,y, batch_size = window_size)
            histories.append(H.history)
        except Exception as e:
            print('** train exception :', e)
            continue
        
    #for days
#for epoch

print('DONE..')
&lt;/denchmark-code&gt;

Data split by days. All days of the same shape 11000, 11 - 7 features and 4 one-hot labels
Data sample:
&lt;denchmark-code&gt;0.322791712104689,0.323336968375136,0.00109051254089421,6.4610961249576E-05,0.746954076850984,0.7572633552015,0.746954076850984,0,1,0,0
0.323882224645583,0.323882224645583,0,6.4610961249576E-05,0.751640112464855,0.801312089971884,0.751640112464855,0,0,0,1
0.323882224645583,0.324427480916031,0.00109051254089421,0.00928782567962655,0.792877225866917,0.817244611059044,0.792877225866917,0,0,1,0
0.323882224645583,0.324427480916031,0,0.00568576458996269,0.837863167760075,0.837863167760075,0.808809746954077,0,0,1,0
0.322791712104689,0.323336968375136,0.00109051254089421,0.000516887689996608,0.820056232427366,0.820056232427366,0.799437675726336,0,0,0,0
0.323882224645583,0.323882224645583,0,3.2305480624788E-05,0.799437675726336,0.817244611059044,0.792877225866917,0,0,0,1

&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='maxima120' date='2019-06-21T05:11:28Z'>
		
code is no secret but data I don't want to put on the open. hope you understand.

Thanks for sharing the code.
		</comment>
		<comment id='7' author='maxima120' date='2019-06-22T15:23:09Z'>
		this has everything you need to reproduce:
&lt;denchmark-link:https://gist.github.com/maxima120/08241b5e00088901f5068ba1d444e82e&gt;https://gist.github.com/maxima120/08241b5e00088901f5068ba1d444e82e&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='maxima120' date='2019-06-24T20:05:02Z'>
		The re-compilation is expected.
Keras model.fit passes over the entire dataset once for each epoch. If the number of examples in the dataset is not divisible by the batch size, the last batch will be of a different size than the other training batches. (This works similarly on a GPU, you just don't see this message).
		</comment>
		<comment id='9' author='maxima120' date='2019-06-24T20:05:03Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=29825&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=29825&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='maxima120' date='2019-06-25T16:42:55Z'>
		Ok please clarify further. If say a regular batch was compiled for 128, then the last was recompiled for 110 for first epoch.. Can I be sure that all the following epoch will use 128 for all the batched but last?
		</comment>
		<comment id='11' author='maxima120' date='2019-06-25T18:49:30Z'>
		Yes, this should be the standard Keras behavior: it uses the requested batch size until the last batch of each epoch.
As compilation is cached for each new batch size, you shouldn't see a recompile after the first epoch.
		</comment>
	</comments>
</bug>