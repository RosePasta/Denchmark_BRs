<bug id='44059' author='bjarrell15' open_date='2020-10-15T18:42:26Z' closed_time='2020-11-18T14:47:50Z'>
	<summary>NotImplementedError: Cannot convert a symbolic Tensor (args_0:0) to a numpy array.</summary>
	<description>
System information

This is custom code
Running Google Colab on Mac
TensorFlow version 2.3.0
Python version 3.6.9
XLA_GPU hosted by Colab; memory_limit = 15695549568

Describe the current behavior
I have a custom augmentation function written in Colab that worked normally until today. The last time I ran the entire code through was 09/08/2020 and the augmentation functioned performed the operations normally. Now, I receive an error that pertains to symbolic tensors, which I have never seen before.
Describe the expected behavior
Augmentation function is meant to map over a batch of images and masks, taking in samples one at a time, converting them to NumPy arrays, performing some sort of augmentation, then returning them back as tensors.

A link to the Colab notebook: &lt;denchmark-link:https://colab.research.google.com/drive/1ujShezPjcveG2kg019c7zqweLtu1ESuW?usp=sharing&gt;https://colab.research.google.com/drive/1ujShezPjcveG2kg019c7zqweLtu1ESuW?usp=sharing&lt;/denchmark-link&gt;

A link to the training data I use in the notebook: &lt;denchmark-link:https://drive.google.com/drive/folders/1ZS1wKjo692Lg7Vuhtr3akz0sYSOmZBKl&gt;https://drive.google.com/drive/folders/1ZS1wKjo692Lg7Vuhtr3akz0sYSOmZBKl&lt;/denchmark-link&gt;

The particular section of code where the error stems from:
&lt;denchmark-code&gt;# Function used to perform "on-the-fly" augmentation during training.
# UPDATED ON 09/21/2020.

def augmentation(img, msk):

  # Call in skimage package, which will be used for transformations.
  from skimage.transform import rotate, AffineTransform, warp
  
  # Create some random floats, which will be used in augmentation steps.
  tilt = tf.random.uniform(shape = [], minval = -90, maxval = 90, dtype = tf.float32)
  dx = tf.random.uniform(shape = [], minval = -20, maxval = 20, dtype = tf.float32)
  dy = tf.random.uniform(shape = [], minval = -20, maxval = 20, dtype = tf.float32)
  
  # Cast image and mask to numpy arrays.
  img = np.array(img)
  msk = np.array(msk)

  # Use TensforFlow-style if conditionals, used to flip image and mask.
  img = tf.cond(tilt &gt; 0, lambda: np.fliplr(img), lambda: np.flipud(img))
  msk = tf.cond(tilt &gt; 0, lambda: np.fliplr(msk), lambda: np.flipud(msk))

  # Rotate the image and mask to some degree.
  img = rotate(img, angle = tilt, mode = 'reflect')
  msk = rotate(msk, angle = tilt, mode = 'reflect')

  # Write the conditions for an affine transformation.
  transform = AffineTransform(translation = (dx,dy))

  # Perform the affine transformation.
  img = warp(img, inverse_map = transform, mode = 'reflect')
  msk = warp(msk, inverse_map = transform, mode = 'reflect')
 
  # Convert the inputs back into tensors, put back into a tuple.
  finalTuple = (tf.convert_to_tensor(img), tf.convert_to_tensor(msk))

  return finalTuple

# Callback for data augmentation.
class aug(tf.keras.callbacks.Callback):
  def on_training_batch_begin(self, batch, logs = None):
    batch.map(augmentation, num_parallel_calls = 5)
    batch.shuffle(10)
    
# Callback for CSV logger (used for charting).
csv = tf.keras.callbacks.CSVLogger(f'/content/gdrive/My Drive/{today}_metrics.csv', separator=',', append=False)

# Callback for saving the model.
save_model_path = f'/content/gdrive/My Drive/{today}_wellpad_model_.h5'
cp = tf.keras.callbacks.ModelCheckpoint(filepath=save_model_path,
                                        monitor='val_loss',
                                        mode='min',
                                        save_best_only=True)
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;# Test the augmentation function on a batch of images.
test_batch = evaluation.take(1)

test_batch = test_batch.map(augmentation,num_parallel_calls=5) # ERROR OCCURS HERE.

# For plotting below.
test_batch = [(image,mask) for image, mask in test_batch]

plt.figure(figsize=(10,10))
plt.subplot(1,2,1)
# Show the original image.
plt.imshow(test_batch[0][0][0])
# Show the masked image.
plt.subplot(1,2,2)
plt.imshow(tf.squeeze(test_batch[0][1][0]))
&lt;/denchmark-code&gt;

Other info / logs
Here is the full error I receive:
&lt;denchmark-code&gt;---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
&lt;ipython-input-17-81c1086f76f0&gt; in &lt;module&gt;()
      2 test_batch = evaluation.take(1)
      3 
----&gt; 4 test_batch = test_batch.map(augmentation,num_parallel_calls=5)
      5 
      6 test_batch = [(image,mask) for image, mask in test_batch]

10 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py in map(self, map_func, num_parallel_calls, deterministic)
   1700           num_parallel_calls,
   1701           deterministic,
-&gt; 1702           preserve_cardinality=True)
   1703 
   1704   def flat_map(self, map_func):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py in __init__(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)
   4082         self._transformation_name(),
   4083         dataset=input_dataset,
-&gt; 4084         use_legacy_function=use_legacy_function)
   4085     if deterministic is None:
   4086       self._deterministic = "default"

/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py in __init__(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)
   3369       with tracking.resource_tracker_scope(resource_tracker):
   3370         # TODO(b/141462134): Switch to using garbage collection.
-&gt; 3371         self._function = wrapper_fn.get_concrete_function()
   3372         if add_to_graph:
   3373           self._function.add_to_graph(ops.get_default_graph())

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in get_concrete_function(self, *args, **kwargs)
   2937     """
   2938     graph_function = self._get_concrete_function_garbage_collected(
-&gt; 2939         *args, **kwargs)
   2940     graph_function._garbage_collector.release()  # pylint: disable=protected-access
   2941     return graph_function

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_garbage_collected(self, *args, **kwargs)
   2904       args, kwargs = None, None
   2905     with self._lock:
-&gt; 2906       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
   2907       seen_names = set()
   2908       captured = object_identity.ObjectIdentitySet(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3211 
   3212       self._function_cache.missed.add(call_context_key)
-&gt; 3213       graph_function = self._create_graph_function(args, kwargs)
   3214       self._function_cache.primary[cache_key] = graph_function
   3215       return graph_function, args, kwargs

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3073             arg_names=arg_names,
   3074             override_flat_arg_shapes=override_flat_arg_shapes,
-&gt; 3075             capture_by_value=self._capture_by_value),
   3076         self._function_attributes,
   3077         function_spec=self.function_spec,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    984         _, original_func = tf_decorator.unwrap(python_func)
    985 
--&gt; 986       func_outputs = python_func(*func_args, **func_kwargs)
    987 
    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py in wrapper_fn(*args)
   3362           attributes=defun_kwargs)
   3363       def wrapper_fn(*args):  # pylint: disable=missing-docstring
-&gt; 3364         ret = _wrapper_helper(*args)
   3365         ret = structure.to_tensor_list(self._output_structure, ret)
   3366         return [ops.convert_to_tensor(t) for t in ret]

/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py in _wrapper_helper(*args)
   3297         nested_args = (nested_args,)
   3298 
-&gt; 3299       ret = autograph.tf_convert(func, ag_ctx)(*nested_args)
   3300       # If `func` returns a list of tensors, `nest.flatten()` and
   3301       # `ops.convert_to_tensor()` would conspire to attempt to stack

/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    256       except Exception as e:  # pylint:disable=broad-except
    257         if hasattr(e, 'ag_error_metadata'):
--&gt; 258           raise e.ag_error_metadata.to_exception(e)
    259         else:
    260           raise

NotImplementedError: in user code:

    &lt;ipython-input-16-3d72fc8870c2&gt;:15 augmentation  *
        img = np.array(img)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:848 __array__  **
        " a NumPy call, which is not supported".format(self.name))

    NotImplementedError: Cannot convert a symbolic Tensor (args_0:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='bjarrell15' date='2020-10-16T13:41:59Z'>
		Was able to reproduce the issue with TF v2.2, TF v2.3 and TF-nightly. Please find the gist of it &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/99e132a15671c712f6e7592f7a608cb7/44059-tf-nightly.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='bjarrell15' date='2020-10-22T16:40:24Z'>
		&lt;denchmark-link:https://github.com/bjarrell15&gt;@bjarrell15&lt;/denchmark-link&gt;
 What version of tensorflow were you using before?
		</comment>
		<comment id='3' author='bjarrell15' date='2020-10-23T17:48:29Z'>
		&lt;denchmark-link:https://github.com/gowthamkpr&gt;@gowthamkpr&lt;/denchmark-link&gt;
 I believe that it was 2.3.0 previously as well.
		</comment>
		<comment id='4' author='bjarrell15' date='2020-11-05T21:29:43Z'>
		The functions passed to APIs like Dataset.map operate in graph mode (it's as if you decorated augment with @tf.function). In graph mode, you can typically only convert NumPy arrays to Tenors, not vice-versa.
There are a couple of ways to make this work:

if you can write augmentation entirely using TF ops, that would be a solution; tf.image and tfa.image (TF Addons) can probably be of help there, and I believe it has all the ops for standard augmentations
another option could be to use tf.py_function, and you can use NumPy inside that; it's not as fast as 1, and has certain limitation, so I only recommend it as last resort

		</comment>
		<comment id='5' author='bjarrell15' date='2020-11-05T21:36:00Z'>
		Side note - Dataset.map uses autograph, so you can write the conditionals like so:
&lt;denchmark-code&gt;if tilt &gt; 0:
  img = tf.image_flip_left_right(img)
  msk = tf.image_flip_left_right(msk)
else:
  img = tf.image_flip_up_down(img)
  msk = tf.image_flip_up_down(msk)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='bjarrell15' date='2020-11-09T14:22:35Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
 Thank you for your suggestions! I have used tf.image and tfa.image augmentations in the past, but found them to be limiting. I have not used tf.py_function, so I will give that a look.
The fundamental issue is that this did work, but now there is (potentially) some sort of TF-side bug which is preventing the operation from working.
		</comment>
		<comment id='7' author='bjarrell15' date='2020-11-13T07:34:06Z'>
		&lt;denchmark-link:https://github.com/bjarrell15&gt;@bjarrell15&lt;/denchmark-link&gt;
,
Can you please elaborate the comment,

The fundamental issue is that this did work, but now there is (potentially) some sort of TF-side bug which is preventing the operation from working.

Do you mean to say that it is working, or is it resulting in any other error? Thanks!
		</comment>
		<comment id='8' author='bjarrell15' date='2020-11-13T14:56:57Z'>
		&lt;denchmark-link:https://github.com/rmothukuru&gt;@rmothukuru&lt;/denchmark-link&gt;
 When I said "this did", I was referring to the original augmentation function.
To reiterate, here's what happened:
1.) On September 8th of this year, I used this augmentation without issue to train a model.
2.) On October 15th (the day I posted this issue), the same augmentation function does not work. Nothing has been changed about it. Instead, it passes me this error: NotImplementedError: Cannot convert a symbolic Tensor (args_0:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported.
Did something happen so that now I cannot use this function? Was Numpy support dropped at some point?
		</comment>
		<comment id='9' author='bjarrell15' date='2020-11-13T15:12:15Z'>
		The most likely cause for this is that previously, either the augmentation function was receiving numpy arrays somehow, or it was executed eagerly.
The latter is impossible if the function is called from a Dataset.map.
The former is possible, but usually leads to extremely slow performance (it would recompile the augment function every time it was called), which is why it's so surprising to hear it worked before.
Mixing symbolic tensors with numpy APIs was definitely never supported, hence the suspicion that somehow a previous inefficiency got suddenly exposed.
It would be super helpful to reproduce the previous behavior. I wanted to try running the colab code with TF 2.2 or TF 2.1 to look at the pre-September8 behavior, but haven't had the chance to do it. If someone could do that, it would be very useful.
		</comment>
		<comment id='10' author='bjarrell15' date='2020-11-16T15:04:04Z'>
		Hey &lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
, were you able to look at that? If not that's fine, just wanted to check in.
I guess it would behoove me to give up on this function and try something else. It's just a shame that there isn't an easier way to rotate/transform an image tensor otf, similar to flip_left_right() and flip_up_down(). That's why I'm upset that this doesn't work: the skimage package makes this super easy. The fact that it used to work pretty effectively makes it sting all the more.
		</comment>
		<comment id='11' author='bjarrell15' date='2020-11-16T22:36:41Z'>
		Yes, I ran the colab with TF 2.0, 2.1, 2.2 and 2.3. They all raise errors, though with slightly different messages. Are you sure last time you tried it was this exact same code?
		</comment>
		<comment id='12' author='bjarrell15' date='2020-11-17T17:00:04Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
 Yes, 100% certain. I must have exposed something that was eventually patched. I don't know.
		</comment>
		<comment id='13' author='bjarrell15' date='2020-11-17T18:27:50Z'>
		Okay, so I've modified the function to remove any reference to numpy and it's now running successfully. I'm now using tensorflow_addons, but I strongly dislike the fact that it can't fill in empty values with reflected values when I rotate it. Nonetheless, it's functional:
&lt;denchmark-code&gt;!pip install -U tensorflow-addons
import tensorflow_addons as tfa

def augmentation(img, msk):

  # tf.math package for graph ops.
  import tensorflow.math as Math

  # Create some random floats, which will be used in augmentation steps.
  tilt = tf.random.uniform(shape = [], minval = -30, maxval = 30, dtype = tf.float32)
  dx = tf.random.uniform(shape = [], minval = -5, maxval = 5, dtype = tf.float32)
  dy = tf.random.uniform(shape = [], minval = -5, maxval = 5, dtype = tf.float32)

  # Use TensforFlow-style if conditionals, used to flip image and mask.
  img = tf.cond(tilt &gt; 0, lambda: tf.image.flip_left_right(img), lambda: tf.image.flip_up_down(img))
  msk = tf.cond(tilt &gt; 0, lambda: tf.image.flip_left_right(msk), lambda: tf.image.flip_up_down(msk))

  # Rotate the image and mask.

  # Convert "tilt" to radians.
  toRads = Math.multiply(Math.divide(tilt,180),tf.constant(math.pi))

  img = tfa.image.rotate(img, toRads)
  msk = tfa.image.rotate(msk, toRads)
 
  # Affine transformation
  img = tfa.image.translate(img, [dx,dy], 'BILINEAR')
  msk = tfa.image.translate(msk, [dx,dy], 'BILINEAR')

  # Put back into a tuple.
  finalTuple = (img, msk)

  return finalTuple
&lt;/denchmark-code&gt;

		</comment>
		<comment id='14' author='bjarrell15' date='2020-11-17T18:39:09Z'>
		Thanks! Can you confirm whether you had TensorFlow stable installed both on Sep 8 and Oct 15? I tried to reproduce it with the &lt;denchmark-link:https://pypi.org/project/tf-nightly/2.4.0.dev20200902/&gt;Sep 2 build of tf-nightly&lt;/denchmark-link&gt;
 just to be sure, and it still raises error. So far, we could not seem to reproduce the circumstances in which the skimage version worked.
The  version that you drafted is something that is supported better for sure. For the rotate fill values I definitely recommend filing a &lt;denchmark-link:https://github.com/tensorflow/addons/issues&gt;bug&lt;/denchmark-link&gt;
, I believe authors or contributors would be interested in improving the feature parity.
		</comment>
		<comment id='15' author='bjarrell15' date='2020-11-17T19:54:00Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
 The way I had tensorflow brought into my script was by using the colab magic cmd %tensorflow-version 2.x. On both days that brought in version 2.3.0. Does that answer your question?
I will definitely heed your advice and create a bug for that. I'll spend some time formulating my request and post it in the next week or so.
Thank you for your help!
		</comment>
		<comment id='16' author='bjarrell15' date='2020-11-18T10:58:46Z'>
		&lt;denchmark-link:https://github.com/bjarrell15&gt;@bjarrell15&lt;/denchmark-link&gt;
,
As you would be filing a separate bug next week, can you please confirm if we can close this issue. Thanks!
		</comment>
		<comment id='17' author='bjarrell15' date='2020-11-18T14:47:52Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44059&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44059&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>