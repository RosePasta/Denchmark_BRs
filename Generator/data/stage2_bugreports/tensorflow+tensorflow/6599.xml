<bug id='6599' author='Mahdizade' open_date='2017-01-02T13:28:25Z' closed_time='2017-05-28T16:27:49Z'>
	<summary>memory leak in tensorflow_gpu 0.12.1</summary>
	<description>
Hi, We use tensorflow for training our OCR system models. I simply train models in tensorflow 0.9 and former versions. after some upgrade in cuda and tensorflow, I see large memory leak in our server with 32GB RAM.
&lt;denchmark-h:h3&gt;I have tried all of suggestions in How to debug a memory leak in TensorFlow and some other github issues and stackoverflow posts. No of them worked.&lt;/denchmark-h&gt;

train code:
&lt;denchmark-code&gt;#!/bin/env python
import tensorflow as tf

import Config
import Utilities
from Dataset import Dataset
from Model import Model

dataset = Dataset()

sess_config = tf.ConfigProto()
sess_config.gpu_options.allow_growth = True
sess = tf.Session(config=sess_config)
images, labels = dataset.train_images_labels()
model = Model(images, labels, training=True)
tf.train.start_queue_runners(sess=sess)


def main(global_step=0):
    if global_step == 0:
        init_op = tf.global_variables_initializer()
        sess.run(init_op)
    else:
        checkpoint_path = Utilities.get_checkpoint_path(global_step)
        model.saver.restore(sess, checkpoint_path)
        Utilities.log_checkpoint_load(checkpoint_path)

    loss = 0
    train_iterations = global_step
    while train_iterations &lt; Config.train_max_iterations:
        # train
        l = model.train(sess)
        loss += l
        train_iterations += 1

        # show train loss
        if train_iterations % Config.display_intervals == 0:
            loss /= float(Config.display_intervals)
            Utilities.log_train_loss(train_iterations, loss)
            loss = 0

        # save checkpoint
        if train_iterations % Config.checkpoint_intervals == 0:
            checkpoint_path = model.saver.save(sess, Utilities.get_checkpoint_path(train_iterations))
            Utilities.log_checkpoint_save(checkpoint_path)


if __name__ == '__main__':
    main()
&lt;/denchmark-code&gt;

images, labels are queues from tf.train.shuffle_batch function. I used Graph.finalize() and I am sure no new operation added to graph in training because the size of stored models' files are equal. I guess the reason of leak is tensorflow queues.
&lt;denchmark-h:h3&gt;Environment info&lt;/denchmark-h&gt;

Ubuntu Server 14.04.5 LTS
Installed version of CUDA and cuDNN:
CUDA V8.0.44, cuDNN V5.1.5.
installed tensorflow from PyPI.

sudo pip install --upgrade tensorflow_gpu.
tensorflow 0.12.1.

	</description>
	<comments>
		<comment id='1' author='Mahdizade' date='2017-01-02T13:58:33Z'>
		It can be useful to do a memory profile using google &lt;denchmark-link:http://goog-perftools.sourceforge.net/&gt;performance tools&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='Mahdizade' date='2017-01-02T14:51:59Z'>
		&lt;denchmark-link:https://github.com/yaroslavvb&gt;@yaroslavvb&lt;/denchmark-link&gt;
 I use google-pprof and heap profile save in /tmp folder. it takes too long time to train so the profile only contain near 150 itrations. the files uploaded &lt;denchmark-link:https://www.dropbox.com/s/2r2pqs4l0bjar9h/tmp.zip?dl=1&gt;here&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='Mahdizade' date='2017-01-02T15:25:47Z'>
		if you combine it with pprof, you can get a graph and track down which function is responsible for allocating extra memory
		</comment>
		<comment id='4' author='Mahdizade' date='2017-01-02T16:01:46Z'>
		It doesn't show any function, this is the output of top30 command in pprof:
&lt;denchmark-code&gt;(pprof) top30
Total: 658.2 MB
   194.3  29.5%  29.5%    194.3  29.5% 00007f8a8afae90e
   172.2  26.2%  55.7%    172.2  26.2% 00007f8a8aedbae3
    84.1  12.8%  68.5%     84.1  12.8% Eigen::internal::TensorExecutor::run::{lambda#1}@266d900
    50.2   7.6%  76.1%     50.2   7.6% 00007f8a8aec2ffe
    25.9   3.9%  80.0%     25.9   3.9% 00007f8a8af5377d
    23.2   3.5%  83.5%     23.4   3.6% PyDict_Clear
    14.2   2.2%  85.7%     14.2   2.2% 00007f8a8aec2778
    10.5   1.6%  87.3%     10.5   1.6% 00007f8a8adef4c8
     8.1   1.2%  88.5%      8.1   1.2% 00007f8a8af826d2
     6.8   1.0%  89.6%      6.8   1.0% 00007f8a8af542a3
     6.8   1.0%  90.6%      6.8   1.0% 00007f8a8af52fed
     5.7   0.9%  91.4%    596.5  90.6% extra_lbits
     4.2   0.6%  92.1%      4.2   0.6% 00007f8a8afad614
     3.7   0.6%  92.7%      3.7   0.6% 00007f8ace0b7249
     3.1   0.5%  93.1%      6.1   0.9% PyMem_Malloc
     3.1   0.5%  93.6%      4.1   0.6% ecp_nistz256_precomputed
     2.8   0.4%  94.0%      9.1   1.4% PyEval_EvalFrameEx
     2.8   0.4%  94.4%      8.6   1.3% std::_Maybe_get_result_type@26ca280
     2.4   0.4%  94.8%      2.5   0.4% PyBuffer_FillInfo
     2.1   0.3%  95.1%      2.1   0.3% __GI___strdup
     2.1   0.3%  95.4%      2.1   0.3% 00007f8a8adee7a7
     2.0   0.3%  95.7%      2.3   0.4% __FRAME_END__
     1.6   0.2%  96.0%      1.6   0.2% 00007f8a8af53762
     1.3   0.2%  96.2%      1.3   0.2% std::_Bind@26bb1c0
     1.2   0.2%  96.4%      1.2   0.2% 00007f8a8af433a1
     1.2   0.2%  96.6%      1.3   0.2% std::_Bind@26c08c0
     1.1   0.2%  96.7%      1.1   0.2% PyObject_RichCompareBool
     1.0   0.2%  96.9%      1.8   0.3% std::_Weak_result_type@26a88a0
     1.0   0.2%  97.0%      1.0   0.2% 00007f8a8aff9f73
     1.0   0.2%  97.2%      1.0   0.2% 00007f8a8b01a2ea
&lt;/denchmark-code&gt;

graph file uploaded &lt;denchmark-link:https://www.dropbox.com/s/mjo4kc17micbfw6/pprof3105.0.ps?dl=1&gt;here&lt;/denchmark-link&gt;
, unfortunately I cant find a function is responsible for allocating extra memory
		</comment>
		<comment id='5' author='Mahdizade' date='2017-01-02T16:47:20Z'>
		That usage doesn't seem very high, why do you think there's a memory leak in queue? Memory usage can increase between versions without having a leak
		</comment>
		<comment id='6' author='Mahdizade' date='2017-01-02T16:54:36Z'>
		because I see 32 GB memory usage after 700000 iterations. I think the cause is data, I use tfrecord and tensorflow queue for data handling. I am sure that I hadn't leak in version 0.9.
		</comment>
		<comment id='7' author='Mahdizade' date='2017-01-04T01:21:49Z'>
		&lt;denchmark-link:https://github.com/zffchen78&gt;@zffchen78&lt;/denchmark-link&gt;
 do you have any idea why google perf tools shows "29.5% 00007f8a8afae90e" instead of symbol name in the memory dump? Does it mean this memory was allocated outside of tensorflow?
		</comment>
		<comment id='8' author='Mahdizade' date='2017-01-27T21:09:23Z'>
		&lt;denchmark-link:https://github.com/yaroslavvb&gt;@yaroslavvb&lt;/denchmark-link&gt;
 the hex address shows up when the symbol lookup failed. Address to symbol name mappings are always very brittle. You can try to fiddle with build options to see if it helps. You need the frame pointers for instance, IIRC. If you gdb attach to it, then info symbol, then you might be able to reverse map it.
		</comment>
		<comment id='9' author='Mahdizade' date='2017-01-30T02:24:13Z'>
		I got exactly the same problem as you, I also use TFRecord and Queue to process data, and my main memory keeps growing until the system goes down...
By the way, I use Tensorflow 0.12.1, CUDA 8.0.44, CUDNN 5.1, CentOS 7
		</comment>
		<comment id='10' author='Mahdizade' date='2017-01-30T18:01:39Z'>
		Main memory as in CPU? Could you build with tcmalloc, then get a heap profile?
		</comment>
		<comment id='11' author='Mahdizade' date='2017-01-30T18:56:26Z'>
		To enable tcmalloc
&lt;denchmark-code&gt;sudo apt-get install google-perftools
export LD_PRELOAD="/usr/lib/libtcmalloc_and_profiler.so.4"

&lt;/denchmark-code&gt;

		</comment>
		<comment id='12' author='Mahdizade' date='2017-02-03T23:15:35Z'>
		Can this problem be solved in the next release?
		</comment>
		<comment id='13' author='Mahdizade' date='2017-02-03T23:26:14Z'>
		@carltonwang Could you give a reproducible example and profile as a picture/graph?
cc &lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 in case he has ideas
		</comment>
		<comment id='14' author='Mahdizade' date='2017-02-04T00:48:32Z'>
		Are you using dynamic or static rnn?
		</comment>
		<comment id='15' author='Mahdizade' date='2017-02-04T02:02:15Z'>
		I use static rnn.
		</comment>
		<comment id='16' author='Mahdizade' date='2017-02-05T07:38:39Z'>
		This is the simplified version of my code and data (in TFRecord), please run it on GPU...
My system is: CentOS 7, GTX 1070, CUDA 8.0.44, CUDNN 5.1, NVIDIA Driver 375.26
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/752806/test.zip&gt;test.zip&lt;/denchmark-link&gt;

		</comment>
		<comment id='17' author='Mahdizade' date='2017-02-07T07:26:09Z'>
		Any idea about this problem?
		</comment>
		<comment id='18' author='Mahdizade' date='2017-02-07T16:49:03Z'>
		@carltonwang can you isolate your problem a bit more? IE, does it happen with latest version? Does it happen with tcmalloc? Can you run it with memory profiler and figure out who allocates all the extra memory?
		</comment>
		<comment id='19' author='Mahdizade' date='2017-02-08T19:42:57Z'>
		It happens on the latest version, with or without tcmalloc...
		</comment>
		<comment id='20' author='Mahdizade' date='2017-02-09T09:10:11Z'>
		I implemented an encoding algorithm in my code. Today I just encapsulated this encoding method as an RNN cell, and then the memory leak disappeared!!!
		</comment>
		<comment id='21' author='Mahdizade' date='2017-02-10T15:33:46Z'>
		Closing since the problem is gone.  Happy to reopen if there is still an underlying bug to be fixed in TensorFlow, but it sounds like everything so far is inconclusive.
		</comment>
		<comment id='22' author='Mahdizade' date='2017-03-16T13:54:50Z'>
		&lt;denchmark-link:https://github.com/Mahdizade&gt;@Mahdizade&lt;/denchmark-link&gt;
 I met similar problem.
My program is similar to yours and after several epoch, 32G memory exhausted. I debugged the program and found each time the saver.save() function is run, the python process got a little bigger. I recommend you try this and see if the same thing happens.
		</comment>
		<comment id='23' author='Mahdizade' date='2017-03-22T11:05:13Z'>
		I have the same issue here. It goes overflowing after couple of iterations on version 1.0.1
		</comment>
		<comment id='24' author='Mahdizade' date='2017-03-23T06:18:35Z'>
		My issue is already exists in TF 1.0. &lt;denchmark-link:https://github.com/benwu232&gt;@benwu232&lt;/denchmark-link&gt;
 Thanks for your comment. I will test your solution but I see the leak on my validation code. till now I cant find the problem. in my validation code leak happened only after the first load of graph parameters (checkpoint), after validate one checkpoint and load another checkpoint, I don't see any leak in htop. I will investigate more on this problem. I don't thing the saver is the cause, I see the steady leak on training, I save the checkpoint every 5000 iteration but leak exists in this intervals.
		</comment>
		<comment id='25' author='Mahdizade' date='2017-03-24T18:05:06Z'>
		Look at this code:
result = tf.contrib.layers.bias_add(inputs=tf.matmul(a=left, b=right), activation_fn=tf.nn.softmax)
If I use "softmax" as the activation function, there will be a memory leak, but if I change the activation function to "relu" or "softplus" or "sigmoid", the memory leak will disappear.
By the way, this "softmax" is not the one used at the last layer for classification.
Platform: CentOS 7, x86_64
TensorFlow version: 1.0_gpu
		</comment>
		<comment id='26' author='Mahdizade' date='2017-03-28T18:18:49Z'>
		Could you provide a complete test case that exhibits this behavior so we can reproduce this?
		</comment>
		<comment id='27' author='Mahdizade' date='2017-03-29T05:00:13Z'>
		After I update from TF 1.0.0 to TF 1.0.1, this problem disappear...
		</comment>
		<comment id='28' author='Mahdizade' date='2017-05-28T16:27:49Z'>
		looks like this was resolved with 1.0 release. Closing the issue.
		</comment>
	</comments>
</bug>