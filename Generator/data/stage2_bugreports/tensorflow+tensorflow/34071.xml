<bug id='34071' author='VoVAllen' open_date='2019-11-07T13:59:33Z' closed_time='2020-02-28T18:56:47Z'>
	<summary>[Bug] Wrong device placement for tf.constant with int32</summary>
	<description>
Colab example: &lt;denchmark-link:https://colab.research.google.com/drive/1iCyTZhKzco4CjxY17g37jKG9GS35-NGb&gt;https://colab.research.google.com/drive/1iCyTZhKzco4CjxY17g37jKG9GS35-NGb&lt;/denchmark-link&gt;

Remember to use GPU instance
When dtype is int32, tf.constant didn't place tensor on gpu but cpu.
with tf.device("/gpu:0"):
  a = tf.constant([0,1], dtype=tf.float32)
print(a.device)
# '/job:localhost/replica:0/task:0/device:GPU:0'

with tf.device("/gpu:0"):
  b = tf.constant([0,1], dtype=tf.int32)
print(b.device)
# '/job:localhost/replica:0/task:0/device:CPU:0'
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):
TensorFlow version (use command below): 2.0.0
Python version:
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

	</description>
	<comments>
		<comment id='1' author='VoVAllen' date='2019-11-08T06:59:02Z'>
		I have tried on colab with TF version 2.0 and was able to reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/3dffedfdab275f96a166afa5963db220/untitled340.ipynb&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='2' author='VoVAllen' date='2019-11-08T09:23:42Z'>
		I think this is intentional, see for example this comment: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/eager/pywrap_tensor.cc#L352-L379&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/eager/pywrap_tensor.cc#L352-L379&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='VoVAllen' date='2019-11-08T10:01:56Z'>
		I didn't find any documentation about this and it makes no sense to me (although it's intentional). You can add op like constant_cpu to force allocating tensor on cpu, but should not break the consistence without any documentation.
		</comment>
		<comment id='4' author='VoVAllen' date='2019-11-08T10:04:40Z'>
		At least it should have some logs telling user it's allocated on cpu
		</comment>
		<comment id='5' author='VoVAllen' date='2019-12-26T22:26:58Z'>
		&lt;denchmark-link:https://github.com/VoVAllen&gt;@VoVAllen&lt;/denchmark-link&gt;
 I'd be happy to review PRs adding documentation / logging that would have helped you figure this out more easily.
CC &lt;denchmark-link:https://github.com/jaingaurav&gt;@jaingaurav&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='VoVAllen' date='2020-02-26T23:31:16Z'>
		I believe this is intentional as can be seen in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/28051&gt;#28051&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='7' author='VoVAllen' date='2020-02-27T07:00:26Z'>
		&lt;denchmark-link:https://github.com/samikama&gt;@samikama&lt;/denchmark-link&gt;
 I would say although it's intentional it's also necessary to mention it somewhere in docs, since this is super counter intuitive. I'm not sure that whether all the output of int32 will on the cpu, so I'm afraid I may not be able to help on the documentation.
		</comment>
		<comment id='8' author='VoVAllen' date='2020-02-28T16:18:39Z'>
		A fix went in for this and other problems with  in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/97cdd4d16a81a349696f10451b7d564bfa99664f&gt;97cdd4d&lt;/denchmark-link&gt;
, which will make it create CPU tensors irrespective of device settings. This fixes other bugs, including one often requested one for "always placing string tensors on the CPU." The documentation was updated to be explicit about this.
There were a few solution alternatives, but the "always CPU tensor" was the most consistent, simple, and the only one which did not introduce performance regressions. One of the ideas behind the solution is that the constant you create with, say,
x = tf.constant(very_large_numpy_array)
Is both potentially large, and already on the CPU, so it makes sense for tf.constant to just keep things there. With the default device policy (see tf.config.experimental.set_device_policy), the CPU tensor will be automatically copied to the GPU whenever needed (only once, and the copy will be cached so there's no performance drawbacks). If you want to explicitly place the tensor on the GPU (essentially controlling when the  copy happens), you can use tf.identity under the proper tf.device context.
		</comment>
		<comment id='9' author='VoVAllen' date='2020-02-28T18:17:10Z'>
		That makes more sense if the gpu copy is cached. I discovered this problem when debugging with tf code's performance. Let's say if I had an op gather_row(index, tensor) and index is tf.int32. And it executes on gpu and requires both index and tensor on gpu. So the index here would be copied only once for multiple execution, right?
		</comment>
		<comment id='10' author='VoVAllen' date='2020-02-28T18:19:01Z'>
		So all tensors created with tf.constant are now on host memory irrelevant to the device context now? No matter what its type is?
		</comment>
		<comment id='11' author='VoVAllen' date='2020-02-28T18:40:38Z'>
		&lt;denchmark-link:https://github.com/VoVAllen&gt;@VoVAllen&lt;/denchmark-link&gt;
: You are absolutely correct about the caching and as part of this placement policy we have added the functionality such that the GPU copy is indeed cached.
		</comment>
		<comment id='12' author='VoVAllen' date='2020-02-28T18:56:47Z'>
		Closing the issue since I believe this issue is now resolved. However, please feel free to continue discussion or ask questions.
		</comment>
	</comments>
</bug>