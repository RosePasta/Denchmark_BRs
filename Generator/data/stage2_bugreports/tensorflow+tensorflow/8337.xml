<bug id='8337' author='zxvix' open_date='2017-03-13T03:22:54Z' closed_time='2017-03-31T07:09:24Z'>
	<summary>tfdbg error: Causality violated in timing relations of debug dumps</summary>
	<description>
&lt;denchmark-h:h3&gt;What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?&lt;/denchmark-h&gt;

Nothing found
&lt;denchmark-h:h3&gt;Environment info&lt;/denchmark-h&gt;

Operating System:
Ubuntu 14.04
Installed version of CUDA and cuDNN:
CUDA 8.0, cuDNN 5.1
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
&lt;denchmark-code&gt;-rw-r--r-- 1 root root   558720  9月 15 07:02 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16  9月 15 07:05 /usr/local/cuda/lib64/libcudart.so -&gt; libcudart.so.8.0
lrwxrwxrwx 1 root root       19  9月 15 07:05 /usr/local/cuda/lib64/libcudart.so.8.0 -&gt; libcudart.so.8.0.44
-rw-r--r-- 1 root root   415432  9月 15 07:02 /usr/local/cuda/lib64/libcudart.so.8.0.44
-rw-r--r-- 1 root root   775162  9月 15 07:02 /usr/local/cuda/lib64/libcudart_static.a
-rwxr-xr-x 1 root root 79337624 11月 16 22:50 /usr/local/cuda/lib64/libcudnn.so
-rwxr-xr-x 1 root root 79337624 11月 16 22:50 /usr/local/cuda/lib64/libcudnn.so.5
-rwxr-xr-x 1 root root 79337624 11月 16 22:50 /usr/local/cuda/lib64/libcudnn.so.5.1.5
-rw-r--r-- 1 root root 69756172 11月 16 22:50 /usr/local/cuda/lib64/libcudnn_static.a
&lt;/denchmark-code&gt;

If installed from source, provide

The commit hash (git rev-parse HEAD)
12a9872
The output of bazel version

&lt;denchmark-code&gt;Build label: 0.4.4
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Feb 1 18:54:21 2017 (1485975261)
Build timestamp: 1485975261
Build timestamp as int: 1485975261
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)&lt;/denchmark-h&gt;

I have a model that basically consists of several bidirectional rnns, the longest of which has hundreds of timesteps.
&lt;denchmark-h:h3&gt;What other attempted solutions have you tried?&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Logs or other output that would be helpful&lt;/denchmark-h&gt;

When I try to launch the first Session.run() call in tfdbg using command r, I got the following error:
&lt;denchmark-code&gt;2017-03-13 10:54:06.173974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties:
name: GeForce GTX 1080
major: 6 minor: 1 memoryClockRate (GHz) 1.7335
pciBusID 0000:81:00.0
Total memory: 7.92GiB
Free memory: 7.81GiB
2017-03-13 10:54:06.174082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0
2017-03-13 10:54:06.174106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y
2017-03-13 10:54:06.174138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 1080, pci bus id: 0000:81:00.0)
Created model with fresh parameters.
Traceback (most recent call last):
  File "train.py", line 143, in &lt;module&gt;
    trainer.train()
  File "train.py", line 76, in train
    step_loss, summary = self.m.train_step(self.sess, batch_docs, batch_queries, batch_answers, batch_target)
  File "/home/zhangx/nn_tool_x33/MR_base/model.py", line 186, in train_step
    feed_dict=feed_dict, options=run_options, run_metadata=run_metadata)
  File "/home/zhangx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/debug/wrappers/framework.py", line 470, in run
    run_end_resp = self.on_run_end(run_end_req)
  File "/home/zhangx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py", line 267, in on_run_end
    self._dump_root, partition_graphs=partition_graphs)
  File "/home/zhangx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/debug/lib/debug_data.py", line 502, in __init__
    self._load_partition_graphs(partition_graphs, validate)
  File "/home/zhangx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/debug/lib/debug_data.py", line 760, in _load_partition_graphs
    self._validate_dump_with_graphs()
  File "/home/zhangx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/debug/lib/debug_data.py", line 927, in _validate_dump_with_graphs
    (node, datum.timestamp, repr(pending_inputs[node])))
ValueError: Causality violated in timing relations of debug dumps: optimizer/gradients/inference/encode/docs/encode/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Enter_1_grad/b_acc_2 (1489373660302841): these input(s) are not satisfied: [('optimizer/gradients/inference/encode/docs/encode/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Enter_1_grad/b_acc_1', 0), ('optimizer/gradients/inference/encode/docs/encode/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Enter_1_grad/NextIteration', 0)]
&lt;/denchmark-code&gt;

Is this a model related error or possibly a tfdbg bug?
	</description>
	<comments>
		<comment id='1' author='zxvix' date='2017-03-13T17:36:01Z'>
		&lt;denchmark-link:https://github.com/zxvix&gt;@zxvix&lt;/denchmark-link&gt;
 From your error message, it appears that you are using . Can you try setting its  parameter to 1 and see if the error still happens?
There may be a bug in how tfdbg handles while_loops with parallel_iterations &gt; 1.
		</comment>
		<comment id='2' author='zxvix' date='2017-03-13T19:41:38Z'>
		I think it might be a GPU thing.
The example below errors if run as python tf_8337_minimal.py but is fine is run as CUDA_VISIBLE_DEVICES=-1 python tf_8337_minimal.py.
from __future__ import division
import logging
import numpy as np
import tensorflow as tf
from tensorflow.contrib.rnn import BasicLSTMCell, MultiRNNCell
from tensorflow.python import debug as tfdbg


def output_layer(inputs, out_size, keep_prob):
  batch_size, time_size, in_size = inputs.get_shape().as_list()
  inputs = tf.reshape(inputs, [batch_size * time_size, in_size])
  weights = tf.get_variable('weights', [in_size, out_size], tf.float32)
  biases = tf.get_variable('biases', [out_size], tf.float32, tf.zeros_initializer())
  outputs = tf.matmul(tf.nn.dropout(inputs, keep_prob), weights) + biases
  return tf.reshape(outputs, [batch_size, time_size, out_size])

tf._log = tf.log
def log(x, name=None):
  return tf._log(x + 1e-10, name)
tf.log = log


class Network(object):
  def __init__(self):
    self.sess = tf.Session()
    self.sess = tfdbg.LocalCLIDebugWrapperSession(self.sess)
    self.sess.add_tensor_filter('has_inf_or_nan', tfdbg.has_inf_or_nan)
    self.x = tf.placeholder(tf.float32, [1, 6, 6])
    self.p = tf.placeholder(tf.float32, [1, 6, 3])
    self.keep_prob = tf.placeholder_with_default(tf.ones([], tf.float32), [])
    sequence_length = self._get_sequence_length()
    multicell = MultiRNNCell([BasicLSTMCell(128) for _ in xrange(2)])
    output, _state = tf.nn.dynamic_rnn(multicell, self.x,
                                       sequence_length=sequence_length,
                                       dtype=tf.float32,
                                       parallel_iterations=1)
    output = output_layer(output, 3, self.keep_prob)
    self.p_hat = tf.nn.softmax(output)
    mask = tf.sequence_mask(sequence_length, 6, tf.float32)
    num_events = tf.to_float(tf.reduce_sum(sequence_length))
    self.loss = tf.reduce_sum(-tf.log(tf.reduce_sum(self.p_hat * self.p, axis=2)) * mask) / num_events
    optimiser = tf.train.GradientDescentOptimizer(learning_rate=0)
    gradients = optimiser.compute_gradients(self.loss)
    self.train_op = optimiser.apply_gradients(gradients)

  def train(self, x, p):
    feed_dict = {self.x: x, self.p: p, self.keep_prob: 0.5}
    fetches = [self.train_op, self.loss]
    return self.sess.run(fetches, feed_dict)[1:]

  def _get_sequence_length(self):
    used = tf.sign(tf.reduce_max(tf.abs(self.x), reduction_indices=2))
    self._sequence_length = tf.cast(tf.reduce_sum(used, reduction_indices=1), tf.int32)
    return self._sequence_length


x = np.hstack((np.random.randn(1, 2, 6), np.zeros((1, 4, 6))))
p = np.zeros((1, 6, 3))
p[0, 0, 2] = 1
p[0, 1, 1] = 1

network = Network()
network.sess.run(tf.global_variables_initializer())
results = network.train(x, p)
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "tf_8337_minimal.py", line 64, in &lt;module&gt;
    results = network.train(x, p)
  File "tf_8337_minimal.py", line 49, in train
    return self.sess.run(fetches, feed_dict)[1:]
  File "/home/arun/tensorflows/master/local/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/framework.py", line 470, in run
    run_end_resp = self.on_run_end(run_end_req)
  File "/home/arun/tensorflows/master/local/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py", line 267, in on_run_end
    self._dump_root, partition_graphs=partition_graphs)
  File "/home/arun/tensorflows/master/local/lib/python2.7/site-packages/tensorflow/python/debug/lib/debug_data.py", line 502, in __init__
    self._load_partition_graphs(partition_graphs, validate)
  File "/home/arun/tensorflows/master/local/lib/python2.7/site-packages/tensorflow/python/debug/lib/debug_data.py", line 760, in _load_partition_graphs
    self._validate_dump_with_graphs()
  File "/home/arun/tensorflows/master/local/lib/python2.7/site-packages/tensorflow/python/debug/lib/debug_data.py", line 927, in _validate_dump_with_graphs
    (node, datum.timestamp, repr(pending_inputs[node])))
ValueError: Causality violated in timing relations of debug dumps: gradients/rnn/while/multi_rnn_cell/cell_0/basic_lstm_cell/basic_lstm_cell_1/MatMul/Enter_grad/b_acc_2 (1489433694505012): these input(s) are not satisfied: [(u'gradients/rnn/while/multi_rnn_cell/cell_0/basic_lstm_cell/basic_lstm_cell_1/MatMul/Enter_grad/b_acc_1', 0), (u'gradients/rnn/while/multi_rnn_cell/cell_0/basic_lstm_cell/basic_lstm_cell_1/MatMul/Enter_grad/NextIteration', 0)]
&lt;/denchmark-code&gt;

The final error tends to change in a non-deterministic fashion. I have also got:
&lt;denchmark-code&gt;ValueError: Causality violated in timing relations of debug dumps: gradients/rnn/while/Switch_5_grad/b_switch (1489433796485511): these input(s) are not satisfied: [(u'gradients/rnn/while/Exit_5_grad/b_exit', 0), (u'gradients/rnn/while/Switch_5_grad_1/NextIteration', 0)]
&lt;/denchmark-code&gt;

Ubuntu 16.04.2
Source: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/12a98726e769e988f6368a029ec2f5b0ac3ccbd4&gt;12a9872&lt;/denchmark-link&gt;

Bazel 0.4.4
CUDA 8.0
CUDNN 5.1.5
python 2.7.12
2 GPUs: Titan X (pascal)
		</comment>
		<comment id='3' author='zxvix' date='2017-03-13T20:35:35Z'>
		Thanks for the info. Ah - I see you have more than one GPUs. As a diagnostic step, does this error occur if you run the code on a single GPU?
		</comment>
		<comment id='4' author='zxvix' date='2017-03-13T23:26:39Z'>
		Yeah, same thing:
&lt;denchmark-code&gt;(master) arun@abacus:~/source/deep_experiments/arun_experiments$ CUDA_VISIBLE_DEVICES=1 python tf_8337_minimal.py 
2017-03-13 23:22:22.446582: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:81:00.0
Total memory: 11.90GiB
Free memory: 10.54GiB
2017-03-13 23:22:22.446634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-03-13 23:22:22.446644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-03-13 23:22:22.446667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: TITAN X (Pascal), pci bus id: 0000:81:00.0)
Traceback (most recent call last):
  File "tf_8337_minimal.py", line 64, in &lt;module&gt;
    results = network.train(x, p)
  File "tf_8337_minimal.py", line 49, in train
    return self.sess.run(fetches, feed_dict)[1:]
  File "/home/arun/tensorflows/master/local/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/framework.py", line 470, in run
    run_end_resp = self.on_run_end(run_end_req)
  File "/home/arun/tensorflows/master/local/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py", line 267, in on_run_end
    self._dump_root, partition_graphs=partition_graphs)
  File "/home/arun/tensorflows/master/local/lib/python2.7/site-packages/tensorflow/python/debug/lib/debug_data.py", line 502, in __init__
    self._load_partition_graphs(partition_graphs, validate)
  File "/home/arun/tensorflows/master/local/lib/python2.7/site-packages/tensorflow/python/debug/lib/debug_data.py", line 760, in _load_partition_graphs
    self._validate_dump_with_graphs()
  File "/home/arun/tensorflows/master/local/lib/python2.7/site-packages/tensorflow/python/debug/lib/debug_data.py", line 927, in _validate_dump_with_graphs
    (node, datum.timestamp, repr(pending_inputs[node])))
ValueError: Causality violated in timing relations of debug dumps: gradients/rnn/while/multi_rnn_cell/cell_1/basic_lstm_cell/basic_lstm_cell_1/BiasAdd/Enter_grad/b_acc_2 (1489447350946236): these input(s) are not satisfied: [(u'gradients/rnn/while/multi_rnn_cell/cell_1/basic_lstm_cell/basic_lstm_cell_1/BiasAdd/Enter_grad/b_acc_1', 0), (u'gradients/rnn/while/multi_rnn_cell/cell_1/basic_lstm_cell/basic_lstm_cell_1/BiasAdd/Enter_grad/NextIteration', 0)]
&lt;/denchmark-code&gt;

Thanks for looking into this.
		</comment>
		<comment id='5' author='zxvix' date='2017-03-28T01:40:23Z'>
		I'm seeing this error on CPU (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/1e33acb381430ed0004eb76b98181624af267b2c&gt;1e33acb&lt;/denchmark-link&gt;
) with , so it's not entirely a GPU thing:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "train.py", line 123, in &lt;module&gt;
    feed_dict={x: batch_xs, y_: batch_ys})
  File "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/debug/wrappers/framework.py", line 470, in run
    run_end_resp = self.on_run_end(run_end_req)
  File "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py", line 275, in on_run_end
    self._dump_root, partition_graphs=partition_graphs)
  File "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/debug/lib/debug_data.py", line 524, in __init__
    self._load_partition_graphs(partition_graphs, validate)
  File "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/debug/lib/debug_data.py", line 782, in _load_partition_graphs
    self._validate_dump_with_graphs()
  File "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/debug/lib/debug_data.py", line 954, in _validate_dump_with_graphs
    (node, datum.timestamp, repr(pending_inputs[node])))
ValueError: Causality violated in timing relations of debug dumps: while/mul (1490664758938833): these input(s) are not satisfied: [('while/mul/Enter', 0)]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='zxvix' date='2017-03-28T01:45:44Z'>
		&lt;denchmark-link:https://github.com/jekbradbury&gt;@jekbradbury&lt;/denchmark-link&gt;
 It will be helpful if you could provide some code for reproducing this error on our and and let us know how reproducible (% time it happens) this crash is. Thanks.
		</comment>
		<comment id='7' author='zxvix' date='2017-03-28T01:56:55Z'>
		My guess is that I've done something dumb in the optimization routine (I'm getting NaNs, which is why I tried the debugger). But here's &lt;denchmark-link:https://gist.github.com/jekbradbury/05e9f17776771f1f501919b7eff58c7f&gt;the gist&lt;/denchmark-link&gt;
 anyway -- it has happened all three times I've tried running it with the debugger; the error comes on the second use of  (the first time the optimization routine happens). On v1.0 stable, it hangs like in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/7774&gt;this issue&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='8' author='zxvix' date='2017-03-28T03:05:27Z'>
		Thanks &lt;denchmark-link:https://github.com/jekbradbury&gt;@jekbradbury&lt;/denchmark-link&gt;
 , your code reproduces the issue on my machine (CPU only).
Also, &lt;denchmark-link:https://github.com/al626&gt;@al626&lt;/denchmark-link&gt;
 's code also errors out on my GPU machine, but works fine on CPU-only.
These may be different manifestations of the same issue. We'll look into it.
		</comment>
		<comment id='9' author='zxvix' date='2017-03-31T13:56:57Z'>
		FYI, this bug should have been fixed by &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/7b410ebea68bb26b3166a357cf7c1e8946a1b35b&gt;7b410eb&lt;/denchmark-link&gt;
. I verified it works with the two reproduction code samples above. The fix will become available in the nightly artifacts on the next successful build.
		</comment>
		<comment id='10' author='zxvix' date='2017-07-26T20:16:40Z'>
		I am having the same problem, getting NaNs when using dynamic_rnn, and the td_debugger exiting with
ValueError: Causality violated in timing relations of debug dumps: gradients/rnn/while/rnn/dnc_cell/head_0/strided_slice_7_grad/StridedSliceGrad/StackPush_1 (1501099455073575): these input(s) are not satisfied: [('gradients/rnn/while/rnn/dnc_cell/head_0/strided_slice_7_grad/StridedSliceGrad/RefEnter_1', 0)] 
Using tensorflow 1.2.0 with python3.
		</comment>
	</comments>
</bug>