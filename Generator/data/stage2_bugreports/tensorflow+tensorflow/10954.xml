<bug id='10954' author='lhlmgr' open_date='2017-06-21T15:08:47Z' closed_time='2017-12-21T15:11:31Z'>
	<summary>Supervisor: SummaryWriter and Saver stop after some time</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 / anaconda3 / python 3.6
TensorFlow installed from (source or binary): binary, (pip install tensorflow-gpu)
TensorFlow version (use command below): 1.2.0
Bazel version (if compiling from source): -
CUDA/cuDNN version: 5.1
GPU model and memory: 1080 / Titan X / K80
Exact command to reproduce: -

System information: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/1091842/tf_env.txt&gt;tf_env.txt&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

Hey guys!
I get the problem on 3 different machines (all on ubuntu 16.04 and the tensorflow 1.2). All experiments were executed on a single machine with a single GPU.
To initialize a session I use a tf.Supervisor and supervisor.managed_sessions() with the default
summary_writer and saver.  It works all well for up to 30mins - 1h30mis. But after that time the
summary_writer stops to write events, and the saver also stops to save the model parameters. However, the model still runs and produces valid outputs.
I also checked the python-log for tensorflow with level DEBUG. But all I got was some information logs, until it suddenly stops (see below).
Is there a way to track the SVSummaryThread / SVTimerCheckpointThread?
Thanks in advance and keep up the good work!
Cheers
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

supervisor = tf.train.Supervisor(logdir=model_path, save_summaries_secs=60, global_step=model.global_step)
  sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True), allow_soft_placement=True)

  with supervisor.managed_session(config=sess_config) as sess, sess.as_default():
    step_time, loss = 0.0, 0.0
    epoch = 0

    dataset_size = reader.train_size
    epoch_time = time.time()
    avg_lm = 0.

    while epoch &lt; config["max_epochs"]:
      sess.run(reader.iterator.initializer)

      while True:
        try:
          step_loss, step_lm = model.step(sess)
          avg_lm += step_lm / float(dataset_size) * float(model.batch_size)
          loss += step_loss / float(dataset_size) * float(model.batch_size)
        except tf.errors.OutOfRangeError:
          break
tensorflow python log:
&lt;denchmark-code&gt;017-06-21 15:57:33,386 - tensorflow - INFO - Starting queue runners.
2017-06-21 15:57:33,393 - tensorflow - INFO - global_step/sec: 0
2017-06-21 15:57:34,539 - tensorflow - INFO - Recording summary at step 0.
2017-06-21 15:58:33,385 - tensorflow - INFO - Recording summary at step 15264.
2017-06-21 15:58:33,391 - tensorflow - INFO - global_step/sec: 254.444
2017-06-21 15:59:33,384 - tensorflow - INFO - Recording summary at step 31239.
2017-06-21 15:59:33,391 - tensorflow - INFO - global_step/sec: 266.25
2017-06-21 16:00:33,385 - tensorflow - INFO - Recording summary at step 47206.
2017-06-21 16:00:33,391 - tensorflow - INFO - global_step/sec: 266.117
2017-06-21 16:01:33,385 - tensorflow - INFO - Recording summary at step 62157.
2017-06-21 16:01:33,391 - tensorflow - INFO - global_step/sec: 249.183
2017-06-21 16:02:33,384 - tensorflow - INFO - Recording summary at step 77621.
2017-06-21 16:02:33,391 - tensorflow - INFO - global_step/sec: 257.733
2017-06-21 16:03:33,383 - tensorflow - INFO - Recording summary at step 93657.
2017-06-21 16:03:33,391 - tensorflow - INFO - global_step/sec: 267.283
2017-06-21 16:04:33,391 - tensorflow - INFO - global_step/sec: 263.883
2017-06-21 16:04:33,545 - tensorflow - INFO - Recording summary at step 109493.
2017-06-21 16:05:33,391 - tensorflow - INFO - global_step/sec: 262.483
2017-06-21 16:05:33,558 - tensorflow - INFO - Recording summary at step 125242.
2017-06-21 16:06:33,383 - tensorflow - INFO - Recording summary at step 141072.
2017-06-21 16:06:33,391 - tensorflow - INFO - global_step/sec: 263.883
2017-06-21 16:07:33,384 - tensorflow - INFO - Recording summary at step 157265.
...
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='lhlmgr' date='2017-12-20T19:09:30Z'>
		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
		</comment>
		<comment id='2' author='lhlmgr' date='2017-12-21T15:11:31Z'>
		It didn't happen wit tf-1.4 anymore. So I'll close it. Thanks!
		</comment>
	</comments>
</bug>