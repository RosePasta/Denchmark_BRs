<bug id='34007' author='durandg12' open_date='2019-11-05T11:36:55Z' closed_time='2020-06-01T04:25:17Z'>
	<summary>Comments on the "Custom model_fn with TF 2.0 symbols" section of the "Migrate your TensorFlow 1 code to TensorFlow 2" guide</summary>
	<description>
&lt;denchmark-h:h2&gt;URL(s) with the issue:&lt;/denchmark-h&gt;

Please provide a link to the documentation entry, for example:
&lt;denchmark-link:https://www.tensorflow.org/guide/migrate#custom_model_fn_with_tf_20_symbols&gt;https://www.tensorflow.org/guide/migrate#custom_model_fn_with_tf_20_symbols&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;Description of issue (what needs changing):&lt;/denchmark-h&gt;

My comments are about this piece of code:
&lt;denchmark-code&gt;def my_model_fn(features, labels, mode):
  model = make_model()

  training = (mode == tf.estimator.ModeKeys.TRAIN)
  loss_obj = tf.keras.losses.SparseCategoricalCrossentropy()
  predictions = model(features, training=training)

  # Get both the unconditional losses (the None part)
  # and the input-conditional losses (the features part).
  reg_losses = model.get_losses_for(None) + model.get_losses_for(features)
  total_loss = loss_obj(labels, predictions) + tf.math.add_n(reg_losses)

  # Upgrade to tf.keras.metrics.
  accuracy_obj = tf.keras.metrics.Accuracy(name='acc_obj')
  accuracy = accuracy_obj.update_state(
      y_true=labels, y_pred=tf.math.argmax(predictions, axis=1))

  train_op = None
  if training:
    # Upgrade to tf.keras.optimizers.
    optimizer = tf.keras.optimizers.Adam()
    # Manually assign tf.compat.v1.global_step variable to optimizer.iterations
    # to make tf.compat.v1.train.global_step increased correctly.
    # This assignment is a must for any `tf.train.SessionRunHook` specified in
    # estimator, as SessionRunHooks rely on global step.
    optimizer.iterations = tf.compat.v1.train.get_or_create_global_step()
    # Get both the unconditional updates (the None part)
    # and the input-conditional updates (the features part).
    update_ops = model.get_updates_for(None) + model.get_updates_for(features)
    # Compute the minimize_op.
    minimize_op = optimizer.get_updates(
        total_loss,
        model.trainable_variables)[0]
    train_op = tf.group(minimize_op, *update_ops)

  return tf.estimator.EstimatorSpec(
    mode=mode,
    predictions=predictions,
    loss=total_loss,
    train_op=train_op,
    eval_metric_ops={'Accuracy': accuracy_obj})

# Create the Estimator &amp; Train.
estimator = tf.estimator.Estimator(model_fn=my_model_fn)
tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
&lt;/denchmark-code&gt;

My first comment is: why write
&lt;denchmark-code&gt;accuracy = accuracy_obj.update_state(
      y_true=labels, y_pred=tf.math.argmax(predictions, axis=1))
&lt;/denchmark-code&gt;

instead of just accuracy_obj.update_state(y_true=labels, y_pred=tf.math.argmax(predictions, axis=1)) ? First, accuracyis never used. Second, this may lead the reader to believe that the tf.keras.metrics.Metric.update_state outputs the accuracy value, just like the tf.keras.metrics.Metric.result method, whereas the output of update_state is accuracy_obj.count. See on the code below:
&lt;denchmark-code&gt;import tensorflow as tf

accuracy_obj = tf.keras.metrics.Accuracy(name='acc_obj')
accuracy = accuracy_obj.update_state(
      y_true=[0, 1], y_pred=tf.math.argmax([[0.3, 0.7], [0.3, 0.7]], axis=1))

tf.print(accuracy)
# 2
tf.print(accuracy_obj.result())
# 0.5
tf.print(accuracy_obj.count)
# 2
&lt;/denchmark-code&gt;

My other comment is that we have the line  whereas in the &lt;denchmark-link:https://www.tensorflow.org/guide/migrate#custom_model_fn_with_minimal_changes&gt;"Custom model_fn with minimal changes" section&lt;/denchmark-link&gt;
 the corresponding line is  without the . Why is that? Is this a mistake?
	</description>
	<comments>
		<comment id='1' author='durandg12' date='2020-06-01T04:25:17Z'>
		&lt;denchmark-link:https://github.com/durandg12&gt;@durandg12&lt;/denchmark-link&gt;
 Thanks for the findings. The guide has been updated addresses the topics discussed in this issue.
		</comment>
	</comments>
</bug>