<bug id='6531' author='ethereon' open_date='2016-12-28T02:41:42Z' closed_time='2017-04-28T17:31:27Z'>
	<summary>tf.losses.softmax_cross_entropy is deviously inconsistent</summary>
	<description>
There are at least three variants of softmax_cross_entropy in TensorFlow:

tf.nn.softmax_cross_entropy_with_logits

Accepts logits as the first argument, and labels as the second argument.

tf.contrib.losses.softmax_cross_entropy

Accepts logits as the first argument, and onehot_labels as the second argument. So far so good. Except this is deprecated, and displays the recommendation Use tf.losses.softmax_cross_entropy instead.

tf.losses.softmax_cross_entropy

Decides to switch things around for fun and have onehot_labels as the first argument and  logits as the second. Since onehot_labels and logits are identically shaped tensors, the call succeeds without any complaints (until something else fails as a consequence, like the gradient).
This inconsistency seems a bit error prone. If nothing else, perhaps the deprecation warning for  tf.contrib.losses.softmax_cross_entropy should include a heads up.
	</description>
	<comments>
		<comment id='1' author='ethereon' date='2017-01-06T00:08:09Z'>
		&lt;denchmark-link:https://github.com/dr4b&gt;@dr4b&lt;/denchmark-link&gt;
 does this need better doc?
		</comment>
		<comment id='2' author='ethereon' date='2017-04-28T17:29:44Z'>
		Thank you for the report! Looks like the deprecation warning got updated by &lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
. I'm going to close this, since I don't see anything more we can do here given API compatibility.
		</comment>
		<comment id='3' author='ethereon' date='2017-05-11T07:42:51Z'>
		What's about
tf.losses.sparse_softmax_cross_entropy(labels,logits, ...)
vs
tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)
?
Is there any reason to provide both functions? I understand that tf.losses also maintains tf.GraphKeys.LOSSES. But wouldn't it be more consistent (kwargs vs. args, '_with_logits' vs. '') to only support a mixture by
tf.metrics.sparse_softmax_cross_entropy_with_logits(labels=labels,logits=logits, 
                                                    ..., use_collection=False)
(I really mean metrics and not losses to have one consistent modul which can also include accuracy.)
This way, tf.nn.sparse_softmax_cross_entropy_with_logits can be just wraptf.metrics.sparse_softmax_cross_entropy_with_logits.
It would be much easier, if it would be possible to just write
prob = tf.nn.softmax(logits, name='prob')
pred = tf.arg_max(prob, 1, name='pred')

cost = tf.metrics.sparse_softmax_cross_entropy_with_logits(labels=labels,logits=logits)
accuracy = tf.metrics.accuracy(labels=labels,pred=pred) # or even support logits
precision = tf.metrics.accuracy(labels=labels,pred=pred)
		</comment>
	</comments>
</bug>