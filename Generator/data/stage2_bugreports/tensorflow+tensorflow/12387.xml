<bug id='12387' author='wenjunpku' open_date='2017-08-18T08:34:47Z' closed_time='2017-08-24T19:00:30Z'>
	<summary>The precision difference between tensorflow and numpy when using tf.reduce_mean and np.mean</summary>
	<description>
&lt;denchmark-code&gt;### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **TensorFlow installed from (source or binary)**:binary
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
== cat /etc/issue ===============================================
Linux quad6 4.4.0-83-generic #106-Ubuntu SMP Mon Jun 26 17:54:43 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION="16.04.1 LTS (Xenial Xerus)"
VERSION_ID="16.04"

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.1-2ubuntu1~16.04) 5.4.1 20160904
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux quad6 4.4.0-83-generic #106-Ubuntu SMP Mon Jun 26 17:54:43 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.1)
protobuf (3.3.0)
tensorflow (1.2.1)
tensorflow-fold (0.0.1)
tensorflow-gpu (1.2.1)
tensorflow-tensorboard (0.1.4)

== check for virtualenv =========================================
True

== tensorflow import ============================================
tf.VERSION = 1.2.1
tf.GIT_VERSION = v1.2.0-5-g435cdfc
tf.COMPILER_VERSION = v1.2.0-5-g435cdfc
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda-8.0/lib64:
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Fri Aug 18 16:14:35 2017
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.66                 Driver Version: 375.66                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  TITAN Xp            Off  | 0000:05:00.0     Off |                  N/A |
| 28%   51C    P0    67W / 250W |      0MiB / 12188MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  TITAN Xp            Off  | 0000:06:00.0     Off |                  N/A |
| 31%   55C    P0    66W / 250W |      0MiB / 12189MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  TITAN Xp            Off  | 0000:09:00.0     Off |                  N/A |
| 54%   84C    P2   130W / 250W |  11655MiB / 12189MiB |     52%      Default |
+-------------------------------+----------------------+----------------------+
|   3  TITAN Xp            Off  | 0000:0A:00.0     Off |                  N/A |
| 23%   33C    P0    61W / 250W |      0MiB / 12189MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    2      5854    C   python3                                      11651MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.27
/usr/local/cuda-8.0/lib64/libcudart_static.a



python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
v1.2.0-5-g435cdfc 1.2.1
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

When using tf.reduce_mean, i found the behaviour between tensorflow and numpy was different when dtype was float32. The experiment shows that this maybe caused by the precision problem in tensorflow. When I changed the dtype to tf.float64, the problem fixed.
Or if I calculate the mean axis by axis, the problem also disappear.
However, if I use numpy array, the result was right no matter the dtype is  float32 or float64.
Is this a normal behaviour or will be fixed later?
The test code I used as following:
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

In [61]: x = tf.constant(0.6931, shape=[32, 100, 79804], dtype=tf.float32)

In [62]: y = tf.reduce_mean(x)

In [63]: sess.run(y)
Out[63]: 0.26278782

In [64]: y = tf.reduce_mean(tf.reduce_mean(tf.reduce_mean(x, axis=2), axis=-1))

In [65]: sess.run(y)
Out[65]: 0.693142

In [66]: x = tf.constant(0.6931, shape=[32, 100, 79804], dtype=tf.float32)

In [67]: y = tf.reduce_mean(x)

In [68]: sess.run(y)
Out[68]: 0.26278782

In [69]: x = tf.constant(0.6931, shape=[32, 100, 79804], dtype=tf.float64)

In [70]: y = tf.reduce_mean(x)

In [71]: sess.run(y)
Out[71]: 0.6931000008030902

In [72]: y = tf.reduce_mean(tf.reduce_mean(tf.reduce_mean(x, axis=2), axis=-1))

In [73]: sess.run(y)
Out[73]: 0.69310000000034644

In [74]: xn = np.full((32, 100, 79084), 0.6931, dtype=np.float64)

In [75]: yn = np.mean(xn)

In [76]: yn
Out[76]: 0.69310000000032723

In [77]: xn = np.full((32, 100, 79084), 0.6931, dtype=np.float32)

In [78]: yn = np.mean(xn)

In [79]: yn = np.mean(xn)

In [80]: yn
Out[80]: 0.69321907
	</description>
	<comments>
		<comment id='1' author='wenjunpku' date='2017-08-18T17:24:44Z'>
		&lt;denchmark-link:https://github.com/ekelsen&gt;@ekelsen&lt;/denchmark-link&gt;
 do you know why this issue is occurring?
		</comment>
		<comment id='2' author='wenjunpku' date='2017-08-18T17:35:42Z'>
		This is on CPU?
There are likely two issues:


The CPU reductions use a numerically terrible algorithm for doing sums.  Summing into one or few accumulators - this leads to inaccurate sums for large N.  Breaking the sum in 3 parts ameliorates this problem.  This problem has come up multiple times before.


The mean reductions are also done by dividing the inputs by N and then summing which losing some accuracy in the mantissa (likely not the big issue here).


The GPU uses a more numerically stable sum algorithm, so (1) is not a problem there.  And the GPU mean calculation will soon also fix (2).  Using float64 is less of a cost on CPU than GPU, so if accurate reductions are important, the best workaround for now is probably to change the type before the reduction to float64 and after back to float32.  Breaking along multiple dimensions also works if you have a multi-dimensional tensor.
		</comment>
		<comment id='3' author='wenjunpku' date='2017-08-19T03:12:37Z'>
		I did a test on GPU, the test code is as following:
import tensorflow as tf

with tf.device('/gpu:0'):
    x = tf.constant(0.6931, shape=[32, 100, 79804], dtype=tf.float32)
    y = tf.reduce_mean(x)
    sess = tf.Session()
    res = sess.run(y)
    print(res)
    x = tf.constant(0.6931, shape=[32, 100, 79804], dtype=tf.float64)
    y = tf.reduce_mean(x)
    res = sess.run(y)
    print(res)
The Result is just the same:
&lt;denchmark-code&gt;CUDA_VISIBLE_DEVICES=1 python3 test.py
2017-08-19 11:05:45.390445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties:
name: TITAN Xp
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:09:00.0
Total memory: 11.90GiB
Free memory: 11.75GiB
2017-08-19 11:05:45.390474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0
2017-08-19 11:05:45.390479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y
2017-08-19 11:05:45.390495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: TITAN Xp, pci bus id: 0000:09:00.0)
0.262788
0.693100000803
&lt;/denchmark-code&gt;

Is the test right?
		</comment>
		<comment id='4' author='wenjunpku' date='2017-08-19T03:18:33Z'>
		Then my intuition that the problem was due to (1) was wrong - it looks it is due to (2).  This should already be fixed for the GPU internally.
		</comment>
		<comment id='5' author='wenjunpku' date='2017-08-19T07:01:59Z'>
		&lt;denchmark-link:https://github.com/wenjunpku&gt;@wenjunpku&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ekelsen&gt;@ekelsen&lt;/denchmark-link&gt;

I have met the same problems. tf.reduce_mean() gets different results from numpy.mean() on GPU (float32 and float16). But code with tensorflow + keras gets the right answer.
How to get the same results as the numpy.mean() with tensorflow?
		</comment>
		<comment id='6' author='wenjunpku' date='2017-08-22T09:09:17Z'>
		you can try do the mean part by part, just like y=tf.reduce_mean(tf.reduce_mean(tf.reduce_mean(x, axis=2), axis=-1))
which fix my problem:)
		</comment>
		<comment id='7' author='wenjunpku' date='2017-08-24T19:00:30Z'>
		I can confirm this is fixed internally on the GPU - it now returns .6931 as expected
		</comment>
		<comment id='8' author='wenjunpku' date='2020-12-05T00:07:13Z'>
		Any updates on this?
		</comment>
		<comment id='9' author='wenjunpku' date='2020-12-05T00:21:48Z'>
		This issue no longer occurs. If you have a different precision problem, file a new issue.
		</comment>
	</comments>
</bug>