<bug id='36998' author='miladtoutounchian' open_date='2020-02-23T20:24:11Z' closed_time='2020-03-12T23:12:13Z'>
	<summary>Keras models train correctly with or without tf.function decorator but this is not correct for custom models</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock
example script provided in TensorFlow):
OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04):
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device:
TensorFlow installed from (source or
binary): - TensorFlow version (use command below):
Python version: - Bazel
version (if compiling from source):
GCC/Compiler version (if compiling from
source):
CUDA/cuDNN version: - GPU model and memory:

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
Describe the expected behavior
Standalone code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='miladtoutounchian' date='2020-02-23T20:27:30Z'>
		Compare the following two codes. If include @tf.function then both works well. If not include @tf.fucntion then the custom low-level model does not train.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/4242370/tf_function_problem.zip&gt;tf_function_problem.zip&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='miladtoutounchian' date='2020-02-24T06:17:40Z'>
		&lt;denchmark-link:https://github.com/miladtoutounchian&gt;@miladtoutounchian&lt;/denchmark-link&gt;

Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?
We ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!
		</comment>
		<comment id='3' author='miladtoutounchian' date='2020-02-25T21:21:13Z'>
		I did pip install tensorflow==2.1, OS is Mac
		</comment>
		<comment id='4' author='miladtoutounchian' date='2020-02-26T09:36:58Z'>
		&lt;denchmark-link:https://github.com/miladtoutounchian&gt;@miladtoutounchian&lt;/denchmark-link&gt;

I have run the code shared by you on tf 2.1 with and with out @tf.function and did not face any issues,please find the &lt;denchmark-link:https://colab.sandbox.google.com/gist/Saduf2019/cf754f626032f37f7edc73d2ea079e39/36998.ipynb&gt;gist&lt;/denchmark-link&gt;
 for the &lt;denchmark-link:https://colab.sandbox.google.com/gist/Saduf2019/809a5453c7b7dbbcc1e231a486865744/36998.ipynb&gt;same&lt;/denchmark-link&gt;
. the same code runs without any issues on nightly as well.
in case your still facing issue please share a gist where the error is seen along with error logs if any.
		</comment>
		<comment id='5' author='miladtoutounchian' date='2020-02-27T00:48:47Z'>
		I am not reporting that we get error, I am saying the model is not trained which means the accuracy performance is very low and is not getting better through epochs if do not use  @tf.function decorator. After 5 epoch, here is the model very poor performance:
Epoch: 5
Avg loss: 0
accuracy: 0.2763499915599823
Add tf.function decorator then the model is being trained.
		</comment>
		<comment id='6' author='miladtoutounchian' date='2020-02-27T06:44:15Z'>
		&lt;denchmark-link:https://github.com/miladtoutounchian&gt;@miladtoutounchian&lt;/denchmark-link&gt;

I believe the difference in performance accuracy is because, "@tf.function decorator."  will be compiled into a graph, which means you get the benefits of faster execution, running on GPU or TPU, or exporting to SavedModel.
This &lt;denchmark-link:https://www.tensorflow.org/guide/function&gt;link&lt;/denchmark-link&gt;
 confirms the same.please le tme now if that answers your query.
		</comment>
		<comment id='7' author='miladtoutounchian' date='2020-02-27T08:11:46Z'>
		&lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;
 Decorating with  may indeed have benefits as to execution runtime, however it should not have any effect on the accuracy reached, unless there is either a tensorflow bug, or some error-inducing side effects within &lt;denchmark-link:https://github.com/miladtoutounchian&gt;@miladtoutounchian&lt;/denchmark-link&gt;
's code.
		</comment>
		<comment id='8' author='miladtoutounchian' date='2020-03-04T00:52:59Z'>
		I have ran both the codes(with and without @tf.function) on colab with tf-nightly but didn't find much of a difference in accuracy. Please find my gist(&lt;denchmark-link:https://colab.sandbox.google.com/gist/gowthamkpr/7f6d5051e5b1cbfebeab3ab962ec6017/with_tf_function.ipynb&gt;with tf.function&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://colab.sandbox.google.com/gist/gowthamkpr/1337b792dedc5305fa9339dcda8b1a5c/without_tf_function.ipynb&gt;without tf.function&lt;/denchmark-link&gt;
)
Please take a look at these gists and if I am wrong please correct me. Thanks!
		</comment>
		<comment id='9' author='miladtoutounchian' date='2020-03-04T08:39:32Z'>
		&lt;denchmark-link:https://github.com/gowthamkpr&gt;@gowthamkpr&lt;/denchmark-link&gt;
 Thank you for running the tests and sharing the gists :)
Interestingly, the accuracy seems slightly (but significantly) lower with , but I think this is actually due to the custom  rather than to the decorator; more precisely, in the custom model, weights initialization does not follow the default scheme used in the  layers, which may cause this difference (alternatively, it may be sheer randomness due to the dataset shuffling, or an interaction between both aspects).
&lt;denchmark-link:https://github.com/miladtoutounchian&gt;@miladtoutounchian&lt;/denchmark-link&gt;
 Could you please check that these tests correspond to what you were running in the first place? If so, I honestly have no idea why you encountered the reported convergence issues...
		</comment>
		<comment id='10' author='miladtoutounchian' date='2020-03-04T17:50:36Z'>
		Please carefully read the title of this issue that I reported. Keras models train correctly with or without tf.function decorator but this is not correct for custom models.
We have two Model cases and each case has two propagation methods, therefore:
Case 1: Model is Keras:
1-1 : propagate function with @tf.function decorator -&gt; Works well
1-2:  propagate function without @tf.function decorator -&gt; Works well
Case 2: Model is custom (not Keras):
2-1: propagate function with @tf.function decorator -&gt; Works well
2-2: propagate function without @tf.function decorator -&gt; the model does not train at all
Can any of you answer why we encounter of model training problem with Case 2-2?
However there is no problem with Case 1-2.
I believe this is a Tensorflow Bug
		</comment>
		<comment id='11' author='miladtoutounchian' date='2020-03-05T09:38:20Z'>
		Hi,
I was able to replicate the issue, thanks for clarifying.
I have two distinct answers, and I will start with the obvious but unsatisfactory one, before moving to what appears like an actual issue.

Your custom Model does not abide by the current API.
If you replace it with a custom keras Model subclass, such as the one implemented below, then it trains perfectly, with or without tf.function decorating your custom training step.

class ModelKeras(tf.keras.Model):

    def __init__(self):
      super().__init__()
      kwargs = {'kernel_initializer': 'normal', 'bias_initializer': 'normal'}
      self.layer_1 = tf.keras.layers.Dense(512, 'relu', **kwargs)
      self.layer_2 = tf.keras.layers.Dense(512, 'relu', **kwargs)
      self.out_layer = tf.keras.layers.Dense(10, **kwargs)

    @property
    def trainable_vars(self):  # merely to leave the rest of the code unchanged
        return self.trainable_variables

    def call(self, inputs):
      output = self.layer_1(inputs)
      output = self.layer_2(output)
      return self.out_layer(output)

Gradient computation differs when tf.function decorates propagate.
I do not get why, but here is the test I ran:

# Define a function to compute gradients of a network's weights w.r.t. a given batch.
def compute_gradients(model, x_batch, y_batch):
    with tf.GradientTape() as tape:
        pred = tf.nn.softmax(model(x_batch))
        loss = tf.keras.losses.sparse_categorical_crossentropy(y_batch, pred)
    return tape.gradient(loss, model.trainable_vars)
# Make a tf.function-decorated copy of the previous.
decorated_gradients = tf.function(compute_gradients)

# Gather a training batch.
x_batch, y_batch = next(iter(mnist_dataset()))
# Instantiate two models and build them.
model_custom = Model()  # weights are built at instantiation
model_keras = ModelKeras()
_ = model_keras(x_batch)  # build weights through sample processing
# Set the second model's weights equal to those of the first one.
weights = [
    w.numpy() for pair in zip(model_custom.trainable_vars[:3], model_custom.trainable_vars[3:])
    for w in pair
]
model_keras.set_weights(weights)

# Compute gradients for both models without tf.function.
# Save for ordering, the results are the same for both, as should be.
compute_gradients(model_custom, x_batch, y_batch)
compute_gradients(model_keras, x_batch, y_batch)

# Compute gradients for both models with tf.function.
# Save for ordering, the results are the same for both, as should be.
# However, they differ from the outputs of the the non-decorated function, which is weird.
decorated_gradients(model_custom, x_batch, y_batch)
decorated_gradients(model_keras, x_batch, y_batch)
So, for some reason, it appears that tf.function decoration changes the way gradients are computed, which might be the cause of the model's lack of convergence. As a matter of fact, when not decorated, the computed gradients tend to be very sparse, i.e. there are a lot of zero values resulting in most weights not being updated during the training step. I do not know why this is the case; it would seem that part of the computation is not properly tracked?
Now, the reason why the keras Model trains better is also that in spite of my forcing the use of random normal weights initializers, the initial weights (when not forcefully replaced as in the previous test) are smaller than that generated in the custom model, which seems to result in smoother initial predictions and may explain why it is easier to train.
		</comment>
		<comment id='12' author='miladtoutounchian' date='2020-03-08T23:17:41Z'>
		What is the next action folks?
		</comment>
		<comment id='13' author='miladtoutounchian' date='2020-03-12T23:02:50Z'>
		&lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
 for more thoughts
GradientTape should be expected to produce different results inside and outside of tf.function due to the sensitivity of floating-point calculation on ordering. That said, the differences should be small, excluding any source of numerical instability like calculating the loss through the softmax function.
As far as I know, Keras runs in graph mode with or without tf.function. So what we're seeing is consistent, in the sense that models train in graph mode, but not in eager mode. I'd expect the eager version to still train, but with a different learning rate. The differences seem indeed significant.
		</comment>
		<comment id='14' author='miladtoutounchian' date='2020-03-12T23:12:12Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
 I look at &lt;denchmark-link:https://github.com/pandrey-fr&gt;@pandrey-fr&lt;/denchmark-link&gt;
 's example code and I see the very problematic pattern of doing softmax and then cross entropy, instead of using softmax_cross_entropy_with_logits or the equivalent.
softmax-then-cross-entropy is really numerically unstable (you throw away most of the bits of your logits when doing softmax) and should never be used. Because of this the keras cross-entropy function has logic to "undo" the softmax in graph mode: 


tensorflow/tensorflow/python/keras/backend.py


         Line 4628
      in
      ead7a37






 if output.op.type == 'Softmax': 




 this code however doesn't work in eager mode because you cannot walk the graph.
So this is a known issue, and the fix is to never use softmax followed by cross entropy. &lt;denchmark-link:https://github.com/MarkDaoust&gt;@MarkDaoust&lt;/denchmark-link&gt;
 recently removed this from all of our official examples, so hopefully this will stop happening.
Sadly, then, I'll have to close this issue as working-as-intended.
		</comment>
		<comment id='15' author='miladtoutounchian' date='2020-03-12T23:12:14Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36998&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36998&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='16' author='miladtoutounchian' date='2020-03-12T23:25:33Z'>
		Thanks Alex,
What are the loss values you're seeing with and without ?  with  and ~15 without?
In my answer on &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/34221&gt;#34221&lt;/denchmark-link&gt;
 I explain a little how this happens and why (unless you have a large number of classes) it's probably also an indicator that  (in addition to the softmax issue) either:

Your inputs are not normalized.
Or your initialization is bad.

		</comment>
		<comment id='17' author='miladtoutounchian' date='2020-03-16T21:47:01Z'>
		I have changed the code based on &lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
 suggestion, now used softmax_cross_entropy_with_logits:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/4340550/keras_vs_custom_TF2.zip&gt;keras_vs_custom_TF2.zip&lt;/denchmark-link&gt;

Now, the custom model loss is really large compared to Keras model. Also the accuracy of custom model is not as good as Keras model.
		</comment>
		<comment id='18' author='miladtoutounchian' date='2020-03-16T21:53:34Z'>
		Please do not do softmax and then cross-entropy; use
softmax_cross_entropy_with_logits instead
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Mon, Mar 16, 2020 at 2:47 PM Milad Toutounchian ***@***.***&gt; wrote:
 I have changed the code based on @alextp &lt;https://github.com/alextp&gt;
 which do softmax first then followed by cross entropy:
 keras_vs_custom_TF2.zip
 &lt;https://github.com/tensorflow/tensorflow/files/4340550/keras_vs_custom_TF2.zip&gt;

 Now, the custom model loss is really large compared to Keras model. Also
 the accuracy of custom model is not as good as Keras model.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#36998 (comment)&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/AAABHRNDZBEURS2H226DOQTRH2M6PANCNFSM4KZ5RKAA&gt;
 .


-- 
 - Alex

		</comment>
		<comment id='19' author='miladtoutounchian' date='2020-03-16T21:57:45Z'>
		&lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
 , exactly did what you said. But please compare the loss for Keras vs custom also compare the lower accuracy of custom model.
		</comment>
		<comment id='20' author='miladtoutounchian' date='2020-03-16T21:59:27Z'>
		That means having no softmax activation. If you add the softmax you won't
get a good loss.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Mon, Mar 16, 2020 at 2:58 PM Milad Toutounchian ***@***.***&gt; wrote:
 @alextp &lt;https://github.com/alextp&gt; , exactly did what you said. But
 please compare the loss for Keras vs custom also compare the lower accuracy
 of custom model.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#36998 (comment)&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/AAABHRLQP7QBQBBMP56ERMTRH2OG3ANCNFSM4KZ5RKAA&gt;
 .


-- 
 - Alex

		</comment>
		<comment id='21' author='miladtoutounchian' date='2020-03-16T22:14:27Z'>
		&lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
 , I do not get what you are saying. Please offer the solution by writing/modifying my attached code.
		</comment>
		<comment id='22' author='miladtoutounchian' date='2020-03-16T22:58:08Z'>
		&lt;denchmark-link:https://github.com/miladtoutounchian&gt;@miladtoutounchian&lt;/denchmark-link&gt;
, again, please see my response on &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/34221&gt;#34221&lt;/denchmark-link&gt;
: Check your layer initialization.
You're using:
tf.random.normal([n_hidden_2, n_classes]) (mean=0, var=1)
Keras is using &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform?hl=en&gt;GlorotUniform&lt;/denchmark-link&gt;
.
Also, for issues like this:

You'll have a much easier time if you go one function at a time, instead of just comparing final results.
Stack overflow is probably a better place for further discussion, since this is not an error in tensorflow.

Good luck.
		</comment>
	</comments>
</bug>