<bug id='6478' author='PhoenixDai' open_date='2016-12-23T15:16:56Z' closed_time='2017-06-16T22:55:17Z'>
	<summary>t.eval() built with slim outputs wrong predictions when input batch contains identical images</summary>
	<description>
When evaluating a model built with slim on a batch that contains identical images, the output of the batch will be wrong (mostly the outputs will be the most frequent label). For example, if my evaluation set has 421 images and my batch size is 40, I filled the last batch with 19 identical images (the 421st one) to avoid tensor shape mismatch error. Then,  the output for the 21st to the 40th image of the last batch will be the most common label in training set. If I replace the 19 images with some randomly selected images, the outputs will be correct.
&lt;denchmark-h:h3&gt;What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?&lt;/denchmark-h&gt;

None.
&lt;denchmark-h:h3&gt;Environment info&lt;/denchmark-h&gt;

Operating System: Ubuntu 14.04.5
Installed version of CUDA and cuDNN:
(please attach the output of ):
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/3335135/21456479/56b99382-c8f6-11e6-88ee-1b83758fb56a.png&gt;&lt;/denchmark-link&gt;

If installed from binary pip package, provide:


A link to the pip package you installed:
https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl


The output from python -c "import tensorflow; print(tensorflow.__version__)".


I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally
0.11.0
&lt;denchmark-h:h3&gt;If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)&lt;/denchmark-h&gt;

The problem can be reproduced with any model built purely with slim but evaluated with output.eval(feed_dict={batch_image:test_batch}) or sess.run(output, feed_dict={batch_image:test_batch})
when the test batch contains several identical images.
&lt;denchmark-h:h3&gt;What other attempted solutions have you tried?&lt;/denchmark-h&gt;

Avoid identical or similar images in evaluation batch.
&lt;denchmark-h:h3&gt;Logs or other output that would be helpful&lt;/denchmark-h&gt;

(If logs are large, please upload as attachment or provide link).
	</description>
	<comments>
		<comment id='1' author='PhoenixDai' date='2017-01-05T19:27:34Z'>
		Could you follow up with a quick concrete repro case that we can try running?
		</comment>
		<comment id='2' author='PhoenixDai' date='2017-01-05T20:20:10Z'>
		Sure, I will wrap up a repro case in a couple of days.
		</comment>
		<comment id='3' author='PhoenixDai' date='2017-01-09T15:48:50Z'>
		I put together a repro case in a .ipynb file. Please place the file in the slim folder (./models/slim) so that it can find the slim dataset lib. The 4th and 7th cell set the dataset path, please point it to the converted cifar dataset tfrecord folder. Then, you should be able to run it.
At the penultimate cell, you can see results of a batch with different images and most of the classifications match their label. However, at the last cell, when I fill the batch with identical images, the classification will always be 5.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/693863/slim_debug.zip&gt;slim_debug.zip&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='PhoenixDai' date='2017-01-09T16:18:54Z'>
		In test time you didn't seem to set batch_norm to is_training=False.
Then with an identical batch it will certainly fail -- variance ended up being 0.
		</comment>
		<comment id='5' author='PhoenixDai' date='2017-01-09T16:24:48Z'>
		Oh, thank you so much for pointing this out. I will have a try and let you know how it goes.
		</comment>
		<comment id='6' author='PhoenixDai' date='2017-01-09T17:06:12Z'>
		I set batch_norm to is_training=False but still have the same issue. Below is the updated evaluation/test graph:
&lt;denchmark-code&gt;graph = tf.Graph()
with graph.as_default():
  batch_images = tf.placeholder(tf.float32, (None,32,32,3))
  batch_labels = tf.placeholder(tf.float32, (None,10))    
  is_training = tf.placeholder(tf.bool)
  
  with slim.arg_scope([slim.batch_norm], is_training=is_training):
    features = classifier_features(batch_images, is_training)
    outputs, output_logits = classifier_outputs(features, dataset.num_classes)
  output_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(output_logits, batch_labels)) 
  output_loss_reg = output_loss + tf.add_n(slim.losses.get_regularization_losses())

  train_vars = tf.trainable_variables() 
&lt;/denchmark-code&gt;

Am I doing it in the right way?
		</comment>
		<comment id='7' author='PhoenixDai' date='2017-01-09T21:42:01Z'>
		I'm not sure what your classifier_features or classifier_outputs do.
If you are using drop_out you also need to set is_training=False.
		</comment>
		<comment id='8' author='PhoenixDai' date='2017-01-10T01:54:02Z'>
		classifier_features and classifier_outputs are used to define the model. Please see the 3rd cell in the attached .ipynb file in my second reply to michaelisard for more details.
Yes, I'm using drop_out and classifier_features(batch_images, is_training) will set is_training=False for drop_out
		</comment>
		<comment id='9' author='PhoenixDai' date='2017-06-16T22:55:17Z'>
		It looks like code in that .ipynb no longer runs (mostly the problems are function signatures that were updated in 1.0).
&lt;denchmark-link:https://github.com/PhoenixDai&gt;@PhoenixDai&lt;/denchmark-link&gt;
, could you update and retry it, let us know if this is still a problem.
		</comment>
	</comments>
</bug>