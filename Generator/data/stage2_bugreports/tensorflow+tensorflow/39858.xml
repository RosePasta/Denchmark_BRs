<bug id='39858' author='lokesharma-dev' open_date='2020-05-25T20:46:13Z' closed_time='2020-05-28T12:23:53Z'>
	<summary>Unable to calculate gradient -&amp;gt; AttributeError: 'Tensor' object has no attribute 'numpy'</summary>
	<description>
System information

OS Platform and Distribution: macOS Catalina
TensorFlow installed from (source or binary): Google Colab
TensorFlow version: &gt;=2.0
Python version: 3.6

** I want to compute the gradient for KL Divergence ( between two distributions aka logits) with respect to noise (kind of input and not wrt weights). It was possible with Tensorflow 1.0 but using GradientTape seems to return me a None for "grads" which I am unable to comprehend**
Code:
import tensorflow as tf
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import Input, Embedding, Dense, LSTM, Bidirectional
MAX_VOCAB_SIZE = 10000 # maximum no of unique words
MAX_DOC_LENGTH = 500 # maximum no of words in each sentence
EMBEDDING_DIM = 300 # Embeddings dimension from Glove directory
def compute_kld(p_logit, q_logit): # computes difference between two distributions
p = tf.nn.softmax(p_logit)
q = tf.nn.softmax(q_logit)
kl_score = tf.reduce_sum( p * (tf.math.log(p+1e-16) - tf.math.log(q+1e-16)), axis = 1)
return kl_score # Scalar -&gt; lower kl means closer the distributions are closer
&lt;denchmark-h:h1&gt;Create Embeddings&lt;/denchmark-h&gt;

inputs2d = Input(shape=(MAX_DOC_LENGTH,)) # takes a sequence of words
clean_embd= Embedding(input_dim=MAX_VOCAB_SIZE + 1, output_dim=EMBEDDING_DIM,
input_length=MAX_DOC_LENGTH)(inputs2d) # converts each input to embedding matrix
r_embd = tf.random.uniform(shape=tf.shape(clean_embd)) # random noise to be added to clean_embd
&lt;denchmark-h:h1&gt;Create Model&lt;/denchmark-h&gt;

inputs3d = Input(shape=(MAX_DOC_LENGTH, EMBEDDING_DIM,)) # Embedding shall be my input to the model
network = Sequential()
network.add(LSTM(units=128,))
network.add(Dense(units=16, activation='relu'))
output = network(inputs3d)
model = Model(inputs3d, output)
model.summary()
&lt;denchmark-h:h1&gt;Calculate gradient&lt;/denchmark-h&gt;

with tf.GradientTape(watch_accessed_variables=False) as tape:
tape.watch(r_embd)
r_embd_ = tf.math.add(clean_embd, r_embd) # Noised input
# Compute logits
p_logit = model(clean_embd) # True logit distribution
p_logit_r = model(r_embd_) # Perturbed logits distribution
# Find the Kl Score
kl_score = tf.reduce_mean(compute_kld(p_logit, p_logit_r))
kl_score = tf.convert_to_tensor(kl_score, dtype=tf.float32)
grads = tape.gradient(kl_score, r_embd) # I wish to differentiate kl_score wrt r_embd. So the Jacobian matrix should be of shape (None, 500, 300) similar to (clean_embd | r_embd_).
# Error is that grads is returned as None; which I can't understand.
&lt;denchmark-h:h1&gt;Future implementation&lt;/denchmark-h&gt;

r_vadv_embd = tf.math.add(clean_embd, g)
q_logits = model(r_vadv_embd) # End goal is to be able to calculate q_logit
&lt;denchmark-h:h2&gt;Any other info / logs&lt;/denchmark-h&gt;

AttributeError                            Traceback (most recent call last)
 in ()
15     kl_score = tf.reduce_mean(compute_kld(p_logit, p_logit_r))
16     kl_score = tf.convert_to_tensor(kl_score, dtype=tf.float32)
---&gt; 17 grads = tape.gradient(kl_score, r_embd)
18
19 # g = [grad if grad is not None else tf.zeros_like(var)for var, grad in zip(r_embd, grads)]
4 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)
1046         output_gradients=output_gradients,
1047         sources_raw=flat_sources_raw,
-&gt; 1048         unconnected_gradients=unconnected_gradients)
1049
1050     if not self._persistent:
/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)
75       output_gradients,
76       sources_raw,
---&gt; 77       compat.as_str(unconnected_gradients.value))
/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py in _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)
155       gradient_name_scope = "gradient_tape/"
156     with ops.name_scope(gradient_name_scope):
--&gt; 157       return grad_fn(mock_op, *out_grads)
158   else:
159     return grad_fn(mock_op, *out_grads)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py in _SumGrad(op, grad)
210       # more sense.
211       output_shape_kept_dims = math_ops.reduced_shape(input_shape,
--&gt; 212                                                       op.inputs[1])
213     grad = array_ops.reshape(grad, output_shape_kept_dims)
214   return [array_ops.broadcast_to(grad, input_shape), None]
/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in reduced_shape(input_shape, axes)
3733   """
3734   if context.executing_eagerly():
-&gt; 3735     input_shape = input_shape.numpy()
3736     axes = axes.numpy()
3737     input_shape[axes] = 1
AttributeError: 'Tensor' object has no attribute 'numpy'
	</description>
	<comments>
		<comment id='1' author='lokesharma-dev' date='2020-05-26T11:09:29Z'>
		&lt;denchmark-link:https://github.com/lokesharma-dev&gt;@lokesharma-dev&lt;/denchmark-link&gt;

Can you please refer to these links and let us know if it helps:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/39817#issuecomment-633675341&gt;link&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://stackoverflow.com/questions/52357542/attributeerror-tensor-object-has-no-attribute-numpy&gt;link1&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://www.reddit.com/r/learnmachinelearning/comments/c1j7i5/attributeerror_tensor_object_has_no_attribute/&gt;link2&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://www.kaggle.com/c/prostate-cancer-grade-assessment/discussion/146248&gt;link3&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/39582&gt;#39582&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/35949&gt;#35949&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/36979&gt;#36979&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='lokesharma-dev' date='2020-05-28T12:23:53Z'>
		Thanks a lot! This was of help and we can close the issue now.
		</comment>
		<comment id='3' author='lokesharma-dev' date='2020-05-28T12:23:54Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39858&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39858&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='lokesharma-dev' date='2020-07-17T21:51:30Z'>
		&lt;denchmark-link:https://github.com/lokesharma-dev&gt;@lokesharma-dev&lt;/denchmark-link&gt;
 so what was the solution for you? I refered all that links and didn't get the answer for me. With tf 2.0 eager execution is enabled, but I see the problem, as you had, in standart reduced_shape function, not in the custom one. As I can see, there is a check that succesfully passes: "if context.executing_eagerly()", but that doesn't help and the error raises while attempted to use to_numpy(). I'm confused what to do with this
 Ok, I've found out, that Jul 7, 2020 commit &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/7d2a2a6a11dd2bf8f4ab95d62bad10d46b953c9f#diff-f0675f2568ff9470bcfc1f2bc79c5386&gt;7d2a2a6&lt;/denchmark-link&gt;
 fixed the reduced_shape function and got rid of .numpy(). Hooray! Unfortunately, it is not included in release(
		</comment>
		<comment id='5' author='lokesharma-dev' date='2020-07-18T09:04:49Z'>
		&lt;denchmark-link:https://github.com/EvsanDlg&gt;@EvsanDlg&lt;/denchmark-link&gt;
 Well TensorFlow 2.x is tricky at times especially the exceptions they provide.
My issue was fixed when I was using an EagerTensor instead of the tensor object. In tf1.x we could compute gradient wrt to a tensor with no data inside it. I do not understand how I was getting an error relating to NumPy() conversion. However, in your case, I won't be able to comment unless have some more information.
Hope this helps.
		</comment>
		<comment id='6' author='lokesharma-dev' date='2020-12-01T07:40:01Z'>
		&lt;denchmark-link:https://github.com/lokesharma-dev&gt;@lokesharma-dev&lt;/denchmark-link&gt;
 i am facing the same issue...can you help as to how you solved the issue by using an eagertensor ??
		</comment>
	</comments>
</bug>