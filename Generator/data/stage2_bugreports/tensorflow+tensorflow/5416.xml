<bug id='5416' author='raphtown' open_date='2016-11-05T20:11:31Z' closed_time='2017-06-16T22:21:33Z'>
	<summary>Possible Bug - Varying session run times when parallelized across GPUs on one node</summary>
	<description>
Hello tensorflow team!
I have been trying to parallelize my 3D convolution network across 2 GPUs on the same node (using data parallelism), and have found that some of my session runs took significantly longer than others.  Upon closer investigation using your timeline feature, I found that sometimes GPU1 would wait a certain amount of time before starting to run its convolutions even as GPU0 was happily chugging along. In the most extreme case, execution would happen essentially sequentially, doubling my runtime!
I am attaching screenshots of my timelines from three sessions to show you what I mean...
In the first one, everything is fine, it is executing both towers in parallel:
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/177576/20033120/cc17df8c-a356-11e6-8ef3-55cfdeb9d244.png&gt;&lt;/denchmark-link&gt;

In the second, I see the "sequential" behaviour:
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/177576/20033121/cd4d9e96-a356-11e6-8ef1-091edd981fdf.png&gt;&lt;/denchmark-link&gt;

In the last one, I see something in between:
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/177576/20033136/2b7f68b4-a357-11e6-9eb5-92f9a30a5fa3.png&gt;&lt;/denchmark-link&gt;

I know this happens even when I am not logging metadata because I still see a large spread of runtime distributions without logging on (see parallel-nometadata.txt at the bottom).  Also, dumping the GraphDef proto at each step tells me that the ops are correctly assigned to the different GPUs (I have attached those, as well as my timeline files, at the bottom).
I actually briefly talked about this with &lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;
 in person a little over a week ago, and we did not find anything obviously wrong at first glance.  Any help you could provide would be much appreciated!
&lt;denchmark-h:h3&gt;What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?&lt;/denchmark-h&gt;

I have not found any related Github or StackOverflow threads.  I have been using the Timeline profiling feature as described &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/1824&gt;here&lt;/denchmark-link&gt;
.
&lt;denchmark-h:h3&gt;Environment info&lt;/denchmark-h&gt;

Operating System: Ubuntu 14.04.5 LTS (running in a &lt;denchmark-link:http://singularity.lbl.gov&gt;singularity&lt;/denchmark-link&gt;
 container on a CentOS 6.7 host).
Installed version of CUDA and cuDNN:
I am using CUDA 8.0 with NVIDIA driver 367.48, and cuDNN v5.1 .
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
&lt;denchmark-code&gt;libOpenCL.so
libOpenCL.so.1
libOpenCL.so.1.0
libOpenCL.so.1.0.0
libcublas.so
libcublas.so.8.0
libcublas.so.8.0.45
libcublas_device.a
libcublas_static.a
libcudadevrt.a
libcudart.so
libcudart.so.8.0
libcudart.so.8.0.44
libcudart_static.a
libcudnn.so
libcudnn.so.5
libcudnn.so.5.1.5
libcudnn_static.a
libcufft.so
libcufft.so.8.0
libcufft.so.8.0.44
libcufft_static.a
libcufftw.so
libcufftw.so.8.0
libcufftw.so.8.0.44
libcufftw_static.a
libcuinj64.so
libcuinj64.so.8.0
libcuinj64.so.8.0.44
libculibos.a
libcurand.so
libcurand.so.8.0
libcurand.so.8.0.44
libcurand_static.a
libcusolver.so
libcusolver.so.8.0
libcusolver.so.8.0.44
libcusolver_static.a
libcusparse.so
libcusparse.so.8.0
libcusparse.so.8.0.44
libcusparse_static.a
libnppc.so
libnppc.so.8.0
libnppc.so.8.0.44
libnppc_static.a
libnppi.so
libnppi.so.8.0
libnppi.so.8.0.44
libnppi_static.a
libnppial.so
libnppial.so.8.0
libnppial.so.8.0.44
libnppicc.so
libnppicc.so.8.0
libnppicc.so.8.0.44
libnppicom.so
libnppicom.so.8.0
libnppicom.so.8.0.44
libnppidei.so
libnppidei.so.8.0
libnppidei.so.8.0.44
libnppif.so
libnppif.so.8.0
libnppif.so.8.0.44
libnppig.so
libnppig.so.8.0
libnppig.so.8.0.44
libnppim.so
libnppim.so.8.0
libnppim.so.8.0.44
libnppist.so
libnppist.so.8.0
libnppist.so.8.0.44
libnppisu.so
libnppisu.so.8.0
libnppisu.so.8.0.44
libnppitc.so
libnppitc.so.8.0
libnppitc.so.8.0.44
libnpps.so
libnpps.so.8.0
libnpps.so.8.0.44
libnpps_static.a
libnvToolsExt.so
libnvToolsExt.so.1
libnvToolsExt.so.1.0.0
libnvblas.so
libnvblas.so.8.0
libnvblas.so.8.0.44
libnvgraph.so
libnvgraph.so.8.0
libnvgraph.so.8.0.44
libnvgraph_static.a
libnvrtc-builtins.so
libnvrtc-builtins.so.8.0
libnvrtc-builtins.so.8.0.44
libnvrtc.so
libnvrtc.so.8.0
libnvrtc.so.8.0.44
stubs
&lt;/denchmark-code&gt;

I installed tensorflow using the 0.11.0rc2-gpu tag on docker hub.
&lt;denchmark-h:h3&gt;If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)&lt;/denchmark-h&gt;

Here is the code I used to generate all the data I am sharing.
&lt;denchmark-code&gt;import os
import time
import timeit

import google
import numpy as np
import tensorflow as tf

grid_size = 9
channel_size = 4
batch_size = 2**9
num_convs = 8
log_metadata = True
towers = 2
num_train_batches = 100


def define_model(towers):
    tower_outs = []
    for i in range(0, towers):
        with tf.device('/gpu:{:}'.format(i)):
            with tf.name_scope('TOWER_{:}'.format(i)):
                tower_outs.append(define_tower(i))
                tf.get_variable_scope().reuse_variables()
    return tower_outs


def define_tower(gpu_num):
    x = tf.placeholder(
        tf.float32,
        shape=[None, grid_size, grid_size, grid_size, channel_size],
        name='grid')

    num_inputs = channel_size
    for i in range(num_convs):
        with tf.variable_scope("conv{:d}".format(i)):
            weights = tf.get_variable(
                "weights",
                [3, 3, 3, num_inputs, 32])
            x = tf.nn.conv3d(x, weights, [1, 1, 1, 1, 1], 'SAME')
            num_inputs = 32

    return x


if __name__ == "__main__":

    # Define model.
    tower_outs = define_model(towers)

    # Create inputs.
    feed_dict = {}
    for i in range(towers):
        feed_dict['TOWER_{:}/grid:0'.format(i)] = np.random.rand(
            batch_size, grid_size, grid_size, grid_size, channel_size)

    # Prepare for logging.
    if log_metadata:
        run_options = tf.RunOptions(
            trace_level=tf.RunOptions.FULL_TRACE,
            output_partition_graphs=True)
        run_metadata = tf.RunMetadata()
        kwargs = {'options': run_options,
                  'run_metadata': run_metadata}

        curr_time = time.strftime("%Y-%m-%d-%H-%M-%S")
        if log_metadata:
            out_dir = curr_time
            os.mkdir(out_dir)
    else:
        kwargs = {}

    # Run model.
    train_learning_times = []
    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:
        tf.initialize_all_variables().run()
        for i in range(num_train_batches):
            learning_time_start = timeit.default_timer()
            _ = sess.run(tower_outs, feed_dict=feed_dict, **kwargs)
            train_learning_times.append(timeit.default_timer() -
                                        learning_time_start)

            if log_metadata:
                from tensorflow.python.client import timeline
                tl = timeline.Timeline(run_metadata.step_stats)
                ctf = tl.generate_chrome_trace_format()
                with open('{}/timeline_parallel_{}.json'
                          .format(curr_time, i), 'w') as f:
                    f.write(ctf)
                with open('{}/run_metadata_{}.txt'
                          .format(curr_time, i), 'w') as f:
                    f.write(
                        google.protobuf.text_format.MessageToString(
                            run_metadata))

    # Print stats
    print 'Histogram counts: {}'.format(
        np.histogram(train_learning_times[2:])[0])
    print 'Histogram edges: {}'.format(
        np.histogram(train_learning_times[2:])[1])
    print 'Median time for learning: {:5.2f}'.format(
        np.median(train_learning_times))
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;What other attempted solutions have you tried?&lt;/denchmark-h&gt;

No other solutions tried.
&lt;denchmark-h:h3&gt;Logs or other output that would be helpful&lt;/denchmark-h&gt;

Dump of GraphDef and timeline json for each of 100 runs.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/573462/2016-11-05-18-52-32.zip&gt;2016-11-05-18-52-32.zip&lt;/denchmark-link&gt;

Output of code with log_metadata set to True:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/573464/parallel.txt&gt;parallel.txt&lt;/denchmark-link&gt;

Output of code with log_metadata set to False:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/573465/parallel-nometadata.txt&gt;parallel-nometadata.txt&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='raphtown' date='2016-11-07T18:19:47Z'>
		Thanks for making such a detailed bug report, &lt;denchmark-link:https://github.com/raphtown&gt;@raphtown&lt;/denchmark-link&gt;
!
The first thing I noticed when looking at the timelines is that all of the GPU kernel profiling events  to be running on ... even if they've been issued by the TensorFlow device . However, this seems to be a red herring, because there's a &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/a6b33c30926b7a11a1ec0a18d01e6f8852561376/tensorflow/core/common_runtime/gpu/gpu_tracer.cc#L561&gt;TODO comment in gpu_tracer.cc that reveals that these prefixes are inaccurate with multiple devices&lt;/denchmark-link&gt;
. &lt;denchmark-link:https://github.com/prb12&gt;@prb12&lt;/denchmark-link&gt;
, do you have a sense of how easy it would be to plumb the appropriate information through here? (I'm guessing that we need to add some reverse map from device contexts (or some other object that we get in a CUPTI callback) back to TensorFlow devices....)
The variability in runtime appears to be caused by (usually)  issuing its first kernel a long time after  starts issuing kernels... and sometimes  doesn't get started until  is finished (and has sync'd its stream). This suggests to me that there's some mutex (in the stream executor? the GPU driver?) being held, or a single worker thread is running a blocking operation and prevent the other device from issuing kernels. I'm not sure how likely either of these scenarios is... &lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
, do you have any ideas?
		</comment>
		<comment id='2' author='raphtown' date='2016-11-09T18:18:26Z'>
		&lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;
 and I had a look at some of these traces this morning.  There definitely seems to be a problem here, and I'm not sure when it crept in.
First, &lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;
 is correct that the gpu tracer in the open source tree is hardwired to report  - this should be a simple fix, which can probably be done at the same time that GPU tracing is enabled for the distributed runtime.
Getting back to the bug - there does appear to be some unexpected and undesirable serialization of op dispatch on the GPU devices here (e.g. the dispatch of the Conv3D ops on different GPU devices do not appear to run concurrently!).  I'm not 100% sure what is going on, but I do know that there were some changes to the threadpool usage for GPU op dispatch a while ago (to deliberately avoid using multiple threads to enqueue ops on the same GPU stream).
Also, in the build you are using, the _SINK NoOp (which is the last op to run on each device) calls StreamExecutor::SynchronizeAllActivity to ensure that all work is done before turning off the GPU tracer.  Looking at the implementation, this does actually appear to a) synchronize the CUDA Context and b) calls BlockOnThreadExecutor(background_threads_.get()); on an internal threadpool.
In later builds the position of this call to SyncAll() had been moved slightly - but I don't think this would affect the behaviour (it would make the Timeline look slightly different - in that the _SINK NoOp would not appear to take as long)
It might be worth trying to explicitly increase the inter-op-parallalelism, (in your ConfigProto) but I suspect the problem is more subtle than this....
What concerns me is that one of your traces has all the ops on GPU1 executing after all the ops on GPU0 have completed.  In this case there is actually a dependency of the first GPU1 Conv3D on a 'read' of a Variable which is assigned to GPU0.  This will actually require a device to device DMA, and involves multiple GPU streams and some CUDA Event synchronization.  None of this is currently visible in the Timeline (and in fact I don't even see a MEMCPYD2D!)   This worries me a little (and I really think that we really need to change the instrumentation for Send/Recv ops so that they are more visible in the Timeline!)
		</comment>
		<comment id='3' author='raphtown' date='2016-11-09T18:59:23Z'>
		
Thanks for making such a detailed bug report, @raphtown!

+1     It's always a pleasure to have a clear bug report with simple repro code.
		</comment>
		<comment id='4' author='raphtown' date='2016-11-10T00:19:05Z'>
		Hello &lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/prb12&gt;@prb12&lt;/denchmark-link&gt;
, and thank you for taking a look into this!
Varying the inter_op_parallelism_threads field in the ConfigProto doesn't seem to do much, except when I set it very high (e.g. 1000) and then more of the runs execute sequentially.
Maybe this will help you debug the issue, but it seems like the number of CPU cores available has a direct impact on if the GPUs execute sequentially or in parallel (I am using the SLURM allocation system which lets me specify how many CPUs to give).  The results I gave in my original bug report were with 2 CPUs available total.  If I increase this amount to 4 CPUs, then the issue disappears.  If I decrease the amount to 1 CPU, then it seems that all runs execute sequentially (worsening the problem).
Lastly, I also tested this with 0.9.0, which displayed similar issues.
		</comment>
		<comment id='5' author='raphtown' date='2016-11-10T00:23:32Z'>
		Also, yes, I would definitely love to have better instrumentation of the Send/Recv ops as well as GPU tracing for distributed tensorflow (which is my next step once I get the single-node case working)!
		</comment>
		<comment id='6' author='raphtown' date='2016-11-10T00:44:58Z'>
		
Maybe this will help you debug the issue, but it seems like the number of CPU cores available has a direct impact on if the GPUs execute sequentially or in parallel (I am using the SLURM allocation system which lets me specify how many CPUs to give). The results I gave in my original bug report were with 2 CPUs available total. If I increase this amount to 4 CPUs, then the issue disappears. If I decrease the amount to 1 CPU, then it seems that all runs execute sequentially (worsening the problem).

I'm glad you mentioned this.  We were looking at your traces earlier today and very bemused as to why the threadpool dispatching GPU ops appeared to be behaving as though there was only 1-2 threads!  (somebody apparently removed the log message which tells us how many worker threads are configured in each threadpool - but it defaults to the number of "physical" cores.)
I can totally understand how this problem occurs with only a couple of threads.  Your workload is very sensitive to the order of op dispatch (due to the way that GPU to GPU memcpys are currently implemented).  If there are only a tiny number of threads available then the executor for one GPU device manages to get all of its ops enqueued on the GPU stream before the other even gets a look in.  The D2D memcpys actually include a stream synchronization - so the destination device of the transfer ends up waiting for all of the previously dispatched ops on the other GPU.
The exact behavior is non-deterministic, but it takes very little time to enqueue a handful of ops on GPU0, and if all threadpool workers are busy doing anything else for even a millisecond then it's game over for GPU1!
I think we probably need to improve the D2D transfer code path, but for now I would recommend using at least 4 or 8 cores in a multi-gpu setting (and probably more!)
		</comment>
		<comment id='7' author='raphtown' date='2016-11-10T01:30:30Z'>
		Hello &lt;denchmark-link:https://github.com/prb12&gt;@prb12&lt;/denchmark-link&gt;
, I think I see what you are saying.  When we enqueue GPU1's read of a convolution layer's weights (in this case, from GPU0), it must wait for all of GPU0's previously enqueued ops to finish.
I can see even in my "parallel" cases (such as the first screenshot above) that some of the later GPU1 reads are delayed and wait for GPU0 to finish a couple conv layers, but this is not an issue because GPU1 has enough computation to do in the mean time.
The nodes I am using have 16 CPUs for 8 GPUs, so I assume this problem will not completely go away but will most likely be minimized for now.
Thank you for being so responsive!
		</comment>
		<comment id='8' author='raphtown' date='2016-11-10T04:05:16Z'>
		
The nodes I am using have 16 CPUs for 8 GPUs, so I assume this problem will not completely go away but will most likely be minimized for now.

Yes - you are right.
The only way to get nice parallelism is if all of the Variable read and Send ops on GPU 0 and the Recv ops on GPU1 issue before any of the Conv ops start getting enqueued on GPU0.  This is a bit of a lottery currently!    (Alternatively, if you keep the Variables on CPU then you probably won't have this issue, but will incur the cost of a separate DMA to each GPU per step)
&lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
 and I talked about a possible workaround this morning.... but it would require recording a CUDA event for each of the Send ops on the source GPU so that the dest GPU could wait for the appropriate point in the execution stream.  Previously we've considered this too expensive.
		</comment>
		<comment id='9' author='raphtown' date='2016-11-13T04:48:14Z'>
		It does indeed seem like a tricky problem to get the reads all synced up properly.  In any case, after running some tests, it appears the problem is minimized for my 16 CPU / 8 GPU setup.  Assuming there is no further follow-up needed on the tensorflow team's end, I would be fine with closing this issue for now.
Thanks for the help :)
		</comment>
		<comment id='10' author='raphtown' date='2016-11-13T05:32:36Z'>
		Do you see any difference if you keep the variables in CPU?
On Saturday, November 12, 2016, raphtown &lt;denchmark-link:mailto:notifications@github.com&gt;notifications@github.com&lt;/denchmark-link&gt;
 wrote:

It does indeed seem like a tricky problem to get the reads all synced up
properly. In any case, after running some tests, it appears the problem is
minimized for my 16 CPU / 8 GPU setup. Assuming there is no further
follow-up needed on the tensorflow team's end, I would be fine with closing
this issue for now.
Thanks for the help :)
—
You are receiving this because you are subscribed to this thread.
Reply to this email directly, view it on GitHub
#5416 (comment),
or mute the thread
https://github.com/notifications/unsubscribe-auth/ABr0fNLPkiaX67Ck3_qUmN2ar_x4Reevks5q9padgaJpZM4KqXs2
.

&lt;denchmark-h:h2&gt;&lt;/denchmark-h&gt;

Sergio
		</comment>
		<comment id='11' author='raphtown' date='2017-05-17T06:58:33Z'>
		Is there an issue tracking the bug in the tracing code that incorrect reports other GPUs under /gpu:0? this can be quite misleading for people looking at profiling results
on the  branch this looks to be a problem still &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_tracer.cc#L566&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_tracer.cc#L566&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='raphtown' date='2017-06-16T22:21:33Z'>
		&lt;denchmark-link:https://github.com/mortada&gt;@mortada&lt;/denchmark-link&gt;
 I am closing this based on the original bug.  For the incorrect reports of GPUs issue please open another github issue.  That will make it easier to track and restart the thread.  Thank you.
		</comment>
	</comments>
</bug>