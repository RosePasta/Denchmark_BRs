<bug id='24826' author='777ki' open_date='2019-01-10T12:13:06Z' closed_time='2019-01-29T00:07:06Z'>
	<summary>tf.estimator rebuild graph at per training step</summary>
	<description>
hello everyone:
i'm working on reading source code of tf.estimator, I found a little problem of this implementation，
  def _train_model_default(self, input_fn, hooks, saving_listeners):
    """Initiate training with `input_fn`, without `DistributionStrategies`.
    Args:
      input_fn: A function that provides input data for training as minibatches.
      hooks: List of `tf.train.SessionRunHook` subclass instances. Used for
        callbacks inside the training loop.
      saving_listeners: list of `tf.train.CheckpointSaverListener` objects. Used
        for callbacks that run immediately before or after checkpoint savings.

    Returns:
      Loss from training
    """
    worker_hooks = []
    with ops.Graph().as_default() as g, g.device(self._device_fn):
      random_seed.set_random_seed(self._config.tf_random_seed)
      global_step_tensor = self._create_and_assert_global_step(g)

      # Skip creating a read variable if _create_and_assert_global_step
      # returns None (e.g. tf.contrib.estimator.SavedModelEstimator).
      if global_step_tensor is not None:
        training_util._get_or_create_global_step_read(g)  # pylint: disable=protected-access
      
      features, labels, input_hooks = (
          self._get_features_and_labels_from_input_fn(
              input_fn, model_fn_lib.ModeKeys.TRAIN))
      worker_hooks.extend(input_hooks)
    
      estimator_spec = self._call_model_fn(
          features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
      global_step_tensor = training_util.get_global_step(g)
      return self._train_with_estimator_spec(estimator_spec, worker_hooks,
                                             hooks, global_step_tensor,
                                             saving_listeners)
this code is part of Estimator.training, look at this
 estimator_spec = self._call_model_fn(
          features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
it will be called at every training step, that is a build graph operation, i think is called one time at the begining is better, am I right?
	</description>
	<comments>
		<comment id='1' author='777ki' date='2019-01-29T00:07:06Z'>
		The code you are referring to is called only once everytime you call  to build the graph, and not per training step. the loop over training steps is here:
&lt;denchmark-link:https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/estimator.py#L1402&gt;https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/estimator.py#L1402&lt;/denchmark-link&gt;

Also, stackoverflow might be a better place for a clarification question like this unless you have seen this is actually a bug in practice.
		</comment>
		<comment id='2' author='777ki' date='2019-01-31T08:01:11Z'>
		
The code you are referring to is called only once everytime you call estimator.train to build the graph, and not per training step. the loop over training steps is here:
https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/estimator.py#L1402
Also, stackoverflow might be a better place for a clarification question like this unless you have seen this is actually a bug in practice.

but, this function is called after :
estimator_spec = self._call_model_fn(
that  _call_model_fn is  at the front of  _train_with_estimator_spec fucntion:
&lt;denchmark-link:https://github.com/tensorflow/estimator/blob/781389ebd8dc87c71711f432e2eae135917429bd/tensorflow_estimator/python/estimator/estimator.py#L1154&gt;https://github.com/tensorflow/estimator/blob/781389ebd8dc87c71711f432e2eae135917429bd/tensorflow_estimator/python/estimator/estimator.py#L1154&lt;/denchmark-link&gt;

so, every estimator.train will call both _call_model_fn and _train_with_estimator_spec at per training step
		</comment>
		<comment id='3' author='777ki' date='2019-01-31T08:07:10Z'>
		so, i suggest that : move _call_model_fn to initialize function to build graph, if it still in train function yet, that way  may be  like "copy on write " or "init on first start", but, may be someone like me , we think that train function it can be called in dynamic（dynamic means i can change my input function in every train step， i will call train function for several times)
		</comment>
	</comments>
</bug>