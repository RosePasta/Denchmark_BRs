<bug id='4128' author='Hvass-Labs' open_date='2016-08-31T15:22:50Z' closed_time='2016-09-13T01:24:29Z'>
	<summary>Docs for Inception Model</summary>
	<description>
TensorFlow 0.10.0rc0 CPU on Linux.
This is about the Inception model and example that is included with TensorFlow:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/imagenet/classify_image.py&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/imagenet/classify_image.py&lt;/denchmark-link&gt;

I am using this version of the Inception model:
&lt;denchmark-link:http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz&gt;http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz&lt;/denchmark-link&gt;

I'm not sure whether to ask these questions here or on StackOverflow, but there are some related questions there that haven't been answered, and this is also sort of a request for improving the TensorFlow docs, so I hope it's OK that I ask the questions here.
My questions are:
(1) The softmax classifier apparently outputs an array of length 1008 - but there is only 1000 classes in the data-set. Why is there a difference? How should I adjust for this difference?
(2) What is the size of the input JPEG-images supposed to be, and is the Inception model rescaling them automatically?
(3) I get a deprecation warning. I don't know what is causing this. Does it mean that this Inception model will stop working in the future?

/home/foo/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py:1811: VisibleDeprecationWarning: converting an array with ndim &gt; 0 to an index will result in an error in the future
result_shape.insert(dim, 1)
W tensorflow/core/framework/op_def_util.cc:332] Op BatchNormWithGlobalNormalization is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().

(4) The sample code uses tf.gfile.Exists() which just wraps os.path.exists() so it seems to be completely redundant. Why is it used?
(5) The sample code also uses tf.gfile.FastGFile() which seems to be the non-thread-safe version of TensorFlow's file-class. Why not use Python's built-in open() instead?
(6) The function run_inference_on_image() creates a new TensorFlow session for each image that is being processed. Is the TensorFlow session so lightweight that this is the preferred way of doing it? Or is it better to create a single session and use it repeatedly?
Thanks.
	</description>
	<comments>
		<comment id='1' author='Hvass-Labs' date='2016-08-31T21:23:34Z'>
		(1) I don't know.
(2) See classify_image.py, the images are automatically resized to 299 x 299, the size expected.
(3) Yes, the model will need to be upgraded at some point or it will fail.
(4&amp;5) Portability &amp; future-proofness
(6) This is done to make the example simple.  If you really want to do inference on a lot of images, it is better to create a single session and use it repeatedly.
		</comment>
		<comment id='2' author='Hvass-Labs' date='2016-09-01T11:12:16Z'>
		Thanks for the quick answer.
(1) Just to clarify the question a bit. After having loaded the graph in my own code, if I print the softmax-tensor then the shape is (1, 1008):
&lt;denchmark-code&gt;print(graph.get_tensor_by_name('softmax:0'))
&lt;/denchmark-code&gt;


Tensor("softmax:0", shape=(1, 1008), dtype=float32)

But in the file imagenet_2012_challenge_label_map_proto.pbtxt the class-numbers range from 1 to 1000 (both inclusive). So there's apparently only 1000 possible classes in the data-set, but the softmax has 1008 output-classes. It seems strange to me and it must mean that we should ignore 8 of the classes from the softmax-ouput - but which ones?
(2) I can't seem to find the image resizing in classify_image.py so I wonder if the resizing is done implicitly inside the TensorFlow graph that is being restored? It's also not clear what happens to the images; are they resized or cropped? using what algorithm? what if they're too small, are they then just padded?
(3) Would it be possible to always have an Inception model compatible with the latest TensorFlow version and stored under e.g.:
&lt;denchmark-link:http://download.tensorflow.org/models/image/imagenet/inception-latest.tgz&gt;http://download.tensorflow.org/models/image/imagenet/inception-latest.tgz&lt;/denchmark-link&gt;

Then I won't need to make any changes to my code when it is updated, as it will just be downloaded automatically. Or what would be the best way of doing this?
Thanks again.
		</comment>
		<comment id='3' author='Hvass-Labs' date='2016-09-01T16:56:09Z'>
		(1) It turns out that the 1008 size softmax output is an artifact of dimension back-compatibility with a older, Google-internal system.  Newer versions of the inception model have 1001 output classes, where one is an "other" class used in training.  You shouldn't need to pay any attention to the extra 8 outputs.
(2) Oops.  The public version of  &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/imagenet/classify_image.py&gt;classify_image.py&lt;/denchmark-link&gt;
 doesn't include an explicit image resize, nor the dimensions.  I guess someone thought it would make a better tutorial to hide that stuff inside the graph.  If you look at &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/main.cc&gt;label_image/main.cc&lt;/denchmark-link&gt;
 used in the C++ API example, you can see the explicit jpg decoding and resizing.  If you dump the graph protobuf in text format (huge!) you can see nodes at the top of the graph doing the same thing.
(3)  Acknowledged, but this might be a community support issue.  We're pretty busy right now.
		</comment>
		<comment id='4' author='Hvass-Labs' date='2016-09-01T16:59:39Z'>
		See &lt;denchmark-link:https://research.googleblog.com/2016/08/improving-inception-and-image.html&gt;https://research.googleblog.com/2016/08/improving-inception-and-image.html&lt;/denchmark-link&gt;
 for the latest imagenet model.
		</comment>
		<comment id='5' author='Hvass-Labs' date='2016-09-05T11:05:53Z'>
		Thanks again for the quick answer!
&lt;denchmark-h:h2&gt;(1)&lt;/denchmark-h&gt;

I suppose that when the Inception model outputs a class-number of zero it really means "other" and class-numbers between 1001-1007 are for back-compatibility. But I think the code breaks if the top-5 includes one of these special classes, because there is no mapping from these special class-numbers to ImageNet uid's in the file imagenet_2012_challenge_label_map_proto.pbtxt I think a comment in the code would be good.
&lt;denchmark-h:h2&gt;(2)&lt;/denchmark-h&gt;

Actually it is much more confusing to omit the information on image rescaling; the file classify_image.py doesn't even mention that images are rescaled to 299x299 pixels. I looked in the C++ file you linked to and it appears to be using ResizeBilinear(), which I can't really find in the source-code, but I guess it's bilinear interpolation. But it's still not clear whether the images are squeezed, cropped or padded if they're not exactly square. I tried doing the resizing manually in an image editor and using these images as input to the Inception model. With this image the Inception model does pretty well in most variants, but you can easily imagine that if e.g. a very wide or very high image is squeezed then the Inception model will no longer perform as well. Similarly with cropping where e.g. an essential part of the image is removed. Perhaps the best solution would be to pad the image so it's square and then resize?
&lt;denchmark-h:h3&gt;Original image&lt;/denchmark-h&gt;

&lt;denchmark-link:https://cloud.githubusercontent.com/assets/13588114/18245844/e5765950-7367-11e6-83cc-2f9b378125b4.jpg&gt;&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Squeezed image, Inception result: Macaw 97.0%&lt;/denchmark-h&gt;

&lt;denchmark-link:https://cloud.githubusercontent.com/assets/13588114/18245414/5f6c6252-7365-11e6-9cf5-aa20565379bf.jpg&gt;&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Cropped image version 1, Inception result: Macaw 97.4%&lt;/denchmark-h&gt;

&lt;denchmark-link:https://cloud.githubusercontent.com/assets/13588114/18245415/5f7460e2-7365-11e6-9f33-35e8143ba7e2.jpg&gt;&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Cropped image version 2, Inception result: Macaw 93.9%&lt;/denchmark-h&gt;

&lt;denchmark-link:https://cloud.githubusercontent.com/assets/13588114/18245416/5f76646e-7365-11e6-9104-019fa5c7ca74.jpg&gt;&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Cropped image version 3, Inception result: Jacamar (another bird) 26.1%, grasshopper 10.6%&lt;/denchmark-h&gt;

&lt;denchmark-link:https://cloud.githubusercontent.com/assets/13588114/18245418/5f799472-7365-11e6-8f26-39a0d07511d8.jpg&gt;&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Padded image, Inception result: Macaw 96.8%&lt;/denchmark-h&gt;

&lt;denchmark-link:https://cloud.githubusercontent.com/assets/13588114/18245417/5f7821fa-7365-11e6-8245-35833b23851d.jpg&gt;&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;(3 + more)&lt;/denchmark-h&gt;

Thanks for the tip on the new Inception models. However, they're not reloaded in the same way as the Inception tutorial. It appears that you have to import the .py files, create a graph using those functions, and then reload the checkpoints in the tar-files. The Inception tutorial uses a frozen graph instead.
I think it would be great if you had a standardized way of creating neural networks from a model-zoo. This ought to be a part of the TensorFlow API. The API could look something like this:
&lt;denchmark-code&gt;model3 = tf.models.Inception3(auto_download=True)

model4 = tf.models.Inception4(auto_download=True)
&lt;/denchmark-code&gt;

These functions would automatically download a graph and checkpoint-file for each Inception model that was working with the user's TensorFlow version (or perhaps just the latest TF version), and then return a model-object which inherits from class ModelZoo and provides the following functions:
&lt;denchmark-code&gt;model3.get_graph()  # Returns the graph for the neural network.
model3.get_placeholder_name()  # Name of placeholder-variable for inputting image.
model3.get_softmax()  # Return tensor for softmax-classifier.
model3.get_last_layer()  # Return tensor for last layer for re-training.
model3.get_class_names(class_numbers)  # Return names of the given classes.
model3.inference(session=None, images)  # Perform inference on the list of images.
                                        # Create new session if None is supplied.
model3.print_scores(predictions)  # Print scores and class-names for predicted classes.
&lt;/denchmark-code&gt;

This would allow users to do inference with only a few lines of code:
&lt;denchmark-code&gt;model = tf.models.Inception3(auto_download=True)
predicted_labels = model.inference(images=my_images)
&lt;/denchmark-code&gt;

Perhaps this is what you want to do with the Slim library? I've looked at the following Notebook which is mostly well-documented (yay, finally some TensorFlow code with lots of comments! The author is apparently &lt;denchmark-link:https://github.com/nathansilberman&gt;@nathansilberman&lt;/denchmark-link&gt;
):
&lt;denchmark-link:https://github.com/tensorflow/models/blob/master/slim/slim_walkthough.ipynb&gt;https://github.com/tensorflow/models/blob/master/slim/slim_walkthough.ipynb&lt;/denchmark-link&gt;

However, it is still quite complicated and I think a model zoo can be made even simpler with an API such as the above.
		</comment>
		<comment id='6' author='Hvass-Labs' date='2016-09-07T00:01:29Z'>
		&lt;denchmark-link:https://github.com/sherrym&gt;@sherrym&lt;/denchmark-link&gt;
 do you have some feedback on the suggestions from &lt;denchmark-link:https://github.com/Hvass-Labs&gt;@Hvass-Labs&lt;/denchmark-link&gt;
, or suggestions on who might be appropriate?  I'm triaging issues this week, and this is beyond my area of expertise.  Thanks!
		</comment>
		<comment id='7' author='Hvass-Labs' date='2016-09-08T14:40:01Z'>
		I have finished my new tutorial on how to use the pre-trained Inception Model and I have two suggestions to the TensorFlow developers:
(A) The Inception v3 model seems to have problems recognizing people. I give several examples of this in the tutorial (link below). This would seem to make the pre-trained Inception model useless in practice. I'm not sure, but I think the problem is maybe in the training-set you're using. Please consider training the Inception model on a data-set that is more relevant. Instead of having 1000 classes, maybe only have 100 classes that it could recognize more accurately and that were more relevant?
(B) As mentioned above, I think the TensorFlow API for using pre-trained "Zoo" models could be much simpler. The current methods for loading pre-trained models are rather convoluted. If users could just do something like the following to load a good pre-trained model, then you might see a multitude of new interesting Android apps using image recognition:
&lt;denchmark-code&gt;import tensorflow as tf
model = tf.models.Inception(auto_download=True)
predicted_labels, predicted_scores = model.classify(image="foo.jpg")
&lt;/denchmark-code&gt;

Here's my new tutorial on the Inception Model as a Notebook and YouTube talk:
&lt;denchmark-link:https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/07_Inception_Model.ipynb&gt;https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/07_Inception_Model.ipynb&lt;/denchmark-link&gt;

&lt;denchmark-link:https://www.youtube.com/watch?v=ZG_hoLgNFNo&gt;https://www.youtube.com/watch?v=ZG_hoLgNFNo&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='Hvass-Labs' date='2016-09-08T18:03:33Z'>
		(A) The training set for inception is from the ImageNet challenge &lt;denchmark-link:http://image-net.org/download-faq&gt;http://image-net.org/download-faq&lt;/denchmark-link&gt;
. It's not an arbitrary set of training images.
(B) Would you like to submit a pull request? Thanks.
		</comment>
		<comment id='9' author='Hvass-Labs' date='2016-09-09T07:07:32Z'>
		Thanks for the quick reply.
(A) As I understand, ImageNet is a data-set used in academic competitions that has been around for several years now. I understand completely that it's a huge achievement to get so good scores on ImageNet. But fact is that the Inception v3 model you include with TensorFlow gives bizarre results when using it on some real-world images that it ought to be able to recognize. I assume the Inception models released last week give equally bizarre results because they were apparently trained on the same ImageNet data-set.
In my own tutorial that I linked to above, the Inception v3 model classified an image of Elon Musk (299x299 pixels) as maybe being a sweatshirt (20% score) or an abaya (17% score). If I resized the image to 100x100 pixels then Inception said it was maybe a sweatshirt (18%) or a cowboy-boot (16%). Inception said that Gene Wilder portraying Willy Wonka was a bow-tie (97%), and that Johnny Depp portraying Willy Wonka was maybe sunglasses (31%) or sunglass (19%), that is, two very similar class-names.
I don't know what the problem is. Is it because the ImageNet-classes really belong to a hierarchy, so that sweatshirt is really a sub-class of person, and the same with bow-tie and sunglasses? But that's not something that can be deduced from the data-files with class-names that you have included with the Inception model.
I would have invited the main author of the Inception model Christian Szegedy to comment on this, but I can't find his GitHub-handle.
(B) I would love to make a pull-request for this - it would be an honour to make such an important contribution to TensorFlow which is quickly becoming the leading AI library! But I'm a beginner in all this and I still only understand a fraction of TensorFlow, so I think it's better for me to just issue a 'feature-request' instead :-) So please don't get annoyed by this, in the end my questions and suggestions might help improve TensorFlow.
At the moment, I try to learn the key points about TensorFlow and make tutorials aimed at beginners in both Deep Learning and TensorFlow - there's apparently a lot of us that found the Udacity course rather confusing.
		</comment>
		<comment id='10' author='Hvass-Labs' date='2016-09-12T12:52:55Z'>
		I found out that the problem with the strange labels predicted by the Inception model is indeed the classes and their names in the ImageNet data-set. This can be seen from e.g. the "bow-tie" label which causes the Inception model to classify people wearing bow-ties as "bow-tie" rather than e.g. "man wearing bow-tie"
&lt;denchmark-link:http://image-net.org/synset?wnid=n02883205#&gt;http://image-net.org/synset?wnid=n02883205#&lt;/denchmark-link&gt;

One solution would be to use the neural networks for generating captions, e.g. "Man wearing tall hat, bow-tie and purple jacket." But I haven't gotten around to caption-generation yet and I suspect it's quite a bit more complicated to implement.
I would still argue that ImageNet's classes are not completely meaningful, and perhaps the team that made the Inception model could make a much better data-set from the vast amounts of data that google has available, so the Inception model would work much better out-of-the-box, without the need for re-training or more complicated caption-generation techniques.
As mentioned above, the Inception model thinks that a picture of Elon Musk shows either a sweat-shirt (20% score) or an abaya (17% score). If you change the resolution of the image, then Inception thinks it shows either a sweat-shirt or a cowboy-boot. If you ask people on the street what they think of the state-of-the-art in AI and image recognition developed by the world's leading scientists and engineers at google, and it gives results like this, well ... people are going to be amused :-)
Just a few thoughts.
		</comment>
		<comment id='11' author='Hvass-Labs' date='2016-09-13T01:24:29Z'>
		The ImageNet dataset is a benchmark dataset that's been widely and successfully used for research. Inception is a model that does fairly well on this dataset and we published the code so others can build on it and improve it. As always in Machine Learning, Inception has its limitations when it comes to generalizing beyond the type of data it was trained on.
We never implied that Inception, trained on imagenet, is suited for any particular purpose (for example, recognizing images outside the categories trained on), or that its design is suitable for production environments with concerns such as oblong images, or requirements on the specific description strings.
We have released the code to train Inception in order to make it possible for people who need a network with such capabilities to train their own, and to modify the network or its preprocessing pipeline to meet their needs. This is not something that a complete beginner in machine learning should attempt, although it has been done. Users generally find the pretrained and retrainable version rather useful since it offers a way to train a custom model much faster.
I will close this issue -- if there a concrete things that can be improved in the documentation, or the model itself, PRs are welcome.
		</comment>
		<comment id='12' author='Hvass-Labs' date='2016-09-13T09:24:38Z'>
		Thanks again for the comments, I appreciate it. This thread may answer questions that others might have in the future so I think it has been a useful discussion.
		</comment>
		<comment id='13' author='Hvass-Labs' date='2016-09-13T14:32:47Z'>
		I agree. It is clearly important to set the expectations for a research
model such as this one.
On Tue, Sep 13, 2016 at 02:26 Hvass-Labs &lt;denchmark-link:mailto:notifications@github.com&gt;notifications@github.com&lt;/denchmark-link&gt;
 wrote:

Thanks again for the comments, I appreciate it. This thread may answer
questions that others might have in the future so I think it has been a
useful discussion.
—
You are receiving this because you modified the open/close state.
Reply to this email directly, view it on GitHub
#4128 (comment),
or mute the thread
https://github.com/notifications/unsubscribe-auth/AAjO_ZggnyOl4Y-OukITzZ7GxNlGvv9Oks5qpmxKgaJpZM4JxvOM
.

		</comment>
		<comment id='14' author='Hvass-Labs' date='2017-06-16T11:24:31Z'>
		&lt;denchmark-link:https://github.com/Hvass-Labs&gt;@Hvass-Labs&lt;/denchmark-link&gt;
 if u look at the 1000 classes used in the ImageNet dataset you will see that their are no tags related to the words - 'human', 'man', 'person'. So it is not surprising that it can't tell you that the image contains a human.
		</comment>
	</comments>
</bug>