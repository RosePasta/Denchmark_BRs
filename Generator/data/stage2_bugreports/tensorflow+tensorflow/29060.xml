<bug id='29060' author='apls777' open_date='2019-05-27T14:33:15Z' closed_time='2019-06-11T22:28:20Z'>
	<summary>"Cache iterator is in an invalid state" error</summary>
	<description>
System information

OS Platform and Distribution: macOS High Sierra 10.13.6
TensorFlow for CPU installed from PyPI
TensorFlow version: v1.13.0-rc2-5-g6612da8951, 1.13.1
Python version: 3.6.6

Describe the current behavior
Minimal not working example:
import tensorflow as tf
from tensorflow.python.framework.errors_impl import OutOfRangeError


dataset = tf.data.Dataset.range(10)
dataset = dataset.cache('cache1')
dataset = dataset.map(lambda a: a)
dataset = dataset.batch(4)

batch = dataset.make_one_shot_iterator().get_next()

with tf.Session() as sess:
    while True:
        try:
            res = sess.run(batch)
            print(res)
        except OutOfRangeError:
            print('out-of-range')
            break
The code above properly iterates through the dataset only the first run when a cache doesn't exist. But when it loads the dataset from the cache file it crashes with an error:
&lt;denchmark-code&gt;tensorflow.python.framework.errors_impl.InternalError: Cache iterator is in an invalid state. (Perhaps GetNext called after end_of_sequence?)
	 [[{{node IteratorGetNext}}]
&lt;/denchmark-code&gt;

A workaround
It happens because the map operation follows right after the cache. It starts working as expected if some other dataset operation is added between cache and map steps. For example:
...

dataset = tf.data.Dataset.range(10)
dataset = dataset.cache('cache1')
dataset = dataset.filter(lambda x: True)  # a fake filter is added
dataset = dataset.map(lambda a: a)
dataset = dataset.batch(4)

...
	</description>
	<comments>
		<comment id='1' author='apls777' date='2019-05-28T09:27:10Z'>
		&lt;denchmark-link:https://github.com/apls777&gt;@apls777&lt;/denchmark-link&gt;
 I have executed the first part of the code in TF 1.12.0, did not receive any error. Please try and let us know how it progresses. Thanks!
		</comment>
		<comment id='2' author='apls777' date='2019-05-28T10:04:23Z'>
		&lt;denchmark-link:https://github.com/muddham&gt;@muddham&lt;/denchmark-link&gt;
 It works in TF 1.12.0. Just changed the filename to  instead of , otherwise, it throws an error for the first run:
&lt;denchmark-code&gt;tensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory
	 [[{{node IteratorGetNext}} = IteratorGetNext[output_shapes=[[?]], output_types=[DT_INT64], _device="/job:localhost/replica:0/task:0/device:CPU:0"](OneShotIterator)]]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='apls777' date='2019-05-28T10:38:00Z'>
		&lt;denchmark-link:https://github.com/apls777&gt;@apls777&lt;/denchmark-link&gt;
 As it is working now, are you happy for this issue to be closed?
		</comment>
		<comment id='4' author='apls777' date='2019-05-28T10:51:15Z'>
		&lt;denchmark-link:https://github.com/muddham&gt;@muddham&lt;/denchmark-link&gt;
 No, clearly itâ€™s a bug that should be fixed in the latest stable version.
		</comment>
		<comment id='5' author='apls777' date='2019-05-30T11:16:29Z'>
		&lt;denchmark-link:https://github.com/apls777&gt;@apls777&lt;/denchmark-link&gt;
 I ran the code in TF 1.13 GPU the output I got was below.
[0 1 2 3]
[4 5 6 7]
[8 9]
out-of-range
In TF 1.13 CPU I got the below error.
AttributeError: 'BatchDataset' object has no attribute 'make_one_shot_iterator'
		</comment>
		<comment id='6' author='apls777' date='2019-05-30T12:20:58Z'>
		&lt;denchmark-link:https://github.com/muddham&gt;@muddham&lt;/denchmark-link&gt;
 Did you run the code twice to load the dataset from a cache? Are you sure you're running it on TF 1.13 ? What version of Python do you use?
		</comment>
		<comment id='7' author='apls777' date='2019-06-06T11:59:44Z'>
		
@muddham Did you run the code twice to load the dataset from a cache? Are you sure you're running it on TF 1.13 .1? What version of Python do you use?

I am able to reproduce the issue with TF 1.13.1 and Python 3.6.7.
		</comment>
		<comment id='8' author='apls777' date='2019-06-11T04:51:18Z'>
		&lt;denchmark-link:https://github.com/apls777&gt;@apls777&lt;/denchmark-link&gt;
 thank you for reporting the issue and providing a minimal reproducible example.
I can confirm this is an issue. The problem is that the existing cache transformation produces an error if the dataset transformation that consumes its input asks for an input after the cache transformations has reached the end of its input. This happens in your repro because  will get fused into  which will ask for  worth of elements at once. Because the input cardinality (10) is not divisible evenly by the batch size (4), the invalid cache op kernel behavior is triggered. Inserting a transformation between  and  will prevent the fusion from happening (and so does disabling the fusion through tf.data &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/data/experimental/OptimizationOptions#map_and_batch_fusion&gt;options&lt;/denchmark-link&gt;
.
I have a fix for this and expect it to merged to master by the end of this week.
		</comment>
		<comment id='9' author='apls777' date='2019-06-11T22:28:21Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=29060&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=29060&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='apls777' date='2019-06-25T16:32:51Z'>
		&lt;denchmark-link:https://github.com/apls777&gt;@apls777&lt;/denchmark-link&gt;

I'm running into this issue when caching before interleave
tf 2.0.0-beta1 and python 3.6.8
    filenames_dataset = filenames_dataset.cache("./some_path")

    return filenames_dataset.interleave(
        lambda f: map_file_to_xy_dataset(f, predict_task_fn, params),
        cycle_length=params[CYCLE_LENGTH_KEY],
        block_length=params[BLOCK_LENGTH_KEY],
        num_parallel_calls=tf.data.experimental.AUTOTUNE)
&lt;denchmark-code&gt;Iterator "Iterator::Model::Prefetch::Batch::Shuffle::ParallelInterleaveV2" returned OutOfRange without setting `*end_of_sequence`. This indicates that an error may have occurred. Original message: Attempting to call get_next after iteration should have finished. [Op:IteratorGetNextSync]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='11' author='apls777' date='2019-06-25T17:18:14Z'>
		&lt;denchmark-link:https://github.com/devstein&gt;@devstein&lt;/denchmark-link&gt;
 thank you for reporting the problem. This is a different issue, so please create a new issue for it and reference it here. Thank you.
		</comment>
		<comment id='12' author='apls777' date='2019-06-25T17:19:49Z'>
		&lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
 Will do!
		</comment>
	</comments>
</bug>