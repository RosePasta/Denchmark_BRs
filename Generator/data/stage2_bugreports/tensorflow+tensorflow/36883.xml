<bug id='36883' author='ankitesh97' open_date='2020-02-19T08:34:58Z' closed_time='2020-03-10T23:29:50Z'>
	<summary>Not able to load a saved model with custom layer</summary>
	<description>
Please make sure that this is a bug. As per our
System information - Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): - OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): - Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: - TensorFlow installed from (source or
binary): - TensorFlow version (use command below): - Python version: - Bazel
version (if compiling from source): - GCC/Compiler version (if compiling from
source): - CUDA/cuDNN version: - GPU model and memory:
OS - CentOs
tensorflow-gpu = 2.1.0
tensorflow = 2.0.0
installed in the conda version - conda version : 4.8.1
using cuda - CUDA Version: 10.1
Gpu - Tesla K80
python - 3.6
You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
I have created a custom Layer and trained the model using that layer, but when I load the model back, it is not able to load all the weight values.
Describe the expected behavior
It should be able to load the weight values.
Code to reproduce the issue Provide a reproducible test case that is the
bare minimum necessary to generate the problem.
from __future__ import absolute_import, division, print_function, unicode_literals
from tensorflow.keras import initializers
from tensorflow.keras import layers
from tensorflow.keras.models import load_model
from tensorflow.keras.layers import Input, Dense
from tensorflow.python.keras import backend as K
from tensorflow.python.keras.utils import tf_utils
from tensorflow.keras.layers import LeakyReLU
import tensorflow as tf
import tensorflow as tf
import tensorflow.keras
from tensorflow.keras.callbacks import *
from tensorflow.keras.losses import mse

### create a custom layer

class CustomBatchNormalization(layers.Layer):
    def __init__(self, momentum=0.99, epsilon=1e-3,beta_initializer='zeros',
                 gamma_initializer='ones', moving_mean_initializer='zeros',
                 moving_range_initializer='ones',**kwargs):
        super(CustomBatchNormalization, self).__init__(**kwargs)
        self.supports_masking = True
        self.momentum = momentum
        self.epsilon = epsilon
        self.beta_initializer = initializers.get(beta_initializer)
        self.gamma_initializer = initializers.get(gamma_initializer)
        self.moving_mean_initializer = initializers.get(moving_mean_initializer)
        self.moving_range_initializer = (
            initializers.get(moving_range_initializer))

    
    def build(self,input_shape):
        dim = input_shape[-1]
        shape = (dim,)
        self.gamma = self.add_weight(shape=shape,
                             name='gamma',
                             initializer=self.gamma_initializer,trainable=True)
        self.beta = self.add_weight(shape=shape,
                            name='beta',
                            initializer=self.beta_initializer,
                                   trainable=True)
        
        self.moving_mean = self.add_weight(
            shape=shape,
            name='moving_mean',
            initializer=self.moving_mean_initializer,
            trainable=False)
        
        self.moving_range = self.add_weight(
            shape=shape,
            name='moving_range',
            initializer=self.moving_range_initializer,
            trainable=False)



    def call(self, inputs,training=None):
        input_shape = inputs.shape
        
        if training == False:
            scaled = (inputs-self.moving_mean)/(self.moving_range+self.epsilon)
            return self.gamma*scaled + self.beta
        
        mean = tf.math.reduce_mean(inputs,axis=0)
        maxr = tf.math.reduce_max(inputs,axis=0)
        minr = tf.math.reduce_min(inputs,axis=0)
        
        range_diff = tf.math.subtract(maxr,minr)
        self.moving_mean = tf.math.add(self.momentum*self.moving_mean, (1-self.momentum)*mean)
        self.moving_range = tf.math.add(self.momentum*self.moving_range,(1-self.momentum)*range_diff)
        scaled = tf.math.divide(tf.math.subtract(inputs,mean),(range_diff+self.epsilon))
        return tf.math.add(tf.math.multiply(self.gamma,scaled),self.beta)
    
    def get_config(self):
        config = {
            'momentum': self.momentum,
            'epsilon': self.epsilon,
            'beta_initializer': initializers.serialize(self.beta_initializer),
            'gamma_initializer': initializers.serialize(self.gamma_initializer),
            'moving_mean_initializer':
                initializers.serialize(self.moving_mean_initializer),
            'moving_range_initializer':
                initializers.serialize(self.moving_range_initializer)
        }
        base_config = super(CustomBatchNormalization, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))
    
    def compute_output_shape(self, input_shape):
        return input_shape


### create your model
inp = Input(shape=(4,))
batch_norm_1 = CustomBatchNormalization(dynamic=True)(inp)
densout = Dense(24, activation='linear')(batch_norm_1)
densout = LeakyReLU(alpha=0.3)(densout)
batch_norm_2 = CustomBatchNormalization(dynamic=True)(densout)
densout = Dense(128, activation='linear')(batch_norm_2)
densout = LeakyReLU(alpha=0.3)(densout)
batch_norm_out = CustomBatchNormalization(dynamic=True)(densout)
out = Dense(5, activation='linear')(batch_norm_out)
test_nw = tf.keras.models.Model(inp, out)

##compile it
test_nw.compile(tf.keras.optimizers.Adam(), loss=mse,experimental_run_tf_function=False)

path_HDF5 = 'PATH_TO_SAVE_THIS_MODEL'
earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')
mcp_save_RH = ModelCheckpoint(path_HDF5,save_best_only=True, monitor='val_loss', mode='min')

X = np.random.randn(4,4)
y = np.random.randn(4,5)
X_val = np.random.randn(4,4)
y_val = np.random.randn(4,5)
test_nw.fit(X,y,batch_size=4, epochs=10,validation_data = (X_val,y_val),callbacks=[earlyStopping, mcp_save_RH] )

######## Now restart the kernel and load the model
dict_lay = {'CustomBatchNormalization':CustomBatchNormalization}
mod = load_model(path_HDF5,custom_objects=dict_lay)
I get an error saying that
&lt;denchmark-code&gt;ValueError                                Traceback (most recent call last)
&lt;ipython-input-9-8655e6764114&gt; in &lt;module&gt;
----&gt; 1 mod = load_model(path_HDF5+'CI01_RH_CBN_test.hdf5',custom_objects=dict_lay)

~/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/save.py in load_model(filepath, custom_objects, compile)
   144   if (h5py is not None and (
   145       isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):
--&gt; 146     return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)
   147 
   148   if isinstance(filepath, six.string_types):

~/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py in load_model_from_hdf5(filepath, custom_objects, compile)
   169 
   170     # set weights
--&gt; 171     load_weights_from_hdf5_group(f['model_weights'], model.layers)
   172 
   173     if compile:

~/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py in load_weights_from_hdf5_group(f, layers)
   695                        str(len(symbolic_weights)) +
   696                        ' weights, but the saved weights have ' +
--&gt; 697                        str(len(weight_values)) + ' elements.')
   698     weight_value_tuples += zip(symbolic_weights, weight_values)
   699   K.batch_set_value(weight_value_tuples)

ValueError: Layer #0 (named "custom_batch_normalization_17" in the current model) was found to correspond to layer custom_batch_normalization_17 in the save file. However the new layer custom_batch_normalization_17 expects 4 weights, but the saved weights have 2 elements.

&lt;/denchmark-code&gt;

Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='ankitesh97' date='2020-02-19T11:48:17Z'>
		&lt;denchmark-link:https://github.com/ankitesh97&gt;@ankitesh97&lt;/denchmark-link&gt;
 I have executed your code on tensorflow 2.0 please find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/Saduf2019/52f7e8ca6a28871831992b2dc3013b05/36883_2-0.ipynb&gt;here&lt;/denchmark-link&gt;
 the issue does not exist in 2.0, also executed it on tensorflow 2.1 gist for the &lt;denchmark-link:https://colab.sandbox.google.com/gist/Saduf2019/a9d49a5f67faf7e8ffb05a331a69aeb7/36883_2-1.ipynb&gt;same&lt;/denchmark-link&gt;
 where the error is not the same as faced by you.
		</comment>
		<comment id='2' author='ankitesh97' date='2020-02-20T05:28:30Z'>
		Thanks, for the reply. I downgraded my TensorFlow to 2.0 it is working now. I don't know what error is happening in tf 2.1 though.
		</comment>
		<comment id='3' author='ankitesh97' date='2020-03-10T23:29:50Z'>
		This works successfully with tf-nighly version '2.2.0-dev20200310'. Thanks!
		</comment>
		<comment id='4' author='ankitesh97' date='2020-03-10T23:29:52Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36883&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36883&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>