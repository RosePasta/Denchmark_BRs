<bug id='40244' author='alexrogozea' open_date='2020-06-07T16:18:44Z' closed_time='2020-07-22T16:52:08Z'>
	<summary>TF2.2: MultiWorkerMirroredStrategy doesn't assign workers correctly and training doesn't start</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):binary via PIP
TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0
Python version: 3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 10.1/7.6.5
GPU model and memory: 960M/4GB

Describe the current behavior
Have custom training script set up with strategy MultiWorkerMirroredStrategy, in this case trying on 2 separate workers using a docker image.
The script crashes on both workers:
worker 0:
&lt;denchmark-code&gt;tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation Identity: {{node Identity}} was explicitly assigned to /job:localhost/replica:0/task:0/device:CPU:0 but available devices are [ /job:worker/replica:0/task:0/device:CPU:0, /job:worker/replica:0/task:0/device:GPU:0, /job:worker/replica:0/task:0/device:XLA_CPU:0, /job:worker/replica:0/task:0/device:XLA_GPU:0 ]. Make sure the device specification refers to a valid device.
	 [[Identity]] [Op:__inference_distributed_train_step_14920]
2020-06-07 16:08:53.189763: W tensorflow/core/common_runtime/eager/context.cc:447] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.
&lt;/denchmark-code&gt;

worker 1:
&lt;denchmark-code&gt;tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation Identity: {{node Identity}} was explicitly assigned to /job:localhost/replica:0/task:0/device:CPU:0 but available devices are [ /job:worker/replica:0/task:1/device:CPU:0, /job:worker/replica:0/task:1/device:GPU:0, /job:worker/replica:0/task:1/device:XLA_CPU:0, /job:worker/replica:0/task:1/device:XLA_GPU:0 ]. Make sure the device specification refers to a valid device.
	 [[Identity]] [Op:__inference_distributed_train_step_14761]
2020-06-07 16:08:53.186955: W tensorflow/core/common_runtime/eager/context.cc:447] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.
&lt;/denchmark-code&gt;

Describe the expected behavior
The script starts training on all workers.
Standalone code to reproduce the issue
I don't have stand-alone code, but I've attached my training script. It was built following documentation and start command includes the TF_CONFIG environment set, for example on worker 0:
&lt;denchmark-code&gt;TF_CONFIG='{"cluster": {"worker": ["192.168.1.130:12345", "192.168.1.131:12345"]}, "task":{"index": 0, "type": "worker"}}' python3 multi_train.py --data_root /datasets --config configs/v53.cfg
&lt;/denchmark-code&gt;


training script - &lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/4742432/multi_train_py.txt&gt;multi_train_py.txt&lt;/denchmark-link&gt;

complete log of worker 0 (similar to worker 1) - &lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/4742437/complete_log.txt&gt;complete_log.txt&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='alexrogozea' date='2020-06-08T23:08:22Z'>
		Hi &lt;denchmark-link:https://github.com/alexrogozea&gt;@alexrogozea&lt;/denchmark-link&gt;
 can you confirm that your code runs when training on a single machine?
		</comment>
		<comment id='2' author='alexrogozea' date='2020-06-08T23:17:16Z'>
		Hi Nikita, it doesn't. Running only with worker 0 on the machine I get the
same error.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Tue, 9 Jun 2020 at 00:08, Nikita Namjoshi ***@***.***&gt; wrote:
 Hi @alexrogozea &lt;https://github.com/alexrogozea&gt; can you confirm that
 your code runs on a single machine?

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#40244 (comment)&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/AF2CGXE4IGOGAXTKLDS5WRTRVVVPNANCNFSM4NXMOLJQ&gt;
 .


-- 
Alex Rogozea

+44 7783 509 193
London, United Kingdom

		</comment>
		<comment id='3' author='alexrogozea' date='2020-06-09T17:09:06Z'>
		I've spent some time on Horovod following the lack of success with TF's solution, and I'd like to know how similar the pipeline requirements are. I did make Horovod train the model on 2 workers.
It's worth noting that my script doesn't run successfully on a normal, single host, getting the same error as above.
		</comment>
		<comment id='4' author='alexrogozea' date='2020-06-09T19:17:32Z'>
		&lt;denchmark-link:https://github.com/alexrogozea&gt;@alexrogozea&lt;/denchmark-link&gt;
 We've recently fixed this issue. can you try using tf-nightly?
		</comment>
		<comment id='5' author='alexrogozea' date='2020-06-09T20:45:52Z'>
		&lt;denchmark-link:https://github.com/anj-s&gt;@anj-s&lt;/denchmark-link&gt;
 Using tf-nightly-gpu and with tf-nightly builds I get the following exception:
&lt;denchmark-code&gt;tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [0] [Op:Assert] name: EagerVariableNameReuse
2020-06-09 20:41:36.344375: W tensorflow/core/common_runtime/eager/context.cc:553] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.
&lt;/denchmark-code&gt;

I'm not running other python/TF processes in the background, and the GPU is not being used by anything other than Xorg and 'compiz'.
More relevant, this steps stops the process before anything so I can't tell if it would work with the initial problem.
		</comment>
		<comment id='6' author='alexrogozea' date='2020-07-06T19:16:50Z'>
		&lt;denchmark-link:https://github.com/alexrogozea&gt;@alexrogozea&lt;/denchmark-link&gt;
 Looks like this might be a separate issue. can you share the complete stack trace? Also is this error something you see on a single host?
		</comment>
		<comment id='7' author='alexrogozea' date='2020-07-15T15:55:17Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='8' author='alexrogozea' date='2020-07-22T16:52:06Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='9' author='alexrogozea' date='2020-07-22T16:52:10Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40244&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40244&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='alexrogozea' date='2020-08-28T14:55:12Z'>
		&lt;denchmark-link:https://github.com/anj-s&gt;@anj-s&lt;/denchmark-link&gt;
 for reference: Do you happen to know which commit fixed this issue? We have users wanting to still use TF 2.2 and are looking into backporting this
		</comment>
	</comments>
</bug>