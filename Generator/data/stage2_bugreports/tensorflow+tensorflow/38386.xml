<bug id='38386' author='zhuzilin' open_date='2020-04-09T11:03:19Z' closed_time='2020-05-03T10:50:12Z'>
	<summary>Distributed training causes cuda OOM</summary>
	<description>
System information

Have I written custom code: Yes
OS Platform and Distribution: CentOS
TensorFlow installed from:
pip3 install tensorflow-gpu==2.0.0
Python version: 3.6.8
CUDA/cuDNN version: 10.0/7.6

Describe the current behavior
When starting the worker using v1 distirbuted training, we could observe the following error:
WARNING:tensorflow:From /usr/local/lib64/python3.6/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0409 18:55:45.554535 140176829970240 deprecation.py:323] From /usr/local/lib64/python3.6/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
2020-04-09 18:55:45.563568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla P40 major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:0d:00.0
2020-04-09 18:55:45.563627: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-04-09 18:55:45.563644: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-04-09 18:55:45.563658: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2020-04-09 18:55:45.563671: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2020-04-09 18:55:45.563687: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2020-04-09 18:55:45.563702: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2020-04-09 18:55:45.563716: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-04-09 18:55:45.568025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
INFO:tensorflow:Graph was finalized.
I0409 18:55:45.699084 140176829970240 monitored_session.py:240] Graph was finalized.
2020-04-09 18:55:47.161048: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 22.40G (24056830208 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.162735: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 20.16G (21651146752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.164113: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 18.15G (19486031872 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.165473: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 16.33G (17537427456 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.166829: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 14.70G (15783684096 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.168178: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 13.23G (14205315072 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.169530: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 11.91G (12784783360 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.170874: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 10.72G (11506305024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.172225: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 9.64G (10355674112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.173575: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 8.68G (9320105984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.174945: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 7.81G (8388094976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.176291: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 7.03G (7549285376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.177639: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 6.33G (6794356736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.178980: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 5.69G (6114920960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.180325: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 5.12G (5503428608 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.181714: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 4.61G (4953085440 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.183053: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 4.15G (4457776640 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.184402: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 3.74G (4011998976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.185768: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 3.36G (3610799104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.187108: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 3.03G (3249719040 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.188455: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 2.72G (2924747008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.189805: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 2.45G (2632272128 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.191145: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 2.21G (2369044736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.192492: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 1.99G (2132140288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.193864: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 1.79G (1918926336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.195203: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 1.61G (1727033600 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.196724: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 1.45G (1554330368 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.198186: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 1.30G (1398897408 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.199649: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 1.17G (1259007744 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.201138: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 1.05G (1133106944 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-04-09 18:55:47.202527: I tensorflow/stream_executor/cuda/cuda_driver.cc:830] failed to allocate 972.55M (1019796224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
INFO:tensorflow:Running local_init_op.
I0409 18:55:47.254083 140176829970240 session_manager.py:500] Running local_init_op.
INFO:tensorflow:Done running local_init_op.
I0409 18:55:47.262292 140176829970240 session_manager.py:502] Done running local_init_op.
2020-04-09 18:55:51.072444: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-04-09 18:55:51.096914: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2020-04-09 18:55:51.110052: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2020-04-09 18:55:51.122872: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2020-04-09 18:55:51.135759: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2020-04-09 18:55:51.148557: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2020-04-09 18:55:51.161419: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2020-04-09 18:55:51.174027: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2020-04-09 18:55:51.187117: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2020-04-09 18:55:51.262033: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2020-04-09 18:55:51.262068: W tensorflow/stream_executor/stream.cc:1919] attempting to perform BLAS operation using StreamExecutor without BLAS support
Traceback (most recent call last):
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1365, in _do_call
    return fn(*args)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1350, in _run_fn
    target_list, run_metadata)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1443, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InternalError: From /job:worker/replica:0/task:0:
Blas GEMM launch failed : a.shape=(32, 784), b.shape=(784, 500), m=32, n=500, k=784
         [[{{node dense/MatMul}}]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "distributed_mnist.py", line 123, in &lt;module&gt;
    tf.app.run()
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/platform/app.py", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File "/usr/local/lib/python3.6/site-packages/absl/app.py", line 299, in run
    _run_main(main, args)
  File "/usr/local/lib/python3.6/site-packages/absl/app.py", line 250, in _run_main
    sys.exit(main(argv))
  File "distributed_mnist.py", line 116, in main
    _, ls, step = mon_sess.run([train_op, loss, global_step])#,
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py", line 756, in run
    run_metadata=run_metadata)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py", line 1261, in run
    run_metadata=run_metadata)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py", line 1362, in run
    raise six.reraise(*original_exc_info)
  File "/usr/local/lib/python3.6/site-packages/six.py", line 703, in reraise
    raise value
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py", line 1347, in run
    return self._sess.run(*args, **kwargs)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py", line 1420, in run
    run_metadata=run_metadata)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py", line 1178, in run
    return self._sess.run(*args, **kwargs)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/client/session.py", line 956, in run
    run_metadata_ptr)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1359, in _do_run
    run_metadata)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1384, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: From /job:worker/replica:0/task:0:
Blas GEMM launch failed : a.shape=(32, 784), b.shape=(784, 500), m=32, n=500, k=784
         [[node dense/MatMul (defined at /usr/local/lib64/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]

Original stack trace for 'dense/MatMul':
  File "distributed_mnist.py", line 123, in &lt;module&gt;
    tf.app.run()
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/platform/app.py", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File "/usr/local/lib/python3.6/site-packages/absl/app.py", line 299, in run
    _run_main(main, args)
  File "/usr/local/lib/python3.6/site-packages/absl/app.py", line 250, in _run_main
    sys.exit(main(argv))
  File "distributed_mnist.py", line 82, in main
    logits = model(images)
  File "distributed_mnist.py", line 28, in model
    net = tf.layers.dense(images, 500, activation=tf.nn.relu)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/util/deprecation.py", line 324, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/layers/core.py", line 187, in dense
    return layer.apply(inputs)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/util/deprecation.py", line 324, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 1695, in apply
    return self.__call__(inputs, *args, **kwargs)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/layers/base.py", line 548, in __call__
    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py", line 847, in __call__
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py", line 234, in wrapper
    return converted_call(f, options, args, kwargs)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py", line 439, in converted_call
    return _call_unconverted(f, args, kwargs, options)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py", line 330, in _call_unconverted
    return f(*args, **kwargs)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/keras/layers/core.py", line 1056, in call
    outputs = gen_math_ops.mat_mul(inputs, self.kernel)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/gen_math_ops.py", line 6136, in mat_mul
    name=name)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py", line 793, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 3360, in create_op
    attrs, op_def, compute_device)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 3429, in _create_op_internal
    op_def=op_def)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 1751, in __init__
    self._traceback = tf_stack.extract_stack()
We've tried with different task number but observed the same problem.
Standalone code to reproduce the issue
import tensorflow.compat.v1 as tf
tf.disable_eager_execution()

mnist = tf.keras.datasets.mnist

tf.app.flags.DEFINE_string("ps_hosts", "localhost:2222", "ps hosts")
tf.app.flags.DEFINE_string("worker_hosts", "localhost:2223,localhost:2224", "worker hosts")
tf.app.flags.DEFINE_string("job_name", "worker", "'ps' or'worker'")
tf.app.flags.DEFINE_integer("task_index", 0, "Index of task within the job")
tf.app.flags.DEFINE_integer("num_workers", 2, "Number of workers")
tf.app.flags.DEFINE_boolean("is_sync", False, "using synchronous training or not")

FLAGS = tf.app.flags.FLAGS

def model(images):
    """Define a simple mnist classifier"""
    net = tf.layers.dense(images, 500, activation=tf.nn.relu)
    net = tf.layers.dense(net, 500, activation=tf.nn.relu)
    net = tf.layers.dense(net, 10, activation=None)
    return net

def main(_):
    ps_hosts = FLAGS.ps_hosts.split(",")
    worker_hosts = FLAGS.worker_hosts.split(",")

    # create the cluster configured by `ps_hosts' and 'worker_hosts'
    cluster = tf.train.ClusterSpec({"ps": ps_hosts, "worker": worker_hosts})

    # create a server for local task
    server = tf.train.Server(cluster, job_name=FLAGS.job_name,
                             task_index=FLAGS.task_index)

    if FLAGS.job_name == "ps":
        server.join()  # ps hosts only join
    elif FLAGS.job_name == "worker":
        # workers perform the operation
        # ps_strategy = tf.contrib.training.GreedyLoadBalancingStrategy(FLAGS.num_ps)

        # Note: tf.train.replica_device_setter automatically place the paramters (Variables)
        # on the ps hosts (default placement strategy:  round-robin over all ps hosts, and also
        # place multi copies of operations to each worker host
        with tf.device(tf.train.replica_device_setter(worker_device="/job:worker/task:%d" % (FLAGS.task_index),
                                                      cluster=cluster)):
            # load mnist dataset
            (x_train, y_train), (x_test, y_test) = mnist.load_data()
            x_train, x_test = x_train / 255.0, x_test / 255.0

            x_train = x_train.reshape(x_train.shape[0], -1)
            x_test = x_test.reshape(x_test.shape[0], -1)

            train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32)
            train_iter = train_ds.make_initializable_iterator()
            train_init = train_iter.make_initializer(train_ds)
            test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)

            print(x_train.shape)
            print(y_train.shape)

            # the model
            #images = tf.placeholder(tf.float32, [None, 784])
            #labels = tf.placeholder(tf.int32, [None, 10])
            xs, ys = train_iter.get_next()
            xs = tf.cast(xs, dtype=tf.float32)
            ys = tf.cast(ys, dtype=tf.int32)
            images = xs
            labels = tf.one_hot(ys, 10)

            logits = model(images)
            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))

            # The StopAtStepHook handles stopping after running given steps.
            hooks = [tf.train.StopAtStepHook(last_step=2000)]

            global_step = tf.train.get_or_create_global_step()
            optimizer = tf.train.AdamOptimizer(learning_rate=1e-04)

            if FLAGS.is_sync:
                # synchronous training
                # use tf.train.SyncReplicasOptimizer wrap optimizer
                # ref: https://www.tensorflow.org/api_docs/python/tf/train/SyncReplicasOptimizer
                optimizer = tf.train.SyncReplicasOptimizer(optimizer, replicas_to_aggregate=FLAGS.num_workers,
                                                       total_num_replicas=FLAGS.num_workers)
                # create the hook which handles initialization and queues
                hooks.append(optimizer.make_session_run_hook((FLAGS.task_index==0)))

            train_op = optimizer.minimize(loss, global_step=global_step,
                                          aggregation_method=tf.AggregationMethod.ADD_N)

            # The MonitoredTrainingSession takes care of session initialization,
            # restoring from a checkpoint, saving to a checkpoint, and closing when done
            # or an error occurs.
            with tf.train.MonitoredTrainingSession(master=server.target,
                                                   is_chief=(FLAGS.task_index == 0),
                                                   #checkpoint_dir="./checkpoint_dir",
                                                   hooks=hooks) as mon_sess:
                mon_sess.run(train_init)
                while not mon_sess.should_stop():
                    # mon_sess.run handles AbortedError in case of preempted PS.
                    _, ls, step = mon_sess.run([train_op, loss, global_step])
                    if step % 100 == 0:
                        print("Train step %d, loss: %f" % (step, ls))

if __name__ == "__main__":
    tf.app.run()
The bash used to run this code (this is a one worker one ps example, we've varied the number of tasks and observed the similar behavior.)
python3 distributed_mnist.py --ps_hosts=localhost:2222 --worker_hosts=localhost:2224 --job_name=ps --task_index=0 --num_worker=1
python3 distributed_mnist.py --ps_hosts=localhost:2222 --worker_hosts=localhost:2224 --job_name=worker --task_index=0 --num_worker=1
	</description>
	<comments>
		<comment id='1' author='zhuzilin' date='2020-04-09T22:50:20Z'>
		You may try &lt;denchmark-link:https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth&gt;limiting gpu memory growth&lt;/denchmark-link&gt;
 in this case.
Put following snippet on top of your code;
import tensorflow as tf
gpus = tf.config.experimental.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(gpus[0], True)
# your code
		</comment>
		<comment id='2' author='zhuzilin' date='2020-04-10T01:45:49Z'>
		
You may try limiting gpu memory growth in this case.
Put following snippet on top of your code;
import tensorflow as tf
gpus = tf.config.experimental.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(gpus[0], True)
# your code

&lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;
 I tried with this snippet, but same error occured. I've also tried  and it didn't work either.
		</comment>
		<comment id='3' author='zhuzilin' date='2020-04-24T22:04:14Z'>
		Apologies for the delay in response. Is this still an issue? Did you try killing all python processes/ exit interpreter and try again? Thanks!
		</comment>
		<comment id='4' author='zhuzilin' date='2020-04-26T11:13:34Z'>
		No... The problem remains and our team has to swtich to tf.distribute.Strategy.
		</comment>
		<comment id='5' author='zhuzilin' date='2020-05-03T10:50:14Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38386&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38386&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>