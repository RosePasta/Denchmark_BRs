<bug id='29687' author='simonsays1980' open_date='2019-06-12T10:04:18Z' closed_time='2019-08-20T05:41:29Z'>
	<summary>HashTable lookup performance very low in comparison to plain Python dictionaries (~5,000x)</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.2 LTS (Bionic Beaver)
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
TensorFlow installed from (source or binary): Don't know (Colab)
TensorFlow version (use command below): 1.13.1
Python version: 3.6.7
Bazel version (if compiling from source): -
GCC/Compiler version (if compiling from source): -
CUDA/cuDNN version: 10.0.130
GPU model and memory:
Exact command to reproduce:  tf.contrib.lookup.HashTable..lookup()

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

While writing a document ranking algorithm in TensorFlow we found out that TensorFlow HashTables appear to be very slow (~5,000x) in comparison to plain Python dictionaries.
Looking into the TensorFlow source code in  (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/lookup_table_op.cc&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/lookup_table_op.cc&lt;/denchmark-link&gt;
) shows that the underlying object structure appears to be an . So we wonder, why the performance is so low?
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

Here is the source code that is similar to a part of what we use and can be easily executed on any system. There is actually a  notebook that can be used: &lt;denchmark-link:https://colab.research.google.com/drive/1bB_sir7-sVd3bNrSgkcdT9UlU9eoyA2Q&gt;https://colab.research.google.com/drive/1bB_sir7-sVd3bNrSgkcdT9UlU9eoyA2Q&lt;/denchmark-link&gt;

import urllib.request
import tensorflow as tf
import re, time
from nltk.corpus import stopwords
from nltk import word_tokenize, download
# For loading the 'punkt' module of 'nltk'. This has to be done only once.
download( 'punkt' )
download( 'stopwords' )

url = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/cowper.txt'
urllib.request.urlretrieve( url, './cowper.txt' )

def parse_text( input_path = './cowper.txt', output_path = 'cowper_tf.txt' ):
    stop_words      = set( stopwords.words( 'english' ) )    
    with open( output_path, 'w+' ) as output_file:
        document = []
        with open( input_path, 'r' ) as file:
            for line in file:
                for token in map( str.lower, word_tokenize( line ) ):                    
                    if not token in stop_words and bool( re.match( r'[a-zA-Z]', token ) ):
                        document.append(token)             
            output_file.write(' '.join( document ) ) 

parse_text()

with open( 'cowper_tf.txt' ) as file:
    for line in file:
        document = re.split( r'\s', line )[1:-1]

k = 100
dictionary_list = []
hashtable_list  = []
frequencies     = {}
for i in range( k ):
    for word in document:    
        if word not in frequencies:
            frequencies[word] = 0        
            frequencies[word] += 1        
    keys           = tf.constant( list( frequencies.keys() ) )
    values         = tf.constant( list( frequencies.values() ) )
    frequencies_tf = tf.contrib.lookup.HashTable( 
        tf.contrib.lookup.KeyValueTensorInitializer(
            keys   = keys,
            values = values ),
        default_value = 0 )
    hashtable_list.append( frequencies_tf )
    dictionary_list.append( frequencies )

n = 100
test_word = tf.constant( 'sing', dtype = tf.string, shape = () )
global tf_time
def test_lookup( test_word, n = 100 ):
    start = time.clock()
    for i in range( n ):    
        a = [hashtable.lookup( test_word ) for hashtable in hashtable_list]
    tf_time = ( time.clock() - start ) / n
    print( 'time elapsed per lookup: ', tf_time )
    return a
with tf.Session() as sess:
    sess.run( tf.global_variables_initializer() )
    sess.run( tf.tables_initializer() )
    sess.run( test_lookup( test_word, n ) )

test_word = 'add'
start = time.clock()
for i in range( n ):
    a = [dictionary[test_word] for dictionary in dictionary_list]
plain_python_time = ( time.clock() - start ) / n
print( 'elapsed time:', plain_python_time )
	</description>
	<comments>
		<comment id='1' author='simonsays1980' date='2019-06-18T12:03:39Z'>
		I am able to reproduce the issue on Colab with Tensorflow 1.13.1. Thanks!
		</comment>
		<comment id='2' author='simonsays1980' date='2019-07-26T16:08:51Z'>
		
I think you are measuring graph construction time not the runtime for the TF numbers.
2 However even more importantly the benchmark does not even run any lookups at all since "return test_word" does not depend on looked up values,
Overall once you run the lookup ops performance will be even more dismal than what you currently claim, since you are creating &gt; 10000 ops and per op overhead is huge. For a more realistic comparison you should be looking up a batch of keys at a time.

		</comment>
		<comment id='3' author='simonsays1980' date='2019-08-02T05:41:19Z'>
		&lt;denchmark-link:https://github.com/simonsays1980&gt;@simonsays1980&lt;/denchmark-link&gt;
, Did you get a chance to look at &lt;denchmark-link:https://github.com/azaks3&gt;@azaks3&lt;/denchmark-link&gt;
's comment. Thanks!
		</comment>
		<comment id='4' author='simonsays1980' date='2019-08-02T15:28:19Z'>
		I didn't open this ticket, but I did try to create a simple test in TF 2.0 that addresses &lt;denchmark-link:https://github.com/azaks3&gt;@azaks3&lt;/denchmark-link&gt;
's points. To allow the  to work efficiently, I batched the inputs in that mode in groups of 1000:
&lt;denchmark-code&gt;keys = range(1,11)
values = [0, 1, 1, 0, 1, 0, 1, 0, 0, 0]
randomizer = tf.random_uniform_initializer(1, 10, 0)

py_dict = dict(zip(keys, values))
py_input = randomizer.__call__((sample_count,), tf.int64)
py_ds = tf.data.Dataset.from_tensor_slices(py_input)
py_iter = iter(py_ds)
py_start = time.clock()
py_results = [py_dict[x.numpy()] for x in py_iter]
py_time = (time.clock() - py_start)
py_time_str = '{:10f}'.format(py_time)
print(f'{py_time}s - total python time')

tf_dict_init = tf.lookup.KeyValueTensorInitializer(keys, values, tf.int64, tf.int64)
tf_dict = tf.lookup.StaticHashTable(tf_dict_init, -1)
tf_input = randomizer.__call__((round(sample_count/1000), 1000), tf.int64)
tf_ds = tf.data.Dataset.from_tensor_slices(tf_input)
tf_iter = iter(tf_ds)
tf_start = time.clock()
tf_results = [tf_dict.lookup(x) for x in tf_iter]
tf_time = (time.clock() - tf_start)
tf_time_str = '{:10f}'.format(tf_time)
print(f'{tf_time}s - total TensorFlow time')
&lt;/denchmark-code&gt;

On my computer, the python version took about 9.8s for 100k records, while the TensorFlow version took about 0.4s.
		</comment>
		<comment id='5' author='simonsays1980' date='2019-08-03T09:29:21Z'>
		

I think you are measuring graph construction time not the runtime for the TF numbers.
2 However even more importantly the benchmark does not even run any lookups at all since "return test_word" does not depend on looked up values,
Overall once you run the lookup ops performance will be even more dismal than what you currently claim, since you are creating &gt; 10000 ops and per op overhead is huge. For a more realistic comparison you should be looking up a batch of keys at a time.


&lt;denchmark-link:https://github.com/azaks3&gt;@azaks3&lt;/denchmark-link&gt;
 First of all, thank you for your efforts looking into this issue and for your comments. In the &lt;denchmark-link:https://colab.research.google.com/drive/1bB_sir7-sVd3bNrSgkcdT9UlU9eoyA2Q&gt;Colab notebook&lt;/denchmark-link&gt;
 I added two more cells, in which I repeat the example with a batch of 1,000 keys. There is still a difference of 7x between TensorFlow and pure Python, however your second comment is totally right: increasing the batch size decreases relative overhead. It's actually impressive to see how the time remains almost the same for TensorFlow s in comparison to the significantly longer run times for pure Python dictionaries when looking up more keys at once.
Now, our use case is actually rather the one with not many keys (~20-50) per lookup but a lookup through many HashTables (or in pure Python dictionaries) and this operation repeated many times (e.g. in inference). I understand now that the overhead is quite a lot and that this might be the reason why we have such a long runtime for the TensorFlow version of our application. I was hoping to find a solution in TensorFlow - something about HashTables I have not known, yet - to make the requests faster. This becomes especially demanding when turning towards inference.
		</comment>
		<comment id='6' author='simonsays1980' date='2019-08-03T09:45:36Z'>
		
I didn't open this ticket, but I did try to create a simple test in TF 2.0 that addresses @azaks3's points. To allow the StaticHashTable to work efficiently, I batched the inputs in that mode in groups of 1000:
keys = range(1,11)
values = [0, 1, 1, 0, 1, 0, 1, 0, 0, 0]
randomizer = tf.random_uniform_initializer(1, 10, 0)

py_dict = dict(zip(keys, values))
py_input = randomizer.__call__((sample_count,), tf.int64)
py_ds = tf.data.Dataset.from_tensor_slices(py_input)
py_iter = iter(py_ds)
py_start = time.clock()
py_results = [py_dict[x.numpy()] for x in py_iter]
py_time = (time.clock() - py_start)
py_time_str = '{:10f}'.format(py_time)
print(f'{py_time}s - total python time')

tf_dict_init = tf.lookup.KeyValueTensorInitializer(keys, values, tf.int64, tf.int64)
tf_dict = tf.lookup.StaticHashTable(tf_dict_init, -1)
tf_input = randomizer.__call__((round(sample_count/1000), 1000), tf.int64)
tf_ds = tf.data.Dataset.from_tensor_slices(tf_input)
tf_iter = iter(tf_ds)
tf_start = time.clock()
tf_results = [tf_dict.lookup(x) for x in tf_iter]
tf_time = (time.clock() - tf_start)
tf_time_str = '{:10f}'.format(tf_time)
print(f'{tf_time}s - total TensorFlow time')

On my computer, the python version took about 9.8s for 100k records, while the TensorFlow version took about 0.4s.

&lt;denchmark-link:https://github.com/novog&gt;@novog&lt;/denchmark-link&gt;
 Thanks for posting your example code in TensorFlow 2.0 here. This points again out - in a more elegant implementation - that overhead is actually the reason for the duration issue when looking up a lot of times only a few keys in many tables. Looking up many keys only a few times is where TensorFlow glances.
&lt;denchmark-link:https://github.com/azaks3&gt;@azaks3&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/novog&gt;@novog&lt;/denchmark-link&gt;
 For the above mentioned use case of looking up many times only a few keys in a lot of tables: how would you implement this in TensorFlow to get good performance?
		</comment>
		<comment id='7' author='simonsays1980' date='2019-08-03T17:09:29Z'>
		I think the standard technique would be to combine the tables and encode table name in the key/output values
		</comment>
		<comment id='8' author='simonsays1980' date='2019-08-20T05:41:28Z'>
		Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
		<comment id='9' author='simonsays1980' date='2019-08-20T05:41:30Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=29687&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=29687&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>