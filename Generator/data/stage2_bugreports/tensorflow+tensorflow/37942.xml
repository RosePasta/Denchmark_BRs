<bug id='37942' author='jliebers' open_date='2020-03-26T12:01:17Z' closed_time='2020-04-11T11:28:22Z'>
	<summary>GPU-accelerated LSTMs crash randomly with: [ InternalError:  [_Derived_] Failed to call ThenRnnBackward with model config ]</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro N, Build 17763
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
TensorFlow installed from (source or binary): Pypi
TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de410 2.1.0
Python version: 3.7.6
Bazel version (if compiling from source): -
GCC/Compiler version (if compiling from source): -
CUDA/cuDNN version: CUDA 10.1, cudnn-10.1-windows10-x64-v7.6.5.32
GPU model and memory: GTX 1060, 6 GB

Describe the current behavior
Dear Tensorflow-Developers,
my jupyter notebook that is training some LSTMs on the GPU crashes after some time with the following traceback:
&lt;denchmark-code&gt;InternalError:  [_Derived_]  Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 100, 100, 1, 249, 32, 100] 
	 [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]
	 [[StatefulPartitionedCall_1]] [Op:__inference_distributed_function_7604]

Function call stack:
distributed_function -&gt; distributed_function -&gt; distributed_function
&lt;/denchmark-code&gt;

This crash happens after a random amount of epochs (sometimes 6, sometimes 130+ sometimes 300+). It also crashes on different Windows machines with different GPUs.
Please see this minimal notebook to reproduce the behaviour that also includes the whole stacktrace: &lt;denchmark-link:https://gist.github.com/jliebers/995c3c4da4ad2a6f9376d31ee2470ec5&gt;https://gist.github.com/jliebers/995c3c4da4ad2a6f9376d31ee2470ec5&lt;/denchmark-link&gt;

In the stacktrace I can find the following line:
130         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?
I wonder if this is connected to this issue? 🙂
On a CPU-training everything works well and stable.
Thank you kindly in advance for your consideration and great work. 🚀
Describe the expected behavior
The GPU-accelerated LSTM should not crash randomly.
Standalone code to reproduce the issue
&lt;denchmark-link:https://gist.github.com/jliebers/995c3c4da4ad2a6f9376d31ee2470ec5&gt;https://gist.github.com/jliebers/995c3c4da4ad2a6f9376d31ee2470ec5&lt;/denchmark-link&gt;

Other info / logs
For the full traceback, please check the gist from above.
	</description>
	<comments>
		<comment id='1' author='jliebers' date='2020-03-26T14:27:57Z'>
		&lt;denchmark-link:https://github.com/jliebers&gt;@jliebers&lt;/denchmark-link&gt;
 This seems highly related to my issue posted earlier today as well: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/37932&gt;#37932&lt;/denchmark-link&gt;
 would love to get some feedback as well.
I've tried limiting the memory usage, using the nightly build and setting the growth to True (gpu = tf.config.experimental.list_physical_devices('GPU'), tf.config.experimental.set_memory_growth(gpu[0], True) to see if it's an issue with GPU memory but I still get the following even when limiting to 2GB:
In Jupyter:
&lt;denchmark-code&gt;[_Derived_]  Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 56, 64, 1, 90, 64, 64] 
	 [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]
	 [[PartitionedCall_2]] [Op:__inference_train_function_47260]

Function call stack:
train_function -&gt; train_function -&gt; train_function
&lt;/denchmark-code&gt;

In my terminal window:
&lt;denchmark-code&gt;2020-03-26 21:34:20.415064: E tensorflow/stream_executor/dnn.cc:613] CUDNN_STATUS_INTERNAL_ERROR
in tensorflow/stream_executor/cuda/cuda_dnn.cc(1986): 'cudnnRNNBackwardData( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, output_desc.handles(), output_data.opaque(), output_desc.handles(), output_backprop_data.opaque(), output_h_desc.handle(), output_h_backprop_data.opaque(), output_c_desc.handle(), output_c_backprop_data.opaque(), rnn_desc.param
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='jliebers' date='2020-03-27T10:49:41Z'>
		&lt;denchmark-link:https://github.com/jliebers&gt;@jliebers&lt;/denchmark-link&gt;
,
I was able to run the above code without any issues for 500 epochs. However I'm facing an error stating
 on running the  cell. Please find the gist &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/413e6d7a250b276a91b72c0166aa4663/notebook.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='3' author='jliebers' date='2020-03-27T11:13:27Z'>
		&lt;denchmark-link:https://github.com/amahendrakar&gt;@amahendrakar&lt;/denchmark-link&gt;

Hi, I am sorry, the very last cell (line 198 to 206) contained a bug and should have been:
&lt;denchmark-code&gt;print("Starting model.evaluate().")
if use_cpu:
    with tf.device("/device:CPU:0"):
        evaluation = model.evaluate(x=DATA_NORMALIZED[split_index:],
                                    y=LABELS_OHC[split_index:])
else:
    evaluation = model.evaluate(x=DATA_NORMALIZED[split_index:],
                                y=LABELS_OHC[split_index:])
print("Finished model.evaluate().")
&lt;/denchmark-code&gt;

I corrected and re-ran your notebook.
Also, maybe a solution was found for my problem (kudos to &lt;denchmark-link:https://github.com/DietmarKracht&gt;@DietmarKracht&lt;/denchmark-link&gt;
). Let me quickly verify it. Will update this issue asap. 
		</comment>
		<comment id='4' author='jliebers' date='2020-03-27T12:45:53Z'>
		Hi,
so the following  has been found thanks to &lt;denchmark-link:https://github.com/DietmarKracht&gt;@DietmarKracht&lt;/denchmark-link&gt;
. Now I am able to train LSTMs on my GPU without the error from the first post in this issue.   
To train the LSTM-model on a GPU on my plattform (Windows 10, tf 2.1.0), the parameter batch_input_shape must be specified during the model creation in the very first layer and the parameter input_shape must be omitted. The first layer of the LSTM-model should looks this:
&lt;denchmark-code&gt;model = Sequential()
model.add(LSTM(100, 
    batch_input_shape=(batch_size, n_timesteps, n_features), 
    return_sequences=True))  # omit return_sequences if no other LSTM-layer follows
[...]
&lt;/denchmark-code&gt;

Notice: I assume that if batch_input_shape is not specified it will default to some value and this issue arises randomly from the consequences. As it could not be reproduced in colab, I guess that it is a platform-specific platform (see first post for my specs).
 An int, , is specified in .  The int must divide divide the length of  ( is passed to )  any rest, i.e. ! Additionally one must not use the -parameter in model.fit() (see issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/37840&gt;#37840&lt;/denchmark-link&gt;
).
Then it works without any issues with tf 2.1.0 on Windows 10 (finally! phew!). 🙂
Please find a minimal example (works for me on GPU and CPU) here: &lt;denchmark-link:https://gist.github.com/jliebers/7effb38e836ab3c6e95bd122589f5f92&gt;https://gist.github.com/jliebers/7effb38e836ab3c6e95bd122589f5f92&lt;/denchmark-link&gt;

Sadly, it is nowhere mentioned in the documentation and it took us a week to solve this issue. I hope this post is helpful for people in the future.
Update:
Should the kernel die at any point with the following trace, then lower your batch_size to a smaller divisor of len(x):
&lt;denchmark-code&gt;2020-03-27 15:01:57.982960: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2020-03-27 15:01:57.983072: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='jliebers' date='2020-03-27T16:25:41Z'>
		After following the solutions suggested like:

Allowing GPU Memory Growth
Using batch_input_shape instead of input_shape
Using drop_remainder=True when creating batches

I'm faced with the following after the first successfully trained model:
&lt;denchmark-code&gt;2020-03-27 17:21:28.275596: F .\tensorflow/core/util/gpu_launch_config.h:129] Check failed: work_element_count &gt; 0 (0 vs. 0)
[I 17:21:29.843 LabApp] KernelRestarter: restarting kernel (1/5), keep random ports
kernel 2bac517a-c195-47ca-952b-c25881cf0757 restarted
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='jliebers' date='2020-03-27T16:31:18Z'>
		Yes, allowing GPU memory growth is also necessary, i.e.
&lt;denchmark-code&gt;gpu_devices = tf.config.experimental.list_physical_devices('GPU')
for device in gpu_devices: tf.config.experimental.set_memory_growth(device, True)
&lt;/denchmark-code&gt;

And I agree with the bullet points you posted. They are required to make it work on my setup.
		</comment>
		<comment id='7' author='jliebers' date='2020-03-30T03:24:02Z'>
		
Hi,
so the following workaround has been found thanks to @DietmarKracht. Now I am able to train LSTMs on my GPU without the error from the first post in this issue. 🎉 🎉 🎉
To train the LSTM-model on a GPU on my plattform (Windows 10, tf 2.1.0), the parameter batch_input_shape must be specified during the model creation in the very first layer and the parameter input_shape must be omitted. The first layer of the LSTM-model should looks this:
model = Sequential()
model.add(LSTM(100, 
    batch_input_shape=(batch_size, n_timesteps, n_features), 
    return_sequences=True))  # omit return_sequences if no other LSTM-layer follows
[...]

Notice: I assume that if batch_input_shape is not specified it will default to some value and this issue arises randomly from the consequences. As it could not be reproduced in colab, I guess that it is a platform-specific platform (see first post for my specs).
Important: An int, batch_size, is specified in batch_input_shape=(batch_size, n_timesteps, n_features). The int must divide divide the length of X (X is passed to model.fit()) without any rest, i.e. len(X) % batch_size == 0! Additionally one must not use the validation_split-parameter in model.fit() (see issue #37840).
Then it works without any issues with tf 2.1.0 on Windows 10 (finally! phew!). 🙂
Please find a minimal example (works for me on GPU and CPU) here: https://gist.github.com/jliebers/7effb38e836ab3c6e95bd122589f5f92
Sadly, it is nowhere mentioned in the documentation and it took us a week to solve this issue. I hope this post is helpful for people in the future.
Update:
Should the kernel die at any point with the following trace, then lower your batch_size to a smaller divisor of len(x):
2020-03-27 15:01:57.982960: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2020-03-27 15:01:57.983072: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1


Really helps a lot! Thanks.
		</comment>
		<comment id='8' author='jliebers' date='2020-04-01T16:49:59Z'>
		&lt;denchmark-link:https://github.com/jliebers&gt;@jliebers&lt;/denchmark-link&gt;
,
Is this still an issue? Please feel free to close the issue if resolved. Thanks!
		</comment>
		<comment id='9' author='jliebers' date='2020-04-01T17:39:50Z'>
		I still have this issue, it happens with every type of RNN i'v tested. I tried the fixes above still nothing.
&lt;denchmark-code&gt;CUDNN_STATUS_INTERNAL_ERROR
in tensorflow/stream_executor/cuda/cuda_dnn.cc(1921): 'cudnnRNNBackwardData( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, output_desc.handles(), output_data.opaque(), output_desc.handles(), output_backprop_data.opaque(), output_h_desc.handle(), output_h_backprop_data.opaque(), output_c_desc.handle(), output_c_backprop_data.opaque(), rnn_desc.params_handle(), params.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), input_desc.handles(), input_backprop_data-&gt;opaque(), input_h_desc.handle(), input_h_backprop_data-&gt;opaque(), input_c_desc.handle(), input_c_backprop_data-&gt;opaque(), workspace.opaque(), workspace.size(), reserve_space_data-&gt;opaque(), reserve_space_data-&gt;size())'
2020-04-01 18:39:13.944813: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at cudnn_rnn_ops.cc:1922 : Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 16, 16, 1, 264, 64, 16]
2020-04-01 18:39:13.953384: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 16, 16, 1, 264, 64, 16]
         [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]
2020-04-01 18:39:13.963840: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: {{function_node __inference___backward_cudnn_lstm_with_fallback_8907_9081_specialized_for_StatefulPartitionedCall_at___inference_train_on_batch_10423_specialized_for_StatefulPartitionedCall_at___inference_train_on_batch_10423}} {{function_node __inference___backward_cudnn_lstm_with_fallback_8907_9081_specialized_for_StatefulPartitionedCall_at___inference_train_on_batch_10423_specialized_for_StatefulPartitionedCall_at___inference_train_on_batch_10423}} Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 16, 16, 1, 264, 64, 16]
         [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]
         [[StatefulPartitionedCall]]
Traceback (most recent call last):
  File "cartpole.py", line 78, in &lt;module&gt;
    dqn.fit(env, callbacks=callbacks, nb_steps=steps, visualize=False, verbose=1,nb_max_episode_steps=(120))
  File "C:\Users\decsg\AppData\Local\Programs\Python\Python37\lib\site-packages\rl\core.py", line 205, in fit
    metrics = self.backward(reward, terminal=done)
  File "C:\Users\decsg\AppData\Local\Programs\Python\Python37\lib\site-packages\rl\agents\dqn.py", line 327, in backward
    metrics = self.trainable_model.train_on_batch(ins + [targets, masks], [dummy_targets, targets])
  File "C:\Users\decsg\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\training.py", line 1078, in train_on_batch
    standalone=True)
  File "C:\Users\decsg\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py", line 433, in train_on_batch
    output_loss_metrics=model._output_loss_metrics)
  File "C:\Users\decsg\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\eager\def_function.py", line 568, in __call__
    result = self._call(*args, **kwds)
  File "C:\Users\decsg\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\eager\def_function.py", line 599, in _call
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
  File "C:\Users\decsg\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\eager\function.py", line 2363, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File "C:\Users\decsg\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\eager\function.py", line 1611, in _filtered_call
    self.captured_inputs)
  File "C:\Users\decsg\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\eager\function.py", line 1692, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "C:\Users\decsg\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\eager\function.py", line 545, in call
    ctx=ctx)
  File "C:\Users\decsg\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\eager\execute.py", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File "&lt;string&gt;", line 3, in raise_from
tensorflow.python.framework.errors_impl.InternalError:  [_Derived_]  Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 16, 16, 1, 264, 64, 16]
         [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]
         [[StatefulPartitionedCall]] [Op:__inference_train_on_batch_10423] ```
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='jliebers' date='2020-04-01T17:42:04Z'>
		This is my model
&lt;denchmark-code&gt;model = Sequential()
model.add(Reshape([264,16], batch_input_shape=(64,1,264,16)))
model.add(LSTM(16, return_sequences=True))
model.add(LSTM(16, return_sequences=True))
model.add(LSTM(16, return_sequences=False))
model.add(Dense(nb_actions, activation='softmax'))
print(model.summary())
&lt;/denchmark-code&gt;

		</comment>
		<comment id='11' author='jliebers' date='2020-04-02T13:31:01Z'>
		&lt;denchmark-link:https://github.com/FunkyGibbon&gt;@FunkyGibbon&lt;/denchmark-link&gt;
,
Could you please create a new issue from &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/new/choose&gt;this&lt;/denchmark-link&gt;
 link and fill in the template details, so that we can track the issue there? Thanks!
		</comment>
		<comment id='12' author='jliebers' date='2020-04-03T05:10:21Z'>
		Hi, I have this issue since yesterday. Before that, everything was working fine. I have not updated either the tensorflow version or the gpu driver or anything else for that matter in the last couple of days. This issue suddenly appeared just yesterday on its own. Below is my model,
&lt;denchmark-code&gt;def neural_network(vocab_size, embedding_dim, max_length, train_padded, train_labels, validation_frac, num_epochs):
    model = Sequential()
    model.add(Embedding(vocab_size, embedding_dim, input_length = max_length))
    model.add(Bidirectional(LSTM(64, return_sequences = True)))
    model.add(GlobalAveragePooling1D())
    model.add(Dropout(0.2))
    model.add(Dense(50, activation = 'relu'))
    model.add(Dropout(0.1))
    model.add(Dense(1, activation = 'sigmoid'))
    model.summary()
    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
    history = model.fit(train_padded, train_labels, epochs = num_epochs, verbose = 2, validation_split = validation_frac)
    return model, history
&lt;/denchmark-code&gt;

I am pretty sure I am not running out of memory as previously I have trained even bigger models (~20M params) but the above model has just 2M elements. Even the GPU usage barely exceeds 5%. I have GTX 1050ti 4GB. Also, I have successfully run the above model plenty of times before but only since yesterday this issue is coming up.
&lt;denchmark-code&gt;Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 200, 128)          1920000   
_________________________________________________________________
bidirectional (Bidirectional (None, 200, 128)          98816     
_________________________________________________________________
global_average_pooling1d (Gl (None, 128)               0         
_________________________________________________________________
dropout (Dropout)            (None, 128)               0         
_________________________________________________________________
dense (Dense)                (None, 50)                6450      
_________________________________________________________________
dropout_1 (Dropout)          (None, 50)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 51        
=================================================================
Total params: 2,025,317
Trainable params: 2,025,317
Non-trainable params: 0
&lt;/denchmark-code&gt;

Below is the exact error.
&lt;denchmark-code&gt;Train on 143613 samples, validate on 15958 samples
Epoch 1/5
Traceback (most recent call last):

  File "C:\Users\admin\Documents\Machine Learning\Projects\Classification\jigsaw-toxic-comment-classification-challenge\toxic_classifier.py", line 122, in &lt;module&gt;
    model, history = neural_network(vocab_size, embedding_dim, max_length, train_padded, toxicity[col], validation_frac, num_epochs)

  File "C:\Users\admin\Documents\Machine Learning\Projects\Classification\jigsaw-toxic-comment-classification-challenge\toxic_classifier.py", line 87, in neural_network
    history = model.fit(train_padded, train_labels, epochs = num_epochs, verbose = 2, validation_split = validation_frac)

  File "C:\Users\admin\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\training.py", line 819, in fit
    use_multiprocessing=use_multiprocessing)

  File "C:\Users\admin\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py", line 342, in fit
    total_epochs=epochs)

  File "C:\Users\admin\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py", line 128, in run_one_epoch
    batch_outs = execution_function(iterator)

  File "C:\Users\admin\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py", line 98, in execution_function
    distributed_function(input_fn))

  File "C:\Users\admin\Anaconda3\lib\site-packages\tensorflow_core\python\eager\def_function.py", line 568, in __call__
    result = self._call(*args, **kwds)

  File "C:\Users\admin\Anaconda3\lib\site-packages\tensorflow_core\python\eager\def_function.py", line 599, in _call
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable

  File "C:\Users\admin\Anaconda3\lib\site-packages\tensorflow_core\python\eager\function.py", line 2363, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access

  File "C:\Users\admin\Anaconda3\lib\site-packages\tensorflow_core\python\eager\function.py", line 1611, in _filtered_call
    self.captured_inputs)

  File "C:\Users\admin\Anaconda3\lib\site-packages\tensorflow_core\python\eager\function.py", line 1692, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))

  File "C:\Users\admin\Anaconda3\lib\site-packages\tensorflow_core\python\eager\function.py", line 545, in call
    ctx=ctx)

  File "C:\Users\admin\Anaconda3\lib\site-packages\tensorflow_core\python\eager\execute.py", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)

  File "&lt;string&gt;", line 3, in raise_from

InternalError:  [_Derived_]  Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] 
	 [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]
	 [[StatefulPartitionedCall_1]]
	 [[Reshape_14/_46]] [Op:__inference_distributed_function_5894]

Function call stack:
distributed_function -&gt; distributed_function -&gt; distributed_function
&lt;/denchmark-code&gt;

Also, once this issue occurs, the kernel keeps crashing on its own even if I am not compiling anything. I am using Spyder IDE and even restarting the kernel does not help; it simply crashes after a few seconds. Below is the log for that (it is written on a red background)
&lt;denchmark-code&gt;An error ocurred while starting the kernel
2020󈚨󈚧 09:10:48.429373: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020󈚨󈚧 10:25:24.630422: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020󈚨󈚧 10:25:24.654232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:26:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1
coreClock: 1.392GHz coreCount: 6 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s
2020󈚨󈚧 10:25:24.655330: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020󈚨󈚧 10:25:24.660386: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020󈚨󈚧 10:25:24.665212: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020󈚨󈚧 10:25:24.667487: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020󈚨󈚧 10:25:24.672356: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020󈚨󈚧 10:25:24.675255: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020󈚨󈚧 10:25:24.685248: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020󈚨󈚧 10:25:24.686442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020󈚨󈚧 10:25:24.687128: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2020󈚨󈚧 10:25:24.689769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:26:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1
coreClock: 1.392GHz coreCount: 6 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s
2020󈚨󈚧 10:25:24.690853: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020󈚨󈚧 10:25:24.691406: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020󈚨󈚧 10:25:24.691954: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020󈚨󈚧 10:25:24.692497: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020󈚨󈚧 10:25:24.693046: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020󈚨󈚧 10:25:24.693600: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020󈚨󈚧 10:25:24.694155: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020󈚨󈚧 10:25:24.695292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020󈚨󈚧 10:25:25.261369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020󈚨󈚧 10:25:25.261990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] 0 
2020󈚨󈚧 10:25:25.262349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0: N 
2020󈚨󈚧 10:25:25.263472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2990 MB memory) ‑&gt; physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:26:00.0, compute capability: 6.1)
2020󈚨󈚧 10:25:27.808120: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020󈚨󈚧 10:25:28.068884: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020󈚨󈚧 10:26:06.218534: E tensorflow/stream_executor/dnn.cc:596] CUDNN_STATUS_INTERNAL_ERROR
in tensorflow/stream_executor/cuda/cuda_dnn.cc(1921): 'cudnnRNNBackwardData( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, output_desc.handles(), output_data.opaque(), output_desc.handles(), output_backprop_data.opaque(), output_h_desc.handle(), output_h_backprop_data.opaque(), output_c_desc.handle(), output_c_backprop_data.opaque(), rnn_desc.params_handle(), params.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), input_desc.handles(), input_backprop_data‑&gt;opaque(), input_h_desc.handle(), input_h_backprop_data‑&gt;opaque(), input_c_desc.handle(), input_c_backprop_data‑&gt;opaque(), workspace.opaque(), workspace.size(), reserve_space_data‑&gt;opaque(), reserve_space_data‑&gt;size())'
2020󈚨󈚧 10:26:06.221888: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at cudnn_rnn_ops.cc:1922 : Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] 
2020󈚨󈚧 10:26:06.223371: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] 
[[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]
2020󈚨󈚧 10:26:06.225063: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: {{function_node __inference___backward_cudnn_lstm_with_fallback_4410_4588_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_5894}} {{function_node __inference___backward_cudnn_lstm_with_fallback_4410_4588_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_5894}} Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] 
[[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]
[[StatefulPartitionedCall_1]]
[[Reshape_14/_46]]
2020󈚨󈚧 10:26:06.228126: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: {{function_node __inference___backward_cudnn_lstm_with_fallback_4410_4588_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_5894}} {{function_node __inference___backward_cudnn_lstm_with_fallback_4410_4588_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_5894}} Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] 
[[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]
[[StatefulPartitionedCall_1]]
2020󈚨󈚧 10:35:24.183417: F .\tensorflow/core/kernels/random_op_gpu.h:232] Non‑OK‑status: GpuLaunchKernel(FillPhiloxRandomKernelLaunch, num_blocks, block_size, 0, d.stream(), gen, data, size, dist) status: Internal: unspecified launch failure
&lt;/denchmark-code&gt;

To get rid of the recurrent kernel crashes I have to restart Spyder every time. None of this had ever occurred before yesterday and I can say for sure I have not updated anything in the last 1 month at least. my TF version is 2.1 and the GPU driver version is 441.22.
		</comment>
		<comment id='13' author='jliebers' date='2020-04-08T14:51:58Z'>
		
@jliebers,
Is this still an issue? Please feel free to close the issue if resolved. Thanks!

Any updates regarding this issue? Thanks!
		</comment>
		<comment id='14' author='jliebers' date='2020-04-08T14:53:29Z'>
		
To get rid of the recurrent kernel crashes I have to restart Spyder every time. None of this had ever occurred before yesterday and I can say for sure I have not updated anything in the last 1 month at least. my TF version is 2.1 and the GPU driver version is 441.22.

&lt;denchmark-link:https://github.com/diggee&gt;@diggee&lt;/denchmark-link&gt;
,
I'd request you to submit a new issue using this &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/new/choose&gt;link&lt;/denchmark-link&gt;
, so that we can track it there.
		</comment>
		<comment id='15' author='jliebers' date='2020-04-09T05:08:04Z'>
		

To get rid of the recurrent kernel crashes I have to restart Spyder every time. None of this had ever occurred before yesterday and I can say for sure I have not updated anything in the last 1 month at least. my TF version is 2.1 and the GPU driver version is 441.22.

@diggee,
I'd request you to submit a new issue using this link, so that we can track it there.

already did.
		</comment>
		<comment id='16' author='jliebers' date='2020-04-10T08:45:37Z'>
		Hey all,
so I could just verify that all of the problems in this issue are connected with the Windows-version of tensorflow. I installed a fresh Ubuntu Bionic and followed the instructions [0] to set it up with CUDA on my homeoffice-workstation with a GTX 1060 6GB and none of the described problems was ever encountered again. :/
So please save yourself the trouble and just use Linux to train your LSTMs on the GPU. Even the validation_split-parameter works there without any problem and I had not to set up any batch_input_shape with divisible sets for training, validation, etc. Now it is just so easy.
Cheers,
Jonathan
[0] &lt;denchmark-link:https://www.tensorflow.org/install/gpu#linux_setup&gt;https://www.tensorflow.org/install/gpu#linux_setup&lt;/denchmark-link&gt;

		</comment>
		<comment id='17' author='jliebers' date='2020-04-10T13:58:30Z'>
		&lt;denchmark-link:https://github.com/jliebers&gt;@jliebers&lt;/denchmark-link&gt;
,
I this still an issue? Please feel free to close the issue if resolved. Thanks!
		</comment>
		<comment id='18' author='jliebers' date='2020-04-10T17:19:18Z'>
		Hey &lt;denchmark-link:https://github.com/amahendrakar&gt;@amahendrakar&lt;/denchmark-link&gt;
 ,
for me personally it is not an issue anymore since I switched my OS.
For tensorflow though, which officially supports MS Windows, it is an open issue and of course for all Windows-users aswell. 😉
		</comment>
		<comment id='19' author='jliebers' date='2020-04-11T11:28:22Z'>
		&lt;denchmark-link:https://github.com/jliebers&gt;@jliebers&lt;/denchmark-link&gt;
,
Thank you for the update. Will pass on the feedback to the concerned team. Marking this issue as closed.
		</comment>
		<comment id='20' author='jliebers' date='2020-04-11T11:28:24Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37942&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37942&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='21' author='jliebers' date='2020-06-15T18:28:02Z'>
		In case it helps anyone. After doing as CherryCheek said, training did work in my case again.
However, when I took a larger part of my dataset, it broke again with the error 
.Lowering my batch size from 128 to 64 solved that, now it runs well. However, it uses now only 3GB of 8GB available gpu memory. Still this is a better fix than downgrading the nvidia driver to 431.86 as mentioned here:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/40403&gt;#40403&lt;/denchmark-link&gt;

		</comment>
		<comment id='22' author='jliebers' date='2020-06-19T19:39:41Z'>
		Same issue here on Windows 10.
Python: 3.7.6
CUDA:
cudatoolkit                      8.0               4  pkgs/main
cudatoolkit                      9.0               1  pkgs/main
cudatoolkit                 10.0.130               0  pkgs/main
cudatoolkit                 10.1.168               0  pkgs/main
cudatoolkit                 10.1.243      h74a9793_0  pkgs/main
cudatoolkit                  10.2.89      h74a9793_0  pkgs/main
cudatoolkit                  10.2.89      h74a9793_1  pkgs/main
Tensorflow: 2.2.0
GPU: RTX 2070 SUPER
batch_size = 8
n_timesteps = 400
n_features = 768
input_x = tf.keras.layers.Input(shape=(n_timesteps,n_features))
bi_rnn = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, batch_input_shape=(batch_size, n_timesteps, n_features),
kernel_regularizer=tf.keras.regularizers.l2(0.01),
recurrent_regularizer=tf.keras.regularizers.l2(0.01),
bias_regularizer=tf.keras.regularizers.l2(0.01)))(input_x)
x = tf.keras.layers.Dropout(0.3)(bi_rnn)
out = tf.keras.layers.Dense(49, activation="softmax")(x)
model = tf.keras.Model(inputs=input_x, outputs=out)
model.compile(loss='sparse_categorical_crossentropy',optimizer="adam",metrics=['accuracy'])
model.summary()
&lt;denchmark-link:https://user-images.githubusercontent.com/21985105/85174219-c8c55400-b229-11ea-9d76-651f006a4f5a.png&gt;&lt;/denchmark-link&gt;

es = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', min_delta=0.01, patience=5)
history = model.fit(TAPEncoded_train, train_labels, batch_size = batch_size, epochs=10, validation_data=(TAPEncoded_val, validation_labels), verbose=2, callbacks=[es])
Then runs for a couple epochs (6/10 last time) and either prints the error message or kernel just dies.
		</comment>
		<comment id='23' author='jliebers' date='2020-06-19T21:21:42Z'>
		New setup with half-precision and now batch_size of 16 but with only ~100K parameters:
&lt;denchmark-link:https://user-images.githubusercontent.com/21985105/85180283-e5688880-b237-11ea-88b1-d04fa1827766.png&gt;&lt;/denchmark-link&gt;

Now I get the error message:
&lt;denchmark-link:https://user-images.githubusercontent.com/21985105/85180335-0b8e2880-b238-11ea-97e3-1f42da97cd6e.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/21985105/85180349-1943ae00-b238-11ea-9926-ac83e80a0d15.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/21985105/85180363-26f93380-b238-11ea-81f1-252941081b24.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='24' author='jliebers' date='2020-06-19T21:59:19Z'>
		I have read that some people believe that this is a memory issue.
I doubt it is. I tried a CNN model with the same data, not even restricting the batch size and with 4 times the parameters and the model ran extremely smoothly.
&lt;denchmark-link:https://user-images.githubusercontent.com/21985105/85182321-32029280-b23d-11ea-8eaf-478953c9bd4f.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/21985105/85182363-49da1680-b23d-11ea-8c0f-9d03512364af.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/21985105/85182383-59595f80-b23d-11ea-8e47-0fc5c35386ce.png&gt;&lt;/denchmark-link&gt;

(There are many things I don't know about the tensorflow LSTM implementation, but I don't understand how with quarter of the parameters, smaller batch size and the same data the kernel dies. To me this clearly shows a problem with LSTM under these versions and Windows.)
		</comment>
		<comment id='25' author='jliebers' date='2020-06-20T06:01:08Z'>
		
I have read that some people believe that this is a memory issue.
I doubt it is. I tried a CNN model with the same data, not even restricting the batch size and with 4 times the parameters and the model ran extremely smoothly.

No, this is not a memory issue. It is definitely a windows implementation of LSTM issue. Like you say, I have also run CNN models significantly bigger than the model that I was running LSTM for, and there was no issue. This error is also hard to reproduce as it does not always happen. Like I said before, if you restart the kernel (or IDE), it works fine then. You can also try the CUDA implementation of LSTM (CuDNNLSTM), I have found it to be more stable.
There are various fixes mentioned in various threads like downgrading gpu driver, using the batch_input_shape parameter instead of the usual input_shape, allowing GPU growth etc etc but please know that at least for me none of the suggestions really made a consistent difference. There are times when I can run the LSTM model continuously for hours at a stretch with millions of params and times when the damn thing crashes with just ten thousand params. The only thing that seems to make a consistent difference is switching the OS to Linux but that is not an option for me.
		</comment>
		<comment id='26' author='jliebers' date='2020-06-22T08:20:46Z'>
		Since I switched to Linux (Ubuntu 18.04) I never had a problem with this issue anymore. Therefore, I highly recommend anyone to switch OS if you want GPU-accelerated LSTMs. It is 100% connected to Windows only and I did not find any working solution (workaround or such) for Windows.
Only solution I know is to switch to Linux. The problem then disappears.
I mean this issue was closed, once I stated that I switched my OS but the underlying problem still exists. 🤷‍♂️
		</comment>
		<comment id='27' author='jliebers' date='2020-07-07T15:09:27Z'>
		I had the same issue on Windows, in my case the error took place only when running Bidirectional LSTM on GPU with a small batch size. As you've experienced the error doesn't show up running on CPU. I managed to find a temporary solution by running the code on Colab with GPU enabled. On Colab the error doesn't persist so as you said it's an error related to Windows OS.
		</comment>
		<comment id='28' author='jliebers' date='2020-07-13T00:08:30Z'>
		I have the same issue with windows. My code in Collab runs fine but when I run Locally I get and error. Now, I am sure that is a problem of tensorflow with windows.
		</comment>
		<comment id='29' author='jliebers' date='2020-07-14T19:27:21Z'>
		Same issue again in another project. The length of the sequences seems to have an impact on stability.
Btw when trying to restart the training, I get a gpu sync error with this in the console:
&lt;denchmark-code&gt;tensorflow/stream_executor/cuda/cuda_driver.cc:940] could not synchronize on CUDA context: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure :: 0x00007FF9F4007E15   tensorflow::CurrentStackTrace
0x00007FF9F3D5D3BE      tensorflow::MetaGraphDef_MetaInfoDef::_Internal::any_info
0x00007FF9F3D63C8E      stream_executor::StreamExecutor::EnablePeerAccessTo
0x00007FF9F21A1A96      tensorflow::StepStats::internal_default_instance
0x00007FF9F21B2EE4      google::protobuf::RepeatedPtrField&lt;tensorflow::InterconnectLink&gt;::Add
0x00007FF9F1DE34C7      std::vector&lt;tensorflow::DtypeAndPartialTensorShape,std::allocator&lt;tensorflow::DtypeAndPartialTensorShape&gt; &gt;::operator=
0x00007FF9F1DC090B      absl::lts_2020_02_25::Span&lt;tensorflow::Tensor const &gt;::end
0x00007FF9EDCEE68F      TFE_TensorHandleResolve
0x00007FF9EDC923C3      TFE_Py_TensorShapeSlice
0x00007FF9EDC8FF7A      std::_Tree&lt;std::_Tmap_traits&lt;std::array&lt;std::basic_string&lt;char,std::char_traits&lt;char&gt;,std::allocator&lt;char&gt; &gt;,0&gt;,tensorflow::monitoring::SamplerCell,std::less&lt;std::array&lt;std::basic_string&lt;char,std::char_traits&lt;char&gt;,std::allocator&lt;char&gt;
0x00007FFA5EC15DC7      PyMethodDef_RawFastCallKeywords
0x00007FFA5EC1649C      PyMethodDef_RawFastCallKeywords
0x00007FFA5EC16C93      PyEval_EvalFrameDefault
0x00007FFA5EC165FB      PyMethodDef_RawFastCallKeywords
0x00007FFA5EC16C93      PyEval_EvalFrameDefault
0x00007FFA5EC165FB      PyMethodDef_RawFastCallKeywords
0x00007FFA5EC16C93      PyEval_EvalFrameDefault
0x00007FFA5EC165FB      PyMethodDef_RawFastCallKeywords
0x00007FFA5EC16C93      PyEval_EvalFrameDefault
0x00007FFA5EC00192      PyEval_EvalCodeWithName
0x00007FFA5EBFFE1A      PyFunction_FastCallDict
0x00007FFA5EBFDEA3      PyTuple_New
0x00007FFA5EC0E947      PyObject_FastCallKeywords
0x00007FFA5EC0E6DA      PyObject_FastCallKeywords
0x00007FFA5EC16759      PyMethodDef_RawFastCallKeywords
0x00007FFA5EC17889      PyEval_EvalFrameDefault
0x00007FFA5EC00192      PyEval_EvalCodeWithName
0x00007FFA5EBFFE1A      PyFunction_FastCallDict
0x00007FFA5EC1EB6D      PySlice_New
0x00007FFA5EC17A04      PyEval_EvalFrameDefault
0x00007FFA5EC00192      PyEval_EvalCodeWithName
0x00007FFA5EC16727      PyMethodDef_RawFastCallKeywords
0x00007FFA5EC17889      PyEval_EvalFrameDefault
0x00007FFA5EC00192      PyEval_EvalCodeWithName
0x00007FFA5EBDFDDF      PyErr_Clear
0x00007FFA5EBDFCB5      PyErr_Clear
0x00007FFA5EC15AF0      PyMethodDef_RawFastCallKeywords
0x00007FFA5EC1665F      PyMethodDef_RawFastCallKeywords
0x00007FFA5EC16D3F      PyEval_EvalFrameDefault
0x00007FFA5EBFC464      PyObject_GetAttrId
0x00007FFA5EC5AB74      PyErr_NoMemory
0x00007FFA5EBFC464      PyObject_GetAttrId
0x00007FFA5EC5AB74      PyErr_NoMemory
0x00007FFA5EBFC464      PyObject_GetAttrId
0x00007FFA5EC15B11      PyMethodDef_RawFastCallKeywords
0x00007FFA5EC1649C      PyMethodDef_RawFastCallKeywords
0x00007FFA5EC16C93      PyEval_EvalFrameDefault
0x00007FFA5EC165FB      PyMethodDef_RawFastCallKeywords
0x00007FFA5EC16D3F      PyEval_EvalFrameDefault
0x00007FFA5EC165FB      PyMethodDef_RawFastCallKeywords
0x00007FFA5EC16C93      PyEval_EvalFrameDefault
0x00007FFA5EC00192      PyEval_EvalCodeWithName
0x00007FFA5EBFFE1A      PyFunction_FastCallDict
0x00007FFA5EBFEDDA      PyMethodDef_RawFastCallDict
0x00007FFA5EC1EAE4      PySlice_New
0x00007FFA5EC17A04      PyEval_EvalFrameDefault
0x00007FFA5EC00192      PyEval_EvalCodeWithName
0x00007FFA5EC16727      PyMethodDef_RawFastCallKeywords
0x00007FFA5EC17889      PyEval_EvalFrameDefault
0x00007FFA5EBFC464      PyObject_GetAttrId
0x00007FFA5ED4F296      PyAST_Optimize
0x00007FFA5EC15AF0      PyMethodDef_RawFastCallKeywords
0x00007FFA5EC1665F      PyMethodDef_RawFastCallKeywords
0x00007FFA5EC16D3F      PyEval_EvalFrameDefault
&lt;/denchmark-code&gt;

Is there any update regarding this issue?
		</comment>
		<comment id='30' author='jliebers' date='2020-07-14T23:54:05Z'>
		Same issue here.
		</comment>
		<comment id='31' author='jliebers' date='2020-07-15T07:53:11Z'>
		Since there are many other issues referencing this one, can we open it again until a solution exists where we do not need to change the OS?
		</comment>
		<comment id='32' author='jliebers' date='2020-07-15T11:28:07Z'>
		
Since there are many other issues referencing this one, can we open it again until a solution exists where we do not need to change the OS?

&lt;denchmark-link:https://github.com/MichaelJanz&gt;@MichaelJanz&lt;/denchmark-link&gt;
,
Could you please submit a new issue from &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/new/choose&gt;this link&lt;/denchmark-link&gt;
 and fill in the template, so that we can track the issue there. Thanks!
		</comment>
		<comment id='33' author='jliebers' date='2020-07-16T09:02:12Z'>
		&lt;denchmark-link:https://github.com/amahendrakar&gt;@amahendrakar&lt;/denchmark-link&gt;
 new issue was created, thank you for your help!
		</comment>
		<comment id='34' author='jliebers' date='2020-07-22T00:27:50Z'>
		
Since I switched to Linux (Ubuntu 18.04) I never had a problem with this issue anymore. Therefore, I highly recommend anyone to switch OS if you want GPU-accelerated LSTMs. It is 100% connected to Windows only and I did not find any working solution (workaround or such) for Windows.
Only solution I know is to switch to Linux. The problem then disappears.
I mean this issue was closed, once I stated that I switched my OS but the underlying problem still exists. 🤷‍♂️

Just switching to Linux (Ubuntu 18.04) and following TF's &lt;denchmark-link:https://www.tensorflow.org/install/gpu&gt;GPU installation guide&lt;/denchmark-link&gt;
 is not sufficient in fixing the issue. The &lt;denchmark-link:https://gist.github.com/jliebers/995c3c4da4ad2a6f9376d31ee2470ec5&gt;gist&lt;/denchmark-link&gt;
 is still crashing with this error and occasionally ThenRnnForward.
I believe OP is using an older nvidia driver than 440 or 450. I've tried both those versions without success. This issue is not reproducible in colab.
		</comment>
		<comment id='35' author='jliebers' date='2020-07-25T17:39:47Z'>
		I was having the same issue and couldn't solve it with any of the proposed fixes (except changing OS, I haven't tried that), but I found that when using recurrent dropout it causes some conflict with the cuDNN kernel when building the model (see warning below)
WARNING:tensorflow:Layer gru1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU
I haven't run into the original error ever since. It's not a fix, but it might help someone.
		</comment>
		<comment id='36' author='jliebers' date='2020-08-13T01:13:13Z'>
		This is not a permanent solution, but I managed to make it work again by downgrading the NVIDIA driver to the last stable studio driver (431.86) as suggested here: &lt;denchmark-link:https://forums.developer.nvidia.com/t/cudnn-lstm-is-broken-above-driver-431-60-unexpected-event-status-1-cuda/108800/2&gt;https://forums.developer.nvidia.com/t/cudnn-lstm-is-broken-above-driver-431-60-unexpected-event-status-1-cuda/108800/2&lt;/denchmark-link&gt;

You need to first download the corresponding studio driver from &lt;denchmark-link:https://www.nvidia.com/Download/index.aspx?lang=en&gt;NVIDIA&lt;/denchmark-link&gt;
, then uninstall the whatever driver version you have now (in my case 442), then install the 431.86 again. This is trickier than it sounds, as NVIDIA utilities only allow you to downgrade to the previous version, and my case I was several versions ahead.
I ended up using &lt;denchmark-link:https://www.guru3d.com/files-details/display-driver-uninstaller-download.html&gt;DDU &lt;/denchmark-link&gt;
utility as suggested in other forums to wipe the previous driver from my machine, it did the job nicely (no safe mode was necessary).
Also, bear in mind that windows will try to automatically update the driver as soon as it gets a chance (creating the problem again). To avoid this you can disable automatic updates for your drivers following &lt;denchmark-link:https://www.windowscentral.com/how-disable-automatic-driver-updates-windows-10&gt;these&lt;/denchmark-link&gt;
 instructions.
By the way, it wasn't necessary to apply the previous fixes suggested in the post (set batch_size or memory growth), just downgrading the driver did the trick.
I hope this helps, I wasted several hours trying to make it work!
		</comment>
		<comment id='37' author='jliebers' date='2020-08-13T22:40:57Z'>
		I followed the downgrade advice and started with the lowest available driver via search which was 441.* . That driver created the same problem, so I tried finding the exact version mentioned here 431.86:
&lt;denchmark-link:https://www.nvidia.co.uk/content/DriverDownload-March2009/confirmation.php?url=/Windows/431.86/431.86-desktop-win10-64bit-international-nsd-whql.exe&amp;lang=uk&amp;type=TITAN&gt;https://www.nvidia.co.uk/content/DriverDownload-March2009/confirmation.php?url=/Windows/431.86/431.86-desktop-win10-64bit-international-nsd-whql.exe&amp;lang=uk&amp;type=TITAN&lt;/denchmark-link&gt;

		</comment>
		<comment id='38' author='jliebers' date='2020-08-24T12:35:29Z'>
		
This is not a permanent solution, but I managed to make it work again by downgrading the NVIDIA driver to the last stable studio driver (431.86) as suggested here: https://forums.developer.nvidia.com/t/cudnn-lstm-is-broken-above-driver-431-60-unexpected-event-status-1-cuda/108800/2
You need to first download the corresponding studio driver from NVIDIA, then uninstall the whatever driver version you have now (in my case 442), then install the 431.86 again. This is trickier than it sounds, as NVIDIA utilities only allow you to downgrade to the previous version, and my case I was several versions ahead.
I ended up using DDU utility as suggested in other forums to wipe the previous driver from my machine, it did the job nicely (no safe mode was necessary).
Also, bear in mind that windows will try to automatically update the driver as soon as it gets a chance (creating the problem again). To avoid this you can disable automatic updates for your drivers following these instructions.
By the way, it wasn't necessary to apply the previous fixes suggested in the post (set batch_size or memory growth), just downgrading the driver did the trick.
I hope this helps, I wasted several hours trying to make it work!

I took daviddiazsolis' advice and downgraded the driver to version 431.86. This was a 100% solution for me.
I have been struggling with this issue for a while and have tried most or all of the other suggestions made in this thread without success. After downgrading the driver there has not been a single "Failed to call ThenRnnBackward with model config"-error.
		</comment>
		<comment id='39' author='jliebers' date='2020-08-26T11:27:43Z'>
		I have "Windows 10 (build 2004)" and the same problem with LSTM layers. And NVidia driver version 431.86 (Studio) or 431.36 (Game Ready) is not compatible with my version of Windows. So I'll have to wait for NVidia to fix and release the new driver.
		</comment>
		<comment id='40' author='jliebers' date='2020-09-06T10:12:22Z'>
		
Yes, allowing GPU memory growth is also necessary, i.e.
gpu_devices = tf.config.experimental.list_physical_devices('GPU')
for device in gpu_devices: tf.config.experimental.set_memory_growth(device, True)

And I agree with the bullet points you posted. They are required to make it work on my setup.

this doesn't solve it for me
		</comment>
		<comment id='41' author='jliebers' date='2020-09-26T09:29:13Z'>
		I share my experience with the same problem:
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Enterprise, Build 2004
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
TensorFlow installed from (source or binary): Pypi
TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0
Python version: 3.7.6
Bazel version (if compiling from source): -
GCC/Compiler version (if compiling from source): -
CUDA/cuDNN version: CUDA 10.1 (10.1.243), cudnn-10.1-windows10-x64-v7.6.5.32
GPU model and memory: Quadro RTX 4000, 8 GB (Laptop version)
I got similar error while testing different NVIDIA driver versions (ranging from 441.66 to the 456.38) that were available on NVIDIA's site. None of these driver versions could fix the problem where the training crashes after the first epoch in the middle of the second one, or somewhere in-between.
1st workaround
One workaround that seems to work (I could get the training to finish) was following tips from &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/37942&gt;#37942&lt;/denchmark-link&gt;

where I had to specify a fixed batch_size on the first layer of the model:
&lt;denchmark-code&gt;x = Input(shape=(timesteps,input_dim), batch_size=64) # need to have fixed batch_size for cudnn Rnns to work on Windows
...
&lt;/denchmark-code&gt;

This alone was not enough (the training still crashed randomly at some point, however, it got sometimes a bit further in the training epochs).
I had to also specify
&lt;denchmark-code&gt;import tensorflow as tf
gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)
&lt;/denchmark-code&gt;

and  that that the input x given to the model in the model.fit() method must be divisible by the  (there must be no incomplete batch with less than  samples at the end), again following &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/37942&gt;#37942&lt;/denchmark-link&gt;
. After that, the training did not crash on a couple of run attempts.
However, this is only a workaround, which is quite annoying to do since it requires to add code that is needed only because of the Windows-related cuDNN bug.
2nd workaround - downgrading driver to 431.86
Multiple issues here (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/41863&gt;#41863&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/41444&gt;#41444&lt;/denchmark-link&gt;
) and on internet (&lt;denchmark-link:https://forums.developer.nvidia.com/t/cudnn-lstm-is-broken-above-driver-431-60-unexpected-event-status-1-cuda/108800&gt;https://forums.developer.nvidia.com/t/cudnn-lstm-is-broken-above-driver-431-60-unexpected-event-status-1-cuda/108800&lt;/denchmark-link&gt;
) related to this problem mention that the problem disappears if one fallbacks the NVIDIA driver version to 431.86. This version is not officially supported on my gpu (Quadro RTX 4000 notebook version), and NVIDIA does not directly even offer this specific version for this gpu (earliest available is 441.66). However, I still managed to install the unsupported version (found using some internet searches directly from NVIDIA's download repository), and the model training seems to work with this old, unsupported 431.86 driver version.
My other failed attempt - Tensorflow 2.3 compiled against CUDA 11 / cuDNN 8
I also tested to install Tensorflow 2.3 compiled for CUDA11.0 / cuDNN 8.0.2 from an unofficial wheel from here &lt;denchmark-link:https://github.com/fo40225/tensorflow-windows-wheel&gt;https://github.com/fo40225/tensorflow-windows-wheel&lt;/denchmark-link&gt;
. I had the specific CUDA 11 and cuDNN version installed and this Tensorflow 2.3 version compiled against these. In addition, I again tried all available NVIDIA driver versions versions (ranging from 441.66 to the 456.38) but I got the same error, so it seems that the problem cannot be solved by moving to a newer CUDA / cuDNN version.
		</comment>
		<comment id='42' author='jliebers' date='2020-10-17T15:38:15Z'>
		
Since I switched to Linux (Ubuntu 18.04) I never had a problem with this issue anymore. Therefore, I highly recommend anyone to switch OS if you want GPU-accelerated LSTMs. It is 100% connected to Windows only and I did not find any working solution (workaround or such) for Windows.
Only solution I know is to switch to Linux. The problem then disappears.
I mean this issue was closed, once I stated that I switched my OS but the underlying problem still exists. 🤷‍♂️

I am having still trouble on with same error on Ubuntu 18.04 actually.
		</comment>
		<comment id='43' author='jliebers' date='2020-10-30T00:39:33Z'>
		same here: "InternalError" on Windows and Ubuntu 18.04 on LSTM layers
		</comment>
		<comment id='44' author='jliebers' date='2020-11-05T06:55:51Z'>
		&lt;denchmark-link:https://github.com/AinTziLLo&gt;@AinTziLLo&lt;/denchmark-link&gt;
,
Could you please submit a new issue from &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/new/choose&gt;this link&lt;/denchmark-link&gt;
 and fill in the template, so that we can track the issue there. Thanks!
		</comment>
		<comment id='45' author='jliebers' date='2020-11-13T09:12:11Z'>
		I am not sure if this is actually the cause. I use window 10, TF 2.2. I had this problem as well. I did the recommended fix like setting batch input shape and letting GPU grow and so on, but it didn't work for me.
But when I turned off antivirus(McAfee_real-time scan), it's been working really good. It had been 3 days since I turned it off and I have not encountered this error again since. It sounds weird and stupid but I think it's worth the shot.
		</comment>
		<comment id='46' author='jliebers' date='2020-12-14T18:11:55Z'>
		We had the same issue; we reduce the embedding size (of the embedding layer) and we dont have any issue now.. We'll try to downgrade to the  431.86 driver version to see if we can keep the same embedding size.
		</comment>
		<comment id='47' author='jliebers' date='2020-12-17T10:20:31Z'>
		had same issue with bi-lstm, on windows 10, tensorflow 2.0.0, cuda 10.1 and cudnn 7.6.5.
it worked but got stuck at end of 1st epoch.
i deleted bi-lstm  and now its on the 3rd epoch.
		</comment>
	</comments>
</bug>