<bug id='33105' author='AlexisBRENON' open_date='2019-10-07T09:20:34Z' closed_time='2019-10-27T18:54:32Z'>
	<summary>tf.data.Dataset.interleave does not seem to respect num_parallel_calls argument</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 5.3.1-arch1-1-ARCH GNU/Linux
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0
Python version: 3.7.4
CUDA/cuDNN version: N/A
GPU model and memory: None

Describe the current behavior
While using tf.data.Dataset.interleave on a Dataset of 9 elements, with the following arguments:

cycle_length = 2
block_length = 1
num_parallel_calls = 2

6 threads seems to be launched concurrently.
Describe the expected behavior
I expect the function to be called concurrently by pair:
&lt;denchmark-code&gt;(call 1, call 2)
(call 3, call 4)
(call 5, call 6)
(call 7, call 8)
(call 9)
&lt;/denchmark-code&gt;

Code to reproduce the issue
import time
import timeit

import tensorflow as tf
from tensorflow.python.data.ops import dataset_ops
from tensorflow_core.python.data.ops.readers import _create_or_validate_filenames_dataset, \
    _create_dataset_reader
from tensorflow.python.framework import tensor_spec

# Clone of TFRecordDataset to add some customization (see creator_fn)
class MyTFRecordDataset(dataset_ops.DatasetV2):
    def _inputs(self):
        return self._impl._inputs()  # pylint: disable=protected-access

    @property
    def element_spec(self):
        return tensor_spec.TensorSpec([], tf.dtypes.string)

    def __init__(self, filenames, compression_type=None, buffer_size=None,
                 num_parallel_reads=None):
        filenames = _create_or_validate_filenames_dataset(filenames)

        self._filenames = filenames
        self._compression_type = compression_type
        self._buffer_size = buffer_size
        self._num_parallel_reads = num_parallel_reads

        def creator_fn(filename):
            tf.print("Creator_fn", filename.numpy())
            start = time.perf_counter()
            time.sleep(0.5)
            result = tf.constant([str(x) for x in range(7)])
            tf.print("Reading time", time.perf_counter() - start)
            return result

        self._impl = _create_dataset_reader(
            lambda x: tf.data.Dataset.from_tensor_slices(tf.py_function(creator_fn, [x], tf.string)),
            filenames, num_parallel_reads)
        variant_tensor = self._impl._variant_tensor  # pylint: disable=protected-access
        super(MyTFRecordDataset, self).__init__(variant_tensor)

def dataset_interleave_ds():
    ds = tf.data.Dataset.from_tensor_slices(
        [str(x) for x in range(9)]
    ).interleave(
        MyTFRecordDataset,
        cycle_length=2,
        block_length=1,
        num_parallel_calls=2
    )

    return ds

def main():
    ds = dataset_interleave_ds()

    def iterate():
        tf.print("Iterating")
        for i, s in ds.enumerate():
            tf.print("Iteration", i, s.shape)
            time.sleep(0.0)

    tf.print(timeit.timeit(iterate, number=1))


if __name__ == '__main__':
    main()
Other info / logs
With num_parallel_calls set to None IÂ obtain the expected behavior:
&lt;denchmark-code&gt;Iterating
Creator_fn b'0'
Reading time 0.5008154709994415
Iteration 0 TensorShape([])
Creator_fn b'1'
Reading time 0.5007261740011018
Iteration 1 TensorShape([])
&lt;/denchmark-code&gt;

Two files loaded, iterate over them
&lt;denchmark-code&gt;Iteration 2 TensorShape([])
Iteration 3 TensorShape([])
Iteration 4 TensorShape([])
Iteration 5 TensorShape([])
Iteration 6 TensorShape([])
Iteration 7 TensorShape([])
Iteration 8 TensorShape([])
Iteration 9 TensorShape([])
Iteration 10 TensorShape([])
Iteration 11 TensorShape([])
Iteration 12 TensorShape([])
Iteration 13 TensorShape([])
&lt;/denchmark-code&gt;

End of sequences. Open new files:
&lt;denchmark-code&gt;Creator_fn b'2'
Reading time 0.500802348000434
Iteration 14 TensorShape([])
Creator_fn b'3'
Reading time 0.5007076660003804
Iteration 15 TensorShape([])
Iteration 16 TensorShape([])
Iteration 17 TensorShape([])
Iteration 18 TensorShape([])
Iteration 19 TensorShape([])
Iteration 20 TensorShape([])
Iteration 21 TensorShape([])
Iteration 22 TensorShape([])
Iteration 23 TensorShape([])
Iteration 24 TensorShape([])
Iteration 25 TensorShape([])
Iteration 26 TensorShape([])
Iteration 27 TensorShape([])
&lt;/denchmark-code&gt;

And so on:
&lt;denchmark-code&gt;Creator_fn b'4'
Reading time 0.5007231710005726
Iteration 28 TensorShape([])
Creator_fn b'5'
Reading time 0.5007872259993746
[...]
4.591206809000141
&lt;/denchmark-code&gt;

Total time is ~4.5 seconds, according the 0.5s sleep executed 9 times.
With num_parallel_calls set to 2 I expect to divide my execution time by 2, each pair of "files" being read in parallel.
&lt;denchmark-code&gt;Iterating
Creator_fn b'0'
Creator_fn b'1'
Creator_fn b'3'
Creator_fn b'5'
Creator_fn b'4'
Creator_fn b'2'
Reading time 0.5008060520012805
Reading time 0.5010762649999378
Reading time 0.501729752000756
Reading time 0.5024584279999544
Reading time 0.503098459001194
Reading time 0.5040528110002924
&lt;/denchmark-code&gt;

6 Files opened in parallel...
&lt;denchmark-code&gt;Iteration 0 TensorShape([])
[...]
Iteration 14 TensorShape([])
Creator_fn b'7'
Creator_fn b'6'
Iteration 15 TensorShape([])
[...]
Iteration 28 TensorShape([])
Creator_fn b'8'
Iteration 29 TensorShape([])
[...]
Iteration 41 TensorShape([])
Reading time 0.5002393860013399
Reading time 0.5006544690004375
Iteration 42 TensorShape([])
Iteration 43 TensorShape([])
Iteration 44 TensorShape([])
Iteration 45 TensorShape([])
Iteration 46 TensorShape([])
Reading time 0.5014183660005074
Iteration 47 TensorShape([])
[...]
Iteration 62 TensorShape([])
1.047351510000226
&lt;/denchmark-code&gt;

Execution time divided by 4!
By the way, what is the meaning of num_parallel_calls set to 1. 1 parallel call is 2 threads or it should be sequential?
	</description>
	<comments>
		<comment id='1' author='AlexisBRENON' date='2019-10-09T11:44:55Z'>
		Could reproduce the issue with Tensorflow Version 2.0. Here is the &lt;denchmark-link:https://colab.sandbox.google.com/gist/rmothukuru/4c2086c7e770d6a51d83983208ed88f8/33105.ipynb&gt;Gist&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='AlexisBRENON' date='2019-10-24T23:01:31Z'>
		Parallel interleave with num_parallel_calls=X also ends up using 2*X background threads to prefetch small amount of data for future cycle elements ahead time. The primary motivation for this is to overlap opening files in remote storage systems (e.g. GCS) with useful computation.
In other words, as the size of the file you interleave over increases, the speed up you would see from using num_parallel_calls=X would approach X in the limit.
num_paralle_calls=1 is different from num_parallel_calls=None. The former will use a kernel that decouples (and thus parallelizes the computation of) the producer and consumer (and also performs the prefetching described above), while the latter will use a kernel the performs the interleave synchronously using the same thread for the producer and consumer.
Let me know if you have any additional questions. If not, please close the issue.
		</comment>
		<comment id='3' author='AlexisBRENON' date='2019-10-27T18:54:32Z'>
		OK. The observed behavior is probably linked to the prefetching mechanism.
Thanks for the clarification about "1 vs. None".
		</comment>
		<comment id='4' author='AlexisBRENON' date='2019-10-27T18:54:33Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33105&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33105&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>