<bug id='19186' author='brge17' open_date='2018-05-09T17:36:48Z' closed_time='2018-05-14T19:38:19Z'>
	<summary>[tf.keras] Bug with Stateful Metrics &amp; Fit Generator</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
TensorFlow installed from (source or binary): Source
TensorFlow version (use command below): 1.8.0
Python version:  2.7
Bazel version (if compiling from source): n/a
GCC/Compiler version (if compiling from source): n/a
CUDA/cuDNN version: n/a
GPU model and memory: n/a
Exact command to reproduce: n/a

The stateful metrics integration with fit generator is not working. This can be demonstrated by taking the Keras metrics tests from the latest release and changing the imports to TensorFlow.keras (see &lt;denchmark-link:https://github.com/keras-team/keras/blob/master/tests/keras/metrics_test.py&gt;https://github.com/keras-team/keras/blob/master/tests/keras/metrics_test.py&lt;/denchmark-link&gt;
):
&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow.python.keras import backend as K

import numpy as np

class BinaryTruePositives(tf.keras.layers.Layer):
    """Stateful Metric to count the total true positives over all batches.
    Assumes predictions and targets of shape `(samples, 1)`.
    # Arguments
        name: String, name for the metric.
    """

    def __init__(self, name='true_positives', **kwargs):
        super(BinaryTruePositives, self).__init__(name=name, **kwargs)
        self.stateful = True
        self.true_positives = K.variable(value=0, dtype='int32')

    def reset_states(self):
        K.set_value(self.true_positives, 0)

    def __call__(self, y_true, y_pred):
        """Computes the number of true positives in a batch.
        # Arguments
            y_true: Tensor, batch_wise labels
            y_pred: Tensor, batch_wise predictions
        # Returns
            The total number of true positives seen this epoch at the
                completion of the batch.
        """
        y_true = K.cast(y_true, 'int32')
        y_pred = K.cast(K.round(y_pred), 'int32')
        correct_preds = K.cast(K.equal(y_pred, y_true), 'int32')
        true_pos = K.cast(K.sum(correct_preds * y_true), 'int32')
        current_true_pos = self.true_positives * 1
        self.add_update(K.update_add(self.true_positives,
                                     true_pos),
                        inputs=[y_true, y_pred])
        return current_true_pos + true_pos

# Test on simple model
inputs = tf.keras.Input(shape=(2,))
outputs = tf.keras.layers.Dense(1, activation='sigmoid', name='out')(inputs)
model = tf.keras.Model(inputs, outputs)

model.compile(optimizer='sgd',
              loss='binary_crossentropy',
              metrics=['acc', BinaryTruePositives()])

samples = 1000
x = np.random.random((samples, 2))
y = np.random.randint(2, size=(samples, 1))

val_samples = 10
val_x = np.random.random((val_samples, 2))
val_y = np.random.randint(2, size=(val_samples, 1))

# Test fit and evaluate
history = model.fit(x, y, validation_data=(val_x, val_y), epochs=2, batch_size=10)
outs = model.evaluate(x, y, batch_size=10)
preds = model.predict(x)

def ref_true_pos(y_true, y_pred):
    return np.sum(np.logical_and(y_pred &gt; 0.5, y_true == 1))

# Test correctness (e.g. updates should have been run)
np.testing.assert_allclose(outs[2], ref_true_pos(y, preds), atol=1e-5)

# Test correctness of the validation metric computation
val_preds = model.predict(val_x)
val_outs = model.evaluate(val_x, val_y, batch_size=10)
np.testing.assert_allclose(val_outs[2], ref_true_pos(val_y, val_preds), atol=1e-5)
np.testing.assert_allclose(val_outs[2], history.history['val_true_positives'][-1], atol=1e-5)

# Test with generators
gen = [(np.array([x0]), np.array([y0])) for x0, y0 in zip(x, y)]
val_gen = [(np.array([x0]), np.array([y0])) for x0, y0 in zip(val_x, val_y)]
history = model.fit_generator(iter(gen), epochs=1, steps_per_epoch=samples,
                              validation_data=iter(val_gen), validation_steps=val_samples)
outs = model.evaluate_generator(iter(gen), steps=samples)
preds = model.predict_generator(iter(gen), steps=samples)

# Test correctness of the metric re ref_true_pos()
np.testing.assert_allclose(outs[2], ref_true_pos(y, preds), atol=1e-5)

# Test correctness of the validation metric computation
val_preds = model.predict_generator(iter(val_gen), steps=val_samples)
val_outs = model.evaluate_generator(iter(val_gen), steps=val_samples)
np.testing.assert_allclose(val_outs[2], ref_true_pos(val_y, val_preds), atol=1e-5)
np.testing.assert_allclose(val_outs[2], history.history['val_true_positives'][-1], atol=1e-5)
&lt;/denchmark-code&gt;

In addition the progress bar is not working with fit generator. &lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='brge17' date='2018-05-09T17:43:16Z'>
		I believe the equivalent tensorflow PR of: &lt;denchmark-link:https://github.com/keras-team/keras/pull/9446&gt;keras-team/keras#9446&lt;/denchmark-link&gt;
 is missing. But this is beyond my capability at the time being (I have to get approval for each open source contribution...)
		</comment>
		<comment id='2' author='brge17' date='2018-05-14T15:29:39Z'>
		The two key issues:


reset_states does not get called when fit_generator is used


Progress bar issues.


See  &lt;denchmark-link:https://github.com/keras-team/keras/pull/9446&gt;keras-team/keras#9446&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='brge17' date='2018-05-14T18:54:07Z'>
		&lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;
 Can you take a look at this?
		</comment>
		<comment id='4' author='brge17' date='2018-05-14T19:38:19Z'>
		Thanks for the report. This has been fixed internally a few days ago. It appears the fix is not yet on GitHub, but it should appear in the next few days.
		</comment>
	</comments>
</bug>