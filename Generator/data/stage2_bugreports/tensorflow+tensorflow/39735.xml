<bug id='39735' author='gtjemwa' open_date='2020-05-20T22:16:31Z' closed_time='2020-05-23T08:53:27Z'>
	<summary>ValueError when attempting to use adapt method from TextVectorization layer</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
TensorFlow installed from (source or binary): pip install tf-nightly
TensorFlow version (use command below): v1.12.1-32169-gf7d038cc3b 2.3.0-dev20200519
Python version: 3.6.9
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

Describe the current behavior
I've tried running the following two text classification guides:

https://keras.io/examples/nlp/text_classification_from_scratch/
https://keras.io/examples/nlp/pretrained_word_embeddings/

In each case, when I reach the call to the adapt  method, e.g. vectorizer.adapt(text_ds), the following error is raised:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/preprocessing/index_lookup.py in set_vocabulary(self, vocab)
282           "Attempted to set a vocabulary larger than the maximum vocab size. "
283           "Passed vocab size is %s, max vocab size is %s." %
--&gt; 284           (total_vocab_size, self.max_tokens))
285
286     start_index = num_special_tokens
ValueError: Attempted to set a vocabulary larger than the maximum vocab size. Passed vocab size is 20001, max vocab size is 20000

Describe the expected behavior
The error should not be raised.
Standalone code to reproduce the issue
&lt;denchmark-code&gt;from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
import numpy as np

#define a set of docs as per https://machinelearningmastery.com/ example
docs = np.array(['Well done!',
		'Good work',
		'Great effort',
		'nice work',
		'Excellent!',
		'Weak',
		'Poor effort!',
		'not good',
		'poor work',
		'Could have done better.'])

vectorizer = TextVectorization(max_tokens=5, output_sequence_length=4)
text_ds = tf.data.Dataset.from_tensor_slices(docs)
vectorizer.adapt(text_ds)

&lt;/denchmark-code&gt;

Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

ValueError                                Traceback (most recent call last)
 in ()
3 vectorizer = TextVectorization(max_tokens=5, output_sequence_length=4)
4 text_ds = tf.data.Dataset.from_tensor_slices(docs)
----&gt; 5 vectorizer.adapt(text_ds)
4 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/preprocessing/text_vectorization.py in adapt(self, data, reset_state)
408           "adapt() requires a Dataset or an array as input, got {}".format(
409               type(data)))
--&gt; 410     super(TextVectorization, self).adapt(preprocessed_inputs, reset_state)
411
412   def get_vocabulary(self):
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_preprocessing_layer.py in adapt(self, data, reset_state)
203
204     updates = self._combiner.extract(accumulator)
--&gt; 205     self._set_state_variables(updates)
206
207   def _set_state_variables(self, updates):
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/preprocessing/text_vectorization.py in _set_state_variables(self, updates)
521           updates[_OOV_IDF_NAME])
522     else:
--&gt; 523       self.set_vocabulary(updates[_VOCAB_NAME])
524
525   def _preprocess(self, inputs):
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/preprocessing/text_vectorization.py in set_vocabulary(self, vocab, df_data, oov_df_value)
471                           "called.").format(mode=self._output_mode))
472
--&gt; 473     self._index_lookup_layer.set_vocabulary(vocab)
474
475     # When doing raw or integer output, we don't have a Vectorize layer to
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/preprocessing/index_lookup.py in set_vocabulary(self, vocab)
282           "Attempted to set a vocabulary larger than the maximum vocab size. "
283           "Passed vocab size is %s, max vocab size is %s." %
--&gt; 284           (total_vocab_size, self.max_tokens))
285
286     start_index = num_special_tokens
ValueError: Attempted to set a vocabulary larger than the maximum vocab size. Passed vocab size is 6, max vocab size is 5.
	</description>
	<comments>
		<comment id='1' author='gtjemwa' date='2020-05-21T06:25:01Z'>
		&lt;denchmark-link:https://github.com/gtjemwa&gt;@gtjemwa&lt;/denchmark-link&gt;

I think  tokens in docs is 17 in your example.Hence max tokens should be considered greater than 17 then you will not have this issue.Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/0b698009a433c05664652475c7da156e/untitled922.ipynb&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='2' author='gtjemwa' date='2020-05-21T09:56:55Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;

Thanks for the response. Correct, setting max tokens to None or greater than number of unique words in the docs, everything does work. However, my understanding is that is I set a value for max_tokens, and the vocabulary is then determined by word frequency, with less frequently occurring tokens considered OOV.  This is the same approach in the demo example here &lt;denchmark-link:https://keras.io/examples/nlp/text_classification_from_scratch&gt;https://keras.io/examples/nlp/text_classification_from_scratch&lt;/denchmark-link&gt;
, where the max_tokens is set to 20000, but we still get the same ValueError.
		</comment>
		<comment id='3' author='gtjemwa' date='2020-05-22T23:59:30Z'>
		&lt;denchmark-link:https://github.com/gtjemwa&gt;@gtjemwa&lt;/denchmark-link&gt;
 I cannot reproduce the error. Am i missing something? Can you please check the &lt;denchmark-link:https://colab.research.google.com/gist/jvishnuvardhan/3cc2e418a749f163298e95d13777ee7f/untitled922.ipynb&gt;gist here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='4' author='gtjemwa' date='2020-05-23T08:53:27Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;

Looks like the error doesn't occur in 2.3.0-dev20200522 (it was on 2.3.0-dev20200519 that I was having the issue.
Thanks. I'll close this issue
		</comment>
		<comment id='5' author='gtjemwa' date='2020-05-23T08:53:29Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39735&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39735&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>