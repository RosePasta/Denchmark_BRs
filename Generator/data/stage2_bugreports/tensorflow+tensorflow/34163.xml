<bug id='34163' author='feihugis' open_date='2019-11-11T18:27:23Z' closed_time='2020-04-11T20:55:02Z'>
	<summary>Do not get performance improvement on tf.matmul when building with AVX2</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7
TensorFlow version (use command below): v1.14
Python version: 3.6.5
Bazel version (if compiling from source): 0.26.1
GCC/Compiler version (if compiling from source): 8.3.1

Describe the current behavior
Run the benchmarking tests for tf.matmul operation under different TensorFlow packages:

the pip package v1.14 released by Tensorflow.
This package is not compiled with AVX2 as indicated by this log info tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
built the TensorFlow package with AVX2 from source using the command bazel build -c opt --copt=-march=native //tensorflow/tools/pip_package:build_pip_package.

However, there is no big performance difference between the above two packages:


tensorflow-v1.14-cpu
8192 x 8192 matmul took: 1.54 sec, 713.30 G ops/sec


build from source with AVX2
8192 x 8192 matmul took: 1.60 sec, 687.73 G ops/sec


Describe the expected behavior
The benchmark with the second package should be faster than the first one.

Borrow the benchmark code from &lt;denchmark-link:https://stackoverflow.com/a/41810634/3471050&gt;here&lt;/denchmark-link&gt;
.
import os
import sys
import tensorflow as tf
import time

n = 8192
dtype = tf.float32

matrix1 = tf.Variable(tf.ones((n, n), dtype=dtype))
matrix2 = tf.Variable(tf.ones((n, n), dtype=dtype))
product = tf.matmul(matrix1, matrix2)


# avoid optimizing away redundant nodes
config = tf.ConfigProto(graph_options=tf.GraphOptions(optimizer_options=tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0)))
sess = tf.Session(config=config)

import os
import sys
import tensorflow as tf
import time

n = 8192
dtype = tf.float32

matrix1 = tf.Variable(tf.ones((n, n), dtype=dtype))
matrix2 = tf.Variable(tf.ones((n, n), dtype=dtype))
product = tf.matmul(matrix1, matrix2)


# avoid optimizing away redundant nodes
config = tf.ConfigProto(graph_options=tf.GraphOptions(optimizer_options=tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0)))
sess = tf.Session(config=config)


sess.run(tf.global_variables_initializer())
iters = 10

# pre-warming
sess.run(product.op)

start = time.time()
for i in range(iters):
  sess.run(product.op)
end = time.time()
ops = n**3 + (n-1)*n**2 # n^2*(n-1) additions, n^3 multiplications
elapsed = (end - start)
rate = iters*ops/elapsed/10**9
print('\n %d x %d matmul took: %.2f sec, %.2f G ops/sec' % (n, n,
                                                            elapsed/iters,
                                                            rate,))
	</description>
	<comments>
		<comment id='1' author='feihugis' date='2019-11-11T18:40:16Z'>
		cc: &lt;denchmark-link:https://github.com/angerson&gt;@angerson&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/yifeif&gt;@yifeif&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='feihugis' date='2020-04-10T22:46:31Z'>
		Sorry for the delay, had a quick chat with Penporn and she mentioned that MKL is already using JIT to run the most optimized code for your cpu architecture, which might explained why you are not seeing a performance boost. cc: &lt;denchmark-link:https://github.com/penpornk&gt;@penpornk&lt;/denchmark-link&gt;
 in case she has more to add. Thanks!
		</comment>
		<comment id='3' author='feihugis' date='2020-04-10T23:08:11Z'>
		Yes to what Yifei said. That message is meant for non-matmul/convolution ops in TensorFlow.
TF has been using &lt;denchmark-link:https://github.com/oneapi-src/oneDNN&gt;oneDNN&lt;/denchmark-link&gt;
 (new name of MKL-DNN) matrix multiplication routine (&lt;denchmark-link:https://cs.opensource.google/tensorflow/tensorflow/+/eb2df6e1addf161595950ec7f280f95596854cb8:tensorflow/core/kernels/eigen_contraction_kernel.h;l=166&gt;sgemm&lt;/denchmark-link&gt;
) as a building block for matmul and convolution operations since release 1.13. sgemm detects the CPU architecture at runtime and generates the most appropriate vector instructions the CPU is capable of, e.g., AVX-512. So, the generic TF wheel can already use up to AVX-512 instructions in matmul/convolution ops even when they are built with just AVX. That's why you didn't see the performance improvement.
Sorry for the confusing message. It was outdated. Fixed that in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/95f92b6d33b7b405c5c327a68c216a5a5b5d9f93&gt;95f92b6&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='feihugis' date='2020-04-11T20:54:56Z'>
		&lt;denchmark-link:https://github.com/yifeif&gt;@yifeif&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/penpornk&gt;@penpornk&lt;/denchmark-link&gt;
 Thanks very much for your detailed answers! Good to know these information. Close this issue as it has been resolved.
		</comment>
		<comment id='5' author='feihugis' date='2020-04-11T20:55:03Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34163&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34163&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>