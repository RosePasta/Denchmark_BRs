<bug id='2732' author='shiviser' open_date='2016-06-08T14:38:47Z' closed_time='2018-09-18T17:10:04Z'>
	<summary>Mention that GPU reductions are nondeterministic in docs</summary>
	<description>
&lt;denchmark-h:h1&gt;The problem&lt;/denchmark-h&gt;

I am trying out the &lt;denchmark-link:https://www.tensorflow.org/versions/r0.7/tutorials/mnist/pros/index.html#deep-mnist-for-experts&gt;MNIST for experts tutorial&lt;/denchmark-link&gt;
 and I have inconsistent results on the GPU.
&lt;denchmark-h:h3&gt;What do I mean by inconsistent?&lt;/denchmark-h&gt;

With the exactly same network parameters (and randomness removed: read below in the post) every time I run the complete train-then-test process the accuracy is slightly different.
&lt;denchmark-h:h3&gt;What have I done to visualize this problem?&lt;/denchmark-h&gt;

For each iteration, I have calculated the differences between the variables (weights, biases) from two independent but identical runs and computed the L1 norm of those differences - 

plot of L1 norm for the first 1000 iterations in steps of 20.

In a consistent world, these differences should be always zero!
&lt;denchmark-h:h3&gt;How did I remove randomness in the code?&lt;/denchmark-h&gt;


Removed dropout entirely
added a graph level seed (tf.set_random_seed(1234)). With this the variable initialization is deterministic and also any other randomization in the code.
The MNIST for experts tutorial uses this script to download/load the MNIST data. I have added numpy.random.seed(3) in DataSet.__init__(self, images, labels, fake_data=False, one_hot=False, dtype=dtypes.float32) in this script to remove randomness during the shuffling process (line 154 in DataSet.next_batch(self, batch_size, fake_data=False))
config = tf.ConfigProto(inter_op_parallelism_threads=1, intra_op_parallelism_threads=1) which goes into the creation of session as sess = tf.Session(config=config)

&lt;denchmark-h:h3&gt;What system am I using?&lt;/denchmark-h&gt;


tensorflow 0.8 gpu version (installed via pip)
OpenSUSE LEAP 42.1 (x86_64)
Cuda Toolkit 7.5
CuDNN 4.0
Tesla K20c card with Nvidia driver 352.79

	</description>
	<comments>
		<comment id='1' author='shiviser' date='2016-06-08T15:49:32Z'>
		Do you get deterministic results if you run this on CPU? Results of optimized GPU computations for NN ops are usually a little bit non-deterministic, I think it's a nature of how modern GPUs work, &lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
 may have more understanding of this
		</comment>
		<comment id='2' author='shiviser' date='2016-06-08T16:14:11Z'>
		I have the same observation on both GPU and CPU. I think whenever parallel computing is use (either multiple core CPU or GPU), the results will not be deterministic. This is due to the randomness of the order of collecting the partial results from all threads, and this can lead to very small difference within machine accuracy at the beginning and then this tiny tiny difference gets amplified during iterations.
		</comment>
		<comment id='3' author='shiviser' date='2016-06-08T16:14:19Z'>
		&lt;denchmark-link:https://github.com/yaroslavvb&gt;@yaroslavvb&lt;/denchmark-link&gt;
 Yes I do get deterministic results on the CPU.
The problem is, in a bigger network that I have designed for another dataset the inconsistencies are quite large (up to +/-25% around the mean).
		</comment>
		<comment id='4' author='shiviser' date='2016-06-08T16:19:32Z'>
		&lt;denchmark-link:https://github.com/jiaboli007&gt;@jiaboli007&lt;/denchmark-link&gt;
 I have seen that it is possible to have parallel computations and yet have deterministic behaviour (e.g.: Matlab simulink can optimize models for parallel computations yet assuring deterministic behaviour).
		</comment>
		<comment id='5' author='shiviser' date='2016-06-08T17:12:00Z'>
		On GPU, small amount of non-deterministic results is expected. TensorFlow uses the Eigen library, which uses Cuda atomics to implement reduction operations, such as tf.reduce_sum etc. Those operations are non-determnistical. Each operation can introduce a small difference. If your model is not stable, it could accumulate into large errors, after many steps.
If you see a large difference, after one or two operations, it would be problematic. Otherwise, it is somewhat expected. Regularizers such as dropout helps the model tolerate that.
		</comment>
		<comment id='6' author='shiviser' date='2016-06-08T21:47:49Z'>
		This is expected behavior: see &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/2652&gt;#2652&lt;/denchmark-link&gt;
 for more discussion.
		</comment>
		<comment id='7' author='shiviser' date='2016-06-09T15:35:44Z'>
		&lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
 I have already tried dropout, it didn't help. After a bit of research over the internet, I have come to know that torch had a similar &lt;denchmark-link:https://github.com/torch/cunn/issues/84&gt;non-deterministic behaviour&lt;/denchmark-link&gt;
 with their SpatialMaxPooling operation but it has been fixed now. Perhaps something along the lines?
&lt;denchmark-link:https://github.com/girving&gt;@girving&lt;/denchmark-link&gt;
 Also on the &lt;denchmark-link:https://github.com/torch/cunn/issues/84&gt;same post&lt;/denchmark-link&gt;
 one can read that Caffe has already acomplished deterministic backward passes. Having said these, and that this "non-deterministic behaviour" is neither fixed nor mentioned in the documentation yet, I would like to re-open the issue.
		</comment>
		<comment id='8' author='shiviser' date='2016-06-09T15:40:11Z'>
		Reopened: We'd be happy to accept a PR adding a note to this effect.
		</comment>
		<comment id='9' author='shiviser' date='2016-06-09T15:46:38Z'>
		&lt;denchmark-link:https://github.com/girving&gt;@girving&lt;/denchmark-link&gt;
 Thank you for re-opening the issue but I believe tensorflow could do better than just documenting this, what seems to be an unintended non-deterministic behaviour. Also, I would have loved to work on the PR but will be too much digression from my work at the moment.
P.S.: Don't know if you saw my last edit - on the &lt;denchmark-link:https://github.com/torch/cunn/issues/84&gt;same post&lt;/denchmark-link&gt;
 one can read that Caffe has already accomplished deterministic backward passes. Perhaps this is interesting to you!
		</comment>
		<comment id='10' author='shiviser' date='2016-06-09T15:50:35Z'>
		&lt;denchmark-link:https://github.com/girving&gt;@girving&lt;/denchmark-link&gt;
 Also we are not yet sure if it is actually the GPU reductions and not something else which is causing the non-deterministic behaviour - this need verification before allowing entry into the docs.
		</comment>
		<comment id='11' author='shiviser' date='2016-06-09T15:57:11Z'>
		&lt;denchmark-link:https://github.com/shiviser&gt;@shiviser&lt;/denchmark-link&gt;
 Let us know what you find out!
		</comment>
		<comment id='12' author='shiviser' date='2016-06-09T16:21:48Z'>
		&lt;denchmark-link:https://github.com/shiviser&gt;@shiviser&lt;/denchmark-link&gt;
, that particular kernel was fixed to be determnistics. I believe TensorFlow picked up the same fix. However, other Cudnn conv algorithms are still non-deterministic, since they use atomics inherently. Most frameworks may encounter them depending on the input and kernel shapes.
That being said, if your investigation reveals something else that is causing the problem, a PR to address the doc and the code is welcome.
		</comment>
		<comment id='13' author='shiviser' date='2017-05-12T03:51:20Z'>
		BUILD
TensorFlow GPU 1.1.0
Ubuntu 16.04
CUDA 8.0.61
CUDNN 5.1
NVIDIA Drivers 375
2 GTX 1080Ti, with 1 1080Ti used for 2 monitors. I set CUDA_VISIBLE_DEVICES=0 for all experiments. This uses the GPU with nothing attached.
&lt;denchmark-link:https://github.com/zheng-xq&gt;@zheng-xq&lt;/denchmark-link&gt;
 why is the forward pass deterministic but the very first backward pass not deterministic?
On a simple MNIST example, the logits are 100% deterministic over 100 runs (I did more runs than that but plotted 100).
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/6997460/25981433/a9473ee0-3707-11e7-8a60-240963c6c471.png&gt;&lt;/denchmark-link&gt;

However, when we look at the gradients computed on the last set of variables, we see small errors of scale 1e-8.
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/6997460/25981444/c9561e0e-3707-11e7-8ce9-20ced60186e2.png&gt;&lt;/denchmark-link&gt;

I proceeded to do a simpler example of simply trying to train a neural network to add.
Inputs: 10x1 (numpy generated with same seed)
Weights: 10x1 (random normal initialized with same seed)
Labels: Sum of the 10x1 input.
Effectively, the neural network is trying to tune the weights such that they all become 1.0.
In fact, it's strange because the randomness has some form of determinism, as seen in the graph below.
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/6997460/25981528/4ea32fca-3708-11e7-98b0-a6abcfd58347.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://cloud.githubusercontent.com/assets/6997460/25981532/516018a4-3708-11e7-8712-6b3824a664cf.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://cloud.githubusercontent.com/assets/6997460/25981535/54733dc8-3708-11e7-9fe8-9911f64942c5.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://cloud.githubusercontent.com/assets/6997460/25981537/596a5de8-3708-11e7-83fe-b10af531501b.png&gt;&lt;/denchmark-link&gt;

Furthermore, the anomalous gradients are consistently the same throughout the runs!
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/6997460/25981559/7d6c0926-3708-11e7-849d-bc30beb2591f.png&gt;&lt;/denchmark-link&gt;

And they have the exact same error.
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/6997460/25981796/e71ee284-3709-11e7-91f2-e0777bd6f48b.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/asimshankar&gt;@asimshankar&lt;/denchmark-link&gt;
 mentioned &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/5527&gt;here&lt;/denchmark-link&gt;
 that mismatches between CUDA and Drivers could be the problem. So I upgraded my driver to 381, and then I got these results. 1 - 7 gradients have errors, but previously, only 5 had errors and when errors happened, these 5 gradients had exactly the same values.
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/6997460/25981589/b574d00a-3708-11e7-8fc9-22e0575ec94b.png&gt;&lt;/denchmark-link&gt;

I haven't looked deeply into the exact 7 gradients that have errors but a brief glance showed that they are the same as the 5 before and more.
Could it be the GPU reduce order? Or could there be errors in the computation of gradients themselves?
If anyone has any insight into what experiments to try next, I'll be glad to do them and post the results here.
		</comment>
		<comment id='14' author='shiviser' date='2017-05-12T14:11:58Z'>
		You could try a simple feed-forward network with squared loss. Training can be implemented with only matmul and reduce_sum, if there's non-determinism there, then pretty much everything is potentially non-deterministic. Addition of floating point numbers is not-associative, so if the other of summing things together in matmul/reduce_sum changes, that can affect results. Typically this is not considered a bug as long as relative error of results stays within machine epsilon (1e-7 for float32). Note that multiplication changes the scale of relative error -- ie, if you have an initial error in result of 1e-7, and multiply result by 10^6, the absolute error blows up, but relative error stays the same
		</comment>
		<comment id='15' author='shiviser' date='2017-05-12T14:55:54Z'>
		&lt;denchmark-link:https://github.com/yaroslavvb&gt;@yaroslavvb&lt;/denchmark-link&gt;
 I'll try that out and post some results here.
&lt;denchmark-link:https://github.com/girving&gt;@girving&lt;/denchmark-link&gt;
 any other things to try so we can include in the docs? I'd be happy to do a PR for this if it's within my capabilities.
		</comment>
		<comment id='16' author='shiviser' date='2017-05-12T19:08:18Z'>
		&lt;denchmark-link:https://github.com/jkschin&gt;@jkschin&lt;/denchmark-link&gt;
 Not sure what you mean by other things.  If the PR is just adding a note that reductions are nondeterministic, keeping is small seems good.
More broadly, I'd love a determinism push to make everything we can actually deterministic, but that will take more work.
		</comment>
		<comment id='17' author='shiviser' date='2017-05-15T01:09:07Z'>
		&lt;denchmark-link:https://github.com/yaroslavvb&gt;@yaroslavvb&lt;/denchmark-link&gt;
 I realized I'm already doing a simple feed-forward network above.
def add_model(v):
    w = tf.get_variable(name='w', shape=[vector_size],
            dtype=tf.float32,
            initializer=tf.random_normal_initializer)
    output = tf.reduce_sum(tf.multiply(w, v), axis=1)
    tf.add_to_collection('weights', w)
    return output
&lt;denchmark-link:https://github.com/girving&gt;@girving&lt;/denchmark-link&gt;
 yeah you mentioned adding a note to this effect. Where should it be added? Agree on a deterministic push!
		</comment>
		<comment id='18' author='shiviser' date='2017-05-15T01:46:36Z'>
		&lt;denchmark-link:https://github.com/jkschin&gt;@jkschin&lt;/denchmark-link&gt;
 as an end-user who got tripped up by non-determinism, you might have an idea where this note should go, so that other people like you would see it
		</comment>
		<comment id='19' author='shiviser' date='2017-05-16T01:21:38Z'>
		Would a post &lt;denchmark-link:https://www.tensorflow.org/performance/&gt;here&lt;/denchmark-link&gt;
 titled "Non-determinism in TensorFlow" with some example code help?
		</comment>
		<comment id='20' author='shiviser' date='2017-05-16T15:50:11Z'>
		Yes!  &lt;denchmark-link:https://github.com/martinwicke&gt;@martinwicke&lt;/denchmark-link&gt;
: That a good place for an overview of TensorFlow nondeterminism?
		</comment>
		<comment id='21' author='shiviser' date='2017-05-16T15:57:52Z'>
		I think it should go into the programmer's guide section, but &lt;denchmark-link:https://github.com/wolffg&gt;@wolffg&lt;/denchmark-link&gt;
 has a better overview of where to put this.
Either way, the content would be the same, and we'd be enthusiastic about it.
		</comment>
		<comment id='22' author='shiviser' date='2017-05-27T21:53:46Z'>
		Check out a &lt;denchmark-link:https://www.twosigma.com/insights/a-workaround-for-non-determinism-in-tensorflow&gt;workaround&lt;/denchmark-link&gt;
 for training a simple fully-connected net with repeatable results on the GPU. It's no panacea, but it seems relevant to the discussion! One caveat is that the reduce_sum replacement in the workaround does not handle partial reduction (i.e. it does not accept an axis argument). This is more a proof of concept, but shows it can be done. Perhaps a slower but deterministic reduce_sum backend for the GPU can be added to TensorFlow that does not rely on CUDA atomics, and an option to Session can determine whether reduce_sum uses the deterministic or fast backend (but haven't studied the TF architecture enough to know if this is feasible).
		</comment>
		<comment id='23' author='shiviser' date='2018-04-25T16:12:26Z'>
		&lt;denchmark-link:https://github.com/ekelsen&gt;@ekelsen&lt;/denchmark-link&gt;
 is this still expected?
It's fine for these to be non-deterministic, but we should mention it in the docs if so.
		</comment>
		<comment id='24' author='shiviser' date='2018-04-25T21:26:58Z'>
		Happy to re-open this PR and refine it: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/10636&gt;#10636&lt;/denchmark-link&gt;
. Thoughts &lt;denchmark-link:https://github.com/martinwicke&gt;@martinwicke&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='25' author='shiviser' date='2018-05-04T00:11:38Z'>
		Sounds good. Thank you!
		</comment>
		<comment id='26' author='shiviser' date='2018-05-07T14:13:54Z'>
		&lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/10636&gt;#10636&lt;/denchmark-link&gt;
 is outdated so I won't be refining it.
It seems like both  and  are deterministic now. &lt;denchmark-link:https://github.com/nikonikolov&gt;@nikonikolov&lt;/denchmark-link&gt;
 do you have a small example to reproduce the non-deterministic behaviour?
		</comment>
		<comment id='27' author='shiviser' date='2018-05-07T22:02:13Z'>
		&lt;denchmark-link:https://github.com/jkschin&gt;@jkschin&lt;/denchmark-link&gt;
 Unfortunately I do not have a simple one. Are  operators determinisitc on GPU now? If they are supposed to be, I can dig into it and compose an example. I am using much more operators than  and  so there is a chance there is another operator causing the non-deterministic behavior.
Also which version exactly has the deterministic behavior?
		</comment>
		<comment id='28' author='shiviser' date='2018-05-14T17:41:24Z'>
		&lt;denchmark-link:https://github.com/jkschin&gt;@jkschin&lt;/denchmark-link&gt;
 Assuming that  and  are now deterministic, are all gradient based operations deterministic too (assuming the same data is passed to the neural network). I am still experiencing the problem in TF 1.8 and the non-determinism happens after a gradient step is taken. I can try to provide example, but first wanted to make sure determinism is expected with gradients.
		</comment>
		<comment id='29' author='shiviser' date='2018-05-14T17:54:26Z'>
		The gradient pass often contains ops that are rare in the forward pass. Is there a way to narrow this down further? There may be a reduction or something else which is non-deterministic.
We probably cannot claim that all of TF is deterministic yet.
		</comment>
		<comment id='30' author='shiviser' date='2018-05-14T21:04:37Z'>
		Hey, so here is a relatively simple example to reproduce. I tried it on CPU and results were the same. However, on GPU they were not. When I decreased number of iterations to 1 or 2, all the time I was able to get the same results (although I did not do a lot of runs). When the number of iterations is higher, say &gt;10, I almost always get different results. Also when there is only 1 dense layer, the results are almost always reproducible.
&lt;denchmark-code&gt;import numpy as np
import tensorflow as tf

ITERATIONS=20

tf.set_random_seed(42)
np.random.seed(42)

x_data = np.random.normal(size=[32, 10])
y_data = np.random.normal(size=[32, 1])
x_test = np.random.normal(size=[32, 10])

x_in  = tf.placeholder(tf.float32, [None, 10])
y_in  = tf.placeholder(tf.float32, [None, 1])
x     = x_in
x     = tf.layers.dense(x, 200, tf.nn.relu)
x     = tf.layers.dense(x, 1, tf.nn.relu)
loss  = tf.losses.mean_squared_error(y_in, x)

mvars = tf.get_default_graph().get_collection(tf.GraphKeys.GLOBAL_VARIABLES)

opt   = tf.train.AdamOptimizer(use_locking=True)
train = opt.minimize(loss)
config= tf.ConfigProto(inter_op_parallelism_threads=1, intra_op_parallelism_threads=1)
sess  = tf.Session(config=config)

allvars = tf.get_default_graph().get_collection(tf.GraphKeys.GLOBAL_VARIABLES)

sess.run(tf.global_variables_initializer())
init_vals = sess.run(allvars)

def run():
  for val, v in zip(init_vals, allvars):
    sess.run(tf.assign(v, val))
  
  ivals = sess.run(allvars)
  out = []
  allvals = []

  for i in range(ITERATIONS):
    l, _ = sess.run([loss, train], feed_dict={x_in: x_data, y_in: y_data})
    out.append(sess.run(x, feed_dict={x_in: x_test}))
    allvals.append(sess.run(allvars))

  fvals = sess.run(allvars)
  # return np.asarray(ivals), np.asarray(fvals), np.asarray(out)
  return np.asarray(ivals), np.asarray(fvals), np.asarray(out), allvals

ivals1, fvals1, out1, all1 = run()
ivals2, fvals2, out2, all2 = run()

same_init = [np.all(v1 == v2) for v1, v2 in zip(ivals1, ivals2)] 
same_fin = [np.all(v1 == v2) for v1, v2 in zip(fvals1, fvals2)] 
print("Forward passes were the same: {}".format( np.all(out1 == out2) ))
print("Final value of variables are the same: {}".format( np.all(same_fin) ))
print("Variables initialized to same values: {}".format( np.all(same_init) ))
&lt;/denchmark-code&gt;

Unfortunately I am really busy with experiments at the moment and do not have the time to narrow this down further, but I hope it will be a good starting point. I tested with python 3.6.5, CUDA 9.0, cudnn 7.1
and TF 1.8.
It will be great if we manage to make all operations deterministic. I am running some Reinforcement Learning algorithms and over time I get huge difference in performance even if I use the same seed.
		</comment>
		<comment id='31' author='shiviser' date='2018-08-31T07:51:56Z'>
		Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the contributions welcome label. Thank you.
		</comment>
		<comment id='32' author='shiviser' date='2018-09-15T18:30:54Z'>
		Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the contributions welcome label. Thank you.
		</comment>
		<comment id='33' author='shiviser' date='2018-09-18T17:10:04Z'>
		GPU reductions are now deterministic.
		</comment>
		<comment id='34' author='shiviser' date='2018-11-29T17:14:51Z'>
		&lt;denchmark-link:https://github.com/ekelsen&gt;@ekelsen&lt;/denchmark-link&gt;
 Which ops are deterministic exactly? Since when / which TF version? Is this documented?
		</comment>
		<comment id='35' author='shiviser' date='2019-03-17T18:51:13Z'>
		&lt;denchmark-link:https://github.com/ekelsen&gt;@ekelsen&lt;/denchmark-link&gt;
, I still experience non-deterministic GPU operations. In comparison, same code is absolutely deterministic when it runs on CPU. I am using TF 1.12.0 and CUDA 9.0
		</comment>
		<comment id='36' author='shiviser' date='2019-03-17T21:58:06Z'>
		Reductions are deterministic.  Many other things are not:

Forward passes of anything that uses auto-tune (convolutions are the prime culprit, but I think also gemm now)
Backward passes of convolutions
Backward passes of bias_add (sometimes depending on auto-tune)
I think cross entropy still hasn't been switched to use the faster and deterministic reductions under the hood.
unsorted_segment_sum (possibly also sorted_segment_sum)
scatter_add/sub
backward pass of depthwise conv, some less common ops used in preprocessing
Possibly more I've forgotten or never knew about.

		</comment>
		<comment id='37' author='shiviser' date='2019-03-17T22:20:42Z'>
		Thanks for clarifying. Are there any plans to make all these deterministic? PyTorch can be completely deterministic on GPU which can often be very useful. In particular, while the non-determinism might be acceptable in supervised learning, in Reinforcement Learning it has very non-trivial impact on results, sometimes leading to big variance due to the constantly shifting data distribution.
		</comment>
		<comment id='38' author='shiviser' date='2019-09-18T08:53:01Z'>
		For anyone still encountering this issue, &lt;denchmark-link:https://github.com/duncanriach&gt;@duncanriach&lt;/denchmark-link&gt;
 has done great work on this problem which he presented at NVIDIA's GTC 2019 conference.
His findings being that there are a multitude of reasons for the non-determinism we see
He has set-up a &lt;denchmark-link:https://github.com/NVIDIA/tensorflow-determinism&gt;repository&lt;/denchmark-link&gt;
 to track the issue which also contains a link to his talk
		</comment>
		<comment id='39' author='shiviser' date='2019-10-08T22:33:28Z'>
		Thanks &lt;denchmark-link:https://github.com/Robbie-Palmer&gt;@Robbie-Palmer&lt;/denchmark-link&gt;
. The current high-level status is that there are now solutions for TensorFlow determinism when running on GPUs related to cuDNN (convolutions and max-pooling) and bias_add. Please see the following repo for up-to-date status: &lt;denchmark-link:https://github.com/NVIDIA/tensorflow-determinism&gt;https://github.com/NVIDIA/tensorflow-determinism&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>