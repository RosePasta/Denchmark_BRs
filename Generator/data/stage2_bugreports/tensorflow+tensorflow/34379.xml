<bug id='34379' author='awav' open_date='2019-11-18T15:49:05Z' closed_time='2019-11-21T21:13:59Z'>
	<summary>Memory leak in custom tensors</summary>
	<description>
Hello everyone,
cc: &lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;

There is a memory leak for custom tensors from &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/24865&gt;#24865&lt;/denchmark-link&gt;
. You can find an MWE here: &lt;denchmark-link:https://colab.research.google.com/drive/1v-PEv3-qubszfK0gFS2RLhcP6qbDUvIK&gt;https://colab.research.google.com/drive/1v-PEv3-qubszfK0gFS2RLhcP6qbDUvIK&lt;/denchmark-link&gt;
. Long story short. In GPflow we define parameters of the model as custom tensors (variables) &lt;denchmark-link:https://github.com/GPflow/GPflow/blob/develop/gpflow/base.py#L36&gt;https://github.com/GPflow/GPflow/blob/develop/gpflow/base.py#L36&lt;/denchmark-link&gt;
. The parameter is a  container with single tf.Variable and TFP bijector inside. The custom tensor behaves as a standard TensorFlow tensor,  it applies transformation on internal  and returns forward transformed Tensor of the variable every-time when someone decides to read it (user, tensorflow function).
When I use a model with Parameter class and compute gradients w.r.t. the loss, I observe constant memory growth. And when I use the model with same variables but do transformation of the variables manually the memory stays the same.
If I do forward mode only, there are no issues. It happens only when GradientTape is involved.
PS: memory profiler doesn't work in colab, you will have to run it on your computer.

macOS 10.14.6
Python 3.7.0
TensorFlow version:

â†’ python -c "import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"
v2.0.0-rc2-26-g64c3d382ca 2.0.0
Middle of the training:
Compute gradient
Iteration 56 has passed with loss = 494609555730402.0
Filename: mwe2.py

Line #    Mem usage    Increment   Line Contents
================================================
   118    484.5 MiB    484.5 MiB   @profile
   119                             def train_step(compute_grads: bool = True):
   120    484.6 MiB      0.1 MiB       data = next(dataset_it)
   121    484.6 MiB      0.0 MiB       variables = model.trainable_variables
   122    484.6 MiB      0.0 MiB       with tf.GradientTape(watch_accessed_variables=False) as tape:
   123    484.6 MiB      0.0 MiB           tape.watch(variables)
   124    490.6 MiB      6.1 MiB           loss = loss_fn(data)
   125
   126    490.6 MiB      0.0 MiB       if compute_grads:
   127    490.6 MiB      0.0 MiB           tf.print(f"Compute gradient")
   128    491.2 MiB      0.6 MiB           grads = tape.gradient(loss, variables)
   129    491.2 MiB      0.0 MiB           grads_and_vals = zip(grads, variables)
   130    491.2 MiB      0.0 MiB           opt.apply_gradients(grads_and_vals)
   131
   132    491.2 MiB      0.0 MiB       tf.print(f"Iteration {opt.iterations.numpy()} has passed with loss = {loss.numpy()}")
In the end:
Compute gradient
Iteration 100 has passed with loss = 273947779893557.2
Filename: mwe2.py

Line #    Mem usage    Increment   Line Contents
================================================
   118    514.8 MiB    514.8 MiB   @profile
   119                             def train_step(compute_grads: bool = True):
   120    514.9 MiB      0.1 MiB       data = next(dataset_it)
   121    514.9 MiB      0.0 MiB       variables = model.trainable_variables
   122    514.9 MiB      0.0 MiB       with tf.GradientTape(watch_accessed_variables=False) as tape:
   123    514.9 MiB      0.0 MiB           tape.watch(variables)
   124    520.9 MiB      6.1 MiB           loss = loss_fn(data)
   125
   126    520.9 MiB      0.0 MiB       if compute_grads:
   127    520.9 MiB      0.0 MiB           tf.print(f"Compute gradient")
   128    521.5 MiB      0.6 MiB           grads = tape.gradient(loss, variables)
   129    521.5 MiB      0.0 MiB           grads_and_vals = zip(grads, variables)
   130    521.5 MiB      0.0 MiB           opt.apply_gradients(grads_and_vals)
   131
   132    521.5 MiB      0.0 MiB       tf.print(f"Iteration {opt.iterations.numpy()} has passed with loss = {loss.numpy()}")
	</description>
	<comments>
		<comment id='1' author='awav' date='2019-11-18T18:11:41Z'>
		&lt;denchmark-link:https://github.com/kkimdev&gt;@kkimdev&lt;/denchmark-link&gt;
 Do you know if this is related to the leak you've been investigating?
		</comment>
		<comment id='2' author='awav' date='2019-11-18T18:32:21Z'>
		It seems like it's using tf.data? There were some recent tf.data memory leak fixes &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/082415b2ff49bfb8890f7d5361585bac04749add&gt;082415b&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/c2fc448fe253bc59d3f0417d7d08e16d53f2a856&gt;c2fc448&lt;/denchmark-link&gt;
 so I encourage to try on tf-nightly or tf-nightly-gpu .
		</comment>
		<comment id='3' author='awav' date='2019-11-18T20:59:11Z'>
		&lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/kkimdev&gt;@kkimdev&lt;/denchmark-link&gt;
 I replaced data with tensors and installed  - still the issue is present.
		</comment>
		<comment id='4' author='awav' date='2019-11-18T21:41:28Z'>
		Looks like it's leaking through tensorflow_probability's FillTriangular class and bijector.  I'm not sure if I'll have time to dig this deeper soon.  I attached the script I used to profile and it's outputs below.
&lt;denchmark-link:https://colab.research.google.com/gist/kkimdev/f69960cb84d05a8d865eaadf44f47967/memory-leak-in-custom-tensor.ipynb&gt;https://colab.research.google.com/gist/kkimdev/f69960cb84d05a8d865eaadf44f47967/memory-leak-in-custom-tensor.ipynb&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;=======================================================================
Type                     Old_ids  Current_ids      New_ids Count_Deltas
=======================================================================
HashableWeakRef              565          604          +39          +39
Dimension                    601          639          +38          +38
tuple                      38841        38870          +30          +29
list                       20449        20470          +23          +21
cell                       14187        14209          +22          +22
function                   76122        76142          +20          +20
dict                       52279        52298          +19          +19
_Mapping                     269          288          +19          +19
TensorShape                  338          357          +19          +19
EagerTensor                  322          341          +19          +19
frame                        126          136          +10          +10
method                      1397         1400           +3           +3
StringIO                       3            3           +1           +0
NullContext                    1            2           +1           +1
zipf_gen                       1            1           +0           +0
yulesimon_gen                  1            1           +0           +0
wrapper_descriptor          3583         3583           +0           +0
wrapcauchy_gen                 1            1           +0           +0
words                         12           12           +0           +0
wishart_gen                    1            1           +0           +0
weibull_min_gen                1            1           +0           +0
weibull_max_gen                1            1           +0           +0
weekday                       14           14           +0           +0
weakref                    19679        19679           +0           +0
weakproxy                     12           12           +0           +0
wald_gen                       1            1           +0           +0
vonmises_gen                   2            2           +0           +0
vectorize                    454          454           +0           +0
validate_nseq_int              1            1           +0           +0
validate_nseq_float            5            5           +0           +0
=======================================================================
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/503414/69096361-1dd98700-0a09-11ea-9602-d254be7f0d41.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='awav' date='2019-11-18T23:42:04Z'>
		&lt;denchmark-link:https://github.com/jvdillon&gt;@jvdillon&lt;/denchmark-link&gt;
 have you seen this leak before?
		</comment>
		<comment id='6' author='awav' date='2019-11-18T23:54:15Z'>
		We definitely used to leak memory but switched to weakref dict. Given the intrinsic complexity here, my guess is we have a bug. Ill see if someone on the team is willing to dig in.
		</comment>
		<comment id='7' author='awav' date='2019-11-19T22:46:41Z'>
		&lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/jvdillon&gt;@jvdillon&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/kkimdev&gt;@kkimdev&lt;/denchmark-link&gt;
 Thanks everyone for quick responses. Welcome &lt;denchmark-link:https://github.com/csuter&gt;@csuter&lt;/denchmark-link&gt;
 ;)
		</comment>
		<comment id='8' author='awav' date='2019-11-21T21:13:59Z'>
		This is a TFP issue, which we're presently resolving. Closing this (please follow &lt;denchmark-link:https://github.com/tensorflow/probability/issues/647&gt;tensorflow/probability#647&lt;/denchmark-link&gt;
 for TFP updates)
		</comment>
		<comment id='9' author='awav' date='2019-11-21T21:14:01Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34379&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34379&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>