<bug id='41697' author='FuchsPhi' open_date='2020-07-24T11:10:40Z' closed_time='2020-07-27T05:56:35Z'>
	<summary>converter.inference_input_type = tf.int8 is been ignored</summary>
	<description>
System information

Docker image tensorflow/tensorflow:2.2.0
Same issue with Windows python 3 and tensorflow 2.2.0 installed via pip

Command used to run the converter or code if youâ€™re using the Python API
Especially the converter.inference_input_type and converter.inference_output_type is imporant.
&lt;denchmark-code&gt;k_model = tf.keras.models.load_model(model_path)
converter = tf.lite.TFLiteConverter.from_keras_model(k_model)
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8

def representative_data_gen():
    for input_value in data_set:
        yield [input_value.astype(np.float32).reshape((1, 10))]

converter.representative_dataset = representative_data_gen

tf_lite_model_quant = converter.convert()
&lt;/denchmark-code&gt;

The output from the converter invocation
The output of the convertion is the same as without specifying the inference_input and output_type.
The tflite outputfiles with and without the specification of the inference input type are attached.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/4971633/tflite_conv_test.zip&gt;tflite_conv_test.zip&lt;/denchmark-link&gt;

Failure details
If the conversion is successful, but the generated model is wrong,
state what is wrong:

I expect a model without the qunatize and the dequantize layer at the beginning and at the end.
The generated model has a infernece_input_type of float32 not the expected int8

Any other info / logs
With tensorflow 1.15 the inference_input was int8 of the generated model when specifing the inference input type. Also no quantization or deqauntization layer were putted in the generated model.
	</description>
	<comments>
		<comment id='1' author='FuchsPhi' date='2020-07-24T20:59:07Z'>
		&lt;denchmark-link:https://github.com/MeghnaNatraj&gt;@MeghnaNatraj&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='FuchsPhi' date='2020-07-24T21:47:48Z'>
		This feature (inference_input_type and inference_output_type) is available starting TF 2.3. Would you able to upgrade the TF version and try again?
		</comment>
		<comment id='3' author='FuchsPhi' date='2020-07-27T05:56:35Z'>
		Worked fine with TF 2.3. Thanks.
		</comment>
		<comment id='4' author='FuchsPhi' date='2020-07-27T05:56:37Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41697&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41697&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>