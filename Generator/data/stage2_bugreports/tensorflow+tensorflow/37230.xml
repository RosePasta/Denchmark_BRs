<bug id='37230' author='GeorgeLiang3' open_date='2020-03-02T12:43:36Z' closed_time='2020-03-30T23:27:38Z'>
	<summary>tf.function second derivative error</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information


Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): yes


OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): macOS Mojave 10.14.6


Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device: N/A


TensorFlow installed from (source or
binary): - TensorFlow version (use command below):binary  v2.1.0-rc2-17


Python version:  - Bazel
version (if compiling from source): Python 3.7.1


GCC/Compiler version (if compiling from
source):


CUDA/cuDNN version: - GPU model and memory:


Describe the current behavior
I have a custom function need to use slicing in a for loop. The first-order derivative is working properly but the second-order derivative gives the error. The error only occurs when the function is decorated with tf.function. Below is a simplified code to reproduce the error.
Describe the expected behavior
Standalone code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
&lt;denchmark-code&gt;import tensorflow as tf
x =tf.random.uniform([9],minval = -50,maxval = -30,seed = 1,dtype = tf.float32)

@tf.function
def A(x):
    return x[1]

@tf.function
def g(x):

    Z_sum = tf.constant([0.],dtype = tf.float32)

    for i in tf.range(x.shape[0]):
        Z_sum = tf.add(Z_sum, A(x))

    return Z_sum

with tf.GradientTape() as t:
    t.watch(x)
    with tf.GradientTape() as tt:
        tt.watch(x)
        loss = g(x)
    jac = tt.gradient(loss,x)
hess = t.gradient(jac,x)
&lt;/denchmark-code&gt;

Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

InvalidArgumentError                      Traceback (most recent call last)
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in get_attr(self, name)
2325       with c_api_util.tf_buffer() as buf:
-&gt; 2326         c_api.TF_OperationGetAttrValueProto(self._c_op, name, buf)
2327         data = c_api.TF_GetBuffer(buf)
InvalidArgumentError: Operation 'gradients/while_grad/while_grad' has no attr named '_XlaCompile'.
During handling of the above exception, another exception occurred:
ValueError                                Traceback (most recent call last)
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)
330     try:
--&gt; 331       xla_compile = op.get_attr("_XlaCompile")
332       xla_separate_compiled_gradients = op.get_attr(
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in get_attr(self, name)
2329       # Convert to ValueError for backwards compatibility.
-&gt; 2330       raise ValueError(str(e))
2331     x = attr_value_pb2.AttrValue()
ValueError: Operation 'gradients/while_grad/while_grad' has no attr named '_XlaCompile'.
During handling of the above exception, another exception occurred:
InvalidArgumentError                      Traceback (most recent call last)
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in get_attr(self, name)
2325       with c_api_util.tf_buffer() as buf:
-&gt; 2326         c_api.TF_OperationGetAttrValueProto(self._c_op, name, buf)
2327         data = c_api.TF_GetBuffer(buf)
InvalidArgumentError: Operation 'gradients/TensorListPushBack_grad/TensorListPopBack' has no attr named '_XlaCompile'.
During handling of the above exception, another exception occurred:
ValueError                                Traceback (most recent call last)
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)
330     try:
--&gt; 331       xla_compile = op.get_attr("_XlaCompile")
332       xla_separate_compiled_gradients = op.get_attr(
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in get_attr(self, name)
2329       # Convert to ValueError for backwards compatibility.
-&gt; 2330       raise ValueError(str(e))
2331     x = attr_value_pb2.AttrValue()
ValueError: Operation 'gradients/TensorListPushBack_grad/TensorListPopBack' has no attr named '_XlaCompile'.
During handling of the above exception, another exception occurred:
ValueError                                Traceback (most recent call last)
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(op_type_name, name, **keywords)
467               as_ref=input_arg.is_ref,
--&gt; 468               preferred_dtype=default_dtype)
469         except TypeError as err:
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)
1313     if ret is None:
-&gt; 1314       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
1315
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)
316   _ = as_ref
--&gt; 317   return constant(v, dtype=dtype, name=name)
318
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in constant(value, dtype, shape, name)
257   return _constant_impl(value, dtype, shape, name, verify_shape=False,
--&gt; 258                         allow_broadcast=True)
259
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)
295           value, dtype=dtype, shape=shape, verify_shape=verify_shape,
--&gt; 296           allow_broadcast=allow_broadcast))
297   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)
438     if values is None:
--&gt; 439       raise ValueError("None values not supported.")
440     # if dtype is provided, forces numpy array to be the type
ValueError: None values not supported.
During handling of the above exception, another exception occurred:
ValueError                                Traceback (most recent call last)
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(op_type_name, name, **keywords)
481             observed = ops.convert_to_tensor(
--&gt; 482                 values, as_ref=input_arg.is_ref).dtype.name
483           except ValueError as err:
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)
1313     if ret is None:
-&gt; 1314       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
1315
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)
316   _ = as_ref
--&gt; 317   return constant(v, dtype=dtype, name=name)
318
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in constant(value, dtype, shape, name)
257   return _constant_impl(value, dtype, shape, name, verify_shape=False,
--&gt; 258                         allow_broadcast=True)
259
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)
295           value, dtype=dtype, shape=shape, verify_shape=verify_shape,
--&gt; 296           allow_broadcast=allow_broadcast))
297   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)
438     if values is None:
--&gt; 439       raise ValueError("None values not supported.")
440     # if dtype is provided, forces numpy array to be the type
ValueError: None values not supported.
During handling of the above exception, another exception occurred:
ValueError                                Traceback (most recent call last)
 in 
21         tt.watch(x)
22         loss = g(x)
---&gt; 23     jac = tt.gradient(loss,x)
24 hess = t.gradient(jac,x)
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)
1027         output_gradients=output_gradients,
1028         sources_raw=flat_sources_raw,
-&gt; 1029         unconnected_gradients=unconnected_gradients)
1030
1031     if not self._persistent:
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/eager/imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)
75       output_gradients,
76       sources_raw,
---&gt; 77       compat.as_str(unconnected_gradients.value))
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _backward_function_wrapper(*args)
1254           break
1255       return backward._call_flat(  # pylint: disable=protected-access
-&gt; 1256           processed_args, remapped_captures)
1257
1258     return _backward_function_wrapper, recorded_outputs
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
1695         possible_gradient_type,
1696         executing_eagerly)
-&gt; 1697     forward_function, args_with_tangents = forward_backward.forward()
1698     if executing_eagerly:
1699       flat_outputs = forward_function.call(
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in forward(self)
1421     """Builds or retrieves a forward function for this call."""
1422     forward_function = self._functions.forward(
-&gt; 1423         self._inference_args, self._input_tangents)
1424     return forward_function, self._inference_args + self._input_tangents
1425
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in forward(self, inference_args, input_tangents)
1183       (self._forward, self._forward_graph, self._backward,
1184        self._forwardprop_output_indices, self._num_forwardprop_outputs) = (
-&gt; 1185            self._forward_and_backward_functions(inference_args, input_tangents))
1186     return self._forward
1187
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _forward_and_backward_functions(self, inference_args, input_tangents)
1329     outputs = self._func_graph.outputs[:self._num_inference_outputs]
1330     return self._build_functions_for_outputs(
-&gt; 1331         outputs, inference_args, input_tangents)
1332
1333
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _build_functions_for_outputs(self, outputs, inference_args, input_tangents)
888             self._func_graph.inputs,
889             grad_ys=gradients_wrt_outputs,
--&gt; 890             src_graph=self._func_graph)
891
892       captures_from_forward = [
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)
667                 # functions.
668                 in_grads = _MaybeCompile(grad_scope, op, func_call,
--&gt; 669                                          lambda: grad_fn(op, *out_grads))
670               else:
671                 # For function call ops, we add a 'SymbolicGradient'
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)
334       xla_scope = op.get_attr("_XlaScope").decode()
335     except ValueError:
--&gt; 336       return grad_fn()  # Exit early
337
338   if not xla_compile:
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in ()
667                 # functions.
668                 in_grads = _MaybeCompile(grad_scope, op, func_call,
--&gt; 669                                          lambda: grad_fn(op, *out_grads))
670               else:
671                 # For function call ops, we add a 'SymbolicGradient'
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/while_v2.py in _WhileGrad(op, *grads)
357   body_grad_graph, args = _create_grad_func(
358       ys, xs, non_none_grads, cond_graph, body_graph,
--&gt; 359       util.unique_grad_fn_name(body_graph.name), op, maximum_iterations)
360
361   if body_grad_graph.while_op_needs_rewrite:
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/while_v2.py in _create_grad_func(ys, xs, grads, cond_graph, body_graph, name, while_op, maximum_iterations)
599       func_graph=_WhileBodyGradFuncGraph(name, cond_graph, body_graph,
600                                          maximum_iterations, while_op,
--&gt; 601                                          body_graph_inputs, body_graph_outputs))
602
603   # Update the list of outputs with tensors corresponding to the captured
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
976                                           converted_func)
977
--&gt; 978       func_outputs = python_func(*func_args, **func_kwargs)
979
980       # invariant: func_outputs contains only Tensors, CompositeTensors,
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/while_v2.py in (*args)
595   grad_func_graph = func_graph_module.func_graph_from_py_func(
596       name,
--&gt; 597       lambda *args: _grad_fn(ys, xs, args, body_graph),
598       args, {},
599       func_graph=_WhileBodyGradFuncGraph(name, cond_graph, body_graph,
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/while_v2.py in _grad_fn(ys, xs, args, func_graph)
655   grad_outs = gradients_util._GradientsHelper(
656       ys, xs, grad_ys=grad_ys, src_graph=func_graph,
--&gt; 657       unconnected_gradients="zero")
658
659   # TODO(b/118712257): Handle the case when grad_outs has None's e.g. when there
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)
667                 # functions.
668                 in_grads = _MaybeCompile(grad_scope, op, func_call,
--&gt; 669                                          lambda: grad_fn(op, *out_grads))
670               else:
671                 # For function call ops, we add a 'SymbolicGradient'
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)
334       xla_scope = op.get_attr("_XlaScope").decode()
335     except ValueError:
--&gt; 336       return grad_fn()  # Exit early
337
338   if not xla_compile:
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in ()
667                 # functions.
668                 in_grads = _MaybeCompile(grad_scope, op, func_call,
--&gt; 669                                          lambda: grad_fn(op, *out_grads))
670               else:
671                 # For function call ops, we add a 'SymbolicGradient'
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/list_ops.py in _PopBackGrad(op, dlist, delement)
187         element_shape=gen_list_ops.tensor_list_element_shape(
188             op.outputs[0], shape_type=dtypes.int32))
--&gt; 189   return gen_list_ops.tensor_list_push_back(dlist, delement), None
190
191
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_list_ops.py in tensor_list_push_back(input_handle, tensor, name)
761   _, _, _op, _outputs = _op_def_library._apply_op_helper(
762         "TensorListPushBack", input_handle=input_handle, tensor=tensor,
--&gt; 763                               name=name)
764   _result = _outputs[:]
765   if _execute.must_record_gradient():
~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(op_type_name, name, **keywords)
484             raise ValueError(
485                 "Tried to convert '%s' to a tensor and failed. Error: %s" %
--&gt; 486                 (input_name, err))
487           prefix = ("Input '%s' of '%s' Op has type %s that does not match" %
488                     (input_name, op_type_name, observed))
ValueError: Tried to convert 'tensor' to a tensor and failed. Error: None values not supported.
	</description>
	<comments>
		<comment id='1' author='GeorgeLiang3' date='2020-03-04T09:14:20Z'>
		I think this is related to issue &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/15219&gt;#15219&lt;/denchmark-link&gt;
, is there any solution for it now?
		</comment>
		<comment id='2' author='GeorgeLiang3' date='2020-03-04T10:20:01Z'>
		Could able to reproduce the issue with Tf 2.1.
Please find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/28674efbe2941bb129d5622c13892277/untitled416.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='3' author='GeorgeLiang3' date='2020-03-05T03:50:45Z'>
		&lt;denchmark-link:https://github.com/GeorgeLiang3&gt;@GeorgeLiang3&lt;/denchmark-link&gt;
 I don't run into any error when I am using tf-nightly. Please find my gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/gowthamkpr/f7bcc6f630e4864f0c269256f9d3df96/untitled37.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='4' author='GeorgeLiang3' date='2020-03-05T10:41:35Z'>
		&lt;denchmark-link:https://github.com/gowthamkpr&gt;@gowthamkpr&lt;/denchmark-link&gt;
  Thanks for your reply! But if I add one more 'if' or 'for' statement in the function, the both give the same error. Please find my gist &lt;denchmark-link:https://colab.research.google.com/drive/1p0bb2ePkc6ect0Q26RVB_BByOUFtppeq&gt;here&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='GeorgeLiang3' date='2020-03-10T12:27:54Z'>
		&lt;denchmark-link:https://github.com/gowthamkpr&gt;@gowthamkpr&lt;/denchmark-link&gt;
 Any solution to this? Thanks in advance
		</comment>
		<comment id='6' author='GeorgeLiang3' date='2020-03-30T23:27:40Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37230&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37230&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>