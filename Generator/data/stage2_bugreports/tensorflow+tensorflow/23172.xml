<bug id='23172' author='Matt-Hicks-Bose' open_date='2018-10-22T21:24:42Z' closed_time='2018-11-21T03:26:26Z'>
	<summary>Error using cohen_kappa metric with tf.contrib.distribute.MirroredStrategy configured estimator</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): ('v1.11.0-0-gc19e29306c', '1.11.0')
Python version: 2.7.12
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: 9.0.176/7.2.1.38
GPU model and memory: 4x Nvidia V100 16GB

Describe the current behavior
When using the tf.contrib.metrics.cohen_kappa metric with an estimator, a TypeError is raised when calling the evaluate method on the estimator if the evaluation is configured to be distributed with tf.contrib.distribute.MirroredStrategy. The evaluation is successful if the estimator is not configured for distributed evaluation.
Describe the expected behavior
The metric tf.contrib.metrics.cohen_kappa should be calculated successfully in both distributed and non-distributed evaluations.
Code to reproduce the issue
test.py:
import tensorflow as tf

DO_EVAL_DIST = True

tf.logging.set_verbosity(tf.logging.INFO)

def model_fn(features, labels, mode):
    layer = tf.layers.Dense(2)
    logits = layer(features)

    if mode == tf.estimator.ModeKeys.PREDICT:
        predictions = {"logits": logits}
        return tf.estimator.EstimatorSpec(mode, predictions=predictions)

    loss = tf.losses.mean_squared_error(
        labels=labels, predictions=tf.reshape(logits, [-1]))

    if mode == tf.estimator.ModeKeys.EVAL:
        class_targets = tf.reshape(tf.argmax(labels, axis=-1, name='class_targets'), [-1])
        preds = tf.reshape(tf.argmax(logits, axis=-1, name='preds'), [-1])
        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops={
            'cohen_k': tf.contrib.metrics.cohen_kappa(class_targets, preds, 2),
        })

    if mode == tf.estimator.ModeKeys.TRAIN:
        train_op = tf.train.GradientDescentOptimizer(0.2).minimize(loss, global_step=tf.train.get_global_step())
        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)

def input_fn():
    features = tf.data.Dataset.from_tensors([[1.]]).repeat()
    labels = tf.data.Dataset.from_tensors([1., 0.]).repeat()
    return tf.data.Dataset.zip((features, labels))

# Configured the estimator for either distributed or non-distributed evaluation
# Training is always distributed
if DO_EVAL_DIST:
    distribution = tf.contrib.distribute.DistributeConfig(
        train_distribute=tf.contrib.distribute.MirroredStrategy(),
        eval_distribute=tf.contrib.distribute.MirroredStrategy()
    )
    config = tf.estimator.RunConfig(experimental_distribute=distribution)
else:
    distribution = tf.contrib.distribute.MirroredStrategy()
    config = tf.estimator.RunConfig(train_distribute=distribution)

classifier = tf.estimator.Estimator(model_fn=model_fn, config=config, model_dir='out_test')

print('********* start train **********')
classifier.train(input_fn=input_fn, steps=1000)
print('********* end train/start eval **********')
classifier.evaluate(input_fn=input_fn, steps=1000)
print('********* end eval **********')
Other info / logs
Error thrown when using distributed evaluation, when DO_EVAL_DIST = True:
Traceback (most recent call last):
  File "test.py", line 49, in &lt;module&gt;
    classifier.evaluate(input_fn=input_fn, steps=1000)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py", line 474, in evaluate
    return _evaluate()
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py", line 469, in _evaluate
    output_dir=self.eval_dir(name))
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py", line 1528, in _evaluate_run
    config=self._session_config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/evaluation.py", line 212, in _evaluate_once
    session.run(eval_ops, feed_dict)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py", line 783, in __exit__
    self._close_internal(exception_type)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py", line 816, in _close_internal
    h.end(self._coordinated_creator.tf_sess)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/basic_session_run_hooks.py", line 941, in end
    self._final_ops, feed_dict=self._final_ops_feed_dict)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 887, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1095, in _run
    self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 429, in __init__
    self._fetch_mapper = _FetchMapper.for_fetch(fetches)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 249, in for_fetch
    return _DictFetchMapper(fetch)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 387, in __init__
    _FetchMapper.for_fetch(fetch) for fetch in fetches.values()
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 255, in for_fetch
    return _ElementFetchMapper(fetches, contraction_fn)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 288, in __init__
    (fetch, type(fetch), str(e)))
TypeError: Fetch argument PerDevice({'/replica:0/task:0/device:GPU:0': &lt;tf.Tensor 'cohen_kappa/value/Merge:0' shape=() dtype=float64&gt;, '/replica:0/task:0/device:GPU:1': &lt;tf.Tensor 'tower_1/cohen_kappa/value/Merge:0' shape=() dtype=float64&gt;, '/replica:0/task:0/device:GPU:2': &lt;tf.Tensor 'tower_2/cohen_kappa/value/Merge:0' shape=() dtype=float64&gt;, '/replica:0/task:0/device:GPU:3': &lt;tf.Tensor 'tower_3/cohen_kappa/value/Merge:0' shape=() dtype=float64&gt;}) has invalid type &lt;class 'tensorflow.contrib.distribute.python.values.PerDevice'&gt;, must be a string or Tensor. (Can not convert a PerDevice into a Tensor or Operation.)
	</description>
	<comments>
		<comment id='1' author='Matt-Hicks-Bose' date='2018-10-23T21:58:08Z'>
		I was able to execute your code successfully with DO_EVAL_DIST = True. Tagging priya to get more insights on this.
		</comment>
		<comment id='2' author='Matt-Hicks-Bose' date='2018-10-23T22:14:31Z'>
		&lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;
 are you running this on a machine with multiple GPUs?
The VM I am testing with has 4x V100 GPUs, and I have updated the issue report to reflect this. I am not seeing this error on a VM with only a single V100 GPU. I have also confirmed that the code will execute successfully on the 4x V100 GPU VM if the eval distribute configuration is changed to the following:
eval_distribute=tf.contrib.distribute.MirroredStrategy(num_gpus=1)
The issue still remains for me when trying to distribute over multiple GPUs.
		</comment>
		<comment id='3' author='Matt-Hicks-Bose' date='2018-10-23T22:26:57Z'>
		&lt;denchmark-link:https://github.com/Matt-Hicks-Bose&gt;@Matt-Hicks-Bose&lt;/denchmark-link&gt;
 Yes you are right about that. I have tested against single GPU and it completed successfully. This issue is likely to occur for multiple GPUs.
		</comment>
		<comment id='4' author='Matt-Hicks-Bose' date='2018-10-31T19:15:27Z'>
		I think the issue here is the metric was never updated to use the _aggregate_across_towers() wrapper that the non contrib tf.metrics use such as tf.mean_per_class_accuracy.
		</comment>
		<comment id='5' author='Matt-Hicks-Bose' date='2018-11-01T03:05:45Z'>
		See 
 for what &lt;denchmark-link:https://github.com/Brian-Hetherman-Bose&gt;@Brian-Hetherman-Bose&lt;/denchmark-link&gt;
 is refering to. I am not sure if any of the tf.contrib.metrics will work with distributed eval until this is updated.
		</comment>
		<comment id='6' author='Matt-Hicks-Bose' date='2018-11-02T14:48:58Z'>
		&lt;denchmark-link:https://github.com/guptapriya&gt;@guptapriya&lt;/denchmark-link&gt;
 Have you had a chance to take a look at this? Would be nice if this was fixed in the TF 1.12 stable release
		</comment>
		<comment id='7' author='Matt-Hicks-Bose' date='2018-11-02T22:11:34Z'>
		Sorry just looking at this now. You guys are right that this is not working because contrib.metrics haven't been modified to be distribution strategy aware.
AFAIK, (and &lt;denchmark-link:https://github.com/tensorflow/community/blob/5c805774c8b45a3a0ce1337ada4a3f1fc3307983/rfcs/20180907-contrib-sunset.md&gt;according to this RFC&lt;/denchmark-link&gt;
), contrib.metrics is not being migrated to core TF. As such, we don't think we will be making an effort to make them distribution aware.
If you guys are interested in the fate of these metrics, please join &lt;denchmark-link:mailto:addons@tensorflow.org&gt;addons@tensorflow.org&lt;/denchmark-link&gt;
 for the Addons SIG, where metrics like this may end up (assuming there is someone who wants to migrate and maintain them).
		</comment>
		<comment id='8' author='Matt-Hicks-Bose' date='2018-11-20T07:51:06Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/guptapriya&gt;@guptapriya&lt;/denchmark-link&gt;
: It has been 17 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
	</comments>
</bug>