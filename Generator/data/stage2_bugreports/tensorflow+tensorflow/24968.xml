<bug id='24968' author='GJBoth' open_date='2019-01-16T16:29:34Z' closed_time='2019-02-11T20:52:14Z'>
	<summary>tf.data.Dataset.from_tensor_slices().batch() broken when batch size equals dataset size</summary>
	<description>
System information

Have I written custom code: yes
OS Platform and Distribution: OS X 10.13.6
Mobile device if the issue happens on mobile device: X
TensorFlow installed from: Binary
TensorFlow version: 1.12.0
Python version: 3.6.7
Bazel version:x
GCC/Compiler version: x
CUDA/cuDNN version: x
GPU model and memory: x

Describe the current behavior
When using tf.data.Dataset.from_tensor_slices(Data).batch(batchsize) using a batchsize which equals to the number of samples,  I get an out of range error:
&lt;denchmark-code&gt;OutOfRangeError: End of sequence
	 [[{{node IteratorGetNext}} = IteratorGetNext[output_shapes=[[?,2], [?,1]], output_types=[DT_FLOAT, DT_FLOAT], _device="/job:localhost/replica:0/task:0/device:CPU:0"](IteratorV2)]]
&lt;/denchmark-code&gt;

I think this due to the iterator wanting to construct a second batch, but there's nothing left due to the size of the first batch. In other words, it's impossible right now (I think) to use the tf.data pipeline without minibatching.
Describe the expected behavior
No error; the iterator produces a single batch.
Code to reproduce the issue
&lt;denchmark-code&gt;dataset = tf.data.Dataset.from_tensor_slices(np.arange(10)).batch(10)
iterator = tf.data.Iterator.from_structure(dataset.output_types, dataset.output_shapes)
x = iterator.get_next()

use_test_set = iterator.make_initializer(dataset)
with tf.Session() as sess:
    sess.run(use_test_set)
    print(sess.run(x)) #This runs
    print(sess.run(x)) #This line gives an error
&lt;/denchmark-code&gt;

Other info / logs
None.
	</description>
	<comments>
		<comment id='1' author='GJBoth' date='2019-01-17T09:41:06Z'>
		Do you mean that OutOfRangeError is raised when you invoke get_next() twice?
If yes,  the result is expected because all data have been used up. If you use other high level modules, like tf.keras or tf.estimator, those modules always catch the Exception and then exit. You can also catch it by yourself if necessary:
  while True:
    try:
      sess.run(next_element)
    except tf.errors.OutOfRangeError:
      break
		</comment>
		<comment id='2' author='GJBoth' date='2019-01-17T09:55:09Z'>
		Thanks for your answer! I am confused with the data API: I was expecting the iterator to return again the first batch. (I just checked the documentation and saw that this can be done by .repeat())
Your answer also implies that the number of training epochs can not be bigger than dataset_size/batch_size, but this is not what I see happening when training my models (in my case, adam optimizer). Do you have any idea why this would not throw an error?
		</comment>
		<comment id='3' author='GJBoth' date='2019-01-17T21:23:41Z'>
		&lt;denchmark-link:https://github.com/GJBoth&gt;@GJBoth&lt;/denchmark-link&gt;
  It would be great if you can provide a small code to reproduce the error. Please provide as many details as possible to find the root cause of the issue. Thanks!
		</comment>
		<comment id='4' author='GJBoth' date='2019-01-21T14:07:13Z'>
		So what I tried doing was having a dataset for my data using the API, combined with a feed_dict for a mask I apply, like this:
&lt;denchmark-code&gt;train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(BATCH_SIZE)
iterator = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)
X_tf, y_tf = iterator.get_next()

use_train_set = iterator.make_initializer(train_dataset)

# Defining the mask
sparsity_mask = tf.placeholder(tf.int32, shape=[16, 1]) # 16 is the total number of terms   
&lt;/denchmark-code&gt;

Now, when I look at what data the iterator returns:
&lt;denchmark-code&gt;for it in np.arange(nIter):
  
    sess.run(use_train_set)
    print(sess.run(X_tf, feed_dict=mask_dict)[0,:])
&lt;/denchmark-code&gt;

they're all the same, meaning that the iterator resets when used in combination with a feed_dict. Is that expected behaviour?
		</comment>
		<comment id='5' author='GJBoth' date='2019-01-28T17:51:07Z'>
		&lt;denchmark-link:https://github.com/GJBoth&gt;@GJBoth&lt;/denchmark-link&gt;
 : Your example code (re)initializes the iterator inside the loop. Initializing the dataset once before the loop should yield the desired behavior.
sess.run(use_train_set)
for it in np.arange(nIter):
    print(sess.run(X_tf, feed_dict=mask_dict)[0,:])
		</comment>
		<comment id='6' author='GJBoth' date='2019-02-11T20:52:14Z'>
		Closing this because &lt;denchmark-link:https://github.com/suphoff&gt;@suphoff&lt;/denchmark-link&gt;
 has figured out the problem (thanks!) and there doesn't seem to be a bug in the underlying library.
		</comment>
	</comments>
</bug>