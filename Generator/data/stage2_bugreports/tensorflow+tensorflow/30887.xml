<bug id='30887' author='mjlbach' open_date='2019-07-19T20:56:28Z' closed_time='2020-03-29T17:38:54Z'>
	<summary>TF 2.0: tf.distribute example not working (NCCL all reduce issue)</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): Binary
TensorFlow version (use command below): 2.0b1
Python version: 3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 10/7.4
GPU model and memory: Titan Xp

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
When I try to run one of the distributed examples, the model fails using the default NCCL all reduce cross devices ops:

 NCCL All Reduce Error 
---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
&lt;ipython-input-15-df42e8590c26&gt; in &lt;module&gt;
----&gt; 1 model.fit(train_dataset, epochs=50, callbacks=callbacks)

~/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    641         max_queue_size=max_queue_size,
    642         workers=workers,
--&gt; 643         use_multiprocessing=use_multiprocessing)
    644 
    645   def evaluate(self,

~/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_distributed.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
    679           validation_steps=validation_steps,
    680           validation_freq=validation_freq,
--&gt; 681           steps_name='steps_per_epoch')
    682 
    683   def evaluate(self,

~/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)
    292           else:
    293             actual_inputs = ins()
--&gt; 294           batch_outs = f(actual_inputs)
    295         except errors.OutOfRangeError:
    296           if is_dataset:

~/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/keras/distribute/distributed_training_utils.py in execution_function(input_fn)
    811     def execution_function(input_fn):
    812       # `numpy` translates Tensors to values in Eager mode.
--&gt; 813       return [out.numpy() for out in distributed_function(input_fn)]
    814     return execution_function
    815 

~/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    426         # Lifting succeeded, so variables are initialized and we can run the
    427         # stateless function.
--&gt; 428         return self._stateless_fn(*args, **kwds)
    429     else:
    430       canon_args, canon_kwds = \

~/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
   1333     """Calls a graph function specialized to the inputs."""
   1334     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
-&gt; 1335     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   1336 
   1337   @property

~/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs)
    587     """
    588     return self._call_flat(
--&gt; 589         (t for t in nest.flatten((args, kwargs), expand_composites=True)
    590          if isinstance(t, (ops.Tensor,
    591                            resource_variable_ops.ResourceVariable))))

~/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args)
    669     # Only need to override the gradient in graph mode and when we have outputs.
    670     if context.executing_eagerly() or not self.outputs:
--&gt; 671       outputs = self._inference_function.call(ctx, args)
    672     else:
    673       self._register_gradient()

~/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args)
    443             attrs=("executor_type", executor_type,
    444                    "config_proto", config),
--&gt; 445             ctx=ctx)
    446       # Replace empty list with None
    447       outputs = outputs or None

~/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     65     else:
     66       message = e.message
---&gt; 67     six.raise_from(core._status_to_exception(e.code, message), None)
     68   except TypeError as e:
     69     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):

~/.virtualenvs/physics3/lib/python3.6/site-packages/six.py in raise_from(value, from_value)

InternalError: 2 root error(s) found.
  (0) Internal:  internal error
	 [[node Adam/NcclAllReduce_8 (defined at &lt;ipython-input-15-df42e8590c26&gt;:1) ]]
  (1) Internal:  internal error
	 [[node Adam/NcclAllReduce_8 (defined at &lt;ipython-input-15-df42e8590c26&gt;:1) ]]
	 [[GroupCrossDeviceControlEdges_4/Adam/Adam/update_9/Const/_679]]
0 successful operations.
9 derived errors ignored. [Op:__inference_distributed_function_9447]

Function call stack:
distributed_function -&gt; distributed_function


Switching to hierarchical all reduce in the scope allows the code to run, but with a performance penalty and more errrors:

 Hierarchical All Reduce Error 
2019-07-19 15:06:47.632060: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-07-19 15:06:47.947109: W tensorflow/core/framework/model.cc:475] Failed to find a tunable parameter that would decrease the output time. This means that the autotuning optimization got stuck in a local maximum. The optimization attempt will be aborted.
2019-07-19 15:06:48.925479: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-19 15:07:02.595311: I tensorflow/core/profiler/lib/profiler_session.cc:174] Profiler session started.
2019-07-19 15:07:02.610888: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcupti.so.10.0
2019-07-19 15:07:07.254401: I tensorflow/core/platform/default/device_tracer.cc:641] Collecting 1045 kernel records, 173 memcpy records.
2019-07-19 15:07:12.732921: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_8}}]]
	 [[GroupCrossDeviceControlEdges_8/Adam/Adam/update_9/Const/_879]]
2019-07-19 15:07:12.732922: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_8}}]]
	 [[replica_4/metrics/accuracy/AssignAddVariableOp_1/_375]]
2019-07-19 15:07:12.732962: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_8}}]]
	 [[Adam/Adam/group_deps/_951]]
2019-07-19 15:07:12.733034: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_8}}]]
	 [[replica_9/metrics/accuracy/AssignAddVariableOp_1/_203]]
2019-07-19 15:07:12.732989: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_8}}]]
	 [[GroupCrossDeviceControlEdges_0/Adam/Adam/update_9/Const/_631]]
2019-07-19 15:07:12.732981: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_8}}]]
	 [[replica_3/metrics/accuracy/AssignAddVariableOp_1/_359]]
2019-07-19 15:07:12.733029: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_8}}]]
	 [[metrics/accuracy/div_no_nan/ReadVariableOp_16/_450]]
2019-07-19 15:07:12.732962: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_9}}]]
2019-07-19 15:07:12.733232: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_8}}]]
	 [[GroupCrossDeviceControlEdges_2/Adam/Adam/update_9/Const/_727]]
2019-07-19 15:07:12.733380: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_8}}]]
	 [[GroupCrossDeviceControlEdges_0/Adam/Adam/update_9/Const/_763]]
2019-07-19 15:07:12.735619: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_8}}]]
[I 15:08:10.336 LabApp] Saving file at /k


Further investigation has revealed that this only occurs when I add the 10th GPU on the node to the strategy... Using the first 9 GPUs is fine, but including the 10th GPU with any other combination of GPUs leads to this behavior. Confirmed on multiple independent systems after reinstalling cuda/drivers and restarting nodes. I still get an out-of-range error, but the model trains. The model also trains when only using the 10th gpu.

 out of range error with nccl all reduce 
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:1c:00.0
2019-07-20 12:10:09.708295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:1d:00.0
2019-07-20 12:10:09.710603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:1e:00.0
2019-07-20 12:10:09.712948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:3d:00.0
2019-07-20 12:10:09.715631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 4 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:3e:00.0
2019-07-20 12:10:09.717939: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 5 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:3f:00.0
2019-07-20 12:10:09.720316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 6 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:40:00.0
2019-07-20 12:10:09.720644: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-20 12:10:09.722035: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-07-20 12:10:09.723341: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-07-20 12:10:09.723662: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-07-20 12:10:09.725225: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-07-20 12:10:09.726450: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-07-20 12:10:09.730176: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-20 12:10:09.761821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6
2019-07-20 12:10:09.762583: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-07-20 12:10:11.263993: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3c306c0 executing computations on platform CUDA. Devices:
2019-07-20 12:10:11.264051: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): TITAN Xp, Compute Capability 6.1
2019-07-20 12:10:11.264071: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): TITAN Xp, Compute Capability 6.1
2019-07-20 12:10:11.264088: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (2): TITAN Xp, Compute Capability 6.1
2019-07-20 12:10:11.264104: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (3): TITAN Xp, Compute Capability 6.1
2019-07-20 12:10:11.264119: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (4): TITAN Xp, Compute Capability 6.1
2019-07-20 12:10:11.264135: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (5): TITAN Xp, Compute Capability 6.1
2019-07-20 12:10:11.264151: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (6): TITAN Xp, Compute Capability 6.1
2019-07-20 12:10:11.273304: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300010000 Hz
2019-07-20 12:10:11.275776: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x45d7fb0 executing computations on platform Host. Devices:
2019-07-20 12:10:11.275796: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;
2019-07-20 12:10:11.308790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:1c:00.0
2019-07-20 12:10:11.310775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:1d:00.0
2019-07-20 12:10:11.312785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:1e:00.0
2019-07-20 12:10:11.314726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:3d:00.0
2019-07-20 12:10:11.316719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 4 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:3e:00.0
2019-07-20 12:10:11.318675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 5 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:3f:00.0
2019-07-20 12:10:11.320680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 6 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:40:00.0
2019-07-20 12:10:11.320722: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-20 12:10:11.320732: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-07-20 12:10:11.320741: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-07-20 12:10:11.320767: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-07-20 12:10:11.320778: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-07-20 12:10:11.320787: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-07-20 12:10:11.320797: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-20 12:10:11.347344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6
2019-07-20 12:10:11.347438: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-20 12:10:11.362271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-20 12:10:11.362286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 4 5 6
2019-07-20 12:10:11.362292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y Y Y Y Y Y
2019-07-20 12:10:11.362296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N Y Y Y Y Y
2019-07-20 12:10:11.362300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   Y Y N Y Y Y Y
2019-07-20 12:10:11.362304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   Y Y Y N Y Y Y
2019-07-20 12:10:11.362309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 4:   Y Y Y Y N Y Y
2019-07-20 12:10:11.362313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 5:   Y Y Y Y Y N Y
2019-07-20 12:10:11.362317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 6:   Y Y Y Y Y Y N
2019-07-20 12:10:11.378594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11424 MB memory) -&gt; physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:1c:00.0, compute capability: 6.1)
2019-07-20 12:10:11.380969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 11424 MB memory) -&gt; physical GPU (device: 1, name: TITAN Xp, pci bus id: 0000:1d:00.0, compute capability: 6.1)
2019-07-20 12:10:11.385188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 11424 MB memory) -&gt; physical GPU (device: 2, name: TITAN Xp, pci bus id: 0000:1e:00.0, compute capability: 6.1)
2019-07-20 12:10:11.389861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 11424 MB memory) -&gt; physical GPU (device: 3, name: TITAN Xp, pci bus id: 0000:3d:00.0, compute capability: 6.1)
2019-07-20 12:10:11.393361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:4 with 11424 MB memory) -&gt; physical GPU (device: 4, name: TITAN Xp, pci bus id: 0000:3e:00.0, compute capability: 6.1)
2019-07-20 12:10:11.397087: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:5 with 11424 MB memory) -&gt; physical GPU (device: 5, name: TITAN Xp, pci bus id: 0000:3f:00.0, compute capability: 6.1)
2019-07-20 12:10:11.400702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:6 with 11424 MB memory) -&gt; physical GPU (device: 6, name: TITAN Xp, pci bus id: 0000:40:00.0, compute capability: 6.1)
2019-07-20 12:10:11.579203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:1c:00.0
2019-07-20 12:10:11.581543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:1d:00.0
2019-07-20 12:10:11.583872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:1e:00.0
2019-07-20 12:10:11.585869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:3d:00.0
2019-07-20 12:10:11.587873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 4 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:3e:00.0
2019-07-20 12:10:11.589889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 5 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:3f:00.0
2019-07-20 12:10:11.591870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 6 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:40:00.0
2019-07-20 12:10:11.591903: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-20 12:10:11.591913: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-07-20 12:10:11.591921: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-07-20 12:10:11.591931: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-07-20 12:10:11.591957: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-07-20 12:10:11.591967: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-07-20 12:10:11.591978: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-20 12:10:11.618747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6
2019-07-20 12:10:11.619236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-20 12:10:11.619246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 4 5 6
2019-07-20 12:10:11.619255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y Y Y Y Y Y
2019-07-20 12:10:11.619261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N Y Y Y Y Y
2019-07-20 12:10:11.619292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   Y Y N Y Y Y Y
2019-07-20 12:10:11.619297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   Y Y Y N Y Y Y
2019-07-20 12:10:11.619303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 4:   Y Y Y Y N Y Y
2019-07-20 12:10:11.619320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 5:   Y Y Y Y Y N Y
2019-07-20 12:10:11.619326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 6:   Y Y Y Y Y Y N
2019-07-20 12:10:11.635672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 11424 MB memory) -&gt; physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:1c:00.0, compute capability: 6.1)
2019-07-20 12:10:11.637653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:1 with 11424 MB memory) -&gt; physical GPU (device: 1, name: TITAN Xp, pci bus id: 0000:1d:00.0, compute capability: 6.1)
2019-07-20 12:10:11.639716: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:2 with 11424 MB memory) -&gt; physical GPU (device: 2, name: TITAN Xp, pci bus id: 0000:1e:00.0, compute capability: 6.1)
2019-07-20 12:10:11.641688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:3 with 11424 MB memory) -&gt; physical GPU (device: 3, name: TITAN Xp, pci bus id: 0000:3d:00.0, compute capability: 6.1)
2019-07-20 12:10:11.643693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:4 with 11424 MB memory) -&gt; physical GPU (device: 4, name: TITAN Xp, pci bus id: 0000:3e:00.0, compute capability: 6.1)
2019-07-20 12:10:11.645687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:5 with 11424 MB memory) -&gt; physical GPU (device: 5, name: TITAN Xp, pci bus id: 0000:3f:00.0, compute capability: 6.1)
2019-07-20 12:10:11.647730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:6 with 11424 MB memory) -&gt; physical GPU (device: 6, name: TITAN Xp, pci bus id: 0000:40:00.0, compute capability: 6.1)
2019-07-20 12:10:26.189075: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1483] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2019-07-20 12:10:26.273130: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-07-20 12:10:28.489688: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-20 12:10:36.757915: I tensorflow/core/profiler/lib/profiler_session.cc:174] Profiler session started.
2019-07-20 12:10:36.759100: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcupti.so.10.0
2019-07-20 12:10:40.328991: I tensorflow/core/platform/default/device_tracer.cc:641] Collecting 722 kernel records, 110 memcpy records.
2019-07-20 12:10:45.501340: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_2}}]]
	 [[GroupCrossDeviceControlEdges_0/Adam/Adam/update_6/Const/_297]]
2019-07-20 12:10:45.501340: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_2}}]]
	 [[replica_3/metrics/accuracy/AssignAddVariableOp_1/_187]]
2019-07-20 12:10:45.501340: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_2}}]]
	 [[GroupCrossDeviceControlEdges_5/Adam/Adam/update_5_1/Const/_365]]
2019-07-20 12:10:45.501340: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_2}}]]
	 [[PermConstNCHWToNHWC-LayoutOptimizer/_16]]
2019-07-20 12:10:45.501348: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_2}}]]
	 [[GroupCrossDeviceControlEdges_2/Adam/Adam/update_6/Const/_421]]
2019-07-20 12:10:45.501348: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_2}}]]
	 [[GroupCrossDeviceControlEdges_2/Identity_1/_523]]
2019-07-20 12:10:45.501354: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_2}}]]
	 [[replica_5/loss/mul/_260]]
2019-07-20 12:10:45.502198: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
	 [[{{node IteratorGetNext_2}}]]


Describe the expected behavior
Code to reproduce the issue
&lt;denchmark-link:https://www.tensorflow.org/beta/tutorials/distribute/keras&gt;https://www.tensorflow.org/beta/tutorials/distribute/keras&lt;/denchmark-link&gt;

Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='mjlbach' date='2019-07-24T17:16:26Z'>
		Hi &lt;denchmark-link:https://github.com/mjlbach&gt;@mjlbach&lt;/denchmark-link&gt;
, thanks for filing the issue.  Please confirm a couple of things:

Are you using tf.distribute.MirroredStrategy on a single machine with multiple GPUs?
Can you run NCCL tests on the machine without any TensorFlow code? https://github.com/NVIDIA/nccl-tests

		</comment>
		<comment id='2' author='mjlbach' date='2019-07-24T17:35:07Z'>
		
Yes a single node
It appears to be an nccl issue; I opened an issue with  NVIDIA/nccl#241

&lt;denchmark-code&gt;mjlbach@node05:~/nccl-tests$  ./build/all_reduce_perf -b 8 -e 128M -f 2 -g 8
# nThread 1 nGpus 8 minBytes 8 maxBytes 134217728 step: 2(factor) warmup iters: 5 iters: 20 validation: 1
#
# Using devices
#   Rank  0 Pid 226099 on node05-ccncluster device  0 [0x1a] TITAN Xp
#   Rank  1 Pid 226099 on node05-ccncluster device  1 [0x1b] TITAN Xp
#   Rank  2 Pid 226099 on node05-ccncluster device  2 [0x1c] TITAN Xp
#   Rank  3 Pid 226099 on node05-ccncluster device  3 [0x1d] TITAN Xp
#   Rank  4 Pid 226099 on node05-ccncluster device  4 [0x1e] TITAN Xp
#   Rank  5 Pid 226099 on node05-ccncluster device  5 [0x3d] TITAN Xp
#   Rank  6 Pid 226099 on node05-ccncluster device  6 [0x3e] TITAN Xp
#   Rank  7 Pid 226099 on node05-ccncluster device  7 [0x3f] TITAN Xp
#
#                                                     out-of-place                       in-place
#       size         count    type   redop     time   algbw   busbw  error     time   algbw   busbw  error
#        (B)    (elements)                     (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)
           8             2   float     sum    42.86    0.00    0.00  1e-07    42.51    0.00    0.00  1e-07
          16             4   float     sum    42.46    0.00    0.00  1e-07    43.06    0.00    0.00  1e-07
          32             8   float     sum    42.90    0.00    0.00  6e-08    42.75    0.00    0.00  6e-08
          64            16   float     sum    42.81    0.00    0.00  6e-08    43.06    0.00    0.00  6e-08
         128            32   float     sum    42.81    0.00    0.01  6e-08    42.92    0.00    0.01  6e-08
         256            64   float     sum    43.05    0.01    0.01  3e-08    43.34    0.01    0.01  3e-08
         512           128   float     sum    42.79    0.01    0.02  3e-08    42.65    0.01    0.02  3e-08
        1024           256   float     sum    42.91    0.02    0.04  1e-07    43.00    0.02    0.04  1e-07
        2048           512   float     sum    43.35    0.05    0.08  2e-07    43.25    0.05    0.08  2e-07
        4096          1024   float     sum    43.46    0.09    0.16  2e-07    43.40    0.09    0.17  2e-07
        8192          2048   float     sum    44.38    0.18    0.32  2e-07    43.88    0.19    0.33  2e-07
       16384          4096   float     sum    49.15    0.33    0.58  2e-07    48.86    0.34    0.59  2e-07
       32768          8192   float     sum    72.44    0.45    0.79  2e-07    71.88    0.46    0.80  2e-07
       65536         16384   float     sum    120.5    0.54    0.95  2e-07    121.7    0.54    0.94  2e-07
      131072         32768   float     sum    129.5    1.01    1.77  2e-07    129.5    1.01    1.77  2e-07
      262144         65536   float     sum    157.1    1.67    2.92  2e-07    157.0    1.67    2.92  2e-07
      524288        131072   float     sum    205.4    2.55    4.47  2e-07    205.3    2.55    4.47  2e-07
     1048576        262144   float     sum    305.1    3.44    6.01  2e-07    305.0    3.44    6.02  2e-07
     2097152        524288   float     sum    647.4    3.24    5.67  2e-07    495.1    4.24    7.41  2e-07
     4194304       1048576   float     sum    900.7    4.66    8.15  2e-07    898.9    4.67    8.17  2e-07
     8388608       2097152   float     sum   1735.0    4.83    8.46  2e-07   1718.9    4.88    8.54  2e-07
    16777216       4194304   float     sum   3425.8    4.90    8.57  2e-07   3406.6    4.92    8.62  2e-07
    33554432       8388608   float     sum   6793.3    4.94    8.64  2e-07   6792.5    4.94    8.64  2e-07
    67108864      16777216   float     sum    13579    4.94    8.65  2e-07    13574    4.94    8.65  2e-07
   134217728      33554432   float     sum    27135    4.95    8.66  2e-07    27134    4.95    8.66  2e-07
# Out of bounds values : 0 OK
# Avg bus bandwidth    : 3.0361
#
mjlbach@node05:~/nccl-tests$  ./build/all_reduce_perf -b 8 -e 128M -f 2 -g 10
# nThread 1 nGpus 10 minBytes 8 maxBytes 134217728 step: 2(factor) warmup iters: 5 iters: 20 validation: 1
#
# Using devices
#   Rank  0 Pid 226138 on node05-ccncluster device  0 [0x1a] TITAN Xp
#   Rank  1 Pid 226138 on node05-ccncluster device  1 [0x1b] TITAN Xp
#   Rank  2 Pid 226138 on node05-ccncluster device  2 [0x1c] TITAN Xp
#   Rank  3 Pid 226138 on node05-ccncluster device  3 [0x1d] TITAN Xp
#   Rank  4 Pid 226138 on node05-ccncluster device  4 [0x1e] TITAN Xp
#   Rank  5 Pid 226138 on node05-ccncluster device  5 [0x3d] TITAN Xp
#   Rank  6 Pid 226138 on node05-ccncluster device  6 [0x3e] TITAN Xp
#   Rank  7 Pid 226138 on node05-ccncluster device  7 [0x3f] TITAN Xp
#   Rank  8 Pid 226138 on node05-ccncluster device  8 [0x40] TITAN Xp
#   Rank  9 Pid 226138 on node05-ccncluster device  9 [0x41] TITAN Xp
Segmentation fault (core dumped)
mjlbach@node05:~/nccl-tests$
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='mjlbach' date='2019-07-24T17:53:25Z'>
		Thanks for opening the NCCL issue.  A potential work around in TensorFlow is to use a different  object with .  The default is &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/f22a98fcf8855cb252658437f248874bd7602082/tensorflow/python/distribute/cross_device_ops.py#L793&gt;NcclAllReduce&lt;/denchmark-link&gt;
 but you can specify a different cross device ops object in the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/d1305cf106fc461aff05f7a08f1ed365f9ade4f6/tensorflow/python/distribute/mirrored_strategy.py#L354&gt;strategy constructor&lt;/denchmark-link&gt;
; perhaps you can try &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/d1305cf106fc461aff05f7a08f1ed365f9ade4f6/tensorflow/python/distribute/cross_device_ops.py#L819&gt;HierarchicalCopyAllReduce&lt;/denchmark-link&gt;
.  Note that you may see lower performance with this workaround.
I'm going to close this because it seems like a platform/NCCL issue, but please re-open if there's anything else.
		</comment>
		<comment id='4' author='mjlbach' date='2019-07-24T17:53:27Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=30887&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=30887&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='mjlbach' date='2019-07-24T23:37:01Z'>
		To follow-up; the issue was an X server was running on the GPUs.
		</comment>
		<comment id='6' author='mjlbach' date='2019-07-25T01:26:33Z'>
		Edit: This issue is not closed, even after fixing NCCL the problem persists.
		</comment>
		<comment id='7' author='mjlbach' date='2019-07-25T01:38:35Z'>
		Do you mean the out of range error?
		</comment>
		<comment id='8' author='mjlbach' date='2019-07-25T04:54:52Z'>
		No, even though NCCL works now, the example still fails with the same NCCL error. NCCL tests compiled against system (2.4.7) NCCL yields:
&lt;denchmark-code&gt;(physics3)mjlbach@node05-ccncluster:~/nccl-tests$ ./build/all_reduce_perf -b 8 -e 128M -f
2 -g 10
# nThread 1 nGpus 10 minBytes 8 maxBytes 134217728 step: 2(factor) warmup iters: 5 iters: 20 validation: 1
#
# Using devices
#   Rank  0 Pid 204463 on node05-ccncluster device  0 [0x1a] TITAN Xp
#   Rank  1 Pid 204463 on node05-ccncluster device  1 [0x1b] TITAN Xp
#   Rank  2 Pid 204463 on node05-ccncluster device  2 [0x1c] TITAN Xp
#   Rank  3 Pid 204463 on node05-ccncluster device  3 [0x1d] TITAN Xp
#   Rank  4 Pid 204463 on node05-ccncluster device  4 [0x1e] TITAN Xp
#   Rank  5 Pid 204463 on node05-ccncluster device  5 [0x3d] TITAN Xp
#   Rank  6 Pid 204463 on node05-ccncluster device  6 [0x3e] TITAN Xp
#   Rank  7 Pid 204463 on node05-ccncluster device  7 [0x3f] TITAN Xp
#   Rank  8 Pid 204463 on node05-ccncluster device  8 [0x40] TITAN Xp
#   Rank  9 Pid 204463 on node05-ccncluster device  9 [0x41] TITAN Xp
#
#                                                     out-of-place                       in-place
#       size         count    type   redop     time   algbw   busbw  error     time   algbw   busbw  error
#        (B)    (elements)                     (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)
           8             2   float     sum    98.43    0.00    0.00  1e-07    110.5    0.00    0.00  1e-07
          16             4   float     sum    109.4    0.00    0.00  1e-07    108.9    0.00    0.00  1e-07
          32             8   float     sum    82.26    0.00    0.00  6e-08    57.92    0.00    0.00  6e-08
          64            16   float     sum    57.85    0.00    0.00  6e-08    58.01    0.00    0.00  6e-08
         128            32   float     sum    57.95    0.00    0.00  6e-08    57.64    0.00    0.00  6e-08
         256            64   float     sum    57.86    0.00    0.01  3e-08    58.32    0.00    0.01  3e-08
         512           128   float     sum    57.67    0.01    0.02  3e-08    58.17    0.01    0.02  3e-08
        1024           256   float     sum    58.26    0.02    0.03  2e-07    58.24    0.02    0.03  2e-07
        2048           512   float     sum    58.88    0.03    0.06  2e-07    58.21    0.04    0.06  2e-07
        4096          1024   float     sum    60.01    0.07    0.12  2e-07    59.67    0.07    0.12  2e-07
        8192          2048   float     sum    59.74    0.14    0.25  2e-07    59.66    0.14    0.25  2e-07
       16384          4096   float     sum    61.11    0.27    0.48  2e-07    60.57    0.27    0.49  2e-07
       32768          8192   float     sum    82.05    0.40    0.72  2e-07    81.89    0.40    0.72  2e-07
       65536         16384   float     sum    140.4    0.47    0.84  2e-07    142.4    0.46    0.83  2e-07
      131072         32768   float     sum    154.1    0.85    1.53  2e-07    154.1    0.85    1.53  2e-07
      262144         65536   float     sum    191.3    1.37    2.47  2e-07    193.2    1.36    2.44  2e-07
      524288        131072   float     sum    246.7    2.12    3.82  2e-07    249.2    2.10    3.79  2e-07
     1048576        262144   float     sum    352.6    2.97    5.35  2e-07    353.3    2.97    5.34  2e-07
     2097152        524288   float     sum    566.8    3.70    6.66  2e-07    564.5    3.72    6.69  2e-07
     4194304       1048576   float     sum    982.8    4.27    7.68  2e-07    981.2    4.27    7.69  2e-07
     8388608       2097152   float     sum   1847.0    4.54    8.18  2e-07   1839.7    4.56    8.21  2e-07
    16777216       4194304   float     sum   3566.8    4.70    8.47  2e-07   3567.5    4.70    8.46  2e-07
    33554432       8388608   float     sum   6890.0    4.87    8.77  2e-07   6893.3    4.87    8.76  2e-07
    67108864      16777216   float     sum    13753    4.88    8.78  2e-07    13756    4.88    8.78  2e-07
   134217728      33554432   float     sum    27445    4.89    8.80  2e-07    27432    4.89    8.81  2e-07
# Out of bounds values : 0 OK
# Avg bus bandwidth    : 3.00048
#
&lt;/denchmark-code&gt;

But when I run the example keras script yields the following python error:
&lt;denchmark-code&gt;InternalError: 2 root error(s) found.
  (0) Internal:  internal error
	 [[node Adam/NcclAllReduce_8 (defined at &lt;ipython-input-14-b87e02e8ae0a&gt;:1) ]]
  (1) Internal:  internal error
	 [[node Adam/NcclAllReduce_8 (defined at &lt;ipython-input-14-b87e02e8ae0a&gt;:1) ]]
	 [[Adam/Adam/Identity_2/ReadVariableOp/_963]]
0 successful operations.
9 derived errors ignored. [Op:__inference_distributed_function_219447]

Function call stack:
distributed_function -&gt; distributed_function
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;         [[{{node Adam/NcclAllReduce_8}}]]
         [[Adam/Adam/Identity_2/ReadVariableOp/_963]]
2019-07-24 21:47:55.279011: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: internal error
         [[{{node Adam/NcclAllReduce_8}}]]
         [[Adam/Adam/group_deps/_919]]
2019-07-24 21:47:55.279054: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: internal error
         [[{{node Adam/NcclAllReduce_8}}]]
         [[GroupCrossDeviceControlEdges_8/Adam/Adam/update_8/Const/_795]]
2019-07-24 21:47:55.279094: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: internal error
         [[{{node Adam/NcclAllReduce_8}}]]
         [[GroupCrossDeviceControlEdges_2/Adam/Adam/update_9/Const/_779]]
2019-07-24 21:47:55.279132: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: internal error
         [[{{node Adam/NcclAllReduce_8}}]]
         [[GroupCrossDeviceControlEdges_8/Adam/Adam/update_9/Const/_639]]
2019-07-24 21:47:55.279178: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: internal error
         [[{{node Adam/NcclAllReduce_8}}]]
         [[GroupCrossDeviceControlEdges_8/Adam/Adam/update_8/Const/_831]]
2019-07-24 21:47:55.279252: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: internal error
         [[{{node Adam/NcclAllReduce_8}}]]
         [[GroupCrossDeviceControlEdges_7/Adam/Adam/update_9/Const/_563]]
2019-07-24 21:47:55.279296: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: internal error
         [[{{node Adam/NcclAllReduce_8}}]]
         [[GroupCrossDeviceControlEdges_0/Adam/Adam/update_9/Const/_647]]
2019-07-24 21:47:55.279336: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: internal error
         [[{{node Adam/NcclAllReduce_8}}]]
         [[GroupCrossDeviceControlEdges_1/Adam/Adam/update_9/Const/_695]]
2019-07-24 21:47:55.279780: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: internal error
         [[{{node Adam/NcclAllReduce_8}}]]
         [[Identity_1/_1008]]
2019-07-24 21:47:55.280420: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: internal error
         [[{{node Adam/NcclAllReduce_8}}]]
2019-07-24 21:47:55.280487: W tensorflow/core/framework/op_kernel.cc:1546] OP_REQUIRES failed at nccl_ops.cc:100 : Internal: internal error
2019-07-24 21:47:55.280545: W tensorflow/core/framework/op_kernel.cc:1546] OP_REQUIRES failed at nccl_ops.cc:100 : Internal: internal error
2019-07-24 21:47:55.280591: W tensorflow/core/framework/op_kernel.cc:1546] OP_REQUIRES failed at nccl_ops.cc:100 : Internal: internal error
2019-07-24 21:47:55.280660: W tensorflow/core/framework/op_kernel.cc:1546] OP_REQUIRES failed at nccl_ops.cc:100 : Internal: internal error
2019-07-24 21:47:55.280738: W tensorflow/core/framework/op_kernel.cc:1546] OP_REQUIRES failed at nccl_ops.cc:100 : Internal: internal error
2019-07-24 21:47:55.280799: W tensorflow/core/framework/op_kernel.cc:1546] OP_REQUIRES failed at nccl_ops.cc:100 : Internal: internal error
2019-07-24 21:47:55.280856: W tensorflow/core/framework/op_kernel.cc:1546] OP_REQUIRES failed at nccl_ops.cc:100 : Internal: internal error
2019-07-24 21:47:55.280916: W tensorflow/core/framework/op_kernel.cc:1546] OP_REQUIRES failed at nccl_ops.cc:100 : Internal: internal error
2019-07-24 21:47:55.280965: W tensorflow/core/framework/op_kernel.cc:1546] OP_REQUIRES failed at nccl_ops.cc:100 : Internal: internal error
&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='mjlbach' date='2019-07-25T16:28:44Z'>
		Okay, can you run with NCCL_DEBUG=INFO and post the logs?
		</comment>
		<comment id='10' author='mjlbach' date='2019-07-25T16:38:10Z'>
		This seems to be the cause... I had this issue running NCCL-tests when X was enabled, but now NCCL-tests runs fine.
&lt;denchmark-code&gt;2019-07-25 09:33:34.093469: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 0 and 9, status: Internal: failed to enable
peer access from 0x7f76d46da2c0 to 0x7f76b4713830: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2019-07-25 09:33:34.105255: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 1 and 9, status: Internal: failed to enable
peer access from 0x7f76cc74aba0 to 0x7f76b4713830: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2019-07-25 09:33:34.115187: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 2 and 9, status: Internal: failed to enable
peer access from 0x7f76b071ae60 to 0x7f76b4713830: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2019-07-25 09:33:34.123710: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 3 and 9, status: Internal: failed to enable
peer access from 0x7f76c870a380 to 0x7f76b4713830: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2019-07-25 09:33:34.134640: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 4 and 9, status: Internal: failed to enable
peer access from 0x7f76c47226a0 to 0x7f76b4713830: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2019-07-25 09:33:34.145270: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 5 and 9, status: Internal: failed to enable
peer access from 0x7f76c0740bc0 to 0x7f76b4713830: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2019-07-25 09:33:34.149014: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 6 and 9, status: Internal: failed to enable
peer access from 0x7f76d072a8e0 to 0x7f76b4713830: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2019-07-25 09:33:34.151168: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 7 and 9, status: Internal: failed to enable
peer access from 0x7f76bc731660 to 0x7f76b4713830: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2019-07-25 09:33:34.151761: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 8 and 9, status: Internal: failed to enable
peer access from 0x7f76b87398c0 to 0x7f76b4713830: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2019-07-25 09:33:34.151981: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 9 and 0, status: Internal: failed to enable
peer access from 0x7f76b4713830 to 0x7f76d46da2c0: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2019-07-25 09:33:34.152197: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 9 and 1, status: Internal: failed to enable
peer access from 0x7f76b4713830 to 0x7f76cc74aba0: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2019-07-25 09:33:34.152413: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 9 and 2, status: Internal: failed to enable
peer access from 0x7f76b4713830 to 0x7f76b071ae60: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2019-07-25 09:33:34.152628: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 9 and 3, status: Internal: failed to enable
peer access from 0x7f76b4713830 to 0x7f76c870a380: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2019-07-25 09:33:34.152846: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 9 and 4, status: Internal: failed to enable
peer access from 0x7f76b4713830 to 0x7f76c47226a0: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2019-07-25 09:33:34.153051: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 9 and 5, status: Internal: failed to enable
peer access from 0x7f76b4713830 to 0x7f76c0740bc0: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2019-07-25 09:33:34.153254: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 9 and 6, status: Internal: failed to enable
peer access from 0x7f76b4713830 to 0x7f76d072a8e0: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2019-07-25 09:33:34.153459: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 9 and 7, status: Internal: failed to enable
peer access from 0x7f76b4713830 to 0x7f76bc731660: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2019-07-25 09:33:34.153664: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 9 and 8, status: Internal: failed to enable
peer access from 0x7f76b4713830 to 0x7f76b87398c0: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted
2019-07-25 09:33:34.156023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:1a:00.0
&lt;/denchmark-code&gt;

		</comment>
		<comment id='11' author='mjlbach' date='2019-07-25T16:53:20Z'>
		I think you may be running into &lt;denchmark-link:https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__PEER__ACCESS.html#group__CUDA__PEER__ACCESS_1g0889ec6728e61c05ed359551d67b3f5a&gt;this issue mentioned in NVIDIA docs&lt;/denchmark-link&gt;
 and also discussed in another &lt;denchmark-link:https://github.com/tensorflow/benchmarks/issues/143&gt;TensorFlow issue thread&lt;/denchmark-link&gt;
.  Can you follow-up with NVIDIA on the NCCL issue and confirm that more than 8 GPUs is actually a well-supported configuration?
		</comment>
		<comment id='12' author='mjlbach' date='2019-07-25T17:15:49Z'>
		In the linked issue in &lt;denchmark-link:https://github.com/NVIDIA/nccl/issues/241&gt;NVIDIA/nccl#241&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/sjeaugey&gt;@sjeaugey&lt;/denchmark-link&gt;
 believes it's an issue on the TF side
		</comment>
		<comment id='13' author='mjlbach' date='2019-07-25T17:28:39Z'>
		Hi &lt;denchmark-link:https://github.com/sjeaugey&gt;@sjeaugey&lt;/denchmark-link&gt;
, can you help us understand what are the possible causes of ?
		</comment>
		<comment id='14' author='mjlbach' date='2019-07-25T17:50:58Z'>
		There is indeed a limitation to 8 peers as described in the documentation, hence if you enable P2P between all GPUs it will fail with 10 GPUs or more per system. NCCL only uses 2 connections at the moment so it does work in that case (provided Xorg didn't allocate all peer connections already).
Whether you want to support that case in TF is up to you. Maybe you don't need to enable p2p between all GPUs if you are using NCCL for intra-node communication.
And while this 10x TitanXp configuration is probably not supported, there could be similar configurations with e.g. T4 which you might want to support.
		</comment>
		<comment id='15' author='mjlbach' date='2019-11-06T22:36:30Z'>
		got some problem on Windows. is it a limitation and we can't do anything with this?
		</comment>
		<comment id='16' author='mjlbach' date='2020-03-29T17:38:56Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30887&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30887&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>