<bug id='29213' author='lgeiger' open_date='2019-05-31T13:00:56Z' closed_time='2019-08-07T09:25:13Z'>
	<summary>GPU OOM error when using keras and distributions strategy</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): tf-nightly-gpu==1.14.1.dev20190524
Python version: Python 3.7.3
CUDA/cuDNN version: CUDA 10, CUDNN 7.4.2.24
GPU model and memory: 4 x NVIDIA V100


Using  together with Keras to train models as described in &lt;denchmark-link:https://www.tensorflow.org/alpha/tutorials/distribute/keras&gt;https://www.tensorflow.org/alpha/tutorials/distribute/keras&lt;/denchmark-link&gt;
 results in GPU out of memory errors appearing after several epochs of training. We excluded our custom written code as the source of the memory leaks and made sure that the model actually fits into memory with enough headroom. It seams that either  or  have a memory leak that starts showing up after several epochs of training and evaluation.
Describe the expected behavior
Tensorflow doesn't throw OOM errors.
Code to reproduce the issue
Unfortunately I cannot give a concrete code example to reproduce this issue since memory leaks appear anytime in between 10min to 12h of training. Though I am happy to provide more information and would be eager to get suggestions on how to properly debug this problem.
Other info / logs
File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py", line 644, in fit
use_multiprocessing=use_multiprocessing)
File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_distributed.py", line 899, in fit
validation_freq=validation_freq)
File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_distributed.py", line 149, in fit_distributed
steps_name='steps_per_epoch')
File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_arrays.py", line 409, in model_iteration
steps_name='validation_steps')
File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_arrays.py", line 274, in model_iteration
batch_outs = f(actual_inputs)
File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py", line 3351, in __call__
run_metadata=self.run_metadata)
File "/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py", line 1458, in __call__
run_metadata_ptr)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.
(0) Resource exhausted: Failed to allocate memory for the batch of component 0

[[{{node MultiDeviceIteratorGetNextFromShard}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
[[RemoteCall]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
[[IteratorGetNext_7]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
(1) Resource exhausted: Failed to allocate memory for the batch of component 0
[[{{node MultiDeviceIteratorGetNextFromShard}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
[[RemoteCall]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
[[IteratorGetNext_7]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
[[metrics_4/categorical_accuracy/Identity_2/_3447]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
0 successful operations.
3 derived errors ignored.
	</description>
	<comments>
		<comment id='1' author='lgeiger' date='2019-06-05T20:31:56Z'>
		Do you have anything in your code such as a callback that keep adding new ops to the graph?
		</comment>
		<comment id='2' author='lgeiger' date='2019-06-05T22:01:47Z'>
		
Do you have anything in your code such as a callback that keep adding new ops to the graph?

As far as we can tell, no. We are using the TensorBoard and ModelCheckpoint callbacks to monitor training process and don't have any other callbacks.
&lt;denchmark-link:https://github.com/yuefengz&gt;@yuefengz&lt;/denchmark-link&gt;
 Is there a way to check if TensorFlow or our code adds new ops to the graph when training the keras model?
We are now able to also reproduce this error on a single GPU without distributions strategy. One thing to note is that we are using tf.data.Dataset::prefetch(tf.data.experimental.AUTOTUNE) as the last operation in the tf.data pipeline, though if I recall correctly tf.data.Dataset::prefetch shouldn't prefetch onto the GPU memory, correct?
Once we are able to reliably reproduce this issue I am happy to share a repo/gist with the code. Any help or pointer for debugging would be greatly appreciated in the meantime.
		</comment>
		<comment id='3' author='lgeiger' date='2019-06-05T23:48:43Z'>
		To check whether there is any new op added you graph after training starts, you can add loggings to the create_op method: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L3221&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L3221&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
 for dataset questions.
		</comment>
		<comment id='4' author='lgeiger' date='2019-06-06T04:33:24Z'>
		&lt;denchmark-link:https://github.com/lgeiger&gt;@lgeiger&lt;/denchmark-link&gt;
 tf.data without distribution strategy will not prefetch any data to GPU (unless you are using  which has been deprecated in lieu of multi-device iterator used by the distribution strategy under the hoods)
		</comment>
		<comment id='5' author='lgeiger' date='2019-06-06T15:33:45Z'>
		
To check whether there is any new op added you graph after training starts, you can add loggings to the create_op method

Thanks. I double checked, that we don't add any operations to the graph after training starts.
		</comment>
		<comment id='6' author='lgeiger' date='2019-06-12T13:15:31Z'>
		Same problem. I also double checked my code that no new op after training starts. Suspect that there's some memory leak somewhere in tf.
		</comment>
		<comment id='7' author='lgeiger' date='2019-08-07T09:25:13Z'>
		We where able to resolve this issue by not using .prefetch(tf.data.experimental.AUTOTUNE) as the last dataset transformation, which caused problems with TensorFlow running inside a Kubernetes cluster without the correct resource requests and limits and data streaming from GCS.
		</comment>
		<comment id='8' author='lgeiger' date='2019-08-07T09:25:15Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=29213&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=29213&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='lgeiger' date='2020-06-11T12:20:34Z'>
		I encountered this issue today. For me it turned out that I accidentally batched my dataset multiple times, instead of once.
		</comment>
		<comment id='10' author='lgeiger' date='2020-06-29T08:52:46Z'>
		Initially, I did train_ds = train_ds.cache().prefetch(buffer_size=2), then i removed .prefetch, still errors, then i removed .cache(), and it worked.
Note: cache.prefetch works fine on 1/5 the number of images. I had this error while scaling up the data.
Also there are other symptoms it will fail. I see my RAM go from 5gb to 15gb (using up all of it) as the training steps begin and always exactly at step 308/330 i get the error Resource Exhausted: Failed to allocate memory for the batch of component 0 [[node IteratorGetNext.
After removing cache and prefetch, my RAM goes up to 9gb and maintains.
Anyone has an explanation for how cache and prefetch affects RAM?
Also, how can we be sure AUTOTUNE will not prefetch too many batches and cause errors in the middle of training?
&lt;denchmark-link:https://github.com/Trezorro&gt;@Trezorro&lt;/denchmark-link&gt;
 Could you share how you batch your dataset multiple times accidentally and why would that cause this ResourceExhaustedError?
Is it something like in a single method chain of data.cache().batch(32).prefetch(1).batch(32)?
Or 4 assignments separated some distance apart in code? eg. data = data.cache(), data=data.batch(32), data = data.prefetch(1), data=data.batch(32)
		</comment>
		<comment id='11' author='lgeiger' date='2020-07-06T10:54:14Z'>
		I used assigments, so that I could put some steps within a conditional
statement. I used prefetch only at the very end however.

While fiddling with my pipeline, I accidentally had two lines with:
` examples = examples.batch(batchsize)`

This of course got me in some memory issues, even when every single example
is just a 224x224 image.

Op ma 29 jun. 2020 om 10:53 schreef gitgithan &lt;notifications@github.com&gt;
 &lt;denchmark-link:https://github.com/Trezorro&gt;@Trezorro&lt;/denchmark-link&gt;
 &lt;&lt;denchmark-link:https://github.com/Trezorro&gt;https://github.com/Trezorro&lt;/denchmark-link&gt;
&gt; Could you share how you batch
 your dataset multiple times accidentally and why would that cause this
 ResourceExhaustedError?

 Is it something like in a single method chain of
 data.cache().batch(32).prefetch(1).batch(32)?
 Or 4 assignments separated some distance apart in code? eg. data =
 data.cache(), data=data.batch(32), data = data.prefetch(1),
 data=data.batch(32)

 â€”
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/29213#issuecomment-651026270&gt;#29213 (comment)&lt;/denchmark-link&gt;
&gt;,
 or unsubscribe
 &lt;&lt;denchmark-link:https://github.com/notifications/unsubscribe-auth/AESTRG2VUV2PNURLJ5OGD5DRZBI7DANCNFSM4HR2RBWA&gt;https://github.com/notifications/unsubscribe-auth/AESTRG2VUV2PNURLJ5OGD5DRZBI7DANCNFSM4HR2RBWA&lt;/denchmark-link&gt;
&gt;
 .

-- 
Met vriendelijke groet,
Milan Tresoor
		</comment>
	</comments>
</bug>