<bug id='37144' author='nsssss' open_date='2020-02-27T21:05:26Z' closed_time='2020-05-06T07:08:00Z'>
	<summary>WARNING:tensorflow:AutoGraph could not transform and will run it as-is. Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.</summary>
	<description>
System information

Have I written custom code: Yes
OS Platform and Distribution : Windows 10
TensorFlow installed from (source or binary): Anaconda
TensorFlow version (use command below): 2.1.0
Python version: 3.7.4

Describe the current behavior
I'm using anaconda tensorflow, Spyder. When run my custom layers, it shows warning as below:
&lt;denchmark-code&gt;WARNING:tensorflow:AutoGraph could not transform &lt;bound method GroupSoftmax.call of &lt;__main__.GroupSoftmax object at 0x000002A957B843C8&gt;&gt; and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: 
WARNING: AutoGraph could not transform &lt;bound method GroupSoftmax.call of &lt;__main__.GroupSoftmax object at 0x000002A957B843C8&gt;&gt; and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
&lt;/denchmark-code&gt;

Describe the method I tried
I have already tried the solution which others provided: pip install gast==0.2.2
I also re-installed all of the softwares (anaconda, tensorflow, spyder).
However, these methods doesn't solve my problem.
Is there any other solution?
Standalone code to reproduce the issue
&lt;denchmark-code&gt;class GroupSoftmax(layers.Layer):
    def __init__(self, axis=-1, **kwargs):
        super(GroupSoftmax, self).__init__(**kwargs)
        self.supports_masking = True
        self.axis = axis

    def call(self, inputs):
        return tf.divide(inputs, tf.reduce_sum(inputs, axis=self.axis))

    def get_config(self):
        config = {'axis': self.axis}
        base_config = super(GroupSoftmax, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))
    
    @tf_utils.shape_type_conversion
    def compute_output_shape(self, input_shape):
        return input_shape

'''
-----------------network of g-----------------
'''
gModel = tf.keras.Sequential([
# 添加一个有Nodes个神经元的全连接层，“input_shape”为该层接受的输入数据的维度，“activation”指定该层所用的激活函数
layers.Dense(Nodes, activation='sigmoid', input_shape=(60,), use_bias = False),#封装数据应该为（3000，10，6）
# 添加第二个网络层
layers.Dense(Nodes, activation='sigmoid', use_bias = False),
# 添加第3个网络层
layers.Dense(Nodes, activation='sigmoid', use_bias = False),
# 添加第4个网络层
layers.Dense(Nodes, activation='sigmoid', use_bias = False),
# 添加第5个网络层
layers.Dense(Nodes, activation='sigmoid', use_bias = False),
# 添加第6个网络层,改变节点数目
layers.Dense(66, activation='sigmoid', use_bias = False),
# 添加第7个网络层,改变shape
layers.Reshape((11, 6)),
# 添加output网络层,分组softmax
#layers.Dense(6, activation=layers.Softmax(axis=0),input_shape=(11,6), use_bias = False), # [11,6]
#layers.Softmax(axis=0)
GroupSoftmax(axis=0)
])

gModel.summary()   
&lt;/denchmark-code&gt;

Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='nsssss' date='2020-02-28T09:07:09Z'>
		&lt;denchmark-link:https://github.com/nsssss&gt;@nsssss&lt;/denchmark-link&gt;

I have tried in colab with TF 2.1.0 and i am not seeing any issue. Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/5acb8f3e34b93bd1a7d237e2f1d64fe9/untitled685.ipynb&gt;here&lt;/denchmark-link&gt;
.I have made few assumptions in code while executing. If you feel there is an issue please do update in attached colab and help me to reproduce the issue. It helps me in localizing the issue faster. Thanks!
		</comment>
		<comment id='2' author='nsssss' date='2020-02-28T17:26:50Z'>
		
@nsssss
I have tried in colab with TF 2.1.0 and i am not seeing any issue. Please, find the gist here.I have made few assumptions in code while executing. If you feel there is an issue please do update in attached colab and help me to reproduce the issue. It helps me in localizing the issue faster. Thanks!

Thanks. there's no problem with colab. But there's always a warning when using spyder in anaconda tensorflow. It seems there's no influence on the results, but I'm not sure if it will influence the speed.
		</comment>
		<comment id='3' author='nsssss' date='2020-03-17T23:34:39Z'>
		You can safely ignore the warning log as its intended to debug logging in AutoGraph issues. Thanks !
		</comment>
		<comment id='4' author='nsssss' date='2020-04-02T01:21:03Z'>
		
It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue?

Yes, this problem still exists.
		</comment>
		<comment id='5' author='nsssss' date='2020-04-21T19:41:27Z'>
		&lt;denchmark-link:https://github.com/nsssss&gt;@nsssss&lt;/denchmark-link&gt;

The warning will not hurt performance, but I'm curious as to what the cause might be. Can you add this line just before importing tensorflow: tf.autograph.set_verbosity(3, True)? That will print additional details with the warning message that we could use to investigate.
		</comment>
		<comment id='6' author='nsssss' date='2020-04-29T07:06:37Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='7' author='nsssss' date='2020-05-06T07:07:59Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='8' author='nsssss' date='2020-05-06T07:08:02Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37144&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37144&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='nsssss' date='2020-05-21T08:40:43Z'>
		Hello,
I have used tf.autograph.set_verbosity(3, True) and here is what I have got:
INFO:tensorflow:Converted call: &lt;function TensorLikeDataAdapter.init..permutation at 0x00000225B41F2D38&gt;
args: (&lt;tf.Tensor 'args_0:0' shape=() dtype=int64&gt;,)
kwargs: {}
Converted call: &lt;function TensorLikeDataAdapter.init..permutation at 0x00000225B41F2D38&gt;
args: (&lt;tf.Tensor 'args_0:0' shape=() dtype=int64&gt;,)
kwargs: {}
INFO:tensorflow:Whitelisted: &lt;function TensorLikeDataAdapter.init..permutation at 0x00000225B41F2D38&gt;: DoNotConvert rule for tensorflow
Whitelisted: &lt;function TensorLikeDataAdapter.init..permutation at 0x00000225B41F2D38&gt;: DoNotConvert rule for tensorflow
INFO:tensorflow:Converted call: &lt;function TensorLikeDataAdapter.init..slice_batch_indices at 0x00000225B41F2438&gt;
args: (&lt;tf.Tensor 'args_0:0' shape=(100,) dtype=int64&gt;,)
kwargs: {}
Converted call: &lt;function TensorLikeDataAdapter.init..slice_batch_indices at 0x00000225B41F2438&gt;
args: (&lt;tf.Tensor 'args_0:0' shape=(100,) dtype=int64&gt;,)
kwargs: {}
INFO:tensorflow:Whitelisted: &lt;function TensorLikeDataAdapter.init..slice_batch_indices at 0x00000225B41F2438&gt;: DoNotConvert rule for tensorflow
Whitelisted: &lt;function TensorLikeDataAdapter.init..slice_batch_indices at 0x00000225B41F2438&gt;: DoNotConvert rule for tensorflow
INFO:tensorflow:Converted call: &lt;function TensorLikeDataAdapter.slice_inputs..grab_batch at 0x000002258096BE58&gt;
args: (&lt;tf.Tensor 'args_0:0' shape=(None,) dtype=int64&gt;, (&lt;tf.Tensor 'args_1:0' shape=(100, 35, 1) dtype=float32&gt;,))
kwargs: {}
Converted call: &lt;function TensorLikeDataAdapter.slice_inputs..grab_batch at 0x000002258096BE58&gt;
args: (&lt;tf.Tensor 'args_0:0' shape=(None,) dtype=int64&gt;, (&lt;tf.Tensor 'args_1:0' shape=(100, 35, 1) dtype=float32&gt;,))
kwargs: {}
INFO:tensorflow:Whitelisted: &lt;function TensorLikeDataAdapter.slice_inputs..grab_batch at 0x000002258096BE58&gt;: DoNotConvert rule for tensorflow
Whitelisted: &lt;function TensorLikeDataAdapter.slice_inputs..grab_batch at 0x000002258096BE58&gt;: DoNotConvert rule for tensorflow
INFO:tensorflow:Converted call: &lt;function Model.make_predict_function..predict_function at 0x00000225B4276EE8&gt;
args: (&lt;tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x000002264AF74A48&gt;,)
kwargs: {}
Converted call: &lt;function Model.make_predict_function..predict_function at 0x00000225B4276EE8&gt;
args: (&lt;tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x000002264AF74A48&gt;,)
kwargs: {}
INFO:tensorflow:Cache hit for entity &lt;function Model.make_predict_function..predict_function at 0x00000225B4276EE8&gt; subkey (&lt;tensorflow.python.autograph.core.converter.ConversionOptions object at 0x000002264AF74FC8&gt;, frozenset({'self'})): _ConvertedEntityFactoryInfo(tf__predict_function in tmpmvnq3xyi)
Cache hit for entity &lt;function Model.make_predict_function..predict_function at 0x00000225B4276EE8&gt; subkey (&lt;tensorflow.python.autograph.core.converter.ConversionOptions object at 0x000002264AF74FC8&gt;, frozenset({'self'})): _ConvertedEntityFactoryInfo(tf__predict_function in tmpmvnq3xyi)
INFO:tensorflow:Error transforming entity &lt;function Model.make_predict_function..predict_function at 0x00000225B4276EE8&gt;
Traceback (most recent call last):
File "C:\Users\katica.ristic\Anaconda3\lib\site-packages\tensorflow\python\autograph\impl\api.py", line 538, in converted_call
converted_f = conversion.convert(target_entity, program_ctx)
File "C:\Users\katica.ristic\Anaconda3\lib\site-packages\tensorflow\python\autograph\impl\conversion.py", line 362, in convert
return _instantiate(entity, converted_entity_info, free_nonglobal_var_names)
File "C:\Users\katica.ristic\Anaconda3\lib\site-packages\tensorflow\python\autograph\impl\conversion.py", line 300, in _instantiate
factory = converted_entity_info.get_factory()
File "C:\Users\katica.ristic\Anaconda3\lib\site-packages\tensorflow\python\autograph\impl\conversion.py", line 94, in get_factory
assert self.module_name in sys.modules
AssertionError
Error transforming entity &lt;function Model.make_predict_function..predict_function at 0x00000225B4276EE8&gt;
WARNING:tensorflow:AutoGraph could not transform &lt;function Model.make_predict_function..predict_function at 0x00000225B4276EE8&gt; and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, export AUTOGRAPH_VERBOSITY=10) and attach the full output.
Cause:
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform &lt;function Model.make_predict_function..predict_function at 0x00000225B4276EE8&gt; and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, export AUTOGRAPH_VERBOSITY=10) and attach the full output.
Cause:
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
INFO:tensorflow:Converted call: &lt;bound method Model.predict_step of &lt;tensorflow.python.keras.engine.sequential.Sequential object at 0x000002264C0EBC88&gt;&gt;
args: ((&lt;tf.Tensor 'IteratorGetNext:0' shape=(None, 35, 1) dtype=float32&gt;,),)
kwargs: {}
Converted call: &lt;bound method Model.predict_step of &lt;tensorflow.python.keras.engine.sequential.Sequential object at 0x000002264C0EBC88&gt;&gt;
args: ((&lt;tf.Tensor 'IteratorGetNext:0' shape=(None, 35, 1) dtype=float32&gt;,),)
kwargs: {}
INFO:tensorflow:Whitelisted &lt;bound method Model.predict_step of &lt;tensorflow.python.keras.engine.sequential.Sequential object at 0x000002264C0EBC88&gt;&gt;: from cache
Whitelisted &lt;bound method Model.predict_step of &lt;tensorflow.python.keras.engine.sequential.Sequential object at 0x000002264C0EBC88&gt;&gt;: from cache
Traceback (most recent call last):
File "C:\Users\katica.ristic\Anaconda3\lib\site-packages\tensorflow\python\autograph\impl\api.py", line 538, in converted_call
converted_f = conversion.convert(target_entity, program_ctx)
File "C:\Users\katica.ristic\Anaconda3\lib\site-packages\tensorflow\python\autograph\impl\conversion.py", line 362, in convert
return _instantiate(entity, converted_entity_info, free_nonglobal_var_names)
File "C:\Users\katica.ristic\Anaconda3\lib\site-packages\tensorflow\python\autograph\impl\conversion.py", line 300, in _instantiate
factory = converted_entity_info.get_factory()
File "C:\Users\katica.ristic\Anaconda3\lib\site-packages\tensorflow\python\autograph\impl\conversion.py", line 94, in get_factory
assert self.module_name in sys.modules
AssertionError
		</comment>
		<comment id='10' author='nsssss' date='2020-05-21T08:45:26Z'>
		Hope this could help solve the issue.
I am using  Windows 10, Anaconda, TensorFlow version 2.2.0
Python version 3.7
		</comment>
		<comment id='11' author='nsssss' date='2020-05-21T11:25:27Z'>
		&lt;denchmark-link:https://github.com/KaticaR&gt;@KaticaR&lt;/denchmark-link&gt;
 thank you for the logs, they seem to indicate an incompatibility with the toolchain, though it's unclear which piece. At any rate, the faulting piece was refactored recently. If you have the chance, please retry with tf-nightly.
		</comment>
		<comment id='12' author='nsssss' date='2020-06-07T13:30:37Z'>
		Hi I got this as well, and as the message says I set the env variable and recorded the stack trace:
&lt;denchmark-code&gt;WARNING: AutoGraph could not transform &lt;function initialize_tpu_system.&lt;locals&gt;._tpu_init_fn at 0x7fdbda21fcb0&gt; and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: No module named 'tensorflow_core.keras'
Traceback (most recent call last):
  File "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py", line 3331, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "&lt;ipython-input-9-2eec46e4b091&gt;", line 13, in &lt;module&gt;
    tf.tpu.experimental.initialize_tpu_system(tpu)
  File "/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/tpu/tpu_strategy_util.py", line 103, in initialize_tpu_system
  File "/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py", line 942, in numpy
  File "/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py", line 910, in _numpy
  File "&lt;string&gt;", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef expected inputs 'string' do not match 0 inputs specified; Op&lt;name=_Send; signature=tensor:T -&gt; ; attr=T:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true&gt;; NodeDef: {{node _Send}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py", line 2044, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'InvalidArgumentError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py", line 1148, in get_records
    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)
  File "/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py", line 316, in wrapped
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py", line 350, in _fixed_getinnerframes
    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))
  File "/opt/conda/lib/python3.7/inspect.py", line 1502, in getinnerframes
    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)
  File "/opt/conda/lib/python3.7/inspect.py", line 1460, in getframeinfo
    filename = getsourcefile(frame) or getfile(frame)
  File "/opt/conda/lib/python3.7/inspect.py", line 696, in getsourcefile
    if getattr(getmodule(object, filename), '__loader__', None) is not None:
  File "/opt/conda/lib/python3.7/inspect.py", line 733, in getmodule
    if ismodule(module) and hasattr(module, '__file__'):
  File "/opt/conda/lib/python3.7/site-packages/tensorflow/__init__.py", line 50, in __getattr__
    from ._api.v2 import data
  File "/opt/conda/lib/python3.7/site-packages/tensorflow/__init__.py", line 44, in _load
    from ._api.v2 import audio
  File "/opt/conda/lib/python3.7/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "&lt;frozen importlib._bootstrap&gt;", line 1006, in _gcd_import
  File "&lt;frozen importlib._bootstrap&gt;", line 983, in _find_and_load
  File "&lt;frozen importlib._bootstrap&gt;", line 965, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'tensorflow_core.keras'
&lt;/denchmark-code&gt;

I was running tensorflow in a Kaggle kernel.
		</comment>
		<comment id='13' author='nsssss' date='2020-06-07T14:13:38Z'>
		When we downgraded the versions of keras and tensorflow to match, then the error disappointed.
		</comment>
		<comment id='14' author='nsssss' date='2020-07-07T19:02:08Z'>
		
When we downgraded the versions of keras and tensorflow to match, then the error disappointed.

Hey KaticaR, may I ask to which version of Keras have you downgraded to match TF 2.1? I know TF 2.2 is out but its GPU version is still not supported by Anaconda yet.
		</comment>
		<comment id='15' author='nsssss' date='2020-07-09T15:24:52Z'>
		i am having this while trying Xception model on tpu :
WARNING: AutoGraph could not transform &lt;function  at 0x7f1e447a1950&gt; and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, export AUTOGRAPH_VERBOSITY=10) and attach the full output.
Cause: Unable to locate the source code of &lt;function  at 0x7f1e447a1950&gt;. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
and also it is taking a lot of time
for 2 epochs it takes 59 minutes on TPU with Xception model
but same code where if i change Xception with efficientnet-b6 or b7 then i can run 13 epochs which will finish within 1 hour!!
here is my code : &lt;denchmark-link:https://www.kaggle.com/mobassir/in-depth-melanoma-and-modeling?scriptVersionId=38354798&gt;https://www.kaggle.com/mobassir/in-depth-melanoma-and-modeling?scriptVersionId=38354798&lt;/denchmark-link&gt;

and here you can see : &lt;denchmark-link:https://www.kaggle.com/agentauers/incredible-tpus-finetune-effnetb0-b6-at-once&gt;https://www.kaggle.com/agentauers/incredible-tpus-finetune-effnetb0-b6-at-once&lt;/denchmark-link&gt;

same code with efficientnet-b6 or b7 is extremely fast compared to my simple Xception model
please help?
		</comment>
		<comment id='16' author='nsssss' date='2020-07-09T15:47:14Z'>
		&lt;denchmark-link:https://github.com/mobassir94&gt;@mobassir94&lt;/denchmark-link&gt;
 try the following to avoid the warning:
&lt;denchmark-code&gt;ds_train     = ds_train.map(lambda img, label: (img, tuple([label])))
&lt;/denchmark-code&gt;

with:
&lt;denchmark-code&gt;unpack_label = lambda img, label: (img, tuple([label]))
unpack_label = tf.autograph.do_not_convert(unpack_label)  # Runtime not compatible
ds_train = ds_train.map(unpack_label)
&lt;/denchmark-code&gt;

The slowness problem is unrelated, though.
		</comment>
		<comment id='17' author='nsssss' date='2020-07-09T16:31:33Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
  thanks
where do i ask about that slowness problem and how do i solve this slowness issue? only thing i changed is model from efficientnet to xception :(
		</comment>
		<comment id='18' author='nsssss' date='2020-07-09T16:38:51Z'>
		I recommend StackOverflow. If you are certain that xception should be as fast as efficientnet, and can reproduce it with a simple example, consider filing a separate  GitHub issue as well.
More info: &lt;denchmark-link:https://www.tensorflow.org/community&gt;https://www.tensorflow.org/community&lt;/denchmark-link&gt;

		</comment>
		<comment id='19' author='nsssss' date='2020-07-09T16:41:12Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
  stackoverflow is a terrible place for data science/Deep learning help, i have asked tons of questions there and never got any help except some downvotes even for a good post!
		</comment>
		<comment id='20' author='nsssss' date='2020-07-09T17:08:09Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
  my tf version 2.2.0 and with your code i get this erro : AttributeError: module 'tensorflow._api.v2.autograph' has no attribute 'do_not_convert'
		</comment>
		<comment id='21' author='nsssss' date='2020-07-09T17:09:58Z'>
		Sorry, for 2.2 please use tf.autograph.experimental.do_not_convert.
		</comment>
		<comment id='22' author='nsssss' date='2020-07-12T13:43:34Z'>
		&lt;denchmark-link:https://github.com/mdanatg&gt;@mdanatg&lt;/denchmark-link&gt;
  Thanks, the problem is fixed with this solution.
		</comment>
		<comment id='23' author='nsssss' date='2020-07-13T02:23:11Z'>
		
@mobassir94 try the following to avoid the warning:
ds_train     = ds_train.map(lambda img, label: (img, tuple([label])))

with:
unpack_label = lambda img, label: (img, tuple([label]))
unpack_label = tf.autograph.do_not_convert(unpack_label)  # Runtime not compatible
ds_train = ds_train.map(unpack_label)

The slowness problem is unrelated, though.

sorry, would you please put your fix in order. what ds_train refers to?
		</comment>
		<comment id='24' author='nsssss' date='2020-07-13T12:47:14Z'>
		&lt;denchmark-link:https://github.com/spawnaga&gt;@spawnaga&lt;/denchmark-link&gt;
 this workaround refers specifically to &lt;denchmark-link:https://github.com/mobassir94&gt;@mobassir94&lt;/denchmark-link&gt;
's code. It is not a general fix. Also note that the issue is fixed in tf-nightly.
		</comment>
		<comment id='25' author='nsssss' date='2020-07-13T12:53:19Z'>
		&lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;
 might have better advice on active user groups who could answer &lt;denchmark-link:https://github.com/mobassir94&gt;@mobassir94&lt;/denchmark-link&gt;
's questions
		</comment>
	</comments>
</bug>