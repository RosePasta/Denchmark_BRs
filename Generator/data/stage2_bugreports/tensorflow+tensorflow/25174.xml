<bug id='25174' author='timudk' open_date='2019-01-24T17:06:21Z' closed_time='2019-02-22T22:51:28Z'>
	<summary>Why does tensorflow element not forget origin after using .eval()</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): Binary
TensorFlow version (use command below): 1.12.0
Python version: Python 3.5.2
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

Describe the current behavior
I am using tensorflow_probability (0.5.0) to sample points (random walk metropolis algorithm) that I want to use as inputs for my neural network. The sample points are Tensor("mcmc_sample_chain/current_state:0", shape=(), dtype=float32) so I use tf.eval() to get the actual float number. After multiplying the input with my first layer I get as an object Tensor("mcmc_sample_chain/mh_bootstrap_results/rwm_bootstrap_results/cond/Add:0", shape=(1, 16), dtype=float32).
Describe the expected behavior
I would expect that tf.eval() gives me a numpy float/array and therefore the ouput after multiplying with my first layer should result in something like Tensor("Add_12:0", shape=(1, 16), dtype=float32). Why does the numpy array/float "remember its origin"?
	</description>
	<comments>
		<comment id='1' author='timudk' date='2019-02-15T00:23:18Z'>
		&lt;denchmark-link:https://github.com/timudk&gt;@timudk&lt;/denchmark-link&gt;
 Could you provide a code to reproduce the bug? Thanks!
		</comment>
		<comment id='2' author='timudk' date='2019-02-22T22:51:28Z'>
		Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
	</comments>
</bug>