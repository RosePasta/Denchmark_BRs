<bug id='21537' author='cag472' open_date='2018-08-10T06:55:19Z' closed_time='2018-11-20T18:32:01Z'>
	<summary>problem when num_enqueue == batch_size</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;



Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes


OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 18.04


Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A


TensorFlow installed from (source or binary):
source


TensorFlow version (use command below):
v1.08, v1.10


Python version:
2.7


Bazel version (if compiling from source):
0.16.0


GCC/Compiler version (if compiling from source):
6.4.0


CUDA/cuDNN version:
9.2


GPU model and memory:
NVIDIA 1080 Ti


Exact command to reproduce:


&lt;denchmark-code&gt;      image_list = [ tf.image.crop_to_bounding_box(image, 16, 16, 224, 224),
                    tf.image.crop_to_bounding_box(image, 0, 0, 224, 224),
                    tf.image.crop_to_bounding_box(image, 32, 0, 224, 224),
                    tf.image.crop_to_bounding_box(image, 0, 32, 224, 224),
                    tf.image.crop_to_bounding_box(image, 32, 32, 224, 224),
                    tf.image.flip_left_right(tf.image.crop_to_bounding_box(image, 16, 16, 224, 224)),
                    tf.image.flip_left_right(tf.image.crop_to_bounding_box(image, 0, 0, 224, 224)),
                    tf.image.flip_left_right(tf.image.crop_to_bounding_box(image, 32, 0, 224, 224)),
                    tf.image.flip_left_right(tf.image.crop_to_bounding_box(image, 0, 32, 224, 224)),
                    tf.image.flip_left_right(tf.image.crop_to_bounding_box(image, 32, 32, 224, 224)) ]
      image = tf.stack([image_list[x] for x in range(10)], axis=0)
      label = tf.stack([label for x in range(10)], axis=0)

    #Make batches
    if objective == "test":  x, y_ = tf.train.batch([image, label], batch_size=10,        capacity=200,  num_threads=2, enqueue_many=True, allow_smaller_final_batch=True)
&lt;/denchmark-code&gt;

And then evaluate accuracy with:
&lt;denchmark-code&gt;  for i in range(int(math.ceil(nTest))):
    pred, trueLabel, top_five_ = sess.run([test_y, test_y_, top_five])
    pred = int(np.argmax(np.mean(pred, axis=0)))
    truth = int(np.argmax(trueLabel[0]))
    test_accuracy_ = (1. if pred == truth else 0.)
    accuracies.append(test_accuracy_)
accuracy = np.mean(accuracies)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

When there are 10 images being enqueued into a tf.train.batch() with batch_size = 10, the reported accuracy is artificially low (e.g., 5%). If you change the batch_size to a higher number (e.g., 100) [and adjust the accuracy calculation accordingly], the reported accuracy is much higher (e.g., 50%). Seems there's some bug about num_enqueued == batch_size.
	</description>
	<comments>
		<comment id='1' author='cag472' date='2018-10-09T20:06:38Z'>
		Its hard for me to say here without understanding your model / problem but if you change the batch size, it'll depend on your optimizer etc. and the learned weights etc. would change leading to different accuracy results. So this isn't that surprising perhaps.
		</comment>
		<comment id='2' author='cag472' date='2018-10-15T18:02:53Z'>
		No, I'm saying that batchsize = 10 with num_enqueue = 10 leads to artificially low error, but batchsize=11 works fine. I ran this past a few other ML experts in my office and we all agreed this is something weird on the TF side.
		</comment>
		<comment id='3' author='cag472' date='2018-10-21T20:27:35Z'>
		So we're moving away from the tf.train.batch type API's that use queues and more towards tf.data
&lt;denchmark-link:https://www.tensorflow.org/guide/datasets&gt;https://www.tensorflow.org/guide/datasets&lt;/denchmark-link&gt;
. Would it be possible for you to give that a try?
		</comment>
		<comment id='4' author='cag472' date='2018-11-05T18:58:43Z'>
		It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue?
		</comment>
		<comment id='5' author='cag472' date='2018-11-14T21:59:53Z'>
		
So we're moving away from the tf.train.batch type API's that use queues and more towards tf.data
https://www.tensorflow.org/guide/datasets. Would it be possible for you to give that a try?

@cag51  Any update on this ?
		</comment>
		<comment id='6' author='cag472' date='2018-11-20T18:32:01Z'>
		In "awaiting response" status for more than 3 days. Hence closing this. Please post the updates here if any, we will reopen the issue. Thanks !
		</comment>
	</comments>
</bug>