<bug id='13536' author='bdaskalov' open_date='2017-10-06T20:53:52Z' closed_time='2017-10-18T05:45:06Z'>
	<summary>BeamSearchDecoder incorrectly truncates results when used with dynamic_decode</summary>
	<description>
&lt;denchmark-h:h3&gt;System information (irrelevant for this bug)&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04/Any
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): v1.3.0-rc2-20-g0787eee 1.3.0
Python version: Python 3.5.2 :: Continuum Analytics, Inc.
Bazel version (if compiling from source): N/A
CUDA/cuDNN version: irrelevant
GPU model and memory: irrelevant
Exact command to reproduce: irrelevant

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

tf.contrib.seq2seq.BeamSearchDecoder incorrectly truncates some of the results because the same index was previously used for a beam member that ended at a earlier step.
The root of the problem is that the while_loop body in dynamic_decode assumes that sequences are independent and will finish only once. In the same time BeamSearchDecoder creates a tree-like structure where a beam index can be reused in a later step for a state that originates from a different parent index.  This causes the decoding loop to sometimes record the wrong sequence length for a beam member. Then this wrong sequence length is passed to BeamSearchDecoder.finalize which returns a truncated sequence.
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

I use the following code to workaround the problem. This causes the right sequence to be returned but still the length returned by dynamic_decode is wrong.
class FixedBeamSearchDecoder(seq2seq.BeamSearchDecoder):
    def finalize(self, outputs, final_state, sequence_lengths):
        # BeamSearchDecoder does not follow the correct semantics of the the finished flag
        # which results in taking wrong length here and getting wrong decoded string.
        # We substitute the sequence length recorded by dynamic_decoder (which is wrong because
        # of the wrong finished flag returned by BeamSearchDecoder.step) with the length
        # recorded in BeamSearchState which is correct.
        return super().finalize(outputs, final_state, final_state.lengths)
	</description>
	<comments>
		<comment id='1' author='bdaskalov' date='2017-10-06T20:55:47Z'>
		&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 Can you take a look? I see that you wrote the seq2seq library. I wanted to submit a fix but I don't see how to correct this problem without changing some of the library's public inteface.
		</comment>
		<comment id='2' author='bdaskalov' date='2017-10-06T21:47:37Z'>
		Seems ok to update the BeamSearchDecoder.finalize to use final_state.lengths instead of sequence_lengths -- looks like this fixes a couple of other open issues.
We could consider having finalize return new updated sequence lengths to decode_dynamic as well.
		</comment>
		<comment id='3' author='bdaskalov' date='2017-10-06T21:48:02Z'>
		Thanks for catching this!  Could you send a PR with the fix and a unit test that catches it?
		</comment>
		<comment id='4' author='bdaskalov' date='2017-10-13T17:14:14Z'>
		Will look into submitting a fix.
		</comment>
		<comment id='5' author='bdaskalov' date='2017-10-15T02:17:02Z'>
		Sorry, I've been meaning to make a PR last week but never got to it.
		</comment>
		<comment id='6' author='bdaskalov' date='2017-10-15T02:19:07Z'>
		No problem. We're evaluating your change internally.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Sat, Oct 14, 2017, 7:17 PM bdaskalov ***@***.***&gt; wrote:
 Sorry, I've been meaning to make a PR last week but never got to it.

 —
 You are receiving this because you were assigned.
 Reply to this email directly, view it on GitHub
 &lt;#13536 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABtimxRoy93pElky6ZtzF-ctLOu6Khocks5ssWs8gaJpZM4PxBUM&gt;
 .



		</comment>
		<comment id='7' author='bdaskalov' date='2017-10-16T19:26:44Z'>
		The problem is deeper and the solution requires some additional changes.  I'll try to submit something in the next couple days.
		</comment>
		<comment id='8' author='bdaskalov' date='2018-04-30T03:37:35Z'>
		Could anyone tell me when this bug was fixed. I couldn't find it in the release notes. Thank you! &lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='bdaskalov' date='2018-04-30T15:21:24Z'>
		It was first released in TensorFlow 1.5.
		</comment>
		<comment id='10' author='bdaskalov' date='2018-04-30T17:16:08Z'>
		&lt;denchmark-link:https://github.com/guillaumekln&gt;@guillaumekln&lt;/denchmark-link&gt;
 Thank you for the info!
		</comment>
	</comments>
</bug>