<bug id='30745' author='rishabhsahrawat' open_date='2019-07-16T09:14:23Z' closed_time='2019-07-17T06:28:49Z'>
	<summary>How to pad input sequences for implementing CuDNN LSTM in TF2.0?</summary>
	<description>
For implementing CuDNN LSTM, according to &lt;denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/LSTM&gt;docs&lt;/denchmark-link&gt;
 there are 6 requirements as follows-

activation == 'tanh'
recurrent_activation == 'sigmoid'
recurrent_dropout == 0
unroll is False
use_bias is True
Inputs are not masked or strictly right padded.

According to my understanding, the last one say to have input sequences not right padded. However, in text classification I want to pad sequences and I am using &lt;denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#padded_batch&gt;the function&lt;/denchmark-link&gt;
, now I do not know how to left pad using this function, as I could check it always right pad the input sequences, which means one can never use CuDNN LSTM for training on GPU(s) if they are padding the inputs. However, it can be done easily using  function, defined &lt;denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences&gt;here&lt;/denchmark-link&gt;
.
	</description>
	<comments>
		<comment id='1' author='rishabhsahrawat' date='2019-07-16T11:57:25Z'>
		I think you are misreading the requirement. Inputs should be either [unmasked] or [strictly right-padded]. For reference, see the following function used to determine right-paddedness for CuDNN compatibility: 


tensorflow/tensorflow/python/keras/layers/recurrent_v2.py


         Line 1182
      in
      8e423e3






 def is_sequence_right_padded(mask, time_major): 





Note: the LSTM with CuDNN is experiencing a lot of bug reports atm (e.g. in loops, in distributed strategies, etc.), all of which are open issues as of right now. However, concerning your question, I think you can close this issue.
		</comment>
		<comment id='2' author='rishabhsahrawat' date='2019-07-16T13:10:39Z'>
		&lt;denchmark-link:https://github.com/jkamalu&gt;@jkamalu&lt;/denchmark-link&gt;
 thank you for your answer and clarification. I still have a question about how to left pad in TF2.0, without using  function. Is it possible?
-Rishabh Sahrawat
		</comment>
		<comment id='3' author='rishabhsahrawat' date='2019-07-16T15:08:23Z'>
		Use tf.pad. Choose the constant mode and parameterize the padding on the left-hand side with the max desired length and current length of the sequence and use no padding on the right side. Play around with eager tensors to get this right if it doesn't seem obvious from the documentation.
&lt;denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/pad&gt;https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/pad&lt;/denchmark-link&gt;

If you want your input data padded, use tf.pad with the Dataset map function during data feeding.
&lt;denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#map&gt;https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#map&lt;/denchmark-link&gt;

However, in my experience, this adds unnecessary overhead and can bottleneck training... So, what you should do if you can is preprocess all sequences to be padded however you like, save them to disk, and then load them as such during Dataset creation/iteration.
		</comment>
		<comment id='4' author='rishabhsahrawat' date='2019-07-17T06:28:49Z'>
		&lt;denchmark-link:https://github.com/jkamalu&gt;@jkamalu&lt;/denchmark-link&gt;
 thank you for your answer. I appreciate your help.
		</comment>
	</comments>
</bug>