<bug id='33177' author='Apm5' open_date='2019-10-09T13:03:28Z' closed_time='2019-10-12T02:50:20Z'>
	<summary>Efficiency of model.fit_generator() are greatly reduced in 2.0.0</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): source
TensorFlow version (use command below):  2.0.0
Python version: 3.5
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 10.0, 7.6
GPU model and memory: Titan Xp, 12G

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
My code takes 21s/epoch in tf 2.0.0alpha while it takes 76s/epoch in tf 2.0.0. I found that the problem is model.fit_generator(). When I replace it with model.fit(), the efficiency of the code is exactly the same in both versions. But I need to use ImageDataGenerator.
Describe the expected behavior
The efficiency of the code should be same.
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
(train_images, train_labels, test_images, test_labels) = load_CIFAR('/home/user/Documents/dataset/Cifar-10')
datagen = ImageDataGenerator(horizontal_flip=True,
width_shift_range=0.125,
height_shift_range=0.125,
fill_mode='constant', cval=0.)
datagen.fit(train_images)
datagenflow = datagen.flow(train_images, train_labels, batch_size=batch_size)
model.fit_generator(datagenflow,
steps_per_epoch=iterations,
epochs=epoch_num,
callbacks=[change_lr],
validation_data=(test_images, test_labels))
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='Apm5' date='2019-10-10T04:21:19Z'>
		&lt;denchmark-link:https://github.com/Apm5&gt;@Apm5&lt;/denchmark-link&gt;
 ,
Thanks for reporting, Can you share a standalone code to reproduce the issue?
		</comment>
		<comment id='2' author='Apm5' date='2019-10-10T05:29:42Z'>
		&lt;denchmark-link:https://github.com/oanush&gt;@oanush&lt;/denchmark-link&gt;
 Thank you for your response. Here is the complete code, which has different efficiency between 2.0.0alpha and 2.0.0.
&lt;denchmark-code&gt;import tensorflow as tf
import os
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import models, optimizers, regularizers
from tensorflow.keras.callbacks import LearningRateScheduler
from tensorflow.keras.layers import Conv2D, AveragePooling2D, BatchNormalization, Flatten, Dense, Input, add, Activation
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

stack_n = 3  # layers = stack_n * 6 + 2
weight_decay = 1e-4
batch_size = 128
iterations = 50000 // batch_size + 1
learning_rate = 1e-1
epoch_num = 200

def Residual_block(inputs, channels, strides=(1, 1)):
    if strides == (1, 1):
        shortcut = inputs
    else:
        shortcut = Conv2D(channels, (1, 1), strides=strides, kernel_regularizer=regularizers.l2(weight_decay))(inputs)
        shortcut = BatchNormalization(momentum=0.9, epsilon=1e-5)(shortcut)
    net = Conv2D(channels, (3, 3), padding='same', strides=strides, kernel_regularizer=regularizers.l2(weight_decay))(inputs)
    net = BatchNormalization(momentum=0.9, epsilon=1e-5)(net)
    net = Activation('relu')(net)
    net = Conv2D(channels, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay))(net)
    net = BatchNormalization(momentum=0.9, epsilon=1e-5)(net)
    net = add([net, shortcut])
    net = Activation('relu')(net)
    return net

def ResNet(inputs):
    net = Conv2D(16, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay))(inputs)
    net = BatchNormalization(momentum=0.9, epsilon=1e-5)(net)
    net = Activation('relu')(net)

    for i in range(stack_n):
        net = Residual_block(net, 16)

    net = Residual_block(net, 32, strides=(2, 2))
    for i in range(stack_n - 1):
        net = Residual_block(net, 32)

    net = Residual_block(net, 64, strides=(2, 2))
    for i in range(stack_n - 1):
        net = Residual_block(net, 64)

    net = AveragePooling2D(8, 8)(net)
    net = Flatten()(net)
    net = Dense(10, activation='softmax')(net)
    return net

def scheduler(epoch):
    if epoch &lt; epoch_num * 0.4:
        return learning_rate
    if epoch &lt; epoch_num * 0.8:
        return learning_rate * 0.1
    return learning_rate * 0.01

if __name__ == '__main__':
    # load data
    (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()
    train_labels = tf.keras.utils.to_categorical(train_labels, 10)
    test_labels = tf.keras.utils.to_categorical(test_labels, 10)

    datagen = ImageDataGenerator(horizontal_flip=True,
                                 width_shift_range=0.125,
                                 height_shift_range=0.125,
                                 fill_mode='constant', cval=0.0)
    datagen.fit(train_images)

    # get model
    img_input = Input(shape=(32, 32, 3))
    output = ResNet(img_input)
    model = models.Model(img_input, output)

    # show
    model.summary()

    # train
    sgd = optimizers.SGD(lr=learning_rate, momentum=0.9, nesterov=True)
    change_lr = LearningRateScheduler(scheduler)
    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
    datagenflow = datagen.flow(train_images, train_labels, batch_size=batch_size)
    model.fit_generator(datagenflow,
                        steps_per_epoch=iterations,
                        epochs=epoch_num,
                        callbacks=[change_lr],
                        validation_data=(test_images, test_labels))
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='Apm5' date='2019-10-12T02:50:20Z'>
		This is the same issue as &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/33024&gt;#33024&lt;/denchmark-link&gt;
. I am working to alias  to , since  now supports generators. In the mean time I would suggest that you use  rather than  to unblock yourself.
		</comment>
		<comment id='4' author='Apm5' date='2019-10-12T02:50:22Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33177&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33177&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='Apm5' date='2019-10-12T03:03:06Z'>
		This is the same issue as &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/33024&gt;#33024&lt;/denchmark-link&gt;
. I am working to alias  to , since  now supports generators. In the mean time I would suggest that you use  rather than  to unblock yourself.
		</comment>
	</comments>
</bug>