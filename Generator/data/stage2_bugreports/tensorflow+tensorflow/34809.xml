<bug id='34809' author='rabitt' open_date='2019-12-03T19:25:12Z' closed_time='2019-12-05T18:35:46Z'>
	<summary>Typo in tf.keras.layers.Attention docs example</summary>
	<description>
&lt;denchmark-h:h2&gt;URL(s) with the issue:&lt;/denchmark-h&gt;

&lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention&gt;https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;Description of issue (what needs changing):&lt;/denchmark-h&gt;

In the code example, it currently reads:
# Variable-length int sequences.
query_input = tf.keras.Input(shape=(None,), dtype='int32')
value_input = tf.keras.Input(shape=(None,), dtype='int32')

# Embedding lookup.
token_embedding = tf.keras.layers.Embedding(max_tokens, dimension)
# Query embeddings of shape [batch_size, Tq, dimension].
query_embeddings = token_embedding(query_input)
# Value embeddings of shape [batch_size, Tv, dimension].
value_embeddings = token_embedding(query_input)
The last line should instead be:
&lt;denchmark-code&gt;value_embeddings = token_embedding(value_input)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='rabitt' date='2019-12-04T18:35:46Z'>
		Fixed by &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/4b2f4f4d25a75593bdeaf993b8cdcfc7055a7a55&gt;4b2f4f4&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='2' author='rabitt' date='2019-12-05T00:54:14Z'>
		&lt;denchmark-link:https://github.com/lamberta&gt;@lamberta&lt;/denchmark-link&gt;
 I believe this can be closed now!
		</comment>
		<comment id='3' author='rabitt' date='2019-12-05T18:35:45Z'>
		Thank you all!
		</comment>
	</comments>
</bug>