<bug id='11471' author='Randl' open_date='2017-07-13T08:23:38Z' closed_time='2017-12-20T19:46:21Z'>
	<summary>XLA crash on Wasserstein GAN</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
TensorFlow installed from (source or binary): TensorFlow installed from source
TensorFlow version (use command below): v1.2.0-1382-g708cbaf 1.2.0
Python version:  Python 3.5.2
Bazel version (if compiling from source):
CUDA/cuDNN version: CUDA 8 / cudnn 6
GPU model and memory: 2x1080 Ti, 12 Gb

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

When I'm running this code: &lt;denchmark-link:https://github.com/Randl/WassersteinGAN.tensorflow&gt;https://github.com/Randl/WassersteinGAN.tensorflow&lt;/denchmark-link&gt;
 with XLA, I get the following error message:
&lt;denchmark-code&gt;LLVM ERROR: Cannot select: 0x7f4c300d4878: i16,ch = AtomicCmpSwap&lt;Volatile LDST1[%_fusion.typed16(addrspace=1)]&gt; 0x7f4c30093858, 0x7f4c300d5098, 0x7f4c300d5168, 0x7f4c300e6c20
  0x7f4c300d5098: i64,ch = CopyFromReg 0x7f4c30093858, Register:i64 %vreg0
    0x7f4c300d5100: i64 = Register %vreg0
  0x7f4c300d5168: i16,ch = CopyFromReg 0x7f4c30093858, Register:i16 %vreg3
    0x7f4c300d56b0: i16 = Register %vreg3
  0x7f4c300e6c20: i16 = and 0x7f4c300d5168, 0x7f4c300d5370
    0x7f4c300d5168: i16,ch = CopyFromReg 0x7f4c30093858, Register:i16 %vreg3
      0x7f4c300d56b0: i16 = Register %vreg3
    0x7f4c300d5370: i16 = AssertZext 0x7f4c300d4c20, ValueType:ch:i1
      0x7f4c300d4c20: i16,ch = CopyFromReg 0x7f4c30093858, Register:i16 %vreg1
        0x7f4c300d4948: i16 = Register %vreg1
In function: _fusion__1
*** Error in `python3': free(): invalid size: 0x00007f4ac8091fe0 ***
======= Backtrace: =========
/lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7f4e6c2507e5]
/lib/x86_64-linux-gnu/libc.so.6(+0x8037a)[0x7f4e6c25937a]
/lib/x86_64-linux-gnu/libc.so.6(cfree+0x4c)[0x7f4e6c25d53c]
/usr/lib/nvidia-375/libnvidia-ptxjitcompiler.so.375.66(+0x6b98aa)[0x7f4a9f3f48aa]
/usr/lib/nvidia-375/libnvidia-ptxjitcompiler.so.375.66(+0xdb80e)[0x7f4a9ee1680e]
/usr/lib/nvidia-375/libnvidia-ptxjitcompiler.so.375.66(+0xc034b)[0x7f4a9edfb34b]
/usr/lib/nvidia-375/libnvidia-ptxjitcompiler.so.375.66(__cuda_CallJitEntryPoint+0xdcc)[0x7f4a9edf1aec]
/usr/lib/nvidia-375/libnvidia-fatbinaryloader.so.375.66(fatBinaryCtl_Compile+0x302)[0x7f4df0b125c2]
/usr/lib/x86_64-linux-gnu/libcuda.so.1(+0x1a1ee2)[0x7f4e07c31ee2]
/usr/lib/x86_64-linux-gnu/libcuda.so.1(+0x1a2a63)[0x7f4e07c32a63]
/usr/lib/x86_64-linux-gnu/libcuda.so.1(+0x1a3133)[0x7f4e07c33133]
/usr/lib/x86_64-linux-gnu/libcuda.so.1(+0xcf7b3)[0x7f4e07b5f7b3]
/usr/lib/x86_64-linux-gnu/libcuda.so.1(cuModuleLoadDataEx+0x75)[0x7f4e07c77055]
/usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so(+0x4d0b7ba)[0x7f4e1370d7ba]
/usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN5Eigen26NonBlockingThreadPoolTemplIN10tensorflow6thread16EigenEnvironmentEE10WorkerLoopEi+0xff)[0x7f4e139e998f]
/usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so(_ZNSt17_Function_handlerIFvvEZN10tensorflow6thread16EigenEnvironment12CreateThreadESt8functionIS0_EEUlvE_E9_M_invokeERKSt9_Any_data+0x2d)[0x7f4e139e977d]
&lt;/denchmark-code&gt;

Without XLA it works OK.
	</description>
	<comments>
		<comment id='1' author='Randl' date='2017-07-13T08:26:16Z'>
		I assumed the problem might be with memory, but for smaller batches I get the same error:
&lt;denchmark-code&gt;LLVM ERROR: Cannot select: 0x7fe90c0d4608: i16,ch = AtomicCmpSwap&lt;Volatile LDST1[%_fusion.typed16(addrspace=1)]&gt; 0x7fe90c07f388, 0x7fe90c0d4e28, 0x7fe90c0d4ef8, 0x7fe90c0e6fe0
  0x7fe90c0d4e28: i64,ch = CopyFromReg 0x7fe90c07f388, Register:i64 %vreg0
    0x7fe90c0d4e90: i64 = Register %vreg0
  0x7fe90c0d4ef8: i16,ch = CopyFromReg 0x7fe90c07f388, Register:i16 %vreg3
    0x7fe90c0d5440: i16 = Register %vreg3
  0x7fe90c0e6fe0: i16 = and 0x7fe90c0d4ef8, 0x7fe90c0d5100
    0x7fe90c0d4ef8: i16,ch = CopyFromReg 0x7fe90c07f388, Register:i16 %vreg3
      0x7fe90c0d5440: i16 = Register %vreg3
    0x7fe90c0d5100: i16 = AssertZext 0x7fe90c0d49b0, ValueType:ch:i1
      0x7fe90c0d49b0: i16,ch = CopyFromReg 0x7fe90c07f388, Register:i16 %vreg1
        0x7fe90c0d46d8: i16 = Register %vreg1
In function: _fusion__1
&lt;/denchmark-code&gt;

Second part (with free()) desn't appear each time, I don't know what it depends on
		</comment>
		<comment id='2' author='Randl' date='2017-07-13T20:15:24Z'>
		&lt;denchmark-link:https://github.com/hawkinsp&gt;@hawkinsp&lt;/denchmark-link&gt;
 XLA crash reported with "free(): invalid size".
		</comment>
		<comment id='3' author='Randl' date='2017-07-28T01:41:51Z'>
		We are seeing similar crash with ArrayFire, which is doing just like XLA (JIT).
nvidia driver 375.51 seems more stable and we dont see crash with it...
TBC
		</comment>
		<comment id='4' author='Randl' date='2017-07-28T19:49:22Z'>
		The crash in
cuModuleLoadDataEx&gt;fatBinaryCtl_Compile&gt;__cuda_CallJitEntryPoint
also disappears with nvidia drivers 384.59.
There seems to be a background bug in nvidia driver sources that is randomly effective according to the version of the drivers. The bugtracker of nvidia is not public so hard to say if they are aware of that bug.
		</comment>
		<comment id='5' author='Randl' date='2017-09-18T07:25:35Z'>
		I got the similar (or the same) error when turning on XLA jit for the WDL example: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/examples/learn/wide_n_deep_tutorial.py&gt;https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/examples/learn/wide_n_deep_tutorial.py&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;LLVM ERROR: Cannot select: t24: i16,ch = AtomicCmpSwap&lt;Volatile LDST1[%fusion.1.typed27(addrspace=1)]&gt; t0, t11, t2, t21

  t11: i64,ch = CopyFromReg t0, Register:i64 %vreg1
    t10: i64 = Register %vreg1
  t2: i16,ch = CopyFromReg t0, Register:i16 %vreg18
    t1: i16 = Register %vreg18
  t21: i16 = and t2, t7
    t2: i16,ch = CopyFromReg t0, Register:i16 %vreg18
      t1: i16 = Register %vreg18
    t7: i16 = AssertZext t5, ValueType:ch:i1
      t5: i16,ch = CopyFromReg t0, Register:i16 %vreg14
        t4: i16 = Register %vreg14
In function: _fusion_1__1
&lt;/denchmark-code&gt;

I tried to use a newer stable version of LLVM in: workspace.bzl:
&lt;denchmark-code&gt;temp_workaround_http_archive(
      name = "llvm",
      urls = [
          "https://github.com/llvm-mirror/llvm/archive/stable.tar.gz",
      ],
      sha256 = "5491bd0608ca59b94d5fe0a9892494fac263c9e1bd6ab2d4f4dc0e495e29670e", 
      strip_prefix = "llvm-stable",
      build_file = str(Label("//third_party/llvm:llvm.BUILD")),
      repository = tf_repo_name,
  )
&lt;/denchmark-code&gt;

But the problem still exist, I am wondering if there is something wrong in the LLVM IR emitter of the XLA code?
		</comment>
		<comment id='6' author='Randl' date='2017-12-20T19:09:19Z'>
		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
		</comment>
		<comment id='7' author='Randl' date='2017-12-20T19:12:37Z'>
		Sorry: I didn't notice this was assigned to me and I'm not working on this code at the moment.
Justin: I think this may have been fixed recently?
		</comment>
		<comment id='8' author='Randl' date='2017-12-20T19:46:21Z'>
		Yes, this should be fixed by &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/6605938c280590ee981470abe87386396cf0e438&gt;6605938&lt;/denchmark-link&gt;
.
I'm pretty confident this will fix the problem, so closing this issue.  But please reopen if you all see this problem after this commit.
		</comment>
	</comments>
</bug>