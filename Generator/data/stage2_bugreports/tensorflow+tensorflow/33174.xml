<bug id='33174' author='don-tpanic' open_date='2019-10-09T12:15:09Z' closed_time='2019-10-12T02:54:09Z'>
	<summary>TF2.0 OOM error training imagenet with vgg, fine when eager execution off</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 2.0
Python version: 3.7
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source): 7.4.0
CUDA/cuDNN version: V10.0.130
GPU model and memory: GeForce RTX 2080ti, 10G

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
Getting OOM error when training vgg16 on imagenet with batch_size=256. Was able to get away with OOM error with batch_size=8 or turning off eager execution.
Describe the expected behavior
The exact same code was running fine with tf1.14 with no memory problem
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
&lt;denchmark-code&gt;import numpy as np
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam


imagenet_test = 'some image directory'

model = VGG16(weights='imagenet', include_top=True, input_shape=(224, 224, 3))
model.compile('adam', loss='sparse_categorical_crossentropy', metrics=['acc'])

datagen = ImageDataGenerator(preprocessing_function=preprocess_input)
test_generator = datagen.flow_from_directory(directory=imagenet_test,
                                            batch_size=256,
                                            class_mode='sparse',
                                            target_size=(224, 224))
steps = np.ceil(len(test_generator.classes) / 256)

model.evaluate_generator(test_generator, steps, verbose=1)
&lt;/denchmark-code&gt;

Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
&lt;denchmark-code&gt;tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[256,64,224,224] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Conv2D]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='don-tpanic' date='2019-10-10T08:23:37Z'>
		I could reproduce the issue with tensorflow 2.0.0. Please see the colab &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/e1417d6841204fb99b7056c8398b551a/untitled189.ipynb&gt;gist&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='don-tpanic' date='2019-10-11T22:38:35Z'>
		&lt;denchmark-link:https://github.com/don-tpanic&gt;@don-tpanic&lt;/denchmark-link&gt;
 Please check &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/32707#issuecomment-539246480&gt;the response&lt;/denchmark-link&gt;
 from &lt;denchmark-link:https://github.com/robieta&gt;@robieta&lt;/denchmark-link&gt;
 for similar OOM issue with 2.0 and no OOM when same code ran in graph mode. The response has a nice presentation on improving the code for better performance. Thanks!
		</comment>
		<comment id='3' author='don-tpanic' date='2019-10-12T02:54:08Z'>
		To add to &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
's answer on why eager matters, see &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/33024&gt;#33024&lt;/denchmark-link&gt;
 which diagnosed that the  methods are running eagerly in 2.0. I am currently working on aliasing them to their normal counterparts (Model.fit_generator -&gt; Model.fit, etc) which will restore graph based optimizations. (Including those around memory use.) In the mean time, you can switch to  and  directly in your code. Sorry for the inconvenience.
		</comment>
		<comment id='4' author='don-tpanic' date='2019-10-12T02:54:10Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33174&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33174&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>