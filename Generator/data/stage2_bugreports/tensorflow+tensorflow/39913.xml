<bug id='39913' author='tarrade' open_date='2020-05-27T16:43:25Z' closed_time='2020-05-28T08:02:51Z'>
	<summary>[TF 2.2.0/TPU]: tf.data.Dataset segmentation fault with "the Encode() method is not implemented for DatasetVariantWrapper objects" after calling TPUCusterResolver()</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.6
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below):
Python version: v2.2.0-0-g2b96f3662b 2.2.0
Bazel version (if compiling from source): NA
GCC/Compiler version (if compiling from source): NA
CUDA/cuDNN version: NA
GPU model and memory: NA
TPU on Colab

Describe the current behavior
Code is crasing wgen using TPU (working fine with CPU and GPU). Exact same crashes on GCP AI Training and Colab (using TPU). After I use in my code: tf.distribute.cluster_resolver.TPUClusterResolver() any action of my tf.data.Dataset (from TFRecord file) like valid_dataset.take(5) will cash with the following error message:
&lt;denchmark-code&gt;INFO:tensorflow:Finished initializing TPU system.
INFO:tensorflow:Found TPU system:
INFO:tensorflow:*** Num TPU Cores: 8
INFO:tensorflow:*** Num TPU Workers: 1
INFO:tensorflow:*** Num TPU Cores Per Worker: 8
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
 here start the issues!
2020-05-27 16:27:47.913455: I tensorflow/core/common_runtime/eager/execute.cc:966] Executing op TakeDataset on task /job:worker/replica:0/task:0/device:CPU:0
2020-05-27 16:27:47.913542: E tensorflow/core/framework/dataset.cc:88] The Encode() method is not implemented for DatasetVariantWrapper objects.
Fatal Python error: Segmentation fault

Current thread 0x00007f67f1a67780 (most recent call first):
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py", line 6007 in take_dataset
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py", line 3640 in __init__
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py", line 1267 in take
  File "test.py", line 70 in main
  File "/usr/local/lib/python3.6/dist-packages/absl/app.py", line 250 in _run_main
  File "/usr/local/lib/python3.6/dist-packages/absl/app.py", line 299 in run
  File "test.py", line 78 in &lt;module&gt;
&lt;/denchmark-code&gt;

Before using TPUCusterResolver() I can access my data without any issue like valid_dataset.take(5)
Some information:

On GCP it is always crashing
On Colab, if I copy the full code in one cell, it working most of the time
On Colab, running the !python test.py from a cell is always cashing

Describe the expected behavior
Should run without crashing as before calling TPUCusterResolver() and the output should like that:
&lt;denchmark-code&gt;INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
 here start the issues!
Executing op TakeDataset on task /job:worker/replica:0/task:0/device:CPU:0
 take(5) not ok 1 &lt;TakeDataset shapes: ({input_ids: (None, None), attention_mask: (None, None), token_type_ids: (None, None)}, (None,)), types: ({input_ids: tf.int32, attention_mask: tf.int32, token_type_ids: tf.int32}, tf.int64)&gt;
Executing op TakeDataset on task /job:worker/replica:0/task:0/device:CPU:0
 take(5) not ok 2 &lt;TakeDataset shapes: ({input_ids: (None, None), attention_mask: (None, None), token_type_ids: (None, None)}, (None,)), types: ({input_ids: tf.int32, attention_mask: tf.int32, token_type_ids: tf.int32}, tf.int64)&gt;
&lt;/denchmark-code&gt;

Standalone code to reproduce the issue

Go on Colab
Choose select TPU in the runtime
check that TF 2.2.0 is installed (is the default right now)
normally no other packages are needed and the data are on a public GCP bucket
In Colab create a python file called test.py and copy the full code below
In a Colab cell run !python test.py --&gt; this will seg fault
In another Colab cell, copy the code below. It should run without any error.

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'
os.environ['TF_CPP_MIN_VLOG_LEVEL'] = '0'
import tensorflow as tf
tf.get_logger().propagate = False
tf.debugging.set_log_device_placement(True)
tf.autograph.set_verbosity(5, alsologtostdout=False)
import model.tf_bert_classification.model as tf_bert
from absl import flags
from absl import app
import sys

use_tpu=True
print("Tensorflow version", tf.__version__)

#FLAGS = flags.FLAGS
#flags.DEFINE_string('f', '', 'kernel') # just for jupyter notebook and avoid : "UnrecognizedFlagError: Unknown command line flag 'f'"

def main(argv):
#def main():
  def parse_tfrecord_glue_files(record):
      print(" ---&gt; parse_tfrecord_glue_files")
      features_spec = {
          'input_ids': tf.io.FixedLenFeature([], tf.string, default_value=''),
          'attention_mask': tf.io.FixedLenFeature([], tf.string, default_value=''),
          'token_type_ids': tf.io.FixedLenFeature([], tf.string, default_value=''),
          'label': tf.io.FixedLenFeature([], tf.int64, default_value=0)
      }
      example = tf.io.parse_single_example(record, features_spec)
      f0 = tf.ensure_shape(tf.io.parse_tensor(example['input_ids'], out_type=tf.int32), (None,))
      f1 = tf.ensure_shape(tf.io.parse_tensor(example['attention_mask'], out_type=tf.int32), (None,))
      f2 = tf.ensure_shape(tf.io.parse_tensor(example['token_type_ids'], out_type=tf.int32), (None,))
      return {'input_ids': f0, 'attention_mask': f1, 'token_type_ids': f2}, example['label']

  def build_dataset(input_tfrecords, batch_size, shuffle_buffer=2048):
      print(" ---&gt; build_dataset")
      file_pattern = input_tfrecords+'/*.tfrecord'
      dataset = tf.data.Dataset.list_files(file_pattern,
                                          shuffle=True,
                                          seed=None
                                          )
      dataset = dataset.interleave(tf.data.TFRecordDataset,
                                  cycle_length=tf.data.experimental.AUTOTUNE,
                                  num_parallel_calls=tf.data.experimental.AUTOTUNE,
                                  deterministic=False)
      dataset = dataset.shuffle(shuffle_buffer)
      dataset = dataset.map(parse_tfrecord_glue_files, num_parallel_calls=tf.data.experimental.AUTOTUNE)
      dataset = dataset.batch(batch_size)
      dataset = dataset.cache()
      dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)
      return dataset

  valid_dataset = build_dataset('gs://public-test-data-gs/valid', 64, 2048)
  valid_dataset = valid_dataset.repeat(2)

  print(" take(5)ok", valid_dataset.take(5))

  if use_tpu:
    print('setting up TPU: cluster resolver')
    tpu_cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver()
    print('setting up TPU: \n {}'.format(tpu_cluster_resolver))
    print('running on TPU: \n {}'.format(tpu_cluster_resolver.cluster_spec().as_dict()['worker'])) 
    tf.config.experimental_connect_to_cluster(tpu_cluster_resolver)
    tf.tpu.experimental.initialize_tpu_system(tpu_cluster_resolver)
    strategy = tf.distribute.experimental.TPUStrategy(tpu_cluster_resolver)
  else:
     strategy = tf.distribute.MirroredStrategy()
  
  print(" here start the issues!")
  print(" take(5) not ok 1", valid_dataset.take(5))

  with strategy.scope():
    print(" take(5) not ok 2", valid_dataset.take(5))

#main()
#main(sys.argv)
if __name__ == '__main__':
  app.run(main)
Other info / logs Include any logs or source code that would be helpful to
	</description>
	<comments>
		<comment id='1' author='tarrade' date='2020-05-27T22:50:09Z'>
		As per &lt;denchmark-link:https://www.tensorflow.org/guide/tpu#tpu_initialization&gt;TPU documentation&lt;/denchmark-link&gt;
, the TPU initialization needs to happen before any TensorFlow operations are executed.
So you will need to do the following:
&lt;denchmark-code&gt;...
  if use_tpu:
    tpu_cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver()
    tf.config.experimental_connect_to_cluster(tpu_cluster_resolver)
    tf.tpu.experimental.initialize_tpu_system(tpu_cluster_resolver)

  valid_dataset = build_dataset('gs://public-test-data-gs/valid', 64, 2048)
  valid_dataset = valid_dataset.repeat(2)

  if use_tpu:
    strategy = tf.distribute.experimental.TPUStrategy(tpu_cluster_resolver)
  else:
     strategy = tf.distribute.MirroredStrategy()
...
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='tarrade' date='2020-05-28T08:02:51Z'>
		&lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
 you are right, this solve this issue. I missed this in the documentation. It was clear that the model need to be created within the strategy scope but not what you mentioned. Thanks a lot. Closing
		</comment>
		<comment id='3' author='tarrade' date='2020-05-28T08:02:53Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39913&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39913&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='tarrade' date='2020-06-02T15:00:01Z'>
		I also missed it. :) Maybe it could be more explicit in the docs that the TPU cluster must be initialized before the dataset is created?
		</comment>
		<comment id='5' author='tarrade' date='2020-06-02T16:07:54Z'>
		&lt;denchmark-link:https://github.com/rxsang&gt;@rxsang&lt;/denchmark-link&gt;
 FYI
		</comment>
	</comments>
</bug>