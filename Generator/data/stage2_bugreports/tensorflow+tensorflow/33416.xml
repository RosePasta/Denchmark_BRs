<bug id='33416' author='vcarpani' open_date='2019-10-16T10:31:56Z' closed_time='2019-11-25T22:57:44Z'>
	<summary>TensorFlow Lite: TensorListFromTensor, TensorListReserve, TensorListStack, While</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
TensorFlow installed from (source or binary): Binaries from pacman
TensorFlow version (or github SHA if from source): 2.0.0

Provide the text output from tflite_convert
&lt;denchmark-code&gt;Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, EXPAND_DIMS, FILL, LOGISTIC, MAXIMUM, MAX_POOL_2D, MINIMUM, MUL, PACK, PAD, RELU, RESHAPE, RESIZE_NEAREST_NEIGHBOR, SHAPE, STRIDED_SLICE, SUB, TILE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.

&lt;/denchmark-code&gt;

The model is very similar to &lt;denchmark-link:https://github.com/fizyr/keras-retinanet&gt;https://github.com/fizyr/keras-retinanet&lt;/denchmark-link&gt;
.
I am converting with target specs , so my problem could be solved by adding TensorListFromTensor, TensorListReserve, TensorListStack, While to the whitelist &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/flex/whitelisted_flex_ops.cc&gt;here&lt;/denchmark-link&gt;
 and use the standard tensorflow implementation.
	</description>
	<comments>
		<comment id='1' author='vcarpani' date='2019-10-21T09:25:34Z'>
		Is it OK to just add TensorListFromTensor, TensorListReserve, TensorListStack, While to the whitelist .  Should I build it from source to get a binary version?  Just add these operators to whitelist and I don't do anything else?  These operators will use standard tensorflow implementation automaticallyï¼Ÿ
		</comment>
		<comment id='2' author='vcarpani' date='2019-11-24T04:55:12Z'>
		Hi,
Sorry for late reply. Can you try out our new TF Lite converter by setting:
converter.experimental_new_converter = True
Please use the tf-nightly pip package so that you can get this working.
Thanks.
		</comment>
		<comment id='3' author='vcarpani' date='2019-11-25T02:43:32Z'>
		
Hi,
Sorry for late reply. Can you try out our new TF Lite converter by setting:
converter.experimental_new_converter = True
Please use the tf-nightly pip package so that you can get this working.
Thanks.

Thanks for you reply.  I have solved it. please keep working on it. It is really a nice framework. By the way, tf1 is very diffcult to use, but tf2.0 is much bettter.
		</comment>
		<comment id='4' author='vcarpani' date='2019-11-25T22:57:45Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33416&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33416&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='vcarpani' date='2020-01-15T02:10:33Z'>
		
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
TensorFlow installed from (source or binary): Binaries from pacman
TensorFlow version (or github SHA if from source): 2.0.0

Provide the text output from tflite_convert
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, EXPAND_DIMS, FILL, LOGISTIC, MAXIMUM, MAX_POOL_2D, MINIMUM, MUL, PACK, PAD, RELU, RESHAPE, RESIZE_NEAREST_NEIGHBOR, SHAPE, STRIDED_SLICE, SUB, TILE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.

The model is very similar to https://github.com/fizyr/keras-retinanet.
I am converting with target specs [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS], so my problem could be solved by adding TensorListFromTensor, TensorListReserve, TensorListStack, While to the whitelist here and use the standard tensorflow implementation.
@vcarpani
I am trying to convert EfficientDet in https://github.com/xuannianz/EfficientDet, which is based on fizyr/keras-retinanet in https://github.com/fizyr/keras-retinanet.
My tensorflow is 1.15.0.
I have converted the model to tflite.When call interpreter.allocate_tensors(), I still get the same error as you said.
My code as follows.
custom_objects = {
'BatchNormalization': layers_new.BatchNormalization,
'swish' : efficientnet.get_swish(backend=keras.backend,layers=keras.layers,models=keras.models,utils=keras.utils),
'FixedDropout' : efficientnet.get_dropout(backend=keras.backend,layers=keras.layers,models=keras.models,utils=keras.utils),
'wBiFPNAdd' : layers_new.wBiFPNAdd,
'PriorProbability' : initializers.PriorProbability,
'RegressBoxes' : layers_new.RegressBoxes,
'FilterDetections' : layers_new.FilterDetections,
'ClipBoxes' : layers_new.ClipBoxes,
'_smooth_l1' : losses.smooth_l1(),
'_focal' : losses.focal(),
}
input_shapes = {'input_2':[None,768,768,3],'input_5':[None,184140,4]}
tf.enable_control_flow_v2()
converter = tf.lite.TFLiteConverter.from_keras_model_file('E:/object_detection/EfficientDet-region_anchor_opt_mbconv-head-ckpts/inference_ckpts/ckpts_B0_image-size-768/mbconv-se-head_1e-5_unfreeze-backbone_freeze-bn/csv_04_0.6736_0.7484.h5',
custom_objects=custom_objects,
input_shapes=input_shapes,
)
converter.experimental_new_converter = True
converter.allow_custom_ops=True
tflite_model = converter.convert()
open("E:/object_detection/EfficientDet-region_anchor_opt_mbconv-head-ckpts/tflites/ckpts_B0_image-size-768/mbconv-se-head_1e-5_unfreeze-backbone_freeze-bn/csv_04_0.6736_0.7484.tflite", "wb").write(tflite_model)
interpreter = tf.lite.Interpreter(model_path="E:/object_detection/EfficientDet-region_anchor_opt_mbconv-head-ckpts/tflites/ckpts_B0_image-size-768/mbconv-se-head_1e-5_unfreeze-backbone_freeze-bn/csv_04_0.6736_0.7484.tflite")
interpreter.allocate_tensors()
And the error is about TensorListFromTensor as follows.
RuntimeError                              Traceback (most recent call last)
 in ()
1 # interpreter = tf.lite.Interpreter(model_content=tflite_model)
----&gt; 2 interpreter.allocate_tensors()
3 # help(tf.lite.Interpreter)

~\AppData\Roaming\Python\Python36\site-packages\tensorflow_core\lite\python\interpreter.py in allocate_tensors(self)
242   def allocate_tensors(self):
243     self._ensure_safe()
--&gt; 244     return self._interpreter.AllocateTensors()
245
246   def _safe_to_run(self):
~\AppData\Roaming\Python\Python36\site-packages\tensorflow_core\lite\python\interpreter_wrapper\tensorflow_wrap_interpreter_wrapper.py in AllocateTensors(self)
104
105     def AllocateTensors(self):
--&gt; 106         return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)
107
108     def Invoke(self):
RuntimeError: Encountered unresolved custom op: TensorListFromTensor.Node number 710 (TensorListFromTensor) failed to prepare.
`
I am new to tflite.Could you share your custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While. Thanks a lot!
		</comment>
		<comment id='6' author='vcarpani' date='2020-01-17T14:44:25Z'>
		&lt;denchmark-link:https://github.com/hzk7287&gt;@hzk7287&lt;/denchmark-link&gt;
 Can you please help that How you have resolved it ?
		</comment>
		<comment id='7' author='vcarpani' date='2020-01-19T01:46:28Z'>
		
@hzk7287 Can you please help that How you have resolved it ?

take a look at &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/33490&gt;#33490&lt;/denchmark-link&gt;

I use tensorflow2.0
		</comment>
		<comment id='8' author='vcarpani' date='2020-01-21T10:11:07Z'>
		As a summary, here is a minimal example for tensorflow 2.0 with an RNN that produced the same error, including one line from the solution mentioned by &lt;denchmark-link:https://github.com/hzk7287&gt;@hzk7287&lt;/denchmark-link&gt;
 in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/33490&gt;#33490&lt;/denchmark-link&gt;
:
adding  was enough to solve the problem in my case. However, for using the converted model with tensorflow lite, one would need to implemented the custom operations, correct? &lt;denchmark-link:https://github.com/hzk7287&gt;@hzk7287&lt;/denchmark-link&gt;
 would you share yours?
import tensorflow as tf

model = tf.keras.Sequential()

model.add(tf.keras.layers.Input(shape=(1, 1,)))

cell = tf.keras.layers.GRUCell(10)

model.add(tf.keras.layers.RNN(cell))

converter = tf.lite.TFLiteConverter.from_keras_model(model)

converter.allow_custom_ops = True # does the trick
#converter.experimental_new_converter = True # does not help

tflite_model = converter.convert()
		</comment>
		<comment id='9' author='vcarpani' date='2020-02-12T08:06:18Z'>
		According to the &lt;denchmark-link:https://www.tensorflow.org/lite/convert/rnn&gt;RNN Lite conversion documentation&lt;/denchmark-link&gt;
, only  without  is supported by Tensorflow Lite. According to the &lt;denchmark-link:https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/nn/static_rnn&gt;static_rnn documentation&lt;/denchmark-link&gt;
,  is the equivalent in Tensorflow 2.
So I added unroll=True and it works. However, unfortunately, the model does not work for Tensorflow Lite Micro.
import tensorflow as tf

model = tf.keras.Sequential()

model.add(tf.keras.layers.Input(shape=(1, 1,)))

cell = tf.keras.layers.GRUCell(10)

model.add(tf.keras.layers.RNN(cell, unroll=True))

converter = tf.lite.TFLiteConverter.from_keras_model(model)

tflite_model = converter.convert()

# for testing if operations are implemented by Tensorflow Lite
interpreter = tf.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors()
interpreter.invoke()
		</comment>
		<comment id='10' author='vcarpani' date='2020-08-20T11:58:20Z'>
		I had a similar error as above using tensorflow 2.2:
&lt;denchmark-code&gt;error: failed to legalize operation 'tf.TensorListReserve'
&lt;/denchmark-code&gt;

and
&lt;denchmark-code&gt;error: requires element_dtype to be 1-bit/8-bit/16-bit/32-bit/64-bit integer or 16-bit/32-bit/64-bit float type during TF Lite transformation pass
&lt;/denchmark-code&gt;

I fixed it by using:
converter = TFLiteConverter.from_saved_model(model_dir)
converter.experimental_new_converter = False
converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
converter.allow_custom_ops=True
tflite_model = converter.convert()
Compared to the fix by &lt;denchmark-link:https://github.com/hzk7287&gt;@hzk7287&lt;/denchmark-link&gt;
 in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/33490#issuecomment-544847214&gt;#33490 (comment)&lt;/denchmark-link&gt;
, this also disables the new experimental converter.
		</comment>
	</comments>
</bug>