<bug id='7679' author='AlvinChen13' open_date='2017-02-20T01:32:20Z' closed_time='2017-05-03T22:40:10Z'>
	<summary>How to reproduce the 58x performance improving claimed by TF r1.0?</summary>
	<description>
TF r1.0 claims that it can achieve 58x performance improving by 64 GPUs for Inception v3. Are there any guidelines or sample code to help us to reproduce the results?
	</description>
	<comments>
		<comment id='1' author='AlvinChen13' date='2017-02-20T06:22:03Z'>
		I believe &lt;denchmark-link:https://github.com/tfboyd&gt;@tfboyd&lt;/denchmark-link&gt;
 is working on reproduction instructions for AWS
		</comment>
		<comment id='2' author='AlvinChen13' date='2017-02-21T23:59:58Z'>
		&lt;denchmark-link:https://github.com/yaroslavvb&gt;@yaroslavvb&lt;/denchmark-link&gt;
 Are we talking about a repro on on EC2 P2 instances with K80s? The Google example mentioned during the TF Dev Summit was on K80s, albeit possibly via Google's internal RPC, rather than grpc.
		</comment>
		<comment id='3' author='AlvinChen13' date='2017-02-22T00:17:13Z'>
		The example at the TF Dev Summit was gRPC on AWS p2.8xLarge systems.  We did that on purpose to ensure it was "commodity" hardware everyone can use.  Most of use took time off after the summit.  While not something I should promise, I will commit to updating this thread once a week until we have all of the code published.  The updates might be boring.  Feel free to ping me if I forget.
		</comment>
		<comment id='4' author='AlvinChen13' date='2017-02-27T18:25:32Z'>
		I have never had to track something publicly.  I am sure I am doing it incorrectly but spending time trying to figure out the PR spin is time that could be spent on the work.  I am going to toss out a date of early April as when the code will be released.  The team has not committed to that date, but without a general date range it is impossible for me to event attempt set expectations with anyone reading this issue.
As of today, we are on track for early April.  I have likely broken a million best practices with this comment.  I'll update again next Monday.
		</comment>
		<comment id='5' author='AlvinChen13' date='2017-03-09T21:17:28Z'>
		Late on the weekly update.  Not much to add beyond we are currently on schedule for early April.
		</comment>
		<comment id='6' author='AlvinChen13' date='2017-03-14T03:44:52Z'>
		Does anyone make sure that the 7.3x &amp; 58x performance data claimed on TF Dev Summit are based on AWS with K80? Google has servers with 8GPUs connected by PCIE, so for the 8 GPU get 7.3x performance improving, it is one node with 8 GPUs, or 8 GPUs with multiple nodes? And what batchsize do they use?
		</comment>
		<comment id='7' author='AlvinChen13' date='2017-03-23T14:28:16Z'>
		&lt;denchmark-link:https://github.com/tfboyd&gt;@tfboyd&lt;/denchmark-link&gt;
 any update on the progress? Thx!
		</comment>
		<comment id='8' author='AlvinChen13' date='2017-03-23T15:40:07Z'>
		I have been negligent in updating.  Testing, documentation, and code clean up is going well.  We are putting in a strong effort to release in early April.  I believe people internally think I have lost my mind in attempting to manage expectations externally.  It may be my cold medication speaking, but I want to be clear what I care about most is getting this code out so the users of TensorFlow can train their models faster and be more efficiently.  I am working get to some documentation created as well, as having example code is not always useful without some explanation.   To answer &lt;denchmark-link:https://github.com/AlvinChen13&gt;@AlvinChen13&lt;/denchmark-link&gt;
 's question.  For the K80 the numbers I used an AWS p2.8xlarge and recently on GCE with 8 K80s GPUs.  Both setups are one server with 8 GPUs.  Keep in mind these numbers may still be improving and this is not an official post, this is very informal although public.

Inceptionv3 K80s had a speed up of 7.5x using synthetic data and 7.34x using real data from 1 GPU to 8 GPUs
InceptionV3 P100s has a speed up of 7.4x (7.37) using synthetic data and 7.2x using real data from 1 GPU to 8 GPUs.

I used a batch-size of 32 per GPU to match some of the other bechmarks.
The 58x from 1 GPU to 64 GPUs was 8 server each with 8 K80 GPUs using AWS p2.8xLarge instances with the standard Enhanced Networking.  Again 32 was the batch-size per GPU.  We used Ubuntu 16.04 as well as Amazon Linux to see if there was a difference.  There is no real difference as long as you install the Enhanced Networking on Ubuntu 16.04.  I wrote up some &lt;denchmark-link:https://github.com/tfboyd/tf-tools/tree/master/install&gt;installation instructions&lt;/denchmark-link&gt;
 for both operating systems and a poorly written &lt;denchmark-link:http://mediocrethoughts.ghost.io/2017/03/04/installing-tensorflow-on-aws/&gt;blog post&lt;/denchmark-link&gt;
 on my single post lame blog.  :-)
I also ran ResNet-50, ResNet-152, AlexNet (batch 128 and 512) and we will be running VGG-16.  I believe that covers the most popular image-classification models, and the techniques should apply to other models.
		</comment>
		<comment id='9' author='AlvinChen13' date='2017-03-24T08:30:06Z'>
		&lt;denchmark-link:https://github.com/tfboyd&gt;@tfboyd&lt;/denchmark-link&gt;
 Great! Thanks for informing the progress. Looking forward to testing it in my training case.
		</comment>
		<comment id='10' author='AlvinChen13' date='2017-03-24T14:09:13Z'>
		I have a question that makes this more of a forum than a GitHub issue but if all of you play along who is going to stop us.  :-)
Question:  Image-classification models are interesting and people like to benchmark them.  Where would you like to see the performance team focus after these common models?  Sure many of the techniques will apply to all types of models but I suspect not all.  Thoughts?
		</comment>
		<comment id='11' author='AlvinChen13' date='2017-03-24T14:37:47Z'>
		Hi &lt;denchmark-link:https://github.com/tfboyd&gt;@tfboyd&lt;/denchmark-link&gt;
, I'm not sure I have completely understand your question. But for me, I would like to know the computation time in forward, backward, forward&amp;backward, under different distributed training settings of workers/ps.
		</comment>
		<comment id='12' author='AlvinChen13' date='2017-03-27T17:31:24Z'>
		Hi &lt;denchmark-link:https://github.com/tfboyd&gt;@tfboyd&lt;/denchmark-link&gt;
,  I often see that image classification models use synchronous data parallelism that at the end of the day has the same effect of using large batch sizes.  I guess a comparison between asynchronous and synchronous updates in terms of final obtainable accuracy and time saved in synchronization could be interesting.
		</comment>
		<comment id='13' author='AlvinChen13' date='2017-04-04T15:22:28Z'>
		Thanks &lt;denchmark-link:https://github.com/pedropgusmao&gt;@pedropgusmao&lt;/denchmark-link&gt;
, that is interesting to us as well.
Quick update.  We are moving into the documentation phase.  We are a little behind my made up goal of early April but progress is good.  I expect the code will be released in a few weeks.  Things rarely go as planned.  It is on me for not pushing harder in late March.
		</comment>
		<comment id='14' author='AlvinChen13' date='2017-04-18T00:10:22Z'>
		&lt;denchmark-link:https://github.com/tfboyd&gt;@tfboyd&lt;/denchmark-link&gt;
 any new updates? Thanks!
		</comment>
		<comment id='15' author='AlvinChen13' date='2017-04-18T15:27:07Z'>
		80% likely next week.  We are branching the benchmark code today and starting the final round of "runs".  The end is near.
		</comment>
		<comment id='16' author='AlvinChen13' date='2017-04-26T15:45:16Z'>
		Great lesson learned that I already knew.  Managing expectations is not fun.
Update:  The draft documents are complete and working through the tech writer process.  While over sharing, I hope to have everything staged late this week and push the "publish" button on Tuesday.  Personally, I am excited to focus on other areas of performance and when TF 1.1 comes out (it is either out now or very near) I highly suggest moving to it or build from head.  There were some great changes pushed into core as a result of our performance benchmark effort.  I hope to write something small up about that early next week as well.
Thank you for all the feedback, the end is finally near.
		</comment>
		<comment id='17' author='AlvinChen13' date='2017-05-03T22:40:10Z'>
		The &lt;denchmark-link:https://www.tensorflow.org/performance/benchmarks&gt;benchmarks&lt;/denchmark-link&gt;
 have been published along with the code.  Because all of you can do math, I want to address something in this thread.  InceptionV3 with batch size 32 no longer scales at x58.  What happened is that we got faster across the board but our gains for 1 GPU were higher than at 64 GPUs and that lowered the scaling factor a little.  Our images/sec, which we did not share, at the conference was ~1,496 and our current number is 1608 images/sec for batch size 32 per GPU.
I apologize for the delay and look forward to focusing on time to accuracy, async training with accuracy, and other interesting problems.  If I missed something let me know.  I personally ran all of the tests and made every attempt to not grab the "best number".  I run each test 5 times and averaged the results.  I suspect I have run 5-10K tests in the past few months (I should sit down and make a more accurate guess) and spent some absurd amount of money.  Thank you for being patient.  In the future, I will do what I can to avoid these types of delays.  I am closing this issue but you can still comment, I think?
I also learned a lot about scaling on Google Cloud and AWS.  If anyone wants to share amusing stories or has questions let me know.  I documented my environment, which I doubt was ideal but they mostly worked well for what I was doing.
.  While, I say in the document that you need to use code post TF 1.1.  If you want to run the scripts with TF 1.1, you will get slightly worse results, but you can do it by commending out this line: .  And if you just want to jump to the &lt;denchmark-link:https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py&gt;scripts&lt;/denchmark-link&gt;

EDIT:  slightly wrong link to master vs. root.  fixed
		</comment>
	</comments>
</bug>