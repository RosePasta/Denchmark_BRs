<bug id='38825' author='TCBocean' open_date='2020-04-23T06:32:28Z' closed_time='2020-06-01T11:52:32Z'>
	<summary>Why the output of my tflite model running on the CPU and GPU of the Android phone is not the same</summary>
	<description>
System information

windows10:
python3.7:
tensorflow-gpu 2.1.0 installed via pip:
androidstudio 3.6.2:
org.tensorflow:tensorflow-lite-gpu:2.1.0:
org.tensorflow:tensorflow-lite:2.1.0:

**As the title says, the tflite model I converted runs on the CPU of the Android phone and the result on the GPU is inconsistent. I tried two Android phones with the same problem (SoC is Snapdragon 660 / Snapdragon 845)
The result of the model running on the Android CPU is consistent with that on the computer. I think this should explain that the model itself is not a problem?
https://github.com/TCBocean/tflite_test
This is the code of my Android Studio project. This is a very simple project. I use Log.e to view the output.
Among them, the 52 ~ 53 behavior of MainActivity.java opens the GpuDelegate, and then deletes it to get the CPU operation result.
My GPU operation results are:
&lt;denchmark-code&gt;2020-04-23 14: 24: 01.682 9335-9335 / com.stars.tflite_test1 E / 1111: output1: 1.2991362E28
2020-04-23 14: 24: 01.682 9335-9335 / com.stars.tflite_test1 E / 1111: output2: Infinity
&lt;/denchmark-code&gt;

My CPU operation result is
&lt;denchmark-code&gt;2020-04-23 14: 25: 59.974 10058-10058 / com.stars.tflite_test1 E / 1111: output1: 378560.0
2020-04-23 14: 25: 59.974 10058-10058 / com.stars.tflite_test1 E / 1111: output2: 6.6762416E10
&lt;/denchmark-code&gt;

It can be seen that there are obvious differences
Below is my model generation code:**
&lt;denchmark-code&gt;import tensorflow as tf

class test_model(tf.keras.Model):
    def __init__(self):
        super(test_model, self).__init__()
        self.conv1 = tf.keras.layers.Conv2D(filters=40, kernel_size=3, padding="SAME", kernel_initializer=tf.ones)
        self.conv2 = tf.keras.layers.Conv2D(filters=56, kernel_size=3, padding="SAME", kernel_initializer=tf.ones)
        self.conv3 = tf.keras.layers.Conv2D(filters=98, kernel_size=3, padding="SAME", kernel_initializer=tf.ones)
        self.conv4 = tf.keras.layers.Conv2D(filters=33, kernel_size=3, padding="SAME", kernel_initializer=tf.ones)
        self.conv5 = tf.keras.layers.Conv2D(filters=14, kernel_size=3, padding="SAME", kernel_initializer=tf.ones)

    @tf.function
    def call(self, inputs):
        output1 = self.conv1(inputs)
        output1 = self.conv2(output1)
        output_temp = output1
        output1 = self.conv4(output1)
        output2 = self.conv3(output1)
        output2 = tf.concat([output2, output_temp], axis=-1)
        output2 = self.conv5(output2)

        return output1, output2


model = test_model()
test_input = tf.ones((1, 6, 6, 1))

tf.keras.backend.set_learning_phase(False)
test_output1 = model(test_input)
for output in test_output1:
    print(output)

model._set_inputs(inputs=test_input)

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
open("./save6/converted_model.tflite", "wb").write(tflite_model)
&lt;/denchmark-code&gt;

Can anyone help me see what went wrong?
thank you very much : )
	</description>
	<comments>
		<comment id='1' author='TCBocean' date='2020-05-22T09:18:56Z'>
		Can no one really help me?
I tried to change the version in build.gradle to 1.15.0 and 2.2.0, where the output of GPU output and CPU output of version 1.15.0 are consistent, and 2.2.0 and 2.1.0 can not get the same result.
But I also found that the GPU inference speed of 2.x.0 is twice that of 1.15.0. Is it because the GPU inference of 2.x.0 has been simplified? Therefore, the output progress of 2.x.0 is not high?
		</comment>
		<comment id='2' author='TCBocean' date='2020-05-22T17:11:10Z'>
		Sorry, I saw those Python code which requires me to install and compile TF (2hrs last time I was debugging substring op test) so I procrastinated and forgot about it.  Now, re-reading your stuff, I see you attached the model, so let's go with that.  I think the shapes of the weights are a bit off.
DEPTHWISE_CONV_2D at the beginning takes in 1x6x6x1 and has the output of 1x6x6x40.  Then the weights must have the shape of 40x?x?x1, but you have 1x?x?x40.  GPU has a different memory layout than the CPU and will produce a different output.
		</comment>
		<comment id='3' author='TCBocean' date='2020-05-23T00:49:52Z'>
		&lt;denchmark-link:https://github.com/impjdi&gt;@impjdi&lt;/denchmark-link&gt;

If you say that, this position is indeed strange. There is also a magical problem. I obviously used ordinary convolution, but it became DEPTHWISE convolution, and such a change did not cause deviation in CPU inference.
		</comment>
		<comment id='4' author='TCBocean' date='2020-05-23T01:14:29Z'>
		&lt;denchmark-link:https://github.com/impjdi&gt;@impjdi&lt;/denchmark-link&gt;

I found out that because the size of the 4th dimension of my input is only 1, it becomes DEPTHWISE convolution.
I changed the input to (1, 6, 6, 2), and it became a normal convolution.
&lt;denchmark-code&gt;
import tensorflow as tf

class test_model(tf.keras.Model):
    def __init__(self):
        super(test_model, self).__init__()
        self.conv1 = tf.keras.layers.Conv2D(filters=40, kernel_size=3, padding="SAME", kernel_initializer=tf.ones)
        self.conv2 = tf.keras.layers.Conv2D(filters=56, kernel_size=3, padding="SAME", kernel_initializer=tf.ones)
        self.conv3 = tf.keras.layers.Conv2D(filters=98, kernel_size=3, padding="SAME", kernel_initializer=tf.ones)
        self.conv4 = tf.keras.layers.Conv2D(filters=33, kernel_size=3, padding="SAME", kernel_initializer=tf.ones)
        self.conv5 = tf.keras.layers.Conv2D(filters=14, kernel_size=3, padding="SAME", kernel_initializer=tf.ones)

    @tf.function
    def call(self, inputs):
        output1 = self.conv1(inputs)  # 40
        output1 = self.conv2(output1)  # 56
        output_temp = output1
        output1 = self.conv4(output1)  # 33
        output2 = self.conv3(output1)  # 98
        output2 = tf.concat([output2, output_temp], axis=-1)  # 98+56
        output2 = self.conv5(output2)  # 14

        return output1, output2


model = test_model()
test_input = tf.ones((1, 6, 6, 2))

tf.keras.backend.set_learning_phase(False)
test_output1 = model(test_input)
for output in test_output1:
    print(output)

model._set_inputs(inputs=test_input)

converter = tf.lite.TFLiteConverter.from_keras_model(model)
# converter.experimental_new_converter = True
tflite_model = converter.convert()
open("./save6/converted_model.tflite", "wb").write(tflite_model)
&lt;/denchmark-code&gt;

After putting the modified model into my Android demo, the CPU operation result is the same as that on the PC side, while the GPU operation result is only the output2 result is the same, the output1 result still shows a big difference, because output1 is taken from the middle layer ?
		</comment>
		<comment id='5' author='TCBocean' date='2020-05-23T01:30:28Z'>
		emmmmmmmmmm, I found that the output2 output is correct. It is not the credit of the first convolution changed to ordinary convolution, but I set it.
&lt;denchmark-code&gt;GpuDelegate.Options gpu_options = new GpuDelegate.Options();
gpu_options.setPrecisionLossAllowed (false); // It seems that the default is true
gpu_options.setInferencePreference (1);
GpuDelegate delegate = new GpuDelegate(gpu_options);
&lt;/denchmark-code&gt;

It seems that setPrecisionLossAllowed is working. When I comment it out, the GPU output will become abnormal again.
However, no matter how it is set, the result of output1 will still have problems on the GPU
		</comment>
		<comment id='6' author='TCBocean' date='2020-06-01T11:52:32Z'>
		I can basically be sure that setPrecisionLossAllowed (false) brought the correct result. I don't know why the official will set it to true by default without explanation.
		</comment>
		<comment id='7' author='TCBocean' date='2020-06-01T11:52:33Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38825&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38825&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='TCBocean' date='2020-06-01T19:55:30Z'>
		Thanks for the update &lt;denchmark-link:https://github.com/TCBocean&gt;@TCBocean&lt;/denchmark-link&gt;

If that's the case, it's often either of the two (or both):

The network is not numerically stable and its design needs to be updated, or
There's a bug in the accumulator and its precision must be increased.

		</comment>
	</comments>
</bug>