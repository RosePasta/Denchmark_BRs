<bug id='36591' author='ndombe' open_date='2020-02-09T11:47:30Z' closed_time='2020-02-10T21:34:51Z'>
	<summary>Nit tutorial inconsistency in text generation with RNN</summary>
	<description>
&lt;denchmark-h:h2&gt;URL(s) with the issue:&lt;/denchmark-h&gt;

&lt;denchmark-link:https://www.tensorflow.org/tutorials/text/text_generation#the_prediction_loop&gt;https://www.tensorflow.org/tutorials/text/text_generation#the_prediction_loop&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;Description of issue (what needs changing):&lt;/denchmark-h&gt;

Consider the previously generated letters for subsequent generations. Right now only the lastly generated letter is considered to generate the immediate next letter. The result reads very far from natural as you can imagine.
&lt;denchmark-h:h3&gt;Clear description&lt;/denchmark-h&gt;

The generate_text() method is supposed to generate text, letter by letter, by taking into account a starting string and any letter that has been generated so far. Right now, it's generating the first letter by taking into account the starting string, then from then on, it only considers the last generated letter.
So in this:
&lt;denchmark-code&gt;def generate_text(model, start_string):
  # Evaluation step (generating text using the learned model)

  # Number of characters to generate
  num_generate = 1000

  # Converting our start string to numbers (vectorizing)
  input_eval = [char2idx[s] for s in start_string]
  input_eval = tf.expand_dims(input_eval, 0)

  # Empty string to store our results
  text_generated = []

  # Low temperatures results in more predictable text.
  # Higher temperatures results in more surprising text.
  # Experiment to find the best setting.
  temperature = 1.0

  # Here batch size == 1
  model.reset_states()
  for i in range(num_generate):
      predictions = model(input_eval)
      # remove the batch dimension
      predictions = tf.squeeze(predictions, 0)

      # using a categorical distribution to predict the character returned by the model
      predictions = predictions / temperature
      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()

      # We pass the predicted character as the next input to the model
      # along with the previous hidden state
      input_eval = tf.expand_dims([predicted_id], 0)

      text_generated.append(idx2char[predicted_id])

  return (start_string + ''.join(text_generated))
&lt;/denchmark-code&gt;

The end of the for-loop needs to be updated to match the comment description, with something like this:
&lt;denchmark-code&gt;# We pass the predicted character as the next input to the model
# along with the previous hidden state
input_eval = tf.concat([input_eval, tf.expand_dims([predicted_id], 0)], -1)
&lt;/denchmark-code&gt;

Where we add the newly predicted ID to the end of what is currently input_eval
&lt;denchmark-h:h3&gt;Submit a pull request?&lt;/denchmark-h&gt;

I plan on submitting a pull request with the quick fix.
	</description>
	<comments>
		<comment id='1' author='ndombe' date='2020-02-10T21:34:51Z'>
		This problem actually wouldn't exist for stateful models, i.e as long as the RNN layer has been specified to be stateful, it will keep track of what it's seen previously (since the last state reset) and we therefore only need to give it the last prediction.
The original implementation is correct!
		</comment>
	</comments>
</bug>