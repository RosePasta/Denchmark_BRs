<bug id='15019' author='taehyunkim1527' open_date='2017-12-01T01:40:47Z' closed_time='2017-12-07T05:13:24Z'>
	<summary>Tensorflow: how to save/restore tf.data.Dataset?</summary>
	<description>
I made a model with tf.data.Dataset() as a data IO function
then i exported the graph and tried to restore it with meta_graph file But it failed and following error messages occurred.
I think that tf.data.Dataset() made a C++ object instead of python queue used before.
And the graph_def only has a C++ object handler reference, so the graph_def alone without real C++ object can't load complete graph.
How can I load a executable graph with tf.data.Dataset()? Or is it impossible for now?
&lt;denchmark-code&gt;File "/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

NotFoundError (see above for traceback): Function
_make_dataset_5150cb86 is not defined.
         [[Node: batch_processing/OneShotIterator = OneShotIterator[container="", dataset_factory=_make_dataset_5150cb86[], output_shapes=[[?,1], [?,299,299,3]], output_types=[DT_INT32, DT_FLOAT], shared_name="",
_device="/job:workers/replica:0/task:0/device:CPU:0"]()]]
&lt;/denchmark-code&gt;

In short, all the tensorflow graphs without tf.data.Dataset work, when i add following codes.
&lt;denchmark-code&gt;graph = tf.train.export_meta_graph()
tf.reset_default_graph()
tf.train.import_meta_graph(graph)

&lt;/denchmark-code&gt;

But the graphs withtf.data.Datasetmake a error message above
	</description>
	<comments>
		<comment id='1' author='taehyunkim1527' date='2017-12-01T13:03:40Z'>
		Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.
Have I written custom code
OS Platform and Distribution
TensorFlow installed from
TensorFlow version
Bazel version
CUDA/cuDNN version
GPU model and memory
Exact command to reproduce
		</comment>
		<comment id='2' author='taehyunkim1527' date='2017-12-01T20:52:09Z'>
		I have a similar issue -
I'm trying to load a trained model for inference. In the code snippet below, features:0 is the name of one of the tensors returned by dataset iterator. Is it possible to directly feed the tensor without having to initialize the iterator ?
Code
with tf.Session(graph=tf.Graph()) as session:
    graph_meta = tf.train.latest_checkpoint(model_dir) + '.meta'
    saver = tf.train.import_meta_graph(os.path.join(model_dir, graph_meta))
    saver.restore(session, tf.train.latest_checkpoint(model_dir))
    feed_dict = {
        'features:0': x_test # shape 102x13
    }
    predictions = session.run('logits:0', feed_dict)
    print(predictions.shape)
Error
&lt;denchmark-code&gt;FailedPreconditionError: GetNext() failed because the iterator has not been initialized. Ensure that you have run the initializer operation for this iterator before getting the next element.
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,13], [?], [?]], output_types=[DT_DOUBLE, DT_DOUBLE, DT_DOUBLE], _device="/job:localhost/replica:0/task:0/device:CPU:0"](Iterator)]]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='taehyunkim1527' date='2017-12-05T19:35:30Z'>
		&lt;denchmark-link:https://github.com/taehyunkim1527&gt;@taehyunkim1527&lt;/denchmark-link&gt;
 Can you share a complete reproducible example of the problem? I was unable to reproduce the problem with the code fragment in your example, although it was possible to reproduce it by adding arguments to . I am in the process of fixing the latter problems, but it would be great to confirm that the fix works for your issue too.
&lt;denchmark-link:https://github.com/suryasumukh&gt;@suryasumukh&lt;/denchmark-link&gt;
 I think you're running into a different problem. You certainly can feed values over the tensors returned from , but you need to ensure that you feed  of the tensors returned by the iterator that might be used in the  call. If you continue to have problems with this, please post a question on Stack Overflow.
		</comment>
		<comment id='4' author='taehyunkim1527' date='2017-12-06T05:28:09Z'>
		&lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;
 When i use that series of code of 'exporting and importing' in tensorflow/benchmarks's ImageNet tasks, I experienced this error message.
I tried to implement a simple code to make the same error message today, but it failed.
I think it is not a problem of tf.data.Dataset() and let me debug again.
Thank you for your kindness.
		</comment>
		<comment id='5' author='taehyunkim1527' date='2017-12-06T15:09:20Z'>
		Thanks for confirming that the problem doesn't arise with that exact code! I have a change in the pipeline that will make this path work in more cases (e.g. when using clear_devices=True or a scope prefix), so I'll reopen this issue until it lands.
		</comment>
		<comment id='6' author='taehyunkim1527' date='2018-01-12T05:08:36Z'>
		&lt;denchmark-link:https://github.com/mrry&gt;@mrry&lt;/denchmark-link&gt;

Hi, I'm who opened this question formerly.
And i found what was the problem.
if we give arg '' as True for the function ')' of 
It starts to make empty graph_def and copy previous node_defs newly.
In this process, this code does not consider about .
But the information about ' resides in graph_def.library
		</comment>
		<comment id='7' author='taehyunkim1527' date='2019-04-07T06:48:55Z'>
		&lt;denchmark-link:https://github.com/suryasumukh&gt;@suryasumukh&lt;/denchmark-link&gt;
 In order to retrain the pre-trained model, the initializer of data iterator can be declared as a tf.operation with a name while training for the first time.
&lt;denchmark-code&gt;    data_iter = dataset.make_initializable_iterator()
    data_iter_init = data_iter.make_initializer(dataset, name='Data_itr_init')
    next_batch = data_iter.get_next()
&lt;/denchmark-code&gt;

Then, it can be sess.run with the name and fed with training data.
sess.run('Data_itr_init', feed_dict={"Model_in:0": train_X, "Model_out:0": train_Y})
		</comment>
		<comment id='8' author='taehyunkim1527' date='2019-08-15T15:14:34Z'>
		
@suryasumukh In order to retrain the pre-trained model, the initializer of data iterator can be declared as a tf.operation with a name while training for the first time.
    data_iter = dataset.make_initializable_iterator()
    data_iter_init = data_iter.make_initializer(dataset, name='Data_itr_init')
    next_batch = data_iter.get_next()

Then, it can be sess.run with the name and fed with training data.
sess.run('Data_itr_init', feed_dict={"Model_in:0": train_X, "Model_out:0": train_Y})

Not the point
		</comment>
	</comments>
</bug>