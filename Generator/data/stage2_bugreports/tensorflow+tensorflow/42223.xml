<bug id='42223' author='j7168908jx' open_date='2020-08-11T08:05:29Z' closed_time='2020-08-20T17:43:48Z'>
	<summary>Wrong output when decorating functions with @tf.function</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Complete test code(minimum version for reproducing the problem) below
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10, tf 2.2.0 py 3.6.8
TensorFlow installed from (source or binary): pip install tensorflow=2.2.0
TensorFlow version (use command below): 2.2.0
Python version: 3.6.8
CUDA/cuDNN version: Using cpu

Describe the current/expected behavior
The following is a simple example for writing an iterative method to solve Ax=b, as the iterative step is
&lt;denchmark-code&gt;x_{k+1} = x_k + \alpha * (b - A x_k)
&lt;/denchmark-code&gt;

while adding/removing the @tf.function decorator in Line 12 will result in different output.
The output with @tf.function:
&lt;denchmark-code&gt;Step  0: loss 1.00000000e+00
Step  1: loss 1.00000000e+00
Step  2: loss 1.00000000e+00
Step  3: loss 1.00000000e+00
Step  4: loss 1.00000000e+00
...
&lt;/denchmark-code&gt;

The output without @tf.function (correct):
&lt;denchmark-code&gt;Step  0: loss 1.00000000e+00
Step  1: loss 7.73297510e-01
Step  2: loss 6.05587767e-01
Step  3: loss 4.81463411e-01
Step  4: loss 3.89456113e-01
...
&lt;/denchmark-code&gt;

Standalone code to reproduce the issue
import tensorflow as tf
import numpy as np


class MyFakeNN(tf.keras.Model):
    def __init__(self, max_num_iter=100, **kwargs):
        super(MyFakeNN, self).__init__(**kwargs)
        self.max_num_iter  = max_num_iter
        self.func = func
        self.curr_iter = 0

    @tf.function
    def train_one_step(self, x, y):
        x_next = self(x, y)
        loss = loss_func(x_next, y)
        return loss

    def train(self, x, y):
        for it in range(self.max_num_iter):
            self.curr_iter = it
            self.trainable = False
            loss = self.train_one_step(x, y)
            print("Step %2d: loss %14.8e" % (it, loss.numpy()))

    def call(self, x, y):
        for _ in range(self.curr_iter):
            r = y - self.func(x)
            x += r * 2e-3
        return x


n = 8
tf.keras.backend.set_floatx('float64')

a = np.eye(n, k=0) * 2 - np.eye(n, k=1) - np.eye(n, k=-1)
a[-1, 0] = -1
a[0, -1] = -1
a *= n ** 2
func = lambda x: tf.matmul(x, a)


def loss_func(x, y):
    return tf.reduce_mean(tf.norm(func(x) - y, axis=1)/tf.norm(y, axis=1))


model = MyFakeNN()
model.trainable = False

y = np.array([[-9.38155831, -28.13152345, 7.78468155, 29.26619534,
               -4.5831364, -25.87319867, 6.18001317, 24.73852678], [-9.83892541, -20.53029054, 20.69956084, 28.01860432,
               -20.97012137, -35.88954846, 10.10948594, 28.40123469], [-21.04290567, -8.05650662, 16.85368166, 1.02738802,
               -22.60513251, -2.13204921, 26.79435652, 9.16116781], [2.04530187, -2.93405974, -2.79706502, -1.04454986,
               -2.07777548, -1.87090609, 2.82953863, 5.84951569], [-4.84959288, 6.48666613, 8.21196668, -4.87053449,
               -9.2887852, 1.73155149, 5.92641139, -3.34768314], [7.66043654, 6.05170581, -3.87066543, 0.99382044,
               10.04477309, 0.69216011, -13.8345442, -7.73768635], [2.54742214, -13.73492348, 0.23540163, 14.91811095,
               -1.34494564, -17.6704306, -1.43787813, 16.48724313], [12.9314438, 3.94013919, -22.58640847, -11.10766707,
               22.104958, 17.59432117, -12.44999333, -10.42679329]])
x = np.zeros(y.shape, dtype='float64')

model.train(x=x, y=y)
	</description>
	<comments>
		<comment id='1' author='j7168908jx' date='2020-08-11T09:29:06Z'>
		I have tried in colab with TF version 2.2, nightly versions() and was able to reproduce the issue .Please, find the gist &lt;denchmark-link:https://colab.research.google.com/gist/ravikyram/d9eb43b9cc99f7b091372b9f3f0a0236/untitled240.ipynb&gt;here&lt;/denchmark-link&gt;
.Thanks!
		</comment>
		<comment id='2' author='j7168908jx' date='2020-08-12T14:43:52Z'>
		Possibly related to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/32895&gt;#32895&lt;/denchmark-link&gt;
? Although it doesn't look like the explanation provided in that issue applies to this case.
		</comment>
		<comment id='3' author='j7168908jx' date='2020-08-19T19:27:38Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='4' author='j7168908jx' date='2020-08-20T17:43:48Z'>
		The problem here is that self.curr_iter is captured once and doesn't change with tf.function. Moving the tf.function to def train() or passing in it explicitly to the tf.function would fix it.
		</comment>
		<comment id='5' author='j7168908jx' date='2020-08-20T17:43:50Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42223&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42223&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>