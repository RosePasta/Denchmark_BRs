<bug id='9012' author='wchen342' open_date='2017-04-06T07:03:03Z' closed_time='2018-01-23T23:50:15Z'>
	<summary>BiasGradOp mistakenly put on CPU</summary>
	<description>
NOTE: Issues that are not bugs or feature requests will be closed. Please ask usage questions on StackOverflow.
&lt;denchmark-h:h3&gt;You must complete this information or else your issue will be closed&lt;/denchmark-h&gt;


Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?:
custom code
TensorFlow installed from (source or binary)?: from binary
TensorFlow version: 1.0.1
Bazel version (if compiling from source): N/A
CUDA/cuDNN version: CUDA 8.0, cuDNN v5.1
GPU Model and Memory: GTX 1080, 8GB
Exact command to reproduce:
Here is a sample script in Python that can reproduce the problem:

&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np
ly = tf.layers


def lrelu(x, leak=0.2, name="lrelu"):
    with tf.variable_scope(name):
        f1 = 0.5 * (1 + leak)
        f2 = 0.5 * (1 - leak)
        return f1 * x + f2 * tf.abs(x)
        # return tf.maximum(leak*x, x)

x = np.ones([16, 3, 32, 32], dtype=np.float32)

with tf.device('/gpu:0'):
    input = tf.placeholder(tf.float32, shape=[16, 3, 32, 32])
    output = ly.conv2d(input, 3, kernel_size=1, data_format='channels_first',
                       strides=1, activation=lrelu)
    loss = tf.gradients(input - output, input)[0]
    optimizer = tf.train.AdamOptimizer()
    gradients = optimizer.compute_gradients(loss)
    grad_op = optimizer.apply_gradients(gradients)


with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    out = sess.run(grad_op, feed_dict={input: x})
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Describe the problem clearly&lt;/denchmark-h&gt;

I am on Ubuntu 16.04. While running the above script, despite the device has been specified to be GPU, tensorflow still try to do the BiasGradOp on CPU and will cause an error because of the data format. If I change the implementation of lrelu() to return tf.maximum(leak*x, x) then the problem goes away.
&lt;denchmark-h:h3&gt;Source Code / Logs&lt;/denchmark-h&gt;

Here is the output from console:
&lt;denchmark-code&gt;I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX 1080
major: 6 minor: 1 memoryClockRate (GHz) 1.797
pciBusID 0000:01:00.0
Total memory: 7.92GiB
Free memory: 7.21GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)
E tensorflow/core/common_runtime/executor.cc:594] Executor failed to create kernel. Invalid argument: CPU BiasGradOp only supports NHWC.
	 [[Node: gradients_1/conv2d/BiasAdd_grad/BiasAddGrad = BiasAddGrad[T=DT_FLOAT, data_format="NCHW", _device="/job:localhost/replica:0/task:0/gpu:0"](gradients_1/gradients/conv2d/lrelu/Abs_grad/Sign_grad/zeros)]]
E tensorflow/core/common_runtime/executor.cc:594] Executor failed to create kernel. Invalid argument: CPU BiasGradOp only supports NHWC.
	 [[Node: gradients_1/conv2d/BiasAdd_grad/BiasAddGrad = BiasAddGrad[T=DT_FLOAT, data_format="NCHW", _device="/job:localhost/replica:0/task:0/gpu:0"](gradients_1/gradients/conv2d/lrelu/Abs_grad/Sign_grad/zeros)]]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='wchen342' date='2017-04-10T20:21:57Z'>
		This does indeed seem like a bug, thanks for the nice reproducible example, it really helps.
Will dig into it some.
		</comment>
		<comment id='2' author='wchen342' date='2017-06-16T19:09:48Z'>
		&lt;denchmark-link:https://github.com/asimshankar&gt;@asimshankar&lt;/denchmark-link&gt;
  hey asim, are you still working on this?
		</comment>
		<comment id='3' author='wchen342' date='2017-12-22T07:46:20Z'>
		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
		</comment>
		<comment id='4' author='wchen342' date='2018-01-05T19:12:47Z'>
		Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='5' author='wchen342' date='2018-01-23T23:13:39Z'>
		A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.
		</comment>
		<comment id='6' author='wchen342' date='2018-01-23T23:50:15Z'>
		I'm very sorry for the delayed response.
I believe this is fixed in TensorFlow 1.4.0 (the same code snippet doesn't not error out).
Please re-open if I'm mistaken. Once again, apologies for the delay.
		</comment>
	</comments>
</bug>