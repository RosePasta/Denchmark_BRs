<bug id='5913' author='cancan101' open_date='2016-11-28T20:54:05Z' closed_time='2017-06-16T20:35:59Z'>
	<summary>strip_unused assumes all placeholders are the same type</summary>
	<description>
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/5657d0dee8d87f4594b3e5902ed3e3ca8d6dfc0a/tensorflow/python/tools/strip_unused.py&gt;strip_unused&lt;/denchmark-link&gt;
 assumes that all of the placeholders are of the same type which may not be the case. This method is also used by &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/5657d0dee8d87f4594b3e5902ed3e3ca8d6dfc0a/tensorflow/python/tools/optimize_for_inference_lib.py#L82-L85&gt;optimize_for_inference&lt;/denchmark-link&gt;
 which leads to errors when the models is loaded:
&lt;denchmark-code&gt; msg std::__1::string "Input 0 of node dropout6/cond/Switch was passed float from Placeholder_2:0 incompatible with expected bool." 
&lt;/denchmark-code&gt;

I suggest having an option to introspect the graph to determine type.
	</description>
	<comments>
		<comment id='1' author='cancan101' date='2016-11-28T21:52:37Z'>
		For example why is this line:
      placeholder_node.attr["dtype"].CopyFrom(tf.AttrValue(
          type=placeholder_type_enum))
not?:
      placeholder_node.attr["dtype"].CopyFrom(
          node.attr["dtype"]
      )
		</comment>
		<comment id='2' author='cancan101' date='2016-11-29T00:18:57Z'>
		This is definitely a problem for me. My X placeholder is of type int32 and my dropout keep probability is of type float32, so this is unusable for me.
		</comment>
		<comment id='3' author='cancan101' date='2017-06-16T17:41:30Z'>
		Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you.
		</comment>
		<comment id='4' author='cancan101' date='2017-06-16T17:47:53Z'>
		this still looks to be relevant.
		</comment>
		<comment id='5' author='cancan101' date='2017-06-16T19:15:13Z'>
		&lt;denchmark-link:https://github.com/petewarden&gt;@petewarden&lt;/denchmark-link&gt;
 do you have any plans to fix this? If not, please unassign yourself and mark contributions welcome (if appropriate).
		</comment>
		<comment id='6' author='cancan101' date='2017-06-16T19:47:04Z'>
		The new graph transform approach fixes this issue:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms/#strip_unused_nodes&gt;https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms/#strip_unused_nodes&lt;/denchmark-link&gt;

We need to update the documentation and code to mark the old strip_unused.py script as deprecated.
		</comment>
		<comment id='7' author='cancan101' date='2017-06-16T20:35:59Z'>
		Thanks, I'll close this then.
		</comment>
		<comment id='8' author='cancan101' date='2018-10-12T16:23:58Z'>
		&lt;denchmark-link:https://github.com/petewarden&gt;@petewarden&lt;/denchmark-link&gt;
 Is  deprecated as well? I see no mention regarding that not even in , so I'm wondering :/
		</comment>
	</comments>
</bug>