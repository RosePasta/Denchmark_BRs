<bug id='30321' author='mketcha' open_date='2019-07-02T18:25:44Z' closed_time='2019-07-25T18:39:02Z'>
	<summary>tensorflow 2.0 keras multi_gpu_model only utilizing one GPU</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary): Binary, pip install
TensorFlow version (use command below): tensorflow-gpu==2.0.0-beta1
Python version: 3.6
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: cudatoolkit 10.0.130, cudnn 7.6.0
GPU model and memory: 4x NVIDIA GeForce GTX 1080 Ti

Describe the current behavior
When using multi_gpu_model (i.e., tf.keras.utils.multi_gpu_model) in tensorflow 2.0 to distribute a job across multiple gpus (4), only one gpu appears to be used. That is when monitoring the GPU usage only one GPU shows substantial dedicated GPU memory usage and GPU utility.
Describe the expected behavior
Each of the 4 GPUs should indicate that memory is being copied to the device and processed.

While my issue arises with custom code using model.fit_generator, I was able to replicate the issue using model.fit with documentation code provided at &lt;denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/utils/multi_gpu_model&gt;https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/utils/multi_gpu_model&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.applications import Xception
from tensorflow.keras.utils import multi_gpu_model
import numpy as np

num_samples = 1000
height = 224
width = 224
num_classes = 1000

# Instantiate the base model (or "template" model).
# We recommend doing this with under a CPU device scope,
# so that the model's weights are hosted on CPU memory.
# Otherwise they may end up hosted on a GPU, which would
# complicate weight sharing.
with tf.device('/cpu:0'):
    model = Xception(weights=None,
                     input_shape=(height, width, 3),
                     classes=num_classes)

# Replicates the model on 8 GPUs.
# This assumes that your machine has 8 available GPUs.
parallel_model = multi_gpu_model(model, gpus=4) # gpus changed to 4
parallel_model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

# Generate dummy data.
x = np.random.random((num_samples, height, width, 3))
y = np.random.random((num_samples, num_classes))

parallel_model.summary()
# This `fit` call will be distributed on 8 GPUs.
# Since the batch size is 256, each GPU will process 32 samples.
parallel_model.fit(x, y, epochs=20, batch_size=16) #batch_sized changed to 16
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='mketcha' date='2019-07-03T19:11:46Z'>
		I have separately encountered the same issue as the OP. The load is distributed across all GPUs as expected with multi_gpu_model in TensorFlow1.13.1, but sits on 1 GPU in 2.0.
Below are screenshots of my nvidia-smi when running the above code with 1.13 and 2.0 respectively.
Have I written custom code: No
OS Platform and Distribution: Ubuntu 18.04
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.13.1 and 2.0.0-beta1, both gpu
tensorflow-gpu==1.13.1
tensorflow-gpu==2.0.0-beta1
Python version: 3.6
CUDA/cuDNN version: CUDA: 10.0, cuDNN: 7.6
GPU model and memory: 4x NVIDIA Tesla M60
Tensorflow 1.13.1:
&lt;denchmark-link:https://user-images.githubusercontent.com/35641527/60617662-cc05cc80-9d99-11e9-812b-592970181a31.png&gt;&lt;/denchmark-link&gt;

Tensorflow 2.0.0-beta1
&lt;denchmark-link:https://user-images.githubusercontent.com/35641527/60617668-cdcf9000-9d99-11e9-8514-8f26f546580a.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='mketcha' date='2019-07-06T22:10:41Z'>
		Use distribute strategy for multi gpu training in tf2.0. As of now multi_gpu_model doesnot work when running in eager mode, which is default in tf2.0. Here is the code with necessary changes made
&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.applications import Xception
import numpy as np

num_samples = 1000
height = 224
width = 224
num_classes = 1000

strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
    parallel_model = Xception(weights=None,
                     input_shape=(height, width, 3),
                     classes=num_classes)
    parallel_model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

# Generate dummy data.
x = np.random.random((num_samples, height, width, 3))
y = np.random.random((num_samples, num_classes))

parallel_model.summary()
# This `fit` call will be distributed on 8 GPUs.
# Since the batch size is 256, each GPU will process 32 samples.
parallel_model.fit(x, y, epochs=20, batch_size=16) #batch_sized changed to 16
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='mketcha' date='2019-07-10T05:09:43Z'>
		Any plans or advise how to use multiple gpus with fit_generator method?
tensorflow/python/keras/engine/training.py", line 1157, in fit_generator
raise NotImplementedError('fit_generator is not supported for '
NotImplementedError: fit_generator is not supported for models compiled with tf.distribute.Strategy.
		</comment>
		<comment id='4' author='mketcha' date='2019-07-10T06:50:09Z'>
		&lt;denchmark-link:https://github.com/IuriiZhakun&gt;@IuriiZhakun&lt;/denchmark-link&gt;
  you can use the api to create a train/validation dataset which can be used with the tf.keras.Model.fit method.  is one the ways to achieve this.
		</comment>
		<comment id='5' author='mketcha' date='2019-07-11T11:03:41Z'>
		training gets stuck with multiple gpus at 100% utilization, after yielding few samples.
the same code works fine without MirroredStrategy.
I guess I shall try training without keras fit method.
&lt;denchmark-link:https://github.com/srihari-humbarwadi&gt;@srihari-humbarwadi&lt;/denchmark-link&gt;
, thanks nevertheless!
		</comment>
		<comment id='6' author='mketcha' date='2019-07-11T16:16:39Z'>
		Thanks for the great suggestions!
I am also using fit_generator with eager execution, though for me it was just easier to revert back to TF1.14 until multi_gpu_model gets fixed in TF2.0 eager mode
		</comment>
		<comment id='7' author='mketcha' date='2019-07-18T17:49:58Z'>
		Thanks &lt;denchmark-link:https://github.com/srihari-humbarwadi&gt;@srihari-humbarwadi&lt;/denchmark-link&gt;
 -- tf.distribute.Strategy would indeed be the recommendation. &lt;denchmark-link:https://github.com/mketcha&gt;@mketcha&lt;/denchmark-link&gt;
 , can you clarify what doesn't work when you try with MirrorStrategy? Do you have code to reproduce?
		</comment>
		<comment id='8' author='mketcha' date='2019-07-18T18:09:33Z'>
		Following &lt;denchmark-link:https://github.com/IuriiZhakun&gt;@IuriiZhakun&lt;/denchmark-link&gt;
 's comment above:

Any plans or advise how to use multiple gpus with fit_generator method?
tensorflow/python/keras/engine/training.py", line 1157, in fit_generator
raise NotImplementedError('fit_generator is not supported for '
NotImplementedError: fit_generator is not supported for models compiled with tf.distribute.Strategy.

I did not attempt the MirrorStrategy as I am also using fit_generator
		</comment>
		<comment id='9' author='mketcha' date='2019-07-23T20:35:49Z'>
		Thank you &lt;denchmark-link:https://github.com/srihari-humbarwadi&gt;@srihari-humbarwadi&lt;/denchmark-link&gt;
. I was at least able to train the model, but I get an unfortunate error at the end about running out of data.

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10.0.18362 Build 18362
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.0.0-beta1
Python version: 3.6.8
Bazel version (if compiling from source): n/a
GCC/Compiler version (if compiling from source): n/a
CUDA/cuDNN version: 10.0/7.6.1
GPU model and memory: 2x MSI GeForce RTX 2080 Ti GAMING X TRIO (no NVlink)

I used the same code that you used in your example except that I changed the batch_size to 64 and the number of epochs to 10:
&lt;denchmark-code&gt;# This `fit` call will be distributed on 2 GPUs.
# Since the batch size is 64, each GPU will process 32 samples.
parallel_model.fit(x, y, epochs=10, batch_size=64)
&lt;/denchmark-code&gt;

It runs perfectly until the final epoch. It always ends with:
&lt;denchmark-code&gt;Epoch 8/10
16/16 [==============================] - 5s 334ms/step - loss: 3590.6503
Epoch 9/10
16/16 [==============================] - 5s 332ms/step - loss: 3597.1092
Epoch 10/10
12/16 [=====================&gt;........] - ETA: 1s - loss: 3603.6067
W0723 14:30:47.582621   232 training_arrays.py:309] Your dataset ran out of data; 
interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` 
batches (in this case, 160 batches). You may need to use the repeat() function when 
building your dataset.
12/16 [=====================&gt;........] - ETA: 1s - loss: 3603.6067
&lt;/denchmark-code&gt;

This is only an issue with MirroredStrategy. When I train on a single GPU, there is no issue.
Edit: I'm getting this output too!
&lt;denchmark-code&gt;2019-07-23 16:52:04.173982: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
         [[{{node IteratorGetNext_1}}]]
         [[GroupCrossDeviceControlEdges_0/RMSprop/RMSprop/update_0/Const/_355]]
2019-07-23 16:52:04.183310: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
         [[{{node IteratorGetNext_1}}]]
         [[Identity_1/_376]]
2019-07-23 16:52:04.189139: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
         [[{{node IteratorGetNext_1}}]]
&lt;/denchmark-code&gt;

TEMPORARY SOLUTION: I converted the numpy arrays to a tf Dataset and used .repeat() while providing the proper number of steps per epoch within fit:
&lt;denchmark-code&gt;train_dataset = tf.data.Dataset.from_tensor_slices((x, y))

BATCH_SIZE = 64
BUFFER_SIZE = 10000

train_dataset = train_dataset.shuffle(BUFFER_SIZE).repeat().batch(BATCH_SIZE)

if BUFFER_SIZE % BATCH_SIZE != 0:
    parallel_steps = BUFFER_SIZE // BATCH_SIZE + 1
else:
    parallel_steps = BUFFER_SIZE // BATCH_SIZE

# This `fit` call will be distributed on 2 GPUs.
# Since the batch size is 64, each GPU will process 32 samples.
parallel_model.fit(train_dataset, epochs=10, steps_per_epoch = parallel_steps)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='mketcha' date='2019-07-25T18:39:02Z'>
		&lt;denchmark-link:https://github.com/mketcha&gt;@mketcha&lt;/denchmark-link&gt;
 You need to switch to  in this case. Closing this issue for now. Please reopen if have any further questions using . Thanks!
		</comment>
		<comment id='11' author='mketcha' date='2019-07-25T18:39:04Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=30321&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=30321&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='mketcha' date='2019-08-23T17:04:50Z'>
		
@IuriiZhakun you can use the tf.data.Dataset api to create a train/validation dataset which can be used with the tf.keras.Model.fit method. tf.data.Dataset.from_generator is one the ways to achieve this.

&lt;denchmark-link:https://github.com/srihari-humbarwadi&gt;@srihari-humbarwadi&lt;/denchmark-link&gt;
 Thank you for the hint, can you provide a few lines of code, if possible connected to your previous code snippet? I am not advanced with tensorflow and can't bring fit_generator to work on distributed strategies.
		</comment>
		<comment id='13' author='mketcha' date='2019-08-27T15:28:39Z'>
		I am using TF 1.14 and my computer has two GPUs. I was reading above posts but was confused finally which of the methods should be used with all the changes mentioned above:
Method 1:
&lt;denchmark-code&gt;with tf.device('/cpu:0'):
    model = ...
parallel_model = multi_gpu_model(model, gpus=2)
parallel_model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

&lt;/denchmark-code&gt;

Method 2:
&lt;denchmark-code&gt;strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
    parallel_model = ...
    parallel_model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

&lt;/denchmark-code&gt;

		</comment>
		<comment id='14' author='mketcha' date='2019-11-01T14:58:15Z'>
		Using TF 2.0, one solution is to switch off eager execution, which is what prevents multi_gpu_model from working. This can be done as follows after importing tensorflow:
&lt;denchmark-code&gt;import tensorflow as tf
tf.compat.v1.disable_eager_execution()
&lt;/denchmark-code&gt;

		</comment>
		<comment id='15' author='mketcha' date='2019-11-04T02:55:09Z'>
		When i use tf.distribute.MirroredStrategy() in TF2.0(stable) and there are three GPUs available(gpu_id: 1、2、3), there are four types of  GPUs utilization.
（1）
&lt;denchmark-link:https://user-images.githubusercontent.com/27215307/68097274-18e8e380-fef1-11e9-8e81-fb79dd361ad7.png&gt;&lt;/denchmark-link&gt;

（2）
&lt;denchmark-link:https://user-images.githubusercontent.com/27215307/68097281-21d9b500-fef1-11e9-91f1-878f71d28e39.png&gt;&lt;/denchmark-link&gt;

（3）
&lt;denchmark-link:https://user-images.githubusercontent.com/27215307/68097282-2736ff80-fef1-11e9-8dc4-9f296b4e3747.png&gt;&lt;/denchmark-link&gt;

（4）
&lt;denchmark-link:https://user-images.githubusercontent.com/27215307/68097284-2c944a00-fef1-11e9-847d-519facae5e07.png&gt;&lt;/denchmark-link&gt;

 &lt;denchmark-link:https://github.com/srihari-humbarwadi&gt;@srihari-humbarwadi&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/IuriiZhakun&gt;@IuriiZhakun&lt;/denchmark-link&gt;

		</comment>
		<comment id='16' author='mketcha' date='2019-11-04T05:03:25Z'>
		Looks like your GPU's are not being kept busy by the input pipeline or the batch size is too small. Please follow this guide and go ahead and tune/ profile your data input pipeline!
&lt;denchmark-link:https://www.tensorflow.org/guide/data_performance&gt;https://www.tensorflow.org/guide/data_performance&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>