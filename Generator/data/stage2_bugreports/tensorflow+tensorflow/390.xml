<bug id='390' author='skearnes' open_date='2015-12-01T17:26:11Z' closed_time='2017-01-05T17:44:24Z'>
	<summary>parse_example can be _much_ faster than parse_single_example</summary>
	<description>
FYI I am working with Example protos for model input, and I am learning that use tf.parse_example to parse a (shuffled) batch of serialized examples is much faster than using tf.parse_single_example prior to batching. For my particular dataset, using parse_single_example allows me to create feed_dicts with batch size 128 at about 100/min; batching the serialized Example protos and then using parse_example is running at around 3000/min.
You may want to update the &lt;denchmark-link:http://www.tensorflow.org/how_tos/reading_data/index.html#file-formats&gt;documentation&lt;/denchmark-link&gt;
 to suggest using  everywhere, as is suggested when using &lt;denchmark-link:http://www.tensorflow.org/how_tos/reading_data/index.html#sparse-input-data&gt;sparse input data&lt;/denchmark-link&gt;
.
	</description>
	<comments>
		<comment id='1' author='skearnes' date='2015-12-23T19:41:00Z'>
		Want to send us a PR to fix?
		</comment>
		<comment id='2' author='skearnes' date='2017-11-28T13:55:38Z'>
		I think parse_example is much faster than parse_single_example. in my case , speed change from 25000 sample/second to 32000 sample/second
		</comment>
	</comments>
</bug>