<bug id='36073' author='marius-sm' open_date='2020-01-20T17:32:27Z' closed_time='2020-01-21T22:29:44Z'>
	<summary>Subclassed tf.keras.Model always in training mode</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0
Python version: 3.6.8
Bazel version (if compiling from source): -
GCC/Compiler version (if compiling from source): -
CUDA/cuDNN version: 10.2
GPU model and memory: P100 16GB

Describe the current behavior
As per the &lt;denchmark-link:https://github.com/tensorflow/docs/blob/r1.14/site/en/api_docs/python/tf/keras/models/Model.md&gt;docs&lt;/denchmark-link&gt;


If you subclass Model, you can optionally have a training argument (boolean) in call, which you can use to specify a different behavior in training and inference

So I expect the dropout in MyModel to affect the result when training.
But this script outputs
&lt;denchmark-code&gt;Predicting :
[[1.]]
Training :
1.0
&lt;/denchmark-code&gt;

Describe the expected behavior
It should output
&lt;denchmark-code&gt;Predicting :
[[1.]]
Training :
0.5
&lt;/denchmark-code&gt;

Code to reproduce the issue
import tensorflow as tf
import numpy as np

class MyModel(tf.keras.Model):

  def __init__(self):

    super(MyModel, self).__init__()

    self.dense = tf.keras.layers.Dense(100, kernel_initializer="ones", trainable=False)
    self.dropout = tf.keras.layers.Dropout(0.5)


  def call(self, inputs, training=False):

    x = self.dense(inputs)

    if training:
      x = self.dropout(x, training=training)

    x = tf.reshape(tf.reduce_sum(x)/100., [1, 1]) # If we dont reshape, we get RuntimeError: Attempted to aggregate unsupported object 1.0.
    
    return x

model = MyModel()

def loss(y_true, y_pred):
    return y_pred

model.compile(optimizer="sgd", loss=loss)

x = np.ones((1, 1), dtype=np.float32)

print("Predicting :")
print(model.predict(x)) # No dropout, output is 1 as expected

print("Training :")
print(model.train_on_batch(x)) # dropout should put half of the activations of model.dense to 0, so I expect this value to be 0.5
	</description>
	<comments>
		<comment id='1' author='marius-sm' date='2020-01-21T11:14:58Z'>
		I have tried on colab with TF version 1.14, 1.15 and was able to reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/26a02592151fb81a7fdf9b8625377a97/untitled578.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='marius-sm' date='2020-01-21T22:29:37Z'>
		The code actually runs correctly, and I think you have a misunderstanding of dropout.
If you change your model body to do:
&lt;denchmark-code&gt;  def call(self, inputs, training=False):
    if training:
      return x + 1
    else:
      return x + 2

x = np.ones((1, 1), dtype=np.float32)
model.predict(x)
model.train_on_batch(x, x)
&lt;/denchmark-code&gt;

The model will correctly output 3 for predict, and 2 for train.
Your original expectation for dropout is not correct since dropout will scale up the unmasked value to maintain the numerical mean value. See &lt;denchmark-link:https://stats.stackexchange.com/questions/205932/dropout-scaling-the-activation-versus-inverting-the-dropout&gt;https://stats.stackexchange.com/questions/205932/dropout-scaling-the-activation-versus-inverting-the-dropout&lt;/denchmark-link&gt;
 for more details about the math.
		</comment>
		<comment id='3' author='marius-sm' date='2020-01-21T22:29:45Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36073&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36073&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='marius-sm' date='2020-01-22T10:09:30Z'>
		I did not know that, thank you ! I think it would be nice to mention that in the docs though.
		</comment>
	</comments>
</bug>