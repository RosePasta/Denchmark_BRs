<bug id='42825' author='cserpell' open_date='2020-08-31T14:31:48Z' closed_time='2020-09-14T20:27:41Z'>
	<summary>Custom loss using intermediate tensorflow probability layer with Eager execution</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, a colab here.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Locally, Ubuntu 20.04, otherwise current colab.
TensorFlow installed from (source or binary): See colab.
TensorFlow version (use command below): v2.3.0-0-gb36436b087
Python version: Locally, 3.8.2

Describe the current behavior
When using a custom loss based on tensorflow probability DistributionLambda, or a distribution without DistributionLambda, I cannot move into Eager execution. In the first case, the gradients are not propagated properly, so variables say they have no gradients when training, and, in the second case, the layers are symbolic tensors when creating the network, so they cannot be used in the Eager execution loss.
Describe the expected behavior
I understand the problem. What I would like is a way to have the intermediate values within the network to be used in the loss. A regularization loss is not enough, as I need the outputs of the whole network, together with the intermediate layer values in order to compute the loss.

A &lt;denchmark-link:https://colab.research.google.com/drive/1nRvN-edSeP1q1Q2lCCwXm3RV7Yunxh--?usp=sharing&gt;colab&lt;/denchmark-link&gt;
.
Other info / logs
Please see the attached colab. A useful piece of information is that I am using a class to create the model because this is part of a bigger project, and the network is much bigger, so while in the example I showed the problem in one of the last layers, this could happen for a layer that is in the middle of the network, and I would not like to add all intermediate layers as outputs of the model.
	</description>
	<comments>
		<comment id='1' author='cserpell' date='2020-09-01T00:48:27Z'>
		The only workaround I have found is to add the intermediate layers as outputs of the model and access them in the loss function, although then I have to post process outputs before giving them to further code (this is part of a bigger project). I think that this solution works, but it is definitely what I wanted.
		</comment>
		<comment id='2' author='cserpell' date='2020-09-01T14:09:00Z'>
		&lt;denchmark-link:https://github.com/cserpell&gt;@cserpell&lt;/denchmark-link&gt;

For the error " No gradients provided for any variable" please refer to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/27949&gt;#27949&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/41162&gt;#41162&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://stackoverflow.com/questions/42498876/tensorflow-no-gradients-provided-for-any-variable-and-partial-run/42504176&gt;link&lt;/denchmark-link&gt;

For the error " SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors" &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/34944&gt;#34944&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://stackoverflow.com/questions/57704771/inputs-to-eager-execution-function-cannot-be-keras-symbolic-tensors&gt;link&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://datascience.stackexchange.com/questions/64433/symbolicexception-inputs-to-eager-execution-function-cannot-be-keras-symbolic-t&gt;link1&lt;/denchmark-link&gt;
, and let us know.
		</comment>
		<comment id='3' author='cserpell' date='2020-09-01T16:52:28Z'>
		Reading many issues and questions like those mentioned, the only workaround was adding them as outputs. There is no bug, but it seems no easy way to do it without adding them as outputs.
		</comment>
		<comment id='4' author='cserpell' date='2020-09-02T10:39:45Z'>
		&lt;denchmark-link:https://github.com/cserpell&gt;@cserpell&lt;/denchmark-link&gt;

Please verify and let us know if it helps resolve the issue faced.
		</comment>
		<comment id='5' author='cserpell' date='2020-09-02T14:56:13Z'>
		As I mentioned, I added the intermediate layers as outputs to the model, and then read the outputs in the loss. Although not what I wanted, it works.
		</comment>
		<comment id='6' author='cserpell' date='2020-09-02T15:06:34Z'>
		&lt;denchmark-link:https://github.com/cserpell&gt;@cserpell&lt;/denchmark-link&gt;

Thank you for your update, can we move this to closed status as issue is resolved.
		</comment>
		<comment id='7' author='cserpell' date='2020-09-12T15:20:59Z'>
		&lt;denchmark-link:https://github.com/cserpell&gt;@cserpell&lt;/denchmark-link&gt;
 you had the same issue as mine, please take look at this gist &lt;denchmark-link:https://colab.research.google.com/drive/1qGZYgWRZoYXIeMKIEVKLf4hTo24b0CHv?usp=sharing&gt;Here&lt;/denchmark-link&gt;
, i think you'll find what you're looking for.
		</comment>
		<comment id='8' author='cserpell' date='2020-09-14T06:15:22Z'>
		&lt;denchmark-link:https://github.com/cserpell&gt;@cserpell&lt;/denchmark-link&gt;

Please update as per above comment.
		</comment>
		<comment id='9' author='cserpell' date='2020-09-14T20:27:41Z'>
		My model has probabilistic random layers, so I needed the actual output instead of re evaluations of the layers, so I concatenated the internal values as outputs of the model, and that helps me at evaluation time as well, as I can print the actual internal layers activations. Thanks anyway, I think that also helps.
		</comment>
		<comment id='10' author='cserpell' date='2020-09-14T20:27:43Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42825&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42825&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>