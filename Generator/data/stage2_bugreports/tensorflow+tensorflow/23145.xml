<bug id='23145' author='kuwii' open_date='2018-10-21T08:22:20Z' closed_time='2019-06-03T01:06:04Z'>
	<summary>Could not initialize a memory descriptor when using softmax layer</summary>
	<description>
I have both CPU and GPU version installed by Miniconda, each with a unique environment. While GPU version works fine, the CPU version seems to throw an error when I try to add a softmax layer after a convolution layer.
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Manjaro 4.14.74
TensorFlow installed from (source or binary): binary from Miniconda
TensorFlow version (use command below): 1.11.0
Python version: Python 3.6.6 :: Anaconda, Inc.
CUDA/cuDNN version: CPU version, no CUDA/cuDNN
Bazel version: N/A
GPU model and memory: N/A
Mobile device: N/A
Exact command to reproduce: python code.py

Describe the current behavior
Run the test code, the program throws AbortedError, info is:
&lt;denchmark-code&gt;AbortedError (see above for traceback): Operation received an exception:Status: 3, message: could not initialize a memory descriptor, in file tensorflow/core/kernels/mkl_softmax_op.cc:163
	 [[{{node Softmax}} = _MklSoftmax[T=DT_FLOAT, _kernel="MklOp", _device="/job:localhost/replica:0/task:0/device:CPU:0"](conv2d/BiasAdd, conv2d/BiasAdd:2)]]
&lt;/denchmark-code&gt;

Describe the expected behavior
The program should finish with no error.
Code to reproduce the issue
&lt;denchmark-code&gt;import tensorflow as tf
import numpy as np

sess = tf.Session()
inputs = tf.placeholder(dtype=tf.float32, shape=(1, 300, 300, 3))
net = tf.layers.Conv2D(filters=2, kernel_size=3)(inputs)
net = tf.nn.softmax(net, axis=-1)
sess.run(tf.global_variables_initializer())
sess.run(net, feed_dict={inputs: np.zeros(shape=(1, 300, 300, 3), dtype=np.float32)})
&lt;/denchmark-code&gt;

Other info / logs


I set up the environment by conda create -n xxx pip python=3 tensorflow


Traceback is:


&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/home/kwy/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1292, in _do_call
    return fn(*args)
  File "/home/kwy/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1277, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/home/kwy/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1367, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.AbortedError: Operation received an exception:Status: 3, message: could not initialize a memory descriptor, in file tensorflow/core/kernels/mkl_softmax_op.cc:163
         [[{{node Softmax}} = _MklSoftmax[T=DT_FLOAT, _kernel="MklOp", _device="/job:localhost/replica:0/task:0/device:CPU:0"](conv2d/BiasAdd, conv2d/BiasAdd:2)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "test.py", line 10, in &lt;module&gt;
    sess.run(net, feed_dict={inputs: np.zeros(shape=(1, 300, 300, 3), dtype=np.float32)})
  File "/home/kwy/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 887, in run
    run_metadata_ptr)
  File "/home/kwy/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1110, in _run
    feed_dict_tensor, options, run_metadata)
  File "/home/kwy/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1286, in _do_run
    run_metadata)
  File "/home/kwy/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1308, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.AbortedError: Operation received an exception:Status: 3, message: could not initialize a memory descriptor, in file tensorflow/core/kernels/mkl_softmax_op.cc:163
         [[{{node Softmax}} = _MklSoftmax[T=DT_FLOAT, _kernel="MklOp", _device="/job:localhost/replica:0/task:0/device:CPU:0"](conv2d/BiasAdd, conv2d/BiasAdd:2)]]

Caused by op 'Softmax', defined at:
  File "test.py", line 7, in &lt;module&gt;
    net = tf.nn.softmax(net, axis=-1)
  File "/home/kwy/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py", line 488, in new_func
    return func(*args, **kwargs)
  File "/home/kwy/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 1746, in softmax
    return _softmax(logits, gen_nn_ops.softmax, axis, name)
  File "/home/kwy/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 1685, in _softmax
    return compute_op(logits, name=name)
  File "/home/kwy/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py", line 7138, in softmax
    "Softmax", logits=logits, name=name)
  File "/home/kwy/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/home/kwy/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py", line 488, in new_func
    return func(*args, **kwargs)
  File "/home/kwy/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 3272, in create_op
    op_def=op_def)
  File "/home/kwy/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1768, in __init__
    self._traceback = tf_stack.extract_stack()

AbortedError (see above for traceback): Operation received an exception:Status: 3, message: could not initialize a memory descriptor, in file tensorflow/core/kernels/mkl_softmax_op.cc:163
         [[{{node Softmax}} = _MklSoftmax[T=DT_FLOAT, _kernel="MklOp", _device="/job:localhost/replica:0/task:0/device:CPU:0"](conv2d/BiasAdd, conv2d/BiasAdd:2)]]
&lt;/denchmark-code&gt;



GPU version works fine.


If i set axis to 0, 1 or 2, the program finishes with no error, but with it set to  -1 or 3, the error occurs.


If the softmax layer is added after a dense layer, it also works fine.


I've also tested on another server with CentOS 7 and a Quadro P2000, the problem still occurs. (GPU version works fine while CPU version not)


This code still not work:


&lt;denchmark-code&gt;net = tf.layers.Conv2D(filters=2, kernel_size=3, activation=tf.nn.softmax)(inputs)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='kuwii' date='2018-10-22T01:15:24Z'>
		Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.
Bazel version
GPU model and memory
Exact command to reproduce
Mobile device
		</comment>
		<comment id='2' author='kuwii' date='2018-10-22T01:57:51Z'>
		
Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.
Bazel version
GPU model and memory
Exact command to reproduce
Mobile device


Bazel version: N/A
GPU model and memory: N/A
Mobile device: N/A
Exact command to reproduce: python code.py

Thanks.
		</comment>
		<comment id='3' author='kuwii' date='2018-10-22T17:22:19Z'>
		&lt;denchmark-link:https://github.com/kawaiiQ&gt;@kawaiiQ&lt;/denchmark-link&gt;
 Please refer to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/17494&gt;#17494&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='4' author='kuwii' date='2018-10-24T16:39:46Z'>
		&lt;denchmark-link:https://github.com/kawaiiQ&gt;@kawaiiQ&lt;/denchmark-link&gt;
  - Hi, please feel free to close this issue if this no longer exists. If the problem still persists, request you to submit all the information asked by the tensorflowbutler. Thank you !
		</comment>
		<comment id='5' author='kuwii' date='2018-10-26T06:02:09Z'>
		I swiched to system python3, and it seems the problem has disappeared. However, the problem still exists on Anaconda.
And I think I have submitted all the required information.
		</comment>
		<comment id='6' author='kuwii' date='2018-10-30T17:19:15Z'>
		I can reproduce this on my system, with Tensorflow 1.11.0 and Python 3.6.7 :: Anaconda, Inc.
However, code.py can successfully run with Python 3.5.6 :: Anaconda, Inc.
		</comment>
		<comment id='7' author='kuwii' date='2018-11-02T07:45:01Z'>
		&lt;denchmark-link:https://github.com/kawaiiQ&gt;@kawaiiQ&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ikarth&gt;@ikarth&lt;/denchmark-link&gt;
 : The conda binaries are not built by the TensorFlow maintainers, but by the Anaconda folks. According to &lt;denchmark-link:https://www.anaconda.com/blog/developer-blog/tensorflow-in-anaconda/&gt;their recent blog post&lt;/denchmark-link&gt;
 those builds require Intel's MKL libraries.
The error message suggests that there is something amiss with the MKL initialization.
&lt;denchmark-link:https://github.com/wei-v-wang&gt;@wei-v-wang&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tatianashp&gt;@tatianashp&lt;/denchmark-link&gt;
 - Any quick ideas here?
		</comment>
		<comment id='8' author='kuwii' date='2018-11-02T08:20:23Z'>
		Thanks &lt;denchmark-link:https://github.com/kawaiiQ&gt;@kawaiiQ&lt;/denchmark-link&gt;
 for reporting and the reproducer and for trying TF w/ MKL-DNN.
Thanks &lt;denchmark-link:https://github.com/asimshankar&gt;@asimshankar&lt;/denchmark-link&gt;
 for including me. I could reproduce the error with build from src with TF v1.11 w/ MKL-DNN, so we take the responsibility to fix the bug. Please stay tuned. Thank you!
Eigen will exit silently (i.e. no error). TF w/ MKL-DNN errors out with the above error.
		</comment>
		<comment id='9' author='kuwii' date='2018-11-20T07:50:52Z'>
		As this issue has invited community support, please remove the assignee. Otherwise, remove the community support label. Thank you.
		</comment>
		<comment id='10' author='kuwii' date='2019-02-21T09:06:05Z'>
		&lt;denchmark-link:https://github.com/wei-v-wang&gt;@wei-v-wang&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/kawaiiQ&gt;@kawaiiQ&lt;/denchmark-link&gt;

were you able to solve this? I'm using python 3.6.5 and tensorflow 1.9
		</comment>
		<comment id='11' author='kuwii' date='2019-02-21T09:19:44Z'>
		&lt;denchmark-link:https://github.com/appyfizzA&gt;@appyfizzA&lt;/denchmark-link&gt;
 this bug seems to be fixed with latest TF. Can you please try?
&lt;denchmark-link:https://github.com/TensorFlow-MKL&gt;@TensorFlow-MKL&lt;/denchmark-link&gt;
 please help confirm/double check on this. Thank you!
		</comment>
		<comment id='12' author='kuwii' date='2019-02-21T14:25:44Z'>
		&lt;denchmark-link:https://github.com/wei-v-wang&gt;@wei-v-wang&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/appyfizzA&gt;@appyfizzA&lt;/denchmark-link&gt;

I've just tried it using python 3.6.8 and tensorflow 1.12.0 on CentOS, Windows10 and WSL(based on Debian), all installed using the lastest Miniconda.
It seems that Windows version works fine, but Linux (CentOS and Debian) version still not work.
		</comment>
		<comment id='13' author='kuwii' date='2019-02-22T15:14:10Z'>
		Yes it's definitely broken from 1.6 to 1.12, we gave up building with MKL here
		</comment>
		<comment id='14' author='kuwii' date='2019-02-22T15:19:51Z'>
		BTW I uploaded on one of the MKL related bug a snippet producing the issue if I recall correctly
		</comment>
		<comment id='15' author='kuwii' date='2019-02-25T08:23:22Z'>
		&lt;denchmark-link:https://github.com/eLvErDe&gt;@eLvErDe&lt;/denchmark-link&gt;
 sorry for the issues. Indeed 1.12 the issue is there.
The good news is I have tested, e.g. this commit id: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/07d5d08579bbbff910653a59163b4f8f180d16ac&gt;07d5d08&lt;/denchmark-link&gt;
 (master branch, tagged with v1.13.0-rc2  v1.13.0-rc1 v1.13.0-rc0), that the issue is gone. Can you please try one last time with this commit id with MKL?
Below is what I got:
2019-02-25 00:20:09.309341: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2019-02-25 00:20:09.335924: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
WARNING:tensorflow:From TF_Public_07d5d08579bbbff910653a59163b4f8f180d16ac/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
i.e. No errors were thrown.
		</comment>
		<comment id='16' author='kuwii' date='2019-06-03T01:06:04Z'>
		
@eLvErDe sorry for the issues. Indeed 1.12 the issue is there.
The good news is I have tested, e.g. this commit id: 07d5d08 (master branch, tagged with v1.13.0-rc2 v1.13.0-rc1 v1.13.0-rc0), that the issue is gone. Can you please try one last time with this commit id with MKL?
Below is what I got:
2019-02-25 00:20:09.309341: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2019-02-25 00:20:09.335924: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
WARNING:tensorflow:From TF_Public_07d5d08579bbbff910653a59163b4f8f180d16ac/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
i.e. No errors were thrown.

Sorry for my late reply. I've tried the latest version and the problem is fixed. Thanks.
		</comment>
		<comment id='17' author='kuwii' date='2019-06-03T01:06:07Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=23145&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=23145&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='18' author='kuwii' date='2019-06-05T07:07:18Z'>
		Yes it's fixed with 1.13 but again, I'd like to get commit id fixing the issue, it would make sense for us to backport the fix to 1.11....
		</comment>
		<comment id='19' author='kuwii' date='2019-07-08T22:11:45Z'>
		Hi Adam &lt;denchmark-link:https://github.com/eLvErDe&gt;@eLvErDe&lt;/denchmark-link&gt;
 , sorry for the delay. But this PR fixed the issue: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/24057&gt;#24057&lt;/denchmark-link&gt;

The exact commit id is: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/15deae5c7957861f912b12cce47956c979f2c11c&gt;15deae5&lt;/denchmark-link&gt;

Please confirm if the above resolves your issue completely. Thanks!
		</comment>
	</comments>
</bug>