<bug id='6111' author='xuancong84' open_date='2016-12-06T03:56:26Z' closed_time='2017-06-16T23:41:48Z'>
	<summary>Flawed memory management: allow_growth=True consumes more memory, causing out-of-memory</summary>
	<description>
To prevent tensorflow (TF) from allocating the totality of graphic memory, I always use the following options when creating sessions:
&lt;denchmark-code&gt;config = tf.ConfigProto()
config.gpu_options.allow_growth = True
sess = tf.Session(config=config)
&lt;/denchmark-code&gt;

However, doing so causes some experiments to run out of memory while not doing so will not cause memory overflow. For example, when running experiments involving RNN, such as translate.py or ptb_word_lm.py in the sample code, if I specify allow_growth=True, I always encounter the following:
&lt;denchmark-code&gt;Training Epoch 0 ; learning_rate= 0.002 :                                                                                                
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 2631 get requests, put_count=2230 evicted_count=1000 eviction_rate=0.44843 and unsatisfied allocation rate=0.570506                                                                              
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 100 to 110                                     
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 2631 get requests, put_count=2546 evicted_count=1000 eviction_rate=0.392773 and unsatisfied allocation rate=0.421133                                                                             
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 256 to 281                                     
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 9384 get requests, put_count=9389 evicted_count=1000 eviction_rate=0.106508 and unsatisfied allocation rate=0.112319                                                                             
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 655 to 720                                     
E tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  
E tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.80G (1932735232 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  
E tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.62G (1739461632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  
E tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.46G (1565515520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  
E tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.31G (1408964096 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  
E tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.18G (1268067840 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  
&lt;/denchmark-code&gt;

However, without specifying allow_growth=True, I can run it successfully. Moreover, the OOM occurs only after going through some epoches in the training data, not right from the beginning.
In principle, for an ideal memory manager, whether OOM will occur should not depends on whether memory is pre-allocated in one go or allocated step-by-step dynamically. Thus, Tensorflow's low-level memory management code must be flawed in one way or another.
Below are my system info:
&lt;denchmark-code&gt;Operating System:
Ubuntu 14.04.5 LTS

Installed version of CUDA and cuDNN:
/usr/local/cuda-8.0
cudnn-8.0-linux-x64-v5.1.tgz

It is installed from binary pip package

A link to the pip package you installed:
https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl

The output from python -c "import tensorflow; print(tensorflow.version)"
xuancong@wxc-i2r:~/projects/tf-rnnlm$ python -c "import tensorflow; print(tensorflow.version)"
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally
0.11.0
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='xuancong84' date='2016-12-09T23:22:41Z'>
		Could you post the entire log somewhere? We need to find out how much memory were requested when OOM happens? Also we would need a small model to reproduce the problem.
allow_growth is expected to introduce some level of fragmentation. So if your model is running very close to GPU memory limit, it is possible that allow_growth can push you over the limit.
The current allocator design is a trade-off between performance and efficiency, and sure it is not perfect. That's why the option exists. It will get better as we keep improving it. If the system still has a lot of free memory, and the OOM still happens, then it is a more serious problem than if the model is already running close to the limit, where we would recommend not to use allow_growth.
		</comment>
		<comment id='2' author='xuancong84' date='2016-12-16T08:06:32Z'>
		In principle, memory fragmentation should only affect the speed rather than the total available memory.
You can reproduce the problem by running translate.py or ptb_word_lm.py in the sample code, by setting a large enough vocabulary size for OOM to occur.
		</comment>
		<comment id='3' author='xuancong84' date='2016-12-16T16:24:43Z'>
		&gt; In principle, memory fragmentation should only affect the speed rather
than the total available memory.

I am not sure about that.

&lt;denchmark-link:https://en.wikipedia.org/wiki/Fragmentation_(computing)&gt;https://en.wikipedia.org/wiki/Fragmentation_(computing)&lt;/denchmark-link&gt;


"In computer storage, fragmentation is a phenomenon in which storage space
is used inefficiently, reducing capacity or performance and often both."
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Fri, Dec 16, 2016 at 12:07 AM, xuancong84 ***@***.***&gt; wrote:
 In principle, memory fragmentation should only affect the speed rather
 than the total available memory.

 You can reproduce the problem by running *translate.py* or
 *ptb_word_lm.py* in the sample code, by setting a large enough vocabulary
 size for OOM to occur.

 —
 You are receiving this because you were assigned.
 Reply to this email directly, view it on GitHub
 &lt;#6111 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/APAgTvY8fOvc7dxm-mh7mW0VYhMYdcMYks5rIkbCgaJpZM4LE_5n&gt;
 .



		</comment>
		<comment id='4' author='xuancong84' date='2017-03-08T09:02:09Z'>
		Hi,
I am testing the TF Slim package, fine-tuning the flowers dataset using the instructions in &lt;denchmark-link:https://github.com/tensorflow/models/tree/master/slim&gt;https://github.com/tensorflow/models/tree/master/slim&lt;/denchmark-link&gt;

To allow for memory growth, I use the following code:
    session_config.gpu_options.allow_growth = True session_config.gpu_options.allocator_type = 'BFC' session_config.gpu_options.per_process_gpu_memory_fraction = 0.4 # gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction) # Run the training: final_loss = slim.learning.train( train_op, logdir=train_dir, init_fn=get_init_fn(), number_of_steps=2000, session_config=session_config)
There are mainly two issues.


As the GPU memory is not big enough, I want to try to allow for GPU growth, so that the process does not crash. The code I am using is the one below, however, it doesn't make any difference at all. Is allow_growth and/or per_process_gpu_memory_fraction really supported for TF slim or not? I suppose this will be anyways an issue, given the original message on the thread. So, I guess all left to ask Is whether there any development on the issue (to begin with, is it really an issue)?


Is all the GPU needed for the allocation of the intermediate activation variables? Because the inception model is about 100 Mb, my total GPU memory (after killing also the X server) is 1.8 Gb, so it feels somewhat weird that the model needs in the end 18X more GPU RAM than the inception model size. When using Caffe with Alexnet (~350 Mb), there was no problem at all, no matter the batch size. I guess the way the gradients are computed is by allocating memory like: #MODEL_PARAMETERS x BATCH_SIZE, since for smaller batch sizes the code works. Given that batch gradients are cumultative over the batch samples, isn't this -although arguably faster- somehow redundant (?).


I will try to get the RAM metadata log file, if that is of any help.
		</comment>
		<comment id='5' author='xuancong84' date='2017-06-16T23:41:48Z'>
		Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you.
		</comment>
	</comments>
</bug>