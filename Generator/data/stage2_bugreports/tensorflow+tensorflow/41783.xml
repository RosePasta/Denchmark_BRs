<bug id='41783' author='aleksandarPlu' open_date='2020-07-27T19:55:01Z' closed_time='2020-08-18T12:27:35Z'>
	<summary>TensorFlow 2.2 using tf.float16 executes only on CPU</summary>
	<description>
I'm using &lt;denchmark-link:https://github.com/vcadillog/PPO-Mario-Bros-Tensorflow-2&gt;this code&lt;/denchmark-link&gt;
 as a starting point for one of my projects. The only modification I did so far is switching to OpenAI Gym Atari because I'm running it on Windows.
The problem I'm having is when I use tf.keras.backend.set_floatx(tf.float16) the code gets stuck executing on CPU. I also changed all the explicit casts throughout the code. The idea is to utilize the tensor cores on my GPU. This only happens with this code. All my other projects work fine.
I'm running it on TF 2.2. I tried removing tf.function annotations. I tried messing around with casts. For example, leaving all the data in fp32 and using fp16 weights in the model. Nothing helped. Honestly, I haven't tried much because I can't find any information on the problem and have no idea where to start debugging. There are no errors and everything works fine, except it runs on CPU instead of GPU. The only difference is in the mess that TF spews out when initializing I get this message:
W tensorflow/compiler/jit/xla_device.cc:398] XLA_GPU and XLA_CPU devices are deprecated and will be removed in subsequent releases. Instead, use either @tf.function(experimental_compile=True) for must-compile semantics, or run with TF_XLA_FLAGS=--tf_xla_auto_jit=2 for auto-clustering best-effort compilation.
I tried both options proposed in the message and nothing happened.
Also, I have tried &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/device&gt;tf.device&lt;/denchmark-link&gt;
 but nothing changed
I really need this to work. Not just for the memory and performance optimizations but part of my project is benchmarking the difference between the two.
Any help would be appreciated or at least if someone could point me in the direction where to look for the information.
Thank you.
	</description>
	<comments>
		<comment id='1' author='aleksandarPlu' date='2020-07-28T04:28:12Z'>
		&lt;denchmark-link:https://github.com/aleksandarPlu&gt;@aleksandarPlu&lt;/denchmark-link&gt;

Request you to fill &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/new/choose&gt;issue template.&lt;/denchmark-link&gt;
.
Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.
Request you to share simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!
		</comment>
		<comment id='2' author='aleksandarPlu' date='2020-07-28T14:25:00Z'>
		&lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;

Thanks for you reply. All other project are working fine, except this one. Unable to create any simpler version to reproduce this issue.
Platform details are:  TF 2.2, Win 10 (1909), nVidia driver 451.67, CUDA 10.1, Python 3.7, i7-4770k, RTX 2060 Super
		</comment>
		<comment id='3' author='aleksandarPlu' date='2020-07-29T03:53:30Z'>
		&lt;denchmark-link:https://github.com/aleksandarPlu&gt;@aleksandarPlu&lt;/denchmark-link&gt;
 Can you try using  This might help you to speed up training by 3 times on a modern GPU as mentioned &lt;denchmark-link:https://www.tensorflow.org/guide/mixed_precision#overview&gt;here&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='aleksandarPlu' date='2020-07-29T21:27:10Z'>
		&lt;denchmark-link:https://github.com/gowthamkpr&gt;@gowthamkpr&lt;/denchmark-link&gt;
 thank you for your suggestion. Now at least was able to run the code on tf16 with your proposed solution. But we did not get any improvement in speed or memory. It is the same as when I used tf32
		</comment>
		<comment id='5' author='aleksandarPlu' date='2020-08-04T11:46:19Z'>
		&lt;denchmark-link:https://github.com/aleksandarPlu&gt;@aleksandarPlu&lt;/denchmark-link&gt;
 Glad you were able to run the code with proposed solution. As mentioned in the comment, it might help you speed up training but I didn't promise it would.
As mentioned &lt;denchmark-link:https://www.tensorflow.org/guide/mixed_precision#overview&gt;here&lt;/denchmark-link&gt;
, NVIDIA GPUs can run operations in float16 faster than in float32. Therefore, these lower-precision dtypes should be used whenever possible on those devices. However, variables and a few computations should still be in float32 for numeric reasons so that the model trains to the same quality. The Keras mixed precision API allows you to use a mix of either float16 or bfloat16 with float32, to get the performance benefits from float16/bfloat16 and the numeric stability benefits from float32.
Some ops are performed on CPU but not on GPU, this might be the reason for not seeing any change in speed.
Moreover, As this issue is not a bug pr feature request can you please post this in stack overflow where community can help you. Thanks!
		</comment>
		<comment id='6' author='aleksandarPlu' date='2020-08-11T12:21:05Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='7' author='aleksandarPlu' date='2020-08-18T12:27:33Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='8' author='aleksandarPlu' date='2020-08-18T12:27:36Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41783&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41783&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>