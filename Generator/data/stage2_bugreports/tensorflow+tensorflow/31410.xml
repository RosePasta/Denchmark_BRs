<bug id='31410' author='rishabhsahrawat' open_date='2019-08-07T12:04:25Z' closed_time='2019-08-16T18:34:33Z'>
	<summary>MirrorStrategy() fills up memory of GPU that is not selected for training in TF 2.0.</summary>
	<description>
I am using  in order to train a  model. I have 2 T4 GPUs from Google available. I want  to train my model consisting LSTM layer on only one GPU i.e. . So, in order to select it, I define  inside  as suggested &lt;denchmark-link:https://www.tensorflow.org/guide/distribute_strategy#mirroredstrategy&gt;here&lt;/denchmark-link&gt;
, now when I run the file for training, the memory of first GPU(/gpu:0) is also filling up completely however, during training only GPU (/gpu:1) is being utilized completely. My question is why is this happening since I only want to utilize GPU (/gpu:1)? In this case, first GPU is useless.
However, if I select the first GPU(/gpu:0) for training then it uses only 112MB of memory of the second GPU(/gpu:1).
&lt;denchmark-code&gt;| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla T4            On   | 00000000:00:04.0 Off |                    0 |
| N/A   60C    P0    28W /  70W |  14449MiB / 15079MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla T4            On   | 00000000:00:05.0 Off |                    0 |
| N/A   75C    P0    74W /  70W |  14517MiB / 15079MiB |     95%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     20576      C   python                                     14439MiB |
|    1     20576      C   python                                     14507MiB |
+-----------------------------------------------------------------------------+
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='rishabhsahrawat' date='2019-08-07T13:11:59Z'>
		&lt;denchmark-link:https://github.com/rishabhsahrawat&gt;@rishabhsahrawat&lt;/denchmark-link&gt;
 are you executing any other op before you declare the distributed strategy ?
is this still happening if you declare distribute strategy before anything else?
It would be easier to comment on this if you could provide a reproducible code snippet!
		</comment>
		<comment id='2' author='rishabhsahrawat' date='2019-08-07T13:57:47Z'>
		Hi &lt;denchmark-link:https://github.com/srihari-humbarwadi&gt;@srihari-humbarwadi&lt;/denchmark-link&gt;
 , thank you for your questions. No, I am not declaring any op as far as I know. I am only creating dataset for training and testing.
Yes, it is still happening even if I define distribute strategy right after defining modules at the top of the file.
If you use the code from this &lt;denchmark-link:https://www.tensorflow.org/beta/tutorials/load_data/text#split_the_dataset_into_text_and_train_batches&gt;tutorial&lt;/denchmark-link&gt;
 after defining the model inside the  like
&lt;denchmark-code&gt;mirrored_strategy = tf.distribute.MirroredStrategy(devices = ['/gpu:1'])# for using second GPU

with mirrored_strategy.scope():
  model = tf.keras.Sequential()
  model.add(tf.keras.layers.Embedding(17179, 64))
  model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, activation= 'tanh', recurrent_activation= 'sigmoid', recurrent_dropout = 0, unroll = False, use_bias= True)))
  # One or more dense layers.
  # Edit the list in the `for` line to experiment with layer sizes.
  for units in [64, 64]:
    model.add(tf.keras.layers.Dense(units, activation='relu'))

  # Output layer. The first argument is the number of labels.
  model.add(tf.keras.layers.Dense(3, activation='softmax'))
  model.compile(optimizer='adam',
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy'])
&lt;/denchmark-code&gt;

, you will be able to see this behaviour. You can do it on your own just to be sure otherwise, here is the full script.
&lt;denchmark-code&gt;import tensorflow as tf
import tensorflow_datasets as tfds
import os
DIRECTORY_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'
FILE_NAMES = ['cowper.txt', 'derby.txt', 'butler.txt']

for name in FILE_NAMES:
  text_dir = tf.keras.utils.get_file(name, origin=DIRECTORY_URL+name)
  
parent_dir = os.path.dirname(text_dir)

print parent_dir
def labeler(example, index):
  return example, tf.cast(index, tf.int64)  

labeled_data_sets = []

for i, file_name in enumerate(FILE_NAMES):
  lines_dataset = tf.data.TextLineDataset(os.path.join(parent_dir, file_name))
  labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))
  labeled_data_sets.append(labeled_dataset)

BUFFER_SIZE = 50000
BATCH_SIZE = 64
TAKE_SIZE = 5000
all_labeled_data = labeled_data_sets[0]
for labeled_dataset in labeled_data_sets[1:]:
  all_labeled_data = all_labeled_data.concatenate(labeled_dataset)
  
all_labeled_data = all_labeled_data.shuffle(
    BUFFER_SIZE, reshuffle_each_iteration=False)

for ex in all_labeled_data.take(1):
  print(ex)


tokenizer = tfds.features.text.Tokenizer()

vocabulary_set = set()
for text_tensor, _ in all_labeled_data:
  some_tokens = tokenizer.tokenize(text_tensor.numpy())
  vocabulary_set.update(some_tokens)

vocab_size = len(vocabulary_set)
print 'coab size: ',vocab_size

encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)
example_text = next(iter(all_labeled_data))[0].numpy()
print(example_text)
encoded_example = encoder.encode(example_text)
print(encoded_example)

def encode(text_tensor, label):
  encoded_text = encoder.encode(text_tensor.numpy())
  #encoded_text = encoded_text[:10]
  return encoded_text, label

def encode_map_fn(text, label):
  return tf.py_function(encode, inp=[text, label], Tout=(tf.int64, tf.int64))

all_encoded_data = all_labeled_data.map(encode_map_fn)

train_data = all_encoded_data.skip(TAKE_SIZE).shuffle(BUFFER_SIZE)
train_data = train_data.padded_batch(BATCH_SIZE, padded_shapes=([-1],[]))
#train_data= train_data.batch(64)
test_data = all_encoded_data.take(TAKE_SIZE)

test_data = test_data.padded_batch(BATCH_SIZE, padded_shapes=([-1],[]), drop_remainder = False)
#test_data = test_data.batch(64)
sample_text, sample_labels = next(iter(test_data))
print '**Test Samples**'
print sample_text[0], sample_labels[0]

vocab_size += 1
###Distributed Strategy###
mirrored_strategy = tf.distribute.MirroredStrategy(devices = ['/gpu:1'])
with mirrored_strategy.scope():
  model = tf.keras.Sequential()
  model.add(tf.keras.layers.Embedding(vocab_size, 64))
  model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, activation= 'tanh', recurrent_activation= 'sigmoid', recurrent_dropout = 0, unroll = False, use_bias= True)))
  # One or more dense layers.
  # Edit the list in the `for` line to experiment with layer sizes.
  for units in [64, 64]:
    model.add(tf.keras.layers.Dense(units, activation='relu'))

  # Output layer. The first argument is the number of labels.
  model.add(tf.keras.layers.Dense(3, activation='softmax'))
  model.compile(optimizer='adam',
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy'])
###Strategy  end###
steps = 5000/64

print steps

model.fit(train_data, epochs=10, validation_steps = steps, validation_data=test_data)
&lt;/denchmark-code&gt;

Thank you,
Rishabh Sahrawat
		</comment>
		<comment id='3' author='rishabhsahrawat' date='2019-08-07T14:20:11Z'>
		The distribute strategy declaration should be done before you use any tf operations (like tf.data, tf.keras etc). Please post what happens when you declare this at the start of your code.
&lt;denchmark-code&gt;mirrored_strategy = tf.distribute.MirroredStrategy(devices = ['/gpu:1'])
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='rishabhsahrawat' date='2019-08-07T14:44:51Z'>
		I already mentioned it in my last comment. But another try is here.

Yes, it is still happening even if I define distribute strategy right after defining modules at the top of the file.

&lt;denchmark-code&gt;import tensorflow as tf
import tensorflow_datasets as tfds
import os
mirrored_strategy = tf.distribute.MirroredStrategy(devices = ['/gpu:1'])
--rest is same like script in last comment (of course after removing the above line from it)--
&lt;/denchmark-code&gt;

The result is still the same, it is using both GPUs for memory but utilising second.
&lt;denchmark-code&gt;| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla T4            On   | 00000000:00:04.0 Off |                    0 |
| N/A   44C    P0    26W /  70W |  14449MiB / 15079MiB |      2%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla T4            On   | 00000000:00:05.0 Off |                    0 |
| N/A   43C    P0    31W /  70W |  14827MiB / 15079MiB |     40%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     16200      C   python                                     14439MiB |
|    1     16200      C   python                                     14817MiB |
+-----------------------------------------------------------------------------+
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='rishabhsahrawat' date='2019-08-07T15:28:27Z'>
		Just add the tf.data ops under the distribute strategy scope
		</comment>
		<comment id='6' author='rishabhsahrawat' date='2019-08-08T13:32:22Z'>
		Unfortunately, it still has the same behaviour.
I tried this exact code, on Google Colab with GPU and the memory it takes there is only 661MiB but when I run it on the two GPUs from Google, it fills up the whole memory. My TF versions are same Beta1. I also tried on Latest nightly build. I request if someone can use this exact code for running on two GPUs and see the memory usage.
		</comment>
		<comment id='7' author='rishabhsahrawat' date='2019-08-15T17:50:28Z'>
		I am surprised that setting the scope:gpu1 at the very top doesn't work as well.
Could you try running with the environmental variable CUDA_VISIBLE_DEVICES=1 (see more info &lt;denchmark-link:http://stackoverflow.com/q/37893755&gt;http://stackoverflow.com/q/37893755&lt;/denchmark-link&gt;
), please?
		</comment>
		<comment id='8' author='rishabhsahrawat' date='2019-08-16T08:21:09Z'>
		Hi &lt;denchmark-link:https://github.com/isaprykin&gt;@isaprykin&lt;/denchmark-link&gt;
 , thank you for your suggestion. I tried it and can confirm this works however one must not define  parameter of the strategy otherwise it will throw error. I think this makes this parameter useless since the way to choose a GPU doesn't depend on  parameter. Also I would request you to add  this in the documentation.
		</comment>
		<comment id='9' author='rishabhsahrawat' date='2019-08-16T09:26:32Z'>
		I am facing a weird situation. When I train the same model on one GPU, it takes 45 secs to process 100 batches during first epoch, but when I use both GPUs for training, it takes 61 secs for the same however it should train faster on more number fo GPUs. Do you guys think there is a reason for that? &lt;denchmark-link:https://github.com/isaprykin&gt;@isaprykin&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/oanush&gt;@oanush&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='rishabhsahrawat' date='2019-08-16T18:34:25Z'>
		Are both GPUs the same?
Profiling tutorial is here: &lt;denchmark-link:https://www.tensorflow.org/tensorboard/r2/tensorboard_profiling_keras&gt;https://www.tensorflow.org/tensorboard/r2/tensorboard_profiling_keras&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='rishabhsahrawat' date='2019-08-16T18:34:35Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=31410&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=31410&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='rishabhsahrawat' date='2019-08-19T07:27:13Z'>
		Yes, both GPUs are same Tesla T4s from Google. I also tried using 2 K80s but it throws OOM errors then.
I already tried using tf.data.prefetch() but it doesn't show any big improvement, with the use of more number fo GPUs it gets slower in training.
		</comment>
	</comments>
</bug>