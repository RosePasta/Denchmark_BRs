<bug id='27119' author='EliasHasle' open_date='2019-03-25T21:04:04Z' closed_time='2019-03-28T00:34:23Z'>
	<summary>window + flat_map fails on dataset of tuples</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home edition
TensorFlow installed from (source or binary): pip install tensorflow-gpu
TensorFlow version (use command below): b'v1.12.0-rc2-3-ga6d8ffae09' 1.12.0
Python version: 3.6.7
CUDA/cuDNN version: Cuda compilation tools, release 9.0, V9.0.176, CuDNN 7.4.1, I believe
GPU model and memory: NVIDIA GTX 1070, 8 GiB memory (maybe 6, as TF is a bit unclear on that point)

Describe the current behavior
On a range dataset, window followed by flat_map works as expected, but on a dataset of tuples (originally encountered using a CSV dataset), flat_map fails, complaining that the input argument to the mapped function has multiple components.
Describe the expected behavior
I expected flat_map to treat the windows as datasets, allowing me to perform dataset operations such as batch on them.
Code to reproduce the issue
&lt;denchmark-code&gt;#Simple window batch test
import tensorflow as tf
data = tf.data.Dataset.range(0,10)
data = data.map(lambda *x: (x[0],x[0]+1))
#Each entry is now a tuple of (n,n+1)
#Without the mapping to tuple, window+flat_map works.
data = data.window(3)
#Each entry is now a window dataset
#map fails, because the dataset is nested
#data.map(lambda x: print(x))
#flat_map fails because x is a tuple, not a dataset
data = data.flat_map(lambda x: x.batch(3))
&lt;/denchmark-code&gt;

Other info / logs

Traceback (most recent call last):
File "window_batch_test.py", line 14, in 
data = data.flat_map(lambda x: x.batch(3))
File "C:\Users\elias\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py", line 1070, in flat_map
return FlatMapDataset(self, map_func)
File "C:\Users\elias\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py", line 2677, in init
experimental_nested_dataset_support=True)
File "C:\Users\elias\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py", line 1860, in init
self._function.add_to_graph(ops.get_default_graph())
File "C:\Users\elias\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\function.py", line 479, in add_to_graph
self._create_definition_if_needed()
File "C:\Users\elias\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\function.py", line 335, in _create_definition_if_needed
self._create_definition_if_needed_impl()
File "C:\Users\elias\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\function.py", line 344, in _create_definition_if_needed_impl
self._capture_by_value, self._caller_device)
File "C:\Users\elias\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\function.py", line 864, in func_graph_from_py_func
outputs = func(*func_graph.inputs)
File "C:\Users\elias\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py", line 1794, in tf_data_structured_function_wrapper
ret = func(*nested_args)
TypeError: () takes 1 positional argument but 2 were given

The (regretably) deprecated sliding_window_batch seems to work, though. It is much more convenient for my purposes to just get the windows as batches.
	</description>
	<comments>
		<comment id='1' author='EliasHasle' date='2019-03-28T00:34:22Z'>
		This works as intended. If the input to window is a dataset of nested elements, then the output of window will be a dataset of nested datasets of (flat) elements.
&lt;denchmark-code&gt;import tensorflow as tf

tf.enable_v2_behavior()

data = tf.data.Dataset.range(0,10)
data = data.map(lambda *x: (x[0],x[0]+1))
data = data.window(3)
data = data.flat_map(lambda x, y: tf.data.Dataset.zip((x.batch(3), y.batch(3))))

for elem in data:
  print(elem)
&lt;/denchmark-code&gt;

produces
&lt;denchmark-code&gt;(&lt;tf.Tensor: id=35, shape=(3,), dtype=int64, numpy=array([0, 1, 2])&gt;, &lt;tf.Tensor: id=36, shape=(3,), dtype=int64, numpy=array([1, 2, 3])&gt;)
(&lt;tf.Tensor: id=39, shape=(3,), dtype=int64, numpy=array([3, 4, 5])&gt;, &lt;tf.Tensor: id=40, shape=(3,), dtype=int64, numpy=array([4, 5, 6])&gt;)
(&lt;tf.Tensor: id=43, shape=(3,), dtype=int64, numpy=array([6, 7, 8])&gt;, &lt;tf.Tensor: id=44, shape=(3,), dtype=int64, numpy=array([7, 8, 9])&gt;)
(&lt;tf.Tensor: id=47, shape=(1,), dtype=int64, numpy=array([9])&gt;, &lt;tf.Tensor: id=48, shape=(1,), dtype=int64, numpy=array([10])&gt;)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='EliasHasle' date='2019-03-28T00:34:24Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=27119&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=27119&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='EliasHasle' date='2019-03-28T07:45:05Z'>
		Thank you for answer, and for providing what appears to be a way to solve the problem. I suppose that for a dataset with many columns the expression would be something like:
&lt;denchmark-code&gt;data.flat_map(lambda *x: tf.data.Dataset.zip(tuple(col.batch(batch_size) for col in x)))
&lt;/denchmark-code&gt;

which worked on my CSV data. :-)
What I don't get is that x cannot just be batched directly. x is a tuple, but was supposed to be a dataset. (flat_map doc says that it applies the function to every element (dataset), and then flattens the result (packing out the datasets).) It should at least be a custom tuple with dataset methods added, as far as I can tell.
The old sliding window batch works well. And, while perhaps being less flexible, it solves a common valid usecase that should not be made less available. The "instructions for updating" that show up in the terminal when I use sliding_window_batch says nothing about the trick you gave me here, and the recommended solution does not work.
To conclude, I think this issue should not be closed yet. But thank you again for providing a workaround.
		</comment>
		<comment id='4' author='EliasHasle' date='2020-10-11T13:23:18Z'>
		What about dict, e.g when the input to window() was already a dictionary? flat_map behaves unintuitively here and doesn't get the job done.
		</comment>
	</comments>
</bug>