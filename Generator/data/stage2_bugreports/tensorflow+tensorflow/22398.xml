<bug id='22398' author='eamartin' open_date='2018-09-19T23:37:32Z' closed_time='2018-10-25T23:47:07Z'>
	<summary>CUDA implementation of BiasAddGrad op is non-determinstic</summary>
	<description>
I'm running TensorFlow 1.5.0 on a K80 GPU on Python 2.7
Failing test case:
from __future__ import print_function
import hashlib
import numpy as np
import tensorflow as tf

np.random.randn(2018)
tf.set_random_seed(2018)

X = np.random.randn(1024, 50).astype(np.float32)
b = tf.get_variable('bias', [50])
z = tf.nn.bias_add(X, b)

grad = tf.gradients(z*z, b)[0]

init_op = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init_op)

    run1 = sess.run(grad)
    run2 = sess.run(grad)

    print(np.all(run1 == run2))
    print(np.max(np.abs(run1 - run2)))

    dohash = lambda X: hashlib.md5(X.tostring()).hexdigest()
    print(dohash(run1))
    print(dohash(run2))
Outputs
&lt;denchmark-code&gt;False
6.10352e-05
b489a1d659518b2ae9213f5a21e35df2
187a57d563468e59ba5f9d9cf51ca5cb
&lt;/denchmark-code&gt;

This bug still exists in master, because the code in master uses the unsafe CUDA atomic floating point add in several places. See &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/abc55107eb7a03fe3d83f95fd5e1b8e4def90826/tensorflow/core/kernels/bias_op_gpu.cu.cc&gt;https://github.com/tensorflow/tensorflow/blob/abc55107eb7a03fe3d83f95fd5e1b8e4def90826/tensorflow/core/kernels/bias_op_gpu.cu.cc&lt;/denchmark-link&gt;

If TensorFlow will be ever be fully determinstic, atomic floating point add should never be used (it is inherently non-determinstic due to non-associativity of floating point).
Notably, Keras's  layer uses , so all networks that use this layer are non-reproducible. This is relevant to &lt;denchmark-link:https://github.com/keras-team/keras/issues/2280&gt;keras-team/keras#2280&lt;/denchmark-link&gt;
 .
This can currently be avoided by using tf.add instead of tf.nn.bias_add at a slightly performance hit. The correct fix would be refactor the BiasAddGrad op to use a (deterministic) reduction tree.
edit with issue template fields:
Have I written custom code: Python test case, see above
OS Platform and Distribution: RHEL 7.5 (Linux)
TensorFlow installed from: source
TensorFlow version: 1.5.0
Bazel version: unknown
CUDA/cuDNN version: CUDA 8.0.61, cuDNN v6
GPU model and memory: Nvidia K80, 12GB memory
Exact command to reproduce: run the above script
Mobile device: NA
	</description>
	<comments>
		<comment id='1' author='eamartin' date='2018-09-20T06:44:24Z'>
		Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.
Have I written custom code
OS Platform and Distribution
TensorFlow installed from
TensorFlow version
Bazel version
CUDA/cuDNN version
GPU model and memory
Exact command to reproduce
Mobile device
		</comment>
		<comment id='2' author='eamartin' date='2018-09-22T00:43:30Z'>
		&lt;denchmark-link:https://github.com/eamartin&gt;@eamartin&lt;/denchmark-link&gt;
 A small amount of non-determinstism in TensorFlow computations is expected on GPU. Ongoing efforts are put forth and FR is in place. &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/18096&gt;18096&lt;/denchmark-link&gt;
 Latest developments from algorithms and hardware architecture perspectives reaped success where challenges remain.
Please resort to workarounds, e.g. tf.add for now until more concrete solutions come forth.
		</comment>
		<comment id='3' author='eamartin' date='2018-09-29T02:57:45Z'>
		Closing this issue for now, will use FR to track progress.
		</comment>
		<comment id='4' author='eamartin' date='2018-09-30T17:27:28Z'>
		&lt;denchmark-link:https://github.com/wt-huang&gt;@wt-huang&lt;/denchmark-link&gt;
 This issue is completely separate from &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/18096&gt;#18096&lt;/denchmark-link&gt;
 as it has nothing to do with cuDNN. Is this the FR (= feature request?) that you are referring to?
The non-determinism here is in the TensorFlow source itself, not a 3rd party library. Adding support for configuring cuDNN determinism will resolve &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/18096&gt;#18096&lt;/denchmark-link&gt;
 but will do nothing to fix this issue. I fear merging this issue with an unrelated cuDNN issue will lead to this bug becoming untracked and neglected.
		</comment>
		<comment id='5' author='eamartin' date='2018-10-01T16:57:57Z'>
		&lt;denchmark-link:https://github.com/eamartin&gt;@eamartin&lt;/denchmark-link&gt;
 This problem is a subset of the non-determinism issue resides in cuDNN. When the feature request is completed and the solution implemented will be transferred to 
		</comment>
		<comment id='6' author='eamartin' date='2018-10-01T17:33:05Z'>
		This problem is a non-determinism issue, but is separate from non-determinism issues with cuDNN.
The solution to cuDNN non-determinism is to pass the correct flags into the library specifying which algorithm to use. See &lt;denchmark-link:https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#reproducibility&gt;https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#reproducibility&lt;/denchmark-link&gt;

The solution of "set the right cudDNN flags" cannot be transferred to non-cuDNN issues such as this. Resolving this issue involves modifying CUDA kernels in the TF source. Resolving &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/18096&gt;#18096&lt;/denchmark-link&gt;
 probably does not.
Additionally, this issue affects more users than &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/18096&gt;#18096&lt;/denchmark-link&gt;
 as  is used for fully connected layers (including through Keras) while cuDNN primarily affects CNNs.
An appropriate "remove non-deterministic behavior" parent issue should not be solely about cuDNN in my opinion. It should include mention of removing all uses of atomic floating point instructions in the TF source.
		</comment>
		<comment id='7' author='eamartin' date='2018-10-03T04:08:58Z'>
		&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/2732&gt;#2732&lt;/denchmark-link&gt;
 is a more relevant issue than &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/18096&gt;#18096&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='eamartin' date='2018-10-18T18:58:14Z'>
		Nagging Assignee &lt;denchmark-link:https://github.com/wt-huang&gt;@wt-huang&lt;/denchmark-link&gt;
: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='9' author='eamartin' date='2018-10-19T00:59:13Z'>
		&lt;denchmark-link:https://github.com/eamartin&gt;@eamartin&lt;/denchmark-link&gt;
 Yes, atomic floating point addition is one of the primary triggers of non-determinism, which happens to be prevalent in CUDA regime. Often times they go hand it hand. &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/2732&gt;#2732&lt;/denchmark-link&gt;
 is related to this issue so are a few others.
We are very well aware of this issue and progress is underway.
		</comment>
		<comment id='10' author='eamartin' date='2018-11-03T22:07:18Z'>
		&lt;denchmark-link:https://github.com/wt-huang&gt;@wt-huang&lt;/denchmark-link&gt;
 Has this issue been solved? Why has it been closed?
		</comment>
		<comment id='11' author='eamartin' date='2018-11-05T19:50:24Z'>
		&lt;denchmark-link:https://github.com/eamartin&gt;@eamartin&lt;/denchmark-link&gt;
 We are aware of this issue, fix is in the progress.
		</comment>
		<comment id='12' author='eamartin' date='2018-11-10T18:53:44Z'>
		Is there a PR or a branch on Github for this in-progress fix?
I'd prefer to leave this issue open until the bug is fixed in master.
		</comment>
		<comment id='13' author='eamartin' date='2020-04-02T10:50:37Z'>
		&lt;denchmark-link:https://github.com/eamartin&gt;@eamartin&lt;/denchmark-link&gt;
, this issue was resolved, in Oct 2019, with PR &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/31465&gt;31465&lt;/denchmark-link&gt;
. In TensorFlow version 2.1 and onwards, you can enable the solution by setting the environment variable  to "1" or "true".
For more information, see &lt;denchmark-link:https://github.com/NVIDIA/tensorflow-determinism&gt;https://github.com/NVIDIA/tensorflow-determinism&lt;/denchmark-link&gt;

		</comment>
		<comment id='14' author='eamartin' date='2020-05-02T22:43:52Z'>
		&lt;denchmark-link:https://github.com/duncanriach&gt;@duncanriach&lt;/denchmark-link&gt;
 thank you very much for that fix as well as all of your other work on TensorFlow determinism!
		</comment>
		<comment id='15' author='eamartin' date='2020-05-04T23:46:43Z'>
		You're welcome, &lt;denchmark-link:https://github.com/eamartin&gt;@eamartin&lt;/denchmark-link&gt;
. Thank you for the appreciation.
		</comment>
	</comments>
</bug>