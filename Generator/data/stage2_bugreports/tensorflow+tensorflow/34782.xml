<bug id='34782' author='byronyi' open_date='2019-12-03T05:11:42Z' closed_time='2020-03-16T19:35:40Z'>
	<summary>[tf.keras] Mixed precision policy "mixed_bfloat16" not supported in Keras compile</summary>
	<description>
System information

Have I written custom code: Yes
OS Platform and Distribution: Co-lab
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): TF 2.1-rc0
Python version: 3.6

Describe the current behavior
We are porting a GPU based model to CloudTPU. We are using Keras mixed_float16 mixed-precision policy to enable TensorCore on GPU. Without any code change, we are trying to use mixed_bfloat16 for CloudTPU for maximal performance.
Describe the expected behavior
model.compile with mixed_bfloat16 policy to enable mixed-precision training on CloudTPU.
Code to reproduce the issue
Colab link: &lt;denchmark-link:https://colab.research.google.com/drive/1-SBnqhsmyjVNJxntB8ZZdqRUk7h7tRs8&gt;https://colab.research.google.com/drive/1-SBnqhsmyjVNJxntB8ZZdqRUk7h7tRs8&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;def compile_keras_model(dtype):
  policy = tf.keras.mixed_precision.experimental.Policy(dtype)
  tf.compat.v2.keras.mixed_precision.experimental.set_policy(policy)

  optimizer = tf.optimizers.SGD(learning_rate=0.1, momentum=0.9)

  model = tf.keras.applications.resnet50.ResNet50(weights=None)
  model.compile(loss='sparse_categorical_crossentropy',
                optimizer=optimizer, 
                metrics=['sparse_categorical_accuracy'])
  return model
  
gpu_model = compile_keras_model('mixed_float16')
tpu_model = compile_keras_model('mixed_bfloat16')
&lt;/denchmark-code&gt;

Other info / logs
&lt;denchmark-code&gt;---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-5-ad9cf91bd793&gt; in &lt;module&gt;()
     12 
     13 gpu_model = compile_keras_model('mixed_float16')
---&gt; 14 tpu_model = compile_keras_model('mixed_bfloat16')

12 frames
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py in _SatisfiesTypeConstraint(dtype, attr_def, param_name)
     59           "allowed values: %s" %
     60           (param_name, dtypes.as_dtype(dtype).name,
---&gt; 61            ", ".join(dtypes.as_dtype(x).name for x in allowed_list)))
     62 
     63 

TypeError: Value passed to parameter 'x' has DataType bfloat16 not in list of allowed values: float16, float32, float64, complex64, complex128
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='byronyi' date='2019-12-04T08:41:09Z'>
		I have tried on colab with TF version 2.1.0-rc0 and was able to reproduce the issue.Please, find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/0d0639c5e7a5dae0937d7aa16b042d59/untitled444.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='byronyi' date='2019-12-04T08:55:23Z'>
		Gently ping &lt;denchmark-link:https://github.com/reedwm&gt;@reedwm&lt;/denchmark-link&gt;
; is this behavior intended?
		</comment>
		<comment id='3' author='byronyi' date='2019-12-05T22:40:10Z'>
		The issue is that we are calling an op that does not support bfloat16 in the Loss. We should do all Losses in float32, but I haven't implemented that yet. I regret not doing this for 2.1, but this will definitely be fixed in 2.2.
Because of this issue, in 2.1, models should end in float32. For models which you cannot modify, like tf.keras.applications.resnet50.ResNet50, you can wrap the model with a version that has a float32 output, as follows:
def compile_keras_model(dtype):
  policy = tf.keras.mixed_precision.experimental.Policy(dtype)
  tf.compat.v2.keras.mixed_precision.experimental.set_policy(policy)

  optimizer = tf.optimizers.SGD(learning_rate=0.1, momentum=0.9)

  model = tf.keras.applications.resnet50.ResNet50(weights=None)

  # Create new model that is `model` except the output is float32
  inp = tf.keras.layers.Input(batch_shape=model.input_shape)
  out = model(inp)
  # This layer simply casts to float32
  out = tf.keras.layers.Activation('linear', dtype='float32')(out)
  model = tf.keras.Model(inp, out)

  model.compile(loss='sparse_categorical_crossentropy',
                optimizer=optimizer,
                metrics=['sparse_categorical_accuracy'])
  return model

gpu_model = compile_keras_model('mixed_float16')
tpu_model = compile_keras_model('mixed_bfloat16')
Additionally, if your model ends in softmax, you should also do the softmax in float32, as a few models require this for numeric stability and it has very little impact on performance. Only softmaxes at the end of the model need to be float32; if a softmax is in the middle of the model, doing it in float16 is fine. For example, if your model ends with the following
out = tf.keras.layers.Dense(32, activation='softmax')(out)
Replace it with the following:
out = tf.keras.layers.Dense(32)(out)
out = tf.keras.layers.Activation('softmax', dtype='float32')(out)
This will cause the softmax to be in float32, and the output of the model in float32.
Unfortunately, none of the keras application models currently do the last softmax in float32, but I will fix in a future release. If feasible, I will try to have this done automatically. Luckily, for most models, including resnet50, doing the softmax at the end of the model in float16 is fine
		</comment>
		<comment id='4' author='byronyi' date='2019-12-06T00:21:37Z'>
		&lt;denchmark-link:https://github.com/reedwm&gt;@reedwm&lt;/denchmark-link&gt;
 that’s very helpful, thanks Reed!
I’ll leave this issue as a marker and close it once it’s fixed in master.
		</comment>
		<comment id='5' author='byronyi' date='2020-03-16T18:59:26Z'>
		&lt;denchmark-link:https://github.com/byronyi&gt;@byronyi&lt;/denchmark-link&gt;
 Looks like this was already implemented in the recent . Please check the &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/12876c8f6f6e2e185b6952ebea37a4da/untitled444.ipynb&gt;gist here&lt;/denchmark-link&gt;
. Thanks.
Please close the issue if this was resolved for you. Thanks!
		</comment>
		<comment id='6' author='byronyi' date='2020-03-16T19:35:39Z'>
		Ah yeah, this was fixed fairly recently.
It turns out doing softmax + loss in fp16 or bf16 is fine in most cases. For this particular issue, the error came from the div_no_nan op, which Keras will now always do in fp32 to avoid the error.
		</comment>
		<comment id='7' author='byronyi' date='2020-03-16T19:35:41Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34782&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34782&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>