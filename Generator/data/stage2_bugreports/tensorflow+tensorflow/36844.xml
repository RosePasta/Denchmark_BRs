<bug id='36844' author='FrankYinXF' open_date='2020-02-18T00:42:18Z' closed_time='2020-03-07T05:56:20Z'>
	<summary>Documentation is very unclear. It lacks formulas in many APIs.</summary>
	<description>
For example, in SparseCategoricalAccuracy(), the words in this API does not help to give a clear picture to understand what it is doing. Why not give a formula. A formula, associated with an example, is clear enough for this API.
	</description>
	<comments>
		<comment id='1' author='FrankYinXF' date='2020-02-18T13:30:17Z'>
		&lt;denchmark-link:https://github.com/FrankYinXF&gt;@FrankYinXF&lt;/denchmark-link&gt;

Can you please provide the links you are referring to.Thanks!
		</comment>
		<comment id='2' author='FrankYinXF' date='2020-02-21T09:56:59Z'>
		Can you please mention which formula you need exactly,I might probably be able to help you with that.
		</comment>
		<comment id='3' author='FrankYinXF' date='2020-02-24T10:16:47Z'>
		&lt;denchmark-link:https://github.com/FrankYinXF&gt;@FrankYinXF&lt;/denchmark-link&gt;

I can relate to you on this ... Let me clarfiy it for you.
The SparseCategoricalEntropy is similar to the CategoricalCrossentropy Function.
The only difference being that the y_true need not be one-hot columns instead they are integer values denoting class number.
Let's try it with an example:
You have a 3 class prediction example namely cat, dog and monkey.
cat is assigned as your first label        --&gt; 0
dog is assigned as your second label --&gt; 1
monkey as your third label                  --&gt; 2
Now while predicting through any network the output logits must have a shape of 3 with one for each of the above mentioned class. When you keep the parameter logits=True in the SparseCategoricalCrossEntropy function we feed this layer directly in the loss function as y_pred.
Here the y_true which the loss function expects is not a one-hot column of shape (3,) instead a single integer value corresponding to the class i.e.
0-&gt; cat
1-&gt; dog
2-&gt; monkey
What the above function does is that it first applies a softmax function(if logits=True) and computes class probailities and on these probabilites it applies a crossentropy function. Now the crossentropy here expects inputs of the same shape so sparsecategoricalcrossEntropy makes a one-hot column in a optimized way to compute the loss instead of feeding a one-hot ourself.
It saves us 2 things , first the burden of making a one-hot column every time and also reduces memory usage as we don't need to save labels as one-hot columns. This is especially useful when the number of classes is extremely high say &gt;1000.
Now if logits=False the function expects softmax probabilities instead of the logits as input.
Please reply if you need any more clarifications.
Happy to help.
		</comment>
		<comment id='4' author='FrankYinXF' date='2020-02-29T06:18:20Z'>
		
@FrankYinXF
Can you please provide the links you are referring to.Thanks!

Any updates regarding this issue? Thanks!
		</comment>
		<comment id='5' author='FrankYinXF' date='2020-03-07T05:56:20Z'>
		Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
		</comment>
	</comments>
</bug>