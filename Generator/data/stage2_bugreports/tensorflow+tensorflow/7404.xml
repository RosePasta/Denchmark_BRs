<bug id='7404' author='malmaud' open_date='2017-02-10T03:14:10Z' closed_time='2018-03-29T18:11:56Z'>
	<summary>No attribute 'outer_context' when calculating gradient from imported graph</summary>
	<description>
It seems when you import a graph with a "while" loop, you can't calculate gradients as you could on the original graph. e.g.
import tensorflow as tf
i=tf.constant(0, name="input")
out=tf.while_loop(lambda i: tf.less(i,5), lambda i: [tf.add(i,1)], [i], name="output")
graph_def = tf.get_default_graph().as_graph_def()

g = tf.Graph()
with g.as_default():
    tf.import_graph_def(graph_def)
s = tf.Session(graph=g)
i_imported = g.get_tensor_by_name("import/input:0")
out_imported = g.get_tensor_by_name("import/output/Exit:0")
tf.gradients(out_imported, i_imported)
&lt;denchmark-code&gt;---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-12-e7e2b78684d3&gt; in &lt;module&gt;()
----&gt; 1 tf.gradients(out_imported, i_imported)

/Users/malmaud/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.pyc in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method)
    439     pending_count, loop_state = _PendingCount(ops.get_default_graph(), to_ops,
    440                                               from_ops,
--&gt; 441                                               colocate_gradients_with_ops)
    442 
    443     # Iterate over the collected ops.


/Users/malmaud/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.pyc in _PendingCount(graph, to_ops, from_ops, colocate_gradients_with_ops)
    184   # 'loop_state' is None if there are no while loops.
    185   loop_state = control_flow_ops.MaybeCreateControlFlowState(
--&gt; 186       between_op_list, between_ops, colocate_gradients_with_ops)
    187 
    188   # Initialize pending count for between ops.

/Users/malmaud/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.pyc in MaybeCreateControlFlowState(between_op_list, between_ops, colocate_gradients_with_ops)
   1293           loop_state.AddWhileContext(op, between_op_list, between_ops)
   1294       else:
-&gt; 1295         loop_state.AddWhileContext(op, between_op_list, between_ops)
   1296   return loop_state
   1297 

/Users/malmaud/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.pyc in AddWhileContext(self, op, between_op_list, between_ops)
   1102     if grad_state is None:
   1103       # This is a new while loop so create a grad state for it.
-&gt; 1104       outer_forward_ctxt = forward_ctxt.outer_context
   1105       if outer_forward_ctxt:
   1106         outer_forward_ctxt = outer_forward_ctxt.GetWhileContext()

AttributeError: 'NoneType' object has no attribute 'outer_context'
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='malmaud' date='2017-02-10T16:43:52Z'>
		&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 Any idea what would be happening here?  Do while loop gradients depend on unserialized information?
		</comment>
		<comment id='2' author='malmaud' date='2017-02-11T07:22:35Z'>
		Yes, though I thought Yuan and Sherry had added the appropriate serialization to the metagraph.  Is this on master branch of tf?
		</comment>
		<comment id='3' author='malmaud' date='2017-02-11T12:46:43Z'>
		Ya, this is master.
		</comment>
		<comment id='4' author='malmaud' date='2017-02-11T17:14:36Z'>
		OK, I see if I save and restore the metagraph (via tf.train.export_meta_graph and tf.train.import_meta_graph, then I can take the gradient in the restored session. So it's just a problem if you try serializing and importing the GraphDef itself.
		</comment>
		<comment id='5' author='malmaud' date='2017-02-13T15:50:44Z'>
		&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 Is there an easy way to produce a nicer error message here, so that users don't have to guess the graph vs. metagraph issue?
		</comment>
		<comment id='6' author='malmaud' date='2017-02-13T17:00:39Z'>
		Yes; I believe that the forward_ctxt object here:

forward_ctxt.outer_context

should have a &lt;denchmark-link:https://github.com/Property&gt;@Property&lt;/denchmark-link&gt;
 outer_context; and if no such internal object is
set, it can raise a ValueError/InternalError that maybe you're trying to
call gradients on a while loop without properly serializing your graph via
MetaGraphDef.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Mon, Feb 13, 2017 at 7:51 AM, Geoffrey Irving ***@***.***&gt; wrote:
 @ebrevdo &lt;https://github.com/ebrevdo&gt; Is there an easy way to produce a
 nicer error message here, so that users don't have to guess the graph vs.
 metagraph issue?

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#7404 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABtimxlSpTQAQlztIVyjpp6fmXDWkZ60ks5rcHvvgaJpZM4L89DK&gt;
 .



		</comment>
		<comment id='7' author='malmaud' date='2017-11-10T16:58:13Z'>
		Hi &lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 , is there any update on this issue? Thanks!
		</comment>
		<comment id='8' author='malmaud' date='2017-11-14T06:10:20Z'>
		&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
, any update on this issue or any workaround? Thanks!
		</comment>
		<comment id='9' author='malmaud' date='2017-12-20T19:27:54Z'>
		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
		</comment>
		<comment id='10' author='malmaud' date='2018-01-04T19:18:55Z'>
		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
		</comment>
		<comment id='11' author='malmaud' date='2018-01-24T13:18:44Z'>
		Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
		</comment>
		<comment id='12' author='malmaud' date='2018-02-08T00:52:28Z'>
		&lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;
 should we mark this contributions welcome?
		</comment>
		<comment id='13' author='malmaud' date='2018-02-08T02:26:46Z'>
		Yes
		</comment>
		<comment id='14' author='malmaud' date='2018-03-28T15:29:35Z'>
		Created a PR &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/18052&gt;#18052&lt;/denchmark-link&gt;
 to fix this.
		</comment>
		<comment id='15' author='malmaud' date='2018-12-01T13:58:02Z'>
		Hello, I came up with the same issue when I use tf.import_graph_def:
AttributeError: 'NoneType' object has no attribute 'outer_context'
Following the instructions above I switched to import_meta_graph like this:
`
def _make_model_and_ops(self, patch_val):
start = time.time()
&lt;denchmark-code&gt;    self.image_input_ = tf.placeholder(tf.float32, shape=(None, None, None, 3), name='image_input')
    saver = tf.train.import_meta_graph(PATH_TO_CKPT + '.meta',
                                       import_scope="detection",
                                       input_map={
                                           'ToFloat_3': self.image_input_
                                       })
    saver.restore(self.sess, PATH_TO_CKPT)
    self.graph = self.sess.graph
    
    with self.sess.graph.as_default():
        tf.set_random_seed(1234)
        
        # Tensors are post-fixed with an underscore!
        #self.image_input_shape_ = tf.placeholder(tf.int32, shape=(1,3), name="image_input_shape")
        
        self.eps_ = tf.placeholder(tf.float32, shape=(1), name='eps')
        
        # The following commented part is the original code using import_graph_def
        '''
        detection_graph_def = tf.GraphDef()
        with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:
            serialized_graph = fid.read()
            detection_graph_def.ParseFromString(serialized_graph)
            tf.import_graph_def(detection_graph_def, name='detection',
                                input_map={
                                           'Preprocessor/map/TensorArrayStack/TensorArrayGatherV3': self.image_input_,
                                           'Preprocessor/map/TensorArrayStack_1/TensorArrayGatherV3': self.image_input_shape_
                                          })
        '''
        
        # Second-stage Class Loss
        self.second_stage_cls_scores_ = self.graph.get_tensor_by_name('detection/SecondStagePostprocessor/convert_scores:0')
        second_stage_cls_logits_ = self.graph.get_tensor_by_name('detection/SecondStagePostprocessor/scale_logits:0')
        self.second_stage_cls_labels_ = tf.placeholder(tf.float32, shape=second_stage_cls_logits_.shape, name='second_stage_cls_labels')
        
        self.second_stage_cls_losses_ = tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf.reshape(self.second_stage_cls_labels_, (-1, self.second_stage_cls_labels_.shape[2])),
                                                                                   logits=tf.reshape(second_stage_cls_logits_, (-1, second_stage_cls_logits_.shape[2]))) 
       
        # Second-stage bounding boxes
        self.second_stage_loc_bboxes_ = self.graph.get_tensor_by_name('detection/SecondStagePostprocessor/Reshape_4:0')
        
        grads = tf.gradients(self.second_stage_cls_losses_, [self.image_input_])
        print(grads)
        self.unclipped_adv_images_ = self.image_input_ + self.eps_ * tf.sign(grads)
        self.adv_images_ = tf.clip_by_value(self.unclipped_adv_images_, clip_value_min=0, clip_value_max=255)


    elapsed = time.time() - start
    print("Finished loading the model, took {:.0f}s".format(elapsed))           
&lt;/denchmark-code&gt;

`
But the problem still exist.
Does this mean that the information required for tf.gradients does not appear in the check point? If so, what else can I do?
Thanks, and sorry for my poor English....
		</comment>
		<comment id='16' author='malmaud' date='2020-03-14T05:59:09Z'>
		Having the same problem as &lt;denchmark-link:https://github.com/ProjectDimlight&gt;@ProjectDimlight&lt;/denchmark-link&gt;
, tried the suggestions above with no luck. Any updates on this?
		</comment>
	</comments>
</bug>