<bug id='33498' author='JesperChristensen89' open_date='2019-10-18T06:57:07Z' closed_time='2020-07-23T08:04:48Z'>
	<summary>TFLite slower than Keras on RPi 4</summary>
	<description>
When doing inference on a Raspberry Pi 4 with Keras and Tensorflow installed using pip, the inference time is slower using TFLite.
The initial Keras model is about 20 mb - after converting it to TFLite it is about 2.4 mb. During inference the Keras model processes a sample in about 50 ms and TFLite does it in about 80 ms.
Initially the pip-installed version of Tensorflow caused errors with TFLite, so I installed TFLite-runtime using this information: &lt;denchmark-link:https://www.tensorflow.org/lite/guide/python&gt;https://www.tensorflow.org/lite/guide/python&lt;/denchmark-link&gt;

During inference in TFLite I use the following snippet:
interpreter = tflite.Interpreter(model_path=CHECKPOINT)
interpreter.allocate_tensors()

input_index = interpreter.get_input_details()[0]["index"]
output_index = interpreter.get_output_details()[0]["index"]

for im in images:
	t = time.time()
	
	inp = np.expand_dims(np.expand_dims(im,-1),0)

	interpreter.set_tensor(input_index, inp)
	interpreter.invoke()
	
	predictions = interpreter.get_tensor(output_index)

	print("Time: {}".format(time.time()-t),end="\r")
Does anybody have any experience with TFLite on Raspberry Pi? Am I missing something in order accelerate inference further? It seems wrong that inference should be faster in Keras with 10x model size.
	</description>
	<comments>
		<comment id='1' author='JesperChristensen89' date='2020-02-25T06:31:11Z'>
		&lt;denchmark-link:https://github.com/JesperChristensen89&gt;@JesperChristensen89&lt;/denchmark-link&gt;
 do you still having the issue with recent (2.1) TFLite package?
		</comment>
		<comment id='2' author='JesperChristensen89' date='2020-02-25T06:53:21Z'>
		&lt;denchmark-link:https://github.com/terryheo&gt;@terryheo&lt;/denchmark-link&gt;
 I actually think I found out that Keras automatically use all cores where as TFlite only use 1.
		</comment>
		<comment id='3' author='JesperChristensen89' date='2020-07-23T08:04:45Z'>
		

From TF2.3, you can control the number of threads.
https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/lite/Interpreter


To get a speed up, you need to quantize your model to integer
https://www.tensorflow.org/lite/performance/model_optimization


Thanks.
		</comment>
		<comment id='4' author='JesperChristensen89' date='2020-07-23T08:04:50Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33498&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33498&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='JesperChristensen89' date='2020-07-30T16:53:50Z'>
		we tried with tf2.2, it was still slower than using h5 model. Will try 2.3 soon.
		</comment>
	</comments>
</bug>