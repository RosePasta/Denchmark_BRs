<bug id='5683' author='indhub' open_date='2016-11-18T02:10:15Z' closed_time='2017-06-16T17:28:44Z'>
	<summary>Synchronous distributed training is too slow</summary>
	<description>
While trying to train Imagenet using Inceptionv3, I noticed the training was proceeding too slow. I then tested with Alexnet and Resnet and I saw the same slow training speed.
Here is the images/sec I'm able to achieve on various models:
&lt;denchmark-code&gt;Inceptionv3:
# GPU, Images/sec
1,        17.525
2,        34.568
4,        62.580
8,        94.832
16,       99.072
32,       140.704
64,       183.488
128,      322.816

Alexnet:
1,        92.464
2,        189.736
4,        344.336
8,        532.40
16,       710.224
32,       878.944
64,       848.128
128,      874.624

Resnet 152:
1,        10.567
2,        20.224
4,        32.332
8,        37.952
16,       42.416
32,       66.016
64,       95.168
128,      152.192

&lt;/denchmark-code&gt;

Is it possible there is some bug causing this slow performance?
I'm using EC2 P2 instances (p2.16xlarge). Each instance has 16 GPUs. Each GPU is a Tesla K80. I'm running one worker per GPU and one PS per host. Here is more info on P2 instances: &lt;denchmark-link:https://aws.amazon.com/ec2/instance-types/p2/&gt;https://aws.amazon.com/ec2/instance-types/p2/&lt;/denchmark-link&gt;

I use randomly generated synthetic data for images and label. I've written some scripts to reproduce this issue in case it helps: &lt;denchmark-link:https://github.com/indhub/tfperftest/tree/master/perftest&gt;https://github.com/indhub/tfperftest/tree/master/perftest&lt;/denchmark-link&gt;


Inception model is based on https://github.com/tensorflow/models/tree/master/inception. I've done some changes to use randomly generated synthetic data for images and labels. Modified code here: https://github.com/indhub/tfperftest/tree/master/inception
Resnet model is based on https://github.com/tensorflow/models/tree/master/resnet. I've done some changes to build Resnet 152. Modified code here: https://github.com/indhub/tfperftest/tree/master/resnet
Alexnet model is based on https://github.com/tensorflow/tensorflow/blob/816ecb7a342c25c19daa8b19627346e1fb38e56f/tensorflow/models/image/alexnet/alexnet_benchmark.py. I've added the fully connected layers at the end. Modified code here: https://github.com/indhub/tfperftest/blob/master/alexnet/alexnet.py

&lt;denchmark-h:h3&gt;What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?&lt;/denchmark-h&gt;

&lt;denchmark-link:http://stackoverflow.com/questions/40411597/synchronous-distributed-training-is-slow&gt;http://stackoverflow.com/questions/40411597/synchronous-distributed-training-is-slow&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Environment info&lt;/denchmark-h&gt;

Operating System: Ubuntu 14.04
Installed version of CUDA and cuDNN:
(please attach the output of ls -l /path/to/cuda/lib/libcud*):
&lt;denchmark-code&gt;ubuntu@ip-172-31-52-161:~$ ls -l /usr/local/cuda/lib64/libcud*
-rw-r--r-- 1 root root 558720 Sep 14 23:02 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root     16 Sep 14 23:05 /usr/local/cuda/lib64/libcudart.so -&gt; libcudart.so.8.0
lrwxrwxrwx 1 root root     19 Sep 14 23:05 /usr/local/cuda/lib64/libcudart.so.8.0 -&gt; libcudart.so.8.0.44
-rw-r--r-- 1 root root 415432 Sep 14 23:02 /usr/local/cuda/lib64/libcudart.so.8.0.44
-rw-r--r-- 1 root root 775162 Sep 14 23:02 /usr/local/cuda/lib64/libcudart_static.a

&lt;/denchmark-code&gt;

If installed from binary pip package, provide:

A link to the pip package you installed:
https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl
The output from python -c "import tensorflow; print(tensorflow.__version__)".
ubuntu@ip-172-31-52-161:~$ python -c "import tensorflow; print(tensorflow.version)"

&lt;denchmark-code&gt;I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally
0.11.0rc2

&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)&lt;/denchmark-h&gt;

Just running &lt;denchmark-link:https://github.com/tensorflow/models/tree/master/inception&gt;https://github.com/tensorflow/models/tree/master/inception&lt;/denchmark-link&gt;
 reproduces the problem.
	</description>
	<comments>
		<comment id='1' author='indhub' date='2016-11-18T16:02:38Z'>
		&lt;denchmark-link:https://github.com/shlens&gt;@shlens&lt;/denchmark-link&gt;
 Do these seem like aberrant numbers?
		</comment>
		<comment id='2' author='indhub' date='2016-11-18T16:08:12Z'>
		&lt;denchmark-link:https://github.com/jmchen-g&gt;@jmchen-g&lt;/denchmark-link&gt;
 Would you be able to take a look?
		</comment>
		<comment id='3' author='indhub' date='2017-06-16T17:28:44Z'>
		Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you.
		</comment>
	</comments>
</bug>