<bug id='25548' author='Kwander' open_date='2019-02-06T11:57:20Z' closed_time='2019-03-07T01:06:53Z'>
	<summary>Tensorflow AOT compilation error: No registered 'DecodeJpeg' OpKernel for XLA_CPU_JIT</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.12.0 (tensorflow-gpu, tensorflow-base)
Python version: 3.6.8
Bazel version (if compiling from source): 0.21.0
GCC/Compiler version (if compiling from source): 5.4.0 20160609
CUDA/cuDNN version: 7.1.2
GPU model and memory: GeForce GTX 1080, 8GB

Describe the current behavior
I am trying to compile a pretrained InceptionV4 model from TF models repository. The last layer is modified to produce our custom set of classes (we're doing transfer learning). I successfully produced both graph.pb and graph.pb.txt files. When I tried to compile the graph, I received an error (see logs below).

I have managed to successfully compile and run Keras ResNet50 and MobileNetV2 models before.
I disabled XLA &amp; CUDA when compiling TF from source.

Other info / logs
&lt;denchmark-code&gt;INVALID ARGUMENTS: Detected unsupported operations when trying to compile graph tfcompile on XLA_CPU_JIT: DecodeJpeg (No registered 'DecodeJpeg' OpKernel for XLA_CPU_JIT devices compatible with node {{node DecodeJpeg}}
	.  Registered:  &lt;no registered kernels&gt;
){{node DecodeJpeg}}

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Kwander' date='2019-02-25T21:47:02Z'>
		&lt;denchmark-link:https://github.com/Kwander&gt;@Kwander&lt;/denchmark-link&gt;
 I have couple of questions.

You mentioned that you are able to successfully compiled for two models before. Did you disable XLA and CUDA while you compiled them?
What is the reason behind the disabling XLA &amp; CUDA? Want to understand context. Thanks!

		</comment>
		<comment id='2' author='Kwander' date='2019-02-28T16:05:50Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 Hey!

Yes, I have.
I thought this might help, as I later discovered that GPU compiling was not supported by TF AOT then.

		</comment>
		<comment id='3' author='Kwander' date='2019-02-28T18:12:15Z'>
		&lt;denchmark-link:https://github.com/Kwander&gt;@Kwander&lt;/denchmark-link&gt;
 Iam not sure what will be the root cause of the issue. Could you share a code to reproduce the issue? I just want to make sure that there is nothing wrong from the coding side. Thanks!
		</comment>
		<comment id='4' author='Kwander' date='2019-02-28T18:17:07Z'>
		I believe this is working as intended.
If you want to use tfcompile to do AOT compilation on CPU, all of the ops in the compiled graph must be supported on XLA:CPU.
XLA:CPU does not support JPEG decoding.
		</comment>
		<comment id='5' author='Kwander' date='2019-02-28T18:18:30Z'>
		
I disabled XLA &amp; CUDA when compiling TF from source.

Note that tfcompile AOT requires XLA.  So if you disable XLA in the build and try to use tfcompile...I'm not sure what's supposed to happen.  :)
		</comment>
		<comment id='6' author='Kwander' date='2019-03-07T01:06:51Z'>
		Eagerly closing this, but please reopen if you have other questions / need more assistance.
		</comment>
	</comments>
</bug>