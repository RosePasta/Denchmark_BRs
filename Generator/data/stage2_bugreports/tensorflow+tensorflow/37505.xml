<bug id='37505' author='nkoerb' open_date='2020-03-11T09:57:12Z' closed_time='2020-09-20T15:47:26Z'>
	<summary>Memory leak in model.fit</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information


Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
minimal working example


OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
windows-server 2016


TensorFlow installed from (source or binary):
conda


TensorFlow version (use command below):
tf 2.1.0


Python version:
3.7


CUDA/cuDNN version:
CUDA 10.1


GPU model and memory:
K80 - 24Gb



memory use increases with consecutive training runs, probably related to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/35524&gt;#35524&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/33030&gt;#33030&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/35124&gt;#35124&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/35835&gt;#35835&lt;/denchmark-link&gt;

side note: I do not understand the warning but this seems to be handled in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/37500&gt;#37500&lt;/denchmark-link&gt;


memory should stay constant
Standalone code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
&lt;denchmark-code&gt;from tensorflow.keras.datasets import cifar10
import tensorflow.keras.callbacks as callbacks
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import LearningRateScheduler
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.layers import Input, Conv2D, GlobalAveragePooling2D, Activation, Dense
from tensorflow.keras.models import Model
import tensorflow.keras.utils as kutils
import numpy as np
import psutil
import gc 


batch_size = 128
epochs = 5
num_classes = 10


def buildmodel():
    img_input = Input(shape=(32, 32, 3))

    x = Conv2D(16, (3, 3), padding='same')(img_input)
    x = Activation("relu")(x)
    x = Conv2D(16, (3, 3), padding='same')(x)
    x = Activation("relu")(x)
    x = GlobalAveragePooling2D()(x)
    prediction = Dense(num_classes,activation='softmax', name = 'classifier') (x)  
    model = Model(inputs=img_input, outputs=prediction)
    return model

(trainX, trainY), (testX, testY) = cifar10.load_data()
mean = np.mean(trainX, axis=0)
std = np.std(trainX)

trainX = trainX.astype('float32')
trainX = (trainX - mean) / std
testX = testX.astype('float32')
testX = (testX - mean) / std

trainY = kutils.to_categorical(trainY)
testY = kutils.to_categorical(testY)

generator = ImageDataGenerator()
generator.fit(trainX)
val_generator = ImageDataGenerator()

for i in range(10):
    tf.keras.backend.clear_session()
    model = buildmodel()
    sgd = SGD(lr=0.1, momentum=0.9, nesterov=True)
    model.compile(loss="categorical_crossentropy", optimizer=sgd, metrics=["acc"])
    model.fit(generator.flow(trainX, trainY, batch_size=batch_size), epochs=epochs, validation_data=val_generator.flow(testX, testY, batch_size = batch_size),verbose=0, workers = 20)
    print('memory usesd: ' + str(psutil.virtual_memory().used // 1e6))
    gc.collect()
&lt;/denchmark-code&gt;

output:
WARNING:tensorflow:sample_weight modes were coerced from
...
to
['...']
WARNING:tensorflow:sample_weight modes were coerced from
...
to
['...']
memory usesd: 38012.0
WARNING:tensorflow:sample_weight modes were coerced from
...
to
['...']
WARNING:tensorflow:sample_weight modes were coerced from
...
to
['...']
memory usesd: 38563.0
WARNING:tensorflow:sample_weight modes were coerced from
...
to
['...']
WARNING:tensorflow:sample_weight modes were coerced from
...
to
['...']
memory usesd: 39288.0
WARNING:tensorflow:sample_weight modes were coerced from
...
to
['...']
WARNING:tensorflow:sample_weight modes were coerced from
...
to
['...']
memory usesd: 40005.0
WARNING:tensorflow:sample_weight modes were coerced from
...
to
['...']
WARNING:tensorflow:sample_weight modes were coerced from
...
to
['...']
memory usesd: 40730.0
WARNING:tensorflow:sample_weight modes were coerced from
...
to
['...']
WARNING:tensorflow:sample_weight modes were coerced from
...
to
['...']
memory usesd: 41490.0
WARNING:tensorflow:sample_weight modes were coerced from
...
to
['...']
WARNING:tensorflow:sample_weight modes were coerced from
...
to
['...']
memory usesd: 42216.0
WARNING:tensorflow:sample_weight modes were coerced from
...
to
['...']
WARNING:tensorflow:sample_weight modes were coerced from
...
to
['...']
memory usesd: 42937.0
WARNING:tensorflow:sample_weight modes were coerced from
...
to
['...']
WARNING:tensorflow:sample_weight modes were coerced from
...
to
['...']
memory usesd: 43659.0
WARNING:tensorflow:sample_weight modes were coerced from
...
to
['...']
WARNING:tensorflow:sample_weight modes were coerced from
...
to
['...']
memory usesd: 44403.0
	</description>
	<comments>
		<comment id='1' author='nkoerb' date='2020-03-12T09:33:44Z'>
		I have replicated the code shared above, please find the &lt;denchmark-link:https://colab.sandbox.google.com/gist/Saduf2019/1df4ffa08b3cd2491fa3395fe4815dac/37505.ipynb&gt;gist here&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='nkoerb' date='2020-03-16T11:22:49Z'>
		I am trying to fix this issue, I do find the issue on Colab, but on my MacBook Pro (13-inch, 2017, Two Thunderbolt 3 ports), there seems no such memory leak issue.
Here is the output of the program (warning msgs are removed);
&lt;denchmark-code&gt;memory usesd: 8968.0
memory usesd: 9009.0
memory usesd: 8402.0
memory usesd: 8299.0
memory usesd: 8257.0
memory usesd: 7761.0
memory usesd: 8642.0
memory usesd: 8833.0
memory usesd: 9008.0
memory usesd: 9267.0
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='nkoerb' date='2020-03-16T12:02:19Z'>
		&lt;denchmark-link:https://github.com/howl-anderson&gt;@howl-anderson&lt;/denchmark-link&gt;
 : do you have something running in parallel? Your memory usage seems not to be constant over time
		</comment>
		<comment id='4' author='nkoerb' date='2020-03-16T14:01:55Z'>
		&lt;denchmark-link:https://github.com/nkoerb&gt;@nkoerb&lt;/denchmark-link&gt;
 Yes, it's a little weird, I will test again without open other applications on Mac and test it on Ubuntu 18.04 too.
		</comment>
		<comment id='5' author='nkoerb' date='2020-03-17T02:46:07Z'>
		&lt;denchmark-link:https://github.com/nkoerb&gt;@nkoerb&lt;/denchmark-link&gt;
 New test on Mac (without open other applications):
&lt;denchmark-code&gt;memory usesd: 6879.0
memory usesd: 7259.0
memory usesd: 7620.0
memory usesd: 7966.0
memory usesd: 8323.0
memory usesd: 8694.0
memory usesd: 9029.0
memory usesd: 9392.0
memory usesd: 9486.0
memory usesd: 9480.0
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='nkoerb' date='2020-03-17T07:59:37Z'>
		&lt;denchmark-link:https://github.com/howl-anderson&gt;@howl-anderson&lt;/denchmark-link&gt;
 looks leaky to me
		</comment>
		<comment id='7' author='nkoerb' date='2020-03-17T08:59:57Z'>
		&lt;denchmark-link:https://github.com/nkoerb&gt;@nkoerb&lt;/denchmark-link&gt;
 Agree! I am will try to fix this later.
		</comment>
		<comment id='8' author='nkoerb' date='2020-03-20T02:23:09Z'>
		This issue likely relates to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/37653&gt;#37653&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='nkoerb' date='2020-04-12T17:45:16Z'>
		Hey, I was having the same issue.
My current workaround is to save the model before deleting it and clearing the backend after each training iteration. Then I reload the model before calling fit again.
I'm no longer experiencing the memory leak with this workaround.
		</comment>
		<comment id='10' author='nkoerb' date='2020-04-15T09:32:34Z'>
		&lt;denchmark-link:https://github.com/DanyelJei&gt;@DanyelJei&lt;/denchmark-link&gt;
 how do you clear the backend, because I also used clear_session() and garbage collector? And what exactly do you mean with 'training iteration': full training, epoch or single iteration on a batch?
		</comment>
		<comment id='11' author='nkoerb' date='2020-04-30T18:26:39Z'>
		Have a similar issue with increasing memory usage when using the ImageDataGenerator and the fit function. Any idea if it will be fixed in 2.2.0?
		</comment>
		<comment id='12' author='nkoerb' date='2020-05-19T02:13:00Z'>
		I have the same memory leaking problem with model.fit when using dataset from generator.
Hopefully it will be fixed some time soon.
		</comment>
		<comment id='13' author='nkoerb' date='2020-06-11T11:22:35Z'>
		Confirm a memory leak, similar symptoms. Seems to eat about 4 Mbytes/second on my workload. tf2.2.0-1, arch 64bit.
		</comment>
		<comment id='14' author='nkoerb' date='2020-06-12T22:52:22Z'>
		Also experiencing this issue
		</comment>
		<comment id='15' author='nkoerb' date='2020-06-13T22:18:08Z'>
		Hey, same issue here on Windows machine. Seems to be related to multiple model fits over generators, it clearly seems like a memory leak from tensorflow side, deleting all variables and calling garbage collector won't fix the issue. Managed to evade the problem by setting workers = 1.
Edit: I thought setting workers=1 would fix the issue as described in a stackoverflow post I can't find right now. Trying now with workers=0.
Edit2: Nope, seems this doesn't work either.
Edit3: Managed to make it work with workers=0 by letting each iteration of training in the loop to be in a function, so that the variables are local, followed by a call to tf.keras.backend.clear_sesion() and then gc.collect().
		</comment>
		<comment id='16' author='nkoerb' date='2020-07-12T00:20:51Z'>
		
Hey, same issue here on Windows machine. Seems to be related to multiple model fits over generators, it clearly seems like a memory leak from tensorflow side, deleting all variables and calling garbage collector won't fix the issue. Managed to evade the problem by setting workers = 1.
Edit: I thought setting workers=1 would fix the issue as described in a stackoverflow post I can't find right now. Trying now with workers=0.
Edit2: Nope, seems this doesn't work either.
Edit3: Managed to make it work with workers=0 by letting each iteration of training in the loop to be in a function, so that the variables are local, followed by a call to tf.keras.backend.clear_sesion() and then gc.collect().

Hi, could you provide some information or example code of fix with your edit3?
		</comment>
		<comment id='17' author='nkoerb' date='2020-08-20T09:59:09Z'>
		i noticed the validation_data is actually causing the memory leak. not sure if it's because of my implementation cus I'm using dataset as a wrapper of my multiprocessing data loader. I'm using tf.data.Dataset.from_generator(generator_fun, ...) What happens to me is that if you dig into the source code, you will actually see that the validation data and the train x in the fit function are treated differently.
In my case, my  is actually getting called every epoch for validation set but for training x, it's not called every time. Then everytime my  generated a new generator which causes the memory leak. I verified this behavior by digging down to the  source code. I'm on tf 2.2.0 and you can see  below under &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/64dbe97a4d5a9a2cc69cacae7762157cd201a1d2/tensorflow/python/keras/engine/training.py#L859&gt;tf.python.keras.engine.training.py&lt;/denchmark-link&gt;
 :
&lt;denchmark-code&gt;        # Run validation.
        if validation_data and self._should_eval(epoch, validation_freq):
          val_x, val_y, val_sample_weight = (
              data_adapter.unpack_x_y_sample_weight(validation_data))
          val_logs = self.evaluate(
              x=val_x,
              y=val_y,
              sample_weight=val_sample_weight,
              batch_size=validation_batch_size or batch_size,
              steps=validation_steps,
              callbacks=callbacks,
              max_queue_size=max_queue_size,
              workers=workers,
              use_multiprocessing=use_multiprocessing,
              return_dict=True)
          val_logs = {'val_' + name: val for name, val in val_logs.items()}
          epoch_logs.update(val_logs)
&lt;/denchmark-code&gt;

So the dataset passed in will be put into the evaluate function where it probably called the as_numpy_iterator() and that caused my generator_fun got called again and again. However the training generator didn't show this behavior. Then I modified my generator_fun so that every time it's called it's not creating a new instance but reusing the same generator that has already been created, then the memory leak is gone.
I'm not sure how the ImageDataGenerator.flow_from_directory work but maybe you can check that. Also i notice they modified this part of code on master. I haven't tried nightly yet. maybe they already fixed it. Just a different perspective.
		</comment>
		<comment id='18' author='nkoerb' date='2020-09-08T22:29:45Z'>
		Is there any solution? I am still facing it now..
		</comment>
		<comment id='19' author='nkoerb' date='2020-09-10T10:48:24Z'>
		I ran into a similar problem. Workaround which seems to work for me is to force garbage collector:
&lt;denchmark-code&gt;import gc
...
model.fit(...)
gc.collect()
&lt;/denchmark-code&gt;

		</comment>
		<comment id='20' author='nkoerb' date='2020-09-16T04:38:05Z'>
		&lt;denchmark-link:https://github.com/nkoerb&gt;@nkoerb&lt;/denchmark-link&gt;
 Related to performance, there were some significant improvements in the recent version. I ran your code with  and I don't see any improvement in the memory as much as you have noticed. When I ran your code with , i see the following ouput.
Downloading data from &lt;denchmark-link:https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz&gt;https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz&lt;/denchmark-link&gt;

170500096/170498071 [==============================] - 2s 0us/step
memory usesd: 3981.0
memory usesd: 3981.0
memory usesd: 3976.0
memory usesd: 3975.0
memory usesd: 3975.0
memory usesd: 3973.0
memory usesd: 3973.0
memory usesd: 3970.0
Please take a look at the &lt;denchmark-link:https://colab.research.google.com/gist/jvishnuvardhan/8557bb197806600d5c998e37f90f8e08/37505.ipynb&gt;gist here&lt;/denchmark-link&gt;
.
Can you please verify once with tf-nightly and close the issue if this was resolved for you. Thanks!
		</comment>
		<comment id='21' author='nkoerb' date='2020-09-20T15:47:26Z'>
		&lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 Thanks for your help and great work, memory is constant with tf-nightly!
As a side note: When I remove either  or  memory is leaking, so both commands are needed for constant memory usage when performing consecutive trainings.
		</comment>
		<comment id='22' author='nkoerb' date='2020-09-20T15:47:28Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37505&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37505&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>