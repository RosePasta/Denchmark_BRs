<bug id='29624' author='marwan116' open_date='2019-06-11T02:50:38Z' closed_time='2020-04-20T21:53:31Z'>
	<summary>on_epoch_end not triggered when workers=0 in fit_generator</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS X 10.14
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.0.0-beta0 (issue also happens on TF 1.12 and 1.13)
Python version: Python 3.6.7
Bazel version (if compiling from source):  N/A
GCC/Compiler version (if compiling from source):  N/A
CUDA/cuDNN version:  N/A
GPU model and memory:  N/A

Describe the current behavior
When running fit_generator on the main thread (i.e. with use_multiprocessing=False and workers=0), on_epoch_end doesn't get triggered
Describe the expected behavior
Having looked at the source code, I would expect on_epoch_end to be triggered at the end of each epoch
Code to reproduce the issue
I create a simple generator by subclassing from tf.keras.utils.Sequence:
&lt;denchmark-code&gt;class DataGenerator(Sequence):
    def __init__(self, split):
        self.split = split
        
    def __len__(self):
        return 2

    def __getitem__(self, index):

        print (f'\n split: {self.split} generator, index: {index}', flush=True)
        y = np.random.uniform(low=0, high=1, size=(2_000, 1))
        y = (y &gt; 0.5).astype(np.int32)
        X = np.random.normal(loc=0, scale=1, size=(2_000, 10))
        X = X.astype(np.float32)
        return X, y

    def on_epoch_end(self):
        print (f'on epoch end: {self.split}', flush=True)
&lt;/denchmark-code&gt;

I then create a simple model
&lt;denchmark-code&gt;model = Sequential()
model.add(Dense(20, activation='relu', input_shape=(10,)))
model.add(Dense(1, activation='sigmoid'))
optimizer = Adam(lr=0.001)
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
&lt;/denchmark-code&gt;

I set up my training and validation generators and run fit_generator
&lt;denchmark-code&gt;training_generator = DataGenerator(
    split='training')

validation_generator = DataGenerator(
    split='validation')

model.fit_generator(
    generator=training_generator,
    validation_data=validation_generator,
    use_multiprocessing=False,
    max_queue_size=10,
    epochs=4,
    shuffle=False,
    workers=0,
    verbose=0)
&lt;/denchmark-code&gt;

I get the following output:
&lt;denchmark-code&gt; split: training generator, index: 0

 split: training generator, index: 1

 split: validation generator, index: 0

 split: validation generator, index: 1

 split: training generator, index: 0

 split: training generator, index: 1

 split: validation generator, index: 0

 split: validation generator, index: 1

 split: training generator, index: 0

 split: training generator, index: 1

 split: validation generator, index: 0

 split: validation generator, index: 1

 split: training generator, index: 0

 split: training generator, index: 1

 split: validation generator, index: 0

 split: validation generator, index: 1
&lt;/denchmark-code&gt;

If I change workers from 0 to 1, then on_epoch_end is triggered (i.e. 'on epoch end: {}' print statements) appear.
	</description>
	<comments>
		<comment id='1' author='marwan116' date='2020-03-27T18:33:05Z'>
		&lt;denchmark-link:https://github.com/marwan116&gt;@marwan116&lt;/denchmark-link&gt;
,
I was able to reproduce the issue with TF v2.1. However, it seems to be fixed with the latest nightly TF v2.2.0-dev20200327. Please find the gist of it &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/2fceed384fd9cfb8966ba9b507b8fcde/tf-nightly.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='marwan116' date='2020-03-27T23:36:21Z'>
		&lt;denchmark-link:https://github.com/amahendrakar&gt;@amahendrakar&lt;/denchmark-link&gt;
 thanks for taking the time to check this. Ok cool so this looks like it is working now - what about TF1 (1.15 ..)?
		</comment>
		<comment id='3' author='marwan116' date='2020-03-30T06:41:21Z'>
		&lt;denchmark-link:https://github.com/marwan116&gt;@marwan116&lt;/denchmark-link&gt;
,
Seems like the issue still persists with TF 1.15.2. Please find the gist &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/a338353b2125128441160f36acd0cd49/29624-1-15.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='4' author='marwan116' date='2020-04-07T20:21:59Z'>
		Hi &lt;denchmark-link:https://github.com/marwan116&gt;@marwan116&lt;/denchmark-link&gt;
. We usually don't patch old releases when we fix bugs. We only do patch releases for security issues. When do those, we investigate other PRs opened against the corresponding release branch and cherry-pick additional fixes, but this is limited. Our recommendation is to always use the latest version.
If someone identifies the fix commit for this and makes a cherrypick against the r1.15 branch, we would be able to apply it when we do the release. Otherwise, unfortunately, we don't have enough cycles to cherry-pick all fix commits on all live branches
		</comment>
		<comment id='5' author='marwan116' date='2020-04-20T21:53:31Z'>
		I am closing this issue as it was resolved in recent tf-nightly. This will be available in stable TF2.2 which will be released in near future. Thanks!
		</comment>
		<comment id='6' author='marwan116' date='2020-04-20T21:53:33Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29624&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29624&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>