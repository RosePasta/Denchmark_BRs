<bug id='33764' author='netw0rkf10w' open_date='2019-10-27T20:32:42Z' closed_time='2020-01-29T01:16:58Z'>
	<summary>Unnecessary tf.distribute.MirroredStrategy() scopes in the distributed custom training tutorial</summary>
	<description>
&lt;denchmark-h:h2&gt;URL(s) with the issue:&lt;/denchmark-h&gt;

&lt;denchmark-link:https://www.tensorflow.org/tutorials/distribute/custom_training&gt;https://www.tensorflow.org/tutorials/distribute/custom_training&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;Description of issue (what needs changing):&lt;/denchmark-h&gt;

In the tutorial,  appears almost everywhere, which gives the impression that it is required that those locations. However, when checking &lt;denchmark-link:https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/densenet/distributed_train.py&gt;this example&lt;/denchmark-link&gt;
, I found that the scope is not required at all! Only  and  suffice. Therefore, I would propose to modify the tutorial code to keep only a minimum amount of  when necessary (e.g. when defining the model).
&lt;denchmark-h:h3&gt;Submit a pull request?&lt;/denchmark-h&gt;

Yes, I can create a PR but only when you have approved that this is valid. Thanks.
	</description>
	<comments>
		<comment id='1' author='netw0rkf10w' date='2020-01-29T01:16:57Z'>
		The example has been modified to use scopes wherever necessary. Since it is a notebook and all cells are independent, we need to add scopes. This is not true for python files where you can only have one scope and everything underneath it.
Closing, feel free to reopen.
		</comment>
		<comment id='2' author='netw0rkf10w' date='2020-01-29T09:49:55Z'>
		&lt;denchmark-link:https://github.com/yashk2810&gt;@yashk2810&lt;/denchmark-link&gt;
 You didn't get it. The point is that not all code should be under the strategy scope.
For example, look at this code in the tutorial:
with strategy.scope():
  def train_step(inputs):
    images, labels = inputs

    with tf.GradientTape() as tape:
      predictions = model(images, training=True)
      loss = compute_loss(labels, predictions)

    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))

    train_accuracy.update_state(labels, predictions)
    return loss 

  def test_step(inputs):
    images, labels = inputs

    predictions = model(images, training=False)
    t_loss = loss_object(labels, predictions)

    test_loss.update_state(t_loss)
    test_accuracy.update_state(labels, predictions)
Do def train_step() and def test_step() have to be inside the scope? I don't think so.
Another example:
with strategy.scope():
  # `experimental_run_v2` replicates the provided computation and runs it
  # with the distributed input.
  @tf.function
  def distributed_train_step(dataset_inputs):
    per_replica_losses = strategy.experimental_run_v2(train_step,
                                                      args=(dataset_inputs,))
    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,
                           axis=None)
 
  @tf.function
  def distributed_test_step(dataset_inputs):
    return strategy.experimental_run_v2(test_step, args=(dataset_inputs,))

  for epoch in range(EPOCHS):
    # TRAIN LOOP
    total_loss = 0.0
    num_batches = 0
    for x in train_dist_dataset:
      total_loss += distributed_train_step(x)
      num_batches += 1
    train_loss = total_loss / num_batches

    # TEST LOOP
    for x in test_dist_dataset:
      distributed_test_step(x)

    if epoch % 2 == 0:
      checkpoint.save(checkpoint_prefix)

    template = ("Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, "
                "Test Accuracy: {}")
    print (template.format(epoch+1, train_loss,
                           train_accuracy.result()*100, test_loss.result(),
                           test_accuracy.result()*100))

    test_loss.reset_states()
    train_accuracy.reset_states()
    test_accuracy.reset_states()
I don't see why the scope is necessary here.
The rest of the tutorial contains a few more examples like this. My apologies if I said something incorrect.
		</comment>
	</comments>
</bug>