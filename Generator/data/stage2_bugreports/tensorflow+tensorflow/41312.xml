<bug id='41312' author='yadavshashank' open_date='2020-07-12T04:33:44Z' closed_time='2020-07-29T12:32:08Z'>
	<summary>model.fit() InvalidArgumentError:  indices[28,13] = -2147483648 is not in [0, 1193514)</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
TensorFlow installed from (source or binary): Conda
TensorFlow version (use command below): 2.1.0
Python version: 3.6.10
Bazel version (if compiling from source): NA
GCC/Compiler version (if compiling from source): NA
CUDA/cuDNN version: NA
GPU model and memory: Disabled (Hardcoding TensorFlow without GPU)

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with:

TF 1.0: python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
TF 2.0: python -c "import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"

Describe the current behavior
I try to fit my model and face this problem, for some reason, the largest negative number is being looked up.
Describe the expected behavior
The model should proceed through ftting seamlessly
Standalone code to reproduce the issue
-&gt;Embedding layer
&lt;denchmark-code&gt;def pretrained_embedding_layer(word_to_vec_map, word_to_index):
   

    vocab_len = len(word_to_index) + 1            #1193514      
    emb_matrix = np.zeros((vocab_len,embedding_dim))
    for word, idx in word_to_index.items():
        emb_matrix[idx, :] = word_to_vec_map[word]

    # Definning a pre-trained Embedding layer
    embedding_layer = layers.Embedding(
                        vocab_len,
                        embedding_dim,
                        trainable = False
                        )

    # Build the embedding layer, it is required before setting the weights of the embedding layer. 
    embedding_layer.build((None,))
    
    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.
    embedding_layer.set_weights([emb_matrix])
    
    return embedding_layer
&lt;/denchmark-code&gt;

-&gt;Model
&lt;denchmark-code&gt;def sentiment_model(input_shape, word_to_vec_map, word_to_index):


    sentence_indices =layers.Input(shape=input_shape, dtype='float32')
    
    # Create the embedding layer pretrained with GloVe Vectors
    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)
    
    # Propagate sentence_indices through your embedding layer
    # (See additional hints in the instructions).
    embeddings = embedding_layer(sentence_indices)   

    x = layers.LSTM(128)(embeddings)
    x = layers.Dropout(0.5)(x)
    predictions = layers.Dense(2, activation="sigmoid", name="predictions")(x)
    
    # Create Model instance which converts sentence_indices into X.
    model = keras.Model(inputs=sentence_indices,outputs=predictions)   
    return model
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;def sentences_to_indices(X, word_to_index, max_len):

    X_indices = np.zeros((m,max_len))
    
    # Assign indices to words
    for i,sentence in enumerate(X):        
        sentence_words = sentence.lower().split()
        for j,word in enumerate(sentence_words):
            X_indices[i, j] = word_to_index[word]
    return X_indices
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;X_train_indices = sentences_to_indices(X_train, word_to_index, max_features)
Y_train_OH = to_categorical(Y_train)
model.fit(X_train_indices, Y_train_OH, epochs = 10, batch_size = 32)
&lt;/denchmark-code&gt;

Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
Train on 28624 samples
Epoch 1/10
32/28624 [..............................] - ETA: 15:20
InvalidArgumentError                      Traceback (most recent call last)
 in 
----&gt; 1 model.fit(X_train_indices, Y_train_OH, epochs = 10, batch_size = 32)
~\Anaconda3\envs\sentiment_analysis\lib\site-packages\tensorflow_core\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
817         max_queue_size=max_queue_size,
818         workers=workers,
--&gt; 819         use_multiprocessing=use_multiprocessing)
820
821   def evaluate(self,
~\Anaconda3\envs\sentiment_analysis\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
340                 mode=ModeKeys.TRAIN,
341                 training_context=training_context,
--&gt; 342                 total_epochs=epochs)
343             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)
344
~\Anaconda3\envs\sentiment_analysis\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)
126         step=step, mode=mode, size=current_batch_size) as batch_logs:
127       try:
--&gt; 128         batch_outs = execution_function(iterator)
129       except (StopIteration, errors.OutOfRangeError):
130         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?
~\Anaconda3\envs\sentiment_analysis\lib\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py in execution_function(input_fn)
96     # numpy translates Tensors to values in Eager mode.
97     return nest.map_structure(_non_none_constant_value,
---&gt; 98                               distributed_function(input_fn))
99
100   return execution_function
~\Anaconda3\envs\sentiment_analysis\lib\site-packages\tensorflow_core\python\eager\def_function.py in call(self, *args, **kwds)
566         xla_context.Exit()
567     else:
--&gt; 568       result = self._call(*args, **kwds)
569
570     if tracing_count == self._get_tracing_count():
~\Anaconda3\envs\sentiment_analysis\lib\site-packages\tensorflow_core\python\eager\def_function.py in _call(self, *args, **kwds)
630         # Lifting succeeded, so variables are initialized and we can run the
631         # stateless function.
--&gt; 632         return self._stateless_fn(*args, **kwds)
633     else:
634       canon_args, canon_kwds = \
~\Anaconda3\envs\sentiment_analysis\lib\site-packages\tensorflow_core\python\eager\function.py in (self, *args, **kwargs)
2361     with self._lock:
2362       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
-&gt; 2363     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
2364
2365   &lt;denchmark-link:https://github.com/Property&gt;@Property&lt;/denchmark-link&gt;

~\Anaconda3\envs\sentiment_analysis\lib\site-packages\tensorflow_core\python\eager\function.py in _filtered_call(self, args, kwargs)
1609          if isinstance(t, (ops.Tensor,
1610                            resource_variable_ops.BaseResourceVariable))),
-&gt; 1611         self.captured_inputs)
1612
1613   def _call_flat(self, args, captured_inputs, cancellation_manager=None):
~\Anaconda3\envs\sentiment_analysis\lib\site-packages\tensorflow_core\python\eager\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
1690       # No tape is watching; skip to running the function.
1691       return self._build_call_outputs(self._inference_function.call(
-&gt; 1692           ctx, args, cancellation_manager=cancellation_manager))
1693     forward_backward = self._select_forward_and_backward_functions(
1694         args,
~\Anaconda3\envs\sentiment_analysis\lib\site-packages\tensorflow_core\python\eager\function.py in call(self, ctx, args, cancellation_manager)
543               inputs=args,
544               attrs=("executor_type", executor_type, "config_proto", config),
--&gt; 545               ctx=ctx)
546         else:
547           outputs = execute.execute_with_cancellation(
~\Anaconda3\envs\sentiment_analysis\lib\site-packages\tensorflow_core\python\eager\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
65     else:
66       message = e.message
---&gt; 67     six.raise_from(core._status_to_exception(e.code, message), None)
68   except TypeError as e:
69     keras_symbolic_tensors = [
~\Anaconda3\envs\sentiment_analysis\lib\site-packages\six.py in raise_from(value, from_value)
InvalidArgumentError:  indices[15,2] = -2147483648 is not in [0, 1193514)
[[node model_1/embedding_1/embedding_lookup (defined at :1) ]] [Op:__inference_distributed_function_6120]
Errors may have originated from an input operation.
Input Source operations connected to node model_1/embedding_1/embedding_lookup:
model_1/embedding_1/embedding_lookup/4992 (defined at C:\Users\shash\Anaconda3\envs\sentiment_analysis\lib\contextlib.py:81)
Function call stack:
distributed_function
	</description>
	<comments>
		<comment id='1' author='yadavshashank' date='2020-07-12T06:52:47Z'>
		The problem is when the words are being replaced by their corresponding index. If the word wasn't found in the vocabulary/word_to_index dictionary it was being stored as nan.
The vocabulary is all the words present in the word embeddings (I have used GloVe twitter embeddings).
Modified function:
&lt;denchmark-code&gt;def sentences_to_indices(X, word_to_index, max_len):

X_indices = np.zeros((m,max_len))

# Assign indices to words
for i,sentence in enumerate(X):        
    sentence_words = sentence.lower().split()
    for j,word in enumerate(sentence_words):
        X_indices[i, j] = word_to_index.get(word,0)
return X_indices
&lt;/denchmark-code&gt;

Though, I am not sure if words not present in word embeddings should be stored as zero. What should they be stored as?
		</comment>
		<comment id='2' author='yadavshashank' date='2020-07-15T10:53:13Z'>
		@shashank1558
I ran the code shared and face a different error, can you please share complete code or share a colab gist with the error faced.
please find &lt;denchmark-link:https://colab.research.google.com/gist/Saduf2019/73029ee3ca93a982638955237c91b89e/untitled265.ipynb&gt;the gist&lt;/denchmark-link&gt;
 or error faced.
		</comment>
		<comment id='3' author='yadavshashank' date='2020-07-22T11:52:11Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='4' author='yadavshashank' date='2020-07-29T12:32:00Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='5' author='yadavshashank' date='2020-07-29T12:32:09Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41312&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41312&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>