<bug id='44502' author='swertz' open_date='2020-11-01T14:36:00Z' closed_time='2020-11-16T20:52:43Z'>
	<summary>Jacobian fails on gradient of tf.function with if-elif-else or nested tf.cond</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
TensorFlow installed from (source or binary): binary (pip)
TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1
Python version: 3.8.5
CUDA/cuDNN version: executing on CPU
GPU model and memory: executing on CPU

Describe the current behavior
Computing the GradientTape.jacobian of the gradient (i.e. the Hessian) of a tf.function with either an if-elif-else construct or nested tf.cond results in a crash:
&lt;denchmark-code&gt;tensorflow.python.framework.errors_impl.InvalidArgumentError:  Trying to add unsupported dtype 10
         [[node gradients/AddN_2 (defined at debug.py:44) ]] [Op:__inference___backward___backward_f_bad_373_1158_1489]
&lt;/denchmark-code&gt;

See full trace attached as &lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/5471053/trace_without_pfor.txt&gt;trace_without_pfor.txt&lt;/denchmark-link&gt;
.
The computation works fine when disabling  tf.functions.
First-order derivatives (gradient or jacobian) work fine too.
Describe the expected behavior
Hessian evaluates to tf.Tensor([[2.]], shape=(1, 1), dtype=float32).
Standalone code to reproduce the issue
The following works if use_function = False, but fails for both f_bad and f_bad_cond when using use_function = True. Both f_good and f_good_const always work fine.
&lt;denchmark-code&gt;import tensorflow as tf

use_function = True
use_pfor = False

tf.config.run_functions_eagerly(not use_function)

@tf.function
def f_bad(x):
    if x &lt; -1.:
        return tf.pow(x, 2)
    elif x &lt;= 1.:
        return tf.pow(x, 2)
    else:
        return tf.pow(x, 2)

@tf.function
def f_bad_cond(x):
    return tf.cond(x &lt; -1.,
                   lambda: tf.pow(x, 2),
                   lambda: tf.cond(x &lt;= 1.,
                                   lambda: tf.pow(x, 2),
                                   lambda: tf.pow(x, 2)))

@tf.function
def f_good(x):
    if x &lt; -1.:
        return tf.pow(x, 2)
    else:
        return tf.pow(x, 2)

@tf.function
def f_good_cond(x):
    return tf.cond(x &lt; -1.,
                   lambda: tf.pow(x, 2),
                   lambda: tf.pow(x, 2))

f = f_bad

x = tf.Variable([0.])
with tf.GradientTape(persistent=not use_pfor) as t2:
    with tf.GradientTape() as t1:
        y = f(x)
    g_y = t1.gradient(y, x)
hess = t2.jacobian(g_y, x, experimental_use_pfor=use_pfor)
print(hess)
&lt;/denchmark-code&gt;

Note that using  with  crashes for all four functions (see the attached &lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/5471052/trace_with_pfor.txt&gt;trace_with_pfor.txt&lt;/denchmark-link&gt;
), but that is probably an entirely different issue...
	</description>
	<comments>
		<comment id='1' author='swertz' date='2020-11-02T06:48:41Z'>
		I have tried on colab with TF version 2.3.1, nightly version() and was able to reproduce the issue. Please, find the gist &lt;denchmark-link:https://colab.research.google.com/gist/ravikyram/cf8df22c346976e418fde3edd82057b2/untitled481.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='swertz' date='2020-11-03T18:19:37Z'>
		In nightly, seems even before  the code is failing. The error message is  Seems to be &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/gradients&gt;from tf.gradients&lt;/denchmark-link&gt;
, which raises a lookup error 'if one of the operations between x and y does not have a registered gradient function.'
		</comment>
		<comment id='3' author='swertz' date='2020-11-03T19:20:24Z'>
		The latest nightly is a bit out of date (it's from last Thursday). I verified that the non-pfor issues have been fixed, likely by &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/07b75ffa453b1ec9189371244d21b6684503340b&gt;07b75ff&lt;/denchmark-link&gt;
.
I'll look into adding vectorization for optionals.
		</comment>
		<comment id='4' author='swertz' date='2020-11-16T20:52:44Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44502&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44502&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='swertz' date='2020-11-16T20:56:00Z'>
		That should handle the pfor case. Please let me know if anything related to this bug is still broken.
		</comment>
		<comment id='6' author='swertz' date='2020-11-18T08:57:56Z'>
		Should be fixed, thanks a lot!!
		</comment>
		<comment id='7' author='swertz' date='2020-11-18T16:51:52Z'>
		The pfor part of the fix was rolled back. Should get back in soon. But the non-pfor bit will still work.
		</comment>
	</comments>
</bug>