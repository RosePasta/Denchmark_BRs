<bug id='36643' author='1-willAll-0' open_date='2020-02-11T03:40:59Z' closed_time='2020-04-16T04:08:01Z'>
	<summary>Error Thrown When Decoration Used: Suggests Using Decoration</summary>
	<description>


I am going through the DeepDream tutorial for Tensorflow 2.0 located here:
https://www.tensorflow.org/tutorials/generative/deepdream


OS Platform and Distribution: Clear Linux OS, VERSION_ID=32270


TensorFlow installed from (source or binary): clear linux machine-learning-tensorflow bundle


TensorFlow version (use command below): 2.0.0


Python version: 3.8.1


CPU model and memory: Intel(R) Xeon(R) Gold 6136 CPU @ 3.00GHz


Describe the current behavior
This works:
&lt;denchmark-code&gt;class DeepDream(tf.Module):
  def __init__(self, model):
    self.model = model

# !!!!! NOTICE THAT THESE LINES ARE COMMENTED OUT !!!!!
# :
#   @tf.function(
#       input_signature=(
#         tf.TensorSpec(shape=[None,None,3], dtype=tf.float32),
#         tf.TensorSpec(shape=[], dtype=tf.int32),
#         tf.TensorSpec(shape=[], dtype=tf.float32),)
#   )
  def __call__(self, img, steps, step_size):
      print("Tracing")
      loss = tf.constant(0.0)
      for n in tf.range(steps):
        with tf.GradientTape() as tape:
          # This needs gradients relative to `img`
          # `GradientTape` only watches `tf.Variable`s by default
          tape.watch(img)
          loss = calc_loss(img, self.model)

        # Calculate the gradient of the loss with respect to the pixels of the input image.
        gradients = tape.gradient(loss, img)

        # Normalize the gradients.
        gradients /= tf.math.reduce_std(gradients) + 1e-8 

        # In gradient ascent, the "loss" is maximized so that the input image increasingly "excites" the layers.
        # You can update the image by directly adding the gradients (because they're the same shape!)
        img = img + gradients*step_size
        img = tf.clip_by_value(img, -1, 1)

      return loss, img
&lt;/denchmark-code&gt;

This (as is given in the tutorial) does not:
&lt;denchmark-code&gt;class DeepDream(tf.Module):
  def __init__(self, model):
    self.model = model

  @tf.function(
      input_signature=(
        tf.TensorSpec(shape=[None,None,3], dtype=tf.float32),
        tf.TensorSpec(shape=[], dtype=tf.int32),
        tf.TensorSpec(shape=[], dtype=tf.float32),)
  )
  def __call__(self, img, steps, step_size):
      print("Tracing")
      loss = tf.constant(0.0)
      for n in tf.range(steps):
        with tf.GradientTape() as tape:
          # This needs gradients relative to `img`
          # `GradientTape` only watches `tf.Variable`s by default
          tape.watch(img)
          loss = calc_loss(img, self.model)

        # Calculate the gradient of the loss with respect to the pixels of the input image.
        gradients = tape.gradient(loss, img)

        # Normalize the gradients.
        gradients /= tf.math.reduce_std(gradients) + 1e-8 

        # In gradient ascent, the "loss" is maximized so that the input image increasingly "excites" the layers.
        # You can update the image by directly adding the gradients (because they're the same shape!)
        img = img + gradients*step_size
        img = tf.clip_by_value(img, -1, 1)

      return loss, img
&lt;/denchmark-code&gt;

I've put the error that's thrown in the sections below.
Describe the expected behavior
I would expect it to work with the decoration tag and (perhaps) not work with the decoration tag commented out. However when I run it with the decoration it throws an error that seems to suggest adding the decoration that appears to already be there, and when I comment out the decoration tag it works.  When I use the decoration tag the final error is:
OperatorNotAllowedInGraphError: iterating over tf.Tensor is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.
(more detailed log below)
However when I remove the decoration by commenting it out, that is when it works.
NOTE:  I have Tensorflow 2.0.0 setup on google collab as well with python 3.6.9 and it does not display the same behavior.  It works either with or without the decoration.
All of this was installed on a fresh Clear Linux install tonight with no tinkering.  I had another installation earlier where I tried a fix seen elsewhere which included downgrading gast to 0.2.2.  I did that and that did not work.
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
&lt;denchmark-code&gt;# [CURRENT DEEP DREAM TUTORIAL AT: https://www.tensorflow.org/tutorials/generative/deepdream

def run_deep_dream_simple(img, steps=100, step_size=0.01):
  # Convert from uint8 to the range expected by the model.
  img = tf.keras.applications.inception_v3.preprocess_input(img)
  img = tf.convert_to_tensor(img)
  step_size = tf.convert_to_tensor(step_size)
  steps_remaining = steps
  step = 0
  while steps_remaining:
    if steps_remaining&gt;100:
      run_steps = tf.constant(100)
    else:
      run_steps = tf.constant(steps_remaining)
    steps_remaining -= run_steps
    step += run_steps

    loss, img = deepdream(img, run_steps, tf.constant(step_size))
    
    display.clear_output(wait=True)
    show(deprocess(img))
    print ("Step {}, loss {}".format(step, loss))


  result = deprocess(img)
  display.clear_output(wait=True)
  show(result)

  return result
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;from datetime import datetime
startTime = datetime.now()

dream_img = run_deep_dream_simple(img=original_img, 
                                  steps=100, step_size=0.01)

print(datetime.now() - startTime)
&lt;/denchmark-code&gt;

Other info / logs
&lt;denchmark-code&gt;WARNING:tensorflow:Entity &lt;bound method DeepDream.__call__ of &lt;tensorflow.python.eager.function.TfMethodTarget object at 0x7faea2d46d30&gt;&gt; could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: invalid value for "node": expected "ast.AST", got "&lt;class 'NoneType'&gt;"; to visit lists of nodes, use "visit_block" instead
WARNING: Entity &lt;bound method DeepDream.__call__ of &lt;tensorflow.python.eager.function.TfMethodTarget object at 0x7faea2d46d30&gt;&gt; could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: invalid value for "node": expected "ast.AST", got "&lt;class 'NoneType'&gt;"; to visit lists of nodes, use "visit_block" instead
Tracing
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/api.py in converted_call(f, options, args, kwargs, caller_fn_scope)
    505         options=options, autograph_module=tf_inspect.getmodule(converted_call))
--&gt; 506     converted_f = conversion.convert(target_entity, program_ctx)
    507 

/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/conversion.py in convert(entity, program_ctx)
    320 
--&gt; 321   converted_entity_info = _convert_with_cache(entity, program_ctx,
    322                                               free_nonglobal_var_names)

/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/conversion.py in _convert_with_cache(entity, program_ctx, free_nonglobal_var_names)
    238 
--&gt; 239     nodes, converted_name, entity_info = convert_entity_to_ast(
    240         entity, program_ctx)

/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/conversion.py in convert_entity_to_ast(o, program_ctx)
    470   elif tf_inspect.ismethod(o):
--&gt; 471     nodes, name, entity_info = convert_func_to_ast(o, program_ctx)
    472   elif hasattr(o, '__class__'):

/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/conversion.py in convert_func_to_ast(f, program_ctx, do_rename)
    668   context = converter.EntityContext(namer, entity_info, program_ctx, new_name)
--&gt; 669   node = node_to_graph(node, context)
    670 

/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/conversion.py in node_to_graph(node, context)
    697 
--&gt; 698   node = converter.standard_analysis(node, context, is_initial=True)
    699   node = converter.apply_(node, context, function_scopes)

/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/core/converter.py in standard_analysis(node, context, is_initial)
    383   node = qual_names.resolve(node)
--&gt; 384   node = activity.resolve(node, context, None)
    385   node = reaching_definitions.resolve(node, context, graphs, AnnotatedDef)

/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/static_analysis/activity.py in resolve(node, context, parent_scope)
    497 def resolve(node, context, parent_scope=None):
--&gt; 498   return ActivityAnalyzer(context, parent_scope).visit(node)

/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/transformer.py in visit(self, node)
    479     if not anno.hasanno(node, anno.Basic.SKIP_PROCESSING):
--&gt; 480       result = super(Base, self).visit(node)
    481     self.ctx.current_origin = parent_origin

/usr/lib/python3.8/ast.py in visit(self, node)
    359         visitor = getattr(self, method, self.generic_visit)
--&gt; 360         return visitor(node)
    361 

/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/static_analysis/activity.py in visit_FunctionDef(self, node)
    441     self._enter_scope(False)
--&gt; 442     node.body = self.visit_block(node.body)
    443     anno.setanno(node, NodeAnno.BODY_SCOPE, self.scope)

/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/transformer.py in visit_block(self, nodes, before_visit, after_visit)
    370 
--&gt; 371       replacement = self.visit(node)
    372 

/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/transformer.py in visit(self, node)
    479     if not anno.hasanno(node, anno.Basic.SKIP_PROCESSING):
--&gt; 480       result = super(Base, self).visit(node)
    481     self.ctx.current_origin = parent_origin

/usr/lib/python3.8/ast.py in visit(self, node)
    359         visitor = getattr(self, method, self.generic_visit)
--&gt; 360         return visitor(node)
    361 

/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/static_analysis/activity.py in visit_Expr(self, node)
    265   def visit_Expr(self, node):
--&gt; 266     return self._process_statement(node)
    267 

/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/static_analysis/activity.py in _process_statement(self, node)
    259     self._enter_scope(False)
--&gt; 260     node = self.generic_visit(node)
    261     anno.setanno(node, anno.Static.SCOPE, self.scope)

/usr/lib/python3.8/ast.py in generic_visit(self, node)
    444             elif isinstance(old_value, AST):
--&gt; 445                 new_node = self.visit(old_value)
    446                 if new_node is None:

/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/transformer.py in visit(self, node)
    479     if not anno.hasanno(node, anno.Basic.SKIP_PROCESSING):
--&gt; 480       result = super(Base, self).visit(node)
    481     self.ctx.current_origin = parent_origin

/usr/lib/python3.8/ast.py in visit(self, node)
    359         visitor = getattr(self, method, self.generic_visit)
--&gt; 360         return visitor(node)
    361 

/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/static_analysis/activity.py in visit_Call(self, node)
    327     self._enter_scope(False)
--&gt; 328     node.args = self.visit_block(node.args)
    329     node.keywords = self.visit_block(node.keywords)

/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/transformer.py in visit_block(self, nodes, before_visit, after_visit)
    370 
--&gt; 371       replacement = self.visit(node)
    372 

/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/transformer.py in visit(self, node)
    457                  type(node))
--&gt; 458       raise ValueError(msg)
    459 

ValueError: invalid value for "node": expected "ast.AST", got "&lt;class 'NoneType'&gt;"; to visit lists of nodes, use "visit_block" instead

During handling of the above exception, another exception occurred:

OperatorNotAllowedInGraphError            Traceback (most recent call last)
&lt;ipython-input-9-43912bcedaa9&gt; in &lt;module&gt;
      2 startTime = datetime.now()
      3 
----&gt; 4 dream_img = run_deep_dream_simple(img=original_img, 
      5                                   steps=100, step_size=0.01)
      6 

&lt;ipython-input-8-193f882b4182&gt; in run_deep_dream_simple(img, steps, step_size)
     14     step += run_steps
     15 
---&gt; 16     loss, img = deepdream(img, run_steps, tf.constant(step_size))
     17 
     18     display.clear_output(wait=True)

/usr/lib/python3.8/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)
    455 
    456     tracing_count = self._get_tracing_count()
--&gt; 457     result = self._call(*args, **kwds)
    458     if tracing_count == self._get_tracing_count():
    459       self._call_counter.called_without_tracing()

/usr/lib/python3.8/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)
    501       # This is the first call of __call__, so we have to initialize.
    502       initializer_map = object_identity.ObjectIdentityDictionary()
--&gt; 503       self._initialize(args, kwds, add_initializers_to=initializer_map)
    504     finally:
    505       # At this point we know that the initialization is complete (or less

/usr/lib/python3.8/site-packages/tensorflow_core/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    405     self._graph_deleter = FunctionDeleter(self._lifted_initializer_graph)
    406     self._concrete_stateful_fn = (
--&gt; 407         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
    408             *args, **kwds))
    409 

/usr/lib/python3.8/site-packages/tensorflow_core/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   1846     if self.input_signature:
   1847       args, kwargs = None, None
-&gt; 1848     graph_function, _, _ = self._maybe_define_function(args, kwargs)
   1849     return graph_function
   1850 

/usr/lib/python3.8/site-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   2148         graph_function = self._function_cache.primary.get(cache_key, None)
   2149         if graph_function is None:
-&gt; 2150           graph_function = self._create_graph_function(args, kwargs)
   2151           self._function_cache.primary[cache_key] = graph_function
   2152         return graph_function, args, kwargs

/usr/lib/python3.8/site-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   2029     arg_names = base_arg_names + missing_arg_names
   2030     graph_function = ConcreteFunction(
-&gt; 2031         func_graph_module.func_graph_from_py_func(
   2032             self._name,
   2033             self._python_function,

/usr/lib/python3.8/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    913                                           converted_func)
    914 
--&gt; 915       func_outputs = python_func(*func_args, **func_kwargs)
    916 
    917       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/lib/python3.8/site-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    356         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    357         # the function a weak reference to itself to avoid a reference cycle.
--&gt; 358         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    359     weak_wrapped_fn = weakref.ref(wrapped_fn)
    360 

/usr/lib/python3.8/site-packages/tensorflow_core/python/eager/function.py in bound_method_wrapper(*args, **kwargs)
   2656     # However, the replacer is still responsible for attaching self properly.
   2657     # TODO(mdan): Is it possible to do it here instead?
-&gt; 2658     return wrapped_fn(*args, **kwargs)
   2659   weak_bound_method_wrapper = weakref.ref(bound_method_wrapper)
   2660 

/usr/lib/python3.8/site-packages/tensorflow_core/python/framework/func_graph.py in wrapper(*args, **kwargs)
    894           # TODO(mdan): Push this block higher in tf.function's call stack.
    895           try:
--&gt; 896             return autograph.converted_call(
    897                 original_func,
    898                 autograph.ConversionOptions(

/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/api.py in converted_call(f, options, args, kwargs, caller_fn_scope)
    532         ' the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and'
    533         ' attach the full output. Cause: %s', target_entity, e)
--&gt; 534     return _call_unconverted(f, args, kwargs, options)
    535 
    536   with StackTraceMapper(converted_f), tf_stack.CurrentModuleFilter():

/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/api.py in _call_unconverted(f, args, kwargs, options, update_cache)
    324 
    325   if inspect_utils.istfmethodtarget(f):
--&gt; 326     return f.__self__.call(args, kwargs)
    327 
    328   try:

/usr/lib/python3.8/site-packages/tensorflow_core/python/eager/function.py in call(self, args, kwargs)
   2618     if tf_inspect.ismethod(wrapped_fn):
   2619       wrapped_fn = six.get_unbound_function(wrapped_fn)
-&gt; 2620     return wrapped_fn(self.weakrefself_target__(), *args, **kwargs)
   2621 
   2622 

&lt;ipython-input-6-3ac13775042b&gt; in __call__(self, img, steps, step_size)
     12       print("Tracing")
     13       loss = tf.constant(0.0)
---&gt; 14       for n in tf.range(steps):
     15         with tf.GradientTape() as tape:
     16           # This needs gradients relative to `img`

/usr/lib/python3.8/site-packages/tensorflow_core/python/framework/ops.py in __iter__(self)
    545   def __iter__(self):
    546     if not context.executing_eagerly():
--&gt; 547       self._disallow_iteration()
    548 
    549     shape = self._shape_tuple()

/usr/lib/python3.8/site-packages/tensorflow_core/python/framework/ops.py in _disallow_iteration(self)
    538       self._disallow_when_autograph_disabled("iterating over `tf.Tensor`")
    539     elif ag_ctx.control_status_ctx().status == ag_ctx.Status.ENABLED:
--&gt; 540       self._disallow_when_autograph_enabled("iterating over `tf.Tensor`")
    541     else:
    542       # Default: V1-style Graph execution.

/usr/lib/python3.8/site-packages/tensorflow_core/python/framework/ops.py in _disallow_when_autograph_enabled(self, task)
    514 
    515   def _disallow_when_autograph_enabled(self, task):
--&gt; 516     raise errors.OperatorNotAllowedInGraphError(
    517         "{} is not allowed: AutoGraph did not convert this function. Try"
    518         " decorating it directly with @tf.function.".format(task))

OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.
&lt;/denchmark-code&gt;

Thank you!
	</description>
	<comments>
		<comment id='1' author='1-willAll-0' date='2020-02-11T21:55:01Z'>
		&lt;denchmark-link:https://github.com/1-willAll-0&gt;@1-willAll-0&lt;/denchmark-link&gt;
 I cannot reproduce the issue. I am using most recent . Please check the &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/8e144b09ad3992b36b798db1ec6e4c46/untitled818.ipynb&gt;gist here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='2' author='1-willAll-0' date='2020-02-12T01:53:14Z'>
		Thanks for the response, jvishnuvardhan.  Perhaps bugs are only relevant to post if I'm using the most updated version?  If so I won't post next time.  I did already mention I myself couldn't reproduce the issue in collab, so maybe that's a non-starter.
I don't want to start building now and disrupt the continuity of the managed bundles in Clear Linux, otherwise I'd be using 2.1.0.  As of now I'm on 2.0.0 and I have a workaround which may or may not pan out as I proceed.  I suppose this is because I'm running Python 3.8?
		</comment>
		<comment id='3' author='1-willAll-0' date='2020-02-12T19:03:44Z'>
		&lt;denchmark-link:https://github.com/1-willAll-0&gt;@1-willAll-0&lt;/denchmark-link&gt;


Thanks for the response, jvishnuvardhan. Perhaps bugs are only relevant to post if I'm using the most updated version? If so I won't post next time. I did already mention I myself couldn't reproduce the issue in collab, so maybe that's a non-starter.

No. we encourage you to post issues with older versions also. I just checked with latest version to see whether the issue is still there in the latest version or not. I also checked with TF2.0 and TF2.1 and I cannot reproduce the issue as well. I noticed your note about colab.
Let me check with my laptop and respond to you soon. Thanks!
		</comment>
		<comment id='4' author='1-willAll-0' date='2020-02-14T00:39:05Z'>
		Ah, good to know!
Thanks for looking into it!
		</comment>
		<comment id='5' author='1-willAll-0' date='2020-02-14T21:20:39Z'>
		&lt;denchmark-link:https://github.com/1-willAll-0&gt;@1-willAll-0&lt;/denchmark-link&gt;
 On My MacBook Pro, I have install 2.0 and ran your code in the gist and I could not see any error with the code. Not sure why you are facing that issue. May be try uninstalling and reinstalling the TF and then check. Thanks!
		</comment>
		<comment id='6' author='1-willAll-0' date='2020-04-16T04:08:01Z'>
		I am closing this as this was resolved. Feel free to reopen the issue if this was not resolved for you. Thanks!
		</comment>
		<comment id='7' author='1-willAll-0' date='2020-04-16T04:08:03Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36643&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36643&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>