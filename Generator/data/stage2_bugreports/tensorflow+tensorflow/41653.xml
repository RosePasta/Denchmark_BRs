<bug id='41653' author='haifengkao' open_date='2020-07-23T09:05:04Z' closed_time='2020-07-23T09:13:07Z'>
	<summary>tf.lite.TFLiteConverter produces inconsistent converted model</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX 10.15.5- TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): v1.12.1-37224-ga6cd18a133 2.4.0-dev20200722
Python version: 3.7

Describe the current behavior
tf.lite.TFLiteConverter.from_saved_model converts LSTM to  fused op, but tf.lite.TFLiteConverter.from_concrete_functions doesn't.
Describe the expected behavior
The converted result should be the same for the same model.
Standalone code to reproduce the issue
&lt;denchmark-code&gt;# produce fused LSTM
import numpy as np
import tensorflow as tf

model = tf.keras.models.Sequential([
    tf.keras.layers.Input(shape=(28, 28), name='input'),
    tf.keras.layers.Bidirectional(
        tf.keras.layers.LSTM(20, return_sequences=True)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax, name='output')
])
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
model.summary()

run_model = tf.function(lambda x: model(x))
# This is important, let's fix the input size.
BATCH_SIZE = 1
STEPS = 28
INPUT_SIZE = 28
concrete_func = run_model.get_concrete_function(
    tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], model.inputs[0].dtype))

# model directory.
MODEL_DIR = "keras_lstm"

# converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])
converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)
tflite_model = converter.convert()

with tf.io.gfile.GFile('tflite_test.tflite', 'wb') as f:
    f.write(tflite_model)
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;# produce original LSTM
import numpy as np
import tensorflow as tf

model = tf.keras.models.Sequential([
    tf.keras.layers.Input(shape=(28, 28), name='input'),
    tf.keras.layers.Bidirectional(
        tf.keras.layers.LSTM(20, return_sequences=True)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax, name='output')
])
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
model.summary()

run_model = tf.function(lambda x: model(x))
# This is important, let's fix the input size.
BATCH_SIZE = 1
STEPS = 28
INPUT_SIZE = 28
concrete_func = run_model.get_concrete_function(
    tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], model.inputs[0].dtype))

# model directory.
MODEL_DIR = "keras_lstm"

converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])
#converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)
tflite_model = converter.convert()

with tf.io.gfile.GFile('tflite_test.tflite', 'wb') as f:
    f.write(tflite_model)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='haifengkao' date='2020-07-23T09:13:07Z'>
		This is an intended behavior by design. Do you have any problems because of this behavior?
		</comment>
		<comment id='2' author='haifengkao' date='2020-07-23T09:13:09Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41653&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41653&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='haifengkao' date='2020-07-23T09:31:38Z'>
		It's strange that the dev described an unexpected program behavior as "intended". This unexpected behavior is not documented at all and will make users spend hours to debug it.
		</comment>
	</comments>
</bug>