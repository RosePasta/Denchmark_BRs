<bug id='39253' author='DocDriven' open_date='2020-05-07T10:19:22Z' closed_time='2020-05-08T07:12:26Z'>
	<summary>[TF Lite C API] TfLiteInterpreterGetOutputTensor returns nullpointer in second iteration</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): GNU/Linux aarch64
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 2.2.0
Python version: 3.6
Bazel version (if compiling from source): 2.0.0
GCC/Compiler version (if compiling from source): gcc version 7.4.0 (Ubuntu/Linaro 7.4.0-1ubuntu1~18.04.1)

Describe the current behavior
I am trying to use the C API for TF Lite on a custom ARM board. I built libtensorflowlite_c.so with select_tf_ops enabled via bazel:
&lt;denchmark-code&gt;bazel build --config=monolithic \
--define=with_select_tf_ops=true \
-c opt //tensorflow/lite/c:tensorflowlite_c \
--config noaws
&lt;/denchmark-code&gt;

I tried to use the API as described as in tensorflow/lite/c/c_api.h, but instead of just doing a single inference, I wanted to have cyclic evaluation of my inputs with the model. To do this, I simply put the code there in a for-loop:
&lt;denchmark-code&gt;#include "tensorflow/lite/c/c_api.h"
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;


int main (int argc, char* argv[]) {

   for(int i = 0; i &lt; 3; i++)
    {
        printf("Iteration: %d\n", i);

        float input[49] = { 0.0 };
        TfLiteModel* model = TfLiteModelCreateFromFile("model.tflite");
        TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();
        TfLiteInterpreterOptionsSetNumThreads(options, 2);
        TfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options);
        TfLiteInterpreterAllocateTensors(interpreter);

        TfLiteTensor* input_tensor = TfLiteInterpreterGetInputTensor(interpreter, 0);
        TfLiteTensorCopyFromBuffer(input_tensor, input, 49 * sizeof(float));

        TfLiteInterpreterInvoke(interpreter);

        const TfLiteTensor* output_tensor = TfLiteInterpreterGetOutputTensor(interpreter, 14);

        float output[49];
        TfLiteTensorCopyToBuffer(output_tensor, output, 49 * sizeof(float));

        printf("Output: \n\n");
        for (int j = 0; j &lt; 49; j++) {
            printf("%d: %f\n", j, output[j]);
        }

        TfLiteInterpreterDelete(interpreter);
        TfLiteInterpreterOptionsDelete(options);
        TfLiteModelDelete(model);
    }
    return 0;
}
&lt;/denchmark-code&gt;

The first iteration runs fine and returns something. But on the second iteration, I get a SegFault when calling TfLiteTensorCopyToBuffer(output_tensor, output, 49 * sizeof(float));. Reason for this is that the previous function TfLiteInterpreterGetOutputTensor returns a nullpointer.
Describe the expected behavior
I expected to run this multiple times without any problems, as I destroy all old instances of variables at the end of the for-loop and thus start a fresh interpreter everytime. Obviously, this is not the case.
Can somebody tell me what is going wrong here? I am currently building the debug version of the shared object, but this will take some time. Any help is appreciated!
	</description>
	<comments>
		<comment id='1' author='DocDriven' date='2020-05-07T13:48:50Z'>
		Faced with similar problem while using TFLite multiple times and still unsolved. When I create multiple TFlite models in one time, the previous models behaves badly. TfLiteInterpreterGetInputTensor gives wrong result.
While debugging, I find that as if the memory inside TFlite interpreter was occupied by others.
		</comment>
		<comment id='2' author='DocDriven' date='2020-05-08T06:52:20Z'>
		&lt;denchmark-link:https://github.com/Huang-Jin&gt;@Huang-Jin&lt;/denchmark-link&gt;

That is interesting, can you provide example code for this? It might be helpful to compare to the example provided by TF.
		</comment>
		<comment id='3' author='DocDriven' date='2020-05-08T07:12:26Z'>
		&lt;denchmark-link:https://github.com/DocDriven&gt;@DocDriven&lt;/denchmark-link&gt;
 you don't need to create interpreter every time.
Just reuse it.
&lt;denchmark-code&gt;#include "tensorflow/lite/c/c_api.h"
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;


int main (int argc, char* argv[]) {

  TfLiteModel* model = TfLiteModelCreateFromFile("model.tflite");
  TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();
  TfLiteInterpreterOptionsSetNumThreads(options, 2);
  TfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options);
  TfLiteInterpreterAllocateTensors(interpreter);
  for(int i = 0; i &lt; 3; i++)
  {
    printf("Iteration: %d\n", i);

    float input[49] = { 0.0 };

    TfLiteTensor* input_tensor = TfLiteInterpreterGetInputTensor(interpreter, 0);
    TfLiteTensorCopyFromBuffer(input_tensor, input, 49 * sizeof(float));

    TfLiteInterpreterInvoke(interpreter);

    const TfLiteTensor* output_tensor = TfLiteInterpreterGetOutputTensor(interpreter, 14);

    float output[49];
    TfLiteTensorCopyToBuffer(output_tensor, output, 49 * sizeof(float));

    printf("Output: \n\n");
    for (int j = 0; j &lt; 49; j++) {
      printf("%d: %f\n", j, output[j]);
    }

  }
  TfLiteInterpreterDelete(interpreter);
  TfLiteInterpreterOptionsDelete(options);
  TfLiteModelDelete(model);
  return 0;
}
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='DocDriven' date='2020-05-08T07:12:28Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39253&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39253&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='DocDriven' date='2020-05-08T07:37:33Z'>
		
@DocDriven you don't need to create interpreter every time.
Just reuse it.
#include "tensorflow/lite/c/c_api.h"
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;


int main (int argc, char* argv[]) {

  TfLiteModel* model = TfLiteModelCreateFromFile("model.tflite");
  TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();
  TfLiteInterpreterOptionsSetNumThreads(options, 2);
  TfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options);
  TfLiteInterpreterAllocateTensors(interpreter);
  for(int i = 0; i &lt; 3; i++)
  {
    printf("Iteration: %d\n", i);

    float input[49] = { 0.0 };

    TfLiteTensor* input_tensor = TfLiteInterpreterGetInputTensor(interpreter, 0);
    TfLiteTensorCopyFromBuffer(input_tensor, input, 49 * sizeof(float));

    TfLiteInterpreterInvoke(interpreter);

    const TfLiteTensor* output_tensor = TfLiteInterpreterGetOutputTensor(interpreter, 14);

    float output[49];
    TfLiteTensorCopyToBuffer(output_tensor, output, 49 * sizeof(float));

    printf("Output: \n\n");
    for (int j = 0; j &lt; 49; j++) {
      printf("%d: %f\n", j, output[j]);
    }

  }
  TfLiteInterpreterDelete(interpreter);
  TfLiteInterpreterOptionsDelete(options);
  TfLiteModelDelete(model);
  return 0;
}


Yes，I used similar codes.
But when I ask for some large memory (like 300KB) after “TfLiteInterpreterAllocateTensors”，the result from interpreter went wrong. It is a very strange thing :(.
		</comment>
		<comment id='6' author='DocDriven' date='2020-05-08T07:41:11Z'>
		It seems that the memory inside interpreter was broken.
Due to security, I can't give the sample codes. sorry!
I'm just faced such problem and when I use valgrind to detect memory leak I found 100+ invalid read in the function "tflite::Interpreter::Invoke".
		</comment>
		<comment id='7' author='DocDriven' date='2020-05-08T07:43:29Z'>
		By the way, I used TfLiteModelCreate rather than TfLiteModelCreateFromFile because I need to read tflite from buffer. Maybe this is the problem? The "file" version doesn't faced such problem.
		</comment>
		<comment id='8' author='DocDriven' date='2020-05-08T07:55:34Z'>
		&lt;denchmark-link:https://github.com/Huang-Jin&gt;@Huang-Jin&lt;/denchmark-link&gt;

I exclusively use the creation from files for my current project, so I cannot tell if this causes the errors you are describing. But I have several models as well, and will test if they behave strangely. If I have the resultls, I can report them to you.
		</comment>
		<comment id='9' author='DocDriven' date='2020-05-14T06:29:36Z'>
		&lt;denchmark-link:https://github.com/DocDriven&gt;@DocDriven&lt;/denchmark-link&gt;
 I think I found the problem. The function "TfLiteModelCreate" needs a "char *" string buffer as input, but it doesn't copy the buffer inside the process. Due to this issue, when I use a local std::string::c_str() as input, the buffer will be deleted when local variable destructed.
		</comment>
	</comments>
</bug>