<bug id='36517' author='raltech' open_date='2020-02-06T16:09:29Z' closed_time='2020-07-13T16:33:23Z'>
	<summary>TensorFlow Lite Conversion Fails (Check failed: start_indices_size &amp;lt;= num_input_axes (2 vs. 1)StridedSlice op requires no more than 1 start indices)</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS 7.5
TensorFlow installed from (source or binary): Tensorflow is preinstalled on the server.
TensorFlow version (or github SHA if from source): Tensorflow/2.0.0

Command used to run the converter or code if you’re using the Python API
&lt;denchmark-code&gt;import tensorflow as tf
# Note: full quantization requires tf 1.15 or higher
import numpy as np
from PIL import Image

def representative_dataset_gen():
  data = np.random.rand(100,416,416,3).astype(dtype=np.float32)
  for i in range(num_calibration_steps):
    # Get sample input data as a numpy array in a method of your choosing.
    # image, = data.take(1)
    image = np.expand_dims(data[i,:,:,:], axis=0)
    yield [image]

num_calibration_steps = 100
saved_model_dir = '/projectnb/ec720prj/nakamura/Spring2020/tf2-yolov3/savedmodel/yolov3/1' # dir of YOLOv3 tf implementation

converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
# converter.representative_dataset = tf.lite.RepresentativeDataset(representative_dataset_gen)
converter.representative_dataset = representative_dataset_gen

# Enforce full integer quantization for all ops and use int input/output
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8

tflite_quant_model = converter.convert()
open("./converted_model.tflite", "wb").write(tflite_quant_model)
&lt;/denchmark-code&gt;

The output from the converter invocation
&lt;denchmark-code&gt;2020-02-06 10:53:25.407210: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX AVX2 FMA
To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2020-02-06 10:53:25.425952: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2394310000 Hz
2020-02-06 10:53:25.427429: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xce6d420 executing computations on platform Host. Devices:
2020-02-06 10:53:25.427467: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2020-02-06 10:53:25.436305: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 28. Tune using inter_op_parallelism_threads for best performance.
2020-02-06 10:53:42.002931: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count &gt;= 8, compute capability &gt;= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)
2020-02-06 10:53:42.003093: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-02-06 10:53:42.111911: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize
2020-02-06 10:53:42.111960: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: Graph size after: 2070 nodes (1696), 5733 edges (5354), time = 66.938ms.
2020-02-06 10:53:42.111973: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 1.093ms.
2020-02-06 10:53:48.038844: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count &gt;= 8, compute capability &gt;= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)
2020-02-06 10:53:48.038997: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-02-06 10:53:49.980217: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize
2020-02-06 10:53:49.980262: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 1268 nodes (-370), 4423 edges (-734), time = 922.534ms.
2020-02-06 10:53:49.980274: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 1268 nodes (0), 4423 edges (0), time = 255.534ms.
Traceback (most recent call last):
  File "coral/coral_quantizer.py", line 38, in &lt;module&gt;
    tflite_quant_model = converter.convert()
  File "/share/pkg.7/tensorflow/2.0.0/install/lib/SCC/../python3.6/site-packages/tensorflow_core/lite/python/lite.py", line 446, in convert
    **converter_kwargs)
  File "/share/pkg.7/tensorflow/2.0.0/install/lib/SCC/../python3.6/site-packages/tensorflow_core/lite/python/convert.py", line 449, in toco_convert_impl
    enable_mlir_converter=enable_mlir_converter)
  File "/share/pkg.7/tensorflow/2.0.0/install/lib/SCC/../python3.6/site-packages/tensorflow_core/lite/python/convert.py", line 200, in toco_convert_protos
    raise ConverterError("See console for info.\n%s\n%s\n" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: See console for info.
2020-02-06 10:54:00.060634: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 676 operators, 1271 arrays (0 quantized)
2020-02-06 10:54:00.077126: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 676 operators, 1271 arrays (0 quantized)
2020-02-06 10:54:00.465306: F tensorflow/lite/toco/graph_transformations/resolve_strided_slice_attributes.cc:95] Check failed: start_indices_size &lt;= num_input_axes (2 vs. 1)StridedSlice op requires no more than 1 start indices
Fatal Python error: Aborted

Current thread 0x00007fbbde05b740 (most recent call first):
  File "/share/pkg.7/tensorflow/2.0.0/install/lib/SCC/../python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py", line 52 in execute
  File "/share/pkg.7/tensorflow/2.0.0/install/lib/SCC/../python3.6/site-packages/absl/app.py", line 250 in _run_main
  File "/share/pkg.7/tensorflow/2.0.0/install/lib/SCC/../python3.6/site-packages/absl/app.py", line 299 in run
  File "/share/pkg.7/tensorflow/2.0.0/install/lib/SCC/../python3.6/site-packages/tensorflow_core/python/platform/app.py", line 40 in run
  File "/share/pkg.7/tensorflow/2.0.0/install/lib/SCC/../python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py", line 89 in main
  File "/share/pkg.7/tensorflow/2.0.0/install/bin/toco_from_protos", line 10 in &lt;module&gt;
&lt;/denchmark-code&gt;

Also, please include a link to the saved model or GraphDef
&lt;denchmark-code&gt;# Put link here or attach to the issue.
https://drive.google.com/a/bu.edu/file/d/1STaqguXrsKaqNbzxmvGVwzpwWZEiC4K6/view?usp=sharing
&lt;/denchmark-code&gt;

Failure details
Hello,
I am trying to run the full YOLOv3 on Coral EdgeTPU. To do this, I am currently trying to convert the savedmodel (.pb format) of trained yolov3 into the quantized .tflite model using the code I attached above.
For some reason, the conversion failed with this error:
&lt;denchmark-code&gt;Check failed: start_indices_size &lt;= num_input_axes (2 vs. 1)StridedSlice op requires no more than 1 start indices
Fatal Python error: Aborted)
&lt;/denchmark-code&gt;

I was able to convert the official pretrained MobileNetv1 savedmodel file into the quantized .tflite model using the exact same code.
I am using this repo (&lt;denchmark-link:https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/4-Object_Detection/YOLOV3&gt;https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/4-Object_Detection/YOLOV3&lt;/denchmark-link&gt;
) for the YOLOv3 Tensorflow 2.0.0 implementation.
Do you know what is causing this failure of conversion?
Thank you.
Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
YOLOv3's implementation code =&gt; (&lt;denchmark-link:https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/4-Object_Detection/YOLOV3&gt;https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/4-Object_Detection/YOLOV3&lt;/denchmark-link&gt;
)
Thank you for your help.
	</description>
	<comments>
		<comment id='1' author='raltech' date='2020-02-07T21:42:20Z'>
		&lt;denchmark-link:https://github.com/raltech&gt;@raltech&lt;/denchmark-link&gt;
 I can reproduce the issue. &lt;denchmark-link:https://colab.research.google.com/gist/jvishnuvardhan/a6cfd1f9542e8159a5248c159115351b/untitled812.ipynb&gt;Here&lt;/denchmark-link&gt;
 is the gist for our reference. Thanks!
		</comment>
		<comment id='2' author='raltech' date='2020-02-09T17:29:24Z'>
		&lt;denchmark-link:https://github.com/miaout17&gt;@miaout17&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 Thank you for your reply. I looked at the gist you shared above; however, the error message you got seems different from mine. In your gist, it seems like the conversion failed because of ResizeNearestNeighbor function? Which, to me, does not make sense because ResizeNearestNeighbor is a supported discretize function according to &lt;denchmark-link:https://coral.ai/docs/edgetpu/models-intro/&gt;this&lt;/denchmark-link&gt;
 page.
Is the difference between our error messages caused by the difference in the minor versions of TensorFlow? (i.e., I used a stable version of tf, but you seem using tf-nightly.)
Thank you!
		</comment>
		<comment id='3' author='raltech' date='2020-02-13T02:49:01Z'>
		&lt;denchmark-link:https://github.com/miaout17&gt;@miaout17&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;

When I switched to tf-nightly, I could reproduce the exact same error as yours.
&lt;denchmark-code&gt;Exception: &lt;unknown&gt;:0: error: loc(fused["model/tf_op_layer_resize/ResizeNearestNeighbor/resize/ResizeNearestNeighbor@__inference__wrapped_model_28454", "StatefulPartitionedCall/model/tf_op_layer_resize/ResizeNearestNeighbor/resize/ResizeNearestNeighbor"]): 'tf.ResizeNearestNeighbor' op is neither a custom op nor a flex op
&lt;unknown&gt;:0: error: loc(fused["model/tf_op_layer_resize_1/ResizeNearestNeighbor/resize_1/ResizeNearestNeighbor@__inference__wrapped_model_28454", "StatefulPartitionedCall/model/tf_op_layer_resize_1/ResizeNearestNeighbor/resize_1/ResizeNearestNeighbor"]): 'tf.ResizeNearestNeighbor' op is neither a custom op nor a flex op
&lt;unknown&gt;:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag): ResizeNearestNeighbor,ResizeNearestNeighbor
&lt;/denchmark-code&gt;

Any updates on this issue?
		</comment>
		<comment id='4' author='raltech' date='2020-02-18T22:40:50Z'>
		&lt;denchmark-link:https://github.com/miaout17&gt;@miaout17&lt;/denchmark-link&gt;

I am still stuck on this issue. Could you tell me how to fix this?
		</comment>
		<comment id='5' author='raltech' date='2020-03-07T03:52:03Z'>
		I have same problem. Did you fix it?&lt;denchmark-link:https://github.com/raltech&gt;@raltech&lt;/denchmark-link&gt;
@miaout17
		</comment>
		<comment id='6' author='raltech' date='2020-04-20T15:28:34Z'>
		Hi &lt;denchmark-link:https://github.com/jdduke&gt;@jdduke&lt;/denchmark-link&gt;
 I'm running into a similar issue.  I'm using tf version 2.2.0-rc3 and and the mlir converter:

I don't get the error using the toco converter :(.
Are y'all aware of this/have any MLIR workarounds? Until then i'll just be turning the mlir converter off.
		</comment>
		<comment id='7' author='raltech' date='2020-04-20T17:16:06Z'>
		Can you share what are the parameters passed to ResizeNearestNeighbor in your model ?
		</comment>
		<comment id='8' author='raltech' date='2020-04-20T17:36:04Z'>
		This is because of the half_pixel_centers argument switched on my default in TF2. The old converter ignores it and converts the op to a ResizeNearestNeighbor with half_pixel_centers=false (which is likely to give inaccurate results). The new converter doesn't recognize this, so fails.
Supporting all arguments in TFLite's ResizeNearestNeighbor should fix this issue. &lt;denchmark-link:https://github.com/ghop02&gt;@ghop02&lt;/denchmark-link&gt;
 I will work on this and ping back on this issue for you to switch on the new converter again.
		</comment>
		<comment id='9' author='raltech' date='2020-04-21T12:29:39Z'>
		I have encountered the same problem. hoping you can solve it as soon as possible. thank you &lt;denchmark-link:https://github.com/srjoglekar246&gt;@srjoglekar246&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='raltech' date='2020-04-22T06:47:48Z'>
		Also facing the same issue.
I will try loading the weighs on a YOLOv3 TF1.15 + Keras implementation and see how it behaves. Anyone tried that yet?
		</comment>
		<comment id='11' author='raltech' date='2020-05-06T16:20:52Z'>
		&lt;denchmark-link:https://github.com/ghop02&gt;@ghop02&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/raltech&gt;@raltech&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/48baba71cf3d115c7ec47f9e3902a78ee71ab8e9&gt;This commit&lt;/denchmark-link&gt;
 should enable the correct resize-NN behavior in TFLite.
Could you verify and close the bug if it works?
		</comment>
		<comment id='12' author='raltech' date='2020-05-06T21:31:26Z'>
		&lt;denchmark-link:https://github.com/srjoglekar246&gt;@srjoglekar246&lt;/denchmark-link&gt;
 With , i get little different error with the gist I uploaded earlier. Here is the full trace of error
&lt;denchmark-code&gt;
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-7-20e4a2d2e08d&gt; in &lt;module&gt;()
     28 converter.inference_output_type = tf.uint8
     29 
---&gt; 30 tflite_quant_model = converter.convert()
     31 open("./converted_model.tflite", "wb").write(tflite_quant_model)

2 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/calibrator.py in calibrate_and_quantize(self, dataset_gen, input_type, output_type, allow_float, resize_input)
     91     return self._calibrator.QuantizeModel(
     92         np.dtype(input_type.as_numpy_dtype()).num,
---&gt; 93         np.dtype(output_type.as_numpy_dtype()).num, allow_float)
     94 
     95   def calibrate_and_quantize_single(self,

RuntimeError: Quantization not yet supported for op: EXP
&lt;/denchmark-code&gt;

		</comment>
		<comment id='13' author='raltech' date='2020-05-06T21:39:16Z'>
		&lt;denchmark-link:https://github.com/jianlijianli&gt;@jianlijianli&lt;/denchmark-link&gt;
 Jian could you take a look at the above quantization error?
		</comment>
		<comment id='14' author='raltech' date='2020-06-15T00:52:49Z'>
		&lt;denchmark-link:https://github.com/srjoglekar246&gt;@srjoglekar246&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jianlijianli&gt;@jianlijianli&lt;/denchmark-link&gt;

While using latest tf-nightly build (tf-nightly-2.3.0.dev20200613), i noticed that it can convert a SavedModel generated by TF 2.2.0 to TFLite flatbuffer successfully. This verified the fix for ResizeNearestNeighbor with MLIR.
However, if I generate a SavedModel using this tf-nightly build, and convert again, it will fail and prompt something like 'tf.All' op is neither a custom op nor a flex op . And then keep prompting a bunch of hex values. This seems to be a regression.
It also happens to tf-nightly-2.3.0.dev20200614. Could you look into this to make sure it won't fail in the coming TF 2.3.0? Or should I make a separate ticket?
		</comment>
		<comment id='15' author='raltech' date='2020-06-15T16:10:38Z'>
		&lt;denchmark-link:https://github.com/ethanyanjiali&gt;@ethanyanjiali&lt;/denchmark-link&gt;
 Looks like there were some changes in the upstream TensorFlow graph-generating code :-/. Could you share the SavedModel (or any other details?). We will need to look at the source ops to see why/how its placing  in the source graph.
		</comment>
		<comment id='16' author='raltech' date='2020-06-16T04:43:02Z'>
		&lt;denchmark-link:https://github.com/srjoglekar246&gt;@srjoglekar246&lt;/denchmark-link&gt;

here's the SavedModel generated by 2.2.0, and 2.3.0-dev20200614 can covert it to TFLite model with no problem. &lt;denchmark-link:https://drive.google.com/file/d/1eR6Iz-RQA44YONqyUSOMFaRo5JAZBbQ-/view?usp=sharing&gt;https://drive.google.com/file/d/1eR6Iz-RQA44YONqyUSOMFaRo5JAZBbQ-/view?usp=sharing&lt;/denchmark-link&gt;

and here's the SavedModel generated by 2.3.0-dev20200614. With this, 2.3.0-dev20200614 will fail after a few optimization steps. &lt;denchmark-link:https://drive.google.com/file/d/1u618HSmT-g6XyLAF9yJ3JSC455CQ4XPZ/view?usp=sharing&gt;https://drive.google.com/file/d/1u618HSmT-g6XyLAF9yJ3JSC455CQ4XPZ/view?usp=sharing&lt;/denchmark-link&gt;

The error may come from tf.image.non_max_suppression_padded in my code, more stack trace here:
&lt;denchmark-code&gt;&lt;unknown&gt;:0: note: loc(callsite(callsite("non_max_suppression_padded/while/suppression_loop_body/while/All@__inference_non_max_suppression_padded_while_suppression_loop_body_while_body_12844_41994" at "non_max_suppression_padded/while/suppression_loop_body/while@__inference_non_max_suppression_padded_while_body_12795_42163") at fused[callsite("non_max_suppression_padded/while@__inference_serve_44818"("/Users/yanjia.li/Projects/perceptron2/env/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py":608:0) at callsite("/Users/yanjia.li/Projects/perceptron2/env/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py":582:0 at callsite("main.py":25:0 at callsite("/Users/yanjia.li/Projects/perceptron2/env/lib/python3.7/site-packages/click/core.py":555:0 at callsite("/Users/yanjia.li/Projects/perceptron2/env/lib/python3.7/site-packages/click/core.py":956:0 at callsite("/Users/yanjia.li/Projects/perceptron2/env/lib/python3.7/site-packages/click/core.py":717:0 at callsite("/Users/yanjia.li/Projects/perceptron2/env/lib/python3.7/site-packages/click/core.py":764:0 at "main.py":65:0))))))), "StatefulPartitionedCall/non_max_suppression_padded/while"])): see current operation: %32 = "tf.All"(%31, %cst_0) {device = "", keep_dims = false} : (tensor&lt;1x512x512xi1&gt;, tensor&lt;1xi32&gt;) -&gt; tensor&lt;1x512xi1&gt;
&lt;unknown&gt;:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):
        tf.All {device = "", keep_dims = false}
        tf.All {device = "/device:CPU:0", keep_dims = false}
&lt;unknown&gt;:0: note: see current operation: "func"() ( {
^bb0(%arg0: tensor&lt;416x416x3xf32&gt;):  // no predecessors
  %cst = "std.constant"() {value = dense&lt;"0x00000000000000000000803F000000000000004000000000000040400000000000008040000000000000A040000000000000C040000000000000E040000000000000004100000000000010410000000000
&lt;/denchmark-code&gt;

and then this hex values just keep going forever
Also I'm using new converter
converter.experimental_new_converter = True
		</comment>
		<comment id='17' author='raltech' date='2020-06-16T15:48:10Z'>
		Argh. This is related to some (known) changes in the upstream TensorFlow, which re-implemented non_max_suppression_with_padding in a way that no longer uses ops fully supported by TFLite. Lemme get back to you with some updates.
		</comment>
		<comment id='18' author='raltech' date='2020-06-17T17:20:19Z'>
		&lt;denchmark-link:https://github.com/ethanyanjiali&gt;@ethanyanjiali&lt;/denchmark-link&gt;
 in your comment, do you mean that with TF2.3, the model does not train as expected (irrespective of TFLite conversion)?
Also, can you share with me the snippet of code where your code calls non_max_suppression_padded?
		</comment>
		<comment id='19' author='raltech' date='2020-06-17T17:35:24Z'>
		&lt;denchmark-link:https://github.com/srjoglekar246&gt;@srjoglekar246&lt;/denchmark-link&gt;

it's only refers to post-training model saving and converting. I haven't tried to use TF2.3dev to train a model yet.
The reason there could be a combination of TF2.3dev SavedModel and TF2.3dev converter is because I reloaded checkpoint and re-generated SavedModel after training. Likewise, I also reloaded checkpoint and re-generated SavedModel using TF2.2. I have to do this because of an issue here: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/39918&gt;#39918&lt;/denchmark-link&gt;

And now that I have two versions of SavedModel from TF2.2 and TF2.3dev, I tried to use TF2.3dev converter to convert them. Only TF2.2 SavedModel works with TF2.3dev converter. The converter will output the error for TF2.3dev SavedModel.
code snippet:
        boxes = boxes[0]
        scores = scores[0]
        class_probs = class_probs[0]

        selected_indices_padded, num_valid = tf.image.non_max_suppression_padded(
            boxes,
            tf.squeeze(scores, axis=-1),
            self.max_detection,
            iou_threshold=self.iou_threshold,
            score_threshold=self.score_threshold,
            pad_to_max_output_size=True,
        )

        selected_boxes = tf.gather(boxes, selected_indices_padded)
        selected_scores = tf.gather(scores, selected_indices_padded)
        classes = tf.argmax(tf.gather(class_probs, selected_indices_padded), axis=-1)
		</comment>
		<comment id='20' author='raltech' date='2020-07-10T19:59:39Z'>
		&lt;denchmark-link:https://github.com/ethanyanjiali&gt;@ethanyanjiali&lt;/denchmark-link&gt;
 Can you check w/ the nightly in a day or so (or build from master)?
TF2.3 rc2 should have the fix too.
		</comment>
		<comment id='21' author='raltech' date='2020-07-11T01:09:34Z'>
		
@ethanyanjiali Can you check w/ the nightly in a day or so (or build from master)?
TF2.3 rc2 should have the fix too.

I had same issue and with tf-nightly I can fix this.
		</comment>
		<comment id='22' author='raltech' date='2020-07-13T16:33:23Z'>
		Awesome. Thanks for verifying!
		</comment>
		<comment id='23' author='raltech' date='2020-07-13T16:33:25Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36517&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36517&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='24' author='raltech' date='2020-07-14T06:43:37Z'>
		&lt;denchmark-link:https://github.com/raltech&gt;@raltech&lt;/denchmark-link&gt;
 I am still stuck on this issue.“Check failed: start_indices_size &lt;= num_input_axes (2 vs. 1)StridedSlice op requires no more than 1 start indices”
Have you solved this problem and how?
thanks!
		</comment>
		<comment id='25' author='raltech' date='2020-11-09T09:36:31Z'>
		
@raltech I am still stuck on this issue.“Check failed: start_indices_size &lt;= num_input_axes (2 vs. 1)StridedSlice op requires no more than 1 start indices”
Have you solved this problem and how?
thanks!

Hi HaoWang I stumble across this exact same issue also, trying to do post quantization on Yunyang1994 Yolov3 implementation, I am using Tensorflow2.1.0 on Windows.
May I know hv you solved this problem?
		</comment>
		<comment id='26' author='raltech' date='2020-11-09T16:32:21Z'>
		&lt;denchmark-link:https://github.com/lucastan5596&gt;@lucastan5596&lt;/denchmark-link&gt;
 Please create a new issue with a simple standalone code to reproduce the error you are noticing? Thanks!
		</comment>
		<comment id='27' author='raltech' date='2020-11-16T00:48:32Z'>
		你好，我也没有解决，最后放弃了
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


------------------ 原始邮件 ------------------发件人：Vishnuvardhan Janapati "notifications@github.com"时　间：2020/11/10 00:32:42 周二收件人：tensorflowtensorflow "tensorflow@noreply.github.com"抄送人：HaoWang1006 "15510991006@189.cn", Comment "comment@noreply.github.com"主　题：Re: [tensorflow/tensorflow] TensorFlow Lite Conversion Fails (Check failed: start_indices_size &lt;= num_input_axes (2 vs. 1)StridedSlice op requires no more than 1 start indices) (#36517) 

  @lucastan5596 Please create a new issue with a simple standalone code to reproduce the error you are noticing? Thanks!
  —You are receiving this because you commented.Reply to this email directly, view it on GitHub, or unsubscribe.

		</comment>
		<comment id='28' author='raltech' date='2020-11-16T01:47:47Z'>
		我倒是解决了 你还有需要吗？
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Mon, Nov 16, 2020, 8:48 AM HaoWang1006 ***@***.***&gt; wrote:






 你好，我也没有解决，最后放弃了



 ------------------ 原始邮件 ------------------发件人：Vishnuvardhan Janapati "
 ***@***.***"时 间：2020/11/10 00:32:42
 周二收件人：tensorflowtensorflow ***@***.***"抄送人：HaoWang1006
 ***@***.***", Comment ***@***.***"主 题：Re:
 [tensorflow/tensorflow] TensorFlow Lite Conversion Fails (Check failed:
 start_indices_size &lt;= num_input_axes (2 vs. 1)StridedSlice op requires no
 more than 1 start indices) (#36517)

 @lucastan5596 Please create a new issue with a simple standalone code to
 reproduce the error you are noticing? Thanks!
 —You are receiving this because you commented.Reply to this email
 directly, view it on GitHub, or unsubscribe.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#36517 (comment)&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/APODHARNNT6QLMRGRQ77FPTSQBZHFANCNFSM4KQ7VZNQ&gt;
 .



		</comment>
	</comments>
</bug>