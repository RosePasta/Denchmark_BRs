<bug id='24751' author='GothamCityPro' open_date='2019-01-08T00:05:42Z' closed_time='2020-01-27T06:07:26Z'>
	<summary>Instantiating Separate Session for Each GPU</summary>
	<description>
I'm trying to speed up Tensorflow inference by using multiple GPUs and instantiating a separate Session for each GPU.
I am using Tensorflow 1.7 and 2 Quadro GV100's for this. The GV100's are device 0 and device 1, respectively.
My C++ code looks something like the following:
auto options0 = SessionOptions();
options0.config.mutable_gpu_options()-&gt;set_visible_device_list("0");
NewSession(options0, &amp;m_session0);
auto options1 = SessionOptions();
options1.config.mutable_gpu_options()-&gt;set_visible_device_list("1");
NewSession(options1, &amp;m_session1);
However, when I execute this code, I get the following error message:
name: Quadro GV100 major: 7 minor: 0 memoryClockRate(GHz): 1.627 pciBusID: 0000:18:00.0 totalMemory: 31.87GiB freeMemory: 31.33GiB 2019-01-07 17:56:09.453901: I C:\tensorflow_1_7\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1423] Adding visible gpu devices: 0 2019-01-07 17:56:10.196800: I C:\tensorflow_1_7\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix: 2019-01-07 17:56:10.202994: I C:\tensorflow_1_7\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:917] 0 2019-01-07 17:56:10.206965: I C:\tensorflow_1_7\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:930] 0: N 2019-01-07 17:56:10.211407: I C:\tensorflow_1_7\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30421 MB memory) -&gt; physical GPU (device: 0, name: Quadro GV100, pci bus id: 0000:18:00.0, compute capability: 7.0) 17:56:11.099: Choosing GPU: 1 2019-01-07 17:56:11.280313: I C:\tensorflow_1_7\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1344] Found device 0 with properties: name: Quadro GV100 major: 7 minor: 0 memoryClockRate(GHz): 1.627 pciBusID: 0000:3b:00.0 totalMemory: 31.87GiB freeMemory: 31.33GiB 2019-01-07 17:56:11.291274: I C:\tensorflow_1_7\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1423] Adding visible gpu devices: 1 2019-01-07 17:56:12.076807: I C:\tensorflow_1_7\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix: 2019-01-07 17:56:12.082561: I C:\tensorflow_1_7\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:917] 1 2019-01-07 17:56:12.088587: I C:\tensorflow_1_7\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:930] 1: N 2019-01-07 17:56:12.095142: F C:\tensorflow_1_7\tensorflow\tensorflow\core\common_runtime\gpu\gpu_id_manager.cc:45] Check failed: cuda_gpu_id.value() == result.first-&gt;second (1 vs. 0)Mapping the same TfGpuId to a different CUDA GPU id. TfGpuId: 0 Existing mapped CUDA GPU id: 0 CUDA GPU id being tried to map to: 1
My question is, what is the proper way to assign/dedicate a GPU to each Tensorflow session?
Thank you very much for your help in advance!
	</description>
	<comments>
		<comment id='1' author='GothamCityPro' date='2019-01-08T00:17:50Z'>
		Additionally, I tried
auto options = SessionOptions();
options.config.mutable_gpu_options()-&gt;set_visible_device_list("0, 1");
NewSession(options, &amp;m_session0);
NewSession(options, &amp;m_session1);
and then loading the same graph twice each time onto a diff Session
GraphDef graph_def;
ReadBinaryProto(Env::Default(), graphPath, &amp;graph_def);
m_session0-&gt;Create(graph_def);
graph::SetDefaultDevice("/device:GPU:0", &amp;graph_def);
GraphDef graph_def1;
ReadBinaryProto(Env::Default(), graphPath, &amp;graph_def1);
m_session1-&gt;Create(graph_def1);
graph::SetDefaultDevice("/device:GPU:1", &amp;graph_def1);
This got past the error.  I am calling m_session0.Run() and m_session1.Run() on separate threads.  This however, did not speed up the inference speed as opposed to using just one Session object and one GPU.  Am I missing something?
		</comment>
		<comment id='2' author='GothamCityPro' date='2019-01-09T21:03:03Z'>
		Hello &lt;denchmark-link:https://github.com/GothamCityPro&gt;@GothamCityPro&lt;/denchmark-link&gt;
 , TensorFlow tends to allocate GPU resources to each session and thread in the order they are created.
Hi &lt;denchmark-link:https://github.com/azaks2&gt;@azaks2&lt;/denchmark-link&gt;
 , can you advice how the user can allocate GPUs to sessions to threads, if possible. Thanks.
		</comment>
		<comment id='3' author='GothamCityPro' date='2019-12-31T22:40:47Z'>
		&lt;denchmark-link:https://github.com/msymp&gt;@msymp&lt;/denchmark-link&gt;
 Are you still blocked on this?  If yes, can you please provide a full reproducer (i.e. something I can run myself to reproduce the problem)?
		</comment>
		<comment id='4' author='GothamCityPro' date='2020-01-27T06:07:26Z'>
		Closing for lack of activity.  If this is still blocking you please reopen with a reproducer.
		</comment>
		<comment id='5' author='GothamCityPro' date='2020-01-27T06:07:28Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24751&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24751&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>