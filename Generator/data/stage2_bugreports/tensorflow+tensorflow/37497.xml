<bug id='37497' author='xlnwel' open_date='2020-03-11T00:11:36Z' closed_time='2020-03-19T21:55:39Z'>
	<summary>Confusing behavior of GRUCell</summary>
	<description>
cell = layers.GRUCell(4)
x = tf.random.normal((2, 2, 3))
print(cell.get_initial_state(x)) # returns a single tensor
cell(x[:, 0], [cell.get_initial_state(x)]) # requires and returns a list of tensors
Why would GRUCell.get_initial_state return a Tensor, while calling GRUCell requires a list of Tensor as input? Is this a desired behavior? (code tested in TF2.1)
Furthermore, while Cell returns output and state separately, RNN returns the output and states together in a single list.
cell = layers.LSTMCell(4)
x = tf.random.normal((2, 2, 3))
s = cell.get_initial_state(x)
cell_output, state = cell(inputs=x[:, 1], states=s)# works fine
print(cell_output)
print(state)
rnn = layers.RNN(cell, return_state=True, return_sequences=True)
rnn_output, state = rnn(x, initial_state=s)# error: rnn returns the output and states in a single list
I'm wondering why rnn and cell behave differently?
	</description>
	<comments>
		<comment id='1' author='xlnwel' date='2020-03-12T07:08:41Z'>
		&lt;denchmark-link:https://github.com/xlnwel&gt;@xlnwel&lt;/denchmark-link&gt;

I have tried on colab with TF version 2.1.0 and and i am seeing the below error . Is this the expected behavior?. It will be helpful if you provide a colab link to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!
		</comment>
		<comment id='2' author='xlnwel' date='2020-03-12T10:16:43Z'>
		A Cell processes a single time step, while a RNN processes all time steps. Setting return_sequences = True and return_state = True in RNN returns three values, of shapes [batch_size, time steps, output_shape], [batch_size, output_shape], [batch_size, output_shape].
Check &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU&gt;the example on the TF GRU page&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='xlnwel' date='2020-03-14T00:15:07Z'>
		Hi, &lt;denchmark-link:https://github.com/ravikyram&gt;@ravikyram&lt;/denchmark-link&gt;
. Yeah, the code provided by me will raise error  at the last line. Here's the &lt;denchmark-link:https://colab.research.google.com/drive/1z58JoPdMDszfXV-rHBAOM6HUDd-W2yVb&gt;colab link&lt;/denchmark-link&gt;
 I use for test. I humbly think that it will be easier to use if

cell.get_initial_state(x) returns a list of tensor, where cell could be any RNN cell, including GRUCell, whose state is a single tensor.
when RNN is parameterized by return_state=True, rnn(x) returns the output and RNN state, where RNN state is a list of tensors. That means for LSTM, rnn(x) returns [output, [h, c]], and for GRU, rnn(x) returns [output, [state]]

That's my personal suggestion. But I believe you team designed the behavior for a reason, I'd like to hear that.
		</comment>
		<comment id='4' author='xlnwel' date='2020-03-14T00:17:32Z'>
		&lt;denchmark-link:https://github.com/Susmit-A&gt;@Susmit-A&lt;/denchmark-link&gt;
, thanks for point out the different behavior of  and  although I've already known them before. Please check this &lt;denchmark-link:https://colab.research.google.com/drive/1z58JoPdMDszfXV-rHBAOM6HUDd-W2yVb&gt;colab link&lt;/denchmark-link&gt;
 and the comment above.
		</comment>
		<comment id='5' author='xlnwel' date='2020-03-19T21:55:39Z'>
		&lt;denchmark-link:https://github.com/xlnwel&gt;@xlnwel&lt;/denchmark-link&gt;
 GRU is a recurrent layer. GRUCell is an object (which happens to be a layer too) used by the GRU layer that contains the calculation logic for one step.
A recurrent layer contains a cell object. The cell contains the core code for the calculations of each step, while the recurrent layer commands the cell and performs the actual recurrent calculations.
As this is not a bug, please post this issue in stackoverflow where there is a broader community to help. Thanks!
		</comment>
		<comment id='6' author='xlnwel' date='2020-03-19T21:55:41Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37497&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37497&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='xlnwel' date='2020-03-22T01:51:50Z'>
		Hi, &lt;denchmark-link:https://github.com/gowthamkpr&gt;@gowthamkpr&lt;/denchmark-link&gt;
. Thanks for your explanation. I know the difference between GRU and GRUCell, and I was not saying this is a bug. I just found their different styles of outputs are confusing and hard to use. Therefore, I'm wondering why you decided to adopt different behaviors for them? Please check my &lt;denchmark-link:https://colab.research.google.com/drive/1z58JoPdMDszfXV-rHBAOM6HUDd-W2yVb&gt;colab link&lt;/denchmark-link&gt;
.
		</comment>
	</comments>
</bug>