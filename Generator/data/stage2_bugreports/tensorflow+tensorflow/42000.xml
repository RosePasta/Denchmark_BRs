<bug id='42000' author='v3551G' open_date='2020-08-03T09:43:33Z' closed_time='2020-08-11T19:36:16Z'>
	<summary>Report error 'non-first' call when repeate experiment</summary>
	<description>
&lt;denchmark-h:h2&gt;Question: The code can successfully run when using autograph for acceleration, whereas it reports 'optimizer non-first call'  error when I repeat the experiment multiple times(10 in the sample code), which confused me.
The code logic is as follows：&lt;/denchmark-h&gt;

&lt;denchmark-code&gt; for i in np.arange(10):
       run_flow(random_seed=i)

def run_flow(random_seed=0):
      optimizer = tf.keras.optimizers.Adam()
      for i in num_epoch:
           train_epoch(optimizer)

def train_epoch(optimzier):
      for i in num_batchs:
            train_step(optimizer)

@tf.function
def train_step(optimizer)
      # .....
      optimizer.apply_gradients(...)
&lt;/denchmark-code&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Clearly, the definition-optimizer  is outside the function-train_step.
	</description>
	<comments>
		<comment id='1' author='v3551G' date='2020-08-03T10:06:00Z'>
		It seems to me in the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/27120&gt;#27120&lt;/denchmark-link&gt;
 "famliy"
		</comment>
		<comment id='2' author='v3551G' date='2020-08-03T10:27:26Z'>
		
It seems to me in the #27120 "famliy"

Thank you. Still different. my code works well both in eager mode and autograph mode when not repeat experiments, I will paste the error details later.
ps： I am wandering is there have any related API in tensorflow-2 to init the runtime environment before conducting the next experiment. Currently, I have to  restart the next experiments manually after current experiment have finished, Which is boring.
		</comment>
		<comment id='3' author='v3551G' date='2020-08-03T10:45:35Z'>
		&lt;denchmark-link:https://github.com/v3551G&gt;@v3551G&lt;/denchmark-link&gt;

Please, fill &lt;denchmark-link:https://github.com/tensorflow/models/issues/new/choose&gt;issue template.&lt;/denchmark-link&gt;
.
Request you to share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!
		</comment>
		<comment id='4' author='v3551G' date='2020-08-03T11:30:57Z'>
		
@v3551G
Please, fill issue template..
Request you to share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!

&lt;denchmark-code&gt;# -*- coding: utf-8 -*-
import tensorflow as tf
import numpy as np

"""
Note: when add "@tf.function" in front from train_step, report error; else, successfully run two times.
Environment; tensorflow2.x python3.x
author: masterqkk
"""


def build_model():
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dense(10, input_shape=[2]))
    model.add(tf.keras.layers.Dense(1))
    return model

def compute_loss(batch_true, batch_pred):
    losses = tf.losses.mean_squared_error(batch_true, batch_pred)
    loss = tf.reduce_mean(losses)
    return loss


@tf.function
def train_step(model, batch_input, batch_label, optimizer):
    with tf.GradientTape() as tape:
        preds = model(batch_input)
        loss = compute_loss(batch_label, preds)
    trainable_variables = model.trainable_variables
    grads = tape.gradient(loss, trainable_variables)
    optimizer.apply_gradients(grads_and_vars=zip(grads, trainable_variables))
    return loss


def train_epoch(model, data_batchs, optimizer):
    for (batch_input, batch_label) in data_batchs:
        loss = train_step(model, batch_input, batch_label, optimizer)


def train_model(model, data_batchs, optimizer):
    for i in np.arange(1):
        train_epoch(model, data_batchs, optimizer)


def load_data():
    np.random.seed(0)
    x = np.array(np.random.random(size=(10, 2)), np.float32)
    y = np.array(np.random.random(size=(10, 1)), np.float32)
    data = tf.data.Dataset.from_tensor_slices((x, y))
    data_batchs = data.batch(batch_size=5)
    return data_batchs


def run_flow():
    data_batchs = load_data()
    model = build_model()
    optimizer = tf.keras.optimizers.Adam()
    train_model(model, data_batchs, optimizer)
    tf.print('model train finished.')


for i in np.arange(2):
    run_flow()
&lt;/denchmark-code&gt;

Waitting your reply.
		</comment>
		<comment id='5' author='v3551G' date='2020-08-03T11:50:55Z'>
		&lt;denchmark-link:https://github.com/v3551G&gt;@v3551G&lt;/denchmark-link&gt;
 I still think that you are in the perimeter of &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/27120&gt;#27120&lt;/denchmark-link&gt;
.
Can you try your code with the same workaround as in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/27120#issuecomment-540071844&gt;#27120 (comment)&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='v3551G' date='2020-08-03T11:55:29Z'>
		
@v3551G I still think that you are in the perimeter of #27120.
Can you try your code with the same workaround as in #27120 (comment)

The demo for reproducing the error is pasted at my last comment， please swipe up the page.
		</comment>
		<comment id='7' author='v3551G' date='2020-08-03T11:59:46Z'>
		I saw your code. Can you try your code with that workaround?
Personally I tried that workaround with your code and It Is working.
		</comment>
		<comment id='8' author='v3551G' date='2020-08-03T12:03:22Z'>
		
I saw your code. Can you try your code with that workaround?
Personally I tried that workaround with your code and It Is working.
do you mean commit the "@tf.function"?
my code work well in eager mode whether repeating experiment or not, if repeat experiment, only work well in eager mode, while fail in autograph mode.

		</comment>
		<comment id='9' author='v3551G' date='2020-08-03T14:12:34Z'>
		&lt;denchmark-code&gt;# -*- coding: utf-8 -*-
import tensorflow as tf
import numpy as np

"""
Note: when add "@tf.function" in front from train_step, report error; else, successfully run two times.
Environment; tensorflow2.x python3.x
author: masterqkk
"""


def build_model():
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dense(10, input_shape=[2]))
    model.add(tf.keras.layers.Dense(1))
    return model

def compute_loss(batch_true, batch_pred):
    losses = tf.losses.mean_squared_error(batch_true, batch_pred)
    loss = tf.reduce_mean(losses)
    return loss

def get_apply_grad_fn():
    @tf.function
    def train_step(model, batch_input, batch_label, optimizer):
        with tf.GradientTape() as tape:
            preds = model(batch_input)
            loss = compute_loss(batch_label, preds)
        trainable_variables = model.trainable_variables
        grads = tape.gradient(loss, trainable_variables)
        optimizer.apply_gradients(grads_and_vars=zip(grads, trainable_variables))
        return loss
    return train_step


def train_epoch(model, data_batchs, optimizer):
    for (batch_input, batch_label) in data_batchs:
        model_apply_grads = get_apply_grad_fn()
        loss = model_apply_grads(model, batch_input, batch_label, optimizer)


def train_model(model, data_batchs, optimizer):
    for i in np.arange(1):
        train_epoch(model, data_batchs, optimizer)


def load_data():
    np.random.seed(0)
    x = np.array(np.random.random(size=(10, 2)), np.float32)
    y = np.array(np.random.random(size=(10, 1)), np.float32)
    data = tf.data.Dataset.from_tensor_slices((x, y))
    data_batchs = data.batch(batch_size=5)
    return data_batchs


def run_flow():
    data_batchs = load_data()
    model = build_model()
    optimizer = tf.keras.optimizers.Adam()
    model_apply_grads = get_apply_grad_fn()
    train_model(model, data_batchs, optimizer)
    tf.print('model train finished.')


for i in np.arange(2):
    run_flow()
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='v3551G' date='2020-08-03T15:28:05Z'>
		Hi &lt;denchmark-link:https://github.com/v3551G&gt;@v3551G&lt;/denchmark-link&gt;
, as &lt;denchmark-link:https://github.com/bhack&gt;@bhack&lt;/denchmark-link&gt;
 explained the comment in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/27120&gt;#27120&lt;/denchmark-link&gt;
 to wrap the train_step is a workaround that will allow you to run multiple experiments. However, you might notice that it can have an impact on performance. The problem of not being able to repeat the experiment is actually a known bug as I've noted in this comment &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/36574#issuecomment-659040854&gt;#36574 &lt;/denchmark-link&gt;
.
		</comment>
		<comment id='11' author='v3551G' date='2020-08-11T19:36:16Z'>
		Closing this issue now since this is a known issue being tracked in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/36574&gt;#36574&lt;/denchmark-link&gt;
 and there is a temporary workaround.
		</comment>
		<comment id='12' author='v3551G' date='2020-08-11T19:36:18Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42000&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42000&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='v3551G' date='2020-12-12T08:34:03Z'>
		As you pointed out, you are creating your optimizer outside the tf.function; however, you are attempting to pass a new optimizer into the same trace of tf.function and the optimizer creates variables the first time it applies gradients. This is why you see the error: the optimizer that is instantiated in run_flow attempts to create variables in train_step when `train_step is retraced in your secod experiment.
Just my two cents:
Suppose you are running a batch of 10 experiments where each experiment is called with run_flow:
&lt;denchmark-code&gt;for i in np.arange(10):
       run_flow(random_seed=i)
&lt;/denchmark-code&gt;

If you are interested in keeping experiments &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/1&gt;#1&lt;/denchmark-link&gt;
 - &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/10&gt;#10&lt;/denchmark-link&gt;
 identical:
&lt;denchmark-code&gt;def build_model():
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dense(10, input_shape=[2]))
    model.add(tf.keras.layers.Dense(1))
    return model

def compute_loss(batch_true, batch_pred):
    losses = tf.losses.mean_squared_error(batch_true, batch_pred)
    loss = tf.reduce_mean(losses)
    return loss

def train_step(model, batch_input, batch_label, optimizer):
    with tf.GradientTape() as tape:
        preds = model(batch_input)
        loss = compute_loss(batch_label, preds)
    trainable_variables = model.trainable_variables
    grads = tape.gradient(loss, trainable_variables)
    optimizer.apply_gradients(grads_and_vars=zip(grads, trainable_variables))
    return loss

def train_model(model, data_batchs, optimizer):
    # Create a new tf.function from train_step that will be used in this experiment only
    train_step_function = tf.function(train_step)
    num_epochs = 10
    for i in np.arange(num_epochs):
        for (batch_input, batch_label) in data_batchs:
            loss = train_step_function(model, batch_input, batch_label, optimizer)

def load_data():
    np.random.seed(0)
    x = np.array(np.random.random(size=(10, 2)), np.float32)
    y = np.array(np.random.random(size=(10, 1)), np.float32)
    data = tf.data.Dataset.from_tensor_slices((x, y))
    data_batchs = data.batch(batch_size=5)
    return data_batchs

def run_flow():
    data_batchs = load_data()
    model = build_model()
    optimizer = tf.keras.optimizers.Adam()
    train_model(model, data_batchs, optimizer)

for i in np.arange(10):
    print("Running experiment {}".format(i))
    run_flow()
&lt;/denchmark-code&gt;

This solution should not have a drastic impact on performance.
		</comment>
	</comments>
</bug>