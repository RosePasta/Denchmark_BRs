<bug id='33388' author='sourcecode369' open_date='2019-10-15T18:36:50Z' closed_time='2019-10-20T13:00:10Z'>
	<summary>TypeError: reduce() missing 1 required positional argument: 'per_replica_value'</summary>
	<description>
&lt;denchmark-code&gt;&gt; with mirrored_strategy.scope():
&gt;     model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])
&gt;     optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
&gt; 
&gt; global_batch_size = 10 * mirrored_strategy.num_replicas_in_sync
&gt; 
&gt; def input_fn():
&gt;     dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(1000).batch(
&gt;     global_batch_size)
&gt;     dist_dataset = mirrored_strategy.experimental_distribute_dataset(dataset)
&gt;     return dist_dataset
&gt;     
&gt; dataset = input_fn()
&gt; 
&gt; @tf.function
&gt; def train_step(dist_inputs):
&gt;     def step_fn(inputs):
&gt;         features, labels = inputs
&gt;         with tf.GradientTape() as tape:
&gt;             logits = model(features)
&gt;             cross_entropy = tf.nn.softmax_cross_entropy_with_logits(
&gt;                 logits=logits, labels=labels
&gt;             )
&gt;             loss = tf.reduce_sum(cross_entropy) * (1.0 / global_batch_size)
&gt;         grads = tape.gradient(loss, model.trainable_variables)
&gt;         optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))
&gt;         return cross_entropy
&gt; 
&gt;     per_example_losses = mirrored_strategy.experimental_run_v2(
&gt;         step_fn, args=(dist_inputs,)
&gt;     )
&gt;     mean_loss = mirrored_strategy.reduce(
&gt;         tf.distribute.ReduceOp.MEAN, per_example_losses, axis=0
&gt;     )    
&gt;     return mean_loss
&gt; 
&gt; with mirrored_strategy.scope():
&gt;     for input in dataset:
&gt;         train_step(inputs)
&lt;/denchmark-code&gt;

Its one of the code snippets from the distributed strategy guide for custom training loops.
Im getting this,
TypeError:TypeError: reduce() missing 1 required positional argument: 'per_replica_value'
but then when I ran the same code today morning it was working perfectly.
	</description>
	<comments>
		<comment id='1' author='sourcecode369' date='2019-10-16T05:25:53Z'>
		&lt;denchmark-link:https://github.com/sourcecode369&gt;@sourcecode369&lt;/denchmark-link&gt;
 ,
Thank you for reporting, Can you please provide complete code to reproduce the above issue ? Also let us know TF version being used.
		</comment>
		<comment id='2' author='sourcecode369' date='2019-10-16T15:09:18Z'>
		I'm using TensorFlow 2.0, there is some kind of bug. I am getting this error again by trying out a different code.
&lt;denchmark-code&gt;import tensorflow as tf
print("TensorFlow version: ", tf.__version__)
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;tf.debugging.set_log_device_placement(True)
tf.config.set_soft_device_placement(True)

gpus = tf.config.experimental.list_physical_devices("GPU")
if gpus:
    try:
        tf.config.experimental.set_virtual_device_configuration(
            gpus[0],
            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024),
             tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)]
        )
        logical_gpus = tf.config.experimental.list_logical_devices("GPU")
        print("Physical GPUs", len(gpus), "Logical GPUs", len(logical_gpus))
    except RuntimeError as e:
        print(e)
else:
    print("No GPUs found.")
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;tf.debugging.set_log_device_placement(True)
tf.config.set_soft_device_placement(True)

mirrorerd_strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.ReductionToOneDevice)
with mirrorerd_strategy.scope():
    inputs = tf.keras.layers.Input(shape=(1,))
    predictions = tf.keras.layers.Dense(1)(inputs)
    model = tf.keras.models.Model(inputs=inputs, outputs=predictions)
    model.compile(optimizer="adam", loss="binary_crossentropy",metrics=["accuracy"])
&lt;/denchmark-code&gt;

I am getting the same error again when I try to run the above code. Please help me out. I have confirmed twice that Im using TensorFlow 2.0 on Google Colaboratory and have installed by,

pip install --upgrade tensorflow

and my gpu runtime is active.
For the above one, that is pretty much the complete code. Just that there was one more line of code above where I create an object of tf.distribute.MirroredStrategy by using the below code,
mirrorerd_strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.ReductionToOneDevice) 

Below I am pasting the complete error message that I am receiving when trying to run both of the code. Hope this helps,
INFO:tensorflow:Error reported to Coordinator: reduce() missing 1 required positional argument: 'per_replica_value'
Traceback (most recent call last):
File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/coordinator.py", line 297, in stop_on_exception
yield
File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py", line 190, in _call_for_each_replica
**merge_kwargs)
File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/utils/metrics_utils.py", line 118, in merge_fn_wrapper
result = distribution.experimental_local_results(merge_fn)0
File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/metrics.py", line 368, in result
return math_ops.div_no_nan(self.total, self.count)
File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/dispatch.py", line 180, in wrapper
return target(*args, **kwargs)
File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_ops.py", line 1111, in div_no_nan
x = ops.convert_to_tensor(x, name="x")
File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 1184, in convert_to_tensor
return convert_to_tensor_v2(value, dtype, preferred_dtype, name)
File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 1242, in convert_to_tensor_v2
as_ref=False)
File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 1296, in internal_convert_to_tensor
ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/values.py", line 1271, in _tensor_conversion_sync_on_read
return var._dense_var_to_tensor(dtype=dtype, name=name, as_ref=as_ref)  # pylint: disable=protected-access
File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/values.py", line 1261, in _dense_var_to_tensor
self.get(), dtype=dtype, name=name, as_ref=as_ref)
File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/values.py", line 322, in get
return self._get_cross_replica()
File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/values.py", line 1237, in _get_cross_replica
self, axis=None)
File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py", line 805, in reduce
return self._extended._reduce(reduce_op, value)  # pylint: disable=protected-access
File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py", line 1436, in _reduce
device_util.current() or "/device:CPU:0"))[0]
File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py", line 703, in _reduce_to
reduce_op, value, destinations=destinations)
TypeError: reduce() missing 1 required positional argument: 'per_replica_value'

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;TypeError                                 Traceback (most recent call last)
&lt;ipython-input-18-2e63ea6eaf5a&gt; in &lt;module&gt;()
      7     predictions = tf.keras.layers.Dense(1)(inputs)
      8     model = tf.keras.models.Model(inputs=inputs, outputs=predictions)
----&gt; 9     model.compile(optimizer="adam", loss="binary_crossentropy",metrics=["accuracy"])
&lt;/denchmark-code&gt;

27 frames
&lt;denchmark-code&gt;/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py in _reduce_to(self, reduce_op, value, destinations)
    701           reduce_op, self._device_map, value, destinations)
    702     return self._get_cross_device_ops().reduce(
--&gt; 703         reduce_op, value, destinations=destinations)
    704 
    705   def _batch_reduce_to(self, reduce_op, value_destination_pairs):
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;TypeError: reduce() missing 1 required positional argument: 'per_replica_value'&lt;/denchmark-h&gt;

Please help me out.
		</comment>
		<comment id='3' author='sourcecode369' date='2019-10-20T09:32:13Z'>
		is there anybody who can help me out with this?
im facing this issue time and again.
		</comment>
		<comment id='4' author='sourcecode369' date='2019-10-20T10:06:37Z'>
		I finally was able to solve this for the keras by removing the metrics argument, though I dont understand why thats the case.
However for custom tensorflow training loop the error still persists. I guess the error is somewhere here,
&lt;denchmark-code&gt;&gt; per_example_losses = mirrored_strategy.experimental_run_v2(
&gt;         step_fn, args=(dist_inputs,)
&gt;     )
&gt; mean_loss = mirrored_strategy.reduce(
&gt;         tf.distribute.ReduceOp.MEAN, per_example_losses, axis=0
&gt; )
&lt;/denchmark-code&gt;

I'll try to look into it and report back.
		</comment>
		<comment id='5' author='sourcecode369' date='2019-10-20T13:00:10Z'>
		I finally solved it.
		</comment>
		<comment id='6' author='sourcecode369' date='2019-10-20T13:00:11Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33388&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33388&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='sourcecode369' date='2019-10-21T05:24:39Z'>
		sourcecode369@ can you tell us how you solved the issue? I was able to repro the bug with the given cross_device_ops argument.
		</comment>
		<comment id='8' author='sourcecode369' date='2019-10-21T12:54:21Z'>
		
sourcecode369@ can you tell us how you solved the issue? I was able to repro the bug with the given cross_device_ops argument.

Yup the problem I found was due toi tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.ReductionToOneDevice) and rather the proper definition would be,
tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.ReductionToOneDevice()). Forgot to put the parenthesis due to which a default argument was not getting passed.
		</comment>
	</comments>
</bug>