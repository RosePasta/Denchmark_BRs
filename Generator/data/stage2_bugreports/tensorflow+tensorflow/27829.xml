<bug id='27829' author='ageron' open_date='2019-04-14T07:26:47Z' closed_time='2019-06-04T21:12:40Z'>
	<summary>Cannot create a stateful RNN with recurrent dropout</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
MacOSX 10.13.6
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
TensorFlow installed from (source or binary):
binary
TensorFlow version (use command below):
tf.version.VERSION=2.0.0-dev20190413
tf.version.GIT_VERSION=v1.12.0-12481-gc7ce6f4cd9
Python version:
3.6.8
Bazel version (if compiling from source):
N/A
GCC/Compiler version (if compiling from source):
N/A
CUDA/cuDNN version:
N/A
GPU model and memory:
N/A

Describe the current behavior
I get an exception when trying to use recurrent_dropout in a stateful RNN:
&lt;denchmark-code&gt;.../tensorflow/python/ops/resource_variable_ops.py in __imul__(self, unused_other)
   1449
   1450   def __imul__(self, unused_other):
-&gt; 1451     raise RuntimeError("Variable *= value not supported. Use "
   1452                        "`var.assign(var * value)` to modify the variable or "
   1453                        "`var = var * value` to get a new Tensor object.")

RuntimeError: Variable *= value not supported. Use `var.assign(var * value)` to modify the variable or `var = var * value` to get a new Tensor object.
&lt;/denchmark-code&gt;

The full stacktrace is below.
Describe the expected behavior
No exception.
Code to reproduce the issue
from tensorflow import keras

model = keras.models.Sequential([
    keras.layers.GRU(128, return_sequences=True, stateful=True,
                     batch_input_shape=[32, None, 5],
                     recurrent_dropout=0.2)
])
Other info / logs
Complete stacktrace:
&lt;denchmark-code&gt;---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-1-3e98e7412ec2&gt; in &lt;module&gt;
      4     keras.layers.GRU(128, return_sequences=True, stateful=True,
      5                      batch_input_shape=[32, None, 5],
----&gt; 6                      recurrent_dropout=0.2)
      7 ])

.../tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    456     self._self_setattr_tracking = False  # pylint: disable=protected-access
    457     try:
--&gt; 458       result = method(self, *args, **kwargs)
    459     finally:
    460       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

.../tensorflow/python/keras/engine/sequential.py in __init__(self, layers, name)
    106     if layers:
    107       for layer in layers:
--&gt; 108         self.add(layer)
    109
    110   @property

.../tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    456     self._self_setattr_tracking = False  # pylint: disable=protected-access
    457     try:
--&gt; 458       result = method(self, *args, **kwargs)
    459     finally:
    460       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

.../tensorflow/python/keras/engine/sequential.py in add(self, layer)
    167           # and create the node connecting the current layer
    168           # to the input layer we just created.
--&gt; 169           layer(x)
    170           set_inputs = True
    171

.../tensorflow/python/keras/layers/recurrent.py in __call__(self, inputs, initial_state, constants, **kwargs)
    620
    621     if initial_state is None and constants is None:
--&gt; 622       return super(RNN, self).__call__(inputs, **kwargs)
    623
    624     # If any of `initial_state` or `constants` are specified and are Keras

.../tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    631                       base_layer_utils.AutoAddUpdates(self,
    632                                                       inputs)) as auto_updater:
--&gt; 633                 outputs = call_fn(inputs, *args, **kwargs)
    634                 auto_updater.set_outputs(outputs)
    635

.../tensorflow/python/keras/layers/recurrent_v2.py in call(self, inputs, mask, training, initial_state)
    328           input_length=timesteps,
    329           time_major=self.time_major,
--&gt; 330           zero_output_for_mask=self.zero_output_for_mask)
    331       # This is a dummy tensor for testing purpose.
    332       runtime = _runtime('unknown')

.../tensorflow/python/keras/backend.py in rnn(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length, time_major, zero_output_for_mask)
   3558     # the value is discarded.
   3559     output_time_zero, _ = step_function(input_time_zero,
-&gt; 3560                                         initial_states + constants)
   3561     output_ta = tuple(
   3562         tensor_array_ops.TensorArray(

.../tensorflow/python/keras/layers/recurrent_v2.py in step(cell_inputs, cell_states)
    316
    317       def step(cell_inputs, cell_states):
--&gt; 318         return self.cell.call(cell_inputs, cell_states, **kwargs)
    319
    320       last_output, outputs, states = K.rnn(

.../tensorflow/python/keras/layers/recurrent.py in call(self, inputs, states, training)
   1706
   1707       if 0. &lt; self.recurrent_dropout &lt; 1.:
-&gt; 1708         h_tm1 *= rec_dp_mask[0]
   1709
   1710       if self.reset_after:

.../tensorflow/python/ops/resource_variable_ops.py in __imul__(self, unused_other)
   1449
   1450   def __imul__(self, unused_other):
-&gt; 1451     raise RuntimeError("Variable *= value not supported. Use "
   1452                        "`var.assign(var * value)` to modify the variable or "
   1453                        "`var = var * value` to get a new Tensor object.")

RuntimeError: Variable *= value not supported. Use `var.assign(var * value)` to modify the variable or `var = var * value` to get a new Tensor object.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='ageron' date='2019-04-16T10:41:41Z'>
		&lt;denchmark-link:https://github.com/ageron&gt;@ageron&lt;/denchmark-link&gt;
 In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!
		</comment>
		<comment id='2' author='ageron' date='2019-04-16T12:52:33Z'>
		Hi &lt;denchmark-link:https://github.com/muddham&gt;@muddham&lt;/denchmark-link&gt;
 ,
I did! It's in the section "Code to reproduce the issue". :)
		</comment>
		<comment id='3' author='ageron' date='2019-04-18T21:29:56Z'>
		&lt;denchmark-link:https://github.com/ageron&gt;@ageron&lt;/denchmark-link&gt;
 I tried to reproduce the bug in TF2.0.0-alpha0 but I don't get the runtime error. I see a warning and a deprecation message as follows. Just for your info, I ran your code in Google colab
WARNING: Logging before flag parsing goes to stderr.
W0418 17:16:01.808634 139806816728960 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:4081: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use rate instead of keep_prob. Rate should be set to rate = 1 - keep_prob.
Please let me know what you think. Thanks!
		</comment>
		<comment id='4' author='ageron' date='2019-04-19T05:06:08Z'>
		Apparently the problem is now fixed, I don't get the error anymore.  Thanks &lt;denchmark-link:https://github.com/jvishnuvardhan&gt;@jvishnuvardhan&lt;/denchmark-link&gt;
 .
		</comment>
		<comment id='5' author='ageron' date='2019-04-19T05:06:09Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=27829&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=27829&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='ageron' date='2019-05-24T20:58:00Z'>
		&lt;denchmark-link:https://github.com/ageron&gt;@ageron&lt;/denchmark-link&gt;
 does it still work for you? I tried using recurrent_dropout with a GRU (as you are) and it seems to break for me. The problem seems to be with recurrent_dropout, cos if you switch it out everything seems to work. This problem also exists with LSTMs, and not just GRUs.
		</comment>
		<comment id='7' author='ageron' date='2019-05-25T01:29:56Z'>
		Apparently the bug is back. Using VERSION='2.0.0-dev20190524' and GIT_VERSION='v1.12.1-2720-geafe861c2b'.
		</comment>
		<comment id='8' author='ageron' date='2019-05-28T19:02:20Z'>
		I am also having a similar issue. Was wondering if there was an update or an older nightly where this is stable?
		</comment>
		<comment id='9' author='ageron' date='2019-05-29T04:19:01Z'>
		&lt;denchmark-link:https://github.com/jlanday&gt;@jlanday&lt;/denchmark-link&gt;
 , it worked when I posted my comment on April 19th, so perhaps try a nightly from April 18th or 19th?
		</comment>
		<comment id='10' author='ageron' date='2019-05-29T16:10:55Z'>
		&lt;denchmark-link:https://github.com/ageron&gt;@ageron&lt;/denchmark-link&gt;
 I tried to use the nightly version from both April 18th and 19th and it looks like it still doesn't work. Does version  work for you?
		</comment>
		<comment id='11' author='ageron' date='2019-05-29T17:40:35Z'>
		&lt;denchmark-link:https://github.com/ageron&gt;@ageron&lt;/denchmark-link&gt;
 I don't see any error with . Gist is &lt;denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/4b32a507c8209a95d38335e8efc3e231/untitled203.ipynb&gt;here&lt;/denchmark-link&gt;
.
But I notice error is back with    pip install tf-nightly-gpu-2.0-preview==2.0.0-dev20190518 Thanks!
		</comment>
		<comment id='12' author='ageron' date='2019-06-04T17:12:17Z'>
		Thanks for reporting the issue, will send a fix very soon.
		</comment>
		<comment id='13' author='ageron' date='2019-06-04T21:12:37Z'>
		Should now be fixed by &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/6a6e8c2586dfd2aeeebe0d94d60dcca4604ab481&gt;6a6e8c2&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='14' author='ageron' date='2019-06-04T21:12:41Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=27829&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=27829&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='15' author='ageron' date='2019-06-21T01:13:54Z'>
		def build_cell(self):
&lt;denchmark-code&gt;    char_input = self.tf.keras.layers.Input(shape=(1), batch_size=1, name="char_input", dtype=self.tf.int32)
    char_input_one_hot = self.tf.keras.backend.one_hot(char_input, self.vocab_size)
    previous_hidden_state_input = self.tf.keras.layers.Input(shape=(self.num_units), batch_size=1, name="previous_hidden_state_input")
    previous_cell_state_input = self.tf.keras.layers.Input(shape=(self.num_units), batch_size=1, name="previous_cell_state_input")`
    
    
    hidden_input_stacked = self.tf.keras.layers.concatenate([self.tf.keras.backend.squeeze(char_input_one_hot, axis=1), previous_hidden_state_input], axis=1)
    #Forget gate
    forget_gate_f = self.tf.keras.layers.Dense(units=self.num_units, activation="sigmoid")(hidden_input_stacked) #Helps us to take decisions about what must be removed from previous hidden state
    #Input gate
    input_gate_i = self.tf.keras.layers.Dense(units=self.num_units, activation="sigmoid")(hidden_input_stacked) #Decides which values to update
    input_gate_g = self.tf.keras.layers.Dense(units=self.num_units, activation="tanh")(hidden_input_stacked)  #Creates a vector for new candidates to added to present cell state.
    #Output_gate
    output_state_o = self.tf.keras.layers.Dense(units=self.num_units, activation="sigmoid")(hidden_input_stacked)
    #Current cell state
    current_cell_state = self.tf.keras.layers.add([self.tf.keras.layers.multiply([input_gate_g,input_gate_i]),self.tf.keras.layers.multiply([previous_cell_state_input,forget_gate_f])], name="current_cell_state")
    #output_hidden_state
    output_hidden_state = self.tf.keras.layers.multiply([self.tf.keras.layers.Activation("tanh")(current_cell_state), output_state_o], name="output_hidden_state")
    #output_char_probs
    output_char_probs = self.tf.keras.layers.Dense(units=self.vocab_size, activation="softmax", name="output_char_probs")(output_hidden_state)
    
    cell = self.tf.keras.Model(inputs=[char_input, previous_hidden_state_input, previous_cell_state_input], outputs=[output_char_probs, output_hidden_state, current_cell_state])
    return cell`
&lt;/denchmark-code&gt;

This code will break tensorflow 2.0, I found the problem to be in tensorflow/tensorflow/python/keras/layers/merge.py  / with the add and multiply functions. Please fix
The error I received trying to feed input in:

raise RuntimeError("Variable *= value not supported. Use "
RuntimeError: Variable *= value not supported. Use `var.assign(var * value)` to modify the variable &gt;     or `var = var * value` to get a new Tensor object.


		</comment>
		<comment id='16' author='ageron' date='2019-12-31T01:34:46Z'>
		Can the line:



tensorflow/tensorflow/python/keras/layers/merge.py


         Line 245
      in
      3a094e6






 output += inputs[i] 





be changed to
output = output + inputs[i]
to fix this?
(and similar within Subtract etc.)
It doesn't like the += notation when applied to a tf.Variable
		</comment>
	</comments>
</bug>