<bug id='37426' author='ghost(ghost)' open_date='2020-03-08T19:22:33Z' closed_time='2020-03-14T15:15:34Z'>
	<summary>NotFoundError: No registered 'Identity' OpKernel for 'TPU' devices compatible with node</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): Google Colab
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device:
TensorFlow installed from (source or
binary):
TensorFlow version (use command below): 2.1.0
Python version: - Bazel
version (if compiling from source):
GCC/Compiler version (if compiling from
source):
CUDA/cuDNN version: - GPU model and memory:

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
Throws error "No registered 'Identity' OpKernel for 'TPU' devices compatible with node".
Script uses efficientnet.tfkeras from efficientnet python package 1.1.0 (pip install efficientnet).
Describe the expected behavior
Script should start training on TPU.

Google Colab:
&lt;denchmark-link:https://drive.google.com/open?id=1QALjqGEX4z5vTBjRBUbyn5WmElYzMJzR&gt;https://drive.google.com/open?id=1QALjqGEX4z5vTBjRBUbyn5WmElYzMJzR&lt;/denchmark-link&gt;

Public access to GCS bucket is granted.
Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
Model: "sequential"
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h1&gt;Layer (type)                 Output Shape              Param #&lt;/denchmark-h&gt;

efficientnet-b0 (Model)      (None, 1280)              4049564
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

dense (Dense)                (None, 120)               153720
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

dense_1 (Dense)              (None, 120)               14520
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h1&gt;dense_2 (Dense)              (None, 2)                 242&lt;/denchmark-h&gt;

Total params: 4,218,046
Trainable params: 4,176,030
Non-trainable params: 42,016
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;WARNING:tensorflow:period argument is deprecated. Please use save_freq to specify the frequency in number of samples seen.
WARNING:tensorflow:period argument is deprecated. Please use save_freq to specify the frequency in number of samples seen.
[INFO] Modelul este salvat pe disc dupa fiecare epoca in formatul: nume_model_weights-epoca-training_set_accuracy-validation_set_accuracy.hdf5
Train for 17.0 steps, validate for 4.0 steps
Epoch 1/25
0/17 [..............................] - ETA: 0s&lt;/denchmark-h&gt;

NotFoundError                             Traceback (most recent call last)
 in ()
165     validation_data=valdataset,
166     validation_steps=np.ceil(totalVal / BS),
--&gt; 167     epochs=NUM_EPOCHS,
168     #callbacks=[checkpointer],
169 )
13 frames
/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)
NotFoundError: No registered 'Identity' OpKernel for 'TPU' devices compatible with node {{node Identity}}
(OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_UINT8
.  Registered:  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_HALF, DT_UINT32, DT_UINT64, DT_RESOURCE, DT_VARIANT]
device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_HALF, DT_UINT32, DT_UINT64, DT_RESOURCE, DT_VARIANT]
device='XLA_TPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_BFLOAT16, DT_UINT32, DT_UINT64, DT_RESOURCE, DT_VARIANT]
device='XLA_CPU'; T in [DT_UINT8, DT_QUINT8, DT_UINT16, DT_INT8, DT_QINT8, ..., DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_BFLOAT16]
device='TPU'; T in [DT_INT32, DT_UINT32, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE, DT_BOOL, DT_COMPLEX64, DT_INT64, DT_UINT64]
device='TPU_SYSTEM'
device='GPU'; T in [DT_HALF]
device='GPU'; T in [DT_BFLOAT16]
device='GPU'; T in [DT_FLOAT]
device='GPU'; T in [DT_DOUBLE]
device='GPU'; T in [DT_INT64]
device='GPU'; T in [DT_UINT16]
device='GPU'; T in [DT_INT16]
device='GPU'; T in [DT_UINT8]
device='GPU'; T in [DT_INT8]
device='GPU'; T in [DT_COMPLEX64]
device='GPU'; T in [DT_COMPLEX128]
device='GPU'; T in [DT_VARIANT]
device='DEFAULT'; T in [DT_STRING]
device='DEFAULT'; T in [DT_VARIANT]
device='DEFAULT'; T in [DT_RESOURCE]
device='CPU'
&lt;denchmark-code&gt; [[Identity]] [Op:MakeIterator]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='ghost(ghost)' date='2020-03-09T05:48:24Z'>
		I am able to replicate the issue, please find the &lt;denchmark-link:https://colab.sandbox.google.com/gist/Saduf2019/c31e0255179d513782e50bbf20609726/37426.ipynb&gt;gist here&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='2' author='ghost(ghost)' date='2020-03-09T20:28:17Z'>
		Having same issue &lt;denchmark-link:https://drive.google.com/open?id=1LCeeTcdJuXGXoddfSPAQAqDgigSdqQTl&gt;here&lt;/denchmark-link&gt;
, although the model seems to work when using the GPU. I believe it may be related to the override of the Keras ImageGenerator but I have not enough knowledge to be sure.
EDIT: I have converted the code to use keras.utils.Sequences, the problem still holds.
		</comment>
		<comment id='3' author='ghost(ghost)' date='2020-03-13T12:31:30Z'>
		I have the same issue here. For training in TPU, do we have to use the tfrecord or we could use our customized data generator?
		</comment>
		<comment id='4' author='ghost(ghost)' date='2020-03-13T21:52:38Z'>
		The error message indicates you're using uint8, which isn't supported on TPU. You can cast to uint32.
		</comment>
		<comment id='5' author='ghost(ghost)' date='2020-03-14T13:57:02Z'>
		
The error message indicates you're using uint8, which isn't supported on TPU. You can cast to uint32.

Could you show how to do that when using a kera.utils.Sequence? Should you override it with a class?
Also,  I need to work with floating point number, am I right to assume that only float16 is supported?
EDIT: I managed to override the Serquence with:
class TypeOverrideSequence(Sequence):

    original_generator = None
    dtype = None

    def __init__(self, generator, dtype= tf.float32 ):
        self.original_generator = generator
        self.dtype = dtype

    def __len__(self):
        return self.original_generator.__len__()

    def on_epoch_end(self):
        self.original_generator.on_epoch_end()

    def __getitem__(self, idx):
        data = self.original_generator.__getitem__(idx)
        return (tf.convert_to_tensor(data[0],dtype=self.dtype), tf.convert_to_tensor(data[1],dtype=self.dtype))
The original error is gone but this new one comes out &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/24099&gt;#24099&lt;/denchmark-link&gt;
. I believe is due to not using  only functions. Yet  does not have easy ways of setting up a feeding pipeline of images from disk, while  offers the  class. Also I have no idea how to proceed further.
		</comment>
		<comment id='6' author='ghost(ghost)' date='2020-03-14T15:15:34Z'>
		Jhseu, for me it is working now, per your suggestion.
I am converting the uint8 image resulted from tf.io.decode_jpeg to float32 (or uint32 if I wish) using:
image = tf.image.decode_jpeg("image.jpg", channels=3)
image = tf.cast(image, tf.float32)
If I want to also cast all values between 0 and 1 I use:
image = tf.cast(image, tf.float32)/255.0    # convert image to floats in [0, 1] range
Also, I found using TFRecords to be very important, using just tf.data.Dataset is impossibly slow.
Thank you. I will close the issue.
		</comment>
		<comment id='7' author='ghost(ghost)' date='2020-03-14T15:15:36Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37426&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37426&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='ghost(ghost)' date='2020-07-31T15:36:31Z'>
		
I have the same issue here. For training in TPU, do we have to use the tfrecord or we could use our customized data generator?

We can use both data but the data should be in cloud storage for tpu training . Also for visualizing the logs to Tensorboard we should train the model only in Cloud TPU . I tried to run the model running in  Colab TPU having the storage bucket but it won't work like that .
		</comment>
	</comments>
</bug>