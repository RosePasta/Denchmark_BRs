<bug id='35603' author='YLaCoon' open_date='2020-01-06T08:39:17Z' closed_time='2020-03-17T21:04:45Z'>
	<summary>[XLA] [TPU] It should not be possible to run out of vmem - please file a bug against XLA.</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):
TensorFlow version (use command below):
Python version:
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
v1.12.1-21643-g03f1214 2.1.0-dev20200105

I modified this script: &lt;denchmark-link:https://github.com/tensorflow/models/blob/master/official/nlp/bert/run_squad.py&gt;https://github.com/tensorflow/models/blob/master/official/nlp/bert/run_squad.py&lt;/denchmark-link&gt;
.
&lt;denchmark-code&gt;2020-01-06 08:30:26.371423: W tensorflow/core/distributed_runtime/eager/remote_tensor_handle_data.cc:75]Unable to destroy remote tensor handles. If you are running a tf.function, it usually indicates some op in the graph gets an error: {{function_node __inference_tpu_function_106662}} Compilation failure: Ran out of memory in memory space vmem. It should not be possible to run out of vmem - please file a bug against XLA.

Largest program allocations in vmem:

  XLA label: %fusion.5840 = f32[512,1024]{1,0:T(8,128)} fusion(s32[512]{0:T(512)}, f32[512,1024]{1,0:T(8,128)}, f32[]{:T(256)}, f32[]{:T(256)}), kind=kCustom, calls=%fused_computation.5742
  Allocation type: scoped

  XLA label: %fusion.5840 = f32[512,1024]{1,0:T(8,128)} fusion(s32[512]{0:T(512)}, f32[512,1024]{1,0:T(8,128)}, f32[]{:T(256)}, f32[]{:T(256)}), kind=kCustom, calls=%fused_computation.5742
  Allocation type: scoped

  XLA label: %fusion.5840 = f32[512,1024]{1,0:T(8,128)} fusion(s32[512]{0:T(512)}, f32[512,1024]{1,0:T(8,128)}, f32[]{:T(256)}, f32[]{:T(256)}), kind=kCustom, calls=%fused_computation.5742
  Allocation type: scoped

        TPU compilation failed
         [[{{node tpu_compile_succeeded_assert/_7693574632605057830/_9}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

Traceback (most recent call last):
  File "train_eval.py", line 482, in &lt;module&gt;
    main()
  File "train_eval.py", line 458, in main
    global_step, tr_loss = train(args, model_class, tokenizer, config, strategy)
  File "train_eval.py", line 129, in train
    running_loss = smooth * running_loss + (1. - smooth) * float(loss)
  File "/usr/local/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 867, in __float__
    return float(self._numpy())
  File "/usr/local/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 918, in _numpy
    six.raise_from(core._status_to_exception(e.code, e.message), None)
  File "&lt;string&gt;", line 3, in raise_from
tensorflow.python.framework.errors_impl.ResourceExhaustedError: {{function_node __inference_tpu_function_106662}} Compilation failure: Ran out of memory in memory space vmem. It should not be possible to run out of vmem - please file a bug against XLA.

Largest program allocations in vmem:

  XLA label: %fusion.5840 = f32[512,1024]{1,0:T(8,128)} fusion(s32[512]{0:T(512)}, f32[512,1024]{1,0:T(8,128)}, f32[]{:T(256)}, f32[]{:T(256)}), kind=kCustom, calls=%fused_computation.5742
  Allocation type: scoped

  XLA label: %fusion.5840 = f32[512,1024]{1,0:T(8,128)} fusion(s32[512]{0:T(512)}, f32[512,1024]{1,0:T(8,128)}, f32[]{:T(256)}, f32[]{:T(256)}), kind=kCustom, calls=%fused_computation.5742
  Allocation type: scoped

  XLA label: %fusion.5840 = f32[512,1024]{1,0:T(8,128)} fusion(s32[512]{0:T(512)}, f32[512,1024]{1,0:T(8,128)}, f32[]{:T(256)}, f32[]{:T(256)}), kind=kCustom, calls=%fused_computation.5742
  Allocation type: scoped

        TPU compilation failed
         [[{{node tpu_compile_succeeded_assert/_7693574632605057830/_9}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
&lt;/denchmark-code&gt;

I debugged this error a bit and found that using the custom optimizer causes the error. That is
&lt;denchmark-code&gt;optimizer = optimization.create_optimizer(args.learning_rate,
            num_steps_per_epoch * args.num_train_epochs, warmup_steps)
&lt;/denchmark-code&gt;

With a default optimizer the program runs fine:
&lt;denchmark-code&gt;optimizer = tf.keras.optimizers.Adam(learning_rate=args.learning_rate)
&lt;/denchmark-code&gt;

Describe the expected behavior
Custom optimizers should work and the error message should be more specific.
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
I was not able to create a minimal example to reproduce a minimal example, but the two lines make the difference between a failed and successful compilation.
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='YLaCoon' date='2020-01-08T00:21:00Z'>
		Sorry for the breakage. I don't think our generated report has enough info for us to reproduce and debug that. We have a PR to add more info in our OOM report system that will hopefully show up in the nightly version later this week. Perhaps then you can rerun your custom optimizer and we can reproduce the issue.
In the meanwhile, assigning this to &lt;denchmark-link:https://github.com/saberkun&gt;@saberkun&lt;/denchmark-link&gt;
 who may have a way to reproduce it internally, then our compiler team can debug it.
		</comment>
		<comment id='2' author='YLaCoon' date='2020-01-09T07:34:12Z'>
		Thanks. I tried to create a self-contained reproduction but it's a little hard to debug on TPUs. The official script runs fine. I debugged it a little more and my current work-around is to comment out this line:
&lt;denchmark-link:https://github.com/tensorflow/models/blob/master/official/nlp/optimization.py#L142&gt;https://github.com/tensorflow/models/blob/master/official/nlp/optimization.py#L142&lt;/denchmark-link&gt;

Gradient clipping is not that important for the final model performance. I will rerun with the future version of tf-nightly.
		</comment>
		<comment id='3' author='YLaCoon' date='2020-01-11T04:44:49Z'>
		Thanks, YLaCoon!
The better report should have been submitted into tf-nightly a couple of days ago. Would be great if you can retry your bug and copy-paste the new report -- once we have it it should be much easier for us to reproduce on compiler side.
		</comment>
		<comment id='4' author='YLaCoon' date='2020-01-17T18:40:13Z'>
		Could you provide the code your modified?
The gradient clipping will cause all reduce and probably more memory cost.
If you reduce your batch size, could you make it work?
		</comment>
		<comment id='5' author='YLaCoon' date='2020-03-17T21:04:47Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35603&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35603&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='YLaCoon' date='2020-09-26T08:08:18Z'>
		&lt;denchmark-link:https://github.com/yunxing&gt;@yunxing&lt;/denchmark-link&gt;
 I am having the same issue with the latest TF  version. Can you suggest any fixes for it? I tried using nightly version but they result in even more bizarre errors.
In my  function I am feeding it 2 numpy arrays. But If I feed a custom generator which takes the arrays in smaller batches then I get the  error
		</comment>
		<comment id='7' author='YLaCoon' date='2020-09-28T06:11:50Z'>
		&lt;denchmark-link:https://github.com/neel04&gt;@neel04&lt;/denchmark-link&gt;
,
Could you please submit a new issue from &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/new/choose&gt;this link&lt;/denchmark-link&gt;
 and fill in the template, so that we can track the issue there. Thanks!
		</comment>
	</comments>
</bug>