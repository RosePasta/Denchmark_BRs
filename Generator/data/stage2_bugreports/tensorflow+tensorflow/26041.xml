<bug id='26041' author='Mrpatekful' open_date='2019-02-23T18:18:09Z' closed_time='2019-03-26T17:52:09Z'>
	<summary>tf-nightly-gpu-2.0-preview dataset and tf.function error</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
TensorFlow installed from (source or binary): pip
TensorFlow version (use command below): tf-nightly-gpu-2.0-preview
Python version: Python 3 in Google Colab
CUDA/cuDNN version: 10
GPU model and memory: Tesla K80

Describe the current behavior
When using GPU runtime in Google Colab with tf-nightly-gpu-2.0-preview tensorflow version, creating a dataset with tensorflow datasets inside a tf.function decorated function results in an exception regarding incompatible device types.
Describe the expected behavior
Expected behavior is not receiving an exception. Removing tf.function from the train method results in correct behavior. The code also works well without GPU support (tf-nightly-2.0-preview).
Code to reproduce the issue
import tensorflow_datasets as tfds
import tensorflow as tf
import numpy as np

class MyModel(tf.keras.Model):

    def __init__(self):
        super(MyModel, self).__init__()
        self._layer1 = tf.keras.layers.Dense(20, activation='relu')
        self._layer2 = tf.keras.layers.Dense(10)

    def call(self, x, training):
        x = self._layer1(x)
        x = self._layer2(x)
        return x


def create_dataset():
    def process(features):
        image, label = features['image'], features['label']
        return tf.reshape(image, [-1]) / np.float32(255.0), label

    data_builder = tfds.builder('mnist')
    dataset = data_builder.as_dataset(split=tfds.Split.TRAIN)
    dataset = (
        dataset.map(process)
        .batch(32)
        .repeat(1)
    )

    return dataset

  
avg_loss = tf.metrics.Mean()

  
@tf.function
def train(model, optimizer):
    dataset = create_dataset()
    step = 0
    for images, labels in dataset:
        step += 1
        with tf.GradientTape() as tape:
            logits = model(images, True)
            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(
                logits=logits, labels=labels)
            loss = tf.math.reduce_mean(loss)
            
        avg_loss.update_state(loss)
        
        grads = tape.gradient(
            loss, model.trainable_variables)
        optimizer.apply_gradients(
            zip(grads, model.trainable_variables))
        
        if tf.equal(step % 20, 0):
            tf.print(avg_loss.result())
            avg_loss.reset_states()
            

NUM_EPOCHS = 2
model = MyModel()
optimizer = tf.keras.optimizers.Adam()
for _ in range(NUM_EPOCHS):
    train(model, optimizer)
Other info / logs
Link to the original Google Colab &lt;denchmark-link:https://gist.github.com/Mrpatekful/92f274756dffd6aab2993e401b7fb7af&gt;file&lt;/denchmark-link&gt;

The encountered exception:
&lt;denchmark-code&gt;InvalidArgumentError                      Traceback (most recent call last)
&lt;ipython-input-43-03d2bfd1799c&gt; in &lt;module&gt;()
     59 optimizer = tf.keras.optimizers.Adam()
     60 for _ in range(NUM_EPOCHS):
---&gt; 61     train(model, optimizer)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    436         # Lifting succeeded, so variables are initialized and we can run the
    437         # stateless function.
--&gt; 438         return self._stateless_fn(*args, **kwds)
    439     else:
    440       canon_args, canon_kwds = self._canonicalize_function_inputs(args, kwds)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
   1251     """Calls a graph function specialized to the inputs."""
   1252     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
-&gt; 1253     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   1254 
   1255   @property

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs)
    537     """
    538     return self._call_flat(
--&gt; 539         (t for t in nest.flatten((args, kwargs))
    540          if isinstance(t, (ops.Tensor,
    541                            resource_variable_ops.ResourceVariable))))

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args)
    590     # Only need to override the gradient in graph mode and when we have outputs.
    591     if context.executing_eagerly() or not self.outputs:
--&gt; 592       outputs = self._inference_function.call(ctx, args)
    593     else:
    594       self._register_gradient()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in call(self, ctx, args)
    380             attrs=("executor_type", executor_type,
    381                    "config_proto", config),
--&gt; 382             ctx=ctx)
    383       # Replace empty list with None
    384       outputs = outputs or None

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     64     else:
     65       message = e.message
---&gt; 66     six.raise_from(core._status_to_exception(e.code, message), None)
     67   except TypeError as e:
     68     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):

/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:CPU:0 vs /job:localhost/replica:0/task:0/device:GPU:0 [Op:__inference_train_925768]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Mrpatekful' date='2019-03-18T16:10:13Z'>
		Any progress on this issue ? I'm facing similar issue here &lt;denchmark-link:https://github.com/tensorflow/agents/issues/19&gt;tensorflow/agents#19&lt;/denchmark-link&gt;

I thought setting the device placement be the solution but I finally can't verify it.
		</comment>
		<comment id='2' author='Mrpatekful' date='2019-03-18T22:35:45Z'>
		I believe &lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
 has fix for this in the works.
		</comment>
		<comment id='3' author='Mrpatekful' date='2019-03-18T22:38:04Z'>
		&lt;denchmark-link:https://github.com/Mrpatekful&gt;@Mrpatekful&lt;/denchmark-link&gt;
 could you sync past &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/11f13a90545ff793762191da02e27669934c9e1c&gt;11f13a9&lt;/denchmark-link&gt;
 and see whether the problem goes away? Thanks.
		</comment>
		<comment id='4' author='Mrpatekful' date='2019-03-22T23:02:01Z'>
		ran into the same problem when run tf-agents example code,
nightly build as of '2.0.0-dev20190322' still have the same issues
		</comment>
		<comment id='5' author='Mrpatekful' date='2019-03-26T12:20:28Z'>
		I'm too facing this. It triggers when moving to the second variable [1] in
for i in tf.range(100):
Commands under loop run correctly for i = 0,
for i in tf.range(100) is inside the function.
Built 2.0 from source around 20th March (+ or - 2 days). Thanks
		</comment>
		<comment id='6' author='Mrpatekful' date='2019-03-26T17:52:09Z'>
		I cannot reproduce the issue using Python 3 Google colab, running:
!pip install tf-nightly-gpu-2.0-preview (this installs tf-nightly-gpu-2.0-preview-2.0.0.dev20190326)
followed by:
&lt;denchmark-code&gt;import tensorflow_datasets as tfds
import tensorflow as tf
import numpy as np

class MyModel(tf.keras.Model):

    def __init__(self):
        super(MyModel, self).__init__()
        self._layer1 = tf.keras.layers.Dense(20, activation='relu')
        self._layer2 = tf.keras.layers.Dense(10)

    def call(self, x, training):
        x = self._layer1(x)
        x = self._layer2(x)
        return x


def create_dataset():
    def process(features):
        image, label = features['image'], features['label']
        return tf.reshape(image, [-1]) / np.float32(255.0), label

    data_builder = tfds.builder('mnist')
    data_builder.download_and_prepare()
    dataset = data_builder.as_dataset(split=tfds.Split.TRAIN)
    dataset = (
        dataset.map(process)
        .batch(32)
        .repeat(1)
    )

    return dataset

  
avg_loss = tf.metrics.Mean()

  
@tf.function
def train(model, optimizer):
    dataset = create_dataset()
    step = 0
    for images, labels in dataset:
        step += 1
        with tf.GradientTape() as tape:
            logits = model(images, True)
            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(
                logits=logits, labels=labels)
            loss = tf.math.reduce_mean(loss)
            
        avg_loss.update_state(loss)
        
        grads = tape.gradient(
            loss, model.trainable_variables)
        optimizer.apply_gradients(
            zip(grads, model.trainable_variables))
        
        if tf.equal(step % 20, 0):
            tf.print(avg_loss.result())
            avg_loss.reset_states()
            

NUM_EPOCHS = 2
model = MyModel()
optimizer = tf.keras.optimizers.Adam()
for _ in range(NUM_EPOCHS):
    train(model, optimizer)
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/bayesian&gt;@bayesian&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/caissalover&gt;@caissalover&lt;/denchmark-link&gt;
 please create a separate issue with instructions on how to reproduce the issue you are seeing. I am closing this issue.
		</comment>
		<comment id='7' author='Mrpatekful' date='2019-03-26T17:52:10Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=26041&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=26041&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='Mrpatekful' date='2019-07-18T01:22:44Z'>
		I went into this error when using tf.keras.layers.GRU. Switching to tf.compat.v1.keras.layers.CuDNNGRU resolved the problem.
		</comment>
	</comments>
</bug>