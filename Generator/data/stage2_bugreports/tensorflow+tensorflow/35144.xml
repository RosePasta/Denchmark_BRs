<bug id='35144' author='olegmyrk' open_date='2019-12-16T02:52:40Z' closed_time='2019-12-23T01:49:24Z'>
	<summary>Low performance in TF2.x Distributed Mirrored Strategy with 4 V100 GPUs</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04.3 LTS
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0
Python version: Python 3.6.8
CUDA/cuDNN version: Driver Version: 440.33.01, CUDA Version: 10.2, cuDNN 7.6.2
GPU model and memory: Tesla V100-SXM2-16GB

Describe the current behavior
With 4 V100 GPUs in Distributed Mirrored GPU Strategy training single step is around 3x slower than with single V100 GPU.
Describe the expected behavior
Single step should be less than 2x slower.

Training loop (hierarchical VAE in the current configuration):
&lt;denchmark-link:https://github.com/olegmyrk/SPADE-Tensorflow/blob/9c1ced5b7b24640aeb7726169e215f5e63d971d3/SPADE.py#L1121&gt;https://github.com/olegmyrk/SPADE-Tensorflow/blob/9c1ced5b7b24640aeb7726169e215f5e63d971d3/SPADE.py#L1121&lt;/denchmark-link&gt;

The code is adapted from TF1.x repository:
&lt;denchmark-link:https://github.com/olegmyrk/SPADE-Tensorflow/blob/8866a0b1457cbd4be5d6f549f9bf4075d49b2486/SPADE.py#L1045&gt;https://github.com/olegmyrk/SPADE-Tensorflow/blob/8866a0b1457cbd4be5d6f549f9bf4075d49b2486/SPADE.py#L1045&lt;/denchmark-link&gt;

and is compiled using TF2.x @tf.function annotation.
It uses a dry-run of the model to pre-create variables using tf.compat.v1.variable_scope(scope, reuse=tf.compat.v1.AUTO_REUSE):
&lt;denchmark-link:https://github.com/olegmyrk/SPADE-Tensorflow/blob/9c1ced5b7b24640aeb7726169e215f5e63d971d3/SPADE.py#L1144&gt;https://github.com/olegmyrk/SPADE-Tensorflow/blob/9c1ced5b7b24640aeb7726169e215f5e63d971d3/SPADE.py#L1144&lt;/denchmark-link&gt;

and then runs the actual training step(s)
&lt;denchmark-link:https://github.com/olegmyrk/SPADE-Tensorflow/blob/9c1ced5b7b24640aeb7726169e215f5e63d971d3/SPADE.py#L1180&gt;https://github.com/olegmyrk/SPADE-Tensorflow/blob/9c1ced5b7b24640aeb7726169e215f5e63d971d3/SPADE.py#L1180&lt;/denchmark-link&gt;

The total number of mirrored parameters is around 500MB.
With 4 V100 GPUs training step is around 3x slower than with single V100 GPU.
Command:
nohup python3 main.py --dataset CelebAMask-HQ --img_height 256 --img_width 256 --ch 16 --img_ch 3 --phase train --save_freq 10000 --batch_size 18 --gan_type hinge --code_gan_type gan --n_critic 1 --code_num_layers=4 --code_dist_num_layers=0 --sn=False --train_main=true --train_nondet=false --lr 0.0002 --print_freq 100 &amp;&gt; train.CelebAMask-HQ.log &amp;
Other info / logs
With CUDA_VISIBLE_DEVICES=0

Startup time: ~9 min
GPU utilization: ~90%
Training step: 0.7 seconds
Log file: train.CelebAMask-HQ.1xgpu.log

With CUDA_VISIBLE_DEVICES=0,1,2,3

Startup time: ~30min

Build variables (dry run): ~10min
Build model: ~20min


GPU utilization: ~50%
Training step: 2 seconds
Log file: train.CelebAMask-HQ.4xgpu.log
train.CelebAMask-HQ.1xgpu.log
train.CelebAMask-HQ.4xgpu.log

	</description>
	<comments>
		<comment id='1' author='olegmyrk' date='2019-12-16T23:41:24Z'>
		Attaching some more logs:

CPU load is low enough so I don't think loading &amp; feeding training data is an issue
cpu.log
Typical GPU utilization: pretty low
nvidia-smi.log
Typical NVLINK utilization: it looks that NVLINK is not used at all?
nvidia-smi-nvlink.log

		</comment>
		<comment id='2' author='olegmyrk' date='2019-12-17T01:35:24Z'>
		I've rerun the script using
NCCL_DEBUG=INFO NCCL_DEBUG_SUBSYS=ALL CUDA_VISIBLE_DEVICES=0,1,2,3
Attaching the NCCL logs
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3970968/train.CelebAMask-HQ.4xgpu.nvlink.log&gt;train.CelebAMask-HQ.4xgpu.nvlink.log&lt;/denchmark-link&gt;

To the extent I can read NCCL logs it seems that it should be working fine?
&lt;denchmark-code&gt;ip-172-31-78-131:88250:88431 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
ip-172-31-78-131:88250:88845 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff
...
ip-172-31-78-131:88250:88842 [3] NCCL INFO Channel 00 :    0   1   2   3
...
ip-172-31-78-131:88250:88844 [1] NCCL INFO Ring 00 : 2[1] -&gt; 3[0] via P2P/direct pointer
...
ip-172-31-78-131:88250:88842 [3] NCCL INFO Using 256 threads, Min Comp Cap 7, Trees disabled
ip-172-31-78-131:88250:88845 [0] NCCL INFO comm 0x7f7628001850 rank 3 nranks 4 cudaDev 0 nvmlDev 0 - Init COMPLETE
...
ip-172-31-78-131:88250:88837 [3] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f7956f63600 recvbuff 0x7f7956f63600 count 47422082 datatype 7 op 0 root 0 comm 0x7f762c0021a0 [nranks=4] stream 0x7f8b24007490
...
ip-172-31-78-131:88250:88837 [3] NCCL INFO Launch mode Group/CGMD
...```
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='olegmyrk' date='2019-12-17T01:43:07Z'>
		There is a perf regression w.r.t. prefetching data for mirrored strategy in 2.0 that is fixed later. Could you try nightly or 2.1-rc0?
		</comment>
		<comment id='4' author='olegmyrk' date='2019-12-17T08:17:03Z'>
		Switching to 2.1-rc0 helped with training step time, thanks!
However it takes 3x longer to build models in multi-GPU setup: less than 10minutes on single GPU vs 30 minutes 4 GPUs. In both cases I am using the same code with Mirrored strategy.
Additionally enabling spectral normalization
&lt;denchmark-link:https://github.com/olegmyrk/SPADE-Tensorflow/blob/4203f30b6253a9d4743962087896fab26381c67b/ops.py#L579&gt;https://github.com/olegmyrk/SPADE-Tensorflow/blob/4203f30b6253a9d4743962087896fab26381c67b/ops.py#L579&lt;/denchmark-link&gt;

increases the time to build models another 2x (60 minutes on 4 gpus), increases the training step duration 3x, and also seems to increase GPU memory required, effectively reducing the batch size. This happens only when multiple GPUs are enabled.
Spectral normalization implementation does unnecessary synchronization of non-trainable, but deterministic (as a function of initialization and weights being normalized) variable:
u = tf.compat.v1.get_variable("u", [1, w_shape[-1]], initializer=tf.compat.v1.random_normal_initializer(), trainable=False, aggregation=tf.VariableAggregation. ONLY_FIRST_REPLICA)
Is there a way to avoid this unnecessary synchronization?
		</comment>
		<comment id='5' author='olegmyrk' date='2019-12-22T22:01:35Z'>
		FYI: I've filed a separate follow-up issue:
AutoGraph is compiled 5x slower in TF2.x Multi-GPU Distributed Mirrored Strategy
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/35346&gt;#35346&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='olegmyrk' date='2019-12-22T22:10:15Z'>
		Also is there a better channel to ask about issues with Spectral Normalization in Mirrored Distributed mode?
&lt;denchmark-link:https://github.com/olegmyrk/SPADE-Tensorflow/blob/4203f30b6253a9d4743962087896fab26381c67b/ops.py#L579&gt;https://github.com/olegmyrk/SPADE-Tensorflow/blob/4203f30b6253a9d4743962087896fab26381c67b/ops.py#L579&lt;/denchmark-link&gt;

Essentially I need to convince TF Mirrored Distributed mode not to synchronize this variable across GPUs:
&lt;denchmark-code&gt;u = tf.compat.v1.get_variable("u", [1, w_shape[-1]], initializer=tf.compat.v1.random_normal_initializer(), trainable=False, aggregation=tf.VariableAggregation. ONLY_FIRST_REPLICA)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='olegmyrk' date='2019-12-22T23:34:08Z'>
		
Essentially I need to convince TF Mirrored Distributed mode not to synchronize this variable across GPUs:
u = tf.compat.v1.get_variable("u", [1, w_shape[-1]], initializer=tf.compat.v1.random_normal_initializer(), trainable=False, aggregation=tf.VariableAggregation. ONLY_FIRST_REPLICA)

If you are using tf.VariableAggregation.ONLY_FIRST_REPLICA then it is expected.
		</comment>
		<comment id='8' author='olegmyrk' date='2019-12-23T01:48:39Z'>
		I've posted more details about variable replication for spectral norm in a separate issue.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/35347&gt;#35347&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='olegmyrk' date='2019-12-23T01:49:24Z'>
		As the initial distributed dataset performance regression in 2.0 was resolved by &lt;denchmark-link:https://github.com/byronyi&gt;@byronyi&lt;/denchmark-link&gt;
 I propose that we close this issue.
Please have a look at followup issues:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/35346&gt;#35346&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/35347&gt;#35347&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='olegmyrk' date='2019-12-23T01:49:26Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35144&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35144&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>