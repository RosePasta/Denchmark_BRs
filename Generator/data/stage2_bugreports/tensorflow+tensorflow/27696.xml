<bug id='27696' author='tarrade' open_date='2019-04-09T20:21:26Z' closed_time='2019-05-09T05:20:12Z'>
	<summary>tf.keras.estimator.model_to_estimator crashing when using tf.distribute.MirroredStrategy() with Only TensorFlow native optimizers are supported with DistributionStrategy</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):
TensorFlow version (use command below): 2.0.0-alpha0
Python version: 3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

Describe the current behavior
I am using a keras model with TF 2.0. I am converting the model to estimator:
tf.keras.estimator.model_to_estimator 
then it is crashing when running estimator_train_model.train(..) when using tf.distribute.MirroredStrategy() (after migrating the code to TF 2.0 of course).
It works fine when using None as strategy
I tried to follow the instruction: &lt;denchmark-link:https://www.tensorflow.org/alpha/guide/distribute_strategy&gt;https://www.tensorflow.org/alpha/guide/distribute_strategy&lt;/denchmark-link&gt;

Describe the expected behavior
The same was working with TF 1.x

Work in progress notebook can be found here:
&lt;denchmark-link:http://localhost:8888/notebooks/proj_DL_models_and_pipelines_with_GCP/notebook/TF_2.0/08-Mnist_keras_estimator.ipynb&gt;http://localhost:8888/notebooks/proj_DL_models_and_pipelines_with_GCP/notebook/TF_2.0/08-Mnist_keras_estimator.ipynb&lt;/denchmark-link&gt;

I am using a very basic Keras model with
&lt;denchmark-code&gt;optimiser = tf.keras.optimizers.Adam(lr=0.01, beta_1=0.9, epsilon=1e-07)
# Compile model
model.compile(loss='categorical_crossentropy',
                  optimizer=optimiser,
                  metrics=['accuracy'])

strategy=None # working
#strategy = tf.distribute.MirroredStrategy() # crashing with TF 2.0 but working with TF 1.X

# config tf.estimator to use a give strategy
training_config = tf.estimator.RunConfig(train_distribute=strategy,
                                         model_dir=FLAGS.model_dir,
                                         save_summary_steps=1,
                                         save_checkpoints_steps=100,
                                         keep_checkpoint_max=3,
                                         log_step_count_steps=10)

# transfor keras model to estimator model
estimator_train_model = tf.keras.estimator.model_to_estimator(keras_model=model_opt_tf,
                                         config=training_config)

# Fit the model (using estimator.train and data.Dataset)
estimator_train_model.train(input_fn=lambda:mnist_v1.input_mnist_tfrecord_dataset_fn(path_train_tfrecords+'*', FLAGS, mode=tf.estimator.ModeKeys.TRAIN, batch_size=FLAGS.batch_size),
                            steps=1000)
&lt;/denchmark-code&gt;

Other info / logs
I0409 22:06:27.293305 4531660224 model.py:211] input_dataset_fn: TRAIN, train
I0409 22:06:27.469371 123145492971520 estimator.py:1126] Calling model_fn.
I0409 22:06:27.475003 123145492971520 coordinator.py:219] Error reported to Coordinator: Only TensorFlow native optimizers are supported with DistributionStrategy.
Traceback (most recent call last):
File "/Users/tarrade/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py", line 297, in stop_on_exception
yield
File "/Users/tarrade/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_strategy.py", line 882, in run
self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
File "/Users/tarrade/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py", line 1127, in _call_model_fn
model_fn_results = self._model_fn(features=features, **kwargs)
File "/Users/tarrade/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/keras.py", line 278, in model_fn
raise ValueError('Only TensorFlow native optimizers are supported with '
ValueError: Only TensorFlow native optimizers are supported with DistributionStrategy.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

ValueError                                Traceback (most recent call last)
 in 
~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)
357
358       saving_listeners = _check_listeners_type(saving_listeners)
--&gt; 359       loss = self._train_model(input_fn, hooks, saving_listeners)
360       logging.info('Loss for final step: %s.', loss)
361       return self
~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)
1135   def _train_model(self, input_fn, hooks, saving_listeners):
1136     if self._train_distribution:
-&gt; 1137       return self._train_model_distributed(input_fn, hooks, saving_listeners)
1138     else:
1139       return self._train_model_default(input_fn, hooks, saving_listeners)
~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model_distributed(self, input_fn, hooks, saving_listeners)
1198       self._config._train_distribute.configure(self._config.session_config)
1199       return self._actual_train_model_distributed(
-&gt; 1200           self._config._train_distribute, input_fn, hooks, saving_listeners)
1201     # pylint: enable=protected-access
1202
~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _actual_train_model_distributed(self, strategy, input_fn, hooks, saving_listeners)
1267                     labels,  # although this will be None it seems
1268                     ModeKeys.TRAIN,
-&gt; 1269                     self.config))
1270           loss = strategy.reduce(reduce_util.ReduceOp.SUM,
1271                                  grouped_estimator_spec.loss)
~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py in call_for_each_replica(self, fn, args, kwargs)
1076     if kwargs is None:
1077       kwargs = {}
-&gt; 1078     return self._call_for_each_replica(fn, args, kwargs)
1079
1080   def _call_for_each_replica(self, fn, args, kwargs):
~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_strategy.py in _call_for_each_replica(self, fn, args, kwargs)
663   def _call_for_each_replica(self, fn, args, kwargs):
664     return _call_for_each_replica(self._container_strategy(), self._device_map,
--&gt; 665                                   fn, args, kwargs)
666
667   def _configure(self,
~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_strategy.py in _call_for_each_replica(distribution, device_map, fn, args, kwargs)
191     for t in threads:
192       t.should_run.set()
--&gt; 193     coord.join(threads)
194
195   return values.regroup(device_map, tuple(t.main_result for t in threads))
~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py in join(self, threads, stop_grace_period_secs, ignore_live_threads)
387       self._registered_threads = set()
388       if self._exc_info_to_raise:
--&gt; 389         six.reraise(*self._exc_info_to_raise)
390       elif stragglers:
391         if ignore_live_threads:
~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/six.py in reraise(tp, value, tb)
691             if value.traceback is not tb:
692                 raise value.with_traceback(tb)
--&gt; 693             raise value
694         finally:
695             value = None
~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py in stop_on_exception(self)
295     """
296     try:
--&gt; 297       yield
298     except:  # pylint: disable=bare-except
299       self.request_stop(ex=sys.exc_info())
~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_strategy.py in run(self)
880               self._captured_var_scope, reuse=self.replica_id &gt; 0), 
881           variable_scope.variable_creator_scope(self.variable_creator_fn):
--&gt; 882         self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
883         self.done = True
884     finally:
~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)
1125
1126     logging.info('Calling model_fn.')
-&gt; 1127     model_fn_results = self._model_fn(features=features, **kwargs)
1128     logging.info('Done calling model_fn.')
1129
~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/keras.py in model_fn(features, labels, mode)
276         not isinstance(keras_model.optimizer,
277                        (tf_optimizer_module.Optimizer, optimizers.TFOptimizer)):
--&gt; 278       raise ValueError('Only TensorFlow native optimizers are supported with '
279                        'DistributionStrategy.')
280
ValueError: Only TensorFlow native optimizers are supported with DistributionStrategy.
	</description>
	<comments>
		<comment id='1' author='tarrade' date='2019-04-10T08:19:05Z'>
		Here a code based on examples from TF 2.0 official doc to reproduce the issue:
pip install tensorflow==2.0.0-alpha0
pip install tensorflow-datasets
&lt;denchmark-code&gt;import tensorflow as tf
import tensorflow_datasets as tfds
from absl import logging

logging.set_verbosity(logging.INFO)
# Define the estimator's input_fn
STEPS_PER_EPOCH = 5
#BUFFER_SIZE = 10 # Use a much larger value for real code. 
BATCH_SIZE = 64
NUM_EPOCHS = 5


def input_fn():
    datasets, ds_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
    mnist_train, mnist_test = datasets['train'], datasets['test']

    BUFFER_SIZE = 10000
    BATCH_SIZE = 64

    def scale(image, label):
        image = tf.cast(image, tf.float32)
        image /= 255
    
        return image, label[..., tf.newaxis]

    train_data = mnist_train.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
    return train_data.repeat()

# Define train &amp; eval specs
train_spec = tf.estimator.TrainSpec(input_fn=input_fn,
                                    max_steps=STEPS_PER_EPOCH * NUM_EPOCHS)
eval_spec = tf.estimator.EvalSpec(input_fn=input_fn,
                                  steps=STEPS_PER_EPOCH)

def make_model():
    return tf.keras.Sequential([
        tf.keras.layers.Conv2D(32, 3, activation='relu',
                               kernel_regularizer=tf.keras.regularizers.l2(0.02),
                               input_shape=(28, 28, 1)),
        tf.keras.layers.MaxPooling2D(),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dropout(0.1),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(10, activation='softmax')
    ])

model = make_model()

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

#####
## works
#strategy=None 
## crash
strategy = tf.distribute.MirroredStrategy()

# config tf.estimator to use a give strategy
training_config = tf.estimator.RunConfig(train_distribute=strategy)
#####

estimator = tf.keras.estimator.model_to_estimator(
    keras_model = model,
    config=training_config
)

tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='tarrade' date='2019-04-21T08:10:17Z'>
		I have the same problem - on colab this is my code
&lt;denchmark-link:https://colab.research.google.com/drive/1mf-PK0a20CkObnT0hCl9VPEje1szhHat#scrollTo=MMbPOC3f5ku3&gt;https://colab.research.google.com/drive/1mf-PK0a20CkObnT0hCl9VPEje1szhHat#scrollTo=MMbPOC3f5ku3&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='tarrade' date='2019-04-22T06:31:48Z'>
		Correct link - &lt;denchmark-link:https://colab.research.google.com/drive/1mf-PK0a20CkObnT0hCl9VPEje1szhHat&gt;https://colab.research.google.com/drive/1mf-PK0a20CkObnT0hCl9VPEje1szhHat&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='tarrade' date='2019-04-26T16:18:18Z'>
		Have you tried the &lt;denchmark-link:https://www.tensorflow.org/alpha/guide/distribute_strategy#using_tfdistributestrategy_with_keras&gt;suggestion from the docs&lt;/denchmark-link&gt;
 of placing the code inside the strategy scope:
mirrored_strategy = tf.distribute.MirroredStrategy()
with mirrored_strategy.scope():
  model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])
  model.compile(loss='mse', optimizer='sgd')
I came here because I'm seeing this same error even when I do the scoping above.
		</comment>
		<comment id='5' author='tarrade' date='2019-04-26T16:29:52Z'>
		&lt;denchmark-link:https://github.com/mckinziebrandon&gt;@mckinziebrandon&lt;/denchmark-link&gt;
 , thanks for the idea. From the same doc (see the section estimator):
"The usage of tf.distribute.Strategy with Estimator is slightly different than the Keras case. Instead of using strategy.scope, now we pass the strategy object into the RunConfig for the Estimator."
This is what I did.
		</comment>
		<comment id='6' author='tarrade' date='2019-05-03T00:29:43Z'>
		This appears to be a real issue.  We flagged it internally and someone will hopefully get to it soon.  Thanks for reporting.
If you are curious about fixing it, then my guess is that keras.optimizer_v2.OptimizerV2 needs to be allowed in the set of classes that are allowed right where that ValueError is thrown.  It should work.
		</comment>
		<comment id='7' author='tarrade' date='2019-05-09T05:20:08Z'>
		This should be fixed in master now.
		</comment>
		<comment id='8' author='tarrade' date='2019-05-09T05:20:16Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=27696&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=27696&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='tarrade' date='2019-05-09T08:12:12Z'>
		I still have the problem with pip install tf-nightly-gpu on &lt;denchmark-link:https://colab.research.google.com/drive/1mf-PK0a20CkObnT0hCl9VPEje1szhHat&gt;https://colab.research.google.com/drive/1mf-PK0a20CkObnT0hCl9VPEje1szhHat&lt;/denchmark-link&gt;

ValueError: Only TensorFlow native optimizers are supported with DistributionStrategy.
		</comment>
		<comment id='10' author='tarrade' date='2019-05-09T15:40:51Z'>
		&lt;denchmark-link:https://github.com/guptapriya&gt;@guptapriya&lt;/denchmark-link&gt;
 same observation as &lt;denchmark-link:https://github.com/AntGul&gt;@AntGul&lt;/denchmark-link&gt;
 I am getting the same error:

I am using tf-nightly-2.0-preview-2.0.0.dev20190509
		</comment>
		<comment id='11' author='tarrade' date='2019-05-15T22:08:34Z'>
		The fix is in estimator: &lt;denchmark-link:https://github.com/tensorflow/estimator/commit/818c6163ba759dc257b7c27421bc46ef8259e217#diff-93ed941ec864a6ef78bd656293c99cea&gt;tensorflow/estimator@818c616#diff-93ed941ec864a6ef78bd656293c99cea&lt;/denchmark-link&gt;

Sometimes the version of estimator pip package might be stale. Would you mind updating the estimator package directly and testing then?
&lt;denchmark-link:https://pypi.org/project/tf-estimator-nightly/&gt;https://pypi.org/project/tf-estimator-nightly/&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='tarrade' date='2019-05-16T13:44:40Z'>
		&lt;denchmark-link:https://github.com/guptapriya&gt;@guptapriya&lt;/denchmark-link&gt;
 you are right. When I did my test I had:
&lt;denchmark-code&gt;tensorflow-estimator-2-0-preview 1.14.0.dev2019042600
tf-nightly-2-0-preview    2.0.0.dev20190509
&lt;/denchmark-code&gt;

I am mot using:
&lt;denchmark-code&gt;tfds-nightly==1.0.2.dev201905160105
tf-nightly-2.0-preview==2.0.0.dev20190516
tf-estimator-nightly==1.14.0.dev2019051601
&lt;/denchmark-code&gt;

and it works.
What is very weird is now I have 2 differents version of estimators (I recreated everything from scratch)
&lt;denchmark-code&gt;tensorflow-estimator-2-0-preview 1.14.0.dev2019051600          pypi_0    pypi
tf-estimator-nightly      1.14.0.dev2019051601          pypi_0    pypi
&lt;/denchmark-code&gt;

Is it expected ? Should I report it ?
		</comment>
		<comment id='13' author='tarrade' date='2019-05-16T14:26:09Z'>
		It's likely that you did pip install on tf-estimator-nightly, while tensorflow-2.0 did an extra tensorflow-estimator-2.0-preview under the hood. Can you pip uninstall everything, and then just pip install tf-nightly-2.0-preview only?
		</comment>
		<comment id='14' author='tarrade' date='2019-05-16T15:05:42Z'>
		&lt;denchmark-link:https://github.com/tanzhenyu&gt;@tanzhenyu&lt;/denchmark-link&gt;
  you are right. Now I got
&lt;denchmark-code&gt;tensorflow-estimator-2-0-preview 1.14.0.dev2019051600          pypi_0    pypi
tensorflow-metadata       0.13.0                   pypi_0    pypi
tf-nightly-2-0-preview    2.0.0.dev20190516          pypi_0    pypi
&lt;/denchmark-code&gt;

All good. I misinterpret what was said earlier.
		</comment>
		<comment id='15' author='tarrade' date='2019-06-19T12:04:38Z'>
		I am facing the same error when using tensorflow run time version 1.13 on AI platform.
		</comment>
		<comment id='16' author='tarrade' date='2019-06-27T22:20:27Z'>
		For anyone facing this issue, you can simply change the optimizer to be from tf.train instead. This will be a compatible optimizer with tf.keras code (worked for me on 1.13).
I found this post helpful on the subject:
&lt;denchmark-link:https://medium.com/tensorflow/multi-gpu-training-with-estimators-tf-keras-and-tf-data-ba584c3134db&gt;https://medium.com/tensorflow/multi-gpu-training-with-estimators-tf-keras-and-tf-data-ba584c3134db&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>