<bug id='33997' author='biendltb' open_date='2019-11-05T05:19:42Z' closed_time='2020-03-05T18:44:57Z'>
	<summary>[TF 2.0] 'Unknown graph' error when using tf.function decorator with pre-trained models</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.0.0-rc0
Python version: 3.6.9
CUDA/cuDNN version: 10.0
GPU model and memory: GTX 1050 Ti

Describe the current behavior
If I remove the @tf.function decorator, the error disappears. Facing this error when I created the loss function where feature extraction with the pre-trained VGG16 is a step in the pipeline.

ValueError: Unknown graph. Aborting.

Code to reproduce the issue
&lt;denchmark-code&gt;"""
Reproduce the error of keras pretrain model with tf.function wrapper in TF 2.0
"""

import tensorflow as tf
from tensorflow.keras.applications import VGG16
from tensorflow.keras.models import Model
import numpy as np


@tf.function
def extract_feat(feat_extractor, _input):
    feat = feat_extractor.predict(_input, steps=1)
    return feat


def main():
    pretrain_vgg16 = VGG16(weights='imagenet', include_top=False)
    feature_extractor = Model(inputs=pretrain_vgg16.input, outputs=pretrain_vgg16.get_layer('block3_conv3').output)

    # create an dummy input
    _input = np.random.rand(1, 224, 224, 3) - 0.5 / 0.5

    feat = extract_feat(feature_extractor, _input)
    print(feat.shape)


if __name__ == '__main__':
    main()
&lt;/denchmark-code&gt;

Other info / logs
&lt;denchmark-code&gt;2019-11-05 12:03:34.045339: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2019-11-05 12:03:34.066201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-05 12:03:34.066747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-11-05 12:03:34.066896: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-11-05 12:03:34.067713: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-11-05 12:03:34.068423: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-11-05 12:03:34.068599: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-11-05 12:03:34.069586: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-11-05 12:03:34.070296: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-11-05 12:03:34.072541: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-11-05 12:03:34.072620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-05 12:03:34.073192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-05 12:03:34.073714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-11-05 12:03:34.073892: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-11-05 12:03:34.101732: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
2019-11-05 12:03:34.102528: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557c6a90fd40 executing computations on platform Host. Devices:
2019-11-05 12:03:34.102541: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2019-11-05 12:03:34.167039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-05 12:03:34.167648: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557c6a942ac0 executing computations on platform CUDA. Devices:
2019-11-05 12:03:34.167662: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1050 Ti, Compute Capability 6.1
2019-11-05 12:03:34.167802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-05 12:03:34.168336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2019-11-05 12:03:34.168363: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-11-05 12:03:34.168374: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-11-05 12:03:34.168384: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-11-05 12:03:34.168393: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-11-05 12:03:34.168403: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-11-05 12:03:34.168412: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-11-05 12:03:34.168421: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-11-05 12:03:34.168455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-05 12:03:34.168990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-05 12:03:34.169514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-11-05 12:03:34.169538: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-11-05 12:03:34.170424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-11-05 12:03:34.170433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2019-11-05 12:03:34.170437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2019-11-05 12:03:34.170550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-05 12:03:34.171093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-05 12:03:34.171631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2787 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
Traceback (most recent call last):
  File "/home/biendltb/Projects/derain_gan/tools/error_reproduce.py", line 29, in &lt;module&gt;
    main()
  File "/home/biendltb/Projects/derain_gan/tools/error_reproduce.py", line 24, in main
    feat = extract_feat(feature_extractor, _input)
  File "/home/biendltb/anaconda3/envs/dynim/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", line 427, in __call__
    self._initialize(args, kwds, add_initializers_to=initializer_map)
  File "/home/biendltb/anaconda3/envs/dynim/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", line 370, in _initialize
    *args, **kwds))
  File "/home/biendltb/anaconda3/envs/dynim/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 1847, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File "/home/biendltb/anaconda3/envs/dynim/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 2147, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File "/home/biendltb/anaconda3/envs/dynim/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 2038, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File "/home/biendltb/anaconda3/envs/dynim/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py", line 915, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "/home/biendltb/anaconda3/envs/dynim/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", line 320, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File "/home/biendltb/anaconda3/envs/dynim/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py", line 905, in wrapper
    raise e.ag_error_metadata.to_exception(e)
ValueError: in converted code:
    relative to /home/biendltb:

    Projects/derain_gan/tools/error_reproduce.py:13 extract_feat  *
        feat = feat_extractor.predict(_input, steps=1)
    anaconda3/envs/dynim/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py:915 predict
        use_multiprocessing=use_multiprocessing)
    anaconda3/envs/dynim/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_arrays.py:722 predict
        callbacks=callbacks)
    anaconda3/envs/dynim/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_arrays.py:189 model_iteration
        f = _make_execution_function(model, mode)
    anaconda3/envs/dynim/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_arrays.py:565 _make_execution_function
        return model._make_execution_function(mode)
    anaconda3/envs/dynim/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py:2155 _make_execution_function
        self._make_predict_function()
    anaconda3/envs/dynim/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py:2145 _make_predict_function
        **kwargs)
    anaconda3/envs/dynim/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py:3658 function
        return EagerExecutionFunction(inputs, outputs, updates=updates, name=name)
    anaconda3/envs/dynim/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py:3542 __init__
        raise ValueError('Unknown graph. Aborting.')

    ValueError: Unknown graph. Aborting.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='biendltb' date='2019-11-05T06:16:00Z'>
		A temporary solution was suggested here: &lt;denchmark-link:https://stackoverflow.com/questions/56615565/evaluating-tf-model-inside-a-tf-op-throws-error/58705303#58705303&gt;https://stackoverflow.com/questions/56615565/evaluating-tf-model-inside-a-tf-op-throws-error/58705303#58705303&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='biendltb' date='2019-11-06T06:41:26Z'>
		I am able to reproduce the issue with Tf 2.0.
Please see the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/c63657b9aa1f9133b430742a689e58fd/untitled236.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks!
		</comment>
		<comment id='3' author='biendltb' date='2019-11-12T21:44:05Z'>
		I think the temporary solution makes sense, but &lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;
 would know for sure.
		</comment>
		<comment id='4' author='biendltb' date='2020-02-14T13:05:19Z'>
		It does not solve the issue, but I found a workaround to this problem. It worked with the @tf.function decorator applying all the layers one by one, replacing the extract_feat method by:
@tf.function
def extract_feat(feat_extractor, _input):
  feat = _input
  for layer in feat_extractor.layers:
    feat = layer(feat)
  return feat
		</comment>
		<comment id='5' author='biendltb' date='2020-02-14T13:44:40Z'>
		It looks like Model.predict is not compatible with tf.function. Note that the error is different in tf-nightly, and I confirmed that it's not caused by autograph by modifying the gist above:
&lt;denchmark-code&gt;@tf.function(autograph=False)
def extract_feat(feat_extractor, _input):
    feat = feat_extractor.predict(_input, steps=1)
    return feat
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
&lt;ipython-input-1-2656cf5dcf1a&gt; in &lt;module&gt;()
     23 
     24 if __name__ == '__main__':
---&gt; 25     main()

16 frames
&lt;ipython-input-1-2656cf5dcf1a&gt; in main()
     18     _input = np.random.rand(1, 224, 224, 3) - 0.5 / 0.5
     19 
---&gt; 20     feat = extract_feat(feature_extractor, _input)
     21     print(feat.shape)
     22 

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)
    574         xla_context.Exit()
    575     else:
--&gt; 576       result = self._call(*args, **kwds)
    577 
    578     if tracing_count == self._get_tracing_count():

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)
    621       # This is the first call of __call__, so we have to initialize.
    622       initializers = []
--&gt; 623       self._initialize(args, kwds, add_initializers_to=initializers)
    624     finally:
    625       # At this point we know that the initialization is complete (or less

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    503     self._concrete_stateful_fn = (
    504         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--&gt; 505             *args, **kwds))
    506 
    507     def invalid_creator_scope(*unused_args, **unused_kwds):

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2438       args, kwargs = None, None
   2439     with self._lock:
-&gt; 2440       graph_function, _, _ = self._maybe_define_function(args, kwargs)
   2441     return graph_function
   2442 

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   2769 
   2770       self._function_cache.missed.add(call_context_key)
-&gt; 2771       graph_function = self._create_graph_function(args, kwargs)
   2772       self._function_cache.primary[cache_key] = graph_function
   2773       return graph_function, args, kwargs

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   2659             arg_names=arg_names,
   2660             override_flat_arg_shapes=override_flat_arg_shapes,
-&gt; 2661             capture_by_value=self._capture_by_value),
   2662         self._function_attributes,
   2663         # Tell the ConcreteFunction to clean up its graph once it goes out of

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    979         _, original_func = tf_decorator.unwrap(python_func)
    980 
--&gt; 981       func_outputs = python_func(*func_args, **func_kwargs)
    982 
    983       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    438         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    439         # the function a weak reference to itself to avoid a reference cycle.
--&gt; 440         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    441     weak_wrapped_fn = weakref.ref(wrapped_fn)
    442 

&lt;ipython-input-1-2656cf5dcf1a&gt; in extract_feat(feat_extractor, _input)
      7 @tf.function(autograph=False)
      8 def extract_feat(feat_extractor, _input):
----&gt; 9     feat = feat_extractor.predict(_input, steps=1)
     10     return feat
     11 

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in predict(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)
    917         max_queue_size=max_queue_size,
    918         workers=workers,
--&gt; 919         use_multiprocessing=use_multiprocessing)
    920 
    921   def reset_metrics(self):

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py in predict(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)
    494         model, ModeKeys.PREDICT, x=x, batch_size=batch_size, verbose=verbose,
    495         steps=steps, callbacks=callbacks, max_queue_size=max_queue_size,
--&gt; 496         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)
    497 
    498 

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py in _model_iteration(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)
    471               mode=mode,
    472               training_context=training_context,
--&gt; 473               total_epochs=1)
    474           cbks.make_logs(model, epoch_logs, result, mode)
    475 

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)
    166       else:
    167         batch_outs = training_v2_utils._aggregate_predict_results(
--&gt; 168             strategy, batch_outs, model)
    169 
    170       if step == 0:

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in _aggregate_predict_results(strategy, batch_outs, model)
    262     nested_outs = batch_outs[i * num_replicas:i * num_replicas + num_replicas]
    263     per_output_result = dist_utils.concat_along_batch_dimension(
--&gt; 264         nest.flatten(nested_outs))
    265 
    266     if need_batch_index_gather:

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/distribute/distributed_training_utils.py in concat_along_batch_dimension(outputs)
   1200   if isinstance(outputs[0], ragged_tensor.RaggedTensor):
   1201     return ragged_concat_ops.concat(outputs, axis=0)
-&gt; 1202   return np.concatenate(outputs)

&lt;__array_function__ internals&gt; in concatenate(*args, **kwargs)

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in __array__(self)
    748   def __array__(self):
    749     raise NotImplementedError("Cannot convert a symbolic Tensor ({}) to a numpy"
--&gt; 750                               " array.".format(self.name))
    751 
    752   def __len__(self):

NotImplementedError: Cannot convert a symbolic Tensor (StatefulPartitionedCall:0) to a numpy array.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='biendltb' date='2020-03-05T18:44:57Z'>
		As shown in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/33997#issuecomment-586279455&gt;this comment&lt;/denchmark-link&gt;
, calling the model/layer's call method can work under a tf.function, but we do not expect calling the model's predict method inside a function to work necessarily. Inside of predict, we wrap the calling of the model itself in a function, but predict takes care of some higher-level processing that doesn't guarantee operation inside of functions.
		</comment>
		<comment id='7' author='biendltb' date='2020-03-05T18:45:00Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33997&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33997&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>