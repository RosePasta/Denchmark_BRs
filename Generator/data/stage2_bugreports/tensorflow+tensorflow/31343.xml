<bug id='31343' author='IanQS' open_date='2019-08-05T21:11:41Z' closed_time='2019-08-06T22:22:03Z'>
	<summary>Behavior of tf.data.Dataset when `steps_per_epoch` is set</summary>
	<description>
Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.
The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: &lt;denchmark-link:https://www.tensorflow.org/community/contribute/docs&gt;https://www.tensorflow.org/community/contribute/docs&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;URL(s) with the issue:&lt;/denchmark-h&gt;

Please provide a link to the documentation entry, for example:
&lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle&gt;tf.data.dataset&lt;/denchmark-link&gt;

&lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit&gt;model.fit&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;Description of issue (what needs changing):&lt;/denchmark-h&gt;

My tf.data.Dataset does not have repeat set which means it should go forever. At the end of steps_per_epoch, does the tf.data.Dataset shuffle itself? Or does it pick up from where it left off? Or does it reset?
I couldn't find a clear explanation online from the googling I did. My dataset is about 14 million examples, and the loss seems to be decreasing between epochs (with steps_per_epoch set). I'm just worried that it's fitting on the same X samples again and again
It's not entirely clear to me what is happening in the background with fit
	</description>
	<comments>
		<comment id='1' author='IanQS' date='2019-08-06T08:03:49Z'>
		Hi,
In this case, the dataset will indeed be consumed sequentially from epoch to epoch without going back through it. You can notably see in the fit method's docstring that the shuffle argument (triggering shuffling of the data between epochs) is inactive when steps_per_epoch is set to something else than None.
A simple way to see it is to set a fake model and data with deterministic loss outputs and see whether you get the expected loss values ; e.g.:
import numpy as np
import tensorflow as tf

# Set up a pseudo model with no trainable weights.
inputs = tf.keras.Input(tuple(), dtype=tf.float32)
model = tf.keras.Model(inputs, inputs + 1)
model.compile('adam', 'mse')  # the optimizer will actually not fit anything

# Set up a mock dataset.
data = tf.data.Dataset.from_tensor_slices(
    (tf.range(50, dtype=tf.float32), tf.zeros(50, dtype=tf.float32))
)

# "Fit" the model - basically, compute the mean sum of squared batched inputs.
model.fit(data.batch(1), epochs=5, steps_per_epoch=10)
# You should actually get a warning about the dataset not being shuffled.
# "Losses" at each epoch are 38.5, 248.5, 658.5, 1268.5 and 2078.5

# Compute the expected losses, see that they are the same.
[np.square(np.arange(i, i + 10) + 1).mean() for i in range(0, 50, 5)]
		</comment>
		<comment id='2' author='IanQS' date='2019-08-06T22:22:03Z'>
		Thank you very much!
		</comment>
		<comment id='3' author='IanQS' date='2019-08-07T07:42:11Z'>
		You are welcome :-)
		</comment>
	</comments>
</bug>