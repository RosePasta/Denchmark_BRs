<bug id='42882' author='xiaoyaozhuzi' open_date='2020-09-02T07:36:50Z' closed_time='2020-09-25T20:50:25Z'>
	<summary>tensorflow lite div quantized problem</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04):macOS 10.14.4
TensorFlow installed from (source or binary):binary(pip install tensorflow==2.3.0)
TensorFlow version (or github SHA if from source):2.3.0

I want to quantify div operations, but I couldn't get a model with quant div Op.
The test code is as follows:
#############################################
import tensorflow.compat.v1 as tf
import numpy as np
tf.disable_eager_execution()
data1 = np.random.rand(3).astype(np.float32)
beginInput1 = tf.placeholder(tf.float32,shape=[3])
data2 = np.random.rand(3).astype(np.float32)
beginInput2 = tf.placeholder(tf.float32,shape=[3])
y = tf.constant([4.0, 6.0, 2.0])
result = tf.div(beginInput1,y)
result = tf.quantization.fake_quant_with_min_max_args(result, 1, 2,name="quant")
init_op = tf.initialize_all_variables()
def representative_data_gen():
yield [data1]
with tf.Session() as sess:
sess.run(init_op)
end = sess.run(result,feed_dict={beginInput1:data1,beginInput2:data2})
&lt;denchmark-code&gt;output_graph_def = tf.graph_util.convert_variables_to_constants(sess,
                                                                sess.graph_def,
                                                                output_node_names=["quant"])

with tf.gfile.FastGFile("../model/model.pb", mode="wb") as f:
    f.write(output_graph_def.SerializeToString())

converter = tf.lite.TFLiteConverter.from_frozen_graph("../model/model.pb",
                                                    input_arrays=["Placeholder"],
                                                    output_arrays=["quant"],
                                                    input_shapes={
                                                        "Placeholder": [3],
                                                    })

converter.inference_type = tf.lite.constants.QUANTIZED_UINT8
input_arrays = converter.get_input_arrays()
converter.quantized_input_stats = {input_arrays[0]: (0., 1.)}

print("converter begin.")
tflite_model = converter.convert()
open("../model/model.tflite", "wb").write(tflite_model)
&lt;/denchmark-code&gt;

#############################################

&lt;denchmark-link:https://user-images.githubusercontent.com/16719562/91945888-b095a900-ed31-11ea-9bae-80046ab5422e.png&gt;&lt;/denchmark-link&gt;

You can see that tensorflow lite has supported quant div op.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r2.3/tensorflow/lite/toco/graph_transformations/quantize.cc#L56&gt;https://github.com/tensorflow/tensorflow/blob/r2.3/tensorflow/lite/toco/graph_transformations/quantize.cc#L56&lt;/denchmark-link&gt;

How to convert a model containing quant div op?
	</description>
	<comments>
		<comment id='1' author='xiaoyaozhuzi' date='2020-09-02T20:18:06Z'>
		&lt;denchmark-link:https://github.com/xiaoyaozhuzi&gt;@xiaoyaozhuzi&lt;/denchmark-link&gt;
 could you try conversion with the following code?
&lt;denchmark-code&gt;   converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.target_spec.supported_ops = [
        tf.lite.OpsSet.TFLITE_BUILTINS_INT8
    ]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='xiaoyaozhuzi' date='2020-09-03T03:26:22Z'>
		
@xiaoyaozhuzi could you try conversion with the following code?
   converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.target_spec.supported_ops = [
        tf.lite.OpsSet.TFLITE_BUILTINS_INT8
    ]


Python 3.7.3 (default, Mar 27 2019, 16:54:48)
[Clang 4.0.1 (tags/RELEASE_401/final)] on darwin
Traceback (most recent call last):
File "/Applications/PyCharm.app/Contents/helpers/pydev/pydev_run_in_console.py", line 52, in run_file
pydev_imports.execfile(file, globals, locals)  # execute the script
File "/Applications/PyCharm.app/Contents/helpers/pydev/_pydev_imps/_pydev_execfile.py", line 18, in execfile
exec(compile(contents+"\n", file, 'exec'), glob, loc)
File "/Users/zhangshuo/githouse/toco-test/tensorflow-test/div_test.py", line 51, in 
tflite_model = converter.convert()
File "/anaconda2/envs/python3/lib/python3.7/site-packages/tensorflow/lite/python/lite.py", line 1970, in convert
return super(TFLiteConverter, self).convert()
File "/anaconda2/envs/python3/lib/python3.7/site-packages/tensorflow/lite/python/lite.py", line 1339, in convert
result = self._calibrate_quantize_model(result, **flags)
File "/anaconda2/envs/python3/lib/python3.7/site-packages/tensorflow/lite/python/lite.py", line 452, in _calibrate_quantize_model
inference_output_type, allow_float, activations_type)
File "/anaconda2/envs/python3/lib/python3.7/site-packages/tensorflow/lite/python/optimize/calibrator.py", line 98, in calibrate_and_quantize
np.dtype(activations_type.as_numpy_dtype()).num)
RuntimeError: Quantization not yet supported for op:
Shows that there are unsupported Quantization ops.
		</comment>
		<comment id='3' author='xiaoyaozhuzi' date='2020-09-03T05:47:09Z'>
		Could you try the original conversion code with the tf-nightly version?
I can create a model with one quantized multiply op node with the same code at the HEAD. FYI, the multiply op is used for the division calculation.
		</comment>
		<comment id='4' author='xiaoyaozhuzi' date='2020-09-03T05:56:40Z'>
		
Could you try the original conversion code with the tf-nightly version?
I can create a model with one quantized multiply op node with the same code at the HEAD. FYI, the multiply op is used for the division calculation.

2.3.0 can also generate a model with only one multiplication operation, but division doesn't work.
		</comment>
		<comment id='5' author='xiaoyaozhuzi' date='2020-09-03T06:04:36Z'>
		Could you elaborate more on your expectations on the division calculation from the generated model? It looks good to me.
		</comment>
		<comment id='6' author='xiaoyaozhuzi' date='2020-09-03T06:15:41Z'>
		
Could you elaborate more on your expectations on the division calculation from the generated model? It looks good to me.

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r2.3/tensorflow/lite/kernels/div.cc#L155&gt;https://github.com/tensorflow/tensorflow/blob/r2.3/tensorflow/lite/kernels/div.cc#L155&lt;/denchmark-link&gt;

I want to verify the accuracy loss of quant div op, so I want to create a tflite model with only one quant div operation.
		</comment>
		<comment id='7' author='xiaoyaozhuzi' date='2020-09-03T08:34:53Z'>
		&lt;denchmark-link:https://github.com/jianlijianli&gt;@jianlijianli&lt;/denchmark-link&gt;
 Could you help create a model with single quantized div operator?
		</comment>
		<comment id='8' author='xiaoyaozhuzi' date='2020-09-04T08:54:23Z'>
		
@jianlijianli Could you help create a model with single quantized div operator?

What else can I try?
		</comment>
		<comment id='9' author='xiaoyaozhuzi' date='2020-09-10T11:15:01Z'>
		&lt;denchmark-link:https://github.com/xiaoyaozhuzi&gt;@xiaoyaozhuzi&lt;/denchmark-link&gt;

Please update, does creating single quantized div operator help.
		</comment>
		<comment id='10' author='xiaoyaozhuzi' date='2020-09-17T11:41:09Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='11' author='xiaoyaozhuzi' date='2020-09-25T20:50:15Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='12' author='xiaoyaozhuzi' date='2020-10-16T22:28:21Z'>
		Can anyone confirm that Quantized Div is not supported as stated by &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/21526&gt;#21526&lt;/denchmark-link&gt;
?
		</comment>
	</comments>
</bug>