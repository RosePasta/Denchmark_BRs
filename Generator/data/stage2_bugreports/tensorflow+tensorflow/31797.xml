<bug id='31797' author='thomasZen' open_date='2019-08-20T13:43:57Z' closed_time='2020-01-03T17:03:17Z'>
	<summary>Deep Learning Image: TensorFlow 1.14.0 m33 on Google Cloud produces wrong and non deterministic loss after backpropagation</summary>
	<description>
System information

Have I written custom code: Yes, the code is attached.
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux thomas-tf-14 4.9.0-9-amd64 #1 SMP Debian 4.9.168-1+deb9u4 (2019-07-19) x86_64 GNU/Linux
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Tested on google cloud
TensorFlow installed from (source or binary): Binary: Deep Learning Image: TensorFlow 1.14.0 m33 on Google Cloud
TensorFlow version: v1.14.0-0-g87989f6 1.14.0
Python version: 2.7.13 / 3.5.3
Bazel version (if compiling from source): Not compiled from source
GCC/Compiler version (if compiling from source): Not compiled from source
CUDA/cuDNN version: Not used
GPU model and memory: None

Describe the current behavior
Current behaviour on Google Cloud using Deep Learning Image: TensorFlow 1.14.0 m33 is non deterministic. During multiple runs the loss after performing backpropagation and updating a variable is different for different runs and does not match the loss of the non optimized standard tensorflow installation. This behaviour exists when using python2 and when using python3.
Created instance with:
gcloud compute instances create "tf-1-14-cpu" --zone="us-west1-b" --image-family="tf-1-14-cpu" --image-project=deeplearning-platform-release
Run 1:
&lt;denchmark-code&gt;Loss during step 0: -0.41999998688697815
Loss during step 1: -3.698721931466375e+19
Loss during step 2: -7.38836981337104e+19
&lt;/denchmark-code&gt;

Run 2:
&lt;denchmark-code&gt;Loss during step 0: -0.41999998688697815
Loss during step 1: -0.41999998688697815
Loss during step 2: -0.41999998688697815
&lt;/denchmark-code&gt;

Run 3:
&lt;denchmark-code&gt;Loss during step 0: -0.41999998688697815
Loss during step 1: 9.46872814455392e+21
Loss during step 2: 1.8914226722229864e+22
&lt;/denchmark-code&gt;

Describe the expected behavior
On the the same machine using a virtualenv to force the use of non optimized tensorflow as follows:
&lt;denchmark-code&gt;virtualenv -p python3 test
source test/bin/activate
pip3 install tensorflow==1.14.0
&lt;/denchmark-code&gt;

Run 1:
&lt;denchmark-code&gt;Loss during step 0: -0.41999998688697815
Loss during step 1: -1.4199999570846558
Loss during step 2: -2.4200000762939453
&lt;/denchmark-code&gt;

Run 2:
&lt;denchmark-code&gt;Loss during step 0: -0.41999998688697815
Loss during step 1: -1.4199999570846558
Loss during step 2: -2.4200000762939453
&lt;/denchmark-code&gt;

Run 3:
&lt;denchmark-code&gt;Loss during step 0: -0.41999998688697815
Loss during step 1: -1.4199999570846558
Loss during step 2: -2.4200000762939453
&lt;/denchmark-code&gt;

Code to reproduce the issue
&lt;denchmark-code&gt;import tensorflow as tf


if __name__ == "__main__":
    session = tf.Session()
    image = tf.get_variable(name="image", shape=[1, 1, 1, 1], initializer=tf.constant_initializer(0.42))
    session.run(tf.global_variables_initializer())
    kernel = tf.constant(1.0, shape=[1, 1, 1, 1], dtype=tf.float32)

    conv_out = tf.nn.conv2d(image, kernel, strides=[1, 1, 1, 1], padding="SAME")
    max_conv_out = tf.math.reduce_max(conv_out, axis=2)
    loss = -tf.reduce_sum(max_conv_out)
    opt = tf.train.GradientDescentOptimizer(learning_rate=1.0, name="sgd")
    optimizer = opt.minimize(loss, var_list=[image], name="sgd_minimize")

    for step in range(3):
        loss_value, _ = session.run([loss, optimizer])
        print("Loss during step {}: {}".format(step, loss_value))
&lt;/denchmark-code&gt;


&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3520927/python-code.txt&gt;python-code.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/3520903/tf_env.txt&gt;tf_env.txt&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='thomasZen' date='2019-08-23T23:16:44Z'>
		I tried your code snippet in Google Colab using TF 1.14 and produced same expected results on 3 consecutive runs.
Loss during step 0: -0.41999998688697815
Loss during step 1: -1.4199999570846558
Loss during step 2: -2.4200000762939453
Perhaps this is an issue with GCP instance and not with TF 1.14 version specifically.
		</comment>
		<comment id='2' author='thomasZen' date='2019-08-25T16:43:49Z'>
		&lt;denchmark-link:https://github.com/ymodak&gt;@ymodak&lt;/denchmark-link&gt;
 thanks for running the code in Google Colab. Given these results I assume it used the standard (non mkl optimized) version of TF 1.14.
Do you have the possibility to test the code with the mkl optimized version of TF 1.14 and run it on a CPU?
		</comment>
		<comment id='3' author='thomasZen' date='2019-08-26T18:27:47Z'>
		That's correct I used non optimized TF version. I will add TensorFlow MKL group to get more information. Thanks!
		</comment>
		<comment id='4' author='thomasZen' date='2019-08-26T20:46:48Z'>
		We are able to reproduce your issue on  1.14. We'll keep you posted on the fix
		</comment>
		<comment id='5' author='thomasZen' date='2019-09-09T09:31:47Z'>
		&lt;denchmark-link:https://github.com/preethivenkatesh&gt;@preethivenkatesh&lt;/denchmark-link&gt;
 Thanks for checking this. Was it already possible to track down the reason for the non deterministic outputs and in which situations these appear?
		</comment>
		<comment id='6' author='thomasZen' date='2019-11-11T14:04:27Z'>
		This bug is also present in tensorflow 1.15 with mkl (Deep Learning Image: TensorFlow 1.15.0 m38).
		</comment>
		<comment id='7' author='thomasZen' date='2020-01-03T17:01:18Z'>
		The issue is resolved and merged into the master branch &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/35201&gt;#35201&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='thomasZen' date='2020-01-03T17:03:19Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31797&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31797&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='thomasZen' date='2020-03-11T08:37:46Z'>
		The non deterministic and incorrect loss for the example above resurfaced in tensorflow 1.15. This happens both when installing the python3 library directly and when using docker:

pip3 install https://storage.googleapis.com/intel-optimized-tensorflow/intel_tensorflow-1.15.2-cp37-cp37m-manylinux2010_x86_64.whl
docker pull "gcr.io/deeplearning-platform-release/tf-cpu.1-15"

(CC: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/35201&gt;#35201&lt;/denchmark-link&gt;
)
		</comment>
		<comment id='10' author='thomasZen' date='2020-03-25T20:41:41Z'>
		&lt;denchmark-link:https://github.com/thomasZen&gt;@thomasZen&lt;/denchmark-link&gt;
 It looks like we have had perf regression on a few models,  and the commit has been reverted. While we try to resolve this w/o impacting our other critical accelerations, are you okay to use this as a patch for your build?
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/pull/35619&gt;#35619&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>