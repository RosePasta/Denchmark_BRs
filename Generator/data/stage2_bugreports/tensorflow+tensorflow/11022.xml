<bug id='11022' author='amelius15' open_date='2017-06-23T22:35:57Z' closed_time='2018-01-05T19:22:25Z'>
	<summary>Major performance hit when running two processes on same GPU.</summary>
	<description>
I've observed a major performance hit when running two identical models on the same GPU, with allow_growth set to True, so that each process, which only needs a small fraction of the GPU memory (~1gb/11gb are used when allow_growth is set to true). When running a single model on a single process, it consistently finished going through the data I have available in approximately 170-174 seconds, and never exceeds 50% Volatile GPU-Utilization according to nvidia-smi . However, when running with two separate, concurrent processes, (each identical to the first), they both finish in approximately 320-340 seconds, which is unexpected, since GPU utilization was not even half in the first scenario, and running two concurrently effectively slows them down to running them sequentially, despite the seemingly available processing power and memory.
Is this intentional, or is there a better way to do this? (Currently launching two workers via Celery, each of which create their own TF session and load the model into it separately, and run the data through it). I would love to make maximal use of available hardware, and this seems like a very counter-intuitive outcome.
I can observe each process allocate roughly 1GiB GPU memory, and each adds approximately 45-50% GPU utilization. For testing purposes, data and model used in both parallel runs is completely identical.
Any thoughts? Am I misusing TF?
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


Using Keras to load model
Ubuntu 16.04
CUDA/CUDNN 8.0/5.1
TF version 1.0.1
GTX 1080 ti (not being used to render screen, second 1080 ti is doing that)
Running in nvidia-docker with a single GPU available to worker process (the one not rendering the screen).
Running two separate sessions initiated via Celery which both create their own session, set allow_growth=True, each load the model into memory separately, and each run with data separately.

	</description>
	<comments>
		<comment id='1' author='amelius15' date='2017-06-26T21:43:04Z'>
		Without code and model and command it's hard to tell anything about it. Maybe in your code, just copying your data from CPU to GPU is already taking a lot of time, in which case adding processes wouldn't help at all.
		</comment>
		<comment id='2' author='amelius15' date='2017-06-26T22:30:00Z'>
		Please include a minimum reproducer and make sure you include the exact command if possible to produce the output included in your case. If you are unclear what to include see the issue template displayed in the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/new&gt;Github new issue template&lt;/denchmark-link&gt;
. Thanks!
Also, did you try this on the latest tensorflow version (r1.2)?
		</comment>
		<comment id='3' author='amelius15' date='2017-12-20T19:07:24Z'>
		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
		</comment>
		<comment id='4' author='amelius15' date='2018-01-04T19:20:12Z'>
		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
		</comment>
		<comment id='5' author='amelius15' date='2018-01-05T19:22:25Z'>
		Note that the GPU utilization shown by nvidia-smi might be inaccurate as during the model running, there might be dynamic device memory being allocated so the peak utilization might be larger. Also, like &lt;denchmark-link:https://github.com/ppwwyyxx&gt;@ppwwyyxx&lt;/denchmark-link&gt;
 mentioned, several other issues might affect the performance.
I will close this issue due to the inactive status. Please feel free to reopen this if you have more insights/code to share. I'd be happy to take a look once more detailed information is provided.
		</comment>
	</comments>
</bug>