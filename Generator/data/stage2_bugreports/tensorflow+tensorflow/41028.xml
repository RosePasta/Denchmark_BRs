<bug id='41028' author='tvatsal1996' open_date='2020-07-02T12:18:42Z' closed_time='2020-08-11T21:55:00Z'>
	<summary>LSTM behaving differently with recurrent activation "sigmoid" and tf.keras.activations.sigmoid</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 3.10
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): pip install tensorflow
TensorFlow version (use command below): 2.1.0
Python version: 3.6.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 10.1/7.6.1
GPU model and memory: Tesla V100-SXM2-32GB

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with:

TF 1.0: python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
TF 2.0: python -c "import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)"

Describe the current behavior
Using tf.keras.layers.LSTM(units, recurrent_activation=tf.keras.activations.sigmoid) - My training is not converging
Using tf.keras.layers.LSTM(units, recurrent_activation='sigmoid') - Same training is converging
Also the time taken in former is higher than the latter. This I have found that in recurrent_v2.py, it is checking if recurrent_activation == 'sigmoid' which fails in case of tf.keras.activations.sigmoid, so the former one is not using CuDNN while latter one is using. What is different in those two cases is that the latter one is converging using CuDNN while the former one is not converging at all not using CuDNN.
Describe the expected behavior
Training should converge for both of the above however training time variation is understood as former is not using CuDNN but the latter one is using.
Standalone code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
No error is coming just training is acting differently
Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='tvatsal1996' date='2020-07-03T10:59:42Z'>
		&lt;denchmark-link:https://github.com/tvatsal1996&gt;@tvatsal1996&lt;/denchmark-link&gt;
,
In order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!
		</comment>
		<comment id='2' author='tvatsal1996' date='2020-07-03T12:41:02Z'>
		Here is a sample code for my issue on the specified system details:
&lt;denchmark-code&gt;import tensorflow as tf

class TranNetwork(tf.keras.Model):
    """Transcription Network
    """
    def __init__(self, num_lstm_layers, lstm_cell_size, dropout=0.0):
        super(TranNetwork, self).__init__()

        self.num_lstm_layers = num_lstm_layers
        self.lstm_cell_size = lstm_cell_size
        self.dropout = dropout
        self.trans_layers = []
        self.pooling = tf.keras.layers.MaxPool1D(pool_size=3, padding="same")

        # HERE IS THE ISSUE, USING RECURRENT ACTIVATION AS tf.keras.activations.sigmoid instead of 'sigmoid'
        # causes model to not converge at all
        recurrent_activation = 'sigmoid'
        #recurrent_activation = tf.keras.activations.sigmoid
        for l in range(num_lstm_layers):
          self.trans_layers.append(tf.keras.layers.LSTM(
                                     self.lstm_cell_size,
                                     return_sequences=True,
                                     dropout=self.dropout,
                                     recurrent_activation=recurrent_activation))

    def call(self, x, x_len, training=True):
        seq_len = x_len
        output = tf.clip_by_value(x, -3.0, 3.0)
        for l in range(self.num_lstm_layers):
            mask = tf.sequence_mask(seq_len)
            output = self.trans_layers[l](output, mask=mask, training=training)
            if l == 0:
              seq_len = tf.cast(tf.math.ceil(tf.divide(seq_len, 3)), dtype=tf.int32)
              output = self.pooling(output, training=training)

        mask = tf.sequence_mask(seq_len)
        output = self.trans_layers[-1](output, mask=mask, training=training)

        return output, seq_len

class RNNTModel(object):
    """ RNNT Model class for training
    """
    def __init__(self, num_lstm_layers, lstm_cell_size, vocab_size, dropout=0.0):
        self.vocab_size = vocab_size
        self.trans = TranNetwork(num_lstm_layers, lstm_cell_size, dropout=dropout)
        self.ctc_layer = tf.keras.layers.Dense(1+self.vocab_size, name='ctc')

    def forward(self, x, y, x_len, y_len, training=True):
        @tf.function(input_signature=[
            tf.TensorSpec(shape=[None, None, 40], dtype=tf.float32),
            tf.TensorSpec(shape=[None, None], dtype=tf.int32),
            tf.TensorSpec(shape=[None, ], dtype=tf.int32),
            tf.TensorSpec(shape=[None, ], dtype=tf.int32),
            tf.TensorSpec(shape=[], dtype=tf.bool)])
        def _forward(x, y, x_len, y_len, training=True):
            trans_output, output_len = self.trans(x, x_len, training=training)
            ctc_out = self.ctc_layer(trans_output, training=training)
            output_d = {'ctc_out': ctc_out, 'output_len': output_len}
            return output_d
        return _forward(x, y, x_len, y_len, training=training)

    def loss(self, y, x_len, y_len, ctc_out):
        @tf.function(input_signature=[
            tf.TensorSpec(shape=[None, None], dtype=tf.int32),
            tf.TensorSpec(shape=[None, ], dtype=tf.int32),
            tf.TensorSpec(shape=[None, ], dtype=tf.int32),
            tf.TensorSpec(shape=[None, None, 1+self.vocab_size], dtype=tf.float32)])
        def _loss(y, x_len, y_len, ctc_out):
            ctc_loss = tf.nn.ctc_loss(y, ctc_out, y_len, x_len,
                                      logits_time_major=False,
                                      blank_index=self.vocab_size)
            # to ignore invalid ctc loss case
            mask = tf.dtypes.cast(
              tf.math.greater_equal(x_len, y_len), dtype=tf.float32)
            ctc_loss = tf.multiply(ctc_loss, mask)
            ctc_loss = tf.reduce_sum(ctc_loss)
            return ctc_loss
        return _loss(y, x_len, y_len, ctc_out)

    @property
    def trainable_variables(self):
        return self.trans.trainable_variables \
               + self.ctc_layer.trainable_variables

model = RNNTModel(2, 32, 29, dropout=0.0)
optimizer = tf.keras.optimizers.Adam(0.00025)

@tf.function(input_signature=[
    tf.TensorSpec(shape=[None, None, 40], dtype=tf.float32),
    tf.TensorSpec(shape=[None, None], dtype=tf.int32),
    tf.TensorSpec(shape=[None, ], dtype=tf.int32),
    tf.TensorSpec(shape=[None, ], dtype=tf.int32)])
def train_step(x, y, x_len, y_len):
    with tf.GradientTape() as tape:
        output_d = model.forward(x, y, x_len, y_len, training=True)
        ctc_loss = model.loss(y, output_d['output_len'],
                              y_len, output_d['ctc_out'])
    variables = model.trainable_variables
    gradients = tape.gradient(ctc_loss, variables)
    gradients, _ = tf.clip_by_global_norm(gradients, 1.0)
    optimizer.apply_gradients(zip(gradients, variables))
    loss_norm_factor = 1.0 / tf.cast(tf.reduce_sum(y_len), dtype=tf.float32)
    return ctc_loss * loss_norm_factor
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='tvatsal1996' date='2020-07-07T18:27:29Z'>
		&lt;denchmark-link:https://github.com/tvatsal1996&gt;@tvatsal1996&lt;/denchmark-link&gt;
,
I did not face any error while running the given code with TF v2.2. Please find the gist of it &lt;denchmark-link:https://colab.research.google.com/gist/amahendrakar/e8109f044f1593a27bd1189197a112c4/41028.ipynb&gt;here&lt;/denchmark-link&gt;
.
Looks like the code doesn't reach the recurrent_activation = tf.keras.activations.sigmoid line. Could you please provide a reproducible example. Thanks!
		</comment>
		<comment id='4' author='tvatsal1996' date='2020-07-08T02:03:25Z'>
		
@tvatsal1996,
I did not face any error while running the given code with TF v2.2. Please find the gist of it here.
Looks like the code doesn't reach the recurrent_activation = tf.keras.activations.sigmoid line. Could you please provide a reproducible example. Thanks!

Hi, it will not throw out any error. I was asking that these two models which are very much the same except recurrent activation is "sigmoid" in the first one and tf.keras.activations.sigmoid in the second one, but with tf.keras.activations.sigmoid, my model does not converge and with "sigmoid" it converges.
Also I have found out that with "sigmoid" it is using my CUDA library while with tf.keras.activations.sigmoid it is not using CUDA library. This is the basic difference. More specifically the class LSTM in tensorflow.python.keras.layers.recurrent_v2 uses these two functions namely standard_lstm and cudnn_lstm for different modes, i.e. with CUDA or without CUDA. While using "sigmoid" it is using cudnn_lstm and while using  tf.keras.activations.sigmoid it is using standard_lstm. This is the only difference which I have figured out with the two executions and is behaving differently.
		</comment>
		<comment id='5' author='tvatsal1996' date='2020-07-26T21:25:23Z'>
		Hi, I am still stuck in this and have not got any workaround till now. Please help !!
		</comment>
		<comment id='6' author='tvatsal1996' date='2020-07-27T10:39:18Z'>
		I was digging through the implementations of GRU and LSTM when I stumbled over your post. Maybe this is an explanation for the behavior you are observing:
In the implementation of recurrent_v2.py, you can choose both recurrent_activation and activation, but this seems to have no effect when using the standard_lstm and standard_gru. Note that for cuDNN you must not change the activations.
Here is the implementation:



tensorflow/tensorflow/python/keras/layers/recurrent_v2.py


         Line 1298
      in
      8c6f2d5






 i = nn.sigmoid(z0) 





This was different in the older implementations, but recent versions seem to ignore the given activations. Or am I wrong?
		</comment>
		<comment id='7' author='tvatsal1996' date='2020-07-29T09:50:47Z'>
		Yes, recent version seems to ignore the given activations in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/b36436b087bd8e8701ef51718179037cccdfc26e/tensorflow/python/keras/layers/recurrent_v2.py#L1238&gt;standard_lstm&lt;/denchmark-link&gt;
, but my issue is that my model is not converging when run without CuDNN kernel. When being run with CuDNN kernel, i.e, when using  &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/b36436b087bd8e8701ef51718179037cccdfc26e/tensorflow/python/keras/layers/recurrent_v2.py#L1320&gt;gpu_lstm&lt;/denchmark-link&gt;
, it was converging with recurrent activation as sigmoid but while not using CuDNN kernel, i.e when using &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/b36436b087bd8e8701ef51718179037cccdfc26e/tensorflow/python/keras/layers/recurrent_v2.py#L1238&gt;standard_lstm&lt;/denchmark-link&gt;
 function, it was not converging. Is there something I am missing ?
		</comment>
		<comment id='8' author='tvatsal1996' date='2020-08-11T04:37:27Z'>
		The check for cudnn kernel compatibility was updated a while ago to fix this case. See 


tensorflow/tensorflow/python/keras/layers/recurrent_v2.py


         Line 1090
      in
      05632ed






 self.recurrent_activation in (activations.sigmoid, nn.sigmoid) and 




.
Could u try with the latest TF release?
		</comment>
		<comment id='9' author='tvatsal1996' date='2020-08-11T04:39:49Z'>
		
I was digging through the implementations of GRU and LSTM when I stumbled over your post. Maybe this is an explanation for the behavior you are observing:
In the implementation of recurrent_v2.py, you can choose both recurrent_activation and activation, but this seems to have no effect when using the standard_lstm and standard_gru. Note that for cuDNN you must not change the activations.
Here is the implementation:



tensorflow/tensorflow/python/keras/layers/recurrent_v2.py


         Line 1298
      in
      8c6f2d5






 i = nn.sigmoid(z0) 





This was different in the older implementations, but recent versions seem to ignore the given activations. Or am I wrong?

Note that for standard_lstm/gpu_lstm, its the code path that _could_use_gpu_kernel = True (which means the value of activation and recurrent_activation should be tanh and sigmoid). The code path for _could_use_gpu_kernel = False is falling back to LSTM v1 which is in recurrent.py.
		</comment>
		<comment id='10' author='tvatsal1996' date='2020-08-11T21:54:56Z'>
		Since this is already fixed in the latest nightly, I am closing this bug now.
		</comment>
		<comment id='11' author='tvatsal1996' date='2020-08-11T21:55:01Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41028&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41028&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>