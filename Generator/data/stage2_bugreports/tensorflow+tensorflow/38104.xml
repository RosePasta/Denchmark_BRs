<bug id='38104' author='sdu2011' open_date='2020-04-01T07:59:42Z' closed_time='2020-04-02T21:50:35Z'>
	<summary>post_training_quant does not change weights from fp32 to int8</summary>
	<description>
System information

Have I written custom code (as opposed to using a stock
example script provided in TensorFlow):
NO
OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04):
ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
the issue happens on mobile device:
TensorFlow installed from (source or
binary): - TensorFlow version (use command below):
conda  tensorflow version: tested on tf1.15 and 2.2.0-dev20200325.
Python version: - Bazel
version (if compiling from source):
GCC/Compiler version (if compiling from
source):
CUDA/cuDNN version: - GPU model and memory:


i follow link:&lt;denchmark-link:https://www.tensorflow.org/lite/performance/post_training_quant&gt;https://www.tensorflow.org/lite/performance/post_training_quant&lt;/denchmark-link&gt;
 ,use code in the doc,
when i use &lt;denchmark-link:https://github.com/lutzroeder/netron&gt;netron&lt;/denchmark-link&gt;
 open mnist_model_quant.tflite,found that weights/bias of layer conv2d are still float32.
&lt;denchmark-link:https://user-images.githubusercontent.com/6041743/78113170-e99fb980-7431-11ea-8e3b-8cccfec1ec57.png&gt;&lt;/denchmark-link&gt;

Describe the expected behavior
after post-training quantize, weights of conv2d should be int8 rather than float32
	</description>
	<comments>
		<comment id='1' author='sdu2011' date='2020-04-02T00:52:21Z'>
		You may want to try post training integer quantization instead.
&lt;denchmark-link:https://www.tensorflow.org/lite/performance/post_training_integer_quant#convert_to_a_tensorflow_lite_model&gt;https://www.tensorflow.org/lite/performance/post_training_integer_quant#convert_to_a_tensorflow_lite_model&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/42785357/78199748-68502300-7441-11ea-90da-f9d3a80e6ee8.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='sdu2011' date='2020-04-02T01:05:00Z'>
		&lt;denchmark-link:https://github.com/sdu2011&gt;@sdu2011&lt;/denchmark-link&gt;
 I tried the same model as in the example and find int8 for the weights of conv2d as shown below. Did you change any part of the code and did you use quantized model?
&lt;denchmark-link:https://screenshot.googleplex.com/ZptgJAm6tg3&gt;Screenshot&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='3' author='sdu2011' date='2020-04-02T02:00:28Z'>
		
You may want to try post training integer quantization instead.
https://www.tensorflow.org/lite/performance/post_training_integer_quant#convert_to_a_tensorflow_lite_model


i know if i do inter quantize, weights would be converted from fp32 to int8, i had already tested it．　my question is why post-training weight quantization does not change conv2d weights from fp32 to int8．
		</comment>
		<comment id='4' author='sdu2011' date='2020-04-02T02:03:56Z'>
		
@sdu2011 I tried the same model as in the example and find int8 for the weights of conv2d as shown below. Did you change any part of the code and did you use quantized model?
Screenshot.

&lt;denchmark-code&gt;import logging
logging.getLogger("tensorflow").setLevel(logging.DEBUG)

try:
  # %tensorflow_version only exists in Colab.
  import tensorflow.compat.v2 as tf
except Exception:
  pass
tf.enable_v2_behavior()

from tensorflow import keras
import numpy as np
import pathlib

# Load MNIST dataset
mnist = keras.datasets.mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

# Normalize the input image so that each pixel value is between 0 to 1.
train_images = train_images / 255.0
test_images = test_images / 255.0

# Define the model architecture
model = keras.Sequential([
  keras.layers.InputLayer(input_shape=(28, 28)),
  keras.layers.Reshape(target_shape=(28, 28, 1)),
  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation=tf.nn.relu),
  keras.layers.MaxPooling2D(pool_size=(2, 2)),
  keras.layers.Flatten(),
  keras.layers.Dense(10, activation=tf.nn.softmax)
])

# Train the digit classification model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
model.fit(
  train_images,
  train_labels,
  epochs=1,
  validation_data=(test_images, test_labels)
)

#　只做了格式转换
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
tflite_models_dir = pathlib.Path("./mnist_tflite_models/")
tflite_models_dir.mkdir(exist_ok=True, parents=True)
tflite_model_file = tflite_models_dir/"mnist_model.tflite"
tflite_model_file.write_bytes(tflite_model)

# 把weights转换成int8  实际测试出来只有全连接层的weights转成了int8
converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
tflite_quant_model = converter.convert()
tflite_model_quant_file = tflite_models_dir/"mnist_model_quant.tflite"
tflite_model_quant_file.write_bytes(tflite_quant_model)
&lt;/denchmark-code&gt;

no,i did not change code.
		</comment>
		<comment id='5' author='sdu2011' date='2020-04-02T21:50:07Z'>
		For accuracy reasons, we don't quantize tensors smaller than 1024.
		</comment>
		<comment id='6' author='sdu2011' date='2020-04-02T21:50:35Z'>
		I am closing please reopen, if you have issues or questions (or create new issue)
Thanks
		</comment>
		<comment id='7' author='sdu2011' date='2020-04-02T21:50:37Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38104&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38104&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='sdu2011' date='2020-06-04T15:46:31Z'>
		I'm also having the same issue, with tensors larger than 1024
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/40129&gt;#40129&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>