<bug id='25724' author='xilenteyex' open_date='2019-02-13T18:09:49Z' closed_time='2020-01-02T18:12:41Z'>
	<summary>GPU is idle even when there are operations ready to be executed</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.1 LTS
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.12.0
Python version: Python 2.7.15rc1
Bazel version (if compiling from source): 0.21
GCC/Compiler version (if compiling from source): (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
CUDA/cuDNN version: 10.0/7.4.2
GPU model and memory: NVIDIA Corporation GP100GL [Tesla P100 PCIe 12GB] (2 GPUs)

You can collect some of this information using our environment capture &lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with
python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"

I am running a toy matrix multiplication example (&lt;denchmark-link:https://gist.github.com/xilenteyex/f569b0e0984808e1fb3a284c03b99984&gt;code here &lt;/denchmark-link&gt;
and &lt;denchmark-link:https://gist.github.com/xilenteyex/c9565c20f32ea73383de96a484d6eae9&gt;timeline can be found here&lt;/denchmark-link&gt;
). If u look at the timeline, operations named  and  are placed on GPU1 while all other matrix multiplications are placed on GPU0. Operation named  takes input from operations named  and .  and  are completed almost at the same time while there is a large gap after completion of these operations before the execution of  starts. I am not sure why is this gap there ?
Describe the expected behavior
Expected behavior according to my understanding is that as soon as MEMCPYPtoP has completed (Op named MatMul is placed on GPU0), excution of MatMul_2 should be started. Is this a performance issue or am I missing something here ?

&lt;denchmark-link:https://gist.github.com/xilenteyex/f569b0e0984808e1fb3a284c03b99984&gt;toy_matmul code&lt;/denchmark-link&gt;


&lt;denchmark-link:https://gist.github.com/xilenteyex/c9565c20f32ea73383de96a484d6eae9&gt;timeline for toy_matmul&lt;/denchmark-link&gt;

Following is the snapshot of the timeline using chrome-tracing visualizer.
&lt;denchmark-link:https://user-images.githubusercontent.com/10864603/52733046-b4b37780-2f8f-11e9-9ab3-1ea51c7e2e14.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='xilenteyex' date='2019-02-18T08:49:32Z'>
		Un-assigning myself as this does not seem related to distribution strategy. Perhaps someone on the GPU team can look into it.
		</comment>
		<comment id='2' author='xilenteyex' date='2019-02-27T00:40:41Z'>
		Can it be reproduced? I tried your script on GCE and it is executed as you expected
&lt;denchmark-link:https://user-images.githubusercontent.com/1269299/53456952-391ce600-39e5-11e9-9d33-b11c35e87585.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='xilenteyex' date='2019-02-27T14:45:38Z'>
		Hi &lt;denchmark-link:https://github.com/qqfish&gt;@qqfish&lt;/denchmark-link&gt;
 ,
Thanks for looking into this.
In this execution, it looks like &lt;denchmark-link:https://gist.github.com/xilenteyex/f569b0e0984808e1fb3a284c03b99984#file-toy_matmul_cdep-py-L37&gt;this control dependency &lt;/denchmark-link&gt;
is ignored. Not sure why. Can you please explain this behavior ?
Thanks!
		</comment>
		<comment id='4' author='xilenteyex' date='2019-03-06T01:54:31Z'>
		Sorry for delay.
First for the control dependency, I think it is expected. The control dependency is created on placeholder. When you call session.run, you already fulfilled the placeholder. So the ops depends on it is ready to run as well.
To fix it, you should move line 38 and 41 out from the control_dependency context.
And back to the original issue, it is actually caused by the profiler. In order to record the runtime of each kernel, our profiler calls cuCtxSynchronize to synchronize CPU and GPU. Unfortunately, right now we call this on the main thread, which blocks scheduling. This issue should go away if you turn off profiling.
We have an experimental feature that create private threads for scheduling, which can fix this issue as well. To turn it on, you need to set two environment variable before executing tf:
export TF_GPU_THREAD_MODE=gpu_private export TF_GPU_THREAD_COUNT=2
This is the timeline I generated after setting these two variables:
&lt;denchmark-link:https://user-images.githubusercontent.com/1269299/53850261-73e7c680-3f6f-11e9-91db-4bcbb887ff1c.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='xilenteyex' date='2019-03-06T19:45:31Z'>
		I am still investigating the gap between MatMul and memcpy.
		</comment>
		<comment id='6' author='xilenteyex' date='2019-03-06T21:41:11Z'>
		Hi &lt;denchmark-link:https://github.com/qqfish&gt;@qqfish&lt;/denchmark-link&gt;
 thanks for your input. I moved the lines outside the control_dependency context and set the environment variables as you suggested. It appears to be working fine for me now (Not sure why are you still seeing a gap in your execution) as shown in the timeline below:
&lt;denchmark-link:https://gist.github.com/xilenteyex/d6606ddcb7a6bc3edba95d1e531cf403&gt;timeline JSON file can be found here&lt;/denchmark-link&gt;

&lt;denchmark-link:https://gist.github.com/xilenteyex/47c0810ec447d87f26cf3ce419f6efb8&gt;Updated toy_matmul_code can be found here&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/10864603/53915020-069a6b00-402d-11e9-9ab4-bd59a952e6c2.png&gt;&lt;/denchmark-link&gt;

Next step for my use-case is to try similar settings for larger real models e.g. &lt;denchmark-link:https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb&gt;RRNLM (Recurrent Neural Network for Language Modeling)&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/tensorflow/models/tree/master/research/slim&gt;Inception, ResNet etc.&lt;/denchmark-link&gt;
 Do you think these environment variable settings are safe to use for those bigger and more complicated control flow graphs as well ?
Also, if you can tell what these environment variables are doing, that will help as well.
Thanks!
		</comment>
		<comment id='7' author='xilenteyex' date='2019-03-07T19:09:39Z'>
		Yes, it should be safe. We may turn this feature on by default recently.
Previous we use one thread on GPU to compute CPU ops and schedule kernels on GPU. These variables create a private scheduling thread for each GPU. Then we don't need to do everything in one thread.
There are two gap in the original timeline: (1) between MatMul and MemCPYPtoP (2) between MemCPYPtoP and MatMul_2. The environment variables should address gap (2). However, (1) is non-deterministic
I am working on a fix to address (1).
		</comment>
		<comment id='8' author='xilenteyex' date='2019-03-08T06:42:35Z'>
		Hi &lt;denchmark-link:https://github.com/qqfish&gt;@qqfish&lt;/denchmark-link&gt;

for variable . Value should be equal to number of GPUs or it should always be 2 ?
Also, after setting both the environment variables, for execution on my local machine 'both the gaps' [(1) and (2) in your last comment] are gone, but not for your case ?
Do you think for me GAP has disappeared by chance ? Like it may appear if I try to run the same thing again  or try to run a different example ?
		</comment>
		<comment id='9' author='xilenteyex' date='2019-03-08T21:28:40Z'>
		If you set TF_GPU_THREAD_MODE=gpu_private then TF_GPU_THREAD_COUNT should always be 2. It means each GPU will have 2 private threads to launch ops.
Yes, I think gap (1) disappear in your case by change. If you rerunning the same thing, it will not appear. But the gap may appear again if running a different example.
		</comment>
		<comment id='10' author='xilenteyex' date='2019-03-08T22:23:25Z'>
		&lt;denchmark-link:https://github.com/qqfish&gt;@qqfish&lt;/denchmark-link&gt;
 Thanks for the explanation. I will wait for your fix then before I try the other larger models. If I can be helpful in anyway, let me know.
Thanks!
		</comment>
		<comment id='11' author='xilenteyex' date='2019-03-14T00:00:50Z'>
		Fixing this issue requires a much bigger change than I originally thought. I cannot find a easy workaround works for all cases. This may require to re-design op scheduling of TF to fully support this kind of fine-grained dependency.
The good news is, someone in our team has already started working on fine-grained dependency support. But it may take a while.
		</comment>
		<comment id='12' author='xilenteyex' date='2019-03-14T00:14:41Z'>
		&lt;denchmark-link:https://github.com/qqfish&gt;@qqfish&lt;/denchmark-link&gt;
 Thanks for the update. Do we know approximately how much time will it take ?
		</comment>
		<comment id='13' author='xilenteyex' date='2019-03-14T01:04:28Z'>
		I am not sure as well. It is part of a larger project that is still in its early stage. So please stay tuned.
		</comment>
		<comment id='14' author='xilenteyex' date='2019-04-03T19:20:06Z'>
		any updates ?
		</comment>
		<comment id='15' author='xilenteyex' date='2019-04-05T01:30:13Z'>
		Sorry, not much yet. We are still working on it.
		</comment>
		<comment id='16' author='xilenteyex' date='2019-05-13T19:40:55Z'>
		Just checking if there are any updates yet?
		</comment>
		<comment id='17' author='xilenteyex' date='2019-07-19T18:10:58Z'>
		Can someone explain why TF_GPU_THREAD_COUNT should be 2 when gpu_thread_mode is private? Why not larger numbers?
Also, how about when thread_mode is shared? Shall I have larger TF_GPU_THREAD_COUNT be larger?
Thanks!
		</comment>
		<comment id='18' author='xilenteyex' date='2019-07-19T19:04:26Z'>
		
Can someone explain why TF_GPU_THREAD_COUNT should be 2 when gpu_thread_mode is private? Why not larger numbers?

When TF_GPU_THREAD_MODE is set to gpu_private, then TF_GPU_THREAD_COUNT is the number of threads per GPU for kernel launching. For some reference / benchmark models we find using 1 or 2 threads per GPU perform well. You can customize it to larger numbers, but be aware of contention if the total number of GPU private threads, data processing threads, and other CPU computation threads.

Also, how about when thread_mode is shared? Shall I have larger TF_GPU_THREAD_COUNT be larger?

When set to shared, it will be the total number of threads for kernel launching on all GPUs. It's supposed to be a larger number (for example, 2 * num_gpus).
		</comment>
		<comment id='19' author='xilenteyex' date='2019-07-19T21:00:41Z'>
		Hi &lt;denchmark-link:https://github.com/haoyuz&gt;@haoyuz&lt;/denchmark-link&gt;
,
Thanks. Is this TF_GPU_THREAD_COUNT thread pool for computation? I'm not very familiar with tensorflow runtime, when you said "kernel launching", do you mean a single GPU can only use 2 threads for computation?
		</comment>
		<comment id='20' author='xilenteyex' date='2019-07-19T21:36:45Z'>
		
Is this TF_GPU_THREAD_COUNT thread pool for computation?

Thread pool is for CPU, and this number is controlling how many CPU threads you want to schedule GPU kernels. You do not have to worry about the number of "threads" on GPU (technically GPU has different concepts controlling different levels of parallelism, and a single GPU stream running ops with a large enough batch size can usually saturate the GPU).
		</comment>
		<comment id='21' author='xilenteyex' date='2019-08-08T19:43:58Z'>
		&lt;denchmark-link:https://github.com/qqfish&gt;@qqfish&lt;/denchmark-link&gt;
, just checking, any updates yet ?
		</comment>
		<comment id='22' author='xilenteyex' date='2019-08-08T19:50:18Z'>
		Sorry, there is still not much yet. I think you shouldn't be blocked by this issue.
		</comment>
		<comment id='23' author='xilenteyex' date='2019-08-12T18:32:57Z'>
		Hi &lt;denchmark-link:https://github.com/qqfish&gt;@qqfish&lt;/denchmark-link&gt;
,
I am trying to create a framework for speeding up TensorFlow programs using model parallelism. When one of the GPUs stays idle for sometime even when there are operations ready to be executed, performance improvement is not as expected and is sometimes worse than the CPU, negatively impacting the performance of my framework.
Because of this, I am unable to tell if performance is getting worse because of bad partitioning by my framework or because of bad behavior of TensorFlow scheduler.
If you can share a temporary workaround for now. That will be helpful as well.
Thanks!
		</comment>
		<comment id='24' author='xilenteyex' date='2019-08-14T16:48:25Z'>
		First, I think TF_GPU_THREAD_MODE is good enough. In our multi-gpu benchmark, we don't see any performance issues causing by this.
If you find any gap causing by this issue in timeline, I think you can use control dependency to work it around. You can add a tf.Identity with control dependency to make sure the Memcpy is scheduled before other heavy gpu ops.
		</comment>
		<comment id='25' author='xilenteyex' date='2019-08-19T17:24:35Z'>
		&lt;denchmark-link:https://github.com/qqfish&gt;@qqfish&lt;/denchmark-link&gt;
, Can you please elaborate this? I am not an expert in TensorFlow yet, sorry :( .
(If you can give a small toy example that will help a lot!)

If you find any gap causing by this issue in timeline, I think you can use control dependency to work it around. You can add a tf.Identity with control dependency to make sure the Memcpy is scheduled before other heavy gpu ops.

Thanks!
		</comment>
		<comment id='26' author='xilenteyex' date='2019-08-19T19:26:00Z'>
		Let say you have a graph like this:
&lt;denchmark-code&gt;with tf.device('cpu:0'):
  X0 = tf.random_uniform([dim, dim], 0, 10)

with tf.device('gpu:0'):
  Y0 = tf.matmul(X0, X0)

with tf.device('gpu:1'):
  X1 = tf.random_uniform([dim, dim], 0, 10)
  with tf.control_dependencies([X0]):
    Y1 = tf.matmul(X1, X1)
&lt;/denchmark-code&gt;

If you notice memory copy is execute after Y1 on gpu, you can work around by adding following control dependency:
&lt;denchmark-code&gt;with tf.device('cpu:0'):
  X0 = tf.random_uniform([dim, dim], 0, 10)
  X0_ = tf.identity(X0)

with tf.device('gpu:0'):
  Y0 = tf.matmul(X0, X0)

with tf.device('gpu:1'):
  X1 = tf.random_uniform([dim, dim], 0, 10)
  with tf.control_dependencies([X0_]):
    Y1 = tf.matmul(X1, X1)
&lt;/denchmark-code&gt;

or:
&lt;denchmark-code&gt;with tf.device('cpu:0'):
  X0 = tf.random_uniform([dim, dim], 0, 10)

with tf.device('gpu:0'):
  Y0 = tf.matmul(X0, X0)

with tf.device('gpu:1'):
  with tf.control_dependencies([X0]):
    X1 = tf.random_uniform([dim, dim], 0, 10)
    Y1 = tf.matmul(X1, X1)
&lt;/denchmark-code&gt;

But like I said, it is a very rare issue, you don't need to worry about it too much.
		</comment>
		<comment id='27' author='xilenteyex' date='2020-01-02T18:12:41Z'>
		Closing as per &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/25724#issuecomment-522719673&gt;#25724 (comment)&lt;/denchmark-link&gt;
: this is a rare issue that has a workaround.
		</comment>
		<comment id='28' author='xilenteyex' date='2020-01-02T18:12:43Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25724&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25724&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>