<bug id='28337' author='ymsaputra' open_date='2019-05-02T12:00:57Z' closed_time='2019-06-05T20:40:32Z'>
	<summary>Distributed TensorFlow does not work when using more than 26 workers</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7.5
TensorFlow version (use command below): TF Alpha 2.0 and TF. 1.13 CPU only
Python version: 2.7
Intel Xeon Gold 6150 2.7GHz 18 cores (16 cores enabled) 24.75MB L3 Cache (Max Turbo Freq. 3.7GHz, Min 3.4GHz), 180GB RAM (Six Channel), 4.8TB of Disk Space

I have a performance issue using distributed TF for CPU machine with MultiWorkerStrategy using 58 workers. However, it seems that it does not work as the following errors:
WARNING: Logging before flag parsing goes to stderr.
W0502 21:43:31.821069 47528963346944 cross_device_ops.py:1106] Not all devices in tf.distribute.Strategy are visible to TensorFlow.
WARNING: Logging before flag parsing goes to stderr.
W0502 21:43:31.821974 47863205896704 cross_device_ops.py:1106] Not all devices in tf.distribute.Strategy are visible to TensorFlow.
terminate called after throwing an instance of 'std::system_error'
what():  Resource temporarily unavailable
terminate called after throwing an instance of 'std::system_error'
what():  Resource temporarily unavailable
terminate called after throwing an instance of 'std::system_error'
what():  Resource temporarily unavailable
Typically, how many workers can be used for this scheme?  and how to solve this problem? Is it because I do not have sufficient memory?
	</description>
	<comments>
		<comment id='1' author='ymsaputra' date='2019-05-03T10:03:28Z'>
		&lt;denchmark-link:https://github.com/ymsaputra&gt;@ymsaputra&lt;/denchmark-link&gt;
 In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!
		</comment>
		<comment id='2' author='ymsaputra' date='2019-05-04T01:23:49Z'>
		The aforementioned error happens before the training process begins. Is it because I have 16 cores only?
Here is the snippet of the code:
&lt;denchmark-code&gt;os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 
	os.environ["CUDA_VISIBLE_DEVICES"]="-1"

	cluster = {'chief': ['localhost:2000'],
             #'ps': ['localhost:2222'],
             'worker': ['localhost:2001', 'localhost:2002', 'localhost:2003', 'localhost:2004', 'localhost:2005', 'localhost:2006', 'localhost:2007', 'localhost:2008', 'localhost:2009', 'localhost:2010', 'localhost:2011', 'localhost:2012', 'localhost:2013', 'localhost:2014', 'localhost:2015', 'localhost:2016', 'localhost:2017', 'localhost:2018', 'localhost:2019', 'localhost:2020', 'localhost:2021', 'localhost:2022', 'localhost:2023', 'localhost:2024', 'localhost:2025', 'localhost:2026', 'localhost:2027', 'localhost:2028', 'localhost:2029', 'localhost:2030', 'localhost:2031', 'localhost:2032', 'localhost:2033', 'localhost:2034', 'localhost:2035', 'localhost:2036', 'localhost:2037', 'localhost:2038', 'localhost:2039', 'localhost:2040', 'localhost:2041', 'localhost:2042', 'localhost:2043', 'localhost:2044', 'localhost:2045', 'localhost:2046', 'localhost:2047', 'localhost:2048', 'localhost:2049', 'localhost:2050', 'localhost:2051', 'localhost:2052', 'localhost:2053', 'localhost:2054', 'localhost:2055', 'localhost:2056', 'localhost:2057']
          }
	# Input Flags

	os.environ['TF_CONFIG'] = json.dumps(
		{'cluster': cluster,
		 'task': {'type': FLAGS.job_name, 'index': FLAGS.task_index}})
	mirrored_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(tf.distribute.experimental.CollectiveCommunication.RING)
	config = tf.estimator.RunConfig(train_distribute=mirrored_strategy, eval_distribute=mirrored_strategy, save_checkpoints_steps=200)
&lt;/denchmark-code&gt;

Then I run multiple pythons as follows:
&lt;denchmark-code&gt;import subprocess

subprocess.Popen('python syn_distributed_tf_energy3b-58.py --job_name "chief" --task_index 0', shell = True)
subprocess.Popen('python syn_distributed_tf_energy3b-58.py --job_name "worker" --task_index 0', shell = True)
subprocess.Popen('python syn_distributed_tf_energy3b-58.py --job_name "worker" --task_index 1', shell = True)
subprocess.Popen('python syn_distributed_tf_energy3b-58.py --job_name "worker" --task_index 2', shell = True)
subprocess.Popen('python syn_distributed_tf_energy3b-58.py --job_name "worker" --task_index 3', shell = True)
subprocess.Popen('python syn_distributed_tf_energy3b-58.py --job_name "worker" --task_index 4', shell = True)
...
subprocess.Popen('python syn_distributed_tf_energy3b-58.py --job_name "worker" --task_index 56', shell = True)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='ymsaputra' date='2019-05-14T21:21:32Z'>
		if i understand correctly, you're creating all the 58 workers on a single machine? if yes, what is the purpose behind that?
		</comment>
		<comment id='4' author='ymsaputra' date='2019-05-15T01:45:58Z'>
		&lt;denchmark-link:https://github.com/guptapriya&gt;@guptapriya&lt;/denchmark-link&gt;
 Yes, I use a single computer with Intel Xeon Gold 6150 2.7GHz 18 cores (16 cores enabled) 24.75MB L3 Cache (Max Turbo Freq. 3.7GHz, Min 3.4GHz), 180GB RAM (Six Channel), 4.8TB of Disk Space. I use 58 workers to represents 58 base stations for simulations. Should I need to use 58 machines to implement this?
		</comment>
		<comment id='5' author='ymsaputra' date='2019-05-16T07:19:04Z'>
		Typically we just run 1 worker on one machine, unless running multiple workers helps in some way in resource utilization. running such a large number of workers on a single machine is definitely not something that has been tested much and I can imagine it running into running out of memory or other resources. Is it possible for you to use more machines instead?
		</comment>
		<comment id='6' author='ymsaputra' date='2019-05-20T05:21:26Z'>
		Does it mean that I need to use more PCs/laptops, e.g., 6 laptops, to run at least 9 workers on each machine (to run 58 workers in total)?
		</comment>
		<comment id='7' author='ymsaputra' date='2019-06-05T20:39:41Z'>
		If you want to run 58 workers, you'll need 58 machines. It is not efficient to run more than one worker on a single machine other than for testing purposes. Each worker would start a TF engine and each would try to use all of your resources.
		</comment>
		<comment id='8' author='ymsaputra' date='2019-06-05T20:40:32Z'>
		Closing this issue for now since it doesn't look like a bug.
		</comment>
		<comment id='9' author='ymsaputra' date='2019-06-05T20:40:42Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=28337&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=28337&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='ymsaputra' date='2019-10-06T21:44:45Z'>
		In a machine with 48 cores, why cannot 48 threads of worker be run. If that is not possible, is there a way to distribute load onto the 48 cores within a single worker ?
		</comment>
		<comment id='11' author='ymsaputra' date='2019-10-07T17:00:05Z'>
		&lt;denchmark-link:https://github.com/TransientObject&gt;@TransientObject&lt;/denchmark-link&gt;
 Please create a new issue with description, platform details, TF version and a simple standalone code to reproduce the issue. Thanks!
		</comment>
		<comment id='12' author='ymsaputra' date='2020-03-30T16:08:12Z'>
		
If you want to run 58 workers, you'll need 58 machines. It is not efficient to run more than one worker on a single machine other than for testing purposes. Each worker would start a TF engine and each would try to use all of your resources.

&lt;denchmark-link:https://github.com/ymsaputra&gt;@ymsaputra&lt;/denchmark-link&gt;
 Efficiency apart, Ubuntu 18.04 seems to reach a limit around ~50 sessions. Raising  inside /etc/systemd/user.conf &amp; system.conf worked for me. Note that I don't know what kind of side effect that could cause.
Also, there is those two variables that seems to reduce the number of threads by a lot:
&lt;denchmark-code&gt;config = tf.ConfigProto()
config.intra_op_parallelism_threads = 1
config.inter_op_parallelism_threads = 1
tf.Session(config=config)
&lt;/denchmark-code&gt;

Again, I don't know what kind of side effect that could cause.
		</comment>
	</comments>
</bug>