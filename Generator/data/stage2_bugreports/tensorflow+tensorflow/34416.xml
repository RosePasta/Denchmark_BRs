<bug id='34416' author='mapeima' open_date='2019-11-19T12:44:50Z' closed_time='2020-12-01T18:11:24Z'>
	<summary>Error trying to convert a model using full integer quantization</summary>
	<description>
System information

OS Platform and Distribution: Linux Ubuntu 18.04.3:
TensorFlow installed from source: pip3 install --upgrade tensorflow==1.15
TensorFlow version: 1.15

Provide the text output from tflite_convert
&lt;denchmark-code&gt;Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: FULLY_CONNECTED, SOFTMAX. Here is a list of operators for which you will need custom implementations: IdentityN.
Traceback (most recent call last):
  File "/usr/local/bin/toco_from_protos", line 8, in &lt;module&gt;
    sys.exit(main())
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py", line 89, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File "/usr/local/lib/python3.6/dist-packages/absl/app.py", line 299, in run
    _run_main(main, args)
  File "/usr/local/lib/python3.6/dist-packages/absl/app.py", line 250, in _run_main
    sys.exit(main(argv))
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py", line 52, in execute
    enable_mlir_converter)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: FULLY_CONNECTED, SOFTMAX. Here is a list of operators for which you will need custom implementations: IdentityN.
&lt;/denchmark-code&gt;

Model:
&lt;denchmark-code&gt;import pathlib

import tensorflow as tf


mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  # tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5)

model.evaluate(x_test,  y_test, verbose=2)


# Save the model into SaveModel format

saved_model_dir = pathlib.Path("./saved_model/")
tf.saved_model.save(model, str(saved_model_dir))
&lt;/denchmark-code&gt;

Convert the model:
&lt;denchmark-code&gt;import pathlib

import tensorflow as tf


mnist = tf.keras.datasets.mnist
x_train = mnist.load_data()[0][0] / 255.0

saved_model_dir = pathlib.Path("./saved_model/")


# Convert the model from saved model

images = tf.cast(x_train, tf.float32)
mnist_ds = tf.data.Dataset.from_tensor_slices(images).batch(1)


def representative_dataset_gen():
    for input_value in mnist_ds.take(100):
        yield [input_value]


converter = tf.lite.TFLiteConverter.from_saved_model(str(saved_model_dir))
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
converter.representative_dataset = representative_dataset_gen

tflite_quant_model = converter.convert()

tflite_quant_model_file = saved_model_dir/"mnist_post_quant_model_io.tflite"
tflite_quant_model_file.write_bytes(tflite_quant_model)
&lt;/denchmark-code&gt;

Any other info / logs
&lt;denchmark-code&gt;/usr/bin/python3.6 /home/mapeima/Code/python/edgeTPU/model_converter.py
2019-11-19 13:34:02.791346: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2019-11-19 13:34:02.791364: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)
2019-11-19 13:34:02.791397: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jihr): /proc/driver/nvidia/version does not exist
2019-11-19 13:34:02.791614: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-11-19 13:34:02.814669: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2712000000 Hz
2019-11-19 13:34:02.815094: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3b64140 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2019-11-19 13:34:02.815124: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/convert_saved_model.py:60: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.
2019-11-19 13:34:02.937627: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count &gt;= 8, compute capability &gt;= 0.0): 0
2019-11-19 13:34:02.937702: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2019-11-19 13:34:02.948371: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize
2019-11-19 13:34:02.948393: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: Graph size after: 189 nodes (144), 355 edges (290), time = 4.452ms.
2019-11-19 13:34:02.948398: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0.08ms.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/util.py:249: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py:854: UserWarning: Property target_ops is deprecated, please use target_spec.supported_ops instead.
  "target_spec.supported_ops instead." % name)
2019-11-19 13:34:02.971121: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count &gt;= 8, compute capability &gt;= 0.0): 0
2019-11-19 13:34:02.971187: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2019-11-19 13:34:02.977074: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize
2019-11-19 13:34:02.977098: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 34 nodes (-8), 57 edges (-12), time = 3.248ms.
2019-11-19 13:34:02.977102: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 34 nodes (0), 57 edges (0), time = 0.73ms.
Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Traceback (most recent call last):
  File "/home/mapeima/Code/python/edgeTPU/model_converter.py", line 30, in &lt;module&gt;
    tflite_quant_model = converter.convert()
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py", line 983, in convert
    **converter_kwargs)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/convert.py", line 449, in toco_convert_impl
    enable_mlir_converter=enable_mlir_converter)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/convert.py", line 200, in toco_convert_protos
    raise ConverterError("See console for info.\n%s\n%s\n" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: See console for info.
2019-11-19 13:34:04.120420: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: IdentityN
2019-11-19 13:34:04.120589: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 17 operators, 26 arrays (0 quantized)
2019-11-19 13:34:04.120707: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 17 operators, 26 arrays (0 quantized)
2019-11-19 13:34:04.120849: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 6 operators, 12 arrays (0 quantized)
2019-11-19 13:34:04.121155: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 5 operators, 11 arrays (0 quantized)
2019-11-19 13:34:04.121196: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 4 operators, 9 arrays (0 quantized)
2019-11-19 13:34:04.121224: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 4 operators, 9 arrays (0 quantized)
2019-11-19 13:34:04.121241: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 4 operators, 9 arrays (0 quantized)
2019-11-19 13:34:04.121273: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 576 bytes, theoretical optimal value: 576 bytes.
2019-11-19 13:34:04.121289: I tensorflow/lite/toco/toco_tooling.cc:439] Estimated count of arithmetic ops: 204042 ops, equivalently 102021 MACs
2019-11-19 13:34:04.121293: I tensorflow/lite/toco/toco_tooling.cc:454] Number of parameters: 101770
2019-11-19 13:34:04.121470: E tensorflow/lite/toco/toco_tooling.cc:481] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: FULLY_CONNECTED, SOFTMAX. Here is a list of operators for which you will need custom implementations: IdentityN.
Traceback (most recent call last):
  File "/usr/local/bin/toco_from_protos", line 8, in &lt;module&gt;
    sys.exit(main())
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py", line 89, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File "/usr/local/lib/python3.6/dist-packages/absl/app.py", line 299, in run
    _run_main(main, args)
  File "/usr/local/lib/python3.6/dist-packages/absl/app.py", line 250, in _run_main
    sys.exit(main(argv))
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py", line 52, in execute
    enable_mlir_converter)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: FULLY_CONNECTED, SOFTMAX. Here is a list of operators for which you will need custom implementations: IdentityN.




Process finished with exit code 1
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='mapeima' date='2019-11-21T06:27:13Z'>
		I tried the conversion with tf 2.0 and the your conversion code worked fine without any errors. Can you try this with 2.0 again?
		</comment>
		<comment id='2' author='mapeima' date='2019-11-21T10:14:51Z'>
		Yes, I know that with tf 2.0 works perfectly but the point is that I need to use tf 1.15 because I want to run that model on the Google edgeTPU and to do that I have to perform a full integer quantization. As they say &lt;denchmark-link:https://coral.withgoogle.com/docs/edgetpu/models-intro/#quantization&gt;here&lt;/denchmark-link&gt;
, I must use tf 1.15:

Note: To use post-training quantization, you must use TensorFlow 1.15 and set both the input and output type to uint8. (Currently, TensorFlow 2.0 does not support uint8 input/output with post-training quantization.) For instructions, see the TensorFlow Lite guide to full integer post-training quantization.

		</comment>
		<comment id='3' author='mapeima' date='2019-11-21T17:30:57Z'>
		Hi &lt;denchmark-link:https://github.com/mapeima&gt;@mapeima&lt;/denchmark-link&gt;

We have code that is about to be submitted to enable the uint8 parameters in the 2.0 converter. The fastest path for you will be to wait until that is in the nightly, and then upgrade to using the new version of the converter that has support for IdentityN. Will let you know when that is ready.
Thanks!
-Suharsh
		</comment>
		<comment id='4' author='mapeima' date='2019-11-29T04:11:54Z'>
		I meet the same problem，even when I  post-quant full integer with MobilenetV2
		</comment>
		<comment id='5' author='mapeima' date='2020-02-14T14:36:47Z'>
		Any updates with tf 1.15 ?
		</comment>
		<comment id='6' author='mapeima' date='2020-11-02T22:33:09Z'>
		&lt;denchmark-link:https://github.com/vinorth05&gt;@vinorth05&lt;/denchmark-link&gt;
 It looks like you can now use TF 2.x (preferably  &gt; TF 2.3 so that you can have a fully int8 quantized model - including uint8 model input and output). Here is the colab that you can use as a reference: [https://colab.sandbox.google.com/github/google-coral/tutorials/blob/master/retrain_classification_ptq_tf2.ipynb]
Does this work for you?
		</comment>
		<comment id='7' author='mapeima' date='2020-12-01T18:11:25Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34416&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34416&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>