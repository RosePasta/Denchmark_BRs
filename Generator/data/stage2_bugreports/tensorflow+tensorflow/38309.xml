<bug id='38309' author='rogeryuchao' open_date='2020-04-07T10:43:41Z' closed_time='2020-05-21T10:26:26Z'>
	<summary>tf.saved_model.save()/load() Issue</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

CNN model is build under subclass
OS Platform and Distribution: Win10/Anaconda/Python3.7/Tensorflow 2.1.0
TensorFlow installed from (source or binary): pip install
Python version: - 3.7
CUDA/cuDNN version: Run under CPU mode

Describe the current behavior

When finish 1 epoch of training for model, use tf.saved_model.save(model, save_model_dir)  to save the whole model to the folder
Try to load the pb file generated by the system called saved_model.pb by command:
model = tf.saved_model.load(save_model_dir)
Try to predict the result by using command:
model(inputs)
The system return following error:
TypeError: '_UserObject' object is not callable
From the API document, the example is clearly use the function as I mentioned I think

What's the error then?
Describe the expected behavior
Standalone code to reproduce the issue
from future import absolute_import, division, print_function
import tensorflow as tf
import math
&lt;denchmark-h:h1&gt;User defined packages&lt;/denchmark-h&gt;

from configuration import IMAGE_HEIGHT, IMAGE_WIDTH, CHANNELS, 
EPOCHS, BATCH_SIZE, save_model_dir, save_every_n_epoch
from prepare_data import generate_datasets, load_and_preprocess_image
from models import mobilenet_v1, mobilenet_v2, mobilenet_v3_large, mobilenet_v3_small, 
efficientnet, resnext, inception_v4, inception_resnet_v1, inception_resnet_v2, 
se_resnet, squeezenet, densenet, shufflenet_v2, resnet
from models.model_selection import get_model
from frozen_pb import get_frozen_model
def print_model_summary(network):
network.build(input_shape=(None, IMAGE_HEIGHT, IMAGE_WIDTH, CHANNELS))
network.summary()
def process_features(features, data_augmentation):
image_raw = features['image_raw'].numpy()
image_tensor_list = []
for image in image_raw:
image_tensor = load_and_preprocess_image(image, data_augmentation=data_augmentation)
image_tensor_list.append(image_tensor)
images = tf.stack(image_tensor_list, axis=0)
labels = features['label'].numpy()
&lt;denchmark-code&gt;return images, labels
&lt;/denchmark-code&gt;

if name == 'main':
# GPU settings
gpus = tf.config.list_physical_devices("GPU")
if gpus:
for gpu in gpus:
tf.config.experimental.set_memory_growth(gpu, True)
&lt;denchmark-code&gt;# get the dataset
train_dataset, valid_dataset, test_dataset, train_count, valid_count, test_count = generate_datasets()

# create model
model = get_model()
print_model_summary(network=model)

# define loss and optimizer
loss_object = tf.keras.losses.SparseCategoricalCrossentropy()
# Tried RMSprop for optimizer, the result is not so good, finetune the optimizer to Adam or Momentum
#optimizer = tf.keras.optimizers.RMSprop(learning_rate = GLOBAL_LEARNING_RATE,
#                                        momentum = MOMENTUM,
#                                        name = 'rms_optimizer')

optimizer = tf.keras.optimizers.Adam(lr = 0.0001, decay = 0.4)

train_loss = tf.keras.metrics.Mean(name='train_loss')
train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')

valid_loss = tf.keras.metrics.Mean(name='valid_loss')
valid_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='valid_accuracy')

# @tf.function
def train_step(image_batch, label_batch):
    with tf.GradientTape() as tape:
        predictions = model(image_batch, training=True)
        loss = loss_object(y_true=label_batch, y_pred=predictions)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(grads_and_vars=zip(gradients, model.trainable_variables))

    train_loss.update_state(values=loss)
    train_accuracy.update_state(y_true=label_batch, y_pred=predictions)
    
    return predictions, tf.math.argmax(predictions, axis =1).numpy()

# @tf.function
def valid_step(image_batch, label_batch):
    predictions = model(image_batch, training=True)
    v_loss = loss_object(label_batch, predictions)

    valid_loss.update_state(values=v_loss)
    valid_accuracy.update_state(y_true=label_batch, y_pred=predictions)
    
    return tf.math.argmax(predictions, axis =1).numpy()

# start training
for epoch in range(EPOCHS):
    step = 0
    for features in train_dataset:
        step += 1
        images, labels = process_features(features, data_augmentation=False)
        predictions, predict_labels = train_step(images, labels)
        print("Epoch: {}/{}, step: {}/{}, loss: {:.5f}, accuracy: {:.5f}".format(epoch,
                                                                                 EPOCHS,
                                                                                 step,
                                                                                 math.ceil(train_count / BATCH_SIZE),
                                                                                 train_loss.result().numpy(),
                                                                                 train_accuracy.result().numpy()),
              predictions,
              predict_labels,
              labels)

    for features in valid_dataset:
        valid_images, valid_labels = process_features(features, data_augmentation=False)
        valid_step(valid_images, valid_labels)

    print("Epoch: {}/{}, train loss: {:.5f}, train accuracy: {:.5f}, "
          "valid loss: {:.5f}, valid accuracy: {:.5f}".format(epoch,
                                                              EPOCHS,
                                                              train_loss.result().numpy(),
                                                              train_accuracy.result().numpy(),
                                                              valid_loss.result().numpy(),
                                                              valid_accuracy.result().numpy()))
    train_loss.reset_states()
    train_accuracy.reset_states()
    valid_loss.reset_states()
    valid_accuracy.reset_states()

    if epoch % save_every_n_epoch == 0:
        tf.saved_model.save(model, save_model_dir)
&lt;/denchmark-code&gt;

Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='rogeryuchao' date='2020-04-07T11:50:32Z'>
		By the way, I've seen the response for previous similar bug.
It is suggested to use tf.keras.models.save_model to save the model, while I tried this function and the result became to be another failure:
input shape is not defined, must use model._set_input(inputs) to manually setup the shape
Here comes the question:
Is it means I must use model.compile and model.fit together with it?
		</comment>
		<comment id='2' author='rogeryuchao' date='2020-04-07T16:28:31Z'>
		&lt;denchmark-link:https://github.com/rogeryuchao&gt;@rogeryuchao&lt;/denchmark-link&gt;
, I am not able to figure out how you are defining model. But still I suggest to use  instead of  to predict the result from loaded model.
		</comment>
		<comment id='3' author='rogeryuchao' date='2020-04-08T01:01:57Z'>
		&lt;denchmark-link:https://github.com/khimraj&gt;@khimraj&lt;/denchmark-link&gt;
 Hi,
I also tried model.predict actually, but still do not work.
please check the following information for the model building class
`import tensorflow as tf
from models.inception_modules import Stem, ReductionA, BasicConv2D, Conv2DLinear
from configuration import NUM_CLASSES, DROPOUT_RATIO, L1_REGULIZER, L2_REGULIZER
class InceptionResNetA(tf.keras.layers.Layer):
def init(self):
super(InceptionResNetA, self).init()
self.b1_conv = BasicConv2D(filters=32,
kernel_size=(1, 1),
strides=1,
padding="same")
self.b2_conv1 = BasicConv2D(filters=32,
kernel_size=(1, 1),
strides=1,
padding="same")
self.b2_conv2 = BasicConv2D(filters=32,
kernel_size=(3, 3),
strides=1,
padding="same")
self.b3_conv1 = BasicConv2D(filters=32,
kernel_size=(1, 1),
strides=1,
padding="same")
self.b3_conv2 = BasicConv2D(filters=48,
kernel_size=(3, 3),
strides=1,
padding="same")
self.b3_conv3 = BasicConv2D(filters=64,
kernel_size=(3, 3),
strides=1,
padding="same")
self.conv = Conv2DLinear(filters=384,
kernel_size=(1, 1),
strides=1,
padding="same")
&lt;denchmark-code&gt;def call(self, inputs, training=None, **kwargs):
    b1 = self.b1_conv(inputs, training=training)
    b2 = self.b2_conv1(inputs, training=training)
    b2 = self.b2_conv2(b2, training=training)
    b3 = self.b3_conv1(inputs, training=training)
    b3 = self.b3_conv2(b3, training=training)
    b3 = self.b3_conv3(b3, training=training)

    x = tf.concat(values=[b1, b2, b3], axis=-1)
    x = self.conv(x, training=training)

    output = tf.keras.layers.add([x, inputs])
    return tf.nn.relu(output)
&lt;/denchmark-code&gt;

class InceptionResNetB(tf.keras.layers.Layer):
def init(self):
super(InceptionResNetB, self).init()
self.b1_conv = BasicConv2D(filters=192,
kernel_size=(1, 1),
strides=1,
padding="same")
self.b2_conv1 = BasicConv2D(filters=128,
kernel_size=(1, 1),
strides=1,
padding="same")
self.b2_conv2 = BasicConv2D(filters=160,
kernel_size=(1, 7),
strides=1,
padding="same")
self.b2_conv3 = BasicConv2D(filters=192,
kernel_size=(7, 1),
strides=1,
padding="same")
self.conv = Conv2DLinear(filters=1152,
kernel_size=(1, 1),
strides=1,
padding="same")
&lt;denchmark-code&gt;def call(self, inputs, training=None, **kwargs):
    b1 = self.b1_conv(inputs, training=training)
    b2 = self.b2_conv1(inputs, training=training)
    b2 = self.b2_conv2(b2, training=training)
    b2 = self.b2_conv3(b2, training=training)

    x = tf.concat(values=[b1, b2], axis=-1)
    x = self.conv(x, training=training)

    output = tf.keras.layers.add([x, inputs])

    return tf.nn.relu(output)
&lt;/denchmark-code&gt;

class InceptionResNetC(tf.keras.layers.Layer):
def init(self):
super(InceptionResNetC, self).init()
self.b1_conv = BasicConv2D(filters=192,
kernel_size=(1, 1),
strides=1,
padding="same")
self.b2_conv1 = BasicConv2D(filters=192,
kernel_size=(1, 1),
strides=1,
padding="same")
self.b2_conv2 = BasicConv2D(filters=224,
kernel_size=(1, 3),
strides=1,
padding="same")
self.b2_conv3 = BasicConv2D(filters=256,
kernel_size=(3, 1),
strides=1,
padding="same")
self.conv = Conv2DLinear(filters=2144,
kernel_size=(1, 1),
strides=1,
padding="same")
&lt;denchmark-code&gt;def call(self, inputs, training=None, **kwargs):
    b1 = self.b1_conv(inputs, training=training)
    b2 = self.b2_conv1(inputs, training=training)
    b2 = self.b2_conv2(b2, training=training)
    b2 = self.b2_conv3(b2, training=training)

    x = tf.concat(values=[b1, b2], axis=-1)
    x = self.conv(x, training=training)

    output = tf.keras.layers.add([x, inputs])

    return tf.nn.relu(output)
&lt;/denchmark-code&gt;

class ReductionB(tf.keras.layers.Layer):
def init(self):
super(ReductionB, self).init()
self.b1_maxpool = tf.keras.layers.MaxPool2D(pool_size=(3, 3),
strides=2,
padding="valid")
self.b2_conv1 = BasicConv2D(filters=256,
kernel_size=(1, 1),
strides=1,
padding="same")
self.b2_conv2 = BasicConv2D(filters=384,
kernel_size=(3, 3),
strides=2,
padding="valid")
self.b3_conv1 = BasicConv2D(filters=256,
kernel_size=(1, 1),
strides=1,
padding="same")
self.b3_conv2 = BasicConv2D(filters=288,
kernel_size=(3, 3),
strides=2,
padding="valid")
self.b4_conv1 = BasicConv2D(filters=256,
kernel_size=(1, 1),
strides=1,
padding="same")
self.b4_conv2 = BasicConv2D(filters=288,
kernel_size=(3, 3),
strides=1,
padding="same")
self.b4_conv3 = BasicConv2D(filters=320,
kernel_size=(3, 3),
strides=2,
padding="valid")
&lt;denchmark-code&gt;def call(self, inputs, training=None, **kwargs):
    b1 = self.b1_maxpool(inputs)

    b2 = self.b2_conv1(inputs, training=training)
    b2 = self.b2_conv2(b2, training=training)

    b3 = self.b3_conv1(inputs, training=training)
    b3 = self.b3_conv2(b3, training=training)

    b4 = self.b4_conv1(inputs, training=training)
    b4 = self.b4_conv2(b4, training=training)
    b4 = self.b4_conv3(b4, training=training)

    return tf.concat(values=[b1, b2, b3, b4], axis=-1)
&lt;/denchmark-code&gt;

def build_inception_resnet_a(n):
block = tf.keras.Sequential()
for _ in range(n):
block.add(InceptionResNetA())
return block
def build_inception_resnet_b(n):
block = tf.keras.Sequential()
for _ in range(n):
block.add(InceptionResNetB())
return block
def build_inception_resnet_c(n):
block = tf.keras.Sequential()
for _ in range(n):
block.add(InceptionResNetC())
return block
class InceptionResNetV2(tf.keras.Model):
def init(self):
super(InceptionResNetV2, self).init()
self.stem = Stem()
self.inception_resnet_a = build_inception_resnet_a(5)
self.reduction_a = ReductionA(k=256, l=256, m=384, n=384)
self.inception_resnet_b = build_inception_resnet_b(10)
self.reduction_b = ReductionB()
self.inception_resnet_c = build_inception_resnet_c(5)
self.avgpool = tf.keras.layers.AveragePooling2D(pool_size=(8, 8))
self.dropout = tf.keras.layers.Dropout(rate=DROPOUT_RATIO)
self.flat = tf.keras.layers.Flatten()
self.fc = tf.keras.layers.Dense(units=NUM_CLASSES,
activation=tf.keras.activations.softmax,
kernel_regularizer=tf.keras.regularizers.l1(L1_REGULIZER),
activity_regularizer=tf.keras.regularizers.l2(L2_REGULIZER))
&lt;denchmark-code&gt;def call(self, inputs, training=None, mask=None):
    x = self.stem(inputs, training=training)
    x = self.inception_resnet_a(x, training=training)
    x = self.reduction_a(x, training=training)
    x = self.inception_resnet_b(x, training=training)
    x = self.reduction_b(x, training=training)
    x = self.inception_resnet_c(x, training=training)
    x = self.avgpool(x)
    x = self.dropout(x, training=training)
    x = self.flat(x)
    x = self.fc(x)

    return x`
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='rogeryuchao' date='2020-04-14T12:36:31Z'>
		&lt;denchmark-link:https://github.com/rogeryuchao&gt;@rogeryuchao&lt;/denchmark-link&gt;

i have tried to replicate the issue shared and face indentation errors the code is too big to fix indentation for entire code, can you please share colab gist for us to analyse the issue if possible or share complete correct indented code
		</comment>
		<comment id='5' author='rogeryuchao' date='2020-04-21T10:43:22Z'>
		&lt;denchmark-link:https://github.com/rogeryuchao&gt;@rogeryuchao&lt;/denchmark-link&gt;

please update as per above comment
		</comment>
		<comment id='6' author='rogeryuchao' date='2020-04-21T12:35:26Z'>
		&lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;

Sorry for the late reply. Because of the china policy, unfortunately I cannot launch colab for sharing the code, while I think you can clone the repo and all the project framework are in it, thank you.
&lt;denchmark-link:mailto:git@github.com&gt;git@github.com&lt;/denchmark-link&gt;
:rogeryuchao/general_cnn.git
		</comment>
		<comment id='7' author='rogeryuchao' date='2020-04-21T14:31:03Z'>
		&lt;denchmark-link:https://github.com/rogeryuchao&gt;@rogeryuchao&lt;/denchmark-link&gt;

please provide us with minimal reproducible code, for us to help you.
		</comment>
		<comment id='8' author='rogeryuchao' date='2020-04-21T14:52:29Z'>
		&lt;denchmark-link:https://github.com/tensorflow/tensorflow/files/4510732/general_cnn-master.zip&gt;general_cnn-master.zip&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='rogeryuchao' date='2020-04-22T07:27:55Z'>
		&lt;denchmark-link:https://github.com/rogeryuchao&gt;@rogeryuchao&lt;/denchmark-link&gt;

can you please share the steps as the repository is huge and which files are to be used to replicate the issue
		</comment>
		<comment id='10' author='rogeryuchao' date='2020-04-22T13:00:46Z'>
		&lt;denchmark-link:https://github.com/Saduf2019&gt;@Saduf2019&lt;/denchmark-link&gt;

I cannot provide the image I use to you, because it is huge files. But I can let you know the steps:

Put the class_name[folder_name]/image_files.jpgs into original_data_set folder
In shell, Run split_dataset.py
In shell, Run to_tfrecord.py
Change the train.py (Now I use model.save_weight[line 202] function instead of tf.save_model/tf.keras.models.save_model[line 204] function), you can swap them
In configuration.py, change THRESHOLD function to 0.0
In configuration.py, if you think the saving timing is too late, you can adjust save_every_n_epoch
In shell, Run train.py 1 ABCD
Wait for the model to finish epochs and save model, the model will be saved under saved_model folder, but it will fail anyway(Even if I define the shape of input in line 203)
If defined the shape of input, the model will be saved, but cannot be used, try evaluate.py 1 ABCD to load the model, eventually it does not work

		</comment>
		<comment id='11' author='rogeryuchao' date='2020-05-04T20:46:43Z'>
		&lt;denchmark-link:https://github.com/rogeryuchao&gt;@rogeryuchao&lt;/denchmark-link&gt;
 Please provide a minimal example for us to reproduce this issue as I cannot reproduce this issue. If you can provide reproduce code with dataset, we can progress further with this discussion. Thanks!
		</comment>
		<comment id='12' author='rogeryuchao' date='2020-05-11T21:08:11Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='13' author='rogeryuchao' date='2020-05-21T10:26:23Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='14' author='rogeryuchao' date='2020-05-21T10:26:27Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38309&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38309&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>