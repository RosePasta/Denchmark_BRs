<bug id='6717' author='lwohlhart' open_date='2017-01-08T00:29:44Z' closed_time='2017-01-24T01:57:33Z'>
	<summary>Incorrect gradient for categorical distribution entropy</summary>
	<description>
The Categorical distribution class provides an awesome entropy operator but apparently the gradient calculation w.r.t. the input operators doesn't work.
logits = tf.Variable(initial_value=[[1., 2., 3.], [2., 5., 1.]])

probabilities = tf.nn.softmax(logits)
log_probabilities = tf.nn.log_softmax(logits)
entropy = - tf.reduce_sum(probabilities * log_probabilities, axis=-1)

# using the actual distribution would be nicer but gradients seem buggy
categorical_distribution = tf.contrib.distributions.Categorical(p=probabilities)
categorical_distribution_entropy = categorical_distribution.entropy()

# initialize
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)

# works
print(sess.run(entropy))
print(sess.run(tf.gradients(entropy, [logits])))

# apparently loses gradient information
print(sess.run(categorical_distribution_entropy))
print(sess.run(tf.gradients(categorical_distribution_entropy, [logits])))
In the outputs we see that the entropy calculation works but the gradients are somehow lost. This obviously also doesn't work when I try to maximize this entropy using any optimizer.
&lt;denchmark-code&gt;[ 0.83239555  0.27431309]
[array([[ 0.14181709,  0.14077036, -0.28258738],
       [ 0.13012242, -0.19513965,  0.06501719]], dtype=float32)]
[ 0.83239555  0.27431309]
[array([[ 0.,  0.,  0.],
       [ 0.,  0.,  0.]], dtype=float32)]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='lwohlhart' date='2017-01-09T02:25:16Z'>
		I suspect this a numerical stability issue. &lt;denchmark-link:https://gist.github.com/yaroslavvb/97504b8221a8529e7a51a50915206d68&gt;Looking&lt;/denchmark-link&gt;
 at your "working" graph, it uses . Whereas in the categorical distribution, it computes  and combines it with  &lt;denchmark-link:https://github.com/ebrevdo&gt;@ebrevdo&lt;/denchmark-link&gt;

&lt;denchmark-link:https://cloud.githubusercontent.com/assets/23068/21756041/dd734ea4-d5d0-11e6-82d0-001d9fab5bea.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='lwohlhart' date='2017-01-18T16:37:43Z'>
		Taking a look.
		</comment>
		<comment id='3' author='lwohlhart' date='2017-01-18T19:33:38Z'>
		For numerical stability, we use tf.nn.softmax_cross_entropy_with_logits to calculate the entropy.  I just found out that this operation does not perform backprop with respect to the "labels" argument (the exponentiated probabilities term of the entropy).  Thus the zeros.
Probably the best way to fix this is for us to implement the numerically stable equivalent of your reduce_sum.
		</comment>
		<comment id='4' author='lwohlhart' date='2017-01-18T19:53:16Z'>
		Fix going into master hopefully today / tomorrow.
		</comment>
		<comment id='5' author='lwohlhart' date='2017-01-24T01:56:15Z'>
		thank you for the fix !
i guess we can close this then
		</comment>
	</comments>
</bug>