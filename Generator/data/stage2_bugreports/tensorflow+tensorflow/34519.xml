<bug id='34519' author='SSSxCCC' open_date='2019-11-22T11:13:57Z' closed_time='2020-04-28T23:06:39Z'>
	<summary>tf.range + for x,y in dataset issue</summary>
	<description>
System information

Have I written custom code: yes
OS Platform and Distribution: win10 1809, pycharm2019.2.3
TensorFlow installed from: pip install tensorflow-gpu
TensorFlow version: tensorflow-gpu 2.0.0
Python version: 3.6
CUDA/cuDNN version: 10.0/7.6.4
GPU model and memory: rtx2080 8g

Describe the current behavior
tf.range + for x,y in dataset raise exception.
Describe the expected behavior
run normally
Code to reproduce the issue
import tensorflow as tf

if __name__ == '__main__':
    dataset = tf.data.Dataset.from_tensor_slices(([1, 2, 3], [4, 5, 6]))

    @tf.function
    def f():
        for e in tf.range(3):
            for x, y in dataset:
                tf.print(x, y, e)
    f()
Other info / logs
&lt;denchmark-code&gt;2019-11-22 18:56:57.100113: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2019-11-22 18:56:58.357108: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2019-11-22 18:56:58.432503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce RTX 2080 major: 7 minor: 5 memoryClockRate(GHz): 1.8
pciBusID: 0000:01:00.0
2019-11-22 18:56:58.432679: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-11-22 18:56:58.433099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-11-22 18:56:58.433463: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-11-22 18:56:58.435936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce RTX 2080 major: 7 minor: 5 memoryClockRate(GHz): 1.8
pciBusID: 0000:01:00.0
2019-11-22 18:56:58.436136: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2019-11-22 18:56:58.436559: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-11-22 18:56:59.016772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-11-22 18:56:59.016930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2019-11-22 18:56:59.017022: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2019-11-22 18:56:59.017714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6273 MB memory) -&gt; physical GPU (device: 0, name: GeForce RTX 2080, pci bus id: 0000:01:00.0, compute capability: 7.5)
2019-11-22 18:56:59.409303: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: No unary variant device copy function found for direction: 1 and Variant type_index: class tensorflow::data::`anonymous namespace'::DatasetVariantWrapper
	 [[{{node while_input_4/_12}}]]
	 [[Func/while/body/_1/input/_47/_20]]
2019-11-22 18:56:59.409640: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: No unary variant device copy function found for direction: 1 and Variant type_index: class tensorflow::data::`anonymous namespace'::DatasetVariantWrapper
	 [[{{node while_input_4/_12}}]]
Traceback (most recent call last):
  File "D:/work/python/PycharmProjects/tftest/test.py", line 83, in &lt;module&gt;
    f()
  File "D:\work\python\PycharmProjects\tftest\venv-gpu\lib\site-packages\tensorflow_core\python\eager\def_function.py", line 457, in __call__
    result = self._call(*args, **kwds)
  File "D:\work\python\PycharmProjects\tftest\venv-gpu\lib\site-packages\tensorflow_core\python\eager\def_function.py", line 526, in _call
    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access
  File "D:\work\python\PycharmProjects\tftest\venv-gpu\lib\site-packages\tensorflow_core\python\eager\function.py", line 1141, in _filtered_call
    self.captured_inputs)
  File "D:\work\python\PycharmProjects\tftest\venv-gpu\lib\site-packages\tensorflow_core\python\eager\function.py", line 1224, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager)
  File "D:\work\python\PycharmProjects\tftest\venv-gpu\lib\site-packages\tensorflow_core\python\eager\function.py", line 511, in call
    ctx=ctx)
  File "D:\work\python\PycharmProjects\tftest\venv-gpu\lib\site-packages\tensorflow_core\python\eager\execute.py", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File "&lt;string&gt;", line 3, in raise_from
tensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.
  (0) Internal:  No unary variant device copy function found for direction: 1 and Variant type_index: class tensorflow::data::`anonymous namespace'::DatasetVariantWrapper
	 [[{{node while_input_4/_12}}]]
	 [[Func/while/body/_1/input/_47/_20]]
  (1) Internal:  No unary variant device copy function found for direction: 1 and Variant type_index: class tensorflow::data::`anonymous namespace'::DatasetVariantWrapper
	 [[{{node while_input_4/_12}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_f_61]

Function call stack:
f -&gt; f


Process finished with exit code 1
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='SSSxCCC' date='2019-11-23T18:52:44Z'>
		&lt;denchmark-link:https://github.com/SSSxCCC&gt;@SSSxCCC&lt;/denchmark-link&gt;
 cannot reproduce the issue
&lt;denchmark-link:https://user-images.githubusercontent.com/24864163/69483691-6b625380-0e50-11ea-8eb7-04affe81ad53.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='SSSxCCC' date='2019-11-24T03:22:48Z'>
		&lt;denchmark-link:https://github.com/srihari-humbarwadi&gt;@srihari-humbarwadi&lt;/denchmark-link&gt;
 thank you for your reply! Are you sure your tf is tensorflow-gpu? When I pip install tensorflow, the code works fine, but when I pip install tensorflow-gpu, the code still raise the exception I mentioned.
		</comment>
		<comment id='3' author='SSSxCCC' date='2019-11-25T06:32:53Z'>
		&lt;denchmark-link:https://github.com/SSSxCCC&gt;@SSSxCCC&lt;/denchmark-link&gt;
 can reproduce with . Removing  seems to fix the issue which is odd, here is the notebook
&lt;denchmark-link:https://colab.research.google.com/drive/1PfiaUAFGD2g-Pbi6Lrd9_hb9dkY0KJcQ&gt;https://colab.research.google.com/drive/1PfiaUAFGD2g-Pbi6Lrd9_hb9dkY0KJcQ&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='SSSxCCC' date='2019-11-25T10:32:25Z'>
		Issue replicating for TF-gpu-2.0. works fine with CPU, kindly find the &lt;denchmark-link:https://colab.sandbox.google.com/gist/oanush/982a0c8f3d937897842b63570c587120/copy-of-untitled.ipynb&gt;gist&lt;/denchmark-link&gt;
 of colab.Thanks!
		</comment>
		<comment id='5' author='SSSxCCC' date='2019-12-19T19:00:53Z'>
		Hi &lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
 could you help to take a look? Thanks
		</comment>
		<comment id='6' author='SSSxCCC' date='2019-12-19T20:21:32Z'>
		As far as I can tell, this is a op placement issue -- the tf.print op will be placed on GPU because tf.range will be placed on GPU (which is the default policy for ops that have GPU kernel in the presence of GPU), but the tf.data input will reside on CPU. The right thing to do is for the tf.print op to be placed on CPU.
&lt;denchmark-link:https://github.com/mhong&gt;@mhong&lt;/denchmark-link&gt;
 could you please triage this to someone on the runtime team to investigate. Thank you.
		</comment>
		<comment id='7' author='SSSxCCC' date='2019-12-23T19:43:51Z'>
		&lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
 Thanks for the information. I will look into this issue.
		</comment>
		<comment id='8' author='SSSxCCC' date='2019-12-28T17:16:37Z'>
		CC &lt;denchmark-link:https://github.com/cheshire&gt;@cheshire&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='SSSxCCC' date='2019-12-30T02:23:35Z'>
		After investigating this bug, I found the following that may help explain the root cause.
After the op placement, the graph contains the follow connected line of nodes, where each node is uni-directionally connected to the next node
node_1: {name="while_input_4", op="_Arg", device="CPU"}
node_2: {name="while/enter/_6", op="Enter", device="GPU"}
node_3: {name="while/merge/_12", op="Merge", device="GPU"}
node_4: {name="while/while_input_4_switch/_19", op="Switch", device="GPU"}
node_5: {name="Func/while/body/_1/input/_49", op="Identity", device="GPU"}
node_6: {name="while/body/_1/ScanDataset", op="Scandataset", device="CPU"}.
node_6 also contains a sub-graph which has the "PrintV2Op". Therefore the first "_Arg" op, the "Scandataset" op as well as the "PrintV2Op" op are all correctly placed on CPU.
The issue is that control flow ops such as node_2 are placed on GPU. The edge that connects node_1 and node_2 has dtype=DT_VARIANT and the underlying datatype = DatasetVariantWrapper. DatasetVariantWrapper does not have encoder/decoder necessary for host-to-device transfer. Therefore we get the above errors.
How to fix this issue
It seems that in this case, the control flow ops (i.e. node_2, node_3, node_4, node_5) should all be placed on the CPU.
One idea is that, if an edge connects from a control flow op to a dataset op, then we should co-locate the control flow op with dataset op.
It will be great if people with more expertise in control flow ops and dataset ops can comment on this.
		</comment>
		<comment id='10' author='SSSxCCC' date='2019-12-30T19:55:49Z'>
		I don't know how much work this will be to retrofit into the current system, but in theory control flow ops like merge and switch should be "device polymorphic" on their data input/outputs since they just forward  the input tensors to the output tensors.
		</comment>
		<comment id='11' author='SSSxCCC' date='2019-12-30T20:06:17Z'>
		The problem here is in an assumption that the placer algorithm makes -- that any tensor can be copied between different devices -- which is generally not true (e.g. for dataset variant tensors). The proper way to fix this would be to make sure that placer only allows placement that collocate a node with its non-copyable inputs.
		</comment>
		<comment id='12' author='SSSxCCC' date='2019-12-30T20:45:11Z'>
		In this specific case, the error happens when:

An edge connects two device
The edge has dtype = DT_VARIANT.
The underlying data type (e.g. tensor::flat().data()[0].TypeName()) is not registered in UnaryVariantOpRegistry.

The issue here is that, the underlying data type is known only when we start to run the graph. This makes it hard for the placement algorithm to decide whether this edge is allowed. Is there is any way to get such information in the graph compile time?
There seems to be two high level solutions. One solution is to let the underlying nodes expose proper information (e.g. via colocation attributes) to the placer algorithm. Another solution is to let control flow ops be device polymorphic.  Either way, we will need expertise in dataset ops or the control flow ops.
		</comment>
		<comment id='13' author='SSSxCCC' date='2020-02-11T16:49:53Z'>
		&lt;denchmark-link:https://github.com/sanjoy&gt;@sanjoy&lt;/denchmark-link&gt;
 I agree; sadly TF doesn't let you specify a kernel which doesn't look at its tensor content (and I don't know how to fold that information in to the current placer algorithm).
&lt;denchmark-link:https://github.com/lindong28&gt;@lindong28&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jsimsa&gt;@jsimsa&lt;/denchmark-link&gt;
 should we add a registry of variant-ops-whose-outputs-cannot-be-copied and seed that with the tf.data ops then?
&lt;denchmark-link:https://github.com/saxenasaurabh&gt;@saxenasaurabh&lt;/denchmark-link&gt;
 maybe we can avoid this placing business altogether if we can delay inlining the control flow functions until after placement?
		</comment>
		<comment id='14' author='SSSxCCC' date='2020-02-11T18:39:18Z'>
		&lt;denchmark-link:https://github.com/alextp&gt;@alextp&lt;/denchmark-link&gt;
 I don't understand your suggestion. IIUC we must place the lowered control flow ops so we will still need update the placement logic?
&lt;denchmark-link:https://github.com/ezhulenev&gt;@ezhulenev&lt;/denchmark-link&gt;
 probably understands the placement logic better. Looking at the example here it seems we need to recognize patterns like Input(CPU) -&gt; Enter -&gt; Merge -&gt; Switch -&gt; Identity -&gt; Consumer (CPU) and if so place the entire chain(-s in case of nesting) on CPU?
		</comment>
		<comment id='15' author='SSSxCCC' date='2020-02-11T18:42:36Z'>
		The lowered ops are currently placed on the wrong device. If we had device
assignments for the ops before/after each lowered op when we added the
lowered op we could add them on a device which would minimize communication.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Tue, Feb 11, 2020 at 10:39 AM Saurabh Saxena ***@***.***&gt; wrote:
 @alextp &lt;https://github.com/alextp&gt; I don't understand your suggestion.
 IIUC we must place the lowered control flow ops so we will still need
 update the placement logic?

 @ezhulenev &lt;https://github.com/ezhulenev&gt; probably understands the
 placement logic better. Looking at the example here it seems we need to
 recognize patterns like Input(CPU) -&gt; Enter -&gt; Merge -&gt; Switch -&gt; Identity
 -&gt; Consumer (CPU) and if so place the entire chain(-s in case of nesting)
 on CPU?

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#34519?email_source=notifications&amp;email_token=AAABHROLCIIYZQXBITFQNRLRCLWF5A5CNFSM4JQPHXJ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELNSMCI#issuecomment-584787465&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/AAABHRLRLIXZULXO45B4XMTRCLWF5ANCNFSM4JQPHXJQ&gt;
 .


-- 
 - Alex

		</comment>
		<comment id='16' author='SSSxCCC' date='2020-02-11T18:52:25Z'>
		I see. Yeah that would be simpler than trying to handle this in the placement code I agree.
		</comment>
		<comment id='17' author='SSSxCCC' date='2020-03-30T11:23:11Z'>
		&lt;denchmark-link:https://github.com/SSSxCCC&gt;@SSSxCCC&lt;/denchmark-link&gt;
, Seems to be fixed in TF-gpu==2.2.rc1. Please find the gist &lt;denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/9827e15c992a573346829d512197d275/untitled480.ipynb&gt;here&lt;/denchmark-link&gt;
. Thanks
		</comment>
		<comment id='18' author='SSSxCCC' date='2020-04-09T08:02:52Z'>
		Hi,
The notebook linked above (&lt;denchmark-link:https://colab.research.google.com/gist/gadagashwini/9827e15c992a573346829d512197d275/untitled480.ipynb#scrollTo=tIcffx9fJXAv&gt;there&lt;/denchmark-link&gt;
) still outputs the same error for me, both with TF 2.2.0-rc1 and rc2 with GPU enabled. So I'm not sure that it has been fixed.
		</comment>
		<comment id='19' author='SSSxCCC' date='2020-04-12T04:27:25Z'>
		I met the similar issue when using tf.range loop and tf.dataset on GPU. I have tried with 2.2.0rc1/2 but with no luck (as mentioned by &lt;denchmark-link:https://github.com/mgoutay&gt;@mgoutay&lt;/denchmark-link&gt;
 ).
		</comment>
		<comment id='20' author='SSSxCCC' date='2020-04-13T10:02:47Z'>
		@houtoms, Can you share the colab gist.
		</comment>
		<comment id='21' author='SSSxCCC' date='2020-04-13T17:01:12Z'>
		I just changed the tf version to 2.2rc2 in the above colab link and it fails &lt;denchmark-link:https://colab.research.google.com/drive/102xUthT3hSfyTuoX-WD3NsYASLXRboQc&gt;https://colab.research.google.com/drive/102xUthT3hSfyTuoX-WD3NsYASLXRboQc&lt;/denchmark-link&gt;

		</comment>
		<comment id='22' author='SSSxCCC' date='2020-04-14T21:08:52Z'>
		Just checked with 2.2rc3 but the issue is still on. The colab link is same with the above.
		</comment>
		<comment id='23' author='SSSxCCC' date='2020-04-14T21:20:23Z'>
		Similar problem was fixed in &lt;denchmark-link:https://github.com/tensorflow/tensorflow/commit/eaa1729a52952f2a541491a1ce2c34af7ab66fc8#diff-e0dcdefdcbc0c9884e4333c1723ef40d&gt;eaa1729#diff-e0dcdefdcbc0c9884e4333c1723ef40d&lt;/denchmark-link&gt;
, if a placer can prove that DT_VARIANT has host only underlying type, it will place all dependent ops on CPU. However in this case DT_VARIANT tensor passed as an input to the function, and placer can't tell anything about the underlying host type.
&lt;denchmark-code&gt;Graph Before calling Placer #nodes 100 #edges 163

(n2:variant@CPU:0) -&gt; () {
   .....
   n22 = Enter[T=variant, frame_name="while", is_constant=false, parallel_iterations=10](n2)
   .....
}
&lt;/denchmark-code&gt;

		</comment>
		<comment id='24' author='SSSxCCC' date='2020-04-21T22:08:30Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
		</comment>
		<comment id='25' author='SSSxCCC' date='2020-04-28T23:06:37Z'>
		Closing as stale. Please reopen if you'd like to work on this further.
		</comment>
		<comment id='26' author='SSSxCCC' date='2020-04-28T23:06:40Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34519&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34519&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='27' author='SSSxCCC' date='2020-05-29T19:07:45Z'>
		Hi,
This is probably not monitored actively, but I am facing the same issue and I managed to hack my way through:
import tensorflow as tf
dataset = tf.data.Dataset.from_tensor_slices(([1., 2, 3], [4., 5, 6]))

with tf.device('/GPU:0'):
    var = tf.ones([500, 50])

@tf.function
def f():
  with tf.device('/CPU:0'):
      iterator = iter(dataset)
  for e in tf.range(3):
      for x, y in iterator:
          z = tf.multiply(x, y, name='mul')
          tf.print(tf.reduce_sum(z * var))
f()
EDIT: Not really sure where the reduce_sum op takes place though, it's not logged by log_device_placement.
		</comment>
		<comment id='28' author='SSSxCCC' date='2020-07-27T13:49:25Z'>
		Thank you &lt;denchmark-link:https://github.com/AdrienCorenflos&gt;@AdrienCorenflos&lt;/denchmark-link&gt;
 for adding your solution!
		</comment>
		<comment id='29' author='SSSxCCC' date='2020-09-02T19:53:57Z'>
		This issue should be fixed as of TF 2.3.
		</comment>
		<comment id='30' author='SSSxCCC' date='2020-09-03T04:48:16Z'>
		Hey Rachel and everyone, I think the issue still exists in TF 2.3. But I validated that it has been fixed in tf-nightly==2.4.0.dev20200902 and thus it should be fixed in the next TF 2.4 release.
It can be validated using this colab example: &lt;denchmark-link:https://colab.research.google.com/drive/1C4AXS8cyEsA_JmU7QVVk7UXM_uygVf05?authuser=1#scrollTo=jJBsJsRSR6AD&gt;https://colab.research.google.com/drive/1C4AXS8cyEsA_JmU7QVVk7UXM_uygVf05?authuser=1#scrollTo=jJBsJsRSR6AD&lt;/denchmark-link&gt;

		</comment>
		<comment id='31' author='SSSxCCC' date='2020-09-03T19:10:16Z'>
		This specific issue (dataset defined outside the @tf.function, and used inside a loop in the tf.function) has been fixed as of TF 2.3. There are still remaining issues with a subtly different setup (dataset defined  the @tf.function and used inside a loop in the tf.function), see &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/34112&gt;#34112&lt;/denchmark-link&gt;
.
		</comment>
	</comments>
</bug>