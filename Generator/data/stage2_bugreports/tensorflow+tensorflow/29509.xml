<bug id='29509' author='lwu025' open_date='2019-06-06T18:47:01Z' closed_time='2020-01-30T22:50:07Z'>
	<summary>How to convert a tensorlfow SpaceToBatchND-Conv2D-BatchToSpaceND to a single Conv2D in tflite</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):NO
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA
TensorFlow installed from (source or binary):source
TensorFlow version (use command below):1.13.1
Python version: 2.7
Bazel version (if compiling from source): 0.22.0
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 

I'm trying to train my own deeplab model using this &lt;denchmark-link:https://github.com/tensorflow/models/tree/master/research/deeplab&gt;code&lt;/denchmark-link&gt;
 and convert it to tflite.
My target is to get a model similar to &lt;denchmark-link:https://storage.googleapis.com/download.tensorflow.org/models/tflite/gpu/deeplabv3_257_mv_gpu.tflite&gt;this&lt;/denchmark-link&gt;

However, the model is obtained contains operations like:
&lt;denchmark-link:https://user-images.githubusercontent.com/43549654/59057361-135a7500-884f-11e9-9546-e2bd20e69c95.png&gt;&lt;/denchmark-link&gt;

SpaceToBatchND and BatchToSpaceND operations are not supported by tflite + opengles backend, they reduced the model's performance on my device.
In your hosted deeplab model, those three ops are replaced by DEPTHWISE_CONV_2D v2, which has options to set dilation factor. This would be the best solution for me but I'm not sure how to convert SpaceToBatchND-Conv2D-BatchToSpaceND into a singe DEPTHWISE_CONV_2D v2(dilation=2).
FYI, I have tried the graph_transforms tool under tensorflow/tools/graph_transforms to flatten the atrous conv. It upsampled the kernels instead of Space_To_Batch + Batch_To_Space. But this transform leads to much more computations that I cannot afford.
Describe the expected behavior
convert SpaceToBatchND-Conv2D-BatchToSpaceND into a singe DEPTHWISE_CONV_2D v2(dilation=2)
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
You can try any model under deeplab model zoo for example &lt;denchmark-link:url&gt;http://download.tensorflow.org/models/deeplabv3_mnv2_ade20k_train_2018_12_03.tar.gz&lt;/denchmark-link&gt;

Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='lwu025' date='2019-09-09T13:21:37Z'>
		Hey, I am currently facing the same problem. One thing that I noticed is when working with quantized model this conversion is being done. Problem is then I end up with a uint8 model.
		</comment>
		<comment id='2' author='lwu025' date='2019-10-10T18:29:03Z'>
		TOCO has a pass to do this kind of transformation:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/toco/graph_transformations/identify_dilated_conv.cc&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/toco/graph_transformations/identify_dilated_conv.cc&lt;/denchmark-link&gt;

Isn't it working in your case?
		</comment>
		<comment id='3' author='lwu025' date='2020-01-17T10:42:00Z'>
		I'm seeing the same issue in TF 2.0.0 and TF2.1.0. This makes e.g. Deeplab V3 effectively not runnable on GPU as it relies extensively on atrous convolutions.
		</comment>
		<comment id='4' author='lwu025' date='2020-01-17T17:34:42Z'>
		
I'm seeing the same issue in TF 2.0.0 and TF2.1.0. This makes e.g. Deeplab V3 effectively not runnable on GPU as it relies extensively on atrous convolutions.

Are you using the old converter or the new MLIR-based converter?
		</comment>
		<comment id='5' author='lwu025' date='2020-01-18T02:59:01Z'>
		Seems to happen regardless of which converter is chosen. I can ask my client to provide an untrained SavedModel as a repro, if that helps.
		</comment>
		<comment id='6' author='lwu025' date='2020-01-18T03:11:34Z'>
		I'm pretty sure it would repro with segmentation specific variant of MobileNet V3, i.e. this one: &lt;denchmark-link:https://github.com/1e100/mobilenet_v3/blob/master/mobilenet_v3_keras.py&gt;https://github.com/1e100/mobilenet_v3/blob/master/mobilenet_v3_keras.py&lt;/denchmark-link&gt;
, using "large_segmentation" config: &lt;denchmark-link:https://github.com/1e100/mobilenet_v3/blob/master/mobilenet_v3_configs.py#L69&gt;https://github.com/1e100/mobilenet_v3/blob/master/mobilenet_v3_configs.py#L69&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='7' author='lwu025' date='2020-01-20T02:33:58Z'>
		This is where it bails in my case (in identify_dilated_conv.cc):
&lt;denchmark-code&gt;120   Operator* bias_add_op = !has_bias_before_bts ? final_op : next_op;
121   if (bias_add_op-&gt;type != OperatorType::kAdd) {
122     // Bias op is required before or after BatchToSpace
123     return false;
124   }
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='lwu025' date='2020-01-20T03:21:09Z'>
		Upon export, a  MobileNet V3 block turns into this:
&lt;denchmark-link:https://user-images.githubusercontent.com/46361887/72696297-7eac3b00-3af0-11ea-8cf6-d45bfc7e40be.png&gt;&lt;/denchmark-link&gt;

In its original form, the block is defined as follows: &lt;denchmark-link:https://github.com/1e100/mobilenet_v3/blob/master/mobilenet_v3_keras.py#L155&gt;https://github.com/1e100/mobilenet_v3/blob/master/mobilenet_v3_keras.py#L155&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='lwu025' date='2020-01-20T03:25:24Z'>
		It expects that there will be bias add either before or after BatchToSpaceNd, but it looks like the bias is folded into DepthwiseConv2D. What that Mul is doing there, I don't know.
		</comment>
		<comment id='10' author='lwu025' date='2020-01-20T04:06:18Z'>
		Minimal repro:
#!/usr/bin/env python3

import pathlib

import tensorflow as tf
from tensorflow import keras

input = keras.Input([128, 128, 3])
x = keras.layers.Conv2D(8, 5, dilation_rate=2, padding="same", use_bias=False)(input)
x = keras.layers.BatchNormalization()(x)
output = keras.layers.ReLU()(x)

m = keras.Model(inputs=input, outputs=output)
out_dir = pathlib.Path("/tmp/minimal_bug_repro")
m.save(str(out_dir), save_format="tf")

converter = tf.lite.TFLiteConverter.from_saved_model(str(out_dir))
tflite_model = converter.convert()
output_file = out_dir / "model.tflite"
output_file.write_bytes(tflite_model)

print(f"Converted model was written to {output_file}")
You get the following on the other end:
&lt;denchmark-link:https://user-images.githubusercontent.com/46361887/72698007-1b71d700-3af7-11ea-9176-bf05f210c17d.png&gt;&lt;/denchmark-link&gt;

Which is not runnable on the GPU.
		</comment>
		<comment id='11' author='lwu025' date='2020-01-21T19:36:35Z'>
		Thanks &lt;denchmark-link:https://github.com/depthwise&gt;@depthwise&lt;/denchmark-link&gt;
 for the repro example.
I'm working on adding support of dilated conv into the MLIR-based converter. I will update this thread when I'm finished.
		</comment>
		<comment id='12' author='lwu025' date='2020-01-28T23:42:22Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29509&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29509&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='lwu025' date='2020-01-29T11:56:28Z'>
		FYI, &lt;denchmark-link:https://github.com/haozha111&gt;@haozha111&lt;/denchmark-link&gt;
, this PR improved the issue, but did not fully fix it. Here's an updated repro which demonstrates the remaining issue:
&lt;denchmark-code&gt;#!/usr/bin/env python3

import pathlib

import tensorflow as tf
from tensorflow import keras

input = keras.Input([128, 128, 3])
x1 = keras.layers.Conv2D(8, 5, dilation_rate=6, padding="same", use_bias=False)(input)
x1 = keras.layers.BatchNormalization()(x1)
output1 = keras.layers.ReLU()(x1)

x2 = keras.layers.Conv2D(8, 5, dilation_rate=12, padding="same", use_bias=False)(input)
x2 = keras.layers.BatchNormalization()(x2)
output1 = keras.layers.ReLU()(x2)

output = tf.concat([x1, x2], axis=3)

m = keras.Model(inputs=input, outputs=output)
out_dir = pathlib.Path("/tmp/minimal_bug_repro")
m.save(str(out_dir), save_format="tf")

converter = tf.lite.TFLiteConverter.from_saved_model(str(out_dir))
tflite_model = converter.convert()
output_file = out_dir / "model.tflite"
output_file.write_bytes(tflite_model)

print(f"Converted model was written to {output_file}")
&lt;/denchmark-code&gt;

This produces the following:
&lt;denchmark-link:https://user-images.githubusercontent.com/46361887/73354717-3bb63a00-424b-11ea-9767-5762e6863f05.png&gt;&lt;/denchmark-link&gt;

Each of the convs in isolation is fixed, but if I concatenate them we're back to the status quo ante.
		</comment>
		<comment id='14' author='lwu025' date='2020-01-29T11:57:07Z'>
		Such dilations are commonly used in the ASPP module of segmentation models.
		</comment>
		<comment id='15' author='lwu025' date='2020-01-29T22:53:45Z'>
		That's a bit interesting. I haven't tested for this case.
So do you mean if you have only one conv2d in your graph, then it can be correctly folded?
		</comment>
		<comment id='16' author='lwu025' date='2020-01-29T22:58:27Z'>
		Looks like conv2d's by themselves work fine. The simpler repro I posted before looks "correct", although I have not tested this in a full blown, trained model yet.
		</comment>
		<comment id='17' author='lwu025' date='2020-01-30T00:22:24Z'>
		I tested with your new code, and convert it. Then I'm getting the tflite graph looks like the following:
&lt;denchmark-link:https://user-images.githubusercontent.com/6316921/73408979-bbc3ba80-42b2-11ea-91ab-46f8a05219f5.png&gt;&lt;/denchmark-link&gt;

I also have the corresponding tflite file, but not sure how to attach it here.
This suggests that the graph is converted as expected. I'm wondering if my previous change has been pushed into the nightly already. Maybe you can download tomorrow's nightly and give a try.
		</comment>
		<comment id='18' author='lwu025' date='2020-01-30T00:25:48Z'>
		OK, I'll try again. My example was converted using a build directly from master as of last night. Maybe something else got submitted in the interim.
		</comment>
		<comment id='19' author='lwu025' date='2020-01-30T00:30:43Z'>
		did you set converter.experimental_new_converter = True between those two lines?
converter = tf.lite.TFLiteConverter.from_saved_model(str(out_dir)) tflite_model = converter.convert()
		</comment>
		<comment id='20' author='lwu025' date='2020-01-30T04:40:35Z'>
		I'm actually converting as follows, using the master branch as of this afternoon:
bazel run --copt="-Wno-unused-result" :tflite_convert -- \
    --saved_model_dir=/tmp/minimal_bug_repro \
    --experimental_new_converter=True --output_file=/tmp/minimal_bug_repro/model.tflite
The original savedmodel is created with TF 2.1.0 release version, the conversion is done with master tflite_convert.
Here's the original model and its conversion, in case it's useful in debugging: &lt;denchmark-link:https://storage.googleapis.com/depthwise-temp/minimal_bug_repro.zip&gt;https://storage.googleapis.com/depthwise-temp/minimal_bug_repro.zip&lt;/denchmark-link&gt;

		</comment>
		<comment id='21' author='lwu025' date='2020-01-30T04:41:31Z'>
		For me the graph looks like the image I posted previously, incorrect.
		</comment>
		<comment id='22' author='lwu025' date='2020-01-30T19:46:44Z'>
		I could reproduce the issue with your saved model, but the graph I posted before is from the keras testing code. I'm not sure why there is a difference here.
Digging it further, I found that there is a difference in the MLIR representation of the two models. For saved model, the IR looks like:
...
%10 = "tf.Identity"(%arg0) {T = f32, device = ""} : (tensor&lt;1x128x128x3xf32&gt;) -&gt; tensor&lt;1x128x128x3xf32&gt;
%11 = "tf.Identity"(%10) {T = f32, device = ""} : (tensor&lt;1x128x128x3xf32&gt;) -&gt; tensor&lt;1x128x128x3xf32&gt;
%12 = "tf.SpaceToBatchND"(%11, %0, %1) {T = f32, Tblock_shape = i32, Tpaddings = i32, device = ""} : (tensor&lt;1x128x128x3xf32&gt;, tensor&lt;2xi32&gt;, tensor&lt;2x2xi32&gt;) -&gt; tensor&lt;36x26x26x3xf32&gt;
%13 = "tf.SpaceToBatchND"(%11, %3, %4) {T = f32, Tblock_shape = i32, Tpaddings = i32, device = ""} : (tensor&lt;1x128x128x3xf32&gt;, tensor&lt;2xi32&gt;, tensor&lt;2x2xi32&gt;) -&gt; tensor&lt;144x15x15x3xf32&gt;
%14 = "tf.Conv2D"(%12, %8) {T = f32, data_format = "NHWC", device = "", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = "VALID", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor&lt;36x26x26x3xf32&gt;, tensor&lt;5x5x3x8xf32&gt;) -&gt; tensor&lt;36x22x22x8xf32&gt;
%15 = "tf.BatchToSpaceND"(%14, %0, %2) {T = f32, Tblock_shape = i32, Tcrops = i32, device = ""} : (tensor&lt;36x22x22x8xf32&gt;, tensor&lt;2xi32&gt;, tensor&lt;2x2xi32&gt;) -&gt; tensor&lt;1x128x128x8xf32&gt;
%y, %batch_mean, %batch_variance, %reserve_space_1, %reserve_space_2, %reserve_space_3 = "tf.FusedBatchNormV3"(%15, %6, %7, %7, %6) {T = f32, U = f32, data_format = "NHWC", device = "", epsilon = 1.000000e-03 : f32, is_training = false} : (tensor&lt;1x128x128x8xf32&gt;, tensor&lt;8xf32&gt;, tensor&lt;8xf32&gt;, tensor&lt;8xf32&gt;, tensor&lt;8xf32&gt;) -&gt; (tensor&lt;1x128x128x8xf32&gt;, tensor&lt;8xf32&gt;, tensor&lt;8xf32&gt;, tensor&lt;8xf32&gt;, tensor&lt;8xf32&gt;, tensor&lt;*xf32&gt;)
%16 = "tf.Conv2D"(%13, %9) {T = f32, data_format = "NHWC", device = "", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = "VALID", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor&lt;144x15x15x3xf32&gt;, tensor&lt;5x5x3x8xf32&gt;) -&gt; tensor&lt;144x11x11x8xf32&gt;
%17 = "tf.BatchToSpaceND"(%16, %3, %2) {T = f32, Tblock_shape = i32, Tcrops = i32, device = ""} : (tensor&lt;144x11x11x8xf32&gt;, tensor&lt;2xi32&gt;, tensor&lt;2x2xi32&gt;) -&gt; tensor&lt;1x128x128x8xf32&gt;
%18 = "tf.Mul"(%17, %cst) : (tensor&lt;1x128x128x8xf32&gt;, tensor&lt;8xf32&gt;) -&gt; tensor&lt;1x128x128x8xf32&gt;
%19 = "tf.Add"(%18, %cst_0) : (tensor&lt;1x128x128x8xf32&gt;, tensor&lt;8xf32&gt;) -&gt; tensor&lt;1x128x128x8xf32&gt;
%20 = "tf.ConcatV2"(%y, %19, %5) {N = 2 : i64, T = f32, Tidx = i32, _cloned = true, device = ""} : (tensor&lt;1x128x128x8xf32&gt;, tensor&lt;1x128x128x8xf32&gt;, tensor) -&gt; tensor&lt;1x128x128x16xf32&gt;
return %20 : tensor&lt;1x128x128x16xf32&gt;
}&lt;144x11x11x8xf32&gt;, tensor&lt;2xi32&gt;, tensor&lt;2x2xi32&gt;) -&gt; tensor&lt;1x128x128x8xf32&gt;
%18 = "tf.Mul"(%17, %cst) : (tensor&lt;1x128x128x8xf32&gt;, tensor&lt;8xf32&gt;) -&gt; tensor&lt;1x128x128x8xf32&gt;
%19 = "tf.Add"(%18, %cst_0) : (tensor&lt;1x128x128x8xf32&gt;, tensor&lt;8xf32&gt;) -&gt; tensor&lt;1x128x128x8xf32&gt;
%20 = "tf.ConcatV2"(%y, %19, %5) {N = 2 : i64, T = f32, Tidx = i32, _cloned = true, device = ""} : (tensor&lt;1x128x128x8xf32&gt;, tensor&lt;1x128x128x8xf32&gt;, tensor) -&gt; tensor&lt;1x128x128x16xf32&gt;
return %20 : tensor&lt;1x128x128x16xf32&gt;
}
For the keras test model, the IR is:
%7 = "tf.Const"() {value = dense&lt;12&gt; : tensor&lt;2xi32&gt;} : () -&gt; tensor&lt;2xi32&gt;
%8 = "tf.Const"() {value = dense&lt;[[24, 28], [24, 28]]&gt; : tensor&lt;2x2xi32&gt;} : () -&gt; tensor&lt;2x2xi32&gt;
%9 = "tf.Const"() {value = dense&lt;3&gt; : tensor} : () -&gt; tensor
%10 = "tf.Identity"(%arg0) {T = f32, device = ""} : (tensor&lt;1x128x128x3xf32&gt;) -&gt; tensor&lt;1x128x128x3xf32&gt;
%11 = "tf.SpaceToBatchND"(%10, %3, %4) {T = f32, Tblock_shape = i32, Tpaddings = i32, device = ""} : (tensor&lt;1x128x128x3xf32&gt;, tensor&lt;2xi32&gt;, tensor&lt;2x2xi32&gt;) -&gt; tensor&lt;36x26x26x3xf32&gt;
%12 = "tf.Conv2D"(%11, %2) {T = f32, data_format = "NHWC", device = "", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = "VALID", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor&lt;36x26x26x3xf32&gt;, tensor&lt;5x5x3x8xf32&gt;) -&gt; tensor&lt;36x22x22x8xf32&gt;
%13 = "tf.BatchToSpaceND"(%12, %3, %5) {T = f32, Tblock_shape = i32, Tcrops = i32, device = ""} : (tensor&lt;36x22x22x8xf32&gt;, tensor&lt;2xi32&gt;, tensor&lt;2x2xi32&gt;) -&gt; tensor&lt;1x128x128x8xf32&gt;
%y, %batch_mean, %batch_variance, %reserve_space_1, %reserve_space_2, %reserve_space_3 = "tf.FusedBatchNormV3"(%13, %0, %1, %1, %0) {T = f32, U = f32, data_format = "NHWC", device = "", epsilon = 1.000000e-03 : f32, is_training = false} : (tensor&lt;1x128x128x8xf32&gt;, tensor&lt;8xf32&gt;, tensor&lt;8xf32&gt;, tensor&lt;8xf32&gt;, tensor&lt;8xf32&gt;) -&gt; (tensor&lt;1x128x128x8xf32&gt;, tensor&lt;8xf32&gt;, tensor&lt;8xf32&gt;, tensor&lt;8xf32&gt;, tensor&lt;8xf32&gt;, tensor&lt;*xf32&gt;)
%14 = "tf.SpaceToBatchND"(%10, %7, %8) {T = f32, Tblock_shape = i32, Tpaddings = i32, device = ""} : (tensor&lt;1x128x128x3xf32&gt;, tensor&lt;2xi32&gt;, tensor&lt;2x2xi32&gt;) -&gt; tensor&lt;144x15x15x3xf32&gt;
%15 = "tf.Conv2D"(%14, %6) {T = f32, data_format = "NHWC", device = "", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = "VALID", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor&lt;144x15x15x3xf32&gt;, tensor&lt;5x5x3x8xf32&gt;) -&gt; tensor&lt;144x11x11x8xf32&gt;
%16 = "tf.BatchToSpaceND"(%15, %7, %5) {T = f32, Tblock_shape = i32, Tcrops = i32, device = ""} : (tensor&lt;144x11x11x8xf32&gt;, tensor&lt;2xi32&gt;, tensor&lt;2x2xi32&gt;) -&gt; tensor&lt;1x128x128x8xf32&gt;
%17 = "tf.Mul"(%16, %cst) : (tensor&lt;1x128x128x8xf32&gt;, tensor&lt;8xf32&gt;) -&gt; tensor&lt;1x128x128x8xf32&gt;
%18 = "tf.Add"(%17, %cst_0) : (tensor&lt;1x128x128x8xf32&gt;, tensor&lt;8xf32&gt;) -&gt; tensor&lt;1x128x128x8xf32&gt;
%19 = "tf.ConcatV2"(%y, %18, %9) {N = 2 : i64, T = f32, Tidx = i32, _cloned = true, device = ""} : (tensor&lt;1x128x128x8xf32&gt;, tensor&lt;1x128x128x8xf32&gt;, tensor) -&gt; tensor&lt;1x128x128x16xf32&gt;
return %19 : tensor&lt;1x128x128x16xf32&gt;
I think the issue is that the IR for saved model is a bit strange, notice those lines:
%13 = "tf.SpaceToBatchND"(%11, %3, %4) {T = f32, Tblock_shape = i32, Tpaddings = i32, device = ""} : (tensor&lt;1x128x128x3xf32&gt;, tensor&lt;2xi32&gt;, tensor&lt;2x2xi32&gt;) -&gt; tensor&lt;144x15x15x3xf32&gt;
%14 = "tf.Conv2D"(%12, %8) {T = f32, data_format = "NHWC", device = "", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = "VALID", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor&lt;36x26x26x3xf32&gt;, tensor&lt;5x5x3x8xf32&gt;) -&gt; tensor&lt;36x22x22x8xf32&gt;
%15 = "tf.BatchToSpaceND"(%14, %0, %2) {T = f32, Tblock_shape = i32, Tcrops = i32, device = ""} : (tensor&lt;36x22x22x8xf32&gt;, tensor&lt;2xi32&gt;, tensor&lt;2x2xi32&gt;) -&gt; tensor&lt;1x128x128x8xf32&gt;
The block_shape parameter before/after the Conv2D op is different, which causes the pattern to not match successfully. Will investigate why the IR is different.
		</comment>
		<comment id='23' author='lwu025' date='2020-01-30T20:01:20Z'>
		Hi, Nupur, do you know what might be the cause for the difference between calling the python API and the command line tool?
		</comment>
		<comment id='24' author='lwu025' date='2020-01-30T20:34:06Z'>
		I also tried to convert the model via command line 'tflite_convert' directly (no bazel) in tf-nightly, and the model could be converted correctly. I think the issue is probably with bazel, it might be calling the TF 1 saved model loader which doesn't produce the expected result.
Can you please try using tf-nightly and then run 'tflite_conveter' tool directly (please avoid using bazel)?
Thanks.
		</comment>
		<comment id='25' author='lwu025' date='2020-01-30T20:36:48Z'>
		Just to be clear, once again, the SavedModel I posted was saved using TF 2.1.0, not the new "master" build. It's only the converter that was built from master. Let me try to replicate this with a wheel of TF build off master to see if the model is being saved incorrectly in the first place. Build times being what they are, this will take a bit of time.
		</comment>
		<comment id='26' author='lwu025' date='2020-01-30T20:44:35Z'>
		I think the saved model is generated correctly, it doesn't matter if you generate it from TF 2.1.0 or the master build.
The issue is with the converter. When you build from master, and then run bazel, even if it's pulling the latest code, I think there is some mis-configuration in bazel that causes the model loading to incorrectly use the old TF 1.x codepath. So what I'm suggesting maybe the easiest approach is to download the tf-nightly, and then call 'tflite_convert' tool directly (in a virtualenv).
		</comment>
		<comment id='27' author='lwu025' date='2020-01-30T22:29:04Z'>
		Nightly as of last night LGTM, in both my full deeplab model and the minimal example. Thanks for the quick fix, much appreciated. This was blocking things in unpleasant ways.
		</comment>
		<comment id='28' author='lwu025' date='2020-01-30T22:50:07Z'>
		Great to hear that!
		</comment>
		<comment id='29' author='lwu025' date='2020-01-30T22:50:09Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29509&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29509&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>