<bug id='32394' author='dekromp' open_date='2019-09-10T21:21:03Z' closed_time='2019-09-13T11:53:36Z'>
	<summary>Memory Leak in tf.keras.Model.fit</summary>
	<description>
Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 1.14.0
Python version: 3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

You can collect some of this information using our environment capture
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh&gt;script&lt;/denchmark-link&gt;

You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
Describe the current behavior
During training of a keras model with fit, the memory consumption is constantly increasing until the machine eventually runs out of memory.
Note: The observed issue is only observed with tensorflow installed with conda or the one used in the AWS Deep Learning AMI that was optimized for the respective compute instance. If I just install tensorflow with pip the memory consumption does not increase until failure. Since I am not an expert on building tensorflow from source I can only guess that it has something to do with the way tensorflow was build from source by the package provider.
Describe the expected behavior
For a small training dataset of a couple of MB, a machine with 16 GiB memory should not run out of memory.
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
I used this code and a fresh conda environment to reproduce the issue:
$ conda create -n myenv python=3.6 tensorflow
$ conda activate myenv
"""Example that reproduces the memory consumption increase during training."""
import numpy as np
import tensorflow as tf


def build_model():
    """Build a simple logistic regression model."""
    x = tf.keras.Input((100,))
    h = tf.keras.layers.Flatten()(x)
    output = tf.keras.layers.Dense(1, activation='sigmoid')(h)
    return tf.keras.Model(inputs=[x], outputs=[output])


def load_data():
    """Load some dummy data."""
    x = np.random.randn(1000000, 100).astype(np.float32)
    y = np.random.choice([0, 1], size=(1000000, 1)).astype(np.float32)
    return x, y


def train(model, x, y):
    """Train the model on some data."""
    model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),
                  loss=tf.keras.losses.binary_crossentropy)
    model.fit(x, y, epochs=10000, batch_size=1280)


if __name__ == '__main__':
    x, y = load_data()
    model = build_model()
    train(model, x, y)
Other info / logs
Here are the logs from the tensorflow version installed with conda maybe it helps someone to reproduce how tensorflow was compiled and reproduce the issue:
&lt;denchmark-code&gt;OMP: Info #212: KMP_AFFINITY: decoding x2APIC ids.
OMP: Info #213: KMP_AFFINITY: cpuid leaf 11 not supported - decoding legacy APIC ids.
OMP: Info #149: KMP_AFFINITY: Affinity capable, using global cpuid info
OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-3
OMP: Info #156: KMP_AFFINITY: 4 available OS procs
OMP: Info #157: KMP_AFFINITY: Uniform topology
OMP: Info #159: KMP_AFFINITY: 1 packages x 1 cores/pkg x 4 threads/core (1 total cores)
OMP: Info #214: KMP_AFFINITY: OS proc to physical thread map:
OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 thread 0
OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 thread 1
OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 thread 2
OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 thread 3
OMP: Info #250: KMP_AFFINITY: pid 20360 tid 20360 thread 0 bound to OS proc set 0
WARNING: Logging before flag parsing goes to stderr.
W0910 21:18:07.612579 139883462330112 deprecation.py:506] From /home/ubuntu/anaconda3/envs/tfkerasbug/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0910 21:18:07.634243 139883462330112 deprecation.py:323] From /home/ubuntu/anaconda3/envs/tfkerasbug/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2019-09-10 21:18:07.824063: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2019-09-10 21:18:07.829994: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199910000 Hz
2019-09-10 21:18:07.830112: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e5e924d9b0 executing computations on platform Host. Devices:
2019-09-10 21:18:07.830142: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;
2019-09-10 21:18:07.830228: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2019-09-10 21:18:07.848855: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
OMP: Info #250: KMP_AFFINITY: pid 20360 tid 20372 thread 1 bound to OS proc set 2
OMP: Info #250: KMP_AFFINITY: pid 20360 tid 20374 thread 2 bound to OS proc set 1
OMP: Info #250: KMP_AFFINITY: pid 20360 tid 20375 thread 3 bound to OS proc set 3
OMP: Info #250: KMP_AFFINITY: pid 20360 tid 20376 thread 4 bound to OS proc set 0
KMP_AFFINITY: pid 20360 tid 20373 thread 5 bound to OS proc set 2
OMP: Info #250: KMP_AFFINITY: pid 20360 tid 20377 thread 6 bound to OS proc set 1
OMP: Info #250: KMP_AFFINITY: pid 20360 tid 20378 thread 7 bound to OS proc set 3
OMP: Info #250: KMP_AFFINITY: pid 20360 tid 20379 thread 8 bound to OS proc set 0
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='dekromp' date='2019-09-11T09:05:25Z'>
		&lt;denchmark-link:https://github.com/dekromp&gt;@dekromp&lt;/denchmark-link&gt;
 ,
When i tried installing tensorflow from  and run the given code,I didn't face any error. Can you provide more info on the issue? Also try running the code on latest tensorflow version.Thanks!
		</comment>
		<comment id='2' author='dekromp' date='2019-09-11T09:26:14Z'>
		How long did you run it? It takes some time until the increase in memory is clearly visible. On an AWS ec2 m4.xlarge instance (Ubuntu 18.04) it growths roughly 6MB/s. I don't think that my minimal example will cause an error on your machine just a constant increase in memory consumption which you can observe when you look at it for  ~10 minutes.
And as I said, I am not an expert in compiling tensorflow from source and do not know how conda/AWS compiled their version of tensorflow. By Installing tensorflow with pip, without any specific optimization everything is fine.
		</comment>
		<comment id='3' author='dekromp' date='2019-09-11T16:18:02Z'>
		Hi.
I have the same problem. I also installed TF2.0 with pip in a conda virtual environment on my computer. I tried to implement DQN with TF2.0. I found the memory would increase if I used model.fit, but it not with model.train_on_batch.
Here's my code:
&lt;denchmark-code&gt;from collections import deque
import random
import gym
import numpy as np
import tensorflow as tf

class DQN(object):
    def __init__(self, state_dim, action_dim, lr, init_buffer_size=100):
        self.step = 0
        self.update_freq = 200
        self.replay_buffer_size = 1000
        self.replay_buffer = deque(maxlen=self.replay_buffer_size)
        self.lr = lr
        self.model = self._create_model(state_dim, action_dim)
        self.target_model = self._create_model(state_dim, action_dim)
        self._state_dim = state_dim
        self._action_dim = action_dim
        self._init_buffer_size = init_buffer_size

    def _create_model(self, state_dim, action_dim):
        inputs = tf.keras.Input(shape=state_dim)
        fc1 = tf.keras.layers.Dense(512)(inputs)
        q_outs = tf.keras.layers.Dense(action_dim)(fc1)

        model = tf.keras.Model(inputs=inputs, outputs=q_outs)
        model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(self.lr))

        return model

    def act(self, state, epsilon=0.1):
        if np.random.uniform() &lt; epsilon:
            return np.random.choice(range(self._action_dim))
        else:
            return np.argmax(self.model(np.array([state])))

    def save_model(self, file_path='MountainCar-v0-dqn.h5'):
        print('model saved')
        self.model.save(file_path)

    def save_transition(self, s, a, r, next_s, terminated):
        self.replay_buffer.append((s, a, r, next_s, terminated))

    def train(self, batch_size=64, lr=1, gamma=0.99):
        if len(self.replay_buffer) &lt; self._init_buffer_size:
            return

        self.step += 1
        
        if self.step % self.update_freq == 0:
            print("Copying weights.")
            self.target_model.set_weights(self.model.get_weights())
        
        # print("Buffer Length:{}".format(len(self.replay_buffer)))

        minibatch = random.sample(self.replay_buffer, batch_size)
        s_batch = np.array([replay[0] for replay in minibatch])
        next_s_batch = np.array([replay[3] for replay in minibatch])

        
        Q = self.model(s_batch).numpy()
        target_Q = self.target_model(next_s_batch).numpy()

        for i, exp in enumerate(minibatch):
            _, a, r, _, done = exp
            if done:
                Q[i][a] = r
            else:
                Q[i][a] = r + gamma * np.amax(target_Q[i])

        self.model.fit(s_batch, Q, verbose=0)
        # self.model.train_on_batch(s_batch, Q)

def main():
    env = gym.make('MountainCar-v0')
    episodes = 10000  # train with 1000 episodes
    score_list = []
    agent = DQN(state_dim=env.observation_space.shape, action_dim=env.action_space.n, lr=0.001)
    
    for i in range(episodes):
        s = env.reset()
        score = 0
        while True:
            a = agent.act(s)
            # env.render()
            next_s, reward, done, _ = env.step(a)
            agent.save_transition(s, a, reward, next_s, done)
            agent.train()
            score += reward
            s = next_s
            if done:
                score_list.append(score)
                print('episode:', i, 'score:', score, 'max:', max(score_list))
                break
        if np.mean(score_list[-10:]) &gt; -160:
            agent.save_model()
            break
        
    env.close()

if __name__ == "__main__":
    main()
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='dekromp' date='2019-09-13T11:53:36Z'>
		We only control the pip package. If you're seeing an issue with conda but not pip I would suggest that you file an issue with conda.
		</comment>
		<comment id='5' author='dekromp' date='2019-09-13T11:53:37Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=32394&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=32394&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='dekromp' date='2019-09-13T15:04:53Z'>
		Not really helpful, but thanks anyway.
		</comment>
		<comment id='7' author='dekromp' date='2020-01-08T21:09:17Z'>
		&lt;denchmark-link:https://github.com/robieta&gt;@robieta&lt;/denchmark-link&gt;

Hi,  not sure why this got closed just because conda was used instead of pip. I'm also running into issues with this (using TF 2.0.0, CPU only on RedHat) and I installed with conda. I'd be happy to show some sample code to replicate and the metrics I've collected from trying to debug this memory lead issue.
If you all do not deal with the conda package....who does? And why would there be any difference, as long as the TF version is the same?
		</comment>
		<comment id='8' author='dekromp' date='2020-04-28T16:33:51Z'>
		i see this with pip too. memory usage keeps increasing for fit.
		</comment>
		<comment id='9' author='dekromp' date='2020-04-28T16:41:49Z'>
		&lt;denchmark-link:https://github.com/ysyyork&gt;@ysyyork&lt;/denchmark-link&gt;
 Can you please create a new issue with a standalone code to reproduce the error? Thanks!
		</comment>
	</comments>
</bug>