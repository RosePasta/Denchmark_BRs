<bug id='28941' author='lioutasb' open_date='2019-05-22T18:52:44Z' closed_time='2019-06-05T21:01:36Z'>
	<summary>Can't use padded_batch on dataset with distributed strategy make_dataset_iterator</summary>
	<description>
I can't use padded_batch when I'm trying to create a distributed iterator using the following code.
&lt;denchmark-code&gt;import tensorflow_datasets as tfds
import tensorflow as tf

tf.enable_v2_behavior()

strategy = tf.distribute.MirroredStrategy()

def encode(lang1, lang2):
    lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(
        lang1.numpy()) + [tokenizer_pt.vocab_size+1]

    lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(
        lang2.numpy()) + [tokenizer_en.vocab_size+1]

    return lang1, lang2

def tf_encode(pt, en):
    return tf.py_function(encode, [pt, en], [tf.int64, tf.int64])

with strategy.scope():
    examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True, as_supervised=True)
    train_examples, test_examples = examples['train'], examples['test']

    tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus((en.numpy() for pt, en in train_examples), target_vocab_size=10000)

    tokenizer_pt = tfds.features.text.SubwordTextEncoder.build_from_corpus((pt.numpy() for pt, en in train_examples), target_vocab_size=10000)

    train_dataset = train_examples.map(tf_encode)
    train_dataset = train_dataset.shuffle(200000)
    train_dataset = train_dataset.padded_batch(64, padded_shapes=([-1], [-1]))
    train_dataset = strategy.make_dataset_iterator(train_dataset)
&lt;/denchmark-code&gt;

It throws this exception:
&lt;denchmark-code&gt;ValueError: Unable to get batched dataset from the input dataset. `batch` `map_and_batch` need to be the last operations on the dataset. The batch operations can be followed by a prefetch.
&lt;/denchmark-code&gt;

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
TensorFlow installed from (source or binary): Binary
TensorFlow version (use command below): 1.13
Python version: 3.7
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: CUDA 10.0/CuDNN 7.3.1
GPU model and memory: Nvidia Titan V

	</description>
	<comments>
		<comment id='1' author='lioutasb' date='2019-05-23T10:58:51Z'>
		I was able to get the mentioned error when tried on Colab with TensorFlow GPU version 1.13.
		</comment>
		<comment id='2' author='lioutasb' date='2019-06-05T20:35:39Z'>
		padded_batch should be supported. Can you try updating your TensorFlow version to 1.14 and try again?
		</comment>
		<comment id='3' author='lioutasb' date='2019-06-05T21:01:36Z'>
		&lt;denchmark-link:https://github.com/anj-s&gt;@anj-s&lt;/denchmark-link&gt;
 I confirm that the issue is fixed on 1.14. Thank you.
		</comment>
		<comment id='4' author='lioutasb' date='2019-06-05T21:01:37Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=28941&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=28941&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>